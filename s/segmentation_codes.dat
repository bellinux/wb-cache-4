0|101|Public
30|$|For our {{computation}} we {{used the}} inbuilt MATLAB Chan-Vese <b>segmentation</b> <b>code.</b>|$|R
40|$|International audiencePerceptual {{approaches}} {{have been widely}} used in many areas of visual information processing. This paper presents an overview of perceptual based approaches for image enhancement, <b>segmentation</b> and <b>coding.</b> The paper also provides a brief review of image quality assessment (IQA) methods, which are used to evaluate the performance of visual information processing techniques. The intent of this paper is not to review all the relevant works that have appeared in the literature, but rather to focus on few topics that have been extensively researched and developed over the past few decades. The goal is to present a perspective as broad as possible on this actively evolving domain due to relevant advances in vision research and signal processing. Therefore, for each topic, we identify the main contributions of perceptual approaches and their limitations, and outline how perceptual vision has influenced current state-of-the-art techniques in image enhancement, <b>segmentation,</b> <b>coding</b> and visual information quality assessment...|$|R
40|$|Cortical {{oscillations}} {{are likely}} candidates for <b>segmentation</b> and <b>coding</b> of continuous speech. Here, we monitored continuous speech processing with magnetoencephalography (MEG) {{to unravel the}} principles of speech <b>segmentation</b> and <b>coding.</b> We demonstrate that speech entrains the phase of low-frequency (delta, theta) and the amplitude of high-frequency (gamma) oscillations in the auditory cortex. Phase entrainment is stronger in the right and amplitude entrainment is stronger in the left auditory cortex. Furthermore, edges in the speech envelope phase reset auditory cortex oscillations thereby enhancing their entrainment to speech. This mechanism adapts to the changing physical features of the speech envelope and enables efficient, stimulus-specific speech sampling. Finally, we show that within the auditory cortex, coupling between delta, theta, and gamma oscillations increases following speech edges. Importantly, all couplings (i. e., brain-speech and also within the cortex) attenuate for backward-presented speech, suggesting top-down control. We conclude that <b>segmentation</b> and <b>coding</b> of speech relies on a nested hierarchy of entrained cortical oscillations...|$|R
40|$|We {{present a}} {{recurrent}} model for semantic instance segmentation that sequentially generates pairs of masks and their associated class probabilities for every object in an image. Our proposed system is trainable end-to-end, {{does not require}} post-processing steps on its output and is conceptually simpler than current methods relying on object proposals. We observe that our model learns to follow a consistent pattern to generate object sequences, which correlates with the activations learned in the encoder part of our network. We achieve competitive results on three different instance segmentation benchmarks (Pascal VOC 2012, Cityscapes and CVPPP Plant Leaf <b>Segmentation).</b> <b>Code</b> is available at [URL]...|$|R
40|$|Segmentation-based {{image and}} video coding is {{desirable}} for many multimedia applications {{due to the}} additional functionality provided by object-based rrresentation. Methods of object-based coding have generally treated the segmentation and encoding processes as separate problems. Here, we present an integrated <b>segmentation</b> and <b>coding</b> method unified by the theoretical structure of morphological local monotonicity...|$|R
40|$|Various {{papers on}} image {{compression}} and automatic target recognition are presented. Individual topics addressed include: target cluster detection in cluttered SAR imagery, model-based target recognition using laser radar imagery, Smart Sensor front-end processor for feature extraction of images, object attitude estimation and tracking {{from a single}} video sensor, symmetry detection in human vision, analysis of high resolution aerial images for object detection, obscured object recognition for an ATR application, neural networks for adaptive shape tracking, statistical mechanics and pattern recognition, detection of cylinders in aerial range images, moving object tracking using local windows, new transform method for image data compression, quad-tree product vector quantization of images, predictive trellis encoding of imagery, reduced generalized chain code for contour description, compact architecture for a real-time vision system, use of human visibility functions in <b>segmentation</b> <b>coding,</b> color texture analysis and synthesis using Gibbs random fields...|$|R
40|$|This paper {{presents}} a new compression technique for multispectral images. The proposed encoding algorithm {{is based on}} two steps: <b>segmentation</b> and transform <b>coding.</b> The <b>segmentation</b> step {{is based on a}} hierarchical tree-structured Markov random field model for the image, which is able to effectively take into account the spatial dependencies. After <b>segmentation,</b> class-adapted transform <b>coding</b> is used to decorrelate information both in the spectral and spatial domain. Simulation results show that the proposed technique exhibits a significant performance gain at very low bit rates, while assuring a satisfactory image quality...|$|R
40|$|Introduction In {{the shared}} task for CoNLL- 2000, words and tags form the basic multi-valued {{features}} for predicting a rich phrase <b>segmentation</b> <b>code.</b> While the tag features, containing WSJ part-ofspeech tags (Marcus et al., 1993), have about 45 values, the word features {{have more than}} 10, 000 values. In our study we have looked at how memory-based learning, as implemented in the TiMBL software system (Daelemans et al., 2000), can handle such features. We have limited our search to single classifiers, thereby explicitly ignoring the possibility to build a metalearning classifier architecture that {{could be expected to}} improve accuracy. Given this restriction we have explored the following: 1. The generalization accuracy of TiMBL with default settings (multi-valued features, overlap metric, feature weighting). 2. The usage of MVDM (Stanfill and Waltz, 1986; Cost and Salzberg, 1993) (Section 2), which should work well on word value pairs with a medium or high frequency, but may wo...|$|R
40|$|Biomedical Optics Laboratory <b>Segmentation</b> <b>Code</b> Directory Contents: Calibration Data True-positive and {{false-positive}} {{sets for}} neural network training. Kept for records. If {{applied to a}} new set of images, it is best to use your own training images for best results. Cropped Images for Hand Segmentation Hand Segmented Images, used for comparison as a standard. GMRF Gaussian Markov Random Field Segmentation Algorithm (Luck et al. 2005) HS_Script Outputs segmentation analysis for given image. Use HS_script. Image Model Image model generator. Use imodel. m to generate initial model, this creates a *. mat file that is used by imodel 2. m. Use imodel 2 to add contrast and noise, you must have a *. mat file generated or the script will fail. The image model generates a " 8 -bit" image in a 16 -bit container, so output may not be visible to the user (This allows for pixel values greater than 255). Overlap Compares segmentation results between two images. Defines object/pixel overlap. Use overlap. m SCM_GUI_V 2 _FINAL SCM segmentation algorithm. Use SCM_seg. m Training Images Training images at 4 depths...|$|R
5000|$|Compression based methods {{postulate}} {{that the}} optimal segmentation {{is the one}} that minimizes, over all possible <b>segmentations,</b> the <b>coding</b> length of the data. The connection between these two concepts is that segmentation tries to find patterns in an image and any regularity in the image can be used to compress it. The method describes each segment by its texture and boundary shape. Each of these components is modeled by a probability distribution function and its coding length is computed as follows: ...|$|R
40|$|This paper {{deals with}} a {{hierarchical}} morphological segmentation algorithm for image sequence coding. Mathematical morphology is very attractive for this purpose because it efficiently deals with geometrical features such as size, shape, contrast, or connectivity that {{can be considered as}} segmentation-oriented features. The algorithm follows a top-down procedure. It first takes into account the global information and produces a coarse segmentation, that is, with a small number of regions. Then, the segmentation quality is improved by introducing regions corresponding to more local information. The algorithm, considering sequences as being functions on a 3 -D space, directly segments 3 -D regions. A 3 -D approach is used to get a segmentation that is stable in time and to directly solve the region correspondence problem. Each segmentation stage relies on four basic steps: simplification, marker extraction, decision, and quality estimation. The simplification removes information from the sequence {{to make it easier to}} segment. Morphological filters based on partial reconstruction are proven to be very efficient for this purpose, especially in the case of sequences. The marker extraction identifies the presence of homogeneous 3 -D regions. It is based on constrained flat region labeling and morphological contrast extraction. The goal of the decision is to precisely locate the contours of regions detected by the marker extraction. This decision is performed by a modified watershed algorithm. Finally, the quality estimation concentrates on the coding residue, all the information about the 3 -D regions that have not been properly segmented and therefore coded. The procedure allows the introduction of the texture and contour coding schemes within the <b>segmentation</b> algorithm. The <b>coding</b> residue is transmitted to the next segmentation stage to improve the <b>segmentation</b> and <b>coding</b> quality. Finally, <b>segmentation</b> and <b>coding</b> examples are presented to show the validity and interest of the coding approach. Peer ReviewedPostprint (published version...|$|R
3000|$|In {{the present}} study we used V/UV <b>segmentation</b> (and adapted <b>coding),</b> but other <b>segmentation,</b> more adapted to concatenative synthesis, can be {{considered}} (e.g., [...] "CV" [...] or [...] "VCV"). Alternately, all voiced or all unvoiced (subsets of) units could be considered in synthesis system using the proposed method.|$|R
40|$|Flavor (Formal Language for Audio-Visual Object Representation) {{is a new}} {{programming}} language for media-intensive applications. It extends the typing system of C++ and Java to incorporate bitstream representation semantics. This allows describing in a single place both the in-memory representation of data {{as well as their}} bitstream-level (compressed) representation as well. In this paper, we present our approach of hybrid variable length <b>segmentation</b> in <b>code</b> generation for parsing entropy-coded data from the bitstream. We also show that this approach produces a very good tradeoff between space (memory required) and time (decoding speed) ...|$|R
40|$|In {{this short}} note we {{introduce}} ResearchDoom, an {{implementation of the}} Doom first-person shooter that can extract detailed metadata from the game. We also introduce the CocoDoom dataset, a collection of pre-recorded data extracted from Doom gaming sessions along with annotations in the MS Coco format. ResearchDoom and CocoDoom {{can be used to}} train and evaluate a variety of computer vision methods such as object recognition, detection and segmentation at the level of instances and categories, tracking, ego-motion estimation, monocular depth estimation and scene <b>segmentation.</b> The <b>code</b> and data are available at [URL]...|$|R
40|$|In object/region-based video coding, {{the side}} {{information}} is increased by the contour information. Thus, efficient lossy contour coding algorithms {{have to be}} developed. A class of very suitable motion segmentation algorithms provide a so-called layered representation of video sequences. In this paper an algorithm for <b>segmentation</b> map <b>coding</b> of layered object representations is proposed. It exploits the temporal connectivity of object masks by a transformation into an object domain. The contour coding {{is done in the}} object domain by a wavelet-based technique. Performance investigations show that very low bit-rates can be reached...|$|R
40|$|This {{publication}} {{presents an}} overview and {{an evaluation of}} low-level features characterizing the noiselike or tonelike nature of an audio signal. Such features are widely used for content classification, <b>segmentation,</b> identification, <b>coding</b> of audio signals, blind source separation, speech enhancement and voice activity detection. Besides the widely used Spectral Flatness Measure various alternative descriptors exist. These features are reviewed and the requirements for these features are discussed. The features in scope are evaluated using synthetic signals and exemplarily real-world applications related to audio content classification, namely voiced-unvoiced discrimination for speech signals and speech detection...|$|R
40|$|This paper {{deals with}} the {{relation}} between <b>segmentation</b> for <b>coding</b> and rate control. The efficiency of a segmentation-based coding scheme heavily relies on this step that defines how many and which regions have to be segmented. In this paper, we show that this problem can be formulated as a rate/distortion problem. The proposed solution not only controls the segmentation, but also defines the coding strategy {{to be used in}} each region. Together with the general approach, several simplified versions of the segmentation control are proposed and discussed. Peer ReviewedPostprint (published version...|$|R
40|$|Qualitative {{social and}} {{cultural}} research is increasingly engaging with visual data. Starting from the premise "all is data" in grounded theory methodology (GTM), we propose a general framework to realize a visual grounded theory methodology (VGTM). Referring to exploratory visual methods based on objective hermeneutics, the documentary method, and segment analysis, as well as existing GTM discourses, we discuss how this text-centered procedure {{can be applied to}} visual data. We focus on the (re) formulation of procedural steps (such as making an inventory, <b>segmentation</b> and <b>coding,</b> memo writing, and sampling strategies), and the examination of images in relation to GTM logic...|$|R
40|$|In this survey, {{the author}} {{will provide a}} brief review of some of the recent and {{relevant}} contributions to the field of automated address reading for Postal Service applications. Literatures in the area of character and zip <b>code</b> <b>segmentation,</b> character recognition, and contextual processing are surveyed. Critical issues and future tasks are identified and discussed based on the survey...|$|R
30|$|The {{motivation}} {{of this paper}} is to provide an overview of the most recent trends and of the future research directions in color image and video processing. Rather than covering all aspects of the domain this survey covers issues related to the most active research areas in the last two years. It presents the most recent trends as well as the state-of-the-art, with a broad survey of the relevant literature, in the main active research areas in color imaging. It also focuses on the most promising research areas in color imaging science. This survey gives an overview about the issues, controversies, and problems of color image science. It focuses on human color vision, perception, and interpretation. It focuses also on acquisition systems, consumer imaging applications, and medical imaging applications. Next it gives a brief overview about the solutions, recommendations, most recent trends, and future trends of color image science. It focuses on color space, appearance models, color difference metrics, and color saliency. It focuses also on color features, color-based object tracking, scene illuminant estimation and color constancy, quality assessment and fidelity assessment, color characterization and calibration of a display device. It focuses on quantization, filtering and enhancement, <b>segmentation,</b> <b>coding</b> and compression, watermarking, and lastly on multispectral color image processing. Lastly, it addresses the research areas which still need addressing and which are the next and future perspectives of color in image and video processing.|$|R
40|$|We {{describe}} {{a method for}} the automatic mapping of coronal holes (CH) using simultaneous multi-instrument EUV imaging data. Synchronized EUV images from STEREO/EUVI A&B 195 A and SDO/AIA 193 A are preprocessed, including PSF deconvolution {{and the application of}} data-derived intensity corrections that account for center-to-limb variations (limb brightening) and inter-instrument intensity normalization. We systematically derive a robust limb-brightening correction that takes advantage of unbiased long-term averages of data and respects the physical nature of the problem. The new preprocessing greatly assists in CH detection, allowing for the use of a simplified variable-connectivity two-threshold region growing image segmentation algorithm to obtain consistent detection results. We generate synchronic EUV and CH maps, and show a preliminary analysis of CH evolution. Several data and code products are made available to the community (www. predsci. com/chd) : For the period of this study (06 / 10 / 2010 to 08 / 18 / 14) we provide synchronic EUV and coronal hole map data at 6 -hour cadence, data-derived limb-brightening corrections for STEREO/EUVI A&B 195 A and SDO/AIA 193 A, and inter-instrument correction factors to equate their intensities. We also provide the coronal hole image <b>segmentation</b> <b>code</b> module (ezseg) implemented in both FORTRAN OpenMP and GPU-accelerated C-CUDA. A complete implementation of our coronal hole detection pipeline {{in the form of a}} ready-to-use MATLAB driver script euv 2 chm utilizing ezseg is also made available. Comment: 37 pages, 24 figure...|$|R
40|$|International audienceThe {{motivation}} {{of this paper}} is to provide an overview of the most recent trends and of the future research directions in color image and video processing. Rather than covering all aspects of the domain this survey covers issues related to the most active research areas in the last two years. It presents the most recent trends as well as the state-of-the-art, with a broad survey of the relevant literature, in the main active research areas in color imaging. It also focuses on the most promising research areas in color imaging science. This survey gives an overview about the issues, controversies, and problems of color image science. It focuses on human color vision, perception and interpretation. It focuses also on acquisition systems, consumer imaging applications, and medical imaging applications. Next it gives a brief overview about the solutions, recommendations, most recent trends and future trends of color image science. It focuses on color space, appearance models, color difference metrics and color saliency. It focuses also on color features, color-based object tracking, scene illuminant estimation and color constancy, quality assessment and fidelity assessment, color characterization and calibration of a display device. It focuses on quantization, filtering and enhancement, <b>segmentation,</b> <b>coding</b> and compression, watermarking, and lastly on multispectral color image processing. Lastly, it addresses the research areas which still need addressing and which are the next and future perspectives of color in image and video processing...|$|R
40|$|This paper {{deals with}} the use of some {{morphological}} tools for image and video coding. Mathematical morphology can be considered as a shape-oriented approach to signal processing, and some of its features make it very useful for compression. Rather than describing a coding algorithm, {{the purpose of this paper}} is to describe some morphological tools that have proved attractive for compression. Four sets of morphological transformations are presented: connected operators, the region-growing version of the watershed, the geodesic skeleton, and a morphological interpolation technique. The authors discuss their implementation, and show how they can be used for image and video <b>segmentation,</b> contour <b>coding,</b> and texture coding. Peer ReviewedPostprint (published version...|$|R
40|$|In this implementation, {{content is}} encoded in an image {{sequence}} by implicitly <b>coding</b> <b>segmentation</b> {{information in the}} rearranging of the look-up-table. Context is encoded by links between sequences. The idea of content-based representations is defended by claiming that content is already inherently interwoven into representations. A discussion is made also about {{the reading of the}} image in the framework of an interactive movie...|$|R
40|$|Abstract—In this paper, we {{describe}} an object-based video compression {{scheme based on}} the derivation and efficient coding of motion boundaries. First, we recursively identify {{a small number of}} global movement classes, each represented by two or more motion parameters. Second, we assign regions of a spatial segmentation to movement classes. Third, we merge segments using various similarity heuristics, while adding movement classes for small objects, if necessary. Finally, the boundaries of motion are coded using an efficient asymmetric binary tree (ABT) coding scheme. Experimental results on standard test sequences qualitatively show that the proposed algorithm gives good segmentation, and that it is suitable for very-low-data-rate object-based coding. Index Terms—Motion estimation, motion <b>segmentation,</b> video <b>coding...</b>|$|R
40|$|This paper {{presents}} an algorithm for segmentation of image sequences where especially aspects for object oriented coding {{are taken into}} account. A fundamental requirement of such applications is the temporal stability of the segmentation. This is improved in this article compared to other existing approaches by including motion estimation into the segmentation process. Additionally a hierarchical approach enables an efficient predictive coding {{on one hand and}} a semantic data access on the other hand. As a direct result from using full colour information for the <b>segmentation</b> process, <b>coding</b> of the chrominance information can be done with extremely high compression ratios. Thus there is nearly no extra coding effort for colour images compared to greyscale images...|$|R
40|$|This article {{presents}} a complete still image coder for gray scale images. The coder {{is based on}} segmenting the image into homogeneous objects which are coded independently. The coder consist of three parts: <b>segmentation,</b> edge <b>coding</b> and texture <b>coding.</b> The <b>segmentation</b> is done by using mathematical morphology. The segments are then coded by representing the edges between segments by a chain code, while the texture within each segment is coded in two steps. A first approximation of the the image texture is made by fitting a second order surface to each object. The error surface is then coded by AR modelling using the analysis-by-synthesis principle. Best results are obtained for head and shoulder images...|$|R
30|$|Lastly, Mbogo et al. (2016) {{conducted}} a 2 -hr-long qualitative study where their mobile learning system provided code writing exercises in Java by designing static scaffolding. The learners {{were able to}} collapse parts of the code and toggle between collapsed and full versions of their programs to support Java code writing on a smartphone. Participants remarked that the scaffolding for <b>code</b> <b>segmentations</b> made the application easier to use.|$|R
40|$|COBOLISRE is a sofhvare {{re-engineering}} and renova-tion {{environment for}} COBOL systems. It supports {{a wide range}} of features such as system-level analysis and brows-ing, program-level analysis and browsing, data model recovery, concept recognition, and <b>code</b> <b>segmentation.</b> COBOLISRE is implemented on top of a distributed execu-tion architecture to address issues of multi-user access, performance, and openness. This paper is an overview of the major features of COBOLISRE and its underlying architecture. 1...|$|R
40|$|International Telemetering Conference Proceedings / October 23 - 26, 2000 / Town & Country Hotel and Conference Center, San Diego, CaliforniaAfter a slow start, the CCSDS Telecommand Recommendation {{is finally}} being embraced {{by a large}} number of NASA, ESA and NASDA space missions. Even some {{commercial}} satellites are exploring the possibility of using this advanced protocol. The CCSDS Telecommand is a closed-loop space communication protocol that offers its users a guaranteed data delivery service, which is essential for the satellite control operations. This paper describes a commercial product that supports the CCSDS Telecommand protocol. This product provides Telecommand uplink <b>segmentation,</b> transfer, <b>coding</b> and physical layer services and Command Operations Procedures (COP). Optionally, it provides corresponding functions at the receiving end for command link verification...|$|R
40|$|Convolutional neural {{networks}} (CNNs) are inherently limited to model geometric transformations {{due to the}} fixed geometric structures in its building modules. In this work, we introduce two new modules to enhance the transformation modeling capacity of CNNs, namely, deformable convolution and deformable RoI pooling. Both {{are based on the}} idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the effectiveness of our approach on sophisticated vision tasks of object detection and semantic <b>segmentation.</b> The <b>code</b> would be released...|$|R
40|$|A novel {{approach}} {{is presented in}} this paper for improving the performance of neural network classifiers in image recognition, <b>segmentation</b> or <b>coding</b> applications, based on a retraining procedure at the user level. The procedure includes a maximum a posteriori (MAP) estimation technique for optimally selecting a retraining data set from the image applied to the network during real life operation, a decision mechanism for automatic activation of network retraining and a neural network module which performs the classification task. The extracted feature set, used for retraining the network, can include additional elements compared to those used in the network initial training phase, so that it better fits the specific application data under consideration. Results are presented which illustrate the theoretical developments as well as the performance of the proposed approach in real life experiments 1...|$|R
40|$|Abstract: In this paper, {{we propose}} an {{efficient}} no-reference perceptual blockiness estimation method based on local features and <b>segmentation</b> for JPEG <b>coded</b> images that can automatically quantify perceptual blocking artifacts. We believe that perceptual blockiness of any image {{is strongly dependent on}} local {{features such as}} edge, flat and texture. Therefore, edge, flat, and texture based blockiness are evaluated in this method. Experimental results on LIVE database show that proposed method has sufficient correlation with subjective evaluation scores...|$|R
30|$|Moreover, due to {{the limited}} number of {{reference}} frames (the maximum is five in practical implementations), uncovered background may not be encoded efficiently using the existing techniques. Some algorithms[15 – 18] determined and exploited uncovered background using pre- and/or post-processing and computationally expensive video <b>segmentation</b> for <b>coding.</b> Uncovered background can also efficiently be encoded using sprite <b>coding</b> through object <b>segmentation.</b> Most of the video coding applications could not tolerate inaccurate video/object segmentations and expensive computational complexity incurred by segmentation algorithms. Ding et al.[18] used a background-frame for video coding. The background frame is made up of blocks which keep unchanged (based on the zero motion vector) in a certain number of continuous frames. Due to the dependency on block-based motion vectors and lack of adaptability in multi-modal backgrounds for dynamic environment, this background frame could not perform well.|$|R
40|$|We {{present a}} new image {{compression}} technique called "DjVu " that is specifically geared towards the compression of high-resolution, high-quality images of scanned documents in color. With DjVu, any screen connected to the Internet can access and display images of scanned pages while faithfully reproducing the font, color, drawing, pictures, and paper texture. A typical magazine page in color at 300 dpi can be compressed down to between 40 to 60 KB, approximately 5 to 10 times better than JPEG for a similar level of subjective quality. B&W documents are typically 15 to 30 KBytes at 300 dpi, or 4 to 8 times better than CCITT-G 4. A real-time, memory efficient version of the decoder was implemented, and is available as a plug-in for popular web browsers. Keywords: digital libraries, image compression, image <b>segmentation,</b> arithmetic <b>coding,</b> wavelet coding, JBIG 2...|$|R
40|$|An {{algorithm}} for the segmentation {{of image}} sequences is presented, {{taking into account}} especially aspects for object oriented coding. A fundamental requirement of such applications is the temporal stability of the segmentation. This is improved in this article compared to other existing approaches by including motion estimation into the segmentation process. Additionally a hierarchical approach enables an efficient predictive coding {{on one hand and}} a semantic data access on the other hand. As a direct result from using full colour information for the <b>segmentation</b> process, <b>coding</b> of the chrominance information can be done with extremely high compression ratios. Instead of full resolution chrominance information only few mean chrominance vectors need to be transferred (which corresponds to a compression factor of about 1000). Additionally object shapes must be coded but this has to be done for greyscale images anyway...|$|R
