3|1218|Public
50|$|A SIMM, or <b>single</b> <b>in-line</b> <b>memory</b> <b>module,</b> {{is a type}} {{of memory}} module {{containing}} random-access memory used in computers from the early 1980s to the late 1990s. It differs from a dual in-line memory module (DIMM), the most predominant form of memory module today, in that the contacts on a SIMM are redundant {{on both sides of the}} module. SIMMs were standardised under the JEDEC JESD-21C standard.|$|E
5000|$|Memory is {{provided}} by the MC3 memory board, which contains thirty-two <b>single</b> <b>in-line</b> <b>memory</b> <b>module</b> (SIMM) slots and two leaf controllers. Fast page mode (FPM) error correcting code (ECC) SIMMs with capacities of 16 MB (known as the [...] "high-density" [...] SIMM) and 64 MB (known as the [...] "super-density" [...] SIMM) are supported, enables the board to provide 64 MB to 2 GB of memory. The SIMMs are installed in groups of four.|$|E
50|$|Kingston Technology {{was founded}} on October 17, 1987, in {{response}} to a severe shortage of 1Mbit surface-mount memory chips. In response, Taiwanese immigrant John Tu designed a new <b>single</b> <b>in-line</b> <b>memory</b> <b>module</b> (SIMM) that used readily available, older-technology through-hole components. In 1990 following year the company branched out into its first non-memory product line, processor upgrades. By 1992, the firm was ranked #1 by Inc. as the fastest-growing privately held company in America. The company expanded into networking and storage product lines, and introduced DataTraveler and DataPak portable products. In September 1994, Kingston became ISO 9000 certified on its first assessment attempt.|$|E
25|$|In August 1991, Wang won a {{suit against}} NEC and Toshiba {{claiming}} violation of Wang's patents on <b>single</b> <b>in-line</b> <b>memory</b> <b>modules</b> (SIMMs). The company still recorded a net loss for the 1991 fiscal year.|$|R
50|$|A DIMM or dual <b>in-line</b> <b>memory</b> <b>module</b> {{comprises}} {{a series}} of dynamic random-access memory integrated circuits. These modules are mounted on {{a printed circuit board}} and designed for use in personal computers, workstations and servers. DIMMs began to replace SIMMs (<b>single</b> <b>in-line</b> <b>memory</b> <b>modules)</b> as the predominant type of <b>memory</b> <b>module</b> as Intel P5-based Pentium processors began to gain market share.|$|R
40|$|This paper {{describes}} {{the results of}} testing 50 <b>single</b> <b>in-line</b> <b>memory</b> <b>modules</b> (SIMMs), each containing 16 16 Mbit DRAM chips (DUTs); 39 SIMMs failed, and of the 800 DUTs, 116 failed. In total 54 different test algorithms have been applied, using up to 168 different stress combinations for each test. The results show that GAL 9 R is the best test. Furthermore, it is shown that burst mode tests detect a com-pletely different class of faults as compared with traditional word mode tests, and that tests with address scrambling en-abled detect more faults...|$|R
5000|$|Rambus <b>In-line</b> <b>Memory</b> <b>Module</b> (RIMM), {{technically}} DIMMs but called RIMMs due {{to their}} proprietary slot.|$|R
5000|$|In 2012 Everspin Technologies {{released}} the first commercially available DDR3 dual <b>in-line</b> <b>memory</b> <b>module</b> ST-MRAM {{which has a}} capacity of 64 Mb.|$|R
50|$|A SO-DIMM, SODIMM, {{or small}} outline dual <b>in-line</b> <b>memory</b> <b>module,</b> {{is a type}} of {{computer}} memory built using integrated circuits. SO-DIMMs are a smaller alternative to a DIMM, being roughly half the size of regular DIMMs.|$|R
5000|$|HCDIMM is a 240-pin, 72bit-wide, Load Reduced, DDR3 SDRAM Dual <b>In-Line</b> <b>Memory</b> <b>Module</b> (DIMM). According to Cirrascale, [...] "while LRDIMM {{requires}} a special BIOS configuration, HyperCloud provides seamless plug-and-play operation with past, {{current and future}} generations of Intel processors." ...|$|R
50|$|HyperCloud Memory (HCDIMM) is a DDR3 SDRAM Dual <b>In-Line</b> <b>Memory</b> <b>Module</b> (DIMM) used in server {{applications}} {{requiring a}} great deal of memory. It was initially launched in 2009 at the International Supercomputing Conference by Irvine, California based company, Netlist Inc. It was never a JEDEC standard, and the main server vendors supporting it was IBM and HPE.|$|R
50|$|Registered (also called buffered) <b>memory</b> <b>modules</b> have a {{register}} between the DRAM modules and the system's memory controller. They place less electrical {{load on the}} memory controller and allow single systems to remain stable with more <b>memory</b> <b>modules</b> {{than they would have}} otherwise. When compared with registered memory, conventional memory is usually referred to as unbuffered memory or unregistered memory. When manufactured as a dual <b>in-line</b> <b>memory</b> <b>module</b> (DIMM), {{a register}}ed <b>memory</b> <b>module</b> is called an RDIMM, while unregistered memory is called UDIMM.|$|R
5000|$|A Non-Volatile Dual <b>In-line</b> <b>Memory</b> <b>Module</b> (NVDIMM) is a random-access {{memory for}} computers. [...] "Non-volatile" [...] {{means that the}} memory retains its {{contents}} even when electrical power is removed either from an unexpected power loss, system crash, or normal shutdown. [...] "Dual in-line" [...] identifies the product as using the DIMM package. NVDIMMs improve application performance, data security, and system crash recovery time. They enhance SSD endurance and reliability.|$|R
50|$|The first PC {{motherboards}} {{with support}} for RDRAM debuted in late 1999, after two major delays. RDRAM was controversial during its widespread use by Intel for having high licensing fees, high cost, being a proprietary standard, and low performance advantages {{for the increased}} cost. RDRAM and DDR SDRAM {{were involved in a}} standards war. PC-800 RDRAM operated at 400 MHz and delivered 1600 MB/s of bandwidth over a 16-bit bus. It was packaged as a 184-pin RIMM (Rambus <b>in-line</b> <b>memory</b> <b>module)</b> form factor, similar to a DIMM (dual <b>in-line</b> <b>memory</b> <b>module).</b> Data is transferred on both the rising and falling edges of the clock signal, a technique known as DDR. To emphasize the advantages of the DDR technique, this type of RAM was marketed at speeds twice the actual clock rate, i.e. the 400 MHz Rambus standard was named PC-800. This was significantly faster than the previous standard, PC-133 SDRAM, which operated at 133 MHz and delivered 1066 MB/s of bandwidth over a 64-bit bus using a 168-pin DIMM form factor.|$|R
2500|$|On 10 August 2017, Microsoft {{announced}} a [...] edition {{to be made}} available in September, along with the Fall Creators Update for Windows 10. This edition is designed for high-end hardware for intensive computing tasks and supports Intel Xeon or AMD Opteron processors, up to 4 CPUs, up to 6TB RAM, the ReFS file system, Non-Volatile Dual <b>In-line</b> <b>Memory</b> <b>Module</b> (NVDIMM) and remote direct memory access (RDMA). The announcement included no licensing details.|$|R
50|$|UniDIMM (short for Universal DIMM) is a {{specification}} for dual <b>in-line</b> <b>memory</b> <b>modules</b> (DIMMs), {{which are}} {{printed circuit boards}} (PCBs) designed to carry dynamic random-access memory (DRAM) chips. UniDIMMs can be populated with either DDR3 or DDR4 chips, with no support for any additional memory control logic; as a result, the computer's memory controller must support both DDR3 and DDR4 memory standards. The UniDIMM specification was created by Intel for its Skylake microarchitecture, whose integrated memory controller (IMC) supports both DDR3 (more specifically, the DDR3L low-voltage variant) and DDR4 memory technologies.|$|R
500|$|Since {{the release}} of Ivy Bridge microarchitecture, Intel Xeon {{processors}} support the so-called pseudo target row refresh (pTRR) {{that can be used}} in combination with pTRR-compliant DDR3 dual <b>in-line</b> <b>memory</b> <b>modules</b> (DIMMs) to mitigate the row hammer effect by automatically refreshing possible victim rows, with no negative impacts on performance or power consumption. [...] When used with DIMMs that are not pTRR-compliant, these Xeon processors by default fall back on performing DRAM refreshes at twice the usual frequency, which results in slightly higher memory access latency and may reduce the memory bandwidth by up to 24%.|$|R
2500|$|DDR4 {{memory is}} {{supplied}} in 288-pin dual <b>in-line</b> <b>memory</b> <b>modules</b> (DIMMs), similar {{in size to}} 240-pin DDR3 DIMMs. The pins are spaced more closely (0.85mm instead of 1.0) to fit the increased number within the same 5¼ inch (...) standard DIMM length, but the height is increased slightly ( [...] instead of [...] ) to make signal routing easier, and the thickness is also increased (to 1.2mm from 1.0) to accommodate more signal layers. [...] DDR4 DIMM modules have a slightly curved edge connector so {{not all of the}} pins are engaged at the same time during module insertion, lowering the insertion force.|$|R
40|$|Abstract—The dual <b>in-line</b> <b>memory</b> <b>module</b> (DIMM) tree {{architecture}} {{was proposed}} to solve signal integrity and data access latency problems of many-DIMM system. Although the DIMM tree demands a memory controller specific to it, {{there has been}} little research on the memory controller for the DIMM tree. For this reason, this paper proposes a new memory controller architecture for the DIMM tree. This architecture was modeled using DRAMSim 2 for its verification and analysis, and the experimental results show that the proposed DIMM tree memory controller works properly and efficiently. Keywords-DIMM tree; many-DIMM system; DIMM to DIMM transfer; memory controller I...|$|R
50|$|Since {{the release}} of Ivy Bridge microarchitecture, Intel Xeon {{processors}} support the so-called pseudo target row refresh (pTRR) {{that can be used}} in combination with pTRR-compliant DDR3 dual <b>in-line</b> <b>memory</b> <b>modules</b> (DIMMs) to mitigate the row hammer effect by automatically refreshing possible victim rows, with no negative impacts on performance or power consumption. When used with DIMMs that are not pTRR-compliant, these Xeon processors by default fall back on performing DRAM refreshes at twice the usual frequency, which results in slightly higher memory access latency and may reduce the memory bandwidth by up to 2 - 4%.|$|R
40|$|A {{reconfigurable}} computing {{development environment}} called Pilchard, employing a field {{programmable gate array}} (FPGA) which plugs into a standard personal computer's (PC) 133 MHz synchronous dynamic RAM Dual <b>In-line</b> <b>Memory</b> <b>Modules</b> (DIMMs) slot is presented. Compared with a traditional PCI interfaced reconfigurable computing board, the DIMM interface offers higher bandwidth, a simpler interface and lower latency. A comparison of the transfer rate of the Pilchard board compared with a standard PCI 32 reconfigurable computing board is presented as well as an implementation of the data encryption standard (DES). Together, the board and interface generator provide an easy to use, low cost and high performance platform for reconfigurable computing. ...|$|R
5000|$|DDR4 {{memory is}} {{supplied}} in 288-pin dual <b>in-line</b> <b>memory</b> <b>modules</b> (DIMMs), similar {{in size to}} 240-pin DDR3 DIMMs. The pins are spaced more closely (0.85 mm instead of 1.0) to fit the increased number within the same 5¼ inch (5+1/4 in) standard DIMM length, but the height is increased slightly (31.25 mm instead of 30.35 mm) to make signal routing easier, and the thickness is also increased (to 1.2 mm from 1.0) to accommodate more signal layers. [...] DDR4 DIMM modules have a slightly curved edge connector so {{not all of the}} pins are engaged at the same time during module insertion, lowering the insertion force.|$|R
25|$|DDR SDRAM modules for desktop computers, dual <b>in-line</b> <b>memory</b> <b>modules</b> (DIMMs), have 184 pins (as {{opposed to}} 168 pins on SDRAM, or 240 pins on DDR2 SDRAM), {{and can be}} {{differentiated}} from SDRAM DIMMs {{by the number of}} notches (DDR SDRAM has one, SDRAM has two). DDR SDRAM for notebook computers, SO-DIMMs, have 200 pins, which is the same number of pins as DDR2 SO-DIMMs. These two specifications are notched very similarly and care must be taken during insertion if unsure of a correct match. Most DDR SDRAM operates at a voltage of 2.5V, compared to 3.3V for SDRAM. This can significantly reduce power consumption. Chips and modules with DDR-400/PC-3200 standard have a nominal voltage of 2.6V.|$|R
50|$|Fast Cycle DRAM (FCRAM) {{is a type}} of {{synchronous}} dynamic random-access memory {{developed by}} Fujitsu and Toshiba. FCRAM has a shorter data access latency compared to contemporary commodity SDRAMs; and is used in where the lower data access latency is more desirable than low cost and high capacity (FCRAM is a moderate cost and capacity speciality DRAM). FCRAM achieves its low latency by dividing each row into multiple sub-rows, only which one is activated during a row-activation operation. This had the effect of reducing the effective array size, improving the access time. FCRAM has a DDR SDRAM-like command set to enable memory controllers that support both DDR SDRAM and FCRAM. It also has a standard dual <b>in-line</b> <b>memory</b> <b>module</b> (DIMM).|$|R
50|$|DDR SDRAM modules for desktop computers, dual <b>in-line</b> <b>memory</b> <b>modules</b> (DIMMs), have 184 pins (as {{opposed to}} 168 pins on SDRAM, or 240 pins on DDR2 SDRAM), {{and can be}} {{differentiated}} from SDRAM DIMMs {{by the number of}} notches (DDR SDRAM has one, SDRAM has two). DDR SDRAM for notebook computers, SO-DIMMs, have 200 pins, which is the same number of pins as DDR2 SO-DIMMs. These two specifications are notched very similarly and care must be taken during insertion if unsure of a correct match. Most DDR SDRAM operates at a voltage of 2.5 V, compared to 3.3 V for SDRAM. This can significantly reduce power consumption. Chips and modules with DDR-400/PC-3200 standard have a nominal voltage of 2.6 V.|$|R
40|$|Recently, NVDIMM (Non-Volatile Dual <b>In-line</b> <b>Memory</b> <b>Module)</b> {{is being}} widely {{supported}} by leading hardware design companies, such as IBM. Nevertheless, existing efforts largely focus on NVDIMM specification and fabrication issues, {{and the potential}} performance gains brought by NVDIMM are not fully investigated. In this paper, we present a NVDIMM-based simulator called MCSSim to help study the memory channel storage techniques. MCSSim is a cycle-accurate simulator that is elaborated with the consideration of differences between the memory channel interface and the NAND flash memory features. MCSSim is also implemented with the DRAMSim 2 [31] simulator thus enabling the simulation {{of a variety of}} hybrid memory systems by combining of DRAM DIMM and NVDIMM. We have done some experiments with MCSSim, and the experimental results show the effectiveness of the proposed simulator. Department of Computin...|$|R
50|$|Two {{types of}} node, {{processor}} and memory, were contained within a blade. Compute blades contain a processor node and consist of two PAC611 sockets for Itanium 2 and Itanium microprocessors, a Super-Hub (SHub) {{application-specific integrated circuit}} (ASIC) (chipset) and eight dual <b>in-line</b> <b>memory</b> <b>module</b> (DIMM) slots for memory. The number of microprocessor sockets in a compute blade is one or two. One-processor socket configurations provide more bandwidth as only one microprocessor socket is using the front side bus and local memory. Two-processor socket configurations do not support hyperthreading. Memory blades are used to expand the amount of memory without {{increasing the number of}} processors. They contain a SHub ASIC and 12 DIMM slots. Both compute and memory blades support 1, 2 4, 8, and 16 GB DIMMs. SGI support does not currently support any installations with 16GB DIMMs.|$|R
40|$|Natural {{convection}} for Dual <b>In-Line</b> <b>Memory</b> <b>Module</b> (DIMM) systems, disposed {{as predicted}} by the recent Balanced Technology Extended (BTX) form factor in tower configuration, is numerically studied in this article. The considered physical system is modelled by horizontal air-filled layers bounded by parallel walls in which multiple heat sources are arranged. Three-dimensional simulations are carried-out by using a multi-physical FEM software. The results, obtained for imposed ambient temperature and operative conditions (power supplied to memories), show as thermoconvective instabilities may be produced and consequently complex fluid motion field could be detected. Simulated temperature fields show good agreement with thermal design data proposed by DIMM leading constructors. In order to improve computational performance of the numerical model, a simplified geometry is also proposed and tested for solving the physical problem. The present study contributes in investigation on critical cooling conditions for BTX form factor and in innovative projects of fan-less computer architecture...|$|R
50|$|Most laptops use SO-DIMM (small outline dual <b>in-line</b> <b>memory</b> <b>module)</b> <b>memory</b> <b>modules,</b> as {{they are}} about {{half the size of}} desktop DIMMs. They are {{sometimes}} accessible {{from the bottom of the}} laptop for ease of upgrading, or placed in locations not intended for user replacement. Most laptops have two memory slots, although some of the lowest-end models will have only one, and some high end models (usually mobile engineering workstations and a few high-end models intended for gaming) have four slots. Most mid-range laptops are factory equipped with 4-6 GB of RAM. Netbooks are commonly equipped with only 1-2 GB of RAM and are generally only expandable to 2 GB, if at all. Due to the limitation of DDR3 SO-DIMM of a maximum of 8 GB per module, most laptops can only be expanded to a total of 16 GB of memory, until systems using DDR4 memory start becoming available. Laptops may have memory soldered to the motherboard to conserve space, which allows the laptop to have a thinner chassis design. Soldered memory cannot be upgraded.|$|R
40|$|Measurements of the curvatures and warpages of {{a printed}} circuit board (PCB) during a thermal solder reflow process using strain gauges are {{proposed}} in this study. In the experiments, a shadow moiré is used for measuring the out-of-plane deformations (or warpage) of a bi-material plate and a PCB with dual <b>in-line</b> <b>memory</b> <b>module</b> (DIMM) sockets during solder reflow heating, while the finite element method (FEM) is used to analyze the thermally-induced deformation of the PCB specimen for ensuring the validity of the measurement. Conventional strain gauges are employed to measure the strains (albeit as in-plane strain data) in both specimens during the solder reflow process. The results indicate that the strain gauge-measured strain data from the top and bottom surfaces of both specimens during the solder reflow can be converted into curvature data with specific equations, and even into global out-of-plane deformations or warpages with a proposed simple beam model. Such results are also consistent with those from the shadow moiré and FEM. Therefore, it has been proved that the strain gauge measurement associated with the simple beam model can provide a method for the real-time monitoring of PCB deformations or warpages with different temperatures during the solder reflow process...|$|R
40|$|Department of Electrical EngineeringAs {{the high}} {{performance}} very-large-scale integration (VLSI) systems operate with high speed and low voltage, the system-level electrostatic discharge (ESD) event is {{becoming one of}} the important noise sources causing logic errors and system malfunctions such as system reboot or fault. To understand the ESD noise phenomena and improve the system-level ESD noise immunity for devices, the accurate ESD noise measurement and analysis of IC logic errors are necessary. Section I is written for the tendency of ESD research and previous research. This paper presents the noise type correlation by measuring the signal-ground noise and power-ground noise simultaneously on the fundamental F/F operation circuit and shows the type of error from chip, in section II. Furthermore, the decoupling capacitors (de-cap) effect that can reduce the error occurrence by checking the error rate are analyzed. A generator is designed on the main board which is based on real operating laptop, and the chip on dual <b>in-line</b> <b>memory</b> <b>module</b> (DIMM) is also designed to perform the basic F/F operation. The clock and data input from generator are connected to the chip on the DIMM through the small outline dual <b>in-line</b> <b>memory</b> <b>module</b> (SODIMM) socket. ESD occurs {{at the corner of the}} ground plane of main board. The specification of the ESD generator satisfies IEC 61000 - 4 - 2 [1]. The ESD current flows along the ground strap, and affects the DIMM. IN-ground, CLK-ground, OUT-ground and power-ground on the DIMM are simultaneously measured to determine the effect of ESD on the main board. To analyze the error ratio according to the ESD voltage level, the voltage setup of the ESD gun is 3 kV, 5 kV and 8 kV. To investigate the effects of chip shielding and DIMM de-caps on the error probability of DIMM, the experiment is conducted under the several conditions. After confirming the normal operation for each condition, the error type on the DIMM due to the ESD occurred in the circuit is analyzed and the statistics are shown. The results are verified by H-spice simulation, Vector Network Analyzer (VNA) and HFSS simulation. In order to obtain the improvement method of the DIMM immunity, experiments are conducted to find out the effective position and number of DIMM de-cap. Accurate measurements of electromagnetic fields are also essential to analyze the radiated noise due to unwanted electrostatic discharge (ESD) events at electronic devices. Usually, to know the radiated noise by ESD events, the voltages induced at field probes are measured, and the fields are obtained from the voltage by de-convolving the probe factor. In section Ⅲ, the two probe-factor deconvolution methods are investigated and compared in the measurements of the fields induced by system-level ESD events. clos...|$|R
40|$|An {{important}} aspect of High-Performance Computing (HPC) system design is the choice of main memory capacity. This choice becomes increasingly important now that 3 D-stacked memories are entering the market. Compared with conventional Dual <b>In-line</b> <b>Memory</b> <b>Modules</b> (DIMMs), 3 D memory chiplets provide better performance and energy efficiency but lower memory capacities. Therefore, the adoption of 3 D-stacked memories in the HPC domain depends on whether we can find use cases that require much less memory than is available now. This study analyzes the memory capacity requirements of important HPC benchmarks and applications. We find that the High-Performance Conjugate Gradients (HPCG) benchmark could be an important success story for 3 D-stacked memories in HPC, but High-Performance Linpack (HPL) {{is likely to be}} constrained by 3 D memory capacity. The study also emphasizes that the analysis of memory footprints of production HPC applications is complex and that it requires an understanding of application scalability and target category, i. e., whether the users target capability or capacity computing. The results show that most of the HPC applications under study have per-core memory footprints in the range of hundreds of megabytes, but we also detect applications and use cases that require gigabytes per core. Overall, the study identifies the HPC applications and use cases with memory footprints that could be provided by 3 D-stacked memory chiplets, making a first step toward adoption of this novel technology in the HPC domain. This work was supported by the Collaboration Agreement between Samsung Electronics Co., Ltd. and BSC, Spanish Government through Severo Ochoa programme (SEV- 2015 - 0493), by the Spanish Ministry of Science and Technology through TIN 2015 - 65316 -P project and by the Generalitat de Catalunya (contracts 2014 -SGR- 1051 and 2014 -SGR- 1272). This work has also received funding from the European Union’s Horizon 2020 research and innovation programme under ExaNoDe project (grant agreement No 671578). Darko Zivanovic holds the Severo Ochoa grant (SVP- 2014 - 068501) of the Ministry of Economy and Competitiveness of Spain. The authors thank Harald Servat from BSC and Vladimir Marjanovi´c from High Performance Computing Center Stuttgart for their technical support. Postprint (published version...|$|R
50|$|A SIPP (<b>single</b> <b>in-line</b> pin package) or SIP (<b>single</b> <b>in-line</b> package) was a {{short-lived}} {{variant of the}} 30-pin SIMM random-access memory.|$|R
50|$|A <b>single</b> <b>in-line</b> (pin) package (SIP or SIPP) has one row of {{connecting}} pins. It {{is not as}} popular as the DIP, but {{has been used for}} packaging RAM chips and multiple resistors with a common pin. SIPs group RAM chips together on a small board either by the DIP process or surface mounting SMD process. The board itself has a single row of pin-leads that resembles a comb extending from its bottom edge, which plug into a special socket on a system or system-expansion board. SIPs are commonly found in <b>memory</b> <b>modules.</b> As compared to DIPs with a typical maximum I/O count of 64, SIPs have a typical maximum I/O count of 24 with lower package costs.|$|R
5000|$|SIMMs {{were invented}} in 1982 by James J. Parker at Zenith Microcircuits {{and the first}} Zenith Microcircuits {{customer}} was Wang Laboratories. Wang Laboratories tried to patent it and were granted a patent in April 1987. [...] That patent was later voided when Wang Laboratories sued multiple companies for infringement {{and it was then}} publicized that they were the prior invention of Parker at Zenith Microcircuits (the Elk Grove Village, Illinois subsidiary of Zenith Electronics Corporation). The lawsuit was then dropped and the patent was vacated. The original <b>memory</b> <b>modules</b> were built upon ceramic substrates with 64K Hitachi [...] "flip chip" [...] parts and had pins, i.e. <b>single</b> <b>in-line</b> package (SIP) packaging. There was an 8-bit part and a 9-bit part both at 64K.The pins were the costliest part of the assembly process and Zenith Microcircuits, in conjunction with Wang and Amp, soon developed an easy insertion, pinless connector. Later the modules were built on ceramic substrates with Fujitsu plastic J-lead chips and still later, they were made on standard PCB material. SIMMs using pins are usually called SIP or SIPP <b>memory</b> <b>modules</b> to distinguish them from the more common modules using edge connectors.|$|R
5000|$|... #Caption: Two {{types of}} DIMMs (dual <b>in-line</b> <b>memory</b> modules): a 168-pin SDRAM module (top) and a 184-pin DDR SDRAM module (bottom).|$|R
