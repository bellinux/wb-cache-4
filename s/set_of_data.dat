7398|10000|Public
5|$|The {{landmark}} scientific expedition (December 1872 – May 1876) of the British Royal Navy {{survey ship}} HMS Challenger yielded {{a tremendous amount}} of bathymetric data, much of which has been confirmed by subsequent researchers. Bathymetric data obtained {{during the course of the}} Challenger expedition enabled scientists to draw maps, which provided a rough outline of certain major submarine terrain features, such as the edge of the continental shelves and the Mid-Atlantic Ridge. This discontinuous <b>set</b> <b>of</b> <b>data</b> points was obtained by the simple technique of taking soundings by lowering long lines from the ship to the seabed.|$|E
5|$|One way of {{clustering}} a <b>set</b> <b>of</b> <b>data</b> {{points in}} a metric space into two clusters is to choose the clusters {{in such a way}} as to minimize the sum of the diameters of the clusters, where the diameter of any single cluster is the largest distance between any two of its points. This is preferable to minimizing the maximum cluster size, which may lead to very similar points being assigned to different clusters. If the target diameters of the two clusters are known, a clustering that achieves those targets may be found by solving a 2-satisfiability instance. The instance has one variable per point, indicating whether that point belongs to the first cluster or the second cluster. Whenever any two points are too far apart from each other for both to belong to the same cluster, a clause is added to the instance that prevents this assignment.|$|E
5|$|The {{first modern}} {{population}} census {{in the country}} was conducted in 1857, and 15 more have been performed since then. Since 1961 the censuses are conducted in regular ten-year intervals, with the latest one in 2011. The first institution {{set up in the}} country specifically for the purposes of maintaining population statistics was the State Statistical Office, founded in 1875. Since its founding, the office changed its name and structure several times and was alternately subordinated to other institutions and independent, until the most recent changes in 1992, when the institution became the Croatian Bureau of Statistics. The 2011 census was performed on 1–28 April 2011, recording situation as of 31 March 2011. The first census results, containing the number of the population by settlement, were published on 29 June 2011, and the final comprehensive <b>set</b> <b>of</b> <b>data</b> was published in December 2012. The 2011 census and processing of the data gathered by the census was expected to cost 171.9million kuna (23.3million euro). The 2011 census was performed using new methodology: the permanent population was determined as the enumerated population who lived in the census area for at least 12 months prior to the census, or plans to live in the same area for at least 12 months after the census. This method was also retroactively applied to the 2001 census data.|$|E
40|$|The present {{invention}} {{relates to}} a method {{and a system}} of securely computing a measure of similarity {{for at least two}} <b>sets</b> <b>of</b> <b>data.</b> A basic idea of the present invention is to securely compare two <b>sets</b> <b>of</b> encrypted <b>data</b> to determine whether the two <b>sets</b> <b>of</b> <b>data</b> resemble each other to a sufficient extent. If the measure of similarity complies with predetermined criteria, the two <b>sets</b> <b>of</b> <b>data</b> from which the encrypted <b>sets</b> <b>of</b> <b>data</b> originate are considered to be identical...|$|R
50|$|When {{the data}} is read out, the two <b>sets</b> <b>of</b> <b>data</b> are compared. A {{disturbance}} is detected if the two data sets are not equal. An error can be reported. If both <b>sets</b> <b>of</b> <b>data</b> are corrupted, a significant error can be reported and the system can react accordingly.|$|R
50|$|The LOC {{currently}} maintains four <b>sets</b> <b>of</b> <b>data</b> {{elements and}} four data exchange format types. The four <b>sets</b> <b>of</b> <b>data</b> elements are: activity codes; expense codes; timekeeper classification codes; and Uniform Task-Based Management System codes. The four data exchange format types are: electronic billing (e-billing); budgeting; timekeeper attributes; and intellectual property matter management.|$|R
25|$|Just as {{absolute}} entropy {{serves as}} theoretical background for data compression, relative entropy serves as theoretical background for data differencing – the absolute entropy of a <b>set</b> <b>of</b> <b>data</b> {{in this sense}} being the data required to reconstruct it (minimum compressed size), while the relative entropy of a target <b>set</b> <b>of</b> <b>data,</b> given a source <b>set</b> <b>of</b> <b>data,</b> is the data required to reconstruct the target given the source (minimum size of a patch).|$|E
25|$|Replace a <b>set</b> <b>of</b> <b>data</b> {{points with}} a family of simplicial complexes, indexed by a {{proximity}} parameter.|$|E
25|$|Restricted maximum likelihood, a {{variation}} using a likelihood function calculated from a transformed <b>set</b> <b>of</b> <b>data.</b>|$|E
5000|$|... #Caption: Four <b>sets</b> <b>of</b> <b>data</b> {{with the}} same {{correlation}} of 0.816 ...|$|R
40|$|We {{describe}} {{a method for}} comparing stochastic outputs of simulation models. The method is distribution-free and allows the comparison <b>of</b> <b>sets</b> <b>of</b> <b>data</b> with different numbers <b>of</b> <b>data</b> points. This makes it ideal for performing comparisons between simulation output and the real output of the system being modelled, when often {{there are many more}} data points available from the output of the simulation model than present in the real data. We calculate the two-sample Cramer-von Mises goodness-of-fit statistic between the two <b>sets</b> <b>of</b> <b>data,</b> using bootstrapping to find the distribution of the statistic, and so the probability that the two <b>sets</b> <b>of</b> <b>data</b> were drawn from the same distribution...|$|R
5000|$|Grouping and {{classification}} {{to determine}} patterns and associations among <b>sets</b> <b>of</b> <b>data.</b>|$|R
25|$|A <b>set</b> <b>of</b> <b>data</b> {{that arises}} from the {{log-normal}} distribution has a symmetric Lorenz curve (see also Lorenz asymmetry coefficient).|$|E
25|$|The {{least squares}} method is used to {{determine}} the best fit line for a <b>set</b> <b>of</b> <b>data.</b> This line will minimize the sum of the squares of the residuals.|$|E
25|$|The quartet {{is still}} {{often used to}} {{illustrate}} the importance of looking at a <b>set</b> <b>of</b> <b>data</b> graphically before starting to analyze according to {{a particular type of}} relationship, and the inadequacy of basic statistic properties for describing realistic datasets.|$|E
5000|$|Several {{overlapping}} <b>sets</b> <b>of</b> <b>data</b> {{exist on}} {{costs related to}} the DTES: ...|$|R
40|$|Biopax {{community}} is producing <b>sets</b> <b>of</b> <b>data</b> in RDF files, {{but most of}} them are not available through query interfaces. The publication of SPARQL endpoints is feasible with current <b>sets</b> <b>of</b> <b>data,</b> but the use of reasoning in these interfaces is unfeasible in many cases. The use of large scale reasoners is a need to take advantage <b>of</b> these <b>data</b> <b>sets...</b>|$|R
5000|$|... #Caption: Finding {{the median}} in <b>sets</b> <b>of</b> <b>data</b> {{with an odd}} and even number of values.|$|R
25|$|Computations where {{a number}} of similar, and often nested, models are {{considered}} for the same data set. That is, where models with the same dependent variable but different sets of independent variables are to be considered, for essentially the same <b>set</b> <b>of</b> <b>data</b> points.|$|E
25|$|An {{experiment}} involving {{measuring the}} time for subjects to recognise hidden images, with morphic resonance being posited to aid in recognition, was conducted in 1984 by the BBC popular science programme Tomorrow's World. In {{the outcome of the}} experiment, one <b>set</b> <b>of</b> <b>data</b> yielded positive results and another set yielded negative results.|$|E
25|$|To perform {{stress testing}} and {{scenario}} analysis, the observed data {{needs to be}} altered, e.g. some payments delayed or removed. To analyze the levels of liquidity, initial liquidity levels are varied. System comparisons (benchmarking) or evaluations of new netting algorithms or rules are performed by running simulations with a fixed <b>set</b> <b>of</b> <b>data</b> and varying only the system setups.|$|E
5000|$|Template capacity: {{the maximum}} number <b>of</b> <b>sets</b> <b>of</b> <b>data</b> that {{can be stored in}} the system.|$|R
30|$|All {{functional}} {{experiments were}} repeated {{at least in}} three independent experiments. The band intensity in Western blot analysis was quantified by ImageJ. Data were analyzed by GraphPad Prism 5.0 software and shown by the mean ± SEM. Multiple <b>sets</b> <b>of</b> <b>data</b> were analyzed by one-way or two-way ANOVA, and unpaired Student’s t {{test was used to}} analyze two <b>sets</b> <b>of</b> <b>data.</b> Significance level was set at P <  0.05.|$|R
2500|$|... 1 is {{the most}} common leading digit in many <b>sets</b> <b>of</b> <b>data,</b> a {{consequence}} <b>of</b> Benford's law.|$|R
25|$|The S3 {{language}} {{that was used}} to write the ICL VME operating system and much other system software on the ICL 2900 Series was a direct derivative of Algol 68. However, it omitted many of the more complex features, and replaced the basic modes with a <b>set</b> <b>of</b> <b>data</b> types that mapped directly to the 2900 Series hardware architecture.|$|E
25|$|The {{experimental}} data will comprise a <b>set</b> <b>of</b> <b>data</b> points. At the i'th data point, the analytical concentrations of the reactants, TA(i), TB(i) etc. will be experimentally known quantities {{and there will}} be one or more measured quantities, yi, that depend in some way on the analytical concentrations and equilibrium constants. A general computational procedure has three main components.|$|E
25|$|Arithmetic word {{problems}} involve written text {{containing a}} <b>set</b> <b>of</b> <b>data</b> followed {{by one or}} more questions and require the use of the four basic arithmetic operations (addition, subtraction, multiplication, or division). Researchers suggest that successful completion of arithmetic word problems involves spatial working memory (involved in building schematic representations) which facilitates the creation of spatial relationships between objects. Creating spatial relationships between objects {{is an important part of}} solving word problems because mental operations and transformations are required.|$|E
50|$|Methods {{carry out}} {{a small number of}} related activities, by {{avoiding}} coarsely grained or unrelated <b>sets</b> <b>of</b> <b>data.</b>|$|R
40|$|Abstract. We {{propose a}} {{classification}} method using a concept lattice, {{and apply it}} to thesaurus extension. In natural language processing, solving a practical task by extending many thesauri with a corpus is timeconsuming. The task can be represented as classifying a <b>set</b> <b>of</b> test <b>data</b> for each <b>of</b> many <b>sets</b> <b>of</b> training <b>data.</b> The method enables us to decrease the time-cost by avoiding feature selection, which is generally performed for each pair <b>of</b> a <b>set</b> <b>of</b> test <b>data</b> and a <b>set</b> <b>of</b> training <b>data.</b> More precisely, a concept lattice is generated from only a <b>set</b> <b>of</b> test <b>data,</b> and then each formal concept is given a score by using a <b>set</b> <b>of</b> training <b>data.</b> The score represents plausibleness as neighbors of an unknown object, and the unknown object classified into classes to which its neighbors belong. Therefore, once we make the lattice, we can classify test <b>data</b> for each <b>set</b> <b>of</b> training <b>data</b> by only scoring, which has a small computational cost. By experiments using practical thesauri and corpora, we show that our method classifies more accurately than k-nearest neighbor algorithm...|$|R
30|$|In {{this section}} {{the results of}} the {{statistical}} analysis of the relations between the proposed <b>sets</b> <b>of</b> <b>data</b> will be reported.|$|R
25|$|Remote Data Services (RDS) {{allowed the}} {{retrieval}} of a <b>set</b> <b>of</b> <b>data</b> from the server, which the client then altered {{in some way}} and then {{sent back to the}} server for further processing. With the popular adoption of Transact-SQL, which extends SQL with such programming constructs as loops and conditional statements, this became less necessary and it was eventually deprecated in MDAC 2.7. Microsoft produced SOAP Toolkit 2.0, which allows clients to do this via an open XML-based standard.|$|E
25|$|Fisher {{emphasized}} the importance of measuring the tail – the observed value of the test statistic and all more extreme – rather than simply the probability of specific outcome itself, in his The Design of Experiments (1935). He explains this as because a specific <b>set</b> <b>of</b> <b>data</b> may be unlikely (in the null hypothesis), but more extreme outcomes likely, so seen in this light, the specific but not extreme unlikely data should not be considered significant.|$|E
25|$|In a 'multiply linked list', each node {{contains}} {{two or more}} link fields, each field being used to connect the same <b>set</b> <b>of</b> <b>data</b> records in a different order (e.g., by name, by department, by date of birth, etc.). While doubly linked lists {{can be seen as}} special cases of multiply linked list, the fact that the two orders are opposite to each other leads to simpler and more efficient algorithms, so they are usually treated as a separate case.|$|E
30|$|The {{computer}} system to support Social CRM will consist <b>of</b> a <b>set</b> <b>of</b> Big <b>Data</b> and social software applications, and a <b>set</b> <b>of</b> Big <b>Data</b> and social software tools.|$|R
3000|$|Specifically, {{the above}} {{numerical}} analysis {{was carried out}} using 31 <b>sets</b> <b>of</b> <b>data,</b> excluding one <b>set</b> <b>of</b> experimental values, from Group B (n[*]=[*] 32) to determine the u[*]−[*]V [...]...|$|R
5000|$|User-defined {{aggregates}} (UDAs) {{which allow}} developers to create custom aggregates that act on <b>sets</b> <b>of</b> <b>data</b> instead <b>of</b> one row at a time, ...|$|R
