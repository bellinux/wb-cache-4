1062|4479|Public
25|$|The {{statistical}} tables for t and for Z provide critical values for both one- and two-tailed tests. That is, {{they provide the}} critical values that cut off an entire region at {{one or the other}} end of the <b>sampling</b> <b>distribution</b> as well as the critical values that cut off the regions (of half the size) at both ends of the <b>sampling</b> <b>distribution.</b>|$|E
25|$|This arises {{because the}} <b>sampling</b> <b>distribution</b> {{of the sample}} {{standard}} deviation follows a (scaled) chi distribution, and the correction factor is {{the mean of the}} chi distribution.|$|E
25|$|An {{estimator}} can be unbiased but not consistent. For example, for an iid sample {x,..., x} {{one can use}} T(X) = x as the estimator of {{the mean}} E. Note that here the <b>sampling</b> <b>distribution</b> of T {{is the same as}} the underlying distribution (for any n, as it ignores all points but the first), so E = E and it is unbiased, but it does not converge to any value.|$|E
40|$|When people revise {{subjective}} probabilities {{in light}} of data, revisions are less than the amount prescribed by the normative model, Bayes's theorem. Previous research suggests that this results from the subjects' {{lack of understanding of}} the implications of the data; i. e., from inaccurate subjective <b>sampling</b> <b>distributions.</b> This experiment examined the effects on conservative revisions of training subjects about the implications of data. The subjects estimated <b>sampling</b> <b>distributions</b> for two binomial populations, were shown samples from the populations in order to teach them veridical distributions, and again estimated <b>sampling</b> <b>distributions.</b> Estimated <b>sampling</b> <b>distributions</b> were good predictors of revisions and, as a result of training, both the <b>sampling</b> <b>distributions</b> and the revisions became more veridical...|$|R
40|$|The <b>sample</b> <b>distribution</b> {{is defined}} as the <b>distribution</b> of the <b>sample</b> {{measurements}} given the selected sample. Under informative <b>sampling,</b> this <b>distribution</b> may be different from the corresponding population <b>distribution.</b> <b>Sampling</b> on two occasions under informative sampling design, utilizing the <b>sample</b> <b>distribution</b> is proposed for predicting the population total of a variable under study for the current (second) occasion, viewing information collected on the first (previous) occasion as auxiliary information...|$|R
40|$|Cumulative {{distribution}} {{functions of}} some <b>sampling</b> <b>distributions</b> {{do not have}} closed form representation. Hence an approximate formula can be of immense use. In this paper, we review the development of literature of approximations to cumulative distribution function of some popular central <b>sampling</b> <b>distributions...</b>|$|R
25|$|Thus {{computing}} a p-value {{requires a}} null hypothesis, a test statistic (together with deciding whether the researcher is performing a one-tailed test or a two-tailed test), and data. Even though computing the test statistic on given data may be easy, computing the <b>sampling</b> <b>distribution</b> under the null hypothesis, and then computing its {{cumulative distribution function}} (CDF) is often a difficult problem. Today, this computation is done using statistical software, often via numeric methods (rather than exact formulae), {{but in the early}} and mid 20th century, this was instead done via tables of values, and one interpolated or extrapolated p-values from these discrete values. Rather than using a table of p-values, Fisher instead inverted the CDF, publishing a list of values of the test statistic for given fixed p-values; this corresponds to computing the quantile function (inverse CDF).|$|E
2500|$|... & {{corresponding}} [...] (Select the <b>Sampling</b> <b>Distribution</b> CLT Experiment {{from the}} drop-down list of [...] ) ...|$|E
2500|$|The <b>sampling</b> <b>distribution</b> of {{responders}} {{in group}} X conditional upon the trial outcome and prevalences, ...|$|E
40|$|<b>Sampling</b> <b>distributions</b> play a {{vital role}} in {{understanding}} how statistical inferences are made, yet students often fail to achieve proficiency in this important topic. The goals of our project are to examine the literature on <b>sampling</b> <b>distributions</b> and concepts that may be prerequisite knowledge for sampling distributions- <b>sampling,</b> variability and <b>distribution,</b> develop an assessment tool that can be used in a timely fashion to provide instructors with feedback on the understanding and misconceptions that students have about <b>sampling</b> <b>distributions</b> and prerequisite concepts, and better understand misconceptions to improve the teaching and learning of basic statistics. We developed new and modified existing assessment items to study the relationship between comprehension of <b>sampling</b> <b>distributions</b> and mastery of each prerequisite area. Open ended questions were refined through a pilot study and we share our experiences in developing items...|$|R
40|$|Using the {{characteristic}} curve method for dichotomously scored test items, the <b>sampling</b> <b>distributions</b> of equating coefficients were examined. Simulated data for broad-range and screening tests {{were analyzed using}} three equating contexts and three anchor-item configurations in horizontal and vertical equating situations. The {{results indicated that the}} <b>sampling</b> <b>distributions</b> were bell-shaped and their standard deviations were uniformly small. There were few differences in the forms of the distributions of the obtained equating coefficients {{as a function of the}} anchor-item configurations or type of test. For the equating contexts studied, the <b>sampling</b> <b>distributions</b> of the equating coefficients appear to have acceptable characteristics, suggesting confidence in the values obtained by {{the characteristic}} curve method. Index terms: anchor items, characteristic curve method, common metric, equating coefficients, <b>sampling</b> <b>distributions,</b> test equating...|$|R
5000|$|... #Subtitle level 3: Natural {{gradient}} descent {{in the space}} of <b>sample</b> <b>distributions</b> ...|$|R
2500|$|In {{parameter}} confidence intervals, one estimates population parameters; if {{one wishes}} to interpret this as prediction of the next sample, one models [...] "the next sample" [...] as a draw from this estimated population, using the (estimated) population distribution. By contrast, in predictive confidence intervals, one uses the <b>sampling</b> <b>distribution</b> of (a statistic of) a sample of n or nnbsp&+nbsp&1 observations from such a population, and the population distribution is not directly used, though the assumption about its form (though not the values of its parameters) is used in computing the <b>sampling</b> <b>distribution.</b>|$|E
2500|$|Another use of Haar {{measure in}} {{statistics}} is in conditional inference, {{in which the}} <b>sampling</b> <b>distribution</b> of a statistic is conditioned on another statistic of the data. In invariant-theoretic conditional inference, the <b>sampling</b> <b>distribution</b> is conditioned on an invariant {{of the group of}} transformations (with respect to which the Haar measure is defined). The result of conditioning sometimes depends on the order in which invariants are used and on the choice of a maximal invariant, so that by itself a [...] statistical principle of invariance fails to select any unique best conditional statistic (if any exist); at least another principle is needed.|$|E
2500|$|From the {{properties}} of the normal distribution, we know the <b>sampling</b> <b>distribution</b> of this statistic: T'n is itself normally distributed, with mean μ and variance σ2/n. Equivalently, [...] has a standard normal distribution: ...|$|E
40|$|By far, {{the most}} popular test for spatial {{correlation}} is the one based on Moran’s (1950) I test statistic. Despite this, the available results in the literature concerning the large <b>sample</b> <b>distribution</b> of this statistic are limited and have been derived under assumptions that do not cover many applications of interest. In this paper we first give a general result concerning the large <b>sample</b> <b>distribution</b> of Moran I type test statistics. We then apply this result to derive the large <b>sample</b> <b>distribution</b> of the Moran I test statistic {{for a variety of}} important models. In order to establish these results we also give a new central limit theorem for linear-quadratic forms...|$|R
40|$|This paper {{considers}} listing {{all possible}} samples of size n with unequal probabilities without replacement {{in order to}} find the <b>sample</b> <b>distribution.</b> The main application of that is to estimate the Horvitz-Thompson (HT) estimator and possibly to know the shape of its <b>sample</b> <b>distribution</b> to construct confidence intervals. The algorithm computes all possible samples of the population, in contrast with PROC SURVEYSELECT which generates any samples of size n, but not all possible samples, and at the end it is possible to plot the <b>sample</b> <b>distribution</b> of the estimator. The equations are encoded in a SAS/IML Macro and the graphics are made using PROC GPLOT. 1...|$|R
40|$|Bayesian {{inference}} {{and decision}} making requires elici 1 :ation of prior probabilities and <b>sampling</b> <b>distributions.</b> In many applica tions such as exploratory data analysis, however, {{it may not be}} possible to construct the prior probabilities or the <b>sampling</b> <b>distributions</b> precisely. The objective of this thesis is to address the issues and provide some solutions to the problem of inference {{and decision making}} with imprecise or partially known priors and <b>sampling</b> <b>distributions.</b> More specifically, we will address the following three interrelated problems:(1) how to describe imprecise priors and <b>sampling</b> <b>distributions,</b> (2) how to proceed from approximate priors and <b>sampling</b> <b>distributions</b> to approximate posteriors and posterior related quantities, and (3) how to make decisions with imprecise posterior probabilities. When the priors and/or <b>sampling</b> <b>distributions</b> are not known precisely, a natural approach is to consider a class or a neighborhood of priors, and classes or collections of <b>sampling</b> <b>distributions.</b> This approach leads naturally to consideration of upper and lower probabilities or interval-valuedl probabilities. We examine the various approaches to representation of imprecision in priors and <b>sampling</b> <b>distributions.</b> We realize that many useful classes, either for the priors or for the <b>sampling</b> <b>distributions,</b> are conveniently described in terms of 2 - Choquet Capacities. We prove the Bayes 2 ̆ 7 Theorem (or Conditioning) for the 2 -Choquet Capacity classes. Since the classes of imprecise probabilities described by the Dempster-Shafer Theory are ∞-Choquet Capacities (and therefore 2 -Choquet Capacities) our result provides another proof of the inconsistency of the Dempster 2 ̆ 7 s rule. We address the problem of combination of various sources of information and the requirements for a reasonable combination rule. Here, we also examine the issues of independence of sources of information which is a crucial issue in combining various sources of information. We consider three methods to combine imprecise information. In method one, we utilizes thle extreme-point representations of the imprecise priors and/or the <b>sampling</b> <b>distributions</b> to obtain the extreme-points of the class of posteriors. This method is usually computationally very demanding. Therefore, we propose a simple iterative procedure that allows direct computation of not only the posterior probabilities, but also many useful posterior related quantities such as the posterior mean, the predictive density that the next observation would lie in a given set, the posterior expected loss of a decision or an action, etc. Finally,, by considering the joint space of observations and parameters, we show that if this class of joint probabilities is a 2 -Choquet capacity class, we can utilize our Bayes 2 ̆ 7 Theorem found earlier to obtain the posterior probabilities. This last approach is computationally the most efficient method. Finally, we address the problem of decision making with imprecise posteriors obtained from imprecise priors and <b>sampling</b> <b>distributions.</b> Even,though, allowing imprecision is a natural approach for representation of lack of information, it sometimes leads to complications in decision making and even indeterminacies. We suggest a few ad-hoc rules to resolve the remaining indeterminacies. The ultimate solution in such cases is to simply gather more data...|$|R
2500|$|Rather {{than using}} sample {{statistics}} as estimators of population parameters and applying confidence intervals to these estimates, one considers [...] "the next sample" [...] as itself a statistic, and computes its <b>sampling</b> <b>distribution.</b>|$|E
2500|$|The {{choice of}} {{quantiles}} from a theoretical distribution can depend upon context and purpose. One choice, given {{a sample of}} size , is [...] for , as these are the quantiles that the <b>sampling</b> <b>distribution</b> realizes. The last of these, , corresponds to the 100th percentile – the maximum value of the theoretical distribution, which is sometimes infinite. Other choices are the use of [...] , or instead to space the points evenly in the uniform distribution, using [...]|$|E
2500|$|The {{t-distribution}} with n − 1 {{degrees of}} freedom is the <b>sampling</b> <b>distribution</b> of the t-value when the samples consist of independent identically distributed observations from a normally distributed population. Thus for inference purposes t is a useful [...] "pivotal quantity" [...] in the case when the mean and variance (μ, σ2) are unknown population parameters, {{in the sense that}} the t-value has then a probability distribution that depends on neither μ nor σ2.|$|E
40|$|We {{present a}} new method for the {{generation}} of anisotropic <b>sample</b> <b>distributions</b> on planar and two-manifold domains. Most previous work that is concerned with aperiodic point distributions is designed for isotropically shaped samples. Methods focusing on anisotropic <b>sample</b> <b>distributions</b> are rare, and either they are restricted to planar domains, are highly sensitive to the choice of parameters, or they are computationally expensive. In this paper, we present a time-efficient approach for {{the generation of}} anisotropic <b>sample</b> <b>distributions</b> that only depends on intuitive design parameters for planar and two-manifold domains. We employ an anisotropic triangulation that serves as basis {{for the creation of}} an initial <b>sample</b> <b>distribution</b> as well as for a gravitational-centered relaxation. Furthermore, we present an approach for interactive rendering of anisotropic Voronoi cells as base element for texture generation. It represents a novel and flexible visualization approach to depict metric tensor fields that can be derived from general tensor fields as well as scalar or vector fields...|$|R
5000|$|... #Caption: <b>Sample</b> <b>distribution</b> of body hair {{in women}} and men. (Excluding underarm and scalp hair) ...|$|R
40|$|The <b>sample</b> <b>distribution</b> {{is defined}} as the <b>distribution</b> of the <b>sample</b> {{measurements}} given the selected sample. Under informative <b>sampling,</b> this <b>distribution</b> may be different from the corresponding population <b>distribution.</b> <b>Sampling</b> on two occasions under informative sampling design, utilizing the <b>sample</b> and sample-complement <b>distributions</b> for occasion one, the matched sample and unmatched <b>sample</b> <b>distributions,</b> and matched sample-complement and unmatched sample-complement for occasion two, is proposed for predicting finite population total of a variable under study for the current (second) occasion, viewing information collected on the first (previous) occasion as auxiliary information. An interesting result of the present analysis is that known predictors in common use are shown to be special cases of the present predictors obtained under informative sampling, thus providing them a new justification...|$|R
2500|$|Confidence {{intervals}} and hypothesis {{tests are}} two statistical procedures {{in which the}} quantiles of the <b>sampling</b> <b>distribution</b> of a particular statistic (e.g. the standard score) are required. [...] In any situation where this statistic is a linear function of the data, divided by the usual estimate of the standard deviation, the resulting quantity can be rescaled and centered to follow Student's t-distribution. [...] Statistical analyses involving means, weighted means, and regression coefficients all lead to statistics having this form.|$|E
2500|$|In the {{approach}} of Ronald Fisher, the null hypothesis H0 will be rejected when the p-value of the test statistic is sufficiently extreme (vis-a-vis the test statistic's <b>sampling</b> <b>distribution)</b> and thus judged unlikely {{to be the result}} of chance. In a one-tailed test, [...] "extreme" [...] is decided beforehand as either meaning [...] "sufficiently small" [...] or meaning [...] "sufficiently large" [...] – values in the other direction are considered not significant. In a two-tailed test, [...] "extreme" [...] means [...] "either sufficiently small or sufficiently large", and values in either direction are considered significant. For a given test statistic there is a single two-tailed test, and two one-tailed tests, one each for either direction. Given data of a given significance level in a two-tailed test for a test statistic, in the corresponding one-tailed tests for the same test statistic it will be considered either twice as significant (half the p-value), if the data is in the direction specified by the test, or not significant at all (p-value above 0.05), if the data is in the direction opposite that specified by the test.|$|E
2500|$|In {{the case}} of a {{discrete}} variable, the <b>sampling</b> <b>distribution</b> of the median for small-samples can be investigated as follows. [...] We take the sample size to be an odd number [...] [...] If a given value [...] is to be the median of the sample then two conditions must be satisfied. [...] The first is that at most [...] observations can have a value of [...] or less. [...] The second is that at most [...] observations can have a value of [...] or more. [...] Let [...] be the number of observations which have a value of [...] or less and let [...] be the number of observations which have a value of [...] or more. [...] Then [...] and [...] both have a minimum value of 0 and a maximum of [...] [...] If an observation has a value below , it is not relevant how far below [...] it is and conversely, if an observation has a value above , it is not relevant how far above [...] it is. [...] We can therefore represent the observations as following a trinomial distribution with probabilities [...] , [...] and [...] [...] The probability that the median [...] will have a value [...] is then given by ...|$|E
5000|$|Nonuniform <b>sampling</b> <b>distributions</b> {{attempt to}} place more {{milestones}} {{in areas that}} improve the connectivity of the roadmap.|$|R
40|$|Abstract: The <b>sample</b> <b>distribution</b> {{is defined}} as the <b>distribution</b> of the <b>sample</b> mea-surements given the {{selected}} sample. Under informative <b>sampling,</b> this <b>distribution</b> is different from the corresponding population distribution, although for several examples the two distributions are shown {{to be in the same}} family and only differ in some or all the parameters. A general approach of approximating the marginal <b>sample</b> <b>distribution</b> for a given population distribution and first order sample se-lection probabilities is discussed and illustrated. Theoretical and simulation results indicate that under common sampling methods of selection with unequal proba-bilities, when the population measurements are independently drawn from some <b>distribution</b> (superpopulation), the <b>sample</b> measurements are asymptotically inde-pendent as the population size increases. This asymptotic independence combined with the approximation of the marginal <b>sample</b> <b>distribution</b> permits the use of stan-dard methods such as direct likelihood inference or residual analysis for inference on the population distribution...|$|R
40|$|Given {{the random}} walk model, we show, for the {{traditional}} unrestricted regression used in testing stationarity, {{that no matter what}} the initial value of the random walk is or its drift or its error standard deviation, the <b>sampling</b> <b>distributions</b> of certain statistics remain unchanged. Using Monte Carlo simulations, we estimate, for different finite <b>samples,</b> the <b>sampling</b> <b>distributions</b> of these statistics. After smoothing the percentiles of the empirical <b>sampling</b> <b>distributions,</b> we {{come up with a new}} set of critical values for testing the existence of a random walk, if each statistic is being used on an individual base. Combining the new sets of critical values, we finally suggest a general methodology for testing for a random walk model. random walk, critical values, uncertainty,...|$|R
5000|$|The <b>sampling</b> <b>distribution</b> of ln(χ2) converges to {{normality}} {{much faster}} than the <b>sampling</b> <b>distribution</b> of χ2, as the logarithm removes much of the asymmetry. Other functions of the chi-squared distribution converge more rapidly to a normal distribution. Some examples are: ...|$|E
5000|$|In {{order to}} derive {{the formula for}} the one-sample {{proportion}} in the Z-interval, a <b>sampling</b> <b>distribution</b> of sample proportions {{needs to be taken}} into consideration. The mean of the <b>sampling</b> <b>distribution</b> of sample proportions is usually denoted as [...] and its standard deviation is denoted as [...] Since the value of [...] is unknown, an unbiased statistic [...] will be used for [...] The mean and standard deviation are rewritten as [...] and [...] respectively. Invoking the Central Limit Theorem, the <b>sampling</b> <b>distribution</b> of sample proportions is approximately normal.|$|E
5000|$|... 1. mu:=-6; sigma2:=100; t:=0; maxits=100; // Initialize {{parameters}} 2. N:=100; Ne:=10; // 3. while t < maxits and sigma2 > epsilon // While maxits {{not exceeded}} and not converged 4. X = SampleGaussian(mu,sigma2,N); // Obtain N samples from current <b>sampling</b> <b>distribution</b> 5. S = exp(-(X-2)^2) + 0.8 exp(-(X+2)^2); // Evaluate objective function at sampled points 6. X = sort(X,S); // Sort X by objective function values (in descending order) 7. mu = mean(X(1:Ne)); sigma2=var(X(1:Ne)); // Update parameters of <b>sampling</b> <b>distribution</b> 8. t = t+1; // Increment iteration counter 9. return mu // Return mean of final <b>sampling</b> <b>distribution</b> as solution ...|$|E
40|$|Abstract: The {{method of}} {{non-linear}} lag correlation rejection from the non-stationary time series is constructed. This method {{is based on}} the analysis of support of the combined <b>sample</b> <b>distribution</b> function density of lag vector components. The method of correlation analysis between two time series is also formulated. The concrete forecast example of non-stationary time series is given for the case of definite level of stationary state of <b>sample</b> <b>distribution</b> function. Note: Publication language:russia...|$|R
3000|$|... 25 The <b>sample</b> <b>distribution</b> by age, type (with/without {{unemployment}} subsidy) {{and duration}} in unemployment {{is presented in}} Fig. 12 in Appendix B.|$|R
40|$|This thesis {{displays}} a <b>sample</b> <b>distribution,</b> generated from both a simulation (for large n) by computer program and explicitly calculated (for smaller n), {{that is not}} governed by the Central Limit Theorem and, in fact seems to display chaotic behavior. To our knowledge, the explicit calculation of the <b>sample</b> <b>distribution</b> function is new. This project outlines the results that have found a relation to number theory in a probabilistic game that has perplexed mathematicians for hundreds of years...|$|R
