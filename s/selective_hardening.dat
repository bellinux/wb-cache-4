21|13|Public
50|$|Differential {{hardening}} (also called differential quenching, selective quenching, <b>selective</b> <b>hardening,</b> {{or local}} hardening) is {{most commonly used}} in bladesmithing to increase the toughness of a blade while keeping very high hardness and strength at the edge. This helps to make the blade very resistant to breaking, by making the spine very soft and bendable, but allows greater hardness at the edge than would be possible if the blade was uniformly quenched and tempered. This helps to create a tough blade that will maintain a very sharp, wear-resistant edge, even during rough use such as found in combat.|$|E
40|$|International audienceDetermining the {{dependability}} {{of integrated}} systems {{with respect to}} soft errors is necessary for {{a growing number of}} applications. The most critical information must be identified when <b>selective</b> <b>hardening</b> is necessary to achieve good efficiency/cost trade-offs. In processor-based systems, the most critical variables must thus be identified in the application program. An improved algorithm for critical variable identification is described and validated with respect to fault injection results...|$|E
40|$|Laser {{transformation}} hardening {{is effective}} technique used for <b>selective</b> <b>hardening</b> of components such as turbine blade, camshafts and gears. Laser hardening provides benefits over other <b>selective</b> <b>hardening</b> processes {{in terms of}} thermal distortion, control of process and appearance of component. The fiber laser is relatively recent development. The single/low mode fiber lasers have good beam quality, high wall plug efficiency, fiber delivery and powers up to few hundred watts. The beam quality of single mode fiber lasers enable it to be focused to a spot size of few tens of mm which can yield hardened tracks of 100 to 500 mm. These fiber lasers can be extremely useful in localized micro-scale surface hardening to create hardened patterns for improving the wear resistance. This paper is focused on developing analytical thermal model of moving heat sources and integrating it with kinetic hardening model to capture the metallurgical changes induced by fiber lasers. An ideal surface hardening technique should give widest hardened track at minimum case depth. To address this issue, an optimization methodology based on statistical approximation of the physics-based engineering models has been developed...|$|E
40|$|Copper electrodeposits are {{employed}} {{for a number}} of applications such as decorative copper-nickel-chromium plating, production of printed circuit boards and <b>selective</b> case <b>hardening.</b> In general, copper is deposited on steel components exclusively from cyanide based electrolytes. In view of the toxicity of such electrolytes, work is being pursued all over the world for development of suitable noncyanide solutions. In this paper, the authors report the use of an electrolyte containing copper-NTA complex for production of adherent copper deposits on stee...|$|R
40|$|AbstractThe growing use {{of fiber}} {{reinforced}} plastics in injection molding greatly reduces {{the lifetime of}} components in dies and molds. In this research, injection nozzles are machined within one set-up, combining machining and <b>selective</b> laser <b>hardening</b> operation. This allows the machining and hardening of die and mold components in one single setup. A thermal model is developed and experimentally validated {{in order to be}} able to select optimal hardening parameters and strategy. Hardness values exceeding conventional hardening techniques on tool steel are presented, aiming for a prolonged lifetime of dies and molds...|$|R
40|$|Manufacturing {{industries}} {{have more and}} more to deal with the machining of complex shaped components, of-ten in small series and fabricated in difficult to cut mate-rials. Hybrid manufacturing technologies can face these challenges. They can be obtained by integrating produc-tion processes on one single machining platform and/or combining process mechanisms to enhance machining. After giving a brief introduction and classification of hy-brid manufacturing technologies, this paper focusses on some hybrid technologies investigated by the KU Leu-ven advanced manufacturing research group. Process-es like vibration assisted machining, integration of ma-chining & <b>selective</b> laser <b>hardening,</b> combination of ECM & Mechanical cutting, will be discussed. To ensure precision machining, the need to apply advanced moni-toring systems will be described as well. status: publishe...|$|R
40|$|Electrolytic {{hardening}} process w as developed in USSR in the 1950 s. The process w as developed but w as not com m ercially exploited. There {{is no evidence}} of w ork done on this process in India. The author has done this originalw ork applied to different m aterials like steel, castiron and alum inum -bronze. This paper gives details ofm icrostructuraltransform ations along w ith hardness value achieved. There is vitalscope for this process to becom e viable for surfacehardening and <b>selective</b> <b>hardening</b> ofsm allcom ponents...|$|E
40|$|This paper {{presents}} the activities performed on a Mori Seiki NTX 2000 mill-turn center, {{made available to}} the KU Leuven by the Machine Tools Technologies Research Foundation (MTTRF). Besides various educational activities, the equipment is used in research focusing on the machining of complex shaped parts, requiring selective hardened regions. This paper focusses on the development of a selective laser hardening set-up, including the optical and mechanical design, within a DMG MORI NTX 2000 machining center. This development is also supported by thermal simulations of the <b>selective</b> <b>hardening</b> process. A proof of concept, based on C 45 steel, is also presented. status: publishe...|$|E
40|$|International audienceDetermining the {{dependability}} {{of integrated}} systems {{with respect to}} soft errors is necessary for {{a growing number of}} applications. The most critical information must be identified to achieve good efficiency/cost trade-offs when <b>selective</b> <b>hardening</b> is necessary. In processor-based systems, the most critical variables and registers must thus be identified for the target application program. An improved algorithm for critical register identification is described and compared to previous work. Fault injection results in a system based on Leon 2 are also reported. They demonstrate the impact of micro-architectural characteristics on evaluating the real criticality of the register file. New refinements are suggested for further work based on data dependency analysis...|$|E
40|$|Abstract. A {{ubiquitous}} computing system derives its operations from the collective interactions of its constituent components. Consequently, a robust ubiquitous system entails that the discrete components must be robust to handle errors arising in themselves and over interactions with other system components. This paper conceptually outlines a profiling framework that assists in finding weaknesses {{in one of}} the fundamental building blocks of most computer based systems, namely the Operating System (OS). The proposed framework allows a system designer to ascertain possible error propagation paths, from drivers through the OS to the applications. This knowledge significantly helps enhance the OS (or driver/application) with <b>selective</b> robustness <b>hardening</b> capabilities, i. e., robustness wrappers. 1 Introductions: The Ubiquitous Computing Perspective A {{ubiquitous computing}} (UC) environment harnesses the collective capabilities of diverse computational components via dynamic resource management as warrante...|$|R
40|$|Plasma {{arc surface}} {{hardening}} {{is an alternative}} <b>selective</b> surface <b>hardening</b> method that is effective, economical and a promising technology in heat treatment industries. In the present work, an investigation was carried out to study the hardness distributions of multiple passes in surface hardening of tool steel by plasma arc. The effects of multiple passes with overlapping and non-overlapping scans were investigated. The {{results show that the}} hardness is higher at centre of the plasma arc hardening tracks, and then decreasing in the region adjacent to each plasma arc track. It was found that the formation of hardened zone hardness in multiple passes non-overlapping scan is more uniform on the each scan when compared to the overlapping scan. However, hardness distribution of overlapping scan in width direction shows that it was more uniform compared with non-overlapping scan...|$|R
40|$|Abstract. Flame {{hardening}} {{has been}} traditionally used for <b>selective</b> surface <b>hardening</b> of steel. This technique frequently resulting in imprecise harden area and part distortion when overheated. Focused heating source such as plasma arc {{can be an}} alternative to overcome this problem. In this work, plasma arc is scanned at the 4340 steel surface to improve the hardness. The variable parameters investigated were at scanning speed and operating current. Four types of surfaces were observed after they are scanned with plasma arc. They are fully-melted, partially-melted, non-melted continuous and non-melted intermittent where each type of surface has different roughness value. This work found that scanning speed and operating current has significantly influence the type of surface and roughness values. Analysis on non-melted surface gives the maximum depth of hardened layer of about 187 µm as well as hardness values of about 990 HV 50. It is also observed that the depth of hardened layer and hardness value is significantly decreasing with increase in scanning speed or the decrease in operating current. Microstructure examination on hardened layer revealed that the increase of hardness is due to formation of fine grain martensitie structure...|$|R
40|$|Abstract 1 In {{this paper}} we propose an {{approach}} to the design optimization of fault-tolerant hard real-time embedded systems, which combines hardware and software fault tolerance techniques. We trade-off between <b>selective</b> <b>hardening</b> in hardware and process re-execution in software to provide the required levels of fault tolerance against transient faults with the lowest-possible system costs. We propose a system failure probability (SFP) analysis that connects the hardening level with {{the maximum number of}} re-executions in software. We present design optimization heuristics, to select the fault-tolerant architecture and decide process mapping such that the system cost is minimized, deadlines are satisfied, and the reliability requirements are fulfilled. 1...|$|E
40|$|Abstract—Defects {{as well as}} soft {{errors are}} a growing concern in micro and nanoelectronics. Multiple faults induced by single event effects are {{expected}} to be seen more often. Thus, reliability has become an important design criterion. In this context we introduce a cost-aware methodology for <b>selective</b> <b>hardening</b> of combinational logic cells. The methodology is based on the SPRA algorithm for calculating logical masking, and it is capable to automatically perform a trade-off between reliability improvements and associated costs, providing a list of the most effective candidates for hardening. The methodology is applied to a set of benchmark circuits using costs extracted from an actual standard cell library. The results then show that the methodology is able to diminish the unreliability of circuits in a cost-effective manner...|$|E
40|$|Practical {{on-line test}} methods do not cover all {{possible}} faults of a system. We propose {{a method to}} identify critical faults and distinguish them from non-critical ones. Low-cost on-line fault detection can focus on the critical faults. Alternatively, the circuit sites associated with critical faults could be selectively hardened to improve the overall reliability of a system. This is done in a costeffective way because no hardening against non-critical faults is required. In this work, we concentrate on faults in imaging applications such as video. We classify faults based on {{their impact on the}} system behavior, i. e., the visibility of their effects by a human end-user. The psychovisual model from the JPEG compression method is used for fault effect classification. Keywords: Low-cost on-line test, <b>Selective</b> <b>hardening,</b> Imaging applications, Error tolerance...|$|E
40|$|The {{design of}} fault {{tolerant}} systems is gaining importance in large domains of embedded applications where design constrains {{are as important}} as reliability. New software techniques, based on selective application of redundancy, have shown remarkable fault coverage with reduced costs and overheads. However, the large number of different solutions provided by these techniques, and the costly process to assess their reliability, make the design space exploration a very difficult and time-consuming task. This paper proposes the integration of a multi-objective optimization tool with a software hardening environment to perform an automatic design space exploration in the search for the best trade-offs between reliability, cost, and performance. The first tool is commanded by a genetic algorithm which can simultaneously fulfill many design goals thanks {{to the use of the}} NSGA-II multi-objective algorithm. The second is a compiler-based infrastructure that automatically produces <b>selective</b> protected (<b>hardened)</b> versions of the software and generates accurate overhead reports and fault coverage estimations. The advantages of our proposal are illustrated by means of a complex and detailed case study involving a typical embedded application, the AES (Advanced Encryption Standard). This work has been funded by the 2010 Research National Plan in Spain of the Ministry of Science and Innovation with the project ‘Integral Analysis of Digital Circuits and Systems for Aerospace Applications (RENASER+) (TEC 2010 - 22095 -C 03 - 01) ...|$|R
40|$|An {{experimental}} investigation with 4 kW diodes {{laser system}} {{was carried out}} to {{study the effects of}} laser hardening process parameters on properties of P/M steels, based on prealloyed and diffusion-bonded powders. Eight different process parameters have been applied to optimize the process afterwards they have been set to two different compositions. The independent variables investigated have been: surface temperature, T, and laser travel speed (mm/min). The microstructural features of the laser hardened P/M steels were analyzed using LOM and the surface morphology has been characterized by SEM. Hardened depth both via hardness test and via optical measures were evaluated as well and used as responses for the ANOVA of the experimental data gathered. This work reveals the possibility to apply a <b>selective</b> and precise <b>hardening</b> treatment, like Laser Transformation hardening (LHT), to high-property P/M steel parts. Thanks to ANOVA analysis the correlation between energy density (ED) and optical dimension of treated zone has been identified. By increasing temperature and speed the density energy ED decreases, indeed at high temperature and speed the heated zone is larger. At the same speed, a temperature increase leads to a surface hardness decrease. A similar value of hardness depth (about 0. 8 mm) has been detected by means of hardness profiles. The result has been strengthened by scratch test. The parabolic zone presents a very fine martensitic structure on the top of laser trail, and bainite microstructure on transition zone, as literature related. The transition zone between the laser affected zone and the base metal exhibits a micro hardness gradient. It is also possible to notice some Cu diffusion from outer to inner zone. By comparison among LHT specimens and sintering-hardened and stress-relieved one, (characterized by similar chemical compositions) penetration depth and hardness values appear similar. Transformation Hardening (LTH) is a suitable process for producing hard surface, on defined spots of P/M components, through the action of a scanning laser beam. The short heating time enables to decrease part distortion and surface oxidation, in comparison with sinter hardened. The possibility of very <b>selective</b> and precise <b>hardening</b> treatment seems to open new possibilities to widen the use of high-property P/M parts, based on advanced design, for demanding applications...|$|R
40|$|We present CLEAR (Cross-Layer Exploration for Architecting Resilience), a {{first of}} its kind {{framework}} which overcomes a major challenge in the design of digital systems that are resilient to reliability failures: achieve desired resilience targets at minimal costs (energy, power, execution time, area) by combining resilience techniques across various layers of the system stack (circuit, logic, architecture, software, algorithm). This is also referred to as cross-layer resilience. In this paper, we focus on radiation-induced soft errors in processor cores. We address both single-event upsets (SEUs) and single-event multiple upsets (SEMUs) in terrestrial environments. Our framework automatically and systematically explores the large space of comprehensive resilience techniques and their combinations across various layers of the system stack (586 cross-layer combinations in this paper), derives cost-effective solutions that achieve resilience targets at minimal costs, and provides guidelines for the design of new resilience techniques. Our results demonstrate that a carefully optimized combination of circuit-level hardening, logic-level parity checking, and micro-architectural recovery provides a highly cost-effective soft error resilience solution for general-purpose processor cores. For example, a 50 x improvement in silent data corruption rate is achieved at only 2. 1 % energy cost for an out-of-order core (6. 1 % for an in-order core) with no speed impact. However, (application-aware) <b>selective</b> circuit-level <b>hardening</b> alone, guided by a thorough analysis of the effects of soft errors on application benchmarks, provides a cost-effective soft error resilience solution as well (with ~ 1 % additional energy cost for a 50 x improvement in silent data corruption rate). Comment: Unedited version of paper published in Transactions on Computer-Aided Design of Integrated Circuits and System...|$|R
40|$|Abstract—Aggressive {{technology}} scaling has necessitated {{the development}} of techniques to ensure resilience to device faults, including soft errors, circuit wear-out, variability, and environmental effects. All error resilience techniques employ some form of redundancy, resulting in added cost such as area or power overhead. Existing <b>selective</b> <b>hardening</b> techniques have been focused on identifying on the most vulnerable components and then statically harden them. This paper proposes a new technique that can further reduce this overhead for error resilience mechanisms that are controllable. The key idea is to generate control predicates 1 that can turn the resilience mechanisms ON and OFF dynamically, at the right time. These predicates are mined using an optimization formulation that leverages fault injection simulation information. Our experimental results demonstrate that our approach significantly outperforms the static hardening approach for optimizing soft error resilience. I...|$|E
40|$|Silver halide {{sensitized}} gelatin (SHSG) holograms {{are similar}} to holograms recorded in dichromated gelatin (DCG), the main recording material for holographic optical elements (HOE’s). The drawback of DCG is its low sensitivity and limited spectral response. Silver halide materials can be processed {{in such a way}} that the final hologram will have properties like a DCG hologram. Recently this technique has become more interesting since the introduction of new ultra-high-resolution silver halide emulsions. An optimized processing technique for transmission HOE’s recorded in these materials is introduced. Diffraction efficiencies over 90 % can be obtained for transmissive diffraction gratings. Understanding the importance of the <b>selective</b> <b>hardening</b> process has made it possible to obtain results similar to conventional DCG processing. The main advantage of the SHSG process is that high-sensitivity recording can be performed with laser wavelengths anywhere within the visible spectrum. This simplifies the manufacturing of high-quality, large-format HOE’s...|$|E
40|$|Digital {{electronic}} systems in automotive applications {{are in charge}} of different tasks, ranging from very critical control functions (e. g., airbag, ABS, ESP) to comfort services (e. g., handling of mirrors, seats, windows, wipers). Hardening these systems involves suitably trading off cost and reliability. Due to standards and regulations in the area, the reliability of subsystems involved even in the least critical applications has to be evaluated, and in most cases hardening has to be performed with very low extra cost. In this work, two approaches are proposed for hardening the LIN bus, which implements a serial communication network typically used in low-throughput and low-cost sub-systems in automotive applications. First, critical elements in LIN nodes are identified and some techniques to harden them are proposed following a <b>selective</b> <b>hardening</b> approach. Secondly, collaborative hardening techniques are proposed for reducing global sensitivity in a LIN network built with commercial devices, trying to achieve a high degree of robustness in the network with low cost solutions. We report some experimental results allowing evaluating the hardware cost and the robustness of the proposed technique...|$|E
40|$|We {{present a}} {{first of its}} kind {{framework}} which overcomes a major challenge in the design of digital systems that are resilient to reliability failures: achieve desired resilience targets at minimal costs (energy, power, execution time, area) by combining resilience techniques across various layers of the system stack (circuit, logic, architecture, software, algorithm). This is also referred to as cross-layer resilience. In this paper, we focus on radiation-induced soft errors in processor cores. We address both single-event upsets (SEUs) and single-event multiple upsets (SEMUs) in terrestrial environments. Our framework automatically and systematically explores the large space of comprehensive resilience techniques and their combinations across various layers of the system stack (586 cross-layer combinations in this paper), derives cost-effective solutions that achieve resilience targets at minimal costs, and provides guidelines for the design of new resilience techniques. We demonstrate the practicality and effectiveness of our framework using two diverse designs: a simple, in-order processor core and a complex, out-of-order processor core. Our results demonstrate that a carefully optimized combination of circuit-level hardening, logic-level parity checking, and micro-architectural recovery provides a highly cost-effective soft error resilience solution for general-purpose processor cores. For example, a 50 x improvement in silent data corruption rate is achieved at only 2. 1 % energy cost for an out-of-order core (6. 1 % for an in-order core) with no speed impact. However, <b>selective</b> circuit-level <b>hardening</b> alone, guided by a thorough analysis of the effects of soft errors on application benchmarks, provides a cost-effective soft error resilience solution as well (with ~ 1 % additional energy cost for a 50 x improvement in silent data corruption rate). Comment: Extended version of paper published in Proceedings of the 53 rd Annual Design Automation Conferenc...|$|R
30|$|The key {{takeaway}} {{from this}} section is that impacts on data quality can provide strong markers for an underlying cyber-attack. Noise, outliers and missing values are all commonly observed issues which quality checking methods may {{be programmed to}} detect, analyze and base decisions on. Certain sophisticated attacks like APTs, insider threats, sniffing, and social engineering have indirect impacts on quality which a checking method {{may not be able}} to detect with enough confidence or precision. Additional solutions are required to mitigate such attacks in the synchrophasor domain. These solutions include statistical methods like divergence, correlation, regression and substitution; intelligent methods like neural networks and evolutionary algorithms for event classification and prediction, logistic regression for substitution; technologies like VPNs, firewalls, ID/IPS, anomaly detectors, <b>selective</b> encryption, port <b>hardening,</b> network isolation and use of TLS/SSL, SSH; and human-in-the-loop solutions like advanced visualization techniques, awareness and training, and stakeholder engagements. While the impacts on quality can also be due to underlying device or measurement errors, most of the works in the literature assume the data has been subject to delay/loss, manipulation or theft intentionally. This paves way for the recommendation that the upcoming research in this area must look at ways to differentiate the impacts on data quality due to attacks from errors.|$|R
40|$|An {{experimental}} investigation with 4 kW diodes {{laser system}} {{was carried out}} to {{study the effects of}} laser hardening process parameters on properties of P/M steels, based on prealloyed, admixed or diffusion-bonded powders. The surface temperature of treated zone has been measured by pyrometer (to avoid local melting or surface damaging) and the travel speed on scanning the surface of test samples has been an investigated variable. A special attention has been given to specimen clamping, to avoid any misalignment with the beam movement. The independent variables investigated have been: surface temperature, T, travel speed (mm/min). The microstructural features of the laser hardened P/M steels were analyzed using LOM and the surface morphology has been characterized by SEM. Hardened depth (HD), hardened width (HW) and overall cross-sectional hardened area (HA) were measured as well and used as responses for the ANOVA of the experimental data gathered. The microhardness profiles present a sharp drop at low distance from the hardened surface. The typical splitting between hardened zone and heat-affected zone (HAZ), well known from laser hardened fully dense steels, holds also for P/M (porous) steels. The research showed that Laser Transformation Hardening (LTH) is a suitable process for producing hard, wear resistant surface, on defined spots of P/M components, through the action of a scanning laser beam. The short heating time enables to decrease part distortion and surface oxidation, in comparison with induction hardening. The possibility of very <b>selective</b> and precise <b>hardening</b> treatment seems to open new possibilities to widen the use of high-property P/M parts, based on advanced design, for demanding applications...|$|R
40|$|International audienceMany {{applications}} impose safety and/or security constraints {{which require}} protections against {{the effects of}} transient faults. The most critical elements must be identified to achieve good efficiency and cost trade-offs when <b>selective</b> <b>hardening</b> is necessary. In embedded microprocessor-based systems (i. e. most of SoCs) the system dependability is strongly correlated with internal register criticality since external memories are protected by error correcting codes. The robustness analysis of these systems consists in precisely assessing the criticality of internal registers used by the application program. The evaluation often aims at selecting a minimum number of registers to be protected. At the same time, the accurate assessment is complicated {{in the case of}} recent processors because of the evolution of architectures and the implementation of new mechanisms to improve performance (e. g., pipeline, forwarding mechanisms,…). Classical fault-injection approaches require long experimental times to determine the most critical set of registers. This paper presents an approach based on modeling the effect of transient faults taking into account the micro-architectural features and proposes a new methodology to refine and accelerate evaluations of register criticality. This new approach is compared with fault injections. The results show the effectiveness of the prediction algorithm...|$|E
40|$|Surface {{treatment}} of steel, {{for a higher}} wear resistance, is necessary in all industrial branches that deal with steel. One {{of the most effective}} methods of metal surface heat treatment is represented by laser hardening. In general, compared to other techniques, laser hardening needs a much lower heat input into the material. Compared with other conventional processes, diode laser treatment allows us to obtain some advantages, such as minimum distortion in the parts and <b>selective</b> <b>hardening</b> of specific areas of components. In this paper, an experimental investigation on the surface thermal {{treatment of}} different steels by means of a diode laser has been carried out. Experimental tests have been performed on three types of steels, selecting different values of beam power and workpiece speed. Hardness and micro-hardness measurements have been executed on the treated samples in order to assess the influence of laser machining parameters and of workpiece materials on the treatment process. Experimental results have been compared with those obtained by using a mono-dimensional thermal model. The morphology of the heat affected zone of the specimens has been investigated by scanning electron microscopy...|$|E
40|$|Nowadays, most of {{the tools}} of Volvo Cars Body Components (VCBC) in Olofström are {{hardened}} by flame hardening or induction hardening process. Indeed, VCBC has developed their own equipment for flame and induction hardening: However, Volvo has detected that 74 % of {{the total cost of}} one year maintenance was caused by trim dies and the main reason is actually trim edge problems which represent 26 % of year maintenance. This is why the R&D Forming &amp; Material department is investigating some ways to reduce those excessive costs caused by cutting tools. As a consequence, in this project, we will study the laser hardening of cutting tools. The aims of this project were to investigate: * The laser hardening process * The limits of different tool materials with this laser process This master thesis should give a beginning of answer to the following question: Is the laser hardening process a suitable alternative to induction hardening process for trim dies at VCBC ? This work has been conducted at Volvo Cars Body Components in the R&D Forming &amp; Material department. This report is divided in four main parts. The two first parts deal with theories about laser and <b>selective</b> <b>hardening</b> processes. The two last ones are the report of my experimental investigation that I run during the thesis at VCBC in Olofström. Validerat; 20101217 (root...|$|E
40|$|Commercial {{off-the-shelf}} microprocessors are {{the core}} of low-cost embedded systems due to their programmability and cost-effectiveness. Recent advances in electronic technologies have allowed remarkable improvements in their performance. However, they have also made microprocessors more susceptible to transient faults induced by radiation. These non-destructive events (soft errors), may cause a microprocessor to produce a wrong computation result or lose control of a system with catastrophic consequences. Therefore, soft error mitigation has become a compulsory requirement for {{an increasing number of}} applications, which operate from the space to the ground level. In this context, this paper uses the concept of <b>selective</b> <b>hardening,</b> which is aimed to design reduced-overhead and flexible mitigation techniques. Following this concept, a novel flexible version of the software-based fault recovery technique known as SWIFT-R is proposed. Our approach makes possible to select different registers subsets from the microprocessor register file to be protected on software. Thus, design space is enriched with a wide spectrum of new partially protected versions, which offer more flexibility to designers. This permits to find the best trade-offs between performance, code size, and fault coverage. Three case studies have been developed to show the applicability and flexibility of the proposal. This work was funded by the Ministry of Science and Innovation in Spain with the project ‘RENASER+: Integral Analysis of Digital Circuits and Systems for Aerospace Applications’ (TEC 2010 - 22095 -C 03 - 01) ...|$|E
40|$|In late-age silicon, soft errors {{become an}} issue even for low-margin products. Since {{classical}} hardening techniques are associated with costs {{which may not be}} acceptable for such ICs, <b>selective</b> <b>hardening</b> which targets only a subset of all possible soft errors has been suggested. We propose a soft error selection method based on severity of an error’s impact on system behavior. Some soft errors disturb the system in an acceptable way; no hardening is required against such soft errors. We suggest a definition of “acceptable misbehavior ” consisting of two conditions: First, the system must return to the fault-free state after a given number of cycles k, called period of grace. During the period of grace, the system is allowed to deviate from its reference behavior. However, the second condition demands that the system misbehavior during the period of grace is acceptable, i. e., the deviation from the reference behavior must be limited. The definition of the acceptable misbehavior requires application-specific information; it is related to the concepts of error tolerance and threshold testing. We propose a methodology for identifying soft errors which are acceptable with respect to the period of grace paradigm. We envision low-cost soft-error hardening methods which guarantee acceptable misbehavior according to the period of grace paradigm (lightweight fault tolerance) rather than ensure total correctness as current fault tolerance approaches do. We report on a first case study which is shown to have acceptable misbehavior for some of the soft errors even without such hardening...|$|E
40|$|Software-based fault {{tolerance}} techniques are a low-cost way to protect processors against soft errors. However, they introduce significant overheads to the execution time and code size, which consequently increases the energy consumption. System operating with time or energy restrictions {{may not be able}} to use these techniques. For this reason, this work proposes new software-based {{fault tolerance}} techniques with lower overheads and similar fault coverage to state-of-the-art software techniques. Thus, they can meet the system constraints. In addition, the shorter execution time reduces the exposure time to radiation. Consequently, the reliability is higher for the same fault coverage. Techniques can work with error correction or error detection. Once detection is less costly than correction, this work focuses on software-based detection techniques. Firstly, a set of data-flow techniques called VAR is proposed. The techniques are based on general building rules to allow an exhaustive assessment, in terms of reliability and overheads, of different technique variations. The rules define how the technique duplicates the code and insert checkers. Each technique uses a different set of rules. Then, a control-flow technique called SETA (Software-only Error-detection Technique using Assertions) is introduced. Comparing SETA with a state-of-the-art technique, SETA is 11. 0 % faster and occupies 10. 3 % fewer memory positions. The most promising data-flow techniques are combined with the control-flow technique in order to protect both dataflow and control-flow of the target application. To go even further with the reduction of the overheads, methods to selective apply the proposed software techniques have been developed. For the data-flow techniques, instead of protecting all registers, only a set of selected registers is protected. The set is selected based on a metric that analyzes the code and rank the registers by their criticality. For the control-flow technique, two approaches are taken: (1) removing checkers from basic blocks: all the basic blocks are protected by SETA, but only selected basic blocks have checkers inserted, and (2) selectively protecting basic blocks: only a set of basic blocks is protected. The techniques and their selective versions are evaluated in terms of execution time, code size, fault coverage, and Mean Work To Failure (MWTF), which is a metric to measure the trade-off between fault coverage and execution time. Results show that was possible to reduce the overheads without affecting the fault coverage, and for a small reduction in the fault coverage it was possible to significantly reduce the overheads. Lastly, since the evaluation of all the possible combinations for <b>selective</b> <b>hardening</b> of every application takes too much time, this work uses a method to extrapolate the results obtained by simulation in order to find the parameters for the selective combination of data and control-flow techniques that are probably the best candidates to improve the trade-off between reliability and overheads...|$|E
40|$|As {{the feature}} size scales down to deep {{nanometer}} regimes, it has enabled the designers to fabricate chips with billions of transistors. The availability of such abundant computational resources {{on a single}} chip {{has made it possible}} to design chips with multiple computational cores, resulting in the inception of Chip Multiprocessors (CMPs). The widespread use of CMPs has resulted in a paradigm shift from computation-centric architectures to communication-centric architectures. With the continuous {{increase in the number of}} cores that can be fabricated on a single chip, communication between the cores has become a crucial factor in its overall performance. Network-on-Chip (NoC) paradigm has evolved into a standard on-chip interconnection network that can efficiently handle the strict communication requirements between the cores on a chip. The components of an NoC include routers, that facilitate routing of data between multiple cores and links that provide raw bandwidth for data traversal. While diminishing feature size has made it possible to integrate billions of transistors on a chip, the advantage of multiple cores has been marred with the waning reliability of transistors. Components of an NoC are not immune to the increasing number of hard faults and soft errors emanating due to extreme miniaturization of transistor sizes. Faults in an NoC result in significant ramifications such as isolation of healthy cores, deadlock, data corruption, packet loss and increased packet latency, all of which have a severe impact on the performance of a chip. This has stimulated the need to design resilient and fault tolerant NoCs. This thesis handles the issue of fault tolerance in NoC routers. Within the NoC router, the focus is specifically on the router pipeline that is responsible for the smooth flow of packets. In this thesis we propose two different fault tolerant architectures that can continue to operate in the presence of faults. In addition to these two architectures, we also propose a new reliability metric for evaluating soft error tolerant techniques targeted towards the control logic of the NoC router pipeline. First, we present Shield, a fault tolerant NoC router architecture that is capable of handling both hard faults and soft errors in its pipeline. Shield uses techniques such as spatial redundancy, exploitation of idle resources and bypassing a faulty resource to achieve hard fault tolerance. The use of these techniques reveals that Shield is six times more reliable than baseline-unprotected router. To handle soft errors, Shield uses <b>selective</b> <b>hardening</b> technique that includes hardening specific gates of the router pipeline to increase its soft error tolerance. To quantify soft error tolerance improvement, we propose a new metric called Soft Error Improvement Factor (SEIF) and use it to show that Shield’s soft error tolerance is three times better than that of the baseline-unprotected router. Then, we present Soft Error Tolerant NoC Router (STNR), a low overhead fault tolerating NoC router architecture that can tolerate soft errors in the control logic of its pipeline. STNR achieves soft error tolerance based on the idea of dual execution, comparison and rollback. It exploits idle cycles in the router pipeline to perform redundant computation and comparison necessary for soft error detection. Upon the detection of a soft error, the pipeline is rolled back to the stage that got affected by the soft error. Salient features of STNR include high level of soft error detection, fault containment and minimum impact on latency. Simulations show that STNR has been able to detect all injected single soft errors in the router pipeline. To perform a quantitative comparison between STNR and other existing similar architectures, we propose a new reliability metric called Metric for Soft error Tolerance (MST) in this thesis. MST is unique in the aspect that it encompasses four crucial factors namely, soft error tolerance, area overhead, power overhead and pipeline latency overhead into a single metric. Analysis using MST shows that STNR provides better reliability while incurring low overhead compared to existing architectures...|$|E

