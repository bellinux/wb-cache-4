79|166|Public
5000|$|Multicolour PPI Raster Scan Display, {{presenting}} both MTI and <b>Synthetic</b> <b>Video</b> ...|$|E
5000|$|... #Caption: Detail of a radarscreen: {{a target}} in skin paint mode (amber) and as <b>synthetic</b> <b>video</b> (white) ...|$|E
40|$|Traditionally, Markovian {{models have}} been used to {{statistically}} represent variable bit rate (VBR) video traffic. These models exhibit short-range dependence (SRD). However, recent measurements show that VBR video traffic possesses self-similar characteristics, meaning that long-range dependence (LRD) in the traffic stream lasts much longer than traditional models can capture. This paper investigates the relative performance impacts of SRD and LRD in VBR video traffic, by conducting a simulation study using four sets of VBR video data: (1) a real video trace, with both SRD and LRD, (2) a <b>synthetic</b> <b>video</b> trace with only LRD, (3) a <b>synthetic</b> <b>video</b> trace with only SRD, and (4) a <b>synthetic</b> <b>video</b> trace without SRD and LRD. The performance study suggests that video traffic models must capture both SRD and LRD. 1 Introduction Variable bit rate (VBR) video is expected {{to become one of the}} major applications used on high-speed networks such as ATM-based B-ISDN [Huang et al., 1995], [Heyman and [...] ...|$|E
40|$|International audienceVisible speech {{movements}} were motion captured and parameterized. Coarticulated targets were extracted from VCVs and modeled to generate arbitrary German utterances by target interpolation. The system {{was extended to}} synthesize English utterances by a mapping to German phonemes. An evaluation {{by means of a}} modified rhyme test reveals that the <b>synthetic</b> <b>videos</b> of isolated words increase the recognition scores from 27 % to 47. 5 % when added to audio only presentatio...|$|R
40|$|Visible speech {{movements}} were motion captured and parameterized. Coarticulated targets were extracted from VCVs and modeled to generate arbitrary German utterances by target interpolation. The system {{was extended to}} synthesize English utterances by a mapping to German phonemes. An evaluation {{by means of a}} modified rhyme test reveals that the <b>synthetic</b> <b>videos</b> of isolated words increase the recognition scores from 27 % to 47. 5 % when added to audio only presentation. Index Terms: talking head, intelligibility, evaluation 1...|$|R
40|$|We {{describe}} algorithms for authoring and viewing {{high resolution}} immersive videos. Given {{a set of}} cameras designed to be aligned more or less at the same nodal point, we first present a process for stitching seamlessly synchronized streams of videos into a single immersive video corresponding to the video of the abstract multi-head camera. We describe a general registration technique onto geometric envelopes based on minimizing a novel appropriate objective function, and detail our compounded image synthesis algorithm of multi-head cameras. Several new environment maps with low discrepancy are presented. Finally, we give details on the viewer implementation. Experimental results on both immersive real and <b>synthetic</b> <b>videos</b> are shown...|$|R
40|$|Advances {{in video}} {{technologies}} combined with increased usage of <b>synthetic</b> <b>video</b> artifacts even in natural video scenes may cause problems in subjective perception of quality. In this paper investigation {{is done on}} how subjective quality assessment scores depend on technical specifications of video interface (YPbPr, YCbCr and YIQ) and video content containing diverse <b>synthetic</b> <b>video</b> artifacts. Single Stimulus (SS) and Simultaneous Double Stimulus (SDS) subjective quality assessment methods were used and results compared. It was also revealed that SDS method allows better differentiation of subjective quality. This result agrees well with statistics obtained from crowd-based questionnaires used in separate study where most observers preferred SDS as a simpler and more comfortable method compared to SS method...|$|E
40|$|This article explores an {{application}} of Bayesian Programming to behaviours for <b>synthetic</b> <b>video</b> games characters. We {{address the problem of}} real-time reactive selection of elementary behaviours for an agent playing a first person shooter game. We show how Bayesian Programming can lead to condensed and easier formalisation of finite state machine-like behaviour selection, and lend itself to learning by imitation, in a fully transparent way for the player. 1...|$|E
30|$|In {{this paper}} we have {{described}} our strategy to perform audiovisual text-to-speech synthesis. We adopted the unit selection method {{to work with}} multimodal units, using audiovisual selection costs. This strategy {{makes it possible to}} create multimodal speech signals of which the synthetic audio mode and the <b>synthetic</b> <b>video</b> mode are highly coherent. This differs from most strategies found in the literature, which use completely separated systems, methods and databases to construct the auditory and the visual mode of the output speech.|$|E
40|$|Publicado en 2015 Institute of Electrical and Electronics Engineers (IEEE) 6 th Latin American Symposium on Circuits & Systems (LASCAS) el 24 / 02 / 2015 We {{present a}} new {{approach}} to compute the motion estimation in digital videos using 2 D amplitude-modulation frequency-modulation (AM-FM) methods. The optical flow vectors are computed using an iteratively reweighted norm for total variation (IRN-TV) algorithm. We compare the proposed method using <b>synthetic</b> <b>videos</b> versus a previous three-dimensional AMFM based method and available motion estimation methods available in free and commercial software. The results are promising, in terms of accuracy, producing a full density estimation with more accurate results than the other methods...|$|R
40|$|Publicado en 2014 Institute of Electrical and Electronics Engineers (IEEE) Global Conference on Signal and Information Processing el 05 / 12 / 2014 "We {{present a}} first {{approach}} to a new method to compute the motion estimation in digital videos using the two-dimensional instantaneous frequency information computed using amplitude-modulation frequency-modulation (AM-FM) methods. The optical flow vectors are computed using an iteratively reweighted norm for total variation (IRN-TV) algorithm. We compare the proposed method using <b>synthetic</b> <b>videos</b> versus a previous three-dimensional AM-FM based method and available motion estimation methods such as a phase-based, Horn-Schunck and the Lucas-Kanade methods. The results are promising producing a full density estimation with more accurate results than the other methods. ...|$|R
40|$|This paper {{deals with}} the {{selection}} of relevant motion from multi-object movement. The proposed method {{is based on a}} multi-scale approach using features extracted from optical flow and global rarity quantification to compute bottom-up saliency maps. It shows good results from four objects to dense crowds with increasing performance. The results are convincing on <b>synthetic</b> <b>videos,</b> simple real video movements, a pedestrian database and they seem promising on very complex videos with dense crowds. This algorithm only uses motion features (direction and speed) but can be easily generalized to other dynamic or static features. Video surveillance, social signal processing and, in general, higher level scene understanding can benefit from this method. Index Terms â€” crowd analysis, social signa...|$|R
40|$|This {{paper is}} {{concerned}} with the efficient temporal propagation of correspondences between frames of two video sequences, an integral component of many video processing tasks. The main contribution of the paper is a framework for the recursive propagation of these correspondences. The propagation consists of a time update step and a measurement update step. The time update depends only on the dynamics of the rotating source cameras, while the measurement update can be tailored to any member of a general class of image correspondence algorithms. Using these results, the correspondence between points of each frame pair can be propagated and updated in a fraction of the time required to estimate correspondences anew at every frame. We discuss an application of the recursive correspondence propagation framework to the creation of virtual video. Previous virtual view algorithms have been used to generate <b>synthetic</b> <b>video</b> of a static scene, in which objects seem frozen in time. In contrast, the algorithms described here allow the creation of "true" virtual video, {{in the sense that the}} <b>synthetic</b> <b>video</b> evolves dynamically along with the scene. While virtual video is our motivating application, the recursive correspondence propagation framework applies to any two-camera video application in which correspondence is difficult and prohibitively time-consuming to estimate by processing frame pairs independently. 2. IMAGE DYNAMIC...|$|E
40|$|We {{describe}} {{a family of}} non-linear sequence models that is substantially more powerful than hidden Markov models or linear dynamical systems. Our models have simple approximate inference and learning procedures that work well in practice. Multilevel representations of sequential data can be learned one hidden layer at a time, and adding extra hidden layers improves the resulting generative models. The models can be trained with very high-dimensional, very non-linear data such as raw pixel sequences. Their performance is demonstrated using <b>synthetic</b> <b>video</b> sequences of two balls bouncing in a box...|$|E
40|$|A <b>synthetic</b> <b>video</b> dataset, scenario, {{and task}} were {{included}} in the 2009 VAST Challenge, to allow participants an opportunity to demonstrate visual analytic tool use on video data. This is the first time a video challenge had been presented as part of the VAST contest and provided interesting challenges in task and dataset development, video analytic tool development, and metrics for judging entries. We describe the considerations and requirements for generation of a usable challenge, the video creation itself, and some submissions and assessments from that mini-challenge. Categories and Subject Descriptors H 5. 1. [Information interfaces and presentation]: Multimedia Information Systems â€“ evaluation / methodology...|$|E
40|$|Ãƒ The {{corresponding}} author. In this paper, {{we propose}} a watermarking technique for MPEG- 4 2 D meshes, the essential {{tool of the}} MPEG- 4 basic animated texture profile. Although watermarking techniques have been successfully applied to natural images and videos, little progress {{is made in the}} area of <b>synthetic</b> <b>videos.</b> By applying the multiresolution analysis of the mesh model in the temporal domain, important feature locations can be extracted for watermarking. We adopt a spread-spectrum approach for watermark embedding, i. e., the watermark is inserted by perturbing some of the significant wavelet coefficients. We have evaluated the proposed scheme against random noise and affine attacks using six different bases of wavelet filters. Among the filters examined the 9 / 7 F filter achieves the best performance. 1...|$|R
40|$|International audienceVisible speech {{movements}} were optically motion captured and parameterized {{by means of}} a guided PCA. Co-articulated consonantal targets were extracted from VCVs, vocalic targets were extracted from these VCVs and from sustained vowels. Targets were selected or combined to derive target sequences for phone chains of arbitrary German utterances. Parameter trajectories for these utterances are generated by interpolating targets through linear to quadratic functions that reflect the degree of co-articulatory influence. Videos of test words embedded in a carrier sentence were rendered from parameter trajectories for an evaluation {{in the form of a}} rhyme test in noise. Results show that the <b>synthetic</b> <b>videos</b> - although intelligible only somewhat above chance level when played alone - significantly increase the recognition scores from 45. 6 % in audio alone presentation to 60. 4 % in audiovisual presentation...|$|R
40|$|This Master's thesis {{deals with}} the design and {{development}} of tools for generating a synthetic dataset for traffic analysis purposes. The first part contains a brief introduction to the vehicle detection and rendering methods. Blender and the set of scripts are used to create highly customizable training images dataset and <b>synthetic</b> <b>videos</b> from a single photograph. Great care is taken to create very realistic output, that is suitable for further processing in field of traffic analysis. Produced images and videos are automatically richly annotated. Achieved results are tested by training a sample car detector and evaluated with real life testing data. Synthetic dataset outperforms real training datasets in this comparison of the detection rate. Computational demands of the tools are evaluated as well. The final part sums up the contribution of this thesis and outlines some extensions of the tools for the future...|$|R
40|$|International audienceIn this article, {{we propose}} a novel {{approach}} to detect moving objects in H. 264 compressed bitstreams. More precisely, we describe a multi-modal background subtraction technique that uses the size of macroblocks in order to label them as belonging to {{the background of the}} observed scene or not. Here, we integrate an adaptive Gaussian mixture-based scheme to model the background. We evaluate our contribution using the PETS video dataset and a realist <b>synthetic</b> <b>video</b> sequence rendered by a 3 -D urban environment simulator. We compare two different background models, and we show that the Gaussian mixture-based is the best and outperforms other techniques that use macro bloc sizes...|$|E
40|$|Principal {{component}} pursuit (PCP) is {{a state-of-the-art}} approach for background estimation problems. Due to their higher computational cost, PCP algorithms, such as robust {{principal component analysis}} (RPCA) and its variants, are not feasible in processing high definition videos. To avoid the curse of dimensionality in those algorithms, several methods have been proposed to solve the background estimation problem in an incremental manner. We propose a batch-incremental background estimation model using a special weighted low-rank approximation of matrices. Through experiments with real and <b>synthetic</b> <b>video</b> sequences, we demonstrate that our method is superior to the state-of-the-art background estimation algorithms such as GRASTA, ReProCS, incPCP, and GFL...|$|E
40|$|An {{innovative}} approach to aiding farmer decisions that involve seasonal climate risk has been trialed with farmers in Andhra Pradesh, India. A key concept {{has been to}} copy concepts of 'discussion-support' approaches, rather than only relying on computerized decision-support approaches. Use of 2 d Life/eLearning distance education methods have been trialed with these farmers and advisers with some success. However, {{it is suggested that}} vital aspects not necessarily related to core climate and crop science issues, such as farmers' dress and informal interactions within a discussion environment must be appropriately captured in any <b>synthetic</b> <b>video</b> discussion-support if this approach is to gain widespread uptake. ...|$|E
40|$|Deep {{learning}} for human action recognition in videos is making significant progress, but is slowed down by its dependency on expensive manual labeling of large video collections. In this work, we investigate {{the generation of}} synthetic training data for action recognition, as it has recently shown promising results {{for a variety of}} other computer vision tasks. We propose an interpretable parametric generative model of human action videos that relies on procedural generation and other computer graphics techniques of modern game engines. We generate a diverse, realistic, and physically plausible dataset of human action videos, called PHAV for "Procedural Human Action Videos". It contains a total of 39, 982 videos, with more than 1, 000 examples for each action of 35 categories. Our approach is not limited to existing motion capture sequences, and we procedurally define 14 synthetic actions. We introduce a deep multi-task representation learning architecture to mix <b>synthetic</b> and real <b>videos,</b> even if the action categories differ. Our experiments on the UCF 101 and HMDB 51 benchmarks suggest that combining our large set of <b>synthetic</b> <b>videos</b> with small real-world datasets can boost recognition performance, significantly outperforming fine-tuning state-of-the-art unsupervised generative models of videos. Comment: Accepted for publication at CVPR 2017. [URL]...|$|R
40|$|International audienceThis paper {{addresses}} {{the problem of}} human action recognition in realistic videos. We follow the recently successful local approaches and represent videos by means of local motion descriptors. To overcome the huge variability of human actions in motion and appearance, we propose a supervised approach to learn local motion descriptors - actlets - from a large pool of annotated video data. The main motivation behind our method is to construct action-characteristic representations of body joints undergoing specific motion patterns while learning invariance with respect to changes in camera views, lighting, human clothing, and other factors. We avoid the prohibitive cost of manual supervision and show how to learn actlets automatically from <b>synthetic</b> <b>videos</b> of avatars driven by the motion-capture data. We evaluate our method and show its significant improvement {{as well as its}} complementarity to existing techniques on the challenging UCF-sports and YouTube-actions datasets...|$|R
40|$|Visible speech {{movements}} were optically motion captured and parameterized {{by means of}} a guided PCA. Co-articulated consonantal targets were extracted from VCVs, vocalic targets were extracted from these VCVs and from sustained vowels. Targets were selected or combined to derive target sequences for phone chains of arbitrary German utterances. Parameter trajectories for these utterances are generated by interpolating targets through linear to quadratic functions that reflect he degree of co-articulatory influence. Videos of test word embedded in a carrier sentence were rendered from parameter trajectories for an evaluation {{in the form of a}} rhyme test with carrier sentence in noise. Results show that the <b>synthetic</b> <b>videos</b> â€“ although intelligible only somewhat above chance level when played alone â€“ significantly increase the recognition scores from 45. 6 % in audio alone presentation to 60. 4 % in audiovisual presentation. Index Terms: talking head, intelligibility, evaluation, trajectory generatio...|$|R
40|$|This {{paper is}} an {{evaluation}} of the distribution of quantized coefficients resulting from the disparity of an automotive stereo vision system. The system captures the scene with two cameras, computes the disparity with Semi-Global Matching and encodes the left view and the disparity for transmission. Real world and <b>synthetic</b> <b>video</b> sequences were used to evaluate the coefficient distributions of the system under normal and challenging weather conditions. The results show, that the quantized disparity coefficients in frequency space have consistently lower entropy compared to the coefficients of the video scenes. Therefore, it is advantageous for the system to compress the disparity instead of one of the two video streams...|$|E
40|$|The {{objective}} of this work is to generate automatically 3 D animations from scenario models defined by human experts for scenario recognition. These animations are useful for two reasons. First, they can help experts to validate their scenario models. Second, <b>synthetic</b> <b>video</b> sequences can be generated from these 3 D animations and serve as reference data to evaluate video interpretation algorithms. An animation is generated in two stages. First, we transform a scenario model into a visualisation model. Second, we generate animation instances from visualisation models. The first results obtained are promising. We have automatically generated 3 D animations for two applications: the monitoring of a bank agency and a metro platform...|$|E
40|$|A novel {{scheme for}} depth {{sequences}} compression, {{based on a}} perceptual coding algorithm, is proposed. A depth sequence describes the object position in the 3 D scene, and is used, in Free Viewpoint Video, for the generation of <b>synthetic</b> <b>video</b> sequences. In perceptual video coding the human visual system characteristics are exploited to improve the compression efficiency. As depth sequences are never shown, the perceptual video coding, assessed over them, is not effective. The proposed algorithm {{is based on a}} novel perceptual rate distortion optimization process, assessed over the perceptual distortion of the rendered views generated through the encoded depth sequences. The experimental results show the effectiveness of the proposed method, able to obtain a very considerable improvement of the rendered view perceptual quality...|$|E
40|$|Traditional {{approach}} to performing motion estimation on video {{is to find}} the frame-to-frame coherence in the image sequence. In a <b>synthetic</b> animation <b>video</b> sequence, we can take advantage of information used in rendering the animation to guide the motion estimation in order to achieve effective compression. In this proposal, we discuss potential solutions towards the problem as future research directions...|$|R
40|$|Microscopic {{features}} of the human retina can be resolved noninvasively using an adaptive optics scanning laser ophthalmoscope (AOSLO). We describe an improved method to track and quantify the speed of moving objects in AOSLO videos, which is necessary for characterizing the hemodynamics of retinal capillaries. During video acquisition, the objects of interest are in constant motion relative to the background tissue (object motion). The background tissue is in constant motion relative to the AOSLO, due to continuous eye motion during video recordings (eye motion). The location at which AOSLO acquires data is also in continuous motion, since the imaging source is swept in a raster scan across the retina (raster scanning). We show {{that it is important}} to take into consideration the combination of object motion, eye motion, and raster scanning for accurate quantification of object speeds. The proposed methods performed well on both experimental AOSLO videos as well as <b>synthetic</b> <b>videos</b> generated by a virtual AOSLO. These methods improve the accuracy of methods to investigate hemodynamics using AOSLO imaging...|$|R
40|$|Abstractâ€”The {{emerging}} multimedia standard MPEG- 4 combines interactivity, {{natural and}} <b>synthetic</b> digital <b>video</b> and computer graphics and typical applications range from video conferencing and mobile video-phones to computer games. To realize the portable video communicator, low power VLSI implementations are mandatory. The memory bandwidth {{has become the}} stumbling block. This paper looks at {{an introduction to the}} problem and some means of reducing it. Index Termsâ€”MPEG- 4, motion estimation, algorithms, VLSI, architecture...|$|R
40|$|We present ViViD, a variability-based tool to {{synthesize}} variants of video sequences. ViViD is developed {{and used in}} the context of an industrial project involving consumers and providers of video processing algorithms. The goal is {{to synthesize}} <b>synthetic</b> <b>video</b> variants with a wide range of characteristics to then test the algorithms. We describe the key components of ViViD (1) a variability language and an environment to model what can vary within a video sequence;(2) a reasoning back-end to generate relevant testing configurations; (3) a video synthesizer in charge of producing variants of video sequences corresponding to configurations. We show how ViViD can synthesize realistic videos with differ-ent characteristics such as luminances, vehicles and persons that cover a diversity of testing scenarios...|$|E
40|$|Abstract. This paper {{studies the}} use of {{temporal}} consistency to match appearance descriptors and handle complex ambiguities when computing dynamic depth maps from stereo. Previous attempts have designed 3 D descriptors over the spacetime volume and have been mostly used for monocular action recognition, as they cannot deal with perspective changes. Our approach {{is based on a}} state-of-the-art 2 D dense appearance descriptor which we extend in time by means of optical flow priors, and can be applied to wide-baseline stereo setups. The basic idea behind our approach is to capture the changes around a feature point in time instead of trying to describe the spatiotemporal volume. We demonstrate its effectiveness on very ambiguous <b>synthetic</b> <b>video</b> sequences with ground truth data, as well as real sequences. Key words: stereo, spatiotemporal, appearance descriptors...|$|E
40|$|Digital {{video is}} one of the major traffic {{components}} in communication networks. Modelling the frame size of encoded video data is a preliminary step in the research and development of <b>synthetic</b> <b>video</b> data generators enabling a thorough analysis of video architecture systems that are often difficult to perform with real digital video data. In this paper, a statistical analysis of frame sizes of High Efficient Video Coding (HEVC) video generated at bit rates of interest for high quality Full HD video applications is performed. The selection of potential distributions for modelling the HEVC frame size distribution is based on the results from the modelling of H. 264 frame size distribution. Experimental results show that the Gamma distribution has a better fit, to the HEVC frame size distribution, than the Weibull distribution...|$|E
40|$|A novel {{technique}} {{to detect and}} localize periodic movements in video is presented. The distinctive feature of the technique is that it requires neither feature tracking nor object segmentation. Intensity patterns along linear sample paths in space-time are used in estimation of period of object motion in a given sequence of frames. Sample paths are obtained by connecting (in space-time) sample points from regions of high motion magnitude {{in the first and}} last frames. Oscillations in intensity values are induced at time instants when an object intersects the sample path. The locations of peaks in intensity are determined by parameters of both cyclic object motion and orientation of the sample path with respect to object motion. The information about peaks is used in a least squares framework to obtain an initial estimate of these parameters. The estimate is further refined using the full intensity profile. The best estimate for the period of cyclic object motion is obtained by looking for consensus among estimates from many sample paths. The proposed technique is evaluated with <b>synthetic</b> <b>videos</b> where ground-truth is known, and with American Sign Language videos where the goal is to detect periodic hand motions. 1...|$|R
40|$|Cell {{tracking}} {{is a key}} task in the high-throughput {{quantitative study}} of important biological processes, such as immune system regulation and neurogenesis. Variability in cell density and dynamics in different videos, hampers portability of existing trackers across videos. We address these potability challenges {{in order to develop}} a portable cell tracking algorithm. Our algorithm can handle noise in cell segmentation as well as divisions and deaths of cells. We also propose a parameter-free variation of our tracker. In the tracker, we employ a novel method for recovering the distribution of cell displacements. Further, we present a mathematically justified procedure for determining the gating distance in relation to tracking performance. For the range of real videos tested, our tracker correctly recovers on average 96 % of cell moves, and outperforms an advanced probabilistic tracker when the cell detection quality is high. The scalability of our tracker was tested on <b>synthetic</b> <b>videos</b> with up to 200 cells per frame. For more challenging tracking conditions, we propose a novel semi-automated framework that can increase the ratio of correctly recovered tracks by 12 %, through selective manual inspection of only 10 % of all frames in a video...|$|R
50|$|Remote {{and virtual}} tower (RVT) {{is a system}} based on air traffic {{controllers}} being located somewhere other than at the local airport tower and still able to provide air traffic control services. Displays for the air traffic controllers may be live <b>video,</b> <b>synthetic</b> images based on surveillance sensor data, or both.|$|R
