572|3|Public
50|$|An {{important}} tool {{in this area}} is the <b>sparsification</b> lemma of , which shows that, for any ε > 0, any k-CNF formula can be replaced by O(2εn) simpler k-CNF formulas in which each variable appears only a constant number of times, and therefore in which the number of clauses is linear. The <b>sparsification</b> lemma is proven by repeatedly finding large sets of clauses that have a nonempty common intersection in a given formula, and replacing the formula by two simpler formulas, one of which has each of these clauses replaced by their common intersection and the other of which has the intersection removed from each clause. By applying the <b>sparsification</b> lemma and then using new variables to split the clauses, one may then obtain a set of O(2εn) 3-CNF formulas, each with a linear number of variables, such that the original k-CNF formula is satisfiable if and only if {{at least one of these}} 3-CNF formulas is satisfiable. Therefore, if 3-SAT could be solved in subexponential time, one could use this reduction to solve k-SAT in subexponential time as well. Equivalently, if sk > 0 for any k > 3, then s3 > 0 as well, and the exponential time hypothesis would be true.|$|E
5000|$|Nikhil Srivastava {{attended}} Union College in Schenectady, New York, graduating summa {{cum laude}} {{with a bachelor}} of science degree in mathematics and computer Science in 2005. He received a PhD in computer science from Yale University in 2010 (his dissertation was called [...] "Spectral <b>Sparsification</b> and Restricted Invertibility").|$|E
5000|$|Zvi Galil (צבי גליל; born 1947) is an Israeli {{computer}} scientist and mathematician. He is {{the dean of}} the Georgia Institute of Technology College of Computing. His research interests include the design and analysis of algorithms, computational complexity and cryptography. He has been credited with coining the terms stringology and <b>sparsification.</b> [...] He has published over 170 scientific papers and is listed as an ISI highly cited researcher.|$|E
5000|$|In {{the past}} two decades, much work has gone into {{improving}} both the differential and integralequation approaches, {{as well as new}} approaches based on random walk methods.Methods of truncating the discretization required by the FD and FEM approaches hasgreatly reduced the number of elements required. Integral equation approacheshave become particularly popular for interconnect extraction due to <b>sparsification</b> techniques,also sometimes called matrix compression, acceleration, or matrix-free techniques,which have brought nearly O(n) growth in storage and solution time to integral equationmethods ...|$|E
40|$|Network <b>sparsification</b> methods play an {{important}} role in modern network analysis when fast estimation of computationally expensive properties (such as the diameter, centrality indices, and paths) is required. We propose a method of network <b>sparsification</b> that preserves a wide range of structural properties. Depending on the analysis goals, the method allows to distinguish between local and global range edges that can be filtered out during the <b>sparsification.</b> First we rank edges by their algebraic distances and then we sample them. We also introduce a multilevel framework for <b>sparsification</b> that can be used to control the <b>sparsification</b> process at various coarse-grained resolutions. Based primarily on the matrix-vector multiplications, our method is easily parallelized for different architectures...|$|E
40|$|Spectral <b>sparsification</b> is {{a general}} {{technique}} developed by Spielman et al. {{to reduce the number}} of edges in a graph while retaining its structural properties. We investigate the use of spectral <b>sparsification</b> to produce good visual representations of big graphs. We evaluate spectral <b>sparsification</b> approaches on real-world and synthetic graphs. We show that spectral sparsifiers are more effective than random edge sampling. Our results lead to guidelines for using spectral <b>sparsification</b> in big graph visualization. Comment: Appears in the Proceedings of the 25 th International Symposium on Graph Drawing and Network Visualization (GD 2017...|$|E
40|$|We {{present the}} first almost-linear time {{algorithm}} for constructing linear-sized spectral <b>sparsification</b> for graphs. This improves all previous constructions of linear-sized spectral <b>sparsification,</b> which requires Ω(n 2) time [1], [2], [3]. A key ingredient in our algorithm {{is a novel}} combination of two techniques used in literature for constructing spectral sparsification: Random sampling by effective resistance [4], and adaptive constructions based on barrier functions [1], [3]. Keywords algorithmic spectral graph theory; spectral <b>sparsification</b> I...|$|E
40|$|Channel matrix <b>sparsification</b> is {{considered}} as a promising approach to reduce the progressing complexity in large-scale cloud-radio access networks (C-RANs) based on ideal channel condition assumption. In this paper, the research of channel <b>sparsification</b> is extend to practical scenarios, in which the perfect channel state information (CSI) is not available. First, a tractable lower bound of signal-to-interferenceplus-noise ratio (SINR) fidelity, which {{is defined as a}} ratio of SINRs with and without channel <b>sparsification,</b> is derived to evaluate the impact of channel estimation error. Based on the theoretical results, a Dinkelbach-based algorithm is proposed to achieve the global optimal performance of channel matrix <b>sparsification</b> based on the criterion of distance. Finally, all these results are extended to a more challenging scenario with pilot contamination. Finally, simulation results are shown to evaluate the performance of channel matrix <b>sparsification</b> with imperfect CSIs and verify our analytical results. Comment: 12 pages, 9 figures, to be published in IEEE Transactions on Vehicular Technolog...|$|E
40|$|In this paper, we generalize Spencer's {{hyperbolic}} cosine algorithm to the matrix-valued setting. We apply the proposed algorithm to several problems by analyzing its computational efficiency under two special cases of matrices; {{one in which}} the matrices have a group structure and an other in which they have rank-one. As an application of the former case, we present a deterministic algorithm that, given the multiplication table of a finite group of size $n$, it constructs an expanding Cayley graph of logarithmic degree in near-optimal O(n^ 2 log^ 3 n) time. For the latter case, we present a fast deterministic algorithm for spectral <b>sparsification</b> of positive semi-definite matrices, which implies an improved deterministic algorithm for spectral graph <b>sparsification</b> of dense graphs. In addition, we give an elementary connection between spectral <b>sparsification</b> of positive semi-definite matrices and element-wise matrix <b>sparsification.</b> As a consequence, we obtain improved element-wise <b>sparsification</b> algorithms for diagonally dominant-like matrices. Comment: 16 pages, simplified proof and corrected acknowledging of prior work in (current) Section...|$|E
30|$|Using the sparsified signals as reference, one {{can observe}} {{that there is}} an {{improvement}} using the sparsified sources in some cases, but the difference between the scores obtained when no <b>sparsification</b> procedure is employed and with the proposed <b>sparsification</b> method is small.|$|E
30|$|In {{order to}} expose our new methodology, {{the paper is}} {{organized}} as follows. In Section 2, we present a perceptually controlled <b>sparsification</b> method, trying to increase the sparsity of the signals in the time-frequency domain but maintaining {{the same level of}} perceptual audio quality. Section 3 is dedicated to the impact of bitrate compression on sparsity and vice-versa: how sparse signals remain after coding-decoding stages? How <b>sparsification</b> modifies the quality of coded-decoded signals? Finally, we study in Section 4 how the proposed <b>sparsification</b> improves source separation.|$|E
40|$|Instance <b>sparsification</b> is {{well-known}} {{in the world}} of exact computation since it is very closely linked to the Exponential Time Hypothesis. In this paper, we extend the concept of <b>sparsification</b> in order to capture subexponential time approximation. We develop a new tool for inapproximability, called approximation preserving <b>sparsification</b> and use it in order to get strong inapproximability results in subexponential time for several fundamental optimization problems as Max Independent Set, Min Dominating Set, Min Feedback Vertex Set, and Min Set Cover. Comment: 16 page...|$|E
40|$|Abstract. Although many RNA {{molecules}} contain pseudoknots, computational {{prediction of}} pseudoknotted RNA structure {{is still in}} its infancy due to high running time and space consumption implied by the dynamic programming formulations of the problem. In this paper, we introduce <b>sparsification</b> to significantly speedup the dynamic programming approaches for pseudoknotted RNA structure prediction, which also lower the space requirements. Although <b>sparsification</b> has been applied to a number of RNA-related structure prediction problems in the past few years, we provide the first application of <b>sparsification</b> to pseudoknotted RNA structure prediction specifically and to handling gapped fragments more generally- which has a much more complex recursive structure than other problems to which <b>sparsification</b> has been applied. We show that <b>sparsification,</b> when applied to the fastest, as well as the most general pseudoknotted structure prediction methods available,- respectively the Reeder-Giegerich algorithm and the Rivas-Eddy algorithm- reduces the number of ”candidate ” substructures to be considered significantly. In fact, experimental results on the sparsified Reeder-Giegerich algorithm suggest a linear speedup over the unsparsified implementation. ...|$|E
40|$|Many machine {{learning}} frameworks, such as resource-allocating networks, kernel-based methods, Gaussian processes, and radial-basis-function networks, require a <b>sparsification</b> scheme {{in order to}} address the online learning paradigm. For this purpose, several online <b>sparsification</b> criteria have been proposed to restrict the model definition on a subset of samples. The most known criterion is the (linear) approximation criterion, which discards any sample that can be well represented by the already contributing samples, an operation with excessive computational complexity. Several computationally efficient <b>sparsification</b> criteria have been introduced in the literature, such as the distance, the coherence and the Babel criteria. In this paper, we provide a framework that connects these <b>sparsification</b> criteria to the issue of approximating samples, by deriving theoretical bounds on the approximation errors. Moreover, we investigate the error of approximating any feature, by proposing upper-bounds on the approximation error for each of the aforementioned <b>sparsification</b> criteria. Two classes of features are described in detail, the empirical mean and the principal axes in the kernel principal component analysis. Comment: 10 page...|$|E
40|$|Background: We {{study the}} <b>sparsification</b> of dynamic {{programming}} folding algorithms of RNA structures. <b>Sparsification</b> {{applies to the}} mfe-folding of RNA structures {{and can lead to}} a significant reduction of time complexity. Results: We analyze the <b>sparsification</b> of a particular decomposition rule, Λ^*, that splits an interval for RNA secondary and pseudoknot structures of fixed topological genus. Essential for quantifying the <b>sparsification</b> is the size of its so called candidate set. We present a combinatorial framework which allows by means of probabilities of irreducible substructures to obtain the expected size of the set of Λ^*-candidates. We compute these expectations for arc-based energy models via energy-filtered generating functions (GF) for RNA secondary structures as well as RNA pseudoknot structures. For RNA secondary structures we also consider a simplified loop-energy model. This combinatorial analysis is then compared to the expected number of Λ^*-candidates obtained from folding mfe-structures. In case of the mfe-folding of RNA secondary structures with a simplified loop energy model our results imply that <b>sparsification</b> provides a reduction of time complexity by a constant factor of 91...|$|E
3000|$|One could {{wonder if}} the {{obtained}} form factors are different from these computed directly after the <b>sparsification</b> in the frequency domain, before the time-domain reconstruction. In other terms, since this step was a critical issue in the <b>sparsification</b> method of [26], what {{is the effect of}} the synthesis through the overlap-add method? [...]...|$|E
40|$|Graph <b>sparsification</b> is the {{approximation}} of an arbitrary graph by a sparse graph. We explain {{what it means}} for one graph to be a spectral {{approximation of}} another and review the development of algorithms for spectral <b>sparsification.</b> In addition to being an interesting concept, spectral <b>sparsification</b> has been an important tool in the design of nearly linear-time algorithms for solving systems of linear equations in symmetric, diagonally dominant matrices. The fast solution of these linear systems has already led to breakthrough results in combinatorial optimization, including a faster algorithm for finding approximate maximum flows and minimum cuts in an undirected network...|$|E
30|$|The {{robustness}} of the <b>sparsification</b> against audio coding must be assessed.|$|E
30|$|Since our {{framework}} {{is a source}} separation based on a classic short-time Fourier transform, the method of [27] is not appropriate here, whereas the irrelevance filter of [26] does not provide satisfactory results. Instead of the binary <b>sparsification</b> proposed by the latter, a ‘smooth’ <b>sparsification,</b> robust to the inter-blocks effects of the temporal reconstruction, was proposed in [22].|$|E
40|$|Abstract — In this paper, {{an online}} {{selective}} kernel-based tem-poral difference (OSKTD) learning {{algorithm is proposed}} to deal with large scale and/or continuous reinforcement learn-ing problems. OSKTD includes two online procedures: online <b>sparsification</b> and parameter updating for the selective kernel-based value function. A new <b>sparsification</b> method (i. e., a kernel distance-based online <b>sparsification</b> method) is proposed based on selective ensemble learning, which is computationally less complex compared with other <b>sparsification</b> methods. With the proposed <b>sparsification</b> method, the sparsified dictionary of samples is constructed online by checking if a sample needs {{to be added to}} the sparsified dictionary. In addition, based on local validity, a selective kernel-based value function is proposed to se-lect the best samples from the sample dictionary for the selective kernel-based value function approximator. The parameters of the selective kernel-based value function are iteratively updated by using the temporal difference (TD) learning algorithm combined with the gradient descent technique. The complexity of the online <b>sparsification</b> procedure in the OSKTD algorithm is O(n). In addition, two typical experiments (Maze and Mountain Car) are used to compare with both traditional and up-to-date O(n) algorithms (GTD, GTD 2, and TDC using the kernel-based value function), and the results demonstrate the effectiveness of our proposed algorithm. In the Maze problem, OSKTD converges to an optimal policy and converges faster than both traditional and up-to-date algorithms. In the Mountain Car problem, OSKTD converges, requires less computation time compared with other <b>sparsification</b> methods, gets a better local optima than the traditional algorithms, and converges much faster than the up-to-date algorithms. In addition, OSKTD can reach a competitive ultimate optima compared with the up-to-date algorithms. Index Terms — Function approximation, online sparsifica-tion, reinforcement learning (RL), selective ensemble learning...|$|E
30|$|In the GSM-coding case, the <b>sparsification</b> {{increases}} slightly the impairment {{due to the}} coding.|$|E
40|$|The {{emergence}} of social networks and other interaction networks {{have brought to}} fore the questions of processing massive graphs. The (semi) streaming model, where {{we assume that the}} space is (near) linear in the number of vertices (but not necessarily the edges) is an useful and efficient model for processing large graphs. In many of these graphs the numbers of vertices are significantly less than the number of edges, and hence attract the semi-streaming model. We focus on the problem of graph <b>sparsification</b> in a single pass, that is, constructing a small space representation of the graph such that we can estimate the size of any cut. Graph <b>sparsification</b> {{is one of the major}} building blocks which is used in a variety of algorithms, and there has been a long history of (non-streaming) sampling that provide sparse approximations. Thus the space requirement for graph <b>sparsification</b> is a natural question. Since Ω(n) space is necessary for a one pass streaming algorithm to determine if a graph is connected, it gives an Ω(n) lower bound for any <b>sparsification</b> algorithms which approximates cuts multiplicatively. We show an essentially tight upper bound, that is, using Õ(n/ǫ 2) space we can create a <b>sparsification</b> in a single pass which approximates each cut to a (1 ± ǫ) factor...|$|E
30|$|Compute FFT of {{the mixing}} signals {{using the same}} {{parameters}} as in the <b>sparsification</b> process.|$|E
30|$|The MST-kNN {{construction}} {{has the advantage}} of its simplicity and robustness, {{and the fact that it}} balances the local and global structure of the data. However, the area of network inference and graph construction from data, and graph <b>sparsification</b> is very active, and several alternative approaches exist based on different heuristics, e.g., Graphical Lasso (Friedman et al. 2008), Planar Maximally Filtered Graph (Tumminello et al. 2005), spectral <b>sparsification</b> (Spielman and Srivastava 2011), or the Relaxed Minimum Spanning Tree (RMST) (Beguerisse-Diaz et al. 2013). We have experimented with some of those methods and obtained comparable results. A detailed comparison of <b>sparsification</b> methods as well as the choice of distance in defining the similarity matrix Ŝ is left for future work.|$|E
30|$|Very recently, a {{solution}} to the kernel-based online classification problem has been given by the adaptive projected subgradient method (APSM). The developed algorithm can be considered as a generalization of a kernel affine projection algorithm (APA) and the kernel normalized least mean squares (NLMS). Furthermore, <b>sparsification</b> of the resulting kernel series expansion was achieved by imposing a closed ball (convex set) constraint on the norm of the classifiers. This paper presents another <b>sparsification</b> method for the APSM approach to the online classification task by generating a sequence of linear subspaces in a reproducing kernel Hilbert space (RKHS). To cope with the inherent memory limitations of online systems and to embed tracking capabilities to the design, an upper bound on the dimension of the linear subspaces is imposed. The underlying principle of the design is the notion of projection mappings. Classification is performed by metric projection mappings, <b>sparsification</b> is achieved by orthogonal projections, while the online system's memory requirements and tracking are attained by oblique projections. The resulting <b>sparsification</b> scheme shows strong similarities with the classical sliding window adaptive schemes. The proposed design is validated by the adaptive equalization problem of a nonlinear communication channel, and is compared with classical and recent stochastic gradient descent techniques, {{as well as with the}} APSM's solution where <b>sparsification</b> is performed by a closed ball constraint on the norm of the classifiers.|$|E
40|$|A graph G ′ (V, E ′) is an ɛ-sparsification of G {{for some}} ɛ> 0, if every (weighted) cut in G ′ is within (1 ± ɛ) of the {{corresponding}} cut in G. A celebrated result of Benczúr and Karger shows {{that for every}} undirected graph G, an ɛ-sparsification with O(n log n/ɛ 2) edges can be constructed in O(m log 2 n) time. The notion of cut-preserving graph <b>sparsification</b> has {{played an important role}} in speeding up algorithms for several fundamental network design and routing problems. Applications to modern massive data sets often constrain algorithms to use computation models that restrict random access to the input. The semistreaming model, in which the algorithm is constrained to use Õ(n) space, has been shown to be a good abstraction for analyzing graph algorithms in applications to large data sets. Recently, a semi-streaming algorithm for graph <b>sparsification</b> was presented by Anh and Guha; the total running time of their implementation is Ω(mn), too large for applications where both space and time are important. In this paper, we introduce a new technique for graph <b>sparsification,</b> namely refinement sampling, that gives an Õ(m) time semi-streaming algorithm for graph <b>sparsification.</b> Specifically, we show that refinement sampling can be used to design a one-pass streaming algorithm for <b>sparsification</b> that takes O(log log n) time per edge, uses O(log 2 n) space per node, and outputs a...|$|E
40|$|We {{propose a}} decision-theoretic <b>sparsification</b> method for Gaussian process {{preference}} learning. This method overcomes the loss-insensitive nature of popular <b>sparsification</b> approaches {{such as the}} Informative Vector Machine (IVM). Instead of selecting a subset of users and items as inducing points based on uncertainty-reduction principles, our <b>sparsification</b> approach is underpinned by decision theory and directly incorporates the loss function inherent to the underlying preference learning problem. We show that by selecting different specifications of the loss function, the IVM’s differential entropy criterion, a value of information criterion, and an upper confidence bound (UCB) criterion used in the bandit setting can all be recovered from our decision-theoretic framework. We refer to our method as the Valuable Vector Machine (VVM) as it selects the most useful items during <b>sparsification</b> to minimize the corresponding loss. We evaluate our approach on one synthetic and two real-world preference datasets, including one generated via Amazon Mechanical Turk and another collected from Facebook. Experiments show that variants of the VVM significantly outperform the IVM on all datasets under similar computational constraints. M. Ehsan Abbasnejad, Edwin V. Bonilla, and Scott Sanne...|$|E
40|$|Background: Although many RNA {{molecules}} contain pseudoknots, computational {{prediction of}} pseudoknotted RNA structure {{is still in}} its infancy due to high running time and space consumption implied by the dynamic programming formulations of the problem. Results: In this paper, we introduce <b>sparsification</b> to significantly speedup the dynamic programming approaches for pseudoknotted RNA structure prediction, which also lower the space requirements. Although <b>sparsification</b> has been applied to a number of RNA-related structure prediction problems in the past few years, we provide the first application of <b>sparsification</b> to pseudoknotted RNA structure prediction specifically and to handling gapped fragments more generally- which has a much more complex recursive structure than other problems to which <b>sparsification</b> has been applied. We analyse how to sparsify four pseudoknot structure prediction algorithms, among those the most general method available (the Rivas-Eddy algorithm) and the fastest one (Reeder-Giegerich algorithm). In all algorithms the number of “candidate ” substructures to be considered is reduced. Conclusions: Our experimental results on the sparsified Reeder-Giegerich algorithm suggest a linear speedup over the unsparsified implementation. Backgroun...|$|E
40|$|This paper proposes sparse and {{redundancy}} representation spectral domain {{compression of}} the speech signal using novel sparsing algorithms {{to the problem of}} speech compression (SC) /enhancement (SE). In Automatic Speaker Recognition (ASR) <b>sparsification</b> can play a major role to resolve big data issues in speech compression and its storage in the database, where the speech signal can be uncompressed before applying to ASR system. The speech signal is converted to a spectral domain using Discrete Rajan Transform (DRT) and only first and mid spectrum component is retained forcing the remaining component to zero. The speech signal spectrum can be maximally compressed 8 : 1 ratio to the unique one. Spectrally compressed speech signal can be stored in the database and during training and testing time it can be synthesized using Inverse Discrete Rajan Transform (IDRT) in ASR. <b>Sparsification</b> and spectral compression up to 75 % with Equal Error Rate (EER) of ASR is 3 %. Percentage of Identification Accuracy (PIA) of ASR with <b>sparsification</b> and speech enhancement is 99. 1 % and without <b>sparsification</b> 98. 8 % for TIMIT database respectively...|$|E
30|$|In the MP 3 -coding case, the {{impairment}} {{due to the}} <b>sparsification</b> {{is small}} compared to this due to the coding.|$|E
40|$|Very recently, a {{solution}} to the kernel-based online classification problem has been given by the adaptive projected subgradient method (APSM). The developed algorithm can be considered as a generalization of a kernel affine projection algorithm (APA) and the kernel normalized least mean squares (NLMS). Furthermore, <b>sparsification</b> of the resulting kernel series expansion was achieved by imposing a closed ball (convex set) constraint on the norm of the classifiers. This paper presents another <b>sparsification</b> method for the APSM approach to the online classification task by generating a sequence of linear subspaces in a reproducing kernel Hilbert space (RKHS). To cope with the inherent memory limitations of online systems and to embed tracking capabilities to the design, an upper bound on the dimension of the linear subspaces is imposed. The underlying principle of the design is the notion of projection mappings. Classification is performed by metric projection mappings, <b>sparsification</b> is achieved by orthogonal projections, while the online system’s memory requirements and tracking are attained by oblique projections. The resulting <b>sparsification</b> scheme shows strong similarities with the classical sliding window adaptive schemes. The proposed design is validated by the adaptive equalization problem of a nonlinear communication channel, and is compared with classical and recent stochastic gradient descent techniques, {{as well as with the}} APSM’s solution where <b>sparsification</b> is performed by a closed ball constraint on the norm of the classifiers. Copyright © 2008 K. Slavakis and S. Theodoridis. This is an open access article distributed under the Creative Common...|$|E
40|$|Community {{detection}} is {{a fundamental}} problem {{in the domain of}} complex-network analysis. It has received great attention, and many community detection methods have been proposed in the last decade. In this paper, we propose a divisive spectral method for identifying community structures from networks, which utilizes a <b>sparsification</b> operation to pre-process the networks first, and then uses a repeated bisection spectral algorithm to partition the networks into communities. The <b>sparsification</b> operation makes the community boundaries more clearer and more sharper, so that the repeated spectral bisection algorithm extract high-quality community structures accurately from the sparsified networks. Experiments show that the combination of network <b>sparsification</b> and spectral bisection algorithm is highly successful, the proposed method is more effective in detecting community structures from networks than the others. Comment: 23 pages, 10 figures, and 2 table...|$|E
40|$|Given an n x n matrix A, {{we present}} a simple, element-wise <b>sparsification</b> {{algorithm}} that zeroes out all sufficiently small elements of A and then retains some of the remaining elements with probabilities proportional to the square of their magnitudes. We analyze the approximation accuracy of the proposed algorithm using a recent, elegant non-commutative Bernstein inequality, and compare our bounds with all existing (to {{the best of our}} knowledge) element-wise matrix <b>sparsification</b> algorithms. Comment: 8 page...|$|E
40|$|Relation {{between a}} family of {{generalized}} Support Vector Machine (SVM) problems and the novel ɛ-sparse representation is provided. In defining ɛ-sparse representations, we use a natural generalization of the classical ɛ-insensitive cost function for vectors. The insensitive parameter of the SVM problem is transformed into component-wise insensitivity and thus overall <b>sparsification</b> is replaced by componentwise <b>sparsification.</b> The connection between these two problems is built through the generalized Moore-Penrose inverse of the Gram matrix associated to the kernel...|$|E
40|$|<b>Sparsification</b> {{reduces the}} size of {{networks}} while preserving structural and statistical properties of interest. Various sparsifying algorithms have been proposed in different contexts. We contribute the first systematic conceptual and experimental comparison of edge <b>sparsification</b> methods on a diverse set of network properties. It is shown {{that they can be}} understood as methods for rating edges by importance and then filtering globally by these scores. In addition, we propose a new <b>sparsification</b> method (Local Degree) which preserves edges leading to local hub nodes. All methods are evaluated on a set of 100 Facebook social networks with respect to network properties including diameter, connected components, community structure, and multiple node centrality measures. Experiments with our implementations of the <b>sparsification</b> methods (using the open-source network analysis tool suite NetworKit) show that many network properties can be preserved down to about 20 % of the original set of edges. Furthermore, the experimental results allow us to differentiate the behavior of different methods and show which method is suitable with respect to which property. Our Local Degree method is fast enough for large-scale networks and performs well across a wider range of properties than previously proposed methods. Comment: 8 page...|$|E
40|$|Abstract. We give an {{improved}} algorithm for computing personalized PageRank vectors with tight error bounds {{which can be}} as small as O(n −k) for any fixed positive integer k. The improved PageRank algorithm is crucial for computing a quantitative ranking for edges in a given graph. We will use the edge ranking to examine two interrelated problems — graph <b>sparsification</b> and graph partitioning. We can combine the graph <b>sparsification</b> and the partitioning algorithms using PageRank vectors to derive {{an improved}} partitioning algorithm. ...|$|E
