0|2487|Public
40|$|This paper {{discusses}} {{within the}} framework of computational learning theory the current state of knowledge and some open problems in three areas of research about learning on feedforward neural nets: [...] <b>Neural</b> <b>nets</b> that learn from mistakes [...] Bounds for the Vapnik-Chervonenkis dimension of <b>neural</b> <b>nets</b> [...] Agnostic PAC-learning of functions on <b>neural</b> <b>nets.</b> All relevant definitions are given in this paper, and no previous knowledge about computational learning theory or <b>neural</b> <b>nets</b> is required. We refer to [RSO] for further introductory material and survey papers about the complexity of learning on <b>neural</b> <b>nets.</b> Throughout this paper we consider the following rather general notion of a (feedforward) <b>neural</b> <b>net...</b>|$|R
40|$|Each spatial region {{viewed by}} an imaging {{spectrometer}} contains various {{elements in a}} mixture. The elements present {{and the amount of}} each are to be determined. A <b>neural</b> <b>net</b> solution is considered. Initial optical <b>neural</b> <b>net</b> hardware is described. The first simulations on the component requirements of a <b>neural</b> <b>net</b> are considered. The pseudoinverse solution is shown to not suffice, i. e. a <b>neural</b> <b>net</b> solution is required...|$|R
40|$|Information theory, {{especially}} Shannon capacity theorem, is used {{to relate}} the <b>neural</b> <b>net</b> capacity to channel capacity. Such an idea benefits the area of <b>neural</b> <b>nets</b> from a well-established area, namely information theory. As a practical example we demonstrate the effectiveness of error correcting codes to mitigate the imperfections of <b>neural</b> <b>nets.</b> IEE...|$|R
40|$|A brief {{introduction}} to the fundamental of <b>Neural</b> <b>Nets</b> is given, followed by two applications in structural optimization. In the first case, the feasibility of simulating with <b>neural</b> <b>nets</b> the many structural analyses performed during optimization iterations was studied. In the second case, the concept of using <b>neural</b> <b>nets</b> to capture design expertise was studied...|$|R
40|$|We give {{a simple}} neural network {{consisting}} of excitatory and inhibitory neural elements and study some fundamental properties with the <b>neural</b> <b>net.</b> We {{clean up the}} competitive process in the <b>neural</b> <b>net</b> {{and the effect of}} the hysterisis of both elements on it. We also give and explain some interesting behavior of responses which the <b>neural</b> <b>net</b> shows under static input stimuli...|$|R
40|$|It {{is shown}} that high order {{feedforward}} <b>neural</b> <b>nets</b> of constant depth with piecewise polynomial activation functions and arbitrary real weights can be simulated for boolean {{inputs and outputs}} by <b>neural</b> <b>nets</b> of a somewhat larger size and depth with heaviside gates and weights from fΓ 1; 0; 1 g. This provides the first known upper bound for the computational power of the former type of <b>neural</b> <b>nets.</b> It is also shown {{that in the case}} of first order nets with piecewise linear activation functions one can replace arbitrary real weights by rational numbers with polynomially many bits, without changing the boolean function that is computed by the <b>neural</b> <b>net.</b> In order to prove these results we introduce two new methods for reducing nonlinear problems about weights in multi-layer <b>neural</b> <b>nets</b> to linear problems for a transformed set of parameters. These transformed parameters can be interpreted as weights in a somewhat larger <b>neural</b> <b>net.</b> As another application of our new proof technique we s [...] ...|$|R
40|$|Abstract. In {{this paper}} {{we show that}} {{programming}} languages can be translated on recurrent (analog, rational weighted) <b>neural</b> <b>nets.</b> Implementation of programming languages in <b>neural</b> <b>nets</b> turns {{to be not only}} theoretical exciting, but has also some practical implications in the recent efforts to merge symbolic and subsymbolic computation. To be of some use, it should be carried in a context of bounded resources. Herein, we show how to use resource bounds to speed up computations over <b>neural</b> <b>nets,</b> through suitable data type coding like in the usual programming languages. We introduce data types and show how to code and keep them inside the information flow of <b>neural</b> <b>nets.</b> Data types and control structures are part of a suitable programming language called NETDEF. Each NETDEF program has a specific <b>neural</b> <b>net</b> that computes it. These nets have a strong modular structure and a synchronization mechanism allowing sequential or parallel execution of subnets, despite the massive parallel feature of <b>neural</b> <b>nets.</b> Each instruction denotes an independent <b>neural</b> <b>net.</b> There are constructors for assignment, conditional and loop instructions. Besides the language core, many other features are possible using the same method. There is als...|$|R
50|$|Supervised neural networks, fuzzy <b>neural</b> <b>nets,</b> and {{combinations}} of <b>neural</b> <b>nets</b> and rules, have been extensively explored {{and used for}} detecting fraud in mobile phone networks and financial statement fraud.|$|R
50|$|Each {{individual}} makes {{decisions based}} on a <b>neural</b> <b>net</b> using Hebbian learning; the <b>neural</b> <b>net</b> is derived from each individual's genome. The genome does not merely specify the wiring of the <b>neural</b> <b>nets,</b> but also determines their size, speed, color, mutation rate {{and a number of}} other factors. The genome is randomly mutated at a set probability, which are also changed in descendant organisms.|$|R
40|$|Abstract—Traditionally, VLSI {{implementations}} of spiking <b>neural</b> <b>nets</b> have featured large neuron {{counts for}} fixed computations or small exploratory, configurable nets. This paper presents the system architecture {{of a large}} configurable <b>neural</b> <b>net</b> system employing a dedicated mapping algorithm for projecting the targeted biology-analog nets and dynamics onto the hardware with its attendant constraints. Keywords—large scale VLSI <b>neural</b> <b>net,</b> topology mapping, complex pulse communication. I...|$|R
40|$|Abstract: This study {{examines}} {{the value of}} utilizing <b>neural</b> <b>net</b> modeling for issues relating to optimization across a network of cities in space. <b>Neural</b> <b>nets</b> {{are made up of}} many nonlinear computational elements that operate in paral-lel and are arranged {{in a manner similar to}} biological <b>neural</b> <b>nets.</b> Defining a <b>neural</b> <b>net</b> model involves specifying a net topology, arrangement of nodes, training or learning rules, adjustment of weights associated with connections, node characteristics, and rules of transformation from input to output. All of these are the major issues in such regional problems as labor force migration and firm location. I...|$|R
40|$|A major {{challenge}} in performing pattern recognition with neural networks is large input data sets; for example, high-resolution static images. There {{is a direct}} relationship {{between the number of}} inputs and the number of neurons and links required to precess those inputs. Specifically, as the number of inputs increases linearly, the complexity of the <b>neural</b> <b>net</b> increases exponentially. We present a new approach to pattern recognition, where input data is "streamed" into a feedback <b>neural</b> <b>net.</b> This is done by distributing the input temporally, such that a portion of the inputs is used for each iteration of the <b>neural</b> <b>net.</b> Therefore, pattern recognition is automatically performed in conveniently sized segments with a single <b>neural</b> <b>net.</b> This reduces the amount of evolution that mus be performed to train the <b>neural</b> <b>net...</b>|$|R
50|$|Backpropagation Through Structure (BPTS) is a {{gradient-based}} {{technique for}} training recursive <b>neural</b> <b>nets</b> (a superset of recurrent <b>neural</b> <b>nets)</b> and is extensively {{described in a}} 1996 paper written by Christoph Goller and Andreas Küchler.|$|R
40|$|Principles of {{intelligent}} information technologies application, particularly artificial <b>neural</b> <b>nets,</b> {{for improvement of}} NPP diagnostics systems are discussed. Study results of using <b>neural</b> <b>net</b> technologies for leak diagnostics and gamma-spectra analysis are presented. ??????????? ???????? ?????????? ???????????????? ?????????????? ??????????, ? ????????? ????????????? ????????? ????? ??? ????????? ????????????? ??????????????? ?????? ???. ????????? ?????????? ???????????? ?????????? ???????????? ?????????? ??? ??????????? ????? ? ??????? ?????-????????...|$|R
40|$|Abstract. It {{is shown}} that {{high-order}} feedforward <b>neural</b> <b>nets</b> of constant depth with piecewise-polynomial activation functions and arbitrary real weights can be simulated for Boolean {{inputs and outputs}} by <b>neural</b> <b>nets</b> of a somewhat larger size and depth with Heaviside gates and weights from f− 1; 0; 1 g. This provides the rst known upper bound for the computational power of the former type of <b>neural</b> <b>nets.</b> It is also shown {{that in the case}} of rst-order nets with piecewise-linear activation functions one can replace arbitrary real weights by rational numbers with polynomially many bits without changing the Boolean function that is computed by the <b>neural</b> <b>net.</b> In order to prove these results, we introduce two new methods for reducing nonlinear problems about weights in multilayer <b>neural</b> <b>nets</b> to linear problems for a transformed set of parameters. These transformed parameters can be interpreted as weights in a somewhat larger <b>neural</b> <b>net.</b> As another application of our new proof technique we show that <b>neural</b> <b>nets</b> with piecewise-polynomial activation functions and a constant number of analog inputs are probably approximately correct (PAC) learnable (in Valiant’s model for PAC learning [Comm. Assoc. Comput. Mach., 27 (1984), pp. 1134 { 1142]). Key words. neural networks, analog computing, threshold circuits, circuit complexity, learning complexit...|$|R
40|$|Possibilities of {{incorporating}} <b>neural</b> <b>nets</b> in different tasks {{of a gas}} turbine performance diagnostic procedure are investigated. The purpose is to examine how <b>neural</b> <b>nets</b> can be implemented and what advantages they may offer. First, the possibility to constitute a performance model by using <b>neural</b> <b>nets</b> is considered. Different modes of operation are examined and the <b>neural</b> <b>net</b> architectures for achieving better accuracy are discussed. Subsequently, different problems of fault detection and identification are considered. Classification of faults is performed {{on the basis of}} diagnostic parameters produced by adaptive modelling. Both sensor faults and actual engine component faults are examined. A decision logic based on several <b>neural</b> <b>nets</b> is proposed. At a first level it is decided whether a fault exists, and if yes, checks are performed in order to identify the fault in as much detail as possible. Summarizing, the paper discusses different aspects of <b>neural</b> <b>net</b> implementation, in an effort to provide guidelines for application of this type of technique in the field of gas turbine diagnostics...|$|R
40|$|We {{consider}} the computational complexity of learning by <b>neural</b> <b>nets.</b> We {{are interested in}} {{how hard it is}} to design appropriate <b>neural</b> <b>net</b> architectures and to train <b>neural</b> <b>nets</b> for general and specialized learning tasks. Our main result shows that the training problem for 2 -cascade <b>neural</b> <b>nets</b> (which have only two non-input nodes, one of which is hidden) is NP-complete, which implies that finding an optimal net (in terms of the number of non-input units) that is consistent with a set of examples is also NP-complete. This result also demonstrates a surprising gap between the computational complexities of one-node (perceptron) and two-node <b>neural</b> <b>net</b> training problems, since the perceptron training problem can be solved in polynomial time by linear programming techniques. We conjecture that training a k-cascade <b>neural</b> <b>net,</b> which is a classical threshold network training problem, is also NP-complete, for each fixed k 2. We also show that the problem of finding an optimal perceptron (in terms of the number of non-zero weights) consistent with a set of training examples is NP-hard...|$|R
40|$|This paper {{discusses}} {{within the}} framework of computational learning theory the current state of knowledge and some open problems in three areas of research about learning on feedforward <b>neural</b> nets: <b>Neural</b> <b>nets</b> that learn from mistakes Bounds for the Vapnik-Chervonenkis dimension of <b>neural</b> <b>nets</b> Agnostic PAC-learning of functions on <b>neural</b> <b>nets.</b> All relevant denitions are given in this paper, and no previous knowledge about computational learning theory or <b>neural</b> <b>nets</b> is required. We refer to [RSO] for further introductory material and survey papers about the complexity of learning on <b>neural</b> <b>nets.</b> Throughout this paper we consider the following rather general notion of a (feed-forward) <b>neural</b> <b>net.</b> De nition 1. 1 A network architecture (or net") N is a labeled acyclic directed graph. Its nodes of fan-in 0 (input nodes"), as well as its nodes of fan-out 0 (output nodes") are labeled by natural numbers. A node g in N with fan-in r> 0 is called a computation node (or gate), and it is labeled by some activation function g: R! R, some polynomial...|$|R
40|$|ABSTRACT- Though the {{time-delay}} <b>neural</b> <b>net</b> architecture {{has been}} recently {{used in a}} number of speech recognition applications, it has the problem that it can not use longer temporal contexts because this increases the number of connection weights in the network. This is a serious bottleneck because the use of larger temporal contexts can improve the recognition performance. In this paper, a time-derivarive <b>neural</b> <b>net</b> architecture is proposed. This architecture has the advantage that it can utilize information about longer temporal contexts without increasing the number of connection weights in the network. This architecture is studied here for speaker-independent isolated-word recognition and its performance is compared with that of the time-delay <b>neural</b> <b>net</b> architecture. It is shown that the time-derivative <b>neural</b> <b>net</b> architecture, in spite of using less number of connection weights, outperforms the time-delay <b>neural</b> <b>net</b> architecture for speech recognition. 1...|$|R
40|$|Fifty {{years after}} the {{pioneering}} work of McCulloch and Pitts, the study of <b>neural</b> <b>nets</b> is alive and active. In this paper, I have discussed {{some of the work}} that is of current interest to me and my co-workers. I would, perhaps, be remiss if I failed to mention some of the current hype about <b>neural</b> <b>nets.</b> Can <b>neural</b> <b>nets</b> quickly solve NP-complete problems? No. A look at the proposed nets will show that {{the question of whether the}} net will converge, or where the net will converge to, are as difficult as the original NP-complete problem. This does not prevent the <b>neural</b> <b>net</b> from giving an approximate solution to a hard optimization problem, but no one has yet proven any approximation bounds. Hard problems are only hard in the worst case, so there may be many easy instances of a hard problem. Nothing prevents a <b>neural</b> <b>net</b> from solving these easy instances quickly. Can analog <b>neural</b> <b>nets</b> compute things not computable a Turing machine? Yes. But any analog device with infinite precision has more computational power than a Turing machine, so a <b>neural</b> <b>net</b> with unlimited precision should be a very powerful device. But practically all devices are constructed with limited precision, and these limited precision devices have no more power than a Turing machine. Can <b>neural</b> <b>nets</b> compute faster than other parallel models? No. <b>Neural</b> <b>nets</b> are in fact equivalent to the usual parallel models. The only difference that can occur is if the <b>neural</b> <b>net</b> has infinite precision which as mentioned above is highly unlikely. Does learning in <b>neural</b> <b>nets</b> make programming unnecessary? No. As we saw in the discussion of learning, learning rules must be devised, and it seems that different learning tasks will require different learning rules. Further, the kind of net to use for a particular task will be an important decision. In our decoding example, some network topologies did not lead to good decoders, while other topologies did. <b>Neural</b> <b>nets</b> will not replace programmers, but give programmers another paradigm in which to program. In spite of the hype, I believe that <b>neural</b> <b>nets</b> will be useful both as biological models and as programming paradigms. Finally, according to an often-told tale, there was a golden age of <b>neural</b> <b>nets</b> which suddenly ended in 1970. Depending on the version of the tale, the golden age ended because of the Vietnam war, or Minsky and Papert's book on perceptions, or cuts in funding, or the rise of artificial intelligence. But I hope that the reader of this paper and the rest of this volume will see that the death of Warren McCulloch had a most profound effect on the field. We miss him as a brilliant scientist, as a warm human being, and as the greatest story-teller of our age. " [...] Conclusion...|$|R
30|$|The {{results of}} three ANN {{application}} (Auto MLp, <b>Neural</b> <b>Net</b> and Perceptron) showed <b>Neural</b> <b>Net</b> {{was the best}} and the most accuracte model when it agained applied on SVM dataset, while the worst performance belonged to Perceptron model; the accuracies of Auto MLp and Perceptron models were high and nearly at the same levels (86 % and 58 %) when they applied on Information Gain and SVM datasets. Generally the Kappa indexes were less accurate, the best index obtained from three models Auto MLp, <b>Neural</b> <b>Net</b> and Perceptron were respectively 77 %, 80 % and 26 %; therefore the best index gained from <b>Neural</b> <b>Net</b> model, too.|$|R
40|$|FFNN Feed Forward <b>Neural</b> <b>Nets</b> {{are one of}} {{the most}} widely used <b>neural</b> <b>nets.</b> In this thesis the FFNN {{architecture}} is examined and compared with statistical time series models for a variety of time series prediction problems FFNN do not assume any probability models while statistical models are based on the probability model. Therefore, if the goal of the modelling is rigorous quantication of uncertainty statistical models are more suitable. However if the goal is merely prediction we demonstrate that <b>neural</b> <b>nets</b> have a lot to offer Widely different parameter settings in the <b>neural</b> <b>net</b> approach often lead to models which make virtually the same predictions. <b>Neural</b> <b>net</b> offer a more exible approach to model building which is especially helpful in nonlinear and nonGaussian situations. In this thesis the performance by NN models and statistical models for prediction is examined by using visualization techniques and statistical tests...|$|R
40|$|In {{this paper}} the {{capability}} of <b>neural</b> <b>nets</b> to control connection admission in Asynchronous Transfer Mode (ATM) networks is investigated. The general problem of connection admission control (CAC) and its formulation as a functional mapping are discussed, leading to applications of learning algorithms to CAC problems. In particular, {{the use of the}} class of feed-forward <b>neural</b> <b>nets</b> with back-propagation learning rule is considered, where various architecture alternatives are presented. As example a simple <b>neural</b> <b>net</b> structure and its use to control connection acceptance is discussed in detail. The <b>neural</b> <b>net</b> performance is compared with other connection admission control mechanisms like the peak bit rate, the equivalent bandwidth and the weighted variance methods. Numerical results for both cases, stationary load and non-stationary pulse-form overload patterns, illustrate {{the capability of}} <b>neural</b> <b>nets</b> to act as connection admission controller in ATM environments. 1 Introduction In b [...] ...|$|R
50|$|In 1950 Frank Rosenblatt {{looked into}} a new {{approach}} in <b>neural</b> <b>net</b> technology. He used a circuit consisting of an array of input units. These units are connected {{through a set of}} intermediate neurons. In Rosenblatt's trials, he used binary words in both input and output, and he called his invention Perception. Perception was the most basic and revolutionary type of <b>neural</b> <b>net.</b> In 1969, the limitations of Perception were explained by Minsky and Papert. <b>Neural</b> <b>nets</b> have been expanded by a variety of neuroscientists and psychologists. Churchland discusses <b>neural</b> <b>nets</b> in the context of facial coding. The example consisted of a bank teller who could not remember the face of a bank robber to provide a description. However, Churchland says the teller would likely recognize and discriminate the robber's face when she sees him again. This example contains not only the idea of <b>neural</b> <b>nets,</b> but also of vector coding.|$|R
40|$|Wolfgang Maass* Institute for Theoretical Computer Science Technische Universitaet Graz Klosterwiesgasse 32 / 2 A- 8010 Graz, Austria e-mail: maass@igi. tu-graz. ac. at October 23, 1992 Abstract It {{is shown}} that {{feedforward}} <b>neural</b> <b>nets</b> of constant depth with piecewise polynomial activation functions and arbitrary real weights can be simulated for boolean {{inputs and outputs}} by <b>neural</b> <b>nets</b> of a somewhat larger size and depth with heaviside gates and weights from f 0; 1 g. This provides the first known upper bound for the computational power and VC-dimension of such <b>neural</b> <b>nets.</b> It is also shown {{that in the case}} of piecewise linear activation functions one can replace arbitrary real weights by rational numbers with polynomially many bits, without changing the boolean function that is computed by the <b>neural</b> <b>net.</b> In addition we improve the best known lower bound for the VC-dimension of a <b>neural</b> <b>net</b> with w weights and gates that use the heaviside function (or other common activation functions suc [...] ...|$|R
40|$|Over {{the last}} five years Deep <b>Neural</b> <b>Nets</b> have offered more {{accurate}} solutions to many problems in speech recognition, and computer vision, and these solutions have surpassed a threshold of acceptability for many applications. As a result, Deep Neural Networks have supplanted other approaches to solving problems in these areas, and enabled many new applications. While the design of Deep <b>Neural</b> <b>Nets</b> is still something of an art form, in our work we have found basic principles of design space exploration used to develop embedded microprocessor architectures to be highly applicable to the design of Deep <b>Neural</b> <b>Net</b> architectures. In particular, we have used these design principles to create a novel Deep <b>Neural</b> <b>Net</b> called SqueezeNet that requires as little as 480 KB of storage for its model parameters. We have further integrated all these experiences to develop something of a playbook for creating small Deep <b>Neural</b> <b>Nets</b> for embedded systems. Comment: Keynote at Embedded Systems Week (ESWEEK) 201...|$|R
40|$|In this paper, we {{introduced}} a new framework of speech recognizer based on HMM and <b>neural</b> <b>net.</b> Unlike the traditional hybrid system, the <b>neural</b> <b>net</b> {{was used as a}} post processor, which classify the speech data segmented by HMM recognizer. The purpose of this method is to improve the top-choice accuracy of HMM based speech recognition system in our lab. Major issues such as how to use the segmentation information of HMM in <b>neural</b> <b>net,</b> the structure of the <b>neural</b> <b>net,</b> the choice of the error metric for training <b>neural</b> <b>net,</b> and the determination of the training procedure are investigated within a set of experiments. In these experiments, we attempt to recognize 68 phoneme like units in continuous speech. Our results indicate that this is a potential method. About 20 % can be obtained to improve the recognition accuracy for multi-speaker system in syllable level, and 10 % for speaker independent system. I...|$|R
40|$|In {{this paper}} {{we show that}} {{programming}} languages can be translated into recurrent (analog, rational weighted) <b>neural</b> <b>nets.</b> Implementation of programming languages in <b>neural</b> <b>nets</b> turns {{to be not only}} theoretical exciting, but has also some practical implications in the recent efforts to merge symbolic and subsymbolic computation. To be of some use, it should be carried in a context of bounded resources. Herein, we show how to use resource bounds to speed up computations over <b>neural</b> <b>nets,</b> through suitable data type coding like in the usual programming languages. We introduce data types and show how to code and keep them inside the information flow of <b>neural</b> <b>nets.</b> Data types and control structures are part of a suitable programming language called NETDEF. Each NETDEF program has a specific <b>neural</b> <b>net</b> that computes it. These nets have a strong modular structure and a synchronization mechanism allowing sequential or parallel execution of subnets, despite the massive parallel f [...] ...|$|R
40|$|We {{consider}} recurrent analog <b>neural</b> <b>nets</b> {{where the}} output of each gate is subject to Gaussian noise, or any other common noise distribution that is nonzero on a large set. We show that many regular languages cannot be recognized by networks of this type, and we give a precise characterization of those languages which can be recognized. This result implies severe constraints on possibilities for constructing recurrent analog <b>neural</b> <b>nets</b> that are robust against realistic types of analog noise. On the other hand we present a method for constructing feedforward analog <b>neural</b> <b>nets</b> that are robust with regard to analog noise of this type. 1 Introduction A fairly large literature (see [Omlin, Giles, 1996] and the references therein) {{is devoted to the}} construction of analog <b>neural</b> <b>nets</b> that recognize regular languages. Any physical realization of the analog computational units of an analog <b>neural</b> <b>net</b> in technological or biological systems is bound to encounter some form of "imprecision" or an [...] ...|$|R
40|$|Symbol {{manipulation}} as used {{in traditional}} Artificial Intelligence has been criticized by <b>neural</b> <b>net</b> researchers for being excessively inflexible and sequential. On the other hand, the application of <b>neural</b> <b>net</b> techniques to the types of high-level cognitive processing studied in traditional artificiaa intelligence presents major problems as well. We claim that a promising {{way out of this}} impasse is to build <b>neural</b> <b>net</b> models that accomplish massively paraliel case-based reasoning. Case-based reasoning, which has received much attention recently, is essentially the same as analogy-based reasoning, and avoids many of the problems leveled at traditional artificial intelligence. Further problems are avoided by doing many strands of case-based reasoning in parallel, and by implementing the whole system as a <b>neural</b> <b>net.</b> In addition, such a system provides an approach to some aspects of the problems of noise, uncertainty and novelty in reasoning systeras. We are accordingly modifying our current <b>neural</b> <b>net</b> system (Conposit), which performs standard rule-based reasoning, into a massively parallel case-based reasoning version...|$|R
40|$|This paper {{presents}} {{details of}} a cellular automata based neural network model which is simple enough to be implementable in evolvable hardware, allowing the growth of <b>neural</b> <b>nets</b> in nanoseconds, {{and the evolution of}} <b>neural</b> <b>nets</b> in seconds. This model and the hardware tool based upon it, should enable the birth of a new research field, called "brain building" because it will become practical to assemble tens of thousands of such evolved <b>neural</b> <b>net</b> modules into humanly architected artificial brains...|$|R
40|$|A {{feedforward}} <b>neural</b> <b>net</b> with d input neurons {{and with}} a single hidden layer of n neurons is given by [GRAPHICS] where a(j), theta(j), w(ji) {{is an element of}} R. In this paper we study the approximation of arbitrary functions f: R-d [...] > R by a <b>neural</b> <b>net</b> in an L-p(mu) norm for some finite measure mu on R-d. We prove that under natural moment conditions, a <b>neural</b> <b>net</b> with non-polynomial function can approximate any given function. (C) 1998 Elsevier Science Ltd. All rights reserved...|$|R
40|$|AbstractWe {{consider}} <b>neural</b> <b>nets</b> whose {{connections are}} defined by growth rules taking the form of recursion relations. These are called genetic <b>neural</b> <b>nets.</b> Learning in these nets is achieved by simulated annealing optimization of the net over the space of recursion relation parameters. The method is tested on a previously defined continuous coding problem. Results of control experiments are presented so {{that the success of}} the method can be judged. Genetic <b>neural</b> <b>nets</b> implement the ideas of scaling and parsimony, features which allow generalization in machine learning...|$|R
40|$|This paper {{introduces}} TiPo, a new <b>neural</b> <b>net</b> {{model with}} superior evolvabilities. TiPo <b>neural</b> <b>nets</b> can dynamically change their structure with each clock tick. This provides enhanced computability for highly dynamic functions, such as curve following. Curve following is valuable for {{applications such as}} robot motion control. ...|$|R
50|$|Similarly, a <b>neural</b> <b>net</b> {{that somehow}} had Chaitin's {{constant}} exactly embedded in its weight function {{would be able}} to solve the halting problem, though constructing such an infinitely precise <b>neural</b> <b>net,</b> even if you somehow know Chaitin's constant beforehand, is impossible under the laws of quantum mechanics.|$|R
40|$|We {{exhibit a}} novel way of {{simulating}} sigmoidal <b>neural</b> <b>nets</b> by networks of noisy spiking neurons in temporal coding. Furthermore it is shown that networks of noisy spiking neurons with temporal coding have a strictly larger computational power than sigmoidal <b>neural</b> <b>nets</b> {{with the same}} number of units...|$|R
