1|2517|Public
40|$|Thread-level {{speculation}} {{is a technique}} that enables parallel execution of sequential applications on a multiprocessor. This paper describes the complete implementation of the support for threadlevel speculation on the Hydra chip multiprocessor (CMP). The support consists {{of a number of}} software speculation control handlers and modifications to the shared <b>secondary</b> <b>cache</b> <b>memory</b> system of the CMP. This support is evaluated using five representative integer applications. Our results show that the speculative support is only able to improve performance when there is a substantial amount of medium–grained loop-level parallelism in the application. When the granularity of parallelism is too small or there is little inherent parallelism in the application, the overhead of the software handlers overwhelms any potential performance benefits from speculative-thread parallelism. Overall, thread-level speculation still appears to be a promising approach for expanding the class of applications that can be automatically parallelized, but more hardware intensive implementations for managing speculation control are required to achieve performance improvements on a wide class of integer applications...|$|E
50|$|The Alpha 21364 was an Alpha 21264 with a 1.75 MB on-die <b>secondary</b> <b>cache,</b> two {{integrated}} <b>memory</b> controllers and {{an integrated}} network controller.|$|R
40|$|We {{present an}} {{investigation}} of the architecture of an optoelectronic cache which can integrate terabit optical memories with the electronic caches associated with high performance uni- and multi- processors. The use of optoelectronic <b>cache</b> <b>memories</b> will enable these terabit technologies to transparently provide low latency secondary memory with frame sizes comparable to disk-pages but with latencies approaching those of electronic <b>secondary</b> <b>cache</b> <b>memories.</b> This will enable the implementation of terabit memories with effective access times comparable to the cycle times of current microprocessors. The cache design is based on the use of a smart-pixel array and combines parallel free space optical I/O to-and-from optical memory with conventional electronic communication to the processor caches. This cache, and the optical memory system to which it will interface, provides for a large random access memory space which has lower overall latency than that of magnetic disks and disk arrays. I [...] ...|$|R
40|$|This paper {{presents}} a simulation-based performance {{evaluation of a}} shared-memory multiprocessor using the Scalable Coherent Interface (IEEE 1596). The machines are assembled with one to 16 processors connected in a ring. The multiprocessor's memory hierarchy consists of split primary <b>caches,</b> coherent <b>secondary</b> <b>caches</b> and <b>memory.</b> For a workload of two parallel loops and three thread-based programs, <b>secondary</b> <b>cache</b> latency has the strongest impact on performance. For programs with high miss ratios, 16 -node rings exhibit high network congestion whereas 4 - and 8 -node rings perform better. With these same programs, doubling the processor speed yields between 20 and 70 % speed gains with higher gains on the smaller rings. 1 Introduction The Scalable Coherent Interface (SCI) is an IEEE standard for high performance interconnects supporting a physically distributed logically shared memory [18]. SCI consists of physical interfaces, a logical communication protocol, and a distributed ca [...] ...|$|R
40|$|The DECchip 21071 and the DECchip 21072 chip {{sets were}} {{designed}} to provide simple, competitive devices for building cost-focused or high-performance PCI-based systems using the DECchip 21064 family of Alpha AXP microprocessors. The chip sets include data slices, {{a bridge between the}} DECchip 21064 microprocessor and the PCI local bus, and a <b>secondary</b> <b>cache</b> and <b>memory</b> controller. The EB 64 + evaluation kit, a companion product, contains an example PC mother board that was built using the DECchip 21064 microprocessor, the DECchip 21072 chip set, and other off-the-shelf PC components. The EB 64 + kit provides hooks for system designers to evaluate cost/performance trade-offs. Either chip set, used with the EB 64 + evaluation kit, enables system designers to develop Alpha AXP PCs with minimal design and engineering effort...|$|R
50|$|Initially, DeskStation {{designed}} and produced MIPS-based workstations, {{such as the}} DeskStation Tyne v4633x and DeskStation rPC44 (Evolution E4400 RISC PC and Evolution R4400 RISC PC), which conformed to the ARC computer specification (and implemented the associated firmware). Rather than adopt the Jazz reference design for its MIPS systems, DeskStation developed its own internal bus and chipset systems which offered greater performance compared to the more common Jazz machines. The resulting DeskStation machines began production in 1994, and had an initial price range from $2,990 for a basic system to $6,000 or more for machines with <b>secondary</b> <b>cache</b> and large <b>memory</b> configurations.|$|R
50|$|Helix Software Company pioneered {{virtual memory}} {{compression}} in 1992, filing a patent application {{for the process}} in October of that year. In 1994 and 1995, Helix refined the process using test-compression and <b>secondary</b> <b>memory</b> <b>caches</b> on video cards and other devices. However, Helix did not release a product incorporating virtual memory compression until July 1996 {{and the release of}} Hurricane 2.0, which used the Stac Electronics Lempel-Ziv-Stac compression algorithm and also used off-screen video RAM as a compression buffer to gain performance benefits.|$|R
40|$|Current {{generation}} RISC microprocessors {{operate at}} clock frequencies ranging up to 1 GHz {{with the ability}} to complete two or more floating point operations (flops) per clock cycle. To sustain a significant percentage of peak performance, large <b>secondary</b> L 2 <b>cache</b> <b>memories</b> based on fast SRAM technology are essential. Single processor optimisations are presented for the MC 2 model code on the MIPS R 10000 and SUN UltraSparc II microprocessors. Ensemble forecast techniques for high resolution mesoscale simulations are applied {{to assess the impact of}} aggressive floating point optimisations on forecast accuracy. Parallel benchmarks of the MC 2 model (adiabatic kernel + physics) on the SGI/Cray Origin 2000 and Fujitsu AP 3000 are also presented. The relative efficiency of line relaxation preconditioners for minimal residual Krylov iterative solvers is reported in the context of real-time mesoscale forecasting. 1 Computational Sciences Section, Scientific Computing Division, National Center for A [...] ...|$|R
40|$|This paper {{presents}} a novel instruction cache prefetching mechanism for multiple-issue processors. Such processors at high clock rates {{often have to}} use a small instruction cache which can have significant miss rates. Prefetching from <b>secondary</b> <b>cache</b> or even <b>memory</b> can hide the instruction cache miss penalties, but only if initiated sufficiently far ahead of the current program counter. Existing instruction cache prefetching methods are strictly sequential and do not prefetch past conditional branches which may occur almost every clock cycle in wide-issue processors. In this study, multi-level branch prediction is used to overcome this limitation. By keeping branch history and target addresses, two methods are defined to predict a future PC several branches past the current branch. A prefetching architecture using such a mechanism is defined and evaluated with respect to its accuracy, the impact of the instruction prefetching on performance, and its interaction with sequential prefetch [...] ...|$|R
40|$|A growing {{gap between}} CPU and DRAM {{performance}} is driving processors {{further away from}} their peak execution rates by {{increasing the amount of}} time spent waiting for the memory system. To date, <b>cache</b> <b>memories</b> have been used to good effect in offsetting lagging DRAM speeds by buffering frequently referenced instructions and data near the CPU. However, continued increases in the cost of DRAM accesses call for improvements in cache performance, and in particular, that of the <b>secondary</b> <b>cache.</b> More specifically, strategies which target the <b>secondary</b> <b>cache</b> hit rate for improvement are becoming increasingly important, even if they result in an increase in the cost of each miss. This research examines a proposed new organization for the memory hierarchy in which main memory is implemented in SRAM (replacing the <b>secondary</b> <b>cache),</b> and the role of DRAM is relegated to that of a paging device. In this model, called the RAMpage memory hierarchy, a software-managed paging system takes the place of th [...] ...|$|R
5000|$|Processor cache is an {{intermediate}} stage between ultra-fast registers and much slower main memory. It was introduced solely {{to improve the}} performance of computers. Most actively used information in the main memory is just duplicated in the <b>cache</b> <b>memory,</b> which is faster, but of much lesser capacity. On the other hand, main memory is much slower, but has a much greater storage capacity than processor registers. Multi-level hierarchical cache setup is also commonly used—primary cache being smallest, fastest and located inside the processor; <b>secondary</b> <b>cache</b> being somewhat larger and slower.|$|R
40|$|In this paper, we {{show how}} a {{multiprocessor}} can be physically detached from its main memory using commercially available <b>secondary</b> <b>cache</b> controllers and highspeed fiber optic links organized {{as a star}} network. The implementation connects up to eight Motorola 88110 processors to a single <b>memory</b> controller/directory <b>cache.</b> The <b>memory</b> access latency in this arrangement is lower. Communication time in the optical fiber and protocol overhead do not substantially affect the memory access time. 1. Introduction In recent years shared-memory multiprocessors have become very popular. Their efficient and easy-to-use programming model represents the most promising approach for application migration from uniprocessor to multiprocessor systems. Most existing systems use buses to interconnect processors and memory. Almost all of these systems now depend {{on the use of}} <b>cache</b> <b>memories.</b> Most concentrate on increasing the bandwidth and reducing the latency at the processor interface. However, the presenc [...] ...|$|R
40|$|We {{describe}} <b>cache</b> <b>memory</b> design {{suitable for}} use in FPGA-based cache controller and processor. Cache systems are on-chip memory element used to store data. Cache serves as a buffer between a CPU and its main <b>memory.</b> <b>Cache</b> <b>memory</b> is used to synchronize the data transfer rate between CPU and main <b>memory.</b> As <b>cache</b> <b>memory</b> closer to the micro processor, it is faster than the RAM and main memory. The advantage of storing data on cache, as compared to RAM, {{is that it has}} faster retrieval times, but it has disadvantage of on-chip energy consumption. In term of detecting miss rate in <b>cache</b> <b>memory</b> and less power consumption, The efficient <b>cache</b> <b>memory</b> will proposed by this research work, by implementation of <b>cache</b> <b>memory</b> on FPGA. We believe that our implementation achieves low complexity and low energy consumption in terms of FPGA resource usage. present in <b>cache</b> <b>memory</b> then the term is called „cache hit‟. The advantage of storing data on cache, as compared to RAM, is that it has faster retrieval times, but it has disadvantage of on-chip energy consumption. This paper deals with the design of efficient <b>cache</b> <b>memory</b> for detecting miss rate in <b>cache</b> <b>memory</b> and less power consumption. This <b>cache</b> <b>memory</b> may used in future work to design FPGA based cache controller...|$|R
5000|$|<b>Cache</b> <b>Memory</b> - Number of cache modules 1-32, Module {{capacity}} 8 or 16GB, Maximum <b>cache</b> <b>memory</b> 512GB ...|$|R
40|$|The {{purpose of}} this study is to explore the {{relationship}} between hit ratio of <b>cache</b> <b>memory</b> and design parameters. <b>Cache</b> <b>memories</b> are widely used in the design of computer system architectures to match relatively slow memories against fast CPUs. Caches hold the active segments of a program which are currently in use. Since instructions and data in <b>cache</b> <b>memories</b> can be referenced much faster than the time required to access main <b>memory,</b> <b>cache</b> <b>memories</b> permit the execution rate of the machine to be substantially increased. In order to function effectively, <b>cache</b> <b>memories</b> must be carefully designed and implemented. In this study, a trace-driven simulation study of direct mapped, associative mapped and set-associative mapped <b>cache</b> <b>memories</b> is made. In the simulation, cache fetch algorithm, placement policy, cache size and various parameters related to cache design and the resulting effect on system performance is investigated. The <b>cache</b> <b>memories</b> are simulated using the C language and the simulation results are analyzed for the design and implementation of <b>cache</b> <b>memories.</b> Department of Physics and AstronomyThesis (M. S. ...|$|R
40|$|In multitask, {{preemptive}} real-time systems, {{the use of}} <b>cache</b> <b>memories</b> make difficult {{the estimation}} of the response time of tasks, due to the dynamic, adaptive and nonpredictable behaviour of <b>cache</b> <b>memories.</b> But many embedded and critical applications need the increase of performance provided by <b>cache</b> <b>memories...</b>|$|R
40|$|<b>Cache</b> <b>memories</b> {{are used}} in modern, medium and {{high-speed}} CPUs to hold temporarily those portions {{of the contents of}} main memory which are {believed to be) currently in use. Since instructions and data in <b>cache</b> <b>memories</b> can usually be referenced in 10 to 25 percent of the time required to access main <b>memory,</b> <b>cache</b> <b>memories</b> permit th...|$|R
40|$|Degraded K-user {{broadcast}} channels (BC) are studied when receivers are facilitated with <b>cache</b> <b>memories.</b> Lower {{and upper}} bounds are derived on the capacity-memory tradeoff, i. e., on the largest rate of reliable communication over the BC {{as a function}} of the receivers' cache sizes, and the bounds are shown to match for some special cases. The lower bounds are achieved by two new coding schemes that benefit from non-uniform cache assignment. Lower and upper bounds are also established on the global capacity-memory tradeoff, i. e., on the largest capacity-memory tradeoff that can be attained by optimizing the receivers' cache sizes subject to a total <b>cache</b> <b>memory</b> budget. The bounds coincide when the total <b>cache</b> <b>memory</b> budget is sufficiently small or sufficiently large, characterized in terms of the BC statistics. For small <b>cache</b> <b>memories,</b> it is optimal to assign all the <b>cache</b> <b>memory</b> to the weakest receiver. In this regime, the global capacity-memory tradeoff grows as the total <b>cache</b> <b>memory</b> budget divided by the number of files in the system. In other words, a perfect global caching gain is achievable in this regime and the performance corresponds to a system where all cache contents in the network are available to all receivers. For large <b>cache</b> <b>memories,</b> it is optimal to assign a positive <b>cache</b> <b>memory</b> to every receiver such that the weaker receivers are assigned larger <b>cache</b> <b>memories</b> compared to the stronger receivers. In this regime, the growth rate of the global capacity-memory tradeoff is further divided by the number of users, which corresponds to a local caching gain. Numerical indicate suggest that a uniform cache-assignment of the total <b>cache</b> <b>memory</b> is suboptimal in all regimes unless the BC is completely symmetric. For erasure BCs, this claim is proved analytically in the regime of small cache-sizes. Comment: Submitted to IEEE Transactions on Information Theor...|$|R
40|$|The first {{methods to}} bound {{execution}} time in computer systems with <b>cache</b> <b>memories</b> {{were presented in}} the late eighties [...] - twenty {{years after the first}} <b>cache</b> <b>memories</b> designed. Today, fifteen years later, methods has been developed to bound execution time with <b>cache</b> <b>memories</b> [...] . that were state-of-the-art twenty years ago. This report presents <b>cache</b> <b>memories</b> and real-time from the very basics to the state-of-the-art of <b>cache</b> <b>memory</b> design, methods to use <b>cache</b> <b>memories</b> in real-time systems and the limitations of current technology. Methods to handle intrinsic and extrinsic behavior on instruction and data caches will be presented and discussed, but also close issues like pipelining, DMA and other unpredictable hardware components will be briefly presented. No method is today able to automatically calculate a safe and tight Worst-Case Execution Time WCETc for any arbitrary program that runs on a modern high-performance system [...] - there are always cases where the method will cross into problems. Many of the methods can although give very tight WCETc or reduce the related problems under specified circumstances...|$|R
40|$|Abstract- <b>Cache</b> <b>memory</b> {{performance}} analysis is a challenging topic upon first introduction. Students must synthesize {{a significant amount}} of computer architecture knowledge, comprehend reasonably complex replacement strategies, and analyze performance. We propose a programming exercise that has students develop a visual <b>cache</b> <b>memory</b> simulator and then use the simulator to analyze several memory reference trace files. Our student learning assessment measured the quality of each team programming exercise solution and each individual’s own cache {{performance analysis}}. In addition, the final exam has several questions related to <b>cache</b> <b>memory.</b> Early results indicate students achieve a better understanding of <b>cache</b> <b>memory</b> and its impact on performance...|$|R
50|$|The MC88110 {{supported}} {{an optional}} external 256 KB to 2 MB <b>secondary</b> <b>cache.</b> The <b>secondary</b> <b>cache</b> controller was not integrated on the MC88110, but was {{located on a}} separate device, the MC88410, to reduce cost.|$|R
40|$|A {{hardware}} prefetching {{mechanism for}} <b>cache</b> <b>memories</b> named Speculative Prefetching is proposed. This scheme detects regular accesses issued by a load/store instruction and prefetches the corresponding data. The scheme requires no software add-on, {{and in some}} cases it is more powerful than software techniques for identifying regular accesses. The tradeoffs related to its hardware implementation are extensively discussed in order to finely tune the mechanism. Experiments show that average memory access time of regular codes is brought within 10 % of optimum for processors with usual issue rates, while performance of irregular codes is little reduced though never degraded. The scheme performance is discussed over a wide range of parameters. Keywords: cache, hardware prefetch, numerical codes, memory latency. 1 Introduction The memory latency observed by current processors is high whether they are superpipelined (fast processor clock), have no <b>secondary</b> <b>caches</b> or belong to a multiprocesso [...] ...|$|R
30|$|No <b>cache</b> <b>memory</b> is used.|$|R
40|$|High {{performance}} {{is the major}} concern {{in the area of}} VLSI Design. <b>Cache</b> <b>memory</b> consumes the half of total power in various systems. Thus, the architecture behavior of the cache governs both high performance and low power consumption. Simulator simulates <b>cache</b> <b>memory</b> design in various formats with help of various simulators like simple scalar, Xilinx etc. This paper explores the issue and consideration involved in designing efficient <b>cache</b> <b>memory</b> and we have discussed the <b>cache</b> <b>memory</b> simulation behavior on various simulators. Memory design concept is becoming dominant; memory level parallism is one of the critical issues concerning its performance. We have to propose high performance cache simulation behavior for performance improvement for future mobile processors design and customize mobile devices...|$|R
50|$|The Ultra 60 came {{equipped}} with 1 or 2 CPUs. The CPUs run at 300,360,450 MHz and have 16-KB data and 16-KB instruction cache on chip with a 2MB or 4MB external <b>secondary</b> <b>cache</b> (<b>secondary</b> <b>cache</b> size depends on CPU model).|$|R
40|$|Abstract—High {{performance}} {{is the major}} concern in VLSI Design. Thus, the architecture behavior of the cache governs both high performance and low power consumption. High performance simulator simulates <b>cache</b> <b>memory</b> design in various formats with help of various simulators like simplescalar, Xilinx, Top spice 8 etc. This paper explores the issue and consideration involved in designing the efficient <b>cache</b> <b>memory.</b> We have discussed the <b>cache</b> <b>memory</b> simulation behavior on various simulators. We propose high performance cache simulation behavior issues for future mobile processors design and customize mobile devices...|$|R
40|$|Embedded {{microprocessor}} <b>cache</b> <b>memories</b> {{suffer from}} limited observability and controllability creating problems during in-system test. The application of test algorithms for SRAM <b>memories</b> to <b>cache</b> <b>memories</b> thus requires opportune transformations. In this paper {{we present a}} procedure to adapt traditional march tests to testing the data and the directory array of k-way set-associative <b>cache</b> <b>memories</b> with LRU replacement. The basic idea is to translate each march test operation into an equivalent sequence of cache operations able to reproduce the desired marching sequence into the data and the directory array of the cach...|$|R
40|$|Abstract—In this paper, an {{efficient}} technique is proposed {{to manage the}} <b>cache</b> <b>memory.</b> The proposed technique introduces some modifications on the well-known set associative mapping technique. This modification requires a little alteration {{in the structure of}} the <b>cache</b> <b>memory</b> and on the way by which it can be referenced. The proposed alteration leads to increase the set size virtually and consequently to improve the performance and the utilization of the <b>cache</b> <b>memory.</b> The current mapping techniques have accomplished good results. In fact, there are still different cases in which <b>cache</b> <b>memory</b> lines are left empty and not used, whereas two or more processes overwrite the lines of each other, instead of using those empty lines. The proposed algorithm aims at finding {{an efficient}} way to deal with such problem...|$|R
40|$|For high {{performance}} processor design, <b>cache</b> <b>memory</b> size {{is an important}} parameter which directly affects performance and the chip area. Modeling performance and area is required for design tradeoff of <b>cache</b> <b>memory.</b> This paper describes a tool which calculates <b>cache</b> <b>memory</b> performance and area. A designer can try variety of cache parameters to complete the specification of a <b>cache</b> <b>memory.</b> Data examples calculated using this tool are shown. Key Words and Phrases: <b>Cache</b> <b>memory,</b> Design tradeoff, Memory modeling, CAD ii Copyright 1994 by Osamu Okuzawa and Michael J. Flynn iii Contents 1. Introduction 1 2. Related Work 1 3. Performance/Area Workbench 2 3. 1 Tool target 2 3. 2 Input 2 3. 3 Output 3 3. 4 Model & Calculation method 3 3. 5 Tool Implementation 6 3. 6 Using Manual 9 4. Results 10 4. 1 Tool Performance 10 4. 2 Calculation result examples and analysis 11 5. Conclusions 16 iv List of Figures 1 CPI and Area Calculation Program Configuration 7 2 Calculation Process 8 3 32 KB spl [...] ...|$|R
40|$|Recently, energy {{dissipation}} by microprocessors is getting larger, {{which leads to}} a serious problem in terms of allowable temperature and performance improvement for future microprocessors. <b>Cache</b> <b>memory</b> is effective in bridging a growing speed gap between a processor and relatively slow external main memory, and has increased in its size. Almost all of today's commercial processors, not only highperformance microprocessors but embedded ones, have onchip <b>cache</b> <b>memories.</b> However, {{energy dissipation}} in the <b>cache</b> <b>memory</b> will approach or exceed 50 % of the increasing total energy dissipation by processors. An important point to note is that, in the near future, static (leakage) energy will dominate the total energy consumption in deep sub-micron processes. This paper describes <b>cache</b> <b>memory</b> architecture, especially for on-chip multiprocessors, that achieves efficient reduction of leakage energy in <b>cache</b> <b>memories</b> by exploiting gated-Vdd control, software selfinvalidation for L 1 cache, and dynamic data compression for L 2 cache. The simulation results show that our techniques can reduce a substantial amount of leakage energy without large performance degradation...|$|R
40|$|A {{family of}} {{high-performance}} 64 -bit RISC workstations and servers {{based on the}} new Digital Alpha AXP architecture is described. The hardware implementation uses the powerful new DECchip 21064 CPU and employs a sophisticated new system interconnect structure to achieve the necessary high bandwidth and low-latency <b>cache,</b> <b>memory,</b> and I/O buses. The memory subsystem of the high-end DEC 3000 AXP Model 500 provides a 512 KB <b>secondary</b> <b>cache</b> and up to 1 GB of memory. The I/O subsystem of the Model 500 has integral two-dimensional graphics, SCSI, ISDN, and six TURBOchannel expansion slots. The DEC 3000 AXP system family consists of both workstations and servers {{that are based on}} Digital's Alpha AXP architecture. [1] The family includes the desktop (DEC 3000 AXP Model 400) and desk-side and rack-mounted (DEC 3000 AXP Model 500) systems. The available operating systems are the DEC OSF/ 1 AXP and the OpenVMS AXP systems. All systems use the DECchip 2106...|$|R
50|$|In {{addition}} to the macro-instruction <b>cache</b> <b>memory</b> {{also found in the}} ND-100, the ND-110 had a unique implementation of <b>cache</b> <b>memory</b> on the micro-instruction level. The step known as mapping in the ND-100 was then avoided because the first micro-instruction word of a macro-instruction was written into the control store cache.|$|R
40|$|<b>Cache</b> <b>memories</b> {{bridge the}} growing access-time {{gap between the}} {{processor}} and the main <b>memory.</b> <b>Cache</b> <b>memories</b> use randomisation functions for two purposes: (i) to {{limit the amount of}} search when looking up an address in the cache and (ii) to interleave the access stream over multiple independent banks, allowing multiple simultaneous accesses...|$|R
40|$|The Web-based {{information}} systems, as {{the network}} traffic and slow remote servers {{can lead to}} long delays in the answer delivery. Client memory is largely used to cache data and minimize future interaction with the servers. In this paper, we propose an extended <b>cache</b> <b>memory</b> to store the frequently used data. We observe that the frequently used data contain the images in a Web site. In this paper we propose that all the data which is frequently used must be stored at user end in an extended <b>cache</b> <b>memory.</b> The extended <b>cache</b> <b>memory</b> {{may be in the}} form of pen drive, CD, DVD or it could be saved at user’s machine. The only difference to access the Web between traditional and in our way is that the user have to provide the path of the data which resides at the extended <b>cache</b> <b>memory...</b>|$|R
40|$|This {{document}} {{is a short}} review about the MESI protocol simulator. This simulator is a tool {{which is used to}} teach the <b>cache</b> <b>memory</b> coherence on the computer systems with hierarchical memory system. The MESI protocol is a well known method to the maintenance of the information coherence in the memory system. The MESI simulator is also used to explain the process of the <b>cache</b> <b>memory</b> location (in multilevel <b>cache</b> <b>memory</b> systems). In this paper, we explain the MESI protocol and we show how the simulator works. Besides, we present some classroom practices carried out by using the MESI simulator...|$|R
30|$|Data (D$): Read/Write Hits and Misses, <b>cache</b> <b>memory</b> {{hits and}} misses.|$|R
