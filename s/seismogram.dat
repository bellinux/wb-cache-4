583|2138|Public
25|$|A simple {{seismometer}} that {{is sensitive}} to up-down motions of the earth can be understood by visualizing a weight hanging on a spring. The spring and weight are suspended from a frame that moves along with the earthʼs surface. As the earth moves, the relative motion between the weight and the earth provides {{a measure of the}} vertical ground motion. If a recording system is installed, such as a rotating drum attached to the frame, and a pen attached to the mass, this relative motion between the weight and earth can be recorded to produce a history of ground motion, called a <b>seismogram.</b>|$|E
25|$|Despite the {{compelling}} <b>seismogram</b> from the 1940 event in El Centro, strong-motion seismology was not explicitly sought until later events occurred—the San Fernando earthquake made evident {{the need for}} more data for earthquake engineering applications. The California Strong Motion Instrumentation Program was initiated in 1971 with the goal of maximizing the volume of data by furnishing and maintaining instruments at selected lifeline structures, buildings, and ground response stations. By the late 1980s, the program had instrumented more than 450 structures, bridges, dams, and power plants. The 1979 Imperial Valley and 1987 Whittier Narrows earthquakes were presented as gainful events that were recorded during that period, because both produced valuable data that increased knowledge of how moderate events affect buildings. The success of the Imperial Valley event was especially pronounced because of a recently constructed and fully instrumented government building that was shaken to the point of failure.|$|E
5000|$|... #Caption: A <b>seismogram</b> being {{recorded}} by a seismograph at Weston Observatory in Massachusetts ...|$|E
40|$|History paper <b>seismograms</b> {{are very}} {{important}} information for earthquake monitoring and prediction. The vectorization of paper <b>seismograms</b> is an import problem to be resolved. Auto tracing of waveform curves is a key technology for the vectorization of paper <b>seismograms.</b> It can transform an original scanning image into digital waveform data. Accurately tracing out all the key points of each curve in <b>seismograms</b> is the foundation for vectorization of paper <b>seismograms.</b> In the paper, we present a new curve tracing algorithm based on local feature, applying to auto extraction of earthquake waveform in paper <b>seismograms.</b> ...|$|R
40|$|The long-period strain <b>seismograms</b> {{from the}} Sumatra-Andaman {{earthquake}} of December 26, 2004 and the Nias earthquake of March 28, 2005 are analyzed {{by comparing the}} observed <b>seismograms</b> with synthetic <b>seismograms</b> calculated by normal mode theory. The multiple CMT source (Tsai et al., 2005) and Harvard CMT solution explain the <b>seismograms</b> recorded by both extensometers and volumetric strainmeters from the 2004 and 2005 earthquakes, respectively. The long-period strain <b>seismograms</b> observed in Japan are consistent with about 9 min source duration and 1200 km fault length proposed by Tsai et al. (2005) and Velasco et al. (2006) ...|$|R
40|$|A new {{processing}} method, {{which we}} {{developed for the}} guided waves generated during acoustic logging, accurately estimates the wavenumber when only a few <b>seismograms</b> are available or when the <b>seismograms</b> are irregularly spaced. The estimates of the attenuation coefficient are seemingly accurate when many <b>seismograms</b> are available but are inaccurate when only a few <b>seismograms</b> are available. The new method does not generate any spurious estimates as the Prony-based method does. Halliburton CompanyMassachusetts Institute of Technology. Full Waveform Acoustic Logging Consortiu...|$|R
50|$|Coppens(1985) {{calculated}} {{the ratio of}} energy of <b>seismogram</b> of the two windows and used that to differentiate in signal and noise.|$|E
5000|$|The {{concept of}} {{deconvolution}} had an early application in reflection seismology. In 1950, Enders Robinson {{was a graduate}} student at MIT. He worked with others at MIT, such as Norbert Wiener, Norman Levinson, and economist Paul Samuelson, to develop the [...] "convolutional model" [...] of a reflection <b>seismogram.</b> This model assumes that the recorded <b>seismogram</b> s(t) is the convolution of an Earth-reflectivity function e(t) and a seismic wavelet w(t) from a point source, where t represents recording time. Thus, our convolution equation is ...|$|E
50|$|An {{earthquake}} radiates {{energy in}} the form of different kinds of seismic waves, whose characteristics reflect the nature of both the rupture and the earth's crust the waves travel through. Determination of an earthquake's magnitude generally involves identifying specific kinds of these waves on a <b>seismogram,</b> and then measuring one or more characteristics of a wave, such as its timing, orientation, amplitude, frequency, or duration. Additional adjustments are made for distance, kind of crust, and the characteristics of the seismograph that recorded the <b>seismogram.</b>|$|E
5000|$|Historically, <b>seismograms</b> were {{recorded}} on paper attached to rotating drums. Some used pens on ordinary paper, while others used light beams to expose photosensitive paper. Today, practically all <b>seismograms</b> are recorded digitally to make analysis by computer easier. [...] Some drum seismometers are still found, especially when used for public display. <b>Seismograms</b> {{are essential for}} finding the location and magnitude of earthquakes.|$|R
40|$|History of {{instrumental}} seismology is short. <b>Seismograms</b> are available {{only for a}} little more than 100 years; high-quality <b>seismograms</b> are available only for the last 50 years and the seismological database is very limited in time. To extend the database, <b>seismograms</b> of old events are of vital importance. Many unusual earthquakes are known to have occurred, but their seismological characteristics are poorly known. The 1907 Sumatra earthquake is one of them (1907 January 4, M= 7. 6). Gutenberg and Richter located this event in the outer-rise area of the Sunda arc. This earthquake is known to be anomalous because of its extensive tsunami, which is disproportionate of its magnitude. The tsunami affected the coastal areas over 950 km along the Sumatran coast. We investigated this earthquake using the historical <b>seismograms</b> we could collect from several seismological observatories. We examined the P-wave arrival times listed in the Strassburg Bulletin (1912) and other station bulletins. The scatter of the Observed−Computed traveltime residuals ranges from – 30 to 30 s, too large to locate the event accurately. The uncertainty of the epicentre estimated from an S-P grid-search relocation study is at least 1 ° (~ 110 km). We interpreted the Omori <b>seismograms</b> from Osaka, Mizusawa and Tokyo, and the Wiechert <b>seismograms</b> from Göttingen and Uppsala by comparing them with the <b>seismograms</b> simulated from modern broad-band <b>seismograms</b> of the 2002, 2008 and two 2010 Sumatra earthquakes which occurred near the 1907 earthquake. From the amplitude of Rayleigh waves recorded on the Omori <b>seismograms</b> we conclude that the magnitude of the 1907 earthquake at about 30 to 40 s is about 7. 8 (i. e. 7. 5 to 8. 0). The SH waveforms recorded on the Göttingen and Uppsala <b>seismograms</b> suggest that the 1907 earthquake is a thrust earthquake at a shallow depth around 30 km. The most likely scenario is that the 1907 earthquake initiated on the subduction interface, and slowly ruptured up-dip into the shallow sediments and caused the extensive tsunami. Although their quantity and quality are limited, historical <b>seismograms</b> provide key quantitative information about old events that cannot be obtained otherwise. This underscores the importance of preserving historical <b>seismograms...</b>|$|R
40|$|Currently, numerically {{simulated}} synthetic <b>seismograms</b> {{are widely}} used by seismologists for seismological inferences. The generation of these synthetic <b>seismograms</b> requires large amount of computing resources, {{and the maintenance of}} these observed <b>seismograms</b> requires massive storage. Traditional high-performance computing platforms is inefficient to handle these applications because rapid computations are needed and large-scale datasets should be maintained. The emerging cloud computing platform provides an efficient substitute. In this paper, we introduce our experience on implementing a computational platform for rapidly computing and delivering synthetic <b>seismograms</b> on Windows Azure. Our experiment shows that cloud computing is an ideal platform for such kind of applications. ...|$|R
50|$|There are {{two types}} of body waves, Pressure waves or Primary waves (P-waves) and Shear or Secondary waves (S-waves). P-waves are {{longitudinal}} waves that involve compression and expansion in the direction that the wave is moving and are always the first waves to appear on a <b>seismogram</b> as they are the fastest moving waves through solids. S-waves are transverse waves that move perpendicular to the direction of propagation. S-waves are slower than P-waves. Therefore, they appear later than P-waves on a <b>seismogram.</b> Fluids cannot support perpendicular motion, so S-waves only travel in solids.|$|E
5000|$|This {{equation}} can {{be written}} in the frequency domain with [...] being the Fourier transform of the <b>seismogram</b> w(t), using {{the definition of the}} wavenumber vector k = ω⋅ s ...|$|E
5000|$|Many of the {{programs}} run simply by a command on the terminal, for instance, to visualize a <b>seismogram,</b> as wiggle traces$ suxwigb < seismogram.suor as an image plot$ suximage < seismogram.su ...|$|E
40|$|Thirty-three <b>seismograms</b> {{from nine}} large quarry blasts {{ranging in size}} from 50, 000 to 2, 138, 000 Ib of {{explosives}} were analyzed for possible reflections from inhomogeneities in the earth's upper mantle. Of the 33 <b>seismograms,</b> four were obtained at temporary seismograph stations positioned between 90 and 243 km from the explosions and an array of three to four seismome-ters was used at each of the stations. The remaining twenty-nine <b>seismograms</b> were obtained from ten permanent seismograph stations located between 76 and 1, 009 km from the explosions. Seven of these latter <b>seismograms</b> were obtained from the seismograph station at Salt Lak...|$|R
40|$|Summary. The {{reflectivity}} {{method for}} complete SH <b>seismograms</b> {{has been extended}} to two-dimensionally layered structures. The Aki-Larner technique is generalized to solve the integral equations for 2 -D boundary conditions, and propagator matrices are enlarged to express a total SH wavefield. Syn-thetic <b>seismograms</b> in a soft basin are calculated for an incident plane-wave. They compare favourably {{with the results of}} the finiteeiement and finite-difference methods even in the later portion where asymptotic ray and beam theories break down. Synthetic <b>seismograms</b> due to a line force and a point dislocation are also presented. Key words: wave theory, synthetic <b>seismograms,</b> irregular layering...|$|R
40|$|We {{evaluate}} local {{effects on}} strain <b>seismograms</b> for a Rayleigh wave observed at Matsushiro Seismological Observatory, Japan Meteorological Agency, central Japan, by applying a method proposed {{in a previous}} report (Okamoto et al. 2007). The method involves examination of polarization angles, local phase velocity, and accuracy of velocity <b>seismograms.</b> The results are as follows: 1) Polarization angles of observed strain <b>seismograms</b> agree with expected ones from those of velocity <b>seismograms</b> also observed at Matsushiro; 2) Local phase velocity estimated by comparison between strain and velocity <b>seismograms</b> is 54 % larger than the theoretical value calculated from the PREM velocity model; 3) Velocity spectra observed at Matsushiro have almost the same amplitude as an average of those at F-net observation stations near Matsushiro. These results indicate that both EW and NS component strain <b>seismograms</b> observed at Matsushiro have been reduced by 35 % in amplitude for a Rayleigh wave due to local heterogeneity. The local effects on a Rayleigh wave are quite different from that on a Love wave obtained in the previous report...|$|R
5000|$|... where xi is {{the times}} series {{representing}} a <b>seismogram</b> {{with the time}} index i=1, 2 … N. {{and the number of}} points in an energy window is ne. Then, the modified energy ratio is defined as ...|$|E
50|$|Magnitude scales {{generally}} {{are based on}} instrumental measurement of {{some aspect of the}} seismic wave as recorded on a <b>seismogram.</b> Where such records do not exist, magnitudes can be estimated from reports of the macroseismic events such as described by intensity scales.|$|E
5000|$|... in the {{frequency}} domain. By assuming that the reflectivity is white, {{we can assume that}} the power spectrum of the reflectivity is constant, and that the power spectrum of the <b>seismogram</b> is the spectrum of the wavelet multiplied by that constant. Thus, ...|$|E
40|$|The great Sumatra-Andaman {{earthquake}} of December 26, 2004 is {{studied by}} using extensometer (strain-meter) as very-broard-band seismograph. We analyze the strain <b>seismograms</b> obtained by 100 m extensometers installed at Matsushiro Seismological Observatory. Japan Meteorological Agency. We compare the observed strain <b>seismograms</b> with thouse calculated by normal mode theory and obtain fairly good {{agreement between the}} observed and calculated <b>seismograms</b> for a five-sourcse model with a moment magnitude of 9. 3 and a source duration of about 600 sec. We find a peculiar strain change right {{after the passage of}} Rayleigh wave in the NS component of the strain <b>seismograms.</b> This strain change cannot be explained by the summation of normal modes and double-couple type sources...|$|R
40|$|Among {{the most}} {{important}} data in geophysics are the <b>seismograms</b> which have produced most of the basic and quantitative information concerning the seismic source and the Earth's interior. Modern seismographic instruments provide high quality <b>seismograms</b> with a wide dynamic range and frequency band. The analysis of these <b>seismograms,</b> together with the recent developments in theory and methodology, has resulted in {{an order of magnitude}} increase, both in quantity and quality, of our knowledge of the Earth's interior and physics of earthquakes. However, the earthquake cycle is a long-term process so that study {{over a long period of}} time is essential for a thorough understanding of the earthquake phenomenon. Furthermore, since earthquakes may not repeat in exactly the same way, detailed analyses of earthquakes in the past are important. For more than a century, a large number of <b>seismograms</b> have been recorded at many stations in the world, but many of them have not been fully utilized mainly because theories and methods had not been developed well enough to fully interpret the <b>seismograms</b> at the time when the earthquakes occurred. In many cases, methods developed in later years enabled seismologists to investigate existing records of earlier events, searching for further information. In this paper, we illustrate the importance of historical <b>seismograms</b> for various geophysical studies...|$|R
40|$|Abstract In {{order to}} improve {{computer}} simulations of earthquakes and com-putations of ground motions, the primary goals {{of this study are}} to: (1) examine the effect of a range of kinematic rupture models and rupture parameters on synthesized <b>seismograms,</b> (2) examine the theoretical relationship between rup-ture models and synthesized <b>seismograms,</b> and (3) demonstrate hat simple ki-nematic earthquake models, used with empirical Green's functions, can be used to predict very realistic <b>seismograms.</b> Earthquake models and synthesized <b>seismograms</b> are examined by computing ground motion for the 1971 San Fernando earthquake (ML = 6. 4) and for two aftershocks (ML = 3. 3 and 3. 5). Well-constrained source parameters and Green's functions allow a controlled examination of rupture models and rupture param-eters. <b>Seismograms</b> are synthesized for the full wave train on three components and for frequencies of 0. 5 to 25. 0 Hz and are compared to observed seismo-grams as a guide to indicate whether the models are realistic. The examination of rupture models and rupture parameters show that: (1) variations in modele...|$|R
5000|$|A <b>seismogram</b> is a {{graph output}} by a seismograph. It {{is a record}} of the ground motion at a {{measuring}} station as a function of time. Seismograms typically record motions in three cartesian axes (x, y, and z), with the z axis perpendicular to the Earth's surface and the x- and y- axes parallel to the surface. The energy measured in a <b>seismogram</b> may result from an earthquake or from some other source, such as an explosion. Seismograms can record lots of things, and record many little waves, called microseisms. These tiny microseisms can be caused by heavy traffic near the seismograph, waves hitting a beach, the wind, and any number of other ordinary things that cause some shaking of the seismograph.|$|E
50|$|This method works fine, if the hypocentral depth Z>50 km because, in that case, {{the direct}} and {{reflected}} phases (waves) are clearly separated on the record. For shallower depths, the delay is so small that the two pulses on the <b>seismogram</b> are not readily recognizable as separate pulses; it takes filtering techniques to separate and identify them.|$|E
50|$|Wong et al.(2009) {{introduced}} STA/LTA ratio method. This {{method is}} similar as Coppens’ algorithm. The difference {{is to do}} the ratio of two averages of energy between a short-term window and a long-term window, which is denoted as STA/LTA (short-term average/long-term average), instead of calculating the ratio of energy of <b>seismogram</b> of the two windows in Coppens’ algorithm.|$|E
40|$|The fourth {{round of}} {{repetitive}} seismic experiment in Sakurajima Volcano on 2012 and {{the analysis of}} compiled data set are presented. The repetitive surveys {{have been carried out}} since 2009 after the pilot survey on 2008. Two lines, NS and EW were deployed in the eastern foot and the northern flank of the volcano, respectively. The survey lines include 14 shot points and 273 temporary stations, those are the same specification as those of the previous observations. More than 90 % of stations are placed at the same points with the previous observations. Data retrieval was successful and 98. 5 % of stations were retrieved. Reference <b>seismograms</b> are derived through stacking the single-year <b>seismograms</b> and the differential <b>seismograms</b> are also calculated with the subtraction of those single-year <b>seismograms.</b> Differential sections are derived from the differential <b>seismograms.</b> Some systematic changes in reflectivity are found in the sections and deeper changes below 3 km in the depth are consistent with development of the edifice inflation. Changes in the shallow part can be correlated with explosions at the crater and can alternate in short cycle...|$|R
5000|$|... #Caption: Four major {{types of}} <b>seismograms,</b> or seismic signatures.|$|R
40|$|Synthetic <b>seismograms</b> {{are used}} to obtain an {{accurate}} control of the stratigraphy on seismic sections. Detailed study of synthetic <b>seismograms</b> accompanied whith the analysis of other well records, have allowed us to know the real origin of seismic responses and to quantify the relevance or not of a given reflecto...|$|R
5000|$|This {{method is}} similar as Coppens’ (1985) algorithm. The {{difference}} {{is to do}} the ratio of two averages of energy between a short-term window and a long-term window, which is denoted as STA/LTA (short-term average/long-term average), instead of calculating the ratio of energy of <b>seismogram</b> of the two windows in Coppens’ algorithm. The numerical derivative of the ratio can be defined as, ...|$|E
50|$|The first {{refinement}} {{that allowed}} {{a more precise}} determination of the location, {{was the use of}} a time scale. Instead of merely noting, or recording, the absolute motions of a pendulum, the displacements were plotted on a moving graph, driven by a clock mechanism. This was the first <b>seismogram,</b> which allowed precise timing of the first ground motion, and an accurate plot of subsequent motions.|$|E
5000|$|Second, Richter {{arbitrarily}} {{defined the}} zero point of the scale to be where an earthquake {{at a distance of}} 100 km makes a maximum horizontal displacement of 0.001 millimeters (1 µm, or 0.00004 in.) on a <b>seismogram</b> recorded with a Wood-Anderson torsion seismograph. Subsequent magnitude scales are calibrated to be approximately in accord with the original [...] "Richter" [...] (local) scale around magnitude 6.|$|E
40|$|The {{historical}} <b>seismograms</b> {{were produced}} before 1970 s {{where there was}} no digital seismometer exists. They are all paper based record of the earthquake and have crucial information for the future earthquake research. In this report, the process of digitization of historical <b>seismograms</b> and the crucial techniques of algorithms are presented...|$|R
30|$|Kawakatsu (1995) {{proposed}} {{a method of}} automatic CMT inversion that discarded waveforms with maximum-to-minimum ratios of root-mean-square amplitudes that were greater than 300 in long-period (45 – 100  s) <b>seismograms</b> at individual stations. We tested this method on the data providing the degraded CMT solutions from the Philippines network. We found that only three <b>seismograms</b> including the non-seismic pulses and long-period noise exceeded the threshold ratio of 300, whereas 45 <b>seismograms</b> including them were discriminated by our method using the source amplitude ratios with R =  11. As the threshold ratio was lowered, more non-seismic pulses and noise were discriminated, but seismic signals were also included. This approach using amplitude ratios is useful to discard <b>seismograms</b> containing transient spikes. However, our study demonstrated that to discriminate non-seismic pulses and long-period noise, source amplitude ratios provide a more effective means of discrimination than amplitude ratios.|$|R
40|$|We {{propose a}} method for fast {{estimation}} of local magnitudes from <b>seismograms</b> (analog or digital) different from the standard Wood-Anderson ones. The method provides a justification of the widely used methods for approximate evaluation of magnitudes from analog <b>seismograms.</b> The method is illustrated through an application to local microseismic data from Baja California, Mexico...|$|R
