0|29|Public
40|$|Abstract: To direct at {{heating system}} when it adopts {{quantity}} adjustment. Taking {{a community in}} Zhangjiakou as an example, using the method of Graphic theory to analysis and compare these two operation schemes: <b>adjusting</b> <b>parallel</b> pump's valve in shared pipeline and <b>adjusting</b> <b>parallel</b> pump's valve in non-shared pipeline. We can know that <b>adjusting</b> <b>parallel</b> pump's valve in non-shared pipeline is energy-saving, when the flow in pipe network is bigger than 79 % of the boiler's total circulating water. There is a lowest limit of system's circulating water, when adopt the method that <b>adjusting</b> the <b>parallel</b> pump's valve in non-shared pipeline, otherwise it would damage the pump. 1...|$|R
50|$|When {{the lens}} and image planes are not <b>parallel,</b> <b>adjusting</b> focusrotates the PoF rather than {{displacing}} {{it along the}} lens axis. The axis of rotation is {{the intersection of the}} lens’s front focal plane and a plane {{through the center of the}} lens parallel to the image plane, as shown in Figure 3. As the image plane is moved from IP1 to IP2, the PoF rotates about the axis G from position PoF1 to position PoF2; the “Scheimpflug line” moves from position S1 to position S2. The axis of rotation has been given many different names: “counter axis” (Scheimpflug 1904), “hinge line” (Merklinger 1996), and “pivot point” (Wheeler).|$|R
40|$|Abstract. Owing to the {{deficiency}} of pinhole imaging model, {{a method to}} measure the depth information of the object based on lens imaging principle is proposed in this paper. By drawing on the two states of the measured object before and after changing position, this method manages to adjust the position of lens until the image become clearest, record the related information of position variation {{in order to obtain}} the depth information of the object. Meanwhile, whether the lens central plane is parallel to the CCD plane or not is also discussed, which is the reason why the image is fuzzy. A new kind of two degrees of freedom <b>parallel</b> <b>adjusting</b> mechanism was designed to solve the two problems above, which can achieve two degrees of freedom including independent moving and independent rotating. The clearest image can be obtained by adjusting the object distance and the image distance without changing the focal length of the camera. Besides, the mechanism {{can also be used to}} detect whether the installation position of lens central plane matches the CCD plane or not...|$|R
50|$|The Fej Farok is a {{parasite}} and a deity, which can cross the membrane {{that separates the}} <b>parallel</b> universes, <b>adjusting</b> their balance if needed. It’s a creature made up of two parts - Fej and Farok - {{and each of them}} is hosted by two bodies, two porters, elected among the descendants of the Abyssals, very intelligent and ancient creatures that form the spiritual and religious side of Lumina. When the Fej Farok picks two individuals, the chosen ones, it literally owns their bodies and gives them unimaginable power. They become Fej (Yin), and Farok (Yang).|$|R
5000|$|The {{temperature}} measurement is critical, and consequently commercial distributors of thermodynamic pump testing equipment often quote an accuracy of greater than 0.001°C. Such accuracy is necessary {{as the temperature}} rise across a pump may be less than 0.05°C. Typically, temperature probes are inserted directly into the flow, and pressure measurements are taken from taps on both the suction and discharge sections of the pipe. Then, the head on the pump is varied by some sort of adjustment such as throttling a discharge valve, utilizing different pump combinations in <b>parallel,</b> or <b>adjusting</b> well levels. [...] This allows the pump’s performance to be tested across its range of operation as its head, and hence flow, is varied.|$|R
40|$|In {{this paper}} {{we present a}} tool that {{performs}} CUDA accelerated LTL Model Checking. The tool exploits <b>parallel</b> algorithm MAP <b>adjusted</b> to the NVIDIA CUDA architecture in order to efficiently detect the presence of accepting cycles in a directed graph. Accepting cycle detection is the core algorithmic procedure in automata-based LTL Model Checking. We demonstrate that the tool outperforms non-accelerated version of the algorithm and we discuss where {{the limits of the}} tool are and what we intend to do in the future to avoid them. ...|$|R
40|$|This paper {{presents}} a control strategy that combines the predictive controller and neuro-fuzzy controller type of ANFIS. An Adaptive Network based Fuzzy Interference System architecture extended {{to cope with}} multivariable systems has been used. The neurofuzzy controller and predictive controller are works <b>parallel.</b> This controller <b>adjusts</b> {{the output of the}} predictive controller, in order to enhance the predicted inputs. The performance of the control strategy is studied on the control of Distillation Column problem. The results confirmed the control quality improvement with MPC and multi-loop PID controller...|$|R
40|$|The {{article focuses}} on the {{relationship}} between street vendors and local authorities in Bangkok. We examine the goals, the means, and the effects of everyday regulation of street vending. We document how the district administration produces and maintains informality by creating a parallel set of rules where street vendors enjoy negligible rents and little competition. We provide detailed empirical evidence on earnings, rents, fines, and rules regarding commercial real estate. The district administration's policy of “managed informality” results in a situation where more established informal vendors control less established ones. We hypothesize in the conclusion that the district administration's <b>parallel</b> legal system <b>adjusts</b> to the population's expectations in a political system where the law has little popular support...|$|R
50|$|Catherine was betrothed to Charles, Count of Charolais, the Burgundian heir, in {{accordance}} with the Treaty of Arras between France and Burgundy from 1435. In 1438, Charles visited the French court with an embassy and formally proposed to Catherine. On 11 June 1439, the couple was officially betrothed at St. Omer, and on May 19, 1440, the wedding was conducted at Blois. The bride was aged 12, the groom 7. They had no children. Reportedly, Catherine and Charles had a friendly relationship, but due to her youth, Catherine did not live with Charles but was handed over to the care of her spouse's mother Isabella of Portugal, Duchess of Burgundy, with whom she apparently went along very well, being treated like a substitute daughter. Catherine was described as intelligent, kind and charming and was well liked in Burgundy, but the frequent travelling she was expected to do in <b>parallel</b> to <b>adjusting</b> to the formal court etiquette, which was required at the Burgundian court, described {{as one of the most}} elaborate in Europe and constantly moving about between the cities of the Low Countries in a cold climate, was reportedly exhausting for Catherine's delicate health. She fell ill in violent coffins in 1446 and died in what was likely tuberculosis.|$|R
5000|$|The method {{can be used}} artistically to [...] "fatten" [...] or [...] "beef up" [...] a mix, by careful {{setting of}} attack and release times on the compressor. These {{settings}} may be adjusted further until the compressor causes the signal to [...] "pump" [...] or [...] "breathe" [...] in tempo with the song, adding its own character to the sound. Unusually extreme implementations have been achieved by studio mix engineers such as New York-based Michael Brauer who uses five <b>parallel</b> compressors, <b>adjusted</b> individually for timbral and tonal variations, mixed and blended to taste, to achieve his target sound on vocals for The Rolling Stones, Aerosmith, Bob Dylan, KT Tunstall and Coldplay. Mix engineer Anthony [...] "Rollmottle" [...] Puglisi uses parallel compression applied conservatively across the entire mix, especially in dance-oriented electronic music: [...] "it gives a track that extra oomph and power (not just make it louder—there's a difference) through quieter portions of the jam without resorting {{to one of those}} horrific 'maximizer' plugins that squeeze the dynamics right out of your song." [...] While parallel compression is widely utilized in electronic dance music, [...] "side-chain" [...] compression is the technique popularly used to give a synth lead or other melodic element the pulsating quality ubiquitous in the genre. One or more tracks may be side-chained to the kick, thereby compressing them only when the beat occurs.|$|R
30|$|Immediately after {{preparation}} of alumina substrate, the Co nanowires were electrochemically deposited at the pores of AAO template {{in an aqueous}} solution of 0.18  M CoSO 4 7 H 2 O, and 0.5  M H 3 BO 3 under 18  V alternating current for 60  s. To have the better results in electrodeposition of Co nanowires {{in the bottom of}} pores, the pH value of the electrolyte was adjusted between 2.0 and 4.0. The electrochemical deposition of Co nanowires was done in a two-electrode system, where resulting AAO template with exposed area of 1  cm ×  1  cm, and untreated aluminum sheet with an exposed area of 2  cm ×  2  cm were used as two electrodes. These two electrodes were <b>adjusted</b> <b>parallel</b> to each other in an electrochemical deposition cell with an electrode gap of 1  cm.|$|R
40|$|International audienceThai laws aim at curbing street vending, which {{business}} {{elites and}} middle classes perceive as deviant to modernist {{conceptions of the}} city. And yet, street vendors thrive throughout the city. Focusing {{on the relationship between}} the district administration and the vendors, we examine the goals, the means and the effects of everyday regulation of street vending. We document how the district administration produces and maintains informality by creating a parallel set of rules where street vendors enjoy negligible rents and little competition. We provide detailed empirical evidence on earnings, rent, fines, and rules regarding commercial real estate. The district administration's policy of ―managed informality‖ results in a situation where more established informal vendors control less established ones. We hypothesize in the conclusion that the district administration's <b>parallel</b> legal system <b>adjusts</b> to the population's expectations in a political system where the law has little popular support...|$|R
40|$|In this work, {{the impact}} of heat {{treatment}} on the real structure of Bi 2 Te 3 /Sb 2 Te 3 multilayers is investigated. The material was heated in situ in the transmission electron microscope (TEM) and ex situ inside a furnace after preparing these layers with the so-called nanoalloying deposition technique via molecular beam epitaxy (MBE) equipment. The samples were prepared as a lamella for TEM studies using focused ion beam technique. EDX elemental mapping and high angle annular dark field mode-STEM were performed to monitor changes of the morphology and interdiffusion phenomena after heating up to 250 °C. A grain growth started during heating and the chemical layer structure was smeared out partly but remained in several grains and {{was found to be}} <b>adjusted</b> <b>parallel</b> to a major lattice plan e in a crystallite. High resolution TEM shows polysynthetic twinning in a number of crystals...|$|R
40|$|A {{simple method}} {{of finding the}} coma-free axis in {{electron}} microscopes with high brightness guns and quantifying the relevant aberration coefficients has been developed which requires only a single image of a monocrystal. Coma-free alignment {{can be achieved by}} means of an appropriate tilt of the electron beam incident on the crystal. Using a small condenser aperture, the zone axis of the crystral is <b>adjusted</b> <b>parallel</b> to the beam and the beam is focused on the observation plane. In the slightly defocused image of the crystal, several spots representing the diffracted beams and the undiffracted one can be detected. Quantitative values for the beam tilt required for coma-free alignment, {{as well as for the}} aberration coefficients of interest, can directly be obtained from measured coordinates of spots that correspond to diffraction angles of equal sizes. Key Words: Coma-free alignment, wave aberration, defocus, spherical aberration, coma, astigmatism, aberration curve, beam tilt, diffraction spots, spot position. *Address for correspondence...|$|R
40|$|The {{physiological}} {{changes that}} occur during the mycelial- to yeast-phase transitions induced by a temperature shift from 25 to 37 degrees C of cultures of Blastomyces dermatitidis and Paracoccidioides brasiliensis {{can be divided into}} three stages. The triggering event is a heat-related insult induced by the temperature shift which results in partial uncoupling of oxidative phosphorylation and declines in cellular ATP levels, respiration rates, and concentrations of electron transport components (stage 1). The cells then enter a stage in which spontaneous respiration ceases (stage 2), and finally, there is a shift into a recovery phase during which transformation to yeast morphology occurs (stage 3). Cysteine is required during stage 2 for the operation of shunt pathways which permit electron transport to bypass blocked portions of the cytochrome system. The mycelial- to yeast-phase transitions of these two fungi are very similar to that of Histoplasma capsulatum. Therefore, these three dimorphic fungal pathogens have evolved <b>parallel</b> mechanisms to <b>adjust</b> to the temperature shifts which induce these mycelial- to yeast-phase transitions...|$|R
40|$|Large scale {{solidarity}} {{must meet}} the problem of establishing both unity and expansion, which is <b>parallel</b> to the <b>adjusting</b> problem between bonding and bridging social capital. In this paper, we explore the functioning of social norms, especially generalized reciprocity, as a social device to promote large scale solidarity beyond the above-mentioned problem. In our original net-base theoretical framework, the key factor that cultivates generalized reciprocity is net-base diversity, because it should stimulate network imagination (i. e. belief that help will be returned someday, by someone, in terms of various net-base linkage). Considering this net-base condition, we examine the original effects of generalized reciprocity on solidarity by utilizing internet survey data we gathered in 2012. Main findings are as follows. Controlling for net-base diversity, generalized reciprocity indicates significant effects, especially on heterogeneous solidarity and on higher order homogeneous solidarity as well. Secondly, generalized reciprocity has different characteristics than trust and tolerance. For instance, it is more strongly correlated {{with the experience of}} receiving help from somebody unknown. We discuss that the underlying mechanism of this property is related to 2 ̆ 7 imagined community 2 ̆ 7 rather than 2 ̆ 7 strength of weak ties...|$|R
40|$|The {{prevalence}} of vertical distortion in the periapical radiograph of the anterior maxillary teeth is quite significant and cingulum {{is commonly used}} as the reference of vertical distortion in anterior radiograph. Objective: To evaluate the limit of vertical angulation error that still can be tolerated. Methods: Periapical radiograph with vertical angle 0 ° was obtained from 30 maxillary incisors as reference, then the vertical angulation was changed into - 10 °, + 10 °, - 15 °, + 15 °, - 20 ° and + 20 °. Long axis of the teeth was <b>adjusted</b> <b>parallel</b> to the film. Tooth length and cingulum width with vertical angulation alteration was measured and compared to the actual length. All of the measurement was tested using T test. Results: There {{were no significant differences}} between all the measurements of tooth length with the alteration in vertical angulation (p> 0. 05), whereas cingulum width had a significant difference at + 15 ° and - 10 °, p< 0. 05. Conclusion: Tooth length in periapical radiograph of maxillary incisor with parallel position is still tolerable until 20 º vertical angle errors. Cingulum width on radiograph with + 15 º vertical angle alteration is significantly narrowed and on radiograph with - 10 ° vertical angle alteration is significantly widened...|$|R
40|$|Luneburg’s {{model has}} been the {{reference}} for experimental studies of visual space for almost seventy years. His claim for a curved visual space {{has been a source}} of inspiration for visual scientists as well as philosophers. The conclusion of many experimental studies has been that Luneburg’s model does not describe visual space in various tasks and conditions. Remarkably, no alternative {{model has been}} suggested. The current study explores perspective transformations of Euclidean space as a model for visual space. Computations show that the geometry of perspective spaces is considerably different from that of Euclidean space. Collinearity but not parallelism is preserved in perspective space and angles are not invariant under translation and rotation. Similar relationships have shown to be properties of visual space. Alley experiments performed early in the nineteenth century have been instrumental in hypothesizing curved visual spaces. Alleys were computed in perspective space and compared with reconstructed alleys of Blumenfeld. Parallel alleys were accurately described by perspective geometry. Accurate distance alleys were derived from <b>parallel</b> alleys by <b>adjusting</b> the interstimulus distances according to the size-distance invariance hypothesis. Agreement between computed and experimental alleys and accommodation of experimental results that rejected Luneburg’s model show that perspective space is an appropriate model for how we perceive orientations and angles. The model is also appropriate for perceived distance ratios between stimuli but fails to predict perceived distances...|$|R
40|$|Building {{realistic}} models requires computationally expensive algorithms. Mining overwhelming {{volumes of}} historical data {{emphasizes the need}} for efficient implementations. The data versus algorithm trade off exists only if resources like memory, storage and processors are limited and/or algorithm implementation does not capitalize on these hardware resources. This research focuses on introducing a terascale “Pervasive ® Parallelism ” Java framework called Pervasive DataRush™, capable of delivering sustained high performance across large scale Netflix [REF 3] datasets for recommender systems on commodity multicore; ours tested on up to 32 x 64 -bit cores. Pervasive DataRush uses dataflow [REF 1] process networks to operate on very large data sets. The Pervasive DataRush Library is an extensive and customizable Java library of massively parallel programming components using dataflow principles. Components are even “self-composing, ” with late-binding facilities to dynamically <b>adjust</b> <b>parallel</b> execution strategies. In this paper, a Minimum Sum-Squared Residue Coclustering [REF 2] algorithm {{is applied to the}} Netflix Competition Dataset [REF 3]. Coclustering simultaneously clusters rows and columns to find natural patterns of customers and movies based on the customer movie ratings. Results demonstrate linear scaling across data and cores. Typical runtimes drop from 226 sec to 9 sec (Netflix 5 %), demonstrating sustained performance and scaling from 1 to 32 cores. Additionally results show that this massively parallel two-dimensional k-means algorithm operates in a dataflow graph...|$|R
40|$|Abstract Deep packet {{inspection}} (DPI) scans both packet headers and payloads {{to search}} for predefined sig-natures. As link rates and traffic volumes of Internet are constantly growing, DPI is facing the high performance challenge of how to achieve line-speed packet processing with limited embedded memory. The recent trie bitmap content analyzer (TriBiCa) suffers from high update overhead and many false positive memory accesses, while the shared-node fast hash table (SFHT) suffers from high update overhead and large memory requirements. This paper presents an index-split Bloom filter (ISBF) to overcome these issues. Given a set of off-chip items, an index of each item is split apart into several groups of constant bits, and each group of bits uses an array of on-chip parallel counting Bloom filters (CBFs) to represent the overall off-chip items. When an item is queried, several groups of on-chip parallel CBFs constitute an index of an off-chip item candidate for a match. Furthermore, we propose a lazy deletion algorithm and vacant insertion algorithm to reduce the update overhead of ISBF, where an on-chip deletion bitmap is used to update on-chip <b>parallel</b> CBFs, not <b>adjusting</b> other related off-chip items. The ISBF is a time/space-efficient data structure, which not only achieves O(1) average memory accesses of insertion, deletion, and query, but also reduces the memory requirements. Experimental results demonstrate that compared with the TriBiCa and SFHT, the ISBF significantly reduces the off-chip memory accesses an...|$|R
40|$|With a {{large number}} of {{companies}} failing to implement Enterprise Resource Planning (ERP) systems a reasonable amount of research has been carried out in this field, nevertheless, “most research has focused on the big business sector, and the findings cannot easily be extended to SMEs because of their particular characteristics”(Poba-Nzaou et al. 2008). Different factors have been identified and ERP vendors are struggling to provide a flexible system that adapts to meet SME requirements. Scheduling systems, especially those handling High Variety Low Volume (HVLV) production, have struggled to perform according to SMEs needs. HVLV in a make to order environment requires precision production planning to reduce lead times, {{to make the most of}} available capacity and to be competitive. Given the lack of high performance ERP software, SMEs <b>adjust</b> <b>parallel</b> systems (analytical and algorithmic aids) to fill the gaps. Those alternative systems interrupt the flow of information and hinder the optimisation of resources. The Knowledge Transfer Partnership between the University of Greenwich and MEP Ltd aims to adjust the ERP scheduling module to company’s requirements. The objectives are to identify the root cause of inconsistencies and to design a scheduling method that is aligned with MEP’s needs. As a result of the Scheduling system inconsistencies, MEP Ltd has developed an alternative tool to schedule production. This analytical system generates a production plan; however, it is neither efficient nor accurate. This research paper presents the issues faced by a SME using an ERP system that does not perform as required. It describes scheduling systems in an ideal environment, compared with an analytical and algorithmic aid used by MEP Ltd. It finally shows the limitations of those analytical systems and why it is required to integrate information systems...|$|R
40|$|Background: Masticatory {{functioning}} alters with age. However, mastication {{has been}} found to be related to, for example, cognitive functioning, food intake, and some aspects of activities of daily living. Since cognitive functioning and activities of daily living show a decline in older adults with dementia, improving masticatory functioning may be of relevance to them. A possible way to improve mastication may be showing videos of people who are chewing. Observing chewing movements may activate the mirror neuron system, which becomes also activated during the execution of that same movement. The primary hypothesis is that the observation of chewing has a beneficial effect on masticatory functioning, or, more specifically, masticatory ability of older adults with dementia. Secondary, the intervention is hypothesized to have beneficial effects on food intake, cognition, activities of daily living, depression, and quality of life. Methods/Design: An <b>adjusted</b> <b>parallel</b> randomized controlled trial is being performed in dining rooms of residential care settings. Older adults with dementia, for whom also additional eligibility criteria apply, are randomly assigned to the experimental (videos of chewing people) or control condition (videos of nature and buildings), by drawing folded pieces of paper. Participants who are able to watch each other's videos are assigned to the same study condition. The intervention takes place during lunchtime, from Monday to Friday, for 3 months. During four moments of measurement, masticatory ability, food intake, cognitive functioning, activities of daily living, depression, and quality of life are assessed. Tests administrators blind to the group allocation administer the tests to participants. Discussion: The goal of this study is to examine the effects of video observation of chewing on masticatory ability and several secondary outcome measures. In this study, the observation of chewing is added to the execution of the same action (i. e., during eating). Beneficial effects on masticatory ability, and consequently on the other outcome measures are hypothesized. The intervention may be easily integrated into daily care, and might add to the lives of the increasing number of older adults with dementia by beneficially influencing multiple daily life functions...|$|R
40|$|The recent {{remarkable}} {{growth in}} the telecommunications industry has caused a huge advance in filter technology. The new communication systems demand stringent filters with high specifications, low costs, compact features, {{and the ability to}} accommodate multiple wireless standards (2 G, 2. 5 G, 3 G, WiMAX, Mobile WiMAX, LTE and LTE-A). In order to realize a multi-standard RF front end and cover all frequency bands, a transceiver requires a customized RF device that includes tunable filters. The main building block of any tunable filter is a tunable single resonator. The thesis introduces two novel techniques to extract an equivalent circuit model for RF resonators. Both techniques are successfully tested and validated for different case studies. Using a systematic approach, the lumped element technique can be used to relate the EM model to an <b>adjusted</b> <b>parallel</b> RLC model that takes into consideration the input coupling. The node-to-node technique is based on using a transmission line equivalent to model combline resonators. Several examples are used to apply the technique to gain more insight into understanding and thus improving resonator performance. The thesis also presents the design and implementation of a high-Q bandpass filter using the proposed angular tuning technique that maintains a constant Q value over a relatively wide tuning range. The traditional technique for tuning combline filters is achieved by changing the gap between the post and the tuning disk. Such a technique is known to yield a Q value that degrades considerably at the lower edge of the tuning range. The proposed angular tuning technique shows a 25 % improvement in the Q value at the lower edge of the tuning range, in comparison to what is typically achieved using the traditional tuning technique. Using the proposed angular tuning technique, a 1 % bandwidth 2 -pole filter is designed, fabricated, and tested with a 430 MHz tuning range at a center frequency of 3. 6 GHz. The filter is integrated with miniature piezoelectric motors, demonstrating an almost constant insertion loss over the tuning range...|$|R
40|$|Particle Image Velocimetry (PIV) is a {{powerful}} tool for the non-intrusive investigation of complex flow fields. The present paper describes Stereo PIV measurements of the flow field behind a counter rotating ultrahigh-bypass fan turbine power simulator (CRUF-TPS) which have been performed at the low speed wind tunnel NWB of the German-Dutch Wind Tunnels (DNW). The experimental data was used for the validation of numerical codes using different turbulence models, as former CFD calculations suffered from the absence of a complete experimental data base. The aim of the study was the investigation of the turbulence and the shear flow mixing in the flow behind the CRUF-TPS. The experimental SPIV set-up was made up of two PCO 4000 cameras (11 Megapixels) providing a very high spatial resolution, and a high power Nd-YAG double pulse laser with 380 mJ per pulse output energy (λ = 532 nm). The flow was seeded with DEHS particles with a diameter of about 1 µm. In two different planes all three components {{of a large number of}} instantaneous velocity vector fields were measured in forward scattering mode for several different flow and turbine conditions. The first plane perpendicular to the main flow with an observation area of 410 x 330 mm² was located at x = 450 mm downstream of the TPS core trailing edge, the second plane was <b>adjusted</b> <b>parallel</b> to the main flow between x = 10 mm and x = 310 mm with a size of 360 x 300 mm². In both cases, the shear layers between free stream, fan flow and core flow of the TPS could be observed. In order to determine the differences between periodic and non-periodic effects of the turbulence mixing in the shear layers, the PIV measurements were triggered both at a random point of time and phase-locked to a fixed blade position of the TPS fan. For the evaluation of the PIV images an iterative multi-grid cross-correlation method with image deformation was applied with 32 x 32 pixels² final window size and a step width of 16 pixels in each direction, leading to instantaneous velocity fields with up to 43000 three-component-vectors. A number of 700 to 1000 double images were acquired per test case for reaching convergence of the averages and RMS-values. For the first PIV set-up (light sheet perpendicular to the main flow), also the x-component of the flow vorticity could be calculated. ...|$|R
40|$|Risk {{management}} is increasingly used in dam safety and includes risk analysis, risk evaluation and risk reduction. Structural Reliability Analysis (SRA) is a probabilistic methodology {{that may be}} used in the risk assessment process. SRA has been frequently used for calibration of partial factors in limit state design codes for structures (not dams). In a reliability analysis a mathematical description of the failure mode, a limit state function, is defined. All parameters describing the limit state function should be random variables and are described by stochastic distributions (or, where appropriate, a deterministic value). The safety index (or probability of failure) may be determined by e. g. First Order Reliability Method and the result is compared to a target safety index to determine if the structure is safe enough. Several difficulties exists in the use of SRA for concrete dams, mainly {{due to the fact that}} only a few examples of such analysis for dams exist. One difficulty is how to define the failure modes. In this thesis a complete system of failure modes is identified, where failure is considered as a series system of “failure in the concrete part”, “failure in the concrete-rock interface” and “failure in the rock mass”. Failure in the concrete-rock interface may occur due to sliding or overturning. Sliding is the joint occurrence of sliding with a partially bonded contact (fails at very small displacement) and sliding with broken contact (fails at larger displacement) and both have to occur for sliding to occur, hence they are treated as a <b>parallel</b> system. <b>Adjusted</b> overturning is a combination of overturning and crushing of the concrete or crushing of the rock. A substantial part of the work has been to define the necessary input data. - Cohesion in the interface is very important. Due to the expected brittle failure in a partly intact interface, treatment of the shear resistance as a brittle parallel system is proposed. - Description of the headwater results in a series system; either failure occurs for water levels at retention water level (rwl) or for water levels above rwl, the latter described by an exponential distribution. - Uplift is one of the most important loads. A geostatistical simulation procedure is presented, where the hydraulic conductivity field of the foundation is described by a variogram and uplift is simulated by a FE-analysis. This methodology is demonstrated to be very useful and gives estimates of the statistical distribution of uplift. Three papers on this subject are included; the first is a description of the methodology, the second presents a sensitivity analysis performed for a large number of different combinations of input data and the last is an application to a Brazilian dam, where water pressure tests and monitoring results are available. In two papers SRA is applied to concrete dams and the system reliability is determined. In the first paper a spillway section where information of e. g. cohesion, friction angles etc. were available is analysed. In the last paper an idealized dam and a power intake structure are analysed. The conclusions are that SRA may be used for assessment of concrete dam stability and that it is well fitted for the dam safety risk management process. Every dam is a unique prototype and SRA enables specific behaviour and properties of a certain structure to be taken to consideration. The system reliability analysis is a very valuable tool in understanding the relationship between failure modes and enables the safety for the whole structure to be determined. In a reliability analysis the most important parameters may be identified and thus safety measures can be focused where it gives the best possible output. A general safety consideration is that development of the safety concept for concrete dams, from deterministic to probabilistic or semi-probabilistic, will give a known and more uniform level of safety...|$|R
40|$|Mobility {{is the key}} to {{the global}} {{business}} which requires people to be always connected to a central server. With the exponential increase in smart phones, tablets, laptops, mobile traffic will soon reach in the range of Exabytes per month by 2018. Applications like video streaming, on-demand-video, online gaming, social media applications will further increase the traffic load. Future application scenarios, such as Smart Cities, Industry 4. 0, Machine-to-Machine (M 2 M) communications bring the concepts of Internet of Things (IoT) which requires high-speed low power communication infrastructures. Scientific applications, such as space exploration, oil exploration also require computing speed in the range of Exaflops/s by 2018 which means TB/s bandwidth at each memory node. To achieve such bandwidth, Input/Output (I/O) link speed between two devices needs to be increased to GB/s. The data at high speed between devices can be transferred serially using complex Clock-Data-Recovery (CDR) I/O links or parallely using simple source-synchronous I/O links. Even though CDR is more efficient than the source-synchronous method for single I/O link, but to achieve TB/s bandwidth from a single device, additional I/O links will be required and the source-synchronous method will be more advantageous in terms of area and power requirements as additional I/O links do not require extra hardware resources. At high speed, there are several non-idealities (Supply noise, crosstalk, Inter- Symbol-Interference (ISI), etc.) which create unwanted skew problem among parallel source-synchronous I/O links. To solve these problems, adaptive trainings are used in time domain to synchronize parallel source-synchronous I/O links irrespective of these non-idealities. In this thesis, two novel adaptive training architectures for source-synchronous I/O links are discussed which require significantly less silicon area and power in comparison to state-of-the-art architectures. First novel adaptive architecture is based on the unit delay concept to synchronize two <b>parallel</b> clocks by <b>adjusting</b> the phase of one clock in only one direction. Second novel adaptive architecture concept consists of Phase Interpolator (PI) -based Phase Locked Loop (PLL) which can adjust the phase in both direction and achieve faster synchronization at the expense of added complexity. With an increase in parallel I/O links, clock skew which is generated by the improper clock tree, also affects the timing margin. Incorrect duty cycle further reduces the timing margin mainly in Double Data Rate (DDR) systems which are generally used to increase the bandwidth of a high-speed communication system. To solve clock skew and duty cycle problems, a novel clock tree buffering algorithm and a novel duty cycle corrector are described which further reduce the power consumption of a source-synchronous system...|$|R
30|$|In {{order to}} {{evaluate}} the implantation of the nail and the system running, a cadaver study using both femora of four human cadavers was conducted. All experiments were approved and conducted {{in accordance with the}} guidelines of the Committee of Medical Ethics. Written informed consent from the donor was obtained prior to their inclusion in this study. Each cadaver was thoroughly checked, and none of the cadavers had a history of musculoskeletal disease that could have {{had an impact on the}} experiment. All cadavers were frozen to a temperature of − 18 ° Celsius exactly 48  h after death and defrosted for 24  h prior to the experiment. Implantation of the CDS was carried out in supine position using standard retrograde access though the knee joint. The femoral canal was reamed over a guide-wire to a diameter of 15.5  mm followed by temporary insertion of the nail. After removal of the nail, a bone defect was created via a medial approach in order to avoid major damage of the surrounding soft tissues of the femur. Forty-millimeter bone segments on the right femora and 60 -mm bone segments on the left femora were generated using a Gigli saw. The osteotomy was performed directly distal to the insertion of adductor brevis in such a way as to preserve as much of the periosteum as possible. The nail was then reinserted into the femoral canal across the bone segment to be transported and then locked proximally in an anteroposterior direction. Standard anteroposterior (AP) and lateral radiographs were obtained to guide the nail to the correct position. Next, a 6.5 -mm lateral drill hole was generated on the bone segment followed by fixation of the bone segment on the threaded rod spindle. In order to perform distal locking of the nail, the femur had to be distracted on the side of the osteotomy as the tension force of the adherent soft tissues reduced the initial size of the bone defect. After the size of the bone defect was readjusted to a total length of 60  mm, distal locking was performed and the threaded rod was inserted into the nail. With the mechanism inserted into the nail, the traction wire was <b>adjusted</b> <b>parallel</b> to the anterior cruciate ligament (ACL) and fixed to the tibial tuberosity using a cancellous bone screw (Fig.  6). At completion, the mechanism and the system were tested by flexing the knee joint. Radiographs were taken in AP and lateral direction in order to ensure correct bone segment transport (Fig.  7). Bone transport was then conducted in all eight femora until impingement of the bone segment at the docking site. In clinical application, the screw in the tibial tuberosity will be removed at the end of segment transport and the traction wire will be cut at the distal end of the nail leading to a retraction of the wire into the nail.|$|R
40|$|The {{emergence}} of Islamic banking and finance since 1984 {{had contributed to}} the financial development leading to economic growth in Malaysia. Since Islamic banks operates within the dual banking system in Malaysia, {{it is inevitable that}} Islamic banking operations have interacted with the monetary policies and outcomes of the existing (conventional) system and as a result interact with interest rate related developments as well. In addition, macroeconomic interaction in the sense of Islamic banking contributing to economic growth will be higher if Islamic banks grow and increase their market share in the industry perhaps through applying legitimate Islamic compliance products and valid contracts. This research, hence, aims {{to assess the impact of}} monetary policy as well as macroeconomic outcomes on Islamic banking operations and vice versa in the case of Malaysia in particular in the post- 1990 period. In doing so, each research question is examined and answered in three separate yet connected empirical papers. Each econometric analysis is based on two steps Engle-Granger tests with different timelines and sampling due to availability of the data. The main tests conducted was two steps Engle-Granger approach which involved Ordinary Least Square estimation, Residual testing, and Cointegration test. Vector Error Correction Model and Granger Casusality were also employed for further investigation of the long and short-run relationship between variables. The finding indicates that the Islamic banks do have long run dynamic relationship with economic growth. However, the conventional banks are the dominion due to small market share of the Islamic banks which is less than 30 per cent. With regards to the interest rate interaction, both conventional and Islamic interbank money market rates moving closely in the long run statistically indicates that Islamic inter-bank money market is co-integrated in long-run because both conventional and Islamic banks are subjected to the same monetary policy. While saving interest rates of the conventional banks and profit rates of the Islamic banks are not associated but moving <b>parallel</b> need to <b>adjust</b> their return rates in the long short and long run either slightly higher or lower to remain competitive in the market. The findings on the relationship between Islamic banks saving profit rates spread show that they are affected by financial factors such as inflation rate and real interest rate. This indicates that Islamic banks are exposed to the interest rates risk, as they operate under the dual banking system and reign by the same monetary policies. The results indicate that there is a close and inevitable interaction between Islamic banking operations and monetary policy outcomes. In particular close operational relationship between Islamic banking profit rates and interest rates should be considered as a source of worry in the face of the religious positioning of the prohibition of interest rate. The outcome reflects that prohibition of riba is only considered as the form nature of Islamic law (Shari’ah), while the operations of Islamic banks are vulnerable to interest rates risk in everyday life. However, in dual banking system such interactions will be inevitable. ...|$|R
40|$|Neutronenspektroskopie ist eine führende Methode zur Untersuchung der Dynamik kondensierter Materie auf atomarer Skala. Die Neutronen Spin Echo (NSE) Technik eröffnet die Möglichkeit, die Energieauflösung dieser Messungen ohne fatale Verluste an Intensität zu erhöhen. Das grosse Interesse an Linienbreiten dispersiver Anregungen, z. B. Phononen in Festkörpern, führte in den letzten fünfzehn Jahren zur Entwicklung einer Methode, die NSE und Dreiachsenspektroskopie (TAS) kombiniert. Diese fokussierende NSE Methode, vorgeschlagen von F. Mezei, erlaubt es, Linienbreiten dispersiver Anregungen in der gesamten Brillouin-Zone mit ?eV Auflösung zu messen, was die typische TAS Auflösung um 2 Grössenordnungen verbessert. Bei dieser Technik werden die Flächen konstanter Spin Echo Phase {{parallel}} zur Dispersionsfläche justiert. Typische Kippwinkel für dispersive Anreg-ungen in Kristallen liegen bei bis zu 50 º, was mit den Standardspulen des klassischen Spin Echos nicht erreichbar ist. Diese grossen Kippwinkel ermöglichte erst die von R. Golub und R. Gähler eingeführte Neutronen Resonanz Spin Echo (NRSE) Methode. Bei dieser Methode ersetzen 2 kurze Hochfrequenz Flipper mit Nullfeld im Zwischenraum eine grosse NSE Spule. Im Rahmen dieser Dissertation wurde am Institut Laue Langevin in Grenoble in den Jahren 2000 2002 ein derartiges Gerät gebaut und erfolgreich am TAS Spektrometer IN 3 getestet. Entwicklung, Test und Aufbau dieser NRSE Option ZETA (Zero field spin Echo for Three Axis) am IN 3 Spektrometer und die Durchführung erster Messungen waren die wesentlichen Teile dieser Dissertation. Die Energieauflösung des IN 3 wurde von 1. 1 meV auf 1 ?eV verbessert. Erste Testexperimente wurden durch-geführt an Linienbreiten in Ge und in superfluidem 4 He Linienbreiten am Roton, an Phonenen mit niedrigem q und jenseits des Rotonen-Minimums. Die Messungen am Rotonen-Minimum und bei niedrigem q stimmen gut mit Literaturwerten überein. ZETA hat seine Leistungsfähigkeit demonstriert und der Umzug vom Test-spektrometer IN 3 zum Hochfluss-Spektrometer IN 20 ist für den 2. Reaktorzyklus 2003 geplant. Die erwartete Flusserhöhung von einer Grössenordnung wird erstmals die Messung von Phononenlebensdauern auf breiter Basis ermöglichen. Neutron spectroscopy is {{a leading}} method for investigating the dynamics of condensed matter on an atomic scale. The neutron spin echo (NSE) technique gives us {{a unique opportunity to}} enhance the energy resolution during these investigations without any fatal loss of neutron intensity. Because of the significant interest in the linewidths of dispersive excitations, such as phonons in solids, a new method, combining NSE and three-axis spectrometry (TAS), has been developed over the last fifteen years. This spin echo focusing method, proposed by F. Mezei, makes it possible to measure the linewidths of dispersive excitations in solids over the entire Brillouin zone with ?eV resolution, thus improving the typical TAS resolution by about 2 orders of magnitude. In this technique, the surfaces of constant spin echo phases are <b>adjusted</b> <b>parallel</b> to the dispersion surface, requiring inclined field boundaries of the spin echo precession fields. Typical inclination angles for dispersive excitations in crystals range up to 50 º and are not attainable with standard coils in classical spin echo. These large inclination angles were first possible {{with the advent of the}} neutron resonance spin echo (NRSE) technique introduced by R. Golub and R. Gähler. With this technique, two short radio frequency flippers with zero magnetic field between them, replace each big NSE coil. As part of this thesis, an instrument of this kind was built at the Institute Laue-Langevin, Grenoble, during the years 2000 - 2002 and was successfully tested at the TAS spectrometer IN 3. The main parts of this thesis were developing, testing and installing this NRSE option ZETA (Zero field spin Echo for Three Axis) on IN 3 and performing the first test measurements. An improvement in the energy resolution of IN 3 from 1. 1 meV to 1 eV was achieved. The first test experiments were done on phonon linewidths in Ge, on linewidths at the roton and at low q phonons in superfluid 4 He and on linewidths in 4 He beyond the roton minimum. The results at the roton minimum and at low q phonons are consistent with the literature. ZETA has demonstrated its reliable performance, and its move from the test spectrometer IN 3 to the high flux TAS-spectrometer IN 20 is planned for the second reactor cycle in 2003. With a flux gain of one order of magnitude, a large area of phonon life times will become accessible for the first time...|$|R

