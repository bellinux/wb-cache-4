204|526|Public
25|$|Windows Vista {{includes}} {{context menu}} commands for folders {{to open a}} command prompt at the selected folder and to copy their path. Shortcut items have a context menu to open their target location. A <b>sound</b> <b>event</b> and shell overlay icon are also present for User Account Control.|$|E
5000|$|Rob Swift, <b>Sound</b> <b>Event,</b> The Ghetto, (bass, trumpet, co-writing) ...|$|E
5000|$|Rob Swift, <b>Sound</b> <b>Event,</b> Salsa Scratch feat. D-Styles & Bob James, Dave Guy (trumpet) ...|$|E
30|$|In {{order to}} have more {{accurate}} modeling, the acoustic models for <b>sound</b> <b>events</b> are trained within each available context. Context-dependent count-based priors for the <b>sound</b> <b>events</b> are collected from the annotations of training material.|$|R
30|$|Finding {{samples that}} provide a good {{representation}} of <b>sound</b> <b>events</b> as defined in the taxonomy was more demanding. We collected samples from three main sources: the <b>Sound</b> <b>Events</b> database ([URL] a collection of sound effects CDs, and Freesound.|$|R
5000|$|New {{configurable}} <b>sound</b> <b>events</b> for Device Connect, Device Disconnect, Device Failed to Connect, Print Complete, New fax, Fax Error, System Notification, Windows Logon and Windows Logoff. Windows XP Service Pack 2 added <b>sound</b> <b>events</b> in Internet Explorer for Blocked {{pop-up window}} and the Information bar.|$|R
50|$|<b>Sound</b> <b>Event</b> is a 2002 studio album by American turntablist Rob Swift. It was {{released}} on Tableturns.|$|E
5000|$|King Tut's have {{launched}} King Tut's Recordings and a Your <b>Sound</b> <b>event</b> every month {{to help support}} unsigned bands become acquainted with the workers in the local music industry ...|$|E
5000|$|Laura Checkoway of Vibe said, [...] "Instead of {{overwhelming}} listeners with robotic DJ overload, <b>Sound</b> <b>Event</b> artfully combines technical dexterity with simple rap lyrics, live instrumentals, and politically charged sound bites." ...|$|E
5000|$|FMOD Designer 2010 - An audio {{designer}} tool {{used for}} authoring complex <b>sound</b> <b>events</b> and music for playback.|$|R
40|$|The audio {{semantic}} concepts (<b>sound</b> <b>events)</b> play {{important roles}} in audio-based content analysis. How to capture the semantic information effectively from the complex occurrence pattern of <b>sound</b> <b>events</b> in YouTube quality videos is a challenging problem. This paper presents a novel framework to handle the complex situation for semantic information extraction in real-world videos and evaluate through the NIST multimedia event detection task (MED). We calculate the occurrence confidence matrix of <b>sound</b> <b>events</b> and explore multiple strategies to generate clip-level semantic features from the matrix. We evaluate the performance using TRECVID 2011 MED dataset. The proposed method outperforms previous HMM-based system. The late fusion experiment with the low-level features and text feature (ASR) shows that audio semantic concepts capture complementary information in the soundtrack...|$|R
40|$|We {{present a}} {{transcription}} system {{that takes a}} music signal as input and returns its musical score. Two stages of processing are used. The first employs a fundamental frequency detector and an onset detector to transform input signals into a sequence of <b>sound</b> <b>events.</b> The onset detection is inherently noisy. This paper focuses on the second stage, going from <b>sound</b> <b>events</b> to a notated score. We use a family of graphical models for this task. We allow the results of onset detection to be noisy, necessitating a search over possible segmentations of the <b>sound</b> <b>events.</b> We use a large corpus of monophonic vocal music to evaluate our system. Our results show that our approach is well-suited {{to the problem of}} music transcription. The initial onset detection reduces the number of observations and makes the system less instrument specific. The search over segmentations corrects the errors in the onset detection...|$|R
50|$|In 2004 Morrow {{participated in}} the Future of <b>Sound</b> <b>event</b> at the British Academy Awards, co-produced the New Sound New York Sound Cube show at The Kitchen, and had a solo sound art show in the MUU Gallery, Helsinki.|$|E
50|$|In 2013 Dan Hornsby {{was given}} a display in the Grammy Museum in Los Angeles, CA, for the year-long Columbia Records 360 <b>Sound</b> <b>event</b> where his grand daughter, Nikki Hornsby, was invited for a private viewing with CJP-NHRecords staff.|$|E
5000|$|Windows Vista {{includes}} {{context menu}} commands for folders {{to open a}} command prompt at the selected folder and to copy their path. Shortcut items have a context menu to open their target location. A <b>sound</b> <b>event</b> and shell overlay icon are also present for User Account Control.|$|E
40|$|Acoustic {{analyses}} were conducted on several groups of <b>sound</b> <b>events.</b> The <b>sound</b> <b>events</b> were derived {{from a combination of}} physical analyses and perceptual data. The acoustic analyses utilized both low‐level properties, such as spectral centroid, and higher‐level properties. The higher‐level properties were based on a combination of the physical sourcecharacteristics of basic classes of sound‐generating events, statistical methods, and perceptual characteristics. A range of metrics are used to evaluate the success and generality of these acoustic properties, including classification performance and comparison to other results in the literature...|$|R
40|$|Abstract. We {{developed}} a mixed reality (MR) system which merges {{the real and}} the virtual worlds in both audio and visual senses. Our new approach “RealSound Interaction ” {{is based on the}} idea that the <b>sound</b> <b>events</b> in the real world can work as interaction devices with an MR space. Firstly, we {{developed a}} sound detection system which localizes a sound source. The system consisted of two types of microphone arrays, fixed type and wearable type. Secondly, we evaluated the accuracy of the system, and proposed three practical usages of the <b>sound</b> <b>events</b> as interactive devices for MR attractions...|$|R
40|$|The Sound Design Toolkit is a {{collection}} of physically informed sound synthesis models, specifically designed for practice and research in Sonic Interaction Design. The collection is based on a hierarchical, perceptually founded taxonomy of everyday <b>sound</b> <b>events,</b> and implemented by procedural audio algorithms which emphasize the role of sound as a process rather than a product. The models are intuitive to control – and the resulting sounds easy to predict – as they rely on basic everyday listening experience. Physical descriptions of <b>sound</b> <b>events</b> are intentionally simplified to emphasize the most perceptually relevant timbral features, and to reduce computational requirements as well...|$|R
50|$|Between 2001 and 2006 Caleb Kelly curated impermanent.audio; an {{independent}} <b>sound</b> <b>event</b> {{primarily concerned with}} the performance of new musics, focused listening, the still contemplation of abstract audio and the act of listening itself. By the time it ended in 2006 Kelly had produced over 100 events and festivals.|$|E
5000|$|Founded by Austin-based {{promotions}} company Margin Walker Presents {{and produced}} by Funhouse Services and Sound on <b>Sound</b> <b>Event</b> Services, SOS Fest {{is home to}} three music stages showcasing emerging and established talent from the worlds of indie, punk, hip hop, metal, and dance music, as well as comedy.|$|E
5000|$|On 14 November 2013, Allen {{made her}} debut live {{performance}} of [...] "Hard out Here" [...] in the YoYos pod at the Red Bull Revolutions in <b>Sound</b> <b>event</b> on the London Eye. The song {{was chosen as}} the opening theme of the eighth season of the German reality television series Ich bin ein Star - Holt mich hier raus!, which premiered on 17 January 2014, the same date as the single's release in Germany.|$|E
50|$|High Reflections was {{a monthly}} series of <b>sound</b> <b>events</b> started in 2009 and curated by Kelly and Alex White at Serial Space in Sydney. It ended in May 2011 with a mini festival at Red Rattler.|$|R
40|$|Environmental sounds {{present a}} {{difficult}} problem for sound modeling because spectral and temporal cues are tightly correlated. These sounds form classes that cannot {{be handled by}} traditional synthesis methods. Micro-level representations provide ways to control spectral and spatial cues in sound synthesis and meso-level representations determine the temporal structure of <b>sound</b> <b>events.</b> By constraining the synthesis parameter space to ecologically meaningful ranges and defining parametric transformations along perceptually relevant dimensions {{we are able to}} model <b>sound</b> <b>events</b> at the micro and meso level. The integration of these approaches into a coherent data structure extends the parameter space of ecological models to the domain of spectral and spatial cues...|$|R
40|$|Presented at the 20 th International Conference on Auditory Display (ICAD 2014), June 22 - 25, 2014, New York, NY. This project, I Hear NY 4 D, {{presents}} a modular auditory display platform for layering recorded sound and sonified data into an immersive environment. Our specific {{use of the}} platform layers Ambisonic recordings of New York City and a palette of virtual <b>sound</b> <b>events</b> that correspond to various static and realtime data feeds based on the listener’s location. This creates a virtual listening environment modeled on an augmented reality stream of sonified data in an existing acoustic soundscape, allowing for closer study {{of the interaction between}} real and virtual <b>sound</b> <b>events</b> and testing the limits of auditory comprehension...|$|R
50|$|In 2006 and 2007 she {{presented}} the Coterie Exchange <b>sound</b> <b>event</b> Sonic Triptych in Oakland, LA, and NY. The NY version was a collaboration with filmmaker/video artist James Schneider (who directed Blue is Beautiful). Sonic Triptych first premiered in San Francisco in 2002 with nine women, including Blevin Blectum {{and members of}} Erase Errata. A video of Duct Tape Piece, a collaboration with Alyssa Lee, was exhibited in Europe through Chicks on Speed in 2007 and 2008.|$|E
50|$|Chen's {{style of}} {{composition}} {{is close to}} the Asian mentality, where the creation and development of the sound is in the centre of attention. His works consist of a seemingly simple <b>sound</b> <b>event</b> which will develop in unexpected ways. Chen regards the process of composing as communication with the sound; he tries to find and show the possibilities the sound provides. It {{is for this reason that}} Chen's works create an open atmosphere which leads, while listening, to the impression that neither composer nor listener can know where the music is leading.|$|E
5000|$|On 14 November 2013, Allen {{made her}} debut live {{performance}} of [...] "Hard out Here" [...] in the YoYos pod at the Red Bull Revolutions in <b>Sound</b> <b>event</b> on the London Eye. During {{an interview with}} Graham Norton on The Graham Norton Show on 21 February 2014, Allen performed the album's second single, [...] "Air Balloon". On 24 May 2014, Allen performed [...] "Sheezus", [...] "Hard Out Here", [...] "URL Badman" [...] and [...] "Our Time" [...] {{as part of her}} set at BBC Radio 1's Big Weekend in Glasgow.|$|E
40|$|In {{this paper}} we {{describe}} {{a method for}} the transcription of percussive audio signals which have been performed with arbitrary nondrum sounds. The system locates <b>sound</b> <b>events</b> from the input signal using an onset detector. Then a set of features is extracted from the onset times. Feature vectors are clustered and the clusters are assigned with labels which describe the rhythmic role of each event. For the labeling, a novel method is proposed {{which is based on}} metrical (temporal) positions of the <b>sound</b> <b>events</b> within the measures. The system is evaluated using monophonic percussive tracks consisting of non-drum sounds. In simulations, the system achieved a total error rate of 33. 7 %. Demo signals are available a...|$|R
40|$|The Sound Design Toolkit is a {{collection}} of physically-informed sound syn- thesis models, specifically designed for practice and research in Sonic In- teraction Design. The collection is based on a hierarchical, perceptually founded taxonomy of everyday <b>sound</b> <b>events,</b> and implemented by procedu- ral audio algorithms which emphasize the role of sound as a process rather than a product, while yielding acoustic results which are easily predictable and intuitively controllable resorting to basic everyday listening experience. Physical descriptions of <b>sound</b> <b>events</b> are intentionally simplified to achieve cartoonification, namely an exaggeration of the most perceptually relevant timbral features and a reduction of synthesis parameters which are functional to achieve both a higher computational efficiency and a better perceptual clarity...|$|R
40|$|This paper {{presents}} {{a system for}} acoustic event detection in recordings from real life environments. The events are modeled using a network of hidden Markov models; their size and topology is chosen based on a study of isolated events recognition. We also studied the effect of ambient background noise on event classifi-cation performance. On real life recordings, we tested recognition of isolated <b>sound</b> <b>events</b> and event detection. For event detection, the system performs recognition and temporal positioning of a se-quence of events. An accuracy of 24 % was obtained in classifying isolated <b>sound</b> <b>events</b> into 61 classes. This corresponds to the ac-curacy of classifying between 61 events when mixed with ambient background noise at 0 dB signal-to-noise ratio. In event detection, the system is capable of recognizing almost {{one third of the}} events, and the temporal positioning of the events is not correct for 84 % of the time. 1...|$|R
5000|$|... 2006 and 2007 were, {{without a}} doubt, {{the most active}} at the DJ and musical career of D-Mark. During this time, he carried his music through all the Spanish {{territory}} in the cities of Ibiza, Barcelona, Madrid or Valladolid, and the countries of Norway, England and Slovenia and performing together {{with some of the}} most influenced DJs of the dance scene at these years, like Above & Beyond, Lemon & Einar K or Inkfish. In 2006 he made his debut abroad, in a Ministry of <b>Sound</b> <b>event</b> at Sentrum Scene in the city of Oslo, Norway.|$|E
5000|$|In Jubiläum, Stockhausen {{composed}} an orchestral <b>sound</b> <b>event</b> with {{superimposed layers}} of different speeds, degrees of noise, degrees of indeterminacy and integration, and simultaneous transitions from ordered to disordered and back [...] The work is built upon a formula, {{announced at the}} beginning as a massive hymn-like chant in the brass and low strings. The harmonic language of Jubiläum {{is reminiscent of the}} music of Stockhausen’s teacher Olivier Messiaen, and the dramatic use of space recalls Hector Berlioz. The formula is presented mainly in a series of dense textures overlaid with shimmering glissandos and rapid melodic figurations, in a [...] "mix of majestic confidence and restless activity" [...] that produces a quality of [...] "breathtaking splendour that is Stockhausen's alone" [...]|$|E
5000|$|Music of Changes {{comprises}} four [...] "books" [...] of music. Cage used {{a heavily}} {{modified version of}} his chart system (previously used in Concerto for prepared piano). Every chart for Music of Changes is 8 by 8 cells, to facilitate working with the I Ching which has a total of 64 hexagrams. The I Ching is first consulted about which <b>sound</b> <b>event</b> to choose from a sounds chart, then a similar procedure is applied to durations and dynamics charts. Thus, a short segment of music is composed. Silences are obtained from the sounds charts: these only contain sounds in the odd-numbered cells. To introduce new material, all charts alternate between mobile and immobile states (the alteration governed by the I Ching as well); in the latter the chart remains unchanged, but in the former, once a particular cell is used, its contents are immediately replaced by something new.|$|E
40|$|Video-based {{surveillance}} {{systems may}} benefit from the integration with microphone arrays for the localization of <b>sound</b> <b>events.</b> Applying the <b>sound</b> localization techniques to the surveillance of large areas requires addressing some open issues, such as the non uniform resolution of the microphones-based localization systems. This paper presents a new method for tracking moving <b>sound</b> <b>events</b> based on an Hidden Markov Model (HMM), which exploits a priori information derived from medium and longterm observations of the monitored area. The results obtained with simulated trajectories show that the HMMbased tracker is able to significantly reduce the localization error. Applications {{can be found in}} surveillance systems for large areas, such as square, streets, or parking lots, where it is of interest the monitoring of moving vehicles and people. 1...|$|R
40|$|Listeners {{were asked}} to {{indicate}} the sources of dozens of <b>sound</b> <b>events</b> presented over headphones. The sounds were environmental sounds made by solid objects, liquids, and/or air undergoing a variety of actions taken from the website www. auditorylab. org. Each listener gave a series of responses about possible sources for each sound so that a similarity matrix could be constructed. The results of data reduction point to a manageable set of perceptual classes of <b>sound</b> <b>events</b> that can be described with a small number of distinctive spectro‐temporal acoustic features. These features will be related to the attributes of the events that generated each class of sounds. Results for normal‐hearing listeners will be analyzed in detail and preliminary results from hearing‐impaired listeners will be previewed...|$|R
40|$|This project {{presents}} {{a new approach}} to sound composition for soundtrack composers and sound designers. We propose a tool for usable sound manipulation and composition that targets sound variety and expressive rendering of the composition. We first automatically segment audio recordings into atomic grains which are displayed on our navigation tool according to their timbre. To perform the synthesis, the user selects one recording as model for rhythmic pattern and timbre evolution, and a set of audio grains. Our synthesis system processes then the chosen sound material to create new <b>sound</b> <b>events</b> based on onset detection of the recording model and similarity measurements between the model and the selected grains. A large variety of <b>sound</b> <b>events</b> such as those encountered in virtual environments or other training simulations...|$|R
