0|19|Public
40|$|In {{automatic}} or <b>semi-automatic</b> <b>document</b> retrieval systems, {{a hierarchical}} arrangement of concepts or terms affords modification of a query in three ways: generalization, specialization, or expansion with synonyms. Hierarchies are usually constructed manually. A method for automatic generation of hierarchies is proposed, and experimental results are presented. 1...|$|R
40|$|The Linguist’s Assistant (LA) is a {{practical}} computational paradigm for describing languages. In this paper we describe how to use LA with naturally occurring texts that exemplify interesting target-language linguistic phenomena. We will describe how such texts can be semantically analyzed using a convenient <b>semi-automatic</b> <b>document</b> authoring interface, in effect adding them to LA’s standard semantic-based elicitation corpus. We then exemplify the language description process using a phenomenon that is prevalent in our research: alienable vs. inalienable nominal possession. ...|$|R
50|$|In April 1978, Wang {{joined the}} People's Liberation Army {{and worked as}} a driver and <b>document</b> <b>handler</b> to support the {{construction}} of a coal mine in the town of Tiefa. His unit was based out of the Diaobingshan area, a suburb of Tieling. In March 1981, Wang returned to his hometown of Arxan and took up a job as a factory worker on a forestry cooperative. In December 1982, he followed his fiancee, Xiao Suli, to a state-owned food processing company {{and worked as a}} driver.|$|R
40|$|In forensics, virtual {{reconstruction}} of shredded documents {{is a well-known}} problem. <b>Semi-automatic</b> <b>document</b> reconstruction systems are usually used for virtual reconstruction. Here a content feature extraction, a content feature representation and a 1 : 1 -matching-method for the use in such reconstruction systems are presented. The content representation is given {{in the form of}} so-called abstract structure objects (ASO), which are calculated based on foreground information distributions and on color categories. The presented 1 : 1 -matching-method calculates local optima and places these optima in a global context in relation to cut-edge-pair. Experiments were performed on different real-world-datasets with different foreground characteristics. We show the good discrimination power of the presented method for the use in reconstruction systems regardless of the type of foreground information...|$|R
50|$|The FBI {{learned that}} Leung had surreptitiously copied a top-secret <b>document</b> that her <b>handler</b> Smith had checked out {{overnight}} from the FBI and transmitted {{the information to}} her MSS handler.|$|R
40|$|Image-based {{document}} management systems {{have become increasingly}} popular for handling documents that contain both text and graphical elements. When such systems are used to store confidential information, document security is a key concern. Conventional encryption techniques used for images fail to provide the level of flexibility required by such {{document management}} systems. Newer image encryption techniques provide improved flexibility {{at the cost of}} backwards compatibility and ease of use. This paper presents a novel approach to document image encryption using an online learning model. The proposed system provides backwards-compatible document image encryption in regions of interest (ROI) with support for multiple levels of authority. Furthermore, the proposed system is capable of learning from user feedback to improve the ROI selection used during the <b>semi-automatic</b> <b>document</b> encryption process. Experimental results from the encryption of test documents demonstrate the effectiveness of the proposed system. KEY WORDS Document image encryption, regions-of-interest, multilevel, online learning 1...|$|R
40|$|In this paper, we analyze {{approaches}} to the <b>semi-automatic</b> conversion of <b>document</b> indices as being used in conventional search engines to semantically enriched Topic Map structures. The examination exclusively draws upon statistical methods, so that {{to a certain extent}} the results can be viewed as being valid independently of the programming language used for an implementation...|$|R
25|$|SVG {{images can}} {{interact}} with users in many ways. In addition to hyperlinks as mentioned below, {{any part of}} an SVG image can be made receptive to user interface events such as changes in focus, mouse clicks, scrolling or zooming the image and other pointer, keyboard and <b>document</b> events. Event <b>handlers</b> may start, stop or alter animations as well as trigger scripts in response to such events.|$|R
40|$|Web {{application}} {{users and}} Web application vulnerabilities are increasing. This will inevitably expose more Web application users to malicious attacks. Security testing {{is one of}} the most important software security practices, which is used to mitigate vulnerabilities in software. Security testing of Web applications is becoming complicated, and there is still need for security testing methodologies. This indicates that security testing methodologies for Web applications needs attention. The student will contribute in this respect by doing the following: 1. Execute a thorough research among state-of-the-art security testing methodologies for Web applications. 2. Elicit a security testing methodology for Web applications based on certain defined criteria. The overall goal is to elicit a security testing methodology that: (a) Formalizes how to detect vulnerabilities in a Web application, and makes the detection process more efficient regarding time spent and the amount of vulnerabilities that are found. (b) Mitigates false-positives during the security testing process. 3. Integrate the elicited security testing methodology from point 2 into the SDLC that is being used by the AIS group at CERN. The integration is to be carried out at a proof of concept level. 4. Perform a security test on parts of CERN’s largest administrative Web application: Electronic <b>Document</b> <b>Handler</b> (EDH), which has approximately 11, 000 users at world basis. (a) A Web Vulnerability Scanner must be evaluated and selected to be used in the testing iterations when there is a need for a testing tool. (b) The security testing will be executed in four iterations; two iterations using the new methodology and two iterations using the old methodology. The testing iterations are executed to collect results based on the old and new security testing methodology. (c) Based on the results from the first and second iteration, an evaluation of the new security testing methodology is to be made...|$|R
40|$|Abstract. In this paper, we {{introduce}} Semantator, a <b>semi-automatic</b> {{tool for}} <b>document</b> annotation with Semantic Web ontologies. With a loaded free text document and an ontology, users can annotate document fragments with {{classes in the}} ontology to create instances and relate created instances with ontology properties. Also, Semantator enables automatic annotation by connecting to the NCBO annotator and the clinical Text Analysis and Knowledge Extraction Systems (cTAKES). By representing annotations in OWL, Semantator has basic reasoning capability based upon the underlying semantics of owl:disjointWith and owl:equivalentClass...|$|R
50|$|Kadish was {{employed}} as a mechanical engineer by the United States Army Armament Research, Development and Engineering Center at the Picatinny Arsenal in Dover, New Jersey from October 1963 to January 1990. Kadish conspired to disclose national defense-related documents to Israel and worked {{as an agent of}} the Israeli government from 1979 to 1985. Kadish took classified <b>documents</b> to his <b>handler's</b> home in Riverdale, Bronx several times (including information about nuclear weapons, a modified F-15 fighter, and the Patriot missiles) and let an unnamed Israeli government worker take photographs of them.|$|R
40|$|Huge {{amounts of}} legacy {{documents}} are being published by on-line digital libraries world wide. However, for these raw digital images {{to be really}} useful, {{they need to be}} transcribed into a textual electronic format that would allow unrestricted indexing, browsing and querying. In some cases, adequate transcriptions of the handwritten text images are already available. In this work three systems are presented to deal with this sort of documents. The first two address two different approaches for <b>semi-automatic</b> transcription of <b>document</b> images. The third system implements an alignment method to find mappings between word images of a handwritten document and their respective words in its given transcription. 1...|$|R
40|$|Semantic search {{has been}} one of the motivations of the Semantic Web since it was envisioned. We propose a model for the {{exploitation}} of ontology-based KBs to improve search over large document repositories. Our approach includes an ontology-based scheme for the <b>semi-automatic</b> annotation of <b>documents,</b> and a retrieval system. The retrieval model is based on an adaptation of the classic vector-space model, including an annotation weighting algorithm, and a ranking algorithm. Semantic search is combined with keyword-based search to achieve tolerance to KB incompleteness. Our proposal is illustrated with sample experiments showing improvements with respect to keyword-based search, and providing ground for further research and discussion...|$|R
5000|$|In 1943, an {{opportunity}} for espionage arose when a fellow anti-Nazi in the ministry reassigned Kolbe to higher-grade work as a diplomatic courier. On 19 August 1943, he was entrusted to travel to Bern in Switzerland with the diplomatic bag. While there, he tried to offer mimeographed secret documents to the British embassy. They rebuffed his approach, {{so he went to}} the Americans, who decided {{to take a chance on}} him. By 1944, they realised they had an agent of the highest quality, though he never received payment for his secret work. [...] He was given the code name [...] "George Wood". His US intelligence handler was Office of Strategic Services agent Allen Welsh Dulles. By the end of the war, he passed 1,600 <b>documents</b> to his <b>handler</b> in Switzerland.|$|R
40|$|To {{facilitate}} clinical research, clinical data {{needs to}} be stored in a machine processable and understandable way. Man-ual annotating clinical data is time consuming. Automatic approaches (e. g., Natural Language Processing systems) have been adopted to convert such data into structured formats; however, the quality of such automatically extracted data {{may not always be}} satisfying. In this paper, we propose Semantator, a <b>semi-automatic</b> tool for <b>document</b> an-notation with Semantic Web ontologies. With a loaded free text document and an ontology, Semantator supports the creation/deletion of ontology instances for any document fragment, linking/disconnecting instances with the properties in the ontology, and also enables automatic annotation by connecting to the NCBO annotator and cTAKES. By repre-senting annotations in Semantic Web standards, Semantator supports reasoning based upon the underlying semantics of the owl:disjointWith and owl:equivalentClass predicates. We present discussions based on user experiences of using Semantator. ...|$|R
40|$|Abstract [...] Semantic search {{has been}} one of the motivations of the Semantic Web since it was envisioned. We propose a model for the {{exploitation}} of ontology-based knowledge bases to improve search over large document repositories. In our view of Information Retrieval on the Semantic Web, a search engine returns documents rather than, or in addition to, exact values in response to user queries. For this purpose, our approach includes an ontology-based scheme for the <b>semi-automatic</b> annotation of <b>documents,</b> and a retrieval system. The retrieval model is based on an adaptation of the classic vector-space model, including an annotation weighting algorithm, and a ranking algorithm. Semantic search is combined with conventional keyword-based retrieval to achieve tolerance to knowledge base incompleteness. Experiments are shown where our approach is tested on corpora of significant scale, showing clear improvements with respect to key-word-based search. Index Terms [...] information retrieval models, ontology languages, semantic search, semantic web I...|$|R
40|$|The Condorcet {{project was}} funded by the Dutch Technology Foundation (STW) through the Werkgemeenschap Informatiewetenschap. The project was carried out at the Vossius Laboratory of the University of Twente. The main {{objective}} of the Condorcet project was to design, build and evaluate a prototype automated indexing system, supporting Information Retrieval from large, reality-level volumes (tens of thousands) of documents. The prototype has been tested on some 400 documents from two scientific domains: mechanical properties of engineering ceramics as a field of engineering, and epilepsy as a subfield of medicine. The Condorcet system is concerned with <b>semi-automatic</b> indexing of <b>documents,</b> thus producing document representations, to be used in matching user requests to these representations. More specifically, the Condorcet system indexes scientific documents by mapping titles and abstracts (henceforth referred to as descriptions) of the documents to concepts and relations, defined in modern versions of classical indexing thesauri, i. e. ontologies. It does so by using three kinds of knowledge: domain knowledge, linguistic knowledge and indexing knowledge...|$|R
40|$|The final {{publication}} {{is available}} at Springer via [URL] of Second European Semantic Web Conference, ESWC 2005, Heraklion, Crete, Greece, May 29 –June 1, 2005. Semantic search {{has been one of}} the motivations of the Semantic Web since it was envisioned. We propose a model for the exploitation of ontology-based KBs to improve search over large document repositories. Our approach includes an ontology-based scheme for the <b>semi-automatic</b> annotation of <b>documents,</b> and a retrieval system. The retrieval model is based on an adaptation of the classic vector-space model, including an annotation weighting algorithm, and a ranking algorithm. Semantic search is combined with keyword-based search to achieve tolerance to KB incompleteness. Our proposal is illustrated with sample experiments showing improvements with respect to keyword-based search, and providing ground for further research and discussion. This research was supported by the European Commission under contract FP 6 - 001765 aceMedia. The expressed content is the view of the authors but not necessarily the view of the aceMedia project as a whole. The authors would like to thank the reviewers for their detailed, accurate and helpful coments...|$|R
50|$|While {{working at}} the Marienfelde Field Site in Berlin, Carney began copying {{classified}} documents which he then provided to the East German Ministry for State Security (MfS) by repeatedly crossing back and forth into East Germany. In 1984 he was involuntarily transferred to Goodfellow Air Force Base in Texas {{to work as a}} technical instructor. Unfortunately, Carney believed, Goodfellow AFB was a training base with no real-world intelligence of any interest to the MfS. He soon discovered that he had been very wrong. He continued providing the MfS with <b>documents,</b> meeting his <b>handlers</b> in Mexico City and Rio de Janeiro In 1985. Feeling cut off from his supervisors in East Berlin and at increasing risk in what became known as 'The Year of the Spy' he sought out the protection of the East German embassy in Mexico City. From there he was flown to Havana {{with the assistance of the}} Cuban government. Weeks later he returned to East Berlin via Prague. There he continued to work for the MfS (HVA Abt. XI and HA III) by intercepting and translating non-secure telephone communications of US military commanders as well as the East German telephone lines dedicated to the US embassy in East Berlin.|$|R

