3|103|Public
40|$|This report {{focuses on}} {{research}} in the following areas: development of function-based screening routines; generating methods for <b>sorting</b> <b>output</b> of database searches; developing strategies for determining chemically relevant three-dimensional scaffolds; and the integration of computational methodologies into user friendly tools...|$|E
40|$|Our work {{represents}} one of the first attempts {{to assess the impact of}} IT (information technology) on both process output and quality. We examine the optical character recognition and barcode sorting technologies in the mail sorting process at the United States Postal Service. Our analysis is at the application level, and thus does not involve the aggregation of IT impact over multiple processes. We use data from 46 mail processing centers over 3 years to study the IT impact. We also use a set of factors in our model to account for differences in input characteristics. Our results show that mail <b>sorting</b> <b>output</b> significantly increases with higher use of IT. In addition, IT improves quality which in turn enhances output. We also find that input characteristics exert considerable influence in determining the output and quality of the mail sorting operation. For example, while absenteeism tends to decrease output and quality due to its disruptive consequences, a higher fraction of barcoded mail seems to enhance both performance measures. information technology, business value, productivity, quality, business process evaluation...|$|E
40|$|Thesis (Ph. D.) [...] University of Washington, 2014 Atmospheric classifications {{are created}} for two regions using an {{automated}} clustering technique first described in Marchand et al. 2009. This method applies an iterative clustering algorithm to regional snapshots of dynamic and thermodynamic variables from the ERA-Interim reanalysis to define atmospheric states. An atmospheric state {{in this context}} is a frequently occurring weather pattern for the region. In creating the states, a time series of atmospheric state for the region is created. These states then serve as a basis for compositing other weather observations, creating distributions of associated weather variables such as cloud occurrence, precipitation, and radiative fluxes. The states are also suitable for <b>sorting</b> <b>output</b> from general circulation models, allowing the comparison of observed and modeled cloud variables on a state-by-state basis. Analysis of which atmospheric states lead to the most error in modeled values provides insight into the particular atmospheric conditions and processes that are problematic for models. A classification for a region surrounding Darwin, Australia is used to define periods of monsoon activity and investigate the interannual and intraseasonal variability of the Australian monsoon, as well as long-term trends in precipitation at Darwin. The classification creates a time series of atmospheric states, two of which are identified as corresponding to the active monsoon and the monsoon break. Occurrence of these states is used to define onset, retreat, and intensity of the monsoon season and the timing of individual active periods. Previous studies investigated the role of the MJO during the monsoon season, but have differed on whether the MJO creates a characteristic period or duration of active monsoon periods. We use our classification-based metrics of monsoon activity to examine the timing of individual active periods each season relative to the phase of the MJO, showing that the passage of the convective anomaly over Darwin helps both trigger and end individual active periods. Lastly, we look at trends in the occurrence of the atmospheric states and find that the number of active monsoon days has increased over the previous 33 years. We show that this, rather than changes in the daily rainfall rate during active monsoon periods, is responsible for an increasing trend in annual precipitation at Darwin during that time. A second classification for the region surrounding the Southern Great Plains (SGP) site in Oklahoma produced 21 atmospheric states. Analysis of these states showed that they represented different stages of passing synoptic systems, with some states representing warm fronts, others cold fronts, still others high pressure systems, and so on. Snapshots from a 2 ° run of the AM 3 model from GFDL were sorted according to these states, producing a time series of state within the model. The time series of model state was used to composite ISCCP simulator output from the model according to each state. These simulated ISCCP composites are compared to composites of observed ISCCP data. In all states the model does not produce nearly enough high thin cloud. The representation of other cloud types depends on the state. We show that for states which have large-scale ascent (warm fronts and cold fronts) the model does not produce enough deep thick cloud, while for states that have parameterized convection (e. g. high pressure systems) the model produces too much cloud. The former we interpret as the model struggling to resolve the dynamics of fronts, while the latter indicates that the deep convective parameterization triggers too often. We demonstrate that the overall cloud amount bias is driven primarily by within-state errors in cloud amount rather than by errors in the relative frequency of occurrence of states. We compare the results of the 2 ° run of the AM 3 to a second run of the model at 0. 5 ° resolution. We find that thick cloud increases in states with large-scale ascent, and interpret this result as a result of fronts being closer to resolved in the higher resolution run of the model. We also find that high thick cloud decreases in states with parameterized convection. We find that this result is due to the distribution of CAPE values becoming more skewed at higher resolution, and thus triggering the deep convective parameterization less often. As a result of these improvements in within-state cloud amount error, the overall bias in cloud amount is the higher resolution run is as much due to errors in the frequency of occurrence of states as it is due to within-state errors. Interestingly, while individual state cloud amount biases decrease, the overall bias increases, due to changes in the distribution of states. Whether the higher resolution run constitutes a better simulation of clouds is thus an interesting question which we address...|$|E
50|$|The full <b>sorted</b> <b>output</b> is the {{concatenation}} of the buckets.|$|R
5000|$|... #Caption: An {{example of}} stable sort on playing cards. When the cards are sorted by rank with a stable sort, the two 5s must {{remain in the}} same order in the <b>sorted</b> <b>output</b> that they were {{originally}} in. When they are sorted with a non-stable sort, the 5s may end up in the opposite order in the <b>sorted</b> <b>output.</b>|$|R
50|$|Merge sort's {{most common}} {{implementation}} does not sort in place; therefore, the memory {{size of the}} input must be allocated for the <b>sorted</b> <b>output</b> to be stored in (see below for versions that need only n/2 extra spaces).|$|R
40|$|Abstract – In Java Collections, the Set {{interface}} {{is implemented}} using a balanced BST and a chained HT as TreeSet class and HashSet class, respectively. TreeSet produces <b>sorted</b> <b>output</b> for the {{values in the}} set, but {{does not support the}} output for the values in their insertion order. HashSet provides neither <b>sorted</b> nor insertion-order <b>output,</b> but a random sequence of values. LinkedHashSet, a subclass of HashSet, supports the insertion-order outputs for the values in the set, but does not produce <b>sorted</b> <b>output.</b> This paper proposes an application-level solution and two class-level solutions for making these classes accommodate the <b>sorted</b> and insertion-order <b>output</b> together in their most efficient ways. The application-level solution requires no change to the current configurations of the set classes in the Java Collections, but its flexibility and portability are restricted to the level of application code only. The LinkedTreeSet solution requires a new class to be implemented using both a balanced BST and doubly linked list. The performance of its basic operations may be affected. By introducing an internal method for producing <b>sorted</b> <b>output</b> into the LinkedHashTable class, we can easily achieve the flexibility in output format without change any other configurations of the class. If this method is placed in the HashSet class, users can freely choose their preferred output format in random order, insertion order, or ascending order. This solution can also be applied to the hash map classes in the Java Collections...|$|R
40|$|Set, a {{collection}} of distinct values, is widely used in many applications. There are three applicable set classes included in the Java Collections. TreeSet produces a <b>sorted</b> <b>output</b> in ascending order whereas HashSet provides an output with random order. LinkedHashSet, a subclass of HashSet, produces an output in insertion order, but {{does not support the}} <b>sorted</b> <b>output.</b> Three algorithms are proposed in this paper to modify the existing set classes in the Java Collections so that they can provide multiple output formats for users to select from. The algorithm at application-level does not change the current configurations of the set classes, but it offers little reusability. The algorithm at method level introduces an internal method for producing <b>sorted</b> <b>output</b> into the LinkedHashTable class, in addition to its default output in insertion order. This can be achieved without change to other configurations of the class. If this method is placed in the HashSet class, users can freely choose their preferred output format from random order, insertion order, or ascending order. The algorithm at class level proposes a new LinkedTreeSet class that is implemented using both a balanced BST and a doubly linked list. The basic operations of this class may be slightly slower than that in the TreeSet class...|$|R
25|$|Insertion sort iterates, {{consuming}} one input element each repetition, {{and growing}} a <b>sorted</b> <b>output</b> list. At each iteration, insertion sort removes one element from the input data, finds the location it belongs within the sorted list, and inserts it there. It repeats until no input elements remain.|$|R
2500|$|Levcopoulos and Petersson {{describe}} {{a variation of}} heapsort based on a Cartesian tree that does not add an element to the heap until smaller values {{on both sides of}} it have already been included in the <b>sorted</b> <b>output.</b> As they show, this modification can allow the algorithm to sort more quickly than [...] for inputs that are already nearly sorted.|$|R
40|$|We {{define a}} model of {{physical}} devices that have a parallel atomic operation that transforms an unordered list input such that the <b>sorted</b> <b>output</b> can be sequentially read off in linear time. We show that commonly used biology, chemistry, and physics laboratory techniques are model instances and provide implementations. Key words: sorting, natural computation, chromatography, gel electrophoresis, mass spectrometry, optics, physical implementation. ...|$|R
5000|$|Levcopoulos and Petersson {{describe}} {{a variation of}} heapsort based on a Cartesian tree that does not add an element to the heap until smaller values {{on both sides of}} it have already been included in the <b>sorted</b> <b>output.</b> As they show, this modification can allow the algorithm to sort more quickly than [...] for inputs that are already nearly sorted.|$|R
50|$|Counting sort is {{an integer}} sorting {{algorithm}} {{that uses the}} prefix sum of a histogram of key frequencies to calculate the position of each key in the <b>sorted</b> <b>output</b> array. It runs in linear time for integer keys that are smaller {{than the number of}} items, and is frequently used as part of radix sort, a fast algorithm for sorting integers that are less restricted in magnitude.|$|R
5000|$|A {{parallel}} {{version of}} the binary merge algorithm {{can serve as a}} building block of a parallel merge sort. The following pseudocode demonstrates this algorithm in a parallel divide-and-conquer style (adapted from Cormen et al.). It operates on two sorted arrays [...] and [...] and writes the <b>sorted</b> <b>output</b> to array [...] The notation [...] denotes the part of [...] from index [...] through , exclusive.|$|R
25|$|Business {{processes}} :are a set {{of activities}} within the organization to realize some <b>sort</b> of <b>output</b> or goal.|$|R
50|$|In {{computer}} science, {{merge sort}} (also commonly spelled mergesort) is an efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, {{which means that}} the implementation preserves the input order of equal elements in the <b>sorted</b> <b>output.</b> Mergesort is a divide and conquer algorithm that was invented by John von Neumann in 1945. A detailed description and analysis of bottom-up mergesort appeared in a report by Goldstine and Neumann as early as 1948.|$|R
5000|$|SORT is {{frequently}} executed as a stand-alone program, where it normally reads input from a file identified by DD [...] and writes <b>sorted</b> <b>output</b> to a file identified by DD [...] It is also often called from another application, via the COBOL [...] verb or calls to PL/I [...] routines, {{where it may}} use either [...] or [...] files or be passed records to be sorted by the caller and/or pass sorted records back to the caller one at a time.|$|R
40|$|Sortingisafundamentaloperationindataprocessing. While {{the problem}} of sorting flat data records has been {{extensively}} studied, {{there is very little}} work on sorting hierarchical data such as XML documents. Existing hierarchy-aware sorting approaches for hierarchical dataare based on creating sorted subtrees as initial sorted runs and merging sorted subtrees to create the <b>sorted</b> <b>output</b> using either explicit pointers or absolute node key comparisons for merging subtrees. In this paper, we propose SliceSort, a novel, level-wise sortingtechniqueforhierarchical datathatavoidsthedrawbacks of subtree-based sorting techniques. Our experimental performance evaluation shows that SliceSort outperforms the state-of-art approach, HErMeS, by up to a factor of 27 %...|$|R
50|$|Merge algorithms, {{computing}} the merge step of a merge sort. The {{input to}} these algorithms {{consists of two}} sorted arrays of numbers; the desired output is {{the same set of}} numbers in a single sorted array. If one concatenates the two sorted arrays, the first in ascending order and the second in descending order, then the predecessor of each value in the output is either its closest previous smaller value or its closest following smaller value (whichever of the two is larger), and the position of each value in the <b>sorted</b> <b>output</b> array may easily be calculated from the positions of these two nearest smaller values.|$|R
40|$|A new modular {{architecture}} is presented for {{the realization of}} high-speed binary sorting engines, based on e-cient rank ordering. Capacitive Threshold Logic (CTL) gates are utilized {{for the implementation of}} the multi-input programmable majority (voting) functions required in the architecture. The overall complexity of the proposed bitserial architecture increases linearly with the number of input vectors to be sorted (window size = m) and with the bit-length of the input vectors (word size = n), and the sorter architecture can be easily expanded to accommodate large vector sets. It is demonstrated that the proposed sorting engine is capable of producing a fully <b>sorted</b> <b>output</b> vector set in (m+n- 1) clock cycles, i. e., in linear time. ...|$|R
5000|$|... sort : The [...] command sorts a BAM {{file based}} on its {{position}} in the reference, as determined by its alignment. The element + coordinate in the reference that the first matched base in the read aligns to is used as the key to order it by. verify. The <b>sorted</b> <b>output</b> is dumped to a new file by default, although it can be directed to stdout (using the -o option). As sorting is memory intensive and BAM files can be large, this command supports a sectioning mode (with the -m options) to use at most a given amount of memory and generate multiple output file. These files can then be merged to produce a complete sorted BAM file [...]|$|R
50|$|Nevertheless, some {{languages}} are particularly well-adapted to produce this <b>sort</b> of self-similar <b>output,</b> {{especially those that}} support multiple options for avoiding delimiter collision.|$|R
40|$|We {{consider}} the often-studied problem of sorting, for a parallel computer. Given an input array distributed evenly over p processors, {{the task is}} to compute the <b>sorted</b> <b>output</b> array, also distributed over the p processors. Many existing algorithms take the approach of approximately load-balancing the output, leaving each processor with Θ(n/p) elements. However, in many cases, approximate load-balancing leads to inefficiencies in both the sorting itself and in further uses of the data after sorting. We provide a deterministic parallel sorting algorithm that uses parallel selection to produce any output distribution exactly, particularly one that is perfectly load-balanced. Furthermore, when using a comparison sort, this algorithm is 1 -optimal in both computation and communication. We provide an empirical study that illustrates the efficiency of exact data splitting, and shows an improvement over two sample sort algorithms. Singapore-MIT Alliance (SMA...|$|R
40|$|Abstract — We {{consider}} the often-studied problem of sorting, for a parallel computer. Given an input array distributed evenly over p processors, {{the task is}} to compute the <b>sorted</b> <b>output</b> array, also distributed over the p processors. Many existing algorithms take the approach of approximately load-balancing the output, leaving each processor with Θ (n) elements. However, in many p cases, approximate load-balancing leads to inefficiencies in both the sorting itself and in further uses of the data after sorting. We provide a deterministic parallel sorting algorithm that uses parallel selection to produce any output distribution exactly, particularly one that is perfectly load-balanced. Furthermore, when using a comparison sort, this algorithm is 1 -optimal in both computation and communication. We provide an empirical study that illustrates the efficiency of exact data splitting, and shows an improvement over two sample sort algorithms. Index Terms — Parallel sorting, distributed-memory algorithms, High-Performance Computing...|$|R
2500|$|The [...] "I'm feeling lucky" [...] search result {{returns the}} highest scoring {{alignment}} {{for the first}} query sequence based on the <b>output</b> <b>sort</b> option selected by the user.|$|R
40|$|A memory-adaptive sort {{is able to}} {{dynamically}} {{change the}} amount of memory used during sorting. The method described in this paper adjusts memory usage according to input size and memory requirements of other sorts running in a database system. It saves memory space for small sorts, reduces sort time for large sorts, and balances memory usage among concurrent sorts. Overall system performance is improved when several sorts are running concurrently. 1 Introduction Sorting is a time consuming and frequently used operation in database systems. It is used not only for producing <b>sorted</b> <b>output,</b> but also in many sort-based algorithms for query processing, such as merge-join, grouping, and duplicate elimination[2]. Sorting is both memory intensive and CPU intensive. The amount of available memory may affect sort time dramatically. In a database system the free memory space available to the system changes continuously. A large sort may experience fluctuations of available memory space during th [...] ...|$|R
2500|$|In {{addition}} to natural advantages {{in the production}} of one <b>sort</b> of <b>output</b> over another (wine vs. rice, say) the infrastructure, education, culture, and [...] "know-how" [...] of countries differ so dramatically that the idea of identical technologies is a theoretical notion. Ohlin said that the H–O model was a long-run model, and that the conditions of industrial production are [...] "everywhere the same" [...] in the long run.|$|R
500|$|Pigeonhole sort or {{counting}} sort {{can both}} sort [...] data items having {{keys in the}} range from [...] to [...] in time [...] In pigeonhole sort (often called bucket sort), pointers to the data items are distributed to a table of buckets, represented as collection data types such as linked lists, using the keys as indices into the table. Then, all of the buckets are concatenated together to form the <b>output</b> list. Counting <b>sort</b> uses a table of counters in place of a table of buckets, to determine the number of items with each key. Then, a prefix sum computation is used to determine the range of positions in the <b>sorted</b> <b>output</b> at which the values with each key should be placed. Finally, in a second pass over the input, each item is moved to its key's position in the output array. Both algorithms involve only simple loops over the input data (taking time [...] ) and over the set of possible keys (taking time [...] ), giving their [...] overall time bound.|$|R
40|$|The CMOS {{realization}} {{of a new}} scalable, modular sorting architecture is presented. The high-performance architecture is based on rank ordering, and on efficient implementation of multi-input majority (voting) functions. The overall complexity of the proposed bit-serial architecture increases linearly {{with the number of}} input vectors to be sorted (window size = m) and with the bit-length of the input vectors (word size = n), and the sorter architecture can be easily expanded to accommodate large vector sets. It is shown that the proposed sorting engine is capable of producing a fully <b>sorted</b> <b>output</b> vector set in (m+n- 1) clock cycles, i. e., in linear time. To demonstrate the concept, a full-custom sorting engine is realized to process 63 input vectors of 16 -bits (m = 63, n = 16), using conventional 0. 35 µm CMOS technology. The resulting sorter chip occupies a silicon area of 13 sqmm, operates at a clock frequency of 200 MHz, and it is capable of completing the sorting operation of 63 16 -bit vectors within 78 clock cycles. 1...|$|R
5000|$|Pigeonhole sort or {{counting}} sort {{can both}} sort [...] data items having {{keys in the}} range from [...] to [...] in time [...] In pigeonhole sort (often called bucket sort), pointers to the data items are distributed to a table of buckets, represented as collection data types such as linked lists, using the keys as indices into the table. Then, all of the buckets are concatenated together to form the <b>output</b> list. Counting <b>sort</b> uses a table of counters in place of a table of buckets, to determine the number of items with each key. Then, a prefix sum computation is used to determine the range of positions in the <b>sorted</b> <b>output</b> at which the values with each key should be placed. Finally, in a second pass over the input, each item is moved to its key's position in the output array. Both algorithms involve only simple loops over the input data (taking time [...] ) and over the set of possible keys (taking time [...] ), giving their [...] overall time bound.|$|R
40|$|Spatial join is an {{important}} yet costly operation in spatial databases. In order {{to speed up the}} execution of a spatial join, the input tables are often indexed based on their spatial attributes. The quadtree index structure is a well-known index for organizing spatial database objects. It has been implemented in several database management systems, e. g., in Oracle Spatial and in PostgreSQL (via SP-GiST). Queries typically involve multiple pipelined spatial join operators that fit together in a query evaluation plan. In order to extend the applicability of these spatial joins, they are optimized so that upon receiving sorted input, they produce <b>sorted</b> <b>output</b> for the spatial join operators in the upperlevels of the query evaluation pipeline. This paper investigates the use of quadtree-based spatial join algorithms and how they can be adapted to answer queries that involve multiple pipelined spatial joins in a query evaluation plan. The paper investigates several adaptations to pipelined spatial join algorithms and their performance for the cases when both input tables are indexed, when only one of the tables is indexed while the second table is sorted, and when both tables are sorted but are not indexed...|$|R
5000|$|... sorts {{the lines}} of infile in {{lexicographical}} order, writes unique lines prefixed {{by the number of}} occurrences, <b>sorts</b> the resultant <b>output</b> numerically, and places the final output in outfile. This type of construction is used very commonly in shell scripts and batch files.|$|R
40|$|Abstract—This paper {{addresses}} {{the problem of}} evaluating ranked top-k queries with expensive predicates. As major DBMSs now all support expensive user-defined predicates for Boolean queries, we believe such support for ranked queries {{will be even more}} important: First, ranked queries often need to model user-specific concepts of preference, relevance, or similarity, which call for dynamic user-defined functions. Second, middleware systems must incorporate external predicates for integrating autonomous sources typically accessible only by per-object queries. Third, ranked queries often accompany Boolean ranking conditions, which may turn predicates into expensive ones, as the index structure on the predicate built on the base table may be no longer effective in retrieving the filtered objects in order. Fourth, fuzzy joins are inherently expensive, as they are essentially user-defined operations that dynamically associate multiple relations. These predicates, being dynamically defined or externally accessed, cannot rely on index mechanisms to provide zero-time <b>sorted</b> <b>output,</b> and must instead require per-object probe to evaluate. To enable probe minimization, we develop the problem as cost-based optimization of searching over potential probe schedules. In particular, we decouple probe scheduling into object and predicate scheduling problems and develop an analytical object scheduling optimization and a dynamic predicate scheduling optimization, which combined together form a cost-effective probe schedule. Index Terms—Database query processing, distributed information systems, database systems. ...|$|R
5000|$|Classification and {{clustering}} {{are examples}} of the more general problem of pattern recognition, which is the assignment of some <b>sort</b> of <b>output</b> value to a given input value. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence; etc.|$|R
40|$|Hardware {{vendors are}} {{improving}} their (database) servers in two main aspects: (1) increasing main memory capacities of several TB per server, mostly with non-uniform memory access (NUMA) among sockets, and (2) massively parallel multi-core pro-cessing. While {{there has been}} research on the parallelization of database operations, still many algorithmic and control techniques in current database technology were de-vised for disk-based systems where I/O dominated the performance. Furthermore, NUMA has only recently caught the community’s attention. In [AKN 12], we ana-lyzed the challenges that modern hardware poses to database algorithms on a 32 -core machine with 1 TB of main memory (four NUMA partitions) and derived three rather simple rules for NUMA-affine scalable multi-core parallelization. Based on our find-ings, we developed MPSM, a suite of massively parallel sort-merge join algorithms, and showed its competitive performance on large main memory databases with bil-lions of objects. In this paper, we go one step further and investigate the effectiveness of MPSM for non-inner join variants and complex query plans. We show that for non-inner join variants, MPSM incurs no extra overhead. Further, we point out ways of exploiting the roughly <b>sorted</b> <b>output</b> of MPSM in subsequent joins. In our evaluation, we compare these ideas to the basic execution of sequential MPSM joins and find that the original MPSM performs very well in complex query plans. ...|$|R
40|$|This paper {{addresses}} {{the problem of}} evaluating ranked top- queries with expensive predicates. As major DBMSs now all support expensive user-defined predicates for Boolean queries, we believe such support for ranked queries {{will be even more}} important: First, ranked queries often need to model user-specific concepts of preference, relevance, or similarity, which call for dynamic user-defined functions. Second, middleware systems must incorporate external predicates for integrating autonomous sources typically accessible only by per-object queries. Third, fuzzy joins are inherently expensive, as they are essentially user-defined operations that dynamically associate multiple relations. These predicates, being dynamically defined or externally accessed, cannot rely on index mechanisms to provide zero-time <b>sorted</b> <b>output,</b> and must instead require per-object probe to evaluate. The current standard sort-merge framework for ranked queries cannot efficiently handle such predicates because it must completely probe all objects, before sorting and merging them to produce top- answers. To minimize expensive probes, we thus develop the formal principle of "necessary probes," which determines if a probe is absolutely required. We then propose Algorithm MPro which, by implementing the principle, is provably optimal with minimal probe cost. Further, we show that MPro can scale well and can be easily parallelized. Our experiments using both a real-estate benchmark database and synthetic datasets show that MPro enables significant probe reduction, which can be orders of magnitude faster than the standard scheme using complete probing...|$|R
