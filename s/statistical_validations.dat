19|611|Public
40|$|Semi-Markov {{processes}} {{constructed by}} means of a time-change, based on the inverse of a subordinator, are considered. The tail of the distribution of the exit time from an arbitrary open set is investigated and its asymptotic behaviour is derived. Our analysis also includes the case of time-dependent open sets. An application about the use of these kind of processes and the obtained results in neuronal modeling is also discussed and some simulation results are given as <b>statistical</b> <b>validations</b> of theoretical results and as examples of stochastic modeling...|$|E
40|$|Background: Archaeology reports millenary {{cultural}} {{contacts between}} Peruvian Coast-Andes and the Amazon Yunga, a rainforest transitional region between Andes and Lower Amazonia. To clarify {{the relationships between}} cultural and biological evolution of these populations, in particular between Amazon Yungas and Andeans, we used DNA-sequence data, a model-based Bayesian approach and several <b>statistical</b> <b>validations</b> to infer a set of demographic parameters. Results: We found that the genetic diversity of the Shimaa (an Amazon Yunga population) is a subset of that of Quechuas from Central-Andes. Using the Isolation-with-Migration population genetics model, we inferred that the Shimaa ancestors were a small subgroup that split less than 5300 years ago (after the development of complex societies) from an ancestral Andean population. After the split, the most plausible scenario compatible with our results is that the ancestors of Shimaas moved toward the Peruvian Amazon Yunga and incorporated the culture and language {{of some of their}} neighbors, but not a substantial amount of their genes. We validated our results using Approximate Bayesian Computations, posterior predictive tests and the analysis of pseudo-observed datasets. Conclusions: We presented a case study in which model-based Bayesian approaches, combined with necessary <b>statistical</b> <b>validations,</b> shed light into the prehistoric demographic relationship between Andeans and a population from the Amazon Yunga. Our results offer a testable model for the peopling of this large transitional environmental region between the Andes and the Lower Amazonia. However, studies on larger samples and involving more populations of these regions are necessary to confirm if the predominant Andean biological origin of the Shimaas is the rule, and not the exception...|$|E
40|$|Integrated TEC {{measurements}} {{derived from}} ground- and space-based GPS observations {{are used to}} image the ionospheric electron density distribution through threedimensional tomographic reconstruction. Here, the reconstruction procedure is presented, that has been applied for ionospheric mapping especially during perturbed geomagnetic conditions. The monitoring technique including tomographic algorithm and external constraints is described and discussed in detail. The procedure {{is based on an}} iterative MART algorithm and is initialised by a combined IRI/GCPM model. Input data are GPS TEC measured from ground-based networks {{as well as from the}} scientific Low Earth Orbiting (LEO) satellite CHAMP. The problem of spaceborne GPS instrumental biases is addressed, and the approach applied for bias estimation by using calibrated zenith antenna TEC data is presented. <b>Statistical</b> <b>validations</b> of tomographic results with independent data are shown...|$|E
30|$|<b>Statistical</b> <b>validation</b> of {{research}} results.|$|R
50|$|Mayu {{performs}} <b>statistical</b> <b>validation</b> {{of protein}} identification by estimating an False Discovery Rate (FDR) on protein level.|$|R
5000|$|Spencer, L. M. (2004). Competency Model <b>Statistical</b> <b>Validation</b> and Business Case Development, HR Technologies White Paper http://www.hrcompass.com/validation.html ...|$|R
40|$|We {{previously}} reported a computational approach to infer alternative splicing patterns from Mus musculus full-length cDNA clones and microarray data. Although we predicted {{a large number}} of unreported splice variants, the general mechanisms regulating alternative splicing were yet unknown. In the present study, we compared alternative exons and constitutive exons in terms of splice-site strength and frequency of potential regulatory sequences. These regulatory features were further compared among five different species: Homo sapiens, M. musculus, Arabidopsis thaliana, Oryza sativa, and Drosophila melanogaster. Solid <b>statistical</b> <b>validations</b> of our comparative analyses indicated that alternative exons have (1) weaker splice sites and (2) more potential regulatory sequences than constitutive exons. Based on our observations, we propose a combinatorial model of alternative splicing mechanisms, which suggests that alternative exons contain weak splice sites regulated alternatively by potential regulatory sequences on the exons...|$|E
40|$|This paper reviews how {{empirical}} research on User Experi-ence (UX) is conducted. It integrates products, dimensions of experience, and methodologies across a systematically se-lected sample of 51 publications from 2005 - 2009, reporting {{a total of}} 66 empirical studies. Results show {{a shift in the}} products and use contexts that are studied, from work to-wards leisure, from controlled tasks towards open use situa-tions, and from desktop computing towards consumer prod-ucts and art. Context of use and anticipated use, often named key factors of UX, are rarely researched. Emotions, enjoy-ment and aesthetics are the most frequently assessed dimen-sions. The methodologies used are mostly qualitative, and known from traditional usability studies, though construc-tive methods with unclear validity are being developed and used. Many studies use self-developed questionnaires with-out providing items or <b>statistical</b> <b>validations.</b> We discuss un-derexplored research questions and potential improvements of UX research...|$|E
40|$|Chiral {{effective}} field theory (EFT) predictions are necessarily truncated at some order in the EFT expansion, which induces an error that must be quantified for robust statistical comparisons to experiment. In previous work, a Bayesian model for truncation errors of perturbative expansions was adapted to EFTs. The model yields posterior probability distribution functions (pdfs) for these errors based on expectations of naturalness encoded in Bayesian priors and the observed order-by-order convergence pattern of the EFT. A first application was made to chiral EFT for neutron-proton scattering using the semi-local potentials of Epelbaum, Krebs, and Meißner (EKM). Here we extend this application to consider a larger set of regulator parameters, energies, and observables as a general example of a statistical approach to truncation errors. The Bayesian approach allows for <b>statistical</b> <b>validations</b> of the assumptions and enables the calculation of posterior pdfs for the EFT breakdown scale. The statistical model is validated for EKM potentials whose convergence behavior is not distorted by regulator artifacts. For these cases, the posterior for the breakdown scale is consistent with EKM assumptions. Comment: 24 pages. Corresponds to published versio...|$|E
40|$|For a {{class of}} {{stationary}} Gaussian processes and for large correlation times, the asymptotic behavior of the upcrossing first passage time probability densities is investigated. Parallel simulations of sample paths of special stationary Gaussian processes for large correlations times provide a <b>statistical</b> <b>validation</b> of the theoretical results...|$|R
30|$|To {{calculate}} {{soil productivity}} for the cropland land use type, the inherent soil productivity and the fertilizer response scores were aggregated, assigning a mechanical {{weight to the}} fertilizer response indices. This weight resulted in the best model fit {{at the end of}} an iterative <b>statistical</b> <b>validation</b> process (see below for details of the <b>statistical</b> <b>validation).</b> Spatially weighted averages of productivity scores were calculated for the SMUs of the SGDBE. In order to avoid the bias originating from the evaluation of non-cropland soils, only those STUs were considered which had cultivated land as the primary or secondary land use type in the SGDBE. Finally, similarly to the concluding step of the grassland productivity evaluation, correction coefficients were applied to evaluate the effect of the topography (slope and aspect) on the productivity of cropland soils.|$|R
30|$|A <b>statistical</b> <b>validation</b> of the {{numerical}} results {{is not yet}} possible due to the relatively small sample of measurements at hand. But the described approach has been implemented in a software tool at the Center of Model-based Medical Decision Support, University Aalborg that is recently applied and tested in a large clinical study.|$|R
40|$|Urbanization is an {{important}} issue concerning diverse scientific and policy communities. Computationalmodels quantifying locations and quantities of urban growth offer numerous environmental and socioeconomic benefits. Traditional urban growthmodels are based on a single-algorithm fitting procedure and thus restricted on their ability to capture spatial heterogeneity. Accordingly, a GIS-based modeling framework titled multi-network urba-nization (MuNU) model is developed that integrates multiple neural networks. The MuNU model enables a filtering approach where input data patterns are automatically reallocated into appropriate neural networks with targeted accuracies. We hypothesize that observa-tions classified by individual neural networks share greater homogeneity, and thus model-ing accuracywill increasewith the integration ofmultiple targeted algorithms. Land use and land cover data sets of two time snapshots (1977 and 1997) covering the Denver Metropolitan Area are used for model training and validation. Compared to a single-step algorithm – either a stepwise logistic regression or a single neural network – several improvements are evident in the visual output of the MuNU model. <b>Statistical</b> <b>validations</b> further quantify the superiority of theMuNUmodel and support our hypothesis of effective incorporation of spatial heterogeneity...|$|E
40|$|Abstract: The {{impacts of}} high {{technology}} industries {{have been growing}} increasingly to technological innovations and global economic developments, while the concerns in sustainability are calling for facilitating green materials and cleaner production in the industrial value chains. Today’s manufacturing companies are not striving for individual capacities but for the effective working with green supply chains. However, in addition to environmental and social objectives, cost and economic feasibility {{has become one of}} the most critical success factors for improving supply chain management with green component procurement collaboration, especially for the electronics OEM (original equipment manufacturing) companies whose procurement costs often make up a very high proportion of final product prices. This paper presents a case study from the systems perspective by using System Dynamics simulation analysis and <b>statistical</b> <b>validations</b> with empirical data. Empirical data were collected from Taiwanese manufacturing chains—among the world’s largest manufacturing clusters of high technology components and products—and their global green suppliers to examine the benefits of green component procurement collaborations in terms of shared costs and improved shipping time performance. Two different supply chain collaboration models, from multi-layer ceramic capacitor (MLCC) and universal serial bus 3. 0 (USB 3. 0) cable procurements...|$|E
40|$|Microarray-based global gene {{expression}} proﬁling, {{with the use}} of sophisticated statistical algorithms is providing new insights into the pathogenesis of autoimmune diseases. We have applied a novel statistical technique for gene selection based on machine learning approaches to analyze microarray expression data gathered from patients with systemic lupus erythematosus (SLE) and primary antiphospholipid syndrome (PAPS), two autoimmune diseases of unknown genetic origin that share many common features. The methodology included a combination of three data discretization policies, a consensus gene selection method, and a multivariate correlation measurement. A set of 150 genes was found to discriminate SLE and PAPS patients from healthy individuals. <b>Statistical</b> <b>validations</b> demonstrate the relevance of this gene set from an univariate and multivariate perspective. Moreover, functional characterization of these genes identiﬁed an interferon-regulated gene signature, consistent with previous reports. It also revealed the existence of other regulatory pathways, including those regulated by PTEN, TNF, and BCL- 2, which are altered in SLE and PAPS. Remarkably, a signiﬁcant number of these genes carry E 2 F binding motifs in their promoters, projecting a role for E 2 F in the regulation of autoimmunity...|$|E
40|$|This paper {{presents}} {{an overview of}} <b>statistical</b> <b>validation</b> of complex computer models. Such models—used, for example, to simulate traffic in a street network, a car crash, the effect of increasing CO 2 on global warming, or the cost-effectiveness of a procedures in cancer screening—play important roles in scientific research and policy and decisio...|$|R
40|$|We {{present an}} {{integrated}} approach for efficient characterization of intrinsically disordered proteins. Batch cell-free expression, fast data acquisition, automated analysis, and <b>statistical</b> <b>validation</b> with data resampling have been combined for achieving cost-effective protein expression, and rapid automated backbone assignment. The new methodology is applied for characterization of five cytosolic domains from T- and B-cell receptors in solution...|$|R
40|$|The {{prediction}} of events is of substantial interest in many research areas. To evaluate {{the performance of}} prediction methods, the <b>statistical</b> <b>validation</b> of these methods is of utmost importance. Here, we compare an analytical validation method to numerical approaches {{that are based on}} Monte Carlo simulations. The comparison is performed in the field of the {{prediction of}} epileptic seizures. In contrast to the analytical validation method, we found that for numerical validation methods insufficient but realistic sample sizes can lead to invalid high rates of false positive conclusions. Hence we outline necessary preconditions for sound statistical tests on above chance predictions. Original Publication: Hinnerk Feldwisch-Drentrup, Andreas Schulze-Bonhage, Jens Timmer and Bjoern Schelter, <b>Statistical</b> <b>validation</b> of event predictors: A comparative study based on the field of seizure prediction, 2011, Physical Review E. Statistical, Nonlinear, and Soft Matter Physics, (83), 6, 066704. [URL] Copyright: American Physical Society [URL]...|$|R
40|$|For {{the last}} decade, {{there has been}} growing {{interest}} in the STEAM approach (essentially combining methods and practices in arts, {{humanities and social sciences}} into STEM teaching and research) to develop better research and education, and enable us to produce students who can work most effectively in the current and developing market-place. However, despite this interest, there seems to be little quantitative evidence of the true power of STEAM learning, especially describing how it compares and performs with respect to more established approaches. To address this, we present a comparative, quantitative study of two distinct approaches to teaching programming, one based on STEAM (with an open-ended inquiry-based approach), the other based on a more traditional, non-STEAM approach (where constrained problems are set and solved). Our key results evidence how students exhibit different styles of programming in different types of lessons and, crucially, that students who tend to exhibit more of the style of programming observed in our STEAM lessons also tend to achieve higher grades. We present our claims through a range of visualisations and <b>statistical</b> <b>validations</b> which clearly show the significance of the results, despite the small scale of the study. We believe that this work provides clear evidence for the advantages of STEAM over non-STEAM, and provides a strong theoretical and technological framework for future, larger studies...|$|E
40|$|ABSTRACT: The {{purpose of}} this work is to develop robust and interpretable {{quantitative}} structure”activity relationship (QSAR) models for assessing the aquatic toxicity of phenols using a combined set of descriptors encompassing the logP and recently developed variables (Monconn-Z variables). The used dataset consists of 250 chemicals with toxicity data to the ciliate Tetrahymena pyriformis. For each compound, a total of 197 physico-chemical descriptors including logP and Molconn-Z descriptors were calculated. Multiple linear regression (MLR) and Partial least squares (PLS) were used to obtain QSARs and the predictive performance of the proposed models were verified using external <b>statistical</b> <b>validations.</b> The results of stepwise-MLR analysis reveal that the AlogP, MlogP and ClogP models were not successful for the prediction of aquatic toxicity for all the compounds. And by using the logP (AlogP and MlogP) with Molconn-Z descriptors, the obtained QSARs were not successful enough nutill removal of some outliers. Two optimal QSARs were built with R 2 of 0. 71 and 0. 70 for the training sets and the external validation Q 2 of 0. 69 and 0. 68 respectively. All these selected descriptors in the best models account for the hydrophobic (AlogP, MlogP) and other electrotopological properties like SHCsatu, Scarboxylicacid, SHBa, gmax and nelem. PLS produces a good model using all the calculated descriptors with R 2 of 0. 78 and Q 2 of 0. 64, and hydrophobic and electrotopological descriptors show importance for the prediction of phenolic toxicity...|$|E
40|$|Background: Breast cancer {{resistant}} protein has {{an essential}} role in active transport of endogenous substances and xenobiotics across extracellular and intracellular membranes along with P-glycoprotein. It also {{plays a major role in}} multiple drug resistance and permeation of blood-brain barrier. Therefore, it is of great importance to derive theoretical models to predict the inhibition of both transporters in the process of drug discovery and development. Hitherto, very limited BCRP inhibition predictive models have been proposed as compared with its P-gp counterpart. Methodology/Principal Findings: An in silico BCRP inhibition model was developed in this study using the pharmacophore ensemble/support vector machine scheme to take into account the promiscuous nature of BCRP. The predictions by the PhE/SVM model were found to be in good agreement with the observed values for those molecules in the training set (n = 22, r 2 = 0. 82, q 2 CV = 0. 73, RMSE = 0. 40, s = 0. 24), test set (n = 97, q 2 = 0. 75 – 0. 89, RMSE = 0. 31, s = 0. 21), and outlier set (n = 16, q 2 = 0. 72 – 0. 91, RMSE = 0. 29, s = 0. 17). When subjected to a variety of <b>statistical</b> <b>validations,</b> the developed PhE/SVM model consistently met the most stringent criteria. A mock test by HIV protease inhibitors also asserted its predictivity. Conclusions/Significance: It was found that this accurate, fast, and robust PhE/SVM model can be employed to predict the BCRP inhibition of structurally diverse molecules that otherwise cannot be carried out by any other methods in a high...|$|E
5000|$|... early {{prevention}} of the coronary disease, objective {{and not only}} <b>statistical</b> risk <b>validation</b> ...|$|R
40|$|Abstract. The highly {{demanding}} {{study of}} meaning, intention, and communication including miscommunication, in human interaction seems {{to call for}} the development of powerful new approaches and in that context the astonishing raw power of modern computers may eventually be harnessed, given that adequate models, methods, algorithms and software be developed and made available. In this context, a proposed data structure, pattern definitions, algorithms, and a new <b>statistical</b> <b>validation</b> test are proposed. New additions are introduced to this theoretical/methodological system (called t-system) including special definitions of well known phenomena such as bursts and cyclical occurrence as well as of more novel concepts called “t-blocks”, ”t-metronomes ” and “ghost cycles”. A method is introduced to deal with the estimation of a priori probability (or statistical significance) of individual patterns without consideration of the arbitrary binary trees used for their detection and in this context “t-templates ” and their matching are introduced. <b>Statistical</b> <b>validation</b> through shuffling of data is compared with a suggested method called (random series) rotation (t-rotation) an...|$|R
40|$|Abstract:This {{article is}} to analyse and study the {{modeling}} and <b>statistical</b> <b>validation</b> and many other dynamic control technologies, {{trying to find a}} balance between the control development and optimize efficiency,and analyse the application principle of improving the layout of the port resource utilization. This paper puts forward the mode of transport based on the construction of the dry port and multimodal combination of land and sea cargo roll thrown. 1...|$|R
40|$|Breast cancer {{resistant}} protein has {{an essential}} role in active transport of endogenous substances and xenobiotics across extracellular and intracellular membranes along with P-glycoprotein. It also {{plays a major role in}} multiple drug resistance and permeation of blood-brain barrier. Therefore, it is of great importance to derive theoretical models to predict the inhibition of both transporters in the process of drug discovery and development. Hitherto, very limited BCRP inhibition predictive models have been proposed as compared with its P-gp counterpart. An in silico BCRP inhibition model was developed in this study using the pharmacophore ensemble/support vector machine scheme to take into account the promiscuous nature of BCRP. The predictions by the PhE/SVM model were found to be in good agreement with the observed values for those molecules in the training set (n=  22, r 2  = 0. 82, qCV 2 = 0. 73, RMSE=  0. 40, s =  0. 24), test set (n = 97, q 2 = 0. 75 - 0. 89, RMSE=  0. 31, s=  0. 21), and outlier set (n=  16, q 2  = 0. 72 - 0. 91, RMSE=  0. 29, s= 0. 17). When subjected to a variety of <b>statistical</b> <b>validations,</b> the developed PhE/SVM model consistently met the most stringent criteria. A mock test by HIV protease inhibitors also asserted its predictivity. It was found that this accurate, fast, and robust PhE/SVM model can be employed to predict the BCRP inhibition of structurally diverse molecules that otherwise cannot be carried out by any other methods in a high-throughput fashion to design therapeutic agents with insignificant drug toxicity and unfavorable drug-drug interactions mediated by BCRP to enhance clinical efficacy and/or circumvent drug resistance...|$|E
40|$|The {{impacts of}} high {{technology}} industries {{have been growing}} increasingly to technological innovations and global economic developments, while the concerns in sustainability are calling for facilitating green materials and cleaner production in the industrial value chains. Today’s manufacturing companies are not striving for individual capacities but for the effective working with green supply chains. However, in addition to environmental and social objectives, cost and economic feasibility {{has become one of}} the most critical success factors for improving supply chain management with green component procurement collaboration, especially for the electronics OEM (original equipment manufacturing) companies whose procurement costs often make up a very high proportion of final product prices. This paper presents a case study from the systems perspective by using System Dynamics simulation analysis and <b>statistical</b> <b>validations</b> with empirical data. Empirical data were collected from Taiwanese manufacturing chains—among the world’s largest manufacturing clusters of high technology components and products—and their global green suppliers to examine the benefits of green component procurement collaborations in terms of shared costs and improved shipping time performance. Two different supply chain collaboration models, from multi-layer ceramic capacitor (MLCC) and universal serial bus 3. 0 (USB 3. 0) cable procurements, were benchmarked and statistically validated. The results suggest that the practices of collaborative planning for procurement quantity and accurate fulfillment by suppliers are significantly related to cost effectiveness and shipping time efficiency. Although the price negotiation of upstream raw materials for the collaborative suppliers has no statistically significant benefit to the shipping time efficiency, the shared cost reduction of component procurement is significantly positive for supply chain collaboration among green manufacturers. Managerial implications toward sustainable supply chain management were also discussed...|$|E
40|$|Abstract Background Tight {{clustering}} arose recently from {{a desire}} to obtain tighter and potentially more informative clusters in gene expression studies. Scattered genes with relatively loose correlations should be excluded from the clusters. However, in the literature there is little work dedicated to this area of research. On the other hand, there has been extensive use of maximum likelihood techniques for model parameter estimation. By contrast, the minimum distance estimator has been largely ignored. Results In this paper we show the inherent robustness of the minimum distance estimator {{that makes it a}} powerful tool for parameter estimation in model-based time-course clustering. To apply minimum distance estimation, a partial mixture model that can naturally incorporate replicate information and allow scattered genes is formulated. We provide experimental results of simulated data fitting, where the minimum distance estimator demonstrates superior performance to the maximum likelihood estimator. Both biological and <b>statistical</b> <b>validations</b> are conducted on a simulated dataset and two real gene expression datasets. Our proposed partial regression clustering algorithm scores top in Gene Ontology driven evaluation, in comparison with four other popular clustering algorithms. Conclusion For the first time partial mixture model is successfully extended to time-course data analysis. The robustness of our partial regression clustering algorithm proves the suitability of the combination of both partial mixture model and minimum distance estimator in this field. We show that tight clustering not only is capable to generate more profound understanding of the dataset under study well in accordance to established biological knowledge, but also presents interesting new hypotheses during interpretation of clustering results. In particular, we provide biological evidences that scattered genes can be relevant and are interesting subjects for study, in contrast to prevailing opinion. </p...|$|E
50|$|Verification and {{validation}} (V&V) of simulation models is extremely important. Verification involves the model being debugged to ensure it works correctly, whereas validation {{ensures that the}} right model has been built. Face validation, sensitivity analysis, calibration and <b>statistical</b> <b>validation</b> have also been demonstrated. A discrete-event simulation framework approach for the validation of agent-based systems has been proposed. A comprehensive resource on empirical validation of agent-based models can be found here.|$|R
2500|$|What is {{expected}} of psychological measurements is [...] "sufficient" [...] accuracy and reliability, i.e. capability to express an indication or focus which clinicians can use as a “guideline” to rapidly and accurately deepen the aspects highlighted by the measurements and check them together with their patients. For this purpose, several <b>statistical</b> <b>validation</b> indexes of psychodiagnostic tests are provided: from standardization to various constructions of validity (internal, external, face, construct, convergent, content, discriminant, etc.).|$|R
50|$|Rapid {{antibody}} {{tests are}} qualitative immunoassays {{intended for use}} in point-of-care testing {{to aid in the}} diagnosis of HIV infection. These tests should be used in conjunction with the clinical status, history, and risk factors of the person being tested. The positive predictive value of Rapid Antibody Tests in low-risk populations has not been evaluated. These tests should be used in appropriate multi-test algorithms designed for <b>statistical</b> <b>validation</b> of rapid HIV test results.|$|R
40|$|A {{kinetic study}} of the {{photocatalytic}} degradation of 4 -nitrophenol (4 -NP) under UV–visible light (330 nm < k < 800 nm) has been performed via a rigorous chemical engineering approach over a Zn 2 + doped TiO 2 catalyst prepared through an environmentally friendly aqueous sol–gel process. The experiments have been performed at three temperatures to enable the global activation energy to be estimated. The influence of the illumination intensity has also been considered. The possibility of internal and external diffusion limitations has been studied and the results obtained demonstrated {{that there is no}} diffusional limitation during the photocatalytic degradation of the 4 -NP using the selected catalyst. Therefore, the apparent specific reaction rate measured corresponds to the actual reaction rate of the chemical reaction. Parameter adjustments show that the kinetic model that provides the best fit to the experimental data corresponds to a first order reaction. A sequence of elementary steps has been considered and a pseudo-steady state approach based upon the stationary state hypothesis for reaction intermediates has been applied to obtain a kinetic rate expression in agreement with the experimental data. The mean values of the reaction rate constant found at 283 K, 288 K and 293 K are respectively equal to k 1 = 0. 094 ± 0. 003 m 3 h- 1 kgcatalyst- 1; k 2 = 0. 119 ± 0. 004 m 3 h- 1 kgcatalyst- 1 and k 3 = 0. 150 ± 0. 023 m 3 h- 1 kg catalyst- 1 and the global activation energy of the degradation reaction was evaluated as 40 kJ mol- 1. A phenomenological kinetic mechanism is proposed to describe the reaction at a molecular scale. Finally, <b>statistical</b> <b>validations</b> and residuals analysis have been performed to confirm that the first order model is suitable to represent the 4 -NP photocatalytic degradation over time. Such studies are essential to design a reactor for water pollutant degradation on an industrial scale. Peer reviewe...|$|E
40|$|This thesis {{provides}} {{a review of}} the state-of-the-art in vision systems and methodologies and an introduction of important surface attributes and representations. Then three novel methods for the dynamic inspection of specular freeform surfaces are presented. These comprise two novel machine vision systems as well as a novel high-speed, multi-scale line tracing algorithm. Both of the novel systems employ a reciprocal deflectometric arrangement. The reflection of a laser line from a surface is monitored on a translucent screen. Here, a complex curve, known as the 'specular signature', is formed that contains all the information on the surface. Methods for extracting and interpreting this information are presented and incorporated into the two vision systems. Prototype demonstrators were designed and assembled to verify the presented methodologies. Extensive experimental validations of all three contributions are shown and the results are compared to ground truth data. <b>Statistical</b> <b>validations</b> of the systems are also presented. Also, the optical and angular resolutions as well as the limitations and the allowable ranges of surface characteristics for both systems, were calculated and presented. It is shown that they are applicable to a range of surface geometries and roughnesses that is comparable to those of existing techniques. The first of the two novel systems is designed for the robust and qualitative detection, classification and localisation of surface defects. It was validated using various real defects on specular freeform surfaces. It is shown that any discontinuity on a surface will be detected and can be classified as long as one criterion regarding the smallest radius of concave curvature on the surface is fulfilled. It is known that this criterion will be fulfilled for a very wide range of common surfaces. The second proposed vision system serves the purpose of a complete, quantitative reconstruction and digitalisation of moving, specular freeform surfaces. While the first system only requires the information from the specular signature, the second system also uses traditional and highly inaccurate surface height data, gathered through laser triangulation. These two data sets, computed from the diffuse as well as the specular reflection, are fused together to generate highly accurate surface bump (gradient) maps. Through the reverse engineering of several real specular specimens and the comparison to ground truth, it is shown that the standard deviation of the error of the height map reaches micrometer levels while that of the angular accuracy reaches levels below one degree. As a third original contribution to knowledge, a novel, high speed, multi-scale line extraction algorithm was developed. Intended for the rapid extraction of the specular signature from the screen images, it combines the processing speed of crude edge detectors with the versatility and accuracy of complex differential geometrical line extractors. It is also multi- scale, with best match scale space being chosen fully automatically. By combining the formerly separated steps of line point detection and line point linkage, the new algorithm is able to increase the processing speed of existing line extractors by up to 50 times. The time requirement is of the same order of magnitude as for crude edge detection algorithms such as Canny. The novel algorithm can also be implemented without the need for any global thresholds as it defines itself a variable local threshold, thereby increasing the sensitivity drastically. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
5000|$|What is {{expected}} of psychological measurements is [...] "sufficient" [...] accuracy and reliability, i.e. capability to express an indication or focus which clinicians can use as a “guideline” to rapidly and accurately deepen the aspects highlighted by the measurements and check them together with their patients. For this purpose, several <b>statistical</b> <b>validation</b> indexes of psychodiagnostic tests are provided: from standardization to various constructions of validity (internal, external, face, construct, convergent, content, discriminant, etc.).|$|R
40|$|This paper {{presents}} an empirical {{study on the}} visual method for cluster validation based on the Fastmap projection. The visual cluster validation method attempts to tackle two clustering problems in data mining: to verify partitions of data created by a clustering algorithm; and to identify genuine clusters from data partitions. They are achieved through projecting objects and clusters by Fastmap to the 2 D space and visually examining the results by humans. A Monte Carlo evaluation of the visual method was conducted. The validation results of the visual method were compared {{with the results of}} two internal <b>statistical</b> cluster <b>validation</b> indices, which shows that the visual method is in consistence with the <b>statistical</b> <b>validation</b> methods. This indicates that the visual cluster validation method is indeed effective and applicable to data mining applications. published_or_final_versio...|$|R
40|$|We {{introduce}} a generalized allometric model to express leaf dry weight {{in terms of}} leaf width and size in Zostera marina L. A formal justification of the derived model is presented. For the <b>statistical</b> <b>validation</b> we used data collected on two well defined strata {{over a period of}} one year. A comparison of the results using an independent data set was also performed. Applications of the model to estimate average leaf production are illustrated as well...|$|R
40|$|The primary aim of {{this paper}} is to present a {{solution}} to the issue of the <b>statistical</b> <b>validation</b> of route models. In addition, it introduces a body of theory taken from the broader field of route studies, isolates individual physical variables commonly used to predict route locations and quantifies them against the preserved hollow ways in the North Jazira Survey area, ending with a discussion of the complexity of human travel and the paramount importance of cultural variables...|$|R
