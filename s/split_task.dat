7|246|Public
40|$|In this paper, we {{investigate}} the Casacore Table Data System (CTDS) {{used in the}} casacore and CASA libraries, and methods to parallelize it. CTDS provides a storage manager plugin mechanism for third-party devel- opers to design and implement their own CTDS storage managers. Hav- ing this in mind, we looked into various storage backend techniques that can possibly enable parallel I/O for CTDS by implementing new storage managers. After carrying on benchmarks showing the excellent parallel I/O throughput of the Adaptive IO System (ADIOS), we implemented an ADIOS based parallel CTDS storage manager. We then applied the CASA MSTransform frequency <b>split</b> <b>task</b> to verify the ADIOS Storage Manager. We also ran a series of performance tests to examine the I/O throughput in a massively parallel scenario. Comment: 20 pages, journal article, 201...|$|E
40|$|Work-stealing is an {{efficient}} method to implement load balancing in fine-grained task parallelism. Typically, concurrent deques {{are used for}} this purpose. A disadvantage of many concurrent deques is that they require expensive memory fences for local deque operations. In this paper, we propose a new non-blocking work-stealing deque based on the <b>split</b> <b>task</b> queue. Our design uses a dynamic split point between the shared and the private portions of the deque, and only requires memory fences when shrinking the shared portion. We present Lace, an implementation of work-stealing based on this deque, with an interface similar to the work-stealing library Wool, and an evaluation of Lace based on several common benchmarks. We also implement a recent approach using private deques in Lace. We show that the split deque and the private deque in Lace have similar low overhead and high scalability as Wool...|$|E
40|$|Abstract—An EDF-based task-splitting {{scheme for}} {{scheduling}} multiprocessor systems is introduced in this paper. For m processors at most m − 1 tasks are split. The {{first part of}} a <b>split</b> <b>task</b> is constrained to have a deadline equal to its computation time. The {{second part of the}} task then has the maximum time available to complete its execution on a different processor. The advantage of this scheme is that no special run-time mechanisms are required and the overheads are kept to a minimum. Analysis is developed that allows the parameters of the split tasks to be derived. This analysis is integrated into the QPA algorithm for testing the schedulability of any task set executing on a single processor under EDF. Evaluation of the C=D scheme is provided via a comparison with a fully partitioned scheme and the theoretical maximum processor utilisation. I...|$|E
40|$|Abstract: Chinese event {{descriptive}} clause <b>splitting</b> is the <b>task</b> of <b>splitting</b> {{a complex}} Chinese sentence into several clauses. In this paper, {{we present a}} discriminative approach for Chinese event descriptive clause <b>splitting</b> <b>task.</b> By formulating the Chinese clause <b>splitting</b> <b>task</b> as a sequence labeling problem, we apply the structured SVMs model to Chinese clause splitting. Compared with other two baseline systems, our approach gives much better performance. ...|$|R
40|$|Distributed {{parallel}} execution systems {{speed up}} applications by <b>splitting</b> <b>tasks</b> into processes whose execution {{is assigned to}} di#erent receiving nodes in a high-bandwidth network. On the distributing side, a fundamental problem is grouping and scheduling such tasks such that each one involves su#cient computational cost {{when compared to the}} task creation and communication costs and other such practical overheads...|$|R
50|$|The {{average age}} Czechs marry vary {{slightly}} between gender groups. Females will normally marry {{in their early}} to mid-twenties, while males usually marry in their mid to late twenties. A wedding at a church is selected by some Czechs, and incorporation of traditional customs is expected. The husband is looked at {{as the head of}} the household, but couples commonly make decisions and <b>split</b> <b>tasks</b> together.|$|R
40|$|International audienceAn EDF-based task-splitting {{scheme for}} {{scheduling}} multiprocessor systems is introduced in this paper. For $m$ processors at most $m$- 1 tasks are split. The {{first part of}} a <b>split</b> <b>task</b> is constrained to have a deadline equal to its computation time. It therefore occupies its processor for a minimum interval. The {{second part of the}} task then has the maximum time available to complete its execution on a different processor. The advantage of this scheme is that no special run-time mechanisms are required and the overheads are kept to a minimum. Analysis is developed that allows the parameters of the split tasks to be derived. This analysis is integrated into the QPA algorithm for testing the schedulability of any task set executing on a single processor under EDF. Evaluation of the $C$=$D$ scheme is provided via a comparison with a fully partitioned scheme and the theoretical maximum processor utilisation...|$|E
40|$|Lipreading is {{the task}} of {{decoding}} text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016 a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To {{the best of our}} knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95. 2 % accuracy in sentence-level, overlapped speaker <b>split</b> <b>task,</b> outperforming experienced human lipreaders and the previous 86. 4 % word-level state-of-the-art accuracy (Gergen et al., 2016) ...|$|E
40|$|International audienceRecent {{technological}} advances {{have led to}} an increasing gap between memory and processor performance, since memory bandwidth is progressing at a much slower pace than processor bandwidth. Pre-fetching techniques are traditionally used to bridge this gap and achieve high processor utilization while tolerating high memory latencies. Following this trend, new computational models have been proposed to <b>split</b> <b>task</b> execution in two consecutive phases: a memory phase in which the required instructions and data are pre-fetched to local memory (M-phase), and an execution phase in which the task is executed with no memory contention (C-phase). Decoupling memory and execution phases not only simplifies the timing analysis, but also allows a more efficient (and predictable) pipelining of memory and execution phases through proper co-scheduling algorithms. This paper takes a further step towards the design of smart co-scheduling algorithms for sporadic real-time tasks complying with the memory-computation (M/C) model, by proposing a theoretical framework aimed at tightly characterizing the schedulability improvement obtainable with the adopted M/C task model on single-core systems. In particular, a critical instant is identified for M/C tasks scheduled with fixed priority and an exact response time analysis with pseudo-polynomial complexity is provided. Then, we investigate the problem of priority assignment for M/C tasks, showing that a necessary condition to achieve optimality is to allow different priorities for the two phases. Our experiments show that the proposed techniques provide a significant schedulability improvement with respect to classic execution models, placing an important building block towards the design of more efficient partitioned multi-core systems...|$|E
40|$|Because job {{deadlines}} are {{not used}} directly to determine job priorities under G-FL (unlike under G-EDF), the computations for tardiness bounds are more complex for G-FL than the computations described in Sec. 3. 1. Here we summarize the analysis from [1] as applied to <b>split</b> <b>tasks.</b> We first assume the absence of critical sections and then describe how our analysis is modified to account for critical sections. Each task is given a relative priority point Y split i = T split i S spli...|$|R
30|$|As {{an example}} of how {{indications}} and contraindications can play out in the stage classification of a particular student, consider the work of Student 4. Student 4 exhibited three, related weak contraindications of having constructed an aTNS by drawing in all the items in three fairly easy tasks, including Cupcake Task A and another task that required simple subtraction. However, these contraindications were outweighed by one strong, one moderate, and one weak indication of having constructed an aTNS. The strong indication was the correct solution of two out of three <b>splitting</b> <b>tasks</b> (the <b>splitting</b> operation is not available until after the construction of equi-partitioning; aTNS students can solve <b>splitting</b> <b>tasks</b> through the strategic use of simultaneous partitioning, see Ulrich, 2016 b for an example). The moderate indication was a correct solution to Cupcake Task B (involving operating on the results of a unit coordination) without showing any work to support the unit coordination needed to enumerate the hidden cupcakes. The weak indication was a characteristic conflation when trying to name a fraction based on an incomplete partition that shows an awareness of multiple levels of units but an inability to keep track of them. Therefore, the stage formula for aTNS indicates that Student 4 should be classified as having constructed an aTNS.|$|R
50|$|Combine/Split task: If the {{programmer}} cannot {{estimate the}} task {{because it is}} too small or too big, the programmer will need to combine or <b>split</b> the <b>task.</b>|$|R
40|$|Graduation date: 2003 The {{research}} {{presented in}} this dissertation addresses the Multi-Mode Resource-Constrained Project Scheduling Problem (MMRCPSP) {{in the presence of}} resource unavailability. This research is motivated by the scheduling of engineering design tasks in automotive product development to minimize the project completion time, but addresses a general scheduling situation that is applicable in many contexts. The current body of MMRCPSP research typically assumes that, 1) individual resource units are available at all times when assigning tasks to resources and, 2) before assigning tasks to resources, there must be enough resource availability over time to complete the task without interruption. In many situations such as assigning engineering design tasks to designers, resources are not available over the entire project-planning horizon. In the case of engineering designers and other human resources, unavailability may be due to several reasons such as vacation, training, or being scheduled to do other tasks outside the project. In addition, when tasks are scheduled they are often split to accommodate unavailable resources and are not completed in one continuous time segment. The objectives of this research are to obtain insight into the types of project scheduling situations where <b>task</b> <b>splitting</b> may result in significant makespan improvements, and to develop a fast and effective scheduling heuristic for such situations. A designed computational experiment was used to gain insight into when <b>task</b> <b>splitting</b> may provide significant makespan improvements. Problem instances were randomly generated using a modification of a standard problem generator, and optimally solved with and without <b>task</b> <b>splitting</b> using a branch and bound algorithm. In total 3, 880 problem instances were solved with and without <b>task</b> <b>splitting.</b> Statistical analysis of the experimental data reveals that high resource utilization is the most important factor affecting the improvements obtained by <b>task</b> <b>splitting.</b> The analysis also shows that splitting is more helpful when resource unavailability occurs in multiple periods of short duration versus fewer periods of long duration. Another conclusion from the analysis indicates that the project precedence structure and the number (not amount) of resources used by tasks do not significantly affect the improvements due to <b>task</b> <b>splitting.</b> Using the insights from the computational testing, a new heuristic is developed that can be applied to large problems. The heuristic is an implementation of a simple priority rule-based heuristic with a new parameter used to control the number of <b>task</b> <b>splits.</b> It is desirable to obtain the majority of <b>task</b> <b>splitting</b> benefits with the smallest number of <b>split</b> <b>tasks.</b> Computational experiments are conducted to evaluate its performance against known optimal solutions for small sized problems. A deterministic version of the heuristic found optimal solutions for 33...|$|R
30|$|Organizing and planning: {{parents were}} shown {{how to set}} up a daily routine for a child, {{schedule}} commitments (e.g., homework), formulate realistic targets, and <b>split</b> larger <b>tasks</b> into small steps.|$|R
50|$|On 2 September 1998, Caldera, Inc. {{announced}} {{the creation of}} two Utah-based wholly owned subsidiaries, Caldera Systems, Inc. and Caldera Thin Clients, Inc., in order to <b>split</b> up <b>tasks</b> and directions.|$|R
50|$|Distributed R {{is an open}} source, {{high-performance}} {{platform for}} the R language. It <b>splits</b> <b>tasks</b> between multiple processing nodes to reduce execution time and analyze large data sets. Distributed R enhances R by adding distributed data structures, parallelism primitives to run functions on distributed data, a task scheduler, and multiple data loaders. It is mostly used to implement distributed versions of machine learning tasks. Distributed R is written in C++ and R, and retains the familiar {{look and feel of}} R. As of February 2015, Hewlett-Packard (HP) provides enterprise support for Distributed R with proprietary additions such as a fast data loader from the Vertica database.|$|R
40|$|Most {{statistical}} {{machine translation}} systems use phrase-to-phrase translations to capture local context information, leading to better lexical choice and more reliable local reordering. The quality of the phrase alignment {{is crucial to the}} quality of the resulting translations. Here, we propose a new phrase alignment method, not based on the Viterbi path of word alignment models. Phrase alignment is viewed as a sentence <b>splitting</b> <b>task.</b> For a given spitting of the source sentence (source phrase, left segment, right segment) find a splitting for the target sentence, which optimizes the overall sentence alignment probability. Experiments on different translation tasks show that this phrase alignment method leads to highly competitive translation results. ...|$|R
40|$|In {{this paper}} we present work to combine fixed {{priority}} scheduling with off-line schedule construction. It assumes a schedule has been constructed off-line {{for a set}} of tasks to meet their complex constraints. Our method takes the schedule and assigns the FPS attributes priority, offset, and period, to the tasks, such that their runtime FPS execution matches the off-line schedule. It does so by dividing the schedule into sequences and deriving priority inequalities, which are then resolved by integer linear programming. As FPS cannot reconstruct all schedules with periodic tasks, we have to <b>split</b> <b>tasks</b> into several instances to achieve con-sistent task attributes. Our algorithm constructs the mini-mum number of such artifact tasks. ...|$|R
40|$|Abstract—Recent {{theoretical}} {{studies have}} shown that partitioning-based scheduling has better real-time performance than other scheduling paradigms like global scheduling on multi-cores. Especially, a class of partitioning-based scheduling algorithms (called semi-partitioned scheduling), which allow to split a small number of tasks among different cores, offer very high resource utilization. The major concern about the semi-partitioned scheduling is that due to the <b>task</b> <b>splitting,</b> some <b>tasks</b> will migrate from one core to another at run time, which incurs higher context switch overhead. So one would suspect whether the extra overhead caused by <b>task</b> <b>splitting</b> would counteract the theoretical performance gain of semi-partitioned scheduling. In this work, we implement a semi-partitioned scheduler in the Linux operating system, and run experiments on an Intel Core-i 7 4 -cores machine to measure the real overhead in both partitioned scheduling and semi-partitioned scheduling. Then we integrate the measured overhead into the state-of-theart partitioned scheduling and semi-partitioned scheduling algorithms, and conduct empirical comparisons of their realtime performance. Our results show that the extra overhead caused by <b>task</b> <b>splitting</b> in semi-partitioned scheduling is very low, and its effect on the system schedulability is very small. Semi-partitioned scheduling indeed outperforms partitioned scheduling in realistic systems. I...|$|R
40|$|The role of {{classroom}} interaction in second language acquisition (SLA) {{has been the}} subject of extensive research in recent years. The purpose of this study was to investigate the claimed superiority of communication tasks involving required information exchange (<b>split</b> information <b>tasks)</b> over tasks involving optional information exchange (shared information tasks) on the basis of how much negotiation of meaning learners produce when performing each type of task. The study also sought to analyze qualitative aspects of negotiation and to assess the theoretical claims made for negotiation in the light of the analysis. Subjects for the study included eight adult students from an English proficiency course who were assigned to two groups each containing four subjects. Over a period of six days the groups performed four communication usks of which two were <b>split</b> information <b>tasks</b> and two were shared information tasks. Full transcriptions of the task performances provided data for the study. Results confirmed that significantly more negotiation and repetition occurred in <b>split</b> information <b>tasks.</b> There was a small movement towards more even distribution of negotiation among interlocutors in <b>split</b> information <b>tasks</b> although the consistency of the differential contributions of specific interlocutors was noticeable across both types of task. The qualitative analysis distinguished six main types of negotiating questions in the data, some of which were shown to be more effective than others in generating comprehensible modifications to input or in extending the language output of the subjects. ln addition, negotiating questions dealt with five broad dimensions of meaning: the form of the message, grammatical and lexical meaning, content, opinions, and procedures. Of these five dimensions, only the first and second sometimes involved new or unfamiliar linguistic features in the input, thus fulfilling a requirement of the interaction hypothesis suggested by Ellis (1991). Significant post-test gains in the subjecrs' knowledge of vocabulary embedded in the tasks suggested that the negotiation of lexical meaning results in measurable learning of new words. Overall however, negotiation dealt more with non-target language features of output than with unfamiliar input and it was this which provided the more promising interactional route to language development. An investigation of other features of interaction revealed no significant difference in the amount of talk produced in split and shared information tasks. Talk was more evenly distributed among interlocutors in the <b>split</b> information <b>tasks</b> although inequalities persisted, with panicular interlocutors dominating interaction across all tasks. In the shared information tasks, turns and utterances were significantly longer, and conjunctions were used more frequently. Prepositions on the other hand were used more frequently in the <b>split</b> information <b>tasks.</b> These results suggest that the greater need to express links between propositions in the shared tasks results in discourse of grearer synracric complexiry. While the study supported the claim that <b>split</b> information <b>tasks</b> produced more negotiation than shared information tasks, a qualitative analysis of the negotiation, and of other aspects of interaction, suggested that more negotiation does not necessarily provide superior conditions for language development...|$|R
40|$|Recent {{theoretical}} {{studies have}} shown that partitioning-based scheduling has better real-time performance than other scheduling paradigms like global scheduling on multi-cores. Especially, a class of partitioning-based scheduling algorithms (called semi-partitioned scheduling), which allow to split a small number of tasks among different cores, offer very high resource utilization, and appear to be a promising solution for scheduling real-time systems on multi-cores. The major concern about the semi-partitioned scheduling is that due to the <b>task</b> <b>splitting,</b> some <b>tasks</b> will migrate from one core to another at run time, and might incur higher context switch overhead than partitioned scheduling. So one would suspect whether the extra overhead caused by <b>task</b> <b>splitting</b> would counteract the theoretical performance gain of semi-partitioned scheduling. In this work, we implement a semi-partitioned scheduler in the Linux operating system, and run experiments on a Intel Core-i 7 4 -cores machine to measure the real overhead in both partitioned scheduling and semi-partitioned scheduling. Then we integrate the obtained overhead into the state-of-the-art partitioned scheduling and semi-partitioned scheduling algorithms, and conduct empirical comparison of their real-time performance. Our results show that the extra overhead caused by <b>task</b> <b>splitting</b> in semi-partitioned scheduling is very low, and its effect on the system schedulability is very small. Semi-partitioned scheduling indeed outperforms partitioned scheduling in realistic systems...|$|R
40|$|Abstract. This paper {{presents}} a compilation technique that performs automatic parallelization of canonical loops. Canonical loops are a pattern observed in many well known algorithms, such as frequent itemsets, K-means and K nearest neighbors. Automatic parallelization allows application developers {{to focus on}} the algorithmic details of the problem they are solving, leaving for the compiler the task of generating correct and efficient parallel code. Our method <b>splits</b> <b>tasks</b> and data among stream processing elements and uses a novel technique based on labeled-streams to minimize the communication between filters. Experiments performed on a cluster of 36 computers indicate that, for the three algorithms mentioned above, our method produces code that scales linearly on the number of available processors. These experiments also show that the automatically generated code is competitive when compared to hand tuned programs. 1...|$|R
40|$|Abstract: In {{this paper}} we show how {{off-line}} scheduling and fixed priority scheduling (FPS) {{can be combined}} to get the advantages of both- the capability to cope with complex timing constraints while providing run-time flexibility. We present a method {{to take advantage of}} the flexibility provided by FPS while guaranteeing complex constraint satisfaction on periodic tasks. We provide mechanisms to include FPS servers to our previous work, to handle non-periodic events, while still fulfilling the original complex constraints on the periodic tasks. In some cases, e. g., when the complex constraints can not be expressed directly by FPS, we <b>split</b> <b>tasks</b> into instances (artifacts) to obtain a new task set with consistent FPS attributes. Our method is optimal in the sense that it keeps the number of artifacts minimized...|$|R
40|$|This paper {{introduces}} architectural {{and interaction}} patterns for integrating crowdsourced human contributions directly into user interfaces. We focus on writing and editing, complex endeavors that span many levels of conceptual and pragmatic activity. Authoring tools offer help with pragmatics, but for higher-level help, writers commonly turn to other people. We thus present Soylent, a word processing interface that enables writers {{to call on}} Mechanical Turk workers to shorten, proofread, and otherwise edit parts of their documents on demand. To improve worker quality, we introduce the Find-Fix-Verify crowd programming pattern, which <b>splits</b> <b>tasks</b> {{into a series of}} generation and review stages. Evaluation studies demonstrate the feasibility of crowdsourced editing and investigate questions of reliability, cost, wait time, and work time for edits. ACM Classification: H 5. 2 [Information interfaces an...|$|R
40|$|In this paper, we {{show that}} spatial joins are very {{suitable}} to be processed on a parallel hardware platform. The parallel system {{is equipped with a}} so-called shared virtual memory which is wellsuited for the design and implementation of parallel spatial join algorithms. We start with an algorithm that consists of three phases: task creation, task assignment and parallel task execution. In order to reduce CPU- and I/O-cost, the three phases are processed in a fashion that preserves spatial locality. Dynamic load balancing is achieved by <b>splitting</b> <b>tasks</b> into smaller ones and reassigning some of the smaller tasks to idle processors. In an experimental performance comparison, we identify {{the advantages and disadvantages of}} several variants of our algorithm. The most efficient one shows an almost optimal speed-up under the assumption that the number of disks is sufficiently large. ...|$|R
5000|$|Dynamic Epistemic Logic (DEL) is {{a logical}} {{framework}} for modeling epistemic situations involving several agents, and changes that occur to these situations {{as a result of}} incoming information or more generally incoming action. The methodology of DEL is such that it <b>splits</b> the <b>task</b> of representing the agents’ beliefs and knowledge into three parts: ...|$|R
40|$|A Dual Layer high {{dynamic range}} LCD display can be built by {{stacking}} two panels {{one on top of}} the other. In this way, the dynamic range is theoretically squared and the bit depth is also increased. Dedicated splitting algorithms are however needed to generate the two images which drive the panels, in order to minimize the parallax and reconstruction errors. In this paper we present an algorithm, based on variational techniques, which seeks the joint minimization of both errors. We propose a simplified visible difference metric that exploits some limitations of the human visual system and can be easily incorporated into an optimization algorithm. The image <b>splitting</b> <b>task</b> is formulated as a quadratic programming problem, which can be efficiently solved by means of appropriate numerical methods. Preliminary tests on medical images showed that the algorithm has good performances and appears robust with respect to the parameter adjustment...|$|R
40|$|Abstract: In this paper, we {{show that}} spatial joins are very {{suitable}} to be processed on a parallel hardware platform. The parallel system {{is equipped with a}} so-called shared virtual memory which is well-suited for the design and implementation of parallel spatial join algorithms. We start with an algorithm that consists of three phases: task creation, task assignment and parallel task execu-tion. In order to reduce CPU- and I/O-cost, the three phases are processed in a fashion that pre-serves spatial locality. Dynamic load balancing is achieved by <b>splitting</b> <b>tasks</b> into smaller ones and reassigning some of the smaller tasks to idle processors. In an experimental performance compar-ison, we identify {{the advantages and disadvantages of}} several variants of our algorithm. The most efficient one shows an almost optimal speed-up under the assumption that the number of disks is sufficiently large. Topics: spatial database systems, parallel database systems...|$|R
40|$|As {{the amount}} of data being {{exchanged}} over the network increases, algorithms originally implemented for running on a single machine have been re-designed {{to work in a}} distributed manner, with a processing platform that <b>splits</b> <b>tasks</b> among machines and cores. Brand new frameworks have emerged for the analysis of unbound streams of data, aiming at processing data and retrieving information nearly real-time by using clusters of machines. Node failure and recovery are crucial issues related to distributed systems, especially when using commodity hardware and when continuously processing data coming real- time into the system. In this paper we present the performance of the distributed stream-processing platform Blockmon, with the novel fault-tolerant mechanism that we implement on top, and compare it against Spark, the state-of-the art in terms of fault-tolerant stream-processing platform. Our experimental results suggest that Blockmon performs around two times faster than Spark, with a twenty times reduced memory footprint, showing the feasibility of using Blockmon on popular energy- efficient architectures such as the ARM ones...|$|R
40|$|The Quasi-Partitioning Scheduling {{algorithm}} optimally {{solves the}} problem of scheduling a feasible set of independent implicit-deadline sporadic tasks on a symmetric multiprocessor. It iteratively combines bin-packing solutions to determine a feasible task-to-processor allocation, <b>splitting</b> <b>task</b> loads as needed along the way so that the excess computation on one processor is assigned to a paired processor. Though different in formulation, QPS belongs {{in the same family}} of schedulers as RUN, which achieve optimality using a relaxed (partitioned) version of proportionate fairness. Unlike RUN, QPS departs from the dual schedule equivalence, thus yielding a simpler implementation with less use of global data structures. One might therefore expect that QPS should outperform RUN in the general case. Surprisingly instead, our implementation of QPS on LITMUS^RT invalidates this conjecture, showing that the QPS offline decisions may have an important influence on run-time performance. In this work, we present an extensive comparison between RUN and QPS, looking at both the offline and the online phases, to highlight their relative strengths and weaknesses...|$|R
40|$|The physics event {{reconstruction}} {{is one of}} {{the biggest}} challenges for the computing of the LHC experiments. Among the different tasks that computing systems of the CMS experiment performs, the reconstruction takes most of the available CPU resources. The reconstruction time of single collisions varies according to event complexity. Measurements were done in order to determine this correlation quantitatively, creating means to predict it based on the data-taking conditions of the input samples. Currently the data processing system <b>splits</b> <b>tasks</b> in groups with the same number of collisions and does not account for variations in the processing time. These variations can be large and can lead to a considerable increase in the time it takes for CMS workflows to finish. The goal of this study was to use estimates on processing time to more efficiently split the workflow into jobs. By considering the CPU time needed for each job the spread of the job-length distribution in a workflow is reduced...|$|R
40|$|Semi-partitioned {{scheduling}} {{has been}} the subject of interest compared to conventional global and partitioned scheduling algorithms for multiprocessors due to better utilization results. In Semi-partitioned scheduling most of tasks are assigned to fixed processors while a few number of <b>tasks</b> are <b>split</b> up and allocated to different processors. Various techniques have been proposed recently on different assigning protocols under semi-partitioned scheduling. Yet an appropriate synchronization mechanism for resource sharing in semi-partitioned scheduling have not been investigated. In this thesis we propose two methods for handling resource sharing under semi-partitioned scheduling on multiprocessor platforms. The main challenge is handling the resource requests of <b>tasks</b> that are <b>split</b> over multiple processors. The solutions include handling non-split tasks as well as <b>split</b> <b>tasks</b> over requests for shared resources in the system. In this thesis we investigate delays caused by blocking on resources. Furthermore, we perform the schedulability analysis for both algorithms. Finally we evaluate the performance of our proposed synchronization algorithms by means of experimental evaluations...|$|R
40|$|This paper {{presents}} PROVERB a text planner for argumentative texts. PROVERB's {{main feature}} {{is that it}} combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way. The former <b>splits</b> the <b>task</b> of presenting a particular proof into subtasks of presenting subproofs. The latter simulates how the next intermediate conclusion to be presented is chosen {{under the guidance of}} the local focus...|$|R
40|$|This paper {{presents}} PROVERB, a text planner for argumentative texts. PIOVERB's main fimtm'e is that {{it combines}} global hierarchical plannillg alld illlphmncd organization of text with respect to local derivation relations in a complementary way. The Ibmmr <b>splits</b> the <b>task</b> of presenting a particnlar proof' subtasks of presenting suhproof. q'hc latter sinmlal. cs how the next intermediate conclnsion to be presented is chosen {{under the guidance of}} the local focus...|$|R
50|$|The {{main purpose}} of {{employing}} loop-level parallelism is to search and <b>split</b> sequential <b>tasks</b> of a program and convert them into parallel tasks without any prior information about the algorithm. Parts of data that are recurring and consume significant amount of execution time are good candidates for loop-level parallelism. Some common applications of loop-level parallelism are found in mathematical analysis that uses multiple-dimension matrices which are iterated in nested loops.|$|R
30|$|Furthermore, the {{resource}} allocation task {{can be solved}} following a joint optimization model with a target utility such as the sum rate. However, due {{to the existence of}} a large solution space when considering all dimensions, simple reactive heuristics yielding sub-optimal, but feasible solutions, are required for practicality. One way to reduce the operational complexity and still reach a feasible solution is to <b>split</b> the <b>task</b> into different domains (i.e. space, time, frequency, power).|$|R
