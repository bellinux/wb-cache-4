500|95|Public
5000|$|Jeffrey C. Miecznikowski, Lori A. Shepherd, Albert Vexler. (2011). R Package ‘dbEmpLikeGOF’: Goodness-of-fit and two sample {{comparison}} tests using <b>sample</b> <b>entropy.</b> https://cran.r-project.org/web/packages/dbEmpLikeGOF/ http://sphhp.buffalo.edu/biostat/research/software/EmpLike.GOF/index.php ...|$|E
50|$|<b>Sample</b> <b>entropy</b> can be {{implemented}} easily in many different programming languages. An example written in Matlab can be found here. An example written for R can be found here.|$|E
50|$|Miecznikowski, J. C., Vexler, A., and Shepherd, L. (2013). dbEmpLikeGOF : An R {{package for}} nonparametric {{likelihood}} ratio tests for goodness-of-fit and two sample comparisons based on <b>sample</b> <b>entropy.</b> Journal of Statistical Software V. 54, Issue 3, 1-19.|$|E
40|$|Certain time domain, {{frequency}} domain and a nonlinear measure of {{heart rate variability}} are studied in women following a meditative practice called cyclic meditation. The nonlinear measure studied is the <b>sampling</b> <b>entropy.</b> We {{show that there is}} an increase in the <b>sampling</b> <b>entropy</b> in the meditative group as compared to the control group. The time domain measure called pNNx is shown to be useful in distinguishing between the meditative state and a normal resting state...|$|R
40|$|By {{introducing}} the <b>sample</b> relative <b>entropy</b> rate {{as a measure}} of the deviation between the arbitrary random fields and the Markov chain fields on Cayley trees, a class of small deviation theorems for the frequencies of state ordered couples is established. In the proof a new analytic technique {{in the study of the}} strong limit theorems for Markov chains is applied. Markov chain fields on Cayley trees <b>Sample</b> relative <b>entropy</b> rate Small deviation theorem Strong deviation theorem Strong limit theorem...|$|R
40|$|We {{demonstrate}} that the Renyi- 2 entropy provides a natural measure of information for any multimode Gaussian state of quantum harmonic systems, operationally linked to the phase-space Shannon <b>sampling</b> <b>entropy</b> of the Wigner distribution of the state. We prove that, in the Gaussian scenario, such an entropy satisfies the strong subadditivity inequality, a key requirement for quantum information theory. This allows us to define and analyze measures of Gaussian entanglement and more general quantum correlations based on such an entropy, which are shown to satisfy relevant properties such as monogamy. Comment: 6 + 5 pages, published in PRL. Typo in Eq. (1) correcte...|$|R
5000|$|Generally we {{take the}} value of [...] to be [...] {{and the value of}} [...] to be [...]Where std stands for {{standard}} deviation which should be taken over a very large dataset. For instance, the r value of 6 ms is appropriate for <b>sample</b> <b>entropy</b> calculations of heart rate intervals, since this corresponds to [...] for a very large population.|$|E
5000|$|With X1, ..., XN iid random variables, an N-dimensional [...] "box" [...] can be {{constructed}} with sides X1, ..., XN. Costa and Cover [...] show that the (Shannon) differential entropy h(X) {{is related to the}} volume of the typical set (having the <b>sample</b> <b>entropy</b> close to the true entropy), while the Fisher information is related to the surface of this typical set.|$|E
5000|$|Like {{approximate}} entropy (ApEn), <b>Sample</b> <b>entropy</b> (SampEn) is {{a measure}} of complexity. But it does not include self-similar patterns as ApEn does. For a given embedding dimension , tolerance [...] and number of data points , SampEn is the negative logarithm of the probability that if two sets of simultaneous data points of length [...] have distance [...] then two sets of simultaneous data points of length [...] also have distance [...] And we represent it by [...] (or by [...] including sampling time [...] ).|$|E
40|$|Contents 1 Introduction 2 2 Quantum Interference and the Construction of Nonclassical States 3 2. 1 Nonclassical Light, Quadratures and Phase [...] Sensitive Observables 3 2. 2 Phase [...] Space and Quasi [...] Probabilities................ 13 2. 3 Quantum Noise as a Stochastic Process.............. 15 2. 4 Marginal {{distributions}} {{and reconstruction}} of Wigner functions.. 18 2. 5 Joint Measurements of p, q in Phase Space............. 20 2. 6 Phase [...] Space <b>Sampling</b> <b>Entropies..................</b> 27 2. 7 Mutual Information and Entropic Uncertainty........... 28 2. 8 Building Nonclassical States by Quantum Interference...... 30 3 Dissipation Versus Coherent Evolution: Decoherence 36 4 Unravelling the Master Equation 37 5 Quantum Jumps 43 5. 1 Intrinsic or Extrinsic Jumps..................... 43 5. 2 Observation of Quantum Jumps............. ...|$|R
40|$|What is the {{appropriate}} amount of past information to use in forecasting univariate linear processes? This paper proposes a non-parametric measure useful for sample size selection involving the data's asymptotic pre-dictability (AP). It is shown that the AP of a strictly stationary process is decreasing in its entropy rate. The finite-sample analog of the AP measure is the <b>sample's</b> <b>entropy</b> normalized by its alphabet size. First, Monte Carlo simulations of stationary pdf's indicate that AP increases with sample size, suggesting that "more is better". Second, computing the AP of long series of daily stock index, foreign exchange and interest rate returns suggests that AP varies non-monotonically with sample size. Moreover, the evolution of AP is characterized by strong breaks and øuctuations over time. The computa-tional framework allows a concrete comparison of the informational content of different datasets and their relative predictability. ...|$|R
40|$|Description Decompose given {{hierarchical}} clustering tree into non-overlapping clusters in a semi-supervised way {{by using}} available patients follow-up information as guidance. Contains functions for snipping HC tree, various cluster quality evaluation criteria, assigning new patients {{to one of}} the two given HC trees, testing the significance of clusters with permutation argument and clusters visualization using <b>sample's</b> molecular <b>entropy...</b>|$|R
5000|$|Now assume {{we have a}} {{time-series data}} set of length [...] with a {{constant}} time interval [...] We define a template vector of length , such that [...] and the distance function [...] (i≠j) {{is to be the}} Chebyshev distance (but it could be any distance function, including Euclidean distance). We count the number of vector pairs in template vectors of length [...] and [...] having [...] and denote it by [...] and [...] respectively. We define the <b>sample</b> <b>entropy</b> to be ...|$|E
5000|$|Consider a density {{operator}} [...] {{with the}} following spectral decomposition:The weakly typical subspace {{is defined as the}} span of all vectors such thatthe <b>sample</b> <b>entropy</b> [...] of their classicallabel is close to the true entropy [...] of the distributionwhereThe projector [...] onto the typical subspace of [...] isdefined aswhere we have [...] "overloaded" [...] the symbol to refer also to the set of -typical sequences:The three important properties of the typical projector are as follows:where the first property holds for arbitrary [...] andsufficiently large [...]|$|E
5000|$|<b>Sample</b> <b>entropy</b> (SampEn) is a {{modification}} of approximate entropy (ApEn), used for assessing the complexity of physiological time-series signals, diagnosing diseased states. SampEn has two advantages over ApEn: data length independence and a relatively trouble-free implementation. Also, {{there is a small}} computational difference: In ApEn, the comparison between the template vector (see below) {{and the rest of the}} vectors also includes comparison with itself. This guarantees that probabilities [...] are never zero. Consequently, it is always possible to take a logarithm of probabilities. Because template comparisons with itself lower ApEn values, the signals are interpreted to be more regular than they actually are. These self-matches are not included in SampEn.|$|E
40|$|This is {{a survey}} of some new {{perspectives}} on transform coding image compression. We mentions some mathematical properties of <b>sampling,</b> <b>entropy,</b> and time-frequency localization of analysis-synthesis functions. Then we discuss several experiments in picture compression using wavelets, wavelet packets, local sines and cosines, and the adaptive "best-basis" method. We calculate the efficiency of several techniques to describe a fast transform from a library, {{with the aim of}} transforming each image into its own best-adapted coordinates for transmission with minimal overhead. Standard C algorithms for the local sine and cosine transforms are included in the appendix. 1 Introduction In this summary I will describe several experiments in picture compression using wavelets and the local cosine transform of Coifman and Meyer. I will also describe an adaptive wavelet transform coding method and a local cosine transform algorithm based on the idea of a "best basis," and provide Standard C algori [...] ...|$|R
40|$|Determining {{free energy}} {{surfaces}} along chosen reaction coordinates {{is a common}} and important task in simulating complex systems. Due {{to the complexity of}} energy landscapes and the existence of high barriers, one widely pursued objective to develop efficient simulation methods is to achieve uniform sampling among thermodynamic states of interest. In this work, we have demonstrated <b>sampling</b> <b>entropy</b> (SE) as an excellent indicator for uniform sampling {{as well as for the}} convergence of free energy simulations. By introducing SE and the concentration theorem into the biasing-potential-updating scheme, we have further improved the adaptivity, robustness, and applicability of our recently developed repository based adaptive umbrella sampling (RBAUS) approach [H. Zheng and Y. Zhang, J. Chem. Phys. 128, 204106 (2008) ]. Besides simulations of one dimensional free energy profiles for various systems, the generality and efficiency of this new RBAUS-SE approach have been further demonstrated by determining two dimensional free energy surfaces for the alanine dipeptide in gas phase as well as in water...|$|R
5000|$|The given {{sample is}} cooled down to (almost) {{absolute}} zero (for example by submerging the sample in liquid helium). At absolute zero temperature any sample {{is assumed to}} contain no entropy (see Third law of thermodynamics for further information). Then the following two active calorimeter types {{can be used to}} fill the <b>sample</b> with <b>entropy</b> until the desired temperature has been reached: (see also Thermodynamic databases for pure substances) ...|$|R
50|$|Given the {{complexity}} of the mechanisms regulating heart rate, {{it is reasonable to assume}} that applying HRV analysis based on methods of non-linear dynamics will yield valuable information. Although chaotic behavior has been assumed, more rigorous testing has shown that heart rate variability cannot be described as a low dimensional chaotic process. However, application of chaotic globals to HRV has been shown to predict diabetes status. The most commonly used non-linear method of analysing heart rate variability is the Poincaré plot. Each data point represents a pair of successive beats, the x-axis is the current RR interval, while the y-axis is the previous RR interval. HRV is quantified by fitting mathematically defined geometric shapes to the data. Other methods used are the correlation dimension, nonlinear predictability, pointwise correlation dimension, detrended fluctuation analysis,approximate entropy, <b>sample</b> <b>entropy,</b> multiscale entropy analysis, sample asymmetry and memory length (based on inverse statistical analysis). It is also possible to represent long range correlations geometrically.|$|E
40|$|<b>Sample</b> <b>entropy</b> {{has been}} applied {{successfully}} for quantification of cardiovascular murmurs, but does <b>sample</b> <b>entropy</b> reflect nonlinear and chaotic characteristics of the murmurs? Seven digital audio recordings from subjects with audible carotid murmurs were recorded. <b>Sample</b> <b>entropy</b> was calculated from periods with and without murmurs and compared with <b>sample</b> <b>entropy</b> from surrogates with linear properties. None of the <b>sample</b> <b>entropy</b> measures from the recordings differed significantly from the surrogates with known linear properties. The <b>sample</b> <b>entropy</b> increased significantly in periods with audible murmurs compared to periods without. A similar difference {{was found between the}} surrogates of periods with and without murmurs. This confirms that <b>sample</b> <b>entropy</b> is suited for quantification of cardiovascular murmurs, but {{there is no evidence that}} nonlinear signal components of the murmursare present or affect the <b>sample</b> <b>entropy.</b> 1...|$|E
40|$|Abstract: <b>Sample</b> <b>entropy</b> can {{reflect the}} change of level of new {{information}} in signal sequence {{as well as the}} size of the new information. Based on the <b>sample</b> <b>entropy</b> as the features of speech classification, the paper firstly extract the <b>sample</b> <b>entropy</b> of mixed signal, mean and variance to calculate each signal <b>sample</b> <b>entropy,</b> finally uses the K mean clustering to recognize. The simulation results show that: the recognition rate can be increased to 89. 2 % based on <b>sample</b> <b>entropy...</b>|$|E
40|$|This paper {{presents}} {{an analysis of}} the Unicode encoding scheme for Tibetan from the standpoint of morpheme entropy. We can speak of two levels of entropy in Tibetan: syllable entropy (a measure of the probability of the sequential occurrence of syllables), and morpheme entropy (a measure of the probability of the sequential occurrence of characters or morphemes), the latter being a measure of the redundancy of the language. Syllable entropy is a purely statistical calculation that {{is a function of the}} domain of the literature <b>sampled,</b> while morpheme <b>entropy,</b> we show, is relatively domain independent given a statistically significant <b>sample.</b> Morpheme <b>entropy</b> can be calculated statistically, though a theoretical upper bound can also be postulated based on language dependent morphology rules. This paper presents both theoretical and statistical estimates of the morpheme entropy for Tibetan, and explores the Tibetan Unicode encoding scheme in relation to data compression, and other issues analyzed in light of entropy-based language modeling...|$|R
40|$|Abstract: Deoxyribonucleic acid (DNA) {{computing}} {{has extended}} {{the frontiers of}} computer science. The field has made possible the creation of programmable DNA molecules that perform complex calculations and autonomous DNA machines. Random numbers are a sequence of numbers that lack any pattern. A random number Generator (RNG) is a computational device designed to generate random numbers. The many applications of random numbers include cryptography, statistical sampling, numerical simulation of physical and biological systems, and lotteries. Random numbers are usually generated by <b>sampling</b> <b>entropy</b> in physical phenomena and processing it through a computer. Examples of such phenomena include a radioactive source, atmospheric noise, and quantum mechanics. Random numbers are hard to characterize mathematically, such that even though there exists several statistical tests to verify the absence of certain patterns in a stream of numbers, no finite set of tests exists for characterizing randomness in numbers, as there may be patterns not considered by such tests. This paper discusses the generation of pseudorandom numbers from DNA Watson-Crick units. 1000 numbers from an experimental DNA segment passed two statistical tests for randomness. This work also reports what might be a breakthrough in DNA structural analysis: the Poisson distribution of DNA bases and amino acids. Key word: Pseudorandom number generator • DNA computing • Poisson distribution I. ...|$|R
50|$|Yarrow has the <b>entropy</b> <b>samples</b> pool {{separated}} from the key, and only reseeds the key when the entropy pool content is completely unpredictable. This design prevents iterative guessing attacks, where an attacker with the key guess the next sample and checks the result by observing the next output.|$|R
40|$|The <b>sample</b> <b>entropy,</b> the {{estimate}} of the entropy per observation, has been introduced by Vasicek (1976) (A test for normality based on <b>sample</b> <b>entropy,</b> J. Royal Statist. Soc. B 38, 730 - 737). In this paper, we provide the <b>sample</b> <b>entropy</b> of order statistics, and present one application of the <b>sample</b> <b>entropy</b> of order statistics {{as a test of}} normality versus skewness. The proposed test statistic has comparable performance with other existing tests. Entropy identity Test of normality Consistency...|$|E
40|$|According to the <b>sample</b> <b>entropy,</b> {{this paper}} {{deals with a}} {{quantitative}} method to evaluate the current stability in double-wire pulsed MIG welding. Firstly, the <b>sample</b> <b>entropy</b> of current signals with different stability but the same parameters is calculated. The {{results show that the}} more stable the current, the smaller the value and the standard deviation of <b>sample</b> <b>entropy.</b> Secondly, four parameters, which are pulse width, peak current, base current, and frequency, are selected for four-level three-factor orthogonal experiment. The calculation and analysis of desired signals indicate that <b>sample</b> <b>entropy</b> values are affected by welding current parameters. Then, a quantitative method based on <b>sample</b> <b>entropy</b> is proposed. The experiment results show that the method can preferably quantify the welding current stability. © 2014 Ping Yao et al...|$|E
40|$|Abstract. Forced power {{oscillation}} {{in power}} systems {{has a serious}} influence to the safe and stable operation of power systems. A fast dynamic <b>sample</b> <b>entropy</b> algorithm is proposed based on <b>sample</b> <b>entropy</b> in this paper. The change of dynamic <b>sample</b> <b>entropy</b> of the tie-line power is analyzed {{to determine whether the}} forced power oscillation happened. Case study on the 4 -machine 2 -area system shows the effectiveness and efficiency of the proposed methodology...|$|E
40|$|Small {{peptides}} {{that might}} have some features of globular proteins can provide important insights into the protein folding problem. Two simulation methods, Monte Carlo Dynamics (MCD), based on the Metropolis <b>sampling</b> scheme, and <b>Entropy</b> <b>Sampling</b> Monte Carlo (ESMC), were applied {{in a study of}} a high-resolution lattice model of the C-terminal fragment of the B 1 domain of protein G. The results provide a detailed description of folding dynamics and thermodynamics and agree with recent experimental findings (. Nature. 390 : 196 - 197). In particular, {{it was found that the}} folding is cooperative and has features of an all-or-none transition. Hairpin assembly is usually initiated by turn formation; however, hydrophobic collapse, followed by the system rearrangement, was also observed. The denatured state exhibits a substantial amount of fluctuating helical conformations, despite the strong beta-type secondary structure propensities encoded in the sequence...|$|R
5000|$|Intel Secure Key is Intel's {{name for}} both the [...] {{instruction}} and the underlying random number generator (RNG) hardware implementation, which was codenamed [...] "Bull Mountain" [...] during development. Intel calls their RNG a [...] "digital random number generator" [...] or DRNG. The generator takes pairs of 256-bit raw <b>entropy</b> <b>samples</b> generated by the hardware entropy source and applies them to an Advanced Encryption Standard (AES) (in CBC-MAC mode) conditioner which reduces them to a single 256-bit conditioned <b>entropy</b> <b>sample.</b> A deterministic random-bit generator called CTR_DRBG defined in NIST SP 800-90A is seeded by the output from the conditioner, providing cryptographically secure random numbers to applications requesting them via the [...] instruction. The hardware will issue a maximum of 511 128-bit samples before changing the seed value. Using the [...] operation provides access to the conditioned 256-bit samples from the AES-CBC-MAC.|$|R
40|$|This paper derives {{the entropy of}} a {{generalized}} half-logistic distribution based on Type-II censored <b>samples,</b> obtains some <b>entropy</b> estimators by using Bayes estimators of an unknown parameter in the generalized half-logistic distribution based on Type-II censored samples and compares these estimators {{in terms of the}} mean squared error and the bias through Monte Carlo simulations...|$|R
40|$|The Shannon-McMillan-Breiman theorem {{asserts that}} the <b>sample</b> <b>entropy</b> of a {{stationary}} and ergodic stochastic process converges to the entropy rate of the same process (as the sample size tends to infinity) almost surely. In this paper, we restrict {{our attention to the}} convergence behavior of the <b>sample</b> <b>entropy</b> of hidden Markov chains. Under certain positivity assumptions, we prove that a central limit theorem (CLT) with some Berry-Esseen bound for the <b>sample</b> <b>entropy</b> of a hidden Markov chain, and we use this CLT to establish a law of iterated logarithm (LIL) for the <b>sample</b> <b>entropy.</b> © 2011 IEEE. published_or_final_versionThe 2011 IEEE International Symposium on Information Theory (ISIT), St. Petersburg, Russia, 31 July- 5 August 2011. In Proceedings of ISIT, 2011, p. 3009 - 301...|$|E
40|$|International audienceThis paper {{deals with}} the {{discrimination}} between suffering foetuses and normal foetuses {{by means of a}} multi-scale similarity entropy. <b>Sample</b> <b>entropy</b> and similarity entropy are evaluated in multi-scale analysis on foetal heart rate signals. Without multi-scale analysis, our results show that only the similarity entropy differentiate suffering foetuses to normal foetuses. Furthermore with the multi-scale analysis, our results show that both the <b>sample</b> <b>entropy</b> and the similarity entropy can discriminate the distressed foetuses to normal foetuses. In all cases the similarity entropy outperforms the <b>sample</b> <b>entropy</b> that is encouraging for another biomedical applications...|$|E
40|$|Variability of {{peak flow}} {{measurements}} has {{been related to}} clinical outcomes in asthma. We hypothesized that the entropy, or information content, of airway impedance over short time scales may predict asthma exacerbation frequency. Sixty-six patients with severe asthma and thirty healthy control subjects underwent impulse oscillometry at baseline, following a deep exhalation manoeuvre, and following bronchodilator administration. On each occasion, airway impedance parameters were measured at 0. 2 second intervals for 150 seconds, yielding a time series, which was then subjected to <b>Sample</b> <b>Entropy</b> analysis. Airway impedance, and <b>Sample</b> <b>Entropy</b> of impedance, was increased in asthmatic patients compared to healthy controls. In a logistic regression model, <b>Sample</b> <b>Entropy</b> of R 5 -R 20, a marker of the fluctuation of the heterogeneity of airway constriction over time, was the variable most strongly associated with the frequent exacerbation phenotype (odds ratio of 3. 23 for every 0. 1 increase in <b>Sample</b> <b>Entropy).</b> Increased airway impedance and <b>Sample</b> <b>Entropy</b> of impedance {{is associated with the}} frequent exacerbation phenotype. Prospective studies are required to assess their predictive value. 10875...|$|E
40|$|AbstractSmall {{peptides}} {{that might}} have some features of globular proteins can provide important insights into the protein folding problem. Two simulation methods, Monte Carlo Dynamics (MCD), based on the Metropolis <b>sampling</b> scheme, and <b>Entropy</b> <b>Sampling</b> Monte Carlo (ESMC), were applied {{in a study of}} a high-resolution lattice model of the C-terminal fragment of the B 1 domain of protein G. The results provide a detailed description of folding dynamics and thermodynamics and agree with recent experimental findings (Munoz et al., 1997. Nature. 390 : 196 – 197). In particular, {{it was found that the}} folding is cooperative and has features of an all-or-none transition. Hairpin assembly is usually initiated by turn formation; however, hydrophobic collapse, followed by the system rearrangement, was also observed. The denatured state exhibits a substantial amount of fluctuating helical conformations, despite the strong β-type secondary structure propensities encoded in the sequence...|$|R
40|$|We {{compared}} a set {{of surface}} EMG (sEMG) parameters in several groups of schizophrenia (SZ, n= 74) patients and healthy controls (n= 11) and coupled them with the clinical data. sEMG records were quantified with spectral, mutual information (MI) based and recurrence quantification analysis (RQA) parameters, and with approximate and <b>sample</b> <b>entropies</b> (ApEn and SampEn). Psychotic deterioration was estimated with Positive and Negative Syndrome Scale (PANSS) and with the positive subscale of PANSS. Neuroleptic-induced parkinsonism (NIP) motor symptoms were estimated with Simpson-Angus Scale (SAS). Dyskinesia was measured with Abnormal Involuntary Movement Scale (AIMS). We {{found that there was}} no difference in values of sEMG parameters between healthy controls and drug-naïve SZ patients. The most specific group was formed of SZ patients who were administered both typical and atypical antipsychotics (AP). Their sEMG parameters were significantly different from those of SZ patients taking either typical or atypical AP or taking no AP. This may represent a kind of synergistic effect of these two classes of AP. For the clinical data we found that PANSS, SAS, and AIMS were not correlated to any of the sEMG parameters. Conclusion: with nonlinear parameters of sEMG it is possible to reveal NIP in SZ patients, and it may help to discriminate between different clinical groups of SZ patients. Combined typical and atypical AP therapy has stronger effect on sEMG than a therapy with AP of only one class...|$|R
40|$|We {{consider}} {{the problem of}} finite <b>sample</b> corrections for <b>entropy</b> estimation. New estimates of the Shannon entropy are proposed and their systematic error (the bias) is computed analytically. We find that our results cover correction formulas of current entropy estimates recently discussed in literature. The trade-off between bias reduction and the increase of the corresponding statistical error is analyzed. Comment: 5 pages, 3 figure...|$|R
