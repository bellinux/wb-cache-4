9|27|Public
50|$|Theomatics is not {{the same}} thing as Bible code; it uses an entirely {{different}} technique. The Bible code (also called ELS for Equidistant Letter Sequences) uses a letter <b>skipping</b> <b>technique.</b> Theomatics, on the other hand, is based on gematria and isopsephia, systems which assign numerical values to letters in the ancient Hebrew/Aramaic and Greek alphabets.|$|E
50|$|Two walking {{events were}} {{held on the}} track at the 1906 Games: a men's 1500 m walk and a men's 3000 m walk. The first final to be held was the shorter distance. American George Bonhag, an {{absolute}} walking novice who had competed in the 5-mile run, came away as the winner after Canada's Don Linden, the eventual runner-up, had given basic technical advice {{to allow him to}} compete. Bonhag used a <b>skipping</b> <b>technique</b> that the judges, after Linden's protestations, later decided was against the rules. To decide the true victor, the judges reorganised a walk-off between the pair, but this did not occur and the results stood. A British and an Austrian were disqualified for running and James Connolly (the first Olympic champion) later stated he believed the head walk judge, James Edward Sullivan, also American, had effectively handed the race to Bonhag.|$|E
40|$|An {{efficient}} {{data compression}} system is presented for satellite pictures and two grey level pictures derived from satellite pictures. The compression techniques take {{advantages of the}} correlation between adjacent picture elements. Several source coding methods are investigated. Double delta coding is presented and shown {{to be the most}} efficient. Both predictive differential quantizing technique and double delta coding can be significantly improved by applying a background <b>skipping</b> <b>technique.</b> An extension code is constructed. This code requires very little storage space and operates efficiently. Simulation results are presented for various coding schemes and source codes...|$|E
40|$|International audienceRecommender systems ﬁlter {{resources}} {{for a given}} user by predicting the most pertinent resource given a speciﬁc context. This paper describes a new approach of generating suitable recommendations based on the active user's navigation stream. The underlying hypothesis is that the resources order in the stream results from the intrinsic logic of the user's behavior. The Sequence Based Recommender we propose is inspired from Language Modeling and integrates <b>skipping</b> <b>techniques.</b> It has been tested on a browsing dataset extracted from Intranet logs provided by a French bank. Results show {{that the use of}} exponential decay weighting schemes when taking into account non contiguous sequences to compute recommendations enhances the accuracy. Moreover, we propose a skipping variant that provides a high accuracy while being less complex...|$|R
40|$|This paper {{presents}} FPGA implementations {{of classical}} algorithms for computing ln(x) with some improvement {{at the level}} of the multiplication steps, and step <b>skipping</b> <b>techniques.</b> One starts from a practical implementation of ln(x) computation using a convergence method. The function is approximated by a multiplicative normalization technique, however, thanks to the peculiarity of the multiplicative factor, namely (1 + ai. 2 -i), with ai ∈ {- 1, 0, 1 }, the successive multiplications have been replaced by additions. Doing so, one saves the use of LUT’s and eventually reduces processing time, as addition is generally faster than multiplication. Further, the acceleration <b>technique,</b> based on <b>skipping</b> trivial steps, improves performances. Implementations for FPGA are presented with time and slice cost evaluations. The Xilinx Virtex IV has been used for comparative analysis of 8 to 64 -bit logarithm computing devices. II Workshop de Arquitecturas, Redes y Sistemas Operativo...|$|R
40|$|International audienceRecommender systems provide {{users with}} {{pertinent}} resources according their context and their proﬁles, by applying statistical and knowledge discovery techniques. This paper describes {{a new approach}} of generating suitable recommendations based on the active user's navigation stream, by considering long and short-distance resources in the history with a tractable model. The Skipping Based Recommender we propose uses Markov models inspired from the ones used in language modeling while integrating <b>skipping</b> <b>techniques</b> to handle noise during navigation. Weighting schemes are also used to alleviate the importance of distant resources. This recommender has also the characteristic to be anytime. It has been tested on a browsing dataset extracted from Intranet logs provided by a French bank. Results show {{that the use of}} exponential decay weighting schemes when taking into account non contiguous resources to compute recommendations enhances the accuracy. Moreover, the skipping variant we propose provides a high accuracy while being less complex than state of the art variants...|$|R
40|$|In {{this work}} {{we present a}} new CORDIC {{algorithm}} for the vectoring mode, based {{on the use of}} radix- 4, preserving a complexity in the microrotations that {{is similar to that of}} the conventional radix- 2 CORDIC. The use of this radix, together with the inclusion in the CORDIC algorithm of the zero <b>skipping</b> <b>technique,</b> reduces by more than half the number of iterations with respect to the conventional radix 2 CORDIC, with the consequent reduction of time in recursive architectures or area in pipelined architectures. In processes such as SVD or matrix triangularization in which the evaluation of the rotation angle is required, this algorithm is shown to be specially efficient...|$|E
40|$|Most CPU-based volume raycasting {{approaches}} achieve {{high performance}} by advanced memory layouts, space subdivision, and excessive pre-computing. Such approaches typically need {{an enormous amount}} of memory. They are limited to sizes which do not satisfy the medical data used in daily clinical routine. We present a new volume raycasting approach based on image-ordered raycasting with object-ordered processing, which is able to perform highquality rendering of very large medical data in real-time on commodity computers. For large medical data such as computed tomographic (CT) angiography run-offs (512 x 512 x 1202) we achieve rendering times up to 2. 5 fps on a commodity notebook. We achieve this by introducing a memory efficient acceleration technique for on-the-fly gradient estimation and a memory efficient hybrid removal and <b>skipping</b> <b>technique</b> of transparent regions. We employ quantized binary histograms, granular resolution octrees, and a cell invisibility cache. These acceleration structures require just a small extra storage of approximately 10 %...|$|E
40|$|Figure 1 : Close-up of {{the visible}} male. Most CPU-based volume raycasting {{approaches}} achieve high performance by advanced memory layouts, space subdivision, and excessive pre-computing. Such approaches typically need {{an enormous amount of}} memory. They are limited to sizes which do not satisfy the medical data used in daily clinical routine. We present a new volume raycasting approach based on image-ordered raycasting with object-ordered processing, which is able to perform highquality rendering of very large medical data in real-time on commodity computers. For large medical data such as computed tomographic (CT) angiography run-offs (512 x 512 x 1202) we achieve rendering times up to 2. 5 fps on a commodity notebook. We achieve this by introducing a memory efficient acceleration technique for on-the-fly gradient estimation and a memory efficient hybrid removal and <b>skipping</b> <b>technique</b> of transparent regions. We employ quantized binary histograms, granular resolution octrees, and a cell invisibility cache. These acceleration structures require just a small extra storage of approximately 10 %...|$|E
40|$|Network {{densification}} is foreseen as {{a potential}} solution to fulfill the 5 G spectral efficiency requirements. The spectral efficiency is improved by shrinking base stations’ (BSs) footprints, thus improving the spatial frequency reuse and {{reducing the number of}} users sharing the resources of each BS. However, the foreseen densification gains are achieved at the expense of increasing handover (HO) rates. Hence, HO rate is a key performance limiting factor that should be carefully considered in densification planning. This paper sheds light on the HO problem that appears in dense 5 G networks and proposes an effective solution via topology aware HO <b>skipping.</b> Different <b>skipping</b> <b>techniques</b> are considered and compared with the conventional best connected scheme. To this end, the proposed schemes are validated via the average user rate in downlink single-tier and two-tier cellular networks, which are modeled using the Poisson point process and the Poisson cluster process, respectively. The proposed skipping schemes show up to 47 % gains in the average throughput, which would maximize the benefit of network densification...|$|R
40|$|The texture-based volume {{rendering}} is a memory-intensive algorithm. Its performance {{relies heavily}} {{on the performance of}} the texture cache. However, most existing texture-based volume rendering methods blindly map computational resources to texture memory and result in incoherent memory access patterns, causing low cache hit rates in certain cases. The distance between samples taken by threads of an atomic scheduling unit (e. g. a warp of 32 threads in CUDA) of the GPU is a crucial factor that affects the texture cache performance. Based on this fact, we present a new sampling strategy, called Warp Marching, for the ray-casting algorithm of texture-based volume rendering. The effects of different sample organizations and different thread-pixel mappings in the ray-casting algorithm are thoroughly analyzed. Also, a pipeline manner color blending approach is introduced and the power of warp-level GPU operations is leveraged to improve the efficiency of parallel executions on the GPU. In addition, the rendering performance of the Warp Marching is view-independent, and it outperforms existing empty space <b>skipping</b> <b>techniques</b> in scenarios that need to render large dynamic volumes in a low resolution image. Through a series of micro-benchmarking and real-life data experiments, we rigorously analyze our sampling strategies and demonstrate significant performance enhancements over existing sampling methods...|$|R
2500|$|P-47 pilots {{frequently}} carried two 500lb (227kg) bombs, using <b>skip</b> bombing <b>techniques</b> for difficult targets (skipping bombs into railroad tunnels {{to destroy}} hidden enemy trains {{was a favorite}} tactic). The adoption of the triple-tube M10 rocket launcher with M8 high-explosive [...] rockets (each with an explosive force similar to a 105mm artillery shell)—much as the RAF's Hawker Typhoon gained when first fitted with its own two quartets of underwing RP-3 rockets for the same purposes—significantly increased the P-47's ground attack capability. Late in the war, the P-47 was retrofitted with more powerful [...] HVAR rockets.|$|R
40|$|Abstract—Context-based {{adaptive}} variable-length coding (CAVLC) {{is a new}} {{and important}} feature of the latest video coding standard, H. 264 /AVC. The direct VLSI implementation of CAVLC modified from the conventional run-length coding architecture will lead to low throughput and utilization. In this brief, an efficient CAVLC design is proposed. The main concept is the two-stage block pipelining scheme for parallel processing of two 4 4 blocks. When one block is processed by the scanning engine to collect the required symbols, its previous block is handled by the coding engine to translate symbols into bitstream. Our dual-block-pipelined architecture doubles the throughput and utilization of CAVLC at high bit rates. Moreover, a zero <b>skipping</b> <b>technique</b> is adopted to reduce up to 90 % of cycles at low bit rates. Last but not least, Exp-Golomb coding for other general symbols and bitstream encapsulation for the network abstraction layer are integrated with CAVLC as a complete H. 264 /AVC baseline profile entropy coder. Simulation shows that our design is capable of real-time processing for 1920 1088 30 -fps videos with 23. 6 K logic gates at 100 MHz. Index Terms—Context-based adaptive variable-length coding (CAVLC), H. 264 /AVC, VLSI architecture. I...|$|E
40|$|Abstract—Skin color {{can provide}} a useful and robust cue for human-related image analysis, such as face {{detection}}, pornographic image filtering, hand detection and tracking, people retrieval in databases and Internet, etc. The major problem of such kinds of skin color detection algorithms {{is that it is}} time consuming and hence cannot be applied to a real time system. To overcome this problem, we introduce a new fast technique for skin detection which can be applied in a real time system. In this technique, instead of testing each image pixel to label it as skin or non-skin (as in classic techniques), we skip a set of pixels. The reason of the skipping process is the high probability that neighbors of the skin color pixels are also skin pixels, especially in adult images and vise versa. The proposed method can rapidly detect skin and non-skin color pixels, which in turn dramatically reduce the CPU time required for the protection process. Since many fast detection techniques are based on image resizing, we apply our proposed pixel <b>skipping</b> <b>technique</b> with image resizing to obtain better results. The performance evaluation of the proposed skipping and hybrid techniques in terms of the measured CPU time is presented. Experimental results demonstrate that the proposed methods achieve better result than the relevant classic method. Keywords—Adult images filtering, image resizing, skin color detection, YcbCr color space. S I...|$|E
40|$|Video quality {{assessment}} {{is becoming more}} important in current digital video application. Most of the video encoders and decoders focus in providing more visual quality in less amount of digital information, being the compression {{one of the most}} important solutions for making possible the transmission of high quality videos in the current digital transmission media. Sometimes, however, not only video compression is good enough for video quality transmission, maybe the available network bandwidth or the ability to process the video by a low-end processor is not sufficient for continuous playback. Many techniques that consider time variabilities of different sorts need to be developed to keep a digital video watchable. This work focus on a new timeline approach to compare the quality of videos for which a presentation time stamp variability is allowed. Based on video quality metrics already established the new approach can be used to help the decision about the most suitable method for real-time video transmission. Among the many possible techniques able to cope with time stamp variability, one can mention the variable delay time in video presentation, or the use of frame <b>skipping</b> <b>technique,</b> or even more compression applied to reduce the bandwidth and/or processing - improving the perceived video quality under scarce resource availability. A new timeline approach to calculate the current video quality metrics used in most of the encoders and decoders available was developed. This timeline approach takes into account not only the frames themselves, as a frame-by-frame evaluation, but both the frame and presentation time stamp are considered in the proposed approach to the problem. The quality comparison is based on a new discretization, which hopefully serves better the quality of digital video without losing perceptual quality in the process. All the work is tested in an offline solution program that is able to give numeric quality index to the tested sample videos. Those samples are selected from the current distribution of the German digital video television system, encoded in MPEG- 2...|$|E
40|$|Mobile {{real-time}} video streaming for web-casting and video conferencing conforming to the ITU-T H. 264 data compression recommendations format is emergent. However, {{the quality of}} the video services for typical applications is hard to foreseen and sustain. Qualities of Services (QoS) approaches, such as video frame <b>skipping</b> <b>techniques,</b> are thus attractive to address the problem. Many conventional video frame-skipping techniques focus on the system-centric quality attributes to adjust the quality of video delivery. They normally ignore the user-centric quality attributes. A usercentric approach, amongst others, is to take human perception in consideration and adapt the video quality expressed in patterns of Group of Pictures (GoP). In a post-processing step of an encoding stage, it filters and re-titles the qualitymatching patterns from the rest. It then reconstructs the filtered GoP according to the H. 264 format. Most existing H. 264 playback engines unfortunately cease to work properly for video steaming with skipped frames. This paper reports our experience on alleviating the playback problem through a case study on our effort to develop a prototype of such a playback engine. It studies how to reconstruct a video steaming in the H. 264 format on the fly in the presence of predictable skipped frames, satisfying certain selected user-centric QoS quality levels. We also analyze the applicability of the approach. Keywords: H. 264, frame skipping, QoS control, QoS-human, video reconstruction, human perception 1...|$|R
40|$|The {{analysis}} of organic samples by flame {{atomic absorption spectrometry}} (FAAS) involves the difficulties of the digestion step. This fact was partially overcome {{by the use of}} the microwave assisted digestion <b>technique</b> (<b>Skip,</b> 1998). The digestion of the samples has the analytical advantage of an appropriated presentation for the analysis by differen...|$|R
50|$|P-47 pilots {{frequently}} carried two 500 lb (227 kg) bombs, using <b>skip</b> bombing <b>techniques</b> for difficult targets (skipping bombs into railroad tunnels {{to destroy}} hidden enemy trains {{was a favorite}} tactic). The adoption of the triple-tube M10 rocket launcher with M8 high-explosive 4.5 in rockets (each with an explosive force similar to a 105 mm artillery shell)—much as the RAF's Hawker Typhoon gained when first fitted with its own two quartets of underwing RP-3 rockets for the same purposes—significantly increased the P-47's ground attack capability. Late in the war, the P-47 was retrofitted with more powerful 5 in HVAR rockets.|$|R
2500|$|USAAF anti-shipping {{operations}} in the Far East were generally unsuccessful. In early operations during the Battle of the Philippines, B-17s claimed to have sunk one minesweeper and damaged two Japanese transports, the cruiser , and the destroyer [...] However, all of these ships {{are known to have}} suffered no damage from air attack during that period. In other early battles, including the Battle of Coral Sea or Battle of Midway, no claims were made at all, although some hits were seen on docked targets. The USAAF eventually replaced all of their anti-shipping B-17s with other aircraft, and came to use the <b>skip</b> bombing <b>technique</b> in direct low-level attacks.|$|R
40|$|A new fault attack, double {{counting}} attack (DCA), on the precomputation of $ 2 ^t$-ary modular exponentiation for a classical RSA digital signature (i. e., RSA without the Chinese remainder theorem) is proposed. The $ 2 ^t$-ary method {{is the most}} popular and widely used algorithm to speed up the RSA signature process. Developers can realize the fastest signature process by choosing optimum $t$. For example, $t= 6 $ is optimum for a 1536 -bit classical RSA implementation. The $ 2 ^t$-ary method requires precomputation to generate small exponentials of message. Conventional fault attack research has paid little attention to precomputation, even though precomputation could be a target of a fault attack. The proposed DCA induces faults in precomputation by using instruction <b>skip</b> <b>technique,</b> which is equivalent to replacing an instruction with a no operation in assembly language. This paper also presents a useful "position checker" tool to determine the position of the $ 2 ^t$-ary coefficients of the secret exponent from signatures based on faulted precomputations. The DCA is demonstrated to be an effective attack method for some widely used parameters. DCA can reconstruct an entire secret exponent using the position checker with $ 63 (= 2 ^ 6 - 1) $ faulted signatures in a short time for a 1536 -bit RSA implementation using the $ 2 ^ 6 $-ary method. The DCA process can be accelerated for a small public exponent (e. g., 65537). The the best of our knowledge, the proposed DCA is the first fault attack against classical RSA precomputation...|$|R
40|$|This paper {{presents}} step <b>skipping</b> acceleration <b>techniques</b> for a {{class of}} convergence algorithms computing arithmetic functions. In particular, {{an extension of the}} fast adder carry-skip procedure is carried out for special purpose cellular array circuits implementing iterative logical functions for which some propagating information may be fruitfully computed ahead of the current step output computation. This information is thus carried to the next stage, accelerating the overall calculation. An application is given for the 2 ´s complement sign changing circuit, then for the step-skipping acceleration circuits used in the implementation of the ln(x) convergence algorithm. FPGA implementations on Xilinx Virtex IV have been achieved with comparative analysis of 32 - to 512 -bit computing algorithms. Workshop de Arquitecturas, Redes y Sistemas Operativos (WARSO...|$|R
40|$|High {{resolution}} surveillance {{video cameras}} are invaluable resources for effective crime prevention and forensic investigations. However, increasing communication bandwidth requirements of high definition surveillance videos are severely {{limiting the number}} of cameras that can be deployed. Higher bitrate also increases operating expenses due to higher data communication and storage costs. Hence, it is essential to develop low complexity algorithms which reduce data rate of the compressed video stream without affecting the image fidelity. In this thesis, a computer vision aided H. 264 surveillance video encoder and four associated algorithms are proposed to reduce the bitrate. The proposed techniques are (I) Speeded up foreground segmentation, (II) Skip decision, (III) Reference frame selection and (IV) Face Region-of-Interest (ROI) coding. In {{the first part of the}} thesis, a modification to the adaptive Gaussian Mixture Model (GMM) based foreground segmentation algorithm is proposed to reduce computational complexity. This is achieved by replacing expensive floating point computations with low cost integer operations. To maintain accuracy, we compute periodic floating point updates for the GMM weight parameter using the value of an integer counter. Experiments show speedups in the range of 1. 33 - 1. 44 on standard video datasets where a large fraction of pixels are multimodal. In the second part, we propose a <b>skip</b> decision <b>technique</b> that uses a spatial sampler to sample pixels. The sampled pixels are segmented using the speeded up GMM algorithm. The storage pattern of the GMM parameters in memory is also modified to improve cache performance. Skip selection is performed using the segmentation results of the sampled pixels. In the third part, a reference frame selection algorithm is proposed to maximize the number of background Macroblocks (MB’s) (i. e. MB’s that contain background image content) in the Decoded Picture Buffer. This reduces the cost of coding uncovered background regions. Distortion over foreground pixels is measured to quantify the performance of skip decision and reference frame selection techniques. Experimental results show bit rate savings of up to 94. 5 % over methods proposed in literature on video surveillance data sets. The proposed techniques also provide up to 74. 5 % reduction in compression complexity without increasing the distortion over the foreground regions in the video sequence. In the final part of the thesis, face and shadow region detection is combined with the skip decision algorithm to perform ROI coding for pedestrian surveillance videos. Since person identification requires high quality face images, MB’s containing face image content are encoded with a low Quantization Parameter setting (i. e. high quality). Other regions of the body in the image are considered as RORI (Regions of reduced interest) and are encoded at low quality. The shadow regions are marked as <b>Skip.</b> <b>Techniques</b> that use only facial features to detect faces (e. g. Viola Jones face detector) are not robust in real world scenarios. Hence, we propose to initially detect pedestrians using deformable part models. The face region is determined using the deformed part locations. Detected pedestrians are tracked using an optical flow based tracker combined with a Kalman filter. The tracker improves the accuracy and also avoids the need to run the object detector on already detected pedestrians. Shadow and skin detector scores are computed over super pixels. Bilattice based logic inference is used to combine multiple likelihood scores and classify the super pixels as ROI, RORI or RONI. The coding mode and QP values of the MB’s are determined using the super pixel labels. The proposed techniques provide a further reduction in bitrate of up to 50. 2 %...|$|R
40|$|The scale-space {{method has}} been widely used in {{handling}} image data at multiple scales. Application of Gaussian filtering in different field includes human vision problem, medical data, financial data and electroencephalogram (EEG) signal. The main {{purpose of this paper}} is to apply the Gaussian scale-space method by determining a suitable σ value in order to smooth rope <b>skipping</b> data. Smoothing <b>technique</b> using a Gaussian kernel with a selection of bandwidth (σ) and time (x) is applied. It is found that the tolerance value of σ can be used to smooth not only one set of data, but also other biomechanical data of different anatomical body landmarks...|$|R
40|$|Abstract Multi-processor {{system-on-chip}} (MPSoC) simulators {{are many}} {{orders of magnitude}} slower than the hardware they simulate due to increasing architectural com-plexity. In this paper, we propose a new application sampling technique to accelerate the simulation of MPSoC design space exploration (DSE). The proposed technique dy-namically combines simultaneously executed phases, thus generating a sampling unit. This technique accelerates the simulation by allowing the repeated combinations of parallel phases to be <b>skipped.</b> A complementary <b>technique,</b> called cluster synthesis, is also proposed to improve the simulation acceleration {{when the number of}} possible phase combinations increases. Our experimental results show that this technique can accelerate the simulation up to a factor of 800 with a relatively small estimation error. ...|$|R
40|$|International audienceMulti-processor {{system-on-chip}} (MPSoC) simulators {{are many}} {{orders of magnitude}} slower than the hardware they simulate due to increasing architectural complexity. In this paper, we propose a new application sampling technique to accelerate the simulation of MPSoC design space exploration (DSE). The proposed technique dynamically combines simultaneously executed phases, thus generating a sampling unit. This technique accelerates the simulation by allowing the repeated combinations of parallel phases to be <b>skipped.</b> A complementary <b>technique,</b> called cluster synthesis, is also proposed to improve the simulation acceleration {{when the number of}} possible phase combinations increases. Our experimental results show that this technique can accelerate the simulation up to a factor of 800 with a relatively small estimation error. Keywords Simulation - MPSoC architectures - Application sampling - Performance evaluatio...|$|R
40|$|Even as {{computer}} processing speeds have become faster {{and the size}} of memory has also increased over the years, the need for elegant algorithms (programs that accomplish such tasks/operations as information retrieval, and manipulation as efficiently as possible) remain as important now {{as it did in the}} past. It is even more so as more complex problems come to the fore. Skip List is a probabilistic data structure with algorithms to efficiently accomplish such operations as search, insert and delete. In this paper, we present the results of implementing the Skip List data structure. The paper also addresses current Web search strategies and algorithms and how the application of <b>Skip</b> List implementation <b>techniques</b> and extensions can bring about optimal search query results...|$|R
40|$|Abstract Background Myostatin is {{a potent}} muscle growth {{inhibitor}} that belongs to the Transforming Growth Factor-β (TGF-β) family. Mutations leading to non functional myostatin {{have been associated with}} hypermuscularity in several organisms. By contrast, Duchenne muscular dystrophy (DMD) is characterized by a loss of muscle fibers and impaired regeneration. In this study, we aim to knockdown myostatin by means of exon <b>skipping,</b> a <b>technique</b> which has been successfully applied to reframe the genetic defect of dystrophin gene in DMD patients. Methods We targeted myostatin exon 2 using antisense oligonucleotides (AON) in healthy and DMD-derived myotubes cultures. We assessed the exon skipping level, transcriptional expression of myostatin and its target genes, and combined myostatin and several dystrophin AONs. These AONs were also applied in the mdx mice models via intramuscular injections. Results Myostatin AON induced exon 2 skipping in cell cultures and to a lower extent in the mdx mice. It was accompanied by decrease in myostatin mRNA and enhanced MYOG and MYF 5 expression. Furthermore, combination of myostatin and dystrophin AONs induced simultaneous skipping of both genes. Conclusions We conclude that two AONs can be used to target two different genes, MSTN and DMD, in a straightforward manner. Targeting multiple ligands of TGF-beta family will be more promising as adjuvant therapies for DMD. </p...|$|R
40|$|Background: Myostatin is {{a potent}} muscle growth {{inhibitor}} that belongs to the Transforming Growth Factor-b (TGF-b) family. Mutations leading to non functional myostatin {{have been associated with}} hypermuscularity in several organisms. By contrast, Duchenne muscular dystrophy (DMD) is characterized by a loss of muscle fibers and impaired regeneration. In this study, we aim to knockdown myostatin by means of exon <b>skipping,</b> a <b>technique</b> which has been successfully applied to reframe the genetic defect of dystrophin gene in DMD patients. Methods: We targeted myostatin exon 2 using antisense oligonucleotides (AON) in healthy and DMD-derived myotubes cultures. We assessed the exon skipping level, transcriptional expression of myostatin and its target genes, and combined myostatin and several dystrophin AONs. These AONs were also applied in the mdx mice models via intramuscular injections. Results: Myostatin AON induced exon 2 skipping in cell cultures and to a lower extent in the mdx mice. It was accompanied by decrease in myostatin mRNA and enhanced MYOG and MYF 5 expression. Furthermore, combination of myostatin and dystrophin AONs induced simultaneous skipping of both genes. Conclusions: We conclude that two AONs can be used to target two different genes, MSTN and DMD, ina straightforward manner. Targeting multiple ligands of TGF-beta family will be more promising as adjuvant therapie...|$|R
40|$|This {{paper we}} {{proposed}} advanced burst mode control technique {{to reduce the}} standby power consumption of the switch mode power supply (SMPS). To reduce the standby power consumption, most of the converter use burst mode or <b>skip</b> mode control <b>technique.</b> However Conventional standby mode control techniques have some problems such as audible noise and poor regulation. In proposed techniques, basically, the burst mode control technique is employed to reduce the fundamental switching frequency while limiting the peak drain current. But, in proposed technique, to improve the regulation characteristic, burst period of the proposed technique is shorter {{than that of the}} conventional burst mode technique. And also, to reduce the switching loss increase due to the short burst period, burst switching signal of the proposed <b>technique</b> is partially <b>skipped.</b> By using proposed advanced burst mode control technique, calculated standby power is 0. 695 W while standby power of the conventional burst mode control is 1. 014 W. </span...|$|R
40|$|We {{present a}} new type of search trees, called Skip trees, which are a {{generalization}} of Skip lists. To be precise, there is a one-to-one mapping between the two data types which commutes with the sequential update algorithms. A Skip list is a data structure used to manage data bases which stores values in a sorted way and in which it is insured that the form of the Skip list is independent of the order of updates by using randomization <b>techniques.</b> <b>Skip</b> trees inherit all the properties of Skip lists, including the time bounds of sequential algorithms. The algorithmic improvement of the Skip tree type is that a concurrent algorithm on the fly approach can be designed. Among other advantages, this algorithm is more compressive than the one designed by Pugh for Skip lists and accepts a higher degree of concurrence because it is based on a set of local updates. From a practical point of view, although the Skip list should be in the main memory, Skip trees can be registered into a secondary [...] ...|$|R
40|$|Exon {{skipping}} using {{antisense oligonucleotides}} (AONs) has successfully {{been used to}} reframe the mRNA in various DMD (Duchenne muscular dystrophy) patients carrying deletions and in the mdx mouse model. This study can be devided in two parts: {{in the first part}} we have tested the feasibility of the exon skipping approach for patients with small mutations in in-frame exons, while in the second part a quantitative comparison of exon <b>skipping</b> revealing <b>techniques</b> is addressed. We first identified 55 novel disease-causing point mutations. We selected 5 patients with nonsense or frameshifting mutations in exons 10, 16, 26, 33 and 34. Wild type and mutation specific 2 ‟OMePS AONs were tested in cell-free splicing assays and in cultured cells derived from the selected patients. The results obtained confirm cell-free splicing assay as an alternative system to test exon skipping propensity when patients‟ cells are unavailable. In myogenic cells, similar levels of exon skipping were observed for wild type and mutation specific AONs for exons 16, 26 and 33, while for exon 10 and exon 34 the efficiency of the AONs was significantly different. Interestingly, in some cases skipping efficiencies for mutated exons were quite dissimilar compared to what previously reported for the respective wild type exons. This behaviour may be related to effect of the mutations on exon skipping propensity and highlights the complexity of identifying optimal AONs for skipping exons with small mutations. In the second part we compared different techniques to reveal the exon skipping levels in the muscles of 7 different mdx mice. An absolute quantification of the dystrophin transcript amount was possible using a digital array. Results underline the low expression of the dytrophin gene and the amount needed to correctly quantify the exon skipping percentage...|$|R
40|$|The {{next-generation}} sequencing (NGS) technology outputs {{a huge number}} of sequences (reads) that require further processing. After applying prefiltering techniques in order to eliminate redundancy and to correct erroneous reads, an overlap-based assembler typically finds the longest exact suffix-prefix match between each ordered pair of the input reads. However, another trend has been evolving for the purpose of solving an approximate version of the overlap problem. The main benefit of this direction is the ability to <b>skip</b> time-consuming error-detecting <b>techniques</b> which are applied in the prefiltering stage. In this work, we present and compare two techniques to solve the approximate overlap problem. The first adapts a compact prefix tree to efficiently solve the approximate all-pairs suffix-prefix problem, while the other utilizes a well-known principle, namely, the pigeonhole principle, to identify a potential overlap match in order to ultimately solve the same problem. Our results show that our solution using the pigeonhole principle has better space and time consumption over an FM-based solution, while our solution based on prefix tree has the best space consumption between all three solutions. The number of mismatches (hamming distance) is used to define the approximate matching between strings in our work...|$|R
40|$|For {{real-time}} {{speech and}} audio encoders used in various multimedia applications, low-complexity encoding algorithms are required. Indeed, accurate classification of input signals {{is the key}} prerequisite for variable bit rate encoding, which has been introduced in order to effectively utilize limited communication bandwidth. This paper investigates implementation issues with a support vector machine (SVM) -based speech/music classifier in the selectable mode vocoder (SMV) framework, which is a standard codec adopted by the Third-Generation Partnership Project 2 (3 GPP 2). While a support vector machine {{is well known for}} its superior classification capability, it is accompanied by a high computational cost. In order to achieve a more realizable system, we propose two techniques for the SVM-based speech/music classifier, aimed at reducing the number of classification requests to the classifier. The first technique introduces a simpler classifier that processes some of the input frames instead of the SVM-based classifier, and the second <b>technique</b> <b>skips</b> a portion of input frames based on strong inter-frame correlation in speech and music frames. Our experimental results show that the proposed techniques can reduce the computational cost of the SVM-based classifier by 95. 4 % with negligible performance degradation, making it plausible for integration into the SMV codec. This work was supported by NRF of Korea grant funded by the MEST (2012 R 1 A 2 A 2 A 01004895) and {{this research was supported by}} the MSIP, Korea, under the ITRC support program supervised by the NIPA (NIPA- 2013 -H 0301 - 13 - 4005...|$|R
40|$|Hereditary {{haemochromatosis}} (HH) is an {{autosomal recessive}} disorder characterized by excessive intestinal iron absorption resulting in increased pathological body iron stores. It is {{typically associated with}} homozygosity for the c. 845 G>A (p. C 282 Y) mutation in the HFE gene. However, other HFE alterations {{have been reported in}} affected individuals but their association with the disease is unclear. This study analysed the functional consequences of two HFE mutations, c. 829 G>A (p. E 277 K) and c. 884 T>C(p. V 295 A). Firstly, it was shown that c. 829 G>A affects the HFE splicing by diminishing the full length HFE and ivs 4 _ 66 bp inclusion transcript levels, while increasing the amount of exon 4 <b>skipping</b> transcript. Immunofluorescent <b>techniques</b> showed that the HFE_E 277 K protein had a diffuse distribution(similar to HFE_C 282 Y) while HFE_V 295 A presented at the cell surface and perinuclear compartments (resembling HFE_wt). Immunoprecipitation assays revealed a decreased association of HFE_E 277 K and HFE_V 295 A with both b 2 -microglobulin (B 2 M; 38 ± 7 % and 66 ± 8 %, respectively) and transferrin receptor (TFRC, also termed TFR 1) (58 ± 2 % and 49 ± 16 %, respectively). Herein, we prove that both mutations partially abrogate HFE association with B 2 M and TFRC, crucial for its correct processing and cell surface presentation. Although E 277 K has a more deleterious effect than V 295 A, we propose that both mutations {{may play a role in}} the development of hereditary haemochromatosis...|$|R
40|$|Among various transforms, the {{discrete}} cosine transform (DCT) is {{the most}} widely used one in multimedia compression technologies for different image or video coding standards. During the development of image or video compression, a lot of interest has been attracted to understand the statistical distribution of DCT coefficients, which would be useful to design compression techniques, such as quantization, entropy coding and rate control. Recently, a bi-geometric transparent composite model (BGTCM) has been developed to provide modelling of distribution of DCT coefficients with both simplicity and accuracy. It has been reported that for DCT coefficients obtained from original images, which is applied in image coding, a transparent composite model (TCM) can provide better modelling than Laplacian. In video compression, such as H. 264 /AVC, DCT is performed on residual images obtained after prediction with different transform sizes. What's more, in high efficiency video coding(HEVC) which is the newest video coding standard, besides DCT as the main transform tool, discrete sine transform (DST) and transform <b>skip</b> (TS) <b>techniques</b> are possibly performed on residual data in small blocks. As such, the distribution of transformed residual data differs from that of transformed original image data. In this thesis, the distribution of coefficients, including those from all DCT, DST and TS blocks, is analysed based on BGTCM. To be specific, firstly, the distribution of all the coefficients from the whole frame is examined. Secondly, in HEVC, the entropy coding is implemented based on the new encoding concept, coefficient group (CG) with size 4 * 4, where quantized coefficients are encoded with context models based on their scan indices in each CG. To simulate the encoding process, coefficients at the same scan indices among different CGs are grouped together to form a set. Distribution of coefficients in each set is analysed. Based on our result, BGTCM is better than other widely used distributions, such as Laplacian and Cauchy distributions, in both x^ 2 and KL-divergence testing. Furthermore, unlike the way based on Laplacian and Cauchy distribution, the BGTCM can be used to model rate-quantization (R-Q) and distortion-quantization (D-Q) models without approximation expressions. R-Q and D-Q models based on BGTCM can reflect the distribution of coefficients, which are important in rate control. In video coding, rate control involves these two models to generate a suitable quantization parameter without multi-passes encoding {{in order to maintain the}} coding efficiency and to generate required rate to satisfy rate requirement. In this thesis, based on BGTCM, rate control in HEVC is revised with much increase in coding efficiency and decrease in rate fluctuation in terms of rate variance among frames for constant bit rate requirement. 1 yea...|$|R

