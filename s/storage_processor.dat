13|156|Public
50|$|The EMC Celerra NAS {{device is}} based on the same X-blade {{architecture}} as the CLARiiON <b>storage</b> <b>processor.</b>|$|E
5000|$|The later IBM System/34 and IBM System/36 partly {{inherited}} the architecture from System/32. They had two processors: a Control <b>Storage</b> <b>Processor</b> (CSP), {{same as in}} System/32, which handled most supervisor and input/output operations, and a Main <b>Storage</b> <b>Processor</b> (MSP), a re-implementation of the System/3 model 15 processor, used to run System/3 applications.So, those systems may {{be thought of as}} System/32 with added [...] "hardware emulation" [...] of System/3.|$|E
5000|$|Other {{than the}} big job mechanism, UMMPS <b>storage,</b> <b>processor,</b> and I/O {{scheduling}} are independent, with each area allowed to [...] "take care of itself".|$|E
50|$|Each CX4 array {{consists}} of dual redundant hot-swappable components including <b>storage</b> <b>processors,</b> mirrored cache and battery backup, {{as well as}} redundant power supplies. The CX4 series supports Fibre Channel and iSCSI host connectivity.|$|R
50|$|In {{a modern}} {{enterprise}} architecture disk array controllers (sometimes also called <b>storage</b> <b>processors,</b> or SPs) {{are parts of}} physically independent enclosures, such as disk arrays placed in a storage area network (SAN) or network-attached storage (NAS) servers.|$|R
50|$|The Processor Products group {{designs and}} markets {{embedded}} microcontrollers {{as well as}} server <b>processor,</b> packet and <b>storage</b> <b>processors.</b> It includes the network processors of former MMC Networks (acquired October 2000) with IBM PowerPC 4xx series microcontrollers (acquired April 2004).|$|R
50|$|S/34s had two processors, the Control <b>Storage</b> <b>Processor</b> (CSP), and the Main <b>Storage</b> <b>Processor</b> (MSP). The MSP was the workhorse, {{based on}} System/3 architecture; it {{performed}} {{the instructions in}} the computer programs. The CSP was the governor, a different processor with different semi-RISC instruction set, based on System/32 architecture; it performed system functions in the background. Special utility programs {{were able to make}} direct calls to the CSP to perform certain functions; these are usually system programs like $CNFIG which was used to configure the computer system. These two processors worked in tandem.|$|E
50|$|S/36s had two sixteen-bit processors, the CSP or Control <b>Storage</b> <b>Processor,</b> and the MSP or Main <b>Storage</b> <b>Processor.</b> The MSP was the workhorse; it {{performed}} {{the instructions in}} the computer programs. The CSP was the governor; it performed system functions in the background. Special utility programs {{were able to make}} direct calls to the CSP to perform certain functions; these are usually system programs like $CNFIG which was used to configure the computer system. These two processors worked in tandem, and it's one reason the S/36 worked so well.|$|E
50|$|The CX4 UltraFlex series {{contains}} multiple models which {{differ in}} {{the maximum number of}} disks (SATA or Fibre Channel) and the number of iSCSI and FC connections. The PCI Express connection between the FC interface and the <b>storage</b> <b>processor</b> allows transfer speeds of up to 4 Gbit/s, while iSCSI supports speeds of max. 1 Gbit/s. All current models support RAID 0, 1, 1/0, 3, 5, and 6; as with the CX series, groups with differing RAID levels can be created.|$|E
50|$|The four {{models of}} the 360/85 are: I85 (512K), J85 (1M), K85 (2M) and L85 (4M), {{configured}} with 2 2365 <b>Processor</b> <b>Storage</b> units, 4 2365 units, an IBM 2385 <b>Processor</b> <b>Storage</b> unit Model 1 (=2M), or an IBM 2385 <b>Processor</b> <b>Storage</b> unit Model 2 (=4M) respectively. The I85 includes two-way interleaved memory while the others provide four-way interleaving of memory access.|$|R
40|$|An {{algorithm}} is described which guarantees reliable {{storage of data}} in a distributed system, even when different portions of the data base, stored on separate machines, are updated {{as part of a}} single transaction. The {{algorithm is}} implemented by a hierarchy of rather simple abstractions, and it works properly regardless of crashes of the client or servers. Some care is taken to state precisely the assumptions about the physical components of the system (<b>storage,</b> <b>processors</b> and communication) ...|$|R
3000|$|... a The NX bit, {{which stands}} for Never eXecute, is a {{technology}} used in CPUs to segregate areas of memory for use by either <b>storage</b> of <b>processor</b> instructions (or code) or for storage of data [...]...|$|R
5000|$|The Clariion {{is built}} on an Intel {{platform}} and has quite unique software layer: it runs two operating environments in parallel. Windows XP Embedded or stripped-down version of Windows Server for management and maintenance tasks and proprietary UNIX-based FLARE as an actual [...] "data mover". Embedded Windows (in the fourth generation this is 64-bit Windows Storage Server, the third generation used 32-bit Windows XP). The form factor is a half-width 1U or 2U device known as an X-blade, {{two of which are}} mounted {{side by side in the}} <b>storage</b> <b>processor</b> enclosure. This provides a fully redundant active-active configuration, with both storage processors serving requests and each acting as failover for the other so that initiators see the array as active-passive. An integrated UPS provides security for data in the event of power failure.|$|E
5000|$|In 1972, Walter Fiers and {{his team}} at the University of Ghent {{were the first to}} {{determine}} the sequence of a gene: the gene for bacteriophage MS2 coat protein. Richard J. Roberts and Phillip Sharp discovered in 1977 that genes can be split into segments. This led to the idea that one gene can make several proteins. The successful sequencing of many organisms' genomes has complicated the molecular definition of genes. In particular, genes do not seem to sit side by side on DNA like discrete beads. Instead, regions of the DNA producing distinct proteins may overlap, so that the idea emerges that [...] "genes are one long continuum". It was first hypothesized in 1986 by Walter Gilbert that neither DNA nor protein would be required in such a primitive system as that of a very early stage of the earth if RNA could perform as simply a catalyst and genetic information <b>storage</b> <b>processor.</b>|$|E
30|$|Ring-fencing of {{each slice}} {{operational}} resources (e.g., <b>storage,</b> <b>processor,</b> operational memory), {{so that one}} slice cannot exhaust other slice’s resources in any situation.|$|E
5000|$|Partitioning of the <b>processors,</b> <b>processor</b> <b>storage,</b> and I/O {{channels}} in a duplex configuration into two separate subsystems ...|$|R
5000|$|Floating Addressing {{to allow}} <b>processor</b> <b>storage</b> in a {{partitioned}} duplex configuration {{to be assigned}} consecutive real memory addresses ...|$|R
5000|$|The {{processor}} {{was stated}} {{to run at}} 1.3 GHz, but on watches with 1GB RAM and 8GB <b>storage,</b> the <b>processor</b> only runs at 1 GHz. Omate later explained that at full speed the heat was not bearable and therefore they reduced the clock speed ...|$|R
40|$|Platform-independent {{source code}} transformations can greatly help {{alleviate}} the data-transfer and storage bottleneck. This article covers global data-flow, loop, and data-reuse-related transformations, and discusses {{their effect on}} data transfer and <b>storage,</b> <b>processor</b> partitioning, and parallelization. The data-transfer and storage bottleneck in modern processor architectures {{is a very important}} and timely problem, as discussed in another article in this issue (“Random-Access Dat...|$|E
40|$|Hierarchical {{temporal}} memory (HTM) is a biomimetic {{machine learning}} algorithm focused upon modeling the structural and algorithmic properties of the neocortex. It is comprised of two components, realizing pattern recognition of spatial and temporal data, respectively. HTM research has gained momentum in recent years, leading to both hardware and software exploration of its algorithmic formulation. Previous work on HTM has centered on addressing performance concerns; however, the memory-bound operation of HTM presents significant challenges to scalability. In this work, a scalable flash-based <b>storage</b> <b>processor</b> unit, Flash-HTM (FHTM), is presented along with {{a detailed analysis of}} its potential scalability. FHTM leverages SSD flash technology to implement the HTM cortical learning algorithm spatial pooler. The ability for FHTM to scale with increasing model complexity is addressed with respect to design footprint, memory organization, and power efficiency. Additionally, a mathematical model of the hardware is evaluated against the MNIST dataset, yielding 91. 98 % classification accuracy. A fully custom layout is developed to validate the design in a TSMC 180 nm process. The area and power footprints of the spatial pooler are 30. 538 mm 2 and 5. 171 mW, respectively. <b>Storage</b> <b>processor</b> units have the potential to be viable platforms to support implementations of HTM at scale...|$|E
40|$|With rapid {{advances}} in sensor, <b>storage,</b> <b>processor,</b> and communication technologies, consumers can now afford to create, store, process, and share large digital photo collections. With {{more and more}} digital photos accumulated, consumers need effective and efficient tools to organize and access photos in a semantically meaningful way without too much manual annotation effort. From user studies, we confirm that users prefer to organize and access photos along semantic axes such as Event, People, Time, and Place. In this paper, we propose a computational learning framework to construct Event Models from sample photos with event labels given by a user and to compute relevance measures of unlabeled photos to the Event Models. We demonstrate event-based retrieval on 2400 genuine home photos using our proposed approach. 1...|$|E
40|$|In {{this paper}} we propose a new {{paradigm}} and algorithms to address cache writeback performance in file systems, servers and storage arrays. As servers and <b>storage</b> <b>processors</b> move to multi-core architecture, with ever increasing memory caches, the cost of flushing these caches to disk has become a problem. The paper introduces new algorithms to change the application data writeback from using watermark or aging based flush to something that approximates the rate of the incoming application I/Os. Our proposed algorithms are applicable to local file systems and remote servers and prove that rate based cache writeback algorithms are the most efficient replacement for watermark and aging based flushing. We are in process to apply the new algorithms to Linux file systems. 1...|$|R
40|$|Auspex builds fast NFS {{file servers}} {{designed}} {{to satisfy the}} I/O demands of large networks and high-performance workstations. The architecture handles NFS operations quickly and efficiently by completely eliminating Unix from the normal path of NFS service. We designed a message passing kernel that allows a slightly modified Unix kernel to execute as a peer processor with Ethernet processors, filesystem <b>processors,</b> and disk <b>storage</b> <b>processors.</b> These non-Unix processors respond efficiently to NFS requests and perform IP packet routing. A separate host processor running SunOS 4. 0 provides full Unix compatibility by servicing less time critical and less frequent requests such as Yellow Pages. Our message passing kernel is small (15 kbytes of object) and fast (10, 000 messages per second into a Motorola 68020) and provides source code debugging for all processors...|$|R
40|$|Modern <b>storage</b> and <b>processor</b> {{technology}} {{makes the}} accumulation of data increasingly easy. Prediction and data analysis tools must be developed and adapted rapidly {{to keep up with}} the dramatic growth in data volume. Data mining pursues the goal of extracting information from large databases with consideration for the storage structure...|$|R
40|$|For a {{very long}} time now I have been quite {{interested}} in having my personal web server. Senior project gave me the perfect opportunity to explore and to custom design web server that would fit my future storage/web needs. There are multiple aspects of the project that I worked on to custom design the web server. The list and brief description of each of these categories is given bellow. 1. 	Ftp Options: There are several ftp options available, so I researched these options and found the client service that would work best with my requirements. 2. 	Number of Users: One of the critical aspects of the server construction requirement is that each user should have access to sufficient amount of <b>storage,</b> <b>processor</b> power, and memory allocated to have flawless usage. Therefore, I would need to estimate my data sharing needs which would determine the type of processor and amount RAM. Hard drive space would also be dependent on this. 3. 	Security Issues: This {{is also one of the}} major parts of the project to determine the system environment for secure data sharing. 4. 	Required Storage: I already have multiple relatively large hard drives that I have been using for quite some time now, but it seems that with growing trends in technology gigabytes are turning into megabytes. Therefore I would want to start with large storage up front, and still have options to increase storage in future. 5. 	Required Speed: Speed is quite critical for large data sharing, so I would need to research and implement solution that would be relatively high speed enabled. 6. 	Power Efficiency: Server by nature is going to be run all the time, so I would want to have system that is power efficient. 7. 	Minimum specification: I would also need to set minimum system specification, so that I can have longer time before next hardware upgrade needed. 8. 	Redundant Components: It is very common to have redundant system components in server system such as power supplies, and ethernet ports. These components would provide uninterrupted access even if one of them fails. I would need to find components that would best fit my needs. 9. 	Operating System Selection: There are many options available for OS ranging from Windows, Redhat, to open linux. Each of these have different functions, and so I would need to closely study them to find appropriate OS for my system. 10. 	Hard Drive Raid Options: There are several different types of hard drive raid options available, some are hardware based and some are software based. I would need to evaluate these options to find appropriate option for me. 11. 	Cost and Benefit Analysis: Last but not least, I would need to do cost and benefit analysis to see how much I am getting for the money I spend. Is it beneficial to choose the highest performance system? Or is it okay to go with second or third best in market? This way I would get most for the money I spend...|$|E
50|$|There {{were four}} {{models of the}} IBM System/360 Model 91. They {{differed}} by their main memory configuration, all using IBM's 2395 <b>Processor</b> <b>Storage.</b>|$|R
50|$|The IBM 2395 <b>Processor</b> <b>Storage</b> is {{a memory}} storage unit {{that was a}} {{component}} of the IBM System/360 Model 91 and Model 95.|$|R
50|$|Major New Features in the CX4 UltraFlex Series include {{support for}} solid state flash drives, 64-bit FLARE Operating Environment and PCI-Express based SLIC I/O cards. The CX4-480 and CX-960 support fibre channel solid state flash drives {{introduced}} by EMC for the Symmetrix DMX-4 in January 2008. Solid State Flash drives offer a significant performance advantage over mechanical drives {{and provide a}} new storage tier which EMC calls Tier-0. The {{new version of the}} FLARE operating environment that ships with the CX4 series includes support for the 64-bit Intel Xeon CPUs in the <b>Storage</b> <b>Processors.</b> The CX4 Series is also the first product to use the new PCI Express based SLIC I/O cards. The product-agnostic SLIC I/O cards provide more flexibility for future upgrades as new technologies become available, such as 8GBit Fibre Channel and 10GBit Ethernet/iSCSI.|$|R
50|$|The NX bit, {{which stands}} for No-eXecute, is a {{technology}} used in CPUs to segregate areas of memory for use by either <b>storage</b> of <b>processor</b> instructions (code) or for storage of data, a feature normally only found in Harvard architecture processors. However, the NX bit is being increasingly used in conventional von Neumann architecture processors, for security reasons.|$|R
50|$|The {{origin of}} the shmoo plot is unclear. It is {{referenced}} in a 1966 IEEE paper. Another early reference is in manuals for IBM 2365 <b>Processor</b> <b>Storage.</b>|$|R
50|$|These components, {{together}} with the 2365 <b>Processor</b> <b>Storage</b> Model 2, 2860 Selector Channel, 2870 Multiplexer Channel, and other System/360 control units and devices were available for use with the S/360-67.|$|R
50|$|The IBM 2365 <b>Processor</b> <b>Storage</b> is a magnetic-core {{memory storage}} unit {{that was a}} {{component}} of the IBM System/360 models 65, 67, 75 and 85 computers, which were released between 1965 and 1968.|$|R
40|$|Cloud {{computing}} is {{an upcoming}} efficient networking computing environment. Our work proposes sharing an application across various clients {{of a host}} system via cloud computing. The sharing is performed between the host storage system and the remote storage system in a cloud computing environment. The Host storage device of the host system is updated to retrieve the application from the remote storage device that is shared through cloud computing environment. The application now can be accessed by the host processing devices. The application can be implemented on the host processing device without requiring any installation of the application on the host processing device. A bunch of function calls are performed between the operating system of the host processing device and the application residing on the host storage device. The system has a cloud interface between remote and host machines to update the host storage device about any changes in the application at the remote storage device. This leads to the better utilization of the cloud bandwidth and the host resources such as <b>storage,</b> <b>processors</b> etc...|$|R
5000|$|Simplex - one IBM 2067-1 processor, {{two to four}} IBM 2365-2 <b>Processor</b> <b>Storage</b> {{components}} (512K to 1M bytes), up {{to seven}} data channels, and other peripherals. This system was called the IBM System/360 model 67-1.|$|R
50|$|It uses a {{customized}} application processor that together with memory, <b>storage</b> and support <b>processors</b> for wireless connectivity, sensors and I/O constitute a complete computer {{in a single}} package. This package is filled with resin for durability.|$|R
5000|$|The IBM System/360 Model 85, when {{configured}} to have 2,097,152 (2 MB) or 4,194,304 (4 MB) bytes, {{used the}} IBM 2385 {{instead of the}} IBM 2365 <b>Processor</b> <b>Storage,</b> which had a cycle time of 960 nanoseconds ...|$|R
