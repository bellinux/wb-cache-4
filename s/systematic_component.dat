212|305|Public
5000|$|Theoretical work of Mella {{served as}} point of {{reference}} for generations and was studied far beyond Spain, from Chile or the United States to Poland. However, it is universally approached as intrinsic part of Traditionalist doctrine, not infrequently presented as its most refined, in-depth, and <b>systematic</b> <b>component,</b> in fact the climax of Traditionalist political philosophy. The term [...] "Mellismo" [...] is not applied to it, used only as reference to political strategy pursued by Vázquez de Mella and his followers; as such, it generated far lesser interest. In historiography until the late 20th century the Mellistas were acknowledged mostly in works dealing with different dimensions of Carlism. The authors {{tended to focus on}} the 1919 breakup, sometimes portrayed as another one in long history of ruptures in the movement; the secession was presented as resulting either from clash of personalities or from conflicting views on Spanish stand during the First World War. It was the first major monograph, published in 2000, which systematically re-defined Mellismo as a strategy to build an ultra-Right formation leading the transition from liberal democracy of late Restauración to corporative Traditionalist monarchy. According to this theory, the grouping envisaged was supposed to consist of three tiers: complete amalgamation based on common program, federation with those who accepted it partially, and circumstantial co-operation with other groups.|$|E
40|$|SUMMARY The variabilities of device {{characteristics}} are usually {{regarded as a}} normal distribution. If we consider the variabilities over the whole wafer, however, they cannot be expressed as a normal distribution due {{to the existence of}} global <b>systematic</b> <b>component.</b> We propose a statistical model, characterizing the global <b>systematic</b> <b>component</b> according to the distance {{from the center of the}} wafer, which can express the variabilities over the whole wafer statistically. key words: MOSFET, variability, systematic, stochastic 1...|$|E
40|$|We {{consider}} {{a pool of}} bank loans subject to a credit risk and develop a method for decomposing the credit risk into idiosyncratic and systematic components. The <b>systematic</b> <b>component</b> accounts for the aggregate statistical difference between credit defaults in a given period and the long-run average of these defaults. We show how financial contracts might be redesigned to allow for banks to manage the idiosyncratic component for their own accounts, while allowing the <b>systematic</b> <b>component</b> to be handled separately. The <b>systematic</b> <b>component</b> can be retained, passed off to the capital markets, or shared with the borrower. In the latter case, we introduce a type of floating rate interest, in which the rate is set in arrears, based on a composite index for the systematic risk. This is shown {{to increase the efficiency}} of risk sharing between borrowers, lenders and the capital market...|$|E
40|$|Abstract- A new {{approach}} to upgrading the type A uncertainty evaluation by investigation the trend and periodical <b>systematic</b> <b>components</b> in the regularly in time performed observations {{is presented in the}} paper. This is a part of best practice, which authors recommend as upgrading the routine procedures in uncertainty evaluation according to ISO GUM recommendation. The cleaning of raw data set by elimination the <b>systematic</b> <b>components</b> and the influence of these cleanings on standard uncertainty is presented and disused. Two numerical examples are discussed. Key words: uncertainty type A, unknown <b>systematic</b> <b>components,</b> trend, harmonic disturbance...|$|R
40|$|When averages of {{different}} experimental determinations {{of the same}} quantity are computed, each with statistical and <b>systematic</b> error <b>components,</b> then frequently the statistical and <b>systematic</b> <b>components</b> of the combined error are quoted explicitly. These are important pieces of information since statistical errors scale differently and often more favorably with the sample size than most systematical or theoretical errors. In this communication we describe a transparent procedure by which the statistical and <b>systematic</b> error <b>components</b> of the combination uncertainty can be obtained. We develop a general method and derive a general formula for the case of Gaussian errors with or without correlations. The method can easily be applied to other error distributions, as well. For the case of two measurements, we also define disparity and misalignment angles, and discuss their relation to the combination weight factors. Comment: 11 page...|$|R
40|$|International audienceThe {{authorities}} {{of the standards}} organization International Organization of Standardization (ISO) advocate mastering any uncertainties {{in all parts of}} the industrialization process. In the three-dimensional (3 D) measurement process, uncertainty is usually obtained at the end of a battery of tests. It is defined as a whole because it includes several types of errors, known <b>systematic</b> <b>components,</b> unknown <b>systematic</b> <b>components</b> and random components. Automated calculations of uncertainty can be made based on statistics. This method is based on statistical concepts, which are in accordance with " The Guide to the expression of the uncertainty in measurement " (GUM). It also enables us to generate uncertainties on the verification of ISO specifications (or specs in the ISO directives). In the course of this article, a usage will be presented that takes the knowledge of uncertainties into account: this usage will help the operator to take a decision on the conformance of a mechanical part in reference to its conformance to geometric tolerance...|$|R
40|$|In this paper, we {{identify}} monetary policy shocks in structural vector autoregressions (SVARs) by imposing sign and zero {{restrictions on the}} <b>systematic</b> <b>component</b> of monetary policy while leaving the remaining equations in the system unrestricted. As in Uhlig (2005), no restrictions are imposed on the response of output to a monetary policy shock. We find that an exogenous increase in the federal funds rate leads to a persistent decline in output and prices. Our {{results show that the}} contractionary effects of monetary policy shocks do not hinge on questionable exclusion restrictions, but are instead consistent with agnostic identification schemes. The anal-ysis is robust to various specifications of the <b>systematic</b> <b>component</b> of monetary policy widely used in the literature...|$|E
40|$|Abstract: This paper {{assesses the}} {{implications}} for optimal discretionary monetary policy if {{the slope of the}} Phillips curve changes. The paper first derives a Phillips curve from the optimal pricing decision of a monopolistic firm that faces a changing cost of price adjustment. The second aspect of the paper constructs a utility-based welfare criterion. A novel feature of this criterion is that is has a relative weight on output gap deviations that changes synchronously with changes in the cost of price adjustment. The <b>systematic</b> <b>component</b> of the targeting rule that implements the optimal discretionary policy under the utility-based criteria is constant. In contrast, the <b>systematic</b> <b>component</b> of the targeting rule under an ad-hoc criteria changes along with changes in the slope of the Phillips curve...|$|E
40|$|In {{an article}} that {{recently}} appeared in this journal, Marshall (2015) argued that the <b>systematic</b> <b>component</b> of the SD of a stock or of a portfolio of stocks is its beta scaled by the SD of the market returns. She also contended that the beta mispredicts the actual systematic risk of a stock or of a portfolio of stocks. In this article, I dispute this conclusion, showing {{that it has been}} induced by an imperfection {{in the construction of the}} empirical application and by some misinterpretations of the results. A corrected replication of the empirical study of Marshall (2015) is provided, along with some comments. I conclude that both the beta and the <b>systematic</b> <b>component</b> inMarshall (2015) are effective measures of systematic risk...|$|E
40|$|Process {{variability}} in deeply scaled CMOS has both random and <b>systematic</b> <b>components,</b> with a varying degree of spatial correlation. A test chip {{has been built}} to {{study the effects of}} circuit layout on variability of delay and power dissipation in 90 nm CMOS. The delay is characterized through the spread of ring oscillator frequencies and the transistor leakage is measured by using an on-chip ADC. 1...|$|R
40|$|The <b>systematic</b> dodecapole <b>component</b> in the Main Quadrupoles of the LHC lattice has {{a strong}} {{influence}} on the machine dynamic aperture at injection. In this paper we quantify this effect {{with the help of}} tracking studies, explain the mechanism for the loss in dynamic aperture and look into potential correction schemes. Finally, we provide an estimate for the maximum allowed <b>systematic</b> dodecapole <b>component</b> in the MQ. ...|$|R
40|$|Abstract: Recent work on {{designs of}} dipoles, quadrupoles and sextupoles for the NLC Main Damping Ring {{has led to}} {{estimates}} of the <b>systematic</b> multipole <b>components</b> {{in the fields of}} these magnets. We report on studies of the effects of these multipoles on the dynamic aperture of the damping ring, and show that the <b>systematic</b> multipole <b>components</b> in the present magnet designs are unlikely to be a severe limitation. LCC- 010...|$|R
40|$|Data {{assimilation}} methods rely {{on numerous}} {{assumptions about the}} errors involved in measuring and forecasting atmospheric fields. One of the more disturbing of these is that short-term model forecasts {{are assumed to be}} unbiased. In case of atmospheric moisture, for example, observational evidence shows that the <b>systematic</b> <b>component</b> of errors in forecasts and analyses is often of the same order of magnitude as the random component. we have implemented a sequential algorithm for estimating forecast moisture bias from rawinsonde data in the Goddard Earth Observing System Data Assimilation System (GEOS DAS). The algorithm is designed to remove the <b>systematic</b> <b>component</b> of analysis errors and can be easily incorporated in an existing statistical data assimilation system. We will present results of initial experiments that show a significant reduction of bias in the GEOS DAS moisture analyses...|$|E
30|$|The utility {{function}} includes a <b>systematic</b> <b>component,</b> which {{takes the form}} of a quadratic polynomial expansion in disposable household income, leisure of the husband and leisure of the wife, and a random taste-shifter which is identically and independently distributed across households, sectors and jobs according to an extreme value distribution.|$|E
3000|$|... 12 This {{functional}} form can {{be justified}} as {{the outcome of a}} random utility maximization model, where Yj* = Uj (Xi) + εj, Uj represents the <b>systematic</b> <b>component</b> and εj the random component of utility, and the εj are distributed independently and identically according to the type 1 extreme-value distribution. See Maddala ([...] [...]...|$|E
40|$|Four B + →J/ψpΛ̄ {{candidates}} in a data set of (88. 9 ± 1. 0) X 10 6 Y(4 S) decays were observed. The probability {{of the expected}} charged B background 0. 21 ± 0. 14 events, producing ≥ 4 events was 2. 5 X 10 - 4. The branching fraction is (12 - 6 + 9) X 10 - 6, where the uncertainty includes both statistical and <b>systematic</b> <b>components...</b>|$|R
40|$|The article {{present the}} tendencies in {{scientific}} pedagogical research of profes-sional further {{training of staff}} {{in the area of}} correctional pedagogics and considers the problems of current interest in social and regulative function of refresher courses, as well as scientific grounds of common and specific <b>systematic</b> <b>components</b> of staff teaching in defectology. The problems of expansion in the sphere of further education and curricula modifications are also touched upon...|$|R
40|$|In {{order to}} {{determine}} if a statistically significant difference exists between shipper and receiver measurements, a statistical combination of the shipper's and receiver's limit-of-error (LOE) is calculated to determine the shipper/receiver limit-of-error, LOES/R. The shipper's and receiver's LOE may possess random and <b>systematic</b> <b>components.</b> Depending on the interpretation of the <b>systematic</b> and random <b>components,</b> the determination of the LOES/R can be performed by several different calculational methods. These calculational methods and their associated underlying assumptions are reviewed {{in the context of the}} LANL shipper receiver program. This paper, by presenting the assumptions that form the basis of a site-specific shipper/receiver difference calculation, can assist those individuals responsible for calculating LOES/R...|$|R
40|$|Credit spreads may be jointly {{driven by}} {{developments}} that are orthogonal {{to the current}} state of the economy. We show that this unobserved <b>systematic</b> <b>component</b> is demanded to hedge against adverse economic fl?uctuations. Using either yield-to-maturity spreads or asset swap spreads for 2345 Eurobonds across euro area non-fi?nancial industries, we estimate a market-wide relative excess bond premium - a function of the unobserved <b>systematic</b> <b>component</b> -, which can predict real economic activity, the stock market and survey-based economic sentiment. This premium was highly negative between March 2003 and June 2007 in all bond segments and turned positive since then up to the launch of the 3 -years long term refinancing operations in December 2011, predicting the financial crisis and the two recessions. Finally, using the countries?excess bond premia, we fi?nd that fragmentation risk increased sharply after Lehman?s bankruptcy and during the sovereign debt crisis...|$|E
30|$|Silverman et al. (1997) {{argue that}} firm {{mortality}} {{is influenced by}} not only firm-specific microeconomic contexts, but also industry-level attributes. Default risk includes a <b>systematic</b> <b>component</b> from a conceptual perspective, aside from company-specific idiosyncratic risk, which is contingent on macroeconomic factors and causes spillover effects as well as industry-specific yet time-varying risk factors (Giesecke and Kim 2011; Vassalou and Xing 2004).|$|E
30|$|The {{intersection}} of GPS-tracked grazing routes with digital land cover data proved very useful to quantify patterns {{of land use}} and herd movement. In view of the utility of such information for inference on rangeland health and livestock nutrition as well as spatio-temporal modelling of livestock distribution, this approach should be a <b>systematic</b> <b>component</b> of quantitative research on pastoral mobility.|$|E
5000|$|Petr Vaníček, a Canadian geodesist of the University of New Brunswick, also {{proposed}} the matching-pursuit approach, {{which he called}} [...] "successive spectral analysis" [...] and the result a [...] "least-squares periodogram", with equally and unequally spaced data, in 1969. He generalized this method to account for <b>systematic</b> <b>components</b> beyond a simple mean, such as a [...] "predicted linear (quadratic, exponential, ...) secular trend of unknown magnitude", and applied it {{to a variety of}} samples, in 1971.|$|R
40|$|The {{time-dependent}} CP-violating observables accessible through Bs->DsK decays {{have been}} measured {{for the first}} time using data corresponding to an integrated luminosity of 1 inverse fb collected in 2011 by the LHCb detector. Using these observables, the CKM angle gamma is determined to be (115 - 43 + 28) modulo 180 degrees at 68 % CL, where the uncertainty contains both statistical and <b>systematic</b> <b>components.</b> Comment: Proceedings of the 8 th International Workshop on the CKM Unitarity Triangle (CKM 2014), Vienna, Austria, September 8 - 12, 201...|$|R
40|$|We {{investigate}} {{price dispersion}} in a retail market (car tyres) characterised by outlets each selling {{a range of}} products, {{some of which are}} manufactured by their owners. Consumers face substantial price dispersion across outlets even for very tightly defined products. We show that this price dispersion has <b>systematic</b> <b>components</b> relating to retailer-manufacturer interactions. One specific result is that chains owned by manufacturers sell other manufacturers' tyres on average nearly 20 % more expensively than do independent stores and over 11 % more expensively than their own equivalent branded product...|$|R
40|$|We {{construct}} models {{describing the}} velocity {{field in the}} infall regions of clusters of galaxies. In all models the velocity field is the superposition of a radial <b>systematic</b> <b>component</b> which is assumed spherically symmetric, and a “noise ” component of random nature. The latter accounts for the combined effects of small-scale substructure and observational errors. The effect of the noise term is to smear out the caustic envelopes searched for by previous groups, resulting in models which resemble the observations quite well. When the <b>systematic</b> <b>component</b> is known, the infall velocity and the mass profile of the infall region of the clusters can be determined. Given a particular model, {{it is possible to}} calculate the distribution function, f(cz, θ, m), at all points in the observable (cz, θ, m) -phase-space outside the virialized region. It is demonstrated, on simulated data, that the use of a maximum-likelihood estimator enables identification of the correct model, even at realistic noise levels. To minimize systematic effects due to deviations fro...|$|E
40|$|We {{measure the}} {{long-term}} <b>systematic</b> <b>component</b> of the astrometric {{error in the}} GeMS MCAO system {{as a function of}} field radius and Ks magnitude. The experiment uses two epochs of observations of NGC 1851 separated by one month. The <b>systematic</b> <b>component</b> is estimated for each of three field of view cases (15 " radius, 30 " radius, and full field) and each of three distortion correction schemes: 8 DOF/chip + local distortion correction (LDC), 8 DOF/chip with no LDC, and 4 DOF/chip with no LDC. For bright, unsaturated stars with 13 < Ks < 16, the <b>systematic</b> <b>component</b> is < 0. 2, 0. 3, and 0. 4 mas, respectively, for the 15 " radius, 30 " radius, and full field cases, provided that an 8 DOF/chip distortion correction with LDC (for the full-field case) is used to correct distortions. An 8 DOF/chip distortion-correction model always outperforms a 4 DOF/chip model, at all field positions and magnitudes and for all field-of-view cases, indicating the presence of high-order distortion changes. Given the order of the models needed to correct these distortions (2 ̆ 23 c 8 DOF/chip or 32 degrees of freedom total), it is expected that at least 25 stars per square arcminute would be needed to keep systematic errors at less than 0. 3 milliarcseconds for multi-year programs. We also estimate the short-term astrometric precision of the newly upgraded Shane AO system with undithered M 92 observations. Using a 6 -parameter linear transformation to register images, the system delivers 2 ̆ 23 c 0. 3 mas astrometric error over short-term observations of 2 - 3 minutes. Peer reviewed: YesNRC publication: Ye...|$|E
40|$|Purpose – This paper aims {{to propose}} a new method for credit risk {{allocation}} among economic agents. Design/methodology/approach – The paper considers a pool of bank loans subject to a credit risk and develops a method for decomposing the credit risk into idiosyncratic and systematic components. The <b>systematic</b> <b>component</b> accounts for the aggregate statistical difference between credit defaults in a given period and the long-run average of these defaults. Findings – The paper shows how financial contracts might be redesigned to allow for banks to manage the idiosyncratic component for their own accounts, while allowing the <b>systematic</b> <b>component</b> to be handled separately. The <b>systematic</b> <b>component</b> can be retained, passed off to the capital markets, or shared with the borrower. In the latter case, the paper introduces a type of floating interest rate, in which the rate is set in arrears, based on a composite index for the systematic risk. This increases the efficiency of risk sharing between borrowers, lenders and the capital market. Practical implications – The paper has several practical implications that are of value for financial engineers, loan market participants, financial regulators, and all economic agents concerned with credit risk. It {{could lead to a}} new class of structured notes being traded in the market. Originality/value – The paper also illustrates the potential benefits of risk decomposition. Of course, as with any innovation, the implementation of the structured contracts would raise practical issues not addressed here. The paper also makes several simplifications: market risk is ignored; the level of default is constant and identical among borrowers. These simplifications could be lifted in future research on this theme. Banking, Capital markets, Contracts, Credit rating, Risk analysis...|$|E
40|$|This study {{assesses the}} {{effectiveness}} of using <b>systematic</b> <b>components</b> of cross-sectional forecast errors from prior years in order to adjust current analysts' earnings forecasts. The empirical results document that a significant component of the cross-sectional MSE in analysts' forecasts is systematic, and that parameter estimates from earlier periods enable the elimination of {{a substantial portion of}} the systematic errors in current forecasts. Further improvements in forecast accuracy are attained by the incorporation of prior-year excess security returns in order to reduce unsystematic error. forecasting, analysts' earnings forecasts, systematic errors...|$|R
40|$|As {{technology}} scales, understanding {{semiconductor manufacturing}} variation becomes essential to effectively design high performance circuits. Knowledge of process variation {{is important to}} optimize critical path delay, minimize clock skew, and reduce crosstalk noise. Conventional circuit techniques typically represent the interconnect and device parameter variations as random variables. However, {{recent studies have shown}} that strong spatial pattern dependencies exist, especially when considering interconnect variation in chemical mechanical polishing (CMP) processes. Therefore, the total variation can be separated into <b>systematic</b> and random <b>components,</b> where {{a significant portion of the}} variation can be modeled based on layout characteristics. Modeling the <b>systematic</b> <b>components</b> of different variation sources and implementing these effects in circuit simulation are key to reduce design uncertainty and maximize circuit performance. This thesis presents a methodology to incorporate systematic pattern dependent interconnec...|$|R
40|$|Asset pricing models {{assume that}} probabilities of future {{outcomes}} are known. In reality, however, there is ambiguity {{with regard to}} these probabilities. Accounting for ambiguity in asset pricing theory results in a model with two <b>systematic</b> <b>components,</b> beta risk and beta ambiguity. The focus {{of this paper is}} to study the empirical implications of ambiguity for the cross section of equity returns. We find that systematic ambiguity is an important determinant of equity returns. We also find that the Fama-French factors contribute to the explanatory power of the two main drivers of returns; namely, systematic risk and systematic ambiguity. ...|$|R
40|$|We {{develop a}} model-based, VAR {{methodology}} for measuring innovations in monetary policy and their macroeconomic effects. Using this framework, {{we are able}} to compare existing approaches to measuring monetary policy shocks and derive a new measure of policy innovations based directly on (possibly timevarying) estimates of the central bank’s operating procedures. We also propose a new measure of the overall stance of policy (including the endogenous or <b>systematic</b> <b>component)</b> that is consistent with our approach. I...|$|E
40|$|Revealed {{preference}} theory {{assumes that}} each consumer has demands that are rational, {{meaning that they}} arise from the maximization {{of his or her}} own utility function. In contrast, econometric or statistical demand models assume that each consumer's demands equal a rational <b>systematic</b> <b>component</b> derived from a common utility function, plus an individual-specific, additive error term. This paper reconciles these differences, by providing necessary and sufficient conditions for rationality of statistical demand models given individual consumer rationality. ...|$|E
40|$|AbstractAbility {{to detect}} leaks in the Strategic Petroleum Reserve's brine {{pipeline}} {{depends on the}} ability to observe small drops in pressure, e. g. changes of the order of 0. 3 psi (pounds per square inch). Typical pressure variation includes a random component (referred to as measurement noise) due primarily to measurement error and a <b>systematic</b> <b>component</b> (referred to as process noise) due to various internal and external disturbances such as offshore tides, temperature changes, and pump action. Much of the <b>systematic</b> <b>component</b> can be removed through time series modeling, with residuals from the model representing the random component. This paper addresses the estimation of the noise components through time series models applied to test data. Effectiveness of leak-detection algorithms based on test statistics (e. g. two-minute averages of pressure readings) can be determined from known or estimated standard deviations of the process and measurement noise components. The U. S. Department of Energy (operator of the Strategic Petroleum Reserve) plans to use the results of the time series analysis, together with hydraulic models, in order to establish leak-detection procedures that will meet Environmental Protection Agency requirements...|$|E
40|$|Myung, Kim and Pitt (2000) {{demonstrated}} that simple power functions almost always {{provide a better}} fit than simple exponential functions to purely random data. This result has important implications because it suggests that high noise levels, which are common in psychological, may cause a bias favouring power functions. We replicate their result, and extend it by showing strong bias for realistic sample sizes. We also show that biases occur for data that contain both random and <b>systematic</b> <b>components,</b> as may be expected in real data. We then demonstrate that these biases disappear for two- or three- parameter functions that include linear parameters (in at least parameterisation) ...|$|R
40|$|International audienceA {{level with}} fern stumps was {{discovered}} in the Aptian Douiret Formation, South-Tunisia. These stumps are preserved as external moulds or casts, without any preservation of anatomical structures. These stumps {{are considered to be}} affiliated with the numerous fossil plants assigned to the fern genera Alstaettia and Piazopteris that are widely distributed in coeval strata from the same region, either as leaf imprints or as permineralized remains. The record of in situ fossil forests for the Southern Hemisphere reveals that their <b>systematic</b> <b>components</b> are different, i. e. mainly corystosperms and/or conifers, and rarely under tidal influence. The way this fern grove settled in a margino-littoral environment is discusse...|$|R
40|$|This paper {{reports the}} result of an {{analysis}} of wind tunnel data acquired in support of the Facility Analysis Verification & Operational Reliability (FAVOR) project. The analysis uses methods referred to collectively at Langley Research Center as the Modern Design of Experiments (MDOE). These methods quantify the total variance in a sample of wind tunnel data and partition it into explained and unexplained components. The unexplained component is further partitioned in random and <b>systematic</b> <b>components.</b> This analysis was performed on data acquired in similar wind tunnel tests executed in four different U. S. transonic facilities. The measurement environment of each facility was quantified and compared...|$|R
