4|42|Public
5000|$|Canonical started Project Harmony [...] "...to assist organisations {{which use}} {{contribution}} agreements by providing <b>standardised</b> <b>variable</b> templates with clear and concise explanations...." ...|$|E
40|$|Slovak medical spas {{are part}} of the public health care system. The {{financial}} stability of these spas is based on the balance between medical products covered by public health insurance and medical and wellness products reimbursed by patients. The aim of this research note is the analysis of prices and the ensuing current financial situation of health spas in Slovakia. By way of a case study, an analysis has been made of the amounts paid by Všeobecná zdravotná poisťovňa, j. s. c., for institutional spa care between 2013 and 2016. Besides this, the research note states their financial rankings using the <b>standardised</b> <b>variable</b> method based on three ratios – the Return on Assets, the Revenue Growth Rate and the Net Profit Ratio. The stagnant amount of payments is one of the factors causing some of the spa facilities to go into the red within the monitored ratios...|$|E
30|$|Unlike MDR, a {{bootstrap}} aggregating or bagging approach {{provides a}} means to determine the prediction accuracy of RF. Given a (case-control) sample set, a bootstrap sample set with the size equals to original sample set is generated by sampling from the original sample set uniformly and with replacement. It is expected that 63.2 % of bootstrap samples are unique while the remaining samples are duplicates. Original samples that are absent from the bootstrap sample set {{are referred to as}} out-of-bag samples. Bootstrap samples are employed during the tree construction while out-of-bag samples are used to evaluate the prediction accuracy. A new bootstrap sample set is generated for the construction of each tree. As a result, the votes are only counted across the trees that the sample is out-of-bag during the prediction accuracy evaluation. The application of a bootstrap aggregating approach also leads to a means to quantify attribute importance, which is commonly referred to as variable importance. The variable importance is measured using a permutation approach. By randomly permuting the value of the attribute of interest, the correlation between the attribute and the (case-control) class can be determined. When the permuted attribute and the remaining non-permuted attributes are used as inputs for RF to identify the class of out-of-bag samples, the prediction accuracy reduces markedly if the attribute of interest is correlated with the class. The average difference between the prediction accuracy obtained using the original attribute inputs and that obtained using the inputs with one permuted attribute over the trees is the variable importance. The <b>standardised</b> <b>variable</b> importance is defined as the quotient between the variable importance and a standard error derived from the between-tree variance of the variable importance. In other words, the <b>standardised</b> <b>variable</b> importance follows a standard normal distribution (Random Forests 2004). An attribute with variable importance in the top five percentiles of a normal distribution is considered to be in a top rank in comparison to other attributes and is hence correlated with the class. This decision criterion is similar to the one based on the extremity of variable importance suggested by Strobl et al. (2009). RF used in this study is publicly available from the Department of Statistics, University of California, Berkeley (Random Forests 2004). A review of RF for genetic association studies can be found in Goldstein et al. (2011). Interested readers should also refer to Schwarz et al. (2010) and Wei et al. (2013) for RF-based techniques that are computationally feasible for genome-wide association studies.|$|E
40|$|The partial {{concentration}} index (PCI) {{is commonly used}} {{as a measure of}} income related inequality in health after removing the effects of <b>standardising</b> <b>variables</b> such as age and gender which affect health, are correlated with income, but not amenable to policy. Both direct and indirect standardisation have been used to remove the effects of <b>standardising</b> <b>variables.</b> The paper shows that with individual level data direct standardisation is possible using the coefficients from a linear regression of health on income and the <b>standardising</b> <b>variables</b> and yields a consistent estimate of the PCI. Indirect standardisation estimates the effects of the <b>standardising</b> <b>variables</b> on health from a health regression which excludes income. The coefficients on the <b>standardising</b> <b>variables</b> include some of the effects of income on health if income is correlated with the <b>standardising</b> <b>variables.</b> Using these coefficients to remove the effects of the <b>standardising</b> <b>variables</b> also removes some of the effect of income on health and leads to an inconsistent estimate of the PCI. Indirect standardisation underestimates the PCI irrespective of the signs of the correlations of <b>standardising</b> <b>variables</b> and income {{with each other and with}} health. An adaptation of the PCI when the marginal effect of income on health depends on the <b>standardising</b> <b>variables</b> is also proposed. Copyright © 2003 John Wiley & Sons, Ltd. ...|$|R
40|$|The partial {{concentration}} index measures income related inequality in health (or health care) after removing {{the effects of}} <b>standardising</b> <b>variables</b> which affect health (or health care), are correlated with income but not amenable to policy. When the marginal effects of income are independent of the <b>standardising</b> <b>variables,</b> direct standardisation yields consistent estimates of the {{partial concentration}} index. Indirect standardisation underestimates the partial concentration index whenever the <b>standardising</b> <b>variables</b> are correlated with income, irrespective of {{the signs of the}} correlation of <b>standardising</b> <b>variables</b> and income {{with each other and with}} health. A generalised version of the partial concentration index is proposed for cases where the marginal effect of income depends on the <b>standardising</b> <b>variables.</b> Direct standardisation again yields a consistent estimate but indirect standardisation does not. It is also shown that the direct standardisation procedure can be applied to individual or grouped data and that the conclusions about the merits of direct and indirect standardisation hold for grouped data. Concentration index, inequality, direct standardisation. ...|$|R
50|$|This result {{agrees with}} that derived using <b>standardised</b> <b>variables.</b>|$|R
30|$|The dataset is {{organised}} as {{a cross-section}} of student-university combinations. This generates {{a large number of}} observations, 306, 580 (the sum of the universities available to every single student). The dependent variable of the empirical model is a dummy variable which takes the value 1 for combination ij if university j has been indicated by student i among her favourite 6 destinations; and 0 otherwise (this variable is called C). For each student, the rank in the IRO ranking is known. This variable is standardised by dividing it {{by the total number of}} applicants in the respective application session; the new, <b>standardised</b> <b>variable</b> is called relrank. A set of university-specific variables (“strategic characteristics”), which depend on the relationship between the partner institution and SBE, was collected from the IRO files. These variables are the number of available places that the partner institution makes available to SBE students (avp), a variable indicating whether the GPA of the student is insufficient for admission at the host institution 1 (labelled graderequisite), and whether or not a partner appears for the first time on the list of partners (new – this variable has been generated because new partners may be chosen less frequently because students did not have the opportunity to learn from the past experience of friends and colleagues).|$|E
3000|$|... 1 Also {{referred}} as conversion to z-scores, z-values, standard scores, normal scores, <b>standardised</b> <b>variables.</b>|$|R
30|$|The three value axes are <b>{{standard}}ised</b> <b>variables</b> (mean =  0 {{and standard}} deviation =  1). The French-speaking Swiss have 0.28 standard deviation {{more than the}} German-speaking Swiss, which is a high score.|$|R
40|$|We {{consider}} {{a set of}} n individuals described by p <b>standardised</b> <b>variables,</b> and we suppose that the individuals are previously selected from a population and the variables are a sample of variables assumed {{to come from a}} mixture of k bipolar Watson distributions defined on the hypersphere. In this context we provide the identification of the mixture through the EM algorithm and we also carry out a simulation study to compare the maximum likelihood estimates obtained from samples of moderate size with the respective asymptotic estimates. Our simulation results revealed good performance of the EM algorithm for moderate sample sizes. Key-Words...|$|R
50|$|Sharpened bounds can {{be derived}} by first <b>standardising</b> the random <b>variable.</b>|$|R
40|$|This no"te {{provides}} a concise summary or the au"thor's work en"titJed "Problems concerning "the variabili"ty or cos"ts in "the road haulage indust,[y". Analysis {{or a large}} number or variables about ~ch data might reasonably {{be expected to be}} avaiTable · is used to develop cost models containingrelative. 1,y rew variables. Linear and log~linear models. are fitted by regression according to a taxonomic procedtirefn which a criterion based on a measure of distance between <b>standardised</b> <b>variables</b> is used in deciding which variables to · include. Use of the resultirigestimated costs in 'the con'text- o. f " break-even analysisin. 'the· managemen't or road hau. lage under'takings is discussed. 1...|$|R
40|$|Amniotic fluid (AF) is {{important}} for the establishment of maternal behaviour in inexperienced ewes, but its role in experienced mothers remains to be studied. Here, the maintenance of post-partum maternal responsiveness and the establishment of exclusive bonding was investigated in multiparous ewes when AF was removed from the neonate or/and physical contact with the young was precluded for the first 4 h post partum. Maintenance of maternal responsiveness and establishment of exclusive bonding were measured by the proportion of mothers accepting their own lamb and alien lambs that had been either washed or not washed, and by comparing an acceptance score for each type of lamb. The acceptance score was computed by summing <b>standardised</b> <b>variables</b> of acceptance (low bleats, acceptance at udder, nursing and licking time) and subtracting <b>standardised</b> <b>variables</b> of rejection (high-pitched bleats, rejection at the udder and aggressive behaviour). Washing the neonate reduced its acceptance score, but the proportion of mothers rejecting their own lamb was reduced only when washing the neonate and prevention of physical contact for 4 h were combined (7 / 15 v. 0 / 10 in controls, P = 0. 02). In addition, washing the neonate increased the acceptance score of the washed alien lamb, but not of the unwashed alien. However, washing and privation of physical contact did not increase significantly the proportion of mothers accepting an alien lamb at 4 h post partum. We conclude that AF {{is important}} in experienced ewes for the establishment of maternal responsiveness, as already found in primiparous mothers. In addition, our results indicate that AF also carries some chemosensory information facilitating exclusive bonding...|$|R
30|$|After having {{normalised}} and <b>standardised</b> the <b>variables</b> {{of urban}} form, we checked {{whether they were}} collinear through the VIF test. Outcomes of this test indeed showed {{that some of the}} variables presented collinearity. In particular, CNR and ID were strongly collinear in all cities, with VIFs significantly greater than 10. PUL was found to be collinear only in Leeds. Such variables were thus discarded from the list of candidates for the regression analysis.|$|R
30|$|Before {{entering}} the models, all context-level covariates were z-standardised {{and are therefore}} centred on their grand means with a common standard deviation of 1.0. <b>Standardised</b> context <b>variables</b> enable the direct comparison of their semi-standardised coefficients, which {{can be interpreted as}} the NEET probability’s percentage point change associated with a covariate increase by one standard deviation. Because all individual-level covariates are dichotomous and 0 / 1 -coded, {{there is no need to}} consider any kind of centring.|$|R
30|$|To date, {{more than}} 14 {{periodic}} surveys had been launched to capture such output indicators across different regions {{such as the}} USA (AUTM 1991 – 2012), Canada (AUTM 2000 – 2012), Europe (ProTon 1991 – 2012) and Singapore (A*STAR 2002 – 2011). The data collected from these surveys form the dataset for many studies in this literature strand. For example, AUTM data were used by Di Gregorio and Shane (2003) {{to show that the}} rate of university spin-offs depends on inventors’ quality and equity stake in the spin-offs. Calls have been made to both AUTM and ASTP to <b>standardise</b> their <b>variable</b> definitions and methodological approaches to facilitate comparability of PRI indicators across the studies. However, there is a dearth of impact studies in the Asia.|$|R
40|$|Objective: To {{investigate}} {{the association between}} metabolic risk factors (individually and in combination) and risk of gallbladder cancer (GBC). Methods: The metabolic syndrome and cancer project (Me-Can) includes cohorts from Norway, Austria, and Sweden with data on 578, 700 men and women. We used Cox proportional hazard regression models to calculate relative risks of GBC by body mass index (BMI), blood pressure, and plasma levels of glucose, cholesterol, and triglycerides as continuous <b>standardised</b> <b>variables</b> and their <b>standardised</b> sum of metabolic syndrome (MetS) z-score. The risk estimates were corrected for random error in measurements. Results: During an average follow-up of 12. 0 years (SD = 7. 8), 184 primary gallbladder cancers were diagnosed. Relative risk of gallbladder cancer per unit increment of z-score adjusted for age, smoking status and BMI (except for BMI itself) and stratified by birth year, sex and sub-cohorts, was for BMI 1. 31 (95 % confidence interval 1. 11, 1. 57) and blood glucose 1. 76 (1. 10, 2. 85). Further analysis showed {{that the effect of}} BMI on GBC risk is larger among women in the premenopausal age group (1. 84 (1. 23, 2. 78)) compared to those in the postmenopausal age group (1. 29 (0. 93, 1. 79)). For the other metabolic factors no significant association was found (mid blood pressure 0. 96 (0. 71, 1. 31), cholesterol 0. 84 (0. 66, 1. 06) and serum triglycerides 1. 16 (0. 82, 1. 64)). The relative risk per one unit increment of the MetS z-score was 1. 37 (1. 07, 1. 73). Conclusion: This study showed that increasing BMI and impaired glucose metabolism pose a possible risk for gallbladder cancer. Beyond the individual factors, the results also showed that the metabolic syndrome as an entity presents a risk constellation for the occurrence of gallbladder cancer. </p...|$|R
30|$|First, we {{keep the}} 38 main items from our questionnaire, {{describing}} attitudes, preferences and personality, and use the Kaiser-Meyer-Olkin measure to see if our data are suited for factor analysis. We find an overall value of 0.6237, which is above the requirement of 0.5 (Frohlich and Westbrook 2001). Thereafter, we run a principal factor analysis with promax rotation. Promax rotation is chosen {{because we want to}} allow our factors to be correlated. The factors created are based on the eigenvalues of the covariance matrix of the <b>standardised</b> attitude <b>variables</b> and presented in the scree plot of the eigenvalues in Fig. 2. Based on the scree plot, we choose to keep five factors for the analysis. The eigenvalues measure the variance in the variables that are grouped into that factor, and only factors with eigenvalues above one are retained.|$|R
40|$|Purpose of the article: Regional {{development}} includes qualitative {{changes in}} economy (e. g. in production, investments, employment) {{as well as}} qualitative changes (regarding the structure of economy and society, changes in the environment). The research of regional development is important and {{necessary in order to}} make appropriate decisions at the regional and local level. The main purpose of the article is comparative analysis of districts in the Lesser Poland Voivodship in the area of economic, social and ecological development. Scientific aim: The scientific aims of paper are verifying the hypothesis concerning eco-development and forecasting the level of regional development in districts of Lesser Poland Voivodship. Methodology/methods: In the research of regional development the quality index of economic, social and ecological development has been proposed which has been calculated {{on the basis of a}} certain aggregation of the results of the Principal Component Analysis made on the correlation matrix of <b>standardised</b> <b>variables</b> being the components of the index. Forecasts of the regional development level in districts were calculated with the use of different econometric models as linear model, exponential model, or power model. Findings: The findings prove that the Lesser Poland Voivodship is characterised by considerable disproportions in regional development. The most favourable conditions for economic and social development are in the districts with large city agglomerations as well as extensive municipality infrastructure and transport infrastructure. The presented results demonstrate that the majority of districts have not exhibited a constant tendency to changes in the positions in successive ranking lists in terms of the economic, social and ecological development. The positions occupied by most districts are generally stable and have not changed considerably in the examined period. Conclusions: The research has confirmed the negative interdependence between the level of socioeconomic development and ecological development. Thus, it was not possible to verify positively the concept of eco-development expressed in keeping balance between the economic, social and ecological system. </p...|$|R
40|$|Collecting data is {{different}} from accumulating knowledge. To succeed, public affairs informatics must develop three infrastructures: 1) protocols for <b>standardising</b> which <b>variables</b> are meaningful and formats for records; 2) storage and retrieval hardware and rules that protect privacy and ensure access; 3) a scholarly interpretive community that can do basic research on patterns in the data, ensuring that policy advice derived from informatics is theory-driven rather than exclusively driven by short-term problems. Two problems with developing these infrastructures are outlined. The first problem is the competing, and often conflicting, sources of authority and legitimacy in public affairs (markets, experts, and politics). The consequence of this conflict is likely to prevent a consensus on protocols or development of infrastructure. The second problem is the boundary between private issues and collectively decided issues. This problem is related to privacy, and it affects the very legitimacy of public policy. informatics infrastructure; information use; legitimacy; public affairs; public policy; informatics; privacy protection; access; data patterns; authority; protocols. ...|$|R
40|$|Geostatistics is {{a branch}} of applied {{mathematics}} that deals with spatially correlated data. Analysing and modelling spatially correlated data can be difficult and time consuming, especially for a multivariate data set. One of the techniques used to make analysis and modelling easier involves decorrelation, whereby a linear transformation on the sample variables is used to associate the spatially correlated variables {{with a set of}} decorrelated factors which are statistically and spatially independent. PCA {{was one of the first}} multivariate techniques and is mostly used as a data reduction technique. A popular alternative decorrelation technique often used in the mining industry is MAF. A study conducted by Bandarian (2008) found a relatively new decorrelation technique known as ACDC to be the method which produced the best spatial decorrelation for a multivariate moderately correlated data set consisting of four variables. In this thesis the PCA, MAF and ACDC methods are described and then applied to a multivariate data set supplied by Rio Tinto 2 ̆ 7 s Iron Ore Operations. Secondly, we explore whether it is preferable for the data set to be standardised or transformed via Gaussian anamorphosis to normal scores before being decorrelated. The data set consists of ten variables; however the three decorrelation methods were only applied to a subset of five variables (Fe, Ah 03, Si 02, LOI and Ti 02) which have the greatest similarity from a statistical and spatial point of view. The three methods were applied to both standardised and normalised data. For ACDC, additional inputs such as weights, number of iterations, tolerance and an initial guess for the diagonalising matrix were explored and investigated in order to get the best spatial decorrelation results possible. The overall best spatial decorrelation was achieved by performing ACDC on the <b>standardised</b> <b>variables,</b> using the matrix of eigenvectors of the correlation matrix as the initial guess for the diagonalising matrix as well as the first four experimental semivariogram matrices in the decorrelation. Transforming the variables to normal scores before decorrelation was found to be of no benefit, as the factors that were derived from the normalised variables with the exception of one, were not normally distributed following the decorrelation...|$|R
40|$|On the {{temperature}} derivative market, modeling temperature volatility {{is an important}} issue for pricing and hedging. In order to apply pricing tools of financial mathematics, one needs to isolate a Gaussian risk factor. A conventional model for temperature dynamics is a stochastic model with seasonality and inter temporal autocorrelation. Empirical work based on seasonality and autocorrelation correction reveals that the obtained residuals are heteroscedastic with a periodic pattern. The object of this research is to estimate this heteroscedastic function so that after scale normalisation a pure <b>standardised</b> Gaussian <b>variable</b> appears. Earlier work investigated this temperature risk in dfferent locations and showed that neither parametric component functions nor a local linear smoother with constant smoothing parameter are flexible enough to generally describe the volatility process well. Therefore, we consider a local adaptive modeling approach to find at each time point, an optimal smoothing parameter to locally estimate the seasonality and volatility. Our approach provides a more flexible and accurate fitting procedure of localised temperature risk process by achieving excellent normal risk factors...|$|R
40|$|The set of {{technical}} procedures {{commonly known as}} factorial ecology acts on a correlation matrix and so is based on <b>variables</b> <b>standardised</b> to the common base of zero mean and unit variance. This, according to the present note, is wasteful of {{the information on the}} data matrix being analysed. Alternative input matrices, to be treated as complementary to the correlation matrix, are suggested. These comprise firstly measures of variance - covariance, and secondly measures of the cross-products. Several examples of small data sets are used to illustrate the interpretative advantages of these matrices. No firm conclusions are offered, since further research is needed and unresolved problems still exist, but the possible extensions offered to the factorial ecology method seem worthy of serious consideration. ...|$|R
40|$|Objective: To {{examine the}} {{phenomenon}} of bruxism and assess {{the current state of}} the literature regarding its role in temporomandibular disorder (TMD). Data sources: A review of the literature contained in the CINAHL; Pubmed; COCHRANE; MANTIS and MEDLINE (ovid) database searches as well as hand searches was conducted between September 2005 and March 2006. Study selection: Studies using the keywords bruxism, temporomandibular, jaw, randomised controlled trial, survey, epidemiological, longitudinal, were screened. Uncontrolled studies, case studies, case series and reviews were ignored. A total of 172 studies were identified and compared in terms of their definition of bruxism, TMD, and conclusions regarding the role of bruxism. Conclusion: The majority of studies found a relationship between bruxism and TMD. However, inconsistency and equivocation of different parafunctional activities are common within the literature. The subject is further complicated by the lack of a universally accepted definition of both TMD and bruxism and an additional failure to <b>standardise</b> investigative <b>variables</b> such as frequency, duration and intensity of episodes. The role of bruxism as it is currently described can therefore be considered a controversial and unresolved issue. The authors suggest that the term bruxism be abandoned in favour of a set of diagnostically more appropriate descriptors. 11 page(s...|$|R
40|$|This {{paper has}} a twofold aim: {{the former is}} to focus on the concept of well-being/quality of life and its {{relationships}} with local sustainable development and the latter is to apply the capability approach at regional level. More specifically, one wants to analyse if sustainable development at a local level serves to better understand both the formation of well-being and/or quality of life. The instrument which will allow us to verify the operational value of the capability approach is the building of a multidimensional synthetic index of sustainability. This index will consists of aggregating a set of variables of different nature from the socio-economic to the environmental ones. It may be considered an alternative to the conventional indices, which are normally founded on GDP, and will be applied to the Italian regions. After having <b>standardised</b> each <b>variable</b> so to make them homogeneous, the methodology proposed, which will allow us to gather and compare the Italian regions according to the higher or lower level of quality of life, is the Wroclaws Economic School taxonomic method. The results obtained may represent an information tool for targeting and zoning sustainable development measures which aim at improving well-being/quality of life at a local level. ...|$|R
40|$|This paper {{analyses}} {{the main}} factors influencing the regional competitiveness of rural {{areas in the}} Tatarstan Republic. Firstly, 19 variables related to the socio-economic situation in the Tatarstan Republic were analysed, these having been taken from the Statistics Committee of the Tatarstan Republic. Principal component analysis (PCA) was then {{used to determine the}} weights of 10 indicators that {{have an effect on the}} level of regional competitiveness. Factor weights are used as weights in the summation of the <b>standardised</b> scores of <b>variables</b> that have an impact on competitiveness. The major factors influencing the level of regional competitiveness are the level of economically active population, investment in housing and the level of education. The following results were obtained: one of the 44 regions is very highly competitive and two are highly competitive; two of 44 regions have a medium level of competitiveness and 39 regions have a low level of competitiveness...|$|R
40|$|This {{research}} {{is aimed at}} finding empirical {{evidence to support the}} relationship between Broad-based Black Economic Empowerment (BBBEE) compliance and the financial performance of South African companies on the JSE. An independent measure of the BEE score was obtained from the Empowerdex Top Empowerment Companies (TEC) ranking from 2004 to 2009. 14 sectors on the JSE were selected to ensure inclusion of all major industries in South Africa. A total of 209 companies were selected, and the multivariate exploratory technique of Cluster Analysis was used. The predictor variable of the company’s BEE status was then compared to a number of financial performance indicators such as annual share price, price-tobook value ratio and the price-to-earnings ratio (i. e. the outcome <b>variables).</b> By <b>standardising</b> the <b>variables</b> of the BEE score and using Compound Annual Growth Rate (CAGR), the k-means Clustering method yielded four interpretable clusters with 15, 64, 95 and 35 companies respectively. The finding indicate that only {{in the case of the}} cluster of companies that increased it’s BEE score, were all three profitability measures significantly different and, according to the means, in the direction of higher profitability. However, there were no significant differences in the results to support the proposition that low-BEE scores of companies had a negative impact on their profitability and their firm’s value over time. Dissertation (MBA) [...] University of Pretoria, 2010. Gordon Institute of Business Science (GIBS) unrestricte...|$|R
40|$|Abstract {{copyright}} UK Data Service {{and data}} collection copyright owner. The General Household Survey Cross-Year Leisure Activities file permits cross-time study of changes in leisure activities in Great Britain gathered during General Household Surveys. The variables in these files have been recoded from the original studies to achieve consistent formats. The GHS included leisure questions in the following years: 1973, 1977, 1980, 1983, 1986, 1987, 1990 - 91, 1993 - 94, 1996 - 97. This file also permits the observation of demographic changes in Great Britain over the same time period. Main Topics : The leisure activities covered in this data series include: participation {{in a variety of}} sports, reading, recreational study, socialising, listening to music, smoking, drinking, gardening, use of electronic media, attending events, visiting tourist facilities, travel, and possession of equipment in the household which can be used for leisure activities. The data also include demographic <b>variables</b> <b>standardised</b> for cross-time comparisons. Standard Measures Registrar General's Social Class Schema...|$|R
40|$|Nitrogen (N) is {{invaluable}} {{for maintaining}} agricultural production, but its use, and particularly inefficient use, {{can lead to}} environmental losses. This paper reviews N use efficiency (NUE) and N surplus indicators for dairy production systems to assess their utility for optimising N use outcomes and minimising environmental N losses. Using case-study examples, we also assess realistic goals for these indicators and discuss key issues associated with their use. Published whole-farm NUE and whole-farm N surplus values ranged within 10 - 65 % and 40 - 700 kg N ha- 1 year- 1 respectively. In a study of five catchments across New Zealand, whole-farm NUE was more strongly affected by catchment differences in soil and climatic conditions than by differences in management. In contrast, whole-farm N surplus differed both between-and within-catchments and was a good indicator of N losses to water. Realistic goals for both NUE and N surplus thus depend on the agro-climatic context of the dairy system and on its economic and environmental goals. Crop and animal NUE values can be valuable indicators for optimising fertiliser and feed use and minimising N losses. However, global or national whole-farm NUE values appear of limited value if the ultimate goal for setting targets {{is to reduce the}} environmental impact of N use; whole-farm level targets based on N surplus would be a more useful indicator for this purpose. Our review also reinforces the importance of <b>standardising</b> the <b>variables</b> that should be used to estimate NUE and N surplus values, to ensure equitable comparisons between different systems. Finally, NUE and N surplus targets should also be set in the context of other agro-environmental considerations...|$|R
40|$|Estimates of hypolimnetic {{water column}} (Jv) and {{sediment}} (JA) dissolved oxygen (DO) consumption rates were made using {{the methodology of}} Livingstone & Imboden (1996); DO, temperature, chlorophyll a (ChI a) and total phosphorus (TP) concentration, and bathymetric {{data were collected from}} five Northern Irish lakes and collated from four lakes in England and Sweden. These data were combined with existing results to examine the influence of physicochemical variables and lake morphometry on the relationships among Jv, JA and lake trophic state. There was a positive relationship between Jv and the growing season (GS) mean ChI a and annual mean (AM) TP concentration, with a maximum Jv evident in eutrophic lakes (0. 218 ± 0. 015 g O 2 m' d-I based on the relationship with GS mean ChI a). The trophic state indicator <b>variables</b> <b>standardised</b> by lake maximum depth (ZMAX) explained the most variation in Jv (83 %) and JA (35 %). The relationships between Jv, JA and the trophic state indicator <b>variables</b> <b>standardised</b> by ZMAX were employed {{in the development of a}} model for predicting hypolimnetic DO concentration-depth profiles. The model was capable of reproducing observed data in four test lakes of varying trophic state and morphometry; the coefficient of determination and standard error of the estimate of the regression between observed and predicted data were 0. 62 (P < 0. 001) and 1. 60 g O 2 m", respectively. Long-term data (1968 to 2008) from two lakes were used to examine the effect of a warming climate and eutrophication on hypolimnetic DO resources. The rate of volumetric hypolimnetic DO depletion (VHODobs) ranged from 0. 035 to 0. 090 g O 2 m- 3 a' and from 0. 131 to 0. 252 g O 2 m" a' in Windermere South Basin (WSB) and Blelham Tarn (BT), respectively. Mixed layer ChI a concentration in WSB and wind speed over BT were important drivers of the variation in VHODobs. Changes in lake thermal structure resulted in the duration of thermal stratification increasing by 25 ± 8 days and 38 ± 8 days in WSB and BT, respectively, over the study period. Results show that climate warming effects on lake thermal structure have the potential to exacerbate hypolimnetic DO depletion, and may undermine management efforts taken to alleviate the impacts of eutrophication. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Objectives: To {{assess the}} utility of the display {{standardisation}} of diffusion-weighted MRI (DWI) and to compare the effectiveness of DWI and fluid-attenuated inversion recovery (FLAIR) MRI for the diagnosis of sporadic Creutzfeldt-Jakob disease (sCJD). Design: A reliability and agreement study. Setting: Thirteen MRI observers comprising eight neurologists and five radiologists at two universities in Japan. Participants: Data of 1. 5 -Tesla DWI and FLAIR were obtained from 29 patients with sCJD and 13 controls. Outcome measures: Standardisation of DWI display was performed utilising b 0 imaging. The observers participated in <b>standardised</b> DWI, <b>variable</b> DWI (the display adjustment was observer dependent) and FLAIR sessions. The observers independently assessed each MRI for CJD-related lesions, that is, hyperintensity in the cerebral cortex or striatum, using a continuous rating scale. Performance was evaluated by the area under the receiver operating characteristics curve (AUC). Results: The mean AUC values were 0. 84 (95 % CI 0. 81 to 0. 87) for standardised DWI, 0. 85 (95 % CI 0. 82 to 0. 88) for variable DWI and 0. 68 (95 % CI 0. 63 to 0. 72) for FLAIR, demonstrating the superiority of DWI (p< 0. 05). There was a trend for higher intraclass correlations of standardised DWI (0. 74, 95 % CI 0. 66 to 0. 83) and variable DWI (0. 72, 95 % CI 0. 62 to 0. 81) than that of FLAIR (0. 63, 95 % CI 0. 53 to 0. 74), although the differences were not statistically significant. Conclusions: Standardised DWI is as reliable as variable DWI, and the two DWI displays are superior to FLAIR for the diagnosis of sCJD. The authors propose that hyperintensity in the cerebral cortex or striatum on 1. 5 -Tesla DWI but not FLAIR can be a reliable diagnostic marker for sCJD...|$|R
40|$|Abstract: The {{recording}} and classification of obstacles that {{can interfere with}} the movements and migrations of fish is critical information to plan river restoration program. To date a major problem {{is to determine the}} potential effect of each obstacle and to select the problematic sites that should be improved to restore longitudinal connectivity (construction of fish passage facilities, removal or modification of the obstacles). Fish passage success about an obstacle is complex and depends on the hydraulic conditions over and {{at the foot of the}} obstacle in relation to swimming and leaping capabilities of the fish species concerned. This selection and classification is too often biased because managers lack information’s on the fish capabilities to leap physical obstacles. The need for a simple, reliable and standardised assessment method for use by a wide range of environmental stakeholders rapidly became evident. We responded to the challenge and developed the ICE protocol as the basis for the required ecological-continuity assessment method, particularly in the context of the European Water Framework Directive implementation. The ICE protocol coordinated by ONEMA is based on an integration of the topographical and hydraulic characteristics of obstacles with the physical capabilities (swimming, jumping or crawling) of the fish species analysed. It requires the gathering of <b>standardised</b> descriptive <b>variables</b> on each obstacle. The result of the analysis is an indication on the risks of a structure constituting a more or less severe obstacle (4 classes) for a given fish species (n= 47) or group of species. Particular attention was paid to the practical aspects of the method (time required and necessary human resources) to facilitate its use in a wide variety of situations and areas. Each procedure is presented as a flow chart to assist in decision-making, thus making the protocol easy to use for a wide range of people...|$|R
40|$|Background and Purpose: One of {{the most}} {{productive}} and economically important tree species in the USA is Douglas fir (Pseudotsuga menziesii (Mirb.)). Because of its broad natural range, in the horizontal {{as well as in}} the vertical sense (from California to British Columbia and from sea level up to an altitude of 1500 m), a variety of its provenances has been differentiated. European foresters, thanks to their insight into Douglas fir productivity and usage in its native country, initiated at the beginning of the 19 th century the establishment of provenance tests and research on the success of individual provenances outside their natural distribution. In the framework of the IUFRO programme, numerous European countries, among them Croatia, started provenance tests with the aim of researching the adaptability of this valuable species. In several bioclimatic localities in Croatia (Istria, North-West, Central and Eastern Croatia) different provenances have shown good success. The importance of Douglas fir is becoming increasingly evident in conditions of a changing climate and when the possible application of Douglas fir in the establishment of forest cultures is taken into consideration. Douglas fir can be established on uncovered forest and non-forest areas for wood supply or as a potential renewable energy source. The exploitation of wood and biomass from forest cultures enables the conservation of biodiversity and the survival of our natural forest ecosystems. Material and Methods: This paper is based on research conducted on a Douglas fir provenance trial in the locality of »Kontija« in Istria. The trial, in which ten Douglas fir provenances were included, started in 1969. On the trial plots, dendrometric parameters such as DBH and tree heights were measured on the basis of which wood volume was calculated. Descriptive statistics were made for all analysed variables (DBH, h, V). The differences between the provenances for the investigated variables were tested with an analysis of variance (ANOVA). Provenances which differ between themselves were determined using the Tukey post hoc test. According to data on the survival rate for all provenances, the average survival rate inside the trial was estimated and tested with a test of proportion to determine which provenances differ statistically from the average survival rate. Using a non-hierarchical (κ-means) cluster analysis, provenances were grouped according to <b>standardised</b> <b>variables</b> (N, DBH, h, V). In the same way, the species’ health condition was determined, as was the resistance of provenances to forest pests. Results and Conclusion: The average survival rate of all provenances in the trial amounts to 64. 8 % (ranging from 52 % for the provenance PE ALL to 77 % for the provenance ELMA). The best height growth was identified in the provenance SHELTON which originates from lower altitudes in Washington while the provenance SHADE COVE from higher areas of Oregon shows lowest growth. High volume is evidenced in the provenance PE ALL. The results of the analysis of variance (ANOVA) show that there is a statistically significant difference for all analysed variables for the investigated provenances. A κ-means cluster analysis grouped ten Douglas fir provenances in four clusters. In cluster 1, only the provenance SHADY COVE is included with a high number of trees with lower heights and smaller DBH and volume. Provenances from cluster 3 (PE ALL and YELM) have, in a small number of trees, higher DBH and volume. The rest of the provenances are grouped in two clusters (cluster 2 and cluster 4) with similar values but with the difference that the provenances SHELTON, CORVALIS, ELMA and HVIDILDE (cluster 2) have higher values than average for the analysed parameters, while the provenances [IPKA, BUZET and CASTLE ROCK (cluster 4) have lower than average values. In general, all the provenances in the trial are vital and show a good health condition without any determined pests...|$|R
40|$|SUMMARY. A {{basic and}} {{elementary}} geometrical proposition {{is used to}} provide a solution of the Behrens-Fisher problem which is both conservative and robust. An explicit formula is given which allows a strict upper bound of p to be obtained and also allows confidence intervals {{of the difference in}} population means to be computed with confidence coefficient strictly greater than 1 − α. (α is a prescribed small positive constant). The main results of this paper are given by (Ia),(Ib), (Ic), (Id) together with (II), in Section 2, and (III) is Section 3. Notation. we conform to the convention that random variables shall be denoted by capital letters, whilst constants and observed values of random variables be denoted by lowercase letters. If X is a random variable with a continuous and strictly increasing (cumulative) distribution function over its range, then the unique upper γ point of X will be denoted by Xγ, i. e. P r{X ≥ Xγ} = γ. The sole exception is that a chi-squared random variable with ν degree of freedom will be denoted by χ 2 ν, and the upper α point of χ 2 ν by χ 2 ν;α. The symbol ∼ will denote “is distributed as”. The letter Z will denote a <b>standardised</b> normal random <b>variable...</b>|$|R
40|$|OBJECTIVE [...] To {{assess the}} {{feasibility}} of extracting data on readmissions and readmission rates from Körner data for use as health service indicators. DESIGN [...] Retrospective analysis of inpatient Körner data for January 1988 to April 1989. SETTING [...] Three districts in North East Thames region. MAIN OUTCOME MEASURES [...] Number of readmissions after index discharge for all acute specialties combined and by specialty (general medicine, general surgery, gynaecology, trauma and orthopaedics, and geriatrics); readmission rates at 28 days after index discharge; and rates standardised for age group and sex by specialty and by consultant. RESULTS [...] All specialties showed an early peak in number of admissions, which levelled off by 28 days. Readmission rates at 28 days were appreciably lower in surgical specialties than in medical specialties (for example, general surgery 4. 1 % v geriatric medicine 15. 1 %). They were related to age and sex of the patient. Rates <b>standardised</b> for these <b>variables</b> did not significantly differ by district. Likewise, significant differences in standardised rates were not obtained for consultants within a specialty in one district. CONCLUSIONS [...] Readmission rates may be measured with Körner data. The pattern of readmissions with time means that readmission rates should be measured at not more than 28 days after the index discharge; the rates require standardisation for age and sex. Annual comparisons of standardised rates may be made among districts for combinations of specialties; those among individual consultants or specialties {{are unlikely to be}} statistically valid...|$|R
40|$|Aims and Objectives To examine wound {{assessment}} and management in patients following surgery and to compare these practices with current evidence-based {{guidelines for the}} prevention of surgical site infection across one healthcare services district in Queensland, Australia. Background Despite innovations in surgical techniques, technological advances and environmental improvements in the operating room, and the use of prophylactic antibiotics, surgical site infections remain a major source of morbidity and mortality in patients following surgery. Design A retrospective clinical chart audit Methods A random sample of 200 medical records of patients who had undergone surgery was undertaken over a two-year period (2010 - 2012). An audit tool was developed to collect the data on wound {{assessment and}} practice. The study was undertaken across one healthcare services district in Australia. Results Of the 200 records that were randomly identified, 152 (76 %) met the inclusion criteria. The excluded records were either miscoded or did not involve a surgical incision. Of the 152 records included, 87 (57 岥) procedures were classified as 'clean' and 106 (69 工) were elective. Wound assessments were fully documented in 63 / 152 (41 崥) of cases, and 59 / 152 (38 帥) charts had assessments documented on a change of patient condition. Of the 15 / 152 (9 幥) patients with charted postoperative wound complications, 4 / 15 (26 嶥) developed clinical signs of wound infection, which were diagnosed on days 3 to 5. Conclusions The timing, content and accuracy of wound assessment documentation are <b>variable.</b> <b>Standardising</b> documentation will increase consistency and clarity and contribute to multidisciplinary communication. Relevance to clinical practice These results suggest that postoperative wound care practices are not consistent with evidence-based guidelines. Consequently, it is important to involve clinicians in identifying possible challenges within the clinical environment that may curtail guideline use. Griffith Health, School of Nursing and MidwiferyFull Tex...|$|R
