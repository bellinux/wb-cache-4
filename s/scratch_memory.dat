10|41|Public
5|$|Each {{background}} processor {{consisted of}} a computation section, a control section and local memory. The computation section performed 64-bit scalar, floating point and vector arithmetic. The control section provided instruction buffers, memory management functions, and a real-time clock. 16 kwords (128 kbytes) of high-speed local memory was incorporated into each background processor for use as temporary <b>scratch</b> <b>memory.</b>|$|E
50|$|With proof-carrying code, the kernel {{publishes a}} {{security}} policy specifying properties that any packet filter must obey: for example, will not access memory {{outside of the}} packet and its <b>scratch</b> <b>memory</b> area. A theorem prover is used {{to show that the}} machine code satisfies this policy. The steps of this proof are recorded and attached to the machine code which is given to the kernel program loader. The program loader can then rapidly validate the proof, allowing it to thereafter run the machine code without any additional checks. If a malicious party modifies either the machine code or the proof, the resulting proof-carrying code is either invalid or harmless (still satisfies the security policy).|$|E
50|$|Multics {{implemented}} a single-level store for data access, discarding the {{clear distinction between}} files (called segments in Multics) and process memory. The memory of a process consisted solely of segments that were mapped into its address space. To read or write to them, the process simply used normal Central processing unit (CPU) instructions, and the operating system took care {{of making sure that}} all the modifications were saved to disk. In POSIX terminology, it was as if every file were ed; however, in Multics there was no concept of process memory, separate from the memory used to hold mapped-in files, as Unix has. All memory in the system was part of some segment, which appeared in the file system; this included the temporary <b>scratch</b> <b>memory</b> of the process, its kernel stack, etc.|$|E
50|$|On-chip caches uses static RAM that consume {{power in}} the range of 25% to 50% of the total chip power and {{occupies}} about 50% of the total chip area. Scratchpad memory occupies lesser area than on-chip caches. This will typically reduce the energy consumption of the memory unit, because less area implies reduction in the total switched capacitance. Current embedded processors particularly in the area of multimedia applications and graphic controllers have on-chip <b>scratch</b> pad <b>memories.</b> In cache memory systems, the mapping of program elements is done during run time, whereas in <b>scratch</b> pad <b>memory</b> systems this is done either by the user or automatically by the compiler using suitable algorithm.|$|R
50|$|CSV is a {{high-tech}} industrial complex. Abou El Enein established the Smart Card factory, {{the first and}} largest factory {{of its kind in}} Egypt. It produces all kinds of smart cards, starting from <b>scratch</b> cards, <b>memory</b> cards, SIM cards to credit cards.|$|R
40|$|Abstract — In {{an effort}} to make {{processors}} more power efficient <b>scratch</b> pad <b>memory</b> (SPM) have been proposed instead of caches, which can consume majority of processor power. However, application mapping on SPMs remain a challenge. We propose a dynamic SPM management scheme for program stack data for processor power reduction. As opposed to previous efforts, our solution does not mandate any hardware changes, does not need profile information, and SPM size at compile-time, and seamlessly integrates support for recursive functions. Our technique manages stack frames on SPM using a <b>scratch</b> pad <b>memory</b> manager (SPMM), integrated into the application binary by the compiler. Our experiments on benchmarks from MiBench [15] show average energy savings of 37 % along with a performance improvement of 18 %. I...|$|R
40|$|State Machines Yuri Gurevich Charles Wallace February 3, 1999 MSR-TR- 99 - 07 Microsoft Research One Microsoft Way Redmond, WA 98052 - 6399 Abstract An ASM {{specification}} of the Windows Card Runtime Environment (RTE) is presented {{and used to}} verify certain safety properties. 1 Introduction The Microsoft Windows Card runtime environment (RTE) is a programming environment for smart card applications. Here we specify the RTE {{in terms of an}} abstract state machine (ASM) and prove that an RTE application never leaves its "sandbox" in <b>scratch</b> <b>memory,</b> never learns anything about the rest of <b>scratch</b> <b>memory</b> and therefore is never a#ected by anything that goes on there. 2 gives {{a brief description of the}} RTE. 3 defines the vocabulary of the ASM, 4 defines some macros, and 5 defines the program of the ASM. In 6 we formalize and prove the safety properties mentioned above. Due to careful design of the RTE, the specification and safety proofs are straightforward. We make some final rem [...] ...|$|E
40|$|This {{application}} note presents several mechanisms {{supported by the}} TMS 320 DSP Algorithm Standard (referred to as XDAIS) which can be employed by algorithm writers to assist application developers in optimizing their system’s performance and sharing of memory resources. The first mechanism is based on using statically allocated global data structures. The second mechanism is built on the concepts of <b>scratch</b> <b>memory</b> and memory overlaying techniques based on algorithm activation and deactivation. The third mechanism {{is referred to as}} the write-once buffers technique and support sharing of relocatable read-only data across instances of a common algorithm. The last mechanism {{is referred to as the}} parent instance technique, whereby the algorithm developer defines and associates an anonymous parent instance with each algorithm instance that gets created. Parent instance memory can b...|$|E
40|$|Network {{processing}} is requisite {{for modern}} day secure networks. Extended finite automata (XFA) are a contemporary {{variation of the}} deterministic and non-deterministic finite automata which were either slow or memory inefficient. XFAs are both fast and memory efficient since they are deterministic in their operation and avoid state explosion {{by the addition of}} small <b>scratch</b> <b>memory</b> which is updated by small programs attached to each state. In this work we propose to implement XFAs on a graphical processing unit. As packet stream processing inherently exposes parallelism, it is a prime candidate for graphical processing units. Current processing rates of GPUs are far faster then with standard CPUs. Additionally, GPUs are growing at much greater rates then CPUs. With this fast processing speed we anticipate a high level of speed-up when utilizing the GPU for XFA packet processing when compared to CPU only...|$|E
40|$|In {{multimedia}} {{and other}} streaming applications {{a significant portion}} of energy is spent on data transfers. Exploiting data reuse opportunities in the application, we can reduce this energy by making copies of frequently used data in a small local memory and replacing speed and power inefficient transfers from main off-chip memory by more efficient local data transfers. In this paper we present an automated approach for analyzing these opportunities in a program that allows modification of the program to use custom <b>scratch</b> pad <b>memory</b> configurations comprising a hierarchical set of buffers for local storage of frequently reused data. Using our approach we are able to reduce energy consumption of the memory subsystem when using a <b>scratch</b> pad <b>memory</b> by a factor of two on averag...|$|R
40|$|Abstract One of {{the most}} {{critical}} components that de-termine the success of an MPSoC based architecture is its on-chip <b>memory.</b> <b>Scratch</b> Pad <b>Memory</b> (SPM) is increasingly being applied to substitute cache as the on-chip memory of embedded MPSoCs due to its superior chip area, power consumption and timing predictabil-ity. SPM can be organized as a Virtually Shared SPM (VS-SPM) architecture that takes advantage of both shared and private SPM. However, making effective use of the VS-SPM architecture strongly depends on two inter-dependent problems: variable partitioning and task scheduling. In this paper, we decouple these two problems and solve them in phase-ordered manner. We propose two variable partitioning heuristics base...|$|R
50|$|Each 64-bit Cyclops64 chip (processor) {{will run}} at 500 {{megahertz}} and contain 80 processors. Each processor {{will have two}} thread units and a floating point unit. A thread unit is an in-order 64-bit RISC core with 32 kB <b>scratch</b> pad <b>memory,</b> using a 60-instruction subset of the Power Architecture instruction set. Five processors share a 32 kB instruction cache.|$|R
40|$|Teaching how {{to design}} and tune an {{embedded}} system is indeed a difficult task, since the student has to learn the many trade-offs {{that lead to the}} final system configuration. Existing tools are often too complex, or do not stress the basic steps in the design path. These steps are very useful during the first training sessions. The environment Csim 2, which is used at our university, permits the student to become familiar with concepts of program locality, cache structure and performance tuning, while analyzing actual data produced by the actual software that has to be tied with the embedded system. The student can analyze program behavior by means of locality graphs, or run extensive parametric simulations in order to find the best configuration that minimize either system cost, power consumption, or execution time. Further optimizations allow the designer to explore more sophisticated features like selective cacheing, cache locking, <b>scratch</b> <b>memory,</b> and code mapping for better cache [...] ...|$|E
40|$|Abstract- Teaching how {{to design}} and tune an {{embedded}} system is indeed a difficult task, since the student has to learn the many trade-offs {{that lead to the}} final system configuration. Existing tools are often too complex, or do not stress the basic steps in the design path. These steps are very useful during the first training sessions. The environment Csim 2, which is used at our university, permits the student to become familiar with concepts of program locality, cache structure and performance tuning, while analyzing actual data produced by the actual software that has to be tied with the embedded system. The student can analyze program behavior by means of locality graphs, or run extensive parametric simulations in order to find the best configuration that minimize either system cost, power consumption, or execution time. Further optimizations allow the designer to explore more sophisticated features like selective cacheing, cache locking, <b>scratch</b> <b>memory,</b> and code mapping for better cache exploitation. In this paper we show the basic capabilities of the environment, and some example of training sessions. By means of graphs about program locality and performance metrics, the student is readily conducted to learn how to select an adequate embedded system configuration. ...|$|E
40|$|Deep packet {{inspection}} {{is becoming}} prevalent for modern network processing systems. They inspect packet payloads {{for a variety}} of reasons, including intrusion detection, traffic policing, and load balancing. The focus of this paper is deep packet inspection in intrusion detection/prevention systems (IPSes). The performance critical operation in these systems is signature matching: matching payloads against signatures of vulnerabilities. Increasing network speeds of today?s networks and the transition from simple string-based signatures to complex regular expressions has rapidly increased the performance requirement of signature matching. To meet these requirements, solutions range from hardware-centric ASIC/FPGA implementations to software implementations using high-performance microprocessors. In this paper, we propose a programmable SIMD architecture design for IPSes and develop a prototype implementation on an Nvidia G 80 GPU. We first present a detailed architectural and microarchitectural analysis of signature matching. Our analysis shows that signature matching is well suited for SIMD processing because of regular control flow and parallelism available at the packet level. We examine the conventional approach of using deterministic finite automata (DFAs) and a new approach called extended finite automata (XFAs) which require far less memory than DFAs, but require <b>scratch</b> <b>memory</b> and small amounts of computation in each state. We then describe a SIMD design to implement DFAs and XFAs. Using a SIMD architecture provides flexibility, programmability, and design productivity which ASICs lack, while being area and power efficient which superscalar processors lack. Finally, we develop a prototype implementation using the G 80 GPU as an example SIMD implementation. This system out-performs a Pentium 4 by up to 9 X and shows SIMD systems are a promising candidate for signature matching...|$|E
40|$|In {{this report}} we {{evaluate}} {{the options for}} low power on-chip memories during system design and configuration. Specifically, we compare the use of <b>scratch</b> pad <b>memories</b> with that of cache {{on the basis of}} performance, area and energy. The target architecture used in our experiments is the AT 91 M 40400 microcontroller containing an ARM 7 TDMI core. A packing algorithm is used to map the memory objects of the benchmarks to the scratch pad. Area and energy for different scratch pad and cache sizes are computed using the CACTI tool while performance is derived using the trace results of the ARMulator. We observe area and performance improvements by using a <b>scratch</b> pad <b>memory.</b> For example, for bubble sort there is a performance improvement of 18 % from a hardware which needs 34 % less area. The scratch pad also needs less energy per access, due to the absence of tag comparison. ...|$|R
40|$|The pursuit {{for higher}} {{performance}} and higher power-efficiency in computing {{has led to}} the evolution of multi-core processor architectures. Early multi-core processors primarily used the shared memory multi-processing paradigm. However, the conventional shared memory architecture, due to its limited scalability becomes a performance bottleneck. Newer architectures like the IBM Cell with 10 cores have adopted new memory architectures to truly enable the peak computing performance available. In order to achieve higher performance, it is necessary to re-design not only the bus topology, but also the memory hierarchy. The distributed memory model used in non-uniform memory access (NUMA) architectures is becoming popular in these modern processors. Conventional on-chip memory like caches have been replaced by a low power, low area alternative called <b>scratch</b> pad <b>memory</b> (SPM). Caches perform the data and code transfers in hardware in an automated fashion. Unlike caches, the transfers in SPM need to be explicitly managed by the compiler. In order to achieve a power-efficient operation, it is important to map the most frequently used objects onto the SPM. In this thesis, a dynamic <b>scratch</b> pad <b>memory</b> management scheme is proposed for program stack data with the objective of processor power reduction. As opposed to previous efforts, this technique does not need the SPM size at compile-time, does not mandate any hardware changes, does not need profile information and seamlessly integrates support for recursive functions. This solution manages stack frames on SPM using a software <b>scratch</b> pad <b>memory</b> manager (SPMM), integrated into the application binary by the compiler. The experiments on benchmarks from MiBench suite show average energy savings of 37 % along with a performance improvement of 18 %...|$|R
40|$|Abstract—Limited Local Memory (LLM) {{architectures}} are power-efficient, scalable memory multi-core architectures, {{in which}} cores have a scratch-pad like local memory that is software controlled. Any data transfers between the main {{memory and the}} local memory must be explicitly present as Direct Memory Access (DMA) commands in the application. Stack data management of the cores is an important problem in LLM architecture, and our previous work outlined a promising scheme for that [1]. In this paper, we improve the previous approach, and now can i) manage limitless stack data, ii) increase the applicability of stack management, and iii) perform stack management with smaller footprint on the local memory. We demonstrate these by executing benchmarks from the MiBench suite on the IBM Cell processor. Index Terms—Stack, local <b>memory,</b> <b>scratch</b> pad <b>memory,</b> embedded systems, multi-core processor, IBM Cell, MPI I...|$|R
50|$|LIPA was {{designated}} in 2006 - {{the first new}} higher education institution to have been started from <b>scratch</b> in living <b>memory.</b> As a performing arts HEI, LIPA is attended by {{the highest number of}} international students in the UK.|$|R
30|$|A {{closely related}} field of {{interest}} lies in the optimization of <b>scratch</b> pad <b>memories</b> (SPM), which are an efficient replacement for caches in embedded systems {{used in conjunction with}} external memory [14]. To identify the optimal selection of address ranges to be mapped to the SPM, ILP models have been developed [4, 6] and dynamic programming has been used [13]; all of which only consider a single fixed size SPM and cannot be used to optimize the SPM’s sub-organization.|$|R
40|$|Abstract—In this paper, {{we present}} a runtime memory {{allocation}} algorithm, that aims to substantially reduce the overhead caused by shared-memory accesses by allocating memory directly in the local <b>scratch</b> pad <b>memories.</b> We target a heterogeneous platform, with a complex memory hierarchy. Using special instrumentation, we determine what memory areas are used in functions that could run on different processing elements, like, for example a reconfigurable logic array. Based on profile information, the programmer annotates some functions as candidates for accelerated execution. Then, an algorithm decides the best allocation, {{taking into account the}} various processing elements and special <b>scratch</b> pad <b>memories</b> of the the heterogeneous platform. Tests are performed on our prototype platform, a Virtex ML 410 with Linux operating system, containing a PowerPC processor and a Xilinx FPGA, implementing the MOLEN programming paradigm. We test the algorithm using both state of the art H. 264 video encoder as well as other synthetic applications. The performance improvement for the H. 264 application is 14 % compared to the software only version while the overhead is less than 1 % of the application execution time. This improvement is the optimal improvement that can be obtained by optimizing the memory allocation. For the synthetic applications the results are within 5 % of the optimum. 1 I...|$|R
40|$|A dynamic <b>scratch</b> pad <b>memory</b> (SPM) {{management}} scheme for program stack data {{with the objective}} of processor power reduction is presented. Basic technique does not need the SPM size at compile time, does not mandate any hardware changes, does not need profile information, and seamlessly integrates support for recursive functions. Stack frames are managed using a software SPM manager, integrated into the application binary, and shows average energy savings of 32 % along with a performance improvement of 13 %, on benchmarks from MiBench. SPM management can be further optimized and made pointer safe, by knowing the SPM size. close 4...|$|R
40|$|In {{this paper}} we {{address the problem of}} on-chip memory {{selection}} for computationally intensive applications, by proposing <b>scratch</b> pad <b>memory</b> as an alternative to cache. Area and energy for different scratch pad and cache sizes are computed using the CACTI tool while performance was evaluated using the trace results of the simulator. The target processor chosen for evaluation was AT 91 M 40400. The results clearly establish scratchpad memory as a low power alternative in most situations with an average energy reduction of 40 %. Further the average area-time reduction for the scratchpad memory was 46 % of the cache memory. 1...|$|R
40|$|<b>Scratch</b> Pad <b>Memories</b> (SPMs) have {{received}} considerable attention lately as on-chip memory building blocks. The main characteristic that distinguishes an SPM from a conventional cache memory {{is that the}} data flow is controlled by software. The main focus {{of this paper is}} the management of an SPM space shared by multiple applications that can potentially share data. The proposed approach has three major components; a compiler analysis phase, a runtime space partitioner, and a local partitioning phase. Our experimental results show that the proposed approach leads to minimum completion time among all alternate memory partitioning schemes tested. Copyright © 2009, Inderscience Publishers...|$|R
40|$|In {{this paper}} we {{address the problem of}} on-chip mem-ory {{selection}} for computationally intensive applications, by proposing <b>scratch</b> pad <b>memory</b> as an alternative to cache. Area and energy for different scratch pad and cache sizes are computed using the CACTI tool while performance was evaluated using the trace results of the simulator. The tar-get processor chosen for evaluation was AT 91 M 40400. The results clearly establish scratehpad memory as a low power alternative in most situations with an average energy re-duction of J 0 %. Further the average area-time reduction for the seratchpad memory was 46 % of the cache memory. 1 1. In t roduct ion The salient feature of portable devices is light weight and low power consumption. Applications n multimedia, video processing, speech processing, DSP applications and wire-less communication require efficient memory design since on chip memory- occupies more than 50 % of the total chip area [1]. This will typically reduce the energy consumption of the memory unit, because l ss area implies reduction in the total switched capacitance. On chip caches using static RAM consume power in the range of 25 % to,~ 5 % of the total chip power [2]. Recently, interest has been fo-cussed on having on chip <b>scratch</b> pad <b>memory</b> to reduce the power and improve performance. On the other hand, they can replace caches only if they axe supported by an effec-tive compiler. Current embedded processors particularly in the area of multimedia applications and graphic ontrollers ZThis project is supported under DST-DAAD grant...|$|R
5000|$|For myself, {{this was}} the most {{challenging}} and rewarding project of my programming career. I had to learn a great deal about PC hardware, for example writing an int9 keyboard ISR replacement, was able to write a number of graphics systems from <b>scratch,</b> an xms <b>memory</b> manager, and was thrilled to see [...] "my game" [...] in a real computer store in 1992. Spelljammer was the only game I did commercially." ...|$|R
40|$|Abstract. The {{design of}} future {{high-performance}} embedded systems is hampered by two problems: First, the required hardware needs {{more energy than}} is available from batteries. Second, current cache-based approaches for bridging the increasing speed gap between processors and memories cannot guarantee predictable real-time behavior. A contribution to solving both problems is made in this paper which describes a comprehensive set of algorithms {{that can be applied}} at design time in order to maximally exploit <b>scratch</b> pad <b>memories</b> (SPMs). We show that both the energy consumption as well as the computed worst case execution time (WCET) can be reduced by up to to 80 % and 48 %, respectively, by establishing a strong link between the memory architecture and the compiler...|$|R
40|$|The initial {{control and}} {{programming}} philosophies of the RELAPSE are discussed. A block diagram showing {{the relationship of}} the Arithmetic Units (composed of Stages and Bit Processors), to the Functional Units, and other components of the RELAPSE is used to guide this discussion. The latest version of the Bit Processor design is presented. Included is a detailed discussion of the Bit Processor's new <b>scratch</b> pad <b>memory</b> component. The section also clarifies the usage of the Bit Processor's processing registers, and Input/Output functions. The final design phase of the Arithmetic Unit is underway by a study of the Proposed IEEE Floating Point Standard. The decisions on conformation to this standard will be used as inputs into the finalization of the designs of the Bit Processor, Stage, and Arithmetic Units of the RELAPSE...|$|R
40|$|The {{computational}} {{complexity of}} cellular telephone standards has increased faster than Moore’s law. New architectural approaches {{will be needed}} {{in order to meet the}} performance needs while staying within acceptable energy budgets. The ACT coprocessor improves on the energy-delay characteristics of embedded systems by 2 to 3 orders of magnitude, and is within 1 to 2 orders of magnitude of an ASIC approach while retaining much of the generality of a general purpose processor. This paper summarizes the ACT architecture, details have been published elsewhere, and then presents the details of new architectural enhancements that have proven particularly effective in improving performance. The enhancements are a mode addressed register file (MARF) and separate address generation units (AGU’s) for each SRAM port and a hardware loop unit (HLU). For the 9 DSP and telephony codes used to evaluate this architecture, 61 % of the load on an Intel Xscale’s execution pipeline can be removed by using the MARF, HLU, and AGU mechanisms. This paper presents architectural mechanisms which enhance AGP ald LCP in the ACT coprocessor. Previous research efforts [8, 9] have focused on less general strategies than those introduced here. 2. ARCHITECTURE The ACT coprocessor architecture [6] is shown in Figure 1. A host processor loads frames of data into the input memory and pulls output frames from the output <b>memory.</b> The two <b>scratch</b> <b>memories</b> are local storage resources. The input memory is dual-ported and used in a double buffered fashion. Memories are 16 -bits wide and have 1024 entries, except for the input memory which has 2048 entries. ACT handles the computationally intense real-time tasks on the four clusters of execution units. The multiple individually addressed SRAMs provides adequate bandwidth to avoid: XU starvation, cache control energy, and delay uncertainty. Caches are not particularly effective for stream based applications. ACT provides more precise program controlled data movement. These choices also increase the need to exploit AGP. 1...|$|R
40|$|The {{concept of}} Parallel Vector (<b>scratch</b> pad) <b>Memories</b> (PVM) was {{introduced}} as one solution for Parallel Computing in DSP, which can provides parallel memory addressing efficiently with minimum latency. The parallel programming more efficient {{by using the}} parallel addressing generator for parallel vector memory (PVM) proposed in this thesis. However, without hiding complexities by cache, the cost of programming is high. To minimize the programming cost, automatic parallel memory address generation is needed to hide the complexities of memory access. This thesis investigates methods for implementing conflict-free vector addressing algorithms on a parallel hardware structure. In particular, match vector addressing requirements extracted from the behaviour model to a prepared parallel memory addressing template, in order to supply data in parallel from the main memory to the on-chip vector memory. According to the template and usage of the main and on-chip parallel vector memory, models for data pre-allocation and permutation in <b>scratch</b> pad <b>memories</b> of ASIP can be decided and configured. By exposing the parallel memory access of source code, the memory access flow graph (MFG) will be generated. Then MFG will be used combined with hardware information to match templates in the template library. When it is matched with one template, suited permutation equation will be gained, and the permutation table that include target addresses for data pre-allocation and permutation is created. Thus {{it is possible to}} automatically generate memory address for parallel memory accesses. A tool for achieving the goal mentioned above is created, Permutator, which is implemented in C++ combined with XML. Memory access coding template is selected, as a result that permutation formulas are specified. And then PVM address table could be generated to make the data pre-allocation, so that efficient parallel memory access is possible. The result shows that the memory access complexities is hiden by using Permutator, so that the programming cost is reduced. It works well in the context that each algorithm with its related hardware information is corresponding to a template case, so that extra memory cost is eliminated...|$|R
40|$|In today's {{embedded}} applications {{a significant}} portion of energy is spent in the memory subsystem. Several approaches have been proposed to minimize this energy, including the use of <b>scratch</b> pad <b>memories,</b> with many based on static analysis of a program. However, often {{it is not possible to}} perform static analysis and optimization of a program's memory access behavior unless the program is specifically written for this purpose. In this paper we introduce the FORAY model of a program that permits aggressive analysis of the application's memory behavior that further enables such optimizations since it consists of 'for' loops and array accesses which are easily analyzable. We present FORAY-GEN: an automated profile-based approach for extraction of the FORAY model from the original program. We also demonstrate how FORAY-GEN enhances applicability of other memory subsystem optimization approaches, resulting in an average of two times increase in the number of memory references that can be analyzed by existing static approaches. Comment: Submitted on behalf of EDAA ([URL]...|$|R
40|$|Limited Local Memory (LLM) {{architectures}} {{are popular}} scalable memory multi-core architectures {{in which each}} core has a local software-controlled memory, e. g., the IBM Cell processor. While similar to the <b>scratch</b> pad <b>memories</b> (SPMs) in embedded systems, local memory architecture is different: instead of being {{in addition to the}} cached memory hier-archy, local memories {{are a part of the}} only memory hierarchy of the core. Consequently, different schemes of managing local memory are needed. The existing circular stack man-agement is a promising approach, but is only for extremely embedded applications, with constraints on the maximum stack size, and limited use of pointers. This research presents a generalized approach to manage the stack data of any application in a constant space on the local memory. The experimental results on benchmarks from MiBench running on the IBM Cell processor in Sony Playstation 3, show gains in programmability, and generalization without much loss of performance. ii...|$|R
40|$|Submitted {{on behalf}} of EDAA ([URL] audienceIn today's {{embedded}} applications {{a significant portion of}} energy is spent in the memory subsystem. Several approaches have been proposed to minimize this energy, including the use of <b>scratch</b> pad <b>memories,</b> with many based on static analysis of a program. However, often {{it is not possible to}} perform static analysis and optimization of a program's memory access behavior unless the program is specifically written for this purpose. In this paper we introduce the FORAY model of a program that permits aggressive analysis of the application's memory behavior that further enables such optimizations since it consists of 'for' loops and array accesses which are easily analyzable. We present FORAY-GEN: an automated profile-based approach for extraction of the FORAY model from the original program. We also demonstrate how FORAY-GEN enhances applicability of other memory subsystem optimization approaches, resulting in an average of two times increase in the number of memory references that can be analyzed by existing static approaches...|$|R
40|$|Recent {{advances}} in circuit and process technologies have pushed non-volatile memory technologies {{into a new}} era. These technologies exhibit appealing properties such as low power consumption, non-volatility, shock-resistivity, and high density. However, there are challenges to which we need answers in the road of applying non-volatile memories as main memory in computer systems. First, non-volatile memories have limited number of write/erase cycles compared with DRAM memory. Second, write activities on non-volatile memory are more expensive than DRAM memory in terms of energy consumption and access latency. Both challenges will benefit from reduction of the write activities on the nonvolatile memory. In this paper, we target embedded Chip Multiprocessors (CMPs) with <b>Scratch</b> Pad <b>Memory</b> (SPM) and non-volatile main memory. We introduce data migration and recomputation techniques {{to reduce the number}} of write activities on non-volatile memories. Experimental results show that the proposed methods can reduce the number of writes by 59. 41 % on average, which means that the non-volatile memory can last 2. 8 times as long as before. Meanwhile, the finish time of programs is reduced by 31. 81 % on average...|$|R
40|$|Abstract—Scratch pad {{memories}} (SPM) {{are attractive}} alternatives for caches on multicore systems since caches are relatively expensive {{in terms of}} area and energy consumption. The key to effectively utilizing SPMs on multicore systems is the data placement algorithm. In this paper, two polynomial time algorithms, regional data placement for multicore (RDPM) and regional data placement for multicore with duplication (RDPM-DUP), have been proposed to generate near-optimal data placement with minimum total cost. There is only one copy for each data in RDPM, while RDPM-DUP allows data duplication. Experimental {{results show that the}} proposed RDPM algorithm alone can reduce the time cost of memory accesses by 32. 68 % on average compared with existing algorithms. With data duplication, the RDPM-DUP algorithm further reduces the time cost by 40. 87 %. In terms of energy consumption, the proposed RDPM algorithm with exclusive copy can reduce the total cost by 33. 47 % on average. When RDPM-DUP is applied, the improvement increases up to 38. 15 % on average. Index Terms—Data duplication, data placement, embedded systems, multicore, <b>scratch</b> pad <b>memory.</b> I...|$|R
40|$|Abstract — The memory {{subsystem}} {{is a major}} contributor to the performance, power, and area of complex SoCs used in feature rich multimedia products. Hence, memory architecture of the embedded DSP is complex and usually custom designed with multiple banks of single-ported or dual ported on-chip <b>scratch</b> pad <b>memory</b> and multiple banks of off-chip memory. Building software for such large complex memories with many of the software components as individually optimized software IPs is a big challenge. In order to obtain good performance and a reduction in memory stalls, the data buffers of the application need to be placed carefully in different types of memory. In this paper we present a unified framework (MODLEX) that combines different data layout optimizations to address the complex DSP memory architectures. Our method models the data layout problem as multi-objective Genetic Algorithm (GA) with performance and power being the objectives and presents a set of solution points which is attractive from a platform design viewpoint. While most of the work in the literature assumes that performance and power are non-conflicting objectives, our work demonstrates that there is significant trade-off (up to 70 %) that is possible between power and performance. I...|$|R
