2405|1028|Public
25|$|The {{running time}} of the {{algorithm}} and the <b>space</b> <b>complexity</b> is , {{much better than the}} O(n) running {{time of the}} naive brute force calculation.|$|E
25|$|Many {{problems}} in graph algorithms may be solved efficiently on graphs of bounded pathwidth, by using dynamic programming on a path-decomposition of the graph. Path decomposition {{may also be}} used to measure the <b>space</b> <b>complexity</b> of dynamic programming algorithms on graphs of bounded treewidth.|$|E
25|$|The {{asymptotic}} complexity {{is defined}} by the most efficient (in terms of whatever computational resource one is considering) algorithm for solving the game; the most common complexity measure (computation time) is always lower-bounded by the logarithm of the asymptotic state-space complexity, since a solution algorithm must work for every possible state of the game. It will be upper-bounded by the complexities of each individual algorithm for the family of games. Similar remarks apply to the second-most commonly used complexity measure, the amount of space or computer memory used by the computation. It is not obvious that there is any lower bound on the <b>space</b> <b>complexity</b> for a typical game, because the algorithm need not store game states; however many games of interest are known to be PSPACE-hard, and it follows that their <b>space</b> <b>complexity</b> will be lower-bounded by the logarithm of the asymptotic state-space complexity as well (technically the bound is only a polynomial in this quantity; but it is usually known to be linear).|$|E
40|$|Abstract: In this paper, {{we propose}} an {{algorithm}} for shattering {{a set of}} disjoint line segments of arbitrary length and orientation placed arbitrarily on a 2 D plane. The time and <b>space</b> <b>complexities</b> of our algorithm are O(n 2) and O(n) respectively. It is an improvement ofthe O(n 2 logn) time algorithm proposed in [5]. A minor modi cation of this algorithm applies when objects are simple polygons, keeping the time and <b>space</b> <b>complexities</b> invariant...|$|R
3000|$|... at most. With the {{constraints}} (6) on the displacement vectors at the block corners, the above {{time and the}} <b>space</b> <b>complexities</b> become [...]...|$|R
40|$|In {{the model}} of local {{computation}} algorithms (LCAs), we aim to compute the queried part of the output by examining only a small (sublinear) portion of the input. Many recently developed LCAs on graph problems achieve time and <b>space</b> <b>complexities</b> with very low dependence on n, the number of vertices. Nonetheless, these complexities are generally at least exponential in d, the upper bound {{on the degree of}} the input graph. Instead, we consider the case where parameter d can be moderately dependent on n, and aim for complexities with subexponential dependence on d, while maintaining polylogarithmic dependence on n. We present: a randomized LCA for computing maximal independent sets whose time and <b>space</b> <b>complexities</b> are quasi-polynomial in d and polylogarithmic in n; for constant ϵ > 0, a randomized LCA that provides a (1 -ϵ) -approximation to maximum matching whose time and <b>space</b> <b>complexities</b> are polynomial in d and polylogarithmic in n...|$|R
25|$|Rather than {{bounding}} {{the time}} complexity of an algorithm that recognizes an MSO property on bounded-treewidth graphs, {{it is also}} possible to analyze the <b>space</b> <b>complexity</b> of such an algorithm; that is, the amount of memory needed above and beyond the size of the input itself (which is assumed to be represented in a read-only way so that its space requirements cannot be put to other purposes).|$|E
25|$|Because of the {{simplicity}} of tic-tac-toe, it is often used as a pedagogical tool for teaching the concepts of good sportsmanship and the branch of artificial intelligence that deals with the searching of game trees. It is straightforward to write a computer program to play tic-tac-toe perfectly, to enumerate the 765 essentially different positions (the state <b>space</b> <b>complexity),</b> or the 26,830 possible games up to rotations and reflections (the game tree complexity) on this space.|$|E
25|$|Merge sort takes {{advantage}} of the ease of merging already sorted lists into a new sorted list. It starts by comparing every two elements (i.e., 1 with 2, then 3 with 4...) and swapping them if the first should come after the second. It then merges each of the resulting lists of two into lists of four, then merges those lists of four, and so on; until at last two lists are merged into the final sorted list. Of the algorithms described here, this is the first that scales well to very large lists, because its worst-case running time is O(n log n). It is also easily applied to lists, not only arrays, as it only requires sequential access, not random access. However, it has additional O(n) <b>space</b> <b>complexity,</b> and involves a large number of copies in simple implementations.|$|E
40|$|AbstractIn this paper, {{we propose}} an {{algorithm}} for shattering {{a set of}} disjoint line segments of arbitrary length and orientation placed arbitrarily on a 2 D plane. The time and <b>space</b> <b>complexities</b> of our algorithm are O(n 2) and O(n), respectively. It is an improvement over the O(n 2 logn) time algorithm proposed in (R. Freimer, J. S. B. Mitchell, C. D. Piatko, On the complexity of shattering using arrangements, Canadian Conference on Computational Geometry, 1990, pp. 218 – 222.). A minor modification of this algorithm applies when objects are simple polygons, keeping the time and <b>space</b> <b>complexities</b> invariant...|$|R
40|$|Abstract: Adirected {{spanning}} {{tree in a}} directed graph G =(V,A) is a {{spanning tree}} such that no two arcs share their tails. In this paper, we propose an algorithm for listing all directed spanning trees of G. Its time and <b>space</b> <b>complexities</b> are O(|A | + ND(|V |, |A|)) and O(|A | +DS(|V |, |A|)), where D(|V |, |A|) and DS(|V |, |A|) are the time and <b>space</b> <b>complexities</b> of the data structure for updating the minimum spanning tree in an undirected graph with |V | vertices and |A | edges. Here N denotes the number of directed spanning trees in G...|$|R
40|$|In this paper, we {{revisit the}} much studied LCS problem for two given sequences. Based on the {{algorithm}} of Iliopoulos and Rahman for solving the LCS problem, we have suggested 3 new improved algorithms. We first reformulate {{the problem in}} a very succinct form. The problem LCS is abstracted to an abstract data type DS on an ordered positive integer set with a special operation Update(S,x). For the two input sequences X and Y of equal length n, the first improved algorithm uses a van Emde Boas tree for DS and its time and <b>space</b> <b>complexities</b> are O(R n+n) and O(R), where R {{is the number of}} matched pairs of the two input sequences. The second algorithm uses a balanced binary search tree for DS and its time and <b>space</b> <b>complexities</b> are O(R L+n) and O(R), where L is the length of the longest common subsequence of X and Y. The third algorithm uses an ordered vector for DS and its time and <b>space</b> <b>complexities</b> are O(nL) and O(R) ...|$|R
25|$|There {{are several}} {{companion}} notions {{closely related to}} the notion of an isoperimetric function. Thus an isodiametric function bounds the smallest diameter (with respect to the simplicial metric where every edge has length one) of a van Kampen diagram for a particular relation w in terms of the length of w. A filling length function the smallest filling length of a van Kampen diagram for a particular relation w in terms of the length of w. Here the filling length of a diagram is the minimum, over all combinatorial null-homotopies of the diagram, of the maximal length of intermediate loops bounding intermediate diagrams along such null-homotopies. The filling length function is {{closely related to the}} non-deterministic <b>space</b> <b>complexity</b> of the word problem for finitely presented groups. There are several general inequalities connecting the Dehn function, the optimal isodiametric function and the optimal filling length function, but the precise relationship between them is not yet understood.|$|E
500|$|... was {{the first}} to define {{symmetric}} Turing machines and symmetric <b>space</b> <b>complexity</b> classes such as SL (an undirected or reversible analogue of nondeterministic <b>space</b> <b>complexity,</b> later shown to coincide with deterministic logarithmic space).|$|E
500|$|... used book {{embedding}} {{to study}} the computational complexity theory of the reachability problem in directed graphs. As they have observed, reachability for two-page directed graphs may be solved in unambiguous logarithmic space (the analogue, for logarithmic <b>space</b> <b>complexity,</b> of the class UP of unambiguous polynomial-time problems). However, reachability for three-page directed graphs requires the full power of nondeterministic logarithmic space. Thus, book embeddings seem intimately connected with the distinction between these two complexity classes.|$|E
40|$|Approximation {{has been}} shown to be an {{effective}} method for reducing the time and space costs of solving various floorplan area minimization problems. In this paper, we present several approximation techniques for solving floorplan area minimization problems. These new techniques enable us to reduce both the time and <b>space</b> <b>complexities</b> of the previously best known approximation algorithms by more than a factor of n and n² for rectangular and L-shaped subfloorplans, respectively (where n is the number of given implementations). The efficiency in the time and <b>space</b> <b>complexities</b> is critical to the applicability of such approximation techniques in floorplan area minimization algorithms. We also give a technique for enhancing the quality of approximation results...|$|R
40|$|AbstractWe {{present an}} {{algorithm}} for finding k nearest neighbors {{of a given}} query line among a set of n points distributed arbitrarily on a two-dimensional plane. Our algorithm requires O(n 2) time and O(n 2 /logn) space to preprocess the given set of points, and it answers the query for a given line in O(k+logn) time, where k may also be an input at the query time. Almost a similar technique works for finding k farthest neighbors of a query line, keeping the time and <b>space</b> <b>complexities</b> invariant. We also show that if k is known {{at the time of}} preprocessing, the time and <b>space</b> <b>complexities</b> for the preprocessing can be reduced keeping the query times unchanged...|$|R
3000|$|The {{utility of}} the <b>complexity</b> <b>space</b> in {{algorithm}} <b>complexity</b> is given, in part, by thecomputational interpretation of the numerical value [...]...|$|R
2500|$|... {{identical}} {{and nearly}} identical sequences (known as repeats) which can, {{in the worst}} case, increase the time and <b>space</b> <b>complexity</b> of algorithms exponentially; ...|$|E
2500|$|The in-place {{version of}} {{quicksort}} has a <b>space</b> <b>complexity</b> of , {{even in the}} worst case, when it is carefully implemented using the following strategies: ...|$|E
2500|$|The Smith–Waterman {{algorithm}} is fairly demanding of time: To align two sequences of lengths [...] and , [...] time is required. Gotoh and Altschul optimized the algorithm to [...] steps. The <b>space</b> <b>complexity</b> was optimized by Myers and Miller from [...] to [...] (linear), where [...] is {{the length of}} the shorter sequence.|$|E
40|$|Abstract. A new {{algorithm}} is presented for rate-fidelity optimal packetization of scalable source bit streams with uneven erasure protection. It provides the globally optimal solution for input sources of convex rate-fidelity function {{and for a}} wide class of erasure channels, including channels for which the probability of losing n packets is monotonically decreasing in n, and independent erasure channels with packet erasure rate smaller than 0. 5. The time and <b>space</b> <b>complexities</b> of the new algorithm are both O(NL), where N {{is the number of}} packets and L is the packet payload size, comparing to the O(NL 2) time and <b>space</b> <b>complexities</b> of the existing globally optimal solution. When applied to SPIHT compressed images, the results of the proposed algorithm are virtually the same as the globally optima. 1...|$|R
40|$|In {{the model}} of local {{computation}} algorithms (LCAs), we aim to compute the queried part of the output by examining only a small (sublinear) portion of the input. Many recently developed LCAs on graph problems achieve time and <b>space</b> <b>complexities</b> with very low dependence on n, the number of vertices. Nonetheless, these complexities are generally at least exponential in d, the upper bound {{on the degree of}} the input graph. Instead, we consider the case where parameter d can be moderately dependent on n, and aim for complexities with quasi-polynomial dependence on d, while maintaining polylogarithmic dependence on n. In this thesis, we give randomized LCAs for com-puting maximal independent sets, maximal matchings, and approximate maximum matchings. Both time and <b>space</b> <b>complexities</b> of our LCAs on these problems ar...|$|R
40|$|Traditionally {{in double}} auctions, offers are cleared at the equilib-rium price. In this paper, we {{introduce}} a novel, non-recursive, matching algorithm for double auctions, {{which aims to}} maximize the amount of commodities to be traded. Our algorithm has lower time and <b>space</b> <b>complexities</b> than existing algorithms. Categories and Subject Descriptor...|$|R
2500|$|The {{beginning}} of systematic studies in computational complexity {{is attributed to}} the seminal 1965 paper [...] "On the Computational Complexity of Algorithms" [...] by Juris Hartmanis and Richard E. Stearns, which laid out the definitions of time complexity and <b>space</b> <b>complexity,</b> and proved the hierarchy theorems. In addition, in 1965 Edmonds suggested to consider a [...] "good" [...] algorithm to be one with running time bounded by a polynomial of the input size.|$|E
2500|$|Generally, [...] {{will not}} be {{specified}} as a table of values, {{the way it is}} shown in the figure above. Rather, a cycle detection algorithm may be given access either to the sequence of values , or to [...] a subroutine for calculating [...] The task is to find [...] and [...] while examining as few values from the sequence or performing as few subroutine calls as possible. Typically, also, the <b>space</b> <b>complexity</b> of an algorithm for the cycle detection problem is of importance: we wish to solve the problem while using an amount of memory significantly smaller than it would take to store the entire sequence.|$|E
2500|$|If {{the input}} is given as a {{subroutine}} for calculating , the cycle detection {{problem may be}} trivially solved using only [...] function applications, simply by computing the sequence of values [...] and using a data structure such as a hash table to store these values and test whether each subsequent value has already been stored. However, the <b>space</b> <b>complexity</b> of this algorithm is proportional to , unnecessarily large. Additionally, to implement this method as a pointer algorithm would require applying the equality test to each pair of values, resulting in quadratic time overall. Thus, {{research in this area}} has concentrated on two goals: using less space than this naive algorithm, and finding pointer algorithms that use fewer equality tests.|$|E
50|$|BitWise is {{the main}} {{flagship}} event of the fest. Team size {{is limited to a}} maximum of three, and solutions have traditionally been expected to be provided in C (programming language) or C++. The solutions are evaluated not only on the basis of correctness, but also on their time and <b>space</b> <b>complexities.</b>|$|R
40|$|AbstractIn 1999, Romaguera and Schellekens {{introduced}} {{the theory of}} dual <b>complexity</b> <b>spaces</b> {{as a part of}} the development of a mathematical (topological) foundation for the complexity analysis of programs and algorithms [S. Romaguera, M. P. Schellekens, Quasi-metric properties of <b>complexity</b> <b>spaces,</b> Topology Appl. 98 (1999) 311 – 322]. In this work we extend the theory of dual <b>complexity</b> <b>spaces</b> to the case that the complexity functions are valued on an ordered normed monoid. We show that the <b>complexity</b> <b>space</b> of an ordered normed monoid inherits the ordered normed structure. Moreover, the order structure allows us to prove some topological and quasi-metric properties of the new dual <b>complexity</b> <b>spaces.</b> In particular, we show that these <b>complexity</b> <b>spaces</b> are, under certain conditions, Hausdorff and satisfy a kind of completeness. Finally, we develop a connection of our new approach with Interval Analysis...|$|R
40|$|AbstractWe study domain theoretic {{properties}} of <b>complexity</b> <b>spaces.</b> Although the so-called <b>complexity</b> <b>space</b> {{is not a}} domain for the usual pointwise order, we show that, however, each pointed <b>complexity</b> <b>space</b> is an ω-continuous domain for which the complexity quasi-metric induces the Scott topology, and the supremum metric induces the Lawson topology. Hence, each pointed <b>complexity</b> <b>space</b> is both a quantifiable domain {{in the sense of}} M. Schellekens and a quantitative domain in the sense of P. Waszkiewicz, via the partial metric induced by the complexity quasi-metric...|$|R
2500|$|Many {{problems}} in graph algorithms may be solved efficiently on graphs of low pathwidth, by using dynamic programming on a path-decomposition of the graph. For instance, if a linear ordering of the vertices of an n-vertex graph G is given, with vertex separation number w, {{then it is}} possible to find the maximum independent set of G in time [...] On graphs of bounded pathwidth, this approach leads to fixed-parameter tractable algorithms, parametrized by the pathwidth. Such results are not frequently found in the literature because they are subsumed by similar algorithms parametrized by the treewidth; however, pathwidth arises even in treewidth-based dynamic programming algorithms in measuring the <b>space</b> <b>complexity</b> of these algorithms.|$|E
50|$|The {{measure of}} <b>space</b> <b>complexity</b> {{in terms of}} DSPACE is useful because it {{represents}} {{the total amount of}} memory that an actual computer would need to solve a given computational problem with a given algorithm. The reason is that DSPACE describes the <b>space</b> <b>complexity</b> used by deterministic Turing machines, which can represent actual computers. On the other hand, NSPACE describes the <b>space</b> <b>complexity</b> of non-deterministic Turing machines, which are not useful when trying to represent actual computers. For this reason, NSPACE is limited in its usefulness to real-world applications.|$|E
5000|$|... #Subtitle level 2: Time and <b>space</b> <b>complexity</b> of {{top-down}} parsing ...|$|E
40|$|A new {{algorithm}} is introduced for analyzing possible nesting in Mobile Ambient calculus. It improves both time and <b>space</b> <b>complexities</b> {{of the technique}} proposed by Nielson and Seidl. The improvements are achieved by enhancing the data structure representations, and by reducing the computation to the Control Flow Analysis constraints that are effectively necessary {{to get to the}} least solution...|$|R
40|$|We {{present an}} {{algorithm}} for computing the zeta function of an arbitrary hyperelliptic curve over a nite eld Fq of characteristic 2, thereby extending the algorithm of Kedlaya for odd characteristic. For a genus g hyperelliptic curve de ned over F 2 n, the average-case time complexity), whereas the worst-case time and <b>space</b> <b>complexities</b> are O(g) and) respectively...|$|R
40|$|AbstractIn this paper, we {{will study}} the problem of {{locating}} {{the center of the}} smallest enclosing circle of a set P of n points, where the center is constrained to lie on a query line segment. The preprocessing time and <b>space</b> <b>complexities</b> of our proposed algorithm are O(nlogn) and O(n) respectively; the query time complexity is O(log 2 n) ...|$|R
