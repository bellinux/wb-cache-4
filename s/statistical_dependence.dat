551|129|Public
25|$|By {{combining}} the Adair treatment with the Hill plot, one {{arrives at the}} modern experimental definition of cooperativity (Hill, 1985, Abeliovich, 2005). The resultant Hill coefficient, or more correctly {{the slope of the}} Hill plot as calculated from the Adair Equation, can be shown to be the ratio between the variance of the binding number to the ratio of the binding number in an equivalent system of non-interacting binding sites. Thus, the Hill coefficient defines cooperativity as a <b>statistical</b> <b>dependence</b> of one binding site on the state of other site(s).|$|E
50|$|If a {{time series}} {Xt} is stationary, then <b>statistical</b> <b>dependence</b> between the pair (Xt , Xs) would imply {{that there is}} <b>statistical</b> <b>dependence</b> between all pairs of values at the same lag s&#x2212;t.|$|E
5000|$|Constraint in {{information}} theory {{refers to the}} degree of <b>statistical</b> <b>dependence</b> between or among variables.|$|E
40|$|Some {{concepts}} of information theory, as entropy, conditional entropy and mutual {{information may be}} very useful to analyse several financial time series, especially <b>statistical</b> <b>dependences.</b> Trough the similarity between those measures and variance analysis is possible {{to get a new}} approach to study the level of <b>statistical</b> <b>dependences</b> in financial time series...|$|R
40|$|A generic {{approach}} to perceptual pattern-recognition and data fusion is presented. Unsupervised neural self-organisation {{is used to}} discover and encode the component features of the training data, so that those components are subsequently detected with generalisation and discrimination. The approach {{is designed to be}} used in hierarchical networks to encode structures at many levels of abstraction. Components are extracted {{on the basis of the}} <b>statistical</b> <b>dependences</b> within the data rather than prior assumptions about the data: properties such as size-scales are deduced from the ranges of <b>statistical</b> <b>dependences.</b> The pattern-recognition functions of generalisation and discrimination emerge from the self-organisation process, without having been optimised explicitly. Neurons acquire unknown transformation invariances through a spontaneous symmetry-breaking mechanism, which allows weight-vector symmetries to emerge in a data-driven way. The principles are demonstrated using a handwriting-i [...] ...|$|R
40|$|The {{components}} light-ship {{displacement of}} the modern tankers, constructed on supervision of domestic and foreign classification societies are analyzed. The <b>statistical</b> <b>dependences</b> allowing at the initial stage of designing to define components mass load of the tanker are received. The analysis of the received results is carried out and their adequacy is checked up...|$|R
5000|$|... if {{more than}} one {{variable}} is measured, a measure of <b>statistical</b> <b>dependence</b> such as a correlation coefficient ...|$|E
50|$|Correlation matrix and variance-covariance matrix: A basic tool in {{understanding}} <b>statistical</b> <b>dependence</b> among variables. The {{degree of correlation}} indicates the tendency of one change to indicate the likely change in another.|$|E
5000|$|A {{measure of}} the <b>statistical</b> <b>dependence</b> between random {{variables}} [...] and [...] (from any domains on which sensible kernels can be defined) can be formulated based on the Hilbert-Schmidt Independence Criterion ...|$|E
40|$|Efficient coding {{of natural}} scenes {{requires}} {{the use of}} a proper filter to reduce their redundancy. In this work we show that nongaussian statistical properties of natural scenes uniquely define a wavelet filter that decomposes the image in a set of statistically independent resolution levels. The spatial <b>statistical</b> <b>dependences</b> still present at a fixed scale are extremely short-ranged...|$|R
40|$|The {{transcription}} {{of genes}} is often regulated {{not only by}} transcription factors binding at single sites per promoter, but by the interplay of multiple copies {{of one or more}} transcription factors binding at multiple sites forming a cis-regulatory module. The computational recognition of cis-regulatory modules from ChIP-seq or other high-throughput data is crucial in modern life and medical sciences. A common type of cis-regulatory modules are homotypic clusters of binding sites, i. e., clusters of binding sites of one transcription factor. For their recognition the homotypic Sunflower Hidden Markov Model is a promising statistical model. However, this model neglects <b>statistical</b> <b>dependences</b> among nucleotides within binding sites and flanking regions, which makes it not well suited for de-novo motif discovery. Here, we propose an extension of this model that allows <b>statistical</b> <b>dependences</b> within binding sites, their reverse complements, and flanking regions. We study the efficacy of this extended homotypic Sunflower Hidden Markov Model based on ChIP-seq data from the Human ENCODE Project and find that it often outperforms the traditional homotypic Sunflower Hidden Markov Model...|$|R
40|$|In this paper, {{we discuss}} the use of Stein’s {{principle}} in or-der to build new multivariate estimators for multispectral images corrupted by noise. The proposed shrinkage rules allow us {{to take into account}} both the spatial and inter-band <b>statistical</b> <b>dependences.</b> We also compare the merits of various pre-processing transforms for improving the per-formance of these new denoising methods. Simulation ex-amples using satellite images are shown so as to validate the proposed approach. 1...|$|R
50|$|In statistics, the Kendall rank {{correlation}} coefficient, {{commonly referred to}} as Kendall's tau coefficient (after the Greek letter τ), is a statistic used to measure the ordinal association between two measured quantities. A tau test is a non-parametric hypothesis test for <b>statistical</b> <b>dependence</b> based on the tau coefficient.|$|E
50|$|Note that, in {{contrast}} to many available methods for generating random numbers from non-uniform distributions, random variates generated directly by this approach will exhibit serial <b>statistical</b> <b>dependence.</b> This is because to draw the next sample, we define the slice based {{on the value of}} f(x) for the current sample.|$|E
50|$|There {{are several}} other {{numerical}} measures that quantify {{the extent of}} <b>statistical</b> <b>dependence</b> between pairs of observations. The most common {{of these is the}} Pearson product-moment correlation coefficient, which is a similar correlation method to Spearman's rank, that measures the “linear” relationships between the raw numbers rather than between their ranks.|$|E
40|$|Two {{measures}} which indicate <b>statistical</b> serial <b>dependence</b> were evaluated. The n-dimensional Average Stored Information index (ndASI) is {{a measure}} of conditional information, which compares the entropies of higher order conditional distributions to estimate average <b>statistical</b> serial <b>dependence.</b> The generalized ranked joint interval histogram (RJIH-o) is a new nonparametric graphical analysis method. It extends the joint interval histogram by depicting longer interval sequences and can be interpereted precisely analagously to the joint interval histogram. The generalized ranked joint interval histogram correctly represents independence in a Poisson process model and <b>statistical</b> serial <b>dependence</b> in a directionally reinforced Markov model. The generalized ranked joint interval histogram correctly depicts the underlying periodic and strange attractors in a standard map model. Both measures can be used to effectively analyze interspike interval sequences from spontaneous neurophsyiological activity effectively...|$|R
40|$|We {{propose a}} new {{approach}} to infer the causal structure that has generated the observed <b>statistical</b> <b>dependences</b> among n random variables. The idea is that the factorization of the joint measure of cause and effect into P(cause) P(effect|cause) leads typically to simpler conditionals than non-causal factorizations. To evaluate the complexity of the conditionals we have tried two methods. First, we have compared them to those which maximize the conditional entropy subject to the observed first and second moments since we consider the latter as the simplest conditionals. Second, we have fitted the data with conditional probability measures being exponents of functions in an RKHS space and defined the complexity by a Hilbert-space semi-norm. Such a complexity measure has several properties that are useful for our purpose. We describe some encouraging results with both methods applied to real-world data. Moreover, we have combined constraint-based approaches to causal discovery (i. e., methods using only information on conditional <b>statistical</b> <b>dependences)</b> with our method in order to distinguish between causal hypotheses which are equivalent with respect to the imposed independences. Furthermore, we compare the performance to Bayesian approaches to causal inference...|$|R
40|$|International audienceThe various {{scales of}} a signal {{maintain}} relations of dependence {{the ones with}} the others. Those can vary in time and reveal speed changes in the studied phenomenon. In the goal to establish these changes, one shall compute first the wavelet transform of a signal, on various scales. Then one shall study the <b>statistical</b> <b>dependences</b> between these transforms thanks to an estimator of mutual information (MI) called divergence. The time-scale representation of the sources representation shall be compared with the representation of the mixtures according to delay in time and in frequency...|$|R
50|$|In {{statistics}} and in probability theory, distance correlation {{is a measure}} of <b>statistical</b> <b>dependence</b> between two random variables or two random vectors of arbitrary, not necessarily equal, dimension. It is zero if and only if the random variables are statistically independent, unlike Pearson's correlation, which can be zero for dependent random variables.|$|E
5000|$|In statistics, Spearman's rank {{correlation}} coefficient or Spearman's rho, named after Charles Spearman and often denoted by the Greek letter [...] (rho) or as , is a nonparametric measure of {{rank correlation}} (<b>statistical</b> <b>dependence</b> between the ranking of two variables). It assesses {{how well the}} relationship between two variables can be described using a monotonic function.|$|E
5000|$|A rank {{correlation}} {{can be used}} to compare two rankings for the same set of objects. For example, Spearman's {{rank correlation}} coefficient is useful to measures the <b>statistical</b> <b>dependence</b> between the rankings of athletes in two tournaments. Another example is the [...] "Rank-rank hypergeometric overlap" [...] approach, which is designed to compare ranking of the genes that are at the [...] "top" [...] of two ordered lists of differentially expressed genes.|$|E
40|$|We {{refine the}} {{classical}} Independent Component Analysis (ICA) decomposition using a multilinear {{expansion of the}} probability density function of the source statistics. In particular, to model the source statistics of natural image textures, we introduce a specific non-linear system {{that allows us to}} elegantly capture the <b>statistical</b> <b>dependences</b> between the responses of the Multilinear ICA (MICA) filters. The resulting multilinear probability density is analytically tractable and does not require Monte Carlo simulations to estimate the model parameters. We demonstrate the success of the MICA model on natural textures and discuss applications to non-stationarity detection and natural scene statistics (NSS) modeling...|$|R
40|$|We {{describe}} a causal learning method, which employs measuring {{the strength of}} <b>statistical</b> <b>dependences</b> {{in terms of the}} Hilbert-Schmidt norm of kernel-based cross-covariance operators. Following the line of the common faithfulness assumption of constraint-based causal learning, our approach assumes that a variable Z {{is likely to be a}} common effect of X and Y, if conditioning on Z increases the dependence between X and Y. Based on this assumption, we collect "votes" for hypothetical causal directions and orient the edges by the majority principle. In most experiments with known causal structures, our method provided plausible results and outperformed the conventional constraint-based PC algorithm...|$|R
40|$|We {{describe}} {{a method that}} infers whether <b>statistical</b> <b>dependences</b> between two observed variables X and Y are due to a " causal link or only due to a connecting causal path that contains an unobserved variable of low complexity, e. g., a binary variable. This problem is motivated by statistical genetics. Given a genetic marker that is correlated with a phenotype of interest, we want to detect whether this marker is causal or it only correlates with a causal one. Our method {{is based on the}} analysis of the location of the conditional distributions P(Y jx) in the simplex of all distributions of Y. We report encouraging results on semi-empirical data...|$|R
5000|$|It {{is common}} {{practice}} in some disciplines, other than statistics and time series analysis, to drop the normalization by σ2 and use the term [...] "autocorrelation" [...] interchangeably with [...] "autocovariance". However, the normalization is important both because {{the interpretation of the}} autocorrelation as a correlation provides a scale-free measure of the strength of <b>statistical</b> <b>dependence,</b> and because the normalization has an effect on the statistical properties of the estimated autocorrelations.|$|E
50|$|In most experiments, {{measurements}} are repeatedly {{made at the}} same two locations. Under local realism, there could be effects of memory leading to <b>statistical</b> <b>dependence</b> between subsequent pairs of measurements. Moreover, physical parameters might be varying in time. It has been shown that, provided each new pair of measurements is done with a new random pair of measurement settings, that neither memory nor time inhomogeneity cannot have a serious effect on the experiment.|$|E
50|$|A {{distance}} between populations {{can be interpreted}} as measuring the {{distance between}} two probability distributions and hence they are essentially measures of distances between probability measures. Where statistical distance measures relate to the differences between random variables, these may have <b>statistical</b> <b>dependence,</b> and hence these distances are not directly related to measures of distances between probability measures. Again, a measure of distance between random variables may relate to the extent of dependence between them, rather than to their individual values.|$|E
40|$|Both contact {{resistivity}} of Au—Ti—Pd—n-Si {{ohmic contact}} and mechanism of current flow are {{studied in the}} 100 — 360 K temperature range. A method is proposed for reduction of error in determination of contact resistivity based on analysis of <b>statistical</b> <b>dependences</b> of the measured contact resistivity values (which {{are in the range}} of (0. 9 — 2) • 10 – 5 Ω•cm 2). On the basis of the contact resistivity temperature dependence, it is found for an ohmic contact with barrier height of 0. 22 eV that the field mechanism of current flow is predominant in the 100 — 200 K temperature range, while thermal-field emission with activation energy of 0. 08 eV is predominant in the 200 — 360 K temperature range...|$|R
40|$|This paper {{presents}} an innovative {{model of a}} program’s internal behavior over a set of test inputs, called the probabilistic program dependence graph (PPDG), that facilitates probabilistic analysis and reasoning about uncertain program behavior, particularly that associated with faults. The PPDG is an augmentation of the structural dependences represented by a program dependence graph with estimates of <b>statistical</b> <b>dependences</b> between node states, which are computed from the test set. The PPDG {{is based on the}} established framework of probabilistic graphical models, which are widely used in applications such as medical diagnosis. This paper presents algorithms for constructing PPDGs and applying the PPDG to fault diagnosis. This paper also presents preliminary evidence indicating that PPDGs can facilitate fault localization and fault comprehension...|$|R
40|$|International audienceThe various {{scales of}} a signal {{maintain}} relations of dependence the on es with the others. Those can vary {{in time and}} reveal speed changes in the studied phenomenon. In the goal to establish these changes, one shall compute first the wavelet transform of a signal, on various scales. Then one shall study the <b>statistical</b> <b>dependences</b> between these transforms thanks to an estimator of mutual information. One shall then propose to summarize the resulting network of dependences by a graph of dependences by thresholding {{the values of the}} mutual information or by quantifying its values. The method can be applied to several types of signals, such as fluctuations of market indexes for instance the S&P 500, or high frequency foreign exchange (FX) rates...|$|R
50|$|Note that, in {{contrast}} to many available methods for generating random numbers from non-uniform distributions, random variates generated directly by this approach will exhibit serial <b>statistical</b> <b>dependence.</b> In other words, not all points have the same independent likelihood of selection. This is because to draw the next sample, we define the slice based {{on the value of}} f(x) for the current sample. However, the generated are markovian, and are therefore expected to converge to the correct distribution in long run.|$|E
5000|$|The {{reasoning}} {{is that if}} the null hypothesis of there being no relation between the two matrices is true, then permuting the rows and columns of the matrix should be equally likely to produce a larger or a smaller coefficient. In addition to overcoming the problems arising from the <b>statistical</b> <b>dependence</b> of elements within each of the two matrices, use of the permutation test means that no reliance is being placed on assumptions about the statistical distributions of elements in the matrices.|$|E
5000|$|Caution must {{be applied}} when using cross {{correlation}} for nonlinear systems. In certain circumstances, which {{depend on the}} properties of the input, cross correlation between the input and output of a system with nonlinear dynamics can be completely blind to certain nonlinear effects. This problem arises because some quadratic moments can equal zero and this can incorrectly suggest that there is little [...] "correlation" [...] (in the sense of <b>statistical</b> <b>dependence)</b> between two signals, when in fact the two signals are strongly related by nonlinear dynamics.|$|E
40|$|Abstract- The various {{scales of}} a signal {{maintain}} relations of dependence {{the ones with}} the others. Those can vary in time and reveal speed changes in the studied phenomenon. In the goal to establish these changes, one shall compute rst the wavelet transform of a signal, on various scales. Then one shall study the <b>statistical</b> <b>dependences</b> between these transforms thanks to an estimator of mutual information. One shall then propose to summarize the resulting network of dependences by a graph of dependences by thresholding the values of the mutual information or by quantifying its values. The method can be applied to several types of signals, such as uctuations of market indexes for instance the S&P 500, or high frequency foreign exchange (FX) rates. Key words- Information theory, time series, dependence structure, correlations, mutual information...|$|R
40|$|We {{describe}} eight {{data sets}} that together formed the CauseEffectPairs {{task in the}} Causality Challenge # 2 : Pot-Luck competition. Each set consists of a sample {{of a pair of}} statistically dependent random variables. One variable is known to cause the other one, but this information was hidden from the participants; the task was to identify which of the two variables was the cause and which one the effect, based upon the observed sample. The data sets were chosen such that we expect common agreement on the ground truth. Even though part of the <b>statistical</b> <b>dependences</b> may also be due to hidden common causes, common sense tells us that there is a significant cause-effect relation between the two variables in each pair. We also present baseline results using three different causal inference methods...|$|R
30|$|The noise {{accompanied}} {{with the}} sensors' observations is Gaussian. Therefore, {{the notions of}} <b>statistical</b> correlation and <b>dependence</b> will be used interchangeably [11].|$|R
