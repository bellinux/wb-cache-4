348|142|Public
50|$|Precision is a {{description}} of random errors, a measure of <b>statistical</b> <b>variability.</b>|$|E
5000|$|No disease (or no abnormality), and {{the finding}} is caused entirely by <b>statistical</b> <b>variability</b> ...|$|E
50|$|A {{process is}} a unique {{combination}} of tools, materials, methods, and people engaged in producing a measurable output; for example a manufacturing line for machine parts. All processes have inherent <b>statistical</b> <b>variability</b> which can be evaluated by statistical methods.|$|E
25|$|Strength (or resistance) {{reduction}} factor in structural engineering, used {{to account for}} <b>statistical</b> <b>variabilities</b> in materials and construction methods.|$|R
40|$|A {{number of}} special radiosondes were {{examined}} {{to study the}} <b>statistical</b> <b>variabilities</b> of atmospheric parameters on a time and space scale commensurate with aircraft flyover noise measurements. Particular {{emphasis is placed on}} those aspects of the data which relate to the temporal variabilities of the temperature and humidity profiles. It is found that during the late morning hours, continuous surface measurements and measurements of airport tower levels are sufficient to monitor the layer of the atmosphere in which the largest variabilities occur...|$|R
5000|$|Analysis {{methodology}} {{that allows}} for data scientific and risk oriented decision making based on <b>statistical</b> data. <b>Variability</b> limits should be defined and contingencies {{in the event of}} non-conforming data established ...|$|R
5000|$|About {{this time}} {{concerns}} were, however, raised regarding the <b>statistical</b> <b>variability</b> with {{measurements of the}} power law exponent, {{and the possibility that}} observations of a power law might reflect more mathematical artefact than a mechanistic process. [...] Taylor et al responded with an additional publication of extensive observations which he claimed refuted Downing’s concerns.|$|E
50|$|A pilot study, {{pilot project}} or pilot {{experiment}} {{is a small}} scale preliminary study conducted in order to evaluate feasibility, time, cost, adverse events, and effect size (<b>statistical</b> <b>variability)</b> {{in an attempt to}} predict an appropriate sample size and improve upon the study design prior to performance of a full-scale research project. Pilot studies, therefore, may not be appropriate for case studies.|$|E
50|$|The {{electrocardiogram}} (ECG) uses electrodes {{placed on}} the torso, wrists, or legs, to measure the electrical activity {{of the heart and}} measures the interbeat interval (distances between successive R-wave peaks in the QRS complex). The interbeat interval, divided into 60 seconds, determines the heart rate at that moment. The <b>statistical</b> <b>variability</b> of that interbeat interval is what we call heart rate variability. The ECG method is more accurate than the PPG method in measuring heart rate variability.|$|E
50|$|In statistics, overdispersion is the {{presence}} of greater <b>variability</b> (<b>statistical</b> dispersion) in a data set than would be expected based on a given statistical model.|$|R
5000|$|A natural {{interpretation}} of majorization {{is that if}} [...] then [...] is more spread out than [...] So it is natural to ask if <b>statistical</b> measures of <b>variability</b> are Schur-convex. The variance and standard deviation are Schur-convex functions, while the Median absolute deviation is not.|$|R
40|$|Key words: {{constrained}} sintering; coatings and films; crack; material point method. Abstract. The sintering of {{thin films}} {{is widely used}} for surface coatings and because of its technological importance has generated extensive research interest. During the sintering process, the thin film is constrained by the substrate, which generates considerably high levels of stresses. Crackings are often observed and are considered {{as one of the}} major problems of the surface coating technique. This paper has proposed a new numerical method in order to tackle the traditional difficulties in simulating multi-crackings during constrained sintering. Main features of the present method include: (i) the material data is represented by an anisotropic constitutive law, (ii) a new numerical scheme is developed for the crack initialization and growth based on the material point method, (iii) the 3 D viscous film shrinkage model is solved by using a dynamic FE scheme, and (iv) the random nature of the initial green body density is represented by <b>statistical</b> <b>variabilities.</b> It is shown that the model proposed by the present paper is capable for the nucleation and propagation of multi-cracks in a straightforward manner. Cracking patterns are shown to be consistent with experimental understandings. The focus of the paper is on the numerical issues and demonstrating the capacity of the model. 1...|$|R
50|$|Consider fitting a {{straight}} line for the relationship of an outcome variable y to a predictor variable x, and estimating {{the slope of the}} line. <b>Statistical</b> <b>variability,</b> measurement error or random noise in the y variable cause uncertainty in the estimated slope, but not bias: on average, the procedure calculates the right slope. However, variability, measurement error or random noise in the x variable causes bias in the estimated slope (as well as imprecision). The greater the variance in the x measurement, the closer the estimated slope must approach zero instead of the true value.|$|E
50|$|Surface-water {{hydrology}} is a {{field that}} encompasses all surface waters of the globe (overland flows, rivers, lakes, wetlands, estuaries, oceans, etc.). This is {{a subset of the}} hydrologic cycle that does not include atmospheric, and ground waters. Surface-water hydrology relates the dynamics of flow in surface-water systems (rivers, canals, streams, lakes, ponds, wetlands, marshes, arroyos, oceans, etc.). This includes the field measurement of flow (discharge); the <b>statistical</b> <b>variability</b> at each setting; floods; drought susceptibility and the development of the levels of risk; and the fluid mechanics of surface waters.|$|E
50|$|In {{defining}} hazard, {{whether it}} be of natural or anthropogenic origin, Keith Smith argues that what may {{be defined as a}} natural hazard is not in fact a hazard unless there is the presence of humans to make it a hazard and that it is merely an event of scientific interest. In this sense the environmental conditions we may consider hostile or hazardous can really be seen as neutral in that it is our perception, human location and actions which identify resources and hazards with the range of natural events. In this regard human sensitivity to environmental hazards is a combination of both physical exposure (natural and/or technological events at a location related to their <b>statistical</b> <b>variability)</b> and human vulnerability (in regard to social and economic tolerance of the same location).|$|E
40|$|International audienceThis {{paper is}} devoted to the {{development}} of a stochastic modeling of the track geometry and its identiication with experimental measurements. This modeleing, which has to integrate the <b>statistical</b> and spatial <b>variabilities</b> and dependencies, is a keyu issue when using simulation for conception, maintenance or certification purposes...|$|R
40|$|<b>Statistical</b> device <b>variability</b> {{may be a}} {{limiting}} factor for further miniaturizing nodes in silicon bulk CMOS technology. On the other hand, in novel technologies such as Carbon Nanotubes Field Effect Transistors (CNFETs), the device variability is also present and is mainly due to imperfections inherent in current carbon nanotube (CNT) growth methods. The goal {{of this paper is}} to evaluate the impact of the main sources of variability in conventional MOSFET and CNFET 6 T SRAM cells through the consideration of random threshold voltage process variations. Peer ReviewedPostprint (published version...|$|R
40|$|The {{source of}} {{variability}} in the interval between action potentials has been identified {{in a class of}} cat spinal motoneurons. The observed random fluctuations in membrane potential (synaptic noise) together with an empirical description of spike generation accurately predict the <b>statistical</b> structure of <b>variability</b> observed to occur in the neuron's discharge...|$|R
50|$|Data {{assimilation}} periodically {{adjusts the}} model state to incorporate new data using statistical methods. Because fire is highly nonlinear and irreversible, data assimilation for fire models poses special challenges, and standard methods, {{such as the}} ensemble Kalman filter (EnKF) do not work well. <b>Statistical</b> <b>variability</b> of corrections and especially large corrections may result in nonphysical states, {{which tend to be}} preceded or accompanied by large spatial gradients. In order to ease this problem, the regularized EnKF penalizes large changes of spatial gradients in the Bayesian update in EnKF. The regularization technique has a stabilizing effect on the simulations in the ensemble but it does not improve much the ability of the EnKF to track the data: The posterior ensemble is made out of linear combinations of the prior ensemble, and if a reasonably close location and shape of the fire cannot be found between the linear combinations, the data assimilation is simply out of luck, and the ensemble cannot approach the data. From that point on, the ensemble evolves essentially without regard to the data. This is called filter divergence. So, there is clearly a need to adjust the simulation state by a position change rather than an additive correction only. The morphing EnKF combines the ideas of data assimilation with image registration and morphing to provide both additive and position correction in a natural manner, and can be used to change a model state reliably in response to data.|$|E
40|$|This paper {{presents}} a {{principal component analysis}} (PCA) -based unified compact modelling strategy for processinduced and <b>statistical</b> <b>variability</b> in 14 -nm double gate SOI FinFET technology. There is strong interplay between process and <b>statistical</b> <b>variability</b> in FinFET technology and failure to capture the correlations between them can lead to an inaccurate estimation of overall <b>statistical</b> <b>variability</b> with errors of up to 30...|$|E
40|$|<b>Statistical</b> <b>variability</b> in ultra-scaled CMOS devices {{is a major}} {{challenge}} faced by the semiconductor industry today. It has critical impact on functionality and yield, particularly of {{static random access memory}} (SRAM) circuits. This chapter focuses on the physical origins of <b>statistical</b> <b>variability</b> and their manifestation in fully depleted (FD) thin-body silicon-on-insulator (TB-SOI) transistors. We first review the major sources of <b>statistical</b> <b>variability</b> in CMOS devices. Then, the unique impact of <b>statistical</b> <b>variability</b> on TB-SOI technology is presented, drawing comparisons with conventional, bulk metal oxide semiconductor field effect transistor (MOSFET). Finally, based on a comparison study between TB-SOI and double gate technologies, the statistical aspects of reliability are discussed...|$|E
40|$|This letter {{addresses}} the statistical assessment of system performance via the efficient estimation of eye diagram parameters under process <b>variability.</b> <b>Statistical</b> information pertaining to eye parameters is reconstructed by interpolation of a reduced set of simulations. The proposed strategy shows a remarkable efficiency improvement {{with respect to}} classical blind and brute-force sampling-based methods...|$|R
40|$|Although {{variability}} is {{of fundamental}} concern and interest to statisticians, often {{this does not}} get communicated to students who are taught instead to view variability as a nuisance parameter. A brief survey of a few case studies, {{as well as a}} recounting of some history, shows that variability is worthy of study in its own right, and examination of variability leads to insights that might have been missed had we focused all of our attention on the "trend" of the data. As one of the key components of <b>statistical</b> thinking, <b>variability</b> deserves more prominence in the classroom...|$|R
40|$|We {{identify}} 339 {{known and}} 316 new variable stars {{of various types}} among 250000 lightcurves obtained by digitizing 167 30 x 30 cm photographic plates of the Moscow collection. We use these data to conduct a comprehensive test of 18 <b>statistical</b> characteristics (<b>variability</b> indices) in search for the best general-purpose variability detection statistic. We find that the highest peak on the DFT periodogram, interquartile range, median absolute deviation, and Stetson's L index are the most efficient in recovering variable objects from the set of photographic lightcurves used in our test. Comment: 4 pages, 2 figures, 1 table; in proceedings of the AstroPlate workshop, Prague, Villa Lanna, March 14 - 18, 201...|$|R
40|$|This paper {{presents}} a hierarchical variability-aware compact model methodology {{based on a}} comprehensive simulation study of global process variation and local <b>statistical</b> <b>variability</b> on 20 nm bulk planar CMOS. The area dependence of <b>statistical</b> <b>variability</b> is carefully examined {{in the presence of}} random discrete dopants; gate line edge roughness; metal gate granularity; and their combination. Hierarchical variability-aware compact models have been developed, extracted and used to evaluate the impact of process variation and <b>statistical</b> <b>variability</b> on SRAM stability and performance...|$|E
40|$|This paper {{presents}} {{an evaluation of}} 14 -nm SOI FinFET CMOS SRAM codesign techniques {{in the presence of}} <b>statistical</b> <b>variability</b> and reliability impact. As <b>statistical</b> <b>variability</b> sources random discrete dopants, gate-edge roughness, fi-edge roughness, metal-gate granularity and random interface trapped charges in N/PBTI are considered...|$|E
40|$|We {{present a}} {{comprehensive}} simulation study of random telegraph signal (RTS) amplitude distributions {{under the influence}} of <b>statistical</b> <b>variability</b> in 20 nm gate-length, lightly-doped channel FinFETs on an SOI substrate. The distribution of threshold voltage RTS shifts, due to single-charge trapping at the interface, is inherently affected by <b>statistical</b> <b>variability</b> sources including random discrete dopants (RDD), gate- and fin- edge roughness (GER and FER), and metal gate granularity (MGG). The threshold voltage RTS amplitudes in SOI FinFETs deviate from an exponential distribution with a reduced tail, but it increases with increased <b>statistical</b> <b>variability.</b> Moreover, the electrical transfer characteristics due to single charge trapping vary with gate-bias...|$|E
40|$|Empirical {{evidence}} shows that the clearing prices for point-to-point congestion revenue rights, also known as financial transmission rights (FTRs), resulting from centralized auctions conducted by Independent System Operators differ significantly and systematically from the realized congestion revenues that determine the accrued payoffs of these rights. The question addressed by this paper is whether such deviations are due to price discovery errors which will eventually vanish or due to inherent inefficiencies in the auction structure. We address this question by studying a hypothetical DC-flow approximation model of a six-node system with known outage probabilities of each element and known <b>statistical</b> demand <b>variability.</b> We show that even with perfect foresight of average congestio...|$|R
40|$|The {{production}} of artificial light curves with known <b>statistical</b> and <b>variability</b> properties {{is of great}} importance in astrophysics. Consolidating the confidence levels during cross-correlation studies, understanding the artefacts induced by sampling irregularities, establishing detection limits for future observatories {{are just some of}} the applications of simulated data sets. Currently, the widely used methodology of amplitude and phase randomisation is able to produce artificial light curves which have a given underlying power spectral density (PSD) but which are strictly Gaussian distributed. This restriction is a significant limitation, since the majority of the light curves e. g. active galactic nuclei, X-ray binaries, gamma-ray bursts show strong deviations from Gaussianity exhibiting `burst-like' events in their light curves yielding long-tailed probability distribution functions (PDFs). In this study we propose a simple method which is able to precisely reproduce light curves which match both the PSD and the PDF of either an observed light curve or a theoretical model. The PDF can be representative of either the parent distribution or the actual distribution of the observed data, depending on the study to be conducted for a given source. The final artificial light curves contain all of the <b>statistical</b> and <b>variability</b> properties of the observed source or theoretical model i. e. same PDF and PSD, respectively. Within the framework of Reproducible Research, the code, together with the illustrative example used in this manuscript, are both made publicly available in the form of an interactive Mathematica notebook. Comment: Accepted for publication in MNRAS. The paper is 23 pages long and contains 21 figures and 2 tables. The Mathematica notebook can be found in the web as part of this paper (Online Material) or at [URL]...|$|R
40|$|As feature sizes shrink, random {{fluctuations}} gain importance in semiconductor manufacturing and integrated circuit design. Therefore, <b>statistical</b> device <b>variability</b> {{has to be}} considered in circuit design and analysis to properly estimate their impact and avoid expensive over-design. Statistical MOSFET compact modeling is required to accurately capture marginal distributions of varying device parameters and to preserve their statistical correlations. Due to limited simulator capabilities, variables are often assumed to be normally distributed. Although correlations may be captured using Principal Component Analysis, such an assumption may be inaccurate. As an alternative, Nonlinear Power Models have been proposed. Since we see some limitations in this approach, we analyze whether the multivariate Generalized Lambda Distribution is an alternative for statistical device modeling. Applying both approaches to extracted statistical device parameters, we conclude that both methods do not differ significantly in accuracy, but the multivariate Generalized Lambda Distribution is more general and less computationally expensive...|$|R
40|$|Comprehensive 3 -D {{simulations}} {{have been}} carried out and compared with experimental data highlighting the dominant sources of <b>statistical</b> <b>variability</b> in 32 -nm high-$kappa/hbox{metal}$ gate MOSFET technology. The <b>statistical</b> <b>variability</b> sources include random discrete dopants, line edge roughness, and metal gate granularity. Their relative importance is highlighted in the numerical simulations. Excellent agreement is achieved between the simulated and measured standard deviation of the threshold voltage...|$|E
40|$|This paper {{presents}} a comprehensive simulation {{study of the}} interactions between long-range process and short-range <b>statistical</b> <b>variability</b> in a 14 -nm technology node silicon-on-insulator FinFET. First, the individual and combined impact of the relevant variability sources, including random discrete dopants, fin line edge roughness (LER), gate LER, and metal gate granularity are studied for the nominal 20 -nm physical gate-length FinFET design. This {{is followed by a}} comprehensive study of the interactions of the channel length, fin width and fin height systematic process variations with the combined <b>statistical</b> <b>variability</b> sources. The simulations follow a $ 3 times 3 times 3 = 27 $ experiment design that covers the process variability space, and 1000 statistical simulations are carried out at each node of the experiment. Both metal-gate-first and metal-gate-last technologies are considered. It is found that <b>statistical</b> <b>variability</b> is significantly dependent on the process-induced variability. The applicability of the Pelgrom law to the FinFET <b>statistical</b> <b>variability,</b> subject to long-range process variations, is also examined. Mismatch factor is strongly dependent on the process variations...|$|E
40|$|Variability is a {{critical}} concern for the stability and yield of SRAM with minimized size. We present a study of a 14 nm node SOI FinFET SRAM cell {{under the influence of}} <b>statistical</b> <b>variability</b> and random charge trapping due to positive/negative bias temperature instability (P/NBTI). Low channel doping is believed {{to be one of the}} main advantages of FinFETs in reducing <b>statistical</b> <b>variability,</b> but fin and gate edge roughness and metal gate granularity can cause significant variability and affect SRAM stability. The noise margins are largely skewed, and read and write noise margins are decorrelated due to <b>statistical</b> <b>variability.</b> Under heavy stress conditions cell read noise margin can be degraded by 30 mV on average due to charge trapping, and its 6 σ-yield becomes even worse due to the enhanced variability in N/PBTI...|$|E
40|$|International audienceThis paper {{presents}} a methodology to build representative railway track geometries {{thanks to a}} stochastic modelling. This modelling, which has to integrate the <b>statistical</b> and spatial <b>variabilities</b> and dependencies, is a key issue when using simulation for conception, maintenance or certification purposes, as the dynamic behaviour of the trains is mainly induced by the track geometry. The stochastic process theory is used, combining Karhunen-Lo'eve and polynomial expansions. Through a practical example, this paper finally shows to what extent this methodology gives rise to new promising opportunities for the track geometry maintenance...|$|R
5000|$|The average {{absolute}} deviation (or mean {{absolute deviation}}) of a data set {{is the average}} of the absolute deviations from a central point. It is a summary statistic of <b>statistical</b> dispersion or <b>variability.</b> In this general form, the central point can be the mean, median, mode, or the result of another measure of central tendency. Furthermore, as described in the article about averages, the deviation averaging operation may refer to the mean or the median. Thus the total number of combinations amounts to at least four types of average absolute deviation.|$|R
40|$|We {{report on}} the study of the {{population}} of B and Be stars in SMC young clusters, performed with the Wide Field Imager in slitless spectroscopic mode at ESO/T 2. 2 m with a filter centered at Halpha. First, we explain the reduction methods we used and our selection of different types of objects. Second, we present results on the proportion of Be stars in SMC clusters, and we compare this proportion to the one observed in the Milky Way. Finally, we also present results on a <b>statistical</b> study of <b>variability</b> of Be stars with OGLE. Comment: sf 2 a 200...|$|R
