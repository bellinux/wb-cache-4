1233|2591|Public
25|$|A <b>sample</b> <b>space,</b> , {{which is}} the set of all {{possible}} outcomes.|$|E
25|$|Continuous {{probability}} theory deals with events {{that occur in}} a continuous <b>sample</b> <b>space.</b>|$|E
25|$|Consider an {{experiment}} {{that can produce}} a number of outcomes. The set of all outcomes is called the <b>sample</b> <b>space</b> of the experiment. The power set of the <b>sample</b> <b>space</b> (or equivalently, the event space) is formed by considering all different collections of possible results. For example, rolling an honest die produces one of six possible results. One collection of possible results corresponds to getting an odd number. Thus, the subset {1,3,5} {{is an element of}} the power set of the <b>sample</b> <b>space</b> of die rolls. These collections are called events. In this case, {1,3,5} is the event that the die falls on some odd number. If the results that actually occur fall in a given event, that event is said to have occurred.|$|E
50|$|The main useful {{property}} of small-bias <b>sample</b> <b>spaces</b> {{is that they}} need far fewer truly random bits than the uniform distribution to fool parities. Efficient constructions of small-bias <b>sample</b> <b>spaces</b> have found many applications in computer science, {{some of which are}} derandomization, error-correcting codes, and probabilistically checkable proofs.The connection with error-correcting codes is in fact very strong since -biased <b>sample</b> <b>spaces</b> are equivalent to -balanced error-correcting codes.|$|R
2500|$|The {{probability}} {{of an event}} A is written as , , or [...] This mathematical definition of probability can extend to infinite <b>sample</b> <b>spaces,</b> and even uncountable <b>sample</b> <b>spaces,</b> using {{the concept of a}} measure.|$|R
40|$|It {{is known}} {{that there is a}} only one {{invariant}} metric, and only one family of invariant affine connections on the the space of probability measures on finite <b>sample</b> <b>spaces.</b> Under certain conditions this may also true for finite measures on finite <b>sample</b> <b>spaces.</b> We extends these results to finite measures on arbitrary <b>sample</b> <b>spaces.</b> It is shown that the generalization of these structures are unique. The metric is induced by the inner product on the Hilbert space of square root of measures, while the affine connection is induced by the affine structures of the Banach spaces of fractional powers of measues...|$|R
25|$|In statistics, {{it is used}} as {{the symbol}} for the <b>sample</b> <b>space,</b> or total set of {{possible}} outcomes.|$|E
25|$|So, the {{probability}} of the entire <b>sample</b> <b>space</b> is 1, and {{the probability}} of the null event is 0.|$|E
25|$|A random {{variable}} X is a measurable function X: Ω → S from the <b>sample</b> <b>space</b> Ω to another measurable space S called the state space.|$|E
40|$|<b>Sampling</b> {{theory in}} <b>spaces</b> {{other than the}} space of band-limited {{functions}} has recently received considerable attention. This is {{in part because the}} band-limitedness assumption is not very realistic in many applications. In addition, band-limited functions can have very slow decay which translates in poor reconstruction. In this article we study the sampling problem in general shift invariant spaces. We characterize the functions in these spaces and provide necessary and sufficient conditions for a function in L 2 (R) to belong to a <b>sampling</b> <b>space.</b> Furthermore we obtain decompositions of a <b>sampling</b> <b>space</b> in <b>sampling</b> subspaces. These decompositions are related with determining sets. Some examples are provided. Key words and phrases: Shift invariant <b>spaces,</b> <b>sampling</b> <b>spaces,</b> frames, determining set...|$|R
25|$|Discrete {{probability}} theory deals with events {{that occur in}} countable <b>sample</b> <b>spaces.</b>|$|R
40|$|We {{present a}} spatial {{adaptive}} asynchronous algorithm for fringe pattern demodulation. The proposed algorithm {{is based on}} the standard five-step asynchronous method with the one modification that we select the best <b>sample</b> <b>spacing</b> for each point of the fringe pattern. As we show, the frequency response of any asynchronous method depends on the <b>sample</b> <b>spacing.</b> This interesting behavior is used to select the best <b>sample</b> <b>spacing</b> as the one that gives the biggest response for each location. The overall result is a spatial demodulation algorithm with an improved frequency response compared to the existing ones. We show the feasibility of the proposed method with theoretical analysis as well as experimental results...|$|R
25|$|A {{probability}} distribution whose <b>sample</b> <b>space</b> is {{the set of}} real numbers is called univariate, while a distribution whose <b>sample</b> <b>space</b> is a vector space is called multivariate. A univariate distribution gives the probabilities of a single random variable taking on various alternative values; a multivariate distribution (a joint {{probability distribution}}) gives the probabilities of a random vector—a list {{of two or more}} random variables—taking on various combinations of values. Important and commonly encountered univariate probability distributions include the binomial distribution, the hypergeometric distribution, and the normal distribution. The multivariate normal distribution is a commonly encountered multivariate distribution.|$|E
25|$|The formal {{mathematical}} {{treatment of}} random variables {{is a topic}} in probability theory. In that context, a random variable is understood as a function defined on a <b>sample</b> <b>space</b> whose outputs are numerical values.|$|E
25|$|In the {{continuous}} univariate case above, the reference measure is the Lebesgue measure. The {{probability mass function}} of a discrete random variable is the density {{with respect to the}} counting measure over the <b>sample</b> <b>space</b> (usually the set of integers, or some subset thereof).|$|E
40|$|Several {{approaches}} {{have been developed}} to estimate probability density functions (pdfs). The pdf has two important properties: the integration of pdf over whole <b>sampling</b> <b>space</b> is equal to 1 and the value of pdf in the <b>sampling</b> <b>space</b> is {{greater than or equal to}} zero. The first constraint can be easily achieved by the normalisation. On the other hand, it is hard to impose the non-negativeness in the <b>sampling</b> <b>space.</b> In a pdf estimation, some areas in the <b>sampling</b> <b>space</b> might have negative pdf values. It produces unreasonable moment values such as negative probability or variance. A transformation to guarantee the negative-free pdf over a chosen <b>sampling</b> <b>space</b> is presented and it is applied to the nonlinear projection filter. The filter approximates the pdf to solve nonlinear estimation problems. For simplicity, one-dimensional nonlinear system is used as an example to show the derivations and it can be readily generalised for higher dimensional systems. The efficiency of the proposed method is demonstrated by numerical simulations. The simulations also show that, for the same level of approximation error in the filter, the required number of basis functions with the transformation is a lot smaller than the ones without transformation. This would largely benefit the computational cost reduction...|$|R
2500|$|These {{concepts}} can {{be generalized}} for multidimensional cases on [...] and other continuous <b>sample</b> <b>spaces.</b>|$|R
50|$|An {{important}} {{application of}} small-bias sets {{lies in the}} construction of almost k-wise independent <b>sample</b> <b>spaces.</b>|$|R
25|$|Consider a <b>sample</b> <b>space</b> Ω {{generated}} by two random variables X and Y. In principle, Bayes’ theorem {{applies to the}} events A={X=x} and B={Y=y}. However, terms become 0 at points where either variable has finite probability density. To remain useful, Bayes’ theorem may be formulated {{in terms of the}} relevant densities (see Derivation).|$|E
25|$|This {{culminated in}} modern {{probability}} theory, on foundations laid by Andrey Nikolaevich Kolmogorov. Kolmogorov combined {{the notion of}} <b>sample</b> <b>space,</b> introduced by Richard von Mises, and measure theory and presented his axiom system for probability theory in 1933. This became the mostly undisputed axiomatic basis for modern probability theory; but, alternatives exist, such as the adoption of finite rather than countable additivity by Bruno de Finetti.|$|E
25|$|Alice knows only {{whether or}} not Arnold Schwarzenegger has {{received}} at least 60 votes. Her incomplete information is described by the σ-algebra Alice that contains: (1) the set of all sequences in Ω where at least 60 people vote for Schwarzenegger; (2) the set of all sequences where fewer than 60 vote for Schwarzenegger; (3) the whole <b>sample</b> <b>space</b> Ω; and (4) the empty set ∅.|$|E
40|$|The aim of {{this paper}} is the {{detailed}} investigation of trigonometric polynomial spaces as a tool for approximation and signal analysis. <b>Sample</b> <b>spaces</b> are generated by equidistant translates of certain de la Vallée Poussin means. The different de la Vallée Poussin means enable us to choose between better time- or frequency-localization. For nested <b>sample</b> <b>spaces</b> and corresponding wavelet spaces, we discuss different bases and their transformations...|$|R
40|$|Subgrade modulus {{values for}} roads around the State of Minnesota can be {{effectively}} modelled as spatially correlated lognormal random variables. Based upon this geostatistical model, this report presents guidelines and nomographs for selecting the preliminary <b>sample</b> <b>spacing</b> {{for assessing the}} subgrade modulus. The maximum <b>sample</b> <b>spacing</b> to achieve a required precision is represented {{as a function of}} the average, standard deviation, and correlation length. Minnesota Department of Transportatio...|$|R
40|$|To {{investigate}} the mutual interactions between droplets in the spray combustion, combustion of 2 -dimensionally arranged quasi-droplet clusters is studied under microgravity. Quasi-droplet samples, which are solid in room temperature and change into liquid {{just after the}} ignition, consist of alcohol (propanol, butanol, pentanol, or hexanol) and polyethylene glycol with a volumetric ratio of 2 : 1. Seven samples sustained by glass rods form a 2 -dimensional quasi-droplet cluster. Electrically heated nichrome wires ignite all samples in the cluster simultaneously. Single envelope flames that surround the clusters appeared. The {{results show that the}} <b>sample</b> <b>spacing</b> has a strong effect on the shape and movement of the flame. Sample clusters with large sample spacings come to the external group combustion through the scavenging combustion mode, whereas the small spacing clusters start directly with the external group combustion. At large sample spacings, the distance {{from the edge of the}} sample cluster to the flame (flame distance) increases to a maximum value and then decreases with time. The period of flame growth is prolonged with decreasing <b>sample</b> <b>spacing</b> and finally, at a small enough <b>sample</b> <b>spacing,</b> the flame distance keeps increasing until the flame disappears. This flame movement is attributed to the fuel vapor accumulation effect, which becomes more dominant with decreasing <b>sample</b> <b>spacing.</b> The burning lifetime decreases monotonically and approaches the value of the single flame with increasing <b>sample</b> <b>spacing.</b> The flame distance decreases monotonically and approaches the single flame radius with increasing <b>sample</b> <b>spacing</b> also. These results render important confirmations of the external group combustion phenomena and prove the importance of the two kinds of unsteadiness, i. e., the scavenging combustion with large droplet interval and the fuel vapor accumulation effect with small droplet interval, in group combustion. Copyright © 2002 The Combustion Institute...|$|R
25|$|Probability {{theory is}} {{the branch of}} {{mathematics}} concerned with probability. Although there are several different probability interpretations, probability theory treats the concept in a rigorous mathematical manner by expressing it {{through a set of}} axioms. Typically these axioms formalise probability in terms of a probability space, which assigns a measure taking values between 0 and 1, termed the probability measure, to a set of outcomes called the <b>sample</b> <b>space.</b> Any specified subset of these outcomes is called an event.|$|E
25|$|One way to {{generate}} random samples from a binomial distribution {{is to use}} an inversion algorithm. To do so, one must calculate the probability that P(X=k) for all values k from 0 through n. (These probabilities should sum to a value close to one, in order to encompass the entire <b>sample</b> <b>space.)</b> Then by using a pseudorandom number generator {{to generate}} samples uniformly between 0 and 1, one can transform the calculated samples U into discrete numbers by using the probabilities calculated in step one.|$|E
25|$|If 100 {{voters are}} to be drawn {{randomly}} from among all voters in California and asked whom they will vote for governor, then the set of all sequences of 100 Californian voters would be the <b>sample</b> <b>space</b> Ω. We assume that sampling without replacement is used: only sequences of 100 different voters are allowed. For simplicity an ordered sample is considered, that is a sequence {Alice, Bryan} is different from {Bryan, Alice}. We also take for granted that each potential voter knows exactly his/her future choice, that is he/she doesn’t choose randomly.|$|E
40|$|Let N be a simply {{connected}}, connected non-commutative nilpotent Lie {{group with}} Lie algebra n having rational structure constants. We assume that N=P M, M is commutative, {{and for all}} λ∈n^∗ in general position the subalgebra p=(P) is a polarization ideal subordinated to λ (p is a maximal ideal satisfying [p,p]⊆λ for all λ in general position and p is necessarily commutative.) Under these assumptions, we prove that there exists a discrete uniform subgroup Γ⊂ N such that L^ 2 (N) admits band-limited spaces {{with respect to the}} group Fourier transform which are <b>sampling</b> <b>spaces</b> with respect to Γ. We also provide explicit sufficient conditions which are easily checked for the existence of <b>sampling</b> <b>spaces.</b> Sufficient conditions for <b>sampling</b> <b>spaces</b> which enjoy the interpolation property are also given. Our result bears a striking resemblance with the well-known Whittaker-Kotel'nikov-Shannon sampling theorem...|$|R
40|$|Discriminant feature {{extraction}} plays {{a fundamental role in}} pattern recognition. In this paper, we propose the Linear Laplacian Discrimination (LLD) algorithm for discriminant {{feature extraction}}. LLD {{is an extension}} of Linear Discriminant Analysis (LDA). Our motivation is to address the issue that LDA cannot work well in cases where <b>sample</b> <b>spaces</b> are non-Euclidean. Specifically, we define the within-class scatter and the between-class scatter using similarities which are based on pairwise distances in <b>sample</b> <b>spaces.</b> Thus the structural information of classes is contained in the within-class and the between-class Laplacian matrices which are free from metrics of <b>sample</b> <b>spaces.</b> The optimal discriminant subspace can be derived by controlling the structural evolution of Laplacian matrices. Experiments are performed on the facial database for FRGC version 2. Experimental results show that LLD is effective in extracting discriminant features. 1...|$|R
30|$|The mixture Kalman filter is {{a general}} {{sequential}} Monte Carlo technique for conditional linear dynamic systems. It generates samples of some indicator variables recursively based on sequential importance sampling (SIS) and integrates out the linear and Gaussian state variables conditioned on these indicators. Due to the marginalization process, {{the complexity of the}} mixture Kalman filter is quite high if the dimension of the indicator <b>sampling</b> <b>space</b> is high. In this paper, we address this difficulty by developing a new Monte Carlo sampling scheme, namely, the multilevel mixture Kalman filter. The basic idea is {{to make use of the}} multilevel or hierarchical structure of the space from which the indicator variables take values. That is, we draw samples in a multilevel fashion, beginning with sampling from the highest-level <b>sampling</b> <b>space</b> and then draw samples from the associate subspace of the newly drawn samples in a lower-level <b>sampling</b> <b>space,</b> until reaching the desired <b>sampling</b> <b>space.</b> Such a multilevel sampling scheme can be used in conjunction with the delayed estimation method, such as the delayed-sample method, resulting in delayed multilevel mixture Kalman filter. Examples in wireless communication, specifically the coherent and noncoherent 16 -QAM over flat-fading channels, are provided to demonstrate the performance of the proposed multilevel mixture Kalman filter.|$|R
25|$|If Ω is un{{countable}}, still, it {{may happen}} that p(ω) ≠ 0 for some ω; such ω are called atoms. They are an at most countable (maybe empty) set, whose probability {{is the sum}} of probabilities of all atoms. If this sum is equal to 1 then all other points can safely be excluded from the <b>sample</b> <b>space,</b> returning us to the discrete case. Otherwise, if the sum of probabilities of all atoms is between 0 and 1, then the probability space decomposes into a discrete (atomic) part (maybe empty) and a non-atomic part.|$|E
25|$|In {{probability}} theory, {{a probability}} density function (PDF), or density of a continuous random variable, is a function, whose value {{at any given}} sample (or point) in the <b>sample</b> <b>space</b> (the set of possible values taken by the random variable) {{can be interpreted as}} providing a relative likelihood that the value of the random variable would equal that sample. In other words, while the absolute likelihood for a continuous random variable to take on any particular value is 0 (since there are an infinite set of possible values to begin with), the value of the PDF at two different samples can be used to infer, in any particular draw of the random variable, how much more likely it is that the random variable would equal one sample compared to the other sample.|$|E
500|$|A {{stochastic}} process {{is defined as}} a collection of random variables defined on a common probability space , where [...] is a <b>sample</b> <b>space,</b> [...] is a -algebra, and [...] is a probability measure, [...] and the random variables, indexed by some set , all take values in the same mathematical space , which must be measurable with respect to some -algebra [...]|$|E
40|$|Practical {{computation}} of {{the minimum}} variance unbiased estimator (MVUE) is often a difficult, if not impossible, task, even though general theory assures its existence under regularity conditions. We propose a new approach based on iterative bootstrap bias correction of the maximum likelihood estimator to accurately approximate the MVUE. Viewing bootstrap iteration as a Markov process, we develop a computational algorithm for bias correction based on arbitrarily many bootstrap iterations. The algorithm, when applied parametrically to finite <b>sample</b> <b>spaces,</b> does not involve Monte Carlo simulation. For infinite <b>sample</b> <b>spaces,</b> a nonparametric version of the algorithm is combined with a preliminary round of Monte Carlo simulation to yield an approximate MVUE. Both algorithms are computationally more efficient and stable than conventional simulation-based bootstrap iterations. Examples are given of both finite and infinite <b>sample</b> <b>spaces</b> to illustrate the effectiveness of our new approach. © Springer Science + Business Media, LLC 2006. link_to_subscribed_fulltex...|$|R
40|$|In our {{previous}} work published in this journal, we showed how the Hit-And-Run (HAR) procedure enables efficient sampling of criteria weights from a space formed by restricting a simplex with arbitrary linear inequality constraints. In this short communication, {{we note that}} the method for generating a basis of the <b>sampling</b> <b>space</b> can be generalized to also handle arbitrary linear equality constraints. This enables the application of HAR to <b>sampling</b> <b>spaces</b> that do not coincide with the simplex, thereby allowing the combined use of imprecise and precise preference statements. In addition, {{it has come to}} our attention that one of the methods we proposed for generating a starting point for the Markov chain was flawed. To correct this, we provide an alternative method that is guaranteed to produce a starting point that lies within the interior of the <b>sampling</b> <b>space.</b> (C) 2014 Elsevier B. V. All rights reserved...|$|R
3000|$|Firstly, {{make the}} {{following}} stipulation to some marks. The <b>sample’s</b> <b>space</b> S {{is composed of}} attribute space I and class space C. Which is denote S[*]=[*]{S [...]...|$|R
