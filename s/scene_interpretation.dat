250|87|Public
2500|$|Knowledge {{representation}} and knowledge engineering {{are central to}} AI research. Many of the problems machines are expected to solve will require extensive knowledge about the world. Among the things that AI needs to represent are: objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge (what we know about what other people know); and many other, less well researched domains. A representation of [...] "what exists" [...] is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The semantics of these are captured as description logic concepts, roles, and individuals, and typically implemented as classes, properties, and individuals in the Web Ontology Language. The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge by acting as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). Such formal knowledge representations are suitable for content-based indexing and retrieval, <b>scene</b> <b>interpretation,</b> clinical decision support, knowledge discovery via automated reasoning (inferring new statements based on explicitly stated knowledge), etc. Video events are often represented as SWRL rules, which can be used, among others, to automatically generate subtitles for constrained videos.|$|E
5000|$|Knowledge {{representation}} and knowledge engineering {{are central to}} AI research. Many of the problems machines are expected to solve will require extensive knowledge about the world. Among the things that AI needs to represent are: objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge (what we know about what other people know); and many other, less well researched domains. A representation of [...] "what exists" [...] is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The semantics of these are captured as description logic concepts, roles, and individuals, and typically implemented as classes, properties, and individuals in the Web Ontology Language. The most general ontologies are called upper ontologies, which act as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). Such formal knowledge representations are suitable for content-based indexing and retrieval, <b>scene</b> <b>interpretation,</b> clinical decision support, knowledge discovery via automated reasoning (inferring new statements based on explicitly stated knowledge), etc. Video events are often represented as SWRL rules, which can be used, among others, to automatically generate subtitles for constrained videos..|$|E
50|$|Automated {{processing}} of information not interpretable by software agents can be improved by adding rich semantics to the corresponding resources, such as video files. One of the approaches for the formal conceptualization of represented knowledge domains {{is the use of}} machine-interpretable ontologies, which provide structured data in, or based on, RDF, RDFS, and OWL. Ontology engineering is the design and creation of such ontologies, which can contain more than just the list of terms (controlled vocabulary); they contain terminological, assertional, and relational axioms to define concepts (classes), individuals, and roles (properties) (TBox, ABox, and RBox, respectively). Ontology engineering is a relatively new field of study concerning the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tool suites and languages that support them.A common way to provide the logical underpinning of ontologies is to formalize the axioms with description logics, which can then be translated to any serialization of RDF, such as RDF/XML or Turtle. Beyond the description logic axioms, ontologies might also contain SWRL rules. The concept definitions can be mapped to any kind of resource or resource segment in RDF, such as images, videos, and regions of interest, to annotate objects, persons, etc., and interlink them with related resources across knowledge bases, ontologies, and LOD datasets. This information, based on human experience and knowledge, is valuable for reasoners for the automated interpretation of sophisticated and ambiguous contents, such as the visual content of multimedia resources. Application areas of ontology-based reasoning include, but are not limited to, information retrieval, automated <b>scene</b> <b>interpretation,</b> and knowledge discovery.|$|E
40|$|Abstract. Part-whole {{relations}} are {{the backbone of}} configuration systems. In this paper, part-whole {{relations are}} combined with other relations and restrictions leading to here-called restricted aggregates. Depending on what is given, aggregates, parts, or/and relations between parts, different tasks have to be solved. General aggregation reasoning chunks are developed and represented with a configuration language. The approach {{is applied to the}} domain of constructing <b>scene</b> <b>interpretations.</b> ...|$|R
40|$|The "generic viewpoint" {{assumption}} {{states that}} an observer {{is not in}} a special position relative to the scene. It is commonly used to disqualify <b>scene</b> <b>interpretations</b> that assume special viewpoints, following a binary decision that the viewpoint was either generic or accidental. In this paper, we apply Bayesian statistics to quantify the probability of a view, and so derive a useful tool to estimate scene parameters...|$|R
40|$|Abstract [...] In {{this paper}} we {{show how the}} use of {{qualitative}} features can enhance the performance of recognition and localization techniques, in particular, the Generalized Hough Transform. Qualitative features (i. e. scene features with qualitative attributes assigned to them) are shown {{to be effective in}} pruning the search space of possible <b>scene</b> <b>interpretations</b> and also reducing the number of spurious interpretations explored by the recognition and localization technique. The redundancy of the computed transform and the probability of spurious peaks of signilicant magnitude due to random accumulation of evidence are two criteria by which the performance of the Generalized Hough Transform is judged. The straightforward Generalized Hough Transform shows a high probability of spurious peaks of significant magnitude even for small values of redundancy and small magnitude of the search space of <b>scene</b> <b>interpretations.</b> The use of qualitative features enables us {{to come up with a}} weighted Generalized Hough Transform where each match of a scene feature with a model feature is assigned a weight based on the qualitative attributes assigned to the scene feature. These weights could be looked upon as membership function values for the fuzzy sets defined by these qualitative attributes. Analytic expressions for the probability of accumulation of random events within a bucket are derived for the weighted Generalized Hough Transform and compared with the corresponding expression for the straightforward Generalized Hough Transform. The weighte...|$|R
40|$|Abstract. From past {{research}} {{it is known}} that both knowledge-based <b>scene</b> <b>interpretation</b> and knowledge-based configuration can be conceived as logical model construction. In this report we show that also from an applicationoriented point of view, both tasks are very similar and existing configuration technology can be used to implement a generic <b>scene</b> <b>interpretation</b> system with highly useful features, in particular expressive knowledge representation, flexible control, knowledge-guided hypothesis generation and constraint management. We describe an experiment where a table laying scene-inprogress is interpreted using the configuration system KONWERK as part of our <b>scene</b> <b>interpretation</b> system SCENIC. ...|$|E
40|$|Abstract ⎯ In this paper, {{we propose}} a novel <b>scene</b> <b>interpretation</b> {{paradigm}} by unified modeling of visual context using hierarchical graphical model. <b>Scene</b> <b>interpretation</b> through object recognition is difficult due to several sources of ambiguities (blur, clutter). We model the visual context of scene, object, and part into our problem to disambiguate them during object recognition. The precisely designed hierarchical graphical model can represent the contexts in a unified way. We also propose a new particle-based belief propagation for inference of the hierarchical graphical model. Such inference method is suitable to reflect high-level context in <b>scene</b> <b>interpretation.</b> In addition, the core inference is so general {{that can be}} used any complex inference problems. Experimental results validate the power of the proposed model of visual context to solve the ambiguities in <b>scene</b> <b>interpretation.</b> Keywords⎯Scene interpretation, Object recognition, Visual context, Hierarchical graphical mode...|$|E
40|$|The <b>scene</b> <b>interpretation</b> and the {{behavior}} planning of a vehicle in real world traffic is a difficult problem to be solved. If different hierarchies of tasks and purposes are built to structure {{the behavior}} of a driver, complex systems can be designed. But finally behavior planning in vehicles can only influence the controlled variables: steering angle and velocity. In this paper a <b>scene</b> <b>interpretation</b> and a behavior planning for a driver assistance system aiming on cruise control is proposed. In this system the controlled variables are determined by an evaluation of the dynamics of a two-dimensional neural field for <b>scene</b> <b>interpretation</b> and two one-dimensional neural fields controlling steering angle and velocity. The stimuli of the fields are determined according to the sensor information...|$|E
40|$|The ¨generic viewpointässumption {{states that}} an {{observer}} {{is not in}} a special position relative to the scene. It is commonly used to disqualify <b>scene</b> <b>interpretations</b> that assume special viewpoints, following a binary decision that the viewpoint was either generic or accidental. In this paper, we apply Bayesian statistics to quantify the probability of a view, and so derive a useful tool to estimate scene parameters. This approach may increase the scope and accuracy of scene estimates. It applies to a range of vision problems. We show shape from shading examples, where we rank shapes or reflectance functions in cases where these are otherwise unknown. The rankings agree with the perceived values...|$|R
5000|$|Arnaud Denis for sa mise en <b>scène</b> and his <b>interpretation</b> in Les Femmes savantes ...|$|R
40|$|An "elephant in the room" {{for most}} current object {{detection}} and localization methods {{is the lack}} of explicit modelling of partial visibility due to occlusion by other objects or truncation by the image boundary. Based on a sliding window approach, we propose a detection method which explicitly models partial visibility by treating it as a latent variable. A novel non-maximum suppression scheme is proposed which takes into account the inferred partial visibility of objects while providing a globally optimal solution. The method gives more detailed <b>scene</b> <b>interpretations</b> than conventional detectors in that we are able to identify the visible parts of an object. We report improved average precision on the PASCAL VOC 2010 dataset compared to a baseline detector...|$|R
40|$|In {{this paper}} the {{proposed}} architecture for a dynamic scene analysis {{is illustrated by}} a driver assistance system. To {{reduce the number of}} traffic accidents and to increase the drivers comfort, the thought of designing driver assistance systems rose in the past years. Principal problems are caused by having a moving observer (ego motion) in predominantly natural surroundings. In this paper we present a solution for a flexible architecture for a driver assistance system. The architecture can be subdivided into four different parts: the object-related analysis, the knowledge base, the behaviorbased <b>scene</b> <b>interpretation,</b> and the behavior planning unit. The object-related analysis is fed with data by the sensors (vision, radar). The sensor data are preprocessed (flexible sensor fusion) and evaluated (saliency map) searching for object-related information (positions, types of objects, etc.). The knowledge base is represented by static and dynamic knowledge. It consists of a set of rules (traffic rules, physical laws), additional information (GPS, lane-information) and it is implicitly used by algorithms in the system. The <b>scene</b> <b>interpretation</b> combines the information extracted by the object-related analysis and inspects the information for contradictions. It is strongly connected to the behavior planning using only information needed for the actual task. In the <b>scene</b> <b>interpretation</b> consistent representations (i. e., bird's eye view) are organized and interpreted as well as a scene analysis is performed. The results of the <b>scene</b> <b>interpretation</b> are used for decision making in behavior planning, which is controlled by the actual task...|$|E
40|$|International audienceIn {{high-level}} <b>scene</b> <b>interpretation,</b> it {{is useful}} to exploit the evolving probabilistic context for stepwise interpretation decisions. We present a new approach based on a general probabilistic framework and beam search for exploring alternative interpretations. As probabilistic scene models, we propose Bayesian Compositional Hierarchies (BCHs) which provide object-centered representations of compositional hierarchies and efficient evidence-based updates. It is shown that a BCH can be used to represent the evolving context during stepwise <b>scene</b> <b>interpretation</b> and can be combined with low-level image analysis to provide dynamic priors for object classification, improving classification and interpretation. Experimental results are presented illustrating the feasibility of the approach for the interpretation of facade images...|$|E
40|$|To {{reduce the}} number of traffic {{accidents}} and to increase the drivers comfort, the thought of designing driver assistance systems rose in the past years. Principal problems are caused by having a moving observer (ego motion) in predominantly natural surroundings. In this paper we present a solution for a flexible architecture for a driver assistance system. The architecture can be subdivided into four different parts: the object-related analysis, the knowledge base, the behavior-based <b>scene</b> <b>interpretation,</b> and the behavior planning unit. The object-related analysis is fed with data by the sensors (e. g., vision, radar). The sensor data are preprocessed (flexible sensor fusion) and evaluated (saliency map) searching for object-related information (positions, types of objects, etc.). The knowledge base is represented by static and dynamic knowledge. It consists of a set of rules (e. g., traffic rules, physical laws), additional information (i. e., GPS, lane-information) and it is implicitly used by algorithms in the system. The <b>scene</b> <b>interpretation</b> combines the information extracted by the object related analysis and inspects the information for contradictions. It is strongly connected to the behavior planning using only information needed for the actual task. In the <b>scene</b> <b>interpretation</b> consistent representations (i. e., bird’s eye view) are organized and interpreted as well as a scene analysis is performed. The results of the <b>scene</b> <b>interpretation</b> are used for decision making in behavior planning, which is controlled by the actual task. The influence of behavior planning on the behavior of the guided vehicle is limited to advices as no mechanical control (e. g., control of the steering angle) was implemented. An Intelligent Cruise Control (ICC) is shown as a spin-off for using this architecture...|$|E
40|$|SummaryRetinal image {{structure}} {{arises from}} the interaction between a surface’s three-dimensional shape, its reflectance and transmittance properties, and the surrounding light field. Any local image structure can be generated by {{an infinite number of}} different combinations of surface properties, which suggests that the visual system must somehow constrain the possible <b>scene</b> <b>interpretations.</b> The research on this has searched for such constraints in statistical regularities of two-dimensional image structure [1, 2]. Here, we present a new class of displays in which the perception of material properties cannot be explained with two-dimensional image properties. The displays manipulate the perceived three-dimensional shape of identical luminance gratings, and demonstrate that perceived three-dimensional shape can alter perceived surface reflectance...|$|R
50|$|The common-law {{concept of}} {{employment}} sets the <b>scene</b> for the <b>interpretation</b> of the Labour Relations Act 1995.|$|R
40|$|This article {{presents}} {{a method for}} cooperative reconstruction of three-dimensional scenes using multiple views. Although {{the use of multiple}} views {{is one of the most}} applied in the 3 D <b>scenes</b> <b>interpretation,</b> its use with a single mobile robot does not guarantee the perfect localization of the environment, due to odometry errors. Also, the reconstruction of dynamic environments is not allowed for a single robot, which takes views in different instants. This work proposes a system formed by multiple robots to get a cooperative reconstruction of the scene. The robots decide the best strategy to acquire the image to get the best reconstruction using an objective function defined. In this function, mainly are considered uncertainty of the reconstruction and view points. The result of this optimization is the next position of the robots...|$|R
40|$|Abduction [12] is a {{well-known}} form of common-sense reasoning that has been widely exploited in Artificial Intelligence applications, including formalization of Diagnostic Reasoning [13] and <b>Scene</b> <b>Interpretation</b> [14]. Abduction has been also proposed as a logical tool for formalizing hypothetical reasoning in E-Commerce [7], Negotiatio...|$|E
40|$|Joint {{reasoning}} about {{objects and}} 3 D scene layout has shown great promise in <b>scene</b> <b>interpretation.</b> One visual cue {{that has been}} overlooked is texture arising from a spatial repetition of objects in the scene (e. g., windows of a building). Such texture provides scene-specific constraints among objects, and thus facilitates <b>scene</b> <b>interpretation.</b> We present an approach to: (1) detecting distinct textures of objects in a scene, (2) reconstructing the 3 D shape of detected texture surfaces, and (3) combining object detections and shape-from-texture toward a globally consistent <b>scene</b> <b>interpretation.</b> Inference is formulated within the reinforcement learning framework as a sequential interpretation of image regions, starting from confident regions to guide the interpretation of other regions. Our algorithm finds an optimal policy that maps states of detected objects and reconstructed surfaces to actions which ought {{to be taken in}} those states, including detecting new objects and identifying new textures, so as to minimize a long-term loss. Tests against ground truth obtained from stereo images demonstrate that we can coarsely reconstruct a 3 D model of the scene from a single image, without learning the layout of common scene surfaces, as done in prior work. We also show that reasoning about texture of objects improves object detection. 1...|$|E
40|$|A {{significant}} problem in automatic <b>scene</b> <b>interpretation</b> {{is the ability}} to perform contextually meaningful segmentation of both static and moving images using a bottom-up approach. We examine and propose an extension to Kadir and Brady’s Scale Saliency Algorithm for quantifying temporal saliency and performing automatic spatial and temporal scale selection. ...|$|E
40|$|We show an {{approach}} to automated control of machine vision systems based on incremental creation and evaluation of a particular family of influence diagrams that represent hypotheses of imagery interpretation and possible subsequent processing decisions. In our approach, model-based machine vision techniques are integrated with hierarchical Bayesian inference to {{provide a framework for}} representing and matching instances of objects and relationships in imagery and for accruing probabilities to rank order conflicting <b>scene</b> <b>interpretations.</b> We extend a result of Tatman and Shachter to show that the sequence of processing decisions derived from evaluating the diagrams at each stage {{is the same as the}} sequence that would have been derived by evaluating the final influence diagram that contains all random variables created during the run of the vision system. Comment: Appears in Proceedings of the Fifth Conference on Uncertainty in Artificial Intelligence (UAI 1989...|$|R
40|$|We {{show the}} {{soundness}} of automated con trol of machine vision systems based on in cremental creation {{and evaluation of}} a par ticular family of influence diagrams that rep resent hypotheses of imagery interpretation and possible subsequent processing decisions. In our approach, model-based machine vi sion techniques are integrated with hierarchi cal Bayesian inference to {{provide a framework for}} representing and matching instances of ob jects and relationships in imagery, and for ac cruing probabilities to rank order con:liicting <b>scene</b> <b>interpretations.</b> We extend a result of Tatman and Shachter to show that the se quence of processing decisions derived from evaluating the diagrams at each stage {{is the same as the}} sequence that would have been derived by evaluating the final influence dia gram that contains all random variables cre ated during the run of the vision system. I...|$|R
2500|$|Crossley, James (April 2011): [...] "For EveryManc a Religion: Biblical and Religious Language in the Manchester Music <b>Scene,</b> 1976–1994". Biblical <b>Interpretation</b> 19 (2): 151–180. DOI: ...|$|R
40|$|Abstract. This paper {{presents}} a distributed multi-camera visual surveillance system for automatic <b>scene</b> <b>interpretation</b> of airport aprons. The system comprises camera based tracking and classification of objects followed by sensor fusion and high level interpretation based on cognitive spatio-temporal reasoning. The {{performance of the}} system is demonstrated for a range of test scenarios. ...|$|E
40|$|<b>Scene</b> <b>interpretation</b> {{has been}} {{identified}} {{as one of the most}} fundamental and challenging tasks already since Shakey [7]. In recent years the problem has often been addressed in-dependently with varying foci within AI, vision, and robotics. But new ambitious (bench-mark) tasks like ambient intelligence and service- and entertainment robotics (e. g., RoboCup...|$|E
40|$|A signicant {{problem in}} <b>scene</b> <b>interpretation</b> is efcient {{bottom-up}} extraction and representation of salient features. In this paper, we {{address the problem}} of correlating salient motion at a spatio-temporal level and also across spatially separated regions since it is in the interactions that more so-phisticated <b>scene</b> <b>interpretation</b> can be found. We show {{that it is possible to}} spatio-temporally locate and detect salient motion events and interactions in two contrasting scenar-ios using the same hierarchical co-occurrence framework. Thus generating a concise description of a dynamic scene from the sequence data alone. Results show it is possible to reduce a highly populated multi-dimensional co-occurrence matrix representing correlations between salient motion re-gions, to a one dimensional vector with clearly separable unusual activity. The results also show that the method in-herently provides a quantiable measure of the saliency of an interaction through its frequency of occurrence. 1...|$|E
40|$|Distributed, mobile {{surveillance}} systems offer {{advantages over}} conventional stationary sensing modalities by exploiting {{their ability to}} adapt to the environment and provide mukiple views of a <b>scene</b> for <b>interpretation.</b> By using mobile robotic technologies to autonomously position sensors in the workspace, a single user can control such a surveillance system {{as if it were a}} single logica...|$|R
40|$|We explore {{recently}} proposed nonparametric Bayesian {{statistical models}} of image partitions. These models are attractive because they adapt to images of different complexity, successfully modeling uncertainty in size, shape, {{and structure of}} human segmentations of natural scenes. We improve upon them {{in a number of}} key ways to achieve performance comparable to state-of-the-art methods. Our first major contribution is a novel discrete search based posterior inference algorithm which, compared to previous approaches, is significantly more robust and accurate. We then present a low rank version of the spatially dependent Pitman-Yor processes model, critical for efficient inference. Furthermore, we show how the Gaussian process covariance functions underlying the proposed models can be calibrated to accurately match the statistics of human segmentations. Finally, we present accurate segmentations of complex scenes as well as multiple hypothesized image partitions (capturing the inherent uncertainty in human <b>scene</b> <b>interpretations)</b> produced by our method. ...|$|R
40|$|The compact {{description}} of a video sequence through a single image map and a dominant motion has applications in several domains, including video browsing and retrieval, compression, mosaicing, and visual summarization. Building such a representation requires the capability to register all the frames {{with respect to the}} dominant object in the scene, a task which has been, in the past, addressed through temporally localized motion estimates. In this paper we show how the lack of temporal consistency associated with such estimates can undermine the validity of the dominant motion assumption, leading to oscillation between different <b>scene</b> <b>interpretations</b> and poor registration. In order to achieve temporal coherence, we augment the motion model with a generic temporal constraint which guarantees smoothness and avoids this oscillation. This increased robustness against competing interpretations leads to a perceptually more meaningful summarization of the video content. 1 Introduction Given [...] ...|$|R
40|$|Abstract—Today, {{building}} automation is advancing from simple {{monitoring and control}} tasks of lightning and heating towards more and more complex applications that require a dynamic perception and interpretation of different scenes occurring in a building. Current approaches cannot handle these newly upcoming demands. In this article, a bionically inspired approach for multimodal, dynamic scene perception and interpretation is presented, {{which is based on}} neuroscientific and neuro-psychological research findings about the perceptual system of the human brain. This approach bases on data from diverse sensory modalities being processed in a so-called neuro-symbolic network. With its parallel structure and with its basic elements being information processing and storing units at the same time, a very efficient method for scene perception is provided overcoming the problems and bottlenecks of classical dynamic <b>scene</b> <b>interpretation</b> systems. Keywords—{{building automation}}, biomimetrics, dynamic <b>scene</b> <b>interpretation,</b> human-like perception, neuro-symbolic networks. I...|$|E
40|$|We {{examine the}} {{possible}} use of Description Logics as a knowledge representation and reasoning system for high-level <b>scene</b> <b>interpretation.</b> It is shown that aggregates composed of multiple parts and constrained primarily by {{temporal and spatial}} relations {{can be used to}} represent high-level concepts such as object configurations, occurrences, events and episodes. <b>Scene</b> <b>interpretation</b> is modelled as a stepwise process which exploits the taxonomical and compositional relations between aggregate concepts while incorporating visual evidence and contextual information. It is shown that aggregates can be represented by a Description Logic ALCF(D) which provides feature chains and a concrete domain extension for quantitative temporal and spatial constraints. Reasoning services of the DL system can be used as building blocks for the interpretation process, but additional information is required to generate preferred interpretations. A probabilistic model is sketched which can be integrated with the knowledge-based framework. 1...|$|E
40|$|In {{order for}} humans and robots to {{cooperate}} in an effective manner, it must be {{possible for them to}} communicate. Spoken language is an obvious candidate for providing a means of communication. In previous research, we developed an integrated platform that combined visual <b>scene</b> <b>interpretation</b> with speech processing to provide input to a language learnin...|$|E
40|$|Abstract — The {{understanding}} of human behaviour in video is a challenging task in that the same behaviour might have several different meanings depending upon the scene and task context {{in which it is}} performed. While human seem to perform <b>scene</b> <b>interpretations</b> without effort, this is a formidable and yet unsolved task for artificial vision systems. One of the main reasons is that there exists a gap between low-level vision at signal level and high-level representation of activities at symbolic level. In this paper, we present an intelligent connection framework using Gaussian Mixture Model-based clustering (GMM) to bridge the low-level vision data and the Qualitative Normalised Templates (QNT) - a symbolic representation for human motion based on fuzzy qualitative robot kinematics, which could link the former with domain-dependent scenarios. The proposed method has been applied to the recognition of eight types of human motions and an empirical comparison with fuzzy hidden Markov-based human motion recognition system. I...|$|R
40|$|Abstract. This {{study is}} a part of a global project on urban <b>scenes</b> <b>interpretation</b> using high {{resolution}} satellite images. Actually, the research is focused on buildings and roads are used to delineate zones of interest. This study details automatic reconstruction of 3 D facets using a region matching approach based on hierarchical segmentation of images. The novelty consists in dedicating algorithms to satellital context to make them more robust to noise and to deal with low stereopair Base to Height ratio. First of all, a hierarchical segmentation process is explained, then matching regions constraints are detailed. Afterwards, optimal cuts, which corresponds to a set of regions that are likely to represent building rooftops, are processed in both hierarchies. Cuts are processed given matching scores and using regions planarity constraint. In the third part, global matching of both cuts is processed in order to obtain final matchings which will allow 3 D scene reconstruction. Eventually, some results are presented...|$|R
40|$|Abstruct- A fuzzy-probabilistic {{model of}} the Generalized Hough Transform (GHT) based on {{qualitative}} labeling of scene features is presented. Qualitative labeling of scene features is shown {{to be effective in}} pruning the search space of possible <b>scene</b> <b>interpretations</b> and also reducing the number of spurious interpretations explored by the GHT. Qualitative labeling of scene features is shown to result in the formulation of a weighted Generalized Hough Transform (WGHT) where each match of a scene feature with a model feature is assigned a weight based on the qualitative attributes assigned to the scene feature. These weights are looked upon as membership function values for the fuzzy sets defined by these qualitative attributes. A fuzzy-probabilistic model for the WGHT is presented. Analytical expressions for the probability of accumulation of random votes are derived for the WGHT and compared with the corresponding expressions for the conventional GHT. The WGHT is shown to perform better than the conventional GHT. Experimental results on intensity and range images are presented. Key Words...|$|R
