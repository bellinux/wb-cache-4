2492|2125|Public
5|$|SAS was {{developed}} at North Carolina State University from 1966 until 1976, when SAS Institute was incorporated. SAS was further {{developed in the}} 1980s and 1990s {{with the addition of}} new <b>statistical</b> <b>procedures,</b> additional components and the introduction of JMP. A point-and-click interface was added in version 9 in 2004. A social media analytics product was added in 2010.|$|E
25|$|Sampling {{theory is}} part of the {{mathematical}} discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of <b>statistical</b> <b>procedures.</b> The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method.|$|E
25|$|Applied {{mathematics}} has substantial {{overlap with}} {{the discipline of}} statistics. Statistical theorists study and improve <b>statistical</b> <b>procedures</b> with mathematics, and statistical research often raises mathematical questions. Statistical theory relies on probability and decision theory, and makes extensive use of scientific computing, analysis, and optimization; {{for the design of}} experiments, statisticians use algebra and combinatorial design. Applied mathematicians and statisticians often work in a department of mathematical sciences (particularly at colleges and small universities).|$|E
40|$|Every <b>statistical</b> <b>procedure</b> {{carries with}} it certain {{assumptions}} that {{must be at least}} approximately true before the procedure can produce reliable, accurate results. Researchers often apply a <b>statistical</b> <b>procedure</b> to their data without checking on the validity of the assumptions of the procedure. If {{one or more of the}} assumptions of a given <b>statistical</b> <b>procedure</b> are violated, then misleading results will be produced by the procedure. It is important that those who analyze data be fully aware of the details of the <b>statistical</b> <b>procedure</b> they are using, including its companion assumptions. If one or more assumptions are violated, an alternative procedure must be used to obtain valid results...|$|R
50|$|Peirce's {{criterion}} is a <b>statistical</b> <b>procedure</b> for eliminating outliers.|$|R
40|$|A <b>statistical</b> <b>procedure</b> {{is applied}} to surveys which elicit {{discrete}} ordinal data so that groupings of questions analogous to factors may be identified. The <b>statistical</b> <b>procedure,</b> termed prediction analysis, develops a measure which reflects the conformability {{of a set of}} data to an articulated expectation. A case is presented. Prediction analysis groupings of survey questions...|$|R
25|$|The primary form of fMRI {{uses the}} blood-oxygen-level {{dependent}} (BOLD) contrast, discovered by Seiji Ogawa. This {{is a type}} of specialized brain and body scan used to map neural activity in the brain or spinal cord of humans or other animals by imaging the change in blood flow (hemodynamic response) related to energy use by brain cells. Since the early 1990s, fMRI has come to dominate brain mapping research because it does not require people to undergo shots or surgery, to ingest substances, or to be exposed to ionising radiation. This measure is frequently corrupted by noise from various sources; hence, <b>statistical</b> <b>procedures</b> are used to extract the underlying signal. The resulting brain activation can be graphically represented by color-coding the strength of activation across the brain or the specific region studied. The technique can localize activity to within millimeters but, using standard techniques, no better than within a window of a few seconds. Other methods of obtaining contrast are arterial spin labeling and diffusion MRI. The latter procedure is similar to BOLD fMRI but provides contrast based on the magnitude of diffusion of water molecules in the brain.|$|E
2500|$|... "If the {{government}} required <b>statistical</b> <b>procedures</b> to carry warning labels like those on drugs, most inference methods would have long labels indeed." [...] This caution applies to hypothesis tests and alternatives to them.|$|E
2500|$|Many {{scores are}} derived from the normal distribution, {{including}} percentile ranks ("percentiles" [...] or [...] "quantiles"), normal curve equivalents, stanines, z-scores, and T-scores. Additionally, some behavioral <b>statistical</b> <b>procedures</b> assume that scores are normally distributed; for example, t-tests and ANOVAs. Bell curve grading assigns relative grades based on a normal distribution of scores.|$|E
5000|$|... 4. Sort the {{questions}} into groups which are called clusters or factors {{by using a}} <b>statistical</b> <b>procedure</b> ...|$|R
30|$|The <b>statistical</b> <b>procedure</b> was {{performed}} by SAS 9.1 and means were compared with the LSD multiple range test.|$|R
40|$|Short {{exposure}} photographs {{capturing the}} momentary {{state of the}} plasma radiation are often used for the primary characterization of discharges. Sequence of such images gives an impression about a variation of radiation intensity in different areas {{and can be used}} to describe the fluctuation intensity. This work presents a <b>statistical</b> <b>procedure</b> which produces brightness fluctuation intensity maps. Two realizations of the <b>statistical</b> <b>procedure</b> in Matlab environment are described and compared...|$|R
2500|$|Confidence {{intervals}} and hypothesis {{tests are}} two <b>statistical</b> <b>procedures</b> {{in which the}} quantiles of the sampling distribution of a particular statistic (e.g. the standard score) are required. [...] In any situation where this statistic is a linear function of the data, divided by the usual estimate of the standard deviation, the resulting quantity can be rescaled and centered to follow Student's t-distribution. [...] Statistical analyses involving means, weighted means, and regression coefficients all lead to statistics having this form.|$|E
2500|$|GeneNetwork is an {{open source}} project {{released}} under the Affero General Public License (AGPLv3). The majority of code is written in Python, but includes modules and other code written in C, R, and JavaScript. The code is mainly Python 2.4. GN2 is mainly written in Python 2.7 in a Flask framework with Jinja2 HTML templates) but with conversion to Python 3.X planned {{over the next few}} years. GN2 calls many <b>statistical</b> <b>procedures</b> written in the R programming language. The original source code from 2010 along with a compact database are available on [...] While [...] is still actively maintened on GitHub code, as of 2017 work is focused on [...]|$|E
2500|$|Regulated {{genes are}} {{categorized}} {{in terms of}} what they are and what they do, important relationships between genes may emerge. For example, we might see evidence that a certain gene creates a protein to make an enzyme that activates a protein [...] to turn on a second gene on our list. This second gene may [...] be a transcription factor that regulates yet another gene from our list. Observing these links we may begin to suspect that they represent much more than chance associations in the results, and that they are all on our list because of an underlying biological process. On the other hand, it could be that if one selected genes at random, one might find many that seem to have something in common. In this sense, we need rigorous <b>statistical</b> <b>procedures</b> to test whether the emerging biological themes is significant or not. That is where gene set analysis comes in.|$|E
40|$|The tracing of {{metabolite}} {{signals in}} LC-MS data using stable isotope-labeled compounds {{has been described}} in the literature. However, the filtering efficiency and confidence when mining metabolite signals in complex LC-MS datasets can be improved. Here, we propose an additional <b>statistical</b> <b>procedure</b> to increase the compound-derived signal mining efficiency. This method also provides a highly confident approach to screen out metabolite signals because the correlation of varying concentration ratios of native/stable isotope-labeled compounds and their instrumental response ratio is used. An in-house computational program [signal mining algorithm with isotope tracing (SMAIT) ] was developed to perform the <b>statistical</b> <b>procedure.</b> To illustrate the SMAIT concept and its effectiveness for mining metabolite signals in LC-MS data, the plasticizer, di-(2 -ethylhexyl) phthalate (DEHP), {{was used as an}} example. The <b>statistical</b> <b>procedure</b> effectively filtered 15 probable metabolite signals from 3617 peaks in the LC-MS data. These probable metabolite signals were considered structurally related to DEHP. Results obtained here suggest that the <b>statistical</b> <b>procedure</b> could be used to confidently facilitate the detection of probable metabolites from a compound-derived precursor presented in a complex LC-MS dataset...|$|R
30|$|The {{estimates}} computed by this <b>statistical</b> <b>procedure</b> can subsequently {{be improved}} by taking geometrical considerations into account {{as shown in the}} next section.|$|R
50|$|Robust {{principal}} component analysis (RPCA) is a modification of the widely used <b>statistical</b> <b>procedure</b> {{principal component}} analysis (PCA) which works well with respect to grossly corrupted observations.|$|R
5000|$|Use <b>statistical</b> <b>procedures</b> {{to create}} an overall {{interview}} score ...|$|E
5000|$|Using {{appropriate}} <b>statistical</b> <b>procedures</b> {{to establish}} norms for the test.|$|E
5000|$|... #Caption: <b>Statistical</b> <b>procedures</b> to {{characterize}} the infection/infestation of a sample of hosts.|$|E
40|$|<b>Statistical</b> <b>procedure</b> to {{evaluate}} map accuracy {{is required to}} answer the user's doubt regarding reliability of maps. <b>Statistical</b> <b>procedure</b> involve sampling, design and determining accuracy of map and categories. Calculation of statistics on a aerial extent of land-use categories and reliable sample size are shown. Sample selection is done by stratified systematic unaligned sampling technique. The lower limit of one-tailed confidence interval is determine the entire map accuracy. The confidence interval is used to define {{the upper and lower}} limits of accuracy of the categories. The importance [...] ...|$|R
40|$|In {{this paper}} <b>statistical</b> {{selection}} <b>procedures</b> {{are discussed in}} general terms. <b>Statistical</b> selection <b>procedures</b> have been designed specifically to answer questions like 2 ̆ 2 Which treatment or variety can {{be considered to be}} the best? 2 ̆ 2. In a certain sense <b>statistical</b> selection <b>procedures</b> are more realistic in answering such a question than the usual testing and multiple comparisons <b>procedures.</b> The <b>statistical</b> selection <b>procedures</b> of Bechhofer and Gupta are considered. Some practical applications are given. Finally, attention is paid to the relatively new combined procedure of (Hsu, 1981, 1984) and the procedures of (Somerville, 1985) ...|$|R
40|$|The {{problem of}} {{introducing}} divergence-based statistics to test composite hypotheses related to s populations {{is still open}} when sample sizes are not equal. On the basis of likelihood divergence statistics, a <b>statistical</b> <b>procedure</b> is introduced in this paper and its large sample behaviour is studied. By using Renyi divergence, the proposed <b>statistical</b> <b>procedure</b> {{is applied to the}} problem of testing for the homogeneity of several variances. Members of the family of likelihood Renyi divergence statistics are compared for power and checked for fidelity to type I error rates with some classical test statistics. Results of the Monte Carlo simulation study are discussed and presented in tables...|$|R
50|$|Some {{packages}} are developed for specific purposes (e.g., time series analysis, factor analysis, calculators for probability distributions, etc.), {{while others are}} general packages, {{with a variety of}} <b>statistical</b> <b>procedures.</b> Others are meta-packages or statistical computing environments, which allow the user to code completely new <b>statistical</b> <b>procedures.</b> This article is a review of the general statistical packages.|$|E
5000|$|Probability {{theory and}} statistics: time-series analysis, nonparametrics, {{asymptotic}} <b>statistical</b> <b>procedures,</b> and computer-intensive statistical methods ...|$|E
5000|$|... #Caption: <b>Statistical</b> <b>procedures</b> {{to compare}} levels of infection/infestation across {{two or more}} samples of hosts.|$|E
50|$|A {{separation}} test is a <b>statistical</b> <b>procedure</b> for early-phase research, {{to decide whether}} to pursue further research. It is designed to avoid the prevalent situation in early-phase research, when a statistically underpowered test gives a negative result.|$|R
50|$|A decision-theoretic {{justification}} {{of the use}} of Bayesian inference was given by Abraham Wald, who proved that every unique Bayesian procedure is admissible. Conversely, every admissible <b>statistical</b> <b>procedure</b> is either a Bayesian procedure or a limit of Bayesian procedures.|$|R
50|$|Like formal {{statistical}} inference, {{the purpose}} of informal inferential reasoning is to draw conclusions about a wider universe (population/process) from data (sample). However, {{it is to be}} contrasted with formal statistical inference that formal <b>statistical</b> <b>procedure</b> or methods are not necessarily used.|$|R
5000|$|The {{following}} <b>statistical</b> <b>procedures</b> {{have been}} found to be useful in carrying out positioning analysis: ...|$|E
5000|$|ASTM E1488 - Standard Guide for <b>Statistical</b> <b>Procedures</b> to Use in Developing and Applying Test Methods ...|$|E
5000|$|... {{the power}} of the {{instruments}} and <b>statistical</b> <b>procedures</b> used to measure and detect the effects, and ...|$|E
40|$|A <b>statistical</b> <b>procedure</b> {{to assess}} level-II {{continental}} resources using Landsat MSS digital data is presented. The <b>statistical</b> <b>procedure</b> involves a two-stage cluster sample within a stratified random sample. The utility {{of this procedure}} is assessed by using it to estimate the areal extent of the conifer and hardwood resources of the continental U. S. National estimates of conifer and hardwood derived using this sampling procedure were within 3 percent of U. S. Forest Service (USFS) figures. According to the Landsat-based study, 11 {{percent of the country}} is conifer forest and 12 percent is hardwood. The corresponding USFS figures are 13 and 15 percent, respectively. Comparison of the MSS classification products and airphotos showed that the conifer cover class was correctly identified 74 percent of the time and hardwood 80 percent of the time. The average classification accuracy countrywide for the four cover types considered (conifer, hardwood, water, and 'other') is 74 percent, and the overall accuracy is 85 percent. The <b>statistical</b> <b>procedure</b> provides a method of incorporating Landsat MSS digital data as a second state for level-II continental resource assessment. Alternate data sources, e. g., satellite and aircraft photographic imagery, may also be used in conjunction with this statistical model...|$|R
50|$|In {{statistics}} education, informal inferential reasoning (also called informal inference) {{refers to}} the process of making a generalization based on data (samples) about a wider universe (population/process) while taking into account uncertainty without using the formal <b>statistical</b> <b>procedure</b> or methods (e.g. P-values, t-test, hypothesis testing, significance test).|$|R
40|$|Computer science {{unplugged}} is one {{of interesting}} approaches for education about information and communication technology using no computers. This is a practical report that we gave tuition in compression technique incorporating educational method of computer science unplugged. Furthermore, questionnaire results of that class were discussed based on <b>statistical</b> <b>procedure...</b>|$|R
