36|10000|Public
5000|$|For {{problems}} {{in which the}} dependence of the performance function is only moderately non-linear {{with respect to the}} parameters modeled as random variables, setting the importance direction as the gradient vector of the performance function in the underlying standard normal space leads to highly efficient Line <b>Sampling.</b> <b>In</b> <b>general</b> it can be shown that the variance obtained by line sampling is always smaller than that obtained by conventional Monte Carlo simulation, and hence the line sampling algorithm converges more quickly [...] The rate of convergence is made quicker still by recent advancements which allow the importance direction to be repeatedly updated throughout the simulation, and this is known as adaptive line sampling [...]|$|E
3000|$|... 80.45  % of the biopsies {{reviewed}} {{in this study}} showed adequate tissue sampling, with {{no difference between the}} groups. There is a debate about the number of glomeruli needed for defining adequate tissue <b>sampling.</b> <b>In</b> <b>general,</b> 3 – 12 glomeruli are needed in each sample to facilitate diagnosis (Oberholzer et al. 1983). It may require one glomerulus to diagnose crescentic glomerulonephritis whereas more than 20 glomeruli may be required to diagnose focal segmental glomerulonephritis. In our study, at least 10 glomeruli were required in each sample to define adequate tissue sampling.|$|E
40|$|We present recent {{results on}} Piecewise Deterministic Markov Processes (PDMPs), {{involved}} in biological modeling. PDMPs, first {{introduced in the}} probabilistic literature by [30], are a very general class of Markov processes and are being increasingly popular in biological applications. They also give new interest- ing challenges from the theoretical point of view. We give here different examples on the long time behavior of switching Markov models applied to population dynamics, on uniform <b>sampling</b> <b>in</b> <b>general</b> branching mod- els applied to structured population dynamic, on time scale separation in integrate-and-fire models used in neuroscience, and, finally, on moment calculus in stochastic models of gene expression...|$|E
40|$|The {{present study}} reports metagenomic shotgun {{sequencing}} of microbial communities of two ancient permafrost horizons of the Russian Arctic. Results demonstrate {{a significant difference}} in microbial community structure of the analyzed <b>samples</b> <b>in</b> <b>general</b> and microorganisms of the methane cycle in particular...|$|R
5000|$|The {{anisotropic}} filtering method {{most commonly}} implemented on graphics hardware is {{the composition of}} the filtered pixel values from only one line of MIP map <b>samples.</b> <b>In</b> <b>general</b> the method of building a texture filter result from multiple probes filling a projected pixel sampling into texture space is referred to as [...] "footprint assembly", even where implementation details vary.|$|R
30|$|The monocytes from septic {{patients}} {{presented with}} preserved phagocytosis, enhanced ROS and NO generation, and decreased production of inflammatory cytokines {{compared with the}} monocytes from healthy volunteers. TNF-α and IL- 6 increased and ROS generation decreased in D 7 compared with D 0 <b>samples.</b> <b>In</b> <b>general,</b> CD 163 + monocytes produced higher amounts of IL- 6 and TNF-α and lower amounts of ROS and NO than did CD 163 − monocytes.|$|R
40|$|Background: Urinary tract {{infection}} (UTI) {{in children}} {{may be associated with}} long-term complications that could be prevented by prompt treatment. Aim: To determine the prevalence of UTI in acutely ill children < 5 years presenting in general practice and to explore patterns of presenting symptoms and urine sampling strategies. Design and setting: Prospective observational study with systematic urine <b>sampling,</b> <b>in</b> <b>general</b> practices in Wales, UK. Method: In total, 1003 children were recruited from 13 general practices between March 2008 and July 2010. The prevalence of UTI was determined and multivariable analysis performed to determine the probability of UTI. Results: Out of 597 (60. 0...|$|E
40|$|Proceedings of the journées MAS 2016, session Probabilistic and Piecewise Deterministic {{models in}} BiologyWe present recent results on Piecewise Deterministic Markov Processes (PDMPs), {{involved}} in biological modeling. PDMPs, first {{introduced in the}} probabilistic literature by Davis (1984), are a very general class of Markov processes and are being increasingly popular in biological applications. They also give new interesting challenges from the theoretical point of view. We give here different examples on the long time behavior of switching Markov models applied to population dynamics, on uniform <b>sampling</b> <b>in</b> <b>general</b> branching models applied to structured population dynamic, on time scale separation in integrate-and-fire models used in neuroscience, and, finally, on moment calculus in stochastic models of gene expression...|$|E
30|$|A less {{satisfactory}} issue {{concerns the}} estimation of RSE with OPSS. The proposed strategy tends to overestimate the actual values and is more marked when RSE is small. If a moderate overestimation is appealing, because it avoids the dangerous incidence of concluding that a strategy is accurate when it is not, some relevant overestimations obtained in our study are unsuitable, because they likely mask the accuracy gained {{by the use of}} OPSS jointly with the D estimator. As pointed out by Grafström (2012), variance estimation is a bit tricky for spatial <b>sampling</b> <b>in</b> <b>general</b> and the construction of less biased variance estimators is a necessary step which calls for additional work in future investigations.|$|E
40|$|Environmental data {{collected}} during 1978 show continued compliance by Hanford with all applicable {{state and federal}} regulations. Data were collected for most environmental media including air, Columbia River water, external radiation, foodstuffs (milk, beef, eggs, poultry, and produce) and wildlife (deer, fish, and game birds), as well as soil and vegetation <b>samples.</b> <b>In</b> <b>general,</b> offsite levels of radionuclides attributable to Hanford operations during 1978 were indistinguishable from background levels...|$|R
50|$|Scanning ion-conductance {{microscopy}} (SICM) is a {{scanning probe}} microscopy technique that uses an electrode as the probe tip. SICM allows for {{the determination of the}} surface topography of micrometer and even nanometer-range structures in aqueous media conducting electrolytes. The samples can be hard or soft, are generally non-conducting, and the non-destructive nature of the measurement allows for the observation of living tissues and cells, and biological <b>samples</b> <b>in</b> <b>general.</b>|$|R
50|$|The {{statistical}} model one uses can also render the data a non-probability sample. For example, Lucas (2014b) notes that several published studies that use multilevel modeling {{have been based}} on samples that are probability <b>samples</b> <b>in</b> <b>general,</b> but nonprobability <b>samples</b> for {{one or more of the}} levels of analysis in the study. Evidence indicates that in such cases the bias is poorly behaved, such that inferences from such analyses are unjustified.|$|R
40|$|AbstractAmong {{the uses}} for global {{sensitivity}} analysis is factor prioritization. A key assumption {{for this is}} that a given factor can, through further research, be fixed to some point on its domain. For factors containing epistemic uncertainty, this is an optimistic assumption, which can lead to inappropriate resource allocation. Thus, this research develops an original method, referred to as distributional sensitivity analysis, that considers which factors would on average cause the greatest reduction in output variance, given that the portion of a particular factor's variance that can be reduced is a random variable. A key aspect of the method is that the analysis is performed directly on the samples that were generated during a global sensitivity analysis using acceptance/rejection <b>sampling.</b> <b>In</b> <b>general,</b> if for each factor, N model runs are required for a global sensitivity analysis, then those same N model runs are sufficient for a distributional sensitivity analysis...|$|E
40|$|The {{properties}} of online social networks are of great interests {{to the general}} public as well as IT professionals. Often the raw data are not available and the summaries released by the service providers are sketchy. Thus sampling is needed to reveal the hidden properties and structure of the underlying network. This thesis conducts comparative studies on various sampling methods, including Random Node (RN), Random Walk (RW) and Random Edge (RE) samplings. The properties to be discovered include the average degree and population size of the network. Additionally, this thesis proposes a new sampling method called STAR sampling and applies this method to an online social network Weibo. Furthermore, visualization of network structure is studied to explain the impact of network structure on the performance of sampling methods. We show that RE sampling is better than RN <b>sampling</b> <b>in</b> <b>general.</b> This result is supported by over 20 real-world networks...|$|E
40|$|We {{propose a}} novel {{algorithm}} for blue noise sampling {{inspired by the}} Smoothed Particle Hydrodynamics (SPH) method. SPH is a well-known method in fluid simulation [...] it computes particle distributions to minimize the internal pressure variance. We found that this results in sample points (i. e., particles) with a high quality blue-noise spectrum. Inspired by this, we tailor the SPH method for blue noise sampling. Our method achieves fast <b>sampling</b> <b>in</b> <b>general</b> dimensions for both surfaces and volumes. By varying a single parameter our method can generate a variety of blue noise samples with different distribution properties, ranging from Lloyd's relaxation to Capacity Constrained Voronoi Tessellations ({CCVT}). Our method is fast and supports adaptive sampling and multi-class sampling. We have also performed experimental studies of the SPH kernel and its influence on the distribution properties of samples. We demonstrate with examples that our method can generate a variety of controllable blue noise sample patterns, suitable for applications such as image stippling and re-meshing...|$|E
40|$|Several {{proteomics}} {{approaches to}} study {{different aspects of}} genetic and metabolic diseases are presented. The choice of technique {{is strongly dependent on}} the biological question to be addressed and the availability and amount of <b>sample.</b> <b>In</b> <b>general,</b> there are three approaches that may be used to study genetic and metabolic diseases: protein profiling of complex biological samples, identification of affected proteins, or a functional proteomics approach to study protein interactions and function. status: publishe...|$|R
50|$|Water <b>sampling</b> is used <b>in</b> <b>general</b> for {{chemical}} analysis and ecotoxicological assessment.|$|R
30|$|As {{shown in}} Table  2, the {{addition}} of GNPs significantly reduced the startup heat of all <b>samples.</b> <b>In</b> <b>general,</b> the startup heat decreased with the GNP concentration except for the 0.5  wt%. When 1.0  wt% NPs were added, the startup heat of the nanofluid decreased by 21.60  J/g (9.1  %). When 2.5  wt% NPs were added, the startup heat of the nanofluid decreased by 52.21  J/g (21.9  %). The reduced startup heat could {{reduce the risk of}} local over-heating of molten salt.|$|R
40|$|Classical {{sampling}} methods {{can be used}} to estimate the mean of a finite or infinite population. Block kriging also estimates the mean, but of an infinite population in a continuous spatial domain. In this paper, I consider a finite population version of block kriging (FPBK) for plot-based sampling. The data are assumed to come from a spatial stochastic process. Minimizing mean-squared-prediction errors yields best linear unbiased predictions that are a finite population version of block kriging. FPBK has versions comparable to simple random sampling and stratified sampling, and includes the general linear model. This method has been tested for several years for moose surveys in Alaska, and an example is given where results are compared to stratified random <b>sampling.</b> <b>In</b> <b>general,</b> assuming a spatial model gives three main advantages over classical sampling: (1) FPBK is usually more precise than simple or stratified random sampling, (2) FPBK allows small area estimation, and (3) FPBK allows nonrandom sampling designs...|$|E
40|$|An {{important}} open {{problem of}} computational neuroscience is the generic organization of computations in networks of neurons in the brain. We show here through rigorous theoretical analysis that inherent stochastic features of spiking neurons, {{in combination with}} simple nonlinear computational operations in specific network motifs and dendritic arbors, enable networks of spiking neurons to carry out probabilistic inference through <b>sampling</b> <b>in</b> <b>general</b> graphical models. In particular, it enables them to carry out probabilistic inference in Bayesian networks with converging arrows ("explaining away") and with undirected loops, that occur in many real-world tasks. Ubiquitous stochastic features of networks of spiking neurons, such as trial-to-trial variability and spontaneous activity, are necessary ingredients of the underlying computational organization. We demonstrate through computer simulations that this approach can be scaled up to neural emulations of probabilistic inference in fairly large graphical models, yielding {{some of the most}} complex computations that have been carried out so far in networks of spiking neurons...|$|E
40|$|The {{properties}} of the Gabor and Morlet transforms are examined {{with respect to the}} Fourier analysis of discretely sampled data. Forward and inverse transform pairs based on a fixed window with uniform sampling of the frequency axis can satisfy numerically the energy and reconstruction theorems; however, transform pairs based on a variable window or nonuniform frequency <b>sampling</b> <b>in</b> <b>general</b> do not. Instead of selecting the shape of the window as some function of the central frequency, we propose constructing a single window with unit energy from an arbitrary set of windows that is applied over the entire frequency axis. By virtue of using a fixed window with uniform frequency sampling, such a transform satisfies the energy and reconstruction theorems. The shape of the window can be tailored to meet the requirements of the investigator in terms of time/frequency resolution. The algorithm extends naturally to the case of nonuniform signal sampling without modification beyond identification of the Nyquist interval...|$|E
5000|$|The theorem gives {{conditions}} on sampling lattices for perfect {{reconstruction of the}} sampled. If the lattices are not fine enough to satisfy the Petersen-Middleton condition, then the field cannot be reconstructed exactly from the <b>samples</b> <b>in</b> <b>general.</b> <b>In</b> this case we say that the samples may be aliased. Again, consider the example in which [...] is a circular disc. If the Petersen-Middleton conditions do not hold, {{the support of the}} sampled spectrum will be as shown in Figure 4. In this case the spectral repetitions overlap leading to aliasing in the reconstruction.|$|R
40|$|The Luminex-based human {{leukocyte}} antigen (HLA) antibody screening {{technology is}} widespread used in laboratories affiliated to kidney transplantation programs and enables both screening (i. e. the definition of positive or negative antibody status) and antibody identification with high sensitivity and specificity. HLA typing {{at different levels of}} resolution with Luminex technology uses sequence-specific oligonucleotide probes bound to color-coded microbeads in order to identify HLA alleles encoded by the DNA <b>sample.</b> <b>In</b> <b>general,</b> the Luminex technology for histocompatibility analyses provides rapid <b>sample</b> processing <b>in</b> a 96 -well format combined with limited technical complexity which means high cost efficiency...|$|R
40|$|Occlusion-compatible traversals and z-buffering {{are often}} {{regarded}} as the only choices for resolving visibility in the image- and point-based rendering community. These algorithms do either per-frame or per-pixel computations. This paper first discusses visibility orderings for point <b>samples</b> <b>in</b> <b>general</b> and then discusses orderings that are valid for multiple views. Then we present a novel, highly efficient, visibility algorithm for point-sampled models that has much smaller per-frame cost than previous approaches. We also discuss a high-performance, cache friendly implementation of this visibility method and present results. Finally, we speculate on possible hardware implementations. 1...|$|R
40|$|ABSTRACT Hidden populations, such as {{injection}} drug users and sex workers, are central {{to a number of}} public health problems. However, {{because of the nature of}} these groups, it is difficult to collect accurate information about them, and this difficulty complicates disease prevention efforts. A recently developed statistical approach called respondent-driven sampling improves our ability to study hidden populations by allowing researchers to make unbiased estimates of the prevalence of certain traits in these populations. Yet, not enough is known about the sample-to-sample variability of these prevalence estimates. In this paper, we present a bootstrap method for constructing confidence intervals around respondent-driven sampling estimates and demonstrate in simulations that it outperforms the naive method currently in use. We also use simulations and real data to estimate the design effects for respondent-driven sampling in a number of situations. We conclude with practical advice about the power calculations that are needed to determine the appropriate sample size for a study using respondent-driven <b>sampling.</b> <b>In</b> <b>general,</b> we recommend a sample size twice as large as would be needed under simple random sampling...|$|E
40|$|The Consumer Expenditure Quarterly Interview Survey is {{an ongoing}} panel survey of U. S. {{households}} in which sample units typically receive the same survey protocol during each interview. Because of the high burden associated with the survey request, the U. S. Bureau of Labor Statistics is exploring alternative designs that, if implemented, would change many features of the data collection process. One such alternative is adaptive matrix <b>sampling.</b> <b>In</b> <b>general,</b> matrix sampling involves dividing a survey into subsets of questions and then based on some probabilistic mechanism administering each to subsamples of the main sample. To potentially compensate for the resulting loss of information, as not all questions are asked of all sample units, we propose an adaptive assignment of subsampling probabilities {{based on data from}} the first interview. We use historical data to explore potential efficiency gains incurred by the use of this form of adaptive matrix sampling, develop point estimators based on simple weighting adjustments for expenditures collected under this design, and evaluate their variance properties...|$|E
30|$|Density of peritrichs at {{the study}} site varied {{markedly}} over time. In general, {{there were some}} periods of severe reduction in abundance in all seasons, {{but it is not}} clear whether these represent a successional cycle, recovery from catastrophic events, or the result of natural variability in <b>sampling.</b> <b>In</b> <b>general,</b> ciliate densities are often positively correlated with increasing food supplies, which suggests that abundance is resource-limited (Fenchel [1968]). Moreover, experimental work has demonstrated that ciliate assemblages also may be strongly affected by direct metazoan interactions (Jack and Gilbert [1993]; Wickham and Gilbert [1993]). These interactions can be predation, mechanical interference, or both. Masclaux et al. ([2012]) demonstrated that the cladoceran Eurycercus lamellatus feeds exclusively on periphyton. Arndt (1993) has also observed that large grasping rotifer species could feed on ciliates, especially peritrichs that they could dislodge from the stalk. This ability of small invertebrates to graze on periphytic organisms would lead to fluctuations in a periphyton community. In the present work, some reductions in density were followed by reductions in species richness, suggesting a severe impact of some sort on the community. This impact could be predation by invertebrates since rotifers, cladocerans, and insect larvae were frequently found in the samples.|$|E
40|$|We {{study the}} {{performance}} of alternative sampling methods for estimating multivariate normal probabilities through the GHK simulator. The sampling methods are random-ized versions of some quasi-Monte Carlo samples (Halton, Niederreiter, Niederreiter-Xing sequences and lattice points) and some samples based on orthogonal arrays (Latin hyper-cube, orthogonal array and orthogonal array based Latin hypercube <b>samples).</b> <b>In</b> <b>general,</b> these <b>samples</b> {{turn out to have}} a better performance than Monte Carlo and antithetic Monte Carlo samples. Improvements over these are large for low-dimensional (4 and 10) cases and still signi…cant for dimensions as large as 50...|$|R
40|$|Measurements of the {{coefficient}} of linear thermal expansion alpha {{have been carried out}} for the orthorhombic and tetragonal phases of RE 1 Ba 2 Cu 3 Oy(RE = Y, Gd, Dy) compounds using a high resolution capacitance dilatometer in the temperature range 77 - 300 K. All the superconducting samples exhibit a jump DELTAalpha at their respective transition temperatures, T(c). Evidences of, sample-to-sample variation in alpha values and dependence of DELTAalpha on the sample preparation conditions, have been obtained. The non-superconducting <b>samples,</b> <b>in</b> <b>general,</b> exhibit lower values of alpha possibly because of lowering of oxygen content...|$|R
40|$|This paper explores {{empirical}} {{linkages between}} credit unions' (CUs') policies and their financial performance, {{as measured by}} loan delinquency and profitability, using a unique micro dataset of credit unions in three Latin American countries. The estimated translog profit function is generalized using a slack variable concept that parameterizes any systematic deviation from profit- maximizing behavior exhibited within the <b>sample.</b> <b>In</b> <b>general,</b> we find that performance depends in important ways on two types of CU policy variables, some associated with the incentives of borrowers to repay and others that affect the CU's ability to screen loans. Credit unions; Latin America...|$|R
40|$|Aircraft pilots (like {{operators}} in many domains) {{are required to}} monitor locations for rare events, while concurrently performing their everyday tasks. In many cases, the visual parameters of the alert are such {{that it is not}} visible unless directly fixated. For this reason, critical alerts should be designed to be visible in peripheral vision and/or augmented by an audio alarm. We use the term "conspicuity" to distinguish the attention-getting power of a visual stimulus from simple visibility in a single-task context. We have measured conspicuity in an experimental paradigm designed to test the N-SEEV model of attention and noticing (Steelman-Allen et al., HFES 2009). The subject performed a demanding central task while monitoring four peripheral locations for color change events. Visibility of the alerting stimuli was measured separately in a control experiment in which the subject maintained steady fixation without the central task. Thresholds in the dual-task experiments were lower than would be expected {{based on the results of}} the control experiment, due to the fact that the subjects actively sampled the alert locations with fixations while performing the central task. Locations of high-frequency alerts are generally sampled more often than locations of low-frequency alerts, and alert location <b>sampling</b> <b>in</b> <b>general</b> increases with practice, presumably because the demands of the central task are reduced...|$|E
40|$|Conflict over {{customary}} land dispute {{in the area}} of wetlands and located in South Kalimantan in order of importance of business expansion by certain groups increasingly rife. The results of the research in the first year indicates that the land-related conflicts in wetland areas associated with layered certificate and between the seal and the certificate in the village of Gambut District of Banjar Regency. Generally, conflicts become apparent when peat lands are uncultivated land previously held a high place in the economy due to the access road. High-value economy is usually associated with the interests of residential, business, and other economic interests. The next approach in this research uses a qualitative approach with descriptive methods. The population derived from the conflicts over indigenous land in wetland areas Banjar regency with the sampling technique is purposive <b>sampling.</b> <b>In</b> <b>general,</b> the Focus Group Discussion (FGD) for two events form the basis for formulating a model of conflict resolution of land in wetland areas. Land conflicts that stem from overlapping certificates, certificates and seals both for personal as well as public and indigenous land has led to the completion of both litigation and non litigation. Outcomes {{of this study is to}} create a model of conflict resolution of land in wetland areas in Banjar district...|$|E
40|$|A {{top-down}} approach to evaluate high ozone (O 3) formation, attributed to different emission sources, is developed for anti-cyclonic conditions {{in a region}} of Hong Kong influenced by meso-scale circulations. A near-explicit photochemical model coupled with the Master Chemical Mechanism (MCMv 3. 2) is used to investigate the chemical characteristics in the region. Important features have been enhanced in this model including the photolysis rates, simulated by the National Center for Atmospheric Research (NCAR) Tropospheric Ultraviolet and Visible (TUV) Radiation Model, as well as hourly variation of Volatile Organic Compound (VOC) concentration input from on-site <b>sampling.</b> <b>In</b> <b>general,</b> the combined model gives a reasonably good representation of high O 3 levels in the region. The model successfully captured a multi-day O 3 event {{in the autumn of}} 2010. Source apportionment via Positive Matrix Factorization (PMF) was carried out on the sampled VOC data, to determine the major sources in the region. Based on the outcomes of the PMF source apportionment, a sensitivity analysis using the developed photochemical model was conducted and used to estimate O 3 reduction under different source removal regimes. Results indicate that vehicular emissions are the dominant VOC source contributing to O 3 formation. This study has demonstrated a potentially efficient secondary pollutants control methodology, using a combined field measurements and modelling approach. Department of Civil and Environmental Engineerin...|$|E
40|$|The {{modified}} {{ring test}} was used to determine the mode I fracture toughness of bedrock cores from the DOE Oak Ridge Reservation in east Tennessee. Low porosity sandstones, limestones, and dolostones from {{the lower part of the}} Paleozoic section in Copper Creek and Whiteoak Mountain thrust sheets were <b>sampled.</b> <b>In</b> <b>general,</b> the average mode I fracture toughness decreases from sandstone, dolostone, and limestone. The fracture toughness of the limestones varies between rock units, which is related to different sedimentologic characteristics. Quality of results was evaluated by testing cores of Berea Sandstone and Indiana Limestone, which produced results similar to published results...|$|R
5000|$|Egon Pearson (Karl's son) and Jerzy Neyman {{introduced}} {{the concepts of}} [...] "Type II" [...] error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random <b>sampling</b> was <b>in</b> <b>general</b> a better method of estimation than purposive (quota) sampling.|$|R
40|$|Organisational {{change can}} {{generate}} skepticism and resistan-ce in employees, making it sometimes {{difficult or impossible}} to implement organisational improvements. To enable the University Hospital Bratislava to manage these realities in a most effective way, the assessment of attitudes to change was conducted among 304 full-time hospital employees {{in the summer of}} 1999. The assessment was based on the Change Climate Survey al-lowing for investigation of attitudes in four main areas: orientation towards change <b>in</b> <b>general,</b> understanding and acceptance of change, management of change, and change outocmes. In spite of some limitations {{due to the fact that}} the survey did not use a true random <b>sample,</b> <b>in</b> <b>general,</b> the re-sults seem to be able to illustrate the University Hospital em-ployees attitudes to change...|$|R
