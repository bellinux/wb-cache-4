277|10000|Public
50|$|He has {{collaborated on}} studies of adults' <b>spoken</b> <b>word</b> <b>recognition,</b> speech perception, and word {{learning}} with Michael Tanenhaus.|$|E
50|$|Grosjean, F. and Frauenfelder, U. (Eds.). (1997). A Guide to <b>Spoken</b> <b>Word</b> <b>Recognition</b> Paradigms. Hove, England: Psychology Press.|$|E
5000|$|Vitevitch, Michael S. & Luce, Paul A. 1999. Probabilistic phonotactics and {{neighborhood}} activation in <b>spoken</b> <b>word</b> <b>recognition.</b> Journal of Memory and Language 40: 374-408.|$|E
40|$|Human speech {{recognition}} in biometric systems is an actual problem, which science intensively deals with. One of most used methods is {{the method of}} hidden Markovov’s models. Attention in isolated <b>words</b> <b>recognition</b> is focused on characteristic speech signal parameters obtaining, enabling most clear identification due to hiddem Markov model application. This work concentrates on biometric systems, its methods, and then is focused on isolated <b>words</b> <b>recognition</b> problems. The hidden Markov model recognition system with usage of some Matlab functions is designed. Concept focuses on characteristic speech signal parameters obtaining, code book making through vector quantization, words model training and finally probability of concrete model and obtained word similarity evaluation. Ratio for one speaker's <b>spoken</b> <b>words</b> <b>recognition</b> reaches 40 %...|$|R
40|$|An {{increasing}} number of studies has demonstrated that learning {{to read and write}} influences the way spoken language is processed. While previous research on this field of study almost exclusively investigated orthographic effects on L 1 <b>spoken</b> <b>word</b> processing, the present thesis is aimed at extending previous findings to L 2 <b>spoken</b> <b>word</b> processing. In {{the first part of the}} thesis, a working model of bilingual <b>word</b> <b>recognition</b> is developed, which is able to explain orthographic and phonological interactions during L 2 word processing. The architecture of the working model is based on previous outcomes of research on auditory and visual <b>word</b> <b>recognition</b> as well as current findings on orthographic influences during <b>spoken</b> auditory <b>word</b> <b>recognition.</b> In the second part of the thesis, the working model is tested in three psycholinguistic experiments on auditory <b>word</b> <b>recognition</b> (rhyme judgement, phoneme deletion and lexical decision). Special emphasis of the investigation lies on a cross-linguistic comparison between native and non-native speakers of English as well as between two groups of non-native speakers of English with different orthographic backgrounds, i. e. a shallow orthography (German) and a deep orthography (Danish). The results of the experiments show that orthographic information is activated during L 2 auditory word processing. However, differences exist between L 2 participants from a shallow and a deep L 1 orthographic system. These results are discussed with reference to the working model of bilingual <b>word</b> <b>recognition...</b>|$|R
40|$|This paper {{discusses}} four {{experiments on}} Dutch which show that distinctive phonological features {{differ in their}} relevance for <b>word</b> <b>recognition.</b> The relevance of a feature for <b>word</b> <b>recognition</b> depends on its phonological stability, that is, {{the extent to which}} that feature is generally realized in accordance with its lexical specification in the relevant word position. If one feature value is uninformative, all values of that feature are less relevant for <b>word</b> <b>recognition,</b> with the least informative feature being the least relevant. Features differ in their relevance both in <b>spoken</b> and written <b>word</b> <b>recognition,</b> though the differences are more pronounced in auditory lexical decision than in self-paced reading...|$|R
5000|$|James Lloyd [...] "Jay" [...] McClelland, FBA (born December 1, 1948) is the Lucie Stern Professor at Stanford University, {{where he}} was {{formerly}} {{the chair of the}} Psychology Department. [...] He {{is best known for his}} work on statistical learning and Parallel Distributed Processing, applying connectionist models (or neural networks) to explain cognitive phenomena such as <b>spoken</b> <b>word</b> <b>recognition</b> and visual word recognition. McClelland is to a large extent responsible for the large increase in scientific interest for connectionism in the 1980s.|$|E
40|$|ABSTRACT: <b>Spoken</b> <b>word</b> <b>recognition</b> was {{investigated}} {{in a group of}} children aged 3 : 0 to 7 : 11 (years:months) to assess the relationship between five measures of language development and <b>spoken</b> <b>word</b> <b>recognition</b> accuracy. Two <b>spoken</b> <b>word</b> <b>recognition</b> tasks, gated words and noisecenter words, were used. In the gating task, participants were asked to identify words whose final consonants had been removed. In the noise-center task, participants were asked to identify words whose medial vowel had been replaced by broadband noise. In both tasks, participants provided a nonverbal picture-pointing response. Five measures of language development were examined as possible predictors of <b>spoken</b> <b>word</b> <b>recognition</b> accuracy: expressive vocabulary, receptive vocabulary, pre-literacy skills, phonological awareness, and articulation accuracy...|$|E
40|$|The {{present study}} aims {{to isolate the}} locus of the {{frequency}} effect within the <b>spoken</b> <b>word</b> <b>recognition</b> architecture. By applying the additive factors logic (Sternberg, 1969) to an auditory lexical decision task where both word frequency and stimulus quality were factorially manipulated, the reaction time data can be analyzed to study processing stages along {{the time course of}} <b>spoken</b> <b>word</b> <b>recognition,</b> and determine if frequency has an early or late locus. A significant underadditive interaction of frequency and stimulus quality was obtained. Surprisingly, the typically robust frequency effect was not reliable for words of low stimulus quality. This finding suggests that word frequency influences a relatively late stage in the <b>spoken</b> <b>word</b> <b>recognition</b> process. Implications for extant models of <b>spoken</b> <b>word</b> <b>recognition</b> are discussed...|$|E
40|$|Three cross-modal priming {{experiments}} {{examined the}} role of suprasegmental information in the processing of <b>spoken</b> <b>words.</b> All primes consisted of truncated <b>spoken</b> Dutch <b>words.</b> <b>Recognition</b> of visually presented word targets was facilitated by prior auditory presentation {{of the first two}} syllables of the same words as primes, but only if they were appropriately stressed (e. g., OKTOBER preceded by okTO-); inappropriate stress, compatible with another word (e. g., OKTOBER preceded by OCto-, the beginning of octopus), produced inhibition. Monosyllabic fragments (e. g., OC-) also produced facilitation when appropriately stressed; if inappropriately stressed, they produced neither facilitation nor inhibition. The bisyllabic fragments that were compatible with only one word produced facilitation to semantically associated words, but inappropriate stress caused no inhibition of associates. The results are explained within a model of spoken-word recognition involving competition between simultaneously activated phonological representations followed by activation of separate conceptual representations for strongly supported lexical candidates; {{at the level of the}} phonological representations, activation is modulated by both segmental and suprasegmental information...|$|R
40|$|Automatic {{detection}} of disfluencies in spoken language {{is important for}} making speech recognition output more readable, and for aiding downstream language processing modules. We compare a generative hidden Markov model (HMM) -based approach and two conditional models — a maximum entropy (Maxent) model and a conditional random field (CRF) — for detecting disfluencies in speech. The conditional modeling approaches provide a more principled way to model correlated features. In particular, the CRF approach directly detects the reparandum regions, and thus avoids the use of ad-hoc heuristic rules. We evaluate performance of these three models across two different corpora (conversational speech and broadcast news) and for two types of transcriptions (human transcriptions and recognition output). Overall we find that that the conditional modeling approaches (Maxent and CRF) provide benefit over the HMM approach. Effects of <b>speaking</b> style, <b>word</b> <b>recognition</b> errors, and future directions are also discussed. 1...|$|R
40|$|Speech {{recognition}} is influential signal processing in communication technology. Speech recognition has allowed software {{to recognize the}} <b>spoken</b> <b>word.</b> Automatic speech <b>recognition</b> could be a solution to recognize the <b>spoken</b> <b>word.</b> This application was developed using Linear Predictive Coding (LPC) for feature extraction of speech signal and Hidden Markov Model (HMM) for generating the model of each the <b>spoken</b> <b>word.</b> The data of speech used for training and testing was produced by 10 speaker (5 men and 5 women) whose each speakers spoke 10 words and each of <b>words</b> <b>spoken</b> for 10 times. This research is tested using 10 -fold cross validation for each pair LPC order and HMM states. System performance is measured based on the average accuracy testing from men and women speakers. According to the test results {{that the amount of}} HMM states affect the accuracy of system and the best accuracy is 94, 20 % using LPC order = 13 and HMM state= 16...|$|R
40|$|This {{chapter is}} {{a review of}} the {{literature}} in experimental psycholinguistics on <b>spoken</b> <b>word</b> <b>recognition.</b> It is organized around eight questions. 1. Why are psycholinguists interested in <b>spoken</b> <b>word</b> <b>recognition?</b> 2. What information in the speech signal is used in word recognition? 3. Where are the words in the continuous speech stream? 4. Which words did the speaker intend? 5. When, as the speech signal unfolds over time, are the phonological forms of words recognized? 6. How are words recognized? 7. Whither <b>spoken</b> <b>word</b> <b>recognition?</b> 8. Who are the researchers in the field...|$|E
40|$|One of {{the largest}} {{remaining}} unsolved mysteries in cognitive science is how the rapid input of spoken language is mapped onto phonological and lexical representations over time. Attempts at psychologically-tractable computational models of <b>spoken</b> <b>word</b> <b>recognition</b> tend either to ignore time or to transform the temporal input into a spatial representation. This is the approach taken in TRACE (McClelland & Elman, 1986), the model of <b>spoken</b> <b>word</b> <b>recognition</b> that has the broadest and deepest coverage of phenomena in speech perception, <b>spoken</b> <b>word</b> <b>recognition,</b> and lexical parsing of multi-word sequences. TRACE reduplicates featural, phonemic, and lexical inputs at every time step in a potentially very large memory trace, and has rich interconnections (excitatory forward and backwar...|$|E
40|$|In spelling-to-dictation tasks, skilled spellers {{consistently}} initiate spelling of high-frequency words {{faster than}} that of low-frequency words. Tainturier and Rapp's model of spelling shows three possible loci for this frequency effect: <b>spoken</b> <b>word</b> <b>recognition,</b> orthographic retrieval, and response execution of the first letter. Thus far, researchers have attributed the effect solely to orthographic retrieval without considering <b>spoken</b> <b>word</b> <b>recognition</b> or response execution. To investigate word frequency effects at {{each of these three}} loci, Experiment 1 involved a delayed spelling-to-dictation task and Experiment 2 involved a delayed/uncertain task. In Experiment 1, no frequency effect was found in the 1200 -ms delayed condition, suggesting that response execution is not affected by word frequency. In Experiment 2, no frequency effect was found in the delayed/uncertain task that reflects the orthographic retrieval, whereas a frequency effect was found in the comparison immediate/uncertain task that reflects both <b>spoken</b> <b>word</b> <b>recognition</b> and orthographic retrieval. The results of this two-part study suggest that frequency effects in <b>spoken</b> <b>word</b> <b>recognition</b> play a substantial role in skilled spelling-to-dictation. Discrepancies between these findings and previous research, and the limitations of the present study, are discussed...|$|E
40|$|Abstract The {{main idea}} {{of this paper is}} to develop a speech {{recognition}} system. By using this system smart home appliances are controlled by <b>spoken</b> <b>words.</b> The <b>spoken</b> <b>words</b> chosen for <b>recognition</b> are Fan On Fan Off Light On Light Off TV On and TV Off. The input of the system takes speech signals to control home appliances. The proposed system has two main parts speech recognition and smart home appliances electronic control system. Speech recognition is implemented in MATLAB environment. In this process it contains two main modules feature extraction and feature matching. Mel Frequency Cepstral Coefficients MFCC is used for feature extraction. Vector Quantization VQ approach using clustering algorithm is applied for feature matching. In electrical home appliances control system RF module is used to carry command signal from PC to microcontroller wirelessly. Microcontroller is connected to driver circuit for relay and motor. The input commands are recognized very well. The system is a good performance to control home appliances by <b>spoken</b> <b>words...</b>|$|R
40|$|Abstract [...] Human {{computer}} {{interaction is}} defined as Users (Humans) interact with the computers. Speech recognition {{is an area of}} computer science that deals with the designing of systems that recognize <b>spoken</b> <b>words.</b> Speech <b>recognition</b> system allows ordinary people to speak to the system. Recognizing and understanding a spoken sentence is obviously a knowledge-intensive process, which must take into account all variable information about the speech communication process, from acoustics to semantics and pragmatics. This paper is the survey of how speech is converted in text and that text in translated into another language. In this paper, we outline a speech recognition system, learning based approach and target language generation mechanism with the help of language English-Sanskrit language pair using rule based machine translation technique [1]. Rule Based Machine Translation provides high quality translation and requires in depth knowledge of the language apart from real world knowledge and the differences in cultural background and conceptual divisions. Here the English speech is first converted into text and that will translated into Sanskrit language...|$|R
40|$|Under what format(s) are <b>spoken</b> <b>words</b> memorized by the brain? Are word forms {{stored as}} {{abstract}} phonological representations? Or rather, are they stored as detailed acoustic-phonetic representations? (For example {{as a set}} of acoustic exemplars associated with each word). We present a series of experiments whose results point to the existence of prelexical phonological processes in <b>word</b> <b>recognition</b> and suggest that <b>spoken</b> <b>words</b> are accessed using a phonological code...|$|R
40|$|Auditory {{processing}} and <b>spoken</b> <b>word</b> <b>recognition</b> difficulties {{have been observed}} in Specific Language Impairment (SLI), raising the possibility that auditory perceptual deficits disrupt word recognition and, in turn, phonological {{processing and}} oral language. In this study, fifty-seven kindergarten children with SLI and fifty-three language typical age-matched controls were assessed with a speech-gating task to measure <b>spoken</b> <b>word</b> <b>recognition,</b> psychophysical tasks to measure auditory Frequency Modulation (FM) detection and Frequency Discrimination (FD), and standardized psychometric tests of phonological processing and oral language. As a group, children with SLI took significantly longer than language-typical controls to recognize words with high neighborhood density, perhaps reflecting subpar phonological representations. FM, but not FD, was significantly worse in SLI. However, while both poorer speech-gating performance and poorer auditory thresholds (FM) were evident in SLI, <b>spoken</b> <b>word</b> <b>recognition</b> did not mediate any relation between auditory perception and either phonological processing or oral language...|$|E
40|$|Auditory word {{recognition}} proceeds fluidly despite numerous perturbations and {{obstacles that}} exist in the form of acoustic-phonetic variability. A significant amount of research suggests that <b>spoken</b> <b>word</b> <b>recognition</b> processes accommodate variability in order to ensure recognition of the intended word. In the research reported here, we investigate the role of additional processing time for <b>spoken</b> <b>word</b> <b>recognition</b> and its role when there is mismatching information (both segmental and subcategorical). Three phoneme monitoring experiments examined the manner in which additional processing time influences <b>spoken</b> <b>word</b> <b>recognition.</b> Experiment 1 a introduced a version of the phoneme monitoring paradigm in which a silent interval is inserted prior to the word-final target phoneme. Phoneme monitoring reaction time decreased as the silent interval increased indicating that lexical knowledge was utilized more effectively with additional processing time. Experiment 1 b used short, medium, and long words and derived nonwords with word-initial mismatching segments. Phoneme monitorin...|$|E
40|$|Abstract. This study {{examined}} the reliability and inter-list equivalency of two new recorded <b>spoken</b> <b>word</b> <b>recognition</b> measures, the Lexical Neighborhood Test (LNT) and the Multisyllabic Lexical Neighborhood Test (MLNT), and evaluated the effects of lexical difficulty on <b>spoken</b> <b>word</b> <b>recognition</b> by children with hearing loss. Participants were 16 children with prelingual, profound deafness who used a cochlear implant. Test-retest reliability was high and no significant inter-list differences were observed for both measures. In addition, we found that lexically “easy ” words (i. e., those that occur often and have few phonemically words {{with which they are}} similar) were recognized correctly more often than lexically “hard ” words (i. e., those with opposite characteristics). The results demonstrate that the LNT and MLNT provide reliable information about <b>spoken</b> <b>word</b> <b>recognition</b> abilities of children with profound hearing loss who use cochlear implants. In addition, these new measures also provide more detailed information about the way in which these children organize and access spoken words from long-term lexical memory...|$|E
40|$|The {{ability of}} a reader to {{recognize}} written words correctly, virtually and effortlessly is defined as <b>Word</b> <b>Recognition</b> or Isolated <b>Word</b> <b>Recognition.</b> It will recognize each word from their shape. Speech Recognition is the operating system which enablesto convert <b>spoken</b> <b>words</b> to written text which is called as Speech to Text (STT) method. Usual Method used in Speech Recognition (SR) is Neural Network, Hidden Markov Model (HMM) and Dynamic Time Warping (DTW). The widely used technique for Speech Recognition is HMM. Hidden Markov Model assumes that successive acoustic features of a <b>spoken</b> <b>word</b> are state independent. The occurrence of one feature is independent of the occurrence of the others state. Here each single unit of word is considered as state. Based upon the probability of the state it generates possible word sequence for the <b>spoken</b> <b>word.</b> Instead of listening to the speech, the generated sequence of text can be easily viewed. Each word is recognized from their shape. People with hearing impaired can make use of this Speech Recognition...|$|R
40|$|Under what format(s) are <b>spoken</b> <b>words</b> memorized by the brain? Are word forms {{stored as}} {{abstract}} phonological representations? Or rather, are they stored as detailed acoustic-phonetic representations? (For example {{as a set}} of acoustic exemplars associated with each word). We present a series of experiments whose results point to the existence of prelexical phonological processes in <b>word</b> <b>recognition</b> and suggest that <b>spoken</b> <b>words</b> are accessed using a phonological code. 1. INTRODUCTION Linguistics makes a strong case for the psychological reality of phonological representations. It is important to assess how and when phonological representations are used by the brain. Some have argued that phonological representations may be used in speech production but not in speech perception [1, 2, 3]. They propose, instead, that <b>word</b> <b>recognition</b> involves a "direct" mapping from an acoustic representation of the input to the lexical representations. The series of experiments presented in this paper a [...] ...|$|R
40|$|The {{performance}} of automatic speech recognizers (ASR) typically degrades for test speakers with "outlier" characteristics, for example, speakers with foreign accent and fast speaking rate. In this work, we {{concentrate on the}} latter. Consistent with other researchers, we have observed that for speakers with exceptionally high <b>speaking</b> rate, the <b>word</b> <b>recognition</b> error is significantly higher. We have investigated two possible causes for this effect. Inherent spectral differences may cause the extracted features for these outliers to be significantly {{different from that of}} normal speech. Also, due to phone omissions and duration reduction, the normal word-models may not be suitable for fast speech. Based on our exploratory experiments on TIMIT and WSJ corpora, we believe the spectral differences and duration reduction are both significant sources of the increased error. By adapting our MLP phonetic probability estimator to fast speech, and employing fast speaker word-models, we have been [...] ...|$|R
40|$|International audienceAccording to McLennan and Luce [1], {{variability}} in talker identity affects <b>spoken</b> <b>word</b> <b>recognition</b> when processing {{is slow and}} effortful. In the present study, we tested this hypothesis by manipulating the neighbourhood density of target words in a repetition priming experiment. Both for words with few and many phonological neighbours, the amount of priming for repeated words was not affected by a voice change. Such observation supports the claim that abstract representations exist and underlie <b>spoken</b> <b>word</b> <b>recognition.</b> Dans cette étude, nous avons examiné l'impact d'un changement de voix sur le processus de reconnaissance des mots parlés...|$|E
40|$|Models of <b>spoken</b> <b>word</b> <b>recognition</b> vary in {{the ways}} in which they capture the {{relationship}} between speech input and meaning. Modular accounts prohibit a word’s meaning from affecting the computation of its form-based representation, whereas interactive models allow activation at the semantic level to affect phonological processing. We tested these competing hypotheses by manipulating word familiarity and imageability, using lexical decision and repetition tasks. Responses to high-imageability words were significantly faster than those to low-imageability words. Repetition latencies were also analyzed as a function of cohort variables, revealing a significant imageability effect only for words that were members of large cohorts, suggesting that when the mapping from phonology to semantics is difficult, semantic information can help the discrimination process. Thus, these data support interactive models of <b>spoken</b> <b>word</b> <b>recognition.</b> Models of <b>spoken</b> <b>word</b> <b>recognition</b> vary {{in the ways}} in which they capture the relationship between phonology and semantics. Accounts that assume a modular architecture, in which the speech input passes through a series of stages until the meaning of the word is accesse...|$|E
40|$|Abstract. Large {{individual}} differences in <b>spoken</b> <b>word</b> <b>recognition</b> performance {{have been found in}} deaf children following cochlear implantation. Recently, Pisoni and Geers (2000) reported that simple forward digit span measures of verbal working memory were significantly correlated with <b>spoken</b> <b>word</b> <b>recognition</b> scores even after potentially confounding variables were statistically controlled for. The present study replicates and extends these initial findings to the full set of 176 participants in the CID cochlear implant study. The pooled data indicate that despite statistical “partialling-out ” of differences in chronological age, communication mode, duration of deafness, duration of device use, age of onset of deafness, number of active electrodes, and speech feature discrimination, significant correlations still remain between digit span and several measures of <b>spoken</b> <b>word</b> <b>recognition.</b> Strong correlations were also observed between speaking rate and both forward and backwards digit span, a result that is similar to previously reported findings in normal-hearing adults and children. The results suggest that perhaps as much as 20 % of the currently unexplained variance in spoken wor...|$|E
30|$|In the 1970 s, {{research}} on speech classification just focused on isolated <b>word</b> <b>recognition</b> [28] {{and the problems}} of connected <b>word</b> <b>recognition</b> was a focus {{of research in the}} 1980 s with the goal of creating a robust system capable of recognizing a fluently <b>spoken</b> string of <b>words</b> based on matching a concatenated pattern of individual words [29].|$|R
2500|$|<b>Word</b> <b>recognition</b> {{is usually}} {{used in both}} narrow and broad ways. When it {{is used in the}} narrow sense, it means the moment when a match occurs between a printed word and its orthographic word-form stored in the lexicon, or a match between a <b>spoken</b> <b>word</b> and its phonological word-form. Only after this match has taken place, all the syntactical and {{morphological}} information of the word and {{the meaning of the word}} will become accessible for further processing. In a broader way, it refers to lexical access is the entire period from the match processing to the retrieval of lexical information. [...] In the research of bilingual lexical access, <b>word</b> <b>recognition</b> uses single, out-of-context words from both languages to investigate all the aspects of bilingual lexical access.|$|R
40|$|Algorithm is {{designed}} for isolated Kannada <b>word</b> <b>recognition</b> of five districts Kannada speakers’ accent. Isolated Kannada <b>words</b> <b>recognition</b> {{is designed}} using the syllables, Baum-Welch algorithm and Normal fit method. The novelty of proposed method is in recognition of five district Kannada speaker accents as well as <b>spoken</b> <b>words.</b> Our model is compared with baseline Hidden Markov Model (HMM) and Gaussian Mixture Model (GMM). Our model is tested for both noisy (little) and noiseless signal. For experiment totally 7056 signals are used for training and 3528 signals are used for testing. The experiment showed that average WRR for known accent speaker is 95. 56 % and of unknown accent speaker is 90. 86 % and average recognition of Kannada speaker accents is 82 %...|$|R
40|$|In this paper, {{we report}} on {{experiments}} that investigated form-based similarity effects in visual <b>spoken</b> <b>word</b> <b>recognition.</b> Specifically, we tested whether accuracy of speechreading a word {{was related to the}} number of words (neighbors) perceptually similar to that stimulus word and to its frequency of occurrence. In the first Experiment, the Neighborhood Activation Model (NAM) [1, 2] was adapted to generate predictions about the accuracy of visual spoken word identification. In the second Experiment, we used the concept of the Lexical Equivalence Class Size [3] to generate predictions regarding the accuracy of visual <b>spoken</b> <b>word</b> <b>recognition.</b> Both experiments provided evidence that words are identified more accurately if they have few neighbors and occur frequently in the language. Correlational analyses provided evidence that a word’s neighbors, or close competitors, are based on perceptually defined similarity. The results of the current experiments are interpreted as evidence of a common <b>spoken</b> <b>word</b> <b>recognition</b> system for both auditory and visual speech information, which retains sensitivity to form-based stimulus similarity among words. 1...|$|E
40|$|According to activation-based {{models of}} <b>spoken</b> <b>word</b> <b>recognition,</b> words with many and high {{frequency}} neighbors are processed {{more slowly than}} words with few and low frequency neighbors. Because empirical support for inhibitory neighborhood effects comes mainly from studies conducted in English, the effects of neighborhood density and neighborhood frequency were examined in French. As typically observed in English, we found that words residing in dense neighborhoods are recognized more slowly than words residing in sparse neighborhoods. Moreover, we showed that words with higher frequency neighbors are processed more slowly than words with no higher frequency neighbors. Implications of theses results for <b>spoken</b> <b>word</b> <b>recognition</b> are discussed...|$|E
40|$|A {{spoken word}} {{with more than}} one {{syllable}} contains a specific stress pattern found to be processed during <b>spoken</b> <b>word</b> <b>recognition.</b> The present study investigated the word's pitch contour as a single auditory parameter that marks stress. Event-related brain potentials (ERPs) were recorded while subjects made decisions to artificially pitch manipulated words. ERPs revealed that pitch contours are discriminated already within the first syllable of a word. Furthermore, behavioral responses for words with incorrect pitch contours were longer than for words with correct pitch contours. The results suggest that the pitch contour is an auditory feature of the spoken word that a listener automatically processes during <b>spoken</b> <b>word</b> <b>recognition...</b>|$|E
50|$|<b>Word</b> <b>recognition</b> {{is usually}} {{used in both}} narrow and broad ways. When it {{is used in the}} narrow sense, it means the moment when a match occurs between a printed word and its orthographic word-form stored in the lexicon, or a match between a <b>spoken</b> <b>word</b> and its phonological word-form. Only after this match has taken place, all the syntactical and {{morphological}} information of the word and {{the meaning of the word}} will become accessible for further processing. In a broader way, it refers to lexical access is the entire period from the match processing to the retrieval of lexical information. In the research of bilingual lexical access, <b>word</b> <b>recognition</b> uses single, out-of-context words from both languages to investigate all the aspects of bilingual lexical access.|$|R
40|$|Many {{disabled}} people usually depend on {{others in their}} daily life especially in getting {{from one place to}} another. For the wheelchair users, they need continuously someone to help them in getting the wheelchair moving. By having a wheelchair control system they become more independent. The system is a wireless wheelchair control system which employs a voice recognition system for triggering and controlling all its movements. The wheelchair responds to the voice command from its user to perform any movements functions. It integrates a microcontroller, wireless microphone, voice recognition processor, motor control interface board to move the wheelchair. By using the system, the users are able to operate the wheelchair by simply speak to the wheelchair microphone. The basic movement functions includes forward and reverse direction, left and right turns and stop. The <b>spoken</b> <b>words</b> are linked to the voice recognition processor via a wireless microphone attached closed to the user’s mouth. The wheelchair is also equipped with two infrared sensors which mounted in front and rear of the wheelchair to detect obstacles for collision avoidance function. It utilizes a PIC controller manufactured by Microchip Technology to control the system operations. It communicates with the voice recognition processor to detect <b>word</b> <b>spoken</b> and then determines the corresponding output command to drive the left and right motors. To accomplish this task, an assembly language program is written and stored in the controller’s memory. In order to recognize the <b>spoken</b> <b>words,</b> the voice <b>recognition</b> processor must be trained with the <b>word</b> <b>spoken</b> out by the user who is going to operate the wheelchair...|$|R
40|$|This paper {{describes}} an integrated framework to paraphrase spontaneous speech into written-style sentences. Most current speech recognition systems try to transcribe whole <b>spoken</b> <b>words</b> correctly. However, <b>recognition</b> results of spontaneous speech are usually difficult to understand, {{even if the}} recognition is perfect, because spontaneous speech includes redundant information, {{and it has a}} different style from that of written sentences. Especially, the style of spoken Japanese is much different from that of written one. Therefore, techniques to paraphrase recognition results are indispensable for generating captions or minutes from speech. To realize efficient speech paraphrasing, we attempt to translate spontaneous speech directly into writtenstyle sentences using a Weighted Finite-State Transducer (WFST). This approach enables to use all the knowledge sources in a one-pass search strategy and reduces the search error, since the constraint of the paraphrasing model is used {{from the beginning of the}} search. We conducted experiments on a 20 k-word Japanese lecture speech recognition and paraphrasing task. Our approach yielded improvements on both recognition accuracy and paraphrasing accuracy compared with other approaches that deal with speech recognition and paraphrasing performed separately. 1...|$|R
