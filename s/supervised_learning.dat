7175|2393|Public
25|$|Tasks {{that fall}} within the {{paradigm}} of <b>supervised</b> <b>learning</b> are pattern recognition (also known as classification) and regression (also known as function approximation). The <b>supervised</b> <b>learning</b> paradigm is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition).|$|E
25|$|The {{three major}} {{learning}} paradigms each correspond {{to a particular}} learning task. These are <b>supervised</b> <b>learning,</b> unsupervised learning and reinforcement learning.|$|E
25|$|Once the stacked auto encoder is trained, its output {{can be used}} as {{the input}} to a <b>supervised</b> <b>learning</b> {{algorithm}} such as support vector machine classifier or a multi-class logistic regression.|$|E
50|$|In <b>supervised</b> feature <b>learning,</b> {{features}} are learned {{in part with}} labeled input data. Examples include supervised neural networks, multilayer perceptron, and (<b>supervised)</b> dictionary <b>learning.</b>|$|R
40|$|One {{distinctive}} feature of Purkinje cells {{is that they}} have two types of discharge: in addition to simple spikes they fire complex spikes in response to input from the climbing fibers. These complex spikes have an initial rapid burst of spikes and spikelets followed by a sustained depolarization; in some models of cerebellar function this climbing fiber input <b>supervises</b> <b>learning</b> in Purkinje cells. On the other hand, synaptic plasticity is often thought to rely on the timing of pre-synaptic and post-synaptic spikes. It is suggested here that the period of depolarization following a complex spike, combined with a simple spike-timing-dependent plasticity rule, gives a mechanism for the climbing fiber to <b>supervise</b> <b>learning</b> in the Purkinje cell. This proposal is illustrated using a simple simulation in which it is seen that the climbing fiber succeeds in <b>supervising</b> the <b>learning...</b>|$|R
50|$|The K-nearest {{neighbor}} classification {{performance can}} often be significantly improved through (<b>supervised)</b> metric <b>learning.</b> Popular algorithms are neighbourhood components analysis and large margin nearest neighbor. <b>Supervised</b> metric <b>learning</b> algorithms use the label information {{to learn a new}} metric or pseudo-metric.|$|R
25|$|Finally, {{diagnosis}} using {{flow cytometry}} {{data can be}} aided by <b>supervised</b> <b>learning</b> techniques, and discovery of new cell types of biological importance by high-throughput statistical methods, as part of pipelines incorporating all of the aforementioned methods.|$|E
25|$|When {{data are}} not labeled, <b>supervised</b> <b>learning</b> is not possible, and an {{unsupervised}} learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The clustering algorithm which provides an improvement to the support vector machines is called support vector clustering and is often used in industrial applications either when {{data are not}} labeled or when only some data are labeled as a preprocessing for a classification pass.|$|E
25|$|The group {{method of}} data {{handling}} (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov-Gabor polynomials that permit additions and multiplications. It used a deep feedforward multilayer perceptron with eight layers. It is a <b>supervised</b> <b>learning</b> network that grows layer by layer, where each layer is trained by regression analysis. Useless items are detected using a validation set, and pruned through regularization. The size {{and depth of}} the resulting network depends on the task.|$|E
50|$|The binary {{representation}} of preference information is called preference relation. For {{each pair of}} alternatives (instances or labels), a binary predicate can be <b>learned</b> by conventional <b>supervising</b> <b>learning</b> approach. Fürnkranz and Hüllermeier proposed this approach in label ranking problem. For object ranking, there is an early approach by Cohen et al.|$|R
5000|$|Classify: {{a set of}} <b>supervised</b> machine <b>learning</b> {{algorithms}} for classification ...|$|R
5000|$|Regression: {{a set of}} <b>supervised</b> machine <b>learning</b> {{algorithms}} for regression ...|$|R
25|$|Early on, deep {{learning}} was also applied to sequence learning with recurrent neural networks (RNNs) which are general computers and can run arbitrary programs to process arbitrary sequences of inputs. The depth of an RNN is unlimited and {{depends on the}} length of its input sequence. RNNs can be trained by gradient descent but suffer from the vanishing gradient problem. In 1992, it was shown that unsupervised pre-training of a stack of recurrent neural networks can speed up subsequent <b>supervised</b> <b>learning</b> of deep sequential problems.|$|E
25|$|In these studies, {{the goal}} usually is to {{diagnose}} a disease (or a sub-class of a disease) using variations {{in one or}} more cell populations. For example, one can use multidimensional clustering to identify a set of clusters, match them across all samples, and then use <b>supervised</b> <b>learning</b> to construct a classifier for prediction of the classes of interest (e.g., this approach can be used to improve the accuracy of the classification of specific lymphoma subtypes). Alternatively, all the cells from the entire cohort can be pooled into a single multidimensional space for clustering before classification. This approach is particularly suitable for datasets with a high amount of biological variation (in which cross-sample matching is challenging) but requires technical variations to be carefully controlled.|$|E
25|$|The {{simplest}} AI applications can {{be divided}} into two types: classifiers ("if shiny then diamond") and controllers ("if shiny then pick up"). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine a closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In <b>supervised</b> <b>learning,</b> each pattern belongs to a certain predefined class. A class {{can be seen as a}} decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.|$|E
50|$|<b>Supervised</b> feature <b>{{learning}}</b> {{is learning}} features from labeled data. Approaches include.|$|R
25|$|Such <b>supervised</b> deep <b>learning</b> {{methods were}} the first {{artificial}} pattern recognizers to achieve human-competitive performance on certain tasks.|$|R
30|$|Another {{technique}} used to adapt I/O access is <b>supervised</b> machine <b>learning,</b> proposed in [24], focusing on automated provisioning of Hadoop jobs.|$|R
25|$|The D-Wave 2X system hosted at NASA Ames Research Center {{has been}} {{recently}} {{used for the}} learning of a special class of restricted Boltzmann machines that {{can serve as a}} building block for deep learning architectures. Complementary work that appeared roughly simultaneously showed that quantum annealing can be used for <b>supervised</b> <b>learning</b> in classification tasks. The same device was later used to train a fully connected Boltzmann machine to generate, reconstruct, and classify down-scaled, low-resolution handwritten digits, among other synthetic datasets. In both cases, the models trained by quantum annealing had a similar or better performance in terms of quality. The ultimate question that drives this endeavour is whether there is quantum speedup in sampling applications. Experience with the use of quantum annealers for combinatorial optimization suggests the answer is not straightforward.|$|E
25|$|Unsupervised {{learning}} {{is the ability}} to find patterns in a stream of input. <b>Supervised</b> <b>learning</b> includes both classification and numerical regression. Classification is used to determine what category something belongs in, after seeing a number of examples of things from several categories. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent uses this sequence of rewards and punishments to form a strategy for operating in its problem space. These three types of learning can be analyzed in terms of decision theory, using concepts like utility. The mathematical analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory.|$|E
500|$|The {{majority}} of the justices held that the school principal was entitled to censor the articles. The majority opinion, written by Associate Justice Byron White, stated that officials had never intended the school paper {{to be a public}} forum, as underground publications were in past cases. White {{went on to say that}} educators do not infringe on First Amendment rights when exercising control over student speech in school-sponsored activities, [...] "so long as their actions are reasonably related to legitimate pedagogical concerns". The court established that the student publication could be regulated by school officials, and that they [...] "reserved the forum for its intended purpose, as a <b>supervised</b> <b>learning</b> experience for journalism students".|$|E
30|$|Classification Finally, {{train the}} <b>supervised</b> machine <b>learning</b> {{classifiers}} SMO, MNB, RF and LR with the different feature sub list for different domain.|$|R
50|$|In the 1990s, the {{statistical}} revolution swept through computational linguistics, and WSD became a paradigm problem {{on which to}} apply <b>supervised</b> machine <b>learning</b> techniques.|$|R
30|$|<b>Supervised</b> and {{unsupervised}} <b>learning</b> (explained below).|$|R
500|$|Kenji Doya {{has argued}} that the cerebellum's {{function}} is best understood not in terms of the behaviors it affects, but the neural computations it performs; the cerebellum consists {{of a large number of}} more or less independent modules, all with the same geometrically regular internal structure, and therefore all, it is presumed, performing the same computation. If the input and output connections of a module are with motor areas (as many are), then the module will be involved in motor behavior; but, if the connections are with areas involved in non-motor cognition, the module will show other types of behavioral correlates. Thus the cerebellum has been implicated in the regulation of many differing functional traits such as affection. emotion and behavior. The cerebellum, Doya proposes, is best understood as predictive action selection based on [...] "internal models" [...] of the environment or a device for <b>supervised</b> <b>learning,</b> in contrast to the basal ganglia, which perform reinforcement learning, and the cerebral cortex, which performs unsupervised learning.|$|E
2500|$|... {{inductive}} learning called <b>supervised</b> <b>learning.</b> [...] In supervised ...|$|E
2500|$|... {{previously}} {{seen by the}} algorithm. [...] The goal of the <b>supervised</b> <b>learning</b> ...|$|E
50|$|Attacks against (<b>supervised)</b> machine <b>learning</b> {{algorithms}} {{have been}} categorized along three primary axes: {{their influence on}} the classifier, the security violation they cause, and their specificity.|$|R
40|$|We {{propose a}} new method to {{retrieve}} similar face images from large face databases. The proposed method extracts {{a set of}} Haar-like features, and integrates these features with <b>supervised</b> manifold <b>learning.</b> Haar-like features are intensity-based features. The values of various Haar-like features comprise our rectangle feature vector (RFV) to describe faces. Compared with several popular unsupervised dimension reduction methods, RFV is more effective in retrieving similar faces. To further improve the performance, we combine RFV and a <b>supervised</b> manifold <b>learning</b> method and obtain satisfactory retrieval results. ...|$|R
40|$|The {{project is}} focused on methods of obtaining large number of images of human faces. Such {{database}} should then serve {{as a set of}} data for face detection and recognition by the means of <b>supervised</b> machine <b>learning.</b> The work deals with the basic principles of <b>supervised</b> machine <b>learning</b> and available data sets for this procedure. Project contains proposals of techniques and implementation of algorithms suitable for acquiring images from video and a concept of user interface for semi-automatic acceptation and annotation of located images...|$|R
2500|$|Support vector {{machines}} [...] <b>supervised</b> <b>learning</b> {{models with}} associated learning algorithms that analyze data and recognize patterns, used for classification and regression analysis.|$|E
2500|$|<b>Supervised</b> <b>{{learning}}</b> can {{be thought}} of as learning with a [...] "teacher", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.|$|E
2500|$|<b>Supervised</b> <b>learning</b> uses a set [...] (where [...] ) of example pairs [...] (for [...] ), that is , and {{the aim is}} to find a {{function}} [...] in the allowed class of functions that matches the examples. In other words, we wish to infer the mapping implied by the data; the cost function is related to the mismatch between our mapping and the data, and it implicitly contains prior knowledge about the problem domain.|$|E
50|$|Structured {{prediction}} or structured (output) {{learning is}} an umbrella term for <b>supervised</b> machine <b>learning</b> techniques that involves predicting structured objects, rather than scalar discrete or real values.|$|R
40|$|Abstract Information {{extraction}} from unstructured, ungrammatical data such as classified listings {{is difficult}} because traditional structural and grammatical extraction methods do not apply. Previous work has exploited reference sets to aid such extraction, but it did so using <b>supervised</b> machine <b>learning.</b> In this paper, we present an unsupervised approach that both selects the relevant reference set(s) automatically and then uses it for unsupervised extraction. We validate our approach with experimental results that show our unsupervised extraction is competitive with <b>supervised</b> machine <b>learning</b> approaches, including the previous supervised approach that exploits reference sets...|$|R
40|$|Abstract. Simulation Learning is a {{frequent}} practice to conduct nearreal, immersive and engaging training sessions. AI Planning and Scheduling systems are used to automatically create and <b>supervise</b> <b>learning</b> sessions; to this end, they need to manage {{a large amount of}} knowledge about the simulated situation, the learning objectives, the participants’ behaviour, etc. In this paper, we explain how Linked Data and Semantic Web technologies can help the creation and management of knowledge bases for Simulation Learning. We also present our experience in building such a knowledge base in the context of Crisis Management Training. ...|$|R
