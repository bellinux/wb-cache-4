261|371|Public
25|$|In 2013, AVST {{released}} Atom, {{a virtual}} 24/7 office manager application with location-based services, federated presence and a multi-lingual <b>speech</b> <b>interface</b> for the workplace. Also during 2013, AVST announced added support of hybrid UC cloud deployments in CX-E with more integration to Gmail and Microsoft Office 365, including unified messaging, calendaring, contacts and message waiting indicator.|$|E
50|$|The W3C's <b>Speech</b> <b>Interface</b> Framework also defines {{these other}} {{standards}} {{closely associated with}} VoiceXML.|$|E
5000|$|A Shared <b>Speech</b> <b>Interface</b> was {{designed}} by researchers at UCSD to facilitate conversations between deaf and non-signing, hearing people. [...] The work was presented at CSCW 2008.|$|E
5000|$|Subjective {{assessment}} of interfaces questionnaires (i.e. SASSI- Subjective Assessment of <b>Speech</b> System <b>Interfaces</b> [...] ) {{that can lead}} to design guidelines to <b>speech</b> <b>interfaces</b> ...|$|R
40|$|This paper {{presents}} some {{motivations for}} using highly personalised <b>speech</b> <b>interfaces.</b> In particular, {{the focus is}} on the requirements for adaptation in mobile environments. Furthermore, SesaME, a framework for personalised and adaptive <b>speech</b> <b>interfaces</b> is described. SesaME supports a multi-domain approach and eventbased, asynchronous dialogue management...|$|R
40|$|In {{this paper}} we {{argue for a}} {{paradigm}} shift towards a more dynamic construction of <b>speech</b> <b>interfaces</b> in the field rather than a detailed in depth tuning and engineering of <b>speech</b> <b>interfaces</b> in the lab, which in general is resulting in interfaces that are fixed in their functionality. We believe dynamic <b>speech</b> <b>interfaces</b> to be an attractive paradigm for user interfaces of mobile systems, as these encounter frequent changes in the environment, especially {{with respect to the}} emerging network technologies that are ready for plug and play and ad hoc networking...|$|R
5000|$|Also MPEG-4 Part 3 audio objects, such as Audio Lossless Coding (ALS), Scalable Lossless Coding (SLS), MP3, MPEG-1 Audio Layer II (MP2), MPEG-1 Audio Layer I (MP1), CELP, HVXC (speech), TwinVQ, Text To <b>Speech</b> <b>Interface</b> (TTSI) and Structured Audio Orchestra Language (SAOL) ...|$|E
50|$|<b>Speech</b> <b>interface</b> {{guideline}} is {{a guideline}} {{with the aim}} for guiding decisions and criteria regarding designing interfaces operated by human voice. <b>Speech</b> <b>interface</b> system has many advantages such as consistent service and saving cost. However, for users, listening is a difficult task. It can become impossible when too many options are provided at once. This may mean that a user cannot intuitively reach a decision. To avoid this problem, limit options and a few clear choices the developer should consider such difficulties are usually provided. The guideline suggests the solution which is able to satisfy the users (customers). The goal of the guideline is to make an automated transaction at least as attractive and efficient as interacting with an attendant.|$|E
50|$|After {{two years}} as {{a faculty member at}} Carnegie Mellon, Lee joined Apple Computer in 1990 as a {{principal}} research scientist. While at Apple (1990-1996), he headed R&D groups responsible for Apple Bandai Pippin, PlainTalk, Casper (<b>speech</b> <b>interface),</b> GalaTea (text to speech system) for Mac Computers.|$|E
40|$|Currently, <b>speech</b> <b>interfaces</b> {{are about}} to be {{integrated}} in consumer appliances and embedded systems and are expected to be used in mobile and ubiquitous computing environments. New usability and human computer interaction related problems may be introduced in these environments. We argue that a user-centered approach, with support for personalization, user modeling and context awareness should be used when designing <b>speech</b> <b>interfaces</b> for these new environments. Further, we are proposing a novel user-centered and application independent architecture for <b>speech</b> <b>interfaces.</b> Finally, we present SesaME, a generic dialogue manager built according to the suggested architecture...|$|R
40|$|This paper {{describes}} {{a novel approach}} to generating a general <b>speech</b> user <b>interface</b> to different applications by combining the existing <b>speech</b> user <b>interfaces</b> of the applications automatically. A general <b>speech</b> user <b>interface</b> enables the user to access different applications via speech simultaneously. One key issue of constructing such an interface is how different applications should be integrated. The approach represented in this paper integrates different applications into a general <b>speech</b> user <b>interface</b> by automatic merging their dialogue specifications into a unified dialogue specification, which provides necessary information for a multi-application supported dialogue system to enable the general <b>speech</b> user <b>interface</b> to these applications. In doing so, three issues in general <b>speech</b> user <b>interfaces</b> are addressed – transparent application switching, task sharing and information sharing. 1...|$|R
40|$|We {{have applied}} {{interaction}} acts, an abstract user-service interaction specification, to <b>speech</b> user <b>interfaces</b> to investigate {{how well it}} lends itself to {{a new type of}} user interface. We used interaction acts to generate VoiceXML-based <b>speech</b> user <b>interface,</b> and identified two main issues connected to the differences between graphical user <b>interfaces</b> and <b>speech</b> user <b>interfaces.</b> The first issue concerns the structure of the user <b>interface.</b> Generating <b>speech</b> user <b>interfaces</b> and GUIs from the same underlying structure easily results in a too hierarchical and difficult to use <b>speech</b> user <b>interface.</b> The second issue is user input. Interpreting spoken user input is fundamentally different from user input in GUIs. We have shown {{that it is possible to}} generate <b>speech</b> user <b>interfaces</b> based on. A small user study supports the results. ...|$|R
50|$|During the 1990s, {{electronic}} musical equipment {{became more}} dependent on large and sophisticated graphical displays. However, the QY10's display is a small LCD panel which accommodates only 16 characters of text. This made it feasible for the Kentucky LCD Interface Project to modify the QY10 with a <b>speech</b> <b>interface</b> as an assistive technology for the blind.|$|E
50|$|In 2013, AVST {{released}} Atom, {{a virtual}} 24/7 office manager application with location-based services, federated presence and a multi-lingual <b>speech</b> <b>interface</b> for the workplace. Also during 2013, AVST announced added support of hybrid UC cloud deployments in CX-E with more integration to Gmail and Microsoft Office 365, including unified messaging, calendaring, contacts and message waiting indicator.|$|E
50|$|Orange {{continued}} to offer Wildfire to its wireless subscribers until 2005, when {{it decided to}} terminate the product due to insufficiently broad use. Orange customers passionate about Wildfire, particularly those with disabilities who found the <b>speech</b> <b>interface</b> empowering, complained with sufficient force to cause a month's delay in shut-off, but Orange did indeed kill the service in July, 2005.|$|E
40|$|Drivers {{often use}} {{infotainment}} systems in motor vehicles, such as systems for navigation, music, and phones. However, operating visual-manual interfaces for these systems can distract drivers. <b>Speech</b> <b>interfaces</b> {{may be less}} distracting. To help designing easy-to-use <b>speech</b> <b>interfaces,</b> this paper identifies key <b>speech</b> <b>interfaces</b> (e. g., CHAT, Linguatronic, SYNC, Siri, and Google Voice), their features, and what was learned from evaluating them and other systems. Also included is information on key technical standards (e. g., ISO 9921, ITU P. 800) and relevant design guidelines. This paper also describes relevant design and evaluation methods (e. g., Wizard of Oz) {{and how to make}} driving studies replicable (e. g., by referencing SAE J 2944). Throughout the paper, there is discussion of linguistic terms (e. g., turn-taking) and principles (e. g., Grice’s Conversational Maxims) that provide a basis for describing user-device interactions and errors in evaluations...|$|R
40|$|Abstract. <b>Speech</b> <b>interfaces</b> {{are about}} to be {{integrated}} in consumer appliances and embedded systems and are expected to be used by mobile users in ubiquitous computing environments. This paper discusses some major usability and HCI related problems that may be introduced by this development. It is argued that a human-centered approach should be employed when designing and developing <b>speech</b> <b>interfaces</b> for mobile environments. Further, the Butler, a generic spoken dialogue system developed according to the human-centered approach is described. The Butler features a dynamic multi-domain approach. ...|$|R
40|$|Surface {{electromyography}} (EMG) {{can be used}} {{to record}} the activation potentials of articulatory muscles while a person speaks. This technique could enable silent <b>speech</b> <b>interfaces,</b> as EMG signals are generated even when people pantomime speech without producing sound. Having effective silent <b>speech</b> <b>interfaces</b> would enable a number of compelling applications, allowing people to communicate in areas where they would not want to be overheard or where the background noise is so prevalent that they could not be heard. In order to use EMG signals in <b>speech</b> <b>interfaces,</b> however, there must be a relatively accurate method to map the signals to speech. Up to this point, it appears that most attempts to use EMG signals for <b>speech</b> <b>interfaces</b> have focused on Automatic Speech Recognition (ASR) based on features derived from EMG signals. Following the lead of other researchers who worked with Electro-Magnetic Articulograph (EMA) data and Non-Audible Murmur (NAM) speech, we explore the alternative idea of using Voice Transformation (VT) techniques to synthesize speech from EMG signals. With speech output, both ASR systems and human listeners can directly use EMG-based systems. We report the results of our preliminary studies, noting the difficulties we encountered and suggesting areas for future work. Index Terms: electromyography, silent speech, voice transformation, speech synthesi...|$|R
50|$|Silent <b>speech</b> <b>interface</b> is {{a device}} that allows speech {{communication}} without using the sound made when people vocalize their speech sounds. As such it {{is a type of}} electronic lip reading. It works by the computer identifying the phonemes that an individual pronounces from nonauditory sources of information about their speech movements. These are then used to recreate the speech using speech synthesis.|$|E
50|$|Silent <b>speech</b> <b>interface</b> {{systems have}} been created using {{ultrasound}} and optical camera input of tongue and lip movements. Electromagnetic devices are another technique for tracking tongue and lip movements. The detection of speech movements by electromyography of speech articulatormuscles and the larynx is another technique. Another source of information is the vocal tract resonance signals that get transmitted through bone conduction called non-audible murmurs. They have also been created as a brain-computer interface using brain activity in the motor cortex obtained from intracortical microelectrodes.|$|E
50|$|For example, a test {{participant}} {{may think}} {{he or she}} is communicating with a computer using a <b>speech</b> <b>interface,</b> when the participant’s words are actually being secretly entered into the computer by a person in another room (the “wizard”) and processed as a text stream, rather than as an audio stream. The missing system functionality that the wizard provides may be implemented in later versions of the system (or may even be speculative capabilities that current-day systems do not have), but its precise details are generally considered irrelevant to the study. In testing situations, the goal of such experiments may be to observe the use and effectiveness of a proposed user interface by the test participants, rather than to measure the quality of an entire system.|$|E
40|$|While the {{technology}} underlying <b>speech</b> <b>interfaces</b> has improved in recent years, {{our understanding of}} the human side of speech interactions remains limited. This paper provides new insight on one important human aspect of speech interactions: the sense of agency - defined as the experience of controlling one 2 ̆ 7 s own actions and their outcomes. Two experiments are described. In each case a voice command is compared with keyboard input. Agency is measured using an implicit metric: intentional binding. In both experiments we find that participants 2 ̆ 7 sense of agency is significantly reduced for voice commands as compared to keyboard input. This finding presents a fundamental challenge for the design of effective <b>speech</b> <b>interfaces.</b> We reflect on this finding and, based on current theory in HCI and cognitive neuroscience, offer possible explanations for the reduced sense of agency observed in <b>speech</b> <b>interfaces.</b> ...|$|R
40|$|Hyperspeech is a speech-only {{hypermedia}} {{application that}} explores issues of <b>speech</b> user <b>interfaces,</b> navigation, and system architecture in a purely audio environment without a visual display. The system uses speech recognition input and synthetic speech feedback {{to aid in}} navigating through a database of digitally recorded speech segments. KEYWORDS <b>Speech</b> user <b>interfaces,</b> <b>speech</b> applications, hypermedia...|$|R
40|$|Computer-based {{interviewing}} {{systems could}} use models of respondent disfluency behaviors to predict {{a need for}} clarification of terms in survey questions. We compare simulated <b>speech</b> <b>interfaces</b> that use two such models - a generic model and a stereotyped model that distinguishes between the speech of younger and older speakers - to several non-modeling <b>speech</b> <b>interfaces</b> in a task where respondents provided answers to survey questions from fictional scenarios. Our modeling procedure found that {{the best predictor of}} conceptual misalignment was a critical Goldilocks range for response latency, outside of which responses {{are more likely to be}} conceptually misaligned. Different Goldilocks ranges are effective for younger and older speakers...|$|R
5000|$|Emacspeak {{is a free}} {{computer}} application, a <b>speech</b> <b>interface,</b> and {{an audio}} desktop (as opposed to a screen reader). It employs Emacs (which is written in C), Emacs Lisp, and Tcl. Developed principally by T. V. Raman (himself blind since childhood, and {{who has worked on}} voice software with Adobe Software and later IBM), it was first released in April 1995. It is portable to all POSIX-compatible OSs. It is tightly integrated with Emacs, allowing it to render intelligible and useful content rather than parsing the graphics (hence it is sometimes referred to not as a separate program, but a subsystem of Emacs proper); its default voice synthesizer (as of 2002, IBM's ViaVoice Text-to-Speech (TTS)) can be replaced with other software synthesizers when a server module is installed. Emacspeak {{is one of the most}} popular speech interfaces for Linux, bundled with most major distributions. The following article is written on 20th anniversary of Emacspeak ...|$|E
50|$|The 1990s saw {{the first}} {{introduction}} of commercially successful speech recognition technologies. Two of the earliest products were Dragon Dictate, a consumer product released in 1990 and originally priced at $9,000, and a recognizer from Kurzweil Applied Intelligence released in 1987. AT&T deployed the Voice Recognition Call Processing service in 1992 to route telephone calls without the use of a human operator. The technology was developed by Lawrence Rabiner and others at Bell Labs.By this point, the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary. Raj Reddy's former student, Xuedong Huang, developed the Sphinx-II system at CMU. The Sphinx-II system was the first to do speaker-independent, large vocabulary, continuous speech recognition and it had the best performance in DARPA's 1992 evaluation. Handling continuous speech with a large vocabulary was a major milestone in the history of speech recognition. Huang went on to found the speech recognition group at Microsoft in 1993. Raj Reddy's student Kai-Fu Lee joined Apple where, in 1992, he helped develop a <b>speech</b> <b>interface</b> prototype for the Apple computer known as Casper.|$|E
40|$|The {{purpose of}} my {{project is to}} build a {{graphical}} user interface to control a light system in a wizard of Oz study that will aid {{in the development of a}} new <b>speech</b> <b>interface</b> for the light control system in a lab. The <b>speech</b> <b>interface</b> will serve the role of a test application for conducting research in speech interfaces. The development of a <b>speech</b> <b>interface</b> system requires paying very close attention to the unique qualities of the interaction between humans and machines. Designing a <b>speech</b> <b>interface</b> system is hard because designers must model as closely as possible the dialogue between humans. But the fact that computers are not humans suggest that we cannot use human to human data as a viable source of design data for the creation of a new <b>speech</b> <b>interface</b> system. Instead, a wizard of Oz study is used, where a human simulates an implemented system, allowing designers to study the language that is used in a unique dialogue situation between users and a computer. The goal of the wizard of Oz study is to collect data to inform the design of a <b>speech</b> <b>interface</b> for the lighting in the lab. The graphical user interface (GUI) for the light system will allow the wizard (the human simulating the system) in the Wizard of Oz study to respond to the user's requests more quickly, allowing him to more closely simulate an implemented <b>speech</b> <b>interface</b> to the lighting in the lab. This will allow us, to collect more realistic data for the future <b>speech</b> <b>interface...</b>|$|E
40|$|Presented at the Ergonomics Conference, Brighton, May 1989 This paper {{considers}} the particular nature of <b>speech</b> <b>interfaces</b> {{and some of}} the problems inherent in <b>speech</b> as an <b>interface</b> medium, and it attempts to overcome these problems by improving the interface design. The need for an adaptable interface is identified and the use of models simulating the application and the user is described. Reference is made to a particular project - the Hatfield Polytechnic Intelligent <b>Speech</b> Driven <b>Interface</b> Project (ISDIP) - and the attempts being made to implement such an interface {{in the context of an}} intelligent knowledge base...|$|R
40|$|Although {{speech is}} the most natural form of {{communication}} between humans, most people find using speech to communicate with machines anything but natural. Drawing from psychology, human-computer interaction, linguistics, and communication theory, Practical <b>Speech</b> User <b>Interface</b> Design provides a comprehensive yet concise survey of practical <b>speech</b> user <b>interface</b> (SUI) design. It offers practice-based and research-based guidance on how to design effective, efficient, and pleasant speech applications that people can really use. Focusing {{on the design of}} <b>speech</b> user <b>interfaces</b> for IVR applicatio...|$|R
40|$|Typically, {{when using}} {{practice}} management systems (PMS), dentists perform data entry by utilizing an assistant as a transcriptionist. This prevents dentists from interacting {{directly with the}} PMSs. <b>Speech</b> recognition <b>interfaces</b> can provide {{the solution to this}} problem. Existing <b>speech</b> <b>interfaces</b> of PMSs are cumbersome and poorly designed. In dentistry, there is a desire and need for a usable natural language interface for clinical data entry. Objectives. (1) evaluate the efficiency, effectiveness, and user satisfaction of the <b>speech</b> <b>interfaces</b> of four dental PMSs, (2) develop and evaluate a speech-to-chart prototype for charting naturally spoken dental exams. Methods. We evaluated the <b>speech</b> <b>interfaces</b> of four leading PMSs. We manually reviewed the capabilities of each system and then had 18 dental students chart 18 findings via speech in each of the systems. We measured time, errors, and user satisfaction. Next, we developed and evaluated a speech-to-chart prototype which contained the following components: speech recognizer; post-processor for error correction; NLP application (ONYX) and; graphical chart generator. We evaluated the accuracy of the speech recognizer and the post-processor. We then performed a summative evaluation on the entire system. Our prototype charted 12 hard tissue exams. We compared the charted exams to reference standard exams charted by two dentists. Results. Of the four systems, only two allowed both hard tissue and periodontal charting via <b>speech.</b> All <b>interfaces</b> required using specific commands directly comparable to using a mouse. The average time to chart the nine hard tissue findings was 2 : 48 and the nine periodontal findings was 2 : 06. There was an average of 7. 5 errors per exam. We created a speech-to-chart prototype that supports natural dictation with no structured commands. On manually transcribed exams, the system performed with an average 80 % accuracy. The average time to chart a single hard tissue finding with the prototype was 7. 3 seconds. An improved discourse processor will greatly enhance the prototype's accuracy. Conclusions. The <b>speech</b> <b>interfaces</b> of existing PMSs are cumbersome, require using specific speech commands, and make several errors per exam. We successfully created a speech-to-chart prototype that charts hard tissue findings from naturally spoken dental exams...|$|R
40|$|This thesis {{presents}} {{the design and}} implementation of a <b>speech</b> <b>interface</b> for bedside data entry in an intensive care unit. A <b>speech</b> <b>interface</b> is a system comprised of a speech recognition system, for speech input, and a speech generation or speech synthesis system, for speech output. These interfaces allow the operation of a computer system using voice commands providing the user with feedback via speech output. Such systems permit users to perform "hands-free" and "eyes-free" data entry or system operation in circumstances where the use of traditional manual input devices, such as a keyboard, cannot be used. This thesis begins with a literature sampling of contemporary computerized medical information systems and <b>speech</b> <b>interface</b> systems followed by {{a description of the}} hardware and software architecture of the <b>speech</b> <b>interface</b> implemented. Test results are then presented and discussed followed by an outline of future extensions for the system...|$|E
40|$|The paper {{describes}} {{an approach to}} improve robustness of the SCORPIO <b>speech</b> <b>interface</b> via increasing speech recognition accuracy in street noise conditions. The SCORPIO <b>speech</b> <b>interface</b> serves forcontrolling secondary functions of the service robot SCORPIO, which is developed with aim to serve for booby-trap disposal. The main workspace of the robot is a noisy outdoor environment. Therefore street-noise robustness of the <b>speech</b> <b>interface</b> is an importantissue. The speech enhancement based on spectral subtraction method was used to achieve this goal. Experiments with simulated outdoor environment were done using recordings with different levels of signal-tonoise ratio. Comparison of basic, modified and iterative algorithm showed significant improvement {{especially in the case}} of iterative spectral subtraction...|$|E
40|$|<b>Speech</b> <b>interface</b> technology, which {{includes}} automatic speech recognition, synthetic speech, and natural language processing, {{is beginning to}} {{have a significant impact on}} business and personal computer use. Today, powerful and inexpensive microprocessors and improved algorithms are driving commercial applications in computer command, consumer, data entry, speech-to-text, telephone, and voice verification. Robust speaker-independent recognition systems for command and navigation in personal computers are now available; telephone-based transaction and database inquiry systems using both speech synthesis and recognition are coming into use. Large-vocabulary <b>speech</b> <b>interface</b> systems for document creation and read-aloud proofing are expanding beyond niche markets. Today's applications represent a small preview of a rich future for <b>speech</b> <b>interface</b> technology that will eventually replace keyboards with microphones and loud-speakers to give easy accessibility to increasingly intelligent machines...|$|E
5000|$|His {{research}} interests are in speech recognition, <b>speech</b> synthesis, <b>speech</b> <b>interfaces</b> and language in general. According to Hauptmann (2008) [...] "Over the years his {{research interests}} have {{led him to}} pursue and combine several different areas of research: man-machine communication, natural language processing and speech understanding".|$|R
40|$|Submission (Technical Paper) Title : A Step Towards <b>Speech</b> <b>Interfaces</b> - A Time-Sliced Neural Network for Continuous Speech Recognition Authors : Ingrid Kirschning and Hideto Tomabechi. Address : Dept. of Information Science and Intelligent Systems Faculty of Engineering Tokushima University 2 - 1 Minami Josanjima-Cho, Tokushima Shi 770, JAPAN. Tel : (+ 81) 886 - 56 - 7487 Fax : (+ 81) 886 - 55 - 4424 E-Mail : ingrid@j-aoe. is. tokushima-u. ac. jp Keywords : <b>Speech</b> <b>Interfaces,</b> <b>Speech</b> Recognition, Recurrent Neural Networks. Preferred {{presentation}} format : Verbal. A B S T R A C T In {{the area}} of natural language <b>interfaces</b> the <b>speech</b> recognition module is an important component, when considering spoken language. The objective of our work is to design and develop a system capable of continuous real-time speech recognition, small enough to run even on PC's. Neural networks represent one of the approaches taken for Automatic Speech Recognition (ASR) because {{of their ability to}} generalize and classify differ [...] ...|$|R
40|$|Speech is {{becoming}} increasingly popular as an interface modality, especially in hands- and eyes-busy situations where {{the use of a}} keyboard or mouse is difficult. However, despite the fact that many have hailed speech as being inherently usable (since everyone already knows how to talk), most users of speech input are left feeling disappointed by the quality of the interaction. Clearly, there is much work to be done on the design of usable spoken interfaces. We believe that there are two major problems in the design of <b>speech</b> <b>interfaces,</b> namely, (a) the people who are currently working on the design of <b>speech</b> <b>interfaces</b> are, for the most part, not interface designers and therefore do not have as much experience with usability issues as we in the CHI community do, and (b) <b>speech,</b> as an <b>interface</b> modality, has vastly different properties than other modalities, and therefore requires different usability measures. Comment: Position paper for CHI 2000 Workshop on Natural-Language Interactio...|$|R
