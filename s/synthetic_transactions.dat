15|20|Public
50|$|NPM uses <b>synthetic</b> <b>transactions</b> to {{test for}} {{reachability}} and calculate network performance metrics across the network. Tests are performed, using either TCP or ICMP and users {{have the option of}} choosing between these protocols. Users must evaluate their environments and weigh {{the pros and cons of}} the protocols. The following is a summary of the differences.|$|E
50|$|Network Performance Monitor (NPM) is {{a network}} {{monitoring}} solution from the Operations Management Suite, that monitors networks. NPM monitors {{the availability of}} connectivity and quality of connectivity between multiple locations within and across campuses, private and public clouds. The solution uses <b>synthetic</b> <b>transactions</b> to test for reachability {{and can be used}} on any IP network, irrespective of the make and model of network routers and switches deployed.|$|E
50|$|System Management mainly {{refers to}} the {{availability}} and capability monitoring of IT infrastructure. Availability monitoring refers to monitoring the status of IT infrastructure components such as servers, routers, networks, etc. This usually entails pinging or polling the component and waiting to receive a response. Capability monitoring usually refers to <b>synthetic</b> <b>transactions</b> where user activity is mimicked by a special software program, and the responses received are checked for correctness.|$|E
40|$|This paper {{analyzes}} the loss allocation to first, second, and third loss positions in European collateralized debt obligation transactions. The {{quality of the}} underlying asset pool plays a predominant role for the loss allocation. A lower asset pool quality induces the originator to take a higher first loss position, but, in a <b>synthetic</b> <b>transaction,</b> a smaller third loss position. The share of expected default losses, borne by the first loss position, is largely independent of asset pool quality but lower in securitizations of corporate loans than in those of corporate bonds. Originators with a good rating and low Tobin's Q prefer <b>synthetic</b> <b>transaction...</b>|$|R
50|$|During {{the period}} from 2010 through 2015, Unify Square offered 4 {{different}} stand-alone software applications targeting Microsoft Lync. PowerProv offered end-to-end provisioning for Lync and Skype for Business. When a new employee joins or leaves, or requires a change within a corporate environment, the PowerProv software automatically creates accounts and sets security settings. These functions reduce manually performed tasks as well as errors. PowerMon was a <b>synthetic</b> <b>transaction</b> monitoring application, that performed fault detection in a business' unified communications systems and automatically sent alerts when the system wasn’t performing properly. PowerView software provided analytics in usage, conferencing and adoption analytics, while PowerSat facilitated end-user satisfaction surveys.|$|R
40|$|Mobile {{money is}} a service for {{performing}} financial transactions using a mobile phone. By law {{it has to have}} protection against money laundering and other types of fraud. Research into fraud detection methods is not as advanced as in other similar fields. However, getting access to real world data is difficult, due to the sensitive nature of financial transactions, and this makes research into detection methods difficult. Thus, we propose an approach based on a Multi-Agent Based Simulation (MABS) for the generation of <b>synthetic</b> <b>transaction</b> data. We present the generation of synthetic data logs of transactions and the use of such a data set for the study of different detection scenarios using machine learning...|$|R
5000|$|Reporting and analytics: {{functionality}} {{to monitor}} API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, {{volume of data}} transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyse historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create <b>synthetic</b> <b>transactions</b> {{that can be used}} to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs.|$|E
50|$|In <b>synthetic</b> <b>transactions,</b> {{credit risk}} of the Reference Portfolio is {{transferred}} to the SPV via credit default swaps. For each name in the portfolio the SPV enters into a credit default swap where the SPV sells credit protection to the bank {{in return for a}} periodically paid premium. The cash raised from the sale of the various classes of notes, i.e.; Class A, B, C and D in the above example, is placed in collateral securities. Typically these are AAA rated notes issued by supranationals, governments, governmental organizations, or covered bonds (Pfandbrief). These are low-risk instruments with a return slightly below the interbank market yield. If there is a default in the portfolio, the credit default swap for that entity is triggered and the bank demands the loss suffered for that entity from the SPV. For example if the bank has entered into a credit default swap for $10,000,000 on Company A and this company is bankrupt the bank will demand $10,000,000 less the recovery amount from the SPV. The recovery amount is the secondary market price of $10,000,000 of bonds of Company A after bankruptcy. Typically recovery amount is assumed to be 40% but this number changes depending on the credit cycle, industry type, and depending on the company in question. Hence, if recovery amount is $4,000,000 (working on 40% recovery assumption), the bank receives $6,000,000 from the SPV. In order to pay this money, the SPV has to liquidate some collateral securities to pay the bank. Having lost some assets, the SPV has to reduce some liabilities as well and it does so by reducing the notional of the equity notes. Hence, after the first default in the portfolio, the equity notes, i.e.; Class D notes in the above example are reduced to $24,000,000 from $30,000,000.|$|E
40|$|This paper {{analyses}} {{the loss}} allocation to First, Second and Third Loss Positions in European collateralized debt obligation transactions. The {{quality of the}} underlying asset pool plays a predominant role for the loss allocation. A lower asset pool quality induces the originator to take a higher First Loss Position, but, in a synthetic transaction, a smaller Third Loss Position. The share of expected default losses, borne by the First Loss Position, is largely independent of asset pool quality, but lower in securitizations of corporate loans than in those of corporate bonds. Originators with a good rating and low Tobin´s Q prefer <b>synthetic</b> <b>transactions.</b> Securitization, collateralized debt obligations, asset pool quality, First Loss Position, <b>synthetic</b> <b>transactions.</b> ...|$|E
5000|$|BTM is {{sometimes}} categorized {{as a form}} of application performance management (APM) or monitoring. It works alongside other IT monitoring systems including End-User Experience Monitoring, <b>Synthetic</b> <b>Transaction</b> Monitoring, Deep-Dive Monitoring and Business Activity Monitoring (BAM) solutions. According to Gartner, BTM and deep dive monitoring are [...] "fundamentally distinct and their associated processes are typically carried out by different communities with different skill sets. The buyer should still implement multiple products, even if it means greater architectural complexity and apparent functional overlap." [...] As the technologies mature APM is now being viewed as a complete solution set. Maximum productivity can be achieved more efficiently through event correlation, system automation and predictive analysis which is now all part of APM.|$|R
40|$|Money flow {{models are}} {{essential}} tools to understand different economical phenomena, like saving propensities and wealth distributions. In {{spite of their}} importance, {{most of them are}} based on <b>synthetic</b> <b>transaction</b> networks with simple topologies, e. g. random or scale-free ones, as the characterisation of real networks is made difficult by the confidentiality and sensitivity of money transaction data. Here we present an analysis of the topology created by real credit card transactions from one of the biggest world banks, and show how different distributions, e. g. number of transactions per card or amount, have nontrivial characteristics. We further describe a stochastic model to create transactions data sets, feeding from the obtained distributions, which will allow researchers to create more realistic money flow models. Comment: Submitted to Physica...|$|R
40|$|This Article {{looks to}} another {{paradigm}} to motivate an answer [...] the exotic financial instruments created on Wall Street. Over {{the last few}} decades, a market has developed in assorted sophisticated financial instruments created by unbundling and repackaging various components of traditional securities. Financial engineering, for example, allows the creation of “synthetics. ” One court has described “synthetic” securities as follows: “A <b>synthetic</b> <b>transaction</b> is typically a contractual agreement between two counterparties, usually an investor and a bank, that seeks to economically replicate the ownership and physical trading of shares and options. ” This Article similarly formulates synthetic rights that, when coupled with the rights possessed by the holder of an illiquid asset, produce {{the equivalent of a}} liquid asset...|$|R
40|$|The {{past three}} decades have seen the {{emergence}} in the market of {{many different types of}} “derivative instruments”, ranging from futures, forwards, options, and swaps 1 to some other hybrid instruments 2 or <b>synthetic</b> <b>transactions</b> 3. Along with insurance, derivative instruments help market participants not only to hedge various types of risks but also to engage in market speculation. A derivative transaction could serve the purpose of avoiding large losses (i. e. hedging) as well as earning a windfall (i. e. speculation). As such, one question arises: Is there any difference between gambling and derivative trading?</p...|$|E
40|$|In this paper, {{we propose}} a novel, exact border-based {{approach}} that provides an optimal {{solution for the}} hiding of sensitive frequent item sets by 1) minimally extending the original database by a synthetically generated database part-the database extension, 2) formulating {{the creation of the}} database extension as a constraint satisfaction problem, 3) mapping the constraint satisfaction problem to an equivalent binary integer programming problem, 4) exploiting underutilized <b>synthetic</b> <b>transactions</b> to proportionally increase the support of nonsensitive item sets, 5) minimally relaxing the constraint satisfaction problem to provide an approximate solution close to the optimal one when an ideal solution does not exist, and 6) using a partitioning in the universe of the items to increase the efficiency of the proposed hiding algorithm. Extending the original database for sensitive item set hiding is proved to provide optimal solutions to an extended set of hiding problems compared to previous approaches and to provide solutions of higher quality. Moreover, the application of binary integer programming enables the simultaneous hiding of the sensitive item sets and thus allows for the identification of globally optimal solutions...|$|E
40|$|Problem {{determination}} {{is one of}} {{the most}} important tasks in managing distributed systems. Probing (both at the transaction and network levels) has been widely used for assessing compliance with Service Level Agreements and locating problems in distributed systems. However a probing scheme which uses a fixed set of regularly scheduled probes can be expensive in terms of the number of <b>synthetic</b> <b>transactions</b> needed, especially for the task of problem determination. This paper introduces an active probing scheme to reduce the number of probes needed. Our key idea is to divide the problem determination task into two steps. We first use a relatively small number of fixed, regularly scheduled, probes for detecting that a problem has occurred. In the second phase, once occurrence of a problem is detected, additional probes are issued on-the-fly to acquire additional information until the problem is localized. We develop algorithms for selecting an optimal set of probes for problem detection and choosing which probes to send next based on what is currently known. We demonstrate through both analysis and simulation that the active probing scheme can greatly reduce the number of probes and the time needed for localizing the problem when compared with a non-active probing scheme. ...|$|E
40|$|The paper {{presents}} {{an analysis of}} the trade-offs of participants of different type between payment delay and liquidity requirement on the basis of synthetically generated data. The generation of the <b>synthetic</b> <b>transaction</b> data set for a simple RTGS system is described and calibrated using real world parameters. The payment system is simulated for various liquidity levels and it is shown that participants of different size in terms of transaction volume and value will have different optimal liquidity requirements, as the payment delays they face for each liquidity level will be different. This is shown using indifference curves between payment delay and liquidity requirements. JEL Classification: C 15, C 5, E 58, L 14, L 41, L 51 competition, data generation, oversight, Payment system, Simulation...|$|R
40|$|Application {{response}} times and availability are {{key elements of}} user satisfaction with enterprise IT services. JCI does not currently have a centralized performance monitoring tool or consolidated performance reporting methodology. End to end application performance is not consistently measured across the enterprise. Two different commercial tools with distinct technical approaches were considered for measuring both real-user and synthetically generated transactions. Ease of use, scalability, and cost considerations led to the selection of an agent based toolset from Symantec Corporation. An experimental testbed was created to demonstrate {{the capabilities of the}} <b>synthetic</b> <b>transaction</b> tool and to develop useful configuration methodologies. Specific architectural and monitoring details were identified for JCI’s Best Business Practices (BBP) Reports application. General implementation guidelines were developed. All future monitored applications at JCI will begin with the same consistent approach to performance monitoring and reporting, allowing for valid performance comparisons across time, application, and location...|$|R
40|$|It is an {{important}} task in data mining to maintain discovered frequent itemsets for association rule mining. Because most time-consuming operation for mining association rules {{is to find the}} frequent itemsets from the transaction database. And the database is always updated. However, the algorithms proposed so far for the maintenance of discovered frequent itemsets can only perform with a minimum support threshold which is {{the same as that of}} previous mining. If the new result derived with such minimum support is unsatisfactory to a user, the maintaining process may fail. In this paper we propose a new algorithm to maintain discovered frequent itemsets. Our algorithm allows users to adjust the minimum support of maintaining process. And it can be performed repeatedly with a different minimum support until the satisfying results are obtained. We prove the efficiency of our algorithm by experiments with several <b>synthetic</b> <b>transaction</b> databases...|$|R
40|$|The {{reorganisation}} {{process that}} took place in the electricity sector in Argentina in the nineties implied an increasing degree of decentralisation and privatisation in the market. Pari passu, there came up a growing need to improve the conditions of optimisation through time. More flexibility was introduced by new clauses in forward contracts and by gradual changes in the admission of participants in the wholesale market. However, these modifications were not sufficient to satisfy a demand for efficient hedging from agents who trade in a market with large size transaction. The newer risk-sharing clauses included in the forward contract were still rigid. This paper aims to examine the conditions of the electricity market in the mid- 1990 's for the establishing of an option and futures market on electricity 1. Then, the evolution of spot prices in the following years was analysed to see if the conclusions arrived at before, were still holding. The analysis was mainly based on the spot price evolution and in the evaluation of equivalent contracts to some already existing. The equivalent contracts are <b>synthetic</b> <b>transactions</b> consisting of individual option contracts, both call and put, which reproduce the original contract and reflect explicitly the cost of hedging...|$|E
40|$|The {{strong growth}} in collateralized debt {{obligation}} transactions {{raises the question}} how these transactions are designed. The originator designs the transaction so as to maximize her benefit subject to requirements imposed by investors and rating agencies. An important issue in these transactions is the information asymmetry between the originator and the investors. First Loss Positions {{are the most important}} instrument to mitigate conflicts due to information asymmetry. We analyse the optimal size of the First Loss Position in a model and the actual size in a set of European collateralized debt obligation transactions. We find that the asset pool quality, measured by the weighted average default probability and the diversity score of the pool, plays a predominant role for the transaction design. Characteristics of the originator play a small role. A lower asset pool quality induces the originator to take a higher First Loss Position and, in a synthetic transaction, a smaller Third Loss Position. The First Loss Position bears on average 86 % of the expected default losses, independent of the asset pool quality. This loss share and the asset pool quality strongly affect the rating and the credit spread of the lowest rated tranche. Securitization, collateralized debt obligations, asset pool quality, First Loss Position, <b>synthetic</b> <b>transactions,</b> tranching...|$|E
40|$|The paper {{provides}} {{empirical evidence}} on the pricing of synthetic credit risk securitisation. Securitisation is advantageous both for originating banks, who incur savings on bank capital costs and for investors, who acquire a stake in leveraged credit portfolio risk, an exposure type rarely accessible otherwise. Securitisation is cost-efficiently implemented by structured credit derivatives, issued securities are therefore referred to as synthetic collateralised debt obligations (CDOs). We perform arbitrage-free pricing of synthetic CDOs referring to investment grade European corporate debt, such that a comparison with observed transactions can be drawn. CDO payout rules are formalised, taking both direct risk transfer structures and <b>synthetic</b> <b>transactions</b> using SPVs for excess spread trapping into account. Underlying losses are captured by a reduced-form affine multi-factor model, exploiting the strong observed comovement of corporate spreads for systematic factor estimation. Estimation is performed by Kalman filter-based QML, applied to implied spread structures of recent European corporate bond issues to which originating banks are exposed. Valuation results are obtained {{for a variety of}} structural variants. A comparison with transactions accommodating our modelling approach with regard to payout rules and reference debt quality supports the hypothesis that investors have a strong preference for CDO investments: Observed note issuance spreads provide no adequate compensation for the leveraged portfolio risk exposure. Keywords: Credit risk securitisation, Multi-name credit derivatives, Corporate bonds, Reduced-form model, Affine defaultable term structure model, Kalman filter...|$|E
40|$|Preliminary draft The {{strong growth}} in collateralized debt {{obligation}} transactions {{raises the question}} how these transactions are designed. The originator designs the transaction so as to maximize her benefit subject to requirements imposed by investors and rating agencies. We analyse a set of European transactions and find that the asset pool quality, measured by the weighted average default probability and the diversity score of the pool, plays a predominant role for the transaction design. Characteristics of the originator play a small role. A lower asset pool quality induces the originator to take a higher First Loss Position and renders a <b>synthetic</b> <b>transaction</b> less attractive. In these transactions the senior, least information sensitive tranche is not sold. The size of this tranche tends to decline with asset pool quality. Both, the weighted average default probability and the diversity score of the pool appear to positively affect the number of tranches and the credit spread of the lowest rated tranche...|$|R
2500|$|... — Under a <b>synthetic</b> {{secondary}} <b>transaction,</b> secondary investors acquire {{an interest}} in a new limited partnership that is formed specifically to hold a portfolio of direct investments. [...] Typically {{the manager of the}} new fund had historically managed the assets as a captive portfolio. The most notable example of this type of transaction is the spinout of MidOcean Partners from Deutsche Bank in 2003.|$|R
5000|$|The {{enterprise}} version {{builds on}} the core version by providing commercial support and additional features, such as <b>synthetic</b> web <b>transactions</b> and global dashboards. [...] "In the enterprise edition," [...] writes Sean Michael Kerner, [...] "Zenoss is adding something it calls end-user experience monitoring which is intended to more accurately simulate end-user application activity." [...] Kerner continues, [...] "Enterprise users also get certified application monitors specifically geared for Microsoft SQL and Exchange." ...|$|R
40|$|The article {{analyzes}} one of {{the approaches}} {{to the development of}} writing skills of foreign students studying in the Russian university program of “Bachelor” on the target language – Russian. The relevance of the issue discussed in the paper is primarily determined by the significance of the written form of communication, which is directly connected with all spheres of human activity. In addition, the methodical feasibility of improving the communicative abilities of the letter {{stems from the fact that}} the written language, requiring form the producer greater awareness actions to make a high level of analytical and <b>synthetic</b> <b>transactions</b> that positively affect the quality of the speech orally and in terms of its grammatical formation, and in terms of its logical-structuring. The current practice of training of foreigners in writing in Russian is reduced mainly to the production of the text as a result of compression source. Information processing in this case will be focused on the definition of communicative structure of the text, and in this base allocate the most important data and their subsequent recording. The authors’ propose is to teach writing on the basis of information deployment contained in such sources as announcement, questionnaire, poll. In this situation, on the basis of knowledge of the rules of analysis and synthesis of linguistic units there is the construction of a variety of grammatically correct and semantically marked utterances, the expediency-organized differently in the text...|$|E
40|$|Abstract — System logs are an {{important}} tool in studying the conditions (e. g., environment misconfigurations, resource status, erroneous user input) that cause failures. However, production system logs are complex, verbose, and lack structural stability over time. These traits make them hard to use, and make solutions that rely on them susceptible to high maintenance costs. Additionally, logs record failures after they occur: by the time logs are investigated, users have already experienced the failures ’ consequences. To detect the environment conditions that are correlated with failures without dealing with the complexities associated with processing production logs, and to prevent failure-causing conditions from occurring before the system goes live, this research suggests a three step methodology: i) using <b>synthetic</b> <b>transactions,</b> i. e., simplified workloads, in pre-production environments that emulate user behavior, ii) recording the result of executing these transactions in logs that are compact, simple to analyze, stable over time, and specifically tailored to the fault metrics of interest, and iii) mining these specialized logs to understand the conditions that correlate to failures. This allows system administrators to configure the system to prevent these conditions from happening. We {{evaluate the effectiveness of}} this approach by replicating the behavior of a service used in production at Microsoft, and testing the ability to predict failures using a synthetic workload on a 650 million events production trace. The synthetic prediction system is able to predict 91 % of real production failures using 50 -fold fewer transactions and logs that are 10, 000 -fold more compact than their production counterparts. Keywords-Failure prediction; failure avoidance; system logs; synthetic transactions; data analysis; data mining. I...|$|E
30|$|Fast Hiding Sensitive Association Rules (FHSAR) {{algorithm}} is introduced by Weng et al. (2008). This secured the SAR with fewer side effects, where a strategy is established to avoid hidden failures. Besides, two heuristic techniques are developed {{to improve the}} efficiency of the system to solve the problems. The heuristic function is further utilized to determine the earlier weight for each particular transaction so that the order of modified transactions can be decided efficiently. Consequently, the connection between the sensitive association rules and each transaction in the original database are analyzed by successfully choosing the suitable item for modification. The efficient sanitization of sensitive information for updated database need to be studied. Dehkordi et al. (2009) presented a new multi-objective technique to hide the sensitive association rules and to enhance the security of database. In fact, this maintained the utility and of mined rules at efficient level. The proposed {{algorithm is}} based on genetic algorithm (GA) concept, where the privacy and accuracy of dataset are enhanced. Gkoulalas-Divanis and Verykios (2009) developed an exact border-based technique to obtain an optimal solution to hide sensitive frequent item sets with minimum extension of the original database generated synthetically via the database extension. This is accomplished via the following: (1) by formulating the generation of the database extension as a constraint satisfaction problem, (2) using mapping of the constraint satisfaction issues to an equivalent binary integer programming problem, (3) via the manipulation of underutilized <b>synthetic</b> <b>transactions</b> to increase the support of non-sensitive item sets, (4) employing the minimally relaxing constraint satisfaction problem to offer an approximate solution close to the optimal one when an ideal solution does not exist, and (5) by partitioning the universe of the items to enhance {{the efficiency of the}} proposed hiding algorithm.|$|E
40|$|We present GHTraffic, a dataset of {{significant}} size comprising HTTP transactions extracted from GitHub data and augmented with <b>synthetic</b> <b>transaction</b> data. This dataset facilitates reproducible research on {{many aspects of}} service-oriented computing. GHTraffic comprises three different editions: Small (S), Medium (M) and Large (L). The S dataset includes HTTP records created from google/guava repository. Guava is a popular Java library containing utilities and data structures. The M dataset includes records from the npm/npm project. It is the popular de-facto standard package manager for JavaScript. The L dataset contains data that were created by selecting eight repositories containing large and very active projects, including twbs/bootstrap, symfony/symfony, docker/docker, Homebrew/homebrew, rust-lang/rust, kubernetes/kubernetes, and angular/angular. js. We also provide access to the scripts used to generate GHTraffic. Using these scripts, the user can modify the configuration properties in the config. properties file {{in order to create}} a customised version of GHTraffic datasets for thier own use. Scripts can be accessed by downloading the pre-configured VirtualBox image (ghtraffic-artifact- 1. 0. 0. ova) or by cloning the repository from [URL]...|$|R
40|$|Support {{confidence}} {{framework is}} misleading in finding statistically meaningful relationships in market basket data. The {{alternative is to}} find strongly correlated item pairs from the basket data. However, strongly correlated pairs query suffered from suitable threshold setting problem. To overcome that, top-k pairs finding problem has been introduced. Most of the existing techniques are multi-pass and computationally expensive. In this work an efficient technique for finding k top most strongly and correlated item pairs from transaction database, without generating any candidate sets has been reported. The proposed technique uses a correlogram matrix to compute support count of all the 1 - and 2 -itemset in a single scan over the database. From the correlogram matrix the positive correlation values of all the item pairs are computed and top-k correlated pairs are extracted. The simplified logic structure makes {{the implementation of the}} proposed technique more attractive. We experimented with real and <b>synthetic</b> <b>transaction</b> datasets and compared the performance of the proposed technique with its other counterparts (TAPER, TOP-COP and Tkcp) and found satisfactory...|$|R
50|$|Sagent's private {{financing}} solutions group provides customized financing {{solutions to}} privately held businesses, public companies and alternative investment firms, {{focusing on the}} following products: Equity & Equity Linked Securities, Structured Equity Securities, Mezzanine Debt Securities, Project Financing, PIPE <b>Transactions,</b> <b>Synthetic</b> Securities, Use of Proceeds- Contingent Acquisition and Bridge Financings, Corporate Spin-Outs, Growth financings, Recapitalizations, and Management Buyouts.|$|R
40|$|We {{present a}} social {{simulation}} model that covers three main financial services: Banks, Retail Stores, and Payments systems. Our {{aim is to}} address the problem of a lack of public data sets for fraud detection research in each of these domains, and provide a variety of fraud scenarios such as money laundering, sales fraud (based on refunds and discounts), and credit card fraud. Currently, there is a general lack of public research concerning fraud detection in the financial domains in general and these three in particular. One reason for this is the secrecy and sensitivity of the customers data that is needed to perform research. We present PaySim, RetSim, and BankSim as three case studies of social simulations for financial transactions using agent-based modelling. These simulators enable us to generate <b>synthetic</b> <b>transaction</b> data of normal behaviour of customers, and also known fraudulent behaviour. This synthetic data can be used to further advance fraud detection research, without leaking sensitive information about the underlying data. Using statistics and social network analysis (SNA) on real data we can calibrate the relations between staff and customers, and generate realistic synthetic data sets. The generated data represents real world scenarios that are found in the original data with the added benefit that this data can be shared with other researchers for testing similar detection methods without concerns for privacy and other restrictions present when using the original data...|$|R
40|$|In {{this work}} {{we discuss the}} {{secondary}} market for life insurance policies in the United States of America. First, we give {{an overview of the}} life settlement market: how it came into existence, its growth prospects and the ethical issues it arises. Secondly, we discuss the characteristics of the 	different life insurance products present in the market and describe how life settlements are originated. Life settlement transactions tend to be long and complex transactions that require the involvement of a number of parties. Also, a direct investment into life insurance policies is fraught with a number of practical issues and entails risks that are not directly related to longevity. This may reduce the efficiency of a direct investment in physical policies. For these reasons, a synthetic longevity market has evolved. The number of parties involved in a <b>synthetic</b> longevity <b>transaction</b> is typically smaller and the broker-dealer transferring the longevity exposure will be retaining most or all of the risks a physical investment entails. Finally, we describe the main methods used in the market to evaluate life settlement investments and the role of life expectancy providers...|$|R
40|$|Networks {{are present}} in many fields such as finance, sociology, and transportation. Often these {{networks}} are dynamic: they have a structural {{as well as a}} temporal aspect. We present a technique that extends the Massive Sequence View (MSV) for the analysis of the temporal and structural aspects of dynamic networks. Using features in the data {{as well as in the}} visualization based on the Gestalt principles closure, proximity, and similarity, we developed node reordering strategies for the MSV to make these features stand out. This enables users to find temporal properties such as trends, counter trends, periodicity, temporal shifts, and anomalies in the network as well as structural properties such as communities and stars. We show the effectiveness of the reordering methods on both <b>synthetic</b> and real-world <b>transaction</b> data sets...|$|R
40|$|This Essay will {{describe}} synthetic lease financings {{and provide an}} analysis of {{the advantages and disadvantages of}} these transactions for the acquisition or construction of a power generation facility. During the past two years, several leading players in the power generation industry have used “synthetic” leases to finance both the construction and acquisition of power generation assets, as well as bulk purchases of combustion turbines. Synthetic leases can offer a tax and balance sheet efficient alternative for the acquisition and construction of a power generation facility and related equipment (collectively referred to in this Essay as a “power generation facility”). A synthetic lease (also known by other names such as “off-balance sheet financing” or “tax oriented operating lease”) is a financing transaction structured through a lease that satisfies the requirements for characterization of a lease as an operating lease set forth in the Financial Accounting Standards Board (“FASB”) Statement 13 (“SFAS 13 ”) and related accounting rules. Because a synthetic lease allows a project sponsor to enjoy operating lease accounting treatments and avoid depreciation charges attributable to the leased asset, power producers employing this technique may obtain tangible economic advantages in the current market-driven environment. Synthetic lease financing may also allow a project sponsor greater financial flexibility to participate in a number of large scale projects and equipment purchases while mitigating the adverse credit impact of any particular project or transaction. The execution of <b>synthetic</b> leasing <b>transactions</b> in the power generation industry by leading players during the past two years may encourage others in the industry to consider such innovative approaches...|$|R

