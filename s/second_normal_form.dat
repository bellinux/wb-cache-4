19|10000|Public
50|$|<b>Second</b> <b>normal</b> <b>form</b> (2NF) is {{a normal}} form used in {{database}} normalization. 2NF was originally defined by E.F. Codd in 1971.|$|E
50|$|Just as a {{function}} (in programming) can provide abstraction, so can a database view. In another parallel with functions, database users can manipulate nested views, thus one view can aggregate data from other views. Without the use of views, the normalization of databases above <b>second</b> <b>normal</b> <b>form</b> would become much more difficult. Views can {{make it easier to}} create lossless join decomposition.|$|E
50|$|A {{relation}} that is {{in first}} normal form (1NF) must meet additional criteria {{if it is to}} qualify for <b>second</b> <b>normal</b> <b>form.</b> Specifically: a relation is in 2NF if it is in 1NF and no non-prime attribute is dependent on any proper subset of any candidate key of the relation. A non-prime attribute of a relation is an attribute that is not a part of any candidate key of the relation.|$|E
5000|$|The <b>normal</b> <b>form</b> {{of a set}} is {{the most}} compact {{ordering}} of the pitches in a set. Tomlin defines the [...] "most compact" [...] ordering as the one where, [...] "the largest of the intervals between any two consecutive pitches is {{between the first and}} last pitch listed". For example, the set (0,2) (a major <b>second)</b> is in <b>normal</b> <b>form</b> while the set (0,10) (a minor seventh, the inversion of a major second) is not, its <b>normal</b> <b>form</b> being (10,0).|$|R
40|$|Abstract. There {{are only}} very few {{approaches}} to normalizing objectoriented data. In this paper we present {{an approach to}} normalization of the object-oriented conceptual model based on UML class diagrams. First part of the paper describes the current status {{in the area of}} formal methods used for object-oriented data modeling. Second part presents four normalization rules, which are based on own experience and modi ed Ambler-Beck approach. These normalization rules are introduced on an example. Our method has been used in education at several universities. It has been and is also used for database design in software development projects, which we carried out. Recently, development of the CASE tool based on this approach has been started. Keywords. Data normalization, object-oriented data model (ODM), relational data model (RDM), rst object-oriented <b>normal</b> <b>form</b> (1 ONF), <b>second</b> objectoriented <b>normal</b> <b>form</b> (2 ONF), third object-oriented <b>normal</b> <b>form</b> (3 ONF), fourth object-oriented <b>normal</b> <b>form</b> (4 ONF). ...|$|R
40|$|We {{provide a}} {{confluent}} and strongly normalizing rewriting system, based on expansion rules, for the extensional second order typed lambda calculus with product and unit types: this system {{corresponds to the}} Intuitionistic Positive Calculus with implication, conjunction, quantification over proposition and the constant True. This result {{is an important step}} towards a new theory of reduction based on expansion rules, and gives a natural interpretation to the notion of <b>second</b> order j-long <b>normal</b> <b>forms</b> used in higher order resolution and unification, that are here just the <b>normal</b> <b>forms</b> of our reduction system...|$|R
50|$|Third {{normal form}} {{is a normal}} form that is used in {{normalizing}} a database design to reduce the duplication of data and ensure referential integrity by ensuring that (1) the entity is in <b>second</b> <b>normal</b> <b>form,</b> and (2) all the attributes in a table are determined only by the candidate keys of that relation and not by any non-prime attributes. 3NF was designed to improve database processing while minimizing storage costs. 3NF data modeling was ideal for online transaction processing (OLTP) applications with heavy order entry type of needs.|$|E
5000|$|Edgar F. Codd, the {{inventor}} of the relational model (RM), introduced the concept of normalization and what is now known as the first normal form (1NF) in 1970. Codd went on to define the <b>second</b> <b>normal</b> <b>form</b> (2NF) and third normal form (3NF) in 1971, and Codd and Raymond F. Boyce defined the Boyce-Codd normal form (BCNF) in 1974. Informally, a relational database relation is often described as [...] "normalized" [...] if it meets third normal form. Most 3NF relations are free of insertion, update, and deletion anomalies.|$|E
40|$|For {{the cost}} {{function}} of CMA blind equalization is not satisfied <b>second</b> <b>normal</b> <b>form</b> and RLS algorithm can not using directly, a cascade filtering method was proposed {{to solve this}} problem. The cost function is simplified as <b>second</b> <b>normal</b> <b>form</b> in the method and the Wavelet Neural Network (WNN) was used as blind equalizer, then RLS algorithm {{can be used to}} update the network parameters to implement blind equalization. Meanwhile the forgetting factor in RLS algorithm was analyzed and adaptive forgetting factor was proposed to improve the performance. The output error can construct a attenuation function to which nonlinear transform was preformed to adaptive adjust the value of forgetting factor. Compared with BP neural network and WNN blind equalization based on gradient descent algorithm and WNN blind equalization based on RLS algorithm with fixed value, the method proposed in this study has faster convergence rate and convergence precision. Acoustic channel simulations and pool experiment proved the method has better performance in underwater communication...|$|E
5000|$|An {{alternative}} design uses two tables:Columns do {{not contain}} more than one telephone number in this design. Instead, each Customer-to-Telephone Number link appears on its own row. Using Customer ID as key, a one-to-many relationship exists between the name and the number tables. A row in the [...] "parent" [...] table, Customer Name, {{can be associated with}} many telephone number rows in the [...] "child" [...] table, Customer Telephone Number, but each telephone number belongs to one, and only one customer. [...] It is worth noting that this design meets the additional requirements for <b>second</b> and third <b>normal</b> <b>form.</b>|$|R
40|$|Abstract — In {{this paper}} a tool called RDBNorma is proposed, {{that uses a}} novel {{approach}} to represent a relational database schema and its functional dependencies in computer memory using only one linked list and used for semi-automating the process of relational database schema normalization up to third <b>normal</b> <b>form.</b> This paper addresses all the issues of representing a relational schema along with its functional dependencies using one linked list along with the algorithms to convert a relation into <b>second</b> and third <b>normal</b> <b>form</b> by using above representation. We have compared performance of RDBNorma with existing tool called Micro using standard relational schemas collected from various resources. It is observed that proposed tool is at least 2. 89 {{times faster than the}} Micro and requires around half of the space than Micro to represent a relation. Comparison is done by entering all the attributes and functional dependencies holds on a relation in the same order and implementing both the tools in same language and on same machine. Index Terms — relational databases, normalization, automation of normalization, <b>normal</b> <b>forms.</b> 1...|$|R
40|$|Distributed {{circuits}} {{composed of}} linear multiconductor transmission lines and terminated with nonlinear weakly active multiport resistors are considered. The line is {{represented as a}} linear dynamic multiport through recursive convolution relations and special considerations are given to some general properties of the line impulse responses. The convolution technique allows the mathematical description of these distributed circuits {{by means of a}} sea: of nonlinear algebraic-integral equations of Volterra type for the terminal voltages and currents. The conditions under which these governing equations can he reformulated as a set of Volterra integral equations of <b>second</b> kind in <b>normal</b> <b>form</b> are given with the explicit means for doing so. These conditions also assure the existence and the uniqueness of the solution. In particular if the terminal multiport resistors are strictly locally passive, then the <b>normal</b> <b>form</b> exists and the solution is unique. Transmission lines with terminal multiport resistors that are locally active may not admit a <b>normal</b> <b>form</b> for the governing equations, and hence, several solutions that have the same initial conditions are possible, In these cases a simple method is presented for revising the original network model so that the <b>normal</b> <b>form</b> exists, and hence, the uniqueness of solution is assured, under mild restrictions...|$|R
40|$|Introduction The {{concept of}} {{relational}} normal forms {{was introduced by}} E. F. Codd [3]. He propounded a classification with three normal forms: first normal form (1 NF), <b>second</b> <b>normal</b> <b>form</b> (2 NF) and third normal form (3 NF). This classification {{is based on the}} concept of key and functional dependencies structure of a relation. The main idea Codd brings out with this classification is the advantage of 3 NF relations according to two essential criterions: ffl To reduce information redundancy...|$|E
40|$|We {{investigate}} {{local and}} global paradigms of reasoning about distributed behaviours, modelled as Mazurkiewicz traces, {{in the context of}} first-order and monadic second-order logics. We describe new normal forms for properties expressible in these logics. The first normal form, surprisingly, yields a decomposition of a global property as a boolean combination of local properties. The <b>second</b> <b>normal</b> <b>form</b> strengthens McNaughton's theorem and states that global properties of infinite behaviours may also be described as boolean combinations of recurring properties of finite local histories of the behaviours. We briefly touch upon some of the interesting applications of these normal forms...|$|E
40|$|The centrepiece of {{this paper}} is a normal form for {{primitive}} elements which facilitates the use of induction arguments to prove properties of primitive elements. The normal form arises from an elementary algorithm for constructing a primitive element p in F(x, y) with a given exponent sum pair (X, Y), if such an element p exists. Several results concerning the primitive elements of F(x, y) are recast as applications of the algorithm and the normal form. Comment: 12 pages. Replaces old version (apologies for uploading wrong version) which contained an error in the statement of <b>Second</b> <b>normal</b> <b>form</b> theore...|$|E
40|$|In {{this paper}} a tool called RDBNorma is proposed, {{that uses a}} novel {{approach}} to represent a relational database schema and its functional dependencies in computer memory using only one linked list and used for semi-automating the process of relational database schema normalization up to third <b>normal</b> <b>form.</b> This paper addresses all the issues of representing a relational schema along with its functional dependencies using one linked list along with the algorithms to convert a relation into <b>second</b> and third <b>normal</b> <b>form</b> by using above representation. We have compared performance of RDBNorma with existing tool called Micro using standard relational schemas collected from various resources. It is observed that proposed tool is at least 2. 89 {{times faster than the}} Micro and requires around half of the space than Micro to represent a relation. Comparison is done by entering all the attributes and functional dependencies holds on a relation in the same order and implementing both the tools in same language and on same machine. Comment: 22 pages and international journa...|$|R
40|$|Second-order {{ordinary}} {{differential equations}} (ODEs) with strongly nonlinear damping (cubic nonlinearities) govern surface wave motions that entail nonlinear surface seismic motions. They apply to dynamic crack propagation and nonlinear oscillation problems in physics and nonlinear mechanics. It is {{shown that the}} nonlinear surface seismic wave equation (Rayleigh equation) admits several functional transformations {{and it is possible}} to reduce it to an equivalent first-order Abel ODE of the <b>second</b> kind in <b>normal</b> <b>form.</b> Based on a recently developed methodology concerning the construction of exact analytic solutions for the type of Abel equations under consideration, exact solutions are obtained for the nonlinear seismic wave (NLSW) equation for initial conditions of the physical problem. The method employed is general and can be applied to a large class of relevant ODEs in mathematical physics and nonlinear mechanics. (c) 2005 Elsevier Ltd. All rights reserved...|$|R
5000|$|A head <b>normal</b> <b>form</b> is {{not always}} a <b>normal</b> <b>form,</b> because the applied {{arguments}} [...] need not be normal. However, the converse is true: any <b>normal</b> <b>form</b> is also a head <b>normal</b> <b>form.</b> In fact, the <b>normal</b> <b>forms</b> are exactly the head <b>normal</b> <b>forms</b> in which the subterms [...] are themselves <b>normal</b> <b>forms.</b> This gives an inductive syntactic description of <b>normal</b> <b>forms.</b>|$|R
40|$|This paper {{presents}} two new normal {{forms for}} fractions of differential polynomials, {{as well as}} algorithms for computing them. The first normal form allows to write a fraction as the derivative of a fraction plus a nonintegrable part. The <b>second</b> <b>normal</b> <b>form</b> {{is an extension of}} the first one, involving iterated differentiations. The main difficulty in this paper consists in defining normal forms which are linear operations over the field of constants, a property which was missing in our previous works. Our normal forms do not require fractions to be converted into polynomials, a key feature for further problems such as integrating differential fractions, and more generally solving differential equations...|$|E
40|$|In many cases, {{classical}} databases need to {{be extended}} in order to represent and manipulate uncertain and imprecise information. In a fuzzy relational data model where attribute values are represented by possibility distributions and domains are associated with closeness relations, the problems of update anomalies and data redundancy may still exist. This paper aims to extend the normalization theory of the classical relational data model so as to provide theoretical guidelines for fuzzy relational database design. Based upon the notion of fuzzy functional dependency (FFD), a number of concepts such as relation keys and normal forms are generalized. As a result, q-keys, Fuzzy First Normal Form (F 1 NF), q-Fuzzy <b>Second</b> <b>Normal</b> <b>Form</b> (q-F 2 NF), q-Fuzzy Third Normal Form (q-F 3 NF), and q-Fuzzy Boyce-Codd Normal Form (q-FBCNF) have been formulated. Finally, dependency-preserving and lossless-join decompositions into q-F 3 NFs are discussed. status: publishe...|$|E
40|$|AbstractIn this paper, {{we propose}} the {{application}} of formal methods to Software Engineering. The most used data model is the relational model and we present, within the general framework of lattice theory, this analysis of functional dependencies. For this reason, we characterize the concept of f-family {{by means of a}} new concept which we call non-deterministic ideal operator (nd. ideal-o). The study of nd. ideal-o. s allows us to obtain results about functional dependencies as trivial particularizations, to clarify the semantics of the functional dependencies and to progress in their efficient use, and to extend the concept of schema. Moreover, the algebraic characterization of the concept of Key of a schema allows us to propose new formal definitions in the lattice framework for classical normal forms in relation schemata. We give a formal definition of the normal forms for functional dependencies more frequently used in the bibliography: the <b>second</b> <b>normal</b> <b>form</b> (2 FN), the third normal form(3 FN) and Boyce–Codd's normal form (FNBC) ...|$|E
50|$|The third <b>normal</b> <b>form,</b> Boyce-Codd <b>normal</b> <b>form,</b> fourth <b>normal</b> <b>form</b> {{and fifth}} <b>normal</b> <b>form</b> are special {{cases of the}} domain/key <b>normal</b> <b>form.</b> All have either functional, multi-valued or join {{dependencies}} that can be converted into (super)keys. The domains on those <b>normal</b> <b>forms</b> were unconstrained so all domain constraints are satisfied. However, transforming a higher <b>normal</b> <b>form</b> into domain/key <b>normal</b> <b>form</b> {{is not always a}} dependency-preserving transformation and therefore not always possible.|$|R
50|$|An {{arbitrary}} propositional formula {{may have}} a very complicated structure. It is often convenient to work with formulas that have simpler <b>forms,</b> known as <b>normal</b> <b>forms.</b> Some common <b>normal</b> <b>forms</b> include conjunctive <b>normal</b> <b>form</b> and disjunctive <b>normal</b> <b>form.</b> Any propositional formula {{can be reduced to}} its conjunctive or disjunctive <b>normal</b> <b>form.</b>|$|R
50|$|A formula in {{negation}} <b>normal</b> <b>form</b> can be {{put into}} the stronger conjunctive <b>normal</b> <b>form</b> or disjunctive <b>normal</b> <b>form</b> by applying distributivity.|$|R
40|$|Normal {{forms for}} logic {{programs}} under stable/answer set semantics are introduced. We {{argue that these}} forms may simplify the study of program properties, viz. consistency, thedesign of deduction algorithms and, potentially, eﬃcient implementations. The ﬁrst normal form, called kernel of the program, is synthetic w. r. t. existence and number of answer sets. While the complexity of answer set computation over kernel programs does not decrease {{as far as the}} polynomial hierarchy is concerned, programs in kernel form tend {{to be a lot more}} compact, thus yielding better computation performance in general. We address computational issues concerning the reduction of a given program to its kernel (called kernelization). We address consistency by deﬁning a <b>second</b> <b>normal</b> <b>form,</b> 3 -kernel, where, apart from kernel form, the length of rule bodies is limited to two. For programs in 3 -kernel form, consistency checking can be done eﬃciently by means of the Cycle Graph representation deﬁned by (Costantini, 2001), which gives the ﬁrst purely-syntactic and complete characterization of consistent (w. r. t. Answer Sets semantics) logic programs...|$|E
40|$|Normal {{forms for}} logic {{programs}} under stable/answer set semantics are introduced. We {{argue that these}} forms can simplify the study of program properties, mainly consistency. The first normal form, called the kernel of the program, is useful for studying existence and number of answer sets. A kernel program is composed of the atoms which are undefined in the Well-founded semantics, which are those that directly affect the existence of answer sets. The body of rules is composed of negative literals only. Thus, the kernel form tends to be significantly more compact than other formulations. Also, {{it is possible to}} check consistency of kernel programs in terms of colorings of the Extended Dependency Graph program representation which we previously developed. The <b>second</b> <b>normal</b> <b>form</b> is called 3 -kernel. A 3 -kernel program is composed of the atoms which are undefined in the Wellfounded semantics. Rules in 3 -kernel programs have at most two conditions, and each rule either belongs to a cycle, or defines a connection between cycles. 3 -kernel programs may have positive conditions. The 3 -kernel normal form is very useful for the static analysis of program consistency, i. e., the syntactic characterization of existence of answer sets. This result can be obtained thanks to a novel graph-like representation of programs, called Cycle Graph which presented in the companion article (Costantini 2004 b) ...|$|E
40|$|AbstractIn {{this paper}} we present some {{characterizations}} of relation schemes in <b>second</b> <b>normal</b> <b>form</b> (2 NF), {{third normal form}} (3 NF) and Boyce-Codd normal form (BCNF). It is known [6]that the set of minimal keys of a relation scheme is a Sperner system (an antichain) and for an arbitrary Sperner system there exists a relation scheme the set of minimal keys of which is exactly the given Sperner system. We investigate families of 2 NF, 3 NF and BCNF relation schemes where the sets of minimal keys are given Sperner systems. We give characterizations of these families. The minimal Armstrong relation has been investigated in the literature [3, 7, 11, 15, 18]. This paper gives new bounds {{on the size of}} minimal Armstrong relations for relation schemes. We show that given a relation scheme s such that the set of minimal keys is the Sperner system K, the number of antikeys (maximal nonkeys) of K is polynomial in the number of attributes iff so is the size of minimal Armstrong relation of s. We give a new characterization of relations and relation schemes that are uniquely determined by their minimal keys. From this characterization we give a polynomial-time algorithm deciding whether an arbitrary relation is uniquely determined by its set of all minimal keys. We present a new polynomial-time algorithm testing BCNF property of a given relation scheme...|$|E
40|$|A key {{is simple}} if it {{consists}} of a single attribute. It is shown that if a relation schema is in third <b>normal</b> <b>form</b> and every key is simple, then it is in projection-join <b>normal</b> <b>form</b> (sometimes called fifth <b>normal</b> <b>form),</b> the ultimate <b>normal</b> <b>form</b> with respect to projections and joins. Furthermore, it is shown that if a relation schema is in Boyce-Codd <b>normal</b> <b>form</b> and some key is simple, then it is in fourth <b>normal</b> <b>form</b> (but not necessarily projection-join <b>normal</b> <b>form).</b> These results give the database designer simple sufficient conditions, {{defined in terms of}} functional dependencies alone, that guarantee that the schema being designed is automatically in higher <b>normal</b> <b>forms...</b>|$|R
50|$|Relations are {{classified}} {{based upon the}} types of anomalies to which they're vulnerable. A database that's in the first <b>normal</b> <b>form</b> is vulnerable to all types of anomalies, while a database that's in the domain/key <b>normal</b> <b>form</b> has no modification anomalies. <b>Normal</b> <b>forms</b> are hierarchical in nature. That is, the lowest level is the first <b>normal</b> <b>form,</b> and the database cannot meet the requirements for higher level <b>normal</b> <b>forms</b> without first having met all {{the requirements of the}} lesser <b>normal</b> <b>forms.</b>|$|R
40|$|We study a {{class of}} {{matrices}} with noncommutative entries, which were first considered by Yu. I. Manin in 1988 in relation with quantum group theory. They are defined as "noncommutative endomorphisms" of a polynomial algebra. More explicitly their defining conditions read: 1) elements in the same column commute; 2) commutators of the cross terms are equal: $[M_{ij}, M_{kl}] = [M_{kj}, M_{il}]$ (e. g. $[M_{ 11 },M_{ 22 }] = [M_{ 21 },M_{ 12 }]$). The basic claim is that despite noncommutativity many theorems of linear algebra hold true for Manin matrices in a form identical {{to that of the}} commutative case. Moreover in some examples the converse is also true. The present paper gives a complete list and detailed proofs of algebraic properties of Manin matrices known up to the moment; many of them are new. In particular we present the formulation in terms of matrix (Leningrad) notations; provide complete proofs that an inverse to a M. m. is again a M. m. and for the Schur formula for the determinant of a block matrix; we generalize the noncommutative Cauchy-Binet formulas discovered recently [arXiv: 0809. 3516], which includes the classical Capelli and related identities. We also discuss many other properties, such as the Cramer formula for the inverse matrix, the Cayley-Hamilton theorem, Newton and MacMahon-Wronski identities, Plucker relations, Sylvester's theorem, the Lagrange-Desnanot-Lewis Caroll formula, the Weinstein-Aronszajn formula, some multiplicativity properties for the determinant, relations with quasideterminants, calculation of the determinant via Gauss decomposition, conjugation to the <b>second</b> <b>normal</b> (Frobenius) <b>form,</b> {{and so on and so}} forth. We refer to [arXiv: 0711. 2236] for some applications. Comment: 80 page...|$|R
40|$|Conceptual {{modeling}} {{is one of}} {{the most}} important phases in designing database applications. The success of this design relies heavily on how clearly the real world requirements are represented in the conceptual model. To date, the Extended Entity Relationship (EER) model extended from the traditional Entity Relationship (ER) model is a widely used modeling technique during the phase of conceptual modeling. This paper identifies semantic ambiguities that are still present in the EER model leading to incorrect knowledge representation and eventually to incorrect design of relational database schema. These ambiguities are identified in case of many-to-many relationships which have their own attributes. This paper shows that mapping such relationships to a relational database schema generates relations having primary keys which cannot guarantee unique tuples for real world data thus violating the definition of a primary key. In addition, it shows that these relations may not satisfy <b>second</b> <b>normal</b> <b>form.</b> A number of such cases are elaborated and a new concept of multi-valued relationship attribute is introduced that can successfully represent these real world constraints. For this concept, a diagrammatic notation to use in ER diagram is introduced. A mapping algorithm to transform the corresponding EER model to a relational database schema is also defined. This concept of multi-valued relationship attribute and its mapping to relational schema generate relations which satisfy higher normal forms...|$|E
40|$|Recursive {{permutations}} whose cycles are {{the classes}} of a decidable equivalence relation are studied; {{the set of}} these permutations is called Perm, the group of all recursive permutations G. Multiple equivalent computable representations of decidable equivalence relations are provided. G-conjugacy in Perm is characterised by computable isomorphy of cycle equivalence relations. This result parallels the equivalence of cycle type equality and conjugacy in the full symmetric group of the natural numbers. Conditions are presented for a permutation f ∈G to be in Perm and for a decidable equivalence relation to appear as the cycle relation {{of a member of}} G. In particular, two normal forms for the cycle structure of permutations are defined and it is shown that conjugacy to a permutation in the first normal form is equivalent to membership in Perm. Perm is further characterised as the set of maximal permutations in a family of preordered subsets of automorphism groups of decidable equivalences. Conjugacy to a permutation in the <b>second</b> <b>normal</b> <b>form</b> corresponds to decidable cycles plus decidable cycle finiteness problem. Cycle decidability and cycle finiteness are both shown to have the maximal one-one degree of the Halting Problem. Cycle finiteness is used to prove that conjugacy in Perm cannot be decided and {{that it is impossible to}} compute cycle deciders for products of members of Perm and finitary permutations. It is also shown that Perm is not recursively enumerable and that it is not a group. Comment: 27 pages, 4 figure...|$|E
40|$|In this paper, {{we present}} a {{practical}} view of denormalization, and provide fundamental guidelines for incorporating denormalization. We have suggested, using denormalization as an intermediate step between logical and physical modeling {{to be used as}} an analytic procedure for the design of the applications requirements criteria. Relational algebra and query trees are used to examine the effect on the performance of relational systems. The guidelines and methodology presented are sufficiently general, and they can be applicable to most databases. It is concluded that denormalization can enhance query performance when it is deployed with a complete understanding of application requirements. 1. Normalization vs. Denormalization Normalization is the process of grouping attributes into refined structures. The normal forms and the process of normalization have been studied by many researchers, since Codd [5] initiated the subject. First Normal Form (1 NF), <b>Second</b> <b>Normal</b> <b>Form</b> (2 NF), and Third Normal Form (3 NF) were the only forms originally proposed by Codd, and they are the normal forms supported by commercial case tools. The higher form of normalization such as Boyce/Codd Normal Form, the Fourth Normal Form (4 NF), and the Fifth Normal Form (5 NF) are academically important but are not widely implemented. The objective of normalization is to organize data into stable structures, and thereby minimize update anomalies and maximize data accessibility. Although normalization is generally regarded as the rule for the statue of relational database design, there are still times when database designers may turn to denormalizing a database in order to enhance performance and ease of use. Even though normalization results in many benefits, {{there is at least one}} major drawback – poor system performance [22],[13], [15], [24], [18], [7]. Normalization can also be used as a supplemental tool to provide an additional check on the stability an...|$|E
40|$|In {{this paper}} we {{introduce}} {{the class of}} additive <b>normal</b> <b>form</b> games, which is a subset of <b>normal</b> <b>form</b> games. In additive <b>normal</b> <b>form</b> games, the actions of each agent contribute some amount to the final payoff of all the agents. The contributions of the agents {{are assumed to be}} additive. We discuss the necessary and sufficient conditions for a <b>normal</b> <b>form</b> game to be an additive <b>normal</b> <b>form</b> game and show exactly how a <b>normal</b> <b>form</b> game may be converted to our additive representation...|$|R
50|$|It's {{much easier}} to build a {{database}} in domain/key <b>normal</b> <b>form</b> {{than it is to}} convert lesser databases which may contain numerous anomalies. However, successfully building a domain/key <b>normal</b> <b>form</b> database remains a difficult task, even for experienced database programmers. Thus, while the domain/key <b>normal</b> <b>form</b> eliminates the problems found in most databases, it tends to be the most costly <b>normal</b> <b>form</b> to achieve. However, failing to achieve the domain/key <b>normal</b> <b>form</b> may carry long-term, hidden costs due to anomalies which appear in databases adhering only to lower <b>normal</b> <b>forms</b> over time.|$|R
40|$|In logic, {{there are}} various <b>normal</b> <b>forms</b> for formulae; for example, {{disjunctive}} and conjunctive <b>normal</b> <b>form</b> for formulae of propositional logic or prenex <b>normal</b> <b>form</b> for formulae of predicate logic. There are algorithms for ‘reducing’ a given formula to a semantically equivalent formula in <b>normal</b> <b>form.</b> <b>Normal</b> <b>forms</b> are used {{in a variety of}} contexts including proofs of completeness, automated theorem proving, logic programming etc. In this paper, we develop a <b>normal</b> <b>form</b> for unitary Euler diagrams with shading. We give an algorithm for reducing a given Euler diagram to a semantically equivalent diagram in <b>normal</b> <b>form</b> and hence a decision procedure for determining whether two Euler diagrams are semantically equivalent. Potential applications of the <b>normal</b> <b>form</b> include clutter reduction and automated theorem proving in systems based on Euler diagrams...|$|R
