0|40|Public
50|$|The game allowed 32 select NCAA I-A {{teams from}} 1960 to 1970, {{including}} a few Ivy League squads, to be pitted against one other. For each team, <b>statistical</b> <b>grids</b> based on actual, specific team strengths balanced nine offensive plays against six defensive formations. There were weighted probabilities were expressed by offensive (216 outcomes) and defensive (36 outcomes) dice rolls for simplified but statistically informed results.|$|R
40|$|An {{analysis}} of factors influencing effective soil acidification management is reported. This {{analysis was conducted}} simultaneously at both national and local levels. These investigations were accomplished in three stages: (i) validation of acid soil spatial patterns using systems analysis and geoinformation methods; (ii) spatial statistical {{analysis of}} soil pH diversity using a <b>statistical</b> <b>grid</b> method; and (iii) development {{of the concept of}} soil acidity management. Results indicate the national spatial distribution of topsoil reaction is a natural and stable phenomenon related to Quaternary sub-surface deposits. However, secondary effects of topsoil liming are evident in both spatial and temporal soil reaction patterns...|$|R
40|$|In {{this paper}} novel {{techniques}} for Microarray Image Analysis are proposed. In particular, we describe MIRA (Microarray Image Rotation Algorithm) and SGRIP (<b>Statistical</b> <b>GRIdding</b> Pipeline) <b>statistical</b> algorithms respectively devoted {{to restore the}} original microarray orientation and detect the correct geometrical information about each spot of input Microarray. Both solutions significantly improve the performances of the segmentation pipeline MISP (Microarray Image Segmentation Pipeline) proposed in [1]. MIRA, SGRIP and MISP modules have been developed as plug-ins for an advanced on-going framework for Microarray Image Analysis. Experiments confirm {{the effectiveness of the}} proposed techniques, in terms of visual and numerical data...|$|R
40|$|The {{difference}} in statistical scales will bring different statistical results, {{so it is}} important and necessary to select <b>statistical</b> <b>grids</b> for the geographical conditions information statistics. This paper proposes a method of selecting statistical scale of geographical information with spatial autocorrelation being taken into account. By using geographic conditions census data and taking building information statistics as an example, this study gets building statistical information at different scales from 100 m to 10000 m, and meanwhile analyzes change trends of spatial autocorrelation of building information at different scales to make statistical scale selection. The results show that for building information statistics, 1000 m is the turning point of statistical scales, {{and can be used}} as a suitable statistical scale in city region. Department of Land Surveying and Geo-Informatic...|$|R
50|$|Real-time {{simulators}} {{are used}} extensively in many engineering fields. As a consequence, {{the inclusion of}} simulation applications in academic curricula can provide great value to the student. <b>Statistical</b> power <b>grid</b> protection tests, aircraft design and simulation, motor drive controller design methods and space robot integration are {{a few examples of}} real-time simulator technology applications.|$|R
40|$|In {{this paper}} novel {{techniques}} for Microarray Image Analysis are proposed. In particular, we describe an overall pipeline {{able to solve}} the most common problems of microarray image analysis. We propose Microarray Image Rotation Algorithm (MIRA) and <b>Statistical</b> <b>GRIdding</b> Pipeline (SGRIP) as two advanced modules respectively devoted to restore the original microarray grid orientation and to detect the correct geometrical information about each spot of input microarray. Both solutions work making use of statistical observations, obtaining adaptive and reliable information about each spot property. They improve the performances of the segmentation pipeline Microarray Image Segmentation Pipeline (MISP) recently developed by the authors. MIRA, MISP and SGRIP modules have been developed as plug-ins for an advanced framework for Microarray Image Analysis. A new quality measure able to evaluate in effective way the adaptive segmentation {{with respect to the}} fixed (e. g. non adaptive) circle segmentation of each spot is proposed. Experiments confirm the effectiveness of the proposed techniques, in terms of visual and numerical data...|$|R
40|$|Standardized indexes of {{abundance}} were estimated for Gulf of Mexico greater amberjack {{using data from}} the Reef Fish Logbook Program. Separate indexes were estimated for the longline, handline 1 - 9 hooks, and handline with more than 10 hooks fisheries. In general, handline indexes showed no trend and remained relatively stable during the period studied. In contrast, the longline index remained stable until year 2000 and showed a constant increase since then. 1. MATERIALS AND METHODS Data for the present analysis was obtained from the Reef Fish logbook Program that collects catch and effort data on a per trip basis. Catch information is reported as total weight landed by species. Trips that reported using more than one gear or that fished in more of one area were not used in any of the analysis of catch rates because {{it is not possible to}} know what proportion of the catch was caught with each gear or in what area. Only trips that fished within <b>statistical</b> <b>grids</b> 2 - 21 were retained for analysis. Area 1 was excluded because McClelland and Cummings (1997) concluded that fish from that area should be considered part of the Atlantic management unit...|$|R
40|$|International audienceAs low target {{strength}} objects can {{be detected}} by {{the new generation of}} mine hunting sonar systems, more and more non mine like objects are passed on to the classification process. In the past, the mine hunting performance was mainly driven by the performance of the mine detector. With this improved detection capability, the mine hunting system performance is now driven by classification. The classification decision process, based on sonar images including shadows and echoes features, corresponds to the discrimination between MILCO (MIne Like Contact) and NON MILCO. This paper describes the operational incidence of a new theoretical approach used for shadow and image based classification performance evaluation. This approach is named DMC for Delta-Mask based Classification as it assumes the knowledge of specific objects images characteristics, represented with a specific <b>statistical</b> <b>grid</b> named the Delta-Mask. This theory leads to the computation of COC (Classification Operational Characteristics) curves, which give the probability of classification (Pc) and the probability of false classification (Pfc) as functions of sonar characteristics and operational conditions. These curves are computed for several types of sonar systems. Results based on real practical sonar images are shown and operational incidence is deduced...|$|R
40|$|A {{desirable}} {{feature of}} bathymetric sonar systems is {{the production of}} statistically independent soundings allowing a system to achieve its full capability in resolution and object detection. Moreover gridding algorithms such as the Combined Uncertainty Bathymetric Estimator (CUBE) rely on the statistical independence of soundings to properly estimate depth and discriminate outliers. Common methods of filtering to mitigate uncertainty in the signal processing of both multibeam and phase-differencing sidescan systems (curve fitting in zero-crossing detections and differential phase filtering respectively) can produce correlated soundings. Here we propose an alternative method for the generation of soundings from differential phase measurements made by either sonar type to produce statistically independent soundings. The method extracts individual, non-overlapping and unfiltered, phase-difference measurements (from either sonar type) converting these to sonar-relative receive angle, estimates their uncertainty, fixes the desired depth uncertainty level and combines these individual measurements into an uncertainty-weighted mean to achieve the desired depth uncertainty, and no more. When the {{signal to noise ratio}} is sufficiently high such that the desired depth uncertainty is achieved with an individual measurement, bathymetric estimates are produced at the sonarâ€™s full resolution capability. When multiple measurements are required, the filtering automatically adjusts to maintain the desired uncertainty level, degrading the resolution only as necessary. Because no two measurements contribute to a single reported sounding, the resulting estimated soundings are statistically independent and therefore better resolve adjacent objects, increase object detectability and are more suitable for <b>statistical</b> <b>gridding</b> methodologies...|$|R
40|$|From Categories to Individual Links : Statistical Analysis of Social Space. The {{categories}} {{used for}} the analysis of social structures and mobility are marked by the ideologies that lead to the devising of <b>statistical</b> <b>grids</b> But the rigidity of these models and causal explanations against which major part of contemporary historio graphy has rebelled is above all due to the workings of traditional descriptive statistics themselves In the first section the authors describe the mechanisms by which these workings shape the objects of analysis By interpreting the same group of 19 th century marriage certificates using variety of grids they highlight the different distorsions caused by the grouping of data into categories and by the successive formation of tables and cross tables In the second section the authors show that it is in fact possible to conceive more subtle forms of statistical description and analysis better adapted {{to the needs of the}} historian The formalisation of data using graphs allows them for example to isolate the specificity of the individual data to go beyond simple grouping and to break away from the traditional qualitative-quantative opposition In this way they suggest that it is possible to develop models for quantitative analysis capable of taking account of the mechanisms of micro-social determinationsGribaudi Maurizio, Blum Alain. Des catÃ©gories aux liens individuels : l'analyse statistique de l'espace social. In: Annales. Ã‰conomies, SociÃ©tÃ©s, Civilisations. 45 áµ‰ annÃ©e, N. 6, 1990. pp. 1365 - 1402...|$|R
40|$|Literature {{introduces}} idea mining as {{an approach}} for extracting interesting ideas from textual information. Related {{research focuses on}} extracting technological ideas as starting point for future technological research and development activities. Thus, it {{is limited to the}} technological domain. The algorithms standing behind idea mining also are optimized for the technological domain. In contrast to previous research, this work transfers idea mining to the social behavior domain by selecting and adapting parameters of the idea mining algorithm. Forward selection as main approach in stepwise regression is used to choose the predictive variables based on their <b>statistical</b> significance. <b>Grid</b> search is used to optimize the parameter values. A case study shows that these optimized idea mining parameters are successful in extracting social behavior ideas of animals in this case of Przewalski horses. Based on these findings, differences between technological ideas and social behavior ideas can be shown...|$|R
40|$|This article {{proposes a}} novel {{approach}} for detecting hazardous events in driving situations. The assessment of hazards {{is based on the}} detection of atypical situations. The main assumption is that driving situations might get dangerous when an implicit normal state is not given any more. A prototype for the detection of atypical driving situations has been developed. In a first step, a multi-sensor multi-level fusion framework is presented and exemplified by object detection based on a camera and a laser scanner. The detected objects with their specific behaviours are transformed into a <b>Statistical</b> Information <b>Grid</b> (SIG), which is filled up with training information. In the working phase, current situations are compared to the statistical information and declared as atypical if under a given threshold. The prototype has been implemented. It has been shown that the recognition of atypical driving events is possible in selected driving situations...|$|R
40|$|Massive {{amounts of}} {{geospatial}} data require better techniques to analyse and display them. A {{common practice in}} applying statistical analysis and reporting units {{is represented by the}} regional <b>statistical</b> unit <b>grids,</b> which are used for national and European statistics and reporting. In line with the EUâ€™s INSPIRE directive, Austria provides statistics constructed on the European Terrestrial Reference System, which uses the Lambert Azimuthal Equal Area projection (ETRS-LAEA) for spatial analysis and display. Although these units are fixed and allow comparability and replicability, there is no relation with the underlying phenomenon. In this study, we evaluate the suitability of SLICO superpixels to replace the artificial ETRF grid squares when delineating an image for further analysis and using reporting units, or for display purposes. In this approach, we minimize any further parametrization, which is introduced by many other available segmentation algorithms, and aim to replicate the ETRF grid by imposing a size constraint and using the squareâ€™s centres as seeds for superpixel generation...|$|R
40|$|Spatial data mining, i. e., {{discovery}} of interesting characteristics and patterns that may implicitly exist in spatial databases, is a challenging task {{due to the}} huge amounts of spatial data and to the new conceptual nature of the problems which must account for spatial distance. Clustering and region oriented queries are common problems in this domain. Several approaches have been presented in recent years, all of which require at least one scan of all individual objects (points). Consequently, the computational complexity is at least linearly proportional {{to the number of}} objects to answer each query. In this paper, we propose a hierarchical <b>statistical</b> information <b>grid</b> based approach for spatial data mining to reduce the cost further. The idea is to capture statistical information associated with spatial cells in such a manner that whole classes of queries and clustering problems can be answered without recourse to the individual objects. In theory, and confirmed by empirical studie [...] ...|$|R
40|$|Abstract Background In this paper, we {{demonstrate}} {{why and how}} both temporality and multimodality {{should be}} integrated in health related studies that include accessibility perspective, in this case healthy food accessibility. We provide evidence regarding the importance of using multimodal spatio-temporal accessibility measures when conducting research in urban contexts and propose a methodological approach for integrating different travel modes and temporality to spatial accessibility analyses. We use the Helsinki metropolitan area (Finland) as our case study region to demonstrate the effects of temporality and modality on the results. Methods Spatial analyses were carried out on 250 Â m <b>statistical</b> <b>grid</b> squares. We measured travel times between the home location of inhabitants and open grocery stores providing healthy food at 5 Â p. m., 10 Â p. m., and 1 a. m. using public transportation and private cars. We applied the so-called door-to-door approach for the travel time measurements to obtain more realistic and comparable results between travel modes. The analyses are based on open access data and publicly available open-source tools, thus similar analyses can be conducted in urban regions worldwide. Results Our results show that both time and mode of transport have a prominent impact {{on the outcome of}} the analyses; thus, understanding the realities of accessibility in a city may be very different according to the setting of the analysis used. In terms of travel time, there is clear variation in the results at different times of the day. In terms of travel mode, our results show that when analyzed in a comparable manner, public transport can be an even faster mode than a private car to access healthy food, especially in central areas of the city where the service network is dense and public transportation system is effective. Conclusions This study demonstrates that time and transport modes are essential components when modeling health-related accessibility in urban environments. Neglecting them from spatial analyses may lead to overly simplified or even erroneous images of the realities of accessibility. Hence, there is a risk that health related planning and decisions based on simplistic accessibility measures might cause unwanted outcomes in terms of inequality among different groups of people...|$|R
40|$|In this paper, we {{demonstrate}} {{why and how}} both temporality and multimodality {{should be}} integrated in health related studies that include accessibility perspective, in this case healthy food accessibility. We provide evidence regarding the importance of using multimodal spatio-temporal accessibility measures when conducting research in urban contexts and propose a methodological approach for integrating different travel modes and temporality to spatial accessibility analyses. We use the Helsinki metropolitan area (Finland) as our case study region to demonstrate the effects of temporality and modality on the results. Spatial analyses were carried out on 250 m <b>statistical</b> <b>grid</b> squares. We measured travel times between the home location of inhabitants and open grocery stores providing healthy food at 5 p. m., 10 p. m., and 1 a. m. using public transportation and private cars. We applied the so-called door-to-door approach for the travel time measurements to obtain more realistic and comparable results between travel modes. The analyses are based on open access data and publicly available open-source tools, thus similar analyses can be conducted in urban regions worldwide. Our results show that both time and mode of transport have a prominent impact {{on the outcome of}} the analyses; thus, understanding the realities of accessibility in a city may be very different according to the setting of the analysis used. In terms of travel time, there is clear variation in the results at different times of the day. In terms of travel mode, our results show that when analyzed in a comparable manner, public transport can be an even faster mode than a private car to access healthy food, especially in central areas of the city where the service network is dense and public transportation system is effective. This study demonstrates that time and transport modes are essential components when modeling health-related accessibility in urban environments. Neglecting them from spatial analyses may lead to overly simplified or even erroneous images of the realities of accessibility. Hence, there is a risk that health related planning and decisions based on simplistic accessibility measures might cause unwanted outcomes in terms of inequality among different groups of people...|$|R
40|$|Water {{pollution}} {{has become}} a growing threat to human society and natural ecosystems in recent decades, increasing the need {{to better understand the}} spatial and temporal variabilities of pollutants within aquatic systems. This study sampled water quality at 12 sampling sites from October 2006 to August 2008 in the Jinshui River of the South Qinling Mts., China. Multivariate <b>statistical</b> techniques and <b>gridding</b> methods were used to investigate the temporal and spatial variations of water quality and identify the main pollution factors and sources. Two-way analysis of variance (ANOVA) showed that 25 studied water quality variables had significant temporal differences (p...|$|R
40|$|Shale is {{a common}} type of {{sedimentary}} rock formed by clay particles and silt inclusions, and, in some cases, organic matter. Typically, shale formations serve as geological caps for hydrocarbon reservoirs. More recently, various shale formations {{have been identified as}} prolific sources of oil and natural gas and as host lithologies for the disposal of CO 2 and nuclear waste. Despite its abundance, the characterization of shale rocks remains a challenging task due to their complex chemistry, heterogeneous microstructure, and multiscale mechanical behaviors. This thesis aims at establishing the link between the composition and mechanics of shale materials at grain scales. A comprehensive experimental program forms the basis for the characterization of the chemical composition and mechanical properties of shale at micrometer and sub-micrometer length scales. The chemical assessment was conducted through a novel experimental design involving grids of wave dispersive spectroscopy (WDS) spot analyses and statistical clustering of the chemical data generated by the experiments. This so-called <b>statistical</b> <b>grid</b> WDS technique was coupled with grid nanoindentation experiments as a means to assess the nanochemomechanics of shale rocks. The similar microvolumes probed by both methods ensure a direct relation between the local chemistry and mechanics response of shale materials. The results of this investigation showed that the grid WDS technique provides quantitative means to determine the chemistries of silt-size inclusions (mainly quartz and feldspars) and the clay matrix. The mineralogy assessments obtained by grid WDS analysis were validated through comparisons with results from X-ray image analysis and X-ray diffraction (XRD) experiments. The direct coupling of the grid WDS and indentation techniques revealed that the porous clay phase, previously inferred from the mechanistic interpretation of indentation experiments, corresponds to the response of clay minerals. The coupling technique also showed that clay minerals located nearby silt inclusions exhibit enhanced mechanical properties due to a composite action sensed by nanoindentation. The new understanding developed in this thesis provides valuable insight into the chemomechanics of shale at nano and microscales. This coupled assessment represents valuable information for the development of predictive models for shale materials which consider the intricate links of composition, microstructure, and mechanical performance. by Amer Deirieh. Thesis (S. M.) [...] Massachusetts Institute of Technology, Dept. of Civil and Environmental Engineering, 2011. Cataloged from PDF version of thesis. Includes bibliographical references (p. 258 - 265) ...|$|R
40|$|International audienceIn {{this paper}} we test {{different}} approaches to the <b>statistical</b> post-processing of <b>gridded</b> numerical surface air temperatures (provided by the European Centre for Medium-Range Weather Forecasts) onto the temperature measured at surface weather stations located in the Italian region of Puglia. We consider simple post-processing techniques, like correction for altitude, linear regression from different input parameters and Kalman filtering, {{as well as a}} neural network training procedure, stabilised (i. e. driven into the absolute minimum of the error function over the learning set) by means of a Simulated Annealing method. A comparative analysis of the results shows that the performance with neural networks is the best. It is encouraging for systematic use in meteorological forecast-analysis service operations...|$|R
40|$|Cluster Analysis, an {{automatic}} process to find similar objects from a database, {{is a fundamental}} operation in data mining. A cluster {{is a collection of}} data objects that are similar to one another within the same cluster and are dissimilar to the objects in other clusters. Clustering techniques have been discussed extensively in SimilaritySearch, Segmentation, Statistics, Machine Learning, Trend Analysis, Pattern Recognition and Classification [1]. Clustering methods can be classified into i) Partitioning methods ii) Hierarchical methods iii) Density-based methods iv) Grid-based methods v) Model-based methods. Grid based methods quantize the object space into a finite number of cells (hyper-rectangles) and then perform the required operations on the quantized space. The main advantage of Grid based method is its fast processing time which depends on number of cells in each dimension in quantized space. In this research paper, we present some of the grid based methods such as CLIQUE (CLustering In QUEst) [2], STING (<b>STatistical</b> INformation <b>Grid)</b> [3], MAFIA (Merging of Adaptive Intervals Approach to Spatial Data Mining) [4], Wave Cluster [5]and O-CLUSTER (Orthogonal partitioning CLUSTERing) [6], as a survey andalso compare their effectiveness in clustering data objects. We also present some of the latest developments in Grid Based methods such as Axis Shifted Grid Clustering Algorithm [7] and Adaptive Mesh Refinement [Wei-Keng Liao etc] [8] to improve the processing time of objects...|$|R
40|$|Abstract. Regional {{climate models}} (RCMs) are {{currently}} one of regularly applied tools for localization {{of global climate}} models (GCMs) outcomes. This study is focused on analyses of mean daily temperature and precipitation using linear and nonlinear <b>statistical</b> techniques. 48 <b>grid</b> points of the regional climate models HIRHAM and RCAO and 29 meteorological stations of the Czech Republic were compared in the period 1961 - 1990. Mean annual courses of mean daily temperature and precipitation in the period 2071 - 2100 were analyzed at selected HIRHAM and RCAO grid points for A 2 and B 2 emission scenarios [IPCC, 2007]. Furthermore, time-delayed average mutual information (AMI) and autocorrelation function were analyzed. AMI and autocorrelation function were calculated for one selected time series of each grid point of the Czech Republic and Europe with a time lag varying from 1 to 10 days. The values of average mutual information were compared with values of autocorrelation function...|$|R
40|$|This paper {{presents}} {{a new approach}} to spatial upsampling of digital video based on super-resolution mosaics. First, we robustly generate a background mosaic of higher resolution than the original video. In order to achieve that goal, we apply hierarchical global image registration estimating an optimal parabolic parameter set for each view of a scene shot. The final mosaic is generated using <b>statistical</b> and projection <b>grid</b> distance measures to avoid the impact of foreground objects and to accomplish super-resolution respectively. Second, arbitrarily moving foreground objects are segmented using MRF-based change detection methods based on the calculated mosaic. For the foreground objects an optical flow field between adjacent frames is computed. Third, we create new views with higher spatial resolution fusing re-projected background content from the mosaic together with super-resolution foreground objects obtained using optical flow field calculation. Results show that this method is able to convert videos into higher spatial resolution with very high objective and subjective quality...|$|R
40|$|Regional {{climate models}} (RCMs) are {{currently}} one of regularly applied tools for localization {{of global climate}} models (GCMs) outcomes. This study is focused on analyses of mean daily temperature and precipitation using linear and nonlinear <b>statistical</b> techniques. 48 <b>grid</b> points of the regional climate models HIRHAM and RCAO and 29 meteorological stations of the Czech Republic were compared in the period 1961 - 1990. Mean annual courses of mean daily temperature and precipitation in the period 2071 - 2100 were analyzed at selected HIRHAM and RCAO grid points for A 2 and B 2 emission scenarios [IPCC, 2007]. Furthermore, time-delayed average mutual information (AMI) and autocorrelation function were analyzed. AMI and autocorrelation function were calculated for one selected time series of each grid point of the Czech Republic and Europe with a time lag varying from 1 to 10 days. The values of average mutual information were compared with values of autocorrelation function...|$|R
40|$|Virtual Observatory (VO) is a {{collection}} of interoperating data archives and software tools. Taking advantages of the latest information technologies, it aims to provide a data-intensively online research environment for astronomers all around the world. A large number of high-qualified astronomical software packages and libraries are powerful and easy of use, and have been widely used by astronomers for many years. Integrating those toolkits into the VO system is a necessary and important task for the VO developers. VO architecture greatly depends on Grid and Web services, consequently the general VO integration route is "Java Ready - Grid Ready - VO Ready". In the paper, we discuss the importance of VO integration for existing toolkits and discuss the possible solutions. We introduce two efforts in the field from China-VO project, "gImageMagick" and " Galactic abundance gradients <b>statistical</b> research under <b>grid</b> environment". We also discuss what additional work should be done to convert Grid service to VO service. Comment: 9 pages, 3 figures, will be published in SPIE 2004 conference proceeding...|$|R
40|$|Abstract Spatial {{scales and}} methods {{for dealing with}} scale have been widely dis-cussed in the water {{resources}} literature. Different spatial processes operate at differ-ent scales so interpretations {{based on data from}} one scale may not apply to another. Understanding the behavior of phenomena at multiple-scales of data aggregation is thus imperative to accurate integrations of data and models at different geographic resolutions. This study tests theoretical concepts of scale by presenting empirical results of multiscale GIS and <b>statistical</b> analyses on <b>gridded</b> water-availability, water use and population data for the Danube Basin in Europe, with results corroborated by similar tests in the Ganges (South Asia) and Missouri (North America) Basins. Fine-resolution datasets were aggregated to coarser grid sizes and standard statistical measures of spatial variability were computed. Statistical analysis of spatial vari-ability demonstrated two distinctly different cases for unscaled and scaled variables. Results show that variance (and standard deviation) in unscaled variables like fresh-water supply, use and population increases at coarser scalesâ€”contrary to the com-mon assumption of decreasing variability as grid-cell size increases. On the othe...|$|R
40|$|Design and {{operation}} of the electric power grid (EPG) relies heavily on computational models. High-fidelity, full-order models are used to study transient phenomena on {{only a small part}} of the network. Reduced-order dynamic and power flow models are used when analysis involving thousands of nodes are required due to the computational demands when simulating large numbers of nodes. The level of complexity of the future EPG will dramatically increase due to large-scale deployment of variable renewable generation, active load and distributed generation resources, adaptive protection and control systems, and price-responsive demand. High-fidelity modeling of this future grid will require significant advances in coupled, multi-scale tools and their use on high performance computing (HPC) platforms. This LDRD report demonstrates SNL's capability to apply HPC resources to these 3 tasks: (1) High-fidelity, large-scale modeling of power system dynamics; (2) <b>Statistical</b> assessment of <b>grid</b> security via Monte-Carlo simulations of cyber attacks; and (3) Development of models to predict variability of solar resources at locations where little or no ground-based measurements are available...|$|R
40|$|Distributed {{generators}} (DGs), {{coupled with}} suitable control and communication infrastructures, {{are expected to}} {{play a key role}} in improving the efficiency of electricity grids. In this paper, we focus on low-voltage and single-phase microgrids exploring the interplay of distributed power loss reduction and communication. We select representative power-loss reduction algorithms from the state of the art and provide design rules for the required networking strategies in the presence of lossy communication links, assessing the impact of communication as well as electrical grid features. Toward this end, we devise a novel <b>statistical</b> cosimulation (electricity <b>grid,</b> communication, and control) framework that faithfully mimics the characteristics of real-world microgrids in terms of communication and grid topologies, power demand, and distributed generation from solar sources. Our numerical results highlight the role of communication procedures and the differences among the selected optimization techniques for power loss reduction, assessing their convergence rate and quantifying the impact of communication failures, line impedance estimation error, communication and electricity grid topologies, network size, and number of DGs...|$|R
40|$|Mechanized {{methods of}} {{vibratory}} concrete consolidation were used on {{all or part}} of fourteen bridge decks in Illinois from 1996 to 1998. The method used on twelve decks consisted of vertical insertion of vibrators in a grid pattern. A transverse drag-through process was used on two decks. Cores were analyzed from decks of both types and compared with cores from conventionally poured decks in order to measure differences in entrapped air voids and in-place density. Based on <b>statistical</b> analysis, the <b>grid</b> pattern method was found to be significantly better than both the drag-through and conventional methods for reducing entrapments per square inch and increasing in-place density. The drag-through method was found to be better than the conventional method for reducing entrapments, but no difference was found for density. Segregation was found in the drag-through cores, but not for either the grid pattern or the conventional method. Some problems were noted on construction sites, mainly having to do with increased weight of the finishing machine and the effect of greater vibration intensity on superplasticized concrete. Costs associated with use of the grid pattern method were approximately $ 6. 58 per square meter ($ 5. 50 per square yard) of bridge deck. Key words: concrete consolidation, grid vibration, entrapped air...|$|R
40|$|AbstractWe {{present a}} live video {{streaming}} system using a low cost 3 D sensor camera like the Microsoft Kinect. The {{huge amount of}} raw point data that Kinect creates has to be stored and transmitted by efficient compact means. Noise and redundancy, however, make the process more difficult to achieve. To overcome these difficulties we propose a live streaming system that streams a 3 D video to an Android Mobile phone and to Linux Desktop systems. For Android mobile phone client, the 3 D video is filtered before streaming. Filtering stage contains 3 types of filters, Voxel <b>Grid,</b> <b>Statistical</b> Outlier Removal and Histogram-based conditional filters. The video is captured by the Kinect and a 3 D point cloud is created. Voxel Grid filter is used since the generated 3 D video will have millions of points; a downsampling procedure is applied to minimize the number of points. To reduce outliers and color information, statistical outlier removal and histogram-based conditional filters are used respectively. Conditional filter is customized by the scene histogram for each channel of RGB color to preserve the dominant color information for each scene. For Linux desktop client, the video is filtered by the histogram-based conditional filter and is compressed using an Octree structure that reduces spatial redundancies across the streamed video...|$|R
40|$|This paper {{presents}} {{a set of}} validation metrics for transmission network parameters that is applicable in both creation of synthetic power system test cases and validation of existing models. Using actual data from two real-world power <b>grids,</b> <b>statistical</b> analyses are performed to extract some useful statistics on transformers and transmission lines electrical parameters including per unit reactance, MVA rating, and their X/R ratio. It is found that conversion of per unit reactance calculated on system common base to transformer own power base will significantly stabilize its range and remove the correlation between per unit X and MVA rating. This is fairly consistent for transformers with different voltage levels and sizes and can be utilized as a strong validation metric for synthetic models. It is found that transmission lines exhibit different statistical properties than transformers with different distribution and range for the parameters. In addition, statistical analysis shows that the empirical PDF of transmission network electrical parameters can be approximated with mathematical distribution functions which would help appropriately characterize them in synthetic power networks. Kullback-Leibler divergence {{is used as a}} measure for goodness of fit for approximated distributions. Comment: To be published (Accepted) in: The 10 th Bulk Power Systems Dynamics and Control Symposium (IREP 2017), Espinho, Portugal, 201...|$|R
40|$|The World Meteorological Organization (WMO) {{established}} Regional Climate Centres (RCCs) {{around the}} world to create science-based climate information on a regional scale within the Global Framework for Climate Services (GFCS). The paper introduces the satellite component of the WMO Regional Climate Centre on Climate Monitoring (RCC-CM) for Europe and the Middle East. The RCC-CM product portfolio is based on essential climate variables (ECVs) as defined by the Global Climate Observing System (GCOS), spanning the atmospheric (radiation, clouds, water vapour) and terrestrial domains (snow cover, soil moisture). In the first part, the input data sets are briefly described, which are provided by the EUMETSAT (European Organisation for the Exploitation of Meteorological Satellites) Satellite Application Facilities (SAF), in particular CM SAF, and by the ESA (European Space Agency) Climate Change Initiative (CCI). In the second part, the derived RCC-CM products are presented, which are divided into two groups: (i) operational monitoring products (e. g. monthly means and anomalies) based on near-real-time environmental data records (EDRs) and (ii) climate information records (e. g. climatologies, time series, trend maps) based on long-term thematic climate data records (TCDRs) with adequate stability, accuracy and homogeneity. The products are provided as maps, <b>statistical</b> plots and <b>gridded</b> data, which are made available through the RCC-CM website (www. dwd. de/rcc-cm) ...|$|R
40|$|Statistical {{downscaling}} has mainly {{been used}} for site (point) scales to provide daily rainfall series for climate change impact studies. The objectives {{of this study are}} to compare three methods of applying statistical downscaling to catchment rainfall and evaluating their hydrological response with a hydrological model: (a) statistically downscaling to sites and then interpolating to gridded rainfall which is accumulated to catchment average rainfall; (b) statistically downscaling to catchment average rainfall directly; and (c) <b>statistical</b> downscaling to <b>grid</b> cells and then accumulating to catchment average rainfall. Results indicate that statistical downscaling can be successfully applied at catchment average and grid cell scales. All three methods of application performed similarly for a range of rainfall characteristics, with directly downscaled catchment average rainfall producing a relatively better result for extreme daily rainfall indices. However, hydrological simulation indicated that the direct downscaling of catchment average rainfall did not have any advantages over the other two downscaling application methods in terms of the runoff statistics evaluated. In addition, all three methods of downscaling application could simulate the spatial correlation of daily and annual runoff across the nine focus catchments investigated. The advantages and limitations of applying statistical downscaling to the assessment of hydrological response to climate change are also discussed. (C) 2013 Elsevier B. V. All rights reserved...|$|R
40|$|Recent {{discharge}} {{observations are}} lacking for most rivers globally. Discharge {{can be estimated}} from remotely sensed floodplain and channel inundation area, but there is currently no method that can be automatically extended to many rivers. We examined whether automated monitoring is feasible by statistically relating inundation estimates from moderate to coarse (> 0. 058) resolution remote sensing to monthly station discharge records. Inundation extents were derived from optical MODIS data and passive microwave sensors, and compared to monthly discharge records from over 8000 gauging stations and satellite altimetry observations for 442 reaches of large rivers. An automated <b>statistical</b> method selected <b>grid</b> cells to construct â€˜â€˜satellite gauging reachesâ€™â€™ (SGRs). MODIS SGRs were generally more accurate than passive microwave SGRs, but there were complementary strengths. The rivers widely varied in size, regime, and morphology. As expected performance was low (R 0. 6. The best results (R> 0. 9) were obtained for large unregulated lowland rivers, particularly in tropical and boreal regions. Relatively poor results were obtained in arid regions, where flow pulses are few and recede rapidly, and in temperate regions, where many rivers are modified and contained. While discharge variations produce clear changes in inundated area and gauge records are available {{for part of the}} satellite record, SGRs can retrieve monthly river discharge values back to around 1998 and up to present. JRC. E. 1 -Disaster Risk Managemen...|$|R
40|$|We {{introduce}} a nonstationary spatio-temporal <b>statistical</b> model for <b>gridded</b> {{data on the}} sphere. The model specifies a computationally convenient covariance structure that depends on heterogeneous geography. Widely used statistical models on a spherical domain are nonstationary for different latitudes, but stationary at the same latitude (axial symmetry). This assumption has been acknowledged to be too restrictive for quantities such as surface temperature, whose statistical behavior is influenced by large scale geographical descriptors such as land and ocean. We propose an evolutionary spectrum approach that is able to account for different regimes across the Earth's geography, and results in a more general and flexible class of models that vastly outperforms axially symmetric models and captures longitudinal patterns {{that would otherwise be}} assumed constant. The model can be estimated with in a multi-step conditional likelihood approximation that preserves the nonstationary features while allowing for easily distributed computations: we show how the fit of a data sets larger than 20 million data can be performed in less than one day on a state-of-the-art workstation. Once the parameters are estimated, it is possible to instantaneously generate surrogate runs from a common laptop. Further, the resulting estimates from the statistical model can be regarded as a synthetic description (i. e. a compression) of the space-time characteristics of an entire initial condition ensemble. Compared to traditional algorithms aiming at compressing the bit-by-bit information on each climate model run, the proposed approach achieves vastly superior compression rates...|$|R
40|$|Imbalance PRice is {{the staple}} food {{for more than}} 50 % of the worldâ€™s {{population}} 1 - 3. Reliable prediction of changes in rice yield is thus central for maintaining global food security. Here, we compare the sensitivity of rice yield to temperature increase derived from field warming experiments and three modelling approaches: statistical models, local crop models and global gridded crop models. Field warming experiments produced a substantial rice yield loss under warming, with an average temperature sensitivity of - 5. 2 Â± 1. 4 % K- 1. Local crop models gave a similar sensitivity (- 6. 3 Â± 0. 4 % K- 1), but <b>statistical</b> and global <b>gridded</b> crop models both suggest less negative impacts of warming on yields (0. 8 Â± 0. 3 % K- 1 and - 2. 4 Â± 3. 7 % 38 K- 1, respectively). Using data from field warming experiments, we further propose a conditional probability approach to constrain the large range of global gridded crop model results for the changes in future yield in response to warming {{by the end of}} the century (from - 1. 3 % K- 1 to - 9. 3 % K- 1). The constraint implies a more negative response to warming (â€“ 8. 3 Â± 1. 4 % K- 1) and reduces the spread of the model ensemble by 35 %. This yield reduction exceeds that estimated by the International Food Policy Research Institute assessment (- 4. 2 to - 6. 4 % K- 1) 4. Our study suggests that without CO 2 fertilization, effective adaptation and genetic improvement, severe rice yield losses are plausible under intensive climate warming scenarios...|$|R
40|$|This paper {{presents}} a statistical methodology whereby the probability limits associated with CFD grid resolution of inlet flow analysis {{can be determined}} which provide quantitative information {{on the distribution of}} that error over the specified operability range. The objectives of this investigation is to quantify the effects of both random (accuracy) and systemic (biasing) errors associated with grid resolution in the analysis of the Lockheed Martin Company (LMCO) N+ 2 Low Boom external compression supersonic inlet. The study covers the entire operability space as defined previously by the High Speed Civil Transport (HSCT) High Speed Research (HSR) program goals. The probability limits in terms of a 95. 0 % confidence interval on the analysis data were evaluated for four ARP 1420 inlet metrics, namely (1) total pressure recovery (PFAIP), (2) radial hub distortion (DPH/P), (3)) radial tip distortion (DPT/P), and (4)) circumferential distortion (DPC/P). In general, the resulting +/- 0. 95 delta Y interval was unacceptably large in comparison to the stated goals of the HSCT program. Therefore, the conclusion was reached that the "standard grid" size was insufficient for this type of analysis. However, in examining the statistical data, it was determined that the CFD analysis results at the outer fringes of the operability space were the determining factor in the measure of <b>statistical</b> uncertainty. Adequate <b>grids</b> are grids that are free of biasing (systemic) errors and exhibit low random (precision) errors in comparison to their operability goals. In order to be 100 % certain that the operability goals have indeed been achieved for each of the inlet metrics, the Y+/- 0. 95 delta Y limit must fall inside the stated operability goals. For example, if the operability goal for DPC/P circumferential distortion is 0. 06, then the forecast Y for DPC/P plus the 95 % confidence interval on DPC/P, i. e. +/- 0. 95 delta Y, must all be {{less than or equal to}} 0. 06...|$|R
