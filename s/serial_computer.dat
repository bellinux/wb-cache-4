94|209|Public
25|$|The {{popularity}} of the Selectric mechanism caused computer manufacturers such as Digital Equipment to support the 134.5 baud data rate on their <b>serial</b> <b>computer</b> interfaces, enabling connection of IBM Type 2741 terminals. The 2741 was available with two different seven-bit codes (Correspondence and PTT/BCD). Code choice affected the font elements which could be used. The host computer had to translate the 2741 code into the host’s internal code (usually ASCII or EBCDIC). Dedicated hardware was also built to drive Selectric printers at 134.5 baud.|$|E
50|$|PCI Express, a <b>serial</b> <b>computer</b> bus, uses message-signalled {{interrupts}} exclusively.|$|E
5000|$|Modified Modular Jack a {{variation}} used by Digital Equipment Corporation for <b>serial</b> <b>computer</b> connections ...|$|E
50|$|<b>Serial</b> <b>computers</b> {{required}} {{much less}} hardware than their parallel computing counterpart, but were much slower.|$|R
50|$|All {{computers}} before 1951, {{and most}} of the early massive parallel processing machines used a bit-serial architecture—they were <b>serial</b> <b>computers.</b>|$|R
40|$|Engineers are {{constantly}} faced with solving problems of increasing complexity and detail. They frequently rely upon numerical methods {{to solve these}} problems, and their insatiable appetite for improved performance from computing hardware has {{reached a point where}} the computational requirements exceed reasonable expectations of the performance of Von-Neumann (<b>serial)</b> <b>computers.</b> Multiple Instruction stream Multiple Data stream (MIMD) computers have been devel-oped to overcome the performance limitations of <b>serial</b> <b>computers.</b> The hardware architec-tures of MIMD computers vary considerably and are much more sophisticated than <b>serial</b> <b>computers.</b> Developing large scale software for a variety of MIMD computers is difficult and expensive. There is a need to provide tools that facilitate programming these machines. The first part of this report examines the issues that must be considered to develop those tools. The two main areas of concern were architecture independence and data man-agement. Architecture independent software facilitates software portability and improves the longevity and utility of the software product. It provides some form of insurance for th...|$|R
5000|$|To Host <b>serial</b> <b>computer</b> interface, {{connecting}} the CS2x {{to a computer}} for sequencing purposes ...|$|E
50|$|Ingrid Hermentin (born September 26, 1951 in Löwenstein, West Germany) is {{an artist}} and pioneer of <b>serial</b> <b>computer</b> graphics.|$|E
5000|$|... {{recognition}} time, in {{the software}} simulation on a <b>serial</b> <b>computer,</b> that is independent {{of the number of}} the learned patterns; ...|$|E
40|$|This paper adapts the nonsymmetric Jacobi {{iteration}} to {{the special}} structure of Hamiltonian matrices. This Hamiltonian-Jacobi algorithm uses symplectic-unitary similarity transformations to solve algebraic Riccati equations through the Hamiltonian-Schur form. It preserves Hamiltonian structure without using a condensed form. Although it converges too slowly for use on conventional <b>serial</b> <b>computers...</b>|$|R
40|$|Incomplete {{factorization}} preconditioners such as ILU, ILUT and MILU are well-known robust general-purpose {{techniques for}} solving linear systems on <b>serial</b> <b>computers.</b> However, they {{are difficult to}} parallelize efficiently. Various techniques {{have been used to}} parallelize these preconditioners, such as multicolor orderings and subdomain preconditioning. These techniques may degrade the performance and robustness of ILU preconditionings. The {{purpose of this paper is}} to perform numerical experiments to compare these techniques in order to assess what are the most effective ways to use ILU preconditioning for practical problems on <b>serial</b> and parallel <b>computers...</b>|$|R
40|$|Abstract:-In {{this paper}} we have {{proposed}} a new solution for sorting algorithms. In {{the beginning of the}} sorting algorithm for <b>serial</b> <b>computers</b> (Random access machines, or RAM’S) that allow only one operation to be executed at a time. We have investigated sorting algorithm based on a comparison network model of computation, in which many comparison operation can be performed simultaneously. Index Terms Sorting algorithms, comparison network, sorting network, the zero one principle, bitonic sorting networ...|$|R
50|$|The Gemini Guidance Computer (sometimes Gemini Spacecraft On-Board Computer (OBC)) was a digital, <b>serial</b> <b>computer</b> {{designed}} for Project Gemini, America's second manned space project. The computer, which facilitated {{the control of}} mission maneuvers, was designed by the IBM Federal Systems Division.|$|E
5000|$|Functionally, EDVAC was {{a binary}} <b>serial</b> <b>computer</b> with {{automatic}} addition, subtraction, multiplication, programmed division and automatic checking with an ultrasonic serial memory capacity of 1,000 44-bit words. EDVACs average addition time was 864 microseconds and its average multiplication time was 2,900 microseconds.|$|E
5000|$|The EDVAC was {{a binary}} <b>serial</b> <b>computer</b> with {{automatic}} addition, subtraction, multiplication, programmed division and automatic checking with an ultrasonic serial memory capacity of 1,000 44-bit words (later set to 1,024 words, thus giving a memory, in modern terms, of 5.5 kilobytes).|$|E
40|$|This paper {{describes}} the connection memory, a machine for concurrently manipulating knowledge stored in semantic networks. We need the connection memory because conventional <b>serial</b> <b>computers</b> cannot move through such networks fast enough. The connection memory sidesteps {{the problem by}} providing processing power proportional {{to the size of}} the network. Each node and link in the network has its own simple processor. These connect to form a uniform locally-connected network of perhaps a million processor/memory cell...|$|R
40|$|This paper {{describes}} serial {{and parallel}} compositional models of multiple objects with part sharing. Objects are built by part-subpart compositions and {{expressed in terms}} of a hierarchical dictionary of object parts. These parts are represented on lattices of decreasing sizes which yield an executive summary description. We describe inference and learning algorithms for these models. We analyze the complexity of this model in terms of computation time (for <b>serial</b> <b>computers)</b> and numbers of nodes (e. g., "neurons") for parallel computers. In particular, we compute the complexity gains by part sharing and its dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for <b>serial</b> <b>computers</b> but can give linear processing on parallel computers. Comment: ICLR 201...|$|R
40|$|In {{order to}} improve our ability to {{simulate}} the complex behavior of polymers, we introduce dynamical models {{in the class of}} Cellular Automata (CA). Space partitioning methods enable us to overcome fundamental obstacles to large scale simulation of connected chains with excluded volume by parallel processing computers. A highly efficient, two-space algorithm is devised and tested on both Cellular Automata Machines (CAMs) and <b>serial</b> <b>computers.</b> Preliminary results on the static and dynamic properties of polymers in two dimensions are reported...|$|R
50|$|A <b>serial</b> <b>{{computer}}</b> is {{a computer}} typified by bit-serial architecture — i.e., internally operating on one bit or digit for each clock cycle. Machines with serial main storage devices such as acoustic or magnetostrictive delay lines and rotating magnetic devices were usually serial computers.|$|E
50|$|Historically, each {{processing}} {{element in}} earlier parallel systems—like all CPUs of that time—was a <b>serial</b> <b>computer</b> {{built out of}} multiple chips.As transistor counts per chip increases, each processing element could be built out of fewer chips, and then later each multi-core processor chip could contain more processing elements.|$|E
50|$|I²C (Inter-Integrated Circuit), {{pronounced}} I-squared-C or I-two-C, is a multi-master, multi-slave, packet switched, single-ended, <b>serial</b> <b>computer</b> bus {{invented by}} Philips Semiconductor (now NXP Semiconductors). It is typically used for attaching lower-speed peripheral ICs to processors and microcontrollers in short-distance, intra-board communication. Alternatively I²C is spelled I2C (pronounced I-two-C) or IIC (pronounced I-I-C).|$|E
40|$|TREAT: A New and Efficient Match Algorithm for AI Production Systems {{describes}} the architecture and software systems embodying the DADO machine, a parallel tree-structured computer {{designed to provide}} significant performance improvements over <b>serial</b> <b>computers</b> of comparable hardware complexity in the execution of large expert systems implemented in production system form. This book focuses on TREAT as a match algorithm for executing production systems that is presented and comparatively analyzed with the RETE match algorithm. TREAT, originally designed specifically for the DADO machine architec...|$|R
50|$|Originally, all {{electronic}} <b>computers</b> were <b>serial</b> (single-bit) <b>computers.</b> The first {{electronic computer}} {{that was not}} a serial computer—the first bit-parallel computer—was the 16-bit Whirlwind from 1951.|$|R
40|$|Much of the {{progress}} in the fields constituting cognitive science has been based upon the use of explicit information processing models, almost exclusively patterned after conventional <b>serial</b> <b>computers.</b> An extension of these ideas to massively parallel, connectianist models appears to offer a number of advantages. After a preliminary discussion, this paper introduces a general connectionist model and considers {{how it might be}} used in cognitive science. Among the issues addressed are: stability and noise-sensitivity, distributed decisionmaking, time and sequence problems, and the representation of complex concepts. 1...|$|R
5000|$|The 2650 {{was also}} used in some large items of {{equipment}} such as the Tektronix 8540, a microprocessor software development system which supported various in-circuit emulator, trace memory and logic analyser cards for real-time debugging of microprocessor systems, as practiced in the 1980s. The 2650 provided the base operating system functions, data transfer, and interface to a host computer or <b>serial</b> <b>computer</b> terminal.|$|E
5000|$|A text {{terminal}}, or {{often just}} terminal (sometimes text console) is a <b>serial</b> <b>computer</b> interface {{for text entry}} and display. Information is presented as an array of pre-selected formed characters. When such devices use a video display such as a cathode-ray tube, they are called a [...] "video display unit" [...] or [...] "visual display unit" [...] (VDU) or [...] "video display terminal" [...] (VDT).|$|E
50|$|Serial {{communication}} {{is used for}} all long-haul communication and most computer networks, where the cost of cable and synchronization difficulties make parallel communication impractical. <b>Serial</b> <b>computer</b> buses are becoming more common even at shorter distances, as improved signal integrity and transmission speeds in newer serial technologies have begun to outweigh the parallel bus's advantage of simplicity (no need for serializer and deserializer, or SerDes) and to outstrip its disadvantages (clock skew, interconnect density). The migration from PCI to PCI Express is an example.|$|E
40|$|AbstractIn this paper, it {{is shown}} how to adapt an {{existing}} package (VODE) for solving systems of ordinary differential equations on <b>serial</b> <b>computers</b> to distributed memory parallel computers. The approach taken {{is based on}} waveform relaxation in which the problem is decomposed into a sequence of subproblems which are then solved independently using VODE on each processor. Communication between subtasks is provided by a generic software environment p 4. This approach allows the development of general purpose parallel software for ODEs which is both reliable and portable...|$|R
40|$|In {{this paper}} we combine finite {{difference}} approximations (for spatial derivatives) and collocation techniques (for the time component) to numerically solve the two dimensional heat equation. We employ respectively a second-order and a fourth-order schemes for the spatial derivatives and the discretization method {{gives rise to}} a linear system of equations. We show that the matrix of the system is non-singular. Numerical experiments carried out on <b>serial</b> <b>computers,</b> show the unconditional stability of the proposed method and the high accuracy achieved by the fourth-order scheme...|$|R
40|$|Parallel Virtual Machine (PVM) is a widely-used {{software}} {{system that allows}} a heterogeneous set of parallel and <b>serial</b> UNIX-based <b>computers</b> to be programmed as a single distributed-memory parallel machine. In this paper, an extension to PVM to support dynamic process migration is presented. Support for migration is important in general-purpose workstation environments since it allows parallel computations to co-exist with other applications, using idle-cycles as they become available and off-loading from workstations when {{they are no longer}} free. A description and evaluation of the design and implementation of the prototype Migratable PVM system is presented together with some performance results. 1 Introduction PVM [1, 2, 3] is a {{software system}} that allows a heterogeneous network of parallel and <b>serial</b> <b>computers</b> to be programmed as a single computational resource. This resource appears to the application programmer as a potentially large distributed-memory virtual computer. Such a s [...] ...|$|R
50|$|The {{popularity}} of the Selectric mechanism caused computer manufacturers such as Digital Equipment to support the 134.5 baud data rate on their <b>serial</b> <b>computer</b> interfaces, enabling connection of IBM Type 2741 terminals. The 2741 was available with two different seven-bit codes (Correspondence and PTT/BCD). Code choice affected the font elements which could be used. The host computer had to translate the 2741 code into the host’s internal code (usually ASCII or EBCDIC). Dedicated hardware was also built to drive Selectric printers at 134.5 baud.|$|E
50|$|PCI Express (Peripheral Component Interconnect Express), officially {{abbreviated}} as PCIe or PCI-e, is {{a high-speed}} <b>serial</b> <b>computer</b> expansion bus standard, designed {{to replace the}} older PCI, PCI-X, and AGP bus standards. PCIe has numerous improvements over the older standards, including higher maximum system bus throughput, lower I/O pin count and smaller physical footprint, better performance scaling for bus devices, a more detailed error detection and reporting mechanism (Advanced Error Reporting, AER), and native hot-plug functionality. More recent revisions of the PCIe standard provide hardware support for I/O virtualization.|$|E
5000|$|Several {{variants}} of work stealing have been proposed. The randomized variant due to Blumofe and Leiserson executes a parallel computation in expected time [...] on [...] processors; here, [...] is the work, or {{the amount of}} time required to run the computation on a <b>serial</b> <b>computer,</b> and [...] is the span, {{the amount of time}} required on an infinitely parallel machine. This means that, in expectation, the time required is at most a constant factor times the theoretical minimum. However, the running time (in particular, the number of steals executed) can be exponential in [...] in the worst case. A localized variant, in which a processor attempts to steal back its own work whenever it is free, has also been analyzed theoretically and practically.|$|E
40|$|In {{this paper}} it is shown how to adapt an {{existing}} package (VODE) for solving systems of ordinary differential equations on <b>serial</b> <b>computers</b> to distributed memory parallel computers. The approach taken {{is based on}} waveform relaxation in which the problem is decomposed into a sequence of subproblems and which are then solved independently using VODE on each processor. Communication between subtasks is provided by a generic software environment p 4. This approach allows the development of general purpose parallel software for ODEs which is both reliable and portable...|$|R
5000|$|The cost, size, {{and power}} {{consumption}} of electronic circuitry was relatively high throughout the infancy {{of the information}} age. Consequently, all <b>serial</b> <b>computers</b> and many early computers, such as the PDP-8, had a simple ALU that operated on one data bit at a time, although they often presented a wider word size to programmers. One of the earliest computers to have multiple discrete single-bit ALU circuits was the 1948 Whirlwind I, which employed sixteen of such [...] "math units" [...] to enable it to operate on 16-bit words.|$|R
25|$|The D17B is a {{synchronous}} <b>serial</b> general-purpose digital <b>computer.</b>|$|R
