164|770|Public
5000|$|View server <b>statistical</b> <b>variables,</b> {{and average}} values per hour & second ...|$|E
50|$|On April 28, 1934, {{he earned}} his Doctorate in Mathematics from the University of Bucharest, {{in front of a}} {{commission}} consisting of Dimitrie Pompeiu, as chairman, Anton Davidoglu, and Octav Onicescu. The subject of his thesis was On the general properties of interdependent <b>statistical</b> <b>variables.</b>|$|E
50|$|Path {{coefficients}} are standardized {{versions of}} linear regression weights {{which can be}} used in examining the possible causal linkage between <b>statistical</b> <b>variables</b> in the structural equation modeling approach. The standardization involves multiplying the ordinary regression coefficient by the standard deviations of the corresponding explanatory variable: these can then be compared to assess the relative effects of the variables within the fitted regression model. The idea of standardization can be extended to apply to partial regression coefficients.|$|E
5000|$|... {{and it is}} {{for this}} reason that the z {{parameter}} is the preferred <b>statistical</b> <b>variable</b> to use in circular statistical analysis rather than the measured angles [...] This suggests, and it is shown below, that the wrapped distribution function may itself be expressed as a function of z so that: ...|$|R
30|$|First, the p {{values of}} the four {{candidate}} distributions were compared. If p[*]>[*] 0.05, it indicated that the corresponding distribution was able to fit the failure data. The distributions with good fitting results were preserved, and then the A-D <b>statistical</b> <b>variable</b> was calculated. The distribution with a minimum A-D value {{was chosen as the}} optimal distribution model.|$|R
5000|$|Using beta as {{a measure}} of {{relative}} risk has its own limitations. Most analyses consider only the magnitude of beta. Beta is a <b>statistical</b> <b>variable</b> and should be considered with its statistical significance (R square value of the regression line). Higher R square value implies higher correlation and a stronger relationship between returns of the asset and benchmark index.|$|R
5000|$|... #Caption: View in {{performance}} space with performance acceptance region A {{and distribution of}} performance (red) under statistical and operation variations from which WCD can be calculated. Note: Often the <b>statistical</b> <b>variables</b> itself are not correlated but the performances do so (acc. to the stretch and rotation of the ellipsoids). Also often the specifications are set by just upper or lower limits - so being straight lines. If we would plot the statistical variable space, the spec-limits would typically become nonlinear shapes. WCD makes use of both ways of looking to the yield problem.|$|E
5000|$|For the {{discussion}} of any case, {{more complex than the}} above-mentioned example, see Antreich et al., 1993. In design environments the WCD calculation is not done analytically but in a numerical way. Most WCD algortihms start with a short Monte-Carlo analysis, and use then optimization techniques to find the point in the statistical variable space which hits the specification boarder with minimum vector length. For cases with many <b>statistical</b> <b>variables,</b> there is usually a filtering step after the MC run. The more points are spent in the MC run, the better the optimization starting point; and the more reliable optional the filtering step.|$|E
50|$|WCD allows {{to simplify}} yield problems, {{but it is}} not the only way to do this. A simpler way is not to find the margin in terms of sigma in the space of <b>statistical</b> <b>variables,</b> but just to {{evaluate}} the performance margin(s) itself (like the Cpk does). The worst-case performance margin WPM is much easier to obtain, but here the problem is usually, that although your <b>statistical</b> <b>variables</b> might be normal Gaussian distributed, the performances will be often not follow that distribution type, usually it will be an unknown more difficult distribution. For this reason the performance margin in terms of sigma is at best a relative criteria for yield optimization. This often leads to pure Monte-Carlo methods for solving the WPM problem, whereas WCD allows a more elegant mathematical treatment, only partially based on Monte-Carlo.Random Monte-Carlo becomes inefficient for high yield estimation if the distribution type is uncertain. One method to speed-up MC is using non-random sampling methods like Latin hyper-cube or low-discrepancy sampling. However, the speed-up is quite limited in real design problems. A promising newer technique is e.g. scale-sigma sampling. With SSS there is a higher chance to hit the fail region and more samples in that will lead to a more stable statistic, thus tighter confidence intervals. In opposite to importance sampling or WCD SSS makes no assumptions on the fail boundary shape or number of such fail regions, so it is (in relation to other methods) most efficient in cases with many variables, strong nonlinearity, difficult and many specifications.|$|E
5000|$|<b>Statistical</b> <b>Variable</b> Bitrate: Ensures optimal video quality {{throughout}} the film by allocating a higher encoding budget to high detail and high motion {{segments of the}} film, while conserving the budget during slower sequences. With peaks as high as 20Mbit/s and as low as 2 Mbit/s, this process allows for the highest possible video quality streaming over a broadband Internet connection.|$|R
50|$|A {{choropleth map}} (from Greek χῶρος ("area/region") + πλῆθος ("multitude")) is a {{thematic}} map in which areas are shaded or patterned {{in proportion to}} the measurement of the <b>statistical</b> <b>variable</b> being displayed on the map, such as population density or per-capita income. Choropleth maps {{can also be used to}} display nominal data such as country names on a world map or most popular car model per region.|$|R
40|$|The paper {{addresses}} {{the problem of}} carrier and timing synchronization for MSK modulation. Based on a second-order and a fourth-order <b>statistical</b> <b>variable,</b> an efficient data-aided algorithm is proposed to estimate the frequency offset and timing error, respectively. Numerical {{results show that the}} proposed algorithm achieves good performance in both AWGN and Rayleigh fading channels and it outperforms previous data-aided synchronization algorithms, even in fading channels. published_or_final_versio...|$|R
50|$|Bivariate mapping is a {{comparatively}} recent graphical method. A bivariate choropleth map uses color {{to solve a}} problem of representation in four dimensions; two spatial dimensions — longitude and latitude — and two <b>statistical</b> <b>variables.</b> Take the example of mapping population density and average daily maximum temperature simultaneously. Population could be given a colour scale of black to green, and temperature from blue to red. Then an area with low population and low temperature would be dark blue, high population and low temperature would be cyan, high population and high temperature would be yellow, while low population and high temperature would be dark red. The eye can quickly see potential relationships between these variables.|$|E
50|$|However, Bell's theorem {{does not}} apply to all {{possible}} philosophically realist theories. It is a common misconception that quantum mechanics is inconsistent with all notions of philosophical realism. Realist interpretations of quantum mechanics are possible, although as discussed above, such interpretations must reject either locality or counter-factual definiteness. Mainstream physics prefers to keep locality, while striving also to maintain a notion of realism that nevertheless rejects counter-factual definiteness. Examples of such mainstream realist interpretations are the consistent histories interpretation and the transactional interpretation (first proposed by John G. Cramer in 1986). Fine's work showed that, taking locality as a given, there exist scenarios in which two <b>statistical</b> <b>variables</b> are correlated in a manner inconsistent with counter-factual definiteness, and that such scenarios are no more mysterious than any other, {{despite the fact that the}} inconsistency with counter-factual definiteness may seem 'counter-intuitive'.|$|E
30|$|Feature {{parameters}} are usually extracted by frame. Since a single frame contains less information, most researchers use feature parameters to calculate <b>statistical</b> <b>variables</b> in multiple frames for emotion recognition tasks. In this paper, five kinds of features are adopted, including MFCC, energy, Fourier coefficients, pitch frequency, and zero-crossing rate, and five <b>statistical</b> <b>variables</b> (i.e., maximum, minimum, mean, standard deviation, and median) of multi-frame features are calculated {{and applied to}} recognition tasks.|$|E
40|$|A new {{atmospheric}} humidity variable called the modified relative humidity (MRH) is proposed based on {{properties of the}} Johnson’s SB distribution function. The frequency distribution of MRH can be roughly approximated by the normal distribution, while other variables such as relative humidity and the water vapor mixing ratio cannot. This characteristic suggests that MRH is convenient for <b>statistical</b> <b>variable</b> controls such as data assimilation and climatological grid data controls. Super-saturation and negative water vapor states induced by positive and negative humidity increments are avoided by using MRH. Three types of MRH, Types-I, -II, and -III, were examined. Type-III, with three fixed parameters, was the best function for approximating to the normal distribution. However, Types-I and -II, each with two fixed parameters, were beneficial for stable <b>statistical</b> humidity <b>variable</b> estimations...|$|R
50|$|However a very {{important}} limitation is on just finding the WCD point, i.e. the set of <b>statistical</b> <b>variable</b> values which hits the spec-region, because even small real-world problems can have thousands (instead of one or two) of such variables (plus the condition variables like temperature, supply voltage, etc.). This makes a slow brute-force search impractical, and very robust optimizers are needed to find the WCDs (e.g. even {{in the presence of}} local optima or split fail regions, etc.).|$|R
50|$|Color is a {{very useful}} {{attribute}} to depict different features on a map. Typical uses of color include displaying different political divisions, different elevations, or different kinds of roads. A choropleth map is a thematic map in which areas are colored differently to show the measurement of a <b>statistical</b> <b>variable</b> being displayed on the map. The choropleth map provides {{an easy way to}} visualize how a measurement varies across a geographic area or it shows the level of variability within a region.|$|R
40|$|Recent {{technological}} advances {{have made it}} feasible to produce full color statistical maps on computer-controlled display systems. This has caused an appraisal of the use of color to represent <b>statistical</b> <b>variables,</b> {{and the development of a}} theoretical structure for the choice of suitable univariate and bivariate map coloring schemes. Realization of such schemes in an intuitive and controlled way is important to the comprehension of <b>statistical</b> <b>variables</b> from maps. Therefore, a method of generating specific color sequences within the framework of a uniform color space, allowing for the intuitive specification of color sequences and for their realization on various display systems, is presente...|$|E
40|$|This paper {{introduces}} a statistical model {{by using the}} statistical methods in 2 G,GSM communication system. Multiple regression formula is to calculate path loss. It is assumed that hb,W and α are three <b>statistical</b> <b>variables.</b> We use nakagami distribution to model hb,W and uniform distribution to model α...|$|E
40|$|This paper {{describes}} {{the design of}} facilities for doing simulation based performance analysis. The performance facilities have three main components: functions for generating random numbers from different distributions, <b>statistical</b> <b>variables</b> for collecting different data while simulating, and reporting facilities for generating output from the <b>statistical</b> <b>variables.</b> We also describe {{the integration of the}} performance facilities into the Design/CPN tool. To illustrate the usability of the performance facilities, we give a nontrivial example of simulation based performance analysis by analysing a multiaccess protocol. Keywords. Performance analysis, Coloured Petri Nets, simulation, random distributions, networks and multiaccess protocol. 1 Introduction Most applications of Coloured Petri Nets (CP-nets) [Jen 92,Jen 94,Jen 97] are used to investigate the logical correctness of a system. This means that we consider the dynamic properties and the functionality of the system. However, CP-net [...] ...|$|E
40|$|This report {{demonstrates}} the successful application of <b>statistical</b> <b>variable</b> selection techniques to fit splines. Major emphasis {{is given to}} knot selection, but order determination is also discussed. Two FORTRAN backward elimination programs using the B-spline basis were developed, and the one for knot elimination is compared in detail with two other spline-fitting methods and several statistical software packages. An example is also given for the two-variable case using a tensor product basis, with a theoretical discussion of the difficulties of their use...|$|R
40|$|We {{develop an}} {{approach}} to coherence between two scalar harmonic light vibrations derived from the ensemble interpretation of statistical optics. Coherence {{is presented as a}} <b>statistical</b> <b>variable</b> itself {{that turns out to be}} the phase difference between the two vibrations. This provides a natural and simple extension of second-order coherence to cover more complicated situations. This includes in a single formalism both classic and quantum light states, allowing the most accurate interferometric measurements, even if they are incoherent according to the standard second-order approach...|$|R
5000|$|Various {{factors have}} been shown to lower {{incidence}} of divorce among church members, including church activity. Heaton says that, “Overall, church attendance is associated with lower rates of nonmarriage and divorce, and higher probabilities of remarriage after divorce.” [...] Studies suggest that the most important <b>statistical</b> <b>variable</b> affecting marital dissolution rates of Latter-day Saints is marriage in the temple, with some studies finding that non-temple marriages entered into by Latter-day Saints are almost five times more likely to result in divorce than are temple marriages.|$|R
40|$|Multivariate {{dimension}} reduction schemes {{could be}} very useful in {{limiting the number of}} random <b>statistical</b> <b>variables</b> needed to represent distributed wind power spatial diversity in transmission integration studies. In this paper, principal component analysis (PCA) is applied to the covariance matrix of distributed wind power data from existing Irish wind farms, with the eigenvector/eigenvalue analysis generating a lower number of uncorrelated alternative variables. It is shown that though uncorrelated, these wind components may not necessarily be statistically independent however. A sample application of PCA combined with multivariate probability discretization is also outlined in detail. In that case study, the capability of PCA to reduce the number and prioritize the order of the alternative <b>statistical</b> <b>variables</b> is key to potential wind power production costing simulation efficiency gains, when compared to exhaustive multiyear time series load flow investigations...|$|E
40|$|This {{bachelor}} thesis {{deals with}} the analysis of economic indicators using <b>statistical</b> <b>variables</b> in a particular organization. It {{is based on the}} results of the analysis of economic indicators of the organization. This paper presents the theory used in the implementation of the specific proposal. Thesis also contains a simple application created in Visual Basic...|$|E
40|$|A {{qualitative}} probabilistic {{network is}} a graphical {{model of the}} probabilistic influences among a set of <b>statistical</b> <b>variables</b> in which each influence {{is associated with a}} qualitative sign. A non-monotonic influence between two variables is associated with the ambiguous sign ' ?', which indicates that the actual sign of the influence depends {{on the state of the}} network...|$|E
40|$|Abstract Background In {{recent years}} {{there has been}} an {{increase}} in the use of population-based linked data. However, there is little literature that describes the method of linked data preparation. This paper describes the method for merging data, calculating the <b>statistical</b> <b>variable</b> (SV), recoding psychiatric diagnoses and summarizing hospital admissions for a perinatal psychiatric study. Methods The data preparation techniques described in this paper are based on linked birth data from the New South Wales (NSW) Midwives Data Collection (MDC), the Register of Congenital Conditions (RCC), the Admitted Patient Data Collection (APDC) and the Pharmaceutical Drugs of Addiction System (PHDAS). Results The master dataset is the meaningfully linked data which include all or major study data collections. The master dataset can be used to improve the data quality, calculate the SV and can be tailored for different analyses. To identify hospital admissions in the periods before pregnancy, during pregnancy and after birth, a <b>statistical</b> <b>variable</b> of time interval (SVTI) needs to be calculated. The methods and SPSS syntax for building a master dataset, calculating the SVTI, recoding the principal diagnoses of mental illness and summarizing hospital admissions are described. Conclusion Linked data preparation, including building the master dataset and calculating the SV, can improve data quality and enhance data function. </p...|$|R
40|$|One of {{the main}} {{propagation}} effects on interference between adjacent Earth-space paths {{is considered to be}} the differential rain attenuation. In the present paper, an existing method for the prediction of the above <b>statistical</b> <b>variable</b> is properly modified by taking into account more realistic considerations for the rain height. The results of the modified method are compared with available simulation data in Montreal area. The influence of the novel assumptions for the rain height on the differential attenuation, as function of the various parameters of the problem, is also examined...|$|R
40|$|Abstract Background The Indigenous {{population}} of Australia was estimated as 2. 5 % and under-reported. The {{aim of this}} study is to improve statistical ascertainment of Aboriginal women giving birth in New South Wales. Methods This study was based on linked birth data from the Midwives Data Collection (MDC) and the Registry of Births Deaths and Marriages (RBDM) of New South Wales (NSW). Data linkage was performed by the Centre for Health Record Linkage (CHeReL) for births in NSW for the period January 2001 to December 2005. The accuracy of maternal Aboriginal status in the MDC and RBDM was assessed by consistency, sensitivity and specificity. A new <b>statistical</b> <b>variable,</b> ASV, or Aboriginal <b>Statistical</b> <b>Variable,</b> was constructed based on Indigenous identification in both datasets. The ASV was assessed by comparing numbers and percentages of births to Aboriginal mothers with the estimates by capture-recapture analysis. Results Maternal Aboriginal status was under-ascertained in both the MDC and RBDM. The ASV significantly increased ascertainment of Aboriginal women giving birth and decreased the number of missing cases. The proportion of births to Aboriginal mothers in the non-registered birth group was significantly higher than in the registered group. Conclusions Linking birth data collections is a feasible method to improve the statistical ascertainment of Aboriginal women giving birth in NSW. This has ramifications for the ascertainment of babies of Aboriginal mothers and the targeting of appropriate services in pregnancy and early childhood. </p...|$|R
40|$|This {{research}} {{reflects the}} importance in statistics of the territorial indexes numbers which express {{the dynamic of}} the <b>statistical</b> <b>variables</b> in territorial profile. The {{purpose of this paper}} consists in to reflect over of the economic life, how we will can to apply some important standardization methods, such as Iughenburgh method, Kazineţ method named the standard coefficients method and Peregudov method...|$|E
40|$|In this work, {{problem solving}} {{procedures}} about <b>statistical</b> <b>variables</b> association using microcomputers are analysed. The effect of several didactical variables on these procedures is also studied. The studied population consisted of 18 prospective teachers {{who had been}} previously trained for 7 weeks. By studying the students' replies we determined their ideas of the scope and {{meaning of the term}} «association» and derived criteria for construction of new didactic situations...|$|E
40|$|Summary: The thesis {{presents}} {{a method of}} regional assessment of vehicles ’ exploitation safety in conditions of heterogeneity of <b>statistical</b> <b>variables.</b> Executed researches concern results of car thefts {{on the basis of}} the data from Regional Police Stations. As an indicator a distribution series was adopted of a coefficient of car theft growth in selected regions of the country. Key words: management of safety, vehicles ’ exploitation, motor crime, regionalization, cluster analysis...|$|E
5000|$|... • Use {{appropriate}} <b>statistical</b> {{techniques in}} <b>variable</b> and attribute control charts and in sampling tables for continuous improvement.|$|R
30|$|Data were analysed, and {{the results}} are {{expressed}} as mean ± standard deviation, median (interquartile range), or percentage. For <b>statistical</b> analysis, <b>variables,</b> which followed a Gaussian distribution, were evaluated for significance by using the t test. Categorical variables were evaluated by the Chi-square test of contingency.|$|R
40|$|We {{present and}} analyze a lattice {{model of a}} disordered {{dielectric}} material. In the model, the local polarizability is a quenched <b>statistical</b> <b>variable.</b> Using a reaction field approach, the dielectric response of the model can be cast {{in terms of an}} effective Hamiltonian for a finite primary system coupled to its effective average medium determined self-consistently. A real space renormalization group analysis is carried out by recursively increasing the size of the primary system. The analysis determines the length scale dependence of the local polarizability distribution. For the case of isotropic disorder considered in this paper, we show that the width of the distribution decays algebraically with increasing lattice spacing. We also compute the distribution of solvation and reorganization energies pertinent to kinetics of electron transfer...|$|R
