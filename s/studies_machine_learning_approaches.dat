0|10000|Public
40|$|Current {{methods of}} {{cardiovascular}} risk assessment are performed using health factors {{which are often}} based on the Framingham study. However, these methods have significant limitations due to their poor sensitivity and specificity. We have compared the parameters from the Framingham equation with linear regression analysis to establish the effect of training of {{the model for the}} local database. Support vector machine was used to determine the effectiveness of <b>machine</b> <b>learning</b> <b>approach</b> with the Framingham health parameters for risk assessment of cardiovascular disease (CVD). The result shows that while linear model trained using local database was an improvement on Framingham model, SVM based risk assessment model had high sensitivity and specificity of prediction of CVD. This indicates that using the health parameters identified using Framingham <b>study,</b> <b>machine</b> <b>learning</b> <b>approach</b> overcomes the low sensitivity and specificity of Framingham model...|$|R
40|$|Subcellular {{localization}} {{is a key}} functional {{characteristic of}} proteins. Optimally combining available information {{is one of the}} key challenges in today 2 ̆ 7 s knowledge-based subcellular localization prediction <b>approaches.</b> This <b>study</b> explores <b>machine</b> <b>learning</b> <b>approaches</b> for the prediction of protein subcellular localization that use resources concerning Gene Ontology and secondary structures. Using the spectrum kernel for feature representation of amino acid sequences and secondary structures, we explore an SVM-based learning method that classifies six subcellular localization sites: endoplasmic reticulum, extracellular, Golgi, membrane, mitochondria, and nucleus...|$|R
40|$|Overburden {{stripping}} in open cast coal mines is extensively {{carried out by}} walking draglines. Draglines’ unavailability and unexpected failures result in delayed productions and increased maintenance and operating costs. Therefore, achieving high availability of draglines {{plays a crucial role}} for increasing economic feasibility of mining projects. Applications of methodologies which can forecast the failure type of dragline based on the available failure data not only help to reduce the maintenance and operating costs but also increase the availability and the production rate. In this <b>study,</b> <b>Machine</b> <b>Learning</b> <b>approaches</b> have been applied for data which has been gathered from an operating coal mine in Turkey. The study methodology consists of three algorithms as: i) implementation of K-Nearest Neighbors, ii) implementation of Multi-Layer Perceptron, and iii) implementation of Radial Basis Function. The algorithms have been utilized for predicting the draglines’ failure types. In this sense, the input data, which are mean time-to-failure, and the output data, failure types, have been fed to the algorithms. The regression analysis of methodologies have been compared and showed the K- Nearest Neighbors has a higher rate of regression which is around 70 percent. Thus, the K-Nearest Neighbor algorithm can be applied in order to preventive components replacement which causes to minimized preventive and corrective cost parameters. The accurate prediction of failure type, indeed, causes to optimized number of inspections. The novelty of this study is application of <b>machine</b> <b>learning</b> <b>approaches</b> in draglines’ reliability subject for first time...|$|R
40|$|Owing to the {{complexity}} and variability of metagenomic <b>studies,</b> modern <b>machine</b> <b>learning</b> <b>approaches</b> have seen increased usage to answer a variety of question encompassing {{the full range of}} metagenomic NGS data analysis. We review here the contribution of <b>machine</b> <b>learning</b> techniques for the field of metagenomics, by presenting known successful approaches in a unified framework. This review focuses on five important metagenomic problems: OTU-clustering, binning, taxonomic profling and assignment, comparative metagenomics and gene prediction. For each of these problems, we identify the most prominent methods, summarize the <b>machine</b> <b>learning</b> <b>approaches</b> used and put them into perspective of similar methods. We conclude our review looking further ahead at the challenge posed by the analysis of interactions within microbial communities and different environments, in a field one could call "integrative metagenomics"...|$|R
30|$|The <b>machine</b> <b>learning</b> <b>approach</b> {{has often}} been adopted in the literature. Many studies have {{attempted}} to apply the <b>machine</b> <b>learning</b> <b>approach</b> as a base to build a classification model. These studies point out that adopting this method leads to outstanding prediction accuracy. Several <b>studies</b> applying a <b>machine</b> <b>learning</b> <b>approach</b> (e.g. SVM, DT, NN, etc.) to GCD, indicating that these approaches are able to forecast the GC status of businesses and provide useful financial data for the GC issue (Brabazon and Keenan 2004; Koh and Low 2004; Martens et al. 2008; Mokhatab et al. 2011; Salehi and Fard 2013; Yeh et al. 2014).|$|R
40|$|Abstract. We {{propose a}} highly {{automated}} {{approach to the}} point correspondence problem for anatomical shapes in medical images. Manual landmarking is performed on a small subset of the shapes in the <b>study,</b> and a <b>machine</b> <b>learning</b> <b>approach</b> is used to elucidate the characteristic shape and appearance features at each landmark. A classifier trained using these features defines a cost function that drives key landmarks to anatomically meaningful locations after MDL-based correspondence establishment. Results are shown for artificial examples as well as real data. ...|$|R
40|$|Email overload, {{even after}} spam filtering, {{presents}} a serious productivity challenge for busy professionals and executives. One solution is automated prioritization of incoming emails {{to ensure the}} most important are read and processed quickly, while others are processed later as/if time permits in declining priority levels. This paper presents a <b>study</b> of <b>machine</b> <b>learning</b> <b>approaches</b> to email prioritization into discrete levels, comparing ordinal regression versus classier cascades. Given the ordinal nature of discrete email priority levels, SVM ordinal regression {{would be expected to}} perform well, but surprisingly a cascade of SVM classifiers significantly outperforms ordinal regression for email prioritization. In contrast, SVM regression performs well [...] better than classifiers [...] on selected UCI data sets. This unexpected performance inversion is analyzed and results are presented, providing core functionality for email prioritization systems...|$|R
40|$|AbstractPrediction {{of surface}} {{roughness}} is always considered {{important in the}} manufacturing field. A product may require a particular roughness that may be specified by the designer for various reasons, either functional requirement or aesthetic appeal. While modern manufacturing systems and machines have always contributed towards better control of surface quality, better computational facilities {{and the availability of}} newer algorithms attract researchers to understand the prediction of quality in a better manner. In this paper, prediction of surface roughness by multiple regression analysis is presented. The predictors are cutting parameters, tool wear and the statistical parameters extracted from the vibration signals of a turning centre. The contribution of various statistical parameters in prediction of surface roughness is <b>studied.</b> A <b>Machine</b> <b>learning</b> <b>approach</b> using feature reduction using principle component analysis is attempted to achieve higher predictability and low computational effort...|$|R
40|$|As {{a result}} of the {{advances}} in skin imaging technology and the development of suitable image processing techniques, during the last decade, there has been a significant increase of interest in the computer-aided diagnosis of skin cancer. Dermoscopy is a non-invasive skin imaging technique which permits visualization of features of pigmented melanocytic neoplasms that are not discernable by examination with the naked eye. One of the useful features in dermoscopic diagnosis is the blue-white veil (irregular, structureless areas of confluent blue pigmentation with an overlying white “ground-glass ” film) which is mostly associated with invasive melanoma. In this preliminary <b>study,</b> a <b>machine</b> <b>learning</b> <b>approach</b> to the detection of blue-white veil areas in dermoscopy images is presented. The method involves pixel classification based on relative and absolute color features using a decision tree classifier. Promising results were obtained on a set of 224 dermoscopy images...|$|R
40|$|This paper {{presents}} a fingerprinting method based on equivalence classes. An equivalence class {{is composed of}} a reference image and all its variations (or replicas). For each reference image, a decision function is built. The latter determines if a given image belongs to its corresponding equivalence class. This function is built in three steps: synthesis, projection, and analysis. In the first step, the reference image is replicated using different image operators (like JPEG compression, average filtering, etc). During the projection step, the replicas are projected onto a distance space. In the final step, the distance space is analyzed, using <b>machine</b> <b>learning</b> algorithms, and the decision function is built. In this <b>study,</b> three <b>machine</b> <b>learning</b> <b>approaches</b> are compared: orthotope, support vectors machine (SVM), and support vectors data description (SVDD). The orthotope is a computationally efficient ad-hoc method. It consists in building a generalized rectangle in the distance space. The SVM and SVDD are two more general learning algorithms...|$|R
40|$|This <b>study</b> {{presents}} a <b>machine</b> <b>learning</b> <b>approach</b> applied to ElectroEnchephaloGraphic (EEG) response {{in a group}} of subjects when exposed to a controlled olfactory stimulation experiment. In the literature, in fact, there are controversial results on EEG response to odorants. This study proposes a robust leave-one-subject-out classification method to recognize features extracted from EEG signals belonging to pleasant or unpleasant olfactory stimulation classes. An accuracy of 75 % has been achieved {{in a group of}} 32 subjects. Moreover a set of features extracted from lateral electrodes emphasized that right and left hemispheres behave differently when the subjects are exposed to pleasant or unpleasant odours stimuli...|$|R
40|$|Imagine {{an agent}} that {{performs}} tasks according to different strategies. The goal of Behavioral Recognition (BR) {{is to identify}} which of the available strategies is the one {{being used by the}} agent, by simply observing the agent?s actions and the environmental conditions during a certain period of time. The goal of Behavioral Cloning (BC) is more ambitious. In this last case, the learner must be able to build a model of the behavior of the agent. In both settings, the only assumption is that the learner has access to a training set that contains instances of observed behavioral traces for each available strategy. This paper <b>studies</b> a <b>machine</b> <b>learning</b> <b>approach</b> based on Probabilistic Finite Automata (PFAs), capable of achieving both the recognition and cloning tasks. We evaluate the performance of PFAs {{in the context of a}} simulated learning environment (in this case, a virtual Roomba vacuum cleaner robot), and compare it with a collection of other <b>machine</b> <b>learning</b> <b>approaches.</b> This work was partially supported by project PAC::LFO (MTM 2014 - 55262 -P) of Programa Estatal de Fomento de la Investigación Científica y Técnica de Excelencia, Ministerio de Ciencia e Innovación (MICINN), Spain, and by the National Science Foundation (NSF) project SCH- 1521943, USA...|$|R
40|$|Several {{studies have}} shown how to {{approximately}} predict public opinion, such as in political elections, by analyzing user activities in blogging platforms and on-line social networks. The task is challenging for several reasons. Sample bias and automatic understanding of textual content are two of several non trivial issues. In this work we study how Twitter can provide some interesting insights concerning the primary elections of an Italian political party. State-of-the-art approaches rely on indicators based on tweet and user volumes, often including sentiment analysis. We investigate how to exploit and improve those indicators {{in order to reduce}} the bias of the Twitter users sample. We propose novel indicators and a novel content-based method. Furthermore, we <b>study</b> how a <b>machine</b> <b>learning</b> <b>approach</b> can <b>learn</b> correction factors for those indicators. Experimental results on Twitter data support the validity of the proposed methods and their improvement over the state of the art...|$|R
40|$|Abstract. In {{the paper}} we <b>study</b> an {{evolutionary}} <b>machine</b> <b>learning</b> <b>approach</b> to data mining and knowledge discovery {{based on the}} classification rules induction. A method for automatic rules induction called AREX using evolutionary induction of decision trees and automatic programming is introduced. The proposed algorithm is applied to a cardiovascular dataset consisting of different groups of attributes which should possibly reveal the presence of some specific cardiovascular problems in young patients. A case study is presented that shows the use of AREX for the classification of patients and for discovering possible new medical knowledge from the dataset. The defined knowledge discovery loop comprises a medical expert’s assessment of induced rules to drive the evolution of rule sets towards more appropriate solutions. The final result is {{the discovery of a}} possible new medical knowledge in the field of pediatric cardiology. Index terms: <b>machine</b> <b>learning,</b> knowledge discovery, classification rules, pediatric cardiology, medical data mining 1...|$|R
40|$|Erroneous {{behavior}} usually elicits {{a distinct}} pattern in neural waveforms. In particular, {{inspection of the}} concurrent recorded electroencephalograms (EEG) typically reveals a negative potential at fronto-central electrodes shortly following a response error (Ne or ERN) {{as well as an}} error-awareness-related positivity (Pe). Seemingly, the brain signal contains information about the occurrence of an error. Assuming a general error evaluation system, the question arises whether this information can be utilized in order to classify behavioral performance within or even across different cognitive tasks. In the present <b>study,</b> a <b>machine</b> <b>learning</b> <b>approach</b> was employed to investigate the outlined issue. Ne as well as Pe were extracted from the single-trial EEG signals of participants conducting a flanker and a mental rotation task and subjected to a <b>machine</b> <b>learning</b> classification scheme (via a support vector machine, SVM). Overall, individual performance in the flanker task was classified more accurately, with accuracy rates of above 85 %. Most importantly, it was even feasible to classify responses across both tasks. In particular, an SVM trained on the flanker task could identify erroneous behavior with almost 70 % accuracy in the EEG data recorded during the rotation task, and vice versa. Summed up, we replicate that the response-related EEG signal can be used to identify erroneous behavior within a particular task. Going beyond this, it was possible to classify response types across functionally different tasks. Therefore, the outlined methodological approach appears promising with respect to future applications...|$|R
40|$|Various {{computational}} {{models have}} been developed to transfer annotations of gene essentiality between organisms. However, despite the increasing number of microorganisms with well-characterized sets of essential genes, selection of appropriate training sets for predicting the essential genes of poorly-studied or newly sequenced organisms remains challenging. In this <b>study,</b> a <b>machine</b> <b>learning</b> <b>approach</b> was applied reciprocally to predict the essential genes in 21 microorganisms. Results showed that training set selection greatly influenced predictive accuracy. We determined four criteria for training set selection: (1) essential genes in the selected training set should be reliable; (2) the growth conditions in which essential genes are defined should be consistent in training and prediction sets; (3) species used as training set should be closely related to the target organism; and (4) organisms used as training and prediction sets should exhibit similar phenotypes or lifestyles. We then analyzed the performance of an incomplete training set and an integrated training set with multiple organisms. We found that the size of the training set should be at least 10 % of the total genes to yield accurate predictions. Additionally, the integrated training sets exhibited remarkable increase in stability and accuracy compared with single sets. Finally, we compared the performance of the integrated training sets with the four criteria and with random selection. The results revealed that a rational selection of training sets based on our criteria yields better performance than random selection. Thus, our results provide empirical guidance on training set selection for th...|$|R
30|$|To {{investigate}} {{what are}} the most important determinant factors {{in the formation of the}} UHIs, who are the most severely affected by the consequence of the UHIs phenomenon, and where the phenomenon is the most intense, this <b>study</b> applied a <b>machine</b> <b>learning</b> <b>approach.</b> As there is no theory that explains the aforementioned relationship, thus the <b>machine</b> <b>learning</b> technique, which allows the computer to learn without being programmed and to identify patterns in data, was used to find highly determinant variables to the formation of UHIs in Marion County, Indiana. To quantify the causal relationship between the land surface temperature and explanatory variables, previous empirical studies have been used regression approaches such as simple regression and spatial regression approaches [19, 20, 26] and correlation analysis [18]. The PCA method, a mathematical procedure that transforms a number of correlated variables into a smaller number of uncorrelated variables called principal component, has been widely used in many of empirical UHIs and social vulnerability studies [12, 13, 19] to reduce the data dimension.|$|R
40|$|Patent {{citation}} analysis {{is considered a}} useful tool for identifying emerging technologies. However, the outcomes of previous methods are likely to reveal no more than current key technologies, since they can only be performed at later stages of technology development due to the time required for patents to be cited (or fail to be cited). This <b>study</b> proposes a <b>machine</b> <b>learning</b> <b>approach</b> to identifying emerging technologies at early stages using multiple patent indicators that can be defined immediately after the relevant patents are issued. For this, first, a total of 18 input and 3 output indicators are extracted from the United States Patent and Trademark Office database. Second, a feed-forward multilayer neural network is employed to capture the complex nonlinear relationships between input and output indicators in a time period of interest. Finally, two quantitative indicators are developed to identify trends of a technology's emergingness over time. Based on this, we also provide the practical guidelines for implementation of the proposed approach. The case of pharmaceutical technology shows that our approach can facilitate responsive technology forecasting and planning...|$|R
40|$|Worldwide diarrheal {{disease is}} {{a leading cause of}} {{morbidity}} and mortality in children less than five years of age. Incidence and disease severity remain the highest in sub-Saharan Africa. Kenya has an estimated 400, 000 severe diarrhea episodes and 9, 500 diarrhea-related deaths per year in children. Current statistical methods for estimating etiological and exposure risk factors for moderate-to-severe diarrhea (MSD) in children are constrained by the inability to assess a large number of parameters due to limitations of sample size, complex relationships, correlated predictors, and model assumptions of linearity. This dissertation examines <b>machine</b> <b>learning</b> statistical methods to address weaknesses associated with using traditional logistic regression models. The studies presented here investigate data from a 4 -year, prospective, matched case-control study of MSD among children less than five years of age in rural Kenya from the Global Enteric Multicenter <b>Study.</b> The three <b>machine</b> <b>learning</b> <b>approaches</b> were used to examine associations with MSD and include: least absolute shrinkage and selection operator, classification trees, and random forest. A principal finding in all three <b>studies</b> was that <b>machine</b> <b>learning</b> methodological <b>approaches</b> are useful and feasible to implement in epidemiological studies. All provided additional information and understanding of the data beyond using only logistic regression models. The results from all three <b>machine</b> <b>learning</b> <b>approaches</b> were supported by comparable logistic regression results indicating their usefulness as epidemiological tools. This dissertation offers an exploration of methodological alternatives that should be considered more frequently in diarrheal disease epidemiology, and in public health in general...|$|R
40|$|Abstract—Network {{services}} are often provided by server clusters. From {{the perspective of}} operational expenditure and the global environment, the power consumption of server clusters should be decreased. This is possible by operating the minimum number of computers required to realize a sufficiently good performance against changes in load. To do this, {{it is necessary to}} accurately determine the number of computers that should be turned on or off for the measured load metrics. This number should be determined by estimating multiple load metrics because a single metric does not adequately represent the statuses of various bottlenecked resources. In addition, decision rules should be appropriately updated if there are changes in the service content or computer specifications. To satisfy these requirements, this <b>study</b> proposes a <b>machine</b> <b>learning</b> <b>approach</b> as a method of determining the number of server computers. Another technical requirement for power management is that the load metrics should be measured nonintrusively for the OS or hardware of the cluster computers. From this viewpoint, we employ traffic parameters as the metrics that reflect resource consumptions. These traffic parameters are passively measured on a machine that is separate from the server cluster. This paper first explains the <b>machine</b> <b>learning</b> <b>approach</b> to determine the number of computers. The implementation of the approach is then presented. The effectiveness of the scheme is confirmed experimentally. Index Terms — machine learning; power management; server cluster; measurement; traffic T I...|$|R
40|$|In {{recent years}} {{there has been}} much focus on the use of single {{nucleotide}} polymorphism (SNP) fine genome mapping to identify causative mutations for traits of interest; however, many studies focus only on the marginal effects of markers, ignoring potential gene interactions. Simulation studies have show that this approach may not be powerful enough to detect important loci when gene interactions are present. While several studies have examined potential gene interaction, they tend to focus on a small number of SNP markers. Given the prohibitive computation cost of modeling interactions in studies involving a large number SNP, methods need to be develop that can account for potential gene interactions in a computationally efficient manner. This <b>study</b> adopts a <b>machine</b> <b>learning</b> <b>approach</b> by adapting the ant colony optimization algorithm (ACA), coupled with logistic regression on haplotypes and genotypes, for association studies involving large numbers of SNP markers. The proposed method is compared to haplotype analysis, implemented using a sliding window (SW/H), and single locus genotype association (RG). Each algorithm was evaluated using a binary trait simulated using an epistatic model and HapMap ENCODE genotype data. Results show that the ACA outperformed SW/H and RG under all simulation scenarios, yielding substantial increases in power to detect genomic regions associated with the simulated trait...|$|R
40|$|Spam {{detection}} is {{a significant}} problem which {{is considered by many}} researchers by various developed strategies. In this study, the popular performance measure is a classification accuracy which deals with false positive, false negative and accuracy. These metrics were evaluated under applying two supervised learning algorithms: hybrid of Artificial Neural Network (ANN) and Genetic Algorithm (GA), Support Vector Machine (SVM) based on classification of Email spam contents were evaluated and compared. In this <b>study,</b> a hybrid <b>machine</b> <b>learning</b> <b>approach</b> inspired by Artificial Neural Network (ANN) and Genetic Algorithm (GA) for effectively detect the spams. Comparisons have been done between classical ANN and Improved ANN-GA and between ANN-GA and SVM to show which algorithm has the best performance in spam detection. These algorithms were trained and tested on a 3 set of 4061 E-mail in which 1813 were spam and 2788 were nonspam. Results showed that the proposed ANN-GA technique gave better result compare to classical ANN and SVM techniques. The results from proposed ANNGA gave 93. 71 % accuracy, while classical ANN gave 92. 08 % accuracy and SVM technique gave the worst accuracy which was 79. 82. The experimental result suggest that the effectiveness of proposed ANN-GA model is promising and this study provided a new method to efficiently train ANN for spam detection...|$|R
40|$|Due to {{the lack}} of {{annotated}} data sets, there are few <b>studies</b> on <b>machine</b> <b>learning</b> based <b>approaches</b> to extract named entities (NEs) in clinical text. The 2009 i 2 b 2 NLP challenge is a task to extract six types of medication related NEs, including medication names, dosage, mode, frequency, duration, and reason from hospital discharge summaries. Several <b>machine</b> <b>learning</b> based systems have been developed and showed good performance in the challenge. Those systems often involve two steps: 1) recognition of medication related entities; and 2) determination of the relation between a medication name and its modifiers (e. g., dosage). A few <b>machine</b> <b>learning</b> algorithms including Conditional Random Field (CRF) and Maximum Entropy have been applied to the Named Entity Recognition (NER) task at the first step. In this study, we developed a Support Vector Machine (SVM) based method to recognize medication related entities. In addition, we systematically investigated various types of features for NER in clinical text. Evaluation on 268 manually annotated discharge summaries from i 2 b 2 challenge showed that the SVM-based NER system achieved the best F-score of 90. 05 % (93. 20 % Precision, 87. 12 % Recall), when semantic features generated from a rule-based system were included. ...|$|R
40|$|Protein {{secondary}} structure prediction is a sub-problem of protein structure prediction. Instead of fully recovering the whole {{three dimensional structure}} from amino acid sequence, protein {{secondary structure}} prediction only aimed at predicting the local structures such as alpha helices, beta strands and turns for each small segment of a protein. Predicted protein secondary structure {{can be used for}} improving fold recognition, ab initial protein prediction, protein motifs prediction and sequence alignment. Protein secondary structure prediction has been extensively <b>studied</b> with <b>machine</b> <b>learning</b> <b>approaches.</b> And in recent years, multiple deep neural network methods have pushed the state-of-art performance of 8 -categories accuracy to around 69 percent. Deep neural networks are good at capturing the global information in the whole protein, which are widely believed to be crucial for the prediction. And due to the development of high level neural network libraries, implementing and training neural networks {{are becoming more and more}} convenient and efficient. This project focuses on empirical performance comparison of various deep neural network architectures and the effects of hyper-parameters for protein secondary structure prediction. Multiple deep neural network architectures representing the state-of-the-art for secondary structure prediction are implemented using TensorFlow, the leading deep learning platform. In addition, a software environment for performing efficient empirical studies are implemented, which includes network input and parameter control, and training, validation, and test performance monitoring. An extensive amount of experiments have been conducted using popular datasets and benchmarks and generated some useful results. For example, the experimental results show that recurrent layers are useful in improving prediction accuracy, achieving up to 5 percent improvement on 8 -category accuracy. This work also shows the trade ix off between running speed and building speed of the model, and the trade off between running speed and accuracy. As a result, a relatively small size recurrent network have been build and achieved 69. 5 percent 8 -category accuracy on dataset CB 513...|$|R
40|$|Background: Depression is {{currently}} underdiagnosed among older adults. As {{part of the}} Novel Assessment of Nutrition and Aging (NANA) validation study, 40 older adults self-reported their mood using a touchscreen computer over three, one-week periods. Here, we demonstrate the potential of these data to predict future depression status. Methods: We analysed data from the NANA validation <b>study</b> using a <b>machine</b> <b>learning</b> <b>approach.</b> We applied the least absolute shrinkage and selection operator with a logistic model to averages of six measures of mood, with depression status according to the Geriatric Depression Scale 10 weeks later as the outcome variable. We tested multiple values of the selection parameter {{in order to produce}} a model with low deviance. We used a cross-validation framework to avoid overspecialisation, and receiver operating characteristic (ROC) curve analysis to determine the quality of the fitted model. Results: The model we report contained coefficients for two variables: sadness and tiredness, as well as a constant. The cross-validated area under the ROC curve for this model was 0. 88 (CI: 0. 69 – 0. 97). Limitations: While results are based on a small sample, the methodology for the selection of variables appears suitable for the problem at hand, suggesting promise for a wider study and ultimate deployment with older adults at increased risk of depression. Conclusions: We have identified self-reported scales of sadness and tiredness as sensitive measures which have the potential to predict future depression status in older adults, partially addressing the problem of underdiagnosis...|$|R
40|$|Research on {{the human}} microbiome, the {{microbiota}} that live in, on, and around the human person, has revolutionized {{our understanding of the}} complex interactions between microbial life and human health and disease. The microbiome may also provide a valuable tool in forensic death investigations by helping to reveal the postmortem interval (PMI) of a decedent that is discovered after an unknown amount of time since death. Current methods of estimating PMI for cadavers discovered in uncontrolled, unstudied environments have substantial limitations, some of which may be overcome through the use of microbial indicators. In this project, we sampled the microbiomes of decomposing human cadavers, focusing on the skin microbiota found in the nasal and ear canals. We then developed several models of statistical regression to establish an algorithm for predicting the PMI of microbial samples. We found that the complete data set, rather than a curated list of indicator species, was preferred for training the regressor. We further found that genus and family, rather than species, are the most informative taxonomic levels. Finally, we developed a k-nearest- neighbor regressor, tuned with the entire data set from all nasal and ear samples, that predicts the PMI of unknown samples with an average error of ± 55 accumulated degree days (ADD). This <b>study</b> outlines a <b>machine</b> <b>learning</b> <b>approach</b> for the use of necrobiome data in the prediction of the PMI and thereby provides a successful proof-of- concept that skin microbiota is a promising tool in forensic death investigations...|$|R
40|$|It is {{observed}} that consumers often share their opinion, views or feeling about any term used on social {{network in the}} form of reviews, comments or feedback. Those feedbacks given by end users have a great impact for evolution of new version of any product. Due to this trend in social media in recent years, sentiment analysis has become an important concern for theoreticians and practitioners Moreover reviews are often written in natural language and are mostly unstructured. Thus, to obtain any meaningful information from these reviews, it needs to be processed. Due to large size of data it is impossible to process this information manually. Hence <b>machine</b> <b>learning</b> algorithms are considered for analysis. Since data are unstructured in nature, unsupervised <b>machine</b> <b>learning</b> algorithm can be helpful in solving this problem. But unsupervised methods have less accuracy; hence not acceptable. In this <b>study,</b> a hybrid <b>machine</b> <b>learning</b> <b>approach</b> is adopted to automatically find the requirements for next version of software. Also some reviews neither belong to positive cluster nor to negative. They mixed reaction or feeling about some topics. Those problem associated with NLP is solved using hybrid technique of the fuzzy c-means and ANN. Moreover in this study, different methods of unsupervised machine leaning algorithm are implemented and their results are compared with each other. The best outcome is used to train the neural network. By using this hybridization technique, accuracy gets increased. And in later stage, this technique is applied to find the new requirement of product...|$|R
40|$|Spam {{detection}} is {{a significant}} problem that {{is considered by many}} researchers through various developed strategies. Creating a particular model to categorize the wide range of spam categories is complex; with understanding of spam types, which are always changing. In spam detection, low accuracy and the high false positive are substantial problems. So the trend to hire a global optimization algorithm is an appropriate way to resolve these problems due to its ability to create new solutions and non-compliance with local solutions. In this <b>study,</b> a hybrid <b>machine</b> <b>learning</b> <b>approach</b> inspired by Artificial Neural Network (ANN) and Differential Evolution (DE) are designed for effectively detect the spams. Comparisons have been done between ANN-DE with Genetic Algorithm (GA) and ANN-DE with InfoGain algorithm to show which approach has the best performance in spam detection. Spambase dataset of 4061 E-mail in which 1813 were spam (39. 40 %) and 2788 were non-spam (59. 60 %) were used to training and testing on these algorithms. The popular performance measure is a classification accuracy, which deals with false positive, false negative, accuracy, precision, and recall. These metrics were used for performance evaluation on the hybrid of ANN-DE with GA and InfoGain algorithm as feature selection algorithms. Performance of ANN-DE with GA and ANN-DE with InfoGain are compared. The experimental results show that the proposed hybrid technique of ANN-DE and GA gives better result with 93. 81 % accuracy compared to ANN-DE and InfoGain with 93. 28 % accuracy. The results recommend that the effectiveness of proposed ANN-DE with GA is promising and this study provided a new method to practically train ANN for spam detection...|$|R
30|$|SA is an {{important}} topic in Natural Language Processing and Artificial Intelligence. Also known as opinion mining, SA mines people’s opinions, sentiments, evaluations, and emotions about entities such as products, services, organizations, individuals, issues, and events, {{as well as their}} related attributes. This kind of analysis has many useful applications. For example, it determines a product’s popularity according to the user’s reviews. If the overall sentiments are negative, further analysis may be performed to identify which features contribute to the negative ratings so companies can reshape their businesses. Numerous studies have been done for sentiment analysis in different domains, languages, and approaches [3 – 5, 8 – 10, 14 – 17]. Among these <b>studies,</b> the <b>machine</b> <b>learning</b> <b>approaches</b> are more popular since the models can be automatically trained and improved with the training datasets. Pang et al. [4] apply supervised <b>machine</b> <b>learning</b> methods such as NB and SVM to sentiment classification. NB, SVM, MEM, and DT are some of the commonly used <b>machine</b> <b>learning</b> <b>approaches</b> [4, 7 – 9, 14]. Feature selection methods are used to rank features so that non-informative features can be removed to improve the classification performance [18]. Some researchers have investigated the effects of feature selection for sentiment analysis [3, 8 – 10, 19 – 25]. For example, Yang and Yu [3] examine IG for feature selection and evaluate its performance using NB, SVM, and C 4.5 (popular implementation for DT) classifiers. Nicholls et al. [8] compare their proposed DFD feature selection method against other feature selection methods, including CHI 2, OCFS [26], and count difference using the MEM classifier. Agarwal et al. [9] investigate minimum redundancy maximum relevancy (mRMR) and IG methods for sentiment classification using NBM and SVM classifiers. The results show that mRMR performs better than IG for feature selection, and NBM performs better than SVM in accuracy and execution time. Abbasi et al. [22] examine a new feature selection method called entropy weighted genetic algorithm (EWGA) and compare the performance of this method using information gain feature selection method. EWGA achieves a relatively high accuracy of 91.7 % using SVM classifier. Xia et al. [24] design two types of feature sets: POS based and word relation based. Their word relation based method improves an accuracy of 87.7 and 85.15 % on movie and product datasets. Bai [25] proposes a Tabu heuristic search-enhanced Markov blanket model that provides a vocabulary to extract sentiment features. Their method achieves an accuracy of 92.7 % for the movie review dataset. Mladenovic et al. [16] propose a feature selection method that is based on mapping {{of a large number of}} related features to a few features. Their proposed method improves the classification performance using unigram features with 95 % average accuracy. Zheng et al. [27] perform comparative experiments to test their proposed improved document frequency feature selection method. Their method achieves significant improvement in sentiment analysis of Chinese online reviews with an accuracy of 97.3 %.|$|R
40|$|The {{artificial}} mouth is a robotic device that simulates a human mouth. It consists of moveable lips and an adjustable air supply. The uses of an {{artificial mouth}} include research for physical modeling of the lips and automatic performance. Automatic {{performance of a}} musical instrument is when an instrument is played without the direct interaction of a human. Typically mechanics and robotics are used instead of a human. In this study {{the use of a}} genetic algorithm to compute air pressure and lip pressure values so that the artificial mouth can correctly play five notes on a brass instrument is investigated. In order to properly playa brass instrument, a player must apply proper tension between the lips and apply proper airflow so that the lips vibrate at the proper frequency. A player changes the notes on a brass instrument by depressing keys and changing lip pressure and air flow. This <b>study</b> investigated a <b>machine</b> <b>learning</b> <b>approach</b> to finding lip pressure and air pressure parameters so that an artificial mouth could play five notes of a scale on a trumpet. A fast search algorithm was needed because it takes about 4 seconds to measure the frequency produced by each combination of pressure parameters. This measurement is slow because of the slow moving mechanics of the system and a delay produced while the notes are measured for pitch. Two different mouthpieces were used to investigate the ability to adapt to different mouthpieces. The algorithm started with a randomly generated population and evolved the lip pressure and air pressure parameters with an evolutionary algorithm using crossover and mutation designed for the knowledge scheme in this application. The efficiency of this algorithm was compared to an exhaustive search. Experimentation was performed using various combinations of genetic parameters including population size, crossover rate, and mutation rate. The evolutionary search was shown to be about 10 times faster than the exhaustive search because the evolutionary algorithm searches only very small portion of the search space. A recommendation for future research is to conduct further experimentation to determine more optimal crossover and mutation rates...|$|R
40|$|Abstract — Recent game-theoretic {{approaches}} to constructing overlay network topologies {{have not been}} scalable. This paper introduces a <b>machine</b> <b>learning</b> <b>approach</b> to constructing overlay networks. The <b>machine</b> <b>learning</b> <b>approach</b> <b>learns</b> characteristics from small networks constructed using a game-theoretic <b>approach.</b> The knowledge <b>learned</b> is then used to construct larger networks. The {{results show that the}} <b>machine</b> <b>learning</b> <b>approach</b> closely approximates the game-theoretic networks {{for a wide range of}} network parameters, while being scalable. I...|$|R
30|$|Some <b>studies</b> use <b>machine</b> <b>learning</b> to find {{students}} who need assistance. Ahadi et al. (2015) and Castro-Wunsch et al. (2017) propose methods to automatically identify students {{in need of}} assistance. They predict such students using students’ source code snapshot data by <b>machine</b> <b>learning</b> <b>approaches</b> such as decision trees. Hong et al. (2015) implemented a function to the learning system called SQL-Tutor, which identifies {{students who}} will abandon the programming task and provides encouragement by displaying motivational messages.|$|R
5000|$|Computational simulations, {{modeling}} and <b>machine</b> <b>learning</b> <b>approaches</b> ...|$|R
40|$|The {{ability of}} two {{different}} <b>machine</b> <b>learning</b> <b>approaches</b> to map non-linear problems from experimental data is evaluated under controlled experiments. A well-known <b>machine</b> <b>learning</b> algorithm (Artificial Neural Network) is compared against a new computing paradigm (Hierarchical Temporal Memory) under a controlled scenario. The chosen scenario is the detection of impacts in a cantilever beam under vibration instrumented with fiber Bragg gratings. The main characteristics of both of the <b>machine</b> <b>learning</b> <b>approaches</b> are analyzed while varying environmental parameters such {{as the number of}} sensing points and their location. From the achieved results some clues can be extracted regarding dealing with noisy or partial data using different <b>machine</b> <b>learning</b> <b>approaches...</b>|$|R
5000|$|Resolving the {{structural}} features of genomic islands: a <b>machine</b> <b>learning</b> <b>approach</b> ...|$|R
30|$|Arrignton et al. [104] {{proposed}} a HIDS {{that depends on}} a <b>machine</b> <b>learning</b> <b>approach</b> for anomaly-based intrusion detection. The <b>machine</b> <b>learning</b> <b>approach</b> {{is based on the}} mechanisms of artificial immune systems. The main features of this system are its use of a behavioral modeling IDS (BMIDS) to decide whether behavior is acceptable and its increased detection sensitivity achieved by canceling out environmental noise.|$|R
