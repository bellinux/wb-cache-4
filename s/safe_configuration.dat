36|47|Public
500|$|In 1962, {{scientists}} at Los Alamos created a mockup of Little Boy known as [...] "Project Ichiban" [...] {{in order to}} answer some of the unanswered questions, but it failed to clear up all the issues. In 1982, Los Alamos created a replica Little Boy from the original drawings and specifications. This was then tested with enriched uranium but in a <b>safe</b> <b>configuration</b> that would not cause a nuclear explosion. A hydraulic lift was used to move the projectile, and experiments were run to assess neutron emission. Based on this and the data from The Great Artiste, the yield was estimated at 16.6 ± 0.3 kilotons. After considering many estimation methods, a 1985 report concluded that the yield was 15 kilotons ± 20%.|$|E
5000|$|... #Caption: Image of the Lady Godiva {{assembly}} in the scrammed (<b>safe)</b> <b>configuration.</b>|$|E
5000|$|In 1962, {{scientists}} at Los Alamos created a mockup of Little Boy known as [...] "Project Ichiban" [...] {{in order to}} answer some of the unanswered questions, but it failed to clear up all the issues. In 1982, Los Alamos created a replica Little Boy from the original drawings and specifications. This was then tested with enriched uranium but in a <b>safe</b> <b>configuration</b> that would not cause a nuclear explosion. A hydraulic lift was used to move the projectile, and experiments were run to assess neutron emission. Based on this and the data from The Great Artiste, the yield was estimated at 16.6 ± 0.3 kilotons. After considering many estimation methods, a 1985 report concluded that the yield was 15 kilotons ± 20%.|$|E
40|$|The growing {{complexity}} of networks requires to delegate management functionnalities to the networks themselves. However, the changes operated by autonomic networks may generate vulnerable configurations. We propose to integrate vulnerability descriptions into the management plane {{in order to}} enable <b>safe</b> <b>configurations</b> in these environments...|$|R
40|$|The {{previously}} studied {{notions of}} smart and foolproof finite order domination {{of a simple}} graph G = (V,E) are generalised {{in the sense that}} <b>safe</b> <b>configurations</b> in G are not merely sought after k ≥ 1 moves, but in the limiting cases where k → ∞. Some general properties of these generalised domination parameters are established, after which the parameter values are found for certain simple graph structures (such as paths, cycles, multipartite graphs and products of complete graphs, cycles and paths) ...|$|R
40|$|Autonomic {{networks}} {{and services are}} exposed to a large variety of risks. The vulnerability management process {{plays a crucial role}} for ensuring their <b>safe</b> <b>configurations</b> and prevent security attacks. We focus in this survey on the assessment of vulnerabilities in autonomic environments. In particular, we analyze current methods and techniques contributing to the discovery, the description and the detection of these vulnerabilities. We also point out important challenges that should be faced in order to fully integrate this process into the autonomic management plane...|$|R
50|$|The 1987 Alianza Lima air {{disaster}} {{took place}} on 8 December 1987, when a Peruvian Navy Fokker F27-400M chartered by Peruvian football club Alianza Lima plunged into the Pacific Ocean six miles short of its destination, off the Ventanilla District {{of the city of}} Callao. On board the flight were a total of 44 players, managers, staff, team supporters, and crewmembers, of whom only the pilot survived the accident. The team was returning from a Peruvian league match in Pucallpa. Uncomfortable with the malfunctioning indicator on his control panel, the pilot requested a flyby of the control tower at Lima's Jorge Chávez International Airport so that spotters on the ground could confirm that the aircraft's landing gear was down and locked. Upon receiving visual confirmation of <b>safe</b> <b>configuration</b> for landing, the aircraft went around for another attempt at a landing, during which the aircraft flew too low, hitting the Pacific Ocean.|$|E
40|$|AbstractA {{connected}} dominating set (CDS) {{is useful}} in forming a virtual backbone in wireless ad hoc or sensor networks because these networks lack a fixed infrastructure and centralized management. Self-stabilization guarantees that the system tolerates any finite number of transient faults and does not need any initialization. The safe convergence property guarantees that the system quickly converges to a feasible <b>safe</b> <b>configuration,</b> and subsequently converges to a legitimate configuration without violating safety. A previous publication on a safely converging algorithm for the minimum CDS assumed a phase clock synchronizer, {{which is a very}} strong assumption. In this paper, we propose the first asynchronous self-stabilizing (6 +ϵ) -approximation algorithm with safe convergence for the minimum CDS in networks modeled by unit disk graphs (UDGs). We assume that the feasible <b>safe</b> <b>configuration</b> satisfies the condition that a dominating set is constructed. The convergence time to a feasible <b>safe</b> <b>configuration</b> is one round, and the convergence time to a legitimate configuration in which an approximated minimum CDS is constructed is O(max⁡{d 2,n}) rounds, and O(n 6) steps...|$|E
40|$|Abstract. We {{propose the}} first robust self-stabilizing {{protocol}} building 1 -hop clusters whose size is bounded, moreover the clusterhead selection is weight-based. The protocol reaches quickly (in 4 rounds) a <b>safe</b> <b>configuration,</b> where the safety property is satistfied: network nodes are partitionned into bounded clusters (clusterheads {{are not the}} most suitable nodes). During the convergence to a legitimate configuration, where more desired properties are guaranteed, the safety property is preserved, ensuring then the continuity functioning of hierarchical protocols...|$|E
40|$|International audienceMobile {{computing}} {{devices and}} the services offered by them are utilized {{by millions of}} users on a daily basis. However, they operate in hostile environments getting exposed {{to a wide variety}} of threats. Accordingly, vulnerability management mechanisms are highly required. We present in this demo a novel approach for increasing the security of mobile devices by efficiently detecting vulnerable configurations. In that context, we propose Ovaldroid, an OVAL-based distributed framework for ensuring <b>safe</b> <b>configurations</b> within the Android platform and we present an implementation prototype developed to this end...|$|R
40|$|International audienceChanges {{that are}} {{operated}} by autonomic networks and systems may generate vulnerabilities {{and increase the}} exposure to security attacks. We present in this paper a new approach for increasing vulnerability awareness in such self-managed environments. Our objective is to enable autonomic networks to exploit the knowledge provided by vulnerability descriptions {{in order to maintain}} <b>safe</b> <b>configurations.</b> In that context, we propose a modeling and an architecture for automatically translating these descriptions into policy rules that are interpretable by an autonomic configuration system. We also describe an implementation prototype and evaluate its performance through an extensive set of experiments...|$|R
40|$|Part 2 : PhD Workshop: Autonomic Network and Service ManagementInternational audienceThe {{autonomic}} paradigm {{has been}} introduced in order {{to cope with the}} growing complexity of management. In that context, autonomic networks and systems are in charge of their own configuration. However, the changes that are operated by these environments may generate vulnerable configurations. In the meantime, a strong standardization effort has been done for specifying the description of configuration vulnerabilities. We propose in this paper an approach for integrating these descriptions into the management plane of autonomic systems in order to ensure <b>safe</b> <b>configurations.</b> We describe the underlying architecture and a set of preliminary results based on the Cfengine configuration tool...|$|R
40|$|Aperture {{measurements}} in the ALICE {{interaction region}} {{were carried out}} to determine a <b>safe</b> <b>configuration</b> of beta* and crossing angle for the 2011 heavy ion run. Proton beams were used {{at the end of}} the proton run, after the commissioning of the squeeze to beta*= 1 m in IR 2. In this paper, the results of aperture measurements are summarised and the final collision configuration is presented. Results of parasitic measurements of the effect of non-linear triplet fields with large orbit bumps in the IRs are also summarise...|$|E
40|$|Costly on-site node repairs in {{wireless}} mesh networks (WMNs) can {{be required}} due to misconfiguration, corrupt software updates, or unavailability during updates. We propose ADAM as a novel management framework that guarantees accessibility of individual nodes in these situations. ADAM uses a decentralised distribution mechanism and self-healing mechanisms for <b>safe</b> <b>configuration</b> and software updates. In order {{to implement the}} ADAM management and self-healing mechanisms, an easy-to-learn and extendable build system for a small footprint embedded Linux distribution for WMNs has been developed. The paper presents the ADAM concept, the build system for the Linux distribution and the management architecture...|$|E
40|$|In the {{population}} protocol model Angluin et al. proposed in 2004, there exists no self-stabilizing leader election protocol for complete graphs, arbitrary graphs, trees, lines, degree-bounded graphs {{and so on}} unless the protocol knows {{the exact number of}} nodes. To circumvent the impossibility, we introduced the concept of loose-stabilization in 2009, which relaxes the closure requirement of self-stabilization. A loosely-stabilizing protocol guarantees that starting from any initial configuration a system reaches a <b>safe</b> <b>configuration,</b> and after that, the system keeps its specification (e. g. the unique leader) not forever, but for a sufficiently long time (e. g. exponentially large time with respect to the number of nodes). Our previous works presented two loosely-stabilizing leader election protocols for arbitrary graphs; One uses agent identifiers and the other uses random numbers to elect a unique leader. In this paper, we present a loosely-stabilizing protocol that solves leader election on arbitrary graphs without agent identifiers nor random numbers. By the combination of virus-propagation and token-circulation, the proposed protocol achieves polynomial convergence time and exponential holding time without such external entities. Specifically, given upper bounds N and Delta of the number of nodes n and the maximum degree of nodes delta respectively, it reaches a <b>safe</b> <b>configuration</b> within O(m*n^ 3 *d + m*N*Delta^ 2 *log(N)) expected steps, and keeps the unique leader for Omega(N*e^N) expected steps where m is the number of edges and d is the diameter of the graph. To measure the time complexity of the protocol, we assume the uniformly random scheduler which is widely used in the field of {{the population}} protocols...|$|E
40|$|Abstract—Mobile {{computing}} {{devices and}} the services offered by them are utilized {{by millions of}} users on a daily basis. However, they operate in hostile environments getting exposed {{to a wide variety}} of threats. Accordingly, vulnerability management mechanisms are highly required. We present in this paper a novel approach for increasing the security of mobile devices by efficiently detecting vulnerable configurations. In that context, we propose a modeling for performing vulnerability assessment activities as well as an OVAL-based distributed framework for ensuring <b>safe</b> <b>configurations</b> within the Android platform. We also describe an implementation prototype and evaluate its performance through an extensive set of experiments. I...|$|R
5000|$|Safe Enclosure (or Safestor(e) SAFSTOR): This option postpones {{the final}} removal of {{controls}} {{for a longer}} period, usually {{on the order of}} 40 to 60 years. The facility is placed into a <b>safe</b> storage <b>configuration</b> until the eventual dismantling and decontamination activities occur.|$|R
40|$|International audienceComputer {{and network}} systems are {{consistently}} exposed to security threats, making their management even more complex. The management of known vulnerabilities plays {{a crucial role}} for ensuring their <b>safe</b> <b>configurations</b> and preventing security attacks. However, this activity should not generate new vulnerable states. In this paper we present a novel approach for autonomously assessing and remediating vulnerabilities. We describe a detailed mathematical model that supports this activity and we formalize the remediation decision process as a SAT problem. We present a framework that is able to assess OVAL vulnerability descriptions and perform corrective actions by using XCCDF-based descriptions of future machine states and the NETCONF protocol. We also provide details of our implemen- tation and evaluate its feasibility through a comprehensive set of experiments...|$|R
40|$|OPODIS 2015 : 19 th International Conference on Principles of Distributed Systems, 14 - 17 Dec. 2015, Rennes, FranceIn the {{population}} protocol model Angluin et al. proposed in 2004, there exists no self-stabilizing leader election protocol for complete graphs, arbitrary graphs, trees, lines, degree-bounded graphs {{and so on}} unless the protocol knows {{the exact number of}} nodes. To circumvent the impossibility, we introduced the concept of loose-stabilization in 2009, which relaxes the closure requirement of self-stabilization. A loosely-stabilizing protocol guarantees that starting from any initial configuration a system reaches a <b>safe</b> <b>configuration,</b> and after that, the system keeps its specification (e. g. the unique leader) not forever, but for a sufficiently long time (e. g. exponentially large time with respect to the number of nodes). Our previous works presented two loosely-stabilizing leader election protocols for arbitrary graphs; One uses agent identifiers and the other uses random numbers to elect a unique leader. In this paper, we present a loosely-stabilizing protocol that solves leader election on arbitrary graphs without agent identifiers nor random numbers. By the combination of virus-propagation and token-circulation, the proposed protocol achieves polynomial convergence time and exponential holding time without such external entities. Specifically, given upper bounds N and Delta of the number of nodes n and the maximum degree of nodes delta respectively, it reaches a <b>safe</b> <b>configuration</b> within O(m*n^ 3 *d + m*N*Delta^ 2 *log(N)) expected steps, and keeps the unique leader for Omega(N*e^N) expected steps where m is the number of edges and d is the diameter of the graph. To measure the time complexity of the protocol, we assume the uniformly random scheduler which is widely used in the field of {{the population}} protocols...|$|E
40|$|A self-stabilizing {{distributed}} {{system is a}} fault-tolerant {{distributed system}} that tolerates any kind and any finite number of transient faults, such as message loss and memory corruption. In this paper, we formu-late a concept of safe convergence {{in the framework of}} self-stabilization. An ordinary self-stabilizing algorithm has no safety guarantee while it is in converging from any initial configuration. The safe convergence prop-erty guarantees that a system quickly converges to a <b>safe</b> <b>configuration,</b> and then, it gracefully moves to an optimal configuration without breaking safety. Then, we propose a minimal independent dominating set al-gorithm with safe convergence property. Especially, the proposed algorithm computes the lexicographically first minimal independent dominating set according to the process identifier as a priority. The priority scheme can be arbitrarily changed such as stability, battery power and/or computation power of node. 1...|$|E
40|$|AbstractIn {{wireless}} ad hoc or sensor networks, a connected dominating set (CDS) {{is useful}} as the virtual backbone {{because there is}} no fixed infrastructure or centralized management. Additionally, in such networks, transient faults and topology changes occur frequently. A self-stabilizing system tolerates any kind and any finite number of transient faults, and does not need any initialization. An ordinary self-stabilizing algorithm has no safety guarantee and requires that the network remains static while converging to a legitimate configuration. Safe converging self-stabilization is one extension of self-stabilization. The safe convergence property guarantees that the system quickly converges to a <b>safe</b> <b>configuration,</b> and then, it moves to an optimal configuration without breaking safety. In this paper, we propose a self-stabilizing fully distributed 6 -approximation algorithm with safe convergence for the minimum CDS in the networks modeled by unit disk graphs...|$|E
40|$|In {{large-scale}} Tokamaks disruptions {{have the}} potential to create serious damage to the facility. Hence disruptions must be avoided, but, when a disruption is unavoidable minimizing its severity is mandatory. A reliable detection of a disruptive event is required to trigger proper mitigation actions. To this purpose machine learning methods have been widely studied to design disruption prediction systems on several experimental devices. In particular, for ASDEX Upgrade, some of the authors presented predictive systems applying data based techniques, such as Multi-layer Perceptron neural network [1], Discriminant Analysis [2], and Self-Organizing Maps [3]. The training phase of the proposed approaches is based on the availability of disrupted and non-disrupted discharges. To accomplish an exhaustive model every disruptive and <b>safe</b> <b>configurations</b> included in the machine operational space should be represented in the training set. <b>Safe</b> <b>configurations</b> were selected from safe discharges, while disruptive configurations were assumed appearing into the last 45 ms of each disruption [1, 3]. Even if the achieved results in terms of correct predictions were good, it has to be highlighted that the choice of such a fixed temporal window might have limited the prediction performance. In fact, it generates ambiguous information in cases of disruptions with disruptive phase shorter than 45 ms. Conversely, missing information is caused in case of disruptions with a disruptive phase longer than the prefixed one. The assessment of a specific disruptive phase for each disruptive discharge represents one of the most relevant issues in understanding the disruptive events. Several similarity measures, such as Mahalanobis distance, and statistical methods, such as Logistic Regression, have been applied to evaluate the membership of each sample to the safe or the disruptive configurations. Preliminary results show that enhancements on the achieved performance on disruption prediction are possible by defining a specific disruptive phase for each disruption...|$|R
40|$|Managment {{of excess}} nuclear {{materials}} from US weapons dismantlement {{has been the}} subject of numerous intellectual discussions during the past 5 years. Although there has been some objective recommendations, there is still much controversy surrounding the procsses that could lead to a national decision on Pu management. Two immediate needs are to secure the inventories of all Pu in <b>safe</b> <b>configurations</b> and to develop strategies for reducing proliferation risks. Specific suggestions discussed here are to (a) accept the deterrence value of Pu, (b) reappraise its potential as an energy resource, (c) recognize limitations to influence the future of Pu use world-wide, (d) isolate recoverable weapons-grade Pu and store it in stable configurations under international safeguards, and (e) manage Pu in spent fuels so that the valuable resources are not lost to a future generation...|$|R
40|$|Concentric tube robots are catheter-sized robots {{that are}} ideally suited for {{navigating}} along natural anatomical pathways and treating deep-seated pathologies. Their telemanipulation in dynamic environments requires on-line computation of inverse kinematics with simultaneous avoidance of anatomical obstacles. Moreover, unstable configurations, which arise for elongated curved robots that navigate extremely tortuous paths, must be avoided. To achieve on-line computations, existing work has investigated Jacobian approximations and configuration-space precomputation. This paper leverages the state-of-the-art multi-core computer architectures to deliver real-time local inverse kinematics solutions using the established concentric tube robot mechanics models while avoiding both instabilities and anatomical collisions. Furthermore, it considers frictional active constraints for concentric tube robots, i. e. viscoelastic force fields that guide the operator away from obstacles and towards <b>safe</b> <b>configurations.</b> The {{value of the}} proposed framework is demonstrated on realistic clinical scenarios...|$|R
40|$|Abstract. We {{propose a}} {{transformer}} building a silent self-stabilizing with service guarantee 1 -hop clustering protocol T P of an input silent self-stabilizing 1 -hop clustering protocol P. From an arbitrary configu-ration, T P reaches a <b>safe</b> <b>configuration</b> in at most 3 rounds, where the following useful minimal service is provided: “each node {{belongs to a}} 1 -hop cluster having an effective leader”. During stabilization of T P, the minimal service is preserved, so the clustering structure is available throughout the entire network. The minimal service is also maintained despite the occurrences of some external disruptions, called highly toler-ated disruptions, denoted HT D. T P reaches a terminal (also legitimate) configuration in at most 4 ∗SP rounds where SP is the stabilization time of P protocol. Moreover, T P requires only 2 bits per node more than P. ...|$|E
40|$|Review of NMP-NCS- 930087, {open_quotes}Nuclear Criticality Safety Evaluation 93 - 04 Enriched Uranium Receipt (U), July 30, 1993, {close_quotes} was {{requested}} of SRTC (Savannah River Technology Center) Applied Physics Group. The NCSE is a criticality {{assessment to}} determine the mass limit for Engineered Low Level Trench (ELLT) waste uranium burial. The intent is to bury uranium in pits that would be separated by a specified amount of undisturbed soil. The scope of the technical review, documented in this report, consisted of (1) an independent check of the methods and models employed, (2) independent HRXN/KENO-V. a calculations of alternate configurations, (3) application of ANSI/ANS 8. 1, and (4) verification of WSRC Nuclear Criticality Safety Manual procedures. The NCSE under review concludes that a 500 gram limit per burial position is acceptable to ensure the burial site remains in a critically <b>safe</b> <b>configuration</b> for all normal and single credible abnormal conditions. This reviewer agrees with that conclusion...|$|E
40|$|Loading (RPL) Program is {{to develop}} a launch vehicle, payload and ground support {{equipment}} that can support a rapid propellant load and launch within one hour. NASA Kennedy Space Center (KSC) has been funded by AFRL to develop hardware and software to demonstrate this capability. The key features of the software would be the ability to recognize and adapt to failures in the physical hardware components, advise operators of equipment faults and workarounds, and put the system in a <b>safe</b> <b>configuration</b> if unable to fly. In December 2008 NASA KSC and NASA Ames Research Center (ARC) demonstrated modelbased simulation and diagnosis capabilities for a scaled-down configuration of the RPL hardware. In this paper we present a description of the model-based technologies that were included as part of this demonstration and the results that were achieved. In continuation of this work we are currently testing the technologies on a simulation of the complete RPL system. Later in the year, when the RPL hardware is ready, we will be integrating these technologies with the real-time operation of the system to provide live state estimates. In future years we will be developing the capability to recover from faulty conditions via redundancy and reconfiguration. 1...|$|E
50|$|Microsoft Windows {{provides}} Recovery Console, Last Known Good <b>Configuration,</b> <b>Safe</b> Mode {{and recently}} Windows Recovery Environment as standard recovery means. Also, bootable BartPE-based third-party recovery discs are available.|$|R
30|$|While the {{previous}} scenario covered all initially <b>safe</b> <b>configurations,</b> this section {{focuses on the}} analysis of the interaction in the testbed between receiving commands and issuing alerts, as presented in Fig.  7. The upper part of Fig.  7 indicates the time when commands are sent by the attacker and the reaction of the monitoring tool to these commands. The events marked with a green pentagon represent alerts issued by Bro upon receiving the command to change the tap switch to a position that would result in a too high secondary voltage. This is a result of implementing the voltage safety requirement (cf. Table  3) upon receiving a new command (cf. the right-side loop of Fig.  2). Note that Fig.  2 indicates that the command that may bring the system to an unsafe state should be discarded. Here, only an alert was given in order to analyze the further behavior of the system.|$|R
40|$|This {{document}} reviews each system inside PRF {{to determine}} the operation and maintenance requirements necessary to maintain safe and predictable system performance for facility systems needed to remain operational while minimizing the maintenance and surveillance being performed. Also covered are the actions required to place PRF in a <b>safe</b> layup <b>configuration</b> while minimizing hazards and {{taking into account the}} need for reactivation of certain equipment when cleanup work commences in the future...|$|R
40|$|The overall {{objective}} of the US Air Force Research Laboratory (AFRL) Rapid Propellant Loading (RPL) Program {{is to develop a}} launch vehicle, payload and ground support equipment that can support a rapid propellant load and launch within one hour. NASA Kennedy Space Center (KSC) has been funded by AFRL to develop hardware and software to demonstrate this capability. The key features of the software would be the ability to recognize and adapt to failures in the physical hardware components, advise operators of equipment faults and workarounds, and put the system in a <b>safe</b> <b>configuration</b> if unable to fly. In December 2008 NASA KSC and NASA Ames Research Center (ARC) demonstrated model based simulation and diagnosis capabilities for a scaled-down configuration of the RPL hardware. In this paper we present a description of the model-based technologies that were included as part of this demonstration and the results that were achieved. In continuation of this work we are currently testing the technologies on a simulation of the complete RPL system. Later in the year, when the RPL hardware is ready, we will be integrating these technologies with the real-time operation of the system to provide live state estimates. In future years we will be developing the capability to recover from faulty conditions via redundancy and reconfiguration...|$|E
40|$|The growing uptake {{of solar}} power in {{low-voltage}} grids {{across the world}} has drawn attention to potential overvoltage issues. In an effort to mitigate overvoltage, automatic Volt-Watt and Volt-VAR inverter response functions have been introduced in recent standards. The utility of these functions {{has been established in}} a number of recent studies. However, relatively little analysis exists on the stability of grid-connected inverters where decentralised operation of these response functions can potentially trigger undesirable interactions. This paper presents a rigorous stability analysis of a grid-connected inverter under simultaneous operation of automatic Volt-Watt and Volt-VAR response functions. Conditions for the existence of an equilibrium voltage are established together with tests that characterise its stability in terms of the inverter and line parameters. The analysis reveals a little-known stability vulnerability arising when both Volt-Watt and Volt-VAR functions are in operation if Watt output takes precedence over the provision of VAR support, a generally recommended setting. To circumvent this vulnerability, the proposed stability tests allow parameter selection for guaranteed stability margins. A <b>safe</b> <b>configuration</b> alternative is also identified to avoid instability at the expense of real power output. Numerical methods to compute equilibrium voltages are presented and used in two illustrative numerical studies. [Final citation details to be advised...|$|E
40|$|A neutron (moisture-sensitive) and gamma (in-situ radiation) probe {{technique}} has been utilized {{at a number}} of Hanford radioactive waste tanks for many years. This technology has been adapted for use in tank 241 -SY- 101 's two Multifunction Instrument Trees (MITs) which have a hollow dry-well center opening two inches (51 cm) in diameter. These probes provide scans starting within a few inches of the tank bottom and traversing up through the top of the tank revealing a variety of waste features as a function of tank elevation. These features have been correlated with void fraction data obtained independently from two other devices, the Retained Gas Sampler (RGS) and the Void Fraction Instrument (VFI). The MIT probes offer the advantage of nearly continuous count-rate versus elevation scans and they can be operated significantly more often and at lower cost than temperature probes or the RGS or VFI devices while providing better depth resolution. The waste level in tank 241 -SY- 101 had been rising at higher rates than expected during 1998 and early 1999 indicating an increasing amount of trapped gas in the waste. The use of the MIT probes has assisted in evaluating changes in crust thickness and level and also in estimating relative changes in gas stored in the crust. This information is important in assuring that the tank remains in a <b>safe</b> <b>configuration</b> and will support safe waste transfer when those operations take place...|$|E
50|$|The 3.1-mile Donnybrooke Road Course has 10 {{turns and}} is {{considered}} wide - the main straight is 60 feet wide. There is essentially no elevation change. BIR is a high-speed course; vehicles can reach speeds of nearly 180 mph and take the slowest corners around 80 mph. There are wide runoff areas at most of the corners, which makes BIR’s road course extremely <b>safe.</b> This <b>configuration</b> uses the dragstrip {{as part of the}} course.|$|R
50|$|The {{accident}} {{was attributed to}} the Cathcart Circle train passing a signal at danger and causing a collision at the single-lead junction, as at Bellgrove in Glasgow {{just over a year}} earlier. The junction's configuration was newly installed at a cost of £5 million and designed to be simpler than the double-lead junction that it replaced. This allowed faster running on the WCML following the East Coast electrification (through Carstairs) but was inherently less <b>safe.</b> The <b>configuration</b> was unnecessarily constrained and was strongly criticised in the accident report and by contemporary commentators (Hall 1999).|$|R
40|$|Border Gateway Protocol (BGP) is the {{de facto}} {{standard}} used for interdomain routing. Since packet forwarding {{may not be possible}} until stable routes are learned, it is not only critical for BGP to converge but {{it is important that the}} convergence be rapid. The distributed and asynchronous nature of BGP in conjunction with local policies makes it difficult to analyze with respect to convergence behavior. We present a novel model which, to our knowledge, is the first one to permit analysis of convergence in the aggregate (i. e., over all message exchange orders between routers regarding route advertisements), rather than worst case behavior. We introduce the notion of probabilistic safety as requiring the probability of convergence to be 1. We provide a necessary and sufficient condition characterizing probabilistic safety that shows that probabilistic safety accommodates BGP configurations whose potential divergence stems solely from pathological message sequences. More generally, we show how to compute for any BGP configuration its probability of convergence. For probabilistically <b>safe</b> <b>configurations,</b> we present procedures for computing their expected time to converge as well as the probability distribution on their convergence times. The ability to compute these quantitative characteristics makes our work “constructive ” and provides the basis for further understanding and deriving procedures for optimizing network characteristics. Finally, we simulate several network examples and verify the consistency between our analysis and the simulations. ...|$|R
