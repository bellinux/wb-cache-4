44|219|Public
50|$|Repeating the {{recordings}} on the BBC7 digital <b>speech</b> <b>channel</b> revived {{interest in the}} serials in the early 2000s. BBC Worldwide then released all the complete recordings known to have survived on CDs and cassettes.|$|E
5000|$|Fletcher's {{contributions}} {{to the theory of}} speech perception are among his best-known work. He showed that speech features are usually spread over a wide frequency range, and developed the articulation index to approximately quantify the quality of a <b>speech</b> <b>channel.</b> [...] He also developed the concepts of equal-loudness contours (commonly known as Fletcher-Munson curves), loudness scaling and summation, and the critical band. At Bell Labs, he oversaw research in electrical sound recording, including the first successful stereophonic recordings, the first live stereo sound transmission, and the production of the first vinyl recording. All of this was done {{with the help of the}} conductor of the Philadelphia Orchestra, Leopold Stokowski between 1931-32. Some of his other accomplishments include the production of the first functional hearing aid, the 2-A audiometer, and the artificial larynx.|$|E
40|$|It is {{demonstrated}} that state-of-the-art communications {{technology can be}} implemented and reliably operated on a global basis to increase the transmission rates and efficiencies on circuits with bandwidths greater than the typical <b>speech</b> <b>channel.</b> Optimization is affected by optimum clock recovery procedures, multilevel pulse amplitude modulation, single sideband amplitude modulation, transversal filter equalizers, data scrambling, and active compensation for phase instability...|$|E
3000|$|... aFor example, to {{estimate}} the clean power spectrum of the 2 i th window W 2 i, the estimated clean power spectra of the 2 (i- 1)th window W 2 (i- 1), the 2 (i- 2)th window W 2 (i- 2), [...]... were used. bFor RWCP database, 4 <b>speech</b> <b>channels</b> shown in Table 4 were used. For CENSREC- 4 database, <b>speech</b> <b>channels</b> 1, 3, 5, and 7 shown in Figure 3 b were used. cFor RWCP database, 2 <b>speech</b> <b>channels</b> shown in Table 4 were used. For CENSREC- 4 database, <b>speech</b> <b>channels</b> 1 and 3 shown in Figure 3 b were used.|$|R
50|$|The {{base station}} {{subsystem}} (BSS) is {{the section of}} a traditional cellular telephone network {{which is responsible for}} handling traffic and signaling between a mobile phone and the network switching subsystem. The BSS carries out transcoding of <b>speech</b> <b>channels,</b> allocation of radio channels to mobile phones, paging, transmission and reception over the air interface and many other tasks related to the radio network.|$|R
40|$|The {{contribution}} of facial, intonational, and <b>speech</b> <b>channels</b> to spontaneous emotional expression was examined in right brain-damaged (RBD), left braindamaged (LBD), and normal control (NC) subjects. Subjects were videotaped while viewing {{and responding to}} a series of emotionally laden slides; the videotapes were then rated for the three channels of communication. Overall, RBDs used facial expression and intonation less frequently than the other two groups. When the <b>speech</b> output <b>channel</b> was analyzed, oral expression of feelings in the RBDs, relative to the LBDs and NCs, was less appropriate, more propositional than prosodic, and more descriptive than affective. When the ratings for the three channels of communication were examined, facial expression and intonation were significantly correlated for all subjects...|$|R
30|$|In this section, we {{describe}} the use of four microphones b to estimate the spectrum of the impulse responses without a particular explanation. Delay-and-sum beamforming (BF) {{was performed on the}} 4 -channel dereverberant speech signals. For the proposed method, each <b>speech</b> <b>channel</b> was compensated by the corresponding estimated impulse response. Preliminary experimental results for isolated word recognition showed that the SS-based dereverberation method significantly improved the speech recognition performance significantly compared with traditional CMN with beamforming [14].|$|E
40|$|For {{machines}} to lipread, or understand speech from lip movement, they decode lip-motions (known as visemes) into the spoken sounds. We investigate the visual <b>speech</b> <b>channel</b> to further {{our understanding of}} visemes. This has applications beyond machine lipreading; speech therapists, animators, and psychologists can benefit from this work. We explain the influence of speaker individuality, and demonstrate how one can use visemes to boost lipreading. Comment: Computer Vision and Pattern Recognition (CVPR) Women in Computer Vision (WiCV) workshop. 201...|$|E
40|$|Accurate {{identification}} of sentence units (SUs) in spontaneous speech {{has been found}} to improve the accuracy of speech recognition, as well as downstream applications such as parsing. In recent multimodal investigations, gestur]al features were utilized, in addition to lexical and prosodic cues from the <b>speech</b> <b>channel,</b> for detecting SUs in conversational interactions using a hidden Markov model (HMM) approach. Although this approach is computationally efficient and provides a convenient way to modularize the knowledge sources, it has two drawbacks for our SU task. First, standard HMM training methods maximize the joint probability of observations and hidden events, as opposed to the posterior probability of a hidden event given observations, a criterion more closely related to SU classification error. A second challenge for integrating gestural features is that their absence sanctions neither SU events nor non-events; it is only the co-timing of gestures with the <b>speech</b> <b>channel</b> that should impact our model. To address these problems, a Maximum Entropy (ME) model is used to combine multimodal cues for SU estimation. Experiments carried out on VACE multi-party meetings confirm that the ME modeling approach provides a solid framework for multimodal integration...|$|E
50|$|Regardless of the {{frequency}} selected by an operator, it is divided into timeslots for individual phones. This allows eight full-rate or sixteen half-rate <b>speech</b> <b>channels</b> per radio frequency. These eight radio timeslots (or burst periods) are grouped into a TDMA frame. Half-rate channels use alternate frames in the same timeslot. The channel data rate for all 8 channels is 270.833 kbit/s, and the frame duration is 4.615 ms.|$|R
50|$|The coding {{sublayer}} provides forward error correction. As {{a general}} rule, each GSM channel uses a block parity code (usually a Fire code), a rate-1/2, 4th-order convolutional code and a 4-burst or 8-burst interleaver. Notable exceptions are the synchronization channel (SCH) and {{random access channel}} (RACH) that use single-burst transmissions and thus have no interleavers. For <b>speech</b> <b>channels,</b> vocoder bits are sorted into importance classes with different degrees of encoding protection applied to each class (GSM 05.03).|$|R
50|$|Later, in 1997, Aculab {{introduced}} its own ISA speech processing board, {{the first of}} its portfolio of DSP-based voice boards. And it was in 1998 that Aculab introduced a PCI product, the first combined trunk card and voice board - Prosody PCI. It was novel for its time, delivering up to 240 <b>speech</b> <b>channels</b> and 4 E1/T1 trunk interfaces on a single card {{at a time when the}} standard was 24 to 30 channels. A Prosody cPCI (CompactPCI) version followed in 2000.|$|R
40|$|M. Ing. (Electrical and Electronic Engineering) Since the {{beginning}} of telecommunications. more than 100 years ago, systems have been developed through a constant interaction between technical inventions. economic applications {{and the desire for}} improved communication facilities to such an extend that there is now a highly efficient communication network whichspans over the entire globe. The still growing demand for data communication. on conventional channels such ascoaxial cables and twisted pairs. made it necessary to develop systems which make use of line codes. In developing countries. like South-Africa. it is not economically feasible to change the existing analog systems to digital systems to enable data communications. In this study. we have done research into the possibility of combining ananalog speech signal with a digital data channel. Ideally the data channel should make use of line codes with low power contents at low frequencies. so that an analog <b>speech</b> <b>channel</b> can be inserted. Different coding methods were used to construct simple encoders and decoders which generate sequences with low power content at low frequencies. The experimental setup used in this thesis. showed that these new line codes exhibit low frequency suppression to such an extend that an analog <b>speech</b> <b>channel</b> can be inserted without the one interfering with the another...|$|E
40|$|In this work, we {{show how}} {{expectation}} maximization based simultaneous channel and noise estimation {{can be derived}} without a vector Taylor series expansion. The central idea is to approximate the distribution of all the random variables involved – that is noisy speech, clean <b>speech,</b> <b>channel</b> and noise – as one large, joint Gaussian distribution. Consequently, instantaneous estimates of the noise and channel distribution parameters {{can be obtained by}} conditioning the joint distribution on observed, noisy speech spectra. This approach allows for the combination of expectation maximization based channel and noise estimation with the unscented transform...|$|E
40|$|Abstract. This paper {{presents}} a novel data modulation scheme PCCD-OFDM-ASK: the phase con-tinuous context dependent orthogonal frequency division multiplexing amplitude shift keying. The proposed modulation is successfully {{applied in the}} mobile payment system. It is used to transmit the digital data over the <b>speech</b> <b>channel</b> of the mobile communication system GSM, as well as CDMA. The main key points of the proposed modulation schemes are: precise signal synchronization be-tween the modulator and demodulator, signal energy independent operation, on line adaptation of frequency characteristics of the transmission channel, and controlled frequency bandwidth thus enabling non-overlapped full duplex communication over the GSM’s voice channel...|$|E
50|$|In the encoder, {{the input}} is {{passed through a}} {{multiband}} filter, then each band is passed through an envelope follower, and the control signals from the envelope followers are transmitted to the decoder. The decoder applies these (amplitude) control signals to corresponding filters for re-synthesis. Since these control signals change only slowly compared to the original speech waveform, the bandwidth required to transmit speech can be reduced. This allows more <b>speech</b> <b>channels</b> to share a single communication channel, such as a radio channel or a submarine cable (i.e. multiplexing).|$|R
50|$|Transcoder and Rate Adaptation Unit, or TRAU, {{performs}} transcoding {{function for}} <b>speech</b> <b>channels</b> and RA (Rate Adaptation) for data channels in the GSM network. The Transcoder/Rate Adaptation Unit (TRAU) is the data rate conversion unit. The PSTN/ISDN switch is a switch for 64 kbit/s voice. Current technology permits {{to decrease the}} bit-rate (in GSM radio interface it is 13 kbit/s for full rate and 6.5 kbit/sfor half rate). Since MSC is basically a PSTN/ISDN switch its bit-rate is still 64 kbit/s. That is why a rate conversion isrequired in between the BSC and MSC...|$|R
5000|$|The system cost a {{total of}} $100 million and spanned 14,000 miles, from Oban in Scotland via CANTAT to Newfoundland, by {{microwave}} link across Canada, then cable on to Hawaii, Suva (Fiji), Auckland (New Zealand), and Sydney (Australia). Three cable ships-CS Mercury, CS Retriever, and HMTS Monarch-did the job. The link contains 11,000 miles of telephone cable, which, at the time, provided 80 two-way <b>speech</b> <b>channels</b> or 1,760 teleprinter circuits. [...] In addition, the cable carries telegraph traffic, leased circuits for airlines, shipping companies and other commercial transmission.|$|R
40|$|DE 2538638 C UPAB: 19930901 The {{communications}} network using fibre-optic transmission lines uses one glass-fibre conductor {{for each of}} the send and receive paths. A subscriber video camera and analog-digital converter output is transmitted using 4 -bit DPCM at 48 Mbit/s for black and white, 64 Mbit/s is used for the <b>speech</b> <b>channel.</b> The video and speech signals are switched to the light-emitting diode or laser transmitter. At the exchange the video and speech signals are separated and pass through digital-analog converters, the speech being routed through the usual telephone coupling circuits and the video information routed to a special television distribution network...|$|E
40|$|The aim of {{this project}} has been to develop the {{assembly}} language functions needed to allow easy implementation in real-time of a secure <b>speech</b> <b>channel.</b> The theory of security systems is introduced and developed. Encryption algorithms are described. A library of multi-precision arithmetic routines has been written for use on the TMS 320 C 25 digital signed processor. These routines are compatible with code produced by the TMS 320 C 25 C Compiler. Multi-precision arithmetic is used in public key encryption which requires large number arithmetic for security and which also has real-time operation requirements. An overview of DSP use {{in this kind of}} application is given, the design, implementation and test of these routines is described and some application examples and timings are shown...|$|E
40|$|An {{analysis}} of intelligibility measurements of ideal binary masked speech in noise {{for a group}} of normal hearing listeners is presented. In the proposed model, speech cues in the processed mixtures are encoded by two information channels: a noisy <b>speech</b> <b>channel</b> and a vocoded noise channel. Results indicate that the former dominates for dense binary mask patterns, and the latter for sparse binary mask patterns, as controlled by a local SNR criterion used for forming the ideal mask. Moreover, speech cues from the target part of the processed mixture may be better utilized by the listeners {{as a result of the}} ideal binary masking. Finally, the analysis is extended to show a good qualitative agreement with several previous studies of intelligibility of ideal binary masked noisy speech. 1...|$|E
50|$|One of {{the issues}} with using this type of {{technology}} was that the users listening on an idled channel can sometimes hear the conversation that has been switched onto it. Generally the sound heard was of very low volume and individual words are not distinguishable. See also crosstalk for a similar phenomenon in telecommunications. Another potential issue was ensuring that non-voice type circuits (e.g. Music or radio type circuits where pauses would occur infrequently) were not routed via TASI <b>speech</b> <b>channels</b> since these could seriously degrade the level of service where callers would encounter frequent clipped speech and breaks in the conversation.|$|R
50|$|DECT {{operates}} as a multicarrier Frequency division multiple access (FDMA) / Time division multiple access (TDMA) system. This means that the radio spectrum is divided into physical carriers in two dimensions: frequency and time. FDMA access provides up to 10 frequency channels, and TDMA access provides 24 time slots per every frame of 10 ms. DECT uses Time division duplex (TDD), which means that down- and uplink use the same frequency but different time slots. Thus a base station provides 12 duplex <b>speech</b> <b>channels</b> in each frame, with each time slot occupying any available channel - thus 10 x 12 = 120 carriers are available, each carrying 32 kbit/s.|$|R
40|$|We are {{interested}} in understanding how digital ink and speech are used together in presentation. Our long range goal is to develop tools to analyze the ink and <b>speech</b> <b>channels</b> of recorded lectures. As {{a first step in}} this process, we are making a detailed study of instructors’ digital ink usage in real university lectures. This work is being done {{in the context of a}} Tablet-PC based presentation system we have developed, but is applicable to other systems which record digital ink and speech. In this paper we concentrate on how instructors draw and use diagrams in the process of lecture delivery and identify phenomena which are important when automatically processing the diagrammatic ink. Backgroun...|$|R
40|$|Since {{listeners}} usually {{look at the}} speaker's face, gestural {{information has}} to be absorbed through peripheral visual perception. In the literature, {{it has been suggested}} that listeners look at gestures under certain circumstances: 1) when the articulation of the gesture is peripheral; 2) when the <b>speech</b> <b>channel</b> is insufficient for comprehension; and 3) when the speaker him- or herself indicates that the gesture is worthy of attention. The research here reported employs eye tracking techniques to study the perception of gestures in face-to-face interaction. The improved control over the listener's visual channel allows us to test the validity of the above claims. We present preliminary findings substantiating claims 1 and 3, and relate them to theoretical proposals in the literature and to the issue of how visual and cognitive attention are related...|$|E
40|$|Abstract. New text {{independent}} speaker {{identification method}} is presented. Phase spectrum of all-pole linear prediction (LP) model {{is used to}} derive the speech features. The features are represented by pairs of numbers that are calculated from group delay extremums of LP model spectrum. The first component of the pair is an argument of maximum of group delay of all pole LP model spec-trum {{and the second is}} an estimation of spectrum bandwidth at the point of spectrum extremum. A similarity metric that uses group delay features is introduced. The metric is adapted for text inde-pendent speaker identification with general assumption that test <b>speech</b> <b>channel</b> may contain mul-tiple speakers. It is demonstrated that automatic speaker recognition system with proposed features and similarity metric outperforms systems based on Gaussian mixture model with Mel frequency cepstral coefficients, formants, antiformants and pitch features...|$|E
40|$|Effective and {{efficient}} access to multiparty meeting recordings requires techniques for meeting analysis and indexing. Since meeting participants are generally stationary, speaker location {{information may be}} used to identify meeting events e. g., detect speaker changes. Time-delay estimation (TDE) utilizing cross-correlation of multichannel speech recordings is a common approach for deriving speech source location information. Recent research improved TDE by calculating TDE from linear prediction (LP) residual signals obtained from LP analysis on each individual <b>speech</b> <b>channel.</b> This paper investigates the use of LP residuals for speech TDE, where the residuals are obtained from jointly modeling the multiple speech channels. Experiments conducted with a simulated reverberant room and real room recordings show that jointly modeled LP better predicts the LP coefficients, compared to LP applied to individual channels. Both the individually and jointly modeled LP exhibit similar TDE performance, and outperform TDE on the speech alone, especially with the real recordings...|$|E
40|$|The {{greatest}} {{single factor}} in enhancing spectral efficiency {{of a network}} is not complex multiple access techniques, efficient <b>speech</b> and <b>channel</b> coding, modulation, powerful protocols, etc., but the mass deployment of microcells. By this simple technique we can repeatedly and efficiently reuse the precious spectrum...|$|R
40|$|Abstract. The present paper {{describes}} {{the work done}} {{during the last year}} in the development of European Portuguese (EP) speech recognition and synthesis channels for an Internet e-commerce application. The objective of this work was to develop the appropriate speech input and output, to enable the user to more comfortably use an e-commerce web site issuing commands and options under guidance of a menu system. The speech interface operation guidelines are briefly described in their principles and resulting specifications for the <b>speech</b> <b>channels,</b> namely, menu structure and operation dynamics, and in the following, the system software approach and the speech recognition and synthesis modules are presented. A discussion is done about some project trade-offs and results obtained so far. Future perspectives of the on-going work are also presented. ...|$|R
40|$|Global System for Mobile Communication is {{the most}} popular {{standard}} for mobile phones in the World. Its promoter, the GSM Association, estimates that 82 % of the global mobile market uses the standard. GSM is used by over 3 billion people across more than 212 countries and union territories. Its ubiquity makes international roaming very common between mobile phone operators, enabling subscribers to use their phones {{in many parts of the}} world. GSM differs from its predecessors in that both signaling and <b>speech</b> <b>channels</b> are digital and thus is considered a second generation (2 G) mobile phone system. This has also meant that data communication was easy to build into the system. The main objective of this paper to overcome the problems of Mobile by using “Talk Safe Mobile Protection chip”...|$|R
40|$|The {{migration}} {{from the first}} generation of cellular telephony to the second has included a transition from an analog <b>speech</b> <b>channel</b> to a digital channel that employs digital speech codecs. As the deployment of these second-generation systems matures, system capacity concerns have increased the pressure for a more efficient encoding of speech. In addition, market pressures have contributed to the contradictory requirement of improved voice quality provided by these wireless systems. This paper presents the motivation for, and the execution of a program of standardizing a new variable-rate speech codec for the cdmaOne/cdma 2000 wireless system. Several codecs have previously been standardized. This new codec, known as the selectable mode vocoder or SMV, offers a significant improvement in voice quality over that of existing codec standards as well as the flexibility of allowing the system operator to make tradeoffs between voice quality and system capacity. 1...|$|E
40|$|Two-dimensional, {{systematic}} and parallel concatenated convolutional codes are compatibly punctured {{in order to}} provide unequal error protection (UEP). They are iteratively decoded by the MAP algorithm and other soft-in/soft-out decoders. In a case study we replace the current GSM <b>speech</b> <b>channel</b> coding scheme by this new "turbo" scheme leaving all the interleaver interfaces and the total delays unchanged. It is shown that for independent and frequency-hopped Rayleigh channels and other standard GSM channels, the frame error rate and the residual BER of the class 1 a bits drop by a factor of 1. 4 and 2. 0 respectively. Likewise the carrier to interference ratio (CIR) can be lowered by 0. 8 dB at the same performance level. Contrary to the common belief that "turbo" codes gain only with large interleavers, we have shown that together with UEP they allow gains also with the modest interleaver sizes of the GSM scheme. All this offers a potential for a future evolutionary improvement of the GSM [...] ...|$|E
40|$|In this paper, we {{analyze the}} {{temporal}} modulation char-acteristics {{of speech and}} noise from a speech/non-speech discrimination point of view. Although previous psychoacous-tic studies [3][10] have shown that low temporal modulation components are important for speech intelligibility, there is no reported analysis on modulation components {{from the point of}} view of speech/noise discrimination. Our data-driven analysis of modulation components of speech and noise reveals that speech and noise is more accurately classified by low-passed modulation frequencies than band-passed ones. Effects of additive noise on the modulation characteristics of speech signals are also analyzed. Based on the analysis, we propose a frequency adaptive modulation processing algorithm for a noise robust ASR task. The algorithm is based on <b>speech</b> <b>channel</b> classification and modulation pattern denoising. Speech recognition experiments are performed to compare the proposed algorithm with other noise robust frontends, including RASTA and ETSI AFE. Recognition results show that the frequency adaptive modulation processing is promising. Index Terms: noise robust speech recognition, temporal mod-ulation processing, spectro-temporal processing 1...|$|E
40|$|I have {{revised and}} {{explained}} {{in the case of}} digital cellular systems, the difference between T’MA and FDA&I. For ZDhAl, 1 give a general expression for the frame efikiency and the number of <b>speech</b> <b>channels</b> per frame. For both, I concentrate my study on the spectral efficiency and the capacity. I give the parameters which influence them and their nature. I have also done a qualitative comparison between the two multiple access technologies and have explained why those days TDMA is popular Lisi of symbols (SI units are implied throughout) i frequency reuse factor of the system: number of cells in a frequency reuse pattern bandwidth of the system radio channel frequency band number of mobile users in a cell number of cells per cluster area of a cel...|$|R
40|$|Part 2 : Work in ProgressInternational audienceSteganography {{plays an}} {{important}} role in the field of secret communication. The security of such communication lies in the impossibility of proving that secret communication is taking place. We evaluate the implementation of a previously published spread spectrum technique for steganography in auditive media. We have unveiled and solved several weaknesses that compromise undetectability. The spread-spectrum approach of the technique under evaluation is rather unusual for steganography and makes the secret message fit to survive A/D and D/A conversions of analogue audio telephony, re-encoded <b>speech</b> <b>channels</b> of GSM/UMTS, or VoIP. Its impact to signal statistics, which is at least concealed by the lossy channel, is reduced. There is little published on robust audio steganography, its steganalysis, and evaluation, with the possible exception of audio watermarking, where undetectability is not as important...|$|R
40|$|Recent {{research}} in speech localization and dereverberation introduced processing of the multichannel linear prediction (LP) residual of speech recorded with multiple microphones. This paper investigates the novel use of intra- and inter-channel speech prediction by proposing {{the use of}} a multichannel LP model derived from multivariate autoregression (MVAR), where current LP approaches are based on univariate autoregression (AR). Experiments were conducted on simulated anechoic and reverberant synthetic speech vowels and real speech sentences; results show that, especially at low reverberation times, the MVAR model exhibits greater prediction gains from the residual signal, compared to residuals obtained from univariate AR models for individually or jointly modelled <b>speech</b> <b>channels.</b> In addition, the MVAR model more accurately models the speech signal when compared to univariate LP of a similar prediction order and when a smaller number of microphones are deployed...|$|R
