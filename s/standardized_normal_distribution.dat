11|10000|Public
5000|$|... where S is the {{underlying}} stock, [...] is the expected {{growth rate of}} , and [...] is the stochastic volatility of [...] at time t. In equation (2), g is the mean reversion rate (gravity), which pulls the variance [...] to its long term mean , and [...] is the volatility of the volatility σ(t). dz(t) is the standard Brownian motion, i.e. , [...] is i.i.d., in particular [...] is a random drawing from a <b>standardized</b> <b>normal</b> <b>distribution</b> n~(0,1). In equation (1), {{the underlying}} [...] follows the standard geometric Brownian motion, which is also applied in Black-Scholes-Merton model, which however assumes constant volatility. The correlation between the stochastic processes (1) and (2) is introduced by correlating the two Brownian motions [...] and [...] The instantaneous correlation [...] between the Brownian motions is ...|$|E
30|$|Use Latin {{hypercube}} sampling {{to obtain}} the independent <b>standardized</b> <b>normal</b> <b>distribution</b> sampling matrix R_ 0 ^t with dimension NH×K.|$|E
30|$|The a posteriori histograms fit the <b>standardized</b> <b>normal</b> <b>distribution</b> curves {{much better}} {{than that of a}} priori ones individually, which tells that the a posteriori {{estimated}} variances for the GNSS measurements were realistic.|$|E
50|$|In the two cases, {{the result}} is a multinomial {{distribution}} with k categories. This is equivalent, with a continuous random distribution, to simulate k independent <b>standardized</b> <b>normal</b> <b>distributions,</b> or a multinormal distribution N(0,I) having k components identically distributed and statistically independent.|$|R
40|$|Abstract: To {{determine}} the safety stock {{required to achieve}} a desired service level, demands are commonly assumed to follow a <b>normal</b> probability <b>distribution.</b> However, since demands are necessarily truncated at values below zero, this paper will show that this approach underestimates the actual demand level and results in achieved service levels that are less than those targeted. Use of a left-truncated <b>normal</b> probability <b>distribution</b> – i. e., a <b>normal</b> probability <b>distribution</b> in which values below a truncation point cannot be observed – for demand to determine safety stocks will be shown to improve the achieved service level. This paper describes a methodology for using <b>standardized,</b> left-truncated <b>normal</b> <b>distributions</b> to determine safety stocks, presents tables for use in implementing this methodology, and discusses the improvements possible through this approach...|$|R
40|$|The {{methods of}} {{evaluating}} the singular multivariate <b>normal</b> <b>distribution</b> have been commonly applied {{even though the}} complete analytical proofs are not found. Recently, those evaluation methods are shown to have some errors. In this paper we present a new approach with a complete proof for evaluating the exact two-sided percentage points of a <b>standardized</b> m-variate <b>normal</b> <b>distribution</b> with a singular negative product correlation structure for m = 3 and with a singular negative equi-correlated structure for m [greater-or-equal, slanted] 3. The results are then applied to modify the existing procedures for estimating joint confidence intervals for multinomial proportions and for determining sample sizes. By extending {{the results from the}} multivariate <b>normal</b> <b>distribution</b> to the multivariate t-distribution with the corresponding singular correlation structure, we obtain the corrected two-sided exact critical values for the Analysis of Means for m = 4, 5...|$|R
30|$|From above, the RMSEs {{are almost}} the same with these two methods. And the PDFs with the {{improved}} CKF are much more fits the <b>standardized</b> <b>normal</b> <b>distribution</b> curve too, {{which is similar with}} Bearing-only tracking method. Thus, the feasibility and the superiority of the improved adaptive filter based on the CKF can be verified.|$|E
30|$|It is {{proved that}} due to the {{rotational}} symmetry of the second-moment representation of the <b>standardized</b> <b>normal</b> <b>distribution</b> Z, the geometrical distance from the origin in Z-space to any point on f(Z) =  0 is simply the number of standard deviations from the mean value point in X-space to the corresponding point on f(X) =  0 (Choi et al. 2007).|$|E
3000|$|... where r 3 denotes {{a uniform}} random number in [0, 1], r 4 {{represents}} a random number that obeys the <b>standardized</b> <b>normal</b> <b>distribution,</b> δ _q,b^t + 1 denotes the quantum rotation angle, and x̅_b^t = (1 /Q)∑_q = 1 ^Qx_q,b^t is the bth {{dimension of the}} average value x̅_^t = [...] [x̅_ 1 ^t,x̅_ 2 ^t,...,x̅_B^t] of Q quantum positions.|$|E
40|$|This Study {{evaluates the}} {{efficacy}} of Palmer drought indices to monitor hydrological droughts ill river discharges and soil moisture in selected watersheds with varying geomorphologic characteristics in region of Thessaly, Greece. The Palmer four indices (PDSI, Weighted PDSI, PHDI and the moisture anomaly Z-index) were used as indicators of meteorological drought severity. The hydrological drought severity was evaluated from the outputs of the monthly UTHBAL conceptual water balance model. The UTHBAL model was calibrated with the available observed runoff data to extend, reconstruct and produce runoff and soil moisture timeseries for the hydrologic period 1960 - 2002 at the study catchments. The produced hydrologic variables were normalized through Box-Cox transformation and <b>standardized</b> to <b>normal</b> <b>distribution.</b> The <b>standardized</b> hydrologic variables were used as all indicator of hydrological drought severity and were compared with the Palmer drought indices timeseries estimated by basin-wide meteorological data. The results showed that, in general, the Weighted PDSI and the Moisture anomaly Z-index were found to better represent river discharges and soil moisture, respectively, for all study watersheds irrespectively to their area, geophysical, and hydroclimatic characteristics. However, the results were quite variable ill the identification of specific historical drought periods. Although, the Palmer indices were Successful {{in the identification of}} drought severity of historical events, they failed to identify the drought duration...|$|R
40|$|A {{method is}} {{described}} which is potentially capable of closely estimating {{the normal range}} from laboratory data. The estimation is made on probability paper using a purposely truncated form of the “normal ” distribution. A fictitious set of data {{has been used to}} illustrate the efficiency of estimation of normals. The method has been used to estimate the normal range of blood urea. HE NEED FOR A SIMPLE AND RELIABLE METHOD for determining normal ranges is widely recognized. A number of methods have been suggested for this purpose (1 - 5); however, to date none {{has been shown to be}} completely satisfactory when treating data from a heterogenous popula-tion such as is obtained in the hospital laboratory as daily routine pro-cedure. This paper discusses a method which has the potential of treating such data, and describes a modification which will improve the results of the procedure. Probit Analysis The method of Hoffman (1) for deriving normal ranges from labora-tory data is a simplified form of a more mathematic treatment called probit analysis. In his book, Finney (6) gives a short history of the probit method, which dates back to a suggestion by Fechner in 1860, progresses to the normal equivalent deviation (N. E. D.) of Gaddum in 1933, and Fisher’s maximum likelihood analysis in 1935. Although probit analysis has had many years in which to be used and developed, relatively little has been done in connection with heterogenous distribu-tions. The probit is a unit defined in terms of the <b>standardized</b> Gaussian (<b>normal)</b> <b>distribution</b> curve...|$|R
5000|$|Multivariate <b>normal</b> <b>distribution</b> (a complex <b>normal</b> <b>distribution</b> is a bivariate <b>normal</b> <b>distribution)</b> ...|$|R
30|$|The probit {{model is}} also an {{important}} classification method under the assumption that the cumulative probability distribution must be a <b>standardized</b> <b>normal</b> <b>distribution.</b> Compared with other approaches, relatively few studies on credit scoring used the probit model. Some significant works include Fisk and Rimlinger 1979, who studied long-term credit worthiness and rescheduling of developing countries based on a probit model. Balkan 1992 predicted country credit default using a probit model that included two important political risk variables (level of democracy and political instability). Haan et al. 1997 employed the probit model for government debt rescheduling.|$|E
40|$|The bivariate normal {{integral}} {{has been}} computed and tabulated in several {{sources in the}} past 40 years. The objective {{of this paper is}} two-fold. First, matrix algebra and geometry is used to reduce the complexity of evaluating the normal double integral thru a transformation to a single integral of the cumulative univariate <b>standardized</b> <b>normal</b> <b>distribution.</b> As a result, the accuracy for the volume under the bivariate normal density is at least through 7 decimals. Secondly, the objective is to present a very user-friendly software that computes the volume over both elliptical and rectangular regions. The user may specify the level of accuracy by inputting the number of subintervals for the Simpson's Rule...|$|E
40|$|Purpose of the article: The paper {{deals with}} a time and {{probability}} analysis of stochastic graph PERT. The paper focuses on the comparison of two different approaches calculation of probability analysis. Concretely the planning time {{of the project was}} calculated. A sample PERT network graph was examined, which comprised 18 nodes and 18 real activities and 6 fictions activities. For the purpose of the analysis, the basic characteristic times were calculated in accordance with traditional approaches related to the PERT method. Methodology/methods: The implementation of the PERT algorithm is based on the critical path method (CPM). It was calculated the basic time charactetistics of the project and identificed the critical path. For probability analysis was also calculated expected value, variance and standard deviance of the activities. For calculation of the planning time was used distribution function of <b>standardized</b> <b>normal</b> <b>distribution.</b> The PERT algorithm is realized by using spreadsheet in the MS Excel. Scientific aim: of the paper is comparison of two different approaches calculation of the probability analysis and their influence on the calculation of the planning time of the project. Findings: Two different approaches calculation of the probability analysis shows on different result of values of project planning time. Approach II better reflects the difference between the values of variances of project activities. The value of variance depends on the input values of three time durations s activity estimates (pessimistic, most likely, optimistic). For higher values of probability there is a bigger difference between the values of planned times that are calculated by two described approaches. Conclusions: The problem was solved using the example project whose model (network graph) contained 18 nodes and 24 activities. For each activity have been known three time estimates (pesimitic, most likely, optimistic). Based on these estimates were calculated expected values of the duration activities and their variances. Expected values of the duration activities were used as input values to calculate the time characteristics. Variances of the activities were used as input values to calculate the variance at the nodes. For these calculations two approaches was used. The expected value of project duration (value of earliest time in last node) was the same for both approaches. For the approach I is a value {{of the variance in the}} last node less than for the approach II. These values were used as input data for calculation of planning time of the project at various levels of probability according to the <b>standardized</b> <b>normal</b> <b>distribution.</b> From obtained results dependence between the probability and size of the differences in planned times were observed. This difference increases with a probability going to one. Based on the analysis a recommendation shows to use the approach II under conditions when there are large variations between optimistic (pessimistic) estimates of activity durations and the most likely estimate of activity duration. It causes great differences in values of the variances of the activities. The approach II better reflects this dissimilarity in the variances of the activities. This approach provides longer planning times of the project opposite the approach I...|$|E
3000|$|... are {{standard}} deviations of these variables. In the proposed procedure, the vector X is transformed into the independent <b>standardized</b> <b>normal</b> vector, U.|$|R
50|$|The <b>normal</b> <b>distribution</b> {{and multivariate}} <b>normal</b> <b>distributions.</b>|$|R
5000|$|Complex <b>normal</b> <b>distribution,</b> an {{application}} of bivariate <b>normal</b> <b>distribution</b> ...|$|R
40|$|Technical and {{editorial}} problems may be encountered during {{the implementation of}} Eurocodes into the systems of national standards. The main technical problems to be solved include specification of the target reliability levels {{of different types of}} construction works, definition of characteristic values, partial and combination factors and load combination rules for structures and geotechnical design. It is expected that in the next generation of Eurocodes further harmonisation and reduction of NDPs on the basis of technical assessment will be provided. Present generation of Eurocodes encompass a suite of 58 Parts which represents a great achievement in European harmonization of structural codes. The key role in the design of new and existing structures represents the concept of reliability differentiation of construction works and the selection of target reliability level. Recommended target reliability level, expressed commonly by reliability index b = − Φ- 1 (p), where Φ denotes the <b>standardized</b> <b>normal</b> <b>distribution</b> function and p the failure probability, are provided in several documents. Load combination rules for verification of geotechnical design (ULS of type GEO) give three alternative approaches leading in some cases to considerably different results. For example, resulting dimensions of common footing may have differences about 20 to 40 %. Other technical problems of implementation include lack of guidance for application of different design situations. For example a structure under fire should be verified according to the accidental design situation for which alternative combination factors are indicated in EN 1990. During subsequent repair the structure should comply with the requirements for the transient design situation, for which, up to now, there are no explicit rules for existing structures provided by Eurocodes. Another problem is a unified definition of the characteristic values for variable actions, particularly for imposed and climatic actions, when 0. 98 fractile for time dependent components should be supplemented by probabilistic specification of resulting action effect (for example, for imposed load and wind). System of Eurocodes constitutes a great achievement in harmonization of European standards for structural design. In national implementation, it is strongly recommended to specify a unique (unambiguous) variant chosen from recommended options. Terminology and translation should be adapted to the sense of the original English version. Other manner of translations may be confusing or misleading...|$|E
40|$|Swedish {{concrete}} dams {{are designed}} and assessed based on deterministic design using safety factors. The combination of increasing age, new methods for calculation of design floods and increasing demands by society {{to ensure a}} high level of safety, has resulted in upgrading and rebuilding needs for dams. When safety is re-evaluated it is important that evaluation is based on modern safety concepts. The objective of this thesis is to describe how reliability based methodology can be used for assessment of concrete dams, how it fits into the dam safety risk management process and the present state of knowledge of relevant statistic information of resistance and load parameters. Risk management is becoming more frequently used in dam safety {{all over the world and}} structural reliability analysis can be used for safety assessment of existing dams. Since object specific information, monitoring results, known loads etcetera can be included in the analysis it gives more reliable results than a more general assessment procedure based on traditional dam safety design guidelines. In this thesis the sliding and overturning failure modes are used for analysis, but there are reasons to further analyse the failure mode formulation. Overturning is not considered in several guidelines, instead resultant location is used as an indicator, and failure is associated with sliding or overstressing. The Swedish guideline on dam safety, RIDAS, does not, unlike guidelines used elsewhere in the world, take account to cohesion when the sliding stability is estimated. Even so, cohesion is included in the sliding criterion used for analysis in this thesis. In structural reliability analysis the failure mode of interest is described by a limit state function, which is a function of a number of basic variables, described by their statistical distribution. For a concrete dam the basic variables for the sliding failure mode are self-weight (G), cohesion (c), base area (A), friction angle (φ), uplift pressure (U), hydrostatic water pressure (H) and ice load (I) and the limit state function becomes Limit state functions for overturning or other failure modes can be defined in the same way. When statistical distributions are known for the basic variables, the safety index, β, can be calculated. β is related to the probability of failure by, where Φ is the <b>standardized</b> <b>normal</b> <b>distribution</b> function. For safety evaluation, β is compared to a target safety index, βT. For dams no such target value exists. There are different approaches how to set a target and the most straightforward is calibration to existing practice, but there are also reasons to believe that target safety for dams should be based also on tolerable risk concepts. The basic variables affecting concrete dam stability are described and special attention is given to uplift, where a thorough state-of-the art is given, as it has been shown in previous research to have a large influence on dam stability. Uplift varies depending on temperature (higher uplift during cold periods), due to loads (increasing water levels may give rise to uplift pressure to increase more than the linear design assumption), is highly dependant on foundation treatment and drainage and is difficult to monitor (uplift in one point represent only a local area). A promising approach on how to use geostatistical modelling to derive statistical distribution for uplift is shown. The hydraulic conductivity beneath the dam is assumed to have certain properties (mean value and variance) and is quantified by a spatial correlation structure. Flow and uplift pressure distribution is then solved by use of a finite element program. A large number of simulations is used to obtain a statistical distribution of uplift force and moment on the whole dam base area. The uplift force active on the dam is described as where u is the uplift force, calculated from the linear uplift reduction assumption used in design, and C is a random variable based on the simulations. The treatment of moment of uplift is similar. This concept can and should be further developed, e. g. by investigating real rock properties and analyzing 3 -D modelling of hydraulic conductivities. Even though there are questions to be solved, this approach gives useful results. In the first of two examples, the capacity of a wall in a spillway chute is analyzed. The result is that the wall can withstand approximately 20 % higher water levels than that resulting from design according to RIDAS. In the second example a dam, fulfilling the requirements in RIDAS, is analysed. The results reveal that the cohesion and the friction coefficient gives the largest contribution to the uncertainty of β for sliding failure, whereas self-weight, followed by uplift and ice load, dominates the uncertainty for overturning failure. The main conclusions are that structural reliability analysis can be used as a tool in the dam safety risk management process and that the most important factors for further analysis are cohesion, friction coefficient, ice load, uplift and self-weight. The examples shown, and the results of a master thesis, indicates that today’s guideline (RIDAS) results in a number of “problems”, among them that the safety index seems to be dependant on dam type and dam height and that cross-sectional design is, at least in one example, conservative...|$|E
5000|$|Combining {{the above}} for a <b>normal</b> <b>distribution</b> [...] with both μ and σ2 unknown yields the {{following}} ancillary statistic:This simple combination is possible because the sample mean and sample variance of the <b>normal</b> <b>distribution</b> are independent statistics; {{this is only}} true for the <b>normal</b> <b>distribution,</b> and in fact characterizes the <b>normal</b> <b>distribution.</b>|$|R
40|$|The {{mixture of}} <b>normal</b> <b>distributions</b> {{provides}} a useful extension of the <b>normal</b> <b>distribution</b> for modeling of daily changes in market variables with fatter-than-normal tails and skewness. An efficient analytical Monte Carlo method is proposed for generating daily changes using a multivariate mixture of <b>normal</b> <b>distributions</b> with arbitrary covariance matrix. The main purpose of this method is to transform (linearly) a multivariate normal with an input covariance matrix into the desired multivariate mixture of <b>normal</b> <b>distributions.</b> This input covariance matrix can be derived analytically. Any linear combination of mixtures of <b>normal</b> <b>distributions</b> can {{be shown to be}} a mixture of <b>normal</b> <b>distributions...</b>|$|R
5000|$|An {{alternative}} parametric {{approach is}} {{to assume that the}} residuals follow a mixture of normal distributions; in particular, a contaminated <b>normal</b> <b>distribution</b> in which the majority of observations are from a specified <b>normal</b> <b>distribution,</b> but a small proportion are from a <b>normal</b> <b>distribution</b> with much higher variance. That is, residuals have probability [...] of coming from a <b>normal</b> <b>distribution</b> with variance , where [...] is small, and probability [...] of coming from a <b>normal</b> <b>distribution</b> with variance [...] for some ...|$|R
5000|$|In general, noncentrality {{parameters}} {{occur in}} distributions that are transformations of a <b>normal</b> <b>distribution.</b> The [...] "central" [...] versions {{are derived from}} <b>normal</b> <b>distributions</b> that have a mean of zero; the noncentral versions generalize to arbitrary means. For example, the standard (central) chi-squared distribution is the distribution of a sum of squared independent standard <b>normal</b> <b>distributions,</b> i.e., <b>normal</b> <b>distributions</b> with mean 0, variance 1. The noncentral chi-squared distribution generalizes this to <b>normal</b> <b>distributions</b> with arbitrary mean and variance.|$|R
25|$|This simple {{combination}} is possible because the sample mean and sample variance of the <b>normal</b> <b>distribution</b> are independent statistics; {{this is only}} true for the <b>normal</b> <b>distribution,</b> and in fact characterizes the <b>normal</b> <b>distribution.</b>|$|R
30|$|The {{white noise}} with the <b>normal</b> <b>distribution</b> {{characteristics}} is {{the premise of}} the time series model, that is, the multi-element fitting residual errors δ (t) should obey the <b>normal</b> <b>distribution.</b> The existing function of the <b>normal</b> <b>distribution</b> test in MATLAB was used to get that the multi-element fitting residual errors δ (t) obeyed the <b>normal</b> <b>distribution.</b>|$|R
5000|$|The two {{generalized}} normal families described here, {{like the}} skew normal family, are parametric families that extends the <b>normal</b> <b>distribution</b> {{by adding a}} shape parameter. Due to {{the central role of}} the <b>normal</b> <b>distribution</b> in probability and statistics, many distributions can be characterized in terms of their relationship to the <b>normal</b> <b>distribution.</b> For example, the lognormal, folded normal, and inverse <b>normal</b> <b>distributions</b> are defined as transformations of a normally-distributed value, but unlike the generalized normal and skew-normal families, these do not include the <b>normal</b> <b>distributions</b> as special cases.Actually all distributions with finite variance are in the limit highly related to the <b>normal</b> <b>distribution.</b> The Student-t distribution, the Irwin-Hall distribution and the Bates distribution also extend the <b>normal</b> <b>distribution,</b> and include in the limit the <b>normal</b> <b>distribution.</b> So there is no strong reason to prefer the [...] "generalized" [...] <b>normal</b> <b>distribution</b> of type 1, e.g. over a combination of Student-t and a normalized extended Irwin-Hall - this would include e.g. the triangular distribution (which cannot be modeled by the generalized Gaussian type 1).|$|R
2500|$|Multivariate <b>normal</b> <b>distribution</b> — a {{generalization}} of the <b>normal</b> <b>distribution</b> in multiple dimensions ...|$|R
50|$|<b>Normal</b> <b>distributions</b> are symmetrical, {{bell-shaped}} distributions {{that are}} useful in describing real-world data. The standard <b>normal</b> <b>distribution,</b> represented by the letter Z, is the <b>normal</b> <b>distribution</b> having a mean of 0 and {{a standard deviation of}} 1.|$|R
5000|$|Each {{component}} of the velocity vector has a <b>normal</b> <b>distribution</b> with mean [...] and standard deviation , so the vector has a 3-dimensional <b>normal</b> <b>distribution,</b> {{a particular kind of}} multivariate <b>normal</b> <b>distribution,</b> with mean [...] and standard deviation [...]|$|R
5000|$|Sampling {{from the}} matrix <b>normal</b> <b>distribution</b> {{is a special}} case of the {{sampling}} procedure for the multivariate <b>normal</b> <b>distribution.</b> Let [...] be an n by p matrix of np independent samples from the standard <b>normal</b> <b>distribution,</b> so that ...|$|R
40|$|In searching a {{opportunity}} distribution {{a function}} {{from one or}} more random usually in finishing with selected method, to be obtained by data is correctness. method of Transformation variable {{is one of the}} technique all important in determining probability functions. Determination of function of densities the probability in pass Jacobian and process integrate from <b>Normal</b> <b>distribution</b> Lean. <b>Normal</b> <b>Distribution</b> of Standard is one of the distribution function which used many from at other distribution functions. At the writing of this thesis aim to show probability functions to be alighted from by <b>Normal</b> <b>distribution</b> of Standard by 2 random variable and 3 random variable in the form of variable transformation. The <b>Normal</b> <b>distribution</b> of Standard with function of transformation selected variable at 2 obtained by random variable of <b>normal</b> <b>distribution</b> form, distribution of exponential-(), distribution of Cauchy-(1, 0), distribution of beta 2 -, distribution of gamma-(1, 1), and distribution of beta 1 -. While from <b>Normal</b> <b>distribution</b> of Standard with function of transformation selected variable at 3 obtained by random variable of <b>normal</b> <b>distribution</b> for-, <b>normal</b> <b>distribution</b> -, and distribution of beta 2...|$|R
5000|$|In {{a special}} case when [...] the split <b>normal</b> <b>distribution</b> reduces to <b>normal</b> <b>distribution</b> with {{variance}} [...]|$|R
5000|$|It also {{coincides with}} a zero-mean <b>normal</b> <b>distribution</b> {{truncated}} from below at zero (see truncated <b>normal</b> <b>distribution)</b> ...|$|R
30|$|<b>Normal</b> <b>distribution</b> of the {{variables}} was evaluated with Kolmogorov-Smirnov tests. The results showed <b>normal</b> <b>distribution</b> for all variables.|$|R
40|$|Abstract: The log <b>normal</b> <b>distribution</b> is {{as popular}} in {{engineering}} as the <b>normal</b> <b>distribution</b> is in statistics. However, {{there has been}} little work relating to order statistics from the log <b>normal</b> <b>distribution.</b> In this note, explicit expressions are derived for the moments of order statistics from the log <b>normal</b> <b>distribution</b> by using a formula due to Withers. The usefulness of the result is illustrated through two data sets...|$|R
