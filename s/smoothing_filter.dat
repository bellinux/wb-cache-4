369|741|Public
25|$|The {{device is}} backward-compatible with {{a subset of}} the PlayStation Portable and PS One games {{digitally}} released on the PlayStation Network via the PlayStation Store. However, PS One Classics and TurboGrafx-16 titles were not compatible at launch. The Vita's dual analog sticks are supported on selected PSP games via button mapping. The graphics for PSP releases are up-scaled, with a <b>smoothing</b> <b>filter</b> to reduce pixelation.|$|E
2500|$|The primary {{application}} of rectifiers is to derive DC power from an AC supply (AC to DC converter). Rectifiers are used inside the power supplies {{of virtually all}} electronic equipment. [...] AC-DC power supplies may be broadly divided into linear power supplies and switched-mode power supplies. [...] In such power supplies, the rectifier will be in series following the transformer, and {{be followed by a}} <b>smoothing</b> <b>filter</b> and possibly a voltage regulator.|$|E
50|$|Co-author with Abraham Savitzky of the Savitzky-Golay <b>smoothing</b> <b>filter.</b>|$|E
40|$|Abstract: The {{combination}} of genetic algorithms, <b>smoothing</b> <b>filters</b> and geophysical tomography {{is used in}} solving the geophysical inversion problem. This hybrid technique is developed to improve the results obtained by using genetic algorithms only. The application of <b>smoothing</b> <b>filters</b> can improve the performance of GA implementation for solving the geophysical inversion problem. Some test-examples and the obtained comparative results are presented...|$|R
5000|$|Step 3 - Apply order {{statistics}} and <b>smoothing</b> <b>filters</b> {{to obtain the}} MAX and MIN filter output ...|$|R
40|$|The {{methods of}} the {{harmonic}} analysis of the rectified voltage at the m-pulse rectifier outlet has been developed, the norm of the psophometric voltage at the outlet of the <b>smoothing</b> <b>filters</b> has been justified, the methods of selection of the diagrams and parameters of the <b>smoothing</b> <b>filters</b> have been offered. The introduction of the developments in the existing systems of the electric supply allows to ensure the normal functioning of the communication lines, layed along the electric railways of direct current, and the devices of automatic blocking, to reduce the losses of the electric energy and labour expenditures for adjustment of the <b>smoothing</b> <b>filters,</b> to obtain {{the economy of the}} electrotechnical materials. Available from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
5000|$|... 1. A {{two-dimensional}} <b>smoothing</b> <b>filter</b> {{is separated}} in this sample: ...|$|E
5000|$|... polynomials {{can be used}} {{to define}} the <b>smoothing</b> <b>filter</b> [...] of a multiresolution {{analysis}} (MRA). Since the appropriate boundary conditions for an MRA are [...] and , the <b>smoothing</b> <b>filter</b> of an MRA can be defined so that the magnitude of the low-pass [...] can be associated to Legendre polynomials according to: [...]|$|E
5000|$|... 2. Another {{two-dimensional}} <b>smoothing</b> <b>filter</b> with stronger {{weight in}} the middle: ...|$|E
50|$|Other edge-preserving <b>smoothing</b> <b>filters</b> include: {{anisotropic}} diffusion, weighted least squares, edge-avoiding wavelets, geodesic editing, guided filtering, and domain transforms.|$|R
30|$|Bilateral {{filtering}} exhibits superior {{properties in}} comparison to the <b>smoothing</b> <b>filters</b> routinely applied for noise reduction in PET. Bilateral filtering allows to increase the SNR of PET image data while preserving spatial resolution at object boundaries, thus maintaining the quantitative accuracy of SUVmax values even in small lesions. Therefore, it seems worthwhile to investigate more thoroughly the potential of this filter as a replacement of the standard <b>smoothing</b> <b>filters</b> and for improving the image quality of diagnostic PET.|$|R
40|$|We {{propose a}} new measure, the method noise, to {{evaluate}} and compare the performance of digital image denoising methods. We first compute and analyze this method noise for a wide class of denoising algorithms, namely the local <b>smoothing</b> <b>filters.</b> Second, we propose a new algorithm, the non local means (NL-means), based on a non local averaging of all pixels in the image. Finally, we present some experiments comparing the NL-means algorithm and the local <b>smoothing</b> <b>filters.</b> 1...|$|R
5000|$|Savitzky-Golay <b>smoothing</b> <b>filter</b> {{based on}} the least-squares fitting of polynomials to {{segments}} of the data ...|$|E
50|$|Iterative {{relaxation}} of solutions is commonly dubbed smoothing because {{relaxation of}} certain equations (such as Laplace's equation) resembles repeated {{application of a}} local <b>smoothing</b> <b>filter</b> to the solution vector.Another name is stationary linear iterative method.|$|E
5000|$|... where [...] {{is defined}} as the square region of window size, and [...] is the window width of the <b>smoothing</b> <b>filter</b> which [...] equals to [...] Therefore, the MAX and MIN filter will form a new 2-D matrix for {{envelope}} surface which will not change the original 2-D input data.|$|E
50|$|In {{analytical}} chemistry, Savitzky-Golay <b>smoothing</b> <b>filters</b> {{are used}} {{for the analysis of}} spectroscopic data. They can improve signal-to-noise ratio with minimal distortion of the spectra.|$|R
40|$|Smoothing along {{structures}} {{apparent in}} seismic images can enhance these structural features while preserving important discontinuities such as faults or channels. Filters appropriate for such smoothing must seamlessly adapt to {{variations in the}} orientation and coherence of image features. I describe an implementation of <b>smoothing</b> <b>filters</b> that does this and is both computationally efficient and simple to implement. Structure-oriented filters lead naturally to the computation of structure-oriented semblance, an attribute commonly used to highlight discontinuities in seismic images. Semblance is defined in this paper as simply the ratio of a squared smoothed-image to a smoothed squared-image. This definition of semblance generalizes that commonly used today, because an unlimited variety of <b>smoothing</b> <b>filters</b> {{can be used to}} compute the numerator and denominator images in the semblance ratio. The <b>smoothing</b> <b>filters</b> described in this paper yield an especially flexible method for computing structure-oriented semblance. Key words: seismic image smoothing semblance...|$|R
5000|$|... #Caption: Figure 1 - Magnitude of the {{transfer}} function for Legendre multiresolution <b>smoothing</b> <b>filters.</b> Filter [...] for a few orders: =1 (solid line), =3 (dot line), and =5 (dashdot line).|$|R
5000|$|... #Caption: Figure 2 - Magnitude of the {{transfer}} function for Mathieu multiresolution analysis filters. (<b>smoothing</b> <b>filter</b> [...] and detail filter [...] for a few Mathieu parameters.) (a) , q=5, a = 1.85818754...; (b) , q = 10, a = −2.3991424...; (c) , q = 10, a = 25.5499717...; (d) , q = 10, a = 27.70376873...|$|E
50|$|The Kuwahara filter is a {{non-linear}} <b>smoothing</b> <b>filter</b> used in {{image processing}} for adaptive noise reduction. Most filters {{that are used}} for image smoothing are linear low-pass filters that effectively reduce noise but also blur out the edges. However the Kuwahara filter is able to apply smoothing on the image while preserving the edges.|$|E
50|$|Further {{developments}} on the Allan variance {{was performed to}} let the hardware bandwidth be reduced by software means. This development of a software bandwidth allowed for addressing the remaining noise and the method is now referred to modified Allan variance. This bandwidth reduction technique {{should not be confused}} with the enhanced variant of modified Allan variance which also changes a <b>smoothing</b> <b>filter</b> bandwidth.|$|E
40|$|International audienceTextual {{document}} image denoising is {{the main}} issue of this work. Therefore, we introduce a comparative study between two state-of-the-art denoising frameworks : local and non-local <b>smoothing</b> <b>filters.</b> The choice of both of these frameworks is directly related to their ability to deal with local data corruption and to process oriented patterns, a major characteristic of textual documents. Local <b>smoothing</b> <b>filters</b> incorporate anisotropic diffusion approaches where as non-local filters introduce non-local means. Experiments conducted on synthetic and real degraded document images illustrate the behaviour of the studied frameworks on the visual quality and even on the optical recognition accuracy rates...|$|R
5000|$|... #Caption: A pixel {{does not}} need to be {{rendered}} as a small square. This image shows alternative ways of reconstructing an image from a set of pixel values, using dots, lines, or <b>smooth</b> <b>filtering.</b>|$|R
30|$|Regarding various {{combinations}} of <b>smoothing</b> <b>filters,</b> {{it was found}} that the combined LOESS–ARMA scheme yields a significantly better noise reduction performance. The combined method is more efficient when it is applied for smoothing the pressure derivative data.|$|R
50|$|The {{device is}} backward-compatible with {{a subset of}} the PlayStation Portable and PS One games {{digitally}} released on the PlayStation Network via the PlayStation Store. However, PS One Classics and TurboGrafx-16 titles were not compatible at launch. The Vita's dual analog sticks are supported on selected PSP games via button mapping. The graphics for PSP releases are up-scaled, with a <b>smoothing</b> <b>filter</b> to reduce pixelation.|$|E
50|$|The {{device is}} fully backwards-compatible with PlayStation Portable games {{digitally}} released on the PlayStation Network via the PlayStation Store. However, PSone Classics and PS2 titles were not compatible {{at the time}} of the primary public release in Japan. The Vita's dual analog sticks will be supported on selected PSP games. The graphics for PSP releases will be up-scaled, with a <b>smoothing</b> <b>filter</b> to reduce pixelation.|$|E
50|$|A {{bilateral}} filter is a non-linear, edge-preserving, and noise-reducing <b>smoothing</b> <b>filter</b> for images. It {{replaces the}} intensity of each pixel with a weighted average of intensity values from nearby pixels. This weight can {{be based on a}} Gaussian distribution. Crucially, the weights depend not only on Euclidean distance of pixels, but also on the radiometric differences (e.g., range differences, such as color intensity, depth distance, etc.). This preserves sharp edges.|$|E
40|$|Abstract—Harris corner {{detection}} algorithm called Harris corner detector is a {{very effective}} corner algorithm for gray-scale images. The corners extracted by Harris corner detector are stable, reliable, homogeneous and reasonable. However, it has own inevitable limitations. For the shape recognition of parts, an improved Harris corner detector is proposed in this paper. Based on the analysis of Harris corner detector, B-spline function is chosen as <b>smooth</b> <b>filter</b> instead of Gaussian function. And then the convolution template of B-spline function is provided. Some experiment {{results indicate that the}} improved Harris corner detector is effective and superior. Index Terms—hape recognition, Harris corner detector, <b>smooth</b> <b>filter,</b> Gaussion function, B-spline function...|$|R
30|$|Bilateral {{filtering}} allows {{to increase}} the SNR of PET image data while preserving spatial resolution and preventing smoothing-induced underestimation of SUVmax values in small lesions. Bilateral filtering seems a promising and superior alternative to standard <b>smoothing</b> <b>filters.</b>|$|R
5000|$|... #Caption: Scale-space kernels. Ideal {{discrete}} gaussian {{based on}} bessel functions (red), and two-pole-pair forward/backward recursive <b>smoothing</b> <b>filters</b> (blue) with poles {{as described in}} the text. Top shows individual kernels, and bottom is their cumulative convolution with each other; t = 1, 2, 4.|$|R
5000|$|... #Caption: Z-plane {{locations}} of four poles (X) and four zeros (circles) for a <b>smoothing</b> <b>filter</b> using forward/backward biquad to smooth to a scale t = 2, {{with half the}} smoothing from the poles and half from the zeros. The zeros are all at Z = -1; the poles are at Z = 0.172 and Z = 5.83. The poles outside the unit circle are implemented by filtering backwards with the stable poles.|$|E
5000|$|The {{size of the}} Gaussian filter: the <b>smoothing</b> <b>filter</b> used in {{the first}} stage {{directly}} affects the results of the Canny algorithm. Smaller filters cause less blurring, and allow detection of small, sharp lines. A larger filter causes more blurring, smearing out the value of a given pixel over a larger area of the image. Larger blurring radii are more useful for detecting larger, smoother edges - for instance, the edge of a rainbow.|$|E
5000|$|Tables of {{convolution}} coefficients, calculated in {{the same}} way for m up to 25, were published for the Savitzky-Golay <b>smoothing</b> <b>filter</b> in 1964, The value of the central point, z = 0, is obtained from a single set of coefficients, a0 for smoothing, a1 for 1st. derivative etc. The numerical derivatives are obtained by differentiating Y. This means that the derivatives are calculated for the smoothed data curve. For a cubic polynomial ...|$|E
40|$|Image Enhancement {{is the art}} of {{examining}} images for identifying objects and judging their significance. The aim of image enhancement is to improve the interpretability or perception of information in images for human viewers, or to provide `better' input for other automated image processing techniques. Smoothing is often usedto reduce noise within an image or to produce a less pixilated image. This paper proposes a new idea for enhancement of an image using <b>smoothing</b> <b>filters</b> and the image noise is mostly unwanted and manifested in the pixels of an image and the main application of image averaging is noise removal. In this paper, we deal withimage enhancement using <b>smoothing</b> <b>filters</b> with the help of parallel model in order {{to improve the quality of}} the image and as well as to help to solve various complex image processing tasks in the future. This paper focuses on an approach which tries to combine the advantages of the various <b>smoothing</b> <b>filters</b> techniques and proposes a parallel model. This parallel model can be implemented using various models, considering available resources with the server. After the review of earlier publications on this topic available and a comparative study of their advantages and disadvantages a parallel model that is simpler to implement and efficient has been proposed in this work...|$|R
40|$|Surface models {{derived from}} medical image data often exhibit artifacts, such as noise and staircases, {{which can be}} reduced by {{applying}} mesh <b>smoothing</b> <b>filters.</b> Usually, an iterative adaption of smoothing parameters to the specific data and continuous re-evaluation of accuracy and curvature is required. Depending {{on the number of}} vertices and the filter algorithm, computation time may vary strongly and interfere with an interactive mesh generation procedure. In this paper, we present an approach to improve the handling of mesh <b>smoothing</b> <b>filters.</b> Based on a GPU mesh smoothing implementation, model quality is evaluated in real-time and provided to the user as quality graphs to support the mental optimization of input parameters. Moreover, this framework is used to find optimal smoothing parameters automatically and to provide data-specific parameter suggestions...|$|R
40|$|Gated SPECT (GSPECT) {{offers the}} {{possibility}} of obtaining additional functional information from perfusion studies, including calculation of left ventricular ejection fraction (LVEF). The calculation of LVEF relies upon {{the identification of the}} endocardial surface, which will be affected by the spatial resolution and statistical noise in the reconstructed images. The aim {{of this study was to}} compare LVEFs and ventricular volumes calculated from GSPECT using six reconstruction filters. GSPECT and radionuclide ventriculography (RNVG) were performed on 40 patients; filtered back projection was used to reconstruct the datasets with each filter. LVEFs and volumes were calculated using the Cedars-Sinai QGS package. The correlation coefficient between RNVG and GSPECT ranged from 0. 81 to 0. 86 with higher correlations for <b>smoother</b> <b>filters.</b> The narrowest prediction interval was 111 +/- 2 %. There was a trend towards higher LVEF values with <b>smoother</b> <b>filters,</b> the ramp filter yielding LVEFs 2. 55 +/- 3. 10 % (p &# 60; 0. 001) lower than the Hann filter. There was an overall fall in ventricular volumes with <b>smoother</b> <b>filters</b> with a mean difference of 13. 98 +/- 10. 15 ml (p &# 60; 0. 001) in EDV between the Butterworth- 0. 5 and Butterworth- 0. 3 <b>filters.</b> In conclusion, <b>smoother</b> reconstruction <b>filters</b> lead to lower volumes and higher ejection fractions with the QGS algorithm, with the Butterworth- 0. 4 filter giving the highest correlation with LVEFs from RNVG. Even if the optimal filter is chosen the uncertainty in the measured ejection fractions is still too great to be clinically acceptable...|$|R
