24|44|Public
50|$|Single-frequency {{signaling}} (SF) is {{line signaling}} (in telephony) in which dial pulses or <b>supervisory</b> <b>signals</b> are conveyed {{by a single}} voice-frequency tone in each direction. SF and similar systems were used in 20th-century carrier systems.|$|E
50|$|The Defense Data Network (DDN) {{transmitted}} DC {{line signaling}} pulses or <b>supervisory</b> <b>signals,</b> or both, over carrier channels or cable pairs on a four wire circuit basis using a 2600 Hz signal tone. The conversion into tones, or vice versa, {{is done by}} SF signal units.|$|E
50|$|A typical System 4 {{blue box}} had a keypad (for sending four-bit digit signals) plus four buttons {{for the four}} <b>supervisory</b> <b>signals</b> (clear-forward, seize-terminal, seize-transit, and transfer-to-operator).After some experimentation, nimble-fingered phreaks found that all they really needed was two buttons, one for each frequency. With practice, it was {{possible}} to generate all the signals with sufficient timing precision manually, including the digit signals. This made it possible to make the blue box quite small.|$|E
40|$|We present {{experimental}} {{results for the}} effect of an increased <b>supervisory</b> <b>signal</b> power in a high-loss loopback supervisory system in an optically amplified wavelength division multiplexing (WDM) transmission line. The study focuses on the investigation of increasing the input power for the <b>supervisory</b> <b>signal</b> and {{the effect on the}} co-propagating WDM data signals using different channel spacing. This investigation is useful for determining the power limitation of the <b>supervisory</b> <b>signal</b> if extra power is needed to improve the monitoring. The study also shows the effect of spacing {{on the quality of the}} <b>supervisory</b> <b>signal</b> itself because of interaction with adjacent data signals...|$|R
5000|$|Frequency-exchange <b>signaling</b> {{applies to}} <b>supervisory</b> <b>signaling</b> and user-information transmission.|$|R
40|$|In this paper, {{we present}} {{experimental}} results for monitoring long distance WDM communication links using a line monitoring system suitable for legacy optically amplified long-haul undersea systems. This monitoring system {{is based on}} setting up a simple, passive, low cost high-loss optical loopback circuit at each repeater that provides {{a connection between the}} existing anti-directional undersea fibres, and can be used to define fault location. Fault location is achieved by transmitting a short pulse <b>supervisory</b> <b>signal</b> along with the WDM data signals where a portion of the overall signal is attenuated and returned to the transmit terminal by the loopback circuit. A special receiver is used at the terminal to extract the weakly returned <b>supervisory</b> <b>signal</b> where each <b>supervisory</b> <b>signal</b> is received at different times corresponding to different optical repeaters. Therefore, the degradation in any repeater appears on its corresponding <b>supervisory</b> <b>signal</b> level. We use a recirculating loop to simulate a 4600 km fibre link, on which a high-loss loopback supervisory system is implemented. Successful monitoring is accomplished through the production of an appropriate <b>supervisory</b> <b>signal</b> at the terminal that is detected and identified in a satisfactory time period after passing through up to 45 dB attenuation in the loopback circuit. © 2012 Elsevier B. V. All rights reserved...|$|R
50|$|Line {{signaling}} {{is concerned}} with conveying information {{on the state of}} the line or channel, such as on-hook, off-hook (answer supervision and disconnect supervision, together referred to as supervision), ringing current (alerting), and recall. In the middle 20th century, supervision signals on long-distance trunks in North America were usually inband, for example at 2600 Hz, necessitating a notch filter to prevent interference. Late in the century, all <b>supervisory</b> <b>signals</b> were out of band. With the advent of digital trunks, supervision signals are carried by robbed bits or other bits in the E1-carrier dedicated to signaling.|$|E
50|$|This {{was also}} an in-band system but, instead of using multifrequency signals for digits, it used four 35 ms pulses of tone, {{separated}} by 35 ms of silence, to represent digits in four-bit binary code, with 2400 Hz as a '0' and 2040 Hz as a '1'. The <b>supervisory</b> <b>signals</b> used the same two frequencies, but each supervisory signal started with both tones together (for 150 ms) followed, without a gap, by a long (350 ms) or short (100 ms) period of a single tone of 2400 Hz or 2040 Hz. Phreaks in Europe built System 4 blue boxes that generated these signals. Because System 4 was used only on international circuits, {{the use of these}} blue boxes was more specialized.|$|E
40|$|The {{current review}} focuses on how {{exposure}} to linguistic input affects performance on various cognitive tasks, including individuation, categorization and category learning, and inductive inference. We review two theoretical accounts of effects of words. Proponents of one account argue that words have top-down effects on cognitive tasks, and, as such, function as <b>supervisory</b> <b>signals.</b> Proponents {{of the other}} account suggest that early in development, words, {{just like any other}} perceptual feature, are first and foremost part of the stimulus input, and they influence cognitive tasks in a bottom-up, non-supervisory fashion. We then review evidence supporting each account. We conclude that, although much research is needed, there is a large body of evidence indicating that words start our like other perceptual features, but they may become <b>supervisory</b> <b>signals</b> in the course of development...|$|E
40|$|In this study, {{we propose}} a novel {{deep neural network}} and its {{supervised}} learning method that uses a feedforward <b>supervisory</b> <b>signal.</b> The method is inspired by the human visual system and performs human-like association-based learning without any backward error propagation. The feedforward <b>supervisory</b> <b>signal</b> that produces the correct result is preceded by the target signal and associates its confirmed label with the classification result of the target signal. It effectively uses {{a large amount of}} information from the feedforward signal, and forms a continuous and rich learning representation. The method is validated using visual recognition tasks on the MNIST handwritten dataset. Comment: Presented at MLINI- 2016 workshop, 2016 (arXiv: 1701. 01437...|$|R
50|$|The {{original}} Picturephone {{system used}} contemporary crossbar and multi-frequency operation. Lines and trunks were six wire, one pair each way for video and one pair two way for audio. MF address signaling on the audio pair was supplemented by a Video <b>Supervisory</b> <b>Signal</b> (VSS) looping {{around on the}} video quad to ensure continuity. More complex protocols were later adopted for conferencing.|$|R
40|$|We {{propose a}} novel {{learning}} method for multilayered neural networks which uses feedforward <b>supervisory</b> <b>signal</b> and associates classification {{of a new}} input with that of pre-trained input. The proposed method effectively uses rich input information in the earlier layer for robust leaning and revising internal representation in a multilayer neural network. Comment: This paper has been withdrawn by the author since the review process for the conference to which it was applied ende...|$|R
40|$|<b>Supervisory</b> <b>signals</b> {{have the}} {{potential}} to make low-dimensional data representations, like those learned by mixture and topic models, more interpretable and useful. We propose a framework for training latent variable models that explicitly balances two goals: recovery of faithful generative explanations of high-dimensional data, and accurate prediction of associated semantic labels. Existing approaches fail to achieve these goals due to an incomplete treatment of a fundamental asymmetry: the intended application is always predicting labels from data, not data from labels. Our prediction-constrained objective for training generative models coherently integrates loss-based <b>supervisory</b> <b>signals</b> while enabling effective semi-supervised learning from partially labeled data. We derive learning algorithms for semi-supervised mixture and topic models using stochastic gradient descent with automatic differentiation. We demonstrate improved prediction quality compared to several previous supervised topic models, achieving predictions competitive with high-dimensional logistic regression on text sentiment analysis and electronic health records tasks while simultaneously learning interpretable topics...|$|E
40|$|The {{ability to}} learn is a {{potentially}} compelling and important quality for interactive synthetic characters. To that end, we describe a practical approach to real-time learning for synthetic characters. Our implementation {{is grounded in the}} techniques of reinforcement learning and informed by insights from animal training. It simpli- es the learning task for characters by (a) enabling them to take advantage of predictable regularities in their world, (b) allowing them to make maximal use of any <b>supervisory</b> <b>signals,</b> and (c) making them easy to train by humans...|$|E
40|$|A long-standing {{question}} in cognitive sciences and machine learning {{is how a}} system can develop highlevel concepts and categories which are useful for motor and cognitive control. I propose an architecture which learns a hierarchy of increasingly abstract, invariant features. Invariance is achieved by selecting information which reflects distinctions present in <b>supervisory</b> <b>signals</b> conveyd by contextual inputs. The main hypothesis is that the right contextual information can be efficiently distributed by associations and attentional process. The original sources of contextual information are specialised systems which reflect the innate, hard-wired behavioural goals of the system. Sensorimotor coordination generates structured sensory stimuli and the intrinsic contextual signals can select the behaviourally significant structures...|$|E
40|$|This work {{proposes a}} {{learning}} method for deep architectures that {{takes advantage of}} sequential data, in particular from the temporal coherence that naturally exists in unlabeled video recordings. That is, two successive frames are likely to contain the same object or objects. This coherence {{is used as a}} <b>supervisory</b> <b>signal</b> over the unlabeled data, and is used to improve the performance on a supervised task of interest. We demonstrate the effectiveness of this method on some pose invariant object and face recognition tasks. 1...|$|R
40|$|In {{this work}} we propose a {{technique}} that transfers supervision between images from different modalities. We use learned representations from a large labeled modality as a <b>supervisory</b> <b>signal</b> for training representations for a new unlabeled paired modality. Our method enables learning of rich representations for unlabeled modalities {{and can be used}} as a pre-training procedure for new modalities with limited labeled data. We show experimental results where we transfer supervision from labeled RGB images to unlabeled depth and optical flow images and demonstrate large improvements for both these cross modal supervision transfers. Code, data and pre-trained models are available at [URL] Updated version (v 2) contains additional experiments and result...|$|R
5000|$|Supervised {{learning}} is the machine learning task of inferring a function from [...] The training data {{consist of a}} set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the <b>supervisory</b> <b>signal).</b> A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a [...] "reasonable" [...] way (see inductive bias).|$|R
30|$|The {{procedure}} for pattern classification using 1 DARMF {{can be further}} developed to an algorithm, named PC 1 DARMF algorithm. It is a supervised pattern classification approach. The fundamental of this algorithm is to realize signal geometry shape matching using 1 DARMF as a tool in an iterative way. If the <b>supervisory</b> <b>signals</b> denote different types of physical meanings, for example representing different operation conditions or fault types in dynamic processes, this algorithm could achieve faults diagnosis through the signal geometry shape matching. In general, PC 1 DARMF algorithm is meaningful in two levels: first, it serves for the type classification purpose and secondly a feature extractor from nonstationary signals with proper parameter settings.|$|E
40|$|We have {{constructed}} an inexpensive, video-based, motorized {{tracking system}} that learns to track a head. It uses real time graphical user inputs or an auxiliary infrared detector as <b>supervisory</b> <b>signals</b> to train a convolutional neural network. The inputs to the neural network consist of normalized luminance and chrominance images and motion information from frame di erences. Subsampled images {{are also used}} to provide scale invariance. During the online training phase, the neural network rapidly adjusts the input weights depending upon {{the reliability of the}} di erent channels in the surrounding environment. This quick adaptation allows the system to robustly track a head even when other objects are moving within a cluttered background. ...|$|E
40|$|Abstract. Engineering {{approaches}} to stereo typically use explicit {{search for the}} best matching between left and right sub-windows, which involves a high cost of search and unstable performance {{in the presence of}} binocular inconsistency and weak texture. The brain does not seem to conduct explicit search in the V 1 and V 2 cortex. But the mechanisms that the brain employs to integrate binocular disparity into 3 -D perception is still largely a mystery. The work presented in this paper focuses on an important issue of integrated stereo: How the same cortex can perform recognition and perception by generating a topographic disparity-tuning map using top-down connections. As top-down connections with objectclass <b>supervisory</b> <b>signals</b> result in topographic class maps, the model presented here clarifies that stereo can be processed by a unified in-place internal representation. ...|$|E
40|$|We {{present an}} {{unsupervised}} learning {{framework for the}} task of monocular depth and camera motion estimation from unstructured video sequences. We achieve this by simultaneously training depth and camera pose estimation networks using the task of view synthesis as the <b>supervisory</b> <b>signal.</b> The networks are thus coupled via the view synthesis objective during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performing comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performing favorably with established SLAM systems under comparable input settings. Comment: Accepted to CVPR 2017. Project webpage: [URL]...|$|R
5000|$|When {{the call}} starts to ring, the caller uses the blue box {{to send a}} 2600 Hz tone (or 2600+2400 Hz on many {{international}} trunks followed by a 2400 Hz tone). The 2600 Hz is a <b>supervisory</b> <b>signal,</b> because it indicates {{the status of a}} trunk; on hook (tone) or off-hook (no tone). By playing this tone, you are convincing {{the far end of the}} connection that you've hung up and it should wait. When the tone stops, the trunk will go off-hook and on-hook (known as a supervision flash), making a [...] "Ka-Cheep" [...] noise, followed by silence. This is the far end of the connection signalling to the near end that it is now waiting for routing digits.|$|R
40|$|The {{first chapter}} of this {{dissertation}} studies a principal-supervisor-agent model in which a privately informed supervisor is susceptible to collusion. We explore how the supervisory information helps the principal to extract information rent from the agent when the informational asymmetry between the supervisor and agent results in inefficient collusion on their part. The optimal collusion-proof mechanism is such that the principal receives expected payoff that {{is as high as}} in the direct supervision benchmark in which the principal publicly observes the <b>supervisory</b> <b>signal.</b> Under some parameters, the principal can extract more rent from the agent, and the equilibrium is more efficient. The second chapter analyzes an informed principal problem in which the principal privately observes the <b>supervisory</b> <b>signal.</b> The principal 2 ̆ 7 s expected payoff is strictly higher than the case in which the signal is publicly observed. This is a common feature of the independent type cases, except that this result holds even if preferences are quasi-linear. Unlike in the independent case, efficiency is constrained by the principal 2 ̆ 7 s incentive problem. The third chapter considers a collusion formation process in which the supervisor proposes the side mechanism. We show that the informed principal problem within the coalition does not affect the optimal design of the grand mechanism. We then consider the supervisor 2 ̆ 7 s incentive to exit once she obtains new information. The principal {{can take advantage of the}} implicit information revealed by the supervisor 2 ̆ 7 s exit decision. The direct supervision benchmark outcome can be attained by a simple mechanism if the coalition cannot write a contract based on the exit decision, or the principal can publicly observe the timing of the supervisor 2 ̆ 7 s exit decision. ...|$|R
40|$|The {{state-of-the-art}} of {{face recognition}} has been significantly advanced by {{the emergence of}} deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID 3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net and GoogLeNet to make them suitable to face recognition. Joint face identification-verification <b>supervisory</b> <b>signals</b> are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99. 53 % LFW face verification accuracy and 96. 0 % LFW rank- 1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end...|$|E
40|$|<b>Supervisory</b> <b>signals</b> {{can help}} topic models {{discover}} low-dimensional data representations {{that are more}} interpretable for clinical tasks. We propose a framework for training supervised latent Dirichlet allocation that balances two goals: faithful generative explanations of high-dimensional data and accurate prediction of associated class labels. Existing approaches fail to balance these goals by not properly handling a fundamental asymmetry: the intended task is always predicting labels from data, not data from labels. Our new prediction-constrained objective trains models that predict labels from heldout data well while also producing good generative likelihoods and interpretable topic-word parameters. In a case study on predicting depression medications from electronic health records, we demonstrate improved recommendations compared to previous supervised topic models and high- dimensional logistic regression from words alone. Comment: Accepted poster at NIPS 2017 Workshop on Machine Learning for Health ([URL]...|$|E
40|$|Occluded face {{detection}} is {{a challenging}} detection task {{due to the}} large appearance variations incurred by various real-world occlusions. This paper introduces an Adversarial Occlusion-aware Face Detector (AOFD) by simultaneously detecting occluded faces and segmenting occluded areas. Specifically, we employ an adversarial training strategy to generate occlusion-like face features that are difficult for a face detector to recognize. Occlusion mask is predicted simultaneously while detecting occluded faces and the occluded area is utilized as an auxiliary instead of being regarded as a hindrance. Moreover, the <b>supervisory</b> <b>signals</b> from the segmentation branch will reversely affect the features, aiding in detecting heavily-occluded faces accordingly. Consequently, AOFD is {{able to find the}} faces with few exposed facial landmarks with very high confidences and keeps high detection accuracy even for masked faces. Extensive experiments demonstrate that AOFD not only significantly outperforms state-of-the-art methods on the MAFA occluded face detection dataset, but also achieves competitive detection accuracy on benchmark dataset for general face detection such as FDDB...|$|E
40|$|We {{present a}} {{framework}} for learning single-view shape and pose prediction without using direct supervision for either. Our approach allows leveraging multi-view observations from unknown poses as <b>supervisory</b> <b>signal</b> during training. Our proposed training setup enforces geometric consistency between the independently predicted shape and pose from two views of the same instance. We consequently learn to predict shape in an emergent canonical (view-agnostic) frame along with a corresponding pose predictor. We show empirical and qualitative results using the ShapeNet dataset and observe encouragingly competitive performance to previous techniques which rely on stronger forms of supervision. We also demonstrate the applicability of our framework in a realistic setting which {{is beyond the scope}} of existing techniques: using a training dataset comprised of online product images where the underlying shape and pose are unknown. Comment: Project url with code: [URL]...|$|R
40|$|We {{introduce}} {{the problem of}} visual hashtag discovery for infographics: extracting visual elements from an infographic that are diagnostic of its topic. Given an infographic as input, our computational approach automatically outputs textual and visual elements predicted to {{be representative of the}} infographic content. Concretely, from a curated dataset of 29 K large infographic images sampled across 26 categories and 391 tags, we present an automated two step approach. First, we extract the text from an infographic and use it to predict text tags indicative of the infographic content. And second, we use these predicted text tags as a <b>supervisory</b> <b>signal</b> to localize the most diagnostic visual elements from within the infographic i. e. visual hashtags. We report performances on a categorization and multi-label tag prediction problem and compare our proposed visual hashtags to human annotations...|$|R
5000|$|E and M {{signaling}} {{is a type}} of <b>supervisory</b> line <b>signaling</b> {{that uses}} DC signals on separate leads, called the [...] "E" [...] lead and [...] "M" [...] lead, traditionally used in the telecommunications industry between telephone switches. Various mnemonic names have been used to memorize these letters, such as Ear and Mouth, the most common variation.|$|R
40|$|State-of-the-art visual {{recognition}} and detection systems increasingly rely on {{large amounts of}} training data and complex classifiers. Therefore it becomes increasingly expensive both to manually annotate datasets and to keep running times at levels acceptable for practical applications. In this paper, we propose two solutions to address these issues. First, we introduce a weakly supervised, segmentation-based approach to learn accurate detectors and image classifiers from weak <b>supervisory</b> <b>signals</b> that provide only approximate constraints on target localization. We illustrate our system {{on the problem of}} action detection in static images (Pascal VOC Actions 2012), using human visual search patterns as our training signal. Second, inspired from the saccade-and-fixate operating principle of the human visual system, we use reinforcement learning techniques to train efficient search models for detection. Our sequential method is weakly supervised and general (it does not require eye movements), finds optimal search strategies for any given detection confidence function and achieves performance similar to exhaustive sliding window search at a fraction of its computational cost...|$|E
40|$|Multitask Learning is an {{approach}} to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of <b>supervisory</b> <b>signals,</b> and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show {{that there are many}} opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can [...] ...|$|E
40|$|We {{introduce}} a new problem of generating an image based on {{a small number of}} key local patches without any geometric prior. In this work, key local patches are defined as informative regions of the target object or scene. This is a challenging problem since it requires generating realistic images and predicting locations of parts at the same time. We construct adversarial networks to tackle this problem. A generator network generates a fake image as well as a mask based on the encoder-decoder framework. On the other hand, a discriminator network aims to detect fake images. The network is trained with three losses to consider spatial, appearance, and adversarial information. The spatial loss determines whether the locations of predicted parts are correct. Input patches are restored in the output image without much modification due to the appearance loss. The adversarial loss ensures output images are realistic. The proposed network is trained without <b>supervisory</b> <b>signals</b> since no labels of key parts are required. Experimental results on six datasets demonstrate that the proposed algorithm performs favorably on challenging objects and scenes. Comment: 16 page...|$|E
40|$|We {{present an}} {{unsupervised}} representation learning approach using videos without semantic labels. We leverage the temporal coherence as a <b>supervisory</b> <b>signal</b> by formulating representation learning as a sequence sorting task. We take temporally shuffled frames (i. e., in non-chronological order) as inputs and train a {{convolutional neural network}} to sort the shuffled sequences. Similar to comparison-based sorting algorithms, we propose to extract features from all frame pairs and aggregate them to predict the correct order. As sorting shuffled image sequence requires {{an understanding of the}} statistical temporal structure of images, training with such a proxy task allows us to learn rich and generalizable visual representation. We validate the effectiveness of the learned representation using our method as pre-training on high-level recognition problems. The experimental results show that our method compares favorably against state-of-the-art methods on action recognition, image classification and object detection tasks. Comment: ICCV 2017. Project page: [URL]...|$|R
40|$|We {{propose a}} weakly-supervised {{structured}} learning approach for recognition and spatio-temporal localization of actions in video. As {{part of the}} proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efficiently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations {{in the form of}} bounding boxes to guide the latent model during training, we utilize human gaze data {{in the form of a}} weak <b>supervisory</b> <b>signal.</b> This is achieved by incorporating eye gaze, along with the classification, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classification, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classification label and localized latent paths. ...|$|R
40|$|In the {{training}} of transition-based dependency parsers, an oracle is used to predict a transition sequence for a sentence and its gold tree. However, the transition system may exhibit ambiguity, that is, there can be multiple correct transition sequences that form the gold tree. We propose {{to make use of}} the property in {{the training}} of neural dependency parsers, and present the Hybrid Oracle. The new oracle gives all the correct transitions for a parsing state, which are used in the cross entropy loss function to provide better <b>supervisory</b> <b>signal.</b> It is also used to generate different transition sequences for a sentence to better explore the training data and improve the generalization ability of the parser. Evaluations show that the parsers trained using the hybrid oracle outperform the parsers using the traditional oracle in Chinese dependency parsing. We provide analysis from a linguistic view. Comment: The code is available at [URL]...|$|R
