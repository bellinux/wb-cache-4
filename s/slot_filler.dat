6|33|Public
40|$|Abstract: The {{relation}} between schematic construction and its <b>slot</b> <b>filler</b> {{is a hot}} topic discussed by both philosophers and linguists worldwide. This topic involves not only the question of meaning in linguistic philosophy but also the relations of word meaning, sentence meaning and syntax meaning. This thesis is supportive {{of the concept of}} mutual definition between schematic construction and its <b>slot</b> <b>filler</b> proposed by Chinese philosopher Chen Jiaying, and the concept of interaction between two sides which has been widely accepted in linguistic community. As an instance of the interactive relationships between construction and lexicon, the Construction and Lexicon Interactive Coercion Model is proposed in the thesis, which can realize two commitments of cognitive linguistics: generalization commitment and cognitive commitment...|$|E
40|$|Tato práce se zabývá backendem překladače jazyka C, konkrétně plánovačem instrukcí. Analyzuje možnosti plánovače instrukcí kompilační platformy LLVM. Popisuje nahrazení stávajícího delay slot filleru pro architekturu MIPS. This work {{is engaged}} in the backend of a C compiler, in {{particular}} the instruction scheduler. It analyzes possibilities of the instruction scheduler in the LLVM compiler platform. It describes substitution of the current delay <b>slot</b> <b>filler</b> for MIPS architecture. ...|$|E
40|$|This is a {{workshop}} paper that we participated in 2016 for KBP slot filling and <b>slot</b> <b>filler</b> validation task organized by TAC (Text Analysis Conference) This paper describes {{the participation of}} IRT SystemX at TAC KBP 2016, for the two tracks, CSSF and SFV (filtering and ensemble). We have submitted 4 runs for each track of SFV which are our first submission and this submissions are applicable for only cold start monolingual English SF/KB runs (for both filtering and ensemble). The classifier models we use for SFV track are the same for both filtering and ensemble task...|$|E
40|$|International audienceIn {{this paper}} {{we present a}} {{relation}} validation method for KBP slot filling task by exploring some graph features to classify the candidate <b>slot</b> <b>fillers</b> as correct or incorrect. The proposed features with voting feature collectively performs better than the baseline voting feature...|$|R
40|$|Zero length, slotted lip inlet {{performance}} and associated fan blade stresses were determined during model tests using a 20 inch diameter fan simulator in the NASA-LeRC 9 by 15 foot low speed wind tunnel. The model configuration variables consisted of inlet contraction ratio, slot width, circumferential extent of <b>slot</b> <b>fillers,</b> {{and length of}} a constant area section between the inlet throat and fan face. The inlet performance was dependent on slot gap width and relatively independent of inlet throat/fan face spacer length and slot flow blockage created by 90 degree <b>slot</b> <b>fillers.</b> Optimum performance was obtained at a slot gap width of 0. 36 inch. The zero length, slotted lip inlet satisfied all critical low speed inlet operating requirements for fixed horizontal nacelles subsonic V/STOL aircraft...|$|R
40|$|To build a {{simulated}} robot that follows route instructions in unconstrained natural language, we propose a frame-segment decoding algorithm that achieves two joint tasks: (1) Segment the route instruction {{so that the}} sequence of segments corresponds to a sequence of actions required to complete the task, and (2) Choose the <b>slot</b> <b>fillers</b> of the frame-based knowledge representation for each action. Our model was trained with voted perceptron. We also created labels for actions by merging the <b>slot</b> <b>fillers</b> of their frame-based knowledge representation, reducing the problem to a sequence labeling task. As a comparison, we applied a linear-chain Conditional Random Fields to this representation. The experimental result shows that our model performs better with a 77. 7 % success rate. ...|$|R
40|$|The Streaming <b>Slot</b> <b>Filler</b> (SSF) task in TREC Knowledge Base Acceleration track {{involves}} detect-ing {{changes to}} slot values (relations) over time. To handle this task, the system needs to extract relations to identify slot-filler values and detect novel values. Being {{the first attempt}} at KBA, the biggest challenge that we faced was {{the scale of the}} data. We present the approach used by University of Wisconsin for the SSF task and the large scale challenge. We used Ele-mentary, a scalable statistical inference and learning system, developed in University of Wisconsin as our core system. We used Stanford NLP Toolkit to gener-ate parse trees, dependency graphs and named-entity recognition information. These were then converted to features for the logistic regression learner of Ele-mentary. To handle the lack of early SSF training data, we used our existing Knowledge Base Popula-tion system to bootstrap a logistic regression model and added rules to handle the new relations. ...|$|E
40|$|This paper {{describes}} a recent {{revision of the}} machine transla-tion system LMT in which (a) source analysis is based on Slot Grammar, and (b) {{there is a large}} language-independent portion of the system, a kind of 'X-to-Y translation shell, ' making it easier to handle new language pairs. Slot Grammar makes a systematic use of slots (essentially syntactic relations) obtained from lexical entries for head words of phrases. No phrase structure rules (augmented or plain) are used Instead, there are <b>slot</b> <b>filler</b> rules and separately stated ordering rules for slots. A great deal of the Slot Grammar system is in the shell. This includes most of the treatment of coordination, which uses a method of 'factoring out ' unfilled slots from elliptical coordi-nated phrases. The parser (a bottom-up chart parser) employs a parse evaluation scheme used for pruning away unlikely analy-ses during parsing as well as for ranking final analyses The transfer step is designed so that all of the transfer rules except those arising from lexical transfer entries (which are database-like) are in the shell Syntactic generation uses a system of transformations, written in a formalism involving an extension of Prolog unification The revision includes a new treatment of transformation rule ordering. LMT is implemented entirely i...|$|E
40|$|Linguistic usage {{patterns}} {{are not just}} coincidental phenomena on the textual surface but constitute a fundamental constructional principle of language. At the same time, however, linguistic patterns are highly idiosyncratic {{in the sense that}} they tend to be item-specific and unpredictable, thus defying all attempts at capturing them by general abstract rules. […] What all these approaches [that deal with constructions, collocations, patterns, etc. K. S. ] share, in addition to their interest in recurrent patterns, is a strong commitment to the value of usage, be it in the wider sense of usage as an empirical basis for sound linguistic analysis and description or in the narrower sense of usage as constituting the basis for the emergence and consolidation of linguistic knowledge. (Herbst et al. 2014 : 1) In consequence of the feasibility of studying language data in new quantitative dimensions, the phraseology faces a paradigm shift. The traditional focus on strongly lexicalized, often idiomatic multi-word expressions (MWE) has led to an overestimation of their unique status in the mental lexicon. The majority of MWEs are typical lexical realisations of templates (‘MW patterns’) that emerged from repeated usage and can be instantiated with ever changing lexical elements. The – primarily functional – pattern restrictions cannot always be predicted with rules, but are the result of recurring context factors. In this article, at first, it has been shown the nature and the interrelations of MW patterns that are reconstructed with complex corpus-driven methods. Furthermore, a vision of a new phraseography of MW pattern that described their hierarchies and functions based on authentic corpus data like KWIC bundles, <b>slot</b> <b>filler</b> tables and collocation profiles has been discussed...|$|E
50|$|Shorty McShorts' Shorts {{was last}} shown in June 2007, before being {{completely}} cancelled. The show {{ended with a}} rating of 5.1 out of 10.0 on TV.com. Reruns were rarely shown, and were used mainly as time <b>slot</b> <b>fillers,</b> until they stopped airing altogether. The show's web page on disneychannel.com has since been removed, and currently, extinct.|$|R
40|$|We {{describe}} {{a system for}} automatic annotation of English text in the FrameNet standard. In addition to the conventional annotation of frame elements and their semantic roles, we annotate additional semantic information such as support verbs and prepositions, aspectual markers, copular verbs, null arguments, and <b>slot</b> <b>fillers.</b> As far as we are aware, {{this is the first}} system that finds this information automatically...|$|R
40|$|We {{address the}} task of {{automatic}} discovery of information extraction template from a given text collection. Our approach clusters candidate <b>slot</b> <b>fillers</b> to identify meaningful template slots. We propose a generative model that incorporates distributional prior knowledge to help distribute candidates in a document into appropriate slots. Empirical {{results suggest that the}} proposed prior can bring substantial improvements to our task as compared to a K-means baseline and a Gaussian mixture model baseline. Specifically, the proposed prior has shown to be effective when coupled with discriminative features of the candidates. ...|$|R
40|$|The YPA is a {{directory}} enquiry system which allows a user to access advertiser information in classified directories. It converts semi-structured {{data in the}} Yellow Pages machine readable classified directories into a set of indices appropriate to the domain and task, and converts natural language queries into filled <b>slot</b> and <b>filler</b> structures appropriate for queries in the domain. The generation of answers requires a domain dependent query construction step, connecting the indices and the <b>slot</b> and <b>fillers.</b> The YPA illustrates an unusual but useful intermediate point between information retrieval and logical knowledge representation...|$|R
40|$|In ongoing work, U. S. Army Corps of Engineers, Portland District (CENWP) {{is seeking}} to better {{understand}} and improve the conditions within the Bonneville Powerhouse 2 (B 2) turbine intakes to improve survival of downstream migrant salmonid smolt. In this study, the existing B 2 forebay computational fluid dynamics (CFD) model was modified to include a more detailed representation of all B 2 turbine intakes. The modified model was validated to existing field-measured forebay ADCP velocities. The initial CFD model scenarios tested a single project operation {{and the impact of}} adding the Behavior Guidance System (BGS) or Corner Collector. These structures had impacts on forebay flows. Most notable was that the addition of the BGS and Corner Collector reduced the lateral extent of the recirculation areas on the Washington shore and Cascade Island and reduced the flow velocity parallel to the powerhouse in front of Units 11 and 12. For these same cases, at the turbine intakes across the powerhouse, there was very little difference in the flow volume into the gatewell for the clean forebay, and the forebay with the BGS in place and/or the Corner Collector operating. The largest differences were at Units 11 to 13. The CFD model cases testing the impact of the gatewell <b>slot</b> <b>fillers</b> showed no impact to the forebay flows, but large differences within the gatewells. With the <b>slot</b> <b>fillers,</b> the flow above the standard traveling screen and into the gatewell increased (about 100 cfs at each turbine intake) and the gap flow decreased across the powerhouse for all cases. The increased flow up the gatewell was further enhanced with only half the units operating. The flow into the gatewell slot was increased about 35 cfs for each bay of each intake across the powerhouse; this change was uniform across the powerhouse. The flows in the gatewell of Unit 12, the most impacted unit for the scenarios, was evaluated. In front of the vertical barrier screen, the CFD model with <b>slot</b> <b>fillers</b> showed reduced the maximum velocities (in spite of the increased the flow into the gatewell), and decreased the area of recirculation. The area near the VBS exceeding the normal velocity criteria of 1 ft/s was reduced and the flows were more balanced...|$|R
50|$|In {{addition}} to movies, the channel also showed children's television shows in an after school <b>slot</b> as <b>fillers.</b> The channel premiered Thundercats before the BBC1 launch {{and was also}} the first channel to show Jayce and the Wheeled Warriors.|$|R
40|$|Zero-length, slotted-lip inlet {{performance}} and associated fan blade stresses were determined during model tests using a 20 -inch diameter fan simulator in the NASA-LeRC 9 - by 15 -foot low-speed wind tunnel. The model configuration variables consisted of inlet contraction ratio, slot width, circumferential extent of <b>slot</b> <b>fillers,</b> {{and length of}} a constant area section between the inlet throat and fan face. Inlet configurations having contraction ratios of 1. 2 and 1. 3 satisfied all critical low-speed inlet operating requirements for a fixed horizontal nacelle and tilt-nacelle-type subsonic V/STOL aircraft, respectively. Relative to a conventional axisymmetric tilt-nacelle inlet, the zero-length, slotted-lip inlet has a 27 -percent smaller inlet lip contraction ratio, an 83 -percent shorter total length, and a 5 -percent smaller maximum cowl diameter...|$|R
40|$|This {{paper is}} a {{quantitative}} multifactorial {{study of the}} near-synonymous constructions let+V, allow+to V and permit+to V based on the British National Corpus. The study investigates the differences between these constructions {{with the help of}} 23 formal, semantic, social and collostructional variables. A Bayesian multinomial mixed-effects model reveals a remarkable alignment of the variables that represent different dimensions of variation, namely, the linguistic distance between the predicates, the conceptual distance between the events they represent, the distance between the speaker and the Permitter and Permittee on the animacy/entrenchment/empathy hierarchy, the social and communicative distance between the interlocutors, as well as the strength of collostructional attraction between the constructions and second verb <b>slot</b> <b>fillers.</b> The paper offers several possible explanations for this alignment from a cognitive, functional and historical perspective...|$|R
40|$|The {{present study}} applies {{conversation}} analysis {{in order to}} describe the form and function of co-constructed turns in conversations involving one or more persons who stutter. Coconstructed turns are turns at talk that are produced {{by more than one}} speaker {{in such a way that}} the turn is initiated by one speaker and completed by another. Examples in the data are classified according to an existing taxonomy distinguishing between <b>slot</b> <b>fillers,</b> extensions and completions. All these types are found in our data, albeit to different extents. External as well as internal reasons for this distributional variation are discussed. The main aim of the study is to illustrate the need for qualitative, context-sensitive approaches to research on verbal interaction in the field of fluency disorders and to focus on some clinical implications of such research findings...|$|R
50|$|As a {{frame-based}} system, every SNePS functional term (including proposition-valued terms) {{is represented}} by a frame with <b>slots</b> and <b>fillers.</b> Each <b>slot</b> may be filled by an arbitrarily-sized set of other terms. However, cycles cannot be constructed. SNePSUL, the SNePS User Language is an input-output language for interacting with SNePS in its guise as a frame-based system.|$|R
40|$|Abstract. Question Answering, {{the process}} of extracting answers to natural {{language}} questions is profoundly different from Information Re-trieval (IR) or Information Extraction (IE). IR systems allow us to locate relevant documents that relate to a query, but do not specify exactly where the answers are. In IR, the documents of interest are fetched by matching query keywords to the index of the document collection. By contrast, IE systems extrat the information of interest provided the do-main of extraction is well defined. In IE systems, the information of interest {{is in the form}} of <b>slot</b> <b>fillers</b> of some predefined templates. The QA technology takes both IR and IE a step further, and provides specific and brief answers to open domain questions formulated natu-rally. This paper presents the major modules used to build IR, IE and QA systems and Shows similarities, differences and possible trade-offs between the three technologies. 1 Information Retrieval The three main technologies used to extract information from large collection...|$|R
40|$|Standard {{algorithms}} for template-based {{information extraction}} (IE) require predefined template schemas, and often labeled data, {{to learn to}} extract their <b>slot</b> <b>fillers</b> (e. g., an embassy is the Target of a Bombing template). This paper describes an approach to template-based IE that removes this requirement and performs extraction without knowing the template structure in advance. Our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events (e. g., bombings include detonate, set off, and destroy events) associated with semantic roles. We also solve the standard IE task, using the induced syntactic patterns to extract role fillers from specific documents. We evaluate on the MUC- 4 terrorism dataset and show that we induce template structure very similar to handcreated gold structure, and we extract role fillers with an F 1 score of. 40, approaching the performance of algorithms that require full knowledge of the templates. ...|$|R
5000|$|The SPECIALIST lexicon is {{available}} in two formats. The [...] "unit record" [...] format can be seen above, and comprises <b>slots</b> and <b>fillers.</b> A <b>slot</b> is the element (i.e. [...] "base=" [...] or [...] "spelling variant=") and the fillers are the values attributable to that slot for that entry. The [...] "relational table" [...] format is not yet normalized and contain {{a great deal of}} redundant data in the files.|$|R
40|$|This paper {{discusses}} the semagram, an {{innovation in the}} way of describing meaning in lexicography, as used in the Algemeen Nederlands Woordenboek (General Dutch Dictionary). A semagram is the representation of knowledge associated with a word in a frame of <b>slots</b> and <b>fillers.</b> <b>Slots</b> are conceptual structure elements which characterise the properties and relations of the semantic class of a word-e. g. colour, smell, taste, composition, components, preparation for the class of beverages. The abstract meaning frame for such a semantic class is called type template. After a motivation for the use of frames in lexicography we reveal how semantic classes are determined and how type templates are composed. We illustrate this with the type template of the animal names and show how the semagram of cow is based upon it. We conclude by summing up the main advantages of the use of semagrams...|$|R
40|$|We {{develop a}} {{linearization}} model that captures {{a broad range}} of constituent order phenomena in clauses of Dutch, English and German (clause union, cross-serial dependencies, scrambling, verb clusters, wh-fronting, extraction, extraposition, etc.). The model is part of the psycholinguistically motivated formalism of Performance Grammar, which has separate components for assembling the hierarchical and the linear structure of sentences. We demonstrate that a few narrowly localized, relatively minor variations of the model suffice to account for {{a great deal of the}} [...] - sometimes widely diverging [...] - word order patterns in the three target languages, and of the constraints that apply to them. This is the first reason for calling our approach `uniform'. The other reason is that the system allows the various linear order phenomena to be viewed as manifestations of the same basic mechanism, which we have dubbed topology sharing. Here, a topology is a one-dimensional array containing a limited number of left-to-right positions (`slots') for clausal constituents (cf. the `topological fields' of traditional German grammar). Topologies at adjacent levels of the syntactic hierarchy sometimes share left- and/or rightperipheral slots, which gives rise to upward `movement' of the <b>slot</b> <b>fillers.</b> We also explore the generative capacity of the linearization system and suggest that it approaches the theoretical minimum of mild context-sensitivity...|$|R
40|$|The data {{extraction}} systems {{studied in the}} MUC- 3 evaluation perform a variety of subtasks in fillin g out templates. Some of these tasks are quite complex, and seem to require a system to represen t {{the structure of a}} text in some detail to perform the task successfully. Capturing reference relation s between <b>slot</b> <b>fillers,</b> distinguishing between historic and recent events, and many other subtask s appear to have this character. Other of the MUC- 3 subtasks, however, appear amenable to simpler techniques. In particular, whenever a slot filling task involves making choices among a relatively small set of prespecifie d alternatives, the potential exists for modeling this task as text categorization [10]. Text categorization systems treat a document as a single unit, and make a decision to assign each document to zero, one or more of a fixed set of categories. Text categorization techniques {{have played a role in}} previous text extraction systems, both for screening out documents which it is not desirable to process, and for directing documents t o category-specific extraction routines [2, 9, 15]. The role of categorization in these systems was relatively limited, however. Analyses of the behavior of these systems have given relatively littl e attention to how categorization operated or to what factors influenced categorization performance...|$|R
50|$|Along with Cebuano News and the 1:30 pm <b>filler</b> <b>slot,</b> Kapampangan News was {{replaced}} on April 3, 2017 by the simulcast of the 1 AM EDT hour of CNN Newsroom with Cyril Vanier and Natalie Allen (Monday) and John Vause and Isha Sesay (Tuesday to Friday) {{as a part}} of Armie Jarin-Bennett's continuing revamps on the network.|$|R
40|$|Text {{analysis}} {{has become an}} important research activity in the Department of Veterans Affairs (VA). Statistical text mining and natural language processing {{have been shown to}} be very effective for extracting useful information from medical documents. However, neither of these techniques is effective at extracting the information stored in semi-structure text elements. A prototype system (TagLine) was developed as a method for extracting information from the semi-structured portions of text using machine learning. Features for the learning machine were suggested by prior work, as well as by examining the text, and selecting those attributes that help distinguish the various classes of text lines. The classes were derived empirically from the text and guided by an ontology developed by the Consortium for Health Informatics Research (CHIR), a nationwide research initiative focused on medical informatics. Decision trees and Levenshtein approximate string matching techniques were tested and compared on 5, 055 unseen lines of text. The performance of the decision tree method was found to be superior to the fuzzy string match method on this task. Decision trees achieved an overall accuracy of 98. 5 percent, while the string match method only achieved an accuracy of 87 percent. Overall, the results for line classification were very encouraging. The labels applied to the lines were used to evaluate TagLines 2 ̆ 7 performance for identifying the semi-structures text elements, including tables, <b>slots</b> and <b>fillers.</b> Results for <b>slots</b> and <b>fillers</b> were impressive while the results for tables were also acceptable...|$|R
40|$|We {{present an}} {{unsupervised}} and unrestricted approach to discovering an infobox like ontology by exploiting the inter-article links within Wikipedia. It discovers new <b>slots</b> and <b>fillers</b> {{that may not}} be available in the Wikipedia infoboxes. Our results demonstrate that there are certain types of properties that are evident in the link structure of resources like Wikipedia that can be predicted with high accuracy using little or no linguistic analysis. The discovered properties can be further used to discover a class hierarchy. Our experiments have focused on analyzing people in Wikipedia, but the techniques can be directly applied to other types of entities in text resources that are rich with hyperlinks. ...|$|R
40|$|The Ypa project (De Roeck et al., 1998) is {{building}} a system to make the information in classified directories more accessible. BT's Yellow Pages 1 {{provides an example of}} a classified database with which this work would be useful. Accessibility in this context means allowing users (or call center operators) to query the Yellow Pages system using Natural Language queries. For this to be possible there must be some method in the Ypa for converting theses queries into some form which allows the database to be queried. The chosen form for this project is a <b>slot</b> and <b>filler</b> construction, and this paper describes the process of slot-filling employed by the Natural Language Frontend of the Ypa...|$|R
40|$|A parser is an {{algorithm}} that assigns {{a structural}} description to a string {{according to a}} grammar. It follows from this definition {{that there are three}} general issues in parser design: the structure to be assigned, the type of grammar, the recognition algorithm. Common parsers employ phrase structure descriptions, rule-based grammars, and derivation or transition oriented recognition. The following choices result in a new parser: The structure to be assigned to the input is a dependency tree with lexical, morpho-syntactic and functional-syntactic information associated with each node and coded by complex categories which are subject to. unification. The grammar is lexicalized, i. e. the syntactical relationships are stated as part of the lexical descriptions of the elements of the language. The algorithm relies on the <b>slot</b> and <b>filler</b> principle in order to draw up complex structures. It utilizes a well-formed substring table (chart) which allows for discontinuous segments...|$|R
40|$|RETRIEVAL OF PASSAGES FOR INFORMATION REDUCTION SEPTEMBER 1997 JODY J. DANIELS B. S., CARNEGIE MELLON UNIVERSITY M. S., UNIVERSITY OF MASSACHUSETTS AMHERST Ph. D., UNIVERSITY OF MASSACHUSETTS AMHERST Directed by: Professor Edwina L. Rissland Information Retrieval (IR) {{typically}} retrieves entire {{documents in}} response to a user's information need. However, many times a user would prefer to examine smaller portions of a document. One example of this is when building a frame-based representation of a text. The user would like to read all and only those portions of the text that are about predefined important features. This research addresses the problem of automatically locating text about these features, where the important features are those defined for use by a case-based reasoning (CBR) system in the form of features and values or <b>slots</b> and <b>fillers.</b> To locate important text pieces we gathered a small set of "excerpts", textual segments, when creating the original case-base representations [...] ...|$|R
50|$|Through {{the first}} half of the 2000s, the OETA state network was one of the few {{remaining}} broadcast television outlets in the United States that had not converted to a 24-hour-a-day broadcast schedule. Until its full-power and translator stations formally switched to a 24-hour schedule in April 2006, by adding content from the PBS Satellite Service feed during the overnight hours, OETA continued to go silent on Sunday through Thursdays from 12:00 to 6:00 a.m., and Fridays and Saturdays from 1:00 to 6:00 a.m. Prior to the conversion, many cable providers around the state (such as Cox Communications) carried other lower-priority cable networks that could not be carried full-time on a separate channel due to limited headend frequency space over OETA's channel <b>slots</b> as <b>filler</b> during overnight/early morning time periods while the broadcast signals were off-the-air until the late 1990s, when OETA began offering an alternate feed of PBS's default satellite schedule to these systems to air during the off-hours.|$|R
50|$|Cebuano News {{was quietly}} axed by the network on March 31, 2017. The {{cancellation}} has been lengthily planned by the network {{partly due to}} the continuing revamps under Armie Jarin-Bennett and partly due to mounting viewer complaints of both Cebuano and Kapampangan News not limited to be carried by CNN PH's regional stations. A simulcast of CNN Newsroom's 1 AM EDT/12 MN EST slot with Cyril Vanier and Natalie Allen (Monday) and John Vause and Isha Sesay (Tuesday to Friday) replaced both regional newscasts and the 1:30 pm current affairs <b>filler</b> <b>slot</b> on April 3, 2017.|$|R
40|$|For {{agricultural}} information access, {{some useful}} {{information such as}} how to analyze symptom of plant diseases and how to protect plant from diseases is in unstructured format and scattered the entire document. Moreover, information is rapidly increasing causing information to become overwhelming in size, so extraction of only significant and interesting information is necessary. Information Extraction in traditional way extracts a set of related entities in the format of <b>slot</b> and <b>filler,</b> but the representation of some information such as symptom and treatment can not be limited to set of related entities. In this paper, we present Information Extraction system for Thai documents in agricultural domain that has information about the analysis and treatment of plant disease. This type of information has to be explained in a set of continuous sentences, so the extraction of the set of related entities is not enough. In this case, we introduce “Explanation Information”, the combination of sentences that describe the topic of interest, to fill in the extraction slot. However, extracting the relevant sentence...|$|R
40|$|Information Retrieval (IR) {{typically}} retrieves entire {{documents in}} response to a user 2 ̆ 7 s information need. However, many times a user would prefer to examine smaller portions of a document. One example of this is when building a frame-based representation of a text. The user would like to read all and only those portions of the text that are about predefined important features. ^ This research addresses the problem of automatically locating text about these features, where the important features are those defined for use by a case-based reasoning (CBR) system in the form of features and values or <b>slots</b> and <b>fillers.</b> ^ To locate important text pieces we gathered a small set of 2 ̆ 2 excerpts 2 ̆ 2, textual segments, when creating the original case-base representations. Each segment contains the local context for a particular feature within a document. We used these excerpts to generate queries that retrieve relevant passages. By locating passages for display to the user, we winnow a text down to sets of several sentences, greatly reducing the time and effort expended searching through each text for important features. ...|$|R
40|$|In this {{analysis}} of cases and clauses in Vietnamese, an attempt {{is made to}} sake use of tagmemics and a case grammar model called lexicase. Such an eclectic {{combination of the two}} theories is not new either in the field of general linguistics or in Vietnamese. This paper recognizes the hierarchical levels in syntax and the grammatical unit or tagmeme as composed of a <b>slot</b> and a <b>filler</b> class; it recognizes the case relations between various nouns and the predicative verb in the clause; and it also recognizes the centrality of the verb. Lexicase introduces both overt case realizations and covert case relations into the grammar as syntactic features assigned to nouns by verbs. The data presented here include: 12 covert case relations in Vietnamese; 11 overt case realizations; a two-dimensional matrix yielding 25 possible combinations of covert case relations and overt case realizations (25 tagmesic slots); 1...|$|R
