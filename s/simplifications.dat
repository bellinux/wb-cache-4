6852|10000|Public
5|$|Hamlet {{is one of}} {{the most}} quoted works in the English language, and is often {{included}} on lists of the world's greatest literature. As such, it reverberates through the writing of later centuries. Academic Laurie Osborne identifies the direct influence of Hamlet in numerous modern narratives, and divides them into four main categories: fictional accounts of the play's composition, <b>simplifications</b> of the story for young readers, stories expanding the role of one or more characters, and narratives featuring performances of the play.|$|E
5|$|The Panzer IV saw {{service in}} all combat theaters {{involving}} Germany {{and was the}} only German tank to remain in continuous production throughout the war. Upgrades and design modifications, intended to counter new threats, extended its service life. Generally, these involved increasing the Panzer IV's armor protection or upgrading its weapons, although during {{the last months of}} the war, with Germany's pressing need for rapid replacement of losses, design changes also included <b>simplifications</b> to speed up the manufacturing process.|$|E
5|$|The motions as {{described}} above are <b>simplifications.</b> Due to the movement of Earth around the Earth–Moon center of mass, the apparent path of the Sun wobbles slightly, with {{a period of about}} one month. Due to further perturbations by the other planets of the Solar System, the Earth–Moon barycenter wobbles slightly around a mean position in a complex fashion. The ecliptic is actually the apparent path of the Sun throughout the course of a year.|$|E
40|$|We give a new <b>simplification</b> method, called E-cycle <b>Simplification,</b> for Basic Completion {{inference}} systems. We {{prove the}} completeness of Basic Completion with E-cycle <b>Simplification.</b> We prove that E-cycle <b>Simplification</b> is strictly {{stronger than the}} only previously known complete <b>simplification</b> method for Basic Completion, Basic <b>Simplification,</b> {{in the sense that}} every derivation involving Basic <b>Simplification</b> is a derivation involving E-cycle <b>Simplification,</b> but not vice versa. E-cycle <b>Simplification</b> is simple to perform, and does not use the reducibility-relative-to condition. ECC implements our method...|$|R
40|$|Colloque avec actes et comité de lecture. We give a new <b>simplification</b> method, called E-cycle <b>Simplification,</b> for Basic Completion {{inference}} systems. We {{prove the}} completeness of Basic Completion with E-cycle <b>Simplification.</b> We prove that E-cycle <b>Simplification</b> is strictly {{stronger than the}} only previously known complete <b>simplification</b> method for Basic Completion, Basic <b>Simplification,</b> {{in the sense that}} every derivation involving Basic <b>Simplification</b> is a derivation involving E-cycle <b>Simplification,</b> but not vice versa. E-cycle <b>Simplification</b> is simple to perform, andq does not use the reducibility-relative-to condition. ECC implements our method...|$|R
5000|$|... <b>simplification</b> to {{a smaller}} {{expression}} or some standard form, including automatic <b>simplification</b> with assumptions and <b>simplification</b> with constraints ...|$|R
5|$|The {{architecture}} of the layer pyramid allows it to be securely dated to the time span between the reigns of king Sekhemkhet and that of king Snofru, {{the founder of the}} 4th Dynasty. Rainer Stadelmann, Miroslav Verner and Jean-Philippe Lauer compare the {{architecture of}} the layer pyramid with that of the step pyramids of Djoser and Sekhemkhet, expecting the layer pyramid to have originally consisted of five steps, just as its near-contemporary predecessors. The layer pyramid exhibits at one site both complex developments concerning its substructures and <b>simplifications</b> concerning the building methods employed for the superstructure. According to these egyptologists, the layer pyramid is a clearly advanced version of the buried pyramid of Sekhemkhet.|$|E
5|$|The fossil has {{a number}} of {{features}} that suggest a relation to a group of oryzomyine rodents that includes the South American marsh rat Holochilus, its living relatives Lundomys and Pseudoryzomys, and the extinct Noronhomys and Holochilus primigenus. They share high-crowned (hypsodont) molars and several <b>simplifications</b> of molar morphology, as well as other features that cannot be assessed in Carletonomys, which indicate specializations towards a semiaquatic lifestyle. It shows the most similarity to Noronhomys and Holochilus, so much so that Pardiñas considered placing it in either of these two genera, but its distinctive morphological features justify placement in a separate genus.|$|E
25|$|A {{second round}} of <b>simplifications</b> was promulgated in 1977, but was later {{retracted}} in 1986 {{for a variety of}} reasons, largely due to the confusion caused and the unpopularity of the second round <b>simplifications.</b> However, the Chinese government never officially dropped its goal of further simplification in the future.|$|E
40|$|We {{present a}} method for lexical <b>simplification.</b> <b>Simplification</b> rules are learned from a {{comparable}} corpus, and the rules are applied in a context-aware fashion to input sentences. Our method is unsupervised. Furthermore, {{it does not require}} any alignment or correspondence among the complex and simple corpora. We evaluate the <b>simplification</b> according to three criteria: preservation of grammaticality, preservation of meaning, and degree of <b>simplification.</b> Results show that our method outperforms an established <b>simplification</b> baseline for both meaning preservation and <b>simplification,</b> while maintaining a high level of grammaticality. ...|$|R
40|$|The growing {{availability}} of massive models {{and the inability}} of most existing visualization tools {{to work with them}} requires efficient new methods for massive mesh <b>simplification.</b> In this paper, we present a completely adaptive, virtual memory based <b>simplification</b> algorithm for large polygonal datasets. The algorithm is an enhancement of RSimp [2], enabling out of core <b>simplification</b> without reducing the output quality of the original algorithm. The primary improvement in our new algorithm, VMRSimp, builds and preserves memory locality throughout the <b>simplification</b> process. This is crucial for successful <b>simplification</b> of massive meshes. An additional enhancement based on a depthfirst <b>simplification</b> approach improves running time further by increasing reuse of the resident working set, at the cost of a minor reduction in output quality. VMRSimp performs completely adaptive <b>simplification</b> of massive meshes at reasonable rates, reducing for example 18 million to 100 thousand vertices in 51 minutes. This permits <b>simplification</b> to output sizes in the millions without thrashing, improves accuracy for all smaller output sizes, and enables sensitivity in <b>simplification</b> to mesh...|$|R
40|$|Abstract—Text <b>simplification</b> modifies syntax and lexicon {{to improve}} the {{understandability}} of language for an end user. This survey identifies and classifies <b>simplification</b> research within the period 1998 - 2013. <b>Simplification</b> {{can be used for}} many applications, including: Second language learners, preprocessing in pipelines and assistive technology. There are many approaches to the <b>simplification</b> task, including: lexical, syntactic, statistical machine translation and hybrid techniques. This survey also explores the current challenges which this field faces. Text <b>simplification</b> is a non-trivial task which is rapidly growing into its own field. This survey gives an overview of contemporary research whilst taking into account the history that has brought text <b>simplification</b> to its current state...|$|R
25|$|The Second Scheme was {{received}} extremely poorly, and {{as early as}} mid-1978, the Ministry of Education and the Central Propaganda Department were asking publishers of textbooks, newspapers and other works to stop using the second-round <b>simplifications.</b> Second-round <b>simplifications</b> were taught inconsistently in the education system, and people used characters at various stages of official or unofficial simplification. Confusion and disagreement ensued.|$|E
25|$|The PRC {{issued its}} {{first round of}} {{official}} character <b>simplifications</b> in two documents, the first in 1956 and the second in 1964.|$|E
25|$|After World War II, Japan also {{simplified}} {{a number}} of Chinese characters (kanji) used in the Japanese language. The new forms are called shinjitai. Compared to Chinese, the Japanese reform was more limited, simplifying {{only a few hundred}} characters, most of which were already in use in cursive script. Further, the list of <b>simplifications</b> was exhaustive, unlike Chinese simplification– thus analogous <b>simplifications</b> of not explicitly simplified characters (extended shinjitai) are not approved, and instead standard practice is to use the traditional forms.|$|E
40|$|After a brief {{review of}} the current state of <b>simplification</b> this paper proposes a {{classification}} of <b>simplification</b> rules that may help in the practical implementation of <b>simplification</b> procedures. The last part of the paper is concerned in the mathematical theory of <b>simplification,</b> and the set of expressions formed from rational powers of polynomials is shown to have a canonical form...|$|R
40|$|In this paper, {{algorithms}} for the <b>simplification</b> {{and reconstruction}} of large triangle meshes are described. The <b>simplification</b> process creates an edge-collapse hierarchy in external memory, {{which is used}} for online reconstruction. The hierarchy indices are renamed after <b>simplification,</b> {{in order to allow}} fast reconstructions and the hierarchy is extended with information for view-dependent rendering. The <b>simplification</b> makes no restrictions with the production of the hierarchy, but produces the same hierarchy as In-Core algorithms. The amount of memory, which is used for the <b>simplification</b> is adjustable...|$|R
40|$|Reusing {{animation}} film {{assets for}} real-time rendering requires extreme <b>simplification.</b> As well-known <b>simplification</b> approaches do not suffice, studios are still forced to manually simplify their assets. To automate this, we employ a pipeline for efficient geometry-based <b>simplification</b> and {{make use of}} normal mapping to ensure visual similarity. Our obtained results are promising: geometric complexity is vastly reduced while maintaining a recognizable model, unlike results with classical <b>simplification</b> approaches as employed by commercial applications. We have compared the approaches in two settings, aiming at a similar number of triangles and aiming at a similar storage size, both of which prove that our extreme asset <b>simplification</b> is a valid alternative for classical topological <b>simplification</b> approaches...|$|R
25|$|This example {{application}} of ABC used <b>simplifications</b> for illustrative purposes. A number of review articles provide pointers to more realistic applications of ABC.|$|E
25|$|Despite these <b>simplifications,</b> Japan {{lacked the}} {{industrial}} infrastructure to produce suitable quantities of the Type 100. By 1945, only 24,000 to 27,000 had been built.|$|E
25|$|The {{very common}} verbs {nbr} 'live, be' and {gbr} 'do' undergo <b>simplifications</b> in the gerundive, where the b is deleted: ነይሩ näyru, ገይሩ gäyru (3p.m.sg.); ኔርካ nerka, ጌርካ gerka (2p.m.sg.); etc.|$|E
40|$|Line <b>simplification</b> is an {{important}} method {{in the context of}} cartographic generalization, which is helpful for improving the visualization of digital vector maps. The evaluation method for the <b>simplification</b> algorithms is still an open issue when facing applications of vector data, including progressive transmission, web mapping, and so on. This paper proposes a novel evaluation approach for line <b>simplification</b> algorithms based on several factors towards vector map visualization, including the features of displays, map scales, and the ability of the human eye to distinguish pixels. In order to ensure the evaluation of the line <b>simplification</b> algorithms is conducted under the consistent strength of <b>simplification,</b> a measurement approach for the difference between an original line and its simplified one is proposed in this study, and the method of solving the appropriate <b>simplification</b> threshold is presented. With this method, four <b>simplification</b> algorithms are evaluated at five map scales using three evaluation indicators: standard deviation, compression ratio, and <b>simplification</b> time. The experiment and results show the evaluation approach in this study is feasible, and represents a good means in which to facilitate the application of line <b>simplification</b> towards progressive transmission and visualization of vector maps...|$|R
40|$|Polygonal {{models are}} being used {{more and more in}} all areas of {{computer}} graphics. As their use proliferates so does their complexity and size; this increase has two ramifications. One, present day <b>simplification</b> algorithms are not able to simplifythe large models quickly. Two, conventional <b>simplification</b> algorithms require large amounts of memory to perform model <b>simplification.</b> Thus extremely large models are difficult to simplify because the <b>simplification</b> process does not fit into core memory. The performance of a <b>simplification</b> algorithm degrades severely when it has to use virtual memory. Given the availability of multi-processor systems, creating a parallel <b>simplification</b> algorithm is a solution to the first problem. A <b>simplification</b> algorithm that is able to operate efficiently in a memory restricted environment would solve the second. To realize both of these solutions several nontrivial issues relating to data partitioning and access need to be resolved. This paper examines th [...] ...|$|R
40|$|Abstract. Syntactic <b>simplification</b> is {{the process}} of {{reducing}} the grammatical complexity of a text, while retaining its information content and meaning. The aim of syntactic <b>simplification</b> is to make text easier to comprehend for human readers, or process by programs. In this paper, we formalise the interactions that take place between syntax and discourse during the <b>simplification</b> process. This is important because the usefulness of syntactic <b>simplification</b> in making a text accessible to a wider audience can be undermined if the rewritten text lacks cohesion. We describe how various generation issues like sentence ordering, cue-word selection, referring-expression generation, determiner choice and pronominal use can be resolved so as to preserve conjunctive and anaphoric cohesive relations during syntactic <b>simplification</b> and present the results of an evaluation of our syntactic <b>simplification</b> system. Key words: anaphoric structure, cue-word selection, determiner choice, discourse structure, sentence ordering, syntactic <b>simplification,</b> text cohesio...|$|R
25|$|Both <b>simplifications</b> {{carried over}} to {{computer}} keyboards and the ASCII character set. However, although these {{are widely used}} due to their ubiquity and convenience, they are deprecated in contexts where proper typography is important.|$|E
25|$|Computable {{functions}} are a fundamental concept within computer science and mathematics. The λ-calculus provides a simple semantics for computation, enabling properties of computation {{to be studied}} formally. The λ-calculus incorporates two <b>simplifications</b> that make this semantics simple.|$|E
25|$|These <b>simplifications</b> make trusses {{easier to}} analyze. Structural {{analysis}} of trusses {{of any type}} can readily be carried out using a matrix method such as the direct stiffness method, the flexibility method, or the finite element method.|$|E
40|$|In this paper, {{we assume}} that cartographic {{boundaries}} have features {{at a variety of}} different degrees of <b>simplification</b> and therefore each line segment showing a different feature must be simplified at its proper degree. Our boundary <b>simplification</b> method preserves the characteristics of the shape features, and therefore avoids missing fine features and overlooking coarse features. Here <b>simplification</b> consists of the dominant points detected on the line which has been previously segmented into a number of parts, each one showing a different feature at an appropiate degree of smoothing. We propose an automatic method to segment the line into a number of non-overlapping parts, each one revealing a different feature. To find the best degree of <b>simplification</b> for each segment, we choose the <b>simplification</b> minimizing a normalized measure of the zeros of curvature of the segment. Key Words: Line <b>simplification,</b> segmentation, feature, degree of <b>simplification,</b> dominant points, Gaussian smoot [...] ...|$|R
40|$|The line <b>simplification</b> {{problem is}} an old and well studied problem in cartography. Although there are several {{algorithms}} to compute a <b>simplification</b> {{there seems to be}} no algorithms that perform line <b>simplification</b> in the context of other geographical objects. This paper presents a nearly quadratic time algorithm for the following line <b>simplification</b> problem: Given a polygonal line, a set of extra points, and a real ϵ > 0, compute a <b>simplification</b> that guarantees (i) a maximum error ϵ; (ii) that the extra points remain on the same side of the simplified chain as on the original chain; and (iii) that the simplified chain has no self-intersections. The algorithm is applied as the main subroutine for subdivision <b>simplification</b> and guarantees that the resulting subdivision is topologically correct...|$|R
40|$|In this work, we analyse whether Wikipedia {{can be used}} to {{leverage}} <b>simplification</b> pairs instead of Simple Wikipedia, which has proved unreliable for assessing automatic <b>simplification</b> systems, and is available only in English. We focus on sentence pairs in which the target sentence is the outcome of a Wikipedia edit marked as `simplified', and manually annotate <b>simplification</b> phenomena following an existing scheme proposed for previous <b>simplification</b> corpora in Italian. The outcome of this work is the SIMPITIKI corpus, which we make freely available, with pairs of sentences extracted from Wikipedia edits and annotated with <b>simplification</b> types...|$|R
25|$|Simplified {{character}} {{forms were}} created by {{reducing the number of}} strokes and simplifying the forms of a sizable proportion of Chinese characters. Some <b>simplifications</b> were based on popular cursive forms embodying graphic or phonetic <b>simplifications</b> of the traditional forms. Some characters were simplified by applying regular rules, for example, by replacing all occurrences of a certain component with a simplified version of the component. Variant characters with the same pronunciation and identical meaning were reduced to a single standardized character, usually the simplest amongst all variants in form. Finally, many characters were left untouched by simplification, and are thus identical between the traditional and simplified Chinese orthographies.|$|E
25|$|A {{substantial}} number of Japanese destroyers were lost in 1942 in actions around the Solomon Islands. The urgent need for replacements necessitated design <b>simplifications</b> to improve construction speed and war experience prompted improvements to damage control and anti aircraft weaponry. The resultant s were commissioned in 1944.|$|E
25|$|Malaysia started {{teaching}} {{a set of}} simplified characters at schools in 1981, which were also completely identical to the Mainland China <b>simplifications.</b> Chinese newspapers in Malaysia are published in either set of characters, typically with the headlines in traditional Chinese while the body is in simplified Chinese.|$|E
40|$|This unit {{focuses on}} line {{generalization}} {{as it relates}} to GIS and cartography. Techniques including <b>simplification,</b> smoothing, feature displacement, line enhancement and merging are described, and several linear <b>simplification</b> algorithms are summarized. Approaches for mathematically evaluating <b>simplification</b> are outlined, and justifications for simplifying linear data are discussed...|$|R
40|$|We {{present the}} first attempt at using {{sequence}} to sequence neural networks to model text <b>simplification</b> (TS). Unlike the previously proposed automated TS systems, our neural text <b>simplification</b> (NTS) systems are able to simultaneously perform lexical <b>simplification</b> and content reduction. An extensive human evaluation of the output has shown that NTS systems achieve almost perfect grammaticality and meaning preservation of output sentences and higher level of <b>simplification</b> than the state-of-the-art automated TS systems...|$|R
40|$|In {{this paper}} {{we present a}} generic {{approach}} to lexical <b>simpliﬁcation,</b> that is easily applicable to many languages. Lexical <b>simpliﬁcation</b> helps children, illiterate, foreign, and disabled people to read texts, by replacing difficult words with words that are easier to understand. Although syntactic <b>simpliﬁcation</b> has {{received a lot of}} attention, the work on lexical <b>simpliﬁcation</b> has been limited. The methods regard the integration of dictionary information with a latent words language model. status: publishe...|$|R
