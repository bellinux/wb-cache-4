244|1970|Public
25|$|While {{the margin}} of error {{typically}} reported in the media is a poll-wide figure that reflects the maximum <b>sampling</b> <b>variation</b> of any percentage based on all respondents from that poll, the term margin of error also refers to the radius of the confidence interval for a particular statistic.|$|E
25|$|Two main {{statistical}} methods {{are used in}} data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, <b>sampling</b> <b>variation).</b> Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes {{the extent to which}} members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.|$|E
50|$|A common idea {{underlying}} both {{of these}} meanings is {{the reduction in the}} effects of <b>sampling</b> <b>variation.</b>|$|E
5000|$|... "Kiro TV"'s opening synth riff is <b>sample</b> <b>variation</b> {{from the}} Ultravox! song [...] "ROckWrok".|$|R
40|$|An {{alternative}} to the Satterswaite-Welch degrees of freedom (df) is developed in this paper based on unbiased estimating equations. The method enables us to evaluate the impact of <b>sample</b> <b>variation</b> due to using estimated df instead of the true df by computer intensive method. It is shown by simulation that the alternative df {{is close to the}} Welch df and the method developed could be also used for evaluating the impact of <b>sample</b> <b>variation</b> due to using estimated df in the Welch test...|$|R
40|$|In low {{temperature}} limit, we study electron counting statistics of a disordered conductor. We derive an expression for {{the distribution of}} charge transmitted over a finite time interval by using a result from the random matrix theory of quasi one dimensional disordered conductors. In the metallic regime, {{we find that the}} peak of the distribution is Gaussian and shows negligible <b>sample</b> to <b>sample</b> <b>variations.</b> We also find that the tails of the distribution are neither Gaussian nor Poisson and exhibit strong <b>sample</b> to <b>sample</b> <b>variations.</b> Comment: 11 pages, REVTEX 3. 0, MIT-CMT-HL 940...|$|R
5000|$|An {{extension}} to this law for small samples {{has been proposed}} by Hanski. For small samples the Poisson variation (P) - the variation that can be ascribed to <b>sampling</b> <b>variation</b> - may be significant. Let S be the total variance and let V be the biological (real) variance. Then ...|$|E
50|$|While {{the margin}} of error {{typically}} reported in the media is a poll-wide figure that reflects the maximum <b>sampling</b> <b>variation</b> of any percentage based on all respondents from that poll, the term margin of error also refers to the radius of the confidence interval for a particular statistic.|$|E
50|$|Statistical {{inference}} is {{the process}} of drawing conclusions from data that are subject to random variation, for example, observational errors or <b>sampling</b> <b>variation.</b> Initial requirements of such a system of procedures for inference and induction are that the system should produce reasonable answers when applied to well-defined situations and that it should be general enough to be applied across a range of situations. Inferential statistics are used to test hypotheses and make estimations using sample data. Whereas descriptive statistics describe a sample, inferential statistics infer predictions about a larger population that the sample represents.|$|E
40|$|Analysis of multivariate {{data sets}} from e. g. {{microarray}} studies frequently results in lists of genes which {{are associated with}} some response of interest. The biological interpretation is often complicated by the statistical instability of the obtained gene lists with respect to <b>sampling</b> <b>variations,</b> which may partly {{be due to the}} functional redundancy among genes, implying that multiple genes can play exchangeable roles in the cell. In this paper we use the concept of exchangeability of random variables to model this functional redundancy and thereby account for the instability attributable to <b>sampling</b> <b>variations.</b> We present a flexible framework to incorporate the exchangeability into the representation of lists. The proposed framework supports straightforward robust comparison between any two lists. It {{can also be used to}} generate new, more stable gene rankings incorporating more information from the experimental data. Using a microarray data set from lung cancer patients we show that the proposed method provides more robust gene rankings than existing methods with respect to <b>sampling</b> <b>variations,</b> without compromising the biological significance...|$|R
40|$|We {{investigate}} {{the potential of}} artificial neural networks in diagnosing thyroid diseases. The robustness of neural networks with regard to <b>sampling</b> <b>variations</b> is examined using a cross‐validation method. We illustrate the link between neural networks and traditional Bayesian classifiers. Neural networks can provide good estimates of posterior probabilities and hence can have better classification performance than traditional statistical methods such as logistic regression. The neural network models are further shown to be robust to <b>sampling</b> <b>variations.</b> It is demonstrated that for medical diagnosis problems where the data are often highly unbalanced, neural networks can be a promising classification method for practical use. Copyright Kluwer Academic Publishers 1998...|$|R
40|$|Efficiency {{scores of}} {{production}} units are generally measured relative {{to an estimated}} production frontier. Nonparametric estimators (DEA, FDH, [...] .) {{are based on a}} finite sample of observed production units. The bootstrap is one easy way to analyze the sensitivity of efficiency scores relative to the <b>sampling</b> <b>variations</b> of the estimated frontier. The main point in order to validate the bootstrap is to define a reasonable data generating process in this complex framework and to propose a reasonable estimator of it. This provides a general methodology of bootstrapping in nonparametric frontier models. Some adapted methods are illustrated in analyzing the bootstrap <b>sampling</b> <b>variations</b> of input efficiency measures of electricity plants. ...|$|R
50|$|In statistics, {{sampling}} error is the error caused by observing a sample {{instead of the}} whole population. The {{sampling error}} {{is the difference between}} a sample statistic used to estimate a population parameter and the actual but unknown value of the parameter.An estimate of a quantity of interest, such as an average or percentage, will generally be subject to sample-to-sample variation. These variations in the possible sample values of a statistic can theoretically be expressed as sampling errors, although in practice the exact sampling error is typically unknown. Sampling error also refers more broadly to this phenomenon of random <b>sampling</b> <b>variation.</b>|$|E
50|$|Two main {{statistical}} methods {{are used in}} data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, <b>sampling</b> <b>variation).</b> Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes {{the extent to which}} members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.|$|E
5000|$|As {{an example}} of the use of the {{relative}} standard error, consider two surveys of household income that both result in a sample mean of $50,000. If one survey has a standard error of $10,000 and the other has a standard error of $5,000, then the relative standard errors are 20% and 10% respectively. The survey with the lower relative standard error can be said to have a more precise measurement, since it has proportionately less <b>sampling</b> <b>variation</b> around the mean. In fact, data organizations often set reliability standards that their data must reach before publication. For example, the U.S. National Center for Health Statistics typically does not report an estimated mean if its relative standard error exceeds 30%. (NCHS also typically requires at least 30 observations - if not more - for an estimate to be reported.) ...|$|E
50|$|Another {{variation}} is the Cosmic <b>Sampler</b> <b>variation,</b> which {{was featured in}} an electronic version for the Palm. A Sampler is when none of the dice on any roll match, similar to a large straight in Yahtzee. Scoring for a Sampler is 25 points.|$|R
40|$|Calcia- and yttria-stabilized zirconias {{have been}} {{synthesised}} by a novel rapid-combustion route. The oxide-ion conductivities of these stabilized zirconias {{are comparable to}} those of commercially prepared <b>samples.</b> <b>Variations</b> in grain boundary impedance of calcia- and yttria-stabilized zirconias with composition and temperature are presented...|$|R
40|$|Abstract — AFM imaging {{requires}} precision {{positioning of}} the AFM probe relative to the sample in all x-y-z axes, especially the vertical z-axis direction. Recently, the current-cycle-feedback iterative-learning-control (CCF-ILC) approach is proposed for high-speed AFM imaging. The CCF-ILC feedforward-feedback 2 degree-of-freedom (DOF) controller design has been successfully implemented for iteratively imag-ing on one scanline. In this article, we extend this CCF-ILC approach to the entire imaging of samples. The main contribution {{of this article is}} the analysis {{and the use of the}} CCF-ILC approach for tracking <b>sample</b> profiles with <b>variations</b> between scanlines (called line-to-line <b>sample</b> <b>variations).</b> The convergence (stability) of the CCF-ILC system is analyzed for the general case where the line-to-line <b>sample</b> <b>variation</b> occurs at each iteration. The allowable line-to-line <b>sample</b> profile <b>variation</b> is quantified. The performance improvement of the CCF-ILC is discussed by comparing the tracking error of the CCF-ILC technique to that of using feedback control alone. The proposed CCF-ILC control approach is illustrated by implementing it to the z-axis direction control in AFM imaging. Experimental results show that the imaging speed can be significantly increased by using the proposed approach. I...|$|R
50|$|The {{basis for}} the profile {{probability}} estimation for Y-STR analysis is the counting method. The application of a confidence interval accounts for database size and <b>sampling</b> <b>variation.</b> The Y haplotype frequency (p) is calculated using the p = x/N formula, where x {{is equal to the}} number of times the haplotype is observed in a database containing N number of haplotypes. For example, if a haplotype has been observed twice in a database of N = 2000, the frequency of that haplotype will be: 2/2000 = 0.001. Reporting a Y haplotype frequency, without a confidence interval, is acceptable but only provides a factual statement regarding observations of a Y haplotype in the database. An upper confidence limit for the probability of the Y haplotype in the population should be calculated using the method described by Clopper and Pearson (1934). This uses the binomial distribution for the probabilities of counts, including zero or other small numbers that are found for Y haplotypes.|$|E
50|$|In several studies, {{the authors}} {{demonstrated}} that the endowment effect {{could be explained by}} loss aversion but not five alternatives: (1) transaction costs, (2) misunderstandings, (3) habitual bargaining behaviors, (4) income effects, or (5) trophy effects. In each experiment half of the subjects were randomly assigned a good and asked for the minimum amount {{they would be willing to}} sell it for while the other half of the subjects were given nothing and asked for the maximum amount they would be willing to spend to buy the good. Since the value of the good is fixed and individual valuation of the good varies from this fixed value only due to <b>sampling</b> <b>variation,</b> the supply and demand curves should be perfect mirrors of each other and thus half the goods should be traded. The authors also ruled out the explanation that lack of experience with trading would lead to the endowment effect by conducting repeated markets.|$|E
5000|$|The {{proportional}} reporting ratio (PRR) is {{a statistic}} {{that is used}} to summarize {{the extent to which a}} particular adverse event is reported for individuals taking a specific drug, compared to the frequency at which the same adverse event is reported for patients taking some other drug (or who are taking any drug in a specified class of drugs). The PRR will typically be calculated using a surveillance database in which reports of adverse events from a variety of drugs are recorded. A PRR greater than 1 suggests that the adverse event is more commonly reported for individuals taking the drug of interest, relative to the comparison drugs. This could indicate that the adverse event is caused by the drug of interest and therefore a [...] "side effect", although a PRR exceeding 1 could also reflect <b>sampling</b> <b>variation</b> in the data, reporting errors, biased reporting, multiple reports of the same case or the same patient, or a number of other causes.|$|E
40|$|A multicenter {{study of}} {{molecular}} detection of enteroviruses was conducted using a proficiency panel. Of 70 data sets, 46 (66 %) reported correct results for samples containing at least 1 50 % infective dose per ml and for negative <b>samples.</b> <b>Variation</b> in performance between laboratories demonstrates {{the need for}} ongoing quality control...|$|R
30|$|Most values {{within the}} long-span {{measurements}} dropped slightly, except for rag paper samples, {{due to the}} accelerated aging (6  days), so that the aging processes exerts a small influence on the strength properties of sample papers. Nevertheless, the tensile index values of some <b>sample</b> <b>variations</b> are higher compared to the aged reference.|$|R
40|$|Two-dimensional {{electrophoresis}} (2 -DE) {{is one of}} {{the most}} commonly used techniques in proteomic investigations. However, due to the complex interplay of incidence including significant biological <b>sample</b> <b>variations,</b> lengthy steps involved in performing 2 -DE as well as exposure time with silver staining, it is sometimes difficult to differentiate authentic differences caused by drug treatment with those artifacts caused by <b>sample</b> <b>variations,</b> running conditions of 2 -DE as well as treatment time in silver staining etc. If we can compare pooled samples of control and treatment groups run in a single gel and stained together, we would be more comfortable with our findings. We propose here a low cost and highly effective method for locating differentially expressed proteins before and after drug treatment. This "two-in-one gel" technique might partially solve the problems mentioned above. Department of Applied Biology and Chemical Technolog...|$|R
3000|$|Percentage of {{variance}} in the study-specific point estimates that is attributable to true between-study heterogeneity as opposed to <b>sampling</b> <b>variation</b> [...]...|$|E
40|$|In {{this paper}} we use cytonuclear disequilibria {{to test the}} {{neutrality}} of mtDNA markers. The data considered here involve sample frequencies of cytonuclear genotypes subject to both statistical <b>sampling</b> <b>variation</b> as well as genetic <b>sampling</b> <b>variation.</b> First, we obtain {{the dynamics of the}} sample cytonuclear disequilibria assuming random drift alone as the source of genetic <b>sampling</b> <b>variation.</b> Next, we develop a test statistic using cytonuclear disequilibria via the theory of generalized least squares to test the random drift model. The null distribution of the test statistic is shown to be approximately chi-squared using an asymptotic argument as well as computer simulation. Power of the test statistic is investigated under an alternative model with drift and selection. The method is illustrated using data from cage experiments utilizing different cytonuclear genotypes of Drosophila melanogaster. A program for implementing the neutrality test is available upon request...|$|E
3000|$|Open {{image in}} new window, like other {{statistics}} {{is subject to}} the <b>sampling</b> <b>variation,</b> it is critical to compute a confidence interval to provide a range which includes the true PCI [...]...|$|E
40|$|Bootstrap {{resampling}} {{was used}} to examine the <b>sampling</b> <b>variations</b> in the average of a population with a known mean. Two thousand bootstrap estimates of the mean were generated from a sample of 50 observations drawn from a population of 10 independent and identically distributed observations with an underlying uniform distribution on the unit interval. Analyses were performed using Mathematica, SAS/IML, S-PLUS, and STATA. Bootstrap approximations to the cumulative distribution function of the mean from these three software packages were identical and consistent with Normal Theory. This study demonstrates that the <b>sampling</b> <b>variations</b> of the mean can be estimated from a population of only 10 observations. The source code can be easily modified for any population. B. INTRODUCTION In ordinary usage, the phrase "resampling methods" refers to methods in which the observed data are used repeatedly, in a computer-intensive analysis, to provide inferences. In simple terms, resampling does with a [...] ...|$|R
40|$|In this study, the {{reproducibility}} of tryptic digestion {{of complex}} solutions was investigated using liquid chromatography Fourier transform {{ion cyclotron resonance}} (LC FT-ICR) mass spectrometry. Tryptic peptides, from human cerebrospinal fluid, (CSF) were labeled with Quantification-Using-Enhanced-Signal-Tags (QUEST) -markers, or 1 -([H- 4]nicotinoyloxy) - and 1 -([D- 4]nicotinoyloxy) -succinimide ester markers. The analysis was performed on abundant proteins with respect-to-intensity ratios and sequence coverage and obtained by comparing differently labeled components from one or different pools. To interpret the dynamics in the proteome, one {{must be able to}} estimate the error introduced in each experimental steps. The intra <b>sample</b> <b>variation</b> due to derivatization was approximately 10 %. The inter <b>sample</b> <b>variation</b> depending on derivatization and tryptic digestion was not more than approximately 30 %. These experimental observations provide a range for the up- and clown-regulations that are possible to study with electrospray ionization LC FT-ICR mass spectrometry...|$|R
30|$|To {{minimize}} {{the effects of}} <b>sample</b> <b>variation</b> on the test results, all the samples were oven-dried first so that the growth stress was released in some extent, and then vacuum pressure treated {{so that all the}} samples were fully water saturated having a similar MC. Based on the oven-dried weight, the MC can be calculated in water-saturated condition and after-compression condition.|$|R
40|$|A new {{algorithm}} is developed to solve models with heterogeneous agents and aggregate uncertainty. Projection methods {{are the main}} building blocks of the algorithm and - {{in contrast to the}} most popular solution procedure - simulations only play a very minor role. The paper also develops a new simulation procedure that not only avoids cross-sectional <b>sampling</b> <b>variation</b> but is 10 (66) times faster than simulating an economy with 10, 000 (100, 000) agents. Because it avoids cross-sectional <b>sampling</b> <b>variation,</b> it can generate an accurate representation of the whole cross-sectional distribution. Finally, the paper outlines a set of accuracy tests. ...|$|E
40|$|Typical {{statistical}} analysis of epidemiologic data captures uncertainty due to random <b>sampling</b> <b>variation,</b> but ignores more systematic sources of variation such as selection bias, measurement error, and unobserved confounding. Such sources are often only mentioned via qualitative caveats, perhaps {{under the heading of}} ‘study limitations. ’ Recently, however, there has been considerable interest and advancement in probabilistic methodologies for more integrated {{statistical analysis}}. Such techniques hold the promise of replacing a confidence interval reflecting only random <b>sampling</b> <b>variation</b> with an interval reflecting all, or at least more, sources of uncertainty. We survey and appraise the recent literature in this area, giving some prominence to the use of Bayesian statistical methodology...|$|E
40|$|In recent {{collaborative}} {{biological sampling}} exercises organised by the Nottingham Regional Laboratory of the Severn-Trent Water Authority, {{the effect of}} handnet <b>sampling</b> <b>variation</b> on the quality and usefulness of the data obtained has been questioned, especially when this data is transcribed into {{one or more of}} the commonly used biological methods of water quality assessment. This study investigates if this effect is constant at sites with similar typography but differing water quality states when the sampling method is standardized and carried out by a single operator. An argument is made for the use of a lowest common denominator approach to give a more consistent result and obviate the effect of <b>sampling</b> <b>variation</b> on these biological assessment methods...|$|E
40|$|For {{bacterial}} {{sampling of}} raw unprocessed fish and frozen fishery products, spread plate method {{is preferable to}} pour plate method; incubation of plates at 30 °C gives a higher count than incubation at 37 °C. Analysis of variance of the data shows that <b>sample</b> <b>variation</b> between different types of fishes is highly significant whereas the variations between triplicate plates are not significant at 5 % level...|$|R
5000|$|This {{correction}} is {{so common}} {{that the term}} [...] "sample variance" [...] and [...] "sample standard deviation" [...] are frequently used to mean the corrected estimators (unbiased <b>sample</b> <b>variation,</b> less biased <b>sample</b> standard deviation), using n − 1. However caution is needed: some calculators and software packages may provide for both or only the more unusual formulation. This article uses the following symbols and definitions: ...|$|R
30|$|Drop casting is a {{slow process}} whereas spin coating {{provides}} a fast track for prototyping. In our study, spin coating of NFC-G films reduced the processing time from 24  h down to a few minutes. In addition, the <b>sample</b> to <b>sample</b> <b>variation</b> was reduced with spin coating. This is especially important with multilayer structures as every additional sample preparation step with successive coating layers can {{be a source of}} error.|$|R
