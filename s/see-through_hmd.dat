35|0|Public
30|$|Another {{requirement}} {{for the development of}} an AR game is the display device. The display devices used in AR may have less stringent requirements than VE systems demand, again because AR does not replace the real world. For example, monochrome displays may be adequate for some AR applications, while virtually all VE systems today use full color. Optical see-through HMDs with a small field-of-view may be satisfactory because the user can still see the real world with his peripheral vision; the <b>see-through</b> <b>HMD</b> does not shut off the user’s normal field-of-view. Furthermore, the resolution of the monitor in an optical <b>see-through</b> <b>HMD</b> might be lower than what a user would tolerate in a VE application, since the optical <b>see-through</b> <b>HMD</b> does not reduce the resolution of the real environment.|$|E
40|$|AbstractIn this paper, {{we propose}} {{a simple and}} {{effective}} system to extract a pupil and gaze detection. This system is developed using an optical based method to extract user's pupil. For extracting the pupil of the eye and detecting gaze, we use a camera with an infrared filter, an optical <b>see-through</b> <b>HMD,</b> and infrared light. We can possible to move freely without fixing it to user's head because of optical <b>see-through</b> <b>HMD</b> with the camera, and infrared light. The proposed system displays the gaze point on the HMD after extracting user's pupil and detecting gaze real time...|$|E
40|$|An augmented-vision {{device for}} {{patients}} with severely restricted peripheral visual field (tunnel vision) was proposed, combining a <b>see-through</b> <b>HMD</b> and minified contour detection. Implemented commercial off-the-shelf (COTS) configurations were tested in real environments by Retinitis Pigmentosa patients and normally sighted subjects...|$|E
40|$|A first {{prototype}} of a bi-directional OLED microdisplay device has been designed, that combines both display and camera functionality {{on a single}} CMOS chip (OLED-on-CMOS). Major aim of this integration is to provide small form-factor display and eye-tracking for <b>see-through</b> <b>HMD</b> applications (augmented reality) ...|$|E
40|$|First prototypes of {{bi-directional}} OLED microdisplay {{devices have}} been designed, that combine both display and camera functionality {{on a single}} CMOS chip (OLED-on-CMOS). Major aim of this integration is to provide capabilities for eye-tracking in <b>see-through</b> <b>HMD</b> to achieve gaze-based human-display-interaction, e. g. in augmented-reality applications...|$|E
40|$|First demonstrators of {{bi-directional}} OLED microdisplay {{devices have}} been developed and integrated into <b>see-through</b> <b>HMD</b> optics. The device combines 'display' and 'imaging' by nested OLED pixels and photodetectors in a single CMOS chip. Major aim of this integration is to provide capabilities for eyetracking to achieve gaze-based human-displayinteraction...|$|E
40|$|In this poster {{we present}} an {{innovative}} daylight blocking optical stereo <b>see-through</b> <b>HMD.</b> Its outstanding capability is to pixelwise block incident daylight before super-imposing virtual content {{on the real}} scene. By doing so the device allows to seamlessly mix real and virtual content without the well-known effects of other optical see-through displays where real content will always shine through virtual content (ghosting) ...|$|E
30|$|The {{augmented}} view {{generated by}} the video compositor is ultimately presented by the Video <b>See-Through</b> <b>HMD.</b> The eMagin Z 800 3 DVisor is chosen as the HMD component of ARMOR because it has remarkable performance in primary factors, including wide view angle, large number of colors, lightweight frame, and comfort. Furthermore, the stereovision capability is another important rendering effect that helps the user to better appreciate the 3 D augmented space.|$|E
40|$|An {{augmented}} vision <b>see-through</b> <b>HMD,</b> which {{displays a}} minified contour {{view of the}} real world over the residual visual field of tunnel vision patients, was evaluated in a visual search experiment. Kinematic evaluation of subjects' eye and head movements showed that tunnel vision patients could find and locate targets outside their visual fields faster and more efficiently with the device than without an aid or with an acoustic cue...|$|E
40|$|Figure 1. Move clip (VariFocalPlane. wmv) of {{a virtual}} torus {{rendered}} by the <b>see-through</b> <b>HMD</b> moving along the z-axis in the augmented space. The three subset clips were captured by a camcorder which was manually focused at (a) 16 cm, (b) 33 cm, and (c) 100 cm, respectively. Most existing stereoscopic head mounted displays (HMDs), presenting {{a pair of}} stereoscopic images at a fixed focal distance, lack the ability to correctly render the naturally coupled accommodation and convergence cues. Psychophysical {{studies have shown that}} such displays may cause many adverse consequences such as visual fatigue, diplopic vision, degraded oculomotor response, and depth perception errors. In this paper, we present a <b>see-through</b> <b>HMD</b> with addressable focal planes utilizing a novel active optical element—a liquid lens. The element, with a varying optical power from- 5 to 20 diopters, is able to address the focal distance of the HMD from infinity to the near point of the eye. A monocular prototype was built from off-the-shelf elements and experimental results are presented to validate the proposed designs. We also describe both subjective and objective measurements of the accommodation responses of the viewer to the focal distances presented by the prototype...|$|E
40|$|In {{this paper}} we present an {{innovative}} daylight blocking optical stereo <b>see-through</b> <b>HMD.</b> Its outstanding capability is to pixelwise block incident daylight before super-imposing virtual content {{on the real}} scene. By doing so the device allows to seamlessly mix real and virtual content without the well-known effects of other optical see-through displays where real content will always shine through virtual content. To our knowledge related work on daylight blocking displays is very limited and we present the first commercial HMD available...|$|E
30|$|DECLARE {{is based}} on a {{centralized}} architecture. It contains different applications for the local and remote user. The applications have different user interfaces which are created using the Unity game engine. The application for the local user is adapted for the optical <b>see-through</b> <b>HMD</b> as shown in Fig.  3. The application for the remote user runs on a desktop computer or laptop. Thus, the remote user gets a screen-based visualization and can interact with the system by using the keyboard and a standard mouse device.|$|E
40|$|We {{describe}} an augmented reality conferencing system which uses the overlay of virtual {{images on the}} real world. Remote collaborators are represented on Virtual Monitors which can be freely positioned about a user in space. Users can collaboratively view and interact with virtual objects using a shared virtual whiteboard. This is possible through precise virtual image registration using fast and accurate computer vision techniques and HMD calibration. We propose a method for tracking fiducial markers and a calibration method for optical <b>see-through</b> <b>HMD</b> based on the marker tracking. 1...|$|E
40|$|Augmented reality, radio {{frequency}} identification, maintenance support, water system isolation, usability evaluation Aiming at improvement of task performance and reduction of human error of water system isolation task in NPP periodic maintenance, {{a support system}} using state-of-art information technology, Augmented Reality (AR) and Radio Frequency Identification (RFID) has been proposed, and a prototype system has been developed. The system has navigation function of which an indication is superimposed directly on the user’s view to help to find the designated valves by AR. It also has valve confirmation function by scanning RFID tag attached on the valve. In case of applying it to practical use, its information presentation device {{is important because it}} affects the task performance. In this study, therefore, a suitable information presentation device has been pursued by conducting subject experiments employing psychological experimental technique. The candidates of the devices are one-eye video <b>see-through</b> <b>HMD</b> (SCOPO) and both-eye video <b>see-through</b> <b>HMD</b> (Glasstron) as wearable system configuration, and tablet PC and compact TV as handheld system configuration. In the experiment, task completion time, number of errors, NASA-TLX score as subjects ’ mental workload and subjective usability questionnaire were measured when using the above devices. As the results, it was found that one-eye video see-though head mounted display, SCOPO was suitable device as wearable system configuration, and compact TV was suitable device as handheld system configuration...|$|E
40|$|Microdisplays {{based on}} organic {{light-emitting}} diodes (OLEDs) achieve high optical performance with excellent contrast ratio and large dynamic range at low power consumption. The direct light emission from the OLED enables small devices without additional backlight, making them suitable for mobile near-to-eye (NTE) {{applications such as}} viewfinders or head-mounted displays (HMD). In these applications the microdisplay acts typically as a purely unidirectional output device [1 - 3]. With the integration of an additional image sensor, the functionality of the microdisplay can be extended to a bidirectional optical input/output device. The major aim is the implementation of eye-tracking capabilities in <b>see-through</b> <b>HMD</b> applications to achieve gaze-based human-display-interaction...|$|E
40|$|Among {{the most}} {{critical}} issues {{in the design of}} immersive virtual environments are those that deal with the problem of technologically induced intersensory conict and one of the results, sensorimotor adaptation. An experiment was conducted to sup-port the design of a prototype see-through, head-mounted display (HMD). When wearing video see-through HMDs in augmented reality systems, subjects see the world around them through a pair of head-mounted video cameras. The study looked at the effects of sensory rearrangement caused by a HMD design that displaced the user’s ‘‘virtual’ ’ eye position forward (165 mm) and above (62 mm) toward the spatial position of the cameras. The position of the cameras creates images of the world that are slightly downward and inward from normal. Measures of hand-eye coordination and speed on a manual pegboard task revealed substantial perceptual costs of the eye displacement initially, but also evidence of adaptation. Upon rst wearing the video <b>see-through</b> <b>HMD,</b> subjects ’ pointing errors increased signicantly along the spatial dimensions displaced (the y dimension, above-below the target, and z dimension, in front-behind the target). Speed of performance on the pegboard task decreased by 43 % compared to baseline performance. Pointing accuracy improved by approxi-mately 33 % as subjects adapted to the sensory rearrangement, but it did not reach baseline performance. When subjects removed the <b>see-through</b> <b>HMD,</b> there was evidence that their hand-eye coordination had been altered. Negative aftereffects were observed in the form of greater errors in pointing accuracy compared to base-line. Although these aftereffects are temporary, the results may have serious practical implications for the use of video see-through HMDs by users (e. g., surgeons) who depend on very accurate hand-eye coordination...|$|E
40|$|In this paper, {{we propose}} a {{wearable}} 3 D Augmented Reality Keyboard (ARKB) which enables a user to type text or control CG objects without using conventional interfaces, such as keyboard or mouse. The proposed ARKB exploits 3 D depth information obtained through a stereo camera attached to an HMD. The ARKB {{consists of three}} modules: (i) 3 D vision-based tracking, (ii) natural interaction with fingers, and (iii) audiovisual feedback on the 3 D video <b>see-through</b> <b>HMD.</b> The proposed ARKB can be applied as an interface for typing in AR environment. The remaining challenges are study on tracking method to improve accuracy and newly designed virtual keyboard which is proper in representing {{the advantage of the}} interaction in 3 D space...|$|E
40|$|In {{the present}} research, we have {{developed}} a system by which to simulate three-dimensional architecture and urban landscapes in any outdoor space. As the basic AR environment, we used Vizard running on a laptop PC, where the urban model component, location tracking component, and image display component work together using original Python scripts. For the urban model component, digital maps data were converted. For the location tracking component, portable DGPS and a high-precision gyroscope were introduced {{in order to minimize}} the locational error. For the image display component, optical <b>see-through</b> <b>HMD</b> was used. Stereovision was also realized with the functions of GPU on the PC. A walking experiment was performed to test the proposed system on a redevelopment plan for our university campus...|$|E
40|$|Conventional head-mounted {{displays}} (HMDs) {{consisting of}} a pair of miniature projection lenses, beam splitters, and miniature displays mounted on the helmet, as well as phase conjugate material placed strategically in the environment have been redesigned to integrate the phase-conjugate material into a complete see-through embodiment. Some initial efforts of demonstrating the concept was followed by an investigation of the diffraction effects versus image degradation caused by integrating the phase-conjugate material internally in the HMD. The key contribution of this paper lies in the conception, and assessment of a novel <b>see-through</b> <b>HMD.</b> Finally, the diffraction efficiency of the phase-conjugate material is evaluated, and the overall performance of the optics is assessed in both object space for the optical designer and visual space for possible users for this technology. 1...|$|E
40|$|This paper {{introduces}} two {{case studies}} of augmented reality #AR# system, which use see-through HMDs. The #rst case is a collaborative AR system called AR 2 Hockey that requires real-time interactive operations, moderate registration accuracy, and a relatively small registration area. The players can hit a virtual puck with physical mallets in a shared physical game #eld. The other case study is the MR Living Room where participants can visually simulate the location of virtual furniture and articles in the physically half-equipped living room. The registration is more crucial {{in this case because}} of the requirement of visual simulation. As well as the system con- #gurations of both systems, the details of registration algorithms implemented are described. Keywords: Augmented reality, mixed reality, collaboration, visual simulation, registration, <b>see-through</b> <b>HMD</b> 1 Introduction Most of VR systems wehave experienced in this decade made it possible for participants to inter [...] ...|$|E
40|$|In {{this paper}} we {{introduce}} an innovative application {{designed to make}} collaborative design review in the architectural and automotive domain more effective. For this purpose we present a system architecture which combines a variety of visualization technologies such as high resolution multi-tile displays, TabletPCs and headmounted displays with innovative 2 D and 3 D Interaction Paradigms to better support collaborative mobile mixed reality design reviews. Our research and development is motivated by two user scenarios: architectural and automotive design review involving real users from Page architects and FIAT Elasis. Our activities are supported by the EU IST project IMPROVE aimed at developing advanced display techniques, fostering activities in the areas of: optical <b>see-through</b> <b>HMD</b> development using unique OLED technology, marker-less optical tracking, mixed reality rendering, image calibration for large tiled displays, collaborative tablet-based and projection wall oriented interaction and stereoscopic video streaming for mobile users. The paper gives {{an overview of the}} targeted scenarios and focuses in particular on the rendering aspects of the project. 1...|$|E
40|$|Advances in helmet-mounted {{displays}} (HMDs) {{have permitted}} {{the design of}} “see-through ” displays in which virtual imagery may be superimposed upon real visual environments. Such displays have numerous potential applications; however, their promise to im-prove human perception and performance in complex task environments is threatened by numerous techno-logical challenges. Moreover, users of HMDs may be vulnerable to symptoms associated with simulator sick-ness. The primary objective {{of this investigation was}} to assess subjective ratings of simulator sickness as a function of time delay, time on task, and task complex-ity. Participants attempted to center a reticle over a moving circular target using a <b>see-through</b> <b>HMD</b> while concurrently performing a visual monitoring task dis-played on a computer monitor. Results indicated that simulator sickness ratings varied directly with time on task, while performance efficiency and ratings of per-ceived mental workload were not mediated by this fac-tor. Furthermore, the time delay manipulation that af-fected performance efficiency and operator workload did not generally influence SSQ ratings. These find-ings are discussed in terms of their implications for practical implementation of see-through HMDs in multi-task environments...|$|E
40|$|AbstractThe {{surgical}} {{navigation system}} has experienced tremendous development {{over the past}} decades for minimizing the risks and improving the precision of the surgery. Nowadays, Augmented Reality (AR) -based surgical navigation is a promising technology for clinical applications. In the AR system, virtual and actual reality are mixed, offering real-time, high-quality visualization of an extensive variety of information to the users (Moussa et al., 2012) [1]. For example, virtual anatomical structures such as soft tissues, blood vessels and nerves can be integrated with the real-world scenario in real time. In this study, an AR-based surgical navigation system (AR-SNS) is developed using an optical <b>see-through</b> <b>HMD</b> (head-mounted display), aiming at improving the safety and reliability of the surgery. With {{the use of this}} system, including the calibration of instruments, registration, and the calibration of HMD, the 3 D virtual critical anatomical structures in the head-mounted display are aligned with the actual structures of patient in real-world scenario during the intra-operative motion tracking process. The accuracy verification experiment demonstrated that the mean distance and angular errors were respectively 0. 809 ± 0. 05 mm and 1. 038 °± 0. 05 °, which was sufficient to meet the clinical requirements...|$|E
40|$|AbstractEvacuation drills are {{commonly}} conducted as traditional disaster education to reduce damages from natural disasters. However participants {{are not always}} interested in or committed to such drills. To improve this situation, we focused on Edutainment and proposed game-based evacuation drill (GBED) using the Real-World Edutainment (RWE) program. There {{are two types of}} GBED systems, i. e. the Tablet-based GBED (T-GBED) and the AR and HMD-based GBED (AH-GBED). We conducted GBED at several schools and determined that it can improve student motivation for disaster prevention. Subduction-zone earthquakes frequently generate tsunamis and can cause catastrophic damage especially to coastal areas. Thus people in coastal areas must move very quickly to evacuation sites when a massive earthquake occurs. Both GBED systems cannot be used directly for tsunami evacuation drills because the participants will not want to sprint while holding a tablet or wearing a HMD and have time to stop to view the digital materials. In this study, we propose a tsunami evacuation drill (TED) and have developed a TED system. The TED system uses smart glasses (a lightweight optical <b>see-through</b> <b>HMD)</b> which allows participants to view digital materials while moving quickly...|$|E
40|$|In mixed reality, occlusions {{and shadows}} are {{important}} to realize a natural fusion between the real and virtual worlds. In order to achieve this, {{it is necessary to}} ac-quire dense depth information of the real world from the observer’s viewing posi-tion. The depth sensor must be attached to the <b>see-through</b> <b>HMD</b> of the observer because he/she moves around. The sensor should be small and light enough to be attached to the HMD and should be able to produce a reliable dense depth map at video rate. Unfortunately, however, no such depth sensors are available. We propose a client/server depth-sensing scheme to solve this problem. A server sen-sor located at a fixed position in the real world acquires the 3 -D information of the world, and a client sensor attached to each observer produces the depth map from his/her viewing position using the 3 -D information supplied from the server. Multi-ple clients can share the 3 -D information of the server; we call it Share-Z. In this paper, the concept and merits of Share-Z are discussed. An experimental system developed to demonstrate the feasibility of Share-Z is also described. ...|$|E
40|$|The brain-machine {{interface}} (BMI) or {{brain-computer interface}} (BCI) {{is a new}} interface technology that uses neurophysiological signals from the brain to control external machines or computers. This technology is expected to support daily activities, especially for persons with disabilities. To expand the range of activities enabled by this type of interface, here, we added augmented reality (AR) to a P 300 -based BMI. In this new system, we used a see-through head-mount display (HMD) to create control panels with flicker visual stimuli to support the user in areas close to controllable devices. When the attached camera detects an AR marker, the position and orientation of the marker are calculated, and the control panel for the pre-assigned appliance is created by the AR system and superimposed on the HMD. The participants were required to control system-compatible devices, and they successfully operated them without significant training. Online performance with the HMD was not different from that using an LCD monitor. Posterior and lateral (right or left) channel selections contributed to operation of the AR-BMI with both the HMD and LCD monitor. Our results indicate that AR-BMI systems operated with a <b>see-through</b> <b>HMD</b> {{may be useful in}} building advanced intelligent environments...|$|E
40|$|OLED-on-CMOS microdisplays can be {{used for}} near-to-eye (NTE) {{applications}} such as viewfinders of cameras or head-mounted displays (HMD). Especially in <b>see-through</b> <b>HMD</b> applications it would be helpful if the user has a possibility to interact with the display content in an augmented reality setting while preserving hands-free operation. Such user interaction can be handled by an eye-tracker based on an integrated near-infrared (NIR) camera. Therefore integration of NIR camera photodetectors and AMOLED microdisplay into a single device will lead to a highly integrated, very light-weight, small-sized bi-directional microdisplay ("OLEDCAM"), which {{can be used}} as visual Input-/Output-Device (I/O) For personal information management (PIM) (see Fig. 1). Applications could be in mobile communication (connected to smartphone), industry (worker assistance), medicine (surgeon assistance), security (surveillance, pilots), barrier-free operation (handicapped people), travel/transport (driver assistance, tourist information), or others). The use of OLEDs in active matrix CMOS substrates requires a top-emitting, low voltage and highly efficient OLED stack. Bi-directional AMOLED microdisplays potentially require high-speed OLED switching behaviour (< 100 µs), since display and eye tracker camera, and to minimise distortion in the eye-tracker camera. A first prototype of a bi-directional OLED microdisplay device has been designed, that combines both display and camera functionality on a single CMOS chip (OLED-on-CMOS). Major aim of this integration is to provide small form-factor display and eye-tracking for see-trough HMD applications (augmented reality) ...|$|E
40|$|Abstract This paper {{presents}} a new media production for sports event like a soccer match. Multiple cameras capture a soccer match at stadium. The soccer match {{can be observed}} at arbitrary viewpoints selected by each viewer. Video at the arbitrary viewpoints is synthesized from the videos captured with the actual cameras by image-based view interpolation technique. Our view interpolation method does not need full calibration of the multiple cameras, but only need projective geometry information such as fundamental matrices among the multiple cameras, which is easily measured even in a large space in an actual stadium. In this paper, two visualization systems are introduced for arbitrary viewpoint observation of soccer matches from multiple video images. The first is Viewpoint on Demand System that enables viewers to select their own viewpoints during observation through GUI. Photorealistic soccer scene, including players, ball and soccer field, is rendered at arbitrary viewpoints by view interpolation method from actual camera positions. The second is observation system via a video <b>see-through</b> <b>HMD.</b> Viewers see a desktop stadium model {{in front of their}} eyes through the HMD, while images of players and ball are overlaid on the display. This system enables viewers to virtually fly through over the soccer field model in the real world. The proposed systems lead to make a new type of immersive media for entertaining such events. 1...|$|E
40|$|Attention-awareness {{is a key}} {{topic for}} the {{upcoming}} generation of computer-human interaction. A human moves his or her eyes to visually attends to a particular region in a scene. Consequently, {{he or she can}} process visual information rapidly and efficiently without being overwhelmed by vast amount of information from the environment. Such a physiological function called visual attention provides a computer system with valuable information of the user to infer his or her activity and the surrounding environment. For example, a computer can infer whether the user is reading text or not by analyzing his or her eye movements. Furthermore, it can infer with which object he or she is interacting by recognizing the object the user is looking at. Recent developments of mobile eye tracking technologies enable us to capture human visual attention in ubiquitous everyday environments. There are various types of applications where attention-aware systems may be effectively incorporated. Typical examples are augmented reality (AR) applications such as Wikitude which overlay virtual information onto physical objects. This type of AR application presents augmentative information of recognized objects to the user. However, if it presents information of all recognized objects at once, the over ow of information could be obtrusive to the user. As a solution for such a problem, attention-awareness can be integrated into a system. If a system knows to which object the user is attending, it can present only the information of relevant objects to the user. Towards attention-aware systems in everyday environments, this thesis presents approaches for analysis of user attention to visual content. Using a state-of-the-art wearable eye tracking device, one can measure the user's eye movements in a mobile scenario. By capturing the user's eye gaze position in a scene and analyzing the image where the eyes focus, a computer can recognize the visual content the user is currently attending to. I propose several image analysis methods to recognize the user-attended visual content in a scene image. For example, I present an application called Museum Guide 2. 0. In Museum Guide 2. 0, image-based object recognition and eye gaze analysis are combined together to recognize user-attended objects in a museum scenario. Similarly, optical character recognition (OCR), face recognition, and document image retrieval are also combined with eye gaze analysis to identify the user-attended visual content in respective scenarios. In addition to Museum Guide 2. 0, I present other applications in which these combined frameworks are effectively used. The proposed applications show that the user can benefit from active information presentation which augments the attended content in a virtual environment with a see-through head-mounted display (HMD). In addition to the individual attention-aware applications mentioned above, this thesis presents a comprehensive framework that combines all recognition modules to recognize the user-attended visual content when various types of visual information resources such as text, objects, and human faces are present in one scene. In particular, two processing strategies are proposed. The first one selects an appropriate image analysis module according to the user's current cognitive state. The second one runs all image analysis modules simultaneously and merges the analytic results later. I compare these two processing strategies in terms of user-attended visual content recognition when multiple visual information resources are present in the same scene. Furthermore, I present novel interaction methodologies for a <b>see-through</b> <b>HMD</b> using eye gaze input. A <b>see-through</b> <b>HMD</b> is a suitable device for a wearable attention-aware system for everyday environments because the user can also view his or her physical environment through the display. I propose methods for the user's attention engagement estimation with the display, eye gaze-driven proactive user assistance functions, and a method for interacting with a multi-focal see-through display. Contributions of this thesis include: • An overview of the state-of-the-art in attention-aware computer-human interaction and attention-integrated image analysis. • Methods for the analysis of user-attended visual content in various scenarios. • Demonstration of the feasibilities and the benefits of the proposed user-attended visual content analysis methods with practical user-supportive applications. • Methods for interaction with a <b>see-through</b> <b>HMD</b> using eye gaze. • A comprehensive framework for recognition of user-attended visual content in a complex scene where multiple visual information resources are present. This thesis opens a novel field of wearable computer systems where computers can understand the user attention in everyday environments and provide with what the user wants. I will show the potential of such wearable attention-aware systems for everyday environments {{for the next generation of}} pervasive computer-human interaction...|$|E
40|$|A {{combination}} of see-through head-worn or helmet-mounted displays (HMDs) and imaging sensors is frequently used {{to overcome the}} limitations of the human visual system in degraded visual environments (DVE). A visual-conformal symbology displayed on the HMD allows the pilots to see objects such as the landing site or obstacles being invisible otherwise. These HMDs are worn by pilots sitting in a conventional cockpit, which provides a direct view of the external scene through the cockpit windows and a user interface with head-down displays and buttons. In a previous publication, we presented the advantages of replacing the conventional head-down display hardware by virtual instruments. These “virtual aircraft-fixed cockpit instruments” were displayed on the Elbit JEDEYE system, a binocular, <b>see-through</b> <b>HMD.</b> The idea of our current work is to not only virtualize the display hardware of the flight deck, but also to replace the direct view of the out-the-window scene by a virtual view of the surroundings. This imagery is derived from various sensors and rendered on an HMD, however without see-through capability. This approach promises many advantages over conventional cockpit designs. Besides potential weight savings, this future flight deck can provide a less restricted outside view as the pilots are able to virtually see through the airframe. The paper presents a concept for the realization of such a virtual flight deck and states the expected benefits as well as the challenges to be met...|$|E
40|$|Abstract—Head-mounted {{displays}} (HMDs) allow {{users to}} observe virtual environments (VEs) from an egocentric perspective. How-ever, several experiments have provided evidence that egocentric distances {{are perceived as}} compressed in VEs relative to the real world. Recent experiments suggest that the virtual view frustum set for rendering the VE has an essential impact on the user’s estimation of distances. In this article we analyze if distance estimation can be improved by calibrating the view frustum for a given HMD and user. Unfortunately, in an immersive virtual reality (VR) environment, a full per user calibration is not trivial and manual per user adjustment often leads to mini- or magnification of the scene. Therefore, we propose a novel per user calibration approach with optical see-through displays commonly used in augmented reality (AR). This calibration takes advantage of a geometric scheme based on 2 D point – 3 D line correspondences, {{which can be used}} intuitively by inexperienced users and requires less than a minute to complete. The required user interaction is based on taking aim at a distant target marker with a close marker, which ensures non-planar measurements covering a large area of the interaction space while also reducing the number of required measurements to five. We found the tendency that a calibrated view frustum reduced the average distance underestimation of users in an immersive VR environment, but even the correctly calibrated view frustum could not entirely compensate for the distance underestimation effects. Index Terms—Optical <b>see-through,</b> <b>HMD</b> calibration, distance perception. ...|$|E
40|$|Improvement of hemispatial neglect by a see-through head-mounted display: a {{preliminary}} study Jong Hun Kim 1, Byung Hwa Lee 2, 3, Seok Min Go 4, Sang Won Seo 2, 3, 5, Kenneth M. Heilman 6 and Duk L. Na 2, 3, 7 * Background: Patients with right hemisphere damage are often unaware of, inattentive to {{and fail to}} interact with stimuli on their left side. This disorder, called hemispatial neglect, is {{a major source of}} disability. Inducing leftward ocular pursuit by optokinetic stimulation (OKS) relieves some of the signs of unilateral neglect. However, it is difficult to provide patients with a continuously moving background that is required for OKS. We studied whether OKS projected onto a see-through head-mounted display (HMD) would help treat neglect. Methods: 14 patients with neglect after cerebral infarction performed line bisections on a computer screen, both with and without OKS that was either delivered by the HMD or on the same screen that was displaying the lines that were to be bisected. Results: The line bisection performances were significantly different in the four conditions (P < 0. 001). The post hoc analyses indicated that the rightward deviation observed in the control conditions on the line bisection tasks without OKS, improved significantly with the use OKS in both the HMD and screen conditions (α < 0. 05). The results between the screen and HMD conditions were also different (α < 0. 05). The OKS in the HMD condition corrected patients ’ rightward deviation more toward the actual midline than did the OKS provided during the screen condition. Conclusions: OKS projected onto the <b>see-through</b> <b>HMD</b> improved hemispatial neglect. The development of a portable device may aid in the treatment of neglect...|$|E
30|$|Few {{research}} {{work has been}} conducted {{in the domain of}} embedded system design for mobile augmented reality applications in the context of emerging light <b>see-through</b> <b>HMD.</b> In this project, we have conducted an in-depth algorithm/architecture study and designed a complete system according to strong footprint constraints. The solution that has been developed is dedicated but flexible. The approach has deliberately been focused on standard protocols and interfaces; it can be interconnected with usual inertial sensor and communication peripherals. This work results in a new approach for the design of AR-specific embedded and reconfigurable systems with three main contributions. The first one is the choice and the full specification of a gyroscope-free set of algorithms for position and attitude estimation, and this solution relies on the association and the adaptation, to the AR domain, of different previous contributions. The second one is the embedded system architecture, where it is introduced as a fast and simple object processor (OP) optimized for the domain of mobile AR. The architecture is especially optimized for data reuse and flexible since objects can be distributed on a given number of OPs. Finally, the OP implements a new pixel rendering method (IPS) implemented in hardware and that takes full advantage of OpenGL ES light model recommendations. The whole architecture has been implemented on various FPGA targets. The results demonstrate that expected performances can be reached and that a low-cost FPGA can implement multiple OPs. This solution is viable when reconfigurable architectures make sense, but the ultimate solution for mass market products would be a chip including a GPP and multiple OPs with power gating capabilities. Current undergoing work focuses on the choice of objects to be displayed in the user field of view. This study is made jointly with ergonomists and based on user feedbacks. The choice of objects to display will be implemented as a software service that will {{be in charge of the}} local memory content.|$|E
40|$|From the {{viewpoint}} of equal accessibility, the hearing impaired should beable to access the same information as others. However, {{it is hard for}} thehearing impaired to acquire acoustic information including verbal informa-tion. For example, in the case of small meetings or lectures at university,hearing impaired students may lose information if no special eorts aremade to help them. Therefore, methods for obtaining information, such asnotetaking, computer assisted notetaking (CAN), real-time captioning orinterpreting sign language are needed. However, to generate appropriateassistive information at the postsecondary stage of education, the genera-tor (stenographer or interpreter) should have a good understanding of therelevant higher education subject. The number of generators who havesuch ability for the wide range of topics of learning at university is limited. Recently, the number of hearing impaired students who go on to universityor college has been increasing, so the shortage of well quali￣ed assistiveinformation generators is becoming a serious problem. To overcome the problem, a remote assistance system has been devel-oped. In the system, assistive information is generated by a stenographeror a sign language interpreter far from the classroom and is then deliveredto the classroom through the Internet. Using the system, we can providemany types of assistive information that the students need without dis-patching generators to the classroom. This saves the time and the cost oftraveling to the classroom. By detaching the information generator fromthe classroom we can supply the generators with additional informationsuch as terminology and annotated material related to the lecture. Thisimproves the quality of assistive information. Therefore, the system im-proves the availability of well quali￣ed generators and the quality of theassistive information. Although the system can provide several kinds of assistive information, astudent needs very few kinds of information (maybe only one). A °ood ofinformation may confuse the students. This suggests the need for the per-sonalization of assistive information so that it delivers only the informationdesired by each student. The Head Mounted Display (HMD) is suitablefor this purpose. In this thesis, we described an improvement of the remote assistancesystem and provided a feasibility evaluation of the HMD as a display devicefor assistive information. First, we showed how additional information for generators worksthrough three practices of assisting information for highly specializededucational materials. Then, we described the evaluation experiments of a monocular non <b>see-through</b> <b>HMD.</b> In the experiments, a subject attended a small lecture whileequipped with various types of display device and received assistive infor-mation through the devices. Using pre-test/post-test comparison method,we showed that 1) The HMD can transmit assistive information to thesubject. 2) The transmission ability of the HMD was comparable to atraditional display device. 電気通信大学 200...|$|E
40|$|The idea of {{integrating}} the surgeon’s perceptive efficiency {{with the aid}} of new AR visualization modalities has become a dominant topic of academic and industrial research in the medical domain since the 90 ’s. From the beginning, AR tech-nology appeared to represent a significant development in the context of image-guided surgery, because it aimed to contextually integrate surgical navigation with virtual planning. Particularly in the realm of surgical navigation, the quality of the AR experience affects the degree of acceptance among physicians and it depends on how well the virtual content is integrated into the real world spatially, photometri-cally and temporally. In this regard, wearable systems based on head-mounted displays (HMDs), offer the most ergo-nomic and easily translatable solution for all those surgical procedures manually performed by the surgeon under di-rect vision. This {{is due to the fact}} that they intrinsically provide the surgeon with an egocentric viewpoint of the aug-mented scene and they do not limit his/her freedom of movement around the patient. However, there have been and still are few technological and human-factor reasons why such systems encounter diffi-culty in being routinely integrated into the surgical workflow. From a general perspective, concerns remain on the tradeoff between technological and human-factor burdens on one side, and proven benefits deriving from the adop-tion of this new technology on the other side. In this thesis, I motivate why a stereoscopic video see-through display is the most effective wearable solution from a technological and ergonomic standpoint, and I introduce a registration method that relies on a video-based tracking modality designed for applications in a clinical scenario, wherever rigid anatomies are involved (e. g. orthopaedic sur-gery, maxillofacial surgery, ENT surgery, and neurosurgery). The proposed video tracking solution does not require the introduction of obtrusive external trackers into the operat-ing room. It solely relies on the stereo localization of three monochromatic markers rigidly constrained to the surgical scene. Small spherical markers can be conveniently anchored to the patient’s body and/or around the surgical area without compromising the surgeon’s field of view. Further, the use of three indistinguishable markers enables to achieve high robustness of the video-based tracking approach, also in presence of non-controllable lighting conditions. The algorithm provides sub-pixel registration accuracy thanks to a two-stage method for camera pose estimation. This accurate and robust registration solution is suitable to being employed in ergonomic wearable AR systems that solely comprise off-the-shelf components: a personal computer, and a video <b>see-through</b> <b>HMD</b> (itself composed of a com-mercial HMD and a pair of external USB cameras). From a human-factor standpoint, video see-through HMDs raise issues related to the user’s interaction with the aug-mented content and to some perceptual conflicts. With stereoscopic video see-through HMDs, the user can perceive relative depths between real and/or virtual objects by providing consistent binocular disparity information in the re-corded images delivered to the left and right eyes by the two displays of the visor. Unfortunately, it is almost impossi-bile to perfectly mimic the perceptive efficiency of the human visual system when designing optimal stereoscopic dis-plays. This is why, such systems often bring to perceptual/visual artifacts when used to interact with the 3 D world. Among these artifacts, the most significant one is related to diplopia or perceptual discomfort for the user, that may arise if the fixation point, determined by the intersection of the optical axes of the stereo camera pair, leads to re-duced stereo overlaps since a large portion of the scene is not represented on both images. This perceptual problem limits the working distance of traditional stereoscopic video see-through HMDs with fixed configuration of the stereo setting. In this thesis, I propose a solution to cope with this problem through a matched hardware/software solution. To restore stereo overlap, and reduce image disparities within the binocular fusional area, the degree of convergence of the ste-reo camera pair is adjusted so to be adapted at different and predefined working distances. For each set of predefined focus/vergence configurations, the intrinsic and extrinsic camera parameters associated with it can be determined offline as a result of a one-time calibration routine, with the calibration data stored for subsequent reuse. The accuracy and the robustness of the two-stages video-based pose estimation algorithm above introduced, allows sub-pixel regis-tration accuracy without requiring additional work to the user (i. e. further calibrations). Additionally, the artificial reproduction of the perceptive efficiency of the human visual system in AR-based surgical navigators, heavily affects the surgeon’s interaction with these new visualization modalities. On this issue, unreliable modalities of AR Visualization can bring cognitive overload and perceptual conflicts causing misinterpretation and hin-dering clinical decision-making. To address this issue, in this study I introduce a new visualization processing modality, named h-PnP, that allows the definition of a task-oriented and ergonomic interaction paradigm aimed to improve image-guidance in maxillofacial surgery and/or in orthopaedic surgery. The interaction paradigm is well suited to guiding the accurate placement of rigid bodies in space. Three clinical in vitro studies are presented, one in maxillofacial surgery, one in orthopaedic surgery, and one in neu-rosurgery, where the realiability of the HMD for surgical navigation was tested in conjunction with task-oriented and/or ergonomic AR visualization modalities. The positive results obtained suggest that wearable video see-through AR displays can be considered as accurate, ro-bust, intuitive, and versatile devices. Their efficacy can significantly improve the postoperative surgical outcome The next appropriate steps for translating research results into a product, will be to proceeding to testing on humans to assess, under real clinical conditions, surgical accuracy and benefits for the patient. This clinical validation will be fun-damental for the engineering of the current prototype, hence for translating the results of my research into a reliable surgical tool...|$|E

