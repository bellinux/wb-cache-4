184|226|Public
5|$|Power remains {{relatively}} inexpensive along the Columbia, {{and in recent}} years high-tech companies like Google have begun to move <b>server</b> <b>farm</b> operations into the area to avail themselves of cheap power. Downriver of Grand Coulee, each dam's reservoir is closely regulated by the Bonneville Power Administration (BPA), the U.S. Army Corps of Engineers, and various Washington public utility districts to ensure flow, flood control, and power generation objectives are met. Increasingly, hydro-power operations are required to meet standards under the US Endangered Species Act and other agreements to manage operations to minimize impacts on salmon and other fish, and some conservation and fishing groups support removing four dams on the lower Snake River, the largest tributary of the Columbia.|$|E
25|$|Happy Feet {{needed an}} {{enormous}} group of computers, and Animal Logic worked with IBM {{to build a}} <b>server</b> <b>farm</b> with sufficient processing potential. The film took four years to make. Ben Gunsberger, Lighting Supervisor and VFX Department Supervisor, says this was partly because they needed to build new infrastructure and tools. The <b>server</b> <b>farm</b> used IBM BladeCenter framework and BladeCenter HS20 blade servers, which are extremely dense separate computer units each with two Intel Xeon processors. Rendering took up 17 million CPU hours over a nine-month period.|$|E
500|$|The lead {{visual effects}} company was Weta Digital in Wellington, New Zealand, {{at one point}} {{employing}} 900 people {{to work on the}} film. Because of the huge amount of data which needed to be stored, cataloged and available for everybody involved, even {{on the other side of}} the world, a new cloud computing and Digital Asset Management (DAM) system named Gaia was created by Microsoft especially for Avatar, which allowed the crews to keep track of and coordinate all stages in the digital processing. To render Avatar, Weta used a [...] <b>server</b> <b>farm</b> making use of 4,000 Hewlett-Packard servers with 35,000 processor cores with 104 terabytes of RAM and three petabytes of network area storage running Ubuntu Linux, Grid Engine cluster manager, and 2 of the animation software and managers, Pixar's RenderMan and Pixar's Alfred queue management system. The render farm occupies the 193rd to 197th spots in the TOP500 list of the world's most powerful supercomputers. A new texturing and paint software system, called Mari, was developed by The Foundry in cooperation with Weta. Creating the Na'vi characters and the virtual world of Pandora required over a petabyte of digital storage, and each minute of the final footage for Avatar occupies 17.28 gigabytes of storage. Often, it would take each frame of the movie several hours to render. To help finish preparing the special effects sequences on time, a number of other companies were brought on board, including Industrial Light & Magic, which worked alongside Weta Digital to create the battle sequences. ILM was responsible for the visual effects for many of the film's specialized vehicles and devised a new way to make CGI explosions. Joe Letteri was the film's visual effects general supervisor.|$|E
40|$|<b>Server</b> <b>farms</b> {{are popular}} {{architectures}} for computing infrastructures such as supercomputing centers, data centers and web <b>server</b> <b>farms.</b> As <b>server</b> <b>farms</b> become larger and their workloads more complex, designing efficient policies {{for managing the}} resources in <b>server</b> <b>farms</b> via trial-and-error becomes intractable. It is hard to predict the exact effect of various parameters on performance. Stochastic modeling and analysis techniques allow us to understand the performance of such complex systems and to guide design of policies to optimize the performance. However, most existing models of <b>server</b> <b>farms</b> are motivated by telephone networks, inventory management systems, and call centers. Modeling assumptions which hold for these problem domains are not accurate for computing <b>server</b> <b>farms.</b> There are numerous gaps between traditional models of multi-server systems and how today’s <b>server</b> <b>farms</b> operate. To cite a few: (i) Unlike call durations, supercomputing jobs and file sizes have high variance in service requirements and this critically affects the optimality and performance of scheduling policies. (ii) Most existing analysis of <b>server</b> <b>farms</b> focuses on the First-Come-First-Served (FCFS) scheduling discipline...|$|R
50|$|<b>Server</b> <b>farms</b> are {{increasingly}} being used instead of or in addition to mainframe computers by large enterprises, although <b>server</b> <b>farms</b> do not yet reach the same reliability levels as mainframes. Because of {{the sheer number of}} computers in large <b>server</b> <b>farms,</b> the failure of an individual machine is a commonplace event, and the management of large <b>server</b> <b>farms</b> needs to take this into account by providing support for redundancy, automatic failover, and rapid reconfiguration of the server cluster.|$|R
50|$|<b>Server</b> <b>farms</b> are {{commonly}} used for cluster computing. Many modern supercomputers comprise giant <b>server</b> <b>farms</b> of high-speed processors connected by either Gigabit Ethernet or custom interconnects such as Infiniband or Myrinet. Web hosting is a common use of a server farm; such a system is sometimes collectively {{referred to as a}} web farm. Other uses of <b>server</b> <b>farms</b> include scientific simulations (such as computational fluid dynamics) and the rendering of 3D computer generated imagery (also see render farm).|$|R
2500|$|In early 2014, {{following}} {{denial of}} access by Google, Gerrard hired a helicopter and produced a detailed photographic survey of one the key physical sites of the internet—a Google data farm in Oklahoma. This survey was {{the starting point of}} the work Farm (Pryor Creek, Oklahoma) 2015. The work features a simulated [...] "twin" [...] of the squat building flanked by diesel generators and powerful cooling towers, as seen from a virtual camera orbiting the facility. As suggested by the title, the architecture of the <b>server</b> <b>farm</b> bears marked resemblance to Gerrard's earlier works depicting livestock facilities, Grow Finish Unit and Sow Farm, Gerrard states that he wants the work to pose the following questions: [...] "What does the internet look like? What are the material qualities of the network? How is it powered? Do we consume this facility—or does it consume us?" ...|$|E
50|$|The EEMBC EnergyBench, SPECpower, and the Transaction Processing Performance Council TPC-Energy are {{benchmarks}} {{designed to}} predict performance per watt in a <b>server</b> <b>farm.</b> The power used by each rack of equipment {{can be measured}} at the power distribution unit.Some servers include power tracking hardware so the people running the <b>server</b> <b>farm</b> can measure the power used by each server. The power used by the entire <b>server</b> <b>farm</b> may be reported in terms of power usage effectiveness or data center infrastructure efficiency.|$|E
5000|$|Google, Inc. has a <b>server</b> <b>farm,</b> or [...] "data center", {{located in}} Lenoir. There was {{controversy}} over the nature, amount, and potential benefits of economic development incentives that the City of Lenoir, Caldwell County, and the State of North Carolina gave Google in 2007 to induce the company to build the <b>server</b> <b>farm.</b> The less celebrated benefits of the investment have been construction employment and spending, a small-time <b>server</b> <b>farm</b> investment just outside downtown, Dacentec, {{as well as local}} charitable and educational endeavors by Google.|$|E
40|$|<b>Server</b> <b>farms</b> {{are popular}} {{architectures}} for computing infrastructures such as supercomputing centers, data centers and web <b>server</b> <b>farms.</b> As <b>server</b> <b>farms</b> become larger and their workloads more complex, designing efficient policies {{for managing the}} resources in <b>server</b> <b>farms</b> via trial-and error becomes intractable. In this thesis, we employ stochastic modeling and analysis techniques to understand the performance of such complex systems and to guide design of policies to optimize the performance. There is a rich literature on applying stochastic modeling to diverse application areas such as telecommunication networks, inventory management, production systems, and call centers, but there are numerous disconnects between the workloads and architectures of these traditional applications of stochastic modeling and how compute <b>server</b> <b>farms</b> operate, necessitating new analytical tools. To cite a few: (i) Unlike call durations, supercomputing jobs and file sizes have high variance in service requirements and this critically affects the optimality and performance of scheduling policies. (ii) Most existing analysis of <b>server</b> <b>farms</b> focuses on the First-Come- First-Served (FCFS) scheduling discipline, while time sharing servers (e. g., web and database servers) are better modeled by the Processor- Sharing (PS) scheduling discipline. (in) Time sharing systems typically exhibit thrashing (resource contention) which limits the achievable concurrency level, but traditional models of time sharing systems ignore this fundamental phenomenon. (iv) Recently, minimizing energy consumption has become an important metric in managing <b>server</b> <b>farms.</b> State-of-the-art <b>servers</b> come with multiple knobs to control energy consumption, but traditional queueing models don’t take the metric of energy consumption into account. In this thesis we attempt to bridge some of these disconnects by bringing the stochastic modeling and analysis literature closer {{to the realities of}} today’s compute <b>server</b> <b>farms.</b> We introduce new queueing models for computing <b>server</b> <b>farms,</b> develop new stochastic analysis techniques to evaluate and understand these queueing models, and use the analysis to propose resource management algorithms to optimize their performance...|$|R
40|$|BMC Software <b>Server</b> <b>Farms</b> {{have gained}} {{popularity}} for providing scalable and reliable computing / Web services. A load balancer {{plays a key}} role in this architecture, serving as a “traffic cop ” to direct the requests to suitable servers. Selecting and using the proper load balancer to match the characteristics of the servers will have a significant performance impact. This paper examines some commonly used loadbalancing algorithms for <b>server</b> <b>farms,</b> introduces a performance model as a basis for the analysis, and will show how to select a load balancer to maximize the performance potential of the <b>server</b> <b>farms.</b> 1...|$|R
50|$|Large <b>server</b> <b>farms</b> {{typically}} also place {{load balancers}} between {{the front end}} servers and the network.|$|R
5000|$|More than 100 desktop {{terminals}} in {{a network}} with a <b>server</b> <b>farm.</b>|$|E
50|$|The Microsoft adCenter Keyword Services Platform <b>server</b> <b>farm</b> {{provides}} a scalable platform for keyword technologies. Each server {{in the farm}} can have different configuration to suit a variety of service providers and stored procedures. A dynamic service load balance server, a cloud server, is {{the hub of the}} KSP <b>server</b> <b>farm.</b> When a KSP server is added to the <b>server</b> <b>farm</b> via the cloud server, all available keyword service providers and stored procedures are dynamically discovered and registered with the server. Any changes in the availability of the KSP server, as well as all its running service providers and stored procedures, are discovered and registered automatically with the server.|$|E
5000|$|Monitoring the {{performance}} and throughput or load on a server, <b>server</b> <b>farm,</b> or property ...|$|E
5000|$|The need {{to control}} large {{multiprocessor}} and cluster installations, for example in <b>server</b> <b>farms</b> and render farms ...|$|R
25|$|The {{software}} that runs Wikipedia, {{and the computer}} hardware, <b>server</b> <b>farms</b> and other systems upon which Wikipedia relies.|$|R
50|$|Blade servers do not, however, {{provide the}} answer to every {{computing}} problem. One can view them {{as a form of}} productized server-farm that borrows from mainframe packaging, cooling, and power-supply technology. Very large computing tasks may still require <b>server</b> <b>farms</b> of blade <b>servers,</b> and because of blade servers' high power density, can suffer even more acutely from the heating, ventilation, and air conditioning problems that affect large conventional <b>server</b> <b>farms.</b>|$|R
5000|$|... #Caption: This <b>server</b> <b>farm</b> {{supports}} the various computer networks of the Joint Task Force Guantanamo ...|$|E
50|$|The main <b>server</b> <b>farm</b> for Wikipedia {{and other}} Wikimedia Foundation {{projects}} {{is located in}} Tampa.|$|E
50|$|Happy Feet {{needed an}} {{enormous}} group of computers, and Animal Logic worked with IBM {{to build a}} <b>server</b> <b>farm</b> with sufficient processing potential. The film took four years to make. Ben Gunsberger, Lighting Supervisor and VFX Department Supervisor, says this was partly because they needed to build new infrastructure and tools. The <b>server</b> <b>farm</b> used IBM BladeCenter framework and BladeCenter HS20 blade servers, which are extremely dense separate computer units each with two Intel Xeon processors. Rendering took up 17 million CPU hours over a nine-month period.|$|E
5000|$|Some {{kinds of}} random-access memory, such as [...] "EcoRAM", are {{specifically}} designed for <b>server</b> <b>farms,</b> where low power consumption {{is more important than}} speed.|$|R
50|$|Some content {{networks}} {{favor the}} use of cold-potato routing (multi exit discriminator exchange/honoring) in order to deliver content from replicated <b>server</b> <b>farms</b> closer to the end-user.|$|R
40|$|This paper {{describes}} a biologically-inspired architecture, called SymbioticSphere, which allows large-scale <b>server</b> <b>farms</b> to autonomously adapt to dynamic environmental changes and survive partial system failures. Symbiotic-Sphere follows biological principles such as decentralization, autonomy, natural selection, emergence and symbiosis to design <b>server</b> <b>farms</b> (application services and middleware platforms). Each application service and middleware platform is {{designed as a}} biological entity, analogous to an individual bee in a bee colony. Simulation results show that, like in biological systems, SymbioticSphere exhibits emergence of desirable system characteristics such as adaptability and survivability. 1...|$|R
50|$|RAF Ash {{was closed}} and the site sold in July 1998. It is now used as a secure <b>server</b> <b>farm</b> by The Bunker, an Internet hosting company.|$|E
50|$|GTunnel is a Windows {{application}} {{developed by}} Garden Networks which {{sets up a}} local HTTP or SOCKS proxy server which tunnels traffic through their <b>server</b> <b>farm</b> before it reaches its intended destination.|$|E
5000|$|In May 2008, a <b>server</b> <b>farm</b> inside China used {{automated}} queries to Google's {{search engine}} to identify SQL server websites which were {{vulnerable to the}} attack of an automated SQL injection tool.|$|E
40|$|Abstract—The {{expense of}} power cost in <b>server</b> <b>farms</b> has driven the recent {{power-aware}} development in both industry and academia. At the same time, a Service Level Agreement (SLA) of service performance between a customer and a service provider is demanded {{to meet the}} customer satisfaction. This paper investigates the queueing-theoretical power-saving design strategy for <b>server</b> <b>farms</b> under a given SLA, which in particular is measured in a certain level percentile of the job response time. We consider <b>server</b> <b>farms</b> with <b>servers</b> that are equipped with the capabilities of Dynamic Voltage Scaling (DVFS) and Dynamic Power Management (DPM). We adopt an M/G/ 1 /PS server model, where the job service time distribution is assumed heavy-tailed, as discovered and validated by previous research. We propose a design strategy called PowerTail to minimize the power consumption under the given SLA. Our data confirms that the proposed PowerTail strategy indeed provides statistical guarantee in comparison with existing dynamic DVFS approaches and significantly outperforms the intuitive load-balancing strategy. I...|$|R
50|$|<b>Server</b> <b>farms</b> {{use more}} {{electricity}} {{as they need}} more disk drives. The extra load is especially notable in corporate domains. This adds to an individual’s or company’s electricity expenses and carbon footprint.|$|R
50|$|The {{commercial}} HTTP tunneling client {{applications are}} provided by companies that run their own mediator <b>server</b> <b>farms.</b> They charge for the service provided, with various tiers of service that depend on the bandwidth provided.|$|R
5000|$|A compile farm is a <b>server</b> <b>farm,</b> a {{collection}} {{of one or more}} servers, which has been set up to compile computer programs remotely for various reasons. Uses of a compile farm include: ...|$|E
5000|$|Microsoft SharePoint's Server Features are {{configured}} either using PowerShell, or a Web UI called [...] "Central Administration". Configuration of <b>server</b> <b>farm</b> settings (e.g. search crawl, {{web application}} services) {{can be handled}} through these central tools.|$|E
50|$|Geoplexing is a {{computer}} science term relating to the duplication of computer storage and applications within a <b>server</b> <b>farm</b> over geographically diverse locations {{for the purpose of}} fault tolerance. The name comes from a contraction of geographical multiplex.|$|E
50|$|As PMBus {{systems are}} deployed, tools to manage those systems should become significant. Some {{of them may}} just be used during manufacturing, to set up system-specific {{parameters}} used with reconfigurable power subsystems. Others will be useful for runtime optimization, for example with <b>server</b> <b>farms.</b>|$|R
50|$|The {{infrastructure}} requirements were {{drastically reduced}} during the mid-1990s, when CMOS mainframe designs replaced the older bipolar technology. IBM claimed that its newer mainframes could reduce data center energy costs {{for power and}} cooling, {{and that they could}} reduce physical space requirements compared to <b>server</b> <b>farms.</b>|$|R
40|$|A novel locking {{protocol}} maintains data {{consistency in}} distributed and clustered file {{systems that are}} used as scalable infrastructure for Web <b>server</b> <b>farms.</b> High-performance Web sites rely on Web server “farms”—hundreds of computers serving the same content—for scalability, reliability, and low-latency access to Internet content. Deploying these scalable farms typically requires the power of distributed or clustered file systems. Building Web <b>server</b> <b>farms</b> on file systems complements hierarchical proxy caching. 1 Proxy caching replicates Web content throughout the Internet, thereby reducing latency from network delays and off-loading traffic from the primary <b>servers.</b> Web <b>server</b> <b>farms</b> scale resources at a single site, reducing latency from queuing delays. Both technologies are essential when building a high-performance infrastructure for content delivery. In this article, we present a cache consistency model and locking protocol customized for file systems that are used as scalable infrastructure for Web <b>server</b> <b>farms.</b> The protocol {{takes advantage of the}} Web’s relaxed consistency semantics to reduce latencies and network overhead. Our hybrid approach preserves strong consistency for concurrent write sharing with time-based consistency and push caching for readers (Web servers). Using simulation, we compare our approach to the Andrew file system and the sequential consistency file system protocols we propose to replace. Data Consistency File system design carries assumptions about workloads and application consistency needs that, when applied to Web serving, lead to inferior performance. File system designers assume that data are shared infrequently and that such data require strong (sequential) consistency. 2 Consistency describes how processors in a parallel or distributed system view shared data; the sidebar on related work in data consistency presents examples. For Web serving, data are widely shared among many servers, as Figure 1 shows, and strong consistency guarantee...|$|R
