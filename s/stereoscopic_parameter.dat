0|11|Public
40|$|The {{production}} {{of high quality}} stereoscopic content demands skilled production staff which is {{able to cope with}} a set of stereoscopic production rules. Among these rules, the mechanical alignment of the two cameras has to be ensured as well as a proper choice of the interaxial distance and convergence plane. Against this background the STAN assists stereographers by analyzing and monitoring important <b>stereoscopic</b> <b>parameters.</b> An image based analysis reconstructs the scene geometry based on the left and right camera images, deduces calibration parameters and proposes a suitable interaxial distance and convergence plane in accordance to the depth structure of the scene. The analyzed parameters can either be used for controlling the stereo rig, or can be stored as meta data for post-production purposes or can be used for real-time corrections of remaining stereoscopic distortions...|$|R
40|$|Stereoscopic {{images are}} hard to get right, and {{comfortable}} images are often only produced after repeated trial and error. The main difficulty is controlling the <b>stereoscopic</b> camera <b>parameters</b> so that the viewer does not experience eye strain or double images from excessive perceived depth. Additionally, for head tracked displays, the perceived objects can distort as the viewer moves to look around the displayed scene. We describe a novel method for calculating <b>stereoscopic</b> camera <b>parameters</b> with the following contributions: (1) Provides the user intuitive controls related to easily measured physical values. (2) For head tracked displays; necessarily ensures {{that there is no}} depth distortion as the viewer moves. (3) Clearly separates the image capture camera/scene space from the image viewing viewer/display space. (4) Provides a transformation between these two spaces allowing precise control of the mapping of scene depth to perceived display depth. The ne...|$|R
40|$|The {{background}} and {{motivation for the}} research performed within this thesis is {{the introduction of the}} Digital Cinema which allows for new workflows based on image processing algorithms. Thereby, the development of algorithms for stereoscopic 3 D and multi-camera productions within the era of the Digital Cinema is of special interest. Several 3 D productions have been released in the cinemas in the past years while the basic principle of 3 D reproduction is still based on Wheatstone’s [Wheatstone 38] and Brewster’s stereoscopic approach [Brewster 56] where two views corresponding to two different viewing positions are presented to the viewer’s left and right eye. However, if the reproduced 3 D content imposes unnatural viewing conditions when watched, e. g. due to an excessive amount of inherent parallax, an impaired 3 D sensation can result which can even lead to visual fatigue and head-ache [IJsselsteijn 00]. Consequently, specific 3 D production rules as described in [Mendiburu 08] and [Knorr 12] have to be obeyed when high quality 3 D content shall be produced. It includes a precise calibration of the two cameras with consistent electronic and optical parameters. Moreover, the stereo baseline and convergence distance have to be chosen according to the depth structure of the scene content. When performed without specific assistance systems, the calibration process and the choice of proper <b>stereoscopic</b> <b>parameters</b> as described by Lipton in [Lipton 82] can be tedious tasks which require trained personnel and increase the overall production costs [Buchs 11]. With the advent of digital cameras, it became possible to analyze and possibly correct the 3 D signal electronically using dedicated stereoscopic image processors [Zilly 10 b, Sony] which facilitates the above mentioned tasks and allows for new 3 D production workflows, possibly lowering the costs and improving the resulting quality. Against this background, within this thesis, a new and robust technique for camera pose estimation and rectification of uncalibrated stereo cameras based on a new method to estimate the fundamental matrix is proposed. The approach is subsequently enhanced towards trifocal setups involving a new estimation method for the trifocal tensor. To rectify the images acquired by uncalibrated cameras, a suitable feature detector is required. In this context, a new feature descriptor (SKB) is proposed and compared to existing descriptors such as SIFT, SURF or BRIEF. The different algorithms are combined, extended by new functions to calculate important <b>stereoscopic</b> <b>parameters,</b> and made accessible through an intuitive graphical user-interface which allows non-expert camera personnel to make use of it using an application which is called stereoscopic analyzer (STAN). Finally a new multi-camera disparity estimation workflow is proposed and applied to a multi-camera setup suitable for the generation of display agnostic 3 D content...|$|R
40|$|Stereoscopic {{visualization}} creates {{illusions of}} depth through {{disparity between the}} images shown to left and right eyes of the viewer. While the stereoscopic visualization is widely adopted in immersive visualization systems to improve user experience, it can also cause visual discomfort if the <b>stereoscopic</b> viewing <b>parameters</b> are not adjusted appropriately. These parameters are usually manually adjusted based on human factors and empirical knowledge of the developer or even the user. However, scenes with dynamic change in scale and configuration can lead into continuous adjustment of these parameters while viewing. In this paper, we propose a method to adjust the interpupillary distance adaptively and automatically according to {{the configuration of the}} 3 D scene, so that the visualized scene can maintain sufficient stereo effect while reducing visual discomfort...|$|R
40|$|Current {{methods of}} {{determining}} <b>stereoscopic</b> <b>parameters</b> for stereo 3 D films away important {{information about the}} relationship between the geometry or scene elements, the camera, and the viewer, reducing the problem to a 2 -dimensional space of camera separation and convergence distance. This ion handicaps the film's artists by narrowing the scope of the stereo aspects of movie making to being a post process, equivalent to applying a 3 D lens to a movie already designed to be flat. In an attempt to counter the effect of this abstraction and to better incorporate a stereo mindset into other components of filmmaking, we have developed an interactive visualization and intuitive pipeline toolset for animation artists that allows them to simultaneously view and manipulate the impact of the higher-dimensional parameters normally abstracted away in common stereo techniques. By visualizing the complete, comprehensive stereo pathway and encouraging meaningful interaction with it, artists are better enabled to create fully informed creative and artistic stereo decisions earlier in the preliminary design stages, foresee issues related to inherent or unintentional distortions that may arise further down the production pipeline forcing costly re-renders and re-animations. By providing a quicker comprehension of the stereo space and {{a better understanding of the}} terminology used, we hope to show how the study of depth perception can guide the creation of better stereo 3 D content. The toolset also provides for the definition of a metric of stereo distortion as it relates to the perception of volume and shape, which allows artists to make informed judgments on what constitutes perceptually good stereo, either through automated optimization or constrained manipulation based on the limits of human perception. This tool and its applications were tested in the production animation pipeline at DreamWorks Animation, and provided sufficiently accurate and helpful feedback and aesthetically pleasing suggestions to artists involved in the production of stereoscopic 3 D animation...|$|R
40|$|An {{algorithm}} for estimating {{reliable and}} accurate depth maps from stereoscopic image pairs is presented, {{which is based}} on block [...] matching techniques for disparity estimation. By taking neighboring disparity values into account, reliability and accuracy of the estimated disparity values are increased and the corona effect at disparity discontinuities is avoided. An interpolation of disparity values within segmented regions of homogeneous disparity enables the computation of dense depth maps by means of triangulation. 1 Introduction Depth estimation is used in applications like 3 D [...] modelling of natural objects [1] [2], 3 D [...] remote handling and quality control [3]. Depth information is obtained by a triangulation of corresponding image points with known <b>stereoscopic</b> camera <b>parameters.</b> Therefore, the coordinate difference between corresponding image points, called disparity, has to be estimated. Applying common block [...] matching techniques for disparity estimation, the correspondence of i [...] ...|$|R
40|$|International audienceNavigation in multi-scale virtual environments (MSVE) {{requires}} the adjustment of the navigation parameters to ensure optimal navigation experiences {{at each level}} of scale. In particular, in immersive stereoscopic systems, e. g. when performing zoom-in and zoom-out operations, the navigation speed and the <b>stereoscopic</b> rendering <b>parameters</b> have to be adjusted accordingly. Although this adjustment can be done manually by the user, it can be complex, tedious and strongly depends on the virtual environment. In this work we propose a new multi-scale navigation technique named GiAnt (GIant/ANT) which automatically and seamlessly adjusts the navigation speed and the scale factor of the virtual environment based on the user's perceived navigation speed. The adjustment ensures an almost-constant perceived navigation speed while avoiding diplopia effects or diminished depth perception due to improper stereoscopic rendering configurations. The results from the conducted user evaluation shows that GiAnt is an efficient multi-scale navigation which minimizes the changes of the scale factor of the virtual environment compared to state-of-the-art multi-scale navigation techniques...|$|R
40|$|Previously we {{reported}} a study into {{the effect of}} <b>stereoscopic</b> filming <b>parameters</b> on perceived quality, naturalness and eye strain. In a pilot experiment, using 25 seconds exposure duration, a marked shift occurred between naturalness and quality ratings {{as a function of}} camera separation. This shift was less clearly present in the main experiment, in which we used an exposure duration of 5 seconds. This suggests a potential effect of exposure duration on observer appreciation of stereoscopic images. To further investigate this, we performed an experiment using exposure durations of both 5 and 10 seconds. For these durations, twelve observers rated naturalness of depth and quality of depth for stereoscopic still images varying in camera separation, convergence distance and focal length. The results showed no significant main effect of exposure duration. A small yet significant shift between naturalness and quality was found for both duration conditions. This result replicated earlier findings, indicating that this is a reliable effect, albeit content-dependent. A second experiment was performed with exposure durations ranging from I to 15 seconds. The results of this experiment showed a small yet significant effect of exposure duration. Whereas longer exposure durations do not {{have a negative impact on}} the appreciative scores of optimally reproduced stereoscopic images, observers do give lower judgements to monoscopic images and stereoscopic images with unnatural disparity values as exposure duration increases...|$|R
40|$|This work {{investigates the}} role of various {{parameters}} on the visibility or strength of motion artifacts in <b>stereoscopic</b> video. The <b>parameters</b> examined in an experimental study were disparity, presentation protocol (double flash or triple flash), seating position of a viewer in the theatre and speed of a moving object in the scene. The {{results of the study}} suggested that disparity and presentation protocol had no significant effect on the visibility or strength of the motion artifacts. Speed of a moving object had a significant effect on both the visibility and strength of the motion artifacts. Although position does not itself have a significant effect, there was an interaction between speed and seating position that had a significant effect. The speed threshold for visibility of motion artifacts was significantly higher for viewers sitting closest to the screen, which is surprising. It is concluded that the effects of viewer seating position on the perception of movement artifacts requires further investigation...|$|R
40|$|We {{present the}} results of a {{comprehensive}} video game study which explores how the gaming experience is effected when several 3 D user interface technologies are used simultane-ously. We custom designed an air-combat game integrating several 3 DUI technologies (stereoscopic 3 D, head tracking, and finger-count gestures) and studied the combined effect of these technologies on the gaming experience. Our game design was based on existing design principles for optimiz-ing the usage of these technologies in isolation. Additionally, to enhance depth perception and minimize visual discomfort, the game dynamically optimizes <b>stereoscopic</b> 3 D <b>parameters</b> (convergence and separation) based on the user’s look direc-tion. We conducted a within subjects experiment where we examined performance data and self-reported data on users perception of the game. Our results indicate that participants performed significantly better when all the 3 DUI technolo-gies (stereoscopic 3 D, head-tracking and finger-count ges-tures) were available simultaneously with head tracking as a dominant factor. We explore the individual contribution of each of these technologies to the overall gaming experience and discuss the reasons behind our findings...|$|R
40|$|Cataloged from PDF {{version of}} article. Thesis (M. S.) : Bilkent University, Department of Computer Engineering, İhsan Doğramacı Bilkent University, 2015. Includes bibliographical {{references}} (leaves 58 - 61). In recent years, significant {{progress has been}} made on controlling the perceived depth range in post-production pipeline. On the other hand, different from of- ine production, in a virtual environment with a mobile camera, there remains a need to keep the perceived depth in the comfortable target range for the viewer. For instance, in a game environment where the stereoscopic output changes dynamically based on the user input, finding optimized <b>stereoscopic</b> camera <b>parameters</b> brings about a great challenge. Addressing such challenges of presenting a comfortable viewing setting, this work demonstrates several methods that are developed towards the goal of providing better stereo 3 D experience in virtual environments. The first part presents an approach for controlling the two stereo camera parameters, camera convergence distance and interaxial separation, in interactive 3 D environments in a way that specifically addresses the interplay of binocular depth perception and saliency of scene contents. The proposed Dynamic Attention- Aware Disparity Control (DADC) method produces depth-rich stereo rendering that improves viewer comfort through joint optimization of stereo parameters. While constructing the optimization model, the importance of scene elements is considered, as well as their distance to the camera and the locus of attention on the display. The method also optimizes the depth effect of a given scene by considering the individual user's stereoscopic disparity range and comfortable viewing experience by controlling accommodation/convergence con ict. The method is validated in a formal user study that also reveals the advantages, such as superior quality and practical relevance, of considering the presented method. In the second part, a novel method is introduced for automatically adjusting the stereo camera parameters, now also including focal length of the virtual camera lens in addition to the previous two, in a given 3 D virtual scene for the scenario where there are scene elements that already have their camera parameters set for a certain perimeter and viewing angle range by the content developer and/or editor. The method, in a nutshell, computes the stereo camera parameters online by continuously scanning the scene as the virtual camera moves about it for changes in the number and the relative distribution of scene elements and the preset parameters, as well. Taking these variables into account, the method produces the camera parameters {{for the rest of the}} entire scene mainly by the employment of a radial basis function interpolation-based approach. As it works online, the framework allows for adjustment of camera parameters per scene element ondemand with an intuitively-designed interface so that the user can fine-tune the overall depth feeling of the scene. by Ufuk Çelikcan. M. S...|$|R

