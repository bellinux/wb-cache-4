57|831|Public
5000|$|Depending on {{how they}} are generated, depth maps may {{represent}} the perpendicular distance between an object and {{the plane of the}} <b>scene</b> <b>camera.</b> For example, a <b>scene</b> <b>camera</b> pointing directly at - and perpendicular to - a flat surface may record a uniform distance for the whole surface. In this case, geometrically, the actual distances from the camera to the areas of the plane surface seen in the corners of the image are greater than the distances to the central area. For many applications, however, this discrepancy is not a significant issue.|$|E
50|$|The code below makes a WHS.App {{instance}} which handles {{all your}} modules and components for better work with WebGL. This one creates a <b>scene,</b> <b>camera</b> and renderer - we add the following modules to the App.|$|E
50|$|Otherwise, HDR {{rendering}} {{systems have}} {{to map the}} full dynamic range to what the eye would see in the rendered situation onto {{the capabilities of the}} device. This tone mapping is done relative to what the virtual <b>scene</b> <b>camera</b> sees, combined with several full screen effects, e.g. to simulate dust in the air which is lit by direct sunlight in a dark cavern, or the scattering in the eye.|$|E
5000|$|SFM {{gives the}} user a [...] "Work Camera" [...] that {{enables them to}} see what they are doing without messing up the <b>scene</b> <b>cameras.</b> It also uses three main user {{interfaces}} for making films with: ...|$|R
30|$|In this step, {{we start}} by extracting {{a set of}} points of {{interest}} from each input frame. We consider the Harris corner to be a point of interest [38]. We also consider that, in video surveillance <b>scenes,</b> <b>camera</b> positions and lighting conditions allow {{a large number of}} corner features to be captured and tracked easily.|$|R
30|$|The Smart Eye Pro 6.0 three-camera eye {{tracking}} system {{was used to}} capture the participant’s eye movements, which was mounted on the front windscreen to accurately capture the driver’s eye movements (fixations and saccades) at a frequency of 60  Hz. Two <b>scenes</b> <b>cameras</b> were fixed right above the wheel to record the steering wheel angles and vehicle lateral deviation.|$|R
50|$|Temporarily {{called the}} film, {{this film is}} set back in Dalat, based on the {{contents}} of the above anecdote to variations. At first look it seems Mr Dam saw horned calves {{do not want to see}} it, but that <b>scene,</b> <b>camera</b> angles and actors considered beautiful new linger until the last minute. Perhaps stories like this coincidence in life is not rare, but not quite as much. Starring in which all the familiar faces.|$|E
5000|$|... var camera, scene, renderer, geometry, material, mesh; init (...) animate (...) {{function}} init (...) { scene = new THREE.Scene (...) camera = new THREE.PerspectiveCamera( [...] 75, window.innerWidth / window.innerHeight, 1, 10000 [...] ); camera.position.z = 1000; geometry = new THREE.BoxGeometry( [...] 200, 200, 200 [...] ); material = new THREE.MeshBasicMaterial( [...] { color: 0xff0000, wireframe: true } [...] ); mesh = new THREE.Mesh( [...] geometry, material [...] ); scene.add( [...] mesh [...] ); renderer = new THREE.WebGLRenderer (...) renderer.setSize( [...] window.innerWidth, window.innerHeight [...] ); document.body.appendChild( [...] renderer.domElement [...] ); } function animate (...) { requestAnimationFrame( [...] animate [...] ); render (...) } function render (...) { mesh.rotation.x += 0.01; mesh.rotation.y += 0.02; renderer.render( [...] <b>scene,</b> <b>camera</b> [...] ); } ...|$|E
50|$|When {{a camera}} is used, {{light from the}} {{environment}} is focused on an image plane and captured. This process reduces {{the dimensions of the}} data taken in by the camera from three to two (light from a 3D scene is stored on a 2D image). Each pixel on the image plane therefore corresponds to a shaft of light from the original <b>scene.</b> <b>Camera</b> resectioning determines which incoming light is associated with each pixel on the resulting image. In an ideal pinhole camera, a simple projection matrix is enough to do this. With more complex camera systems, errors resulting from misaligned lenses and deformations in their structures can result in more complex distortions in the final image.The camera projection matrix is derived from the intrinsic and extrinsic parameters of the camera, and is often represented by the series of transformations; e.g., a matrix of camera intrinsic parameters, a 3 &times; 3 rotation matrix, and a translation vector. The camera projection matrix can be used to associate points in a camera's image space with locations in 3D world space.|$|E
40|$|To fully {{understand}} how it works, build it yourself Babcock and Pelz (2004) {{were one of the}} first to describe a DIY wearable analog video tracker an expensive video multiplexer was needed to synchronize eye and <b>scene</b> <b>cameras</b> system relied on somewhat dated (by today’s standards) video recorder (a Sony DCR-TRV 19 DVR) nevertheless, fostered nascent open-source movemen...|$|R
5000|$|American: <b>scene</b> number, <b>camera</b> {{angle and}} take number; e.g. scene 24, C, take 3; ...|$|R
5000|$|In {{the opening}} <b>scene,</b> the <b>camera</b> pans in on The Jersey Journal sign in Jersey City.|$|R
5000|$|Following the episode's broadcast, {{the tabloid}} Daily Mail {{reported}} that Irene Adler's nude scene {{early in the}} episode had been met with disapproval from some viewers who were concerned {{that it had been}} shown before the 21:00 watershed hour, before which adult-oriented content is not supposed to air. In the nude <b>scene,</b> <b>camera</b> angles and body posture hide sensitive body parts. The Guardian claimed that there was [...] "no mystery about the Mail's reaction", observing that the paper was [...] "so outraged by the scenes" [...] that it illustrated its story [...] "with a big blow-up picture on page 9". Following the broadcast, a BBC spokesman said: [...] "We're delighted with the critical and audience response to the first episode, which has been extremely positive, and have received no complaints at this stage." [...] As of 4 January 2012, the BBC had received 102 complaints [...] "relating to inappropriate scenes broadcast before the Watershed", although [...] "it could not tell when the complaints had been made, or how many came before and after the Daily Mail story". The regulator Ofcom also received 20 complaints about the episode, but they decided that it didn't warrant investigating.|$|E
40|$|This study {{assessed}} {{the value of}} two video configurations - a head-mounted camera with eye tracking capability and a <b>scene</b> <b>camera</b> providing {{a view of the}} work environment - on remote collaboration on physical (3 D) tasks. Pairs of participants performed five robot construction tasks in five media conditions: side-by-side, audio-only, head-mounted camera, <b>scene</b> <b>camera,</b> and scene plus head cameras. Task completion times were shortest in the side-by-side condition, and shorter with the <b>scene</b> <b>camera</b> than in the audio-only condition. Participants rated their work quality highest when side-by-side, intermediate with the <b>scene</b> <b>camera,</b> and worst in the audio-only and head-camera conditions. Similarly, helpers 2 ̆ 7 self-rated ability to assist workers and pairs 2 ̆ 7 communication efficiency were highest in the side-by-side condition, but significantly higher with the <b>scene</b> <b>camera</b> than in the audio-only condition. The results demonstrate the value of a shared view of the work environment for remote collaboration on physical tasks...|$|E
3000|$|... the ISMS {{processes}} and stores {{data from the}} high-resolution <b>scene</b> <b>camera</b> {{in order to be}} reproduced in the laboratory.|$|E
40|$|Abstract — We {{present a}} robust and {{accurate}} technique for the cross-calibration of 3 D remote gaze trackers with stereoscopic scene vision systems between which no common imaging area exists. We empirically demonstrate that a multidepth calibration approach yields remarkably superior results for obtaining 3 D Point-of-Gaze (PoG) {{when compared with}} traditional methods using monocular <b>scene</b> <b>cameras</b> and coplanar eye gaze calibration points. I...|$|R
40|$|Old man Manel Osil, {{house in}} Majosik' below the school, weaving a chojak', talking to AO. Plus a few stories. He still knows the batz'il months, was {{using them to}} {{calculate}} {{the days of the}} k'in. Lots of other people there having side conv. s but with the head mikes maybe okay. Two cameras, cam 1 with head mikes, cam 2 <b>scene.</b> <b>Cameras</b> pointing roughly NW; they are facing uphill towards the school. Good natural conversation...|$|R
50|$|In a mid-credits <b>scene,</b> the <b>camera</b> zooms in on Suzie's {{window in}} Jane's {{photo of the}} old house as Patrick screams in the background.|$|R
40|$|Gaze {{estimation}} error {{is inherent in}} head-mounted eye trackers and seriously impacts performance, usability, and user experience of gaze-based interfaces. Particularly in mobile settings, this error varies constantly as users move in front and look at different parts of a display. We envision {{a new class of}} gaze-based interfaces that are aware of the gaze {{estimation error}} and adapt to it in real time. As a first step towards this vision we introduce an error model that is able to predict the gaze estimation error. Our method covers major building blocks of mobile gaze estimation, specifically mapping of pupil positions to <b>scene</b> <b>camera</b> coordinates, marker-based display detection, and mapping of gaze from <b>scene</b> <b>camera</b> to on-screen coordinates. We develop our model through a series of principled measurements of a state-of-the-art head-mounted eye tracker...|$|E
40|$|Pervasive eye-based {{interaction}} {{refers to}} the vision of eye-based interaction becoming ubiquitously usable in everyday life, e. g. across multiple displays in the environment. While current head-mounted eye trackers work well for interaction with displays at similar distances, the <b>scene</b> <b>camera</b> often fails to cover both remote and close proximity displays, e. g. a public display on a wall and a handheld portable device. In this paper we describe an approach that allows for robust detection and gaze mapping across multiple such displays. Our approach uses an additional <b>scene</b> <b>camera</b> to extend the viewing and gaze mapping area of the eye tracker and automatically switches between both cameras depending on the display in view. Results from a pilot study show that our system achieves a similar gaze estimation accuracy to a single-camera system {{while at the same}} time increasing usability...|$|E
40|$|Progress in modeling, {{animation}} and rendering {{means that}} rich, high fidelity interactive virtual worlds are now commonplace. But as photographers and cinematographers know, {{achievement of the}} intended informational and aesthetic goals is highly dependent on the position and motion of the camera {{in relation to the}} elements of the <b>scene.</b> <b>Camera</b> control encompasses interactive approaches, semi-automatic camera positioning, and fully declarative approaches to the management of a user’s viewpoint on a <b>scene.</b> <b>Camera</b> control is required in nearly all interactive 3 D applications and presents a particular combination of technical challenges for which {{there have been a number}} of recent proposals (e. g. specific path-planning, management of occlusion, modeling of high-level communicative goals). We present classify the approaches, analyze the requirements and limits of solving techniques and explore in detail the main difficulties and challenges in automatic camera control. 1...|$|E
50|$|Recently, UVIS {{systems have}} also {{integrated}} license plate recognition (LPR) software that can identify stolen or suspect vehicles, and help security personnel monitor suspected {{changes to the}} undercarriage of a returning vehicle. UVIS providers have also developed a variety of security add-ons such as external <b>scene</b> <b>cameras</b> to help personnel better detect, deter and communicate potential threats. Many systems also feature network integration, allowing the facility to access and use data from perimeter choke points for broader applications such as resource planning.|$|R
50|$|The best of this year's series. Deleted and {{extended}} <b>scenes,</b> backstage <b>cameras,</b> behind the <b>scenes</b> footage, {{interviews with the}} six finalists, the judges and the coaches.|$|R
50|$|In {{a reverse}} {{of the opening}} <b>scene,</b> the <b>camera</b> pulls up. As the shot widens, we see tree growing rapidly, {{apparently}} right out of Asha's body.|$|R
40|$|Tanate', yixlel (AO's yZ) and AO's wife; in {{the middle}} XTujk, the husband, returned. Good {{conversation}}, about guess what, sickness. Two cameras only after X appeared: one with head mikes (cam 1), sound good, one <b>scene</b> <b>camera</b> (cam 2). The problem with cam 1 version {{is that there is}} quite a lot of conversation with X – best transcribed from cam 2 version Pointing S-ish, protagonists facing SE...|$|E
40|$|Abstract – Although iris {{recognition}} {{is one of}} the biometric techniques that showing good performance, it is not been used widely because iris image capturing needs much effort in practical situation. Thus iris image acquisition in user convenient environment is very important factor for the popularization of {{iris recognition}}. Generally, remote iris acquisition system has two cameras: <b>scene</b> <b>camera</b> for user detection and iris camera for high resolution iris image capturing. According to the geometric relation between the <b>scene</b> <b>camera</b> and iris camera, remote iris image acquisition system could be classified into three categories: separated type, parallel type and coaxial type. In particular, this paper focused on the comparison between parallel type and coaxial type in the viewpoint of y-axis displacement, y. Experimental results show that if camera to target distance is getting offset away, the y is getting smaller. It means that benefit of coaxial type is limited to the near distance and offset difference between coaxial type and parallel type becomes ignorable beyond a certain distance. I...|$|E
30|$|We used a {{binocular}} head-mounted {{eye tracker}} (SMI Eye Tracking Glasses) to collect gaze data. The tracker {{is equipped with}} a high-resolution <b>scene</b> <b>camera</b> (1280 × 960) recording at 24 Hz and two eye cameras recording at 30 Hz. The user’s head position and orientation are integrated by GazInG into a situation model in real time. This is realized by instrumenting the environment with low-cost printable fiducial markers (see the tablecloth in Fig.  1). These are located in known positions relative to the stimuli and are tracked by the <b>scene</b> <b>camera</b> of the eye tracker using computer vision. Fusing the thus-derived head position and orientation with eye tracking data from the glasses reconstructs the user’s gaze direction. This allows the system to cast a 3 D gaze ray into the situation model (see the yellow arrow in Fig.  1). The intersections of the ray with the geometric models of the stimuli identify gazed-at objects. At this point, GazInG has semantically mapped the listener’s inspections. For further technical details of the approach, see Pfeiffer and Renner (2014) and Pfeiffer et al. (2016 b). In this experiment, feedback was triggered by pooled inspections with a dwell time larger than 200 ms.|$|E
5000|$|In a post-credits <b>scene,</b> the <b>camera</b> pans {{over the}} debris-strewn {{scene on the}} border between the Koreas. EDI's [...] "brain" [...] turns back on, {{implying}} it is still functional.|$|R
50|$|Original Air Date—25 May 2007The best of this year's series. Deleted and {{extended}} <b>scenes,</b> backstage <b>cameras,</b> behind the <b>scenes</b> footage, {{interviews with the}} six finalists, the judges and the teachers.|$|R
50|$|Finally, each episode's credits {{end with}} a topless sunbathing <b>scene.</b> The <b>camera</b> is aimed high enough {{to cut off the}} image and prevent the need for {{additional}} censorship, but the bounce effect is used.|$|R
40|$|For solving complex tasks cooperatively {{in close}} {{interaction}} with robots, {{they need to}} understand natural human communication. To achieve this, robots could benefit from {{a deeper understanding of the}} processes that humans use for successful communication. Such skills can be studied by investigating human face-to-face interactions in complex tasks. In our work the focus lies on shared-space interactions in a path planning task and thus 3 D gaze directions and hand movements are of particular interest. However, the analysis of gaze and gestures is a time-consuming task: Usually, manual annotation of the eye tracker's <b>scene</b> <b>camera</b> video is necessary in a frame-by-frame manner. To tackle this issue, based on the EyeSee 3 D method, an automatic approach for annotating interactions is presented: A combination of geometric modeling and 3 D marker tracking serves to align real world stimuli with virtual proxies. This is done based on the <b>scene</b> <b>camera</b> images of the mobile eye tracker alone. In addition to the EyeSee 3 D approach, face detection is used to automatically detect fixations on the interlocutor. For the acquisition of the gestures, an optical marker tracking system is integrated and fused in the multimodal representation of the communicative situation. ...|$|E
40|$|Abstract. Advances in eye {{tracking}} technology have allowed gaze {{to become a}} viable input modality for pervasive displays. Hand-held devices are typically located below the visual field of a standard mobile eye tracker. To enable eye-based interaction with a public display and a hand-held device, we have developed a dual <b>scene</b> <b>camera</b> system with an extended field of view. Our system enables new interaction techniques that take advantage of gaze on remote and close proximity displays to select and move information for retrieval and manipulation...|$|E
40|$|Abstract — A unique color {{interpolation}} {{approach for}} digital still cameras is introduced and {{described in terms}} of color vectors. A new difference plane model is introduced and used in conjunction with the proposed here correction process. This avoids edge blurring while improving on the color appearance of previous methods. Experimental results indicate that the proposed method exhibits superior performance over state of-the-art color interpolation methods 1. Index Terms — Camera image processing, Bayer pattern, color filter array interpolation, color image restoration. (a) (b) image scene image <b>scene</b> <b>camera</b> lens camera lens optical filter optical filte...|$|E
50|$|Klickstein {{has written}} for {{multiple}} publications, including Wired, New York Daily News, Alternative Press, Boulder Weekly, Baltimore Sun, Baltimore Jewish Times, Splitsider, Yellow <b>Scene,</b> Daily <b>Camera,</b> Colorado Daily, iTech Post, Ink Magazine and OC Weekly.|$|R
5000|$|Ueda Shōji besutan shashinchō: Shiroi kaze (...) / Brilliant <b>Scenes.</b> Tokyo: Nippon <b>Camera,</b> 1981[...]|$|R
40|$|This thesis {{examines}} how Hollywood films facilitate the normalization of surveillance within society. In particular, this thesis delves {{into the realm}} of Hollywood cinema to lend understanding into messages that <b>scenes</b> with <b>camera</b> surveillance images may impress upon its audience. This thesis adds to the surveillance studies literature and contributes to criminology by examining <b>scenes</b> with <b>camera</b> surveillance images from 30 Hollywood films using both a content analysis and a critical discourse analysis. The results indicate that Hollywood facilitates the normalization of surveillance through various ways. While some aspects of cinema appear to be critical of camera surveillance ultimately cinema actually displays the necessity of camera surveillance to society...|$|R
