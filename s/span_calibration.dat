2|10|Public
40|$|A compact {{microwave}} {{plasma device}} {{that can be}} attached to a stack for near in situ atomic emission spectroscopy, with a real-time <b>span</b> <b>calibration</b> injector for accuracy, is under development for continuous metals monitoring. It uses a commercially available 2. 45 GHz, 1. 5 kW microwave source to generate a plasma in an undiluted, isokinetic sustained in a shorted waveguide, which together with a short sample line is maintained at high temperature (> 150 C) to avoid condensation. Relative to other plasma based CEMs the microwave plasma has a significant advantage to continuously operate reliably in large volumes (- 50 cm 3) of fast flowing (> 14 1 /minute, up to 40 ft/sec) air or undiluted stack exhaust. A pneumatic nebulizer attached to the sample line can momentarily, on command, inject a known trace concentration of metals to provide a real-time <b>span</b> <b>calibration</b> whenever needed. Novel grating spectrometers using low cost detectors {{have been used for}} simultaneous multiple metals monitoring with continuous rapid signal acquisition making possible the observation of discreet particles. Since the particles are observed as plasma induced sources of light, the usual limits on monitoring discreet submicron particles by conventional external light instruments are not encountered. Detection limits for Be, Cr, and Pb of < 3 ug/m 3 have been shown for one minute signal averaging in plasmas with 6 % water content. A one minute detection limit for Hg of < 1 ug. m 3 has been recently achieved in ambient laboratory air plasmas using a dedicated spectrometer. Lower detection limits are possible with longer signal integration time. Pasting testing on an EPA research incinerator demonstrated the capability for monitoring metal concentrations with a relative accuracy of 20 % to EPA method 29...|$|E
40|$|In {{this study}} we {{investigated}} several most popular Loss Given Default (LGD) models (LSM, Tobit, Three-Tiered Tobit, Beta Regression, Inflated Beta Regression, Censored Gamma Regression) in order to compare their performance. We show that for a given input data set, {{the quality of the}} model calibration depends mainly on the proper choice (and availability) of explanatory variables (model factors), but not on the fitting model. Model factors were chosen based on the amplitude of their correlation with historical LGDs of the calibration data set. Numerical values of non-quantitative parameters (industry, ranking, type of collateral) were introduced as their LGD average. We show that different debt instruments depend on different sets of model factors (from three factors for Revolving Credit or for Subordinated Bonds to eight factors for Senior Secured Bonds). Calibration of LGD models using distressed business cycle periods provide better fit than data from total available time <b>span.</b> <b>Calibration</b> algorithms and details of their realization using the R statistical package are presented. We demonstrate how LGD models can be used for stress testing. The results of this study can be of use to risk managers concerned with the Basel accord compliance. ...|$|E
40|$|This paper {{describes}} {{the development of}} an in situ calibration procedure for combustible gas detectors (CGD). The CGD will be a necessary device for future space vehicles as many subsystems in the Environmental Control/Life Support System utilize or produce hydrogen (H 2) gas. Existing calibration techniques are time-consuming and require support equipment such as an environmental chamber and calibration gas supply. The in situ calibration procedure involves utilization of a water vapor electrolysis cell for the automatic in situ generation of a H 2 /air calibration mixture within the flame arrestor of the CGD. The development effort concluded with the successful demonstration of in situ <b>span</b> <b>calibrations</b> of a CGD...|$|R
40|$|We have {{measured}} the absolute wave numbers of 39 transitions of 130 Te 2 spanning the spectral region of 420. 9 - 464. 6 nm to an accuracy of better than 2 parts in 109 {{by use of}} saturation spectroscopy and Fabry-Pérot interferometry. These measurements provide a set of convenient and accurate transfer standards for laser wavelength <b>calibration</b> <b>spanning</b> the entire Stilbene- 420 dye-tuning curve...|$|R
40|$|The present paper {{deals with}} technologically advanced, programmable, {{intelligent}} transceiver for the wireless {{measurement of the}} strain, static and dynamic stresses and forces using the strain gauges. The intelligence of the measurement modules enables the long time operation (battery life is Ca. 12 months). The system zero-scale <b>calibration,</b> <b>span</b> and offset limits can be set remotely. Digital filters are used for the optimum frequency response of the measuring signal. The transceiver is also equipped with the input for the latest generation of the Dallas digital temperature sensors...|$|R
40|$|We {{describe}} a subspace Monte Carlo (SSMC) technique {{that reduces the}} burden of calibration-constrained Monte Carlo when undertaken with highly parameterized models. When Monte Carlo methods are {{used to evaluate the}} uncertainty in model outputs, ensuring that parameter realizations reproduce the calibration data requires many model runs to condition each realization. In the new SSMC approach, the model is first calibrated using a subspace regularization method, ideally the hybrid Tikhonov-TSVD "superparameter 2 ̆ 72 ̆ 7 approach described by Tonkin and Doherty (2005). Sensitivities calculated with the calibrated model are used to define the calibration null-space, which is spanned by parameter combinations that have no effect on simulated equivalents to available observations. Next, a stochastic parameter generator is used to produce parameter realizations, and for each a difference is formed between the stochastic parameters and the calibrated parameters. This difference is projected onto the calibration null-space and added to the calibrated parameters. If the model is no longer calibrated, parameter combinations that <b>span</b> the <b>calibration</b> solution space are reestimated while retaining the null-space projected parameter differences as additive values. The recalibration can often be undertaken using existing sensitivities, so that conditioning requires {{only a small number of}} model runs. Using synthetic and real-world model applications we demonstrate that the SSMC approach is general (it is not limited to any particular model or any particular parameterization scheme) and that it can rapidly produce a large number of conditioned parameter sets...|$|R
40|$|For {{more than}} a century, members of the {{traditional}} avian order Galliformes (i. e., pheasants, partridges, junglefowl, and relatives) {{have been among the}} most intensively studied birds, but still a comprehensive timeframe for their evolutionary history is lacking. Thanks to a number of recent cladistic interpretations for several galliform fossils, candidates now exist that can potentially be used as accurate internal calibrations for molecular clocks. Here, we describe a molecular timescale for Galliformes based on cytochrome b and ND 2 using nine mostly internal fossil-based anchorpoints. Beyond application of <b>calibrations</b> <b>spanning</b> the entire evolutionary history of Galliformes, care was taken to investigate the effects of calibration choice, substitution saturation, and rate heterogeneity among lineages on divergence time estimation. Results show broad consistency in time estimation with five out of the nine total calibrations. Our divergence time estimates, based on these anchorpoints, indicate that the early history of Galliformes took place in the Cretaceous, including the origin of the basal-most megapode and perhaps cracid lineages, but that the remaining morphological diversification likely started in the earliest Tertiary. The multi-calibration/multi-genetic partition approach used here highlights the importance of understanding the genetic saturation, variation, and rate constancy spectra for the accurate calculation of divergence times by use of molecular clocks...|$|R
40|$|The Global Positioning System (GPS), {{among other}} duties, serves countless {{worldwide}} precise time users. These users are both civilian and military, {{are located on}} ground, in air, at sea, and in space. The respective missions of these users <b>span</b> metrology & <b>calibration,</b> research & development, test & evaluation, communication synchronization, and surveillance. The vast majority of these users employ what is called “one-way ” synchronization using GPS. One-way synchronization allows users to realize precise time, autonomously and anonymously, using the direct precise time broadcast of GPS. Using data collected, processed, and provided by the United States Naval Observatory (USNO), the authors present an analysis of one-way GPS time transfer performance for FY 2002, utilizing a metric in use since the Initial Operational Capability (IOC) of GPS. This metric describes how well a fixed (surveyed) location user, employing an authorized receiver, tracking one satellite at a time, can obtain precise time using GPS one-way synchronization, without any special augmentation. The performance one-way users experience will {{vary depending on the}} particular application. Though no one metric can possibly represent all types of users, the fixed-location analysis provides other types of users a baseline for deriving theoretical assessments of performance that can fairly represent their respectively unique applications...|$|R
40|$|The {{advent of}} continuous-flow isotope-ratio mass {{spectrometry}} (CF-IRMS) {{coupled with a}} high temperature conversion (HTC) system enabled faster, more cost effective, and more precise δ 2 H analysis of hydrogen-bearing solids. Accurate hydrogen isotopic analysis by on-line or off-line techniques requires appropriate isotopic reference materials (RMs). A strategy of two-point <b>calibrations</b> <b>spanning</b> δ 2 H range of the unknowns using two RMs is recommended. Unfortunately, the supply of the previously widely used isotopic RM, NBS 30 biotite, is exhausted. In addition, recent measurements {{have shown that the}} determination of δ 2 H values of NBS 30 biotite on the VSMOW-SLAP isotope-delta scale by on-line HTC systems with CF-IRMS may be unreliable because hydrogen in this biotite may not be converted quantitatively to molecular hydrogen. The δ 2 HVSMOW-SLAP values of NBS 30 biotite analyzed by on-line HTC systems can be as much as 21 mUr (or ‰) too positive compared to the accepted value of − 65. 7 mUr, determined by only a few conventional off-line measurements. To ensure accurate and traceable on-line hydrogen isotope-ratio determinations in mineral samples, we here propose two isotopically homogeneous, hydrous mineral RMs with well-characterized isotope-ratio values, which are urgently needed. The U. S. Geological Survey (USGS) has prepared two such RMs, USGS 57 biotite and USGS 58 muscovite. The δ 2 H values were determined by both glassy carbon-based on-line conversion and chromium-based on-line conversion, and results were confirmed by off-line conversion. The quantitative conversion of hydrogen from the two RMs using the on-line HTC method was carefully evaluated in this study. The isotopic compositions of these new RMs with 1 -σ uncertainties and mass fractions of hydrogen are: USGS 57 (biotite) δ 2 HVSMOW-SLAP = − 91. 5 ± 2. 4 mUr (n = 24) Mass fraction hydrogen = 0. 416 ± 0. 002 % (n = 4) Mass fraction water = 3. 74 ± 0. 02 % (n = 4) USGS 58 (muscovite) δ 2 HVSMOW-SLAP = − 28. 4 ± 1. 6 mUr (n = 24) Mass fraction hydrogen = 0. 448 ± 0. 002 % (n = 4) Mass fraction water = 4. 03 ± 0. 02 % (n = 4). These δ 2 HVSMOW-SLAP values encompass typical ranges for solid unknowns of crustal and mantle origin and are available to users for recommended two-point calibration...|$|R
40|$|The CERN Axion Solar Telescope (CAST) is an {{experiment}} {{that uses the}} world’s highest sensitivity Helioscope to date for solar Axions searches. Axions are weakly interacting pseudoscalar particles proposed to solve the so-called Strong Charge-Parity Problem of the Standard Model. The principle of detection is the inverse Primakoff Effect, which is a mechanism for converting the Axions into easily detectable X-ray photons in a strong transverse magnetic field. The solar Axions are produced due to the Primakoff effect in the hot and dense core of from the coupling of a real and a virtual photon. The solar models predict a peak Axion luminosity at an energy of 3 keV originating mostly from the inner 20 % of the solar radius. Thus an intensity peak at an energy of 3 keV is also expected {{in the case of}} the X-ray radiation resulting from Axion conversion. CAST uses a high precision movement system for tracking the Sun twice a day with a LHC dipole twin aperture prototype magnet, 9. 26 meters long and with a field of 9 Tesla. On the four apertures of the magnet, X-ray detectors look for photons resulted from Axion conversion. For investigating different Axion masses, 3 He and 4 He buffer gas was injected in the magnetic region, restoring the coherence for Axion-to-photon conversion into mass regions so far unexplored, favoured by QCD Axion models. Using this scanning strategy, Axion masses were investigated in the range 0. 02 eV to 1. 17 eV between 2003 and 2013. One of CAST Detectors is a pn-CCD chip placed in the focal plane of an X-ray Telescope. In this thesis an overview of 2009, 2010 and 2011 data taken with this detector is presented. Signal and background levels were extracted, indicating that no conversion signature was detected. The analysed data is being used within the collaboration for improving the combined upper limits on the Axion-to- photon coupling constant parameter space (g!! ≲ 3. 3 × 10 !!" !! 95 %..), for the mass range 0. 65 eV to 1. 17 eV, by merging it with the results from the other detectors. Besides this, X-ray Telescope mirror module reflectivity checks and alignment were performed, together with long-term detector system monitoring, stability studies and <b>calibrations,</b> <b>spanning</b> the interval 2009 - 2014. In order to ensure the highest discovery potential and the best sensitivity of the setup, the mirror module was tested at the Panter X-ray test facility in Munich, and the performance was found to be within the expected margins after 11 years operation of...|$|R
40|$|This {{investigation}} {{is motivated by}} the current need for a detailed post launch calibration of the Thematic Mapper (TM) thermal band (Band 6), aboard NASA’s Landsat 5 spacecraft. The historical <b>calibration</b> <b>spans</b> the period from 1984 to 2007. It is through fusion of environmental data sources (i. e. buoy observations, surface observations, and radiosonde observations) that a vicarious calibration approach will be implemented to construct the complete calibration record of the Landsat 5 TM thermal band. The vicarious calibration process {{takes advantage of the}} long standing National Data Buoy Center (NDBC) moored buoy fleet to acquire historic ground truth measurements needed over the lifetime of Landsat 5. These measurements are propagated to the sensor through the use of physics based models to establish a predicted at sensor radiance. Through comparison of the predicted at sensor radiance and the actual sensor observed radiance, a calibration metric is established. Results indicate the Landsat 5 TM thermal band, originally planned for a 3 year mission, has fluctuated only slightly (1 K) over the 24 + years in orbit. The calibration curve developed in this study is consistent with previous results from campaigns preformed in 1985 and post 1999. The data indicated that the sensor exhibited a clear gain issue (i. e. over estimates low radiance targets and under estimates high radiance targets) found to be approximately consistent over time. Additionally, an event occurring either prior to or during 1999, caused a discernible fluctuation in sensor performance (i. e. dominant cold bias) for all data post 1999. It is the recommendation of this vicarious calibration I II campaign that a linear (Dual: slope 2 ̆ 6 intercept) correction be applied to the Landsat 5 data archive. As a result of the correction, the Landsat 5 TM Band 6 is radiometrically calibrated to within ± 0. 488 K, in reference to a 300 K blackbody. This result was verified through an extensive error propagation analysis, which found the proposed methodology to have an expected error of 0. 454 K. The proposed methodology was also verified by a comparison study to the traditional approach (i. e. non buoy derived ground truth) using the closely monitored and trusted Landsat 7 data calibrated using the traditional approach. The comparison found the two methods were not statistically different, which offered the confidence that this methodology could be applied successfully over the domain of this study. This comparison not only validates the calibration record of Landsat 5, but also demonstrates the utility of the method in future efforts. This work has demonstrated that a successful historical vicarious calibration campaign can be conducted using exclusively free and easily accessible data. It has been established that the proposed methodology can be implemented to achieve a high level of radiometric integrity, which includes both historic and future efforts, in the calibration of remote thermal infrared systems...|$|R
40|$|This {{dissertation}} {{is concerned}} with the development of a methodology and appropriate tools for the investigation of chemical reaction systems using measured data. More specifically, the determination of reaction stoichiometry and kinetics from concentration or, preferably, spectral measurements is considered. The main contribution of this work is the derivation of a nonlinear transformation of the dynamic model that enables the separation of the evolution of the states into three parts: (i) the reaction-variant part (related to the reactions), (ii) the reaction-invariant and flow-variant part (related to the inlet and outlet streams), and (iii) the reaction- and flow-invariant part (related to the initial conditions). This transformation is very helpful in the analysis of concentration and spectral data. Dynamic model First-principles models of reaction systems are gaining importance in chemical and biotechnological production. They can considerably reduce process development costs and be used for simulation, model-based monitoring, control, and optimization, thus leading to improved product quality, productivity, and process safety. These models include information regarding both the chemical reactions and the operational mode of the reaction system. For the analysis of these models, it is important to distinguish between the states that depend on the reactions and those which do not. The concept of reaction invariants is extended to include the flow invariants of reaction systems with inlet and outlet streams. A nonlinear transformation of the first-principles dynamic model to normal form is proposed. Model reduction, state accessibility, and feedback linearizability are analyzed in the light of this transformation. Concentration data Concentration data collected from reaction systems are highly structured, a result of the underlying reactions and the presence of material exchange terms. It is shown that concentration data can be analyzed in the framework of the three-level decomposition provided by the transformation to normal form. The resulting factorization, termed the factorization of concentration data, enables (i) the separation of the reaction and flow variants/invariants and (ii) the segregation of the dynamics (extents of reaction, integral of flows) from the static information (stoichiometry, initial and inlet concentrations). Using the factorization of concentration data, it is possible to isolate the reaction variant part by subtracting the reaction-invariant part from measured concentrations. The reaction-variant part is often unknown, since it depends on the kinetic description (typically the main difficulty in modeling chemical reaction systems). In contrast, the reaction-invariant part is usually known or measured. It is shown that, in cases where the reaction variants can be computed from the concentrations of a few measured species, the concentrations of the remaining species can be reconstructed using the known reaction-invariant part. Target factor analysis has been used successfully with concentration data to determine, without knowledge of reaction kinetics, the number of reactions and the corresponding stoichiometries. It is shown that, when only the reaction-variant part of the data is considered, existing target factor-analytical techniques can be readily applied. However, if target factor analysis needs to be applied directly to measured concentrations, knowledge of reaction-invariant relationships is required to specify necessary and sufficient conditions for the acceptance of stoichiometric targets. Spectral data In current practice, concentration measurements {{during the course of a}} reaction are generally not available, neither on-line nor off-line. Owing to new measurement technologies, spectral measurements are now available in both the laboratory and production. Various spectral instruments enable non-destructive indirect concentration measurement of most of the species in-situ/on-line during the course of a reaction. Measurements are available at high sampling rates and delay-free at low costs. Furthermore, in most cases, the spectral data are linear, i. e., the mixture spectrum is a linear combination of the pure-component spectra weighted by the concentrations. It is shown that the three-level interpretation provided by the transformation to normal form is applicable to spectral data from reacting mixtures. Similarly to traditional wet-chemical analysis methods, a calibration model must also be estimated that provides concentration estimates from spectral measurements. All calibration methods require that a new spectrum lies in the space <b>spanned</b> by the <b>calibration</b> spectral data (space-inclusion condition). To verify this space-inclusion condition, it is proposed to build a calibration model for the reaction-variant part only. Once the reaction variants are predicted from a new spectrum, the (known) reaction invariants can be added to reconstruct the concentrations. Concentration measurements for some species of interest are often not available due to difficulties/costs in sampling, sample preparation, and development of analytical techniques. Thus, traditional calibration of spectral measurements for the purpose of concentration estimation is not possible. Instead, explicit or implicit knowledge about the kinetic structure will be used (prior knowledge about the reaction-variant part), thus enabling the formulation of factor-analytical methods as a calibration problem. For pedagogical reasons, the results are developed for isothermal, constant-density reaction systems with inlet and outlet streams. The results are then extended to various scenarios such as reaction systems with varying density and temperature. Furthermore, factorizations of concentration data are presented that include temperature or calorimetric measurements. Several special cases are considered, encompassing continuous stirred-tank reaction systems, semibatch and batch reaction systems, systems with reactions in quasi-equilibrium conditions, and non-reacting mixtures with closure...|$|R

