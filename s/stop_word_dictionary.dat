0|1520|Public
40|$|The aim of {{this thesis}} is to {{implement}} Multipass-Apriori method for mining association rules from text data. After {{the introduction to the}} field of knowledge discovery, the specific aspects of text mining are mentioned. In the mining process, preprocessing is a very important problem, use of stemming and <b>stop</b> <b>words</b> <b>dictionary</b> is necessary in this case. Next part of thesis deals with meaning, usage and generating of association rules. The main part is focused on the description of Multipass-Apriori method, which was implemented. On the ground of executed tests the most optimal way of dividing partitions was set and also the best way of sorting the itemsets. As a part of testing, Multipass-Apriori method was compared with Apriori method...|$|R
40|$|We {{propose a}} new {{adaptive}} strategy for text recognition {{that attempts to}} derive knowledge about the dominant font on a given page. The strategy uses a linguistic observation that over half of all words in a typical English passage are contained in a small set of less than 150 <b>stop</b> <b>words.</b> A small <b>dictionary</b> of such <b>words</b> are compiled from the Brown corpus. An arbitrary text page first goes through layout analysis that produces word segmentation. A fast procedure is then applied to locate the most likely candidates for those words, using only widths of the word images. The identity of each word is determined using a word shape classifier. Using the word images together with their identities, character prototypes can be extracted using a previously proposed method. We describe experiments using simulated and real images. In an experiment using 400 real page images, we show that on average, 8 distinct characters {{can be learned from}} each page, and the method is successful on 90 % of all the pages. These can serve as useful seeds to bootstrap font learning...|$|R
5000|$|Chinese-English <b>word</b> <b>dictionary</b> (based on Paul Denisowski's CEDICT) ...|$|R
40|$|A Semantic Search Engine (SSE) is {{a program}} that {{produces}} semantic-oriented concepts from the Internet. A web crawler is {{the front end of}} our SSE; its primary goal is to supply important and necessary information to the data analysis component of SSE. The main function of the analysis component is to produce the concepts (moderately frequent finite sequences of keywords) from the input; it uses some variants of TF-IDF as a primary tool to remove <b>stop</b> <b>words.</b> However, it is a very expensive way to filter out <b>stop</b> <b>words</b> using the idea of TF-IDF. The goal of this project is to improve the efficiency of the SSE by avoiding feeding junk data (<b>stop</b> <b>words)</b> to the SSE. In this project, we classify formally three classes of stop words: English-grammar-based <b>stop</b> <b>words,</b> Metadata <b>stop</b> <b>words,</b> and Topic-specific <b>stop</b> <b>words.</b> To remove English-grammar-based <b>stop</b> <b>words,</b> we simply use a list of <b>stop</b> <b>words</b> that {{can be found on the}} Internet. For Metadata <b>stop</b> <b>words,</b> we create a simple web crawler and add a modified HTML parser to it. The HTML parser is used to identify and remove Metadata <b>stop</b> <b>words.</b> So, our web crawler can remove most of the Metadata <b>stop</b> <b>words</b> and reduce the processing time of SSE. However, we do not know much about Topic-specific <b>stop</b> <b>words.</b> So, Topic-specific <b>stop</b> <b>words</b> are identified by a randomly selected sample of documents, instead of identifying all keywords (equal or above a threshold) and all <b>stop</b> <b>words</b> (below the threshold) on the whole set of documents. MapReduce is applied to reduce the complexity and find Topic- specific <b>stop</b> <b>words</b> such as “acm” (Association for Computing Machinery) that we find on IEEE data mining papers. Then, we create a Topic-specific <b>stop</b> <b>word</b> list and use it to reduce the processing time of SSE...|$|R
40|$|Microsoft, Motorola, Siemens, Hitachi, IAPR, NICI, IUF This paper {{presents}} the on-line handwriting recognition system NPen++ {{developed at the}} University of Karlsruhe and the Carnegie Mellon University. The NPen++ recognition engine {{is based on a}} Multi-State Time Delay Neural Network and yields recognition rates from 96 % for a 5000 <b>word</b> <b>dictionary</b> to 93. 4 % on a 20, 000 <b>word</b> <b>dictionary</b> and 91. 2 % for a 50, 000 <b>word</b> <b>dictionary.</b> The proposed tree search and pruning technique reduces the search space considerably without loosing too much recognition performance compared to an exhaustive search. This allows running the NPen++ recognizer in real-time with large dictionaries. ...|$|R
40|$|Abstract: In modern {{information}} retrieval systems, effective indexing {{can be achieved}} by removal of <b>stop</b> <b>words.</b> Till now many <b>stop</b> <b>word</b> lists have been developed for English language. However, no standard <b>stop</b> <b>word</b> list has been constructed for Chinese language yet. With the fast development of {{information retrieval}} in Chinese language, exploring Chinese <b>stop</b> <b>word</b> lists becomes critical. In this paper, to save the time and release the burden of manual <b>stop</b> <b>word</b> selection, we propose an automatic aggregated methodology based on statistical and information models for extraction of a <b>stop</b> <b>word</b> list in Chinese language. Result analysis shows that our stop list is comparable with a general English <b>stop</b> <b>word</b> list, and our list is much more general than other Chinese stop lists as well. Our <b>stop</b> <b>word</b> extraction algorithm is a promising technique, which saves the time for manual generation and constructs a standard. It could be applied into other languages in the future. Key-Words: <b>stop</b> <b>word</b> list, statistical modeling, information theory...|$|R
5000|$|References: 1. Managing Gigabytes: {{book with}} an {{implementation}} of canonical huffman codes for <b>word</b> <b>dictionaries.</b>|$|R
3000|$|Search {{candidate}} {{by matching}} {{items in the}} <b>word</b> <b>dictionary</b> and build up wordnet like {(a,ab),(b,bcd),(c),(d)} [...]...|$|R
30|$|<b>Stops</b> <b>words</b> {{are words}} {{that do not}} contain {{important}} significance for building the model. Some example <b>stop</b> <b>words</b> include the, at, like, etc. We remove <b>stop</b> <b>words</b> from all the tokenized email text.|$|R
5000|$|In {{compiling}} his 24,500 <b>word</b> <b>dictionary,</b> {{he gave up}} {{on trying}} to [...] "fix" [...] the language: ...|$|R
3000|$|<b>Stop</b> <b>word</b> removal The <b>stop</b> <b>words</b> {{are used}} {{frequently}} in natural language. These include ‘is’, ‘to’, ‘for’, ‘an’, ‘are’, ‘in’ and, ‘at’. The <b>stop</b> <b>word</b> elimination plays {{a pivotal role}} for dimensionality reduction of the text for further analysis. It assists {{in the identification of}} the remaining key words in the natural language becomes easy, and subsequent analysis can be performed efficiently. A list compiled by Savoy (2005), contains vast collection of <b>stop</b> <b>words.</b> The <b>stop</b> <b>word</b> elimination process start with the selection of words and ends by discarding such words from the text. In this work, we propose python-based algorithm for <b>stop</b> <b>words</b> removal process shown as follows: [...]...|$|R
30|$|Removal of <b>stop</b> <b>words</b> <b>Stop</b> <b>words</b> {{are very}} common and {{high-frequency}} words. This process {{carried out by}} removing frequently used <b>stop</b> <b>words</b> (prepositions, irrelevant words, special character, ASCII code), new line, extra white spaces etc. to enhance the performance of feature selection technique.|$|R
30|$|Research {{agrees that}} unigrams with removal of <b>stop</b> <b>words</b> and {{stemming}} {{provide the best}} results for LSA [14]. Nevertheless, when we use bigrams, the <b>stop</b> <b>words</b> take {{on the role of}} “function words”; as so, we opted to count the <b>stop</b> <b>words.</b>|$|R
50|$|In computing, <b>stop</b> <b>words</b> {{are words}} which are {{filtered}} out {{before or after}} processing of natural language data (text). Though <b>stop</b> <b>words</b> usually refer to the most common words in a language, {{there is no single}} universal list of <b>stop</b> <b>words</b> used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these <b>stop</b> <b>words</b> to support phrase search.|$|R
40|$|In modern {{information}} retrieval systems, effective indexing {{can be achieved}} by removal of <b>stop</b> <b>words.</b> Till now many <b>stop</b> <b>word</b> lists have been developed for English language. However, no standard <b>stop</b> <b>word</b> list has been constructed for Chinese language yet. With the fast development of {{information retrieval}} in Chinese language, exploring the evaluation of Chinese <b>stop</b> <b>word</b> lists becomes critical. In this paper, to save the time and release the burden of manual comparison, we propose a novel <b>stop</b> <b>word</b> list evaluation method with a mutual information-based Chinese segmentation methodology. Experiments have been conducted on training texts taken from a recent international Chinese segmentation competition. Results show that effective <b>stop</b> <b>word</b> lists can improve the accuracy of Chinese segmentation significantly. 1...|$|R
40|$|Abstract. This paper {{describes}} {{a method for}} correction of optically read Devanagari character strings using a Hindi <b>word</b> <b>dictionary.</b> The <b>word</b> <b>dictionary</b> is partitioned {{in order to reduce}} the search space besides preventing forced match to an incorrect <b>word.</b> The <b>dictionary</b> partitioning strategy takes into account the underlying OCR process. The <b>dictionary</b> <b>words</b> at the top level have been divided into two partitions, namely: short words partition and the remaining words partition. The short word partition is sub-partitioned using the envelop information of the words. The envelope consists of the number of top, lower, core modifiers along with the number of core characters. Devanagari characters are written in three strips. Most of the characters referred to as core characters are written in the middle strip. The remainin...|$|R
40|$|Abstract. Katakana, Japanese {{phonogram}} mainly {{used for}} loan words, is a troublemaker in Japanese word segmentation. Since Katakana words are heavily domaindependent {{and there are}} many Katakana neologisms, {{it is almost impossible to}} construct and maintain Katakana <b>word</b> <b>dictionary</b> by hand. This paper proposes an automatic segmentation method of Japanese Katakana compounds, which makes it possible to construct precise and concise Katakana <b>word</b> <b>dictionary</b> automatically, given only a medium or large size of Japanese corpus of some domain. ...|$|R
5000|$|Mayabi Bangla Keyboard (...) is an {{on-screen}} Bengali soft keyboard for Android platform. Bengali <b>word</b> <b>dictionary</b> {{included with}} the keyboard as well for word prediction.|$|R
30|$|The next part {{after that}} is {{keywords}} selection. The process includes various steps: The {{first step is}} to remove the <b>stop</b> <b>words</b> (Chen and Chen 2001). <b>Stop</b> <b>words</b> are meaningless words that occur after the word segmentation, such as prepositions just like “on”, “to”, “of”, etc. There are various types of <b>stop</b> <b>words</b> lists on the Internet, and it doesn’t exist a special <b>stop</b> <b>words</b> list that can be applied to all natural language studies. Therefore, we establish a new <b>stop</b> <b>words</b> list. Our <b>stop</b> <b>words</b> list includes 1601 words which cover the most common <b>stop</b> <b>words,</b> as well as the words we selected based on the word what is commonly used on the phone, such as Hello, Good, Hang Up, Hold on, and so on. After removing the <b>stop</b> <b>word,</b> we write programs to select keywords based on the part-of-speech. For example, we remove prepositions, adverbs and other meaningless words, {{at the same time we}} retain nouns, verbs, and other meaningful words. We continue to filter the keyword list manually after selecting keywords by programs. This step is mainly to remove the words which meaningless such as personal names and geographic names. After that, we get a keywords list extracted from textual data came from telecommunication fraud related data.|$|R
5000|$|The {{final step}} for the BoW model is to convert vector-represented patches to [...] "codewords" [...] (analogous to words in text documents), which also {{produces}} a [...] "codebook" [...] (analogy to a <b>word</b> <b>dictionary).</b> A codeword {{can be considered}} as a representative of several similar patches. One simple method is performing k-means clustering over all the vectors. Codewords are then defined as the centers of the learned clusters. The number of the clusters is the codebook size (analogous {{to the size of the}} <b>word</b> <b>dictionary).</b>|$|R
3000|$|Calibration was {{performed}} during {{the running of}} the experiment: it was feasible to use an approach that found the best possible values for each parameter, for the best accuracy. On preprocessing, variations were considered for the entries of A: a) Counting all the <b>stop</b> <b>words</b> b) Removing all the <b>stop</b> <b>words</b> c) Removing all the <b>stop</b> <b>word</b> plus a stemming process [...]...|$|R
50|$|Johannes de Garlandia {{is thought}} to have invented the term dictionarius, the source of the English <b>word</b> <b>dictionary</b> and of similar words in many other modern languages.|$|R
3000|$|C, the {{database}} storage module: {{based on the}} returned data format, to use MongoDB, NoSql database, {{and set up a}} collection of four different emotional <b>words</b> <b>dictionaries</b> [...]...|$|R
40|$|This paper {{presents}} the on-line handwriting recognition system NPen++ {{developed at the}} University of Karlsruhe and the Carnegie Mellon University. The NPen++ recognition engine {{is based on a}} Multi-State Time Delay Neural Network and yields recognition rates from 96 % for a 5000 <b>word</b> <b>dictionary</b> to 93. 4 % on a 20, 000 <b>word</b> <b>dictionary</b> and 91. 2 % for a 50, 000 <b>word</b> <b>dictionary.</b> The proposed tree search and pruning technique reduces the search space considerably without loosing too much recognition performance compared to an exhaustive search. This allows running the NPen++ recognizer in real-time with large dictionaries. 1 Introduction This paper describes the preprocessing steps, the computation of features, the recognizer with training and testing, and the dictionary based search of the NPen++ handwriting recognition system. Section 2 begins with the description of the normalizing preprocessing steps in NPen++. Section 3 shows different features computed after preprocessing. The core [...] ...|$|R
40|$|In {{this paper}} we {{describe}} the NPen ++ system for writer independent on-line handwriting recognition. This recognizer needs no training for a particular writer and can recognize any common writing style (cursive, hand-printed, or a mixture of both). The neural network architecture, which was originally proposed for continuous speech recognition tasks, and the preprocessing techniques of NPen ++ are designed to make heavy use of the dynamic writing information, i. e. the temporal sequence of data points recorded on a LCD tablet or digitizer. We present results for the writer independent recognition of isolated words. Tested on different dictionary sizes from 1, 000 up to 100, 000 words, recognition rates range from 98. 0 % for the 1, 000 <b>word</b> <b>dictionary</b> to 91. 4 % on a 20, 000 <b>word</b> <b>dictionary</b> and 82. 9 % for the 100, 000 <b>word</b> <b>dictionary.</b> No language models are used to achieve these results. 1 Introduction The success and user acceptance of pen computing or multi-modal systems highly depends on [...] ...|$|R
40|$|This paper {{discusses}} the CICC related concept hierarchies and the alignment tasks {{of the concepts}} and the hierarchies. We formalize the difficulties in the alignment, and then propose a mechanism to maintain {{the quality of the}} lexical knowledge by means of cross-language investigation. A flexible hierarchy and a mechanism for constructing expressive concepts are required to keep the lexical knowledge and the hierarchy in a manageable size. 1 Introduction CICC (Center of the International Cooperation for Computerization) gave in the final report recovering that in the basic term dictionary there were around 20 % to 30 % of the concepts de ned in each languages could be shared pairwise. And, only around 10 % to 20 % of the concepts could be shared among them. Likewise, there were less than 10 % of the concepts defined in the Japanese <b>word</b> <b>dictionary</b> that could be shared with the ones in the English <b>word</b> <b>dictionary</b> of the EDR (Electronic <b>Dictionary</b> Research Laboratory) <b>word</b> <b>dictionaries,</b> as rep [...] ...|$|R
40|$|The Web {{has plenty}} of reviews, {{comments}} and reports about products, services, government policies, institutions, etc. The opinions expressed in these reviews influence how people regard these entities. For example, a product with consistently good reviews is likely to sell well, while a product with numerous bad reviews is likely to sell poorly. Our aim {{is to build a}} sentimental <b>word</b> <b>dictionary,</b> which is larger than existing sentimental <b>word</b> <b>dictionaries</b> and has high accuracy. We introduce rules for deduction, which take words with known polarities as input and produce synsets (a set of synonyms with a definition) with polarities. The synsets with deduced polarities can then be used to further deduce the polarities of other words. Experimental results show that for a given sentimental <b>word</b> <b>dictionary</b> with D <b>words,</b> approximately an additional 50 % of D words with polarities can be deduced. An experiment is conducted to find the accuracy of a random sample of the deduced words. It is found that the accuracy is about {{the same as that of}} comparing the judgment of one human with that of another...|$|R
30|$|Removing <b>stops</b> <b>words</b> {{and small}} words.|$|R
30|$|Tweet: we {{represent}} a tweet as bag of words. We removed all the <b>stop</b> <b>words</b> (based {{on the standard}} INQUERY stop list). The final representation is a clean tweet without <b>stop</b> <b>words</b> or useless words. We used t as a symbol for tweet object.|$|R
40|$|This paper gives {{description}} about methodology {{to understand}} parallel English-Hindi sentences using word alignment. This methodology is foundation {{to develop the}} parallel English-Hindi <b>word</b> <b>dictionary</b> after syntactically and semantically analysis of the English-Hindi source text. Methodology of proposed system {{is used for the}} English and Hindi sentences; also the methodology can be used for other languages. Outsized parallel corpus of English-Hindi pair language is not frequently available. Development is based on two strategies to solve this problem. First is normalization of tagged English sentences and Hindi sentences. Second is mapping English-Hindi sentence using parallel English-Hindi <b>word</b> <b>dictionary.</b> Fortunately <b>word</b> alignment is clearly known and few aligning algorithms are without restraint accessible...|$|R
40|$|We have {{developed}} a multimedia communication environment where multimedia is used to fully utilize available communication (including teaching and learning) capabilities of different users. This environment is based on "film" formats of multimedia words and sentences. Corresponding films are multiple views of objects, processes, etc. They are "pieces of knowledge. " These pieces are acquired in a film database. Multimedia <b>word</b> <b>dictionary</b> panels provide effective access to the database items. In this paper, film technology concept, overview of the environment, and filmification of words and sentences to support communication of children, handicapped and elderly people are presented. A special {{attention is paid to}} implementing a multimedia <b>word</b> <b>dictionary...</b>|$|R
50|$|<b>Stop</b> <b>Words,</b> Ormond, Vic.: Hybrid Publishers, 2011. 87pp.|$|R
3000|$|F(t) denotes {{whether the}} word t {{exists in the}} word bag or not, and is mainly decided {{according}} to the part of speech and <b>stopping</b> <b>words.</b> If the word t is a <b>stopping</b> <b>word</b> and {{in the part of}} speech that does not belong to verb, noun, and adjective, L [...]...|$|R
40|$|This manual {{describes}} {{the operation of}} the Constituent Object parser (COP). it covers the following features and tools of COP: how to start the TI Explorer and load COP; How to parse a sentence or a batch of sentences (i. e., from abstracts); how to look at the results of the parse, including two forms of tree structures and {{a list of all the}} constituents that were built during the parse; how to compare a query sentence with a target sentence or abstract(s); how to process the grammars and dictionary after changes have been made to them; how to identify words in a file (e. g. of abstracts) that are not in the dictionary; how to use the Single <b>Word</b> <b>Dictionary</b> Tool and the Multiple <b>Word</b> <b>Dictionary</b> Tool to look at sets <b>words</b> in the <b>dictionary,</b> add <b>word</b> entries, or change existing word entries...|$|R
50|$|This is {{different}} from harmless but useless words that are called <b>stop</b> <b>words.</b>|$|R
40|$|Abstract – In this paper, {{we present}} a {{methodology}} for one to one (1 : 1) mapping of parallel English-Hindi parallel sentences. This methodology {{is based on the}} development of parallel English-Hindi <b>word</b> <b>dictionary</b> after syntactically and semantically analysis of the English-Hindi source text. We are using this methodology for the English and Hindi sentences, but the methodology can also be used for other languages. As big parallel corpus of English-Hindi pair language is not usually available, we design and develop two strategies to overcome this problem: normalization of tagged English sentences and Hindi sentences, on the one hand; mapping English-Hindi sentence using parallel English-Hindi <b>word</b> <b>dictionary,</b> on the other. Fortunately, this task, word alignment is well known, and some aligning algorithms are freely available...|$|R
