3909|10000|Public
5|$|Michael J. Flynn created {{one of the}} {{earliest}} classification systems for parallel (and sequential) computers and programs, now known as Flynn's taxonomy. Flynn classified programs and computers by whether they were operating using a single set or multiple sets of instructions, and whether or not those instructions were using a single set or multiple <b>sets</b> <b>of</b> <b>data.</b>|$|E
5|$|A vector {{processor}} is a CPU or computer {{system that can}} execute the same instruction on large <b>sets</b> <b>of</b> <b>data.</b> Vector processors have high-level operations that work on linear arrays of numbers or vectors. An example vector operation is A = B × C, where A, B, and C are each 64-element vectors of 64-bit floating-point numbers. They {{are closely related to}} Flynn's SIMD classification.|$|E
5|$|As {{technology}} improved, {{measurement of}} depth, {{latitude and longitude}} became more precise and it became possible to collect more or less continuous <b>sets</b> <b>of</b> <b>data</b> points. This allowed researchers to draw accurate and detailed maps of {{large areas of the}} ocean floor. Use of a continuously recording fathometer enabled Tolstoy & Ewing in the summer of 1947 to identify and describe the first abyssal plain. This plain, south of Newfoundland, is now known as the Sohm Abyssal Plain. Following this discovery many other examples were found in all the oceans.|$|E
40|$|The {{invention}} {{relates to}} {{a method for}} use in a packet-switched communication system, the method comprising: sending a first packet from a source node to a destination node, the first packet containing a first <b>set</b> <b>of</b> <b>data</b> bits; sending a second packet from the source node to the destination node, the second packet containing a second <b>set</b> <b>of</b> <b>data</b> bits; wherein, in the event that the first <b>set</b> <b>of</b> <b>data</b> bits cannot be successfully decoded in the destination node, the second packet further comprises a first <b>set</b> <b>of</b> error correction bits for the first <b>set</b> <b>of</b> <b>data</b> bits...|$|R
25|$|Just as {{absolute}} entropy {{serves as}} theoretical background for data compression, relative entropy serves as theoretical background for data differencing – the absolute entropy <b>of</b> a <b>set</b> <b>of</b> <b>data</b> {{in this sense}} being the data required to reconstruct it (minimum compressed size), while the relative entropy <b>of</b> a target <b>set</b> <b>of</b> <b>data,</b> given a source <b>set</b> <b>of</b> <b>data,</b> is the data required to reconstruct the target given the source (minimum size of a patch).|$|R
30|$|In this study, two {{different}} <b>sets</b> <b>of</b> meteorological <b>data</b> {{are used for}} the calculation of the heating and cooling needs of the university building. The first <b>set</b> <b>of</b> <b>data</b> was collected by a weather station located downtown Modena, while the second <b>set</b> <b>of</b> <b>data</b> was collected by another station, located in the surrounding area of the city, near to the studied building.|$|R
25|$|Techniques {{can also}} be combined. For sorting very large <b>sets</b> <b>of</b> <b>data</b> that vastly exceed system memory, even the index {{may need to be}} sorted using an {{algorithm}} or combination of algorithms designed to perform reasonably with virtual memory, i.e., {{to reduce the amount of}} swapping required.|$|E
25|$|Exploratory data {{analysis}} {{refers to a}} variety of practices which researchers can use to visualize and analyze existing <b>sets</b> <b>of</b> <b>data.</b> In Peirce's three modes of inference, exploratory data anlysis corresponds to abduction, or hypothesis formation. Meta-analysis is the technique of integrating the results from multiple studies and interpreting the statistical properties of the pooled dataset.|$|E
25|$|In {{his book}} , Dr. Paul recounts {{the events of}} David Hartman's abduction. Leading up to it Hartman, a {{computer}} programmer, claims he experienced strange phenomenon with his work tech including date and time changes, words typing by themselves, and whole <b>sets</b> <b>of</b> <b>data</b> moving {{from one place to}} another without human intervention. He also told of physical signs such as nausea, insomnia, and nightmares of inhuman faces coming out of the dark.|$|E
50|$|Data element {{definitions}} {{are critical}} for external users <b>of</b> any <b>data</b> system. Good definitions can dramatically ease the process <b>of</b> mapping one <b>set</b> <b>of</b> <b>data</b> into another <b>set</b> <b>of</b> <b>data.</b> This is a core feature of distributed computing and intelligent agent development.|$|R
30|$|In {{the field}} of {{mathematical}} statistics, variance is usually {{used to measure the}} fluctuation <b>of</b> a <b>set</b> <b>of</b> <b>data,</b> and its value is positive correlated to the degree that a <b>set</b> <b>of</b> <b>data</b> deviates from the average. Its definition is as follows.|$|R
5000|$|A {{classification}} model [...] {{with some}} parameter vector [...] {{is said to}} shatter a <b>set</b> <b>of</b> <b>data</b> points [...] if, for all assignments of labels to those points, there exists a [...] such that the model [...] makes no errors when evaluating that <b>set</b> <b>of</b> <b>data</b> points.|$|R
25|$|Standard {{conditions}} for temperature and pressure are standard sets of {{conditions for}} experimental measurements {{to be established}} to allow comparisons to be made between different <b>sets</b> <b>of</b> <b>data.</b> The most used standards {{are those of the}} International Union of Pure and Applied Chemistry (IUPAC) and the National Institute of Standards and Technology (NIST), although these are not universally accepted standards. Other organizations have established a variety of alternative definitions for their standard reference conditions.|$|E
25|$|Joel Mokyr's {{estimates}} at an aggregated county {{level range}} from 1.1 million to 1.5 million deaths between 1846 and 1851. Mokyr produced two <b>sets</b> <b>of</b> <b>data</b> which contained an upper-bound and lower-bound estimate, which showed not much difference in regional patterns. The true figure {{is likely to}} lie between the two extremes of half and {{one and a half}} million, and the most widely accepted estimate is one million.|$|E
500|$|Task parallelisms is the {{characteristic}} of a parallel program that [...] "entirely different calculations can be performed on either the same or different sets of data". This contrasts with data parallelism, where the same calculation is performed on the same or different <b>sets</b> <b>of</b> <b>data.</b> Task parallelism involves the decomposition of a task into sub-tasks and then allocating each sub-task to a processor for execution. The processors would then execute these sub-tasks simultaneously and often cooperatively. Task parallelism does not usually scale {{with the size of}} a problem.|$|E
40|$|International audienceSugeno {{integrals}} are aggregation {{functions that}} return a global evaluation that {{is between the}} minimum and the maximum of the combined evaluations. The paper addresses {{the problem of the}} elicitation of (families of) Sugeno integrals agreeing with a <b>set</b> <b>of</b> <b>data,</b> made <b>of</b> tuples gathering the partial evaluations according to the different evaluation criteria together with the corresponding global evaluation. The situation where there is no Sugeno integral that is compatible with a whole <b>set</b> <b>of</b> <b>data</b> is especially studied. The representation <b>of</b> mental workload <b>data</b> is used as an illustrative example, where several distinct families of Sugeno integrals are necessary for covering the <b>set</b> <b>of</b> <b>data</b> (since the way mental workload depends on its evaluation criteria may vary with contexts). Apart this case study illustration, the contributions of the paper are an analytical characterization <b>of</b> the <b>set</b> <b>of</b> Sugeno integrals compatible with a <b>set</b> <b>of</b> <b>data,</b> the expression <b>of</b> conditions ensuring that pieces <b>of</b> <b>data</b> are compatible with a representation by a common Sugeno integral, and a simulated annealing optimization algorithm for computing a minimal number of families of Sugeno integrals sufficient for covering a <b>set</b> <b>of</b> <b>data...</b>|$|R
40|$|In today’s {{world to}} access the large <b>set</b> <b>of</b> <b>data</b> is more complex, because the data may be {{structured}} and unstructured like {{in the form of}} text, images, videos, etc., it cannot be controlled from the internet users this is known as Big data. Useful data can be accessed through extracting from big data with the help <b>of</b> <b>data</b> mining algorithms. Data mining is a technique for determine the patterns; classify the data, clustering from the large <b>set</b> <b>of</b> <b>data.</b> In this paper we will discuss how large <b>set</b> <b>of</b> <b>data</b> can be access through data mining algorithms over cloud environment...|$|R
5000|$|Assessing {{the scope}} of a model, that is, {{determining}} what situations the model is applicable to, can be less straightforward. If the model was constructed based on a <b>set</b> <b>of</b> <b>data,</b> one must determine for which systems or situations the known data is a [...] "typical" [...] <b>set</b> <b>of</b> <b>data.</b>|$|R
2500|$|... 1 is {{the most}} common leading digit in many <b>sets</b> <b>of</b> <b>data,</b> a {{consequence}} of Benford's law.|$|E
2500|$|One {{application}} of the algorithm is finding sequence alignments of DNA or protein sequences. [...] It is also a space-efficient way to calculate the longest common subsequence between two <b>sets</b> <b>of</b> <b>data</b> such as with the common diff tool.|$|E
2500|$|A {{teaching}} and research tool presenting a comprehensive picture of Nineveh within the history of archaeology in the Near East, including a searchable data repository for meaningful analysis of currently unlinked <b>sets</b> <b>of</b> <b>data</b> from {{different areas of the}} site and different episodes in the 160-year history of excavations ...|$|E
40|$|A method <b>of</b> {{providing}} graphics <b>data,</b> comprising {{generating a}} first <b>set</b> <b>of</b> <b>data</b> vectors specifying geometrical {{characteristics of a}} graphical object in a first digital picture, generating a second <b>set</b> <b>of</b> <b>data</b> vectors specifying geometrical characteristics of the graphical object in a second digital picture to be displayed after the first digital picture, generating a parameter st comprising information specifying intermediate geometrical characteristics of the graphical object in the first digital picture, and of the graphical object in the second digital picture, wherein the intermediate geometrical characteristics are geometrical characteristics of the graphical object {{in at least one}} third digital picture to be displayed after the first digital picture and before the second digital picture, and generating at least one data file comprising the first <b>set</b> <b>of</b> <b>data</b> vectors, the second <b>set</b> <b>of</b> <b>data</b> vectors and the parameter set...|$|R
40|$| {{parameters}} and behavior {{given the same}} <b>set</b> <b>of</b> <b>data.</b> The results indicate|$|R
5000|$|Abstract {{data type}} for an {{abstract}} description <b>of</b> a <b>set</b> <b>of</b> <b>data</b> ...|$|R
2500|$|PivotTables, {{which are}} used to create {{analysis}} reports out of <b>sets</b> <b>of</b> <b>data,</b> can now support hierarchical data by displaying a row in {{the table with a}} [...] "+" [...] icon, which, when clicked, shows more rows regarding it, which can also be hierarchical. PivotTables can also be sorted and filtered independently, and conditional formatting used to highlight trends in the data.|$|E
2500|$|The {{function}} A(t|ν) is {{the integral}} of Student's probability density function, f(t) between −t and t, for t ≥ 0. [...] It thus gives {{the probability that}} a value of t less than that calculated from observed data would occur by chance. [...] Therefore, the function A(t|ν) can be used when testing whether the difference between the means of two <b>sets</b> <b>of</b> <b>data</b> is statistically significant, by calculating the corresponding value of t and the probability of its occurrence if the two <b>sets</b> <b>of</b> <b>data</b> were drawn from the same population. [...] This is used in a variety of situations, particularly in t-tests. [...] For the statistic t, with ν degrees of freedom, A(t|ν) is the probability that t would be less than the observed value if the two means were the same (provided that the smaller mean is subtracted from the larger, so that t ≥ 0). [...] It can be easily calculated from the cumulative distribution function Fν(t) of the t-distribution: ...|$|E
2500|$|An {{implication}} of the theorem is that when using likelihood-based inference, two <b>sets</b> <b>of</b> <b>data</b> yielding the same value for the sufficient statistic T(X) will always yield the same inferences about θ. [...] By the factorization criterion, the likelihood's dependence on θ is only in conjunction with T(X). [...] As {{this is the same}} in both cases, the dependence on θ will be the same as well, leading to identical inferences.|$|E
5000|$|... a {{particular}} {{realization of the}} random process; i.e., a <b>set</b> <b>of</b> <b>data.</b>|$|R
5000|$|Any <b>set</b> <b>of</b> <b>data</b> can be {{represented}} by a string of symbols from a finite (say, binary) alphabet.MDL Principle {{is based on the}} following insight: any regularity in a given <b>set</b> <b>of</b> <b>data</b> can be used to compress the data, i.e. to describe it using fewer symbols than needed to describe the data literally. (Grünwald, 1998) ...|$|R
50|$|A {{point cloud}} is a <b>set</b> <b>of</b> <b>data</b> points in some {{coordinate}} system.|$|R
2500|$|A t-test is most {{commonly}} applied when the test statistic would follow a normal distribution if {{the value of}} a scaling term in the test statistic were known. [...] When the scaling term is unknown and is replaced by an estimate based on the data, the test statistics (under certain conditions) follow a Student's t distribution. The t-test can be used, for example, to determine if two <b>sets</b> <b>of</b> <b>data</b> are significantly different from each other.|$|E
2500|$|In general, if {{a set of}} data {{structures}} {{needs to}} be included in linked lists, external storage is the best approach. [...] If a set of data structures need {{to be included in}} only one linked list, then internal storage is slightly better, unless a generic linked list package using external storage is available. [...] Likewise, if different <b>sets</b> <b>of</b> <b>data</b> that can be stored in the same data structure are to be included in a single linked list, then internal storage would be fine.|$|E
2500|$|The source {{locations}} can {{be combined}} with magnetic resonance imaging (MRI) images to create magnetic source images (MSI). [...] The two <b>sets</b> <b>of</b> <b>data</b> are combined by measuring {{the location of a}} common set of fiducial points marked during MRI with lipid markers and marked during MEG with electrified coils of wire that give off magnetic fields. [...] The locations of the fiducial points in each data set are then used to define a common coordinate system so that superimposing the functional MEG data onto the structural MRI data ("coregistration") is possible.|$|E
5000|$|Spearman's rank {{correlation}} coefficient - {{measure of how}} monotonic a <b>set</b> <b>of</b> <b>data</b> ...|$|R
5000|$|Module - a <b>set</b> <b>of</b> <b>data</b> {{relating}} {{to a specific}} type of content ...|$|R
5000|$|Updates to {{any given}} <b>set</b> <b>of</b> <b>data</b> are {{immediately}} received by every end-user.|$|R
