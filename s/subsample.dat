7344|5481|Public
5|$|A 2007 {{study of}} young chess {{players in the}} United Kingdom found that strong players tended to have above-average IQ scores, but, within that group, the {{correlation}} between chess skill and IQ was moderately negative, meaning that smarter children tended to achieve {{a lower level of}} chess skill. This result was explained by a negative correlation between intelligence and practice in the elite <b>subsample,</b> and by practice having a higher influence on chess skill.|$|E
25|$|<b>Subsample</b> pump: To pull {{air through}} the analyzers, a small, stable, {{reliable}} pump is used.|$|E
25|$|Since 1951, the U.S. Census Bureau has {{published}} the Retail Sales report every month. It {{is a measure}} of consumer spending, an important indicator of the US GDP. Retail firms provide data on the dollar value of their retail sales and inventories. A sample of 12,000 firms is included in the final survey and 5,000 in the advanced one. The advanced estimated data is based on a <b>subsample</b> from the US CB complete retail & food services sample.|$|E
40|$|An {{integral}} part of many algorithms for S-estimators of linear regression is random <b>subsampling.</b> For problems with only continuous predictors simple random <b>subsampling</b> is a reliable method to generate initial coefficient estimates that can then be further refined. For data with categorical predictors, however, random <b>subsampling</b> often does not work, thus limiting {{the use of an}} otherwise fine estimator. This also makes the choice of estimator for robust linear regression dependent on the type of predictors, which is an unnecessary nuisance in practice. For data with categorical predictors random <b>subsampling</b> often generates singular <b>subsamples.</b> Since these <b>subsamples</b> cannot be used to calculate coefficient estimates, they have to be discarded. This makes random <b>subsampling</b> slow, especially if some levels of categorical predictors have low frequency, and renders the algorithms infeasible for such problems. This paper introduces an improved <b>subsampling</b> algorithm that only generates nonsingular <b>subsamples.</b> We call it nonsingular <b>subsampling.</b> For data with continuous variables it is as fast as simple random <b>subsampling</b> but much faster for data with categorical predictors. This is achieved by using a modified LU decomposition algorithm that combines the generation of a sample and the solving of the least squares problem...|$|R
40|$|Abstract- In a {{spatially}} adaptive <b>subsampling</b> scheme, the <b>subsampling</b> lattice {{is adapted}} to the local spatial frequency content of an image sequence. In this paper, we use rate-distortion theory to show that spatially adaptive <b>subsampling</b> gives a better performance than <b>subsampling</b> with a fixed sampling lattice. A new algorithm that optimally assigns sampling lattices to {{different parts of the}} image is presented. The proposed spatially adaptive <b>subsampling</b> method can be applied within a motion-compensated coding scheme as well. Experiments show an increased perfor-mance over fixed lattice <b>subsampling.</b> I...|$|R
40|$|We {{characterize}} the robustness of <b>subsampling</b> procedures by deriving {{a formula for}} the breakdown point of <b>subsampling</b> quantiles. This breakdown point can be very low for moderate <b>subsampling</b> block sizes, which implies the fragility of <b>subsampling</b> procedures, {{even when they are}} applied to robust statistics. This instability arises also for data driven block size selection procedures minimizing the minimum confidence interval volatility index, but can be mitigated if a more robust calibration method can be applied instead. To overcome these robustness problems, we introduce a consistent robust <b>subsampling</b> procedure for M-estimators and derive explicit <b>subsampling</b> quantile breakdown point characterizations for MM-estimators in the linear regression model. Monte Carlo simulations in two settings where the bootstrap fails show the accuracy and robustness of the robust <b>subsampling</b> relative to the <b>subsampling...</b>|$|R
500|$|The philosopher Frederick Suppe {{considered}} Sexual Preference a {{very important}} study that failed to duplicate the findings of Bieber et al. or the predictions of symbolic interactionism, labeling theory, and societal reaction theory approaches. Suppe considered Bell et al.′s sample of homosexuals, while highly biased, to nevertheless be the most representative ever made, and argued that biased samples can be adequate {{for the purposes of}} refuting theories propounded in other studies [...] "so long as the types of subjects used in those other studies constitute a <b>subsample</b> of the replicative study′s sample and the latter's population does not go beyond the claimed scope of the replicated studies." [...] He maintained that Bell et al.′s study meets these requirements, that their use of path analysis was appropriate, and that their procedures for developing a composite etiology model, which contained [...] "virtually all paths advanced in the literature", are legitimate. He argued that the only plausible basis for disputing that the study definitively refutes [...] "social learning theories of homosexual etiology" [...] is to challenge the adequacy of Bell et al.′s models and the questions they used. However, he questioned some of the details of Bell et al.′s methodology, including the questions asked. He wrote that while Bell et al. did not use the same specific questions that Bieber et al. employed, they did use [...] "questions directed at the same concerns." [...] He noted that Bell et al.′s data regarding subjects′ negative feelings toward and relationships with their fathers were based on open-ended interview questions, adding {{that it would have been}} preferable had they employed the same [...] "structured-answer questions" [...] used in Bieber et al.′s earlier study. He rejected Bell et al.′s claim that their study supports a biological explanation of sexual orientation. He wrote that since Bell et al.′s study, research into the [...] "social causes of homosexuality" [...] has become [...] "moribund." ...|$|E
2500|$|In a {{rarefied}} {{sample a}} random <b>subsample</b> n in chosen {{from the total}} N items. In this sample some groups may be necessarily absent from this <b>subsample.</b> Let [...] be the number of groups still present in the <b>subsample</b> of n items. [...] is less than K the number of categories whenever at least one group is missing from this <b>subsample.</b>|$|E
2500|$|Planetary nebulae {{have been}} {{detected}} as members in four Galactic globular clusters: Messier 15, Messier 22, NGC 6441 and Palomar 6. Evidence also points to the potential discovery of planetary nebulae in globular clusters in the galaxy M31. [...] However, there is currently only one case of a planetary nebula discovered in an open cluster that is agreed upon by independent researchers. That case pertains to the planetary nebula PHR 1315-6555 and the open cluster Andrews-Lindsay 1. [...] Indeed, through cluster membership PHR 1315-6555 possesses among the most precise distances established for a planetary nebula (i.e., a 4% distance solution). The cases of NGC 2818 and NGC 2348 in Messier 46, exhibit mismatched velocities between the planetary nebulae and the clusters, which indicates they are line-of-sight coincidences. [...] A <b>subsample</b> of tentative cases that may potentially be cluster/PN pairs includes Abell 8 and Bica 6, and He 2-86 and NGC 4463.|$|E
40|$|We {{compute the}} {{breakdown}} {{point of the}} <b>subsampling</b> quantile of a general statistic, and show that it is increasing in the <b>subsampling</b> block size and the breakdown point of the statistic. These results imply fragile <b>subsampling</b> quantiles for moderate block sizes, also when <b>subsampling</b> procedures are applied to robust statistics. This instability is inherited by data driven block size selection procedures based on the minimum confidence interval volatility (MCIV) index. To overcome these problems, we propose for the linear regression setting a robust <b>subsampling</b> method, which implies a su±ciently high breakdown point and is consistent under standard conditions. Monte Carlo simulations and sensitivity analysis in the linear regression setting show that the robust <b>subsampling</b> with block size selection based on the MCIV index outperforms the <b>subsampling,</b> the classical bootstrap and the robust bootstrap, in terms of accuracy and robustness. These results show that robustness is a key aspect in selecting data driven <b>subsampling</b> block sizes. <b>Subsampling,</b> bootstrap, breakdown point, robustness, regression...|$|R
40|$|<b>Subsampling</b> is a {{commonly}} used technique for modern image and video compression. Existing image and video standards such as JPEG, MPEG, and H. 264 {{provide support for}} uniform chroma <b>subsampling.</b> This paper presents an algorithm that uses adaptive luma <b>subsampling</b> based {{on a combination of}} three perceptually significant image characteristics (texture, edges, and brightness) to complement uniform chroma <b>subsampling.</b> The algorithm is computationally efficient, simple to implement, and easy to integrate into existing standards. Experimental results show that the introduction of adaptive luma <b>subsampling</b> improves image quality both quantitatively and qualitatively when compared with the sole use of uniform chroma <b>subsampling.</b> 1...|$|R
40|$|Fast classier {{induction}} {{from large}} data set {{is one of}} the main issues in the eld of data mining. One of the approaches on this issue is to reduce the training data size by <b>subsampling.</b> In many cases, the accuracy of the induced classiers becomes worse when the training data is <b>subsampled.</b> We proposed S 3 Bagging (Small <b>SubSampled</b> Bagging) that adopts both <b>subsampling</b> and Bagging. The main parameter of S 3 Bagging is <b>subsampling</b> rate. The <b>subsampling</b> rate determines the performance of S 3 Bagging, i. e., the prediction accuracy of induced classier and the processing time. In this paper, the eect of <b>subsampling</b> rate on the performance of S 3 Bagging is investigated by carefully designed experiments. 1...|$|R
2500|$|Other {{examples}} of species introduced {{for the purposes}} of benefiting agriculture, aquaculture or other economic activities are widespread. [...] Eurasian carp was first introduced to the United States as a potential food source. The apple snail was released in Southeast Asia with the intent that it be used as a protein source, and subsequently to places like Hawaii to establish a food industry. [...] In Alaska, foxes were introduced to many islands to create new populations for the fur trade. About twenty species of African and European dung beetles have established themselves in Australia after deliberate introduction by the Australian Dung Beetle Project in an effort to reduce the impact of livestock manure. The timber industry promoted the introduction of Monterey pine (Pinus radiata) from California to Australia and New Zealand as a commercial timber crop. [...] These examples represent only a small <b>subsample</b> of species that have been moved by humans for economic interests.|$|E
5000|$|In a {{rarefied}} {{sample a}} random <b>subsample</b> n in chosen {{from the total}} N items. In this sample some groups may be necessarily absent from this <b>subsample.</b> Let [...] be the number of groups still present in the <b>subsample</b> of n items. [...] is less than K the number of categories whenever at least one group is missing from this <b>subsample.</b>|$|E
5000|$|... cross-validation, {{in which}} the {{parameters}} (e.g., regression weights, factor loadings) that are estimated in one <b>subsample</b> are applied to another <b>subsample.</b>|$|E
50|$|Most {{digital video}} formats {{corresponding}} to PAL use 4:2:0 chroma <b>subsampling,</b> {{with the exception}} of DVCPRO25, which uses 4:1:1 chroma <b>subsampling.</b> Both the 4:1:1 and 4:2:0 schemes halve the bandwidth compared to no chroma <b>subsampling.</b>|$|R
40|$|<b>Subsampling</b> {{techniques}} {{are important for}} the determination of precise plankton density estimates. A binomial model of random <b>subsampling,</b> and its Poisson extension, were developed {{for the purpose of}} evaluating the performance of compartment-type plankton subsamplers. Two approaches were used to assess the performance of the Folsom plankton splitter on an extensive series of nearshore Lake Michigan crustacean zooplankton samples collected between 1974 and 1979. First, Folsom <b>subsamples</b> were observed to be significantly (p < 0. 05) more variable than expected from the random model of <b>subsampling.</b> Second, a random effects ANOVA model was used to compare fractions of the total variance in density estimates that were attributable to <b>subsampling</b> and sampling phases of a specially designed study. Departures from randomness in <b>subsampling</b> were sufficiently small that an analysis of optimal allocation of effort between <b>subsampling</b> and sampling phases, based on the ANOVA model, indicated that only one to three <b>subsamples</b> needed to be examined per sample...|$|R
25|$|After <b>subsampling,</b> each channel must {{be split}} into 8×8 blocks. Depending on chroma <b>subsampling,</b> this yields Minimum Coded Unit (MCU) blocks of size 8×8 (4:4:4– no <b>subsampling),</b> 16×8 (4:2:2), or most {{commonly}} 16×16 (4:2:0). In video compression MCUs are called macroblocks.|$|R
5000|$|In a {{rarefied}} sample we {{have chosen}} a random <b>subsample</b> n from the total N items. The relevance of a rarefied sample is that some groups may now be necessarily absent from this <b>subsample.</b> We therefore let: ...|$|E
5000|$|... {{the number}} of groups still present in the <b>subsample</b> of [...] "n" [...] itemsIt is true that [...] is less than K {{whenever}} at least one group is missing from this <b>subsample.</b> Therefore the rarefaction curve, [...] is defined as: ...|$|E
5000|$|The {{jackknife}} {{estimate of}} a parameter {{can be found}} by estimating the parameter for each <b>subsample</b> omitting the i-th observation. For example, if the parameter to be estimated is the population mean of x, we compute the mean [...] for each <b>subsample</b> consisting of all but the i-th data point: ...|$|E
40|$|We {{propose a}} method for image <b>subsampling</b> using {{feedforward}} articial neural networks (FANNs). Our method employs pattern matching in order to extract information {{about the presence of}} local edges. This information is then used to select the FANN desired output values during training. We show that, when applied to rst{order and high{order image <b>subsampling,</b> our method eectively <b>subsamples</b> both high detail and smooth image areas, consistently outperforming traditional lowpass ltering and <b>subsampling</b> (LPFS) methods. 1 Introduction Image <b>subsampling</b> has a strong impact on performance and complexity in many image and video processing applications [1]{[3]. Traditional methods for image <b>subsampling</b> are based on lowpass ltering followed by <b>subsampling</b> (LPFS). By lowpass ltering (LPF) the image, most of the high frequency information is permanently lost. Moreover, the LPF stage has generally high computational demands. To simultaneously minimize the information loss while maintaining hi [...] ...|$|R
40|$|In {{this paper}} we {{investigate}} {{the possibility of}} the application of <b>subsampling</b> procedure for testing cointegration relations in large multivariate systems. The <b>subsampling</b> technique is applied to overcome the difficulty of nonstandard distribution and nuisance parameters in testing for cointegration rank without an explicitly formulated structural model. The contribution in this paper is twofold: theoretically this paper shows that the <b>subsampling</b> testing procedure is consistent and has asymptotically power 1;practically this paper demonstrates that the <b>subsampling</b> procedure can be applied to determine the cointegration rank in large scale models, where the standard procedures hits already its limit. For empirical relevant cases our simulation studies show that centered <b>subsampling</b> improves decisively the performance of <b>subsampling</b> test procedure and makes it applicable also for cases when the number of independent stochastic trends are very large. Cointegration, Large Systems, Nonparametric Tests, <b>Subsampling...</b>|$|R
50|$|After <b>subsampling,</b> each channel must {{be split}} into 8×8 blocks. Depending on chroma <b>subsampling,</b> this yields Minimum Coded Unit (MCU) blocks of size 8×8 (4:4:4 - no <b>subsampling),</b> 16×8 (4:2:2), or most {{commonly}} 16×16 (4:2:0). In video compression MCUs are called macroblocks.|$|R
5000|$|Fit a {{logistic}} regression {{model to the}} <b>subsample</b> , obtaining the unadjusted estimates [...]|$|E
50|$|Studies of the {{mortality}} of A-bomb survivors. I. Plan {{of study and}} mortality in the medical <b>subsample</b> (selection I), 1950-1958.|$|E
5000|$|Extract a 100-word {{passage from}} the selection. If the {{material}} is long, take a <b>subsample</b> from the beginning, middle, and end.|$|E
50|$|Nair and Shrivastava in 1942 {{suggested}} a similar idea but instead advocated dividing the sample into three equal parts before calculating {{the means of}} the <b>subsamples.</b> Brown and Mood in 1951 proposed the idea of using the medians of two <b>subsamples</b> rather the means. Tukey combined these ideas and recommended dividing the sample into three equal size <b>subsamples</b> and estimating the line based on the medians of the <b>subsamples.</b>|$|R
40|$|In {{this paper}} we {{investigate}} {{the possibility of}} the application of <b>subsampling</b> procedure for testing cointegration relations in large multivariate systems. The <b>subsampling</b> technique is applied to overcome the difficulty of nonstandard distribution and nuisance parameters in testing for cointegration rank without an explicitly formulated structural model. The contribution in this paper is twofold: theoretically this paper shows that the <b>subsampling</b> testing procedure is consistent and asymptotically most powerful; practically this paper demonstrates that the <b>subsampling</b> procedure can be applied to determine the cointegration rank in large scale models, where the standard procedures hits already its limit. Especially for the cases of few stochastic trends in a system, the <b>subsampling</b> procedure shows robust and reliable results. Cointegration, Large System, Nonparametric Tests, <b>Subsampling,</b> PPP...|$|R
40|$|All known robust {{location}} and scale estimators with high breakdown point for multivariate sample's are very expensive to compute. In practice, this computation {{has to be}} carried out using an approximate <b>subsampling</b> procedure. In this work we describe an alternative <b>subsampling</b> scheme, applicable to both the Stahel-Donoho estimator and the estimator based on the Minimum Volume Ellipsoid, with the property that the number of <b>subsamples</b> required is substantially reduced with respect to the standard <b>subsampling</b> procedures used in both cases. We also discuss some bias and variability properties of the estimator obtained from the proposed <b>subsampling</b> process...|$|R
50|$|Anthropometric: Interviewers {{measured}} weight, height; waist, hip, and calf circumference, knee length, and timed one-leg {{stands for}} a random <b>subsample</b> (20%) of respondents.|$|E
5000|$|Some {{software}} like SAS {{will use}} a predictive Chow test when {{the size of a}} <b>subsample</b> is less than the number of regressors.|$|E
50|$|The grand mean is {{the mean}} of the means of several subsamples, as long as the subsamples have the same number of data points. For example, {{consider}} several lots, each containing several items. The items from each lot are sampled for a measure of some variable and the means of the measurements from each lot are computed. The mean of the measures from each lot constitutes the <b>subsample</b> mean. The mean of these <b>subsample</b> means is then the grand mean.|$|E
40|$|This paper {{considers}} {{confidence intervals}} {{based on the}} <b>subsampling</b> approach for the largest root in possibly unstable AR(p) models with stationary exogenous regressors. The <b>subsampling</b> approach proposed by Politis and Romano (Annals of Statistics, 1994), is {{able to deal with}} discontinuities in the asymptotic distribution of a (studentized) estimator. We show that inference based on <b>subsampling</b> intervals is asymptotically correct. The finite-sample behavior of the <b>subsampling</b> approach is investigated by a small simulation study. It turns out that an equal-tailed <b>subsampling</b> interval based on a calibration rule leads to accurate inference in samples of size 100...|$|R
40|$|In {{a recent}} paper we have {{introduced}} {{the class of}} realised kernel estimators of the increments of quadratic variation {{in the presence of}} noise. We showed that this estimator is consistent and derived its limit distribution under various assumptions on the kernel weights. In this paper we extend our analysis, looking at the class of <b>subsampled</b> realised kernels and we derive the limit theory for this class of estimators. We find that <b>subsampling</b> is highly advantageous for estimators based on discontinuous kernels, such as the truncated kernel. For kinked kernels, such as the Bartlett kernel, we show that <b>subsampling</b> is impotent, in the sense that <b>subsampling</b> has no effect on the asymptotic distribution. Perhaps surprisingly, for the efficient smooth kernels, such as the Parzen kernel, we show that <b>subsampling</b> is harmful as it increases the asymptotic variance. We also study the performance of <b>subsampled</b> realised kernels in simulations and in empirical work. Bipower Variation, Long Run Variance Estimator, Market Frictions, Quadratic Variation, Realised Kernal, Realised Variance, <b>Subsampling...</b>|$|R
40|$|A simple {{adaptive}} subsampling/de-subsampling {{scheme for}} video delivery over the QoS-aware residential gateway (QRG) during network congestion is presented. The network performs better by transmitting fewer data by fewer bits required during network congestion. The proposed <b>subsampling</b> scheme can be adaptively adjust with {{variable compression ratio}} using different kind of <b>subsampling</b> algorithms according to the feedback information from the QRG; no <b>subsampling</b> is allowed in normal operation, which provided sufficient network bandwidth; intra-frame <b>subsampling</b> scheme is adopted for medium, and inter-frame <b>subsampling</b> scheme for heavily traffic during network congestion. The receiver adaptively de-subsamples the <b>subsampled</b> frame with a simple linear interpolation to reconstruct the image-frame. QRG is experimentally implemented in a FDIXP 425 network-based embedded system on a Linux platform. Experimental results prove the proposed scheme successfully performs across QRG...|$|R
