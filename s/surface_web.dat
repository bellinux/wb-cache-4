100|131|Public
50|$|According to one source, , Google's {{index of}} the <b>surface</b> <b>web</b> {{contains}} about 14.5 billion pages.|$|E
50|$|The {{reciprocal}} {{term for}} an encrypted darknet is clearnetor <b>surface</b> <b>web</b> {{when referring to}} search engine indexable content.|$|E
50|$|The {{term has}} been used synonymously with the {{non-traditional}} search engine indexable <b>surface</b> <b>web</b> due to the historical overlap when darknets were all {{a part of the}} deep web.|$|E
50|$|A. labyrinthica build {{flat plate}} <b>surface</b> <b>webs</b> {{connected}} to funnel-shaped retreats similar to labyrinths, which are typically constructed between low lying grass and vegetation. These webs {{can be at}} ground level, or up to 1.5 m from the ground, however, the majority are found approximately 60 cm off of the ground. These spiders are fairly common in Europe and Central Europe, and are typically concentrated in areas near forests and low lying vegetation, {{as well as in}} dry grasslands.|$|R
5000|$|In Chapter 14, Hypertext Spanning the Internet:WWW, we'll discuss ...The World Wide <b>Web.</b> On the <b>surface,</b> the <b>Web</b> {{looks like}} a {{variation}} on GOPHER (p. 189).|$|R
2500|$|Notable {{previously}} unheard audience recordings that <b>surfaced</b> on the <b>Web</b> from 2000's: ...|$|R
5000|$|Through both {{deep web}} (password protected, encrypted) and dark web (special portal browsers) markets, {{participants}} can trade and transact illegal substances, including wildlife. However {{the amount of}} activity is still negligible compared to the amount on the open or <b>surface</b> <b>web.</b> As stated in an examination of search engine key words relating to wildlife trade {{in an article published}} by Conservation Biology, [...] "This negligible level of activity related to the illegal trade of wildlife on the dark web relative to the open and increasing trade on the <b>surface</b> <b>web</b> may indicate a lack of successful enforcement against illegal wildlife trade on the surface web." ...|$|E
50|$|In {{the year}} 2001, Michael K. Bergman said how searching on the Internet can be {{compared}} to dragging a net {{across the surface of the}} ocean: a great deal may be caught in the net, but there is a wealth of information that is deep and therefore missed. Most of the web's information is buried far down on sites, and standard search engines do not find it. Traditional search engines cannot see or retrieve content in the deep web. The portion of the web that is indexed by standard search engines is known as the <b>surface</b> <b>web.</b> , the deep web was several orders of magnitude larger than the <b>surface</b> <b>web.</b> An analogy of an iceberg used by Denis Shestakov represents the division between <b>surface</b> <b>web</b> and deep web respectively:It is impossible to measure, and harsh to put estimates on the size of the deep web because the majority of the information is hidden or locked inside databases. Early estimates suggested that the deep web is 400 to 550 times larger than the <b>surface</b> <b>web.</b> However, since more information and sites are always being added, it can be assumed that the deep web is growing exponentially at a rate that cannot be quantified.Estimates based on extrapolations from a study done at University of California, Berkeley in 2001 speculate that the deep web consists of about 7.5 petabytes. More accurate estimates are available for the number of resources in the deep web: research of He et al. detected around 300,000 deep web sites in the entire web in 2004, and, according to Shestakov, around 14,000 deep web sites existed in the Russian part of the Web in 2006.|$|E
50|$|The <b>Surface</b> <b>Web</b> (also {{called the}} Visible Web, Indexed Web, Indexable Web or Lightnet) is {{that portion of}} the World Wide Web that is readily {{available}} to the general public and searchable with standard web search engines. It is the opposite of the deep web.|$|E
5000|$|... #Caption: QPweb1.0: an {{interactive}} <b>web</b> <b>surface</b> to upload and analyse your data through the web ...|$|R
40|$|The paper {{deals with}} the impact of {{internal}} geometry and micro-geometry of functional surfaces of the cone roller bearing on internal resistance—friction of roller bearings. It describes an ideal point of contact at intersection of cone axes and raceway values, as well as the necessary values of micro-geometry, which enable the use of the bearings under the most demanding installations in the automotive industry— in differential gears, or for installation of pinions of differential gears and cog wheels in gearboxes. It describes optimisation (modification) of production of supporting face of the inner ring of the bearing, the purpose of which is to ensure this perfect point of contact, as well as required values of microgeometry of functional <b>surfaces.</b> <b>Web</b> of Science 825 - 81106109...|$|R
5000|$|... 75% of new key {{information}} sources that <b>surface</b> on the <b>Web</b> are not {{passed on to}} users within the year (agility) ...|$|R
50|$|Repeated {{opening of}} the silky flaps create an {{impression}} in the sand which can be likened to a cleft hoof imprint or, in some species, {{the shape of a}} four-leaf clover. Sticky silk threads along the margins of the silk mats entangle small arthropods that venture too close. The <b>surface</b> <b>web</b> structures are susceptible to damage by strong winds or heavy rainfall, and the loss of a web may prove fatal to the female.|$|E
50|$|To {{discover}} {{content on}} the web, search engines use web crawlers that follow hyperlinks through known protocol virtual port numbers. This technique {{is ideal for}} discovering content on the <b>surface</b> <b>web</b> but is often ineffective at finding deep web content. For example, these crawlers do not attempt to find dynamic pages that {{are the result of}} database queries due to the indeterminate number of queries that are possible. It has been noted that this can be (partially) overcome by providing links to query results, but this could unintentionally inflate the popularity for a member of the deep web.|$|E
5000|$|Commercial {{search engines}} have begun {{exploring}} alternative methods to crawl the deep web. The Sitemap Protocol (first developed, and introduced by Google in 2005) and mod oai are mechanisms that allow search engines and {{other interested parties}} to discover deep web resources on particular web servers. Both mechanisms allow web servers to advertise the URLs that are accessible on them, thereby allowing automatic discovery of resources that are not directly linked to the <b>surface</b> <b>web.</b> Google's deep web surfacing system computes submissions for each HTML form and adds the resulting HTML pages into the Google search engine index. The surfaced results account for a thousand queries per second to deep web content. In this system, the pre-computation of submissions is done using three algorithms: ...|$|E
40|$|Web {{traction}} over rollers {{is known}} to deteriorate due to air entrainment at high web speeds. In this paper, a general model is presented to predict the traction capability of an impermeable web over a smooth roller. The model considers {{the effects of the}} web and roller speeds, roller radius, combined roughness of the two <b>surfaces,</b> <b>web</b> tension and thickness, friction coefficient, and compressible air bearing. The change of tension N∆ due to mechanical slip between the roller and the web is calculated by a simultaneous solution of the in-plane and out-of-plane equilibrium of the web. The problem is non-dimensionalized and the effects of nine of the eleven non-dimensional parameters on traction are investigated for a range of values. Formulas involving the non-dimensional parameters for the traction capability are presented in two variable polynomial forms. Traction Between a Web and a Smooth Roller 10 / 15 / 02...|$|R
40|$|We present ISENS, a distributed, end-to-end, ontology-based {{information}} integration system. In {{response to}} a user’s query, our system is capable of retrieving facts from data sources that {{are found in the}} <b>surface</b> Semantic <b>Web</b> {{as well as in the}} Semantic Deep Web. Furthermore, it retrieves facts from sources where the data is not directly described in terms of the query ontology. Instead, its ontology can be translated from the query ontology using mapping axioms. In our solution, we use the concept of source relevance to summarize the content of a data source. Our system can then use this information to select the needed sources to an-swer a given query. Source relevance is general enough that it can be used with both the <b>surface</b> Semantic <b>Web</b> and the Semantic Deep Web. In this paper, we show how we have incorporated three particular Deep Web data sources into our system to enable answering queries by composing in-formation from the integrated sources. ...|$|R
50|$|Largest {{parts of}} the earth <b>surface</b> {{displayed}} on <b>web</b> mapping services like Google Maps/Google Earth, MSN Maps or Yahoo Maps are based on enhanced and color balanced Landsat 7 imagery.|$|R
40|$|This paper {{presents}} the results of a survey conducted in 2005 – 2006 re-garding the extent of use of digital resources by students and researchers in fi ve universities in Israel and the ratio of use between authorized elec-tronic information resources provided by academic libraries, and the <b>Surface</b> <b>Web.</b> About 80 percent of respondents reported a high or very high frequency of use of the <b>Surface</b> <b>Web</b> for seeking information for their study or research. In contrast, only about 28 percent of the respondents reported high or very high use of academic e-journals, 40 percent high or very high use of digital databases, and only about 13 percent high or very high use of e-books. A situation in which academics use the <b>Surface</b> <b>Web</b> two or three times more frequently than more authoritative digital information sources provided by their library indicates a severe problem related to the quality of information used, which may severely harm the quality and credibility of research based upon it. The survey fi ndings are worrying since much <b>Surface</b> <b>Web</b> information is not reliable or author-itative. The current research does not indicate what causes students and researchers to depend so heavily on <b>Surface</b> <b>Web</b> information for their research, but {{it is reasonable to assume}} that it derives mainly from the ease and convenience of using Internet search engines...|$|E
30|$|The fetcher {{component}} is configured appropriately {{in order to}} support fetching Web pages both from the <b>Surface</b> <b>Web</b> and the darknets of the Dark Web.|$|E
40|$|Web {{search engines}} use web {{crawlers}} that follow hyperlinks. This technique {{is ideal for}} discovering resources on the <b>surface</b> <b>web</b> but is often ineffective at finding deep web resources. The <b>surface</b> <b>web</b> is {{the portion of the}} World Wide Web that is indexed by conventional search engines whereas the deep web refers to web content that {{is not part of the}} <b>surface</b> <b>web.</b> The deep web represents a major gap in the coverage of web search engines as believed to be of a very high quality and is estimated to be several orders of magnitude larger than the <b>surface</b> <b>Web.</b> Understanding the nature of the deep web resources as being massively increased give us a conclusion that to be efficiently explored need an approach based on two main concepts, the first concept is to solve the problem from the side of web servers and the second concept is to automate the discovery process. In this paper we developed and implemented the Host List Protocol model that is depending on such approach to discover hidden web hosts and provide a way to be indexed through web search engines. Key words: Deep web, hidden web, invisible web, search engines, web crawlers, host list protocol, deep web resources, discover web resources, discover hidden web 1...|$|E
5000|$|Quantitative Parasitology on the Web 1.0 is an {{interactive}} <b>web</b> <b>surface</b> {{that comes with}} an extended functionality. It works with Windows, Linux, Mac etc. operating systems and Explorer, Firefox, Google Chrome etc. browsers.|$|R
40|$|We discuss our {{experience}} in creating scalable systems for distributing and rendering gigantic 3 D <b>surfaces</b> on <b>web</b> environments and common handheld devices. Our methods {{are based on}} compressed streamable coarse-grained multiresolution structures. By combining CPU and GPU compression technology with our multiresolution data representation, {{we are able to}} incrementally transfer, locally store and render with unprecedented performance extremely detailed 3 D mesh models on WebGL-enabled browsers, as well as on hardware-constrained mobile devices...|$|R
5000|$|Spider-Woman {{possesses}} superhuman strength, speed, stamina, agility, and reflexes. She also {{possesses the}} ability to spin a [...] "psi-web" [...] of psionic energy between two <b>surfaces.</b> This <b>web,</b> once solidified, possesses sufficient tensile strength to support a 10-ton weight. It remains in effect for up to approximately 1 hour. She can also project and release sufficient psionic energy through her hands and feet to enable her to walk on walls and ceilings.|$|R
40|$|Use of the {{internet}} as a trade platform has resulted in a shift in the illegal wildlife trade. Increased scrutiny of illegal wildlife trade has led to concerns that online trade of wildlife will move onto the dark web. To provide a baseline of illegal wildlife trade on the dark web, we downloaded and archived 9852 items (individual posts) from the dark web, then searched these based on a list of 121 keywords associated with illegal online wildlife trade, including 30 keywords associated with illegally traded elephant ivory on the <b>surface</b> <b>web.</b> Results were compared with items known to be illegally traded on the dark web, specifically cannabis, cocaine, and heroin, to compare the extent of the trade. Of these 121 keywords, 4 resulted in hits, of which only one was potentially linked to illegal wildlife trade. This sole case was the sale and discussion of Echinopsis pachanoi (San Pedro cactus), which has hallucinogenic properties. This negligible level of activity related to the illegal trade of wildlife on the dark web relative to the open and increasing trade on the <b>surface</b> <b>web</b> may indicate a lack of successful enforcement against illegal wildlife trade on the <b>surface</b> <b>web...</b>|$|E
40|$|Nowadays, most of {{the search}} engines are {{competing}} to index {{as much of the}} <b>Surface</b> <b>Web</b> as possible with leaving a lurch at the OAI content (pdf documents), which holds a huge amount of information than <b>surface</b> <b>web.</b> In this paper, a novel framework for OAI-PMH based Crawler is being proposed that uses agents to extract the metadata about the OAI resources and store them in a repository which is later on queried through the OAI-PMH layer to generate the XML pages containing the metadata. These pages are further added to the search engines repository for indexing that makes in turn increases therelevancy of Search Engine. Agents are being used to parallelizethe whole process so that metadata extraction from multiple resources can be carried out simultaneously...|$|E
40|$|Web {{is a wide}} term which mainly {{consists}} of <b>surface</b> <b>web</b> and hidden web. One can easily access the <b>surface</b> <b>web</b> using traditional web crawlers, {{but they are not}} able to crawl the hidden portion of the web. These traditional crawlers retrieve contents from web pages, which are linked by hyperlinks ignoring the information hidden behind form pages, which cannot be extracted using simple hyperlink structure. Thus, they ignore large amount of data hidden behind search forms. This paper emphasizes on the extraction of hidden data behind html search forms. The proposed technique makes use of semantic mapping to fill the html search form using domain specific database. Using semantics to fill various fields of a form leads to more accurate and qualitative data extraction. Comment: 12 pages, 10 figure...|$|E
50|$|On February 18, 2009 {{the opening}} scenes <b>surfaced</b> on the <b>web</b> for the Wall Street Warriors Season 3. Even {{though it was}} a very brief video it seems to have been made during the peak of the October 2008 stockmarket turmoil.|$|R
40|$|Consider a {{commercial}} Web site for {{an insurance company}} (Figure 1). The <b>web</b> site <b>surfaces</b> <b>Web</b> pages that allow customers, agents, employees and partners to interact with applications. The site also exposes a Web service interface using WSDL [WSDL] to document interfaces and a WS-Interoperability [WSI] binding for access from partner systems. This scenario introduces several important use cases for rule technology. These include enabling interoperability, model IBM 1 Ferguson & Linehandriven architecture and development, implementing services using rules, customization of process and services, portability, and event analysis. We explore these concepts below. Interoperability WS-Interoperability protocols provide support for runtime interoperability. There is also a need for interoperability between “tools, ” based on a common way to describe service interfaces, the data that is being interchanged and related metadata. WSDL provides basic support for defining the interfaces to services. WS-Policy [WSPO] provides support for documenting Web service protocol extensions for various qualities of service, for example security [WSSE] or reliable messaging [WSRM]. WS-BPEL [BPEL] defines abstract processes that describe valid sequences of invoking service operation. Ontology languages [OWL] allow annotations of Web services and XM...|$|R
40|$|Keywords-surface defect inspection; SWT; image fusion; image {{segmentation}} Abstract—Typical characteristics of web manufacturing process,when {{compared with other}} sheet or flat product manufacturing, are the large value of web width and production speed. So {{the development of new}} and efficient algorithm invokes the interests of many researchers. This paper describes a novel approach based on stationary wavelet transform for the segmentation of <b>web</b> <b>surface</b> defect. The segmentation is performed firstly by decomposing the gray level image into sub-band images and then by an image fusion scheme for the sub-images. Compared with orthogonal wavelet transform (OWT), the notable advantage of stationary wavelet transform (SWT) is its shift invariance. These properties are especially important for defect detection. Image fusion makes full use of available information in each sub-band images to obtain better output results when compared with ordinary image enhancement. Experimental results demonstrate the validity of our method. The proposed method is targeted for <b>web</b> <b>surface</b> defect inspection but has the potential for broader application areas such as steel, wood and fabric defect detection. With the development of high performance signal processors, spectral analysis or a combination of statistical and spectral analysis would be the trend of <b>web</b> <b>surface</b> defect inspection...|$|R
40|$|Abstract—Nowadays, most of {{the search}} engines are {{competing}} to index {{as much of the}} <b>Surface</b> <b>Web</b> as possible with leaving a lurch at the OAI content (pdf documents), which holds a huge amount of information than <b>surface</b> <b>web.</b> In this paper, a novel framework for OAI-PMH based Crawler is being proposed that uses agents to extract the metadata about the OAI resources and store them in a repository which is later on queried through the OAI-PMH layer to generate the XML pages containing the metadata. These pages are further added to the search engines repository for indexing that makes in turn increases the relevancy of Search Engine. Agents are being used to parallelize the whole process so that metadata extraction from multiple resources can be carried out simultaneously. Keywords-OAI-PMH; Agents; Surface web;Hidden Web I...|$|E
40|$|Abstract. Where {{are all the}} {{semantic}} Web services today? In this paper, we briefly provide the preliminary results of searching the <b>surface</b> <b>Web</b> and the prominent citeseer archive as one element of the deep Web for publicly available semantic Web service descriptions written in OWL-S, WSML, WSDL-S or SAWSDL {{by means of the}} specialized meta-search engine Sousuo 1. 4...|$|E
40|$|Abstract — World Wide Web is {{developing}} rapidly, {{there are large}} number of Web databases available for users to access. This fast development of the World Wide Web has changed {{the way in which}} information is managed and accessed. So the Web can be divided into the <b>Surface</b> <b>Web</b> and the Deep Web. <b>Surface</b> <b>Web</b> refers to the Web pages that are static and linked to other pages, while Deep Web refers to the Web pages created dynamically as the result of specific search. This literature paper focuses on querying the Deep Web. Deep Web refers to the databases accessible through query interfaces on the World Wide Web. A Deep Web query system presents to users a single interface for querying multiple Web databases in a domain such as airline booking and extracts the relevant information from different web databases sources, and then returns results for users...|$|E
50|$|Because of {{specific}} music and film licensing, Moonwalker {{has never been}} officially released by Warner Bros, or any other studio, in North America on DVD or Blu-ray. However imported region-free DVD copies of the film from Japan or China have <b>surfaced</b> throughout the <b>web.</b>|$|R
40|$|Designing Beneath the <b>Surface</b> of the <b>Web</b> At {{its most}} basic, the web allows for two modes of access: visual and non-visual. For the most part, our design {{attention}} is focused on making decisions that affect the visual, or surface, layer — colors and type, screen dimensions, fixed or flexible layouts. However, much {{of the power of the}} technology lies beneath the surface, in the underlying code of the page. There, in the unseen depths of the page code, we make decisions that influence how well, or poorly, our pages are read and interpreted by software. In this paper, we shift our attention beneath the <b>surface</b> of the <b>web</b> and focus on design decisions that affect nonvisual access to web pages...|$|R
40|$|DE 102009054605 B 3 UPAB: 20110624 NOVELTY - The open-cellular, {{titanium}} {{metal foam}} comprises a three-dimensional network of webs connected to each other, whose covering surface (4) consists of titanium {{and has a}} roughness of less than 2 mu m in smooth <b>web</b> <b>surface.</b> The metal foam is produced by a Schwartzwalder method. The web has a smooth surface with roughness of less than 1 mu m. The surface area of the web consists of 85 % of titanium, where the residue is titanium compound and/or titanium alloy component. The titanium-alloy is Ti 6 Al 7 Nb and Ti 6 Al 4 V. The web is present in interior cavity (6) of the foam. DETAILED DESCRIPTION - The open-cellular, titanium metal foam comprises a three-dimensional network of webs connected to each other, whose covering surface (4) consists of titanium and has a roughness of less than 2 mu m in smooth <b>web</b> <b>surface.</b> The metal foam is produced by a Schwartzwalder method. The web has a smooth surface with roughness of less than 1 mu m. The surface area of the web consists of 85 % of titanium, where the residue is titanium compound and/or titanium alloy component. The titanium-alloy is Ti 6 Al 7 Nb and Ti 6 Al 4 V. The web is present in interior cavity (6) of the foam and has cracks (7) and pores (8) in its wall. The covering <b>surface</b> of the <b>web</b> has pores, whose portion is less than 5 % related to the surface. The pores in the covering <b>surface</b> of the <b>web</b> have maximum dimension of less than 1 mu m. USE - Open-cellular, titanium metal foam useful for medical implants. ADVANTAGE - The open-cellular, titanium metal foam can be manufactured with improved roughness...|$|R
