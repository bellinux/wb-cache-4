37|10000|Public
50|$|Lease {{and release}} is {{literally}} the lease (tenancy) of non-tenanted property by its owner {{followed by a}} release (relinquishment) of the landlord's interest in the property. This <b>sequence</b> <b>of</b> <b>transactions</b> was commonly used to transfer full title to real estate under real property law. Lease and release was a mode of conveyance of freehold estates formerly common in England and in New York for tax avoidance and speed. Between its parties it achieves the same outcome as a deed of grant/transfer/conveyance.|$|E
5000|$|A Java {{application}} {{which acts}} as a workload driver. It is configurable and can spawn 1 to n parallel threads to simulate concurrent database users. Each user connects to the database and executes a random <b>sequence</b> <b>of</b> <b>transactions</b> defined in the workload. Parameter markers in the transactions are replaced by real values that are drawn from random value distributions. The workload driver collects and reports performance metrics, such as the transaction throughput as well as minimum, maximum and average response times.|$|E
50|$|The {{journaling}} format {{consists of}} a <b>sequence</b> <b>of</b> <b>transactions,</b> or updates. An update contains 4 types of blocks: a transaction header block, a sequence of data blocks, a corresponding sequence of fragment tables, and a sequence of index blocks. A transaction header block contains the transaction date and a pointer skipping over the data blocks to allow the archive index to be read quickly. The data blocks contain a sequence of file fragments compressed together. The fragment tables give the size and SHA-1 hash of each fragment. The index blocks contain a list of edits to the global archive index. An edit is either a file update or a file deletion. An update includes a file name, last modified date, attributes, {{and a list of}} fragment pointers into the current and previous transactions. Fragments may be shared by more than one file. A deletion does not remove any data from the archive, but rather indicates that the file is not to be extracted unless the archive is rolled back to an earlier date.|$|E
5000|$|Game {{analysis}} - repeating <b>sequences</b> <b>of</b> <b>transactions</b> {{that lead}} to a result subconsciously agreed to by the parties involved in the game ...|$|R
25|$|Each {{participant}} on the EIB has one 16 byte read {{port and}} one 16 byte write port. The limit {{for a single}} participant is {{to read and write}} at a rate of 16 byte per EIB clock (for simplicity often regarded 8 byte per system clock). Note that each SPU processor contains a dedicated DMA management queue capable <b>of</b> scheduling long <b>sequences</b> <b>of</b> <b>transactions</b> to various endpoints without interfering with the SPU's ongoing computations; these DMA queues can be managed locally or remotely as well, providing additional flexibility in the control model.|$|R
40|$|Inferring user {{characteristics}} such as demographic attributes is {{of the utmost importance}} in many user-centric applications. Demographic data is an enabler of personalization, identity security, and other applications. Despite that, this data is sensitive and often hard to obtain. Previous work has shown that purchase history can be used for multi-task prediction of many demographic fields such as gender and marital status. Here we present an embedding based method to integrate multifaceted <b>sequences</b> <b>of</b> <b>transaction</b> data, together with auxiliary relational tables, for better user modeling and demographic prediction. Comment: IFUP 2018 (WSDM workshop...|$|R
3000|$|Proving the {{correctness}} of read-only transactions is more subtle. To be serializable, read-only transactions {{must read}} values from a consistent {{view of the}} database that {{is the result of}} a serial execution of some finite <b>sequence</b> <b>of</b> <b>transactions.</b> Let i [...]...|$|E
40|$|Abstract. Many {{recently}} developed information visualization techniques are radial variants of originally Cartesian visualizations. Almost {{none of these}} radial variants have been evaluated {{with respect to their}} benefits over their original visualizations. In this work we compare a radial and a Cartesian variant of a visualization tool for sequences of transactions in information hierarchies. The Timeline Trees (TLT) approach uses a Cartesian coordinate system to represent both the hierarchy and the <b>sequence</b> <b>of</b> <b>transactions</b> whereas the TimeRadarTrees (TRT) technique is the radial counterpart which makes use of a radial tree, as well as circle slices and sectors to show the <b>sequence</b> <b>of</b> <b>transactions.</b> For the evaluation we use both quantitative as well as qualitative evaluation methods including eye tracking. ...|$|E
40|$|A <b>sequence</b> <b>of</b> <b>transactions</b> {{represents}} {{a complex and}} multi dimensional type of data. Feature construction {{can be used to}} reduce the data´s dimensionality to find behavioural patterns within such sequences. The patterns can be expressed using the blue prints of the constructed relevant features. These blue prints can then be used for real time classification on other sequences...|$|E
40|$|A basic if {{neglected}} step in monetary {{theory is}} to show that a given amount of money will enable all transactions to take place in money. But if the money advanced is no more than current costs, how are profits to be realized in money? The answer requires tracing the pattern of circulation, which, in turn depends on the structure of production and distribution. The sectors have different patterns of interdependence, so imply different <b>sequences</b> <b>of</b> <b>transactions.</b> Borrowing is costly, so the amount of money must be minimized. These issues have been brought into focus in the interesting article of J-F Renaud. ...|$|R
40|$|The UniTree product {{as it is}} {{released}} emits a rather large amount of logging information into various log files, but this data typically is meant for operator information in difficult operational situations or directly for debugging purposes. It is strongly advised that the existing logging functionality in the standard release is advanced by adding messages with relevant parameters for every major event {{in the course of}} executing file or device oriented requests as described. This upgrade would be relatively easy in terms of implementation effort. The benefit is a complete <b>sequence</b> <b>of</b> <b>transaction</b> descriptions generated by external user requests or by internal administration commands. This collection <b>of</b> <b>transaction</b> records can be used for intensive statistics and performance evaluations. It is furthermore a perfect source to drive realistic system simulations to study the effects of possible hardware or software changes...|$|R
5000|$|A {{long-lived}} transaction is {{a transaction}} that spans multiple database transactions. The transaction is considered [...] "long-lived" [...] because its boundaries must, by necessity of business logic, extend past a single database transaction. A long-lived transaction {{can be thought}} <b>of</b> as a <b>sequence</b> <b>of</b> database <b>transactions</b> grouped to achieve a single atomic result.|$|R
40|$|AbstractThis paper {{presents}} a software model checking algorithm that combats state explosion by decomposing each thread's execution into a <b>sequence</b> <b>of</b> <b>transactions</b> that execute atomically. Our algorithm infers transactions using {{the theory of}} reduction, and supports both left and right movers, thus yielding larger transactions and fewer context switches than previous methods. Our approach uses access predicates to support {{a wide variety of}} synchronization mechanisms. In addition, we automatically infer these predicates for programs that use lock-based synchronization...|$|E
40|$|This {{research}} {{presents a}} new technique for plagiarism detection using sequential pattern mining titled EgyCD. Over the last decade many techniques and tools for software clone detection have been proposed such as textual approaches, lexical approaches, syntactic approaches, semantic approaches …, etc. In this paper, the research explores the potential of data mining techniques in plagiarism detection. In particular, the research proposed a plagiarism technique based on sequential pattern mining (SPM), words/statements are treated as a <b>sequence</b> <b>of</b> <b>transactions</b> processed by the SPM algorithm to find frequent itemsets. The research submits an experiment to discover copy/paste in the text source and it gave good results in a reasonable and acceptable time...|$|E
40|$|Abstract — In {{this paper}} will try to explain how Hidden Markov Model or HMM is useful {{technique}} {{for the purpose of}} banking application OR we can show how this technique is better for security banking application. As online as well as physical banking fraud increases day by day so fraud detection is need of today and in future. Here we use the <b>sequence</b> <b>of</b> <b>transactions</b> and compare it with their past or historical transactions and this processing will do by using our model called as Hidden Markov Model (HMM). So our model will decide or detect that this coming transaction is fraud or not based on their past behaviour or past data. Also in this process will try to ensure that genuine transactions are not rejected...|$|E
40|$|Credit {{networks}} are an abstraction for modeling trust between agents in a network. Agents {{who do not}} directly trust each other can transact through exchange of IOUs (obligations) along a chain of trust in the network. Credit {{networks are}} robust to intrusion, can enable transactions between strangers in exchange economies, and have the liquidity to support a high rate <b>of</b> <b>transactions.</b> We study the formation of such networks when agents strategically decide how much credit to extend each other. When each agent trusts a fixed set of other agents, and transacts directly only with those it trusts, the formation game is a potential game and all Nash equilibria are social optima. Moreover, the Nash equilibria of this game are equivalent in a very strong sense: the <b>sequences</b> <b>of</b> <b>transactions</b> that can be supported from each equilibrium credit network are identical. When we allow transactions over longer paths, the game may no...|$|R
40|$|We {{address the}} problem of formalizing the {{evolution}} of a database under the effect <b>of</b> an arbitrary <b>sequence</b> <b>of</b> update <b>transactions.</b> We do so by appealing to a first order representation language called the situation calculus, which is a standard approach in artificial intelligence to the formalization of planning problems. We formalize database transactions {{in exactly the same way}} as actions in the artificial intelligence planning domain. This leads to a database version of the frame problem in artificial intelligence. We provide a solution to the frame problem for a special, but substantial, class <b>of</b> update <b>transactions.</b> We next briefly describe some of the results obtained within this axiomatization. Specifically, we provide procedures for determining whether a given <b>sequence</b> <b>of</b> update <b>transactions</b> is legal, and for query evaluation in an updated database. These procedures have the nice property that they appeal to theorem-proving only with respect to the initial database state. We a [...] ...|$|R
40|$|Transformation {{rules for}} (Flat) GHC {{programs}} are presented, which refine the previous rules proposed {{by one of}} the authors (Furukawa et al. 1987). The rules are based on unfolding/folding and are novel in that they are stated in terms of idempotent substitutions with preferred directions of bindings. They are more general than the old rules in that no mode system is assumed and that the rule of folding is included. The presentation of the rules suggests that idempotent substitutions with preferred directions of bindings are an appropriate tool for modeling information in (concurrent) logic programming. A semantic model is given which associates a multiset of goals with the set <b>of</b> possible finite <b>sequences</b> <b>of</b> <b>transactions</b> (via substitutions) with the multiset. A transformation preserves the set <b>of</b> <b>transaction</b> <b>sequences</b> that are made without the risk of the failure of unification. The model also deals with anomalous behavior such as the failure of unification and deadlock, so it can be s [...] ...|$|R
40|$|We {{introduce}} a new analysis of transaction costs that explicitly recognizes {{the importance of the}} timing of execution in assessing transaction costs. Time induces a risk/cost tradeoff. The price of immediacy results in higher costs for quickly executed orders while more gradual trading results in higher risk since the value of the asset can vary more over longer periods of time. We use a novel data set that allows a <b>sequence</b> <b>of</b> <b>transactions</b> to be associated with individual orders and measure and model the expected cost and risk associated with different order execution approaches. The model yields a risk/cost tradeoff that depends upon the state of the market and characteristics of the order. We show how to assess liquidation risk using the notion of liquidation value at risk (LVAR) ...|$|E
40|$|Abstract Resource {{reallocation}} problems aim {{to determine}} an allocation maximizing a given objective function. Numerous applications {{are based on}} the assumption of restricted contacts between entities but, up to now, studies have been based on unre-alistic contexts. Indeed, most of the time, agents are omniscient and/or have complete communication abilities, which are not plausible assumptions in many applications. A solution does not only consist in an optimal allocation, but in a sequence of trans-actions changing an initial allocation into an optimal solution. We show that the indi-vidual rationality does not allow the achievement of socially optimal allocations, and we propose a more suitable criterion: the sociability. Our method provides a <b>sequence</b> <b>of</b> <b>transactions</b> leading to an optimal allocation, with any restriction on agents ’ com-munication abilities. Provided solutions can be viewed as emergent phenomena...|$|E
40|$|In {{this paper}} {{we present a}} syntactical class of {{historical}} queries in databases, and an algorithm for answering them. More precisely, we consider the problem of answering queries about {{the evolution of the}} database along a <b>sequence</b> <b>of</b> <b>transactions.</b> To make the algorithm efficient, we introduce procedural notions of relevant transactions and tuples, that allow us to know which transactions to execute and which tuples to update. We start from Ray Reiter's formalism [14, 16] based on the situation calculus [11] for specifying database updates. This paper extends previous work by Aris Zakinthinos on historical queries [19], which is also based on Reiter's work. Our algorithms have been implemented in SCDBR [3, 2, 1], a computational system that is able to reason from and about database specifications written in Reiter's formalism. We discuss some implementation issues...|$|E
40|$|Online {{credit card}} fraud {{presents}} a significant {{challenge in the}} field of eCommerce. In 2012 alone, the total loss due to credit card fraud in the US amounted to $ 54 billion. Especially online games merchants have difficulties applying standard fraud detection algorithms to achieve timely and accurate detection. This paper describes the Special constrains of this domain and highlights the reasons why conventional algorithms are not quite effective to deal with this problem. Our suggested solution for the problem originates from the fields of feature construction joined with the field <b>of</b> temporal <b>sequence</b> data mining. We present Feature construction techniques, which are able to create discriminative features based on a <b>sequence</b> <b>of</b> <b>transaction</b> and are able to incorporate the time into the classification process. In addition to that, a framework is presented that allows for an automated and adaptive change of features in case the underlying pattern is changing...|$|R
40|$|The Author 2016. In many markets, {{goods flow}} from initial {{producers}} to final customers travelling through {{many layers of}} intermediaries and information is asymmetric. We study a dynamic model of bargaining in networks that captures these features. We show that the equilibrium price demanded over time is non-monotonic, but the <b>sequence</b> <b>of</b> <b>transaction</b> prices declines over time, {{with the possible exception}} of the last period. The price dynamic is, therefore, reminiscent of fire-sales and hot-potato trading. Traders who intermediate the object arise endogenously and make a positive profit. The profit-earning intermediaries are not necessarily traders with many connections; for the case of multilayer networks, they belong to the path that reaches the maximum number of potential buyers using the minimal number of intermediaries. This is not necessarily the path of the network that maximizes the probability of consumption by traders who value the most the object (i. e. welfare) ...|$|R
40|$|The {{contribution}} {{deals with}} the compound (cumulative) random process, i. e. the <b>sequence</b> <b>of</b> random increments at random moments. The process is modeled as a 2 D point process (or 2 D random counting measure), via the intensities of both components. The study starts from the case of compound Poisson process and generalizes to the cases when the components depend on each other, and also {{on a set of}} covariates. An example {{deals with the}} <b>sequence</b> <b>of</b> financial <b>transactions...</b>|$|R
40|$|Long lived {{transactions}} (LLTs) {{hold on to}} database {{resources for}} relatively long periods of time, slgmficantly delaymg the termmatlon of shorter and more common transactions To alleviate these problems we propose {{the notion of a}} saga A LLT 1 s a saga if it can be written as a <b>sequence</b> <b>of</b> <b>transactions</b> that can be interleaved with other transactions The database manage-ment system guarantees that either all the tran-sactions m a saga are successfully completed or compensatmg transactions are run to amend a partial execution Both the concept of saga and its lmplementatlon are relatively simple, but they have the potential to improve performance slgmficantly We analyze the various lmplemen-tatron issues related to sagas, including how they can be run on an exlstmg system that does not directly support them We also discuss tech-niques for database and LLT design that make it feasible to break up LLTs mto sagas 1...|$|E
40|$|In {{this paper}} we {{describe}} a language for long duration transactions. Our language {{is inspired by}} both action-description languages used in artificial intelligence and earlier transaction specification languages such as NT/PV [7]. We allow nested transactions, specification of ordering between sub-transactions, and constraints that hold {{at different levels of}} nesting. The semantics of our language is based on the automata-based semantics of action description languages, and allows us to define an entailment relation between a transaction description and queries that specify if a fluent is true after the execution of a <b>sequence</b> <b>of</b> <b>transactions.</b> This entailment relation forms the basis of our formalization of notions such as the correctness of a transaction definition, serializability of a schedule etc. We also define a language to specify semantic informations about allowable interleaving and show how it increases concurrency. 1 Introduction and motivation Long-duration transactions and [...] ...|$|E
40|$|This paper {{presents}} {{a new technique}} for clone detection using sequential pattern mining titled EgyCD. Over the last decade many techniques and tools for software clone detection have been proposed such as textual approaches, lexical approaches, syntactic approaches, semantic approaches …, etc. In this paper, we explore the potential of data mining techniques in clone detection. In particular, we developed a clone detection technique based on sequential pattern mining (SPM). The source code is treated as a <b>sequence</b> <b>of</b> <b>transactions</b> processed by the SPM algorithm to find frequent itemsets. We run three experiments to discover code clones of Type I, Type II and Type III and for plagiarism detection. We compared the results with other established code clone detectors. Our technique discovers all code clones in the source code and hence it is slower than the compared code clone detectors since they discover few code clones compared with EgyCD...|$|E
40|$|Abstract—Online {{credit card}} fraud {{presents}} a significant {{challenge in the}} field of eCommerce. In 2012 alone, the total loss due to credit card fraud in the US amounted to $ 54 billion. Especially online games merchants have difficulties applying standard fraud detection algorithms to achieve timely and accurate detection. This paper describes the special constrains of this domain and highlights the reasons why conventional algorithms are not quite effective to deal with this problem. Our suggested solution for the problem originates from the fields of feature construction joined with the field <b>of</b> temporal <b>sequence</b> data mining. We present feature construction techniques, which are able to create discriminative features based on a <b>sequence</b> <b>of</b> <b>transaction</b> and are able to incorporate the time into the classification process. In addition to that, a framework is presented that allows for an automated and adaptive change of features in case the underlying pattern is changing. Keywords-feature construction, temporal data mining, binary classification, credit card fraud I...|$|R
40|$|The Ninth Circuit {{recently}} {{reversed the}} district court’s summary judgment {{in favor of}} the government in Linton on the issues of indirect gift and the applicability <b>of</b> the step <b>transaction</b> doctrine. The circuit court’s analysis focused on the taxpayers’ donative intent. With that emphasis, the Ninth Circuit remanded the case to the district court to determine the <b>sequence</b> <b>of</b> the relevant <b>transactions...</b>|$|R
40|$|The {{popular and}} {{scientific}} literature has been discussing {{the advent of}} ‘big data’ {{with a measure of}} excitement and apprehension. For the first time in history, it seems, every breath we take, every move we make, someone’s watching us. But beyond their unprecedented volumes and the anxieties they raise, new communication data have a less obvious aspect, in so far as they are (arguably) of a fundamentally different kind, compared to traditional network datasets. Traditionally, social network data describe relationships between individuals; quasistatic social ties such as friendship, trust, kinship and employment relations. But when they are used to model digitally mediated communicative transactions, the connections are of a different nature. Instead of representing stable social ties, transactions (such as emails, text messages and phone calls) constitute <b>sequences</b> <b>of</b> shortlived events, with each transaction being a possible response to a preceding one and a potential stimulus to the next. The point of departure of this dissertation is the distinction between the topology of the tie structure and the temporal structure <b>of</b> <b>sequences</b> <b>of</b> communicative <b>transactions.</b> Theoretically, the dissertation explores mechanisms of co-evolution between these two structures at three levels of aggregation: (i) the macro-level consisting of the network itself or substructures within it, the level of an organization or a community as a whole; (ii) the meso-level consisting of nodes and social ties; and (iii) the micro-level consisting <b>of</b> <b>sequences</b> <b>of</b> interrelated communicative <b>transactions.</b> On the one hand, networks, individuals and ties are seen as the backdrop against which <b>sequences</b> <b>of</b> <b>transactions</b> unfold. On the other hand, transactions are considered to have (cumulative) consequences on the evolving structure of social ties and the network at large. Methodologically, the thesis uses a publicly available dataset consisting <b>of</b> email <b>transactions</b> within Enron, an American energy and services company, during the few months of its bankruptcy. Two methods are applied to identify and explore the mechanisms. First, the dataset is disaggregated into various types <b>of</b> email <b>transactions,</b> revealing how different transactions contribute to various structural properties of the network. Second, a multilevel analysis approach is used to reveal how structural and transactional mechanisms combine to elicit new communicative transactions on the part of email recipients. The mechanisms identified in the empirical chapters challenge received wisdom about the nature of social networks and their link to the notion of social (trans) action {{while at the same time}} addressing practical problems faced by network modellers who need to construct networks out <b>of</b> digitally mediated <b>transaction</b> datasets. In addition, the findings raise general questions about new types of data and the consequences they may have, not only for the field of social networks, but also for popular ways of thinking about ‘the social’ and ways of intervening in its course...|$|R
40|$|We {{offer an}} {{interdisciplinary}} governance framework that combines insights from organizational economics, sociology and psychology on the facilitation of cooperation among and {{the reduction of}} the incidence of opportunism by counterparties or stakeholders. We highlight the complementarity of regulative (e. g., litigation and arbitration), normative (e. g., expectations of reciprocal or fair behavior) and cognitive (e. g., senses of shared identity) institutional supports for relational contracting. We also draw attention to specific strategies that can invoke these supports. These strategies rely upon social sanction within existing social structures, craft new social structures in which social sanction may be operative by invoking existing norms of procedural justice or alter cognitive senses of identity. While we develop our theoretical arguments with examples drawn from infrastructure projects, we conclude by noting the applicability of these arguments to a wide array of economic activity in which multiple counterparties engage in a <b>sequence</b> <b>of</b> <b>transactions</b> characterized by a relatively high level of contractual hazards...|$|E
40|$|ATM card {{fraud is}} causing {{billions of dollars}} in losses for the card payment industry. In today’s world the most {{accepted}} payment mode is Debit card for both online and also for regular purchasing; hence frauds related with it are also growing. To find the fraudulent transaction, we implement an Advanced Security Model for ATM payment using Hidden Markov Model (HMM), which detects the fraud by using customers spending behavior. This Security Model is primarily focusing on the normal spending behavior of a cardholder and some advanced securities such as Location, Amount, Time and <b>Sequence</b> <b>of</b> <b>transactions.</b> If the trained Security model identifies any misbehavior in upcoming transaction, then that transaction is permanently blocked until the user enter High Security Alert Password (HSAP). This paper provides an overview of frauds and begins with ATM card statistics and the definition of ATM card fraud. The main outcome of the paper is to find the fraudulent transaction and avoids the fraud before it happens...|$|E
40|$|AbstractSuspicious {{activity}} reporting {{has been}} {{a crucial part of}} anti-money laundering systems. Financial transactions are considered suspicious when they deviate from the regular behavior of their customers. Money launderers pay special attention to keep their transactions as normal as possible to disguise their illicit nature. This may deceive the classical deviation based statistical methods for finding anomalies. This study presents an approach, called SARDBN (Suspicious Activity Reporting using Dynamic Bayesian Network), that employs a combination of clustering and dynamic Bayesian network (DBN) to identify anomalies in <b>sequence</b> <b>of</b> <b>transactions.</b> SARDBN applies DBN to capture patterns in a customer’s monthly transactional sequences as well as to compute an anomaly index called AIRE (Anomaly Index using Rank and Entropy). AIRE measures the degree of anomaly in a transaction and is compared against a pre-defined threshold to mark the transaction as normal or suspicious. The presented approach is tested on a real dataset of more than 8 million banking transactions and has shown promising results...|$|E
40|$|Social {{scientists}} are not computer scientists, but {{their skills in}} the field have to become better and better {{to cope with the}} growing field of social simulation and agent based modelling techniques. A way to reduce the weight of software development is to employ generalised agent development tools, accepting both the boundaries necessarily existing in the various packages and the subtle and dangerous differences existing in the concept of agent in computer science, artificial intelligence and social sciences. The choice of tools based on the object oriented paradigm that offer libraries of functions and graphic widgets is a good compromise. A product with this kind of capability is Swarm, developed at the Santa Fe Institute and freely available, {{under the terms of the}} GNU license. A small example of a model developed in Swarm is introduced, in order to show directly the possibilities arising from the use of these techniques, both as software libraries and methodological guidelines. With simple agents - interacting in a Swarm context to solve both memory and time simulation problems - we observe the emergence <b>of</b> chaotic <b>sequences</b> <b>of</b> <b>transaction</b> prices. Agent Based Models (ABM), Chaos, Intelligent Agents, Social Simulation, Swarm...|$|R
40|$|AbstractWe {{address the}} problem of formalizing the {{evolution}} of a database under the effect <b>of</b> an arbitrary <b>sequence</b> <b>of</b> update <b>transactions.</b> We do so by appealing to a first-order representation language called the situation calculus, which is a standard approach in artificial intelligence to the formalization of planning problems. We formalize database transactions {{in exactly the same way}} as actions in the artificial intelligence planning domain. This leads to a database version of the frame problem in artificial intelligence. We provide a solution to the frame problem for a special, but substantial, class <b>of</b> update <b>transactions.</b> Using the axioms corresponding to this solution, we provide procedures for determining whether a given <b>sequence</b> <b>of</b> update <b>transactions</b> is legal, and for query evaluation in an updated database. These procedures have the desirable property that they appeal to theorem-proving only with respect to the initial database state. We next {{address the problem}} of proving properties true in all states of the database. It turns out that mathematical induction is required for this task, and we formulate a number of suitable induction principles. Among those properties of database states that we wish to prove are the standard database notions of static and dynamic integrity constraints. In our setting, these emerge as inductive entailments of the database. Finally, we discuss various possible extensions of the approach of this paper, including transaction logs and historical queries, the complexity of query evaluation, actualized transactions, logic programming approaches to updates, database views, and state constraints...|$|R
40|$|Users of {{electronic}} devices, e. g., laptop, smartphone, etc. have characteristic behaviors while surfing the Web. Profiling this behavior can help identify the person using a given device. In this paper, we introduce {{a technique to}} profile users based on their web transactions. We compute several features extracted from a <b>sequence</b> <b>of</b> web <b>transactions</b> and use them with one-class classification techniques to profile a user. We assess the efficacy and speed of our method at differentiating 25 users on a dataset representing 6 months of web traffic monitoring from a small company network. Comment: Extended technical report of an IEEE ICDCS 2017 publicatio...|$|R
