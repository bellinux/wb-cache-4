1842|10000|Public
25|$|Hexagonal Fast Fourier Transform aims at {{computing}} {{an efficient}} FFT for the hexagonally <b>sampled</b> <b>data</b> {{by using a}} new addressing scheme for hexagonal grids, called Array Set Addressing (ASA).|$|E
25|$|Volume {{rendering}} is {{a technique}} used to display a 2D projection of a 3D discretely <b>sampled</b> <b>data</b> set. A typical 3D data set {{is a group of}} 2D slice images acquired by a CT or MRI scanner.|$|E
25|$|Volume {{rendering}} is {{a technique}} used to display a 2D projection of a 3D discretely <b>sampled</b> <b>data</b> set. A typical 3D data set {{is a group of}} 2D slice images acquired by a CT or MRI scanner. Usually these are acquired in a regular pattern (e.g., one slice every millimeter) and usually have a regular number of image pixels in a regular pattern. This {{is an example of a}} regular volumetric grid, with each volume element, or voxel represented by a single value that is obtained by sampling the immediate area surrounding the voxel.|$|E
40|$|Abstract. Sampling is {{a common}} way to collect {{execution}} information with performance event counters. However, the <b>sampling</b> <b>data</b> generated from performance event counters tend to be massive if sampling with high frequencies. Storing and processing {{a large amount of}} <b>sampling</b> <b>data</b> require much disk space and computing time. In this paper, we propose the online linear regression method {{to reduce the size of}} the <b>sampling</b> <b>data.</b> Our idea is to fit the <b>sampling</b> <b>data</b> with a series of straight lines. Each line represents the variation trend of the sample values within the corresponding section. Then we store the parameters of the lines instead of the sample values, resulting in a reduction of the <b>sampling</b> <b>data</b> size. The SPEC CPU 2006 benchmarks are tested to verify the proposed online linear regression method. The experimental result shows the online linear regression method can reduce the <b>sampling</b> <b>data</b> size effectively with a low overhead, and retain the variation characteristic of the <b>sampling</b> <b>data</b> with a normalized estimated standard deviation less than 0. 1. Key words: <b>sampling</b> <b>data</b> compression, performance event counters, online linear regression...|$|R
40|$|The <b>sample</b> <b>data</b> for {{selection}} of 11 radionuclides and 24 chemical analytes were extracted from six separate <b>sample</b> <b>data</b> sets, were {{arranged in a}} tabular format and were plotted on scatter plots {{for all of the}} 149 single-shell tanks, the 24 double-shell tanks and the four aging waste tanks. The solid and liquid <b>sample</b> <b>data</b> was placed in separate tables and plots. The <b>sample</b> <b>data</b> and plots were compiled from the following data sets: characterization raw <b>sample</b> <b>data,</b> recent core <b>samples,</b> D. Braun <b>data</b> base, Wastren (Van Vleet) data base, TRAC and HTCE inventories. This document is Volume I of the Letter Report entitled Tank Waste Source Term Inventory Validation...|$|R
40|$|Abstract: For data {{warehouse}} and OLAP (On-Line Analytical Processing) systems various preparation tasks {{are necessary to}} qualify and improve {{the effectiveness of their}} usage. In order to put into operation such a task the creation of statistical sound <b>sample</b> <b>data</b> is indispensable. Applying statistical models for the generation of <b>sample</b> <b>data</b> promises a high-quality prospect which allows the production of <b>sample</b> <b>data</b> based on different user-controllable statistical parameters. The <b>sample</b> <b>data,</b> which follows the defined statistical requirements, can be conveniently used for testing, benchmarking, demonstrating and training in the fields of {{data warehouse}}. In this paper, we address the activities of generation sample data; classify the generating methods and using BEDAWA [3] tool as illustration purposing to generate <b>sample</b> <b>data.</b> Furthermore, we discuss the uses of <b>sample</b> <b>data</b> in the field of data warehouse...|$|R
500|$|Today, most {{developed}} countries have {{a network of}} weather radars, which remains the main method of detecting signatures probably associated with tornadoes. In the United States {{and a few other}} countries, Doppler weather radar stations are used. These devices measure the velocity and radial direction (towards or away from the radar) of the winds in a storm, and so can spot evidence of rotation in storms from more than [...] away. When storms are distant from a radar, only areas high within the storm are observed and the important areas below are not <b>sampled.</b> <b>Data</b> resolution also decreases with distance from the radar. Some meteorological situations leading to tornadogenesis are not readily detectable by radar and on occasion tornado development may occur more quickly than radar can complete a scan and send the batch of data. Doppler radar systems can detect mesocyclones within the supercell of a thunderstorm. This allows meteorologists to predict tornado formations throughout thunderstorms.|$|E
2500|$|Many {{real-world}} phenomena exhibit {{limited or}} statistical fractal properties and fractal dimensions {{that have been}} estimated from <b>sampled</b> <b>data</b> using computer based fractal analysis techniques.|$|E
2500|$|Encoding: Sound-waves {{are sampled}} and {{converted}} to digital form by an A/D converter. The sound wave is sampled for amplitude (at 44.1nbsp&kHz or 44,100 pairs, one each {{for the left}} and right channels of the stereo sound). The amplitude at an instance is assigned a binary string of length 16. Thus, each sample produces two binary vectors from [...] or 4 [...] bytes of data. Every second of sound recorded results in 44,100nbsp&times&nbsp&32 = 1,411,200 bits (176,400 bytes) of data. The 1.41 Mbit/s <b>sampled</b> <b>data</b> stream passes through the error correction system eventually getting converted to a stream of 1.88 Mbit/s.|$|E
40|$|This {{document}} comprises Volume II of the Letter Report entitled Tank Waste Source Term Inventory Validation. This volume contains Appendix C, Radionuclide Tables, and Appendix D, Chemical Analyte Tables. The <b>sample</b> <b>data</b> for {{selection of}} 11 radionuclides and 24 chemical analytes were extracted from six separate <b>sample</b> <b>data</b> sets, were {{arranged in a}} tabular format and were plotted on scatter plots {{for all of the}} 149 single-shell tanks, the 24 double-shell tanks and the four aging waste tanks. The solid and liquid <b>sample</b> <b>data</b> was placed in separate tables and plots. The <b>sample</b> <b>data</b> and plots were compiled from the following data sets: characterization raw <b>sample</b> <b>data,</b> recent core <b>samples,</b> D. Braun <b>data</b> base, Wastren (Van Vleet) data base, TRAC and HTCE inventories...|$|R
40|$|This paper {{discusses}} the {{theoretical and practical}} aspects of new methods for solving DEA problems under real-life geometrical uncertainty and probability uncertainty of <b>sample</b> <b>data.</b> The proposed minimax approach to solve problems with geometrical uncertainty of <b>sample</b> <b>data</b> involves an implementation of linear programming or minimax optimization, whereas the problems with probability uncertainty of <b>sample</b> <b>data</b> are solved through implementing of econometric and new stochastic optimization methods, using the stochastic frontier functions estimation. DEA, <b>Sample</b> <b>data</b> uncertainty, Linear programming, Minimax optimization, Stochastic optimization, Stochastic frontier functions...|$|R
30|$|In {{order to}} avoid the {{influence}} of the dispersibility of <b>sample</b> <b>data</b> acquired during normal and various fault conditions, analysis of the separability of the <b>sample</b> <b>data</b> is indispensable.|$|R
2500|$|The SID chip {{has three}} channels, {{each with its}} own ADSR {{envelope}} generator and filter capabilities. Ring modulation makes use of channel N°3, to work with the other two channels. Bob Yannes developed the SID chip and later co-founded synthesizer company Ensoniq. Yannes criticized other contemporary computer sound chips as [...] "primitive, obviously...designed by people who knew nothing about music". Often the game music has become a hit of its own among C64 users. Well-known composers and programmers of game music on the C64 are Rob Hubbard, Jeroen Tel, David Whittaker, Chris Hülsbeck, Ben Daglish, Martin Galway, Kjell Nordbø and David Dunn among many others. Due to the chip's three channels, chords are often played as arpeggios, coining the C64's characteristic lively sound. It was also possible to continuously update the master volume with <b>sampled</b> <b>data</b> to enable the playback of 4-bit digitized audio. As of 2008, it became possible to play four channel 8-bit audio samples, 2 SID channels and still use filtering.|$|E
5000|$|DataFormat: format used {{to encode}} the <b>sampled</b> <b>data</b> - {{examples}} include Offset Binary and Binary Coded Decimal ...|$|E
5000|$|Synchronization is {{important}} in fields such as digital telephony, video and digital audio where streams of <b>sampled</b> <b>data</b> are manipulated.|$|E
30|$|Xi = each {{data point}} i, X = the average {{of all the}} <b>sample</b> <b>data</b> points, S = the <b>sample</b> {{standard}} deviation of all <b>sample</b> <b>data</b> points, zi = the data point i standardized to s, also known as Z-score.|$|R
40|$|Abstract: This paper {{discusses}} the {{theoretical and practical}} aspects of new methods for solving DEA problems under real-life geometrical uncertainty and probability uncertainty of <b>sample</b> <b>data.</b> The proposed minimax approach to solve problems with geometrical uncertainty of <b>sample</b> <b>data</b> involves an implementation of linear programming or minimax optimization, whereas the problems with probability uncertainty of <b>sample</b> <b>data</b> are solved through implementing of econometric and new stochastic optimization methods, using the stochastic frontier functions estimation. ...|$|R
30|$|In [4], Markov-based <b>sample</b> <b>data</b> was {{reflected}} on the networked control systems, in which the sampling-induced delay index was modeled by a Markov chain. Different from [4], our model includes discrete time-delays, distributed time-delays (∑_r= 1 ^+∞μ_rx(k-r)) and Markov-based <b>sample</b> <b>data.</b>|$|R
5000|$|Sequential {{analysis}} - {{evaluation of}} <b>sampled</b> <b>data</b> {{as it is}} collected, until the criterion of a stopping rule is met ...|$|E
5000|$|<b>Sampled</b> <b>Data</b> Transfer — Schemes {{are also}} defined to handle {{transfer}} of sampled values using Sampled Value Control blocks (SVCB) ...|$|E
5000|$|... #Caption: Figure 1. The figure {{illustrates}} {{representation of}} a hexagonally <b>sampled</b> <b>data</b> {{in the form of}} rectangular arrays using ASA coordinate system.|$|E
50|$|Pixels in PNG {{images are}} numbers {{that may be}} either indices of <b>sample</b> <b>data</b> in the palette or the <b>sample</b> <b>data</b> itself. The palette is a {{separate}} table contained in the PLTE chunk. <b>Sample</b> <b>data</b> for a single pixel consists of a tuple of between one and four numbers. Whether the pixel data represents palette indices or explicit sample values, the numbers {{are referred to as}} channels and every number in the image is encoded with an identical format.|$|R
50|$|FortiGuard {{also provide}} a visual output of global <b>sample</b> <b>data</b> {{collected}} from all FortiGate appliances connected to the FortiGuard labs databases. The threat map is publicly available for <b>sample</b> <b>data</b> and can also be viewed, specific to a single FortiGate appliance within FortiOS.|$|R
30|$|In summary, {{for each}} range bin, a <b>sample</b> <b>data</b> matrix of 21 [*]×[*] 21 [*]×[*] 4 and a <b>sample</b> label <b>data</b> can be obtained.|$|R
50|$|If <b>sampled</b> <b>data</b> from a {{function}} or a physical object is available, spline interpolation is {{an approach to}} creating a spline that approximates that data.|$|E
50|$|The {{technique}} is very reliable, as {{the reconstruction of}} the magnetic field maps with different algorithms yield almost identical results, even with poorly <b>sampled</b> <b>data</b> sets.|$|E
5000|$|From {{the basic}} k-space formula, it follows {{immediately}} that we reconstruct an image [...] simply {{by taking the}} inverse Fourier transform of the <b>sampled</b> <b>data,</b> viz.|$|E
2500|$|Pixels in PNG {{images are}} numbers {{that may be}} either indices of <b>sample</b> <b>data</b> in the palette or the <b>sample</b> <b>data</b> itself. [...] The palette is a {{separate}} table contained in the PLTE chunk. [...] <b>Sample</b> <b>data</b> for a single pixel consists of a tuple of between one and four numbers. [...] Whether the pixel data represents palette indices or explicit sample values, the numbers {{are referred to as}} channels and every number in the image is encoded with an identical format.|$|R
30|$|Determine the <b>sample</b> <b>data</b> be used.|$|R
50|$|The {{basic idea}} of {{bootstrapping}} is that inference about a population from <b>sample</b> <b>data,</b> (<b>sample</b> → population), can be modelled by resampling the <b>sample</b> <b>data</b> and performing inference about a <b>sample</b> from resampled <b>data,</b> (resampled → <b>sample).</b> As {{the population is}} unknown, the true error in a sample statistic against its population value is unknown. In bootstrap-resamples, the 'population' {{is in fact the}} sample, and this is known; hence the quality of inference of the 'true' <b>sample</b> from resampled <b>data,</b> (resampled → <b>sample),</b> is measurable.|$|R
5000|$|The {{one-dimensional}} RDFS {{proposed by}} Arruda (1992a) can be formulated {{in a very}} straightforward way. Given a <b>sampled</b> <b>data</b> vector (signal) , one can write the algebraic expression: ...|$|E
50|$|Hexagonal Fast Fourier Transform aims at {{computing}} {{an efficient}} FFT for the hexagonally <b>sampled</b> <b>data</b> {{by using a}} new addressing scheme for hexagonal grids, called Array Set Addressing (ASA).|$|E
50|$|In {{scientific}} visualization {{and computer}} graphics, volume rendering {{is a set}} of techniques used to display a 2D projection of a 3D discretely <b>sampled</b> <b>data</b> set, typically a 3D scalar field.|$|E
5|$|Expressed {{sequence}} tags {{with their}} associated <b>sample</b> <b>data.</b>|$|R
3000|$|... where DC is {{the duty}} cycle, Ndata the <b>sampling</b> <b>data</b> bits within one frame time, Noh the {{overhead}} bits within one frame time, Nsync the synchronization bits within one frame time, fc the communication data rate (bits per s) and fs the <b>sampling</b> <b>data</b> rate (bits per s).|$|R
40|$|Second alpha {{release of}} the Panoptes 2. 0 series. Continuing to {{redesign}} and reimplement the UI to reach parity with v 1. 6. 2, improving user experience and future development prospects. This release includes: New track to indicate scale invariants Improved efficiency and performance on data fetching Improved visualisation of indicator-dense genomic regions Updated documentation for import settings Fixed tests for importing settings Updated example <b>sample</b> <b>data</b> settings for geographic datasets Tests for importing <b>sample</b> <b>data</b> settings Tests for converting YAML and JSON formats in <b>sample</b> <b>data</b> setting...|$|R
