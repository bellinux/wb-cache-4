3|26|Public
40|$|Provisional ed. Uncontoured (planimetric) {{map series}} of Papua New Guinea, {{compiled}} from aerial photography, showing minor roads, tracks, water features, buildings and populated places. Relief shown by spot heights.; Produced by National Mapping Office to 1955, then a namechange to Division of National Mapping 1955 / 56.; 34 zBThis provisional edition {{has not been}} field checked. It is distributed at this stage {{and in this form}} in order that topographic information of some practical value may be available for immediate use. 34 yB; Each sheet is named and numbered individually.; Title of Gwoira Range sheet is misspelt as Gwoina Range.; Includes either 34 zBindex to block assembly 34 y Bor 34 zBindex to sheets laid down as block <b>slotted</b> <b>template</b> assembly 34 yB; National Library of Austrlaia's holdings include annotated copies of some sheets showing corrections to locations and placenames of features. A handwritten sheet of notes accompanies this annotatted copy for each of the Annanberg and Toro sheets. An enlargment of the Buna sheet to 1 : 43, 000 is also held. ANL. Addu [...] Amaimon [...] Annanberg [...] Bena Bena [...] Bogia [...] Buna [...] Cape Nelson [...] Cape Vogel [...] Chauve [...] Chimbu [...] Danawatu [...] Douglas Harbour [...] Dumpu [...] Finintegu [...] Goodenough Bay [...] Gwoina Range [i. e. Gwoira Range] [...] Hagen [...] Ioma [...] Kainantu [...] Karkar West [...] Kerowagi [...] Kokoda [...] Korogo East [...] Korogo West [...] Kwagira River [...] Malala [...] Matuka [...] Misima Island [...] Moi Biri Bay [...] Mt Otto [...] Mt Parkes [...] Musak [...] Nubia [...] Omba East [...] Omba West [...] Opi River [...] Oro Bay [...] Popondetta [...] Posa Posa Harbour [...] Ramu [...] Tamol [...] Toro [...] Uvo [...] Wagi [...] Wanigela [...] Yogei East [...] Yogei West...|$|E
40|$|Genomic {{diversity}} within the species Zea mays has been examined {{by measuring the}} variation in the repetitive component of the nuclear genome among North American inbred lines and varieties. This was done by preparing a set of clones of repetitive maize sequences that differ in function, molecular arrangement and multiplicity and then using these as probes for quantitative hybridization to DNA from various maize genotypes. The comparison showed {{that the majority of}} repeated sequences are markedly variable in copy number among the ten maize strains tested. —The clone sample contained the rDNA and 5 S genes, the major repeat of the chromosome knobs, sequences functioning as origins of DNA replication in yeast (ARS sequences) and randomly cloned sequences of unknown function and chromosomal location. The sequences ranged in reiteration frequency from 200 to greater than 105 copies and included both tandemly arrayed and dispersed repeats. The copy numbers were measured by hybridizing labeled cloned sequences to aliquots of high molecular weight genomic DNA that were applied to nitrocellulose filters through a <b>slotted</b> <b>template</b> (slot blotting). The hybridization signal on an autoradiogram occurred in a narrow band that could be scored reliably with a densitometer. This provided a rapid method of determining the abundance of particular repeated sequences in individual plants and plant populations. Using this technique, we found that the copy number of repeated sequences of all types generally varied among the strains by two- to threefold, although at least one sequence showed no detectable variation. —In contrast to the variability found between strains, individuals within an inbred line or variety were found to be indistinguishable in terms of specific sequence multiplicity. Each genotype has a different pattern of copy numbers for the set of repeated sequence clones, and this pattern is characteristic of all individuals of a particular genotype. The data also show that the copy number of each sequence varies independently. No strains had uniformly high or low copy numbers for the entire set of probes...|$|E
40|$|Graduation date: 1952 There are {{approximately}} 4, 000, 000 acres of cut-over and burned-over land in western Washington and Oregon which are either non-stocked or unsatisfactorily stocked with young forest growth. This brings about {{a great need}} for a quick and efficient method of land examination from which to plan reforestation measures and make up management plans. This thesis is an explanation of one method, that of using aerial photographs in conjunction with limited ground observation. An example is given which illustrates that each $l. 00 saved in the planning stage of forest regeneration means $l 9. 23 more profit {{at the end of}} 100 years using an interest rate of 3 % compounded annually. More and more forest industries are beginning to use aerial photos for management plans, reforestation programs, and even cutting plans and preliminary road location. The first part of this thesis deals with the using of the aerial photographs and the recognition of the information found on them as substantiated by ground examination. The Blodgett tract is 2400 acres of logged and burned-over land in northwestern Oregon which was deeded to the Oregon State College School of Forestry for the purpose of establishing a reforestation study area. The photographs used were standard 9 x 9 inch contact prints of negative photographed with a f-l 2 " Aerial Mapping Camera by Delano Aerial Surveys of Portland, Oregon. The average scale of photography was 1 : 12, 000. The photographs were taken to the Blodgett tract and used as a guide in examining certain areas of the tract. Next a base map was made of the area using a <b>slotted</b> <b>template</b> laydown for horizontal control. The planimetry was transferred to the base map from the aerial photos by means of a multiscope. Fuel types were also determined from the photos. Other maps, including the proposed road and water development map, the hour control map and the seen area map, were also prepared. They were not particularly dependent upon the photographs for their information, except that the roads were located along old railroad grades and the water holes were beaver ponds, both of which could be observed on the photographs. A second land examination was made to correlate the types of vegetation on representative areas with the vegetational types given those areas from aerial photograph interpretation. Certain changes were made where the land examination showed the stocking or the species composition to differ from the photo interpretation. Well defined areas of each type were examined by the stocked quadrat method of reproduction examination and final classifications were based on the results. In conclusion to {{the first part of the}} thesis the author states that the field work in preparing a reforestation plan can be reduced to one-fifth of the time required by land examination alone. Forest boundaries and type boundaries may be easily recognized on the photographs and transferred to the base map. Fuel type and hour control maps may be made from the photos. Seen area maps may not be made from photographs without first constructing a topographic map. Tree species can be recognized to a certain degree from the photographs. Pure fir stands can be differentiated from pure hemlock stands, but they are not easily separated when in a mixture. Alder stands can be separated from the willow-vine maple stands, but again not in mixtures. However it is easy to distinguish coniferous stands from the hardwoods. The degree of stocking on reforestated areas can be determined from aerial photo examination, but only when the young trees average better than five feet in height and two feet in crown diameter. Part Two of the thesis is a brief management plan for the Blodgett tract. It includes the history, physiographic features, and condition of the forest on the tract. Silivicultural recommendations are made and also the fire protection maps previously mentioned. The area has high site quality, ample moisture, deep soil, and is ideally suited to the growing of Douglas-fir veneer stock on a rotation of between 100 and 150 years. It is located close to the Columbia River, which facilitates the transportation of the timber products. There is an adequate supply of local labor. The author recommends the planting of non-stocked and understocked areas, the pruning of selected crop tress, and periodic thinnings. Also recommended is the construction of access and fire roads, and a program of rodent and game control to assist in the re-establishment of the forest...|$|E
40|$|We {{describe}} two tasks—named entity recog-nition (Task 1) and <b>template</b> <b>slot</b> filling (Task 2) —for clinical texts. The tasks leverage an-notations {{from the}} ShARe corpus, which con-sists of clinical notes with annotated men-tions disorders, {{along with their}} normaliza-tion to a medical terminology and eight addi-tional attributes. The purpose of these tasks was to identify advances in clinical named en-tity recognition and establish {{the state of the}} art in disorder <b>template</b> <b>slot</b> filling. Task 2 consisted of two subtasks: <b>template</b> <b>slot</b> fill-ing given gold-standard disorder spans (Task 2 a) and end-to-end disorder span identifica-tion together with <b>template</b> <b>slot</b> filling (Task 2 b). For Task 1 (disorder span detection and normalization), 16 teams participated. The best system yielded a strict F 1 -score of 75. 7, with a precision of 78. 3 and recall of 73. 2. For Task 2 a (<b>template</b> <b>slot</b> filling given gold-standard disorder spans), six teams partici-pated. The best system yielded a combined overall weighted accuracy for slot filling of 88. 6. For Task 2 b (disorder recognition and <b>template</b> <b>slot</b> filling), nine teams participated. The best system yielded a combined relaxed F (for span detection) and overall weighted ac-curacy of 80. 8. ...|$|R
40|$|Prosthetics {{devices have}} open {{possibilities}} {{for the implementation of}} new technology that approximate the motion of human limbs. Nonetheless, some limitations are set by the weight and the complexity prostheses can have. Focusing on upper limb prostheses, a digital actuator with stable output positions could drive the system with lower energy requirements. The major achievements of this study are the conceptualization of an actuator that can harvest and store energy from the biomechanical motion of the hand; the implementation of permanent magnets as boundary constraints that can provide both holding or driving forces; and the use of <b>slot</b> <b>Templates</b> as geometric constraints to increase the mechanical advantage and reduce the energy requirement of an actuator driven by permanent magnets. The conceptual design presents the main components an energy harvesting-storage device may have in order to operate properly. To compensate the possible small amount of energy that may be harvested, permanent magnets represent an alternative to reduce the energy required to operate. Furthermore, the present study mathematically characterized the interaction between permanent magnets, explained its physical behavior and prototyped two actuators that represent the conceptual design with two and three stable output positions. The major impact of the prototype was a conceptual proof that the magnetic field of permanent magnets can be used as both holding and driving force. In conclusion, a bi-stable and a tri-stable magnetic actuators were prototyped proving {{that it is possible to}} drive a discrete angular joint by using magnetic constraints only, or by combining the magnetic and geometric constraints to move a pin through a slot, providing stable outputs where no energy is required to maintain the system at a stable position. The <b>slot</b> <b>templates</b> were implemented in order to increase the mechanical advantage and the range of forces the system can support. Future directions are proposed with conceptual design of elements of an energy harvesting-energy storage device that can indeed be implemented to activate the motion between permanent magnets and drive the system between output positions, which potentially could be used in prosthetic arm. ...|$|R
40|$|This paper {{describes}} {{our approach}} to applying typeoriented inductive logic programming (ILP) to information extraction (IE) tasks and the latest experimental results in learning IE rules from the data generated from 100 newspaper articles. Information extraction involves extracting key information from text corpus in order to fill empty <b>slots</b> of given <b>templates.</b> A bottle neck in building IE systems is that constructing and verifying IE rules is labor intensive and time consuming...|$|R
40|$|In this paper, {{we discuss}} the {{effectiveness}} of a particular selection and sequencing of two emerging language technologies, namely information extraction (IE) followed by machine translation (MT), and report and evaluate our experiment results. The experiments have shown that different <b>template</b> <b>slot</b> types affect the quality of MT results, and that MT quality of some slot values is actually helped by IE while others, though less frequent, suffer from a lack of contextual information as the result of IE. ...|$|R
40|$|A {{method of}} {{generating}} conceptual graphs representing {{the meanings of}} real natural language texts is advanced. Three language specialists are employed, each of which may {{make a contribution to}} the final conceptual structure. A syntactic specialist trys to assemble a graph by maximally joining graphs accumulated while recursively ascending a phrase structure tree from a syntactic parser. A semantic specialist trys to fill thematic role <b>slots</b> in <b>template</b> graphs by the successive application of selectional constraints to alternatives. A pragmatic specialist trys to classify text according to its intended purpose as a speech act, and handle idioms. Methods of deciding between policies for controlling the specialists and integrating the contributions of the language specialists, to be used by a control specialist, are described. An implementation capable of handling real direction texts for physical navigation is under development...|$|R
40|$|We {{address the}} task of {{automatic}} discovery of information extraction template from a given text collection. Our approach clusters candidate slot fillers to identify meaningful <b>template</b> <b>slots.</b> We propose a generative model that incorporates distributional prior knowledge to help distribute candidates in a document into appropriate slots. Empirical {{results suggest that the}} proposed prior can bring substantial improvements to our task as compared to a K-means baseline and a Gaussian mixture model baseline. Specifically, the proposed prior has shown to be effective when coupled with discriminative features of the candidates. ...|$|R
40|$|ISpec is an {{interface}} specification approach where <b>templates</b> provide <b>slots</b> to write interface requirements. These requirements {{can be written}} in various 2 ̆ 2 plugin 2 ̆ 2 formalisms. The practical question how to implement this in a tool is answered for regular expressions as a plug-in language. The requirements expressed by the regular expressions are {{used to assess the}} correctness of requirements expressed in sequence diagrams. In fact, an editor is coupled to the tool in which a plug-in language can be defined and a <b>slot</b> in a <b>template</b> can be linked to a particular language. The theoretical question how to formalise plug-ins in a relation calculus framework is investigated...|$|R
40|$|We {{describe}} a graph-based approach to Scenario Template Creation, {{which is the}} task of creating a representation of multiple related events, such as reports of different hurricane incidents. We argue that context is valuable to identify important, semantically similar text spans from which <b>template</b> <b>slots</b> could be generalized. To leverage context, we represent the input {{as a set of}} graphs where predicate-argument tuples are vertices and their contextual relations are edges. A context-sensitive clustering framework is then applied to obtain meaningful tuple clusters by examining their intrinsic and extrinsic similarities. The clustering framework uses Expectation Maximization to guide the clustering process. Experiments show that: 1) our approach generates high quality clusters, and 2) information extracted from the clusters is adequate to build high coverage templates. ...|$|R
40|$|Information {{extraction}} {{is a form}} of shallow {{text processing}} that locates a specified set of relevant items in a natural-language document. Systems for this task require signicant domain-specific knowledge and are time-consuming and difficult to build by hand, making them a good application for machine learning. We present a system, Rapier, that uses pairs of sample documents and filled templates to induce pattern-match rules that directly extract llers for the <b>slots</b> in the <b>template.</b> Rapier employs a bottom-up learning algorithm which incorporates techniques from several inductive logic programming systems and ac-quires unbounded patterns that include constraints on the words, part-of-speech tags, and semantic classes present in the ller and the surrounding text. We present encouraging experimental results on two domains...|$|R
40|$|Information {{extraction}} {{is a form}} of shallow {{text processing}} that locates a specified set of relevant items in a natural-language document. Systems for this task require significant domain-specific knowledge and are time-consuming and difficult to build by hand, making them a good application for machine learning. We present an algorithm, RAPIER, that uses pairs of sample documents and filled templates to induce pattern-match rules that directly extract fillers for the <b>slots</b> in the <b>template.</b> RAPIER is a bottom-up learning algorithm that incorporates techniques from several inductive logic programming systems. We have implemented the algorithm in a system that allows patterns to have constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text. We present encouraging experimental results on two domains...|$|R
40|$|The Foundational Model (FM) of anatomy, {{developed}} as an anatomical enhancement of UMLS, classifies anatomical entities in a structural context. Explicit definitions {{have played a}} critical role in the establishment of FM classes. Essential structural properties that distinguish a group of anatomical entities serve as the differentiae for defining classes. These, as well as other structural attributes, are introduced as <b>template</b> <b>slots</b> in Protege, a frame-based knowledge acquisition system, and are inherited by descendants of the class. A set of desiderata has evolved during the instantiation of the FM for formulating definitions. We contend that 1. these desiderata generalize to non-anatomical domains and 2. satisfying them in constituent vocabularies of UMLS would enhance the quality of information retrievable through UMLS...|$|R
40|$|Information {{extraction}} {{is usually}} approached as an annota-tion task: Input texts run through several analysis steps of an extraction {{process in which}} different semantic concepts are annotated and matched against the <b>slots</b> of <b>templates.</b> We argue that such an approach lacks an efficient control of the input of the analysis steps. In this paper, we hence propose and evaluate a model and a formal approach that consistently put the filtering view in the focus: Before spend-ing annotation effort, filter those portions of the input texts that may contain relevant information for filling a template and discard the others. We model all dependencies between the semantic concepts sought for with a truth maintenance system, which then efficiently infers the portions of text to be annotated in each analysis step. The filtering view en-ables an information extraction system (1) to annotate only relevant portions of input texts and (2) to easily trade its run-time efficiency for its recall. We provide our approach as an open-source extension of Apache UIMA and we show the potential of our approach {{in a number of}} experiments. c©Wachsmuth et. al. (2013). This is the author’s version of the work. It is posted here for your personal use. Not for redistribution...|$|R
40|$|In this paper, {{we propose}} a novel {{approach}} to automatic generation of summary templates from given collections of summary articles. This kind of summary templates {{can be useful in}} various applications. We first develop an entity-aspect LDA model to simultaneously cluster both sentences and words into aspects. We then apply frequent subtree pattern mining on the dependency parse trees of the clustered and labeled sentences to discover sentence patterns that well represent the aspects. Key features of our method include automatic grouping of semantically related sentence patterns and automatic identification of <b>template</b> <b>slots</b> that need to be filled in. We apply our method on five Wikipedia entity categories and compare our method with two baseline methods. Both quantitative evaluation based on human judgment and qualitative comparison demonstrate the effectiveness and advantages of our method. ...|$|R
40|$|We {{developed}} {{a system to}} participate in shared tasks on the analyzing clinical text. Our system approaches are both machine learning-based and rule-based. We applied the machine learning-based approach for Task 1 : disorder identification, and the rule-based approach for Task 2 : <b>template</b> <b>slot</b> filling for the disorder. In Task 1, we {{developed a}} supervised conditional random fields model {{that was based on}} a rich set of features, and used for predicting disorder mentions. In Task 2, we based on the de-pendency tree to build a rule set. This rule set was extracted from the training data and applied to fill values of disorder attribute types on the test data. The evaluation on the test data showed that our system achieved the F-score of 0. 656 (0. 685 in case of re...|$|R
40|$|Information {{extraction}} {{is a form}} of shallow {{text processing}} which locates a specified set of relevant items in natural language documents. Such systems can be useful, but require domain-specific knowledge and rules, and are time-consuming and difficult to build by hand, making infomation extraction a good testbed for the application of machine learning techniques to natural language processing. This paper presents a system, Rapier, that takes pairs of documents and filled templates and induces pattern-match rules that directly extract fillers for the <b>slots</b> in the <b>template.</b> The learning algorithm incorporates techniques from several inductive logic programming systems and learns unbounded patterns that include constraints on the words and part-of-speech tags surrounding the filler. Encouraging results are presented on learning to extract information from computer job postings from the newsgroup misc. jobs. offered. Introduction Text understanding is a difficult and kno [...] ...|$|R
40|$|Information {{extraction}} systems process {{natural language}} documents and locate a {{specific set of}} relevant items. Given the recent success of empirical or corpusbased approaches in other areas of natural language processing, machine learning has the potential to significantly aid the development of these knowledge-intensive systems. This paper presents a system, Rapier, that takes pairs of documents and filled templates and induces pattern-match rules that directly extract fillers for the <b>slots</b> in the <b>template.</b> The learning algorithm incorporates techniques from several inductive logic programming systems and learns unbounded patterns that include constraints on the words and partof -speech tags surrounding the filler. Encouraging results are presented on learning to extract information from computer job postings from the newsgroup misc. jobs. offered. 1 Introduction An increasing amount of information is available in the form of electronic documents. The need to intelligently process s [...] ...|$|R
40|$|Recent work in machine {{learning}} for information extraction {{has focused on}} two distinct sub-problems: the conventional problem of filling <b>template</b> <b>slots</b> from natural language text, {{and the problem of}} wrapper induction, learning simple extraction procedures ("wrappers") for highly structured text such as Web pages produced by CGI scripts. For suitably regular domains, existing wrapper induction algorithms can efficiently learn wrappers that are simple and highly accurate, but the regularity bias of these algorithms makes them unsuitable for most conventional information extraction tasks. Boosting is a technique for improving the performance of a simple {{machine learning}} algorithm by repeatedly applying it to the training set with different example weightings. We describe an algorithm that learns simple, low-coverage wrapper-like extraction patterns, which we then apply to conventional information extraction problems using boosting. The result is BWI, a trainable informati [...] ...|$|R
40|$|A {{system is}} {{described}} which digests {{large volumes of}} text, filtering out irrelevant articles and distilling the remainder into templates that represent infor-mation from the articles in simple slot/filler pairs. The system is highly modular in that it consists {{of a series of}} programs, each of which contributes information to the text to help in the final analysis of determining which strings constitute valid values for the <b>slots</b> in the <b>template.</b> This modular design has the dual advantage of allowing relatively easy debugging and of permitting many of the component programs to participate in other projects. The system is customized to specific domains, taking advantage of simple string matching techniques to improve the effectiveness of more complex sentence-level semantic processes. The extension to new domains has been facilitated by dividing system data files into generic vs. specific categories; domain extension re-quires the creation of only the domain-specific files...|$|R
40|$|This paper {{presents}} an Information Extraction (IE) approach for spoken language understanding. The goal in IE {{is to find}} proper values for pre-defined <b>slots</b> of given <b>templates.</b> IE for spoken language understanding proposes a concept spotting approach for spoken language because IE approach is interested in only pre-defined concept slots. In spite of this partial understanding, we can acquire necessary information for an application from the values of pre-defined slots because the slots are properly designed for speech understanding in a specific domain. Spoken language has so many recognition errors especially in a poor environment so {{it is more difficult}} to understand than textual language. Considering this fact, we attempt to understand the languages by concentrating on the specified information. In experiments on the car navigation domain, F-measure for concept spotting for textual input (WER 0 %) and spoken input (WER 39 %) are 96. 33 % and 78. 30 % respectively. 1...|$|R
40|$|This paper {{describes}} {{an approach to}} using semantic representations for learning information extraction (IE) rules by a type-oriented inductive logic programming (ILP) system. NLP components of a machine translation system are used to automatically generate semantic representations of text corpus that can be given directly to an ILP system. The latest experimental results show high precision and recall of the learned rules. 1 Introduction Information extraction (IE) tasks in this paper involve the MUC- 3 style IE. The input for the information extraction task is an empty template {{and a set of}} natural language texts that describe a restricted target domain, such as corporate mergers or terrorist attacks in South America. Templates have a record-like data structure with slots that have names, e. g., "company name" and "merger date", and values. The output is a set of filled templates. IE tasks are highly domain-dependent, so rules and dictionaries for filling values in the <b>template</b> <b>slots</b> [...] ...|$|R
40|$|The recent {{growth of}} online {{information}} {{available in the}} form of natural language documents creates a greater need for computing systems with the ability to process those documents to simplify access to the information. One type of processing appropriate for many tasks is information extraction, a type of text skimming that retrieves specific types of information from text. Although information extraction systems have existed for two decades, these systems have generally been built by hand and contain domain speci c information, making them di cult to port to other domains. A few researchers have begun to apply machine learning to information extraction tasks, but most of this work has involved applying learning to pieces of a much larger system. This paper presents a novel rule representation specific to natural language and a learning system, Rapier, which learns information extraction rules. Rapier takes pairs of documents and lled templates indicating the information to be extracted and learns patterns to extract llers for the <b>slots</b> in the <b>template.</b> This proposal presents initial results on a small corpus of computer-related job postings with a preliminar...|$|R
40|$|Recent work in machine {{learning}} for information extraction {{has focused on}} two distinct sub-problems: the conventional problem of filling <b>template</b> <b>slots</b> from natural language text, {{and the problem of}} wrapper induction, learning simple extraction procedures ("wrappers") for highly structured text such as Web pages. For suitable regular domains, existing wrapper induction algorithms can efficiently learn wrappers that are simple and highly accurate, but the regularity bias of these algorithms makes them unsuitable for most conventional information extraction tasks. This paper describes a new approach for wrapping semistructured Web pages. The wrapper is capable of learning how to extract relevant information from Web resources on the basis of user supplied examples. It is based on inductive learning techniques as well as fuzzy logic rules. Experimental results show that our approach achieves noticeably better precision and recall coefficient performance measures than SoftMealy, {{which is one of the}} most recently reported wrappers capable of wrapping semi-structured Web pages with missing attributes, multiple attributes, variant attribute permutations, exceptions, and typos. Comment: International Journal of Computational Science - 200...|$|R
40|$|Only {{a subset}} of the MUC- 4 slots were {{generated}} by the UM/ConQuest ICTOAN system. Our scores fo r the slots we attempted to fill are shown in Figure 1. LIMITING FACTORS The most significant limiting factor we experienced was development time. The ICTOAN system was writte n from scratch during the first five months of 1992; only the template generation software was reused from our MUG 3 system. The effect of the short development time was compounded by our selection of C as the programming language for the project. We chose C for its speed, and to gain leverage from existing ConQuest software. However, because C is such a low-level language, development of a piece of code in C takes longer than development of equivalent code in a higher-level language such as LISP. Recall scores for the evaluation system were uniformly poor. The main factors contributing to the low recall scores were: • The evaluation system did not attempt to fill all <b>template</b> <b>slots.</b> In particular, we did not attempt t o guess any slot values that were not known, even when guessing would lead to a significant partial scor e (e. g. in the data slot). The following slots were not filled...|$|R
40|$|Information {{extraction}} {{consists in}} identifying classes {{of events and}} relationships between extracted instances of these classes. In general, extracted data usually fills <b>slots</b> in a <b>template</b> and is stored in tables. We propose to extend the usual approach {{to the use of}} an object database. Information extraction tools have a conceptual representation as schema components: concept classes, meta-concepts and attributes. The user expresses in his query a structure (target structure) which corresponds to his understanding of the domain and is used as a schema for the database. We use the object data model whose syntax matches both the user 2 ̆ 7 s target structure and the conceptual representation of extracting capabilities. Query evaluation consists in first determining the schema of the database as expressed by the user, and secondly populating the database through methods invoking extraction tools on a given source of documents. In a third step, it returns the output of the query against the resulting database. The two first steps define an object view of the given source(s) as a materialized extension of the current schema (each refinement of a query may add more structure, and thus more extracted data) followed by a non-materialized projection. Our approach is user-oriented: the object representation of data provides the user with the flexibility of asking his query with his understanding of the domain, and object views are built on-the-fly according to the user 2 ̆ 7 s organization of data. The modularity of the conceptual representation of extraction capabilities in a pool of schema components enables easy plug-in of new extracting tools...|$|R
50|$|Information Extraction is {{the part}} of a greater puzzle which deals with the problem of devising {{automatic}} methods for text management, beyond its transmission, storage and display. The discipline of information retrieval (IR) has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents. Another complementary approach is that of natural language processing (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. In terms of both difficulty and emphasis, IE deals with tasks in between both IR and NLP. In terms of input, IE assumes the existence of a set of documents in which each document follows a template, i.e. describes one or more entities or events {{in a manner that is}} similar to those in other documents but differing in the details. An example, consider a group of newswire articles on Latin American terrorism with each article presumed to be based upon one or more terroristic acts. We also define for any given IE task a template, which is a(or a set of) case frame(s) to hold the information contained in a single document. For the terrorism example, a <b>template</b> would have <b>slots</b> corresponding to the perpetrator, victim, and weapon of the terroristic act, and the date on which the event happened. An IE system for this problem is required to “understand” an attack article only enough to find data corresponding to the <b>slots</b> in this <b>template.</b>|$|R
40|$|In {{this chapter}} we are {{concerned}} with the use of CALL technology for literacy and (second) language learning automatic tutoring. In particular we shall be concerned with the use of text-to-speech synthesis, or TTS, as a tool for second language learning and for literacy tutoring particularly in presence of language disabilities. We will present a number of language learning applications in which TTS plays a fundamental role: for instance, TTS can be used to foster awareness of the contrastive features of the phonology of the second language. It can be used to power a speaking tutor for any self-instructional system and to voice feedback on an exercise. It {{can be used as a}} reader for dictation exercises where the need is to vary voice quality and speaking rate. TTS can also be used to give students hints about a listening comprehension task and other drills. We shall illustrate these various roles for TTS in computer-assisted language learning (CALL). Of course, we do not assume that synthesized speech is the only way to cope with oral language practice. In general, having a human tutor present this kind of activity guarantees much better results than TTS. The question is whether a human tutor may always be available when the student needs one. Because access to a human tutor is not always possible, TTS is worth pursuing. Moreover, there is at least one case in which a computer-based speaking tutor constitutes the only viable alternative to the human tutor: when mimicking the evolving levels of speaking proficiency in the second language (L 2), also known as levels of interlanguage (Selinker 1992), as will be explained further on. Finally, if feedback is generated on the fly – through automatic means such as filling <b>template</b> <b>slots</b> with words from an exercise – then use of synthesized voice is the only practical way to avoid having to predict and record all possible combinations of words in templates. Besides spoken and written text comprehension and sound-grapheme correspondences, an important and related goal of second language learning is acquiring the lexicon of the language. Indeed, lexical learning is the first challenge a second language student has to face...|$|R

