302|124|Public
5000|$|Visual <b>scene</b> <b>segmentation</b> is a pre-attentive {{process where}} stimuli are grouped {{together}} into specific objects against a background. Figure and background regions {{of an image}} activate different processing centres: figures use the lateral occipital areas (which involve object processing) and background engages dorso-medial areas.|$|E
50|$|The second {{semester}} {{took place}} in the Universitat de Girona, in Catalunya, Spain. Topics were currently highly relevant to scientific and industrial communities like the segmentation strategies, object recognition and description, the acquisition of 3D information, and the application of hardware for specific real time applications. This semester covered: fundamentals on robotics, autonomous robots, <b>scene</b> <b>segmentation</b> and interpretation, visual perception, real time image processing and study of the local culture.|$|E
40|$|Abstract. <b>Scene</b> <b>segmentation</b> is {{a crucial}} task in the {{structural}} analysis of film. State-of-the-art <b>scene</b> <b>segmentation</b> algorithms target Hollywoodtype films. There is an abundance of films that differ significantly from Hollywood films and need specific <b>scene</b> <b>segmentation</b> techniques. We propose a <b>scene</b> <b>segmentation</b> algorithm targeted at artistic archive documentaries. We evaluate the performance of our technique with archive and Hollywood film and obtain satisfactory results. ...|$|E
40|$|Short paperInternational audienceVideo {{segmentation}} requires separating foreground from background, but {{the general}} problem extends to more complicated <b>scene</b> <b>segmentations</b> of different objects and their multiple parts. We develop {{a new approach to}} interactive multi-label video segmentation where many objects are segmented simultaneously with consistent spatio-temporal boundaries, based on intuitive multi-colored brush scribbles. From these scribbles, we derive constraints to define a combinatorial problem known as the multicut—a problem notoriously difficult and slow to solve. We describe a solution using efficient heuristics to make multi-label video segmentation interactive. As our solution generalizes typical binary segmentation tasks, while also improving efficiency in multi-label tasks, our work shows the promise of multicuts for interactive video segmentation...|$|R
40|$|Abstract—This letter {{presents}} a benchmarking study for aerial image segmentation. We construct an image data set consisting of various aerial <b>scenes.</b> <b>Segmentations</b> generated by different human subjects {{are used as}} ground truth. We analyze the con-sistency between segmentations from different subjects. We select six leading segmentation algorithms, which include not only the algorithms specifically designed for aerial images but also more generally applicable algorithms. We also select a recently pro-posed algorithm due to its promising performance in handling texture regions. We apply these algorithms to the aerial image data set and quantitatively evaluate their performance. We interpret the evaluation results based {{on the characteristics of}} algorithms, which provide general guidance for selecting proper algorithms in specific applications. Index Terms—Aerial image dataset, image segmentation. I...|$|R
40|$|We {{propose a}} self-supervised {{framework}} that learns to group visual entities {{based on their}} rate of co-occurrence in space and time. To model statistical dependencies between the entities, {{we set up a}} simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie <b>scene</b> <b>segmentations</b> that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories...|$|R
40|$|Video <b>scene</b> <b>segmentation</b> {{plays an}} {{important}} role in video structure analysis. In this paper, we propose a time constraint dominant-set clustering algorithm for shot grouping and <b>scene</b> <b>segmentation,</b> in which the similarity between shots is based on autocorrelogram feature, motion feature and time constraint. Therefore, the visual evidence and time constraint contained in the video content are effectively incorporated into a unified clustering framework. Moreover, the number of clusters in our algorithm does not need to be predefined and thus it provides an automatic framework for <b>scene</b> <b>segmentation.</b> Compared with normalized cut clustering based <b>scene</b> <b>segmentation,</b> our algorithm can achieve more accurate results and requires less computing resources...|$|E
30|$|Unlike the {{traditional}} object detection and classification which globally works on an image or a patch, the <b>scene</b> <b>segmentation</b> is a pixel-wise classification which requires more accurate boundary localization of each object and area inside the images. For instance {{in case of}} the road <b>scene</b> <b>segmentation,</b> one needs to precisely separate the sidewalk for the pedestrian from the road body.|$|E
40|$|Abstract. This work {{deals with}} the problem of {{automatic}} temporal segmentation of a video into elementary semantic units known as scenes. Its novelty lies in the use of high-level audio information, in the form of audio events, for the improvement of <b>scene</b> <b>segmentation</b> performance. More specifically, the proposed technique is built upon a recently proposed audio-visual <b>scene</b> <b>segmentation</b> approach that involves the construction of multiple scene transition graphs (STGs) that separately exploit information coming from different modalities. In the extension of the latter approach presented in this work, audio event detection results are introduced to the definition of an audio-based scene transition graph, while a visual-based scene transition graph is also defined independently. The results of these two types of STGs are subsequently combined. The results of the application of the proposed technique to broadcast videos demonstrate the usefulness of audio events for <b>scene</b> <b>segmentation</b> and highlight the importance of introducing additional high-level information to the <b>scene</b> <b>segmentation</b> algorithms...|$|E
40|$|We {{propose a}} novel <b>scene</b> image <b>segmentation</b> {{algorithm}} based on Perceptual Organization. We develop a Perceptual Organization model by quantitatively incorporating {{a list of}} Gestalt laws. The Perceptual Organization model can capture the non-accidental structural relations among the constituent parts of an object. The experimental results show that our proposed method outperformed two competing image segmentation approaches and achieved good segmentation quality on various natural scene environments...|$|R
40|$|Topic modeling: Finding {{meaningful}} topics {{discussed in}} large scale documents. Beneficial to automatic document analysis and understanding. Computational linguistic: e. g., the n-gram model, adaptor grammar. Computer vision. Using PDP/HPDP for image annotation, image <b>segmentation,</b> <b>scene</b> learning, and etc...|$|R
50|$|Stereoscopic to 2D-plus-Depth {{conversion}} involves several algorithms including <b>scene</b> change detection, <b>segmentation,</b> {{motion estimation}} and image matching. Automatic stereo to 2D+Depth conversion {{is now possible}} due to new high performance software and GPU technology, even in live real-time mode.|$|R
40|$|<b>Scene</b> <b>segmentation</b> {{is one of}} {{the most}} {{important}} tasks in research and commercial applications. With the rapid development of Internet, there is an increasing demand for real-time, mobile and autonomous multi-media system to increase the user-friendliness of e-shopping experience and to provide value-added e-services. Traditional <b>scene</b> <b>segmentation</b> techniques, which are mainly sequential and offline, cannot segment images in a real-time manner. This paper introduces a real-time, multi-media system known as Intelligent Java Agent Development Environment (iJADE) Scene Segmentater-an agent-based <b>scene</b> <b>segmentation</b> system using Solely Excitatory Oscillator networks (SEON) for real-time <b>scene</b> <b>segmentation.</b> It is based on an intelligent multi-agent based model, namely, iJADE, which supports various e-commerce applications. Using a gallery of 1200 images, our system shows an average segmentation rate of over 98 %. iJADE Scene Segmentater segments faster and more powerfully than other contemporary image segmentation methods. The improvement in speed is more significant for large images. Department of Computin...|$|E
40|$|Contextual {{information}} provides important cues for disambiguating visually similar pixels in <b>scene</b> <b>segmentation.</b> In this paper, we {{introduce a}} neuron-level Selective Context Aggregation (SCA) module for <b>scene</b> <b>segmentation,</b> comprised of a contextual dependency predictor and a context aggregation operator. The dependency predictor is implicitly trained to infer contextual dependencies between different image regions. The context aggregation operator augments local representations with global context, which is aggregated selectively at each neuron {{according to its}} on-the-fly predicted dependencies. The proposed mechanism enables data-driven inference of contextual dependencies, and facilitates context-aware feature learning. The proposed method improves strong baselines built upon VGG 16 on challenging <b>scene</b> <b>segmentation</b> datasets, which demonstrates its effectiveness in modeling context information...|$|E
40|$|This work {{deals with}} the problem of {{automatic}} temporal segmentation of a video into elementary semantic units known as scenes. Its novelty lies in the use of high-level audio information in the form of audio events for the improvement of <b>scene</b> <b>segmentation</b> performance. More specifically, the proposed technique is built upon a recently proposed audio-visual <b>scene</b> <b>segmentation</b> approach that involves the construction of multiple scene transition graphs (STGs) that separately exploit information coming from different modalities. In the extension of the latter approach presented in this work, audio event detection results are introduced to the definition of an audio-based scene transition graph, while a visual-based scene transition graph is also defined independently. The results of these two types of STGs are subsequently combined. The application of the proposed technique to broadcast videos demonstrates the usefulness of audio events for <b>scene</b> <b>segmentation.</b> 1...|$|E
30|$|The image {{segmentation}} procedure {{is one of}} the most important steps involved in the process of image analysis. This refers to the task of partitioning a given image into multiple regions and is typically used to locate and mark objects and boundaries in input <b>scenes.</b> After <b>segmentation,</b> the image represents a set of data far more suitable for further algorithmic processing and decision making. Image segmentation algorithms are a very broad field and they have received significant amount of research interest.|$|R
40|$|Image {{segmentation}} is {{the process}} of partitioning an image into multiple parts, so that each part or each region corresponds to an object or area of interest that is more significant and easier to analyze. Several general-purpose algorithms and techniques have been developed for image segmentation. This paper describes the different segmentation techniques used to achieve outdoor <b>scene</b> image <b>segmentation.</b> Unlike other surveys that only describe and compare qualitatively different approaches, this survey deals with a real quantitative comparison of the F-measure...|$|R
40|$|We {{explore the}} {{capabilities}} of Auto-Encoders to fuse the information available from cameras and depth sensors, and to reconstruct missing data, for scene understanding tasks. In particular we consider three input modalities: RGB images; depth images; and semantic label information. We seek to generate complete <b>scene</b> <b>segmentations</b> and depth maps, given images and partial and/or noisy depth and semantic data. We formulate this objective of reconstructing one or more types of scene data using a Multi-modal stacked Auto-Encoder. We show that suitably designed Multi-modal Auto-Encoders can solve the depth estimation and the semantic segmentation problems simultaneously, in the partial or even complete absence {{of some of the}} input modalities. We demonstrate our method using the outdoor dataset KITTI that includes LIDAR and stereo cameras. Our results show that as a means to estimate depth from a single image, our method is comparable to the state-of-the-art, and can run in real time (i. e., less than 40 ms per frame). But we also show that our method has a significant advantage over other methods in that it can seamlessly use additional data that may be available, such as a sparse point-cloud and/or incomplete coarse semantic labels. Cesar Cadena, Anthony Dick and Ian D. Rei...|$|R
40|$|<b>Scene</b> <b>segmentation</b> is a {{challenging}} problem {{that can be}} a prelude to many further applications in computer vision. This paper describes a novel MAP framework for automatic and robust <b>scene</b> <b>segmentation</b> in image sequences under mobile camera without any human interaction. First, an initial partition of the scene is achieved by watershed segmentation algorithm. However segmentation based on single frame often suffers from over-segmentation and simplified-segmentation problem due to the absence of 3 D information. To eliminate these problems, we propose to analyze the 2 D motions on {{the two sides of the}} segmentation contours to judge whether a contour is the ”space contour ” or a ”texture contour”, thus eliminate over-segmentations and simplified-segmentations. To achieve the best segmentation, we use multi key frames of image sequences to seek for the global optimal <b>scene</b> <b>segmentation</b> by employing a MAP estimator. We demonstrate high accuracy and robustness of our algorithm by experiments on real video sequences...|$|E
40|$|We {{present a}} new {{approach}} for automatic video <b>scene</b> <b>segmentation</b> and content based indexing. Our approach detects video shots and builds a collection of key frames and representative frames. <b>Scene</b> <b>segmentation</b> and video indexing {{are based on a}} temporally windowed principal component analysis of a subsampled version of the videosequence. Two discriminants are derivedfrom the principal components on a frame by frame basis. The discriminants are used for scene change detection and key frame extraction and classification into relevant clips. The system creates an adjacency matrix to build the scene transition graph. The scene transition graph allows easy access to video image-based information...|$|E
40|$|We {{propose a}} stereo vision based {{obstacle}} detection and <b>scene</b> <b>segmentation</b> algorithm appropriate for autonomous vehicles. Our algorithm {{is based on}} an innovative extension of the Stixel world, which neglects computing a disparity map. Ground plane and stixel distance estimation is improved by exploiting an online learned color model. Furthermore, the stixel height estimation is leveraged by an innovative joined membership scheme based on color and disparity information. Stixels are then used as an input for the semantic <b>scene</b> <b>segmentation</b> providing scene understanding, which can be further used as a comprehensive middle level representation for high-level object detectors...|$|E
40|$|In {{this paper}} {{we present a}} {{semi-automatic}} segmentation approach suitable for extracting object contours as a precursor to 2 D shape modeling. The approach is a modified and extended version of an existing state-of-the-art approach based {{on the concept of}} a Binary Partition Tree (BPT) [1]. The resulting segmentation tool facilitates quick and easy extraction of an object’s contour via a small amount of user interaction that is easy to perform, even in complicated <b>scenes.</b> Illustrative <b>segmentation</b> results are presented and the usefulness of the approach in generating object shape models is discussed...|$|R
40|$|This {{research}} {{deals with}} the problem of text <b>segmentation</b> in <b>scene</b> images. Introduction {{deals with the}} information contained in an image and the different properties that will be useful for image segmentation. After that, the process of extraction of textual information is explained step by step. Furthermore, the problem of <b>scene</b> text <b>segmentation</b> is described more precisely and an overview of more popular existing methods is given. Text segmentation method is created and implemented using C++ programming language with OpenCV library. Finally, algorithm is evaluated with images from ICDAR 2013 test dataset...|$|R
40|$|Seghers D., "Local {{graph-based}} probabilistic {{representation of}} object shape and appearance for model-based medical image segmentation", Proefschrift voorgedragen tot het behalen van het doctoraat in de ingenieurswetenschappen, K. U. Leuven, November 2008, Leuven, Belgium. Image segmentation {{is the process}} of partitioning a digital image into r egions originating from different objects in the <b>scene.</b> The <b>segmentation</b> of anatomical objects is indispensable for the analysis of medical imag es. It enables the assessment of anatomical measurements and it is a pos sible means towards diagnosis, therapy planning and visualization.|$|R
40|$|Digital {{video is}} rapidly {{becoming}} {{an important source}} of information and entertainment, and is used in a host of multimediapplications. With the size of digital video collections growing to many thousands of hours, technology is needed to allow rapid browsing of videos. One way to summarize a video is to select poster frames to represent segments of the video. Previous techniques for extracting poster frames were based on <b>scene</b> <b>segmentation,</b> using color histograms or optical flow. To provide more informative poster frames, this work combines algorithms for extracting image content, specifically faces and on-screen text, with existing <b>scene</b> <b>segmentation</b> technology...|$|E
40|$|This paper {{presents}} {{a novel approach}} to natural <b>scene</b> <b>segmentation.</b> It uses both color and texture features in cooperation to provide comprehensive knowledge about every pixel in the image. A novel scheme for the collection of training samples, based on homogeneity, is proposed. Natural <b>scene</b> <b>segmentation</b> is carried out using a two-stage hierarchical self-organizing map (HSOM). The proposed method confirms that the sample selection based on homogeneity and the self-learning ability and adaptability of the HSOM, coupled with the information fusion mechanism, can lead to good segmentation result, which is validated by experiments {{on a variety of}} natural scene images...|$|E
40|$|Abstract—Stereo vision {{systems for}} 3 D {{reconstruction}} have been deeply studied and are nowadays capable {{to provide a}} reasonably accurate estimate of the 3 D geometry of a framed scene. They are commonly used to merely extract the 3 D structure of the scene. However, {{a great variety of}} applications is not interested in the geometry itself, but rather in scene analysis operations, among which <b>scene</b> <b>segmentation</b> is a very important one. Classically, <b>scene</b> <b>segmentation</b> has been tackled by means of color information only, but {{it turns out to be}} a badly conditioned image processing operation which remains very challenging. This paper proposes a new framework for <b>scene</b> <b>segmentation</b> where color information is assisted by 3 D geometry data, obtained by stereo vision techniques. This approach resembles in some way what happens inside our brain, where the two different views coming from the eyes are used to recognize the various object in the scene and by exploiting a pair of images instead of just one allows to greatly improve the segmentation quality and robustness. Clearly the performance of the approach is dependent on the specific stereo vision algorithm used in order to extract the geometry information. This paper investigates which stereo vision algorithms are best suited to this kind of analysis. Experimental results confirm the effectiveness of the proposed framework and allow to properly rank stereo vision systems on the basis of their performances when applied to the <b>scene</b> <b>segmentation</b> problem. Keywords-scene segmentation; stereo; 3 D; I...|$|E
40|$|We {{consider}} {{object recognition}} {{as the process}} of attaching meaningful labels to specific regions of an image, and propose a model that learns spatial relationships between objects. Given a set of images and their associated text (e. g. keywords, captions, descriptions), {{the objective is to}} segment an image, in either a crude or sophisticated fashion, then to find the proper associations between words and regions. Previous models are limited by the scope of the representation. In particular, they fail to exploit spatial context in the images and words. We develop a more expressive model that takes this into account. We formulate a spatially consistent probabilistic mapping between continuous image feature vectors and the supplied word tokens. By learning both word-to-region associations and object relations, the proposed model augments <b>scene</b> <b>segmentations</b> due to smoothing implicit in spatial consistency. Context introduces cycles to the undirected graph, so we cannot rely on a straightforward implementation of the EM algorithm for estimating the model parameters and densities of the unknown alignment variables. Instead, we develop an approximate EM algorithm that uses loopy belief propagation in the inference step and iterative scaling on the pseudo-likelihood approximation in the parameter update step. The experiments indicate that our approximate inference and learning algorithm converges to good local solutions. Experiments on a diverse array of images show that spatial context considerably improves the accuracy of object recognition. Mos...|$|R
40|$|Due to the {{complexity}} of real scenes and the fragility of fully automated vision techniques, results from many automated modeling systems are disappointing. Automated techniques often require manual clean-up and postprocessing to segment the scene into coherent objects and surfaces, or to triangulate sparse point matches. They may also be required to enforce geometric constraints such as known orientations of surfaces. For instance, building interiors and exteriors provide vertical and horizontal lines and parallel and perpendicular planes. In this paper, we attack the 3 D modeling problem from the other side: we specify some geometric knowledge ahead of time (e. g., known orientations of lines, co-planarity of points, initial <b>scene</b> <b>segmentations),</b> and use these constraints to guide our matching and reconstruction algorithms. We present two interactive (semi-automated) systems for recovering 3 D models of large-scale environments from multiple images. 2 3 D modeling from panoramas Our first system[1] uses one or more panoramic image mosaics, i. e., collections of images taken from the same viewpoint that have been registered together. Unlike previous work on 3 D reconstruction from multiple panoramas, our 3 D modeling system exploits important regularities present in the environment, such as walls with known orientations. Fortunately, the man-made world is full of constraints such as parallel lines, lines with known directions, planes with lines and points on them. Using these constraints, we can construct a fairly complex 3 D mode...|$|R
40|$|Shadow {{physical}} phenomena {{observed in}} natural <b>scenes.</b> Image <b>segmentation,</b> tracking, recognition algorithms to fail or {{can cause a}} shadow. In this paper, conducted a {{research in the field}} of research on the perception of image complexity and classification of images, remove the shade. The {{purpose of this paper is}} two-fold: in the first place, an attempt is to be presented for consideration to overcome some of the fundamental problems of light reflection, color constancy, and at the second place segmentation is applied in place to provide better color information. Experimental results show that the proposed method can detect and eliminate the shadows effectively and maintained the color information properly...|$|R
40|$|Stereo vision {{systems for}} 3 D {{reconstruction}} have been deeply studied and are nowadays capable {{to provide a}} reasonably accurate estimate of the 3 D geometry of a framed scene. They are commonly used to merely extract the 3 D structure of the scene. However, {{a great variety of}} applications is not interested in the geometry itself, but rather in scene analysis operations, among which <b>scene</b> <b>segmentation</b> is a very important one. Classically, <b>scene</b> <b>segmentation</b> has been tackled by means of color information only, but {{it turns out to be}} a badly conditioned image processing operation which remains very challenging. This paper proposes a new framework for <b>scene</b> <b>segmentation</b> where color information is assisted by 3 D geometry data, obtained by stereo vision techniques. This approach resembles in some way what happens inside our brain, where the two different views coming from the eyes are used to recognize the various object in the scene and by exploiting a pair of images instead of just one allows to greatly improve the segmentation quality and robustness. Clearly the performance of the approach is dependent on the specific stereo vision algorithm used in order to extract the geometry information. This paper investigates which stereo vision algorithms are best suited to this kind of analysis. Experimental results confirm the effectiveness of the proposed framework and allow to properly rank stereo vision systems on the basis of their performances when applied to the <b>scene</b> <b>segmentation</b> problem...|$|E
40|$|Abstract. Content-based video {{retrieval}} {{requires an}} effective <b>scene</b> <b>segmentation</b> technique to divide a long video file into meaningful high-level aggregates of shots called scenes. Each scene {{is part of}} a story. Browsing these scenes unfolds the entire story of a film. In this paper, we first investigate recent <b>scene</b> <b>segmentation</b> techniques that belong to the visual-audio alignment approach. This approach segments a video stream into visual scenes and an audio stream into audio scenes separately and later aligns these boundaries to create the final scene boundaries. In contrast, we propose a novel audio-assisted <b>scene</b> <b>segmentation</b> technique that utilizes audio information to remove false boundaries generated from segmentation by visual information alone. The crux of our technique is the new dissimilarity measure based on analysis of statistical properties of audio features and a concept in information theory. The experimental results on two full-length films {{with a wide range of}} camera motion and a complex composition of shots demonstrate the effectiveness of our technique compared with that of the visual-audio alignment techniques. ...|$|E
40|$|Automatic video {{segmentation}} is {{the first}} and necessary step for organizing a long video file into smaller units for subsequent browsing and retrieval. The smallest basic unit is shot [...] a contiguous sequence of frames recorded from a single camera operation. Relevant shots are then grouped into a high-level unit called scene that conveys some meaning to viewers. In this paper, we first give a stricter scene definition. We then investigate the performance of two recent <b>scene</b> <b>segmentation</b> techniques. Both utilize visual features of entire video frames for grouping shots into scenes. Our experimental results on two full-length films reveal an interesting insight, suggesting a better approach for <b>scene</b> <b>segmentation...</b>|$|E
40|$|Abstract. We {{consider}} {{object recognition}} {{as the process}} of attaching meaningful labels to specific regions of an image, and propose a model that learns spatial relationships between objects. Given a set of images and their associated text (e. g. keywords, captions, descriptions), {{the objective is to}} segment an image, in either a crude or sophisticated fashion, then to find the proper associations between words and regions. Previous models are limited by the scope of the representation. In particular, they fail to exploit spatial context in the images and words. We develop a more expressive model that takes this into account. We formulate a spatially consistent probabilistic mapping between continuous image feature vectors and the supplied word tokens. By learning both word-to-region associations and object relations, the proposed model augments <b>scene</b> <b>segmentations</b> due to smoothing implicit in spatial consistency. Context introduces cycles to the undirected graph, so we cannot rely on a straightforward implementation of the EM algorithm for estimating the model parameters and densities of the unknown alignment variables. Instead, we develop an approximate EM algorithm that uses loopy belief propagation in the inference step and iterative scaling on the pseudo-likelihood approximation in the parameter update step. The experiments indicate that our approximate inference and learning algorithm converges to good local solutions. Experiments on a diverse array of images show that spatial context considerably improves the accuracy of object recognition. Most significantly, spatial context combined with a nonlinear discrete object representation allows our models to cope well with over-segmented scenes. 2 Peter Carbonetto et al. ...|$|R
40|$|In {{this work}} we {{investigate}} {{the problem of}} road <b>scene</b> semantic <b>segmentation</b> using Deconvolutional Networks (DNs). Several constraints limit the practical performance of DNs in this context: firstly, the paucity of existing pixel-wise labelled training data, and secondly, the memory constraints of embedded hardware, which rule out the practical use of state-of-the-art DN architectures such as fully convolutional networks (FCN). To address the first constraint, we introduce a Multi-Domain Road <b>Scene</b> Semantic <b>Segmentation</b> (MDRS 3) dataset, aggregating data from six existing densely and sparsely labelled datasets for training our models, and two existing, separate datasets for testing their generalisation performance. We show that, while MDRS 3 offers a greater volume and variety of data, end-to-end training of a memory efficient DN does not yield satisfactory performance. We propose a new training strategy to overcome this, based on (i) {{the creation of a}} best-possible source network (S-Net) from the aggregated data, ignoring time and memory constraints; and (ii) the transfer of knowledge from S-Net to the memory-efficient target network (T-Net). We evaluate different techniques for S-Net creation and T-Net transferral, and demonstrate that training a constrained deconvolutional network in this manner can unlock better performance than existing training approaches. Specifically, we show that a target network can be trained to achieve improved accuracy versus an FCN despite using less than 1 % of the memory. We believe that our approach can be useful beyond automotive scenarios where labelled data is similarly scarce or fragmented and where practical constraints exist on the desired model size. We make available our network models and aggregated multi-domain dataset for reproducibility. Comment: submitted as a conference pape...|$|R
40|$|AbstractBased {{on the law}} {{of visual}} perception, we {{proposed}} three elements to describe the scene image: ground, objects and sky, and the equation of single view scene image. By using the fast classification, which is based on the independent graphic function code and three elements, the single view <b>scene</b> image <b>segmentation</b> is realized. The main innovation of this method is the use of ICA methods based on three elements to define the graphic, the independent graphic function code, the parent element and the selection principle and to divide the image according to the characteristics of the three elements without the precise analysis of the color, texture and geometric shapes of the image. The experimental results show good performance for outdoor scene...|$|R
