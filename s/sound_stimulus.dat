172|513|Public
25|$|Pitch {{constancy}} {{refers to}} the ability to perceive pitch identity across changes in acoustical properties, such as loudness, temporal envelope, or timbre. The importance of cortical regions lateral to A1 for pitch coding is also supported by studies of human cortical lesions and functional magnetic resonance imaging (fMRI) of the brain. These data suggest a hierarchical system for pitch processing, with more abstract properties of <b>sound</b> <b>stimulus</b> processed further along the processing pathways.|$|E
25|$|Test: Finally, infants {{are tested}} in the lab using the Headturn-preference procedure, a {{behavioral}} data-collection tool that measures preferences for one kind of auditory stimulus over another. The Headturn-preference procedure maintains that an infant will turn its head towards a stimulus it prefers. This procedure is conducted in a testing booth, with the infant sitting on the lap {{of his or her}} mother. A light is located {{on either side of the}} infant. The trial begins when the infant is looking straight ahead. Mother and experimenter are required to wear tight-fitting earphones which deliver masking music for the duration of the entire procedure. This is done to guarantee that neither mother, nor experimenter bias the infant's response. During each trial, one sidelight flashes, urging the infant to look at it. Once the infant turns his or her head and looks at the light, the <b>sound</b> <b>stimulus</b> is played. The stimulus continues to play until the sound finishes or the infant looks away. When the infant turns away from the source for at least two seconds, sound and light turn off and the trial ends. A new trial begins when the infant looks at the center panel again.|$|E
5000|$|... #Caption: Volley Theory of Hearing {{demonstrated}} by four neurons firing at a phase-locked frequency to the <b>sound</b> <b>stimulus.</b> The total response corresponds with the stimulus.|$|E
40|$|Visual {{attention}} in tranquility evaluations has been examined by eye tracking experiments using audiovisual materials collected in traditional villages of China. The {{results show that}} without <b>sound</b> <b>stimuli,</b> the attention areas in tranquility evaluations are more concentrated, compared with those in visual aesthetic quality evaluations. With <b>sound</b> <b>stimuli,</b> the attention areas of tranquility evaluations disperse significantly from those without <b>sound</b> <b>stimuli,</b> where artificial <b>sounds</b> tend to expand the visual attention area on corresponding artificial landscape elements, whereas natural sounds promote larger attention areas on natural landscape elements. During information extraction for tranquility evaluations, both with and without <b>sound</b> <b>stimuli,</b> buildings and facilities, the sky, and vegetation are attractive landscape elements...|$|R
3000|$|As {{shown in}} Figure 1 (b), for the {{clearness}} factor, the factor {{scores for the}} major and minor onomatopoeic representations were close to those for the real <b>sound</b> <b>stimuli</b> as a whole. Table 3 also shows that the averaged factor score differences between the real <b>sound</b> <b>stimuli</b> and both the major and minor onomatopoeia were the smallest for the clearness factor. Furthermore, the correlation coefficients of the clearness factor scores between the real <b>sound</b> <b>stimuli</b> and the major or minor onomatopoeic stimuli were both statistically significant at [...]...|$|R
30|$|For the powerfulness factor, factor {{scores for}} the major and minor onomatopoeia were {{different}} from those for the corresponding <b>sound</b> <b>stimuli</b> as a whole, as shown in Figure 1 (c) and Table 3. Moreover, no correlation of the powerfulness factor scores between the real <b>sound</b> <b>stimuli</b> and the onomatopoeic stimuli was found.|$|R
5000|$|Kravkov S. V. The {{influence}} of the loudness of the indirect <b>sound</b> <b>stimulus</b> on the color sensitivity of the eye / Acta Ophthalmologica, Volume 17, Issue 3, October 1939, pp. 324-331 ...|$|E
50|$|While the VCN bushy cells {{aid in the}} {{location}} of a <b>sound</b> <b>stimulus</b> on the horizontal axis via their inputs to the superior olivary complex, type IV cells may participate in localization of the <b>sound</b> <b>stimulus</b> on the vertical axis. The pinna selectively amplifies frequencies, resulting in reduced sound energy at specific frequencies in certain regions of space. The complicated firing patterns of type IV cells makes them especially suited to detecting these notches, and with the combined power of these two localization systems, an ordinary person can locate where a firework explodes without the use of their eyes.|$|E
50|$|The {{acoustic}} reflex threshold (ART) {{is the sound}} pressure level (SPL) from which a <b>sound</b> <b>stimulus</b> with a given frequency will trigger the {{acoustic reflex}}. The ART {{is a function of}} sound pressure level and frequency.|$|E
40|$|In {{the present}} study, babies calming {{responses}} to two different cases of swinging stimuli (including mothers swung the babies in their arms, and mechanical/electric swinging), and <b>sound</b> <b>stimuli</b> (including brown noise), {{as well as}} 2 ̆ 7 no stimuli 2 ̆ 7 case were assessed with behavioral and physiological indices. The 2 ̆ 7 brown noise 2 ̆ 7 which {{was presented to the}} subjects was 70 dB (A-weighted sound pressure level). To carry out this study, 8 babies (two month old) were exposed to both swinging and <b>sound</b> <b>stimuli,</b> whether individually or simultaneously. The main results of this study revealed that each swinging <b>stimuli</b> and <b>sound</b> <b>stimuli</b> calmed crying babies down in compare with the 2 ̆ 7 no stimuli 2 ̆ 7 case. The calming effect appeared within one minute after the stimulation. On the other hand, there were not multiplier effects when each baby was exposed to both swinging and <b>sound</b> <b>stimuli</b> at the same time. Overall, {{it can be said that}} both of swinging <b>stimuli,</b> and <b>sound</b> <b>stimuli</b> were effective ways to calm down two month old babies within one minute...|$|R
3000|$|According to Table 3, for {{the emotion}} factor, the factor {{scores for the}} real <b>sound</b> <b>stimuli</b> were closer to those for the major onomatopoeic {{representations}} than to those for the minor onomatopoeic representations. The correlation coefficient of the emotion factor scores between the real <b>sound</b> <b>stimuli</b> and the major onomatopoeic stimuli was statistically significant at [...]...|$|R
30|$|The {{obtained}} rating {{scores were}} averaged across participants for each scale {{and for each}} onomatopoeic stimulus. To compare impressions between actual <b>sound</b> <b>stimuli</b> and onomatopoeic representations, factor analysis {{was applied to the}} averaged scores for onomatopoeic representations together with those for the <b>sound</b> <b>stimuli</b> (i.e., the rating results of auditory impressions) obtained in our previous experiments [7].|$|R
50|$|A {{number of}} studies have shown that a human fetus will respond to sound stimuli coming from the outside world. In a series of 214 tests {{conducted}} on 7 pregnant women, a reliable increase in fetal movement was detected in the minute directly following the application of a <b>sound</b> <b>stimulus</b> to the abdomen of the mother with a frequency of 120 per second.|$|E
50|$|Vibroacoustic {{stimulation}} (VAS), {{sometimes referred}} to as fetal vibroacoustic stimulation or fetal acoustic stimulation test (FAST), is the application of a vibratory <b>sound</b> <b>stimulus</b> to the abdomen of a pregnant woman to induce FHR (fetal heart rate) accelerations. The presence of FHR accelerations reliably predicts the absence of fetal metabolic acidemia. Vibroacoustic stimulation is typically used during a nonstress test (NST).|$|E
50|$|Pitch {{constancy}} {{refers to}} the ability to perceive pitch identity across changes in acoustical properties, such as loudness, temporal envelope, or timbre. The importance of cortical regions lateral to A1 for pitch coding is also supported by studies of human cortical lesions and functional magnetic resonance imaging (fMRI) of the brain. These data suggest a hierarchical system for pitch processing, with more abstract properties of <b>sound</b> <b>stimulus</b> processed further along the processing pathways.|$|E
50|$|There {{are many}} {{different}} qualities in <b>sound</b> <b>stimuli</b> including loudness, pitch and timbre.|$|R
50|$|ASSR is evoked using {{repeated}} <b>sound</b> <b>stimuli</b> {{presented at}} a high rep rate rather than an abrupt sound at a relatively low rep rate.|$|R
3000|$|From Figure 1 (a), <b>sound</b> <b>stimuli</b> such as [...] "owl hooting (no. 6)," [...] [...] "vehicle horn (no. 9)," [...] [...] "sound of {{a flowing}} stream (no. 11)," [...] [...] "sound of a noisy {{construction}} site (no. 12)," [...] and [...] "sound of a wind chime (no. 34)" [...] displayed highly {{positive or negative}} emotion factor scores (e.g., inducing strong impressions of tastefulness or tastelessness and pleasantness or unpleasantness). However, the factor scores for the onomatopoeic representations of the same <b>sound</b> <b>stimuli</b> were not as positively or negatively high. On the other hand, the factor scores for the [...] "major" [...] onomatopoeic representations of stimuli such as [...] "sound of water dripping (no. 3)," [...] [...] "sound of a temple bell (no. 25)," [...] and [...] "beach sound (no. 30)" [...] were nearly equal {{to those of the}} corresponding real <b>sound</b> <b>stimuli.</b>|$|R
50|$|Sound {{localization}} is {{the ability}} to correctly identify the directional location of sounds. A <b>sound</b> <b>stimulus</b> localized in the horizontal plane is called azimuth; in the vertical plane it is referred to as elevation. The time, intensity, and spectral differences in the sound arriving at the two ears are used in localization. Localization of low frequency sounds is accomplished by analyzing interaural time difference (ITD). Localization of high frequency sounds is accomplished by analyzing interaural level difference (ILD).|$|E
50|$|When {{presented}} with a high-intensity <b>sound</b> <b>stimulus,</b> the stapedius and tensor tympani muscles of the ossicles contract. The stapedius stiffens the ossicular chain by pulling the stapes (stirrup) {{of the middle ear}} away from the oval window of the cochlea and the tensor tympani muscle stiffens the ossicular chain by loading the tympanic membrane when it pulls the malleus (hammer) in toward the middle ear. The reflex decreases the transmission of vibrational energy to the cochlea, where it is converted into electrical impulses to be processed by the brain.|$|E
50|$|The {{integration}} of a <b>sound</b> <b>stimulus</b> {{is a result}} of analyzing frequency (pitch), intensity, and spatial localization of the sound source. Once a sound source has been identified, the cells of lower auditory pathways are specialized to analyze physical sound parameters. Summation is observed when the loudness of a sound from one stimulus is perceived as having been doubled when heard by both ears instead of only one. This process of summation is called binaural summation and is the result of different acoustics at each ear, depending on where sound is coming from.|$|E
3000|$|... [...]). The {{impressions}} of muddiness (or clearness) and brightness (or darkness) for the onomatopoeic representations {{were similar to}} those for the corresponding real <b>sound</b> <b>stimuli.</b>|$|R
40|$|It {{is known}} {{that there is a}} little {{difference}} between Japanese and Westerners in the perception for sounds. It may be caused by the differences of their culture and different parts of their brain where the sounds are perceived. In order to make the causes clear, a few experiments were performed. The electroencephalogram (EEG) of twelve Japanese subjects and nein German subjects were measured without <b>sound</b> <b>stimuli.</b> Then their EEG was measured while being exposed to <b>sound</b> <b>stimuli.</b> As these stimuli, white noises, road traffic noises and three kinds of bird singings were used. In addition to the measurement of EEG, the subjects were asked to judge the "noisiness " of the <b>sound</b> <b>stimuli</b> by "the method of magnitude estimation". The results may be summarized as follows: (1) For all kinds of <b>sound</b> <b>stimuli,</b> the values of the "noisiness " evaluated by Japanese were higher than by Germans, but for white noises, {{there were no significant differences}} between the two. (2) In regard to EEG, the personal differences between individuals were not small. But the differences between Japanese and Germans were not clear except bird singings. In regard to electric potentials of alpha- 2 waves, there were also differences in bird singings between the two. ...|$|R
5000|$|Jombik P, Bahyl V, Drobny M, Spodniak P. Vestibulo-ocular (oVEMP) {{responses}} {{produced by}} bone-conducted <b>sound</b> <b>stimuli</b> {{applied to the}} mid-sagittal plane of the head. J Vestib Res. 2008;18(2-3):117-28.|$|R
50|$|Of {{the various}} {{theories}} and notions created by Rinne, Rutherford, and their followers, the frequency theory was born. In general, it claimed that all sounds were encoded {{to the brain}} by neurons firing {{at a rate that}} mimics the frequency of the sound. However, because humans can hear frequencies up to 20,000 Hz but neurons cannot fire at these rates, the frequency theory had a major flaw. In an effort to combat this fault, Ernest Wever and Charles Bray, in 1930, proposed the volley theory, claiming that multiple neurons could fire in a volley to later combine and equal the frequency of the original <b>sound</b> <b>stimulus.</b> Through more research, it was determined that because phase synchrony is only accurate up to about 1000 Hz, volley theory cannot account for all frequencies at which we hear.|$|E
5000|$|Vocalizations of {{this species}} are very complex, {{especially}} a long-call display utilized by these animals, perhaps to regulate spacing and defined territory. Surprisingly, experimental playback of solo male calls caused the owners of a particular territory {{to move away from}} the recording, and recordings of duetting caused the territory owners to duet in return and to travel parallel to the speaker. However, any approximate <b>sound</b> <b>stimulus</b> can cause duetting of territory owners, and many direct observations of duetting neighbors were observed to cause the territorial owners to move towards the calling, where they sometimes confronted each other across a small space. Lone individuals in the established territory of another pair do not normally vocalize, since they may be vigorously attacked if the [...] "owners" [...] of the territory find them.|$|E
50|$|Test: Finally, infants {{are tested}} in the lab using the Headturn-preference procedure, a {{behavioral}} data-collection tool that measures preferences for one kind of auditory stimulus over another. The Headturn-preference procedure maintains that an infant will turn its head towards a stimulus it prefers. This procedure is conducted in a testing booth, with the infant sitting on the lap {{of his or her}} mother. A light is located {{on either side of the}} infant. The trial begins when the infant is looking straight ahead. Mother and experimenter are required to wear tight-fitting earphones which deliver masking music for the duration of the entire procedure. This is done to guarantee that neither mother, nor experimenter bias the infant's response. During each trial, one sidelight flashes, urging the infant to look at it. Once the infant turns his or her head and looks at the light, the <b>sound</b> <b>stimulus</b> is played. The stimulus continues to play until the sound finishes or the infant looks away. When the infant turns away from the source for at least two seconds, sound and light turn off and the trial ends. A new trial begins when the infant looks at the center panel again.|$|E
30|$|As {{mentioned}} {{in the previous section}} regarding the emotion factor, there is the possibility that differences in impressions between real <b>sound</b> <b>stimuli</b> and onomatopoeic representations may be influenced by sound source recognition. That is, impressions of onomatopoeic representations may be similar to those for real <b>sound</b> <b>stimuli</b> when the <b>sound</b> source can be correctly recognized from the onomatopoeic representations. To investigate this point {{for each of the three}} factors, the absolute differences between the factor scores for the onomatopoeic representations and those for the corresponding <b>sound</b> <b>stimuli</b> were averaged for each of two groups of onomatopoeic representations: one group comprised of onomatopoeic stimuli for which more than 50 % of the participants correctly answered the sound source question, and another group comprised of those for which less than 50 % of the participants correctly answered the sound source question (see Figure 2). These two groups comprised 30 and 42 representations, respectively, from the 72 total onomatopoeic representations.|$|R
50|$|Nanchung is an {{invertebrate}} TRP {{channel that}} acts to sense mechanical force. Drosophila nanchung mutants show deficits in antennal sensation, including hearing and hygrosensation, and {{are unable to}} transduce <b>sound</b> <b>stimuli.</b>|$|R
30|$|<b>Sound</b> <b>stimuli</b> and AEP waves were {{generated}} and recorded by a Tucker-Davis Technologies (TDT) (Gainesville, FL, USA) modular rack mount system, which {{was controlled by}} a TDT AP 2 board with TDT BioSig software.|$|R
5000|$|To {{determine}} what {{parts of the}} auditory cortex contribute to sound localization, investigators implanted cryoloops to deactivate the 13 known regions of acoustically responsive cortex of the cat. [...] Cats learned to make an orienting response by moving their heads and approaching a 100-ms broad-band noise stimulus emitted from a central speaker or one of 12 peripheral speakers located at 15° intervals from left 90° to right 90°along the horizontal plane after attending to a central visual stimulus generated by a red LED. After the cats had reached at least 80% accuracy in identifying {{the location of the}} <b>sound</b> <b>stimulus,</b> each was implanted with one or two [...] pairs of cryoloops over the different sections of the auditory cortex; 10 sections were defined. Cryoloops were turned on so that the loops reached a temperature of 3°C (plus or minus 1°C), first unilaterally, then bilaterally, next unilaterally on the other side, and finally baseline task performance was recorded after recovering from cooling. This cycle was repeated several times for each cat. Of the 10 sections that were deactivated, only deactivation of 3 sections, the AI (primary auditory cortex)/DZ (dorsal zone), PAF (posterior auditory field), and AES (anterior ectosylvian sulcus) sections, were found to have an effect on sound localization. At baseline, cats were able to locate 90% of the sound stimuli. Unilateral deactivation of any one of these sections resulted in a contralateral impairment in sound localization, or 10% accuracy. Bilateral deactivation of any combination of these three sections resulted in a 180° deficit to 10% of sound locations identified, although this accuracy implied that cats were still able to orient to the hemifield where the sound occurred above chance (7.7%). Since the primary auditory cortex and dorsal zone were concurrently cooled, the investigators performed another study in which the AI and DZ were examined as separate entities to further establish the sections of auditory cortex contributing to sound localization. The experimental design was the same as the above-mentioned design with the exception that only the AI and DZ sections were implanted with separate cryoloops. Again, it was found that unilateral simultaneous cooling deactivation of the AI and DZ generated contralateral sound localization deficits while bilateral deactivation created a deficit in both hemifields (10% sound location identification). Bilateral deactivation of AI alone resulted in only 45% accuracy within 30° of the target. Bilateral deactivation of DZ resulted in 60% accuracy but with larger errors, often into the hemifield opposite the target. Therefore, AZ deactivation produces a higher number of small errors while deactivation of DZ leads to larger but fewer errors. This finding that AI and DZ deactivation produce partial deficits in sound localization implies that the previous finding that PAF and AES deactivation have more considerable contributions to sound localization than either the AI or DZ.|$|E
40|$|The {{present study}} investigates the {{neuronal}} {{mechanism of the}} corticofugal modulation on the thalamic neurons, which transmit auditory information from the periphery to the cortex. It was reported in a previous study that electrical activation of the primary auditory cortex induced both facilitatory and inhibitory effects on {{the response of the}} medial geniculate body (MGB) neurons to auditory stimulus. We hypothetized that the direct corticothalamic input to the auditory thalamic neurons is the main cause of the facilitatory effect in the MGB. This was examined by using in vivo intracellular recording method to measure the membrane potential of the MGB neurons during the presentation of <b>sound</b> <b>stimulus</b> and/or cortical activation. The neuronal responses to <b>sound</b> <b>stimulus</b> were studied while the resting potential was manipulated by injecting positive/negative electrical current. For most neurons, the response to <b>sound</b> <b>stimulus</b> increased while it was depolarized and decreased while hyperpolarized. Electrical stimulation on the auditory cortex resulted in either a slow elevation or a slow depression of the membrane potential, thereby leading to an increase or a decrease in the neuronal response to <b>sound</b> <b>stimulus...</b>|$|E
40|$|Results of the {{research}} are evidence of changing muscles reflex activity of human lower extremity {{under the influence of}} <b>sound</b> <b>stimulus</b> of various frequency range together with the vestibular burden. The most change of the H-reflex was observed under the <b>sound</b> <b>stimulus</b> of 800 hertz. Not only the proprioceptive but auditory sensory system takes part in the regulation of the brain reflex activity. Existence of different labyrinths actions, according to the situation, on the interneuronic inhibitory ways of the postsynaptic inhibition of the salens muscle’s motoneurons is supposed...|$|E
50|$|The {{acoustic}} reflex (also {{known as the}} stapedius reflex, middle-ear-muscles (MEM) reflex, attenuation reflex, or auditory reflex) is an involuntary muscle contraction that occurs in the middle ear in response to high-intensity <b>sound</b> <b>stimuli</b> or when the person starts to vocalize.|$|R
40|$|The {{similarity}} of musical scales and consonance judgments across human populations has no generally accepted explanation. Here we present {{evidence that these}} aspects of auditory perception arise from the statistical structure of naturally occurring periodic <b>sound</b> <b>stimuli.</b> An analysis of speech sounds, the principal source of periodic <b>sound</b> <b>stimuli</b> in the human acoustical environment, shows that the probability distribution of amplitude–frequency combinations in human utterances predicts both {{the structure of the}} chromatic scale and consonance ordering. These observations suggest that what we hear is determined by the statistical relationship between acoustical stimuli and their naturally occurring sources, rather than by the physical parameters of the stimulus per se. Key words: audition; auditory system; perception; music; scales; consonance; tones; probabilit...|$|R
40|$|Slow {{oscillations}} at frequencies < 1 Hz {{manifest in}} many brain regions as discrete transitions between a depolarized up {{state and a}} hyperpolarized down state of the neuronal membrane potential. Although up and down states are known to differentially affect sensory-evoked responses, whether {{and how they are}} modulated by sensory stimuli are not well understood. In the present study, intracellular recording in anesthetized guinea pigs showed that membrane potentials of nonlemniscal auditory thalamic neurons exhibited spontaneous up/down transitions at random intervals in the range of 2 - 30 s, which could be entrained to a regular interval by repetitive <b>sound</b> <b>stimuli.</b> After termination of the entraining stimulation (ES), regular up/down transitions persisted for several cycles at the ES interval. Furthermore, the efficacy of weak <b>sound</b> <b>stimuli</b> in triggering the up-to-down transition was potentiated specifically at the ES interval for at least 10 min. Extracellular recordings in the auditory thalamus of unanesthetized guinea pigs also showed entrainment of slow oscillations by rhythmic <b>sound</b> <b>stimuli</b> during slow wave sleep. These results demonstrate a novel form of network plasticity, which could help to retain the information of stimulus interval on the order of seconds. Department of Rehabilitation Science...|$|R
