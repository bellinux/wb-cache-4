3650|8639|Public
5|$|Based on {{apparent}} {{changes in}} the position of Epsilon Eridani between 1938 and 1972, Peter van de Kamp proposed that an unseen companion with an orbital period of 25years was causing gravitational perturbations in its position. This claim was refuted in 1993 by Wulff-Dieter Heintz and the false detection was blamed on a <b>systematic</b> <b>error</b> in the photographic plates.|$|E
5|$|A {{theodolite}} can {{be considerably}} more accurate if used correctly, {{but it is}} also considerably more difficult to use correctly. There is no inherent way to align a theodolite with North and so the scale has to be calibrated using astronomical observation, usually the position of the Sun. Because the position of celestial bodies changes with the time of day due to the Earth's rotation, the time of these calibration observations must be accurately known, or else there will be a <b>systematic</b> <b>error</b> in the measurements. Horizon altitudes can be measured with a theodolite or a clinometer.|$|E
5|$|A key {{development}} of the testing program was a miss-distance indicator system, which independently measured {{the distance between the}} Zeus and the target at the instant the computers initiated the detonation of the warhead. There were concerns that if the Zeus' own radars were used for this ranging measure, any <b>systematic</b> <b>error</b> in ranging would also be present in the test data, and thus would be hidden. The solution was the use of a separate UHF-frequency transmitter in the warhead reentry vehicle, and a receiver in the Zeus. The received signal was retransmitted to the ground, where its Doppler shift was examined to extract the range information. These instruments eventually demonstrated that the Zeus' own tracking information was accurate. For visual tracking, a small conventional warhead was used, which provided a flash that could be seen on long exposure photographs of the interceptions.|$|E
40|$|The {{influence}} of <b>systematic</b> <b>errors</b> on {{the calculation of}} the statistical significance of a γ-ray signal with the frequently invoked Li and Ma method is investigated. A simple criterion is derived {{to decide whether the}} Li and Ma method can be applied in the presence of <b>systematic</b> <b>errors.</b> An alternative method is discussed for cases where <b>systematic</b> <b>errors</b> are too large for the application of the original Li and Ma method. This alternative method reduces to the Li and Ma method when <b>systematic</b> <b>errors</b> are negligible. Finally, it is shown that the consideration of <b>systematic</b> <b>errors</b> will be important in many analyses of data from the planned Cherenkov Telescope Array. Comment: Accepted for publication in Astroparticle Physic...|$|R
25|$|A good plate design {{helps to}} {{identify}} <b>systematic</b> <b>errors</b> (especially those linked with well position) and determine what normalization {{should be used}} to remove/reduce the impact of <b>systematic</b> <b>errors</b> on both QC and hit selection.|$|R
40|$|This paper reconciles {{contradictory}} findings {{obtained from}} forecast evaluations: {{the existence of}} <b>systematic</b> <b>errors</b> {{and the failure to}} reject rationality in the presence of such <b>errors.</b> <b>Systematic</b> <b>errors</b> in one economic state may offset the opposite types of errors in the other state such that the null of rationality is not rejected. A modified test applied to the Fed forecasts shows that the forecasts were ex post biased. Greenbook Forecasts, forecast evaluation, <b>systematic</b> <b>errors...</b>|$|R
25|$|A {{mistake in}} coding that affects all {{responses}} {{for that particular}} question {{is another example of}} a <b>systematic</b> <b>error.</b>|$|E
25|$|These {{confidence}} intervals reflect random error, {{but do not}} compensate for <b>systematic</b> <b>error,</b> {{which in this case}} can arise from, for example, the reference group not having fasted long enough before blood sampling.|$|E
25|$|A <b>systematic</b> <b>error</b> or bias {{occurs when}} there is a {{difference}} between the true value (in the population) and the observed value (in the study) from any cause other than sampling variability. An example of <b>systematic</b> <b>error</b> is if, unknown to you, the pulse oximeter you are using is set incorrectly and adds two points to the true value each time a measurement is taken. The measuring device could be precise but not accurate. Because the error happens in every instance, it is systematic. Conclusions you draw based on that data will still be incorrect. But the error can be reproduced in the future (e.g., by using the same mis-set instrument).|$|E
50|$|<b>Systematic</b> <b>errors</b> are offsets to the {{predicted}} “correct” value. <b>Systematic</b> <b>errors</b> generally occur due to the human component of these measurements, the device itself, or the setup of the experiment. Things such as calibration errors, stray light, and incorrect settings, are all potential issues.|$|R
3000|$|For deeper earthquakes, a {{qualitative}} consideration {{could be given}} as follows. At short epicentral distances compared with the focal depth, <b>systematic</b> <b>errors</b> reach a level depending on the focal depth. On the other hand, at longer distances, <b>systematic</b> <b>errors</b> increase with distance mostly {{similar to those of}} shallow earthquakes. These result in smaller variances of residuals than those of shallower earthquakes, since a range of <b>systematic</b> <b>errors</b> becomes smaller than those of shallower earthquakes. This implies that a significant bias may remain, even if [...]...|$|R
40|$|Chromosome {{conformation}} capture (3 C) -based {{techniques have}} recently been used to uncover the mystic genomic architecture in the nucleus. These techniques yield indirect data on the distances between genomic loci {{in the form of}} contact frequencies that must be normalized to remove various errors. This normalization process determines the quality of data analysis. In this study, we describe two <b>systematic</b> <b>errors</b> that result from the heterogeneous local density of restriction sites and different local chromatin states, methods to identify and remove those artifacts, and three previously described sources of <b>systematic</b> <b>errors</b> in 3 C-based data: fragment length, mappability, and local DNA composition. To explain the effect of <b>systematic</b> <b>errors</b> on the results, we used three different published data sets to show the dependence of the results on restriction enzymes and experimental methods. Comparison of the results from different restriction enzymes shows a higher correlation after removing <b>systematic</b> <b>errors.</b> In contrast, using different methods with the same restriction enzymes shows a lower correlation after removing <b>systematic</b> <b>errors.</b> Notably, the improved correlation of the latter case caused by <b>systematic</b> <b>errors</b> indicates that a higher correlation between results does not ensure the validity of the normalization methods. Finally, we suggest a method to analyze random error and provide guidance for the maximum reproducibility of contact frequency maps...|$|R
25|$|In practice, due to {{the extra}} {{complexity}} involved in taking the slope correction factor into detailed account, many authors (in effect) put σFN = 1 in eq. (49), thereby generating a <b>systematic</b> <b>error</b> in their estimated values of β and/or γ, thought usually to be around 5%.|$|E
25|$|The Off-line {{strategy}} {{determines the}} best patient position through accumulated data gathered during treatment sessions, almost always initial treatments. Physicians and staff measure {{the accuracy of}} treatment and devise treatment guidelines during using information from the images. The strategy requires greater coordination than on-line strategies. However, the use of off-line strategies does {{reduce the risk of}} <b>systematic</b> <b>error.</b> The risk of random error may still persist, however.|$|E
25|$|The {{assumption}} {{is not unreasonable}} when m>>n. If the experimental errors are normally distributed the parameters will belong to a Student's t-distribution with m'n degrees of freedom. When m>>n Student's t-distribution approximates a normal distribution. Note, however, that these confidence limits cannot take <b>systematic</b> <b>error</b> into account. Also, parameter errors should be quoted to one significant figure only, as {{they are subject to}} sampling error.|$|E
40|$|Abstract: In {{the case}} of {{traditional}} GPS data processing algorithms, <b>systematic</b> <b>errors</b> in GPS measurements cannot be eliminated completely, nor accounted for satisfactorily. These <b>systematic</b> <b>errors</b> can {{have a significant effect}} on both the ambiguity resolution process and the GPS positioning results. Hence this is a potentially critical problem for high precision GPS positioning applications. It is therefore necessary to develop an appropriate data processing algorithm which can effectively deal with <b>systematic</b> <b>errors</b> in a nondeterministic manner. Recently several approaches have been suggested to mitigate the impact of <b>systematic</b> <b>errors</b> on GPS positioning results: the semi-parametric model, the use of wavelets and new stochastic modelling techniques. These approaches use different bases and have different implications for data processing. This paper aims to compare the above three methods, in both the theoretical and numerical sense...|$|R
40|$|The Hamiltonian {{control of}} n qubits {{requires}} precision {{control of both}} the strength and timing of interactions. Compensation pulses relax the precision requirements by reducing unknown but <b>systematic</b> <b>errors.</b> Using composite pulse techniques designed for single qubits, we show that <b>systematic</b> <b>errors</b> for n qubit systems can be corrected to arbitrary accuracy given either two non-commuting control Hamiltonians with identical <b>systematic</b> <b>errors</b> or one error-free control Hamiltonian. We also examine composite pulses {{in the context of}} quantum computers controlled by two-qubit interactions. For quantum computers based on the XY interaction, single-qubit composite pulse sequences naturally correct <b>systematic</b> <b>errors.</b> For quantum computers based on the Heisenberg or exchange interaction, the composite pulse sequences reduce the logical single-qubit gate errors but increase the errors for logical two-qubit gates. Comment: 9 pages, 5 figures; corrected reference formattin...|$|R
40|$|Global {{climate models}} (GCMs) have good skill in {{simulating}} climate {{at the global}} scale yet they show significant <b>systematic</b> <b>errors</b> at regional scale. For example, many GCMs exhibit significant biases in South Asian summer monsoon (SASM) simulations. Those errors not only limit application of such GCM output in driving regional climate models (RCMs) over these regions but also raise questions on the usefulness of RCMs derived from those GCMs. We focus on process studies where the RCM is driven by realistic lateral boundary conditions from atmospheric re-analysis which prevents remote <b>systematic</b> <b>errors</b> from influencing the regional simulation. In this context it is pertinent to investigate whether RCMs also suffer from similar errors when run over regions where their parent models show large <b>systematic</b> <b>errors.</b> Furthermore, the general sensitivity of the RCM simulation to domain size is informative in understanding remote drivers of <b>systematic</b> <b>errors</b> in the GCM and in choosing a suitable RCM domain that minimizes those errors. We investigate Met Office Unified Model <b>systematic</b> <b>errors</b> in SASM by comparing global and regional model simulations with targeted changes to the domain and forced with atmospheric re-analysis. We show that excluding remote drivers of <b>systematic</b> <b>errors</b> from the direct area of interest allows the application of RCMs for process studies of the SASM, despite the large errors in the parent global model. The {{findings in this study}} are also relevant to other models, many of which suffer from a similar pattern of <b>systematic</b> <b>errors</b> in global model simulations of the SASM...|$|R
25|$|The need {{to control}} <b>systematic</b> <b>error</b> in the {{measurement}} of the CMB anisotropy and measuring the zodiacal cloud at different elongation angles for subsequent modeling required that the satellite rotate at a 0.8 rpm spin rate. The spin axis is also tilted back from the orbital velocity vector as a precaution against possible deposits of residual atmospheric gas on the optics as well against the infrared glow that would result from fast neutral particles hitting its surfaces at extremely high speed.|$|E
25|$|Modern {{nuclear physics}} and {{particle}} physics experiments often involve {{large numbers of}} data analysts working together to extract quantitative data from complex datasets. In particular, the analysts want to report accurate <b>systematic</b> <b>error</b> estimates for all of their measurements; this is difficult or impossible {{if one of the}} errors is observer bias. To remove this bias, the experimenters devise blind analysis techniques, where the experimental result is hidden from the analysts until they've agreed—based on properties of the data set other than the final value—that the analysis techniques are fixed.|$|E
25|$|The early accuracy, however, was poor. The {{results were}} argued {{by some to}} have been plagued by <b>systematic</b> <b>error</b> and {{possibly}} confirmation bias, although modern reanalysis of the dataset suggests that Eddington's analysis was accurate. The measurement was repeated by a team from the Lick Observatory in the 1922 eclipse, with results that agreed with the 1919 results and has been repeated several times since, most notably in 1953 by Yerkes Observatory astronomers and in 1973 by a team from the University of Texas. Considerable uncertainty remained in these measurements for almost fifty years, until observations started being made at radio frequencies.|$|E
30|$|This {{parameter}} represents {{influence of}} <b>systematic</b> <b>errors</b> on measured value.|$|R
5000|$|... #Caption: <b>Systematic</b> <b>Errors</b> of the Joback Method (Normal Boiling Point) ...|$|R
50|$|All these {{deviations}} can {{be classified}} as <b>systematic</b> <b>errors</b> or random <b>errors.</b> <b>Systematic</b> <b>errors</b> can sometimes be compensated for by means {{of some kind of}} calibration strategy. Noise is a random error that can be reduced by signal processing, such as filtering, usually {{at the expense of the}} dynamic behavior of the sensor.|$|R
25|$|In most larger {{samplings}} of data, {{some data}} points will be {{further away from}} the sample mean than what is deemed reasonable. This can be due to incidental <b>systematic</b> <b>error</b> or flaws in the theory that generated an assumed family of probability distributions, {{or it may be}} that some observations are far {{from the center of the}} data. Outlier points can therefore indicate faulty data, erroneous procedures, or areas where a certain theory might not be valid. However, in large samples, a small number of outliers is to be expected (and not due to any anomalous condition).|$|E
500|$|In {{testing on}} the bomb range, the Mk. XIV {{demonstrated}} an average accuracy of [...] from [...] altitude. In service, however, the average <b>systematic</b> <b>error</b> was , while the random error was [...] In comparison, units using {{the much more}} complex Stabilized Automatic Bomb Sight (SABS) improved this to [...] under the same operational conditions and altitude.|$|E
500|$|The {{observed}} velocities {{predicted by}} the Michaelis–Menten equation {{can be used to}} directly model the time course disappearance of substrate and the production of product through [...] incorporation of the Michaelis–Menten equation into the equation for first order chemical kinetics. [...] This can only be achieved however if one recognises the problem {{associated with the use of}} Euler's number in the description of first order chemical kinetics. i.e. e−k is a split constant that introduces a <b>systematic</b> <b>error</b> into calculations and can be rewritten as a single constant which represents the remaining substrate after each time period.|$|E
5000|$|... #Subtitle level 2: <b>Systematic</b> <b>errors</b> and {{experiments}} {{to weigh the}} Earth ...|$|R
50|$|There are {{two types}} of {{measurement}} error: <b>systematic</b> <b>errors</b> and random errors.|$|R
5000|$|<b>Systematic</b> <b>errors</b> as {{a result}} of {{selection}} bias, information bias and confounding ...|$|R
2500|$|The {{validity}} {{of a study}} {{is dependent on the}} degree of <b>systematic</b> <b>error.</b> Validity is usually separated into two components: ...|$|E
2500|$|It is also {{possible}} to derive an uncertainty relation that, as the Ozawa's one, combines both the statistical and <b>systematic</b> <b>error</b> components, but keeps a form {{very close to the}} Heisenberg original inequality. [...] By adding Robertson ...|$|E
2500|$|The inequalities above {{focus on}} the {{statistical}} imprecision of observables as quantified by the standard deviation [...] Heisenberg's original version, however, was dealing with the <b>systematic</b> <b>error,</b> a disturbance of the quantum system produced by the measuring apparatus, i.e., an observer effect.|$|E
40|$|Neutrino {{oscillations}} {{physics is}} {{entered in the}} precision era. In this context accelerator-based neutrino experiments need a reduction of <b>systematic</b> <b>errors</b> {{to the level of}} a few percent. Today one of the most important sources of <b>systematic</b> <b>errors</b> are neutrino-nucleus cross sections which in the hundreds-MeV to few-GeV energy region are known with a precision not exceeding 20...|$|R
40|$|It {{has been}} proven that DMC images (as images of other digital aerial cameras) are not free of <b>systematic</b> <b>errors</b> in the virtual image space. Not {{properly}} modelled estimated exterior orientation can absorb propagated errors in the bundle block adjustment (from different errors sources as image <b>systematic</b> <b>errors,</b> poor GPS/INS observations or insufficient ground control points set up) and could generate unwanted large <b>systematic</b> <b>errors</b> in the object space, especially in height. To keep error propagation under control two different approaches are considered in bundle adjustment: i) an appropriate set of self-calibration parameters and ii) calibration/characterisation grid, compensating image <b>systematic</b> <b>errors</b> in each virtual image. The calibration grid is derived from a calibration flight and should be valid for images acquired in other projects. This paper focuses on two topics: firstly a comparison between the performance of both approximations in aerotriangulation, and, secondly, {{the impact of these}} <b>systematic</b> <b>errors</b> on stereoplotting. The first analysis uses seven different data sets (including the calibration flight) and three bundle adjustment set ups: i) not using any model at all, ii) using a self-calibration parameter set, and iii) using the calibration gird without using any self-calibration parameter set. Independent check points are used to assess the performance of both techniques in bundle adjustment. In the second analysis object coordinates for points of single models are calculated compensating and also not compensating for <b>systematic</b> <b>errors</b> their image coordinates. Later, these two sets of points are compared to the respective estimated object coordinates of the bundle adjustment. Main results of this work suggest that application of calibration grid (as it is derived in this analysis) is not able to isolate the <b>systematic</b> <b>errors</b> in virtual image from other errors sources in the calibration block. The comparison with self...|$|R
5000|$|... <b>systematic</b> <b>errors</b> (or gross errors) due to sensor {{calibration}} or faulty data transmission.|$|R
