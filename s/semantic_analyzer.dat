66|27|Public
50|$|A {{more complex}} {{example is the}} lexer hack in C, where the token class of a {{sequence}} of characters cannot be determined until the semantic analysis phase, since typedef names and variable names are lexically identical but constitute different token classes. Thus in the hack, the lexer calls the <b>semantic</b> <b>analyzer</b> (say, symbol table) and checks if the sequence requires a typedef name. In this case, information must flow back not from the parser only, but from the <b>semantic</b> <b>analyzer</b> back to the lexer, which complicates design.|$|E
5000|$|Currently we have {{constructed}} lexer,parser {{and most}} of <b>semantic</b> <b>analyzer.</b> The first code gen is basically an AST-walking interpreter. We have plans to keep working on this project and start using LLVM for machine code JIT compilation. One of the compilation targets is native executable (no VM) just like with C/C++.|$|E
50|$|In general, a syntax and <b>semantic</b> <b>analyzer</b> {{tries to}} {{retrieve}} {{the structure of the}} program from the source code, while a code generator uses this structural information (e.g., data types) to produce code. In other words, the former adds information while the latter loses some of the information. One consequence of this information loss is that reflection becomes difficult or even impossible. To counter this problem, code generators often embed syntactic and semantic information in addition to the code necessary for execution.|$|E
50|$|The Syntax/Semantic Language (S/SL) is an {{executable}} {{high level}} specification language for recursive descent parsers, <b>semantic</b> <b>analyzers</b> and code generators developed by James Cordy, Ric Holt and David Wortman at the University of Toronto in 1980.|$|R
5000|$|S/SL's [...] "semantic mechanisms" [...] {{extend its}} {{capabilities}} to {{all phases of}} compiling, {{and it has been}} used to implement all phases of compilation, including scanners, parsers, <b>semantic</b> <b>analyzers,</b> code generators and virtual machine interpreters in multi-pass language processors.|$|R
40|$|IARG-AnCora is {{an ongoing}} project which aim is to {{annotate}} the implicit arguments of deverbal nominalizations in AnCora corpus. This corpus will {{be the basis for}} systems of automatic semantic role labeling based on machine learning techniques. <b>Semantic</b> <b>analyzers</b> are essential components in the current applications of language technologies, in which it is important to obtain {{a deeper understanding of the}} text to make inferences on the highest level in order to obtain qualitative improvements in the results. Postprint (published version...|$|R
40|$|This paper {{represents}} a <b>Semantic</b> <b>Analyzer</b> for checking the semantic correctness of the given input text. We describe our system {{as the one}} which analyzes the text by comparing it with {{the meaning of the}} words given in the WordNet. The <b>Semantic</b> <b>Analyzer</b> thus developed not only detects and displays semantic errors in the text but it also corrects them...|$|E
40|$|A natural {{language}} understanding system is described which extracts contextual information from Japanese texts. It integrates syntactic, semantic and contextual processing serially. The syntactic analyzer obtains rough syntactic structures from the text. The <b>semantic</b> <b>analyzer</b> treats modifying relations inside noun phrases and case relations among verbs and noun phrases. Then, the contextual analyzer obtains contextual information from the semantic structure extracted by the <b>semantic</b> <b>analyzer.</b> Our system understands the context using precoded contextual knowledge on terrorism and plugs the event information in input sentences into-the contextual structure...|$|E
40|$|Natural Languages Processing (NLP) {{has many}} {{applications}} such as Database user interfaces, Machine Translation. Knowledge Acquisition and Report Abstraction. Several approaches {{have been used in}} dealing with NLP. This paper describes an ongoing research project about understanding natural language text using object-oriented techniques. It starts with difficulties and challenges in semantic analysis. Then, a description of the object-oriented <b>semantic</b> <b>analyzer</b> is given. Advantages and disadvantages of using object orientation in semantic analysis are discussed. Then the paper describes evaluation criteria for the <b>semantic</b> <b>analyzer...</b>|$|E
50|$|From a {{computational}} perspective, treebanks {{have been}} used to engineer state-of-the-art natural language processing systems such as part-of-speech taggers, parsers, <b>semantic</b> <b>analyzers</b> and machine translation systems. Most computational systems utilize gold-standard treebank data. However, an automatically parsed corpus that is not corrected by human linguists can still be useful. It can provide evidence of rule frequency for a parser. A parser may be improved by applying it to large amounts of text and gathering rule frequencies. However, it should be obvious that only by a process of correcting and completing a corpus by hand is it possible then to identify rules absent from the parser knowledge base. In addition, frequencies {{are likely to be more}} accurate.|$|R
40|$|This paper {{addresses}} {{the problem of}} the development of Arabic Natural Language Interface for Information System Design. Most Arabic systems focus on processing Arabic text for machine translation or information retrieval. We present in this paper some aspects for identify Entities and Relationships from a description of the application domain given with a subset of the Arabic Natural Language. The tool starts, in a first step with an interpretation of Arabic text through the system’s components (morphological, syntax, <b>semantic</b> <b>analyzers)</b> and generates the meaning representation in a first order logic form. Then in the second step, it uses entities rules and relationships rules for extracting Entities and Relationships, which describes the elements of the Conceptual Schem...|$|R
40|$|In {{previous}} work, {{we introduced}} scope graphs as a formalism for describing program binding structure and performing name resolution in an AST-independent way. In this paper, we show {{how to use}} scope graphs to build static <b>semantic</b> <b>analyzers.</b> We use constraints extracted from the AST to specify facts about binding, typing, and initialization. We treat name and type resolution as separate building blocks, but our approach can handle language constructs [...] such as record field access [...] for which binding and typing are mutually dependent. We also refine and extend our previous scope graph theory to address practical concerns including ambiguity checking and support for {{a wider range of}} scope relationships. We describe the details of constraint generation for a model language that illustrates many of the interesting static analysis issues associated with modules and records. Programming LanguagesSoftware Engineerin...|$|R
40|$|Existing search {{systems are}} review. It is offered the new {{approach}} for creation a search system {{with the help of}} neural networks which will be containing <b>semantic</b> <b>analyzer.</b> ??????????????? ???????????? ????????? ??????? ? ???????????? ????? ?????? ? ?????????? ????????? ??????? ? ??????? ????????? ?????, ??????? ????? ????????? ????????????? ??????????...|$|E
40|$|This paper {{describes}} a first {{approach to a}} computational <b>semantic</b> <b>analyzer</b> for Urdu {{on the basis of}} the deep syntactic analysis done by the Urdu grammar ParGram. Apart from the semantic construction, external lexical resources such as an Urdu WordNet and a preliminary VerbNet style resource for Urdu are developed and connected to the <b>semantic</b> <b>analyzer.</b> These resources allow for a deeper level of representation by providing real-word knowledge such as hypernyms of lexical entities and information on thematic roles. We therefore contribute to the overall goal of providing more insights into the computationally efficient analysis of Urdu, in particular to computational semantic analysis. ...|$|E
40|$|This paper {{describes}} {{a method for}} semantic analysis of natural language queries for Natural Language Interface to Database (NLIDB) using domain ontology. Implementation of NLIDB for serious applications like railway inquiry, airway inquiry, corporate or government call centers requires higher precision. This {{can be achieved by}} increasing role of language knowledge and domain knowledge at semantic level. Also design of <b>semantic</b> <b>analyzer</b> should be such that it can easily be ported for other domains as well. In this paper a design of <b>semantic</b> <b>analyzer</b> for railway inquiry domain is reported. Intermediate result of the system is evaluated for a corpus of natural language queries collected from casual users who were not involved in the system design...|$|E
40|$|A {{prominent}} {{stumbling block}} in {{the spread of the}} C++ programming language has been a lack of programming and analysis tools to aid development and maintenance of C++ systems. One way to make the job of tool developers easier and to increase the quality of the tools they create is to factor out the common components of tools and provide the components as easily (re) used building blocks. Those building blocks include lexical, syntactic, and <b>semantic</b> <b>analyzers,</b> tailored database derivers, code annotators and instrumentors, and code generators. From these building blocks, tools such as structure browsers, data-flow analyzers, program/specification verifiers, metrics collectors, compilers, interpreters, and the like can be built more easily and cheaply. We believe that for C++ programming and analysis tools the most primitive building blocks are centered around a common representation of semantically analyzed C++ code. In this paper we describe such a representation, called Repri [...] ...|$|R
40|$|A key {{step in the}} {{semantic}} analysis of network traffic is to parse the traffic stream according to the high-level protocols it contains. This process transforms raw bytes into structured, typed, and semantically meaningful data fields that provide a high-level representation of the traffic. However, constructing protocol parsers by hand is a tedious and error-prone affair due to the complexity and sheer number of application protocols. This paper presents binpac, a declarative language and compiler designed to simplify the task of constructing robust and efficient <b>semantic</b> <b>analyzers</b> for complex network protocols. We discuss {{the design of the}} binpac language and a range of issues in generating efficient parsers from high-level specifications. We have used binpac to build several protocol parsers for the “Bro” network intrusion detection system, replacing some of its existing analyzers (handcrafted in C++), and supplementing its operation with analyzers for new protocols. We can then use Bro’s powerful scripting language to express application-level analysis of network traffic in high-level terms that are both concise and expressive. binpac is now part of the open-source Bro distribution...|$|R
40|$|If we {{knew what}} we were doing, it wouldn’t be called research, would it? 1 Imagine you are a robot. Imagine you are a robot visiting a {{cocktail}} party — groups of people standing around chatting with each other. Imagine you are a robot {{trying to understand what}} those people talk about. You have no problems when facing one of those humans alone. You have built-in textto-speech and speech-to-text converters. You have syntactic and <b>semantic</b> <b>analyzers.</b> You have a knowledge-base where you put all the information. So you are pretty good at understanding a human, but still — standing around at this cocktail party, you do not understand a thing. There are just too many people talking, too many signals coming at you from too many different directions. You miss a little device that humans often take for granted. Something that we would, for now, like to call an independent component analyzer. Something that this work talks about. What would such an analyzer have to do so that you could listen to th...|$|R
40|$|In {{this paper}} {{a system which}} understands and conceptualizes scenes {{descriptions}} in natural language is presented. Specifically, the following components of the system are described: the syntactic analyzer, based on a Procedural Systemic Grammar, the <b>semantic</b> <b>analyzer</b> relying on the Conceptual Dependency Theory, and the dictionary...|$|E
40|$|Linguist’s Assistant (LA) is a {{large scale}} <b>semantic</b> <b>analyzer</b> and multi-lingual natural {{language}} generator designed and developed entirely from a linguist’s perspective. The system incorporates extensive typological, semantic, syntactic, and discourse research into its semantic representational system and its transfer and synthesizing grammars. LA has been tested with English, Korean, Kewa (Papu...|$|E
40|$|This work {{is part of}} a large {{research}} project entitled "Oreillodule" aimed at developing tools for automatic speech recognition, translation, and synthesis for Arabic language. Our attention has mainly been focused on an attempt to present the <b>semantic</b> <b>analyzer</b> developed for the automatic comprehension of the standard spontaneous arabic speech. The findings on the effectiveness of the semantic decoder are quite satisfactory...|$|E
40|$|This book {{comes with}} “batteries {{included}} ” (a {{reference to the}} phrase often used to explain {{the popularity of the}} Python programming language). It is the companion book to an impressive open-source software library called the Natural Language Toolkit (NLTK), written in Python. NLTK combines language processing tools (tokenizers, stemmers, taggers, syntactic parsers, <b>semantic</b> <b>analyzers)</b> and standard data sets (corpora and tools to access the corpora in an efficient and uniform manner). Although the book builds on the NLTK library, it covers only a relatively small part of what can be done with it. The combination of the book with NLTK, a growing system of carefully designed, maintained, and documented code libraries, is an extraordinary resource that will dramatically influence the way computational linguistics is taught. The book attempts to cater to a large audience:It is a textbook on computational linguistics for science and engineering students; it also serves as practical documentation for the NLTK library, and it finally attempts to provide an introduction to programming and algorithm design for humanities students. I have used the book and its earlie...|$|R
40|$|Morphological {{analysis}} {{is an essential}} stage in language engineering applications. For the Arabic language, this stage {{is not easy to}} develop because the Arabic language has some particularities such as the phenomena of agglutination and a lot of morphological ambiguity phenomenon. These reasons make the design of the morphological analyzer for Arabic somewhat difficult and require lots of other tools and treatments. The volume of the lexicon is another big problem of the morphological analysis of the Arabic Language which affects directly the process of the analyzing. In this paper we present a Morphological Analyzer for Modern Standard Arabic based on Arabic Morphological Automaton technique and using a new and innovative language (XMODEL) to represent the Arabic morphological knowledge in an optimal way. Both the Arabic Morphological Analyzer and Arabic Morphological Automaton are implemented in Java language and used XML technology. Buckwalter Arabic Morphological Analyzer and Xerox Arabic Finite State Morphology are two of the best known morphological analyzers for Modern Standard Arabic and they are also available and documented. Our Morphological Analyzer can be exploited by Natural Language Processing (NLP) applications such as machine translation, orthographical correction, information retrieval and both syntactic and <b>semantic</b> <b>analyzers.</b> At the end, an evaluation of Xerox and our system is done...|$|R
50|$|According {{to former}} and current US {{government}} officials, {{there are more}} than a dozen major US based Internet switching stations where this kind of filtering takes place, which is not only near the sites where the main undersea Internet cables enter the US. An actual example of a facility where NSA taps Internet backbone cables is Room 641A in the San Francisco switching station of AT&T, which was revealed in 2006. One of the devices used to filter the Internet traffic is the <b>Semantic</b> Traffic <b>Analyzer</b> or STA 6400 made by Boeing subsidiary Narus.|$|R
40|$|This paper {{introduces}} a semantic language developed {{with the objective}} {{to be used in}} a <b>semantic</b> <b>analyzer</b> based on linguistic and world knowledge. Linguistic knowledge is provided by a Combinatorial Dictionary and several sets of rules. Extra-linguistic information is stored in an Ontology. The meaning of the text is represented by means of a series of RDF-type triples of the form predicate (subject, object). <b>Semantic</b> <b>analyzer</b> is one of the options of the multifunctional ETAP- 3 linguistic processor. The analyzer can be used for Information Extraction and Question Answering. We describe semantic representation of expressions that provide an assessment of the number of objects involved and/or give a quantitative evaluation of different types of attributes. We focus on the following aspects: 1) parametric and non-parametric attributes; 2) gradable and non-gradable attributes; 3) ontological representation of different classes of attributes; 4) absolute and relative quantitative assessment; 5) punctual and interval quantitative assessment; 6) intervals with precise and fuzzy boundarie...|$|E
40|$|The Internet has {{facilitated}} {{the growth of}} recommendation system owing to the ease of sharing customer experiences online. It is a challenging task to summarize and streamline the online textual reviews. In this paper, we propose a new framework called Fuzzy based contextual recommendation system. For classification of customer reviews we extract {{the information from the}} reviews based on the context given by users. We use text mining techniques to tag the review and extract context. Then we find out the relationship between the contexts from the ontological database. We incorporate fuzzy based <b>semantic</b> <b>analyzer</b> to find the relationship between the review and the context when they are not found therein. The sentence based classification predicts the relevant reviews, whereas the fuzzy based context method predicts the relevant instances among the relevant reviews. Textual analysis is carried out with the combination of association rules and ontology mining. The relationship between review and their context is compared using the <b>semantic</b> <b>analyzer</b> which is based on the fuzzy rules...|$|E
40|$|OntoSoar is {{a system}} to extract {{information}} from genealogy texts and populate a conceptual model with the extracted information. It does this by doing a deep linguistic analysis and then mapping the meaning structures found to an ontology provided by the user. The system is built using a processing pipeline that includes the open-source Link Grammar Parser using a grammar customized for this application and an innovative <b>semantic</b> <b>analyzer</b> built on the Soar cognitive architecture. Here we describe the how OntoSoar, currently still in development, works...|$|E
40|$|Language-based {{programming}} environments {{provide some}} or all of the functionality of a compiler, an interactive debugger, a browser, and a configuration manager behind a unified user interface based on an editing paradigm. As the user edits a program, the changes are processed incrementally, allowing for low-latency updates to derived information. This information can be made available to interactive environment services, such as browsing, navigation, and "real time" error-reporting. In this dissertation, we address an important subproblem in the construction of such environments, the generation of static <b>semantic</b> <b>analyzers</b> that operate in an incremental mode. Our work is embodied in the Colander II system, which introduces both a new metalanguage for the declarative specification of static semantic analyses and new techniques for generating an incremental analyzer from these specifications automatically. Our specification metalanguage melds the advantages of traditional attribute grammars, including amenability to extensive generation-time analysis, with the expressiveness and clientindependence characteristic of Ballance's Logical Constraint Grammars. In comparison to traditional attribute grammars, our metalanguage allows much more of the incrementality inherent in a particular analysis task to be exposed within the formalism itself, where it can be exploited automatically by our implementation. Our incremental analysis algorithms exploit the attributed objects and function-valued attributes provided by our metalanguage, mapping these expressive notations onto a fine-grained incremental implementation. We are thus able to automatically generate incremental analyzers that handle longdistance dependencies and aggregate attributes efficiently. Our methods allow unusua [...] ...|$|R
40|$|Abstract. To find {{malicious}} information jamming countermeasures {{and techniques}} of content attack, the paper proposed two malicious jamming algorithms, including the Blind Jamming Algorithm and the Precise Jamming Algorithm. Experiments, conducted on 3, 183 texts with the Chinese Lexical Analysis System (ICTCLAS) shows {{the effectiveness of}} Blind Jamming Algorithm, making the accuracy rate of Chinese word segmentation from 94 % dropped to 43 %. Besides, topic classification experiments show that the Blind Jamming Algorithm could affect the topic classification significant. Further experiments on 5, 999 texts with the <b>semantic</b> orientation <b>analyzer</b> (Sentifier) shows that the Sentifier’s average precision is declined from 77 % to 60 % by jamming, which shows the jamming effectiveness of the two algorithms we have proposed...|$|R
40|$|Abstract. To develop {{sophisticated}} {{database management}} systems, {{there is a}} need to incorporate more understanding of the real world in the information that is stored in a database. Semantic data models have been developed to try to capture some of the meaning, as well as the structure, of data using abstractions such as inclusion, aggregation, and association. Besides these well-known relationships, a number of additional semantic relationships have been identified by researchers in other disciplines such as linguistics, logic, and cognitive psychology. This article explores some of the lesser-recognized semantic relationships and discusses both how they could be captured, either manually or by using an automated tool, and their impact on database design. To demonstrate the feasibility of this research, a prototype system for analyzing semantic relationships, called the <b>Semantic</b> Rela-tionship <b>Analyzer,</b> is presented. Key Words. Database design, entity-relationship model, relational model, seman-tic relationships, database design systems...|$|R
40|$|This paper {{describes}} a Japanese language <b>semantic</b> <b>analyzer</b> {{based on an}} extended case frame model, which consists of a re la t ive ly large col lect ion of case re lat ions, modalities and conjunctive re la t ions. The analyzer performs four stage analysis using a frame type knowledge base. It also u t i l i zes p laus ib i l i t y scores for dealing with ambiguities and local scene frames for the prediction of omitted case elements. ...|$|E
40|$|AbstractWe {{show how}} {{restructuring}} a denotational definition {{leads to a}} more efficient compiling algorithm. Three semantics-preserving transformations (static replacement, factoring, and combinator selection) are used to convert a continuation semantics into a formal description of a <b>semantic</b> <b>analyzer</b> and code generator. The compiling algorithm derived below performs type checking before code generation so that type-checking instructions may be omitted from the target code. The optimized code is proved correct {{with respect to the}} original definition of the source language. The proof consists of showing that all transformations preserve the semantics of the source language...|$|E
40|$|This article {{describes}} {{the construction of a}} morphological, syntactic and <b>semantic</b> <b>analyzer</b> to operate a high-grade search engine for Hebrew texts. A good search engine must be complete and accurate. In Hebrew or Arabic script most of the vowels are not written, many particles are attached to the word without space, a double consonant is written with one letter, and some letters signify both vowels and consonants. Thus, almost every string of characters may designate many words (the average in Hebrew is almost three words). As a consequence, deciphering...|$|E
40|$|Parallel {{changes are}} {{becoming}} increasingly prevalent {{in the development of}} large scale software system. To further study the relationship between parallel changes and faults, we have designed and implemented a <b>semantic</b> conflict <b>analyzer</b> (SCA) to detect semantic interference between parallel changes. SCA combines data dependency analysis and program slicing. Data dependency analysis can disclose the semantic structure of the program. And program slicing can identify which semantic structures are impacted by a change. By comparing the overlap between impacts of two changes, SCA can detect if there are semantic interference between the two changes. An experiment with an industrial project shows that SCA can detect {{a significant portion of the}} faults in highly parallel changes. SCA is effective in predicting faults (based on “direct ” semantic interference detection) in changes made within a short time period. SCA is both efficient (averaging less than two minutes) and scalable (requiring only the local context...|$|R
40|$|Automatic {{extraction}} {{and analysis}} of meaning-related information from natural language data {{has been an important}} issue in a number of research areas, such as natural language processing (NLP), text mining, corpus linguistics, and data science. An important aspect of such information extraction and analysis is the semantic annotation of language data using a semantic tagger. In practice, various semantic annotation tools have been designed to carry out different levels of semantic annotation, such as topics of documents, semantic role labeling, named entities or events. Currently, the majority of existing semantic annotation tools identify and tag partial core semantic information in language data, but they tend to be applicable only for modern language corpora. While such <b>semantic</b> <b>analyzers</b> have proven useful for various purposes, a semantic annotation tool that is capable of annotating deep semantic senses of all lexical units, or all-words tagging, is still desirable for a deep, comprehensive semantic analysis of language data. With large-scale digitization efforts underway, delivering historical corpora with texts dating from the last 400 years, a particularly challenging aspect is the need to adapt the annotation in the face of significant word meaning change over time. In this paper, we report on {{the development of a new}} semantic tagger (the Historical Thesaurus Semantic Tagger), and discuss challenging issues we faced in this work. This new semantic tagger is built on existing {NLP} tools and incorporates a large-scale historical English thesaurus linked to the Oxford English Dictionary. Employing contextual disambiguation algorithms, this tool is capable of annotating lexical units with a historically-valid highly fine-grained semantic categorization scheme that contains about 225, 000 semantic concepts and 4, 033 thematic semantic categories. In terms of novelty, it is adapted for processing historical English data, with rich information about historical usage of words and a spelling variant normalizer for historical forms of English. Furthermore, it is able to make use of knowledge about the publication date of a text to adapt its output. In our evaluation, the system achieved encouraging accuracies ranging from 77. 12 to 91. 08 on individual test texts. Applying time-sensitive methods improved results by as much as 3. 54 and by 1. 72 on average...|$|R
40|$|With the {{continuous}} improvements in computer-vision techniques, automatic low-cost video surveillance gradually emerges for consumer applications. Successful trajectory estimation and human-body modeling facilitate the semantic analysis of human activities in video sequences. We propose a fast analyzer for surveillance video, which aims at automatic analysis {{of human behavior}} and <b>semantic</b> events. Our <b>analyzer</b> employs visual cues to track moving persons and classify human-body postures from a monocular video. It consists of three processing steps: (1) multi-person detection, (2) persons tracking with trajectory estimation, and (3) posture classification. We show attractive experimental results, highlighting the system efficiency and classification capability...|$|R
