171|106|Public
50|$|IUCLID 5 is a Java-based application, {{using the}} Hibernate {{framework}} for persistence. It features a Java Swing {{graphical user interface}} (GUI) and can be deployed on both <b>single</b> <b>workstation</b> and distributed environments.|$|E
5000|$|Data is {{consolidated}} onto {{a single}} system to improve reporting, information management and decision-making. Integrating and managing the HVAC, energy, security, digital video and life safety applications from a <b>single</b> <b>workstation</b> allows facility-wide insight and control for better performance.|$|E
50|$|Statistica Desktop: the Statistica Desktop line of {{products}} {{is designed for}} deployment on a <b>single</b> <b>workstation.</b> Statistica spreadsheets, configurations and macros are all stored on the User's local workstation as a stand-alone application. Statistica includes general purpose statistical, graphical, and analytic data management procedures.|$|E
40|$|The paper {{presents}} {{a survey of}} out-of-core methods available {{for the analysis of}} large Markov chains on <b>single</b> <b>workstations.</b> First, we discuss the main sparse matrix storage schemes and review iterative methods for the solution of systems of linear equations typically used in disk-based methods. Next, various out-of-core approaches for the steady state solution of CTMCs are described. In this context, serial out-ofcore algorithms are outlined and analysed with the help of their implementations. A comparison of time [...] ...|$|R
40|$|Teaching {{software}} {{development in a}} computer laboratory with <b>single</b> <b>workstations</b> inevitably leads to problems: different papers have different software needs. For example, one paper may require students to build a database application, while another may require students to install an operating system. Clearly {{the work of the}} second group will negate all the work from the first. In this study the use of "virtual machines " using software to alleviate these problems will be described, as well as pointing out some lessons learned...|$|R
40|$|Abstract. The paper {{presents}} {{a survey of}} out-of-core methods avail-able {{for the analysis of}} large Markov chains on <b>single</b> <b>workstations.</b> First, we discuss the main sparse matrix storage schemes and review iterative methods for the solution of systems of linear equations typically used in disk-based methods. Next, various out-of-core approaches for the steady state solution of CTMCs are described. In this context, serial out-of-core algorithms are outlined and analysed with the help of their imple-mentations. A comparison of time and memory requirements for typical benchmark models is given. ...|$|R
50|$|The C20 {{workstation}} was compact, {{designed to}} be easy to mount on a rack. This compact size allowed up to 14 workstations to be stacked in a standard 42U rack. It also meant that users who used a <b>single</b> <b>workstation</b> were offered extra space either on or beneath their desk.|$|E
50|$|In September 2010, Autodesk {{introduced}} Flame Premium 2011, {{a single}} license for running Flame, Smoke Advanced and Lustre {{together on a}} <b>single</b> <b>workstation.</b> At launch, new licenses were priced from US$129,000 excluding hardware, with upgrades from existing Flame licenses priced from US$10,000. Existing users of Smoke Advanced or Lustre could upgrade from US$25,000.|$|E
50|$|Work Place Integration (WPI), {{introduced}} in Cassino with the Giulia, is the approach {{used to make}} workstations according to better ergonomic standards. Each <b>single</b> <b>workstation</b> was designed and built by involving people and their experience. Many positive changes have been introduced on the Assembly Line, where the incidence of manual work is higher.|$|E
40|$|This paper {{presents}} a portable parallel programming environment for Modula- 2 * - an explicitly parallel machine-independent extension of Modula-. Modula- offers synchronous and asynchronous parallelism, a global single address space, and automatic data and process distribution. The Modula- 2 * system {{consists of a}} compiler, a debugger, a cross-architecture make, a runtime systems for different machines, {{and a set of}} scalable parallel libraries. Implementations exist for the MasPar MP series of massively parallel processors (SIMD), the KSR- 1 parallel computer (MIMD), heterogeneous LANs of <b>workstations</b> (MIMD), and <b>single</b> <b>workstations</b> (SISD). The pape...|$|R
40|$|MSC. NASTRAN is {{the main}} {{structural}} FE code and the second most used CAE code on compute servers at Ford worldwide. To reduce costs, alternative ways of computing are being investigated such as using idle cycles on {{the large number of}} available Ford workstations. While smaller analyses run efficiently on <b>single</b> <b>workstations,</b> more complex calculations require larger computer resources as potentially available with several workstations clustered together using a distributing memory parallel code. The presented paper outlines results from evaluating and benchmarking a V 70. 7 development version of the distributed memory parallel MSC. NASTRAN code on an IBM RS/ 6000 model 590 workstation cluster at For...|$|R
40|$|This paper {{considers}} parallel {{generation and}} adaptation of partitioned, three dimensional, unstructured meshes for time-dependent CFD solvers. Firstly the parallel generation of partitioned Delaunay meshes based on geometry decomposition is described. A post processing step is then employed {{to improve the}} mesh quality at the inter-domain boundaries. The use of dynamic load balancing allows effective memory use for <b>single</b> <b>workstations</b> as well as improving performance on parallel machines. Secondly, we consider partitioned 3 D adaptive meshing techniques. In particular, we study how to effectively partition a hierarchical data-structure. Finally, we give {{a model of a}} solver which includes machine parameters and models communication. This is used to compare the efficiency of a partition on machines with differing characteristics...|$|R
50|$|DMI can co-exist with SNMP {{and other}} {{management}} protocols. For example, when an SNMP query arrives, DMI can {{fill out the}} SNMP MIB with data from its MIF. A <b>single</b> <b>workstation</b> or server {{can serve as a}} proxy agent that would contain the SNMP module and service an entire LAN segment of DMI-capable machines.|$|E
50|$|Each ISPW card had two Intel i860 {{microprocessors}} (running at 80 MFLOPS). An additional {{card with}} eight channels of audio I/O was also available for multi-channel sound recording and playback. A three-board ISPW provided {{what was at}} the time unsurpassed signal processing and audio synthesis power on a <b>single</b> <b>workstation.</b> A single ISPW card cost approximately $12,000US (not including the computer), which made it prohibitively expensive outside of research institutes and universities.|$|E
50|$|Early {{presentation}} graphics software ran on computer workstations, {{such as those}} manufactured by Trollman, Genigraphics, Autographix, and Dicomed. It became quite easy to make last-minute changes compared to traditional typesetting and pasteup. It was also {{a lot easier to}} produce a large number of slides in a small amount of time. However, these workstations also required skilled operators, and a <b>single</b> <b>workstation</b> represented an investment of $50,000 to $200,000 (in 1979 dollars).|$|E
50|$|Equalizer (software) is an {{open source}} {{rendering}} framework and resource management system for multipipe applications, ranging from <b>single</b> pipe <b>workstations</b> to VR installations. Equalizer provides an API to write parallel, scalable visualization applications which are configured at run-time by a resource server.|$|R
50|$|Compression is {{supported}} only in JFS1 on AIX {{and uses a}} variation of the LZ algorithm. Because of high CPU usage and increased free space fragmentation, compression is not recommended for use other than on a <b>single</b> user <b>workstation</b> or off-line backup areas.|$|R
40|$|Modern {{computing}} centers {{provide their}} users {{with a variety}} of computing resources ranging from <b>single</b> processor <b>workstations</b> to high-performance parallel computers. Often included in the mix are Beowulf class machines [1], that is, clusters of commodity personal computers (PCs) configured to operate as paralle...|$|R
50|$|ArchivistaBox {{comes with}} a web-based user {{interface}} and is set {{up with a few}} mouse clicks whether a <b>single</b> <b>workstation</b> is concerned or a storage cluster. No further software is needed. Document scanners, multi function copiers and internal services can be connected any time. The integrated print server (CUPS), for example, converts spool files automatically into an archive friendly format and sends them to the database where they remain in absolutely audit proof storage.|$|E
50|$|Central Management, a {{component}} of distributed firewalls, makes it practical to secure enterprise-wide servers, desktops, laptops, and workstations. Central management provides greater control and efficiency and it decreases the maintenance costs of managing global security installations. This feature addresses the need to maximize network security resources by enabling policies to be centrally configured, deployed, monitored, and updated. From a <b>single</b> <b>workstation,</b> distributed firewalls can be scanned to understand the current operating policy and to determine if updating is required.|$|E
5000|$|... boincmgr (or boincmgr.exe), a GUI which {{communicates}} {{with the}} core application using remote procedure calls. By default a core client only allows connections {{from the same}} computer, {{but it can be}} configured to allow connections from other computers (optionally using password authentication); this mechanism allows one person to manage a farm of BOINC installations from a <b>single</b> <b>workstation.</b> A drawback to the use of RPC mechanisms is that they are often felt to be security risks because they can be the route by which hackers can intrude upon targeted computers (even if it's configured for connections from the same computer).|$|E
40|$|In {{electronically}} mediated distance collaborations involving scientific data, {{there is}} often the need to stream the graphical output of individual computers or entire visualization clusters to remote displays. This paper presents TeraVision as a scalable platform-independent solution which is capable of transmitting multiple synchronized high-resolution video streams between <b>single</b> <b>workstations</b> and/or clusters without requiring any modifications {{to be made to}} the source or destination machines. Issues addressed include: how to synchronize individual video streams to form a single larger stream; how to scale and route streams generated by an array of MxN nodes to fit a XxY display; and how TeraVision exploits a variety of transport protocols. Results from experiments conducted over gigabit local-area networks and wide-area networks (between Chicago and Amsterdam), are presented. Finally, we propose the Scalable Adaptive Graphics Environment (SAGE) - an architecture to support future collaborative visualization environments with potentially billions of pixels. 1...|$|R
40|$|This paper {{presents}} a portable parallel programming environment for Modula- 2 * [...] an explicitly parallel machine-independent extension of Modula- 2. Modula- 2 * offers synchronous and asynchronous parallelism, a global single address space, and automatic data and process distribution. The Modula- 2 * system {{consists of a}} compiler, a debugger, a cross-architecture make, a runtime systems for different machines, {{and a set of}} scalable parallel libraries. Implementations exist for the MasPar MP series of massively parallel processors (SIMD), the KSR- 1 parallel computer (MIMD), heterogeneous LANs of <b>workstations</b> (MIMD), and <b>single</b> <b>workstations</b> (SISD). The paper presents the important components of the Modula- 2 * environment and discusses selected implementation issues. We focus on how we achieve a high degree of portability for our system {{while at the same time}} ensuring efficiency. 1 Introduction The demand for increasing computer performance at reasonable cost leads to rising interest in parall [...] ...|$|R
40|$|Grid {{computing}} [...] the {{assemblage of}} heterogeneous distributed clusters of computers {{viewed as a}} single virtual machine [...] promises {{to serve as the}} next major paradigm in distributed computing. Since Grids are assemblages of (usually) autonomous systems (autonomous clusters, supercomputers, or even <b>single</b> <b>workstations)</b> scheduling can become a complex affair which must take into consideration not just the requirements (and scheduling decisions) made at the point of the job's origin, but also the scheduling requirements (and decisions) made at remote points on the fabric, and in particular scheduling decisions made by a remote autonomous system onto which the local job has been scheduled. The current existing scheduling models range from static, where each of the programs is assigned once to a processor before execution of the program commences, to dynamic, where a program may be reassigned to different processors, or a hybrid approach, which combines characteristics of both techniques [1, 4, 5]...|$|R
50|$|The basic {{algorithm}} {{is similar to}} other text indexing and retrieval engines, except that the text records in the index are huge, consisting of multiple files each. This index is searched using a boolean matching algorithm like most other text indexing and retrieval engines. After {{one or more of}} these large text records is matched, Agrep is used to actually scan for the exact text desired. While this is slower than traditional totally indexed approaches, the advantage of the smaller index is seen to be advantageous to the individual user. This approach would not work particularly well across websites, but it would work reasonably well for a single site, or a <b>single</b> <b>workstation.</b> In addition, the smaller index can be created more quickly than a full index.|$|E
5000|$|On 9 July 2006, Plusnet lost 700 GB of {{customer}} email data due to human error. During a routine maintenance upgrade to the email system, an engineer mistakenly reformatted a live disk pack {{instead of the}} intended backup disk pack. Plusnet provided updates on their investigation but did not reveal the size or {{cause of the problem}} until 10 July 2006 at 15:39. Plusnet explained that the engineer responsible had accessed both the live and backup disk packs from a <b>single</b> <b>workstation.</b> The engineer believed his reconfiguration was to the backup storage when it was actually connected to the live email disk pack. In the following days, Plusnet did recover some email data and explained that other data may have been lost to corruption during the recovery. The official Plusnet UserGroup launched an [...] "Email Stability & Resiliency Campaign" [...] to attempt to ensure Plusnet made suitable investments and put in place measures to prevent future issues.|$|E
40|$|In this paper, the {{implementation}} of open-source parallel-version FDTD (Finite-Difference-Time-Domain) software, MEEP, on Texas A&M supercomputers and commercial finite element package, COMSOL, on a <b>single</b> <b>workstation</b> for the design design of nano-optical device is reported. The the computer architecture and performance of both numerical methods on the same design will be briefly described. Comment: 2 pages, 2 figure...|$|E
40|$|EPPP is an Environment for Portable Parallel Programming (EPPP) {{for current}} and future {{generation}} parallel computers. It is portable {{in the sense that}} the user can develop and tune his/her application on <b>single</b> <b>workstations</b> and rapidly port it and run it efficiently on a variety of parallel distributed-memory machines. In order to achieve this goal, EPPP consists of mainly four components that are integrated together: a programming language based on the data-parallel programming paradigm, an advanced compilation technology, a simulator, and a performance debugger. The topic of this paper is to present an overview of these components, their current status and their future development. 1 Background A program is said to be portable when it can easily be compiled and executed efficiently on various platforms. Portability is known to be a major concern for people like scientists and engineers who have developped very large software over the years and are pursuing their development for furt [...] ...|$|R
40|$|Laboratory {{staff with}} an {{effective}} way to exploit cluster computing as a solution to the demand for computational power in large-scale algorithm development, data analysis, and simulation tasks. Because sensor capabilities and demands continue to increase, the dataset sizes and algorithm complexities of today’s challenging applications have outgrown the processing capabilities of <b>single</b> <b>workstations.</b> Cluster computing technology, where a networked set of workstations is used as a parallel processor, can provide the throughput and storage demands of these applications. Programming a cluster, however, requires algorithm developers to become parallel programmers, which is difficult, time-consuming, and distracting. To allow a large research community (who primarily use MATLAB) to exploit cluster computing, we have developed a parallel programming toolbox called pMatlab, which consists of a library of objects and routines for distributing numerical arrays onto multiple processors, and then carrying out parallel computations on these distributed arrays. A typical MATLAB programmer can use pMatlab to convert a program to a parallel implementation in a few hours o...|$|R
40|$|International audienceDL_POLY is {{a general}} purpose {{molecular}} dynamics simulation package with in-built parallel algorithms. It may be run on {{a wide selection of}} distributed memory parallel computers, from national supercomputers with thousands of processors, to <b>single</b> processor <b>workstations</b> and can simulate small systems with order 100 atoms, to systems with millions of atoms. This introduction provides an outline of the features of the package and the underlying methodology...|$|R
40|$|It {{is argued}} that digital {{libraries}} {{of the future will}} contain terabyte-scale collections of digital text and that full-text searching techniques will be required to operate over collections of this magnitude. Algorithms expected to be capable of scaling to these data sizes using clusters of modern workstations are described. First, basic indexing and retrieval algorithms operating at performance levels comparable to other leading systems over gigabytes of text on a <b>single</b> <b>workstation</b> are presented. Next, simple mechanisms for extending query processing capacity to much greater collection sizes are presented, to tens of gigabytes for single workstations and to terabytes for clusters of such workstations. Query-processing efficiency on a <b>single</b> <b>workstation</b> is shown to deteriorate dramatically when data size is increased above a certain multiple of physical memory size. By contrast, the number of clustered workstations necessary to maintain a constant level of service increases linearl [...] ...|$|E
40|$|We have {{developed}} and implemented parallel algorithms for the {{molecular dynamics simulation}} of synthetic polymer chains. Our package has been specifically designed for distributed–memory machines like the widespread Cray T 3 E, {{but it can also}} be used on clusters of workstations and on a <b>single</b> <b>workstation</b> (i. e., it runs also sequentially). The target molecules are single synthetic polymer chains in solution...|$|E
40|$|Remote Unix Lab Environment (RULE) {{system when}} {{deployed}} on workstation class PCs. Two different scenarios were tested: performing standard tasks using a shell and serving web pages with content sourced from a database. The RULE system {{was found to}} perform satisfactorily with up to 100 virtual hosts on a <b>single</b> <b>workstation</b> PC and RAM usage {{was found to be}} the main factor limiting scalability. Keywords- Virtual Hosts, Scalability I...|$|E
40|$|This thesis proposes {{parallel}} {{and distributed}} algorithms for solving very largescale sparse optimization problems on computer clusters and clouds. Many modern applications problems from compressive sensing, machine learning and signal and image processing involve large-scale data {{and can be}} modeled as sparse optimization problems. Those problems are in such a large-scale that {{they can no longer}} be processed on <b>single</b> <b>workstations</b> running single-threaded computing approaches. Moving to parallel/distributed/cloud computing becomes a viable option. I propose two approaches for solving these problems. The first approach is the distributed implementations of a class of efficient proximal linear methods for solving convex optimization problems by taking advantages of the separability of the terms in the objective. The second approach is a parallel greedy coordinate descent method (GRock), which greedily choose several entries to update in parallel in each iteration. I establish the convergence of GRock and explain why it often performs exceptionally well for sparse optimization. Extensive numerical results on a computer cluster and Amazon EC 2 demonstrate the efficiency and elasticity of my algorithms...|$|R
40|$|Teklab is a {{manufacturer}} of electrical workstations. One major client group for the workstations is schools. Recently Teklab has started to concentrate on offering laboratory packages instead of <b>single</b> <b>workstations.</b> Laboratory packages include e. g. workstations, furniture, assembly and maintenance program. Teklab is looking for ideas to create more value for the laboratory package. One possibility is to design an educationalintroduction package to electric laboratories. In this thesis basic research isdone by studying the teaching methods in use today. Also modern education technology is presented. The thesis analyses different technologies for the laboratory introduction package. The technologies are analysed e. g. by applicability and feasibility. As {{a result of the}} work, the introduction has been done using an interactive presentation system. The presentation uses a software program and wireless student handsets designed by Dolphin Interactive. As a part of the work, also the recreation of the Teklab devices' instruction manuals has been started. For the future work, a digital learning object is under work, and a laboratory test is planned...|$|R
40|$|Complex {{analysis}} of {{large amounts of}} medical image data quickly exceeds storage capacity and computing power of <b>single</b> <b>workstations</b> or small local networks. When limited hardware resources impede full utilization of medical image analysis, a possible solution is the usage of computing grids, the collaboration of distributed resources across institutional borders. Many existing image processing problems would benefit from parallel processing, e. g. on single image or volume slice level. Such coarse-grained parallelization can easily be achieved by implementation into a grid infrastructure. Furthermore, grids allow distributed users to share their code, promoting collaborative projects. In this paper, we describe the grid implementation of existing code using grid workflows. The workflow management system is able to execute all tasks related to grid communication, such as authorization, scheduling and monitoring. It remains to the developer to make the code accessible for the workflow manager, and to define, where the code is found on the grid {{and what to do}} with it. We describe the procedure how to bring the code to the grid and show exemplarily the implementation of segmentation and registration algorithms for transrectal ultrasound guided prostate biopsies...|$|R
