575|648|Public
500|$|<b>Soft</b> <b>error</b> a type {{of error}} {{involving}} erroneous changes to signals or data but no changes to the underlying device or circuit ...|$|E
50|$|In a computer's memory system, a <b>soft</b> <b>error</b> changes an {{instruction}} {{in a program}} or a data value. Soft errors typically can be remedied by cold booting the computer. A <b>soft</b> <b>error</b> will not damage a system's hardware; the only damage is to the data that is being processed.|$|E
50|$|In {{electronics}} and computing, a <b>soft</b> <b>error</b> {{is a type}} of error where a signal or datum is wrong. Errors may be caused by a , usually understood either to be a mistake in design or construction, or a broken component. A <b>soft</b> <b>error</b> is also a signal or datum which is wrong, but is not assumed to imply such a mistake or breakage. After observing a <b>soft</b> <b>error,</b> there is no implication that the system is any less reliable than before. In the spacecraft industry this kind of error is called a single event upset.|$|E
40|$|Abstract—Feature sizes in VLSI {{circuits}} are steadily shrinking. This {{results in}} increasing susceptibility to <b>soft</b> <b>errors,</b> e. g. due to environmental radiation. Precautions against <b>soft</b> <b>errors</b> {{can be taken}} on all design stages, e. g. the architectural level, algorithmic level, or on the layout level. Whether the final implementation contains flaws or really provides robustness to <b>soft</b> <b>errors</b> remains to be checked. Here, we propose an approach to formally verify the robustness of a circuit with respect to multiple <b>soft</b> <b>errors.</b> We propose a fault model that prunes the exponentially sized space of multiple <b>soft</b> <b>errors</b> and an algorithm that automatically analyzes a given circuit. Keywords-robustness, multiple event upsets, <b>soft</b> <b>errors,</b> formal verification I...|$|R
40|$|<b>Soft</b> <b>errors</b> due to {{radiation}} {{are expected to}} increase in nanoelectronic circuits. Methods to reduce system failures due to <b>soft</b> <b>errors</b> include use of redundancy and making circuit elements robust such that <b>soft</b> <b>errors</b> do not upset signal values. Recent works have noted that electronic circuits have partial intrinsic immunity to <b>soft</b> <b>errors</b> since single event upsets on {{a large percentage of}} signal lines do not cause errors on circuit outputs. Using ISCAS- 89 benchmark circuits we present experimental evidence that the partial immunity to single event upsets is in most cases due to redundancy in the circuits and thus immunity to <b>soft</b> <b>errors</b> may not be available in irredundant circuits. Thus goals on immunity to <b>soft</b> <b>errors</b> may not be achievable in highly optimized circuits without adding circuit redundancy and/or relaxing the requirements on system failures due to <b>soft</b> <b>errors...</b>|$|R
40|$|Abstract. Feature sizes in VLSI {{circuits}} are steadily shrinking. This {{results in}} increasing susceptibility to <b>soft</b> <b>errors,</b> e. g. due to environmental radiation. Precautions against <b>soft</b> <b>errors</b> {{can be taken}} on all design stages, e. g. the architectural level, algorithmic level, or on the layout level. Whether the final implementation contains flaws or really provides robustness to <b>soft</b> <b>errors</b> remains to be checked. Here, we propose an approach to formally verify the robustness of a circuit with respect to multiple <b>soft</b> <b>errors.</b> We propose a fault model that prunes the exponentially sized space of multiple <b>soft</b> <b>errors</b> and an algorithm that automatically analyzes a given circuit. ...|$|R
5000|$|<b>Soft</b> <b>error</b> a type {{of error}} {{involving}} erroneous changes to signals or data but no changes to the underlying device or circuit ...|$|E
50|$|Dr Emer {{has also}} contributed to {{simultaneous}} multithreading (SMT), memory dependence prediction via store sets, <b>soft</b> <b>error</b> analysis, and led {{the development of the}} Asim simulator.|$|E
5000|$|There {{has been}} work {{addressing}} soft errors in processor and memory resources using both {{hardware and software}} techniques. Several research efforts addressed soft errors by proposing error detection and recovery via hardware-based redundant multi-threading.These approaches used special hardware to replicate an application execution to identify errors in the output, which increased hardware design complexity and cost including high performance overhead. Software-based <b>soft</b> <b>error</b> tolerant schemes, one the other hand, are flexible and can be apply on commercial off-the-shelf microprocessors. Many works propose compiler-level instruction replication and result checking for <b>soft</b> <b>error</b> detection.|$|E
40|$|Abstract—In the multi-peta-flop era for supercomputers, {{the number}} of {{computing}} cores is growing exponentially. However, with integrated circuit technology scaling below 65 nm, the critical charge required to flip a gate or a memory cell is dangerously reduced. Combined with higher vulnerability to cosmic radiation, <b>soft</b> <b>errors</b> are expected to become anything but inevitable for modern supercomputer systems. As a result, for long running applications on high-end machines, including linear solvers for dense matrices, <b>soft</b> <b>errors</b> have become a serious concern. Classical checkpoint and restart (C/R) scheme loses effectiveness against this threat {{because of the difficulty}} to detect <b>soft</b> <b>errors</b> in the form of transient bit flips that do not interrupt program execution and therefore leave no trace of error occurrence. Current research of <b>soft</b> <b>errors</b> resilience for dense linear solvers offers limited capability when faced with large scale computing systems that suffer both round-off error from floating point arithmetic and the presence followed by propagation of multiple <b>soft</b> <b>errors.</b> The use of error correcting codes based on Galois fields requires high computing cost for recovery. This work proposes a fault tolernat algorithm for dense linear system solver that is resilient to multiple spatial and temporal <b>soft</b> <b>errors.</b> This algorithm is designed to work with floating point data and is capable of recovering the solution of Ax = b from multiple <b>soft</b> <b>errors</b> that affect any part of the matrix during computation. Additionally, the computational complexity of the error detection and recovery is optimized through novel methods. Experimental results on cluster systems confirm that the proposed fault tolerance functionality can successfully detect and locate <b>soft</b> <b>errors</b> and recover the solution of the linear system. The performance impact is negligible and the <b>soft</b> <b>errors</b> resilient algorithm’s performance scales well on large scale systems. Keywords-soft error; fault tolerance; multiple errors; dense linear system solver...|$|R
40|$|The {{effects of}} <b>soft</b> <b>errors</b> in {{processor}} cores {{have been widely}} studied. However, little has been published about <b>soft</b> <b>errors</b> in uncore components, such as memory subsystem and I/O controllers, of a System-on-a-Chip (SoC). In this work, we study how <b>soft</b> <b>errors</b> in uncore components affect system-level behaviors. We have created a new mixed-mode simulation platform that combines simulators at two different levels of abstraction, and achieves 20, 000 x speedup over RTL-only simulation. Using this platform, we present the first study of the system-level impact of <b>soft</b> <b>errors</b> inside various uncore components of a large-scale, multi-core SoC using the industrial-grade, open-source OpenSPARC T 2 SoC design. Our results show that <b>soft</b> <b>errors</b> in uncore components can significantly impact system-level reliability. We also demonstrate that uncore <b>soft</b> <b>errors</b> can create major challenges for traditional system-level checkpoint recovery techniques. To overcome such recovery challenges, we present a new replay recovery technique for uncore components belonging to the memory subsystem. For the L 2 cache controller and the DRAM controller components of OpenSPARC T 2, our new technique reduces the probability that an application run fails to produce correct results due to <b>soft</b> <b>errors</b> by more than 100 x with 3. 32 % and 6. 09 % chip-level area and power impact, respectively. Comment: {{to be published in}} Proceedings of the 52 nd Annual Design Automation Conferenc...|$|R
40|$|Abstract—Soft {{errors are}} changes in memory value caused by {{external}} radiation or electrical noise. Decreases in computing feature sizes and power usages and shorting the microcycle period enhance {{the influence of}} <b>soft</b> <b>errors.</b> Self-stabilizing systems {{are designed to be}} started in an arbitrary, possibly a corrupted, state due to, say, <b>soft</b> <b>errors,</b> and to converge to a desired behavior. Self-stabilization is defined by the state space of the components and is essentially a well-founded, clearly defined form of the terms self-healing, automatic-recovery, automatic-repair, and autonomic-computing. To implement a self-stabilizing system, one needs to ensure that the microprocessor that executes the program is self-stabilizing. A self-stabilizing microprocessor copes with any combination of <b>soft</b> <b>errors,</b> converging to perform fetch-decode-execute in fault-free periods. Still, {{it is important that the}} microprocessor will avoid convergence periods if possible by masking the effect of <b>soft</b> <b>errors</b> immediately. In this work, we present design schemes for a selfstabilizing microprocessor and a new technique for analyzing the effect of <b>soft</b> <b>errors.</b> Previous schemes for analyzing the effect of <b>soft</b> <b>errors</b> were based on simulations. In contrast, our scheme computes a lower bound on microprocessor reliability and enables the microprocessor designer to evaluate the reliability of the design and to identify reliability bottlenecks. When analyzing the resiliency of digital circuits to <b>soft</b> <b>errors,</b> we examine the logical masking, i. e., errors in internal nodes of the circuits that are masked later by the computation. We show that the problem of computing the reliability of a circuit such that logical masking is taken into account is an NP-hard problem. Index Terms—Self-stabilization, microprocessor, <b>soft</b> <b>errors,</b> single event upset...|$|R
50|$|In {{critical}} designs, depleted boronconsisting {{almost entirely}} of boron-11is used, to avoid this effect and therefore to reduce the <b>soft</b> <b>error</b> rate. Boron-11 is a by-product of the nuclear industry.|$|E
5000|$|Shielding {{the chips}} {{themselves}} {{by use of}} depleted boron (consisting only of isotope boron-11) in the borophosphosilicate glass passivation layer protecting the chips, as boron-10 readily captures neutrons and undergoes alpha decay (see <b>soft</b> <b>error).</b>|$|E
5000|$|... 1T-SRAM-R : Incorporates ECC {{for lower}} <b>soft</b> <b>error</b> rates. To avoid an area penalty, it uses smaller bit cells, which have an {{inherently}} higher error rate, but the ECC more than {{makes up for}} that.|$|E
40|$|Terrestrial neutron-induced <b>soft</b> <b>errors</b> in {{semiconductor}} memory devices are currently {{a major concern}} in reliability issues. Understanding the mechanism and quantifying soft-error rates are primarily crucial for the design and quality assurance of {{semiconductor memory}} devices. This book covers the relevant up-to-date topics in terrestrial neutron-induced <b>soft</b> <b>errors,</b> and aims to provide succinct knowledge on neutron-induced <b>soft</b> <b>errors</b> to the readers by presenting several valuable and unique features. Sample Chapter(s). Chapter 1 : Introduction (238 KB). Table A. 30 mentioned in Appendix A. 6 o...|$|R
40|$|This paper {{addresses}} {{the issue of}} <b>soft</b> <b>errors</b> in quasi delay-insensitive (QDI) asynchronous circuits. We propose a general method to make QDI circuits tolerant of <b>soft</b> <b>errors</b> by duplicating and double-checking variables. Finally, we present {{a case study of}} a buffer and show SPICE-simulation results. 1...|$|R
40|$|A {{method is}} {{proposed}} for making asynchronous (QDI) circuits entirely tolerant to <b>soft</b> <b>errors</b> caused by radiation or other noise effects. The method has three components: (1) {{a special kind}} of duplication for random logic, (2) special circuitry for arbiter and synchronizer (as needed for example for external interrupts), and (3) error correction for memory arrays. An entire microcontroller has been designed with the method. Simulations at the digital level in the presence of random <b>soft</b> <b>errors</b> show that the system recovers from all <b>soft</b> <b>errors.</b> SPICE simulations also confirm the results...|$|R
5000|$|Single-event {{transient}} (SET) {{happens when}} the charge collected from an ionization event discharges {{in the form of}} a spurious signal traveling through the circuit. This is de facto the effect of an electrostatic discharge. <b>Soft</b> <b>error,</b> reversible.|$|E
50|$|Soft errors {{can also}} be caused by random noise or signal {{integrity}} problems, such as inductive or capacitive crosstalk. However, in general, these sources represent a small contribution to the overall <b>soft</b> <b>error</b> rate when compared to radiation effects.|$|E
50|$|Heisenbugs can {{be viewed}} as {{instances}} of the observer effect in information technology. Frustrated programmers may humorously blame a heisenbug on the phase of the moon, or (if it has occurred only once) may explain it away as a <b>soft</b> <b>error</b> due to alpha particles or cosmic rays affecting the hardware.|$|E
40|$|Abstract Exponentially {{increasing}} {{with technology}} scaling, <b>soft</b> <b>errors</b> {{have become a}} serious design concern in the deep sub-micron embedded systems. Partially Pro-tected Cache (PPC) is a promising microarchitectural feature to mitigate failures due to <b>soft</b> <b>errors</b> in embedded processors. A processor with PPC maintains two caches, one protected and the other unprotected, {{both at the same}} level of memory hierarchy. By finding out the data more prone to <b>soft</b> <b>errors</b> and mapping only that to the protected cache, the failure rate can be significantly improved at minimal power and performance penalty. While the effectiveness of PPCs has been demonstrated on multimedia applications – where the multimedia data is inherently resilient to <b>soft</b> <b>errors</b> – no such obvious data partitioning exists for applications in general. This paper proposes profile-based data partitioning schemes that are applicable to applications in general and effectively reduce failures due to <b>soft</b> <b>errors</b> at mini-mal power and performance overheads. Our experimental results demonstrate that our algorithm reduces the failure rate by 47 × on benchmarks from MiBench while incurring only 0. 5 % performance and 15 % power overheads. ...|$|R
40|$|<b>Soft</b> <b>errors</b> are faults {{which are}} not caused by {{defective}} hardware, rather they are induced due to noise or transient events. In this paper we describe defensive programming and redundancy techniques to detect and deal with <b>soft</b> <b>errors.</b> These techniques are categorized according to data and control flow errors. status: publishe...|$|R
40|$|A novel {{approach}} to minimizing {{the risks of}} <b>soft</b> <b>errors</b> at modelling level of mobile and ubiquitous systems is outlined. From a pure dependability viewpoint, critical components, whose failure is likely to impact on system functionality, attract more attention of protection/prevention mechanisms (against <b>soft</b> <b>errors)</b> than others do. Tolerating <b>soft</b> <b>errors</b> can be much improved if critical components can be identified at an early design phase and measures are taken to lower their criticalities at that stage. This improvement is achieved by presenting a criticality ranking (among the components) formed by combining a prediction of <b>soft</b> <b>errors,</b> consequences of them, and a propagation of failures at system modelling phase; and pointing out the ways to apply changes in the model to minimize the risks of degradation of desired functionalities. Case study results are given to illustrate and validate the approach...|$|R
5000|$|... "Soft errors" [...] of {{electronic}} devices due to cosmic rays on earth are, however, mostly due to neutrons {{which do not}} directly interact with the material and whose passage can therefore not be described by LET. Rather, one measures their effect in terms of neutrons per cm2 per hour, see <b>Soft</b> <b>error.</b>|$|E
50|$|While many {{electronic}} systems have an MTBF that exceeds the expected lifetime of the circuit, the SER {{may still be}} unacceptable to the manufacturer or customer. For instance, many failures per million circuits due to soft errors can be expected in the field if the system does not have adequate <b>soft</b> <b>error</b> protection. The failure of even a few products in the field, particularly if catastrophic, can tarnish {{the reputation of the}} product and company that designed it. Also, in safety- or cost-critical applications where the cost of system failure far outweighs the cost of the system itself, a 1% chance of <b>soft</b> <b>error</b> failure per lifetime may be too high to be acceptable to the customer. Therefore, it is advantageous to design for low SER when manufacturing a system in high-volume or requiring extremely high reliability.|$|E
50|$|There are {{two types}} of soft errors, chip-level <b>soft</b> <b>error</b> and {{system-level}} <b>soft</b> <b>error.</b> Chip-level soft errors occur when particles hit the chip, e.g., when the radioactive atoms in the chip's material decay and release alpha particles into the chip. Because the alpha particle contains a positive charge and kinetic energy, the particle can hit a memory cell and cause the cell to change state to a different value. The atomic reaction in this example is so tiny that it does not damage the physical structure of the chip. System-level soft errors occur when the data being processed is hit with a noise phenomenon, typically when the data is on a data bus. The computer tries to interpret the noise as a data bit, which can cause errors in addressing or processing program code. The bad data bit can even be saved in memory and cause problems at a later time.|$|E
2500|$|The {{frequency}} of <b>soft</b> <b>errors</b> that the card's controller must re-try ...|$|R
40|$|Soft-errors are {{changes in}} memory value caused by cosmic rays. Decrease in {{computing}} features size, decrease in power usage and shorting the micro-cycle period, enhances {{the influence of}} soft-errors. Self-stabilizing systems {{is designed to be}} started in an arbitrary, possibly a corrupted state, due to, say, <b>soft</b> <b>errors,</b> and to converge to a desired behavior. Self-stabilization is defined by the state space of the components, and essentially is a well founded, clearly defined, form of the terms: self-healing, automatic-recovery, automatic-repair, and autonomic-computing. To implement a self-stabilizing system one needs to ensure that the micro-processor that executes the program is self-stabilizing. A self-stabilizing microprocessor copes with any combination of <b>soft</b> <b>errors,</b> converging to perform fetch-decode-execute in fault free periods. Still, {{it is important that the}} micro-processor will avoid convergence periods as possible, by masking the effect of <b>soft</b> <b>errors</b> immediately. In this work we present design schemes for self-stabilizing microprocessor, and a new technique for analyzing the effect of <b>soft</b> <b>errors.</b> Previous schemes for analyzing the effect of <b>soft</b> <b>errors</b> were based on simulations. In contrast, our scheme computes lower bound on the micro-processor reliability and enables the micro-processor designer to evaluate the reliability of the design, and to identify reliability bottlenecks. When analyzing the resiliency of digital circuits to <b>soft</b> <b>errors,</b> we examine the logical masking, i. e., errors in internal nodes of the circuits which are masked later by the computation. W...|$|R
40|$|Abstract — Continuous {{shrinking}} in feature size, increasing {{power density}} etc. increase {{the vulnerability of}} microprocessors against <b>soft</b> <b>errors</b> even in terrestrial applications. The register file {{is one of the}} essential architectural components where <b>soft</b> <b>errors</b> can be very mischievous, because errors may rapidly spread from there throughout the whole system. Thus, register files are {{recognized as one of the}} major concerns when it comes to reliability. This paper introduces Self-Immunity, a technique that improves the integrity of the register file with respect to <b>soft</b> <b>errors.</b> Based on the observation that a certain number of register bits are not always used to represent a value stored in a register. This paper deals with the difficulty to exploit this obvious observation to enhance the register file integrity against <b>soft</b> <b>errors.</b> We show that our technique can reduce the vulnerability of the register file considerably while exhibiting smaller overhead in terms of area and power consumption compared to state-of-the-art in register file protection...|$|R
50|$|The normal memory reads {{issued by}} the CPU or DMA devices are checked for ECC errors, but due to data {{locality}} reasons they can be confined to a small range of addresses and keeping other memory locations untouched {{for a very long}} time. These locations can become vulnerable to more than one <b>soft</b> <b>error,</b> while scrubbing ensures the checking of the whole memory within a guaranteed time.|$|E
5000|$|Dragon uses a [...] "radiation-tolerant" [...] {{design in}} the {{electronic}} hardware and software that make up its s. The system uses three pairs of computers, each constantly checking on the others, to instantiate a fault-tolerant design. In {{the event of a}} radiation upset or <b>soft</b> <b>error,</b> one of the computer pairs will perform a soft reboot.Including the six computers that make up the main flight computers, Dragon employs a total of 18 triple-processor computers.|$|E
50|$|Designers {{can choose}} to accept that soft errors will occur, and design systems with {{appropriate}} error detection and correction to recover gracefully. Typically, a semiconductor memory design might use forward error correction, incorporating redundant data into each word to create an error correcting code. Alternatively, roll-back error correction can be used, detecting the <b>soft</b> <b>error</b> with an error-detecting code such as parity, and rewriting correct data from another source. This technique is often used for write-through cache memories.|$|E
40|$|Electronic {{equipment}} {{operating in}} harsh environments such as space {{is subjected to}} a range of threats. The most important of these is radiation that gives rise to permanent and transient errors on microelectronic components. The occurrence rate of transient errors is significantly more than permanent errors. The transient <b>errors,</b> or <b>soft</b> <b>errors,</b> emerge in two formats: control flow errors (CFEs) and data errors. Valuable research results have already appeared in literature at hardware and software levels for their alleviation. However, there is the basic assumption behind these works that the operating system is reliable and the focus is on other system levels. In this paper, we investigate the effects of <b>soft</b> <b>errors</b> on the operating system components and compare their vulnerability with that of application level components. Results show that <b>soft</b> <b>errors</b> in operating system components affect both operating system and application level components. Therefore, by providing endurance to operating system level components against <b>soft</b> <b>errors,</b> both operating system and application level components gain tolerance...|$|R
40|$|This paper proposes {{the use of}} metrics {{to refine}} system design for <b>soft</b> <b>errors</b> {{protection}} in system on chip architectures. Specifically this research shows the use of metrics in design space exploration that highlight where {{in the structure of}} the model and at what point in the behaviour, protection is needed against <b>soft</b> <b>errors.</b> As these metrics improve the ability of the system to provide functionality, they are referred to here as reliability metrics. Previous approaches to prevent <b>soft</b> <b>errors</b> focused on recovery after detection. Almost no research has been directed towards preventive measures. But in real-time systems, deadlines are performance requirements that absolutely must be met and a missed deadline constitutes an erroneous action and a possible system failure. This paper focuses on a preventive approach as a solution rather than recovery after detection. The intention of this research is to prevent serious loss of system functionality or system failure though it may not be able to eliminate the impact of <b>soft</b> <b>errors</b> completely...|$|R
40|$|Abstract. Soft-errors are {{changes in}} memory value caused by cosmic rays. Decrease in {{computing}} features size, decrease in power usage and shorting the micro-cycle period, enhances {{the influence of}} softerrors. Self-stabilizing systems {{is designed to be}} started in an arbitrary, possibly corrupted state, due to, say, <b>soft</b> <b>errors,</b> and to converge to a desired behavior. Self-stabilization is defined by the state space of the components, and essentially is a well founded, clearly defined, form of the terms: self-healing, automatic-recovery, automatic-repair, and autonomic-computing. To implement a self-stabilizing system one needs to ensure that the micro-processor that executes the program is self-stabilizing. The self-stabilizing microprocessor copes with any combination of <b>soft</b> <b>errors,</b> converging to perform fetch-decode-execute in fault free periods. Still, {{it is important that the}} micro-processor will avoid convergence periods as possible, by masking the effect of <b>soft</b> <b>errors</b> immediately. In this work we present design schemes for self-stabilizing microprocessor, and a new technique for analyzing the effect of <b>soft</b> <b>errors.</b> Previous schemes for analyzing the effect of <b>soft</b> <b>errors</b> were based on simulations. In contrast, our scheme computes lower bound on the micro-processor reliability and enables the micro-processor designer to evaluate the reliability of the design, and to identify reliability bottlenecks. ...|$|R
