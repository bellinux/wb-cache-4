5|39|Public
50|$|The UUHash is a 160-bit string that {{is usually}} Base64-encoded for presentation. It is a {{concatenation}} of an MD5 hash and a CRC32 sum of <b>selected</b> <b>chunks</b> of the file.|$|E
5000|$|The A.V. Club {{rated the}} episode an A-, concluding: [...] "As a creepy, {{up-to-date}} parable that still tells a tale {{as old as}} time, 'The Entire History Of You' is pretty outstanding. It builds to a climax the audience may well have predicted (Liam forces Ffion to show him her most recent encounter with Jonas), but we are smartly spared from seeing. Every time a character plays back something on their grain, their eyes glow dully as the images are accessed, giving them a demonic look. I'm sure that was an intentional decision." [...] Den of Geek said: [...] "As {{is often the case}} in science fiction, 'The Entire History Of You' explores the pitfalls of future technology. Given our current appetite for sharing carefully <b>selected</b> <b>chunks</b> of our personal lives on the Internet, the idea of people in the future recording and sharing memories isn't too much of a stretch, and the way the episode depicts it is quite convincing, and extremely eerie." [...] The Daily Telegraph gave the episode 3 out of 5 stars, and wrote: [...] "This was the least effective of the Black Mirror dramas, because the technological element wasn't so crucial to the trajectory of the story. Jealous people will always find ways to destroy their relationships without the recourse to memory databanks." [...] Metro gave the episode an A, writing: [...] "Tonight's final episode of Black Mirror however left me sitting in front of an appropriately black screen with the expression {{of a man who has}} just witnessed the murder of an entire litter of kittens." ...|$|E
40|$|Classic clone {{detection}} {{approaches are}} hardly capable of finding redundant code {{that has been}} developed independently, i. e., is {{not the result of}} copy&paste. To automatically detect such functionally similar code of independent origin, we experimented with a dynamic detection approach that applies random testing to <b>selected</b> <b>chunks</b> of code similar to Jiang&Su's approach. We found that such an approach faces several limitations in its application to diverse Java systems. This paper details on our insights regarding these challenges of dynamic detection of functionally similar code fragments. Our findings support a substantiated discussion on detection approaches and serve {{as a starting point for}} future research. Comment: 11 pages, 3 figure...|$|E
50|$|IE7Pro modifies Internet Explorer's default tab {{management}} {{features to}} add several options like double clicking {{to close the}} tab, undo closing tabs, enable Super Drag n Drop where dragging a link and dropping it anywhere else on the page opens it in a new tab or dragging and dropping any <b>selected</b> <b>chunk</b> of text opens the results of searching it via the default search engine. IE7Pro also adds an ad blocker that uses a blacklist of sites to block ads in web pages. It can also block ads delivered as rich media such as flash movies. It also allows crash recovery, which lets a browsing session be restored in case the browser crashed. The contents of a tab can be saved as an image as well.|$|R
50|$|Despite {{the fact}} that a proof of concept exists, the attack may succeed in very limited cases: such as small chunk size (32kB). By <b>selecting</b> larger <b>chunks</b> (i.e. >256kB) the amount of {{resources}} required to find SHA1 collision is tremendous, which makes the attack virtually impossible.|$|R
40|$|In P 2 P {{assisted}} multi-channel {{live streaming}} systems, {{it is commonly}} believed that in unpopular channels, quality degradation {{is due to the}} small number of participating peers with almost-the-same set of available data; this phenomena prevents effective data exchanges among peers themselves and automatically leads to data request contentions once a new data chunk becomes available. In popular programs, our measurement on PPLive for a continuous three-month period at various locations also shows numerous occurrences of quality degradation because of the even higher ratio (up to 190 %) of repetitive data requests for the same data chunks. These results motivate us to investigate effective data exchange strategies to reduce data request contentions for improved streaming quality. IDEA, an Improved peer Data Exchange Algorithm, is proposed to carefully <b>select</b> <b>chunks</b> to request from different peers. We conduct extensive simulations and the results show that IDEA significantly outperforms the widely used algorithms in deployed systems...|$|R
40|$|This study {{investigates the}} {{linguistic}} behaviour {{of the character}} Abu Jawdat, the chief police officer, with minor characters such as Abu Satoor and Subhi, and the major character of Muataz, in the fifth season of the Syrian TV series, Bab Al-Hara. In particular, it studies the linguistic behaviour of the above mentioned characters in light of (im) politeness theories, specifically Brown and Levinson’s (1987) model of politeness and Culpeper’s (1996) framework of impoliteness as well as Grice’s (1975) cooperative principles and its maxims. The data {{of the study was}} transcribed in situations of police interrogations conducted by Abu Jawdat with Abu Satoor, Subhi and Muataz. The analysis of the <b>selected</b> <b>chunks</b> of conversation revealed that the change in the interactive linguistic behaviour of the characters can be explained by means of (im) politeness theories. Normally characters that possess power will get hold of the conversational floor and will have more chances to attack face. </p...|$|E
40|$|This paper {{presents}} a fully automatic, multi-stage compaction approach to broadcast news summarisation, targeting transcripts from {{automatic speech recognition}} (ASR) systems. It employs a network of multi-layer perceptrons to remove incorrectly transcribed words based on confidence scores, and to <b>select</b> significant <b>chunks</b> at multiple stages based on tf. idf scores and named entity frequency. The resulting summaries are assessed {{using a combination of}} cross comprehension test and a fluency test, finally compared with an automatic evaluation scheme. The experimental results show the approach can produce summaries with good information content. 1...|$|R
40|$|The rational-empirical {{process was}} first {{proposed}} by the pragmatic philosopher Peirce to describe {{the manner in which}} a rational agent’s hypotheses are empirically validated and functioning in a social context. Sowa (2002) has updated and broadened these ideas as part of a knowledge cycle made up interacting abductive processes and a “knowledge soup ” of micro-theories that are loosely assembled in lattices. Abductive process <b>select</b> <b>chunks</b> of knowledge from the “soup”, evaluate their relevance to the situation/problem at hand, and form locally consistent theories to predict observations. In this view the human mind is not a highly organized knowledge base so much as cognitive content that is assembled as needed into pragmatically consistent, local theory. This allows for a natural growth of knowledge flowing inconsistent, loosely organized, and in perpetual flux. The concept is captured by the term knowledge soup: fluid, lumpy, with adherable chunks of theories and hypotheses that float in and out of awareness. This formulation, which is centered in agent’s intentions and empirical learning may be useful for designing cognitive architecture supporting interactive and inherently scruffy tasks, such as found in medicine and investigatory activities including planetary exploration. The paper connects Sowa’s framework with Bratman’s formulation of intention as a mental state that constrains future actions. Intention-based models of clinical guidelines, such as investigated in the Asgaard project, are discussed as fruitful avenues of research on the interaction between abductive reasoning and the weighing of alternative courses of action...|$|R
50|$|A {{wood carver}} begins a new carving by <b>selecting</b> a <b>chunk</b> of wood the {{approximate}} {{size and shape}} of the figure he or she wishes to create or, if the carving is to be large, several pieces of wood may be laminated together to create the required size. The type of wood is important. Hardwoods are more difficult to shape but have greater luster and longevity. Softer woods may be easier to carve but are more prone to damage. Any wood can be carved but they all have different qualities and characteristics. The choice will depend on the requirements of carving being done: for example a detailed figure would need a wood with a fine grain and very little figure as strong figure can interfere with 'reading' fine detail.|$|R
40|$|Acting in {{a dynamic}} {{environment}} {{is a complex}} task that requires several issues to be investigated, {{with the aim of}} controlling the associated search complexity. In this paper, a life-cycle for implementing adaptive capabilities on intelligent agents is proposed, which integrates planning and learning within a hierarchical framework. The integration between planning and learning is promoted by an agent architecture explicitly designed for supporting abstraction. Planning is performed by adopting a hierarchical interleaved planning and execution approach. Learning is performed by exploiting a chunking technique on successful plans. A suitable feedforward neural network <b>selects</b> relevant <b>chunks</b> used to identify new abstract operators. Due to the dependency between abstract operators and alreadysolved planning problems, each agent is able to develop its own abstract layer, thus promoting an individual adaptation to the given environment...|$|R
40|$|We study {{epidemic}} schemes in {{the context}} of collaborative data delivery. In this context, multiple chunks of data reside at different nodes, and the challenge is to simultaneously deliver all chunks to all nodes. Here we explore the inter-operation between the gossip of multiple, simultaneous message-chunks. In this setting, interacting nodes must <b>select</b> which <b>chunk,</b> among many, to exchange in every communication round. We provide an efficient solution that possesses the inherent robustness and scalability of gossip. Our approach maintains the simplicity of gossip, and has low message, connections and computation overhead. Because our approach differs from solutions proposed by network coding, we are able to provide insight into the tradeoffs and analysis of the problem of collaborative content distribution. We formally analyze the performance of the algorithm, demonstrating its efficiency with high probability. 1...|$|R
40|$|People {{browsing}} the web {{or reading}} a document may see text passages that describe {{a topic of}} interest, and {{want to know more}} about it by searching. Manually formulating a query from that text can be difficult, however, and an effec-tive search is not guaranteed. In this paper, to address this scenario, we propose a learning-based approach which gener-ates effective queries from the content of an arbitrary user-selected text passage. Specifically, the approach extracts and <b>selects</b> representative <b>chunks</b> (noun phrases or named entities) from the content (a text passage) using a rich set of features. We carry out experiments showing that the se-lected chunks can be effectively used to generate queries both in a TREC environment, where weights and query structure can be directly incorporated, and with a “black-box ” web search engine, where query structure is more limited...|$|R
40|$|In {{this paper}} we {{describe}} how progressive download and adaptive streaming {{can be combined}} into a simple and efficient streaming framework. Based on the MPEG- 4 file format (MP 4) we use HTTP for transport and argue that these two components are sufficient for specifying an open streaming architecture. The client <b>selects</b> appropriate <b>chunks</b> from the MP 4 file to be transferred based on (1) the header information (i. e. the "moov" box) {{in the first part}} of the file and (2) his observation of network throughput. The framework is completely client driven which allows for better server scalability and reduces signalling overhead. We discuss architecture and implementation issues such as complexity, interoperability and scalability and compare to 3 GPP PSS Rel- 6 adaptive streaming when appropriate. Measurements from a simple MP 4 /HTTP streaming client are presented showing that appropriate <b>chunks</b> are <b>selected</b> such that increased reliability is achieved...|$|R
40|$|This {{contribution}} {{is about the}} functionality of multi-word lexical items in teaching medical English. Corpus and text studies have shown {{how much of the}} language we use consists of multi-word phrases or lexical chunks. Medical English, like all other varieties of ESP, is highly phraseological and draws on a large stock of prefabricated or semi-prefabricated linguistic composites used to fulfil different functions in discourse. Therefore, {{it is up to the}} teacher to raise the students’ awareness of the lexical nature of the texts being analysed. Medical English learning may best be improved by encouraging students to develop their ability to <b>select</b> lexical <b>chunks</b> that are significant and useful to them. The focus on lexis involves activities targeted at noticing, recording and recycling language items which are typical of medical discourse. An electronic lexical notebook may be a useful tool for storing lexis and working cooperatively...|$|R
40|$|A {{fundamental}} proposition is {{that the}} accuracy of the designer's tender price forecasts is positively correlated with the amount of information available for that project. The paper describes an empirical study {{of the effects of the}} quantity of information available on practicing Quantity Surveyors' forecasting accuracy. The methodology involved the surveyors repeatedly revising tender price forecasts on receipt of chunks of project information. Each of twelve surveyors undertook two projects and <b>selected</b> information <b>chunks</b> from a total of sixteen information types. The analysis indicated marked differences in accuracy between different project types and experts/non-experts. The expert surveyors' forecasts were not found to be significantly improved by information other than that of basic building type and size, even after eliminating project type effects. The expert surveyors' forecasts based on the knowledge of building type and size alone were, however, found to be of similar accuracy to that of average practitioners pricing full bills of quantities...|$|R
40|$|What is {{the neural}} {{representation}} of a speech code as it evolves in time? A neural model simulates data concerning segregation and integration of phonetic percepts. Hearing two phonetically related stops in a VC-CV pair (V = vowel; C = consonant) requires 150 ms more closure time than hearing two phonetically different stops in a VC,-C 2 V pair. Closure time also varies with long-term stimulus rate. The model simulates rate-dependent category boundaries that emerge from feedback: interactions between a working memory for short-term storage of phonetic items and a list categorization network for grouping sequences of items. The conscious speech code is a resonant wave. It emerges after bottom-up signals from the working memory <b>select</b> list <b>chunks</b> which read out top-down expectations that amplify and focus attention on consistent working memory items. In VCi-C 2 V pairs, resonance is reset by mismatch of Cj with the C, expectation. In VC-CV pairs, resonance prolongs a repeated C...|$|R
40|$|Abstract. This paper {{describes}} {{affective and}} psychophysiological foundations {{used to help}} to control affective content in music production. Our work includes the proposal of a knowledge base grounded {{on the state of}} the art done in areas of Music Psychology. This knowledge base has relations between affective states (happiness, sadness, etc.) and high level music features (rhythm, melody, etc.) to assist in the production of affective music. A computer system uses this knowledge base to <b>select</b> and transform <b>chunks</b> of music. The methodology underlying this system is essentially founded on Affective Computing topics. Psychophysiology measures will be used to detect listener’s affective state. ...|$|R
30|$|The {{simplest}} way {{to form a}} chunked code is to use disjoint subsets of the input packets as chunks [8], which {{has been used in}} some applications of RLNC [9, 24, 25]. To decode a chunk, the transfer matrix of the chunk must have full rank of m; otherwise, none of the packets in the chunk could be recovered with high probability. However, it is not always a simple task to guarantee the success of decoding a chunk at the destination node. One approach is to use feedback-based chunk transmission mechanism [24]. While some efficient feedback protocols for specific applications have been developed [25, 26], in general, such feedback incurs an inevitable delay and also consumes network resources, resulting in degraded system performance. Besides, for some scenarios such as satellite and deep-space communications, feedbacks are not even available. Another approach is to employ a random scheduling-based chunk transmission scheme [27], where every network node always randomly <b>selects</b> a <b>chunk</b> for transmission. But this scheme has poor performance for small chunk sizes [10, 11].|$|R
40|$|We {{describe}} an editor for problem-solving knowledge that communicates with the user through English paraphrases of the knowledge. Although {{it does not}} support the full range of modifications one might want to make, the value of the tool {{lies in the fact that}} the user need not understand the syntax of the expert system to make modifications. By analyzing the problem-solving knowledge, the tool can allow the user to <b>select</b> semantically coherent <b>chunks</b> of the knowledge. It then presents English paraphrases of possible substitutions which would result in new problem-solving knowledge that is syntactically correct. In this way the tool expands the range of modifications that a naïve user can make to problem-solving knowledge in an expert system...|$|R
30|$|In {{previous}} works, NVBE {{has shown}} to be a good empirical estimate of syntactic autonomy of a n gram. The segmentation was performed by maximizing the autonomy estimate. However, for our task, we do not need the actual segmentation. We simply extract a list of autonomous expressions (possibly larger than words) in which their polarity is tested afterwards. For our task, we do not need to actually perform the whole segmentation procedure (i.e., without outputting/using the final segmented results) but to train and use the algorithms from the proposed unsupervised CWS system, {{in order to get a}} list of chunks for the follow-up polarity testing. We <b>select</b> an autonomous <b>chunk</b> based on NVBE values, with positive values presented on the left and right branchings.|$|R
40|$|Stream {{reasoning}} {{is the task}} of continuously de-riving conclusions on streaming data. To get re-sults instantly one evaluates a query repeatedly on recent data <b>chunks</b> <b>selected</b> by window operators. However, simply recomputing results from scratch is impractical for rule-based reasoning with seman-tics similar to Answer Set Programming, due to the trade-off between complexity and data throughput. To address this problem, we present a method to efficiently update models of a rule set. In particu-lar, we show how an answer stream (model) of a LARS program can be incrementally adjusted to new or outdated input by extending truth mainte-nance techniques. We obtain in this way a means towards practical rule-based stream reasoning with nonmonotonic negation, various window operators and different forms of temporal reference. ...|$|R
5000|$|Oberauer has {{extended}} Cowan's model {{by adding a}} third component, a more narrow focus of attention that holds only one chunk at a time. The one-element focus {{is embedded in the}} four-element focus and serves to <b>select</b> a single <b>chunk</b> for processing. For example, four digits can be held in mind {{at the same time in}} Cowan's [...] "focus of attention". When the individual wishes to perform a process on each of these digits—for example, adding the number two to each digit—separate processing is required for each digit, as most individuals can not perform several mathematical processes in parallel. Oberauer's attentional component selects one of the digits for processing, and then shifts the attentional focus to the next digit, continuing until all digits have been processed.|$|R
40|$|P 2 P-TV systems {{distribute}} {{live streaming}} contents by organizing the information flow in small chunks that are exchanged among peers. Different strategies {{can be implemented}} at the peers to <b>select</b> the <b>chunk</b> to distribute and the destination neighboring peer. Recent work showed that a good strategy consists in selecting the latest received chunk and a random neighboring peer (latest useful chunk, random peer). In this paper, leveraging {{on the idea that}} it is convenient to favor those peers that can contribute the most to the chunk distribution, we propose to select the destination peer with a probability proportional to the peer upload bandwidth. Considering the overlay topology, we evaluate both systems in which nodes have fixed degree and systems whose overlay setup takes into account the actual peer bandwidth by assigning more neighbors to peer with higher bandwidth. We show that the proposed scheme has a limited sensitivity to cheating peers that maliciously declare higher bandwidth than they actually have. We evaluate the performance in terms of delay percentiles and loss probability and evaluate the achieved improvements. Simulation results considering scenarios with up to 10, 000 peers shows that the proposed schemes significantly outperform the traditional ones, so that the chunk distribution delay drops to less than 2 s from about 12 s. 1...|$|R
40|$|Abstract—In this paper, we analyze {{processing}} divisible loads {{in systems}} with a memory hierarchy. Divisible loads are computations {{that can be}} divided into parts of arbitrary sizes and these parts can be independently processed in a distributed system. The problem is to partition the load so that the total processing time, including communications and computations, is the shortest possible. Earlier works in the divisible load theory assumed distributed systems with a flat memory model. The dependence of the processing time {{on the size of the}} assigned load was assumed to be linear. A new mathematical model relaxing the above two assumptions is proposed in this article. We study distributed systems which have both the hierarchical memory model and a piecewise linear dependence of the processing time on the size of the assigned load. Performance of such systems is modeled and evaluated. Finally, we compare the efficiency of distributed processing divisible loads in multiinstallment and out-of-core modes. Multiinstallment processing consists in sending multiple small chunks of the load to processors instead of a single chunk which needs external memory. It turns out that multiinstallment is an advantageous strategy for reasonably <b>selected</b> load <b>chunks</b> sizes. Index Terms—Divisible load theory, scheduling, performance evaluation, memory hierarchy, communication delays. æ...|$|R
40|$|An Intelligent Tutoring System (ITS) {{should be}} able to <b>select</b> {{appropriate}} <b>chunks</b> of learning materials as well as evaluate learning outcomes while keeping in mind learner’s various meta-cognitive and meta-affective factors. But literature review suggests that such systems are rare as they are complex and time consuming to develop. We have designed an adaptive intelligent tutoring system which is being implemented as a rules-based-expert-system for the dual purpose of- i) adaptive content selection and ii) evaluation of learning gain along with remedial actions. The system is in implementation stage and through this work, we inform in details about the developmental strategies adopted, e. g., use of Java Expert System Shell (JESS) for rules and fact base, Apachetomcat-server for Java implementation. This work also highlights the rule based implementation of domain and affective planner along with details about the rules in textual formats. Our student model is able to recognize learner’s guessing (gaming) behavior, interest, independence, and confidence level. It can also differentiate- a learner’s incorrect answer due to a guess from that due to lack of sufficient domain knowledge. This framework {{can be used as a}} guiding principle to build a more robust tutoring system by incorporating other student modeling attributes...|$|R
40|$|HTTP Adaptive Streaming (HAS) {{delivers}} {{video streaming}} services {{according to a}} client-server architecture where the client originates consecutive HTTP requests to download chunks of encoded video. In state-of-the-art systems, the client <b>selects</b> the <b>chunk</b> out of a finite set of differently encoded {{versions of the same}} video made available at the server site; the selection is driven by a client-centered buffer management procedure. Still, dynamic bitstream switching may have drawbacks in terms of undesirable visual quality fluctuations artifacts at the final user; besides, it may result in oscillatory behavior of the overall traffic in case of multiple users. Therefore, this paper proposes a sender-assisted procedure for HTTP Adaptive Streaming (HAS) services with improved user Quality of Experience (QoE) that proactively avoids buffer underflow events at the receiver side, thus reducing the need for dynamic bitstream switching. In the proposed sender-assisted approach, HAS leverages information on the encoded video available at the server side to assist the client in originating the data requests. Specifically, the sender-assisted HAS procedure exploits information on the encoded video content available at the sender side to regulate the interval between consecutive client-originated download requests. Significant QoE improvements brought by the proposed sender-assisted video streaming procedure are demonstrated in challenging fluctuating throughput conditions encountered in wireless ad hoc networks...|$|R
40|$|Modern {{applications}} of Web Services (WSs) that involve {{the processing of}} large amounts of data tend to transmit data in chunks. Several performance control techniques have been proposed to dynamically <b>select</b> the appropriate <b>chunk</b> size {{with a view to}} minimize the communication cost. However, when the data consumer is slower than the data producer, the consumer applications may suffer from memory shortage if high volumes of data arrive in the incoming buffers. To this end, we propose a specific approach to coupling performance control with congestion control features, in order to consider both performance and memory overflow issues in an integrated manner. The performance results with real data show that we can combine these controllers effectively and efficiently, so that no memory overflow occurs at the expense of negligible performance degradation. 1...|$|R
40|$|Abstract. In {{this paper}} we propose a new {{strategy}} for gravitational waves detection from coalescing binaries, using IIR Adaptive Line Enhancer (ALE) filters. This strategy is a classical hierarchical strategy in which the ALE filters have the role of triggers, used to <b>select</b> data <b>chunks</b> which may contain gravitational events, to be further analyzed with more refined optimal techniques, like the the classical Matched Filter Technique. After a direct comparison of the performances of ALE filters with the Wiener-Komolgoroff optimum filters (matched filters), necessary to discuss their performance and to evaluate the statistical limitation in their use as triggers, we performed a series of tests, demonstrating that these filters are quite promising both for the relatively small computational power needed and for the robustness of the algorithms used. The performed tests have shown a weak point of ALE filters, that we fixed by introducing a further strategy, based on a dynamic bank of ALE filters, running simultaneously, but started after fixed delay times. The results of this global trigger strategy seems to be very promising, and can be already used in the present interferometers, since it has the great advantage of requiring a quite small computational power and can easily run in real-time, in parallel with other data analysis algorithms. PACS numbers: 04. 80. Nn, 07. 05. Kf, 07. 60. Ly, 05. 40. Ca 1...|$|R
40|$|The {{increasing}} {{complexity of}} Information Systems (IS) calls for IS development methods to {{be adapted to}} the specific situations of the projects at hand. Method engineering {{is important because it}} focus on the creation of new methods {{that can be used in}} the system development process with the aim of constructing new information systems. The size and complexity of projects for developing information systems are becoming larger and more complicated. Therefore, development methods turn out to be one of the most significant key factors to achieve great success of development projects. The discipline of Situational Method Engineering (SME) focuses on the creation of new project specific methods. SME is a reuse strategy to assemble reusable method fragments or method chunks originating from different methods. New methods can be constructed from a method repository by <b>selecting</b> the <b>chunks</b> that are the most appropriate to a given situation. Thus, method chunks are the basic building blocks for constructing methods in a modular way. This dissertation have identified, analyzed and categorized a set of interoperability problems for the content of a method chunk repository. These problems have been represented as a set of patterns. The patterns will facilitate the understanding of specific interoperability problems that belong to a specific method chunk. If we are aware of the different interoperability problems that exist it is more likely that we can use a method chunk successfully within the context of SME...|$|R
40|$|Abstract—Similarity joins play an {{important}} role in many application areas, such as data integration and cleaning, record linkage, and pattern recognition. In this paper, we study efficient algorithms for similarity joins with an edit distance constraint. Currently, the most prevalent approach is based on extracting overlapping grams from strings and considering only strings that share a certain number of grams as candidates. Unlike these existing approaches, we propose a novel approach to edit similarity join based on extracting non-overlapping substrings, or chunks, from strings. We propose a class of chunking schemes based on the notion of tail-restricted chunk boundary dictionary. A new algorithm, VChunkJoin, is designed by integrating existing filtering methods and several new filters unique to our chunk-based method. We also design a greedy algorithm to automatically <b>select</b> a good <b>chunking</b> scheme for a given dataset. We demonstrate experimentally that the new algorithm is faster than alternative methods yet occupies less space. Index Terms—Edit similarity joins, approximate string matching, prefix filtering, q-gram, edit distance...|$|R
40|$|Abstract. In {{this paper}} we present {{concepts}} for and experiences with a Situated Public Display system deployed {{in a university}} setting. We identify the rate with which information is updated as an important property to distinguish different kinds of information. With a first slideshow based prototype {{it was very difficult}} for users to predict whether information was updated since they last looked. To solve this problem, we took a broader view and conducted a contextual inquiry to investigate how people deal with paper based posters. We deduced an information flow diagram that identifies roles of people and categories of posters and noticeboards. We identified actionables, that is, posters that offer people to take a specific action, as a special type of information to support. We identified two strategies, planning and opportunism, to deal with actionable information. We present a system using two kinds of displays, News Displays and Reminder Displays, to support both strategies. We show how auctions can be used for Reminder Displays to <b>select</b> those information <b>chunks</b> that are most important in a particular context. Finally, we present an evaluation and lessons from the deployment. ...|$|R
40|$|This paper studies a cloud-assisted {{procedure}} {{to improve the}} user's Quality of Experience (QoE) in HTTP Adaptive Streaming (HAS) services. HAS delivers video streaming services following a client-server architecture and requires the client to originate repeated HTTP requests to download chunks of encoded video. In state-of-the-art systems, the client <b>selects</b> the actual <b>chunk</b> to be downloaded from a finite set of differently encoded video versions available at the server site, according to a client-based buffer management procedure. In a multimedia cloud framework, HAS can leverage knowledge {{of the characteristics of}} the encoded video available at the server side. Therefore, we propose a cloud-assisted HAS procedure that exploits information on the encoded video content available at the cloud side to control the client-originated download requests. The proposed approach balances client-related quality issues, which would require intensive video chunks download to avoid playout stalls, with cloud related system constraints, which require the average download rate not to overcome the average video encoding rate. Finally, this approach procedure alleviates the computational load at the client, since the downloading strategy is computed at the cloud side. We demonstrate that significant QoE improvements are achievable through the proposed cloud-assisted buffer management procedure. © 2013 ACM...|$|R
40|$|How do {{reactive}} {{and planned}} behaviors interact in real time? How are sequences of such behaviors released at appropriate times during autonomous navigation to realize valued goals? Controllers for both animals and mobile robots, or animats, need reactive mechanisms for exploration, and learned plans to reach goal objects once an environment becomes familiar. The SOVEREIGN (Self-Organizing, Vision, Expectation, Recognition, Emotion, Intelligent, Goaloriented Navigation) animat model embodies these capabilities, and is {{tested in a}} 3 D virtual reality environment. SOVEREIGN includes several interacting subsystems which model complementary properties of cortical What and Where processing streams and which clarify similarities between mechanisms for navigation and arm movement control. As the animat explores an environment, visual inputs are processed by networks that are sensitive to visual form and motion in the What and Where streams, respectively. Position-invariant and sizeinvariant recognition categories are learned by real-time incremental learning in the What stream. Estimates of target position relative to the animat are computed in the Where stream, and can activate approach movements toward the target. Motion cues from animat locomotion can elicit head-orienting movements to bring a new target into view. Approach and orienting movements are alternately performed during animat navigation. Cumulative estimates of each movement are derived from interacting proprioceptive and visual cues. Movement sequences are stored within a motor working memory. Sequences of visual categories are stored in a sensory working memory. These working memories trigger learning of sensory and motor sequence categories, or plans, which together control planned movements. Predictively effective chunk combinations are selectively enhanced via reinforcement learning when the animat is rewarded. <b>Selected</b> planning <b>chunks</b> effect a gradual transition from variable reactive exploratory movements to efficient goal-oriented planned movement sequences. Volitional signals gate interactions between model subsystems {{and the release of}} overt behaviors. The model can control different motor sequences under different motivational states and learns more efficient sequences to rewarded goals as exploration proceeds. Riverside Reserach Institute; Defense Advanced Research Projects Agency (N 00014 - 92 -J- 4015); Air Force Office of Scientific Research (F 49620 - 92 -J- 0225); National Science Foundation (IRI 90 - 24877, SBE- 0345378); Office of Naval Research (N 00014 - 92 -J- 1309, N 00014 - 91 -J- 4100, N 00014 - 01 - 1 - 0624, N 00014 - 01 - 1 - 0624); Pacific Sierra Research (PSR 91 - 6075 - 2...|$|R
40|$|How do {{listeners}} integrate temporally distributed phonemic {{information into}} coherent representations of syllables and words? During fluent speech perception, {{variations in the}} durations of speech sounds and silent pauses can produce different perceived groupings. For example, increasing the silence interval between the words “gray chip ” may result in the percept “great chip”, whereas increasing the duration of fricative noise in “chip ” may alter the percept to “great ship ” (Repp et al., 1978). The ARTWORD neural model quantitatively simulates such contextsensitive speech data. In ARTWORD, sequential activation and storage of phonemic items in working memory provides bottom-up input to unitized representations, or list chunks, that group together sequences of items of variable length. The list chunks {{compete with each other}} as they dynamically integrate this bottom-up information. The winning groupings feed back to provide top-down support to their phonemic items. Feedback establishes a resonance which temporarily boosts the activation levels of <b>selected</b> items and <b>chunks,</b> thereby creating an emergent conscious percept. Because the resonance evolves more slowly than working memory activation, it can be influenced by information presented after relatively long intervening silence intervals. The same phonemic input can hereby yield different groupings depending on its arrival time. Processes of resonant transfer and competitive teaming help determine which groupings win the competition. Habituating levels of neurotransmitter along the pathways that sustain the resonant feedback lead to a resonant collapse that permits the formation of subsequent resonances. Key words: speech perception, word recognition, consciousness, adaptive resonance, context effects, consonant perception, neural network, silence duration, working memory, categorization, clustering...|$|R
40|$|AbstractWe have {{developed}} an ab initio protein structure prediction method called chunk-TASSER that uses ab initio folded supersecondary structure chunks of a given target as well as threading templates for obtaining contact potentials and distance restraints. The predicted <b>chunks,</b> <b>selected</b> {{on the basis of}} a new fragment comparison method, are folded by a fragment insertion method. Full-length models are built and refined by the TASSER methodology, which searches conformational space via parallel hyperbolic Monte Carlo. We employ an optimized reduced force field that includes knowledge-based statistical potentials and restraints derived from the chunks as well as threading templates. The method is tested on a dataset of 425 hard target proteins ≤ 250 amino acids in length. The average TM-scores of the best of top five models per target are 0. 266, 0. 336, and 0. 362 by the threading algorithm SP 3, original TASSER and chunk-TASSER, respectively. For a subset of 80 proteins with predicted α-helix content ≥ 50 %, these averages are 0. 284, 0. 356, and 0. 403, respectively. The percentages of proteins with the best of top five models having TM-score ≥ 0. 4 (a statistically significant threshold for structural similarity) are 3. 76, 20. 94, and 28. 94 % by SP 3, TASSER, and chunk-TASSER, respectively, overall, while for the subset of 80 predominantly helical proteins, these percentages are 2. 50, 23. 75, and 41. 25 %. Thus, chunk-TASSER shows a significant improvement over TASSER for modeling hard targets where no good template can be identified. We also tested chunk-TASSER on 21 medium/hard targets < 200 amino-acids-long from CASP 7. Chunk-TASSER is ∼ 11 % (10 %) better than TASSER for the total TM-score of the first (best of top five) models. Chunk-TASSER is fully automated and can be used in proteome scale protein structure prediction...|$|R
