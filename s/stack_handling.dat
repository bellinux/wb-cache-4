4|69|Public
40|$|This paper {{defines the}} grammar class of {{sequentially}} indexed grammars. Sequentially indexed grammars {{are the result}} of a change in the index <b>stack</b> <b>handling</b> mechanism of indexed grammars [Aho 68, Aho 69]. Sequentially indexed grammars are different from linear indexed language class. Unlike indexed languages, sequentially indexed languages allow polynomial parsing algorithms. We give a polynomial algorithm for parsing with sequentially indexed gramamrs that is an extension of the Earley algorithm for parsing with context free grammars...|$|E
40|$|This paper {{defines the}} grammar class of {{sequentially}} indexed grammars (SIGs) that {{results of a}} change in the index <b>stack</b> <b>handling</b> mechanism of indexed grammars [Aho 68, Aho 69]. SIGs are different from linear indexed grammars [Gaz 88] (the rule format is simpler) and they generate a strictly larger language class. We give a polynomial algorithm for parsing with SIGs that is a rather straightforward extension of the Earley algorithm for parsing with context free grammars. SIGs are attractive because of the simple rule format, the natural correspondence between indices and traces, and the perspicuity of the parsing scheme...|$|E
40|$|This thesis {{deals with}} a {{hardware}} accelerated Java virtual machine, named REALJava. The REALJava virtual machine is targeted for resource constrained embedded systems. The goal is to attain increased computational performance with reduced power consumption. While these objectives are often seen as trade-offs, in this context both {{of them can be}} attained simultaneously by using dedicated hardware. The target level of the computational performance of the REALJava virtual machine is initially set to be as fast as the currently available full custom ASIC Java processors. As a secondary goal all of the components of the virtual machine are designed so that the resulting system can be scaled to support multiple co-processor cores. The virtual machine is designed using the hardware/software co-design paradigm. The partitioning between the two domains is flexible, allowing customizations to the resulting system, for instance the floating point support can be omitted from the hardware in order to decrease the size of the co-processor core. The communication between the hardware and the software domains is encapsulated into modules. This allows the REALJava virtual machine to be easily integrated into any system, simply by redesigning the communication modules. Besides the virtual machine and the related co-processor architecture, several performance enhancing techniques are presented. These include techniques related to instruction folding, <b>stack</b> <b>handling,</b> method invocation, constant loading and control in time domain. The REALJava virtual machine is prototyped using three different FPGA platforms. The original pipeline structure is modified to suit the FPGA environment. The performance of the resulting Java virtual machine is evaluated against existing Java solutions in the embedded systems field. The results show that the goals are attained, both in terms of computational performance and power consumption. Especially the computational performance is evaluated thoroughly, and the results show that the REALJava is more than twice as fast as the fastest full custom ASIC Java processor. In addition to standard Java virtual machine benchmarks, several new Java applications are designed to both verify the results and broaden the spectrum of the tests...|$|E
5000|$|... #Caption: <b>Stack</b> content while <b>handling</b> {{a signal}} (linux x86/64) {{including}} sigcontext structure ...|$|R
40|$|Neural {{networks}} are a revolutionary but immature technique that is fast evolving and heavily relies on data. To {{benefit from the}} newest development and newly available data, we want the gap between research and production as small as possibly. On the other hand, differing from traditional machine learning models, neural network is not just yet another statistic model, but {{a model for the}} natural processing engine [...] - the brain. In this work, we describe a neural network library named akid. It provides higher level of abstraction for entities (abstracted as blocks) in nature upon the abstraction done on signals (abstracted as tensors) by Tensorflow, characterizing the dataism observation that all entities in nature processes input and emit out in some ways. It includes a full stack of software that provides abstraction to let researchers focus on research instead of implementation, {{while at the same time}} the developed program can also be put into production seamlessly in a distributed environment, and be production ready. At the top application stack, it provides out-of-box tools for neural network applications. Lower down, akid provides a programming paradigm that lets user easily build customized models. The distributed computing <b>stack</b> <b>handles</b> the concurrency and communication, thus letting models be trained or deployed to a single GPU, multiple GPUs, or a distributed environment without affecting how a model is specified in the programming paradigm stack. Lastly, the distributed deployment <b>stack</b> <b>handles</b> how the distributed computing is deployed, thus decoupling the research prototype environment with the actual production environment, and is able to dynamically allocate computing resources, so development (Devs) and operations (Ops) could be separated. Please refer to [URL] for documentation...|$|R
50|$|It relocates DOS kernel, COMMAND.COM interpreter, DOS {{resources}} (e.g.: buffers, file <b>handles,</b> <b>stacks,</b> lastdrive). It supports DOS 3.2 or higher.|$|R
40|$|International {{transportation}} is rapidly growing. Even {{during the recent}} recession, trade growth percentages world-wide exceeded 5 - 10 %. As a consequence, container handling capacity is rapidly growing as well. Larger vessels, bigger container terminals: {{it appears to be}} an on-going process of which the limits have not yet been reached. At the same time, there is an increasing emphasis on cost and environmental control, which forces terminal operators to search for innovative solutions. Solutions that require less space and cost less per handled container. Here robotization and automation come into play, as they allow reducing labour by a significant amount, and allow decreasing the space usage of a terminal by percentages up to 50 %. However, this comes at a cost: the terminals that have implemented large-scale robotization and automation, have suffered from lower productivity than aimed for, as well as significant start-up problems. Many of these problems are on behalf of the terminal control software, as case research has shown us. In detail, we analysed the establishment of the ECT-DSL terminal in Rotterdam, which among others showed that: ? The occurrence rate of system failures had been underestimated, which led to inefficient recovery procedures. ? The time pressure in the project led to a focus on getting the system to run, instead of realising the functional specifications. This caused much of the specified functionality not to be implemented. ? The interfaces between various control system components were a result of a negotiation process between various design groups, instead of a rational architecture design. ? The terminal was used in a different way than planned by the terminal operator. ? The terminal was not designed from a holistic point of view, which led to sub-optimisation and components that did not work properly together. Besides, literature and other recent case studies taught us that: ? A large gap exists between functional design of automated terminals and the technical design and software realisation. ? There is a lack of interaction between the design of robotized equipment and its control software, leading to sub-optimisation of each component. Even the equipment design is fragmented, which leads to different solutions for similar problems. ? Too little attention is paid to the interaction between the operator of the automated system and the system. ? A gap exists between aggregate, strategic targets, like throughput volumes and vessel service times, and operational, day-to-day, hour-to-hour operational targets, such as quay crane productivity and truck service times. ? There are no tools available to provide insight into the operation of automated equipment and/or automated terminals, including solutions for process control systems. ? A common-off-the-shelve process control system (PCS) for automated terminals does not exist (yet), which increases the risk of realising an automated terminal. ? There is a lack of integration between cost analysis tools and performance analysis tools. ? Current design approaches do not address the activities after commissioning, apart from monitoring and post-evaluation. Thus, the container handling industry and in particular container terminals, are arrived at a point where new solutions are required; solutions that provide higher performance at lower cost and less space usage. Here, robotization and large-scale automation is the way to go. However, up to now, robotized terminal development has suffered from problems during and after the design-engineering process. Therefore, we defined the following objective for our research: To develop an approach for designing robotized, marine container terminals, which addresses the specific characteristics of such terminals, and considers the specific properties of terminal environments. We have explored and pursued this objective within the boundaries of marine, robotized container terminals. In our design process, we focus on the reduction of risk of development of robotized terminals, and intend to include the innovative nature within the design-engineering process. The research methodology follows the inductive hypothetic cycle. Based on inductive case studies we have designed a framework of guidelines, and developed based on these guidelines an approach for designing robotized container terminals. The approach is accompanied by an extensive suite of (simulation) models that we developed and applied in many projects (see Dobner et al. 1999, 2000, 2001, 2002, 2003, and 2004). The model suite has a building-block based architecture (Verbraeck et al., 2002), which basically means that each component is defined by its interface, whereas the implementation internally may vary according to the scope of the analysis for which the model is used. Throughout the design process, the interfaces remain unchanged, but the behaviour may change, independent of the implementation of other components. The approach and model suite have been applied in a number of case studies during which we have gained experience with its applicability and contribution to the design process and the suite of models. We have also questioned an expert panel on the main guidelines about their usefulness. As a starting point for our approach we chose a simulation approach (Sol, 1982), which {{has proven to be a}} solid problem solving approach for complex problems. The approach we have designed is founded on four main activities within the design-engineering process: ? Functional design ? Technical design ? Implementation ? Commissioning and operations. In each activity we propose applying a simulation approach, which means that for design and engineering issues a problem solving approach is used. This approach heavily relies on the use of models, especially simulation models. For this purpose we developed a model suite that may support the entire design-engineering process until the terminal has been commissioned. Even during operations, the model suite may be used for fine-tuning, or problem solving when the operational conditions change. The basis of the approach consists of a framework of guidelines, which are the following: ? Using an object-oriented world-view. ? Applying a holistic, layered view on the terminal processes. ? Mirroring the real system’s architecture into the model’s architecture. ? Taking uncertainty and process variability into account. ? Using the operational processes as a leitmotiv for the design. ? Integrating the design of manual operations and automated operations. ? Integrating hardware and software design. ? Defining comprehensive and measurable objectives to assess the design. ? Basing the decisions within the design process on performance measurements. ? Continuing monitoring and measuring after commissioning. These guidelines have been elaborated into a detailed approach, including a stepwise, iterative approach to design a terminal, supported by a model suite that can be applied during the various activities, providing a way to manage the process. The design process consists of the following main steps: ? Defining the function of the terminal, the throughput capacity, and the services the terminal should provide. ? Designing the terminal’s key components, i. e. quay wall length, terminal geometry, layout of the <b>stack,</b> <b>handling</b> system, and logistical control concept. ? Designing the equipment and the process control system. After the functional design of the terminal’s components, they can be further detailed into the technical design (and specification). Subsequently, they are built (hardware) and implemented (software). After the implementation, commissioning takes place to verify whether every component works as it should. If this is successful, operations may start, during which a period of fine-tuning will take place. The entire process, here described in a nutshell, is completely performed following a simulation approach. This means that in all activities the use of models to evaluate and assess the quality in terms of the objectives of the terminal – at various levels of detail. During the functional design, the typical questions to be answered are to determine the right quay length, the required number of quay cranes, the required storage capacity, and the rail and gate capacity. Subsequently, the handling system is selected by assessing various alternatives, and the logistical control concept is being developed to match with the handling system. During the technical design, the process control system (or terminal operating system – TOS) is designed at a more detailed level, prototyping control algorithms, specifying the parameters, and configuring the terminal. As for most robotized terminals, there is no common-off-the-shelf software yet; much of it has to be designed and developed. During the realisation and implementation work, a simulation approach is used to provide a test-environment for components, i. e. equipment and software components. Because during this process, components become available piece by piece, the models may provide the remainder. This can be the rest of the TOS, or the real-life input from operators, or the equipment, or the representation of arrivals of vessels and/or trucks at the terminal. The model system provides an environment, manageable by the designers to create realistic circumstances to try out and test, especially from a performance point of view. During commissioning and operations, a simulation approach may be used to find bottlenecks, to perform quick analyses in case something appears not to work as planned. It may also fulfil a role as yardstick for the production software, as the software should provide the same performance level as the model system under the same conditions. Throughout this process, a suite of models is used that has an architecture similar to the real system’s architecture. This makes it possible to exchange model components with real components, and to link various modes of implementation to each other without needing to change any of them. The approach has been applied in various cases. In the software re-design and replacement project at ECT, the approach so-far has been applied to support the functional design, technical design, and software implementation. The research can be classified as action research as we have been involved in the process ourselves. However, we have found the approach well applicable in terms of finding the solutions that contribute to the terminal’s objectives, and in terms of finding the problems in the software that may hamper performance during the operation. Besides, the feedback from the functional design teams and the software development teams has been very positive so-far. The second case, in which the approach has been applied, is the design of a high density stacking crane, an overhead bridge crane. Here, the simulation approach has supported an integrated design of all components of the stacking crane. The crane has been modelled – both on behalf of equipment kinematics, as well as control software rules - at a very detailed level. The result was that many components could be optimized from a holistic point of view, i. e. aiming at the crane’s performance as part of an entire terminal system. This approach has saved a significant amount of money and has led to a more productive crane. Finally, the approach has been discussed, via a remote expert survey, with various experts on behalf of container terminal and/or the use of simulation. Most of the concepts that underpin the approach are considered a contribution to the quality of the design-engineering process. As a result the research has led to the following conclusions with regard to the research questions: ? In order to create an effective simulation approach, the models need to represent the TOS in particular, because the functionality of the TOS is a critical success factor. Representing the TOS of an automated terminal means – as compared to the representation of the TOS at manned terminals – the modelling of the software that controls the automated equipment, i. e. routing, collision avoidance, deadlock avoidance, et cetera. On top of the representation of the functionality of the TOS, it is also important to represent the technical behaviour of the TOS, i. e. response times, asynchronous behaviour, and limited possibilities to optimize decisions. ? In order to reduce the risk of automation in terms of performance loss in the operation, the approach we propose focuses on the achievement of the performance goals throughout the process, i. e. including the development and realisation processes. Secondly, our approach proposes to test software in an early stage by linking it to a realistic test environment containing the specific dynamic elements that make an operation at a container terminal so complicated. As such, complex interaction can be found and dealt with as early as possible. Thirdly, the simulation environment allows for testing the system including the interaction with an operator. ? In order to ensure that the insight provided by the simulation approach during the process is correct, the models need to be valid. In cases where a system of a novel nature is built, especially on behalf of the TOS, the simulation can be considered a prototype of the TOS. However, during the implementation, one has to be sure that the software is built after the prototype, either by working closely together with the software supplier, or by developing a detailed software specification combined with performance requirements based on the prototype and the achieved performance levels in the design phase. ? The design approach should be independent of the technology of the solutions that are designed, compared, and assessed, to assure a comparison in the same formats and driven by the same set of variables. The set of guidelines we propose appears valid for any container terminal design, as most variables are similar to any container terminal design project. On top of the model suite we developed as part of this research, it may require additional model development in case of a completely new container terminal design. Currently most of the common handling systems at container terminals have been covered, but as technology progresses, new systems may arise. However, the basic structure of the model environment is not likely to change, just as the function of a container terminal is not likely to change. Therefore, we may presume that the model environment will be able to depict future container terminal systems as well. With regard to applying a simulation approach throughout the entire design-engineering process, the following concluding observations can be made: ? First, we experienced that simulation is still associated to a large extent with indicative sayings rather than accurate assessments, which would make them principally unsuited for precise tasks such as prototyping and/or testing of software. However, we argue this not to be true for the model systems that we developed during our research, which contain a high level of detail to represent operations at a container terminal with sufficient realism to provide the possibility to prototype software components. This is an approach that is – based literature review – not often followed (see e. g. Wysk, 2001), which also contributes to the fact that a simulation approach is not commonly used for purposes and/or decisions to be taken further in the design process. ? Secondly, a simulation approach requires additional time, especially in the beginning of the project. This additional time is invested with the promise of a return on investment by means of better solutions that reduce the risk of the investment. However, the time needed, is not always available. Especially during the development of software, the time it takes to provide the feedback, may be longer than feasible, which leads to decisions made on perceptions rather than scientific analysis. ? Thirdly, the professional environment within container terminals can be characterised as primarily focussed on operations. Although we observe a trend of an increasing level of education within the managerial staff of container terminals, many positions are still occupied by people that come from operations. These people are less trained in using scientific approaches when solving problems, finding bottlenecks and taking decisions: they rather depend on observations in the operations. Although this leads to good results in many cases, the risk of taking the wrong solution, or a solution that is less effective than expected, is relatively large. Moreover, when it concerns solutions of a novel nature, as is the case in robotized container terminals, experience from the past is not always the best advisor. In the two test cases – as well as many other projects we carried out over the last years -, we have gathered concrete experiences with most of the guidelines as proposed in our design approach: ? The approach proved applicable to new developments, terminal extensions, and terminal improvement programmes. The scope and the set of feasible solutions are determined by the type of project, but in terms of the methodology, we conclude that a simulation approach from start to finish appears to be viable. Even in projects where product-based TOS software is being implemented, many questions have to be answered, and many uncertainties concerning performance and technical robustness remain to be answered during the design and implementation process. ? A crucial contribution of a simulation approach in the context of an entire design-engineering project appears to be the see-throughs that have to be made, when creating the functional design. This process of elaboration on specific solutions – these may be equipment specifications, but also software components or algorithms - provides not only feedback to the functional design; it also provides an outlook to the technical design in terms of a prototype. Especially when the prototype (within the simulation environment) is built in accordance with the architecture of the real system – both hardware and software -, it can provide useful input to the technical specification and the implementation. ? Said prototyping functionality by means of simulation models is of high added value because most terminals are similar, which allows for re-use of components in the model suite. Here, a building block based approach appears to be valuable, as it allows changing components internally without affecting its interface. ? Our initial choice to apply a building block based architecture of our model suite has proven to be very valuable to enable the support throughout the design-engineering process. It clearly reduces development time, and therefore reduces the response time whenever a question pops up, especially during the software development. It also allows for a stable architecture of the model system during the process, where in the beginning a more aggregate behaviour is modelled, which is more detailed in later stages. However, the interface of each component remains unchanged. For particular purposes, it may even be the case that some components are applied in a detailed implementation, whereas others are present in a more aggregated mode. This significantly reduces development time, and speeds up the experimentation. ? The use of a simulation approach fosters continuous attention for performance issues. As every project is under time pressure, it is common that during implementation/realisation the focus of the people involved – including the managerial level – moves from the functional requirements to the basic requirement of getting the terminal running. The attention for performance issues is then deliberately delayed to a later point in time. However, this may cause changing things at this later point in time to be impossible, because of decisions made to bring the system to work. When a model system is available allowing the people involved to assess whether these decisions would affect performance, different solutions can be sought and decisions can be carried out knowing the consequences. Of course, it is essential to make sure that this model system is valid and trustworthy for these kinds of assessments. ? In addition to the previous point, the simulation approach can not only be used to keep the focus on performance (parallel to getting the system to work), but it also can be used to test the system under all kinds of operational circumstances. This means that the likelihood of huge performance losses in the case of break-downs, or other disrupting factors becomes less, herewith reducing the risk for the terminal operator. When the results of these tests are shared with the people that will operate the terminal after going live, the negative effects are likely to be smaller during the start-up of operations. ? A prerequisite to the success of the approach proposed in this research is the cooperation and coordination between the simulation group and the software engineering team. We have experienced that this process is not always easy because of two reasons. The main reason is the difference in objectives. The prototype in the simulation environment is built to assess the contribution to performance, rather than to develop a piece of software that can be used in an operational environment. Although the two can go together, it is not always the case, which may lead to a solution that works perfectly in the model system, but difficult to transfer to the production software. The second reason is the concurrency between the simulation approach and the actual software engineering and development. As the simulation approach continues during th...|$|E
5000|$|Convenience - Packages {{can have}} {{features}} which add convenience in distribution, <b>handling,</b> <b>stacking,</b> display, sale, opening, reclosing, use, and reuse.|$|R
50|$|In 2013, an IPv6 {{version of}} the ping of death {{vulnerability}} was discovered in Microsoft Windows. Windows TCP/IP <b>stack</b> didn't <b>handle</b> memory allocation correctly when processing incoming malformed ICMPv6 packets, which could cause remote denial of service. This vulnerability was fixed in MS13-065 in August 2013. The CVE-ID for this vulnerability is CVE-2013-3183.|$|R
2500|$|Convenience – Packages {{can have}} {{features}} that add convenience in distribution, <b>handling,</b> <b>stacking,</b> display, sale, opening, reclosing, using, dispensing, reusing, recycling, {{and ease of}} disposal ...|$|R
50|$|The Killer NIC (Network Interface Card), from Killer Gaming, is {{designed}} to circumvent the Microsoft Windows TCP/IP <b>stack,</b> and <b>handle</b> processing on the card via a dedicated network processor. Most standard network cards are host based, and {{make use of the}} primary CPU. The manufacturer claims that the Killer NIC is capable of reducing network latency and lag. The card was first introduced in 2006.|$|R
40|$|Fitting and Showing {{training}} teaches 4 -H {{members to}} properly care for and exhibit their animals. This {{is a completely}} different competition from Obedience. Don't confuse the two and don't confuse your dog. Included in this publication are: learning about the breed, choosing a show lead, general appearance, preparing your dog for the ring, exhibitor, gaiting, lineup, <b>stacking,</b> showing, <b>handling,</b> baiting, individual exam, general knowledge and ring procedure. Illustrated. 24 pages...|$|R
50|$|Obstack code {{typically}} provides C macros {{which take}} care of memory allocation and management for the user. Basically, obstacks are used {{as a form of}} memory management which can be more efficient and less difficult to implement than malloc/free in several situations. For example, say one needs to set up a <b>stack</b> for <b>handling</b> data items whose number grows {{for a while and then}} reach a final form; such a stack could be defined in obstack.h.|$|R
5000|$|Sierra cups are {{typically}} small containers, holding about 10 oz. Sierra cups do have advantages: they are inexpensive and <b>stack</b> easily. Their <b>handles</b> and wide tops make them {{well suited for}} dispensing food out of a group pot, {{as well as a}} personal bowl/plate.|$|R
50|$|Each program {{uses only}} {{a finite number}} of cells, but the numbers in the cells can be {{arbitrarily}} large. Data structures such as lists or <b>stacks</b> can be <b>handled</b> by interpreting the number in a cell in specific ways, that is, by Gödel numbering the possible structures.|$|R
40|$|This paper {{reports the}} results of a {{simulation}} study into the <b>stacking</b> and <b>handling</b> of containers with the same dimensions. The measures of performance include volumetric utilisation, wasteful handling ratios, shortage ratio, and rejection ratio. The decision variables include the maximum dimensions of the store, stacking policies, and the different number of types of containers and their relative frequencies. The results indicate that the number of different types of containers has the largest impact on the measures of performance. The effects of the stacking policy and maximum store dimensions are also significant. ...|$|R
40|$|TU Dortmund University {{developed}} a simulation suite for the Container Terminal Dortmund GmbH {{seated in the}} largest inland port in Europe located in Dortmund. This suite focuses on modelling processes, resources and strategies for container terminals and enables to optimize a terminal with simulation. The aim is to optimize the terminal by determining the best mix of operating strategies for crane control, <b>stacking</b> area, <b>handling</b> area and resource management. To achieve this, the current situation of the terminal and different future scenarios for operating the terminal were modelled with the simulation suite...|$|R
5000|$|Poker chips: Currency is {{difficult}} to <b>stack</b> or <b>handle,</b> so most poker games are played with chips, or coin-shaped tokens of uniform size and weight, usually 39mm wide and anywhere from 5 to 16 grams in weight, whose money value is determined by their color. Historically, poker chips were made of bone; however, modern casino chips are often made of clay or a clay composite and are considered the most upscale variety of poker chip; other high-end chips are made of ceramic. Plastic chips are also available, at {{a wide variety of}} quality levels.|$|R
40|$|Current {{client-server}} {{applications such}} as online banking employ the same client-side software <b>stack</b> to <b>handle</b> information with differing security and functionality requirements, thereby increasing the size and complexity of software {{that needs to be}} trusted. While the high complexity of existing software is a significant hindrance to testing and analysis, existing software and interfaces are too widely used to be entirely abandoned. We present a proxy-based approach called FlowGuard {{to address the problem of}} large and complex client-side software stacks. FlowGuard’s proxy employs mappings from sensitiveness of information to trustworthiness of software stacks to demultiplex incoming messages amongst multiple client-side software stacks. One of these stacks is a fully-functional legacy software stack and another is a small and simple <b>stack</b> designed to <b>handle</b> sensitive information. In contrast to previous approaches, FlowGuard not only reduces the complexity of software handling sensitive information but also minimizes modifications to legacy software stacks. By allowing users and service providers to define the mappings, FlowGuard also provides flexibility in determining functionality-security tradeoffs. We demonstrate the feasibility of our approach by implementing a FlowGuard, called BLAC, fo...|$|R
2500|$|Conditions are a {{generalization}} of exceptions. When a condition arises, an appropriate condition handler is searched for [...] and selected, in <b>stack</b> order, to <b>handle</b> the condition. Conditions {{that do not}} represent errors may safely go unhandled entirely; their only purpose may be to propagate hints or warnings toward the user.|$|R
40|$|Agent {{technology}} {{is emerging as}} a new paradigm {{in the areas of}} distributed and mobile computing. Agent is a computational entity capable of relocating code, data and execution- state to another host. Mobile agents' code often experience transient faults resulting in a partial or complete loss during execution at a host machine. Protocol for fault – tolerant agent prevents a partial or complete loss of a mobile agent at a host. This article describes how to detect and recover random transient bit-errors at an agent before starting its execution at a host after its arrival at a host, in order to maintain availability of an agent by comparing an agent's states by using time and space redundancy. In this proposed self-repair approach, a software fix for fault – tolerance exists along with an agent. This generalized scheme is useful for recovering any kind of distributed agents against hardware transient faults (at a host). This paper presents a fault-tolerance mechanism for mobile agents that attempts to detect and correct any bit errors that may occur at a host after agents' mobility on a Web Agent-based Service Providing (WASP) platform. Though in modern distributed systems, the communication <b>stack</b> <b>handles</b> any bit errors and error correction is used on multiple layers (for example, in transport layer), the proposed approach is intended to be a supplement one to the conventional error detecting and correcting codes...|$|R
5000|$|Dedicated {{stacking}} bandwidth. Some switches {{come with}} built-in ports dedicated for stacking, which can preserve other ports for data network connections and {{can avoid the}} possible expense of an additional module to add <b>stacking.</b> Proprietary data <b>handling</b> or cables {{can be used to}} achieve higher bandwidths than standard Gigabit or 10-Gigabit connections.|$|R
40|$|In any programming, {{occurance}} {{of errors}} {{is a distinct}} reality. ◮ Errors need to be handled gracefully to avoid abrupt failures. ◮ Code that detects error may not have context to handle it. ◮ For example, attempting to open a file that doesn’t exist is acceptable in some circumstances and is a fatal error at other times. What does the file-handling module do? ◮ Conventionally it was done using error-checking and return-codes mechanism. ◮ Functions were checked for return values, and if the return code indicated failure, this code was interpreted and passed up the call <b>stack.</b> ‘Exception <b>Handling</b> in RUBY...|$|R
50|$|Bleached kraft is {{received}} in bales from Canadian mills. Fork trucks deliver a six-bale <b>stack</b> to bale <b>handling</b> equipment for destacking and dewiring. A dewiring system, the first automatic {{one in the}} industry, dewires bales before they are dumped from the conveyor to the pulper; the slush pulp is then pumped to a soak chest, refiners, and storage tank.|$|R
30|$|Impairments {{occurring}} on transmission channels usually {{results in}} data erasure or data corruption. Corruption means that data may be received with errors, while erasure means that data is not received at all. A system transmitting data {{directly on the}} channel would likely undergo corruption. More complex system including an IP <b>stack</b> would internally <b>handle</b> the detection of errors, resulting in data erasure.|$|R
50|$|If it were {{produced}} without any additives, {{the surface of}} the film would be so smooth that layers would adhere strongly to one another when the film is wound up, similar to the sticking of clean glass plates when <b>stacked.</b> To make <b>handling</b> possible, microscopic inert inorganic particles are usually embedded in the PET to roughen {{the surface of the}} film.|$|R
5000|$|The company {{closed its}} Moss Point sawmill in 1942, {{and moved the}} company office to Ten Mile, near Perkinston, Mississippi, where they opened a new sawmill. [...] During World War II, Dantzler Lumber Company entered into a {{contract}} with the War Department to use labor from the prisoner-of-war camp in Saucier, Mississippi for <b>stacking,</b> loading, and <b>handling</b> lumber at their Ten Mile sawmill.|$|R
5000|$|Fibre Channel {{switches}} may {{be deployed}} {{one at a}} time or in larger multi-switch configurations. SAN administrators typically add new switches as their server and storage needs grow, connecting switches together via fiber optic cable using the standard device ports. Some switch vendors offer dedicated high-speed <b>stacking</b> ports to <b>handle</b> inter-switch connections (similar to existing stackable Ethernet switches), allowing high-performance multi-switch configurations to be created using fewer switches overall.|$|R
5000|$|The SOG Knife was {{designed}} by Benjamin Baker, the Deputy Chief of the U.S. Counterinsurgency Support Office (CISO). [...] A chrome-moly steel known as SKS-3 was chosen for the blade and hardened to a Rockwell hardness of 55-57. [...] The blade pattern featured a convex false edge on the clip point of a Bowie knife. [...] The <b>stacked</b> leather <b>handle</b> {{was inspired by a}} Marbles Gladstone Skinning Knife made in the 1920s owned by Baker, into which finger grooves were molded. [...] The blade was typically parkerized or blackened to reduce glare. This was done so by applying a dark gun-blue finish (similar to those used on guns) on this carbon steel knife. The knife was carried in a leather sheath which contained a sharpening steel or whetstone.|$|R
40|$|Using an {{apparently}} simple problem, ‘‘Design a cylindrical can {{that will hold}} a liter of milk,’’ this paper demonstrates how engineeringdesign may facilitate {{the teaching of the}} following ideas to secondary students: linear and non-linear relationships; basic geometry ofcircles, rectangles, and cylinders; unit measures of area and volume; solving systems of equations with at least two variables;minimization of area to control materials costs and to prevent heat exchange; packing geometry to minimize space for transportation andstorage and for controlling for heat exchange; golden ratio as a design aesthetic; ergonomic factors in design including considerations ofcomfort of handling and safety; and strength of design for <b>stacking</b> and <b>handling</b> {{as well as for the}} prevention of accidental tipping. Thisinterdisciplinary curriculum uses engineering design challenges to engage students with meaningful and fun group activities anddiscussions that also teach a multitude of diverse and powerful mathematical concepts...|$|R
40|$|Abstract. MPI {{implementations}} {{are faced}} with growingly complex network configurations containing multiple network interfaces per node, NAT, or dual <b>stacks.</b> To implement <b>handling</b> logic correctly, thorough testing is necessary. However, the cost of providing such diverse setups in real hardware is prohibitively high, resulting in a lack of testing. In this article, we present a Virtual Test Environment (VTE) that considerably lowers this barrier by providing complex network environments on a single computer and thus enables testing in situations where it otherwise would not be feasible. ...|$|R
40|$|Any channel {{crossing}} {{the perimeter of}} a system provides an attack surface to the adversary. Standard network interfaces, such as TCP/IP stacks, constitute one such channel, and security researchers and exploit developers have invested much effort into exploring the attack surfaces and defenses there. However, channels such as USB have been overlooked, even though such code {{is at least as}} complexly layered as a network <b>stack,</b> and <b>handles</b> even more complex structures; drivers are notorious as a breeding ground of bugs copypasted from boilerplate sample code. This paper maps out the bus-facing attack surface of a modern operating system, and demonstrates that effective and efficient injection of traffic into the buses is real and easily affordable. Further, it presents a simple and inexpensive hardware tool for the job, outlining the architectural and computation-theoretic challenges to creating a defensive OS/driver architecture comparable to that which has been achieved for network stacks. 1...|$|R
40|$|AbstractIn this paper, the {{research}} object {{is the use}} of corrugated carton in packaging low-temperature yogurt, with the focus on hazard factors of external packaging cartons of yogurt in logistics process; on the basis of production, storage and transport links of yoghurt, the factors investigated in this study include relative humidity of storage environment, stock time, storage and <b>stacking,</b> circulation, transportation, <b>handling,</b> loading and unloading procedure; hazard factors and hazard degree encountered in each logistics link are analyzed; finally, the measures to ensure maximum safety and service life of packages of yogurt products are discussed...|$|R
40|$|We {{present some}} {{on-going}} research on phrase-based Statistical Machine Translation using flexible phrases that may contain gaps of variable lengths. This {{allows us to}} naturally handle various linguistic phenomena such as negations or separable particles. We integrate this within the standard Maximum Entropy model using some dedicated feature functions, and describe a beam-search <b>stack</b> decoder that <b>handles</b> these noncontiguous, elastic phrases. Preliminary experimental {{results show that the}} translation performance compares favourably with phrase-based MT using fixed gap size. We expect that future results may allow us to leverage the added flexibility of elastic chunks to further increase translation performance. ...|$|R
5000|$|A few new {{instructions}} were introduced with the 80186 (referred {{to as the}} 8086-2 instruction set in some datasheets): enter/leave (replacing several instructions when <b>handling</b> <b>stack</b> frames), pusha/popa (push/pop all general registers), bound (check array index against bounds), and ins/outs (input/output of string). A useful immediate mode was added for the push, imul, and multi-bit shift instructions. These {{instructions were}} {{also included in the}} contemporary 80286 and in successor chips. (The instruction set of the 80286 is exactly the instruction set of the 80186 with instructions added only for operations related to the 80286 Protected mode.) ...|$|R
40|$|The Allen Telescope Array (ATA) is a Large-Number-Small-Diameter radio {{telescope}} array currently with 42 individual antennas and 5 independent back-end science systems (2 imaging FX correlators and 3 time domain beam formers) {{located at the}} Hat Creek Radio Observatory (HCRO). The goal of the ATA is to run multiple back-ends simultaneously, supporting multiple science projects commensally. The primary software control systems {{are based on a}} combination of Java, JRuby and Ruby on Rails. The primary control API is simplified to provide easy integration with new back-end systems while the lower layers of the software <b>stack</b> are <b>handled</b> by a master observing system. Scheduling observations for the ATA is based on finding a union between the science needs of multiple projects and automatically determining an efficient path to operating the various sub-components to meet those needs. When completed, the ATA {{is expected to be a}} world-class {{radio telescope}}, combining dedicated SETI projects with numerous radio astronomy science projects. Comment: SPIE Conference Proceedings, Software and Cyberinfrastructure for Astronomy, Nicole M. Radziwill; Alan Bridger, Editors, 77400 Z, Vol 774...|$|R
