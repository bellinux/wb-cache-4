0|83|Public
50|$|The {{disadvantage}} of this fine-grained {{control is a}} complicated syntax, but, because all generic formal parameters are completely defined in the <b>specification,</b> the <b>compiler</b> can instantiate generics without looking at {{the body of the}} generic.|$|R
40|$|Abstract. Much {{work has}} been done in verifying a <b>compiler</b> <b>specification,</b> both in hand-written and {{mechanical}} proofs. However, there is still a gap between a correct <b>compiler</b> <b>specification</b> and a correct compiler implementation. To fill this gap and obtain a correct compiler implementation, we take the approach of generating a <b>compiler</b> from its <b>specification.</b> We verified the correctness of a <b>compiler</b> <b>specification</b> with the theorem prover Isabelle/HOL, and generated a Standard ML code corresponding to the specification with Isabelleâ€™s code generation facility. The generated compiler can be executed with some hand-written codes, and it compiles a small functional programming language into the Java virtual machine with several program transformations. ...|$|R
40|$|Managing {{tradeoffs}} between program {{structure and}} program efficiency {{is one of}} the most difficult problems facing software engineers. Decomposing programs into abstractions simplifies the construction and maintenance of software and results in fewer errors. However, the introduction of these abstractions often introduces significant inefficiencies. This paper describes a strategy for eliminating many of these inefficiencies. It is based upon providing alternative implementations of the same abstraction, and using information contained in formal specifications to allow a compiler to choose the appropriate one. The strategy has been implemented in a prototype compiler that incorporates theorem proving technology. Keywords: Program Modularity, Software Interfaces, Formal <b>Specifications,</b> <b>Compilers,</b> Program Optimization. 1 INTRODUCTION Many approaches to programming emphasize the use of specifications of interfaces. The basic idea is to achieve a separation of concerns. The client of an [...] ...|$|R
50|$|In recent years, {{researchers}} in Runtime Verification {{have recognized the}} potential of using Aspect-oriented Programming as a technique for defining program instrumentation in a modular way. Aspect-oriented programming (AOP) generally promotes the modularization of crosscutting concerns. Runtime Verification naturally is one such concern and can hence benefit from certain properties of AOP. Aspect-oriented monitor definitions are largely declarative, and hence tend to be simpler to reason about than instrumentation expressed through a program transformation written in an imperative programming language. Further, static analyses can reason about monitoring aspects more easily than about other forms of program instrumentation, as all instrumentation is contained within a single aspect. Many current runtime verification tools are hence built {{in the form of}} <b>specification</b> <b>compilers,</b> that take an expressive high-level specification as input and produce as output code written in some Aspect-oriented programming language (most often AspectJ).|$|R
40|$|We {{describe}} a methodology for the specification and verification {{of the tasks}} of compiler frontends. The semantics of the language to be compiled is specified with Evolving Algebras. With respect to the semantics of the language we define the static semantics. The semantical analysis checks whether a program satisfies the static semantics. We introduce a notion of correctness of semantic analysis and show {{how it can be}} proven. The technique will be demonstrated for a subset of real life imperative programming languages. Keywords: <b>Compiler</b> <b>specification,</b> <b>Compiler</b> Verification, Evolving Algebra, Semantic Analysis 1 Introduction Compiler correctness is essential for correct software development. If the compiler that transforms high level language programs (source programs) to machine code (target programs) is erroneous, it can not be assured that the application works correctly. In [GDG + 96] we described our notion of compiler correctness. We distinguish between the correctnes [...] ...|$|R
40|$|A {{certified}} binary is a value {{together with}} a proof that the value satisfies a given <b>specification.</b> Existing <b>compilers</b> that generate certified code have focused on simple memory and control-flow safety rather than more advanced properties. In this paper, we present a general framework for explicitly representing complex propositions and proofs in typed intermediate and assembly languages. The new framewor...|$|R
5000|$|The {{compiler}} creation process {{went through}} four steps: {{development of a}} formal specification of Ada, development of a formal <b>specification</b> of the <b>compiler</b> components; development of more detailed formal <b>specifications</b> of particular <b>compiler</b> passes; implementation of these specifications in Ada itself. [...] The general idea was {{to prove that the}} implementation was equivalent to the specification. Well-formedness criteria were used to supply additional constraints on operations beyond what was defined by the abstract syntax. The first step in the process, a formal specification for Ada, had already been started by five students at DTU in 1980 as part of their master's theses.|$|R
40|$|We {{describe}} the formal machine-checked verification {{of a simple}} <b>compiler</b> <b>specification</b> using the HOL theorem proving system. The language and microprocessor considered are {{a subset of the}} structured assembly language Vista, and the VIPER microprocessor, respectively. Our work is directly applicable to a family of languages and compilers. We discuss how the correctness theorem and verified compiler fit into a wider context of ensuring that object code is correct. We show how the compiler correctness result can be formally combined with a proof system for application programs. We have implemented a tool that executes the verified <b>compiler</b> <b>specification</b> using formal proof. We also suggest a way that a dependable implementation might be obtained...|$|R
40|$|Memory {{consistency}} models (MCMs) which govern inter-module {{interactions in}} a shared memory system, are a significant, yet often under-appreciated, aspect of system design. MCMs are defined {{at the various}} layers of the hardware-software stack, requiring thoroughly verified <b>specifications,</b> <b>compilers,</b> and implementations at the interfaces between layers. Current verification techniques evaluate segments of the system stack in isolation, such as proving compiler mappings from a high-level language (HLL) to an ISA or proving validity of a microarchitectural implementation of an ISA. This paper makes a case for full-stack MCM verification and provides a toolflow, TriCheck, capable of verifying that the HLL, compiler, ISA, and implementation collectively uphold MCM requirements. The work showcases TriCheck's ability to evaluate a proposed ISA MCM {{in order to ensure}} that each layer and each mapping is correct and complete. Specifically, we apply TriCheck to the open source RISC-V ISA, seeking to verify accurate, efficient, and legal compilations from C 11. We uncover under-specifications and potential inefficiencies in the current RISC-V ISA documentation and identify possible solutions for each. As an example, we find that a RISC-V-compliant microarchitecture allows 144 outcomes forbidden by C 11 to be observed out of 1, 701 litmus tests examined. Overall, this paper demonstrates the necessity of full-stack verification for detecting MCM-related bugs in the hardware-software stack. Comment: Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating System...|$|R
40|$|AbstractThe {{relations}} between two {{different approaches to}} program and <b>compiler</b> <b>specification,</b> i. e. attribute grammars and computational models are considered. Constructions are given which transform attribute grammars into semantically equivalent computational models. It enables one to use the algorithms of structural synthesis for generating effective attribute evaluation programs. As an example, {{the implementation of the}} semantics of programming languages by means of the NUT-systems is described...|$|R
40|$|AbstractThis paper {{provides}} {{a contribution to}} the formal verification of programs written in the concurrent functional programming language Erlang, which is designed for telecommunication applications. It presents a formal description of this language in Rewriting Logic, a unified semantic framework for concurrency which is semantically founded on conditional term rewriting modulo equational theories. The formalization is tailored to the SLC <b>Specification</b> Language <b>Compiler</b> Generator, which is being developed at Aachen University of Technology as part of the Truth verification platform. SLC can be used to automatically translate the Erlang description into a verification tool frontend, i. e., a parser and a semantic evaluator which, given an Erlang program, compute the associated transition system...|$|R
40|$|Abstraction in {{hardware}} description languages stalled at the register-transfer level decades ago, yet few alternatives {{have had}} much success, {{in part because}} they provide only mod-est gains in expressivity. We propose to make a much larger jump: a compiler that synthesizes hardware from behavioral functional <b>specifications.</b> Our <b>compiler</b> translates general Haskell programs into a restricted intermediate representa-tion before applying a series of semantics-preserving trans-formations, concluding with a simple syntax-directed trans-lation to SystemVerilog. Here, we present the overall frame-work for this compiler, focusing on the intermediate repre-sentations involved and our method for translating general recursive functions into equivalent hardware. We conclude with experimental results that depict the performance and resource usage of the circuitry generated with our compiler...|$|R
40|$|Responding to {{marketplace}} needs, todayâ€™s embedded processors must {{feature a}} flexible core that allows easy modification with fast time to market. In this environment, embedded processors are increasingly reliant on flexible support tools. This paper presents one such tool, called Quick Piping, a new, high-level formalism for modeling processor pipelines. Quick Piping {{consists of three}} primary components that together provide an easy-to-build, reusable processor description: Pipeline graphsâ€”a new high-level formalism for modeling processor pipelines, pipeâ€”a companion domain-specific language for specifying a pipeline graph, pipe minerâ€”a <b>compiler</b> <b>specification</b> generator for pipe descriptions. pipe miner processes a pipe description and produces a <b>compiler</b> <b>specification</b> {{that is used to}} build a compiler that reads the corresponding machineâ€™s instruction set and automatically generates resource vectors. Despite their ubiquity and importance in achieving high performance in modern processors, pipelinesâ€”and improving the mechanisms for specifying their operationâ€”have received little attention. Until now, handwritten resource vectors have served to specify information about a processorâ€™s pipeline and encode relevant information about each instructionâ€™s resource usage. Describing the complete set of resource vectors for a machine can be quite tedious and error prone, since it commonly must be developed by hand on an instruction-by-instruction basis. With its use of pipeline graphs, the pipe language, and the pipe miner <b>compiler</b> <b>specification</b> generator, Quick Piping gives the embedded processor architect and compiler writer an intuitive high-level abstraction of pipelines, a language for specifying a pipeline, and a tool for automatically producing pipeline resource vectors. The resulting specifications are quick to develop, easy to understand, simple to modify and maintain, and can be automatically processed to produce the low-level information required by processor control units and instruction schedulers...|$|R
50|$|ALCOR is {{an early}} {{computer}} language definition created by the ALCOR Group, a consortium of universities, research institutions and manufacturers in Europe and the United States {{which was founded in}} 1959 and which had 60 members in 1966. The group had the aim of a common <b>compiler</b> <b>specification</b> for a subset of ALGOL 60 after the ALGOL meeting in Copenhagen in 1958. ALCOR is an acronym, from ALGOL Converter.|$|R
40|$|Abstract: Two {{possibilities}} of automated CSP (Communicating Sequential Processes) support are introduced in [11] and [10] using either behavioral diagrams or application source code. While {{in the first}} approach a tool generates CSP specification from behavioral diagrams, based on UML Composite States diagram, in the second approach an application source code is translated directly into CSP <b>specification</b> using a <b>compiler.</b> This paper reviews tools related to both techniques...|$|R
40|$|A {{certified}} binary is a value {{together with}} a proof that the value satisfies a given <b>specification.</b> Existing <b>compilers</b> that generate certified code have focused on simple memory and control-flow safety rather than more advanced properties. In this article, we present a general framework for explicitly representing complex propositions and proofs in typed intermediate and assembly languages. The new framework allows us to reason about certified programs that involve effects while still maintaining decidable typechecking. We show how to integrate an entire proof sys-tem (the calculus of inductive constructions) into a compiler intermediate language and how the intermediate language can undergo complex transformations (CPS and closure conversion) while preserving proofs represented in the type system. Our work provides a foundation for the proces...|$|R
50|$|A {{large number}} of compile-time checks are {{supported}} to help avoid bugs {{that would not be}} detectable until run-time in some other languages or would require explicit checks {{to be added to the}} source code. For example, the syntax requires explicitly named closing of blocks to prevent errors due to mismatched end tokens. The adherence to strong typing allows detection of many common software errors (wrong parameters, range violations, invalid references, mismatched types, etc.) either during compile-time, or otherwise during run-time. As concurrency is part of the language <b>specification,</b> the <b>compiler</b> can in some cases detect potential deadlocks. Compilers also commonly check for misspelled identifiers, visibility of packages, redundant declarations, etc. and can provide warnings and useful suggestions on how to fix the error.|$|R
40|$|AbstractThe JastAdd system enables modular <b>specifications</b> of {{extensible}} <b>compiler</b> {{tools and}} languages. Java {{has been extended}} with the Rewritable Circular Reference Attributed Grammars formalism that supports modularization and extensibility through several synergistic mechanisms. Object-orientation and static aspect-oriented programming are combined with declarative attributes and context-dependent rewrites to allow highly modular specifications. The techniques have been verified by implementing a full Java 1. 4 compiler with modular extensions for non-null types and Java 5 features...|$|R
40|$|A {{certified}} binary is a value {{together with}} a proof that the value satisfies a given <b>specification.</b> Existing <b>compilers</b> that generate certified code have focused on simple memory and control-flow safety rather than more advanced properties. In this paper, we present a general framework for explicitly representing complex propositions and proofs in typed intermediate and assembly languages. The new framework allows us to reason about certified programs that involve effects while still maintaining decidable typechecking. We show how to integrate an entire proof system (the calculus of inductive constructions) into a compiler intermediate language and how the intermediate language can undergo complex transformations (CPS and closure conversion) while preserving proofs represented in the type system. Our work provides a foundation for the process of automatically generating certified binaries in a type-theoretic framework. ...|$|R
40|$|In {{the paper}} a design and {{implementation}} of Simple Object Description Language SODL for automatic interface creation are presented. First, problem domain [...] developing network applications and reasons for developing new domain-specific language are described. Since the cross network method calls slow down performance of our applications the solution was Tier to Tier Object Transport (TTOT). However, with this approach the network application development time has been increased. To enhance our productivity a new domain-specific SODL language has been designed. Syntax and semantics of SODL language are formally defined in an incremental way by special kind of attribute grammars that allows extensions and modifications in an easy way. From formal <b>specifications</b> SODL <b>compiler</b> is automatically generated using compiler/interpreter generator tool LISA. Finally, the benefits of our approach have been discussed. 1...|$|R
40|$|We give a {{comprehensive}} technical overview {{of our work}} on rigorous verification of compiling <b>specification</b> and <b>compiler</b> implementation of an initial correct binary compiler executable. We will concentrate on implementation verification. Machine program correctness is proved by a special bootstrapping technique with a posteriori code inspection. Our contribution is to perform this work for compilers and, hence, to relieve the application programmer's burden to prove implementation correctness again and again, as this is done today for safety and security critical applications. Once our work has been finished conscientiously and is accepted to reach sucient mathematical certainty, compilers {{may be used for}} proved correct program implementation, safely in the sense that every result a target program execution returns is guaranteed to be correct with respect to the source program semantics...|$|R
40|$|We {{present and}} formalize h, a core (or "plank") {{calculus}} {{that can serve}} {{as the foundation for}} several <b>compiler</b> <b>specification</b> languages, notably CRSX (Combinatory Reductions Systems with eXtensions), HACS (Higher-order Attribute Contraction Schemes), and TransScript. We discuss how the h typing and formation rules introduce the necessary restrictions to ensure that rewriting is well-defined, even in the presence of h's powerful extensions for manipulating free variables and environments as first class elements (including in pattern matching). Comment: workshop proceedings for HOR 201...|$|R
40|$|This article {{describes}} {{the process of developing}} a system for translating VDM executable specifications into Lazy ML prototypes. The system was specified in VDM and a Lazy ML prototype implementation was derived from its specification. This article concentrates on discussing the lessons learned in each stage of the development process, evaluating the adequacy of the used methodology. Keywords: Formal Methods, Rapid Prototyping, Functional Languages, <b>Specification</b> Languages, <b>Compilers.</b> Introduction This {{article describes}} the development process of a system for translating VDM [1] executable specifications into Lazy ML (LML) prototypes [2, 3]. This system provides machine support for software development methodologies based on VDM [4]. Prototypes may be used as animation tools for validating software system specifications in the very early stages of the development process. This validation is essential for formal development of software in practice. As observed in Reference 4, those pr [...] ...|$|R
40|$|The POEMS {{project is}} {{creating}} an environment for end-to-end performance modeling of complex parallel and distributed systems, spanning {{the domains of}} application software, runtime and operating system software, and hardware architecture. Towards this end, the POEMS framework supports composition of component models from these different domains into an end-to-end system model. This composition can be specified using a generalized graph model of a parallel system, together with interface specifications that carry information about component behaviors and evaluation methods. The POEMS <b>Specification</b> Language <b>compiler,</b> under development, will generate an end-to-end system model automatically from such a specification. The components of the target system may be modeled using different modeling paradigms (analysis, simulation, or direct measurement) and may be modeled at various levels of detail. As a result, evaluation of a POEMS end-to-end system model may require a variety of eval [...] ...|$|R
40|$|This report {{looks at}} a new {{specification}} language for expressing letters in a typeface {{as a collection of}} separate components. It allows components to be used in one or more letters in order to maintain consistency. We have furthermore developed a compiler that translates from this specification language to the PostScript Type 1 font format using our own algorithm to merge components to a global outline for each letter. Finally, we describe an instance of using our <b>specification</b> language and <b>compiler</b> to create a serif typeface...|$|R
40|$|A data {{language}} is a declarative language that enables database users to access and manipulate data. There are families of related data languages; each family member is targeted for a particular application. Unfortunately, building compilers for such languages is largely an ad hoc process; there are no tools and design methods that allow programmers to leverage the design and code of compilers for similar languages, or to simplify the evolution of existing languages to include more features. Rosetta is a generator of relational data language compilers that demonstrates practical solutions to these problems. We explain how domain analysis identifies primitive building blocks of these compilers, and how grammar-based definitions (`a la GenVoca) of the legal compositions of these blocks yields compact and easily-evolvable specifications of data languages. Rosetta automatically transforms such <b>specifications</b> into <b>compilers.</b> Experiences with Rosetta are discussed. 1 Introduction Families o [...] ...|$|R
40|$|A source-to-source {{translator}} is {{a program}} which translates programs written in a given high-level programming language into another high-level language. They provide a reliable means for the re-use, sharing, and development of software. [...] In this thesis, the design and implementation of a source-to-source translator which converts Fortran- 77 programs into semantically 'equivalentâ€™ Modula- 2 programs is described. [...] An attribute grammar is used to formally describe the translation. Attribute grammars are typically used in the <b>specification</b> of <b>compilers</b> and translators, and describe translation in a syntax-directed fashion. [...] The translator was generated from the attribute grammar using the GAG system, a translator/compiler writing system based on attribute grammars. Attributed parse trees are used for the intermediate representation of the syntax and semantics of Fortran programs during translation. [...] Keywords: source-to-source translation; program transformation; attribute grammars; translator-writing systems; the GAG system; programming languages; Fortran- 77; Modula- 2...|$|R
40|$|This text {{presents}} an {{implementation of a}} concurrent attribute evaluator system. This system was developed with the main objective of allowing the implementation of several strategies of concurrent attribute evaluation and not to build the faster compiler to a specific case. The system is implemented in a tightly-coupled machine. One realistic compiler was built and the first results are discussed. 1 Introduction This text refers to parallelism in what concerns to compilation, in particular it refers to parallel compilers obtained from an attribute grammar (AG) <b>specification.</b> Since <b>compilers</b> {{are one of the}} tools programmers use the most, it is obviously relevant to decrease the compilation time. There are several approaches to speedup compilation time: incremental compilation, separated compilation of program modules and parallel compilation. The first decreases the compilation time since it compiles, only the parts of the source text, that have been changed since the last compilation [...] ...|$|R
40|$|In this paper, {{we present}} an {{object-oriented}} approach to <b>compiler</b> <b>specification.</b> Our method treats grammar symbols as templates which instantiate nodes of parse tree for object-oriented semantic analysis. To have better classification and inheritance for semantic description, it uses restricted CFG to specify grammar ules. The semantic specification is done {{based on a}} class hierarchy generated from the restricted CFG. Besides, that a path expression is booted to describe the possible receivers of a message releases the restriction on the specification of message (value) propagations in attribute grammar methods...|$|R
40|$|Preliminary {{considerations}} for a C++ animator design for SDL 92 {{are presented as}} the starting point for the corresponding project. The design involves compiling SDL 92 into C++ code. C++ has been chosen as the target code in a hope to capture the object oriented features of SDL 92 in a straightforward manner. The paper emphasises this point just after the top level considerations. Keywords : Communication Software, <b>Specification</b> Languages, <b>Compilers,</b> Object Orientation 1 Introduction SDL (Specification and Description Language) is a formal specification language which is a joint CCITT-ISO standard [1, 2]. Originally conceived for telephony network specification in early 1970 ies it was later extended to incorporate data communication systems. Recently the SDL 88 version was updated and a new standard called SDL 92 was issued. SDL 92 incorporates new features that gives it a strong object orientation flavor and for that reason hopes have been invested in it as a productive CASE tool t [...] ...|$|R
40|$|Colloque avec actes et comitÃ© de lecture. Modern {{technologies}} including {{rapid increase}} in speed and availability of network of supercomputer provides the infrastructure necessary to develop distributed applications and making high performance global computing possible. The performance potential offered is enormous but generally is hard and tedious to obtain due {{to the complexity of}} developing applications in such distributed environment. In this paper, we introduce which is a specification language of communications for parallel applications. Its aim is to provide to the user an abstract vision of his application in terms of tasks and exchanges between these tasks independently of material architecture and the means of communication used. Based on this <b>specification,</b> the <b>compiler</b> generates several levels of implementation for various communication primitives. More precisely, we will detail in this article, a new approach of this language allowing the implementation of the communications based on ideas underlying the Common Object Request Broker Architecture CORBA...|$|R
40|$|The {{secure and}} robust {{functioning}} {{of a network}} relies on the defect-free implementation of network applications. As network protocols have become increasingly complex, however, hand-writing network message processing code has become increasingly error-prone. In this paper, we present a domain-specific language, Zebu, for describing protocol message formats and related processing constraints. From a Zebu <b>specification,</b> a <b>compiler</b> automatically generates stubs {{to be used by}} an application to parse network messages. Zebu is easy to use, as it builds on notations used in RFCs to describe protocol grammars. Zebu is also efficient, as the memory usage is tailored to application needs and message fragments can be specified to be processed on demand. Finally, Zebu-based applications are robust, as the Zebu <b>compiler</b> automatically checks <b>specification</b> consistency and generates parsing stubs that include validation of the message structure. Using a mutation analysis in the context of SIP and RTSP, we show that Zebu significantly improves application robustness...|$|R
40|$|Thesis (M. Sc.) [...] Memorial University of Newfoundland, 1990. Computer ScienceBibliography: leaves 124 - 127. A source-to-source {{translator}} is {{a program}} which translates programs written in a given high-level programming language into another high-level language. They provide a reliable means for the re-use, sharing, and development of software. [...] In this thesis, the design and implementation of a source-to-source translator which converts Fortran- 77 programs into semantically 'equivalentâ€™ Modula- 2 programs is described. - An attribute grammar is used to formally describe the translation. Attribute grammars are typically used in the <b>specification</b> of <b>compilers</b> and translators, and describe translation in a syntax-directed fashion. [...] The translator was generated from the attribute grammar using the GAG system, a translator/compiler writing system based on attribute grammars. Attributed parse trees are used for the intermediate representation of the syntax and semantics of Fortran programs during translation. [...] Keywords: source-to-source translation; program transformation, attribute grammars, translator-writing systems,the GAG system, programming languages, Fortran- 77, Modula- 2...|$|R
40|$|The NIST EXPRESS toolkit is a {{software}} library for building EXPRESS-related tools. The toolkit was previously released in 1991, based on ISO TC 184 /SC 4 N 14 (familiarly called "EXPRESS N 14 "). The current release {{is based on}} Draft International Standard (DIS) 10303 - 11 (N 151) and while the philosophical underpinnings are similar, much of the interface has changed significantly. This paper describes changes that {{must be made to}} existing applications so that they can work with the new toolkit. This paper should be read by anyone maintaining software based upon the NIST EXPRESS toolkit. This paper will also provide insight to people interested in the internals of EXPRESS implementations and some of the ways they have changed over time due to experience and the different EXPRESS <b>specifications.</b> Keywords: <b>compiler,</b> EXPRESS; implementation; National PDES Testbed; PDES; STEP Context The PDES (Product Data Exchange using STEP) activity is the United States' effort in support of the Standard for [...] ...|$|R
40|$|Abstract â€” This paper {{presents}} the fundamental concepts of model-based design {{to the broader}} software engineering community. We examine model-based design {{from the perspective of}} domain-specific modeling languages (DSMLs). DSMLs capture the structure, behavioral characteristics, and abstractions of complex problem domains. Model transformations defined between language syntaxes serve as high-level <b>specifications</b> of domain-specific <b>compilers.</b> Additionally, transformations are used to change abstraction levels. This paper is example driven and includes examples from a number of tools including ASML [1], Ptolemy II [2], GME [3], and GReAT [4]. Index Terms â€” model-based design, domain-specific modeling languages, structural semantics, model of computation, model transformations I...|$|R
40|$|AbstractThe use of {{object-oriented}} {{techniques and}} concepts, like encapsulation and inheritance, greatly improves language specifications towards better modularity, reusability and extensibility. Additional improvements {{can be achieved}} with aspect-oriented techniques since semantic aspects also crosscut many language constructs. Indeed, aspect-oriented constructs have been already added to some language <b>specifications.</b> The LISA <b>compiler</b> construction system follows an object-oriented approach and has already implemented mechanisms for inheritance, modularity and extensibility. Adding aspects to LISA {{will lead to more}} reusable language specifications. In the paper, aspect-oriented attribute grammars are introduced, and the underlying ideas are incorporated into AspectLISA, an aspect-oriented compiler generator based on attribute grammars...|$|R
