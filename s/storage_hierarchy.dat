145|48|Public
5000|$|... #Subtitle level 3: Memory and <b>storage</b> <b>hierarchy,</b> storage technology, CD-ROM, firmware, and micro-programming ...|$|E
5000|$|The HP TRIM Space Management module {{defined a}} <b>storage</b> <b>hierarchy</b> model to {{organize}} content ...|$|E
50|$|Implementation of that {{principle}} {{required that the}} addressing mechanism {{at the heart of}} the machine would incorporate a complete <b>storage</b> <b>hierarchy</b> management system and major portions of a data base management system, that until then were implemented as add-on software.|$|E
50|$|Other data {{injectors}} include policy-based hierarchical {{storage management}} (HSM) components for AIX, Linux and Windows. These allow migration of data from production disk into {{one or more of}} the TSM <b>storage</b> <b>hierarchies</b> while maintaining transparent access to that data by the use of DMAPI or NTFS reparse points.|$|R
40|$|In this paper, we give an {{overview}} of the HDF 5 technology suite and some of its applications. We discuss the HDF 5 data model, the HDF 5 software architecture and some of its performance enhancing capabilities. Categories and Subject Descriptors D. 4. 2 [Operating Systems]: Storage management- secondary <b>storage,</b> <b>storage</b> <b>hierarchies.</b> D. 4. 3 [Operating Systems]: File System Management- access methods, file organizatio...|$|R
40|$|Abstract—Most {{existing}} {{studies of}} file access prediction are experimental {{in nature and}} rely on trace driven simulation to predict {{the performance of the}} schemes being investigated. We present a first order Markov analysis of file access prediction, discuss its limitations and show how {{it can be used to}} estimate the performance of file access predictors, such as First Successor, Last Successor, Stable Successor and Best-k-out-of-n. We compare these analytical results with experimental measurements performed on several file traces and find out that specific workloads, and indeed individual files, can exhibit very different levels of nonstationarity. Overall, at least 60 percent of access requests appear to remain stable over at least a month. Index Terms—File access prediction, <b>storage</b> <b>hierarchies,</b> <b>storage</b> management. ...|$|R
50|$|Storage {{technologies}} at {{all levels}} of the <b>storage</b> <b>hierarchy</b> can be differentiated by evaluating certain core characteristics as well as measuring characteristics specific to a particular implementation. These core characteristics are volatility, mutability, accessibility, and addressability. For any particular implementation of any storage technology, the characteristics worth measuring are capacity and performance.|$|E
50|$|Objectivity/DB {{provides}} a flexible approach for defining how objects are placed {{within a given}} <b>storage</b> <b>hierarchy.</b> Database designers can define a custom placement strategy that is encapsulated in an XML configuration file and {{made available to the}} application. This strategy can define which persistent objects are stored together, which are distributed, and which are stored near designated objects.|$|E
5000|$|Transactional (OLTP) workloads on Exadata {{benefit from}} the {{incorporation}} of flash memory into Exadata’s <b>storage</b> <b>hierarchy,</b> and the automatic [...] "tiering" [...] of data into memory, flash or disk storage. Special flash algorithms optimize flash for response time sensitive database workloads such as log writes. For high-end OLTP, all-flash storage eliminates the latency of disk media completely.|$|E
40|$|Optimization {{is often}} a {{question}} of where one should put one’s money in improving performance. As far as large <b>storage</b> <b>hierarchies</b> go, intuition suggests (and common practice supports) adding as much as is affordable of the fastest technology available. Many cache hierarchy {{studies have shown that}} this is often not the optimal approach, and we show that for mass <b>storage</b> <b>hierarchies</b> it always tends to be the wrong approach. For large data sets, as is the case for network file servers, a machine with no RAM and several gigabytes of disk performs 30 % faster than a machine with no disk and a cost-equivalent amount of RAM. This paper presents a mathematical analysis of the optimization of an I/O hierarchy, as well as trace driven simulations of a network file server in support of the analysis. Over the past several years, there has been a substantial increase in the speed and capacity demands placed on computer memory systems. Great strides {{have been made in the}} capacity of mass storage devices such as magnetic disk, tape, and optical media, but improvements in the speed of these devices has not kept up with the improvements in CPU speeds and semiconductor memory. Caching is used to hide the deficiency, but the widening gap between semiconductor memory an...|$|R
40|$|Abstract: The {{applicability}} of stack processing {{for evaluation of}} <b>storage</b> <b>hierarchies</b> has been limited to two-level systems and to a very special group of multilevel hierarchies. A generalization of stack processing, called joint stack processing, is introduced. This technique makes possible the efficient determination of hit ratios for a class of multilevel hierarchies- staging hierarchies. These hierarchies are rather realistic {{in the sense that}} they allow for multiple block sizes and multiple copies of data in the <b>hierarchy.</b> Properties of <b>storage</b> management schemes that lend themselves to ioint stack processing are studied, and the notion of distributed hierarchy management is described and illustrated...|$|R
40|$|In {{spite of}} the {{dramatic}} improvements {{in the cost of}} secondary storage, magnetic disks alone are inadequate for meeting the storage and delivery needs of many modern applications. Novel designs of <b>storage</b> <b>hierarchies</b> are needed if we wish {{to take advantage of the}} low cost of tape media but still provide reasonable performance characteristics, in the context of modern applications. Specifically, we are interested in multimedia storage server applications, which in addition to high storage and high bandwidth requirements must support display of continuous data streams (e. g., video) and thus satisfy real time constraints. In this paper we focus on the issues and tradeoffs involved in the design of multimedia tertiary storage systems that store and retrieve heterogeneous multimedia objects...|$|R
50|$|A database, {{while in}} operation, resides {{simultaneously}} in {{several types of}} storage, forming a <b>storage</b> <b>hierarchy.</b> By the nature of contemporary computers most of the database part inside a computer that hosts the DBMS resides (partially replicated) in volatile storage. Data (pieces of the database) that are being processed/manipulated reside inside a processor, possibly in processor's caches. These data are being read from/written to memory, typically through a computer bus (so far typically volatile storage components). Computer memory is communicating data (transferred to/from) external storage, typically through standard storage interfaces or networks (e.g., fibre channel, iSCSI). A storage array, a common external storage unit, typically has <b>storage</b> <b>hierarchy</b> of its own, from a fast cache, typically consisting of (volatile and fast) DRAM, which is connected (again via standard interfaces) to drives, possibly with different speeds, like flash drives and magnetic disk drives (non-volatile). The drives may be connected to magnetic tapes, on which typically the least active parts of a large database may reside, or database backup generations.|$|E
50|$|Objectivity/DB uses a {{distributed}} <b>storage</b> <b>hierarchy.</b> Objects {{are stored}} in logical clusters called containers. The containers {{are stored in}} databases that are cataloged in a federated database. Every object has a unique 64-bit Object Identifier (OID) that is a composite logical structure. The physical address space limitation for a single federation is in the millions of Terabytes range. The largest publicized Objectivity/DB installation, at SLAC's BaBar experiment, stored over a Petabyte of objects.|$|E
5000|$|The central {{processing}} unit (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a <b>storage</b> <b>hierarchy,</b> which puts fast but expensive and small storage options close to the CPU and slower but larger and cheaper options farther away. Generally the fast volatile technologies (which lose data when off power) {{are referred to as}} [...] "memory", while slower persistent technologies are referred to as [...] "storage"; however, [...] "memory" [...] is sometimes also used when referring to persistent storage.|$|E
40|$|Multithreaded {{execution}} models {{attempt to}} combine {{some aspects of}} dataflow-like execution with von Neumann model of execution, {{with the objective of}} masking the latency of inter-processor communications and remote memory accesses in multiprocessors. Two important issues in multithreading are the thread execution models, and the synchronization schemes. Depending on the execution model a thread can be either blocking or non-blocking. The synchronization can be done either at the thread or code-block levels. This thesis describes an experimental study of various issues in multithreaded execution. Issues of particular interest are: multithreaded code generation, the multithreaded execution models and the impact of various <b>storage</b> <b>hierarchies.</b> A new code generation scheme for the blocking model of thread execution is presented. The blocking and nonblocking models of execution are compared to study [...] ...|$|R
40|$|Abstract – In {{this chapter}} we examine storage {{management}} {{issues in the}} context of a multimedia information system supporting thousands of users and stored media objects. The design of an efficient MMIS storage server must consider the effects of user access behavior as well as the bandwidth and storage requirements of the stored media objects on system performance. Our presentation addresses these issues by describing a policy for resource allocation and replication yielding efficient operation of the MMIS server. Resulting observations are qualitatively compared against alternative <b>storage</b> <b>hierarchies</b> for large scale multimedia servers constructed in a centralized or distributed fashion. We subsequently examine models for evaluating user access behavior and their application in server design. The chapter concludes with a discussion of the impact of the World Wide Web on the design of future MMIS server architectures...|$|R
40|$|National audienceThis paper {{presents}} {{a set of}} models dedicated to describe a flash storage subsystem structure, functions, performance and power consumption behaviors. These models cover a large range of today's NAND flash memory applications. They {{are designed to be}} implemented in simulation tools allowing to estimate and compare performance and power consumption of I/O requests on flash memory based storage systems. Such tools can also help in designing and validating new flash storage systems and management mechanisms. This work is integrated in a global project aiming to build a framework simulating complex flash <b>storage</b> <b>hierarchies</b> for performance and power consumption analysis. This tool will be highly configurable and modular with various levels of usage complexity according to the required aim: from a software user point of view for simulating storage systems, to a developer point of view for designing, testing and validating new flash storage management systems...|$|R
5000|$|There is a {{three level}} storage hierarchy: (1) real memory, (2) high speed paging devices, and (3) paging disks. High speed paging devices include the IBM 2301 Drum, IBM 2305 Fixed Head File, and various third party [...] "solid-state" [...] I/O {{devices such as}} the STC 4305 and Intel 3805 that {{simulate}} spinning disks or more often provide more efficient fixed block architecture (FBA) access to external RAM based storage. The high speed paging devices are attached using [...] "two-byte" [...] I/O channels operating at up to 3.0 MB per second whenever possible. The paging disks were separate from the disks used for the file system and were used if the higher speed paging devices became full. Virtual memory pages migrate between real memory and the paging devices. In the early versions of MTS pages did not migrate between individual paging devices. In later versions, less frequently used pages would migrate from the high speed paging devices to the paging disks, when the high speed devices were close to being full. Later in its life the system was changed to use IBM S/370-XA Extended Storage {{as part of the}} second level of the <b>storage</b> <b>hierarchy</b> and to use the same disks for the file system and for paging.|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThe Least Privilege Separation Kernel (LPSK) {{is part of}} the Trusted Computing Exemplar (TCX) project. Separation kernels may be used to partition resources in support of the enforcement of mandatory security policies. The LPSK provides services that allow each subject to access resources configured as part of its domain. To ensure permanence of information the LPSK requires a <b>storage</b> <b>hierarchy</b> for its data resources. This thesis describes the design for a LPSK <b>storage</b> <b>hierarchy</b> based on existing LPSK requirements. The design was implemented in a Linux environment to produce a <b>storage</b> <b>hierarchy</b> prototype. Implementation of the prototype proceeded in keeping with principles for developmental security which include minimization, modularity, and hierarchical dependencies. The LPSK <b>storage</b> <b>hierarchy</b> external interfaces belong in three distinct categories: The configuration interfaces are used to construct the <b>storage</b> <b>hierarchy</b> and its contents in a non-LPSK context, initialization interfaces associate data segment handles with data segments that are exported to LPSK subjects, and runtime interfaces support the reading and writing to secondary storage data segments exported to non-LPSK subjects. Testing showed that <b>storage</b> <b>hierarchy</b> interfaces behaved according to specification. This study shows that a <b>storage</b> <b>hierarchy</b> prototype can be designed and implemented based on the LPSK functional specification. Naval Postgraduate School author (civilian...|$|E
40|$|This paper {{examines}} {{the argument for}} dataflow architectures in "Two Fundamental Issues in Multiprocessing[5]. " We observe two key problems. First, the justification of extensive multithreading {{is based on an}} overly simplistic view of the <b>storage</b> <b>hierarchy.</b> Second, the local greedy scheduling policy embodied in dataflow is inadequate in many circumstances. A more realistic model of the <b>storage</b> <b>hierarchy</b> imposes significant constraints on the scheduling of computation and requires a degree of parsimony in the scheduling policy. In particular, it is important to establish a scheduling hierarchy that reflects the underlying <b>storage</b> <b>hierarchy.</b> However, even with this improvement, simple local scheduling policies are unlikely to be adequate. Keywords: dataflow, multiprocessing, multithreading, latency tolerance, <b>storage</b> <b>hierarchy,</b> scheduling hierarchy. 1 Introduction The advantages of dataflow architectures were argued persuasively in a seminal 1983 paper by Arvind and Iannucci[4] and in [...] ...|$|E
40|$|The {{low cost}} per {{megabyte}} of optical disk and {{magnetic tape storage}} make these technologies particularly attractive for use in large capacity storage servers, including multimedia servers. However, these devices have performance problems that range from high costs for many optical drives to low performance and lack of random access in tape drives. We evaluate the performance on multimedia applications of several tertiary storage systems, including optical disk jukeboxes, arrays of optical drives, and <b>storage</b> <b>hierarchies</b> composed of magnetic disk arrays and magnetic tape libraries. We conclude that striped arrays of next-generation write-once optical disks will offer the best performance at acceptable cost. 1 Introduction Tertiary storage devices such as optical disk drives and tape drives are characterized by very low cost per megabyte of storage capacity. As a result, tertiary devices appear particularly appropriate for use in large capacity storage servers, including those that stor [...] ...|$|R
40|$|One of {{the primary}} motivations for {{implementing}} virtual memory {{is its ability to}} automatically manage a <b>hierarchy</b> of <b>storage</b> systems with different characteristics. The composite system behaves {{as if it were a}} single-level system having the more desirable characteristics of each of its constituent levels. In this paper we extend the virtual memory concept to within the top level of a two-level hierarchy. Here, the top level is thought of as containing two additional levels within it. This hierarchy is not a physical one, but rather an artificial one arising from the employment of two different replacement algorithms. Given two replacement algorithms, one of which has good performance but high implementation cost and the other poor performance but low implementation cost, we propose and analyze schemes that result in an overall algorithm having the performance characteristics of the former and the cost characteristics of the latter. We discuss the suitability of such schemes in the management of <b>storage</b> <b>hierarchies</b> that lack page reference bits...|$|R
40|$|In this paper, we {{evaluate}} three storage {{systems for}} movieson -demand video servers. We show that striped disk farms, in which portions of movies are interleaved among disks, perform best. They achieve close to full disk utilization, good load balancing, {{and the lowest}} cost per video stream. Disk farms that store one movie per disk have lower performance, since disks holding less popular movies are under-utilized; however, these systems achieve relatively good performance by replicating movies according to their popularity in the request pattern. Finally, we evaluate the use of <b>storage</b> <b>hierarchies</b> to support movies-on-demand video service. We show that their performance is so poor that their effective cost per video stream is two orders of magnitude {{higher than that of}} magnetic disk systems. We conclude that for the foreseeable future, most video servers will store movies entirely on disk. INTRODUCTION This paper explores storage system alternatives for movieson -demand video servers [...] ...|$|R
40|$|Abstract: This paper {{examines}} {{the argument for}} dataflow architectures in “Two Fundamental Issues in Multiprocessing[5]. ” We observe two key problems. First, the justification of extensive multithreading {{is based on an}} overly simplistic view of the <b>storage</b> <b>hierarchy.</b> Second, the local greedy scheduling policy embodied in dataflow is inadequate in many circumstances. A more realistic model of the <b>storage</b> <b>hierarchy</b> imposes significant constraints on the scheduling of computation and requires a degree of parsimony in the scheduling policy. In particular, it is important to establish a scheduling hierarchy that reflects the underlying <b>storage</b> <b>hierarchy.</b> However, even with this improvement, simple local scheduling policies are unlikely to be adequate...|$|E
40|$|Abstract Robotic {{storage devices}} offer huge storage {{capacity}} at alow cost per byte, but with large access times. Integrating these devices into the <b>storage</b> <b>hierarchy</b> presents a chal-lenge to file system designers. Log-structured file systems (LFSs) {{were developed to}} reduce latencies involved in ac-cessing disk devices, but their sequential write patterns match well with tertiary storage characteristics. Unfortu-nately, existing versions only manage memory caches and disks, and do not support a broader <b>storage</b> <b>hierarchy.</b> HighLight extends 4. 4 BSD LFS to incorporate both sec-ondary storage devices (disks) and tertiary storage device...|$|E
40|$|This paper {{describes}} how federated and object/relational database systems can exploit cost-effective active storage hierarchies. By active <b>storage</b> <b>hierarchy</b> we mean a database system that uses all storage media (i. e. optical, tape, and disk) to store and retrieve data {{and not just}} disk. A detailed discussion of the Atomic Data Store data warehouse concept {{can be found in}} [CB 99]. These also describe a commercial relational database product, StorHouse/Relational Manager (RM), that executes SQL queries directly against data stored in a complete <b>storage</b> <b>hierarchy.</b> This paper focuses on applications that can use, and may even require the use of, emerging federated and object/relational database technologies. Our analysis is based on two products now in development. We will refer to these as StorHouse/Fed (a federated database system that includes StorHouse/RM) and StorHouse/ORM (an Object-Relational database system). We conclude by describing candidate applications (with an emphasis on the federal sector) that can exploit the combination of costeffective active <b>storage</b> <b>hierarchy</b> with federated and/or object/relational database technology...|$|E
40|$|This paper {{considers}} issues {{relevant to}} Operating System control of systems with compressed main memory. The notion of "allocated but unused storage" is introduced. This represents storage {{which has been}} recently allocated by the Operating System, but which does not yet occupy physical memory because the lines {{have not yet been}} cast out of the cache. We propose that control policies incorporate estimates of allocated but unused storage, as such storage represents a major way in which compressibility can dramatically change. The paper addresses the estimation of allocated but unused storage and analyzes its accuracy on system traces. 1 Introduction There is increasing interest in incorporating what might be termed a compression-store in <b>storage</b> <b>hierarchies</b> for server-class computing systems. Here the contents of main memory are are held in compressed form, and decompressed when loaded into a cache upon cache faults [2, 3], with this process largely transparent to the software. As pr [...] ...|$|R
40|$|The ever growing {{needs of}} large {{multimedia}} systems cannot {{be met by}} magnetic disks due to their high cost and low storage density. Consequently, cheaper and denser tertiary storage systems are being integrated into the <b>storage</b> <b>hierarchies</b> of these applications. Although tertiary storage is cheaper, the access latency is very high due {{to the need to}} load and unload media on the drives. This high latency and the bursty nature of I/O traffic result in the accumulation of I/O requests for tertiary storage. We study the problem of scheduling these requests to improve performance. In particular we address the issues of scheduling across multiple tapes or disks as opposed to most other studies which consider only one or two media. We focus on algorithms that minimize the number of switches and show through simulation that these result in near-optimal schedules. For single drive libraries an efficient algorithm that produces optimal schedules is developed. For multiple drives the problem is sh [...] ...|$|R
40|$|The {{large storage}} {{requirements}} of many commerical and scientific applications cannot {{be met by}} magnetic disks due to their high cost and low storage density. Consequently, cheaper and more dense tertiary storage systems are being intergrated into the <b>storage</b> <b>hierarchies</b> of these applications. Although tertiary storage can accomodate large amounts of data, the access latency is very high due {{to the need to}} load and unload media from the read/write drives. These media exchanges are very slow with typical times in the order of tens of seconds. In order to reduce this high latency research efforts have focussed on minimizing media exchanges under the assumption that it is always beneficial to eliminate exchanges. We analyze the validity of this assumption. It is shown that there are instances when the assumption does not hold. It is further shown that various factors influence the effect of an exchange, making it difficult to establish simple heuristics for scheduling media exchan [...] ...|$|R
40|$|We {{present an}} {{allocation}} algorithm for small L 0 compiler-managed instruction stores (cmiss) that significantly reduces the energy {{consumed by the}} instruction <b>storage</b> <b>hierarchy.</b> With our algorithm, cmiss simultaneously achieve low access energy, low performance overhead, and high filter rate. Despite the lack of associativity in cmiss, our algorithm achieves filter rates {{similar to those of}} filter caches by pinning— allocating frequently executed instructions to exclusive locations. An evaluation of our algorithm on 17 embedded applications shows that the energy consumed by the 1 instruction <b>storage</b> <b>hierarchy</b> is reduced by 84 %, with a performance overhead of 2 %. ...|$|E
40|$|This paper {{describes}} how database systems can use and exploit a cost-effective active <b>storage</b> <b>hierarchy.</b> By active <b>storage</b> <b>hierarchy</b> we mean a database system that uses all storage media (i. e. optical, tape, and disk) to store and retrieve data {{and not just}} disk. We describe and emphasize the active part, whereby all storage types are used to store raw data that is converted to strategic business information. We describe an evolution to the Data Warehouse concept, called Atomic Data Store, whereby atomic data is stored in the database system. Atomic data is defined as storing all the historic data values and executing queries against the historic queries. We also describe a Data Warehouse information collection, flow and central data store Hub-and-Spoke architecture, used to feed data into Data Marts. We also describe a commercial product; StorHouse/Relational Manager (RM). RM is a commercial relational database system that executes SQL queries directly against data stored on the <b>storage</b> <b>hierarchy</b> (i. e. tape, optical, disk). We conclude with {{a brief overview of}} a real world AT&T Call Detail Warehouse (CDW) case study...|$|E
40|$|Cost {{effective}} terabyte memory {{systems are}} now becoming possible. New methods for high capacity storage {{systems have been}} made possible by low cost, small formfactor magnetic and optical tape systems. To achieve low latency and high bandwidth access to the storage system, we must interleave data transfer {{at all levels of}} the storage system, including devices, controllers, servers, and communica- tions links. Latency can be reduced by extensive caching throughout the <b>storage</b> <b>hierarchy.</b> In addition, we tut provide effective management of a <b>storage</b> <b>hierarchy,</b> extending the techniques already developed by Ousterhout for his Log Structured File System. We are incorporating these ideas into a protototype high capacity file server...|$|E
40|$|Storage {{infrastructure}} in large-scale cloud data center environments must support applications with diverse, time-varying data access patterns while observing {{the quality of}} service. Deeper <b>storage</b> <b>hierarchies</b> induced by solid state and rotating media are enabling new storage management tradeoffs that do not apply uniformly to all application phases at all times. To meet service level requirements in such heterogeneous application phases, storage management needs to be phase-aware and adaptive, i. e., to identify specific storage access patterns of applications as they occur and customize their handling accordingly. This paper presents LoadIQ, a novel, versatile, adaptive, application phase detector for networked (file and block) storage systems. In a live deployment, LoadIQ analyzes traces and emits phase labels learnt on the fly by using Support Vector Machines(SVM), {{a state of the}} art classifier. Such labels could be used to generate alerts or to trigger phase-specific system tuning. Our results show that LoadIQ is able to identify workload phases (such as in TPC-DS) with accuracy> 93 %. ...|$|R
40|$|Abstract: A simple {{model of}} the <b>storage</b> <b>hierarchies</b> is {{formulated}} with the assumptions {{that the effect of}} the storage management strategy is characterized by the hit ratio fqnction. The hit ratio function and the device technology-cost function are assumed to be representable by power functions (or piece-wise power functions). The optimization of this model is a geometric programming problem. An explicit formula for the minimum hierarchy access time is derived; the cqpacity and technology of each storage level are determined. The opfimal number of storage levels in a hierarchy is shown to be directly proportional to the logarithm of the systems capacity with the constant of propottionality dependent upon the technolagy and hit ratio characteristics. The optimal cost ratio of adjacent storage levels is constant, as are the ratios of the device access'times and storage capacities of the adjacent levels. An illustration of the effect of overhead cost and level-dependent cost, such as the cost per &quot;box &quot; and coq for managing. memory. faults is given and several generalizations are presented...|$|R
40|$|The {{performance}} of streaming media servers {{has been limited}} by the dual requirements of high disk throughput (to service more clients simultaneously) and low memory use (to decrease system cost). To achieve high disk throughput, disk drives must be accessed with large IOs to amortize disk access overhead. Large IOs imply an increased requirement of expensive DRAM, and, consequently, greater overall system cost. MEMS-based storage, an emerging storage technology, is predicted to offer a price-performance point between those of DRAM and disk drives. In this study, we propose storage architectures that use the relatively inexpensive MEMS-based storage devices as an intermediate layer (between DRAM and disk drives) for temporarily staging large disk IOs at a significantly lower cost. We present data layout mechanisms and synchronized IO scheduling algorithms for the real-time storage and retrieval of streaming data within such an augmented storage system. Analytical evaluation suggests that MEMS-augmented <b>storage</b> <b>hierarchies</b> can reduce the cost and improve the throughput of streaming servers significantly. Categories and Subject Descriptors: C. 0. a [Computer System Organization]: General—Syste...|$|R
