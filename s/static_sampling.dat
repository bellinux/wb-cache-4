45|263|Public
40|$|Accurate traffic {{characterization}} by packet {{source is}} needed to predict network behavior and to properly allocate network resources to achieve a desired Quality of Service for all network users. As networks have become faster, the processing load required for complete packet sampling has also grown. In some cases, for example Gigabit Ethernet, the network can deliver packets faster than a network management subsystem can process them. In order to prevent inaccurate traffic statistics due to "clipping" of traffic peaks, Claffy et al. applied several <b>static</b> <b>sampling</b> strategies to network traffic characterization. As shown in this paper, <b>static</b> <b>sampling</b> may produce inaccurate traffic statistics. In this paper, adaptive sampling methods are developed and evaluated to address inaccuracies of <b>static</b> <b>sampling.</b> In addition, the estimation of the Hurst parameter, a measure of traffic self-similarity, is studied for static and adaptive sampling. It is shown that adaptive sampling results in a mo [...] ...|$|E
40|$|Abstract—Spatial {{sampling}} {{is traditionally}} studied in a static setting where static sensors scattered around space take {{measurements of the}} spatial field at their locations. In this paper we study the emerging paradigm of sampling and reconstructing spatial fields using sensors that move through space. We show that mobile sensing offers some unique advantages over static sensing in sensing bandlimited spatial fields. Since a moving sensor encounters such a spatial field along its path as a time-domain signal, a time-domain anti-aliasing filter can be employed prior to sampling the signal received at the sensor. Such a filtering procedure, when used by a configuration of sensors moving at constant speeds along equispaced parallel lines, leads to a complete suppression of spatial aliasing {{in the direction of}} motion of the sensors. We analytically quantify the advantage of using such a sampling scheme over a <b>static</b> <b>sampling</b> scheme by computing the reduction in sampling noise due to the filter. We also analyze the effects of non-uniform sensor speeds on the reconstruction accuracy. Using simulation examples we demonstrate the advantages of mobile sampling over <b>static</b> <b>sampling</b> in practical problems. We extend our analysis to sampling and reconstruction schemes for monitoring time-varying bandlimited fields using mobile sensors. We demonstrate that in some situations we require a lower density of sensors when using a mobile sensing scheme instead of the conventional static sensing scheme. The exact advantage is quantified for a problem of sampling and reconstructing an audio field. I...|$|E
40|$|International audienceThis paper {{studies the}} skip, under some assumptions, of process control operations. The case of one tool, one {{enhanced}} buffer and one metrology tool of a monotonic parameter is analyzed. The paper presents {{circumstances in which}} control plan can be optimized due to buffer behavior's. After presenting the industrial issue of defectivity, the article goes through literature review. The article follows by presenting the model and steps toward industrial development. A demonstrator is then presented applied at {{a case study of}} defectivity sampling. A test over a 300 mm wafer-fab data set shows serious improvements: around 35 % of defectivity controls have been allowed to be skipped compared to the <b>static</b> <b>sampling</b> plan...|$|E
40|$|Personal and <b>static</b> <b>sample</b> {{measurements}} are related The paper from Harrison {{and his co-workers}} 1 and the subsequent correspondence 2 3 has reignited a debate about the relation between personal and <b>static</b> <b>sample</b> measurements that started more than 40 years ago. In 1957 the personal sampling pump had just been invented by Jerry Sherwood and Don Greenhalgh from the UK Atomic Energy Authority. 4 They compared their new perso-nal sampler with the conventional <b>static</b> <b>sampler</b> and showed that personal exposures were generally higher than those made at a fixed location. This classic paper has recentl...|$|R
25|$|This {{is mostly}} {{achieved}} by imaging a sufficiently <b>static</b> <b>sample</b> multiple times and either modifying the excitation light or observing stochastic {{changes in the}} image.|$|R
40|$|The use of {{a direct}} {{electron}} detector for the simple acquisition of 2 D electron backscatter diffraction (EBSD) maps and 3 D EBSD datasets with a <b>static</b> <b>sample</b> geometry has been demonstrated in a focused ion beam scanning electron microscope. The small size and flexible connection of the Medipix direct electron detector enabled the mounting of sample and detector on the same stage at the short working distance required for the FIB. Comparison of 3 D EBSD datasets acquired by this means and with conventional phosphor based EBSD detectors requiring sample movement showed that the former method with a <b>static</b> <b>sample</b> gave improved slice registration. However, for this sample detector configuration, significant heating by the detector caused sample drift. This drift and ion beam reheating both necessitated the use of fiducial marks to maintain stability during data acquisition...|$|R
40|$|Volume Rendering of {{time-varying}} datasets {{is essential}} in several scientific applications. Due to the enormous amount of data involved, in datasets with <b>static</b> <b>sampling</b> regions {{it is common to}} consider only time-varying scalar fields (TVSFs). The use of Vector Quantization (VQ) to compress scalar fields {{has been shown to be}} quite effective when combined with texture-based volume rendering algorithms for structured grids. In this paper we discuss how to apply VQ to volume render unstructured grids (meshes of tetrahedra). We extended two of the fastest unstructured grid algorithms (both use programmable GPUs) to handle time-varying scalar fields, discuss advantages and disadavantages of each extension, and show results that allows us to interactively render meshe...|$|E
40|$|International audienceThis article {{studies the}} skip, under some assumptions, of process control operations. The case of one tool, one {{enhanced}} buffer and one metrology tool of a monotonic parameter is analysed. This article presents {{circumstances in which}} control plan can be optimised due to the buffer's behaviour. After discussing the industrial issue of defectivity, this article presents a literature review followed by the model and steps towards industrial development. Then demonstrator, which is applied at {{a case study of}} defectivity sampling, is presented. A test of over a 300 -mm wafer fabrication data set shows serious improvements - around 35 % of defectivity controls have been skipped compared to the <b>static</b> <b>sampling</b> plan...|$|E
40|$|We {{present an}} {{algorithm}} to extract an high-quality approximation of the (top-k) Frequent itemsets (FIs) from random samples of a transactional dataset. With high probability the approximation is a superset of the FIs, and no itemset with frequency {{much lower than}} the threshold is included in it. The algorithm employs progressive sampling, with a stopping condition based on bounds to the empirical Rademacher average, a key concept from statistical learning theory. The computation of the bounds uses characteristic quantities that can be obtained efficiently with a single scan of the sample. Therefore, evaluating the stopping condition is fast, and does not require an expensive mining of each sample. Our experimental evaluation confirms the practicality of our approach on real datasets, outperforming approaches based on one-shot <b>static</b> <b>sampling.</b> ...|$|E
40|$|The article {{deals with}} {{algorithmic}} and software, {{which have been}} developed for research of communications of parameters, registered in systems of various nature. Distinctive feature of a realized way of research is reception of the decision of tasks of multidimensional statistics not on <b>static</b> <b>sample,</b> but on set consecutive data that corresponds to dynamic character of real systems...|$|R
50|$|There {{are several}} direct-reading {{instruments}} for measuring aerosol particle emissions. The condensation particle counter and differential mobility particle sizers, including the scanning mobility particle sizer and fast mobility particle sizer, can measure aerosol concentration; the diffusion charger and electric low pressure impactor can measure surface area; the size selective <b>static</b> <b>sampler</b> and tapered element oscillating microbalance can measure mass.|$|R
5000|$|Late R&B singer <b>Static</b> Major <b>sampled</b> {{the chorus}} on his song [...] "Infatuated".|$|R
40|$|Twelve garages {{representing}} the local motor vehicle repair and servicing industry were surveyed {{to assess the}} asbestos exposure of workers {{as a basis for}} determining health protection strategy. Chryso-tile fibres arising from brake dust or machining asbestos containing friction material were present in air samples from all workplaces. With maximum levels in personal sampling averaging 013 fibre/ml and <b>static</b> <b>sampling</b> averaging 005 fibre/ml exposure appeared to be well within the accepted hygiene standards for occupational expo-sure. Although dust concentrations were low and the hazard of pulmonary fibrosis negligible, uncertainty about the relationship between fibre levels and carcinogenic risk indicated the need to alert local industry to possible health hazard and preventive measures by publication of an appropriate code of practice...|$|E
40|$|Figure 1 : Different time {{steps of}} the SPX dataset. Volume Rendering of {{time-varying}} datasets is essential in several scientific applications. Due to the enormous amount of data involved, in datasets with <b>static</b> <b>sampling</b> regions {{it is common to}} consider only time-varying scalar fields (TVSFs). The use of Vector Quantization (VQ) to compress scalar fields {{has been shown to be}} quite effective when combined with texture-based volume rendering algorithms for structured grids. In this paper we discuss how to apply VQ to volume render unstructured grids (meshes of tetrahedra). We extended two of the fastest unstructured grid algorithms (both use programmable GPUs) to handle time-varying scalar fields, discuss advantages and disadavantages of each extension, and show results that allows us to interactively render meshes composed of nearly one million tetrahedra and several hundred time instances...|$|E
40|$|Adaptive {{sampling}} [a 1] is a probabilistic algorithm {{invented by}} M. Wegman (unpublished) around 1980. It provides an unbiased estimator {{of the number}} of distinct elements (the &quot;cardinality &quot;) of a file (a sequence of data items) of potentially large size that contains unpredictable replications. The algorithm is useful in data-base query optimization and in information retrieval. By standard hashing techniques [a 3], [a 6] the problem reduces to the following. A sequence of real numbers is given. The sequence has been formed by drawing independently and randomly an unknown number of real numbers from, after which the elements are replicated and permuted in some unknown fashion. The problem is to estimate the cardinality in a computationally efficient manner. Three algorithms can perform this task. 1) Straight scan computes incrementally the sets, where replications are eliminated on the fly. (This can be achieved by keeping the successive in sorted order.) The cardinality is then determined exactly by but the auxiliary memory needed is, which may be as large as, resulting in a complexity that is prohibitive in many applications. 2) <b>Static</b> <b>sampling</b> is based on a fixed sampling ratio, where (e. g.,). One computes sequentially the samples. The cardinality estimate returned is. The estimator is unbiased and the memory used is on average. 3) Adaptive sampling is based on a design parameter (e. g.,) and it maintains a dynamically changing sampling rate and a sequence of samples. Initially, and. The rule is like that of <b>static</b> <b>sampling,</b> but with divided by each time the cardinality of would exceed and with modified accordingly in order to contain only. The estimator (where the final value of is used) is proved to be unbiased and the memory used is at most. The accuracy of any such unbiased estimator of is measured by the standard deviation of divided by. For adaptive sampling, the accuracy is almost constant as a function of and asymptotically close t...|$|E
40|$|A novel NMR {{spectrometer}} {{is described}} that is uniquely versatile {{in its ability}} to accurately record broad lines by sweeping the superconducting magnetic field and to perform standard high resolution solid state NMR experiments. Broadline observation is illustrated by Al- 27 spectra from <b>static</b> <b>samples.</b> Such an instrument opens up many nuclei for serious study by NMR in the solid state for the first time. (C) 1998 Elsevier Science B. V. All rights reserved...|$|R
40|$|A {{relationship}} between the dipolar and the chemical-shift scaling factors of cyclic radio-frequency irradiation schemes is introduced. This scaling factor theorem is derived analytically using Average Hamiltonian Theory, and its validity is illustrated numerically with homonuclear dipolar decoupling sequences generated randomly, and with the analysis of existing sequences. While derived for a <b>static</b> <b>sample,</b> the theorem provides insight into homonuclear dipolar decoupling schemes that combine radio-frequency irradiation with fast rotation of the sample at the magic-angle {{with respect to the}} static magnetic fiel...|$|R
40|$|A {{scheme of}} {{adiabatic}} cross-polarization is described. It {{is based on}} demagnetization - remagnetization, when the Zeeman order of abundant nuclei in the laboratory frame is first adiabatically converted into the dipolar order, and then, into the Zeeman order of rare nuclei. The scheme, implemented with two low-power frequency-sweeping pulses, is very efficient for <b>static</b> <b>samples</b> and can significantly increase polarization of rare nuclei, compared to the conventional Hartmann-Hahn cross-polarization. The experimental examples are presented for solids, liquid crystal, and molecules in a liquid-crystalline solvent. Comment: 16 pages including 6 figure...|$|R
40|$|ABSTRACT Network {{operators}} need {{to determine}} {{the composition of the}} traffic mix on links when looking for dominant applications, users, or estimating traffic matrices. Cisco's NetFlow has evolved into a solution that satisfies this need by reporting flow records that summarize a sample of the traffic traversing the link. But sampled NetFlow has shortcomings that hinder the collection and analysis of traffic data. First, during flooding attacks router memory and network bandwidth consumed by flow records can increase beyond what is available; second, selecting the right <b>static</b> <b>sampling</b> rate is difficult because no single rate gives the right tradeoff of memory use versus accuracy for all traffic mixes; third, the heuristics routers use to decide when a flow is reported are a poor match to most applications that work with time bins; finally, it is impossible to estimate without bias the number of active flows for aggregates with non-TCP traffic...|$|E
40|$|Network {{operators}} need {{to determine}} {{the composition of the}} traffic mix on links when looking for dominant applications, users, or estimating traffic matrices. Cisco's NetFlow has evolved into a solution that satisfies this need by reporting flow records that summarize a sample of the traffic traversing the link. But sampled NetFlow has shortcomings that hinder the collection and analysis of traffic data. First, during flooding attacks router memory and network bandwidth consumed by flow records can increase beyond what is available; second, selecting the right <b>static</b> <b>sampling</b> rate is difficult because no single rate gives the right tradeoff of memory use versus accuracy for all traffic mixes; third, the heuristics routers use to decide when a flow is reported are a poor match to most applications that work with time bins; finally, it is impossible to estimate without bias the number of active flows for aggregates with non-TCP traffic. In thi paper we propose [...] ...|$|E
40|$|This work proposes and evaluates a {{sampling}} algorithm based on wavelet transforms with Coiflets basis {{to reduce the}} data sensed in wireless sensor networks applications. The Coiflets basis is more computationally efficient when data are smooth, which means that, data are well approximated by a polynomial function. As expected, this algorithm reduces the data traffic in wireless sensor network and, consequently, decreases the energy consumption and the de-lay to delivery the sensed information. The main contribution of this algorithm is the capability to detect some event by adjusting the sampling dynamically. In order to evaluate the algorithm, we compare it with a <b>static</b> <b>sampling</b> strategy considering a real sens-ing data where an external event is simulated. The results reveal {{the efficiency of the}} proposed method by reducing the data with-out loosing its representativeness, including when some event oc-curs. This algorithm can be very useful to design energy-efficient and time-constrained sensor networks when it is necessary to detect some event...|$|E
40|$|Pentamidine isethionate is {{currently}} {{used for the}} prophylaxis and treatment of Pneumocystis carinii pneumonia. Its use {{has been associated with}} a number of symptoms in staff administering treatment, and there are some additional concerns about possible adverse health effects of long term exposure. The aim {{of this study was to}} quantify exposure of health care staff administering nebulized pentamidine to patients. Personal breathing zone and <b>static</b> air <b>samples</b> at the height of the patient's head were collected during the nebulization of pentamidine to nine sequential out-patients attending a haemophilia unit. These were analysed using a standard method allowing the exposure of staff to be estimated. The duration of treatment varied between 15 and 60 min. Personal breathing zone samples showed exposure to be between 2 and 100 ug/m 3. <b>Static</b> <b>samples</b> showed the concentration of pentamidine in the room varied from 15 to 2, 100 ug/m 3. While these exposures were relatively low, they were higher than some other studies have reported, and may pose some risk of adverse effects to staff. Some simple measures could reduce staff exposure...|$|R
40|$|The {{principal}} {{elements of}} the 199 Hg chemical-shift (CS) tensors of the mercuric halides (HgX 2, X = F, Cl, Br, and I) and the mercurous halides (Hg 2 X 2, X = F and Cl) were determined from spectra of static polycrystalline powders and from magic-angle spinning (MAS) spectra. The CS tensors of both HgCl 2 and Hg 2 Cl 2 are axially symmetric (η = 0) within experimental error, differing from literature reports of η= 0. 12 and η= 0. 14, respectively. The principal {{elements of the}} axially symmetric CS tensor in HgBr 2 were also measured using a <b>static</b> <b>sample,</b> and the wideline spectra of HgF 2 and HgI 2 (red polymorph) give chemical-shift tensors that suggest, within experimental error, that the mercury sits in sites of cubic symmetry. The 199 Hg CS tensor for Hg 2 F 2 is asymmetric. Experiments with <b>static</b> polycrystalline <b>samples</b> may allow {{the determination of the}} elements of the 199 Hg CS tensors even when MAS fails to completely average the dipolar coupling of the spin-½ 199 Hg and the quadrupolar halide nucleus...|$|R
40|$|PuppetCast is a {{protocol}} for secure peer sampling in large-scale distributed systems. A peer sampling protocol continuously provides each node {{in the system}} with a uniform random sample of the node population, and is an important building block for gossip-based protocols for information dissemination, aggregation, load balancing and network management. Existing peer sampling protocols are either very vulnerable to attacks by malicious nodes, do not scale to large systems or provide only a <b>static</b> <b>sample</b> of the population. PuppetCast continues to operate when 50 % (or more) of the nodes are acting maliciously, is shown to scale to systems of significant size and continuously provides new samples. 1...|$|R
40|$|International audienceWe {{present a}} hybrid scheme for the {{parameter}} and state estimation of nonlinear continuous-time systems, which {{is inspired by}} the supervisory setup used for control. State observers are synthesized for some nominal parameter values and a criterion is designed to select one of these observers {{at any given time}} instant, which provides state and parameter estimates. Assuming that a persistency of excitation condition holds, the convergence of the parameter and state estimation errors to zero is ensured up to a margin, which can be made as small as desired by increasing the number of observers. To reduce the potential computational complexity of the scheme, we explain how the sampling of the parameter set can be dynamically updated using a zoom-in procedure. This strategy typically requires a fewer number of observers for a given estimation error margin compared to the <b>static</b> <b>sampling</b> policy. The results are shown to be applicable to linear systems and to a class of nonlinear systems. We illustrate the applicability of the approach by estimating the synaptic gains and the mean membrane potentials of a neural mass model...|$|E
40|$|We {{present a}} hybrid scheme for the {{parameter}} and state estimation of nonlinear continuous-time systems, which {{is inspired by}} the supervisory setup used for control. State observers are synthesized for some nominal parameter values and a criterion is designed to select one of these observers {{at any given time}} instant, which provides state and parameter estimates. Assuming that a persistency of excitation condition holds, the convergence of the parameter and state estimation errors to zero is ensured up to a margin, which can be made as small as desired by increasing the number of observers. To reduce the potential computational complexity of the scheme, we explain how the sampling of the parameter set can be dynamically updated using a zoom-in procedure. This strategy typically requires a fewer number of observers for a given estimation error margin compared to the <b>static</b> <b>sampling</b> policy. The results are shown to be applicable to linear systems and to a class of nonlinear systems. We illustrate the applicability of the approach by estimating the synaptic gains and the mean membrane potentials of a neural mass model. Comment: Submitted to IEEE Transactions of Automatic Contro...|$|E
40|$|Network traffic {{matrices}} {{are important}} for various network planning and management operations. Previous work for estimation of traffic matrices is based on either link load records obtained from SNMP or flow level sampled data available from Net-flow records. Sampled flow-level data provides us good approximations for traffic matrices, however, <b>static</b> <b>sampling</b> rates are not desired. People have proposed sampling solutions with dynamic sampling rates which adapt according to the changing behavior of the network. SNMP link level reports, {{on the other hand}} are computationally less extensive but do not provide optimum solutions for network traffic estimation. This work addresses the problem of measurement in a wider scope and we propose to study merging together data from multiple sources and using it effectively for network monitoring and measurement tasks. We address this problem in both time and spatial domain and intend to develop a framework for dynamically choosing monitoring points in network and blend the existing and readily information from different sources for network measurement. This information mix from multiple sources can be used for more promising and generalized anomaly detection models. We address this problem in three dimensional space of time, space and application level details...|$|E
40|$|During May 2002 {{we carried}} out studies with {{tropical}} phytoplankton assemblages from coastal waters off SE China {{to assess the}} combined effects of solar UV radiation (UVR, 280 to 400 nm) and mixing rates. Water samples were taken daily and incubated using in situ and simulated in situ conditions under 3 radiation regimes (photosynthetically active radiation [PAR] + UVR, PAR + UV-A and PAR only). Variable radiation regimes, to simulate the irradiance field in the upper mixed layer (UML), were obtained by using a device consisting of 1 fixed (<b>static</b> <b>samples)</b> and 1 rotating system (moving samples). Solar UVR inhibited phytoplankton photosynthesis {{in the water column}} (i. e. during in situ experiments), and this inhibition (mean value at surface = 24 %) decreased with depth, so that at 1. 2 m {{there were no significant differences}} between radiation treatments. However, at 1. 8 m depth, the samples receiving UV-A had significantly higher carbon fixation than those receiving only PAR. Simulated in situ experiments showed that solar UVR stimulated phytoplankton photosynthesis under fast mixing conditions (i. e. when the irradiance levels changed from 100 to 6 % and back in less than 30 min). With slower circulation periods, solar UVR reduced carbon fixation and consequently the integrated inhibition within the UML approached the values from the <b>static</b> <b>samples.</b> Previous model predictions based on the interactive effects of UVR and mixing might have underestimated phytoplankton photosynthesis in these regions. Overall, our results suggest a high resistance of these Coastal tropical assemblages to solar UVR as compared to other regions of the planet...|$|R
40|$|There has been, {{in recent}} years, a great {{interest}} in twisted orientation patterns in <b>static</b> <b>samples</b> of nematic liquid crystals (NLC). Here we investigate the behaviour of such structures when the limiting surfaces induce not planar alignment but homogeneously tilted alignment. It is shown that when the overall twist is equal to Π, there exists a critical value of the tilt angle at the surface where the expected twisted structure spontaneously transforms into a non-twisted orientation pattern. This transformation presents all the features of a phase transition, which can be either of first or of second order depending on the set of elastic constants of the material. Experimental confirmation has been obtained with MBBA...|$|R
5000|$|Writing for AllMusic, William York {{comments}} positively, “Fifth album, Perdition City is {{an album}} of moody, atmospheric electronica, built up around basic down-tempo beats and noir-ish electronic piano harmonies, and then fleshed out with various blips and bleeps, <b>static</b> noises, <b>samples,</b> and occasional vocals.” ...|$|R
40|$|International audienceIn a {{globally}} competitive environment, sustaining {{high yield}} {{with a minimum}} number of quality controls is key for manufacturing plants to remain competitive. In modern semiconductor manufacturing facilities, with the moves to ever smaller geometries and the variety among products to be run concurrently, designing efficient control plans is becoming increasingly complex. Since a 100 % of inspection is neither feasible nor interesting {{because of the cost}} and reliability of each control, dynamically identifying the right product to inspect is {{one of the keys to}} achieve high yield and reduce the cycle time. However, when control parameters are over- or under-estimated, a dynamic sampling <b>static</b> <b>sampling</b> strategy can lead to poor results. In this paper we propose an integer linear programming approach to optimize the use of inspection capacity through dynamic sampling. The goal is to determine two key parameters (called warning limit and inhibit limit) that are related to the resulting level of risk and the available inspection capacity. The model has been implemented on a commercial solver and tested using actual industrial data. Results show that the overall risk can be strongly reduced without any additional capacity...|$|E
40|$|Collecting and {{maintaining}} radio fingerprint for wireless indoor positioning systems involves considerable time and labor. We have proposed the quick radio fingerprint collection (QRFC) algorithm which employed the built-in accelerometer of Android smartphones to implement step detection {{in order to}} assist in collecting radio fingerprints. In the present study, we divided the algorithm into moving sampling (MS) and stepped MS (SMS), and describe the implementation of both algorithms and their comparison. Technical details and common errors concerning the use of Android smartphones to collect Wi-Fi radio beacons were surveyed and discussed. The results of signal sampling experiments performed in a hallway measuring 54 m in length showed {{that in terms of}} the amount of time required to complete collection of access point (AP) signals, <b>static</b> <b>sampling</b> (SS; a traditional procedure for collecting Wi-Fi signals) took at least 2 h, whereas MS and SMS took approximately 150 and 300 s, respectively. Notably, AP signals obtained through MS and SMS were comparable to those obtained through SS in terms of the distribution of received signal strength indicator (RSSI) and positioning accuracy. Therefore, MS and SMS are recommended instead of SS as signal sampling procedures for indoor positioning algorithms...|$|E
40|$|In past years, {{numerous}} electronic nose (e-nose) {{developments have}} been published describing analyses of solid-, liquid- or gaseous media in microbiological-, environmental-, agricultural- or medical applications. However, little has been reported about complex methodological pitfalls that might be associated with commercially available e-nose technology. In this paper, some of these pitfalls such as temperature, the use of filters and mass flow using different sampling methods (static- and dynamic sampling) are described for two generations of conducting polymer e-noses (ST 114 / 214, CPs, both Scensive Tech. Ltd.). A comparison with metal oxide semiconducting field effect transistor/metal oxide semiconductor (MOSFET/MOS) e-noses regarding stability across replicates and over time was made. Changes in temperature were found to give larger sensor responses, whereas the application of filters led to quantitative and qualitative changes in sensor responses due {{to a change in}} mass flow which was also affected by the sampling method. <b>Static</b> <b>sampling</b> provided more stable flows across replicates. Variation was investigated for CPs and MOSFET/MOS e-noses that gave different responses over time and across replicates. These methodological factors cause a lack of stability and reproducibility, demonstrating the pitfalls of e-nose technology and therefore limit their utility for discriminating between samples. ...|$|E
40|$|Ln layered {{paramagnetic}} compounds, such as La 2 Li 0. 5 Ni 0. 5 O 4 with a perovskite structure, the Li- 7 NMR {{spectrum is}} broadened by anisotropic quadrupolar {{as well as}} paramagnetic dipolar interactions. We have used a two-dimensional spin echo (SE) experiment to separate the quadrupolar interaction and obtain a clean quadrupolar spectrum along the wi dimension. This is demonstrated in La 2 Li 0. 5 B 0. 5 O 4 (B = Cu, Ni), through 2 D SE experiments conducted in <b>static</b> <b>samples</b> {{as well as those}} in spinning at the magic angle. A quadrupole coupling constant of 92 kHz is estimated from the Wi spectrum for paramagnetic La 2 Li 0. 5 Ni 0. 5 O 4...|$|R
40|$|We {{developed}} random-access optical-resolution photoacoustic microscopy using {{a digital}} micromirror device. This system can rapidly scan arbitrarily shaped regions of interest within a 40 [*][*]μm× 40 [*][*]μm imaging {{area with a}} lateral resolution of 3. 6 μm. To identify a region of interest, a global structural image is first acquired, then the selected region is scanned. The random-access ability was demonstrated by imaging two <b>static</b> <b>samples,</b> a carbon fiber cross and a monolayer of red blood cells, with an acquisition rate up to 4 kHz. The system was then used to monitor blood flow in vivo in real time within user-selected capillaries in a mouse ear. By imaging only the capillary of interest, the frame rate was increased by up to 9. 2 times...|$|R
40|$|Techniques for {{sampling}} Internet traffic {{are very}} important to understand the traffic characteristics of the Internet [14, 8]. In spite of all the research efforts on packet sampling, none has taken into account of self-similarity of Internet traffic in devising sampling strategies. In this paper, we perform an in-depth, analytical study of three sampling techniques for self-similar Internet traffic, namely <b>static</b> systematic <b>sampling,</b> stratified random sampling and simple random sampling. We show that while all three sampling techniques can accurately capture the Hurst parameter (second order statistics) of Internet traffic, they fail to capture the mean (first order statistics) faithfully. We also show that <b>static</b> systematic <b>sampling</b> renders the smallest variation of sampling results in different instances of sampling (i. e., it gives sampling results of high fidelity). Based on an important observation, we then devise a new variation of <b>static</b> systematic <b>sampling,</b> called biased systematic sampling (BSS), that gives much more accurate estimates of the mean, while keeping the sampling overhead low. Both the analysis on the three sampling techniques and the evaluation of BSS are performed on synthetic and real Internet traffic traces. Our performance study shows that BSS gives a performance improvement of 40 % and 20 % (in terms of efficiency) as compared to static systematic and simple random sampling. ...|$|R
