52|18|Public
40|$|Financial trading {{rules have}} {{the aim of}} {{continuously}} evaluating available information in order to make timely decisions. This is also the aim of methods for <b>statistical</b> <b>surveillance.</b> Many results are available regarding the properties of surveillance methods. We give a review of financial trading rules and use the theory of <b>statistical</b> <b>surveillance</b> to find properties of some commonly used trading rules. In addition, a nonparametric and robust surveillance method is proposed as a trading rule. Evaluation measures used in <b>statistical</b> <b>surveillance</b> are compared with those used in finance. The Hang Seng Index is used for illustration. Swedish Research Council, The Bank of Sweden Tercentenary Foundation, Kungliga and Hvitdfeldtska Stiftelsen, Wilhelm and Martina Lundgrens Vetenskapsfond and the West Sweden Chamber of Commerce and Industr...|$|E
40|$|Detection {{turning points}} in unimodel has various {{applications}} to time series which have cyclic periods. Related techniques are widely explored {{in the field}} of <b>statistical</b> <b>surveillance,</b> that is, on-line turning point detection procedures. This paper will first present a power controlled turning point detection method based on the theory of the likelihood ratio test in <b>statistical</b> <b>surveillance.</b> Next we show how outliers will influence the performance of this methodology. Due to the sensitivity of the surveillance system to outliers, we finally present a wavelet multiresolution (MRA) based outlier elimination approach, which can be combined with the on-line turning point detection process and will then alleviate the false alarm problem introduced by the outliers. Unimodel, Turning point, <b>Statistical</b> <b>surveillance,</b> Outlier, Wavelet multiresolution, Threshold. ...|$|E
40|$|The aim of {{this report}} is to {{describe}} if and how <b>statistical</b> <b>surveillance</b> methods for monitoring {{of the performance of}} fund managers has been used. <b>Statistical</b> <b>surveillance</b> is a methodology for on-line monitoring, in which a warning signal is given if the performance declines. Since these methods are advanced you can expect the methods to be described in scientific literature. Problems in this area which will not be treated in this report are: 1. 	Estimation of the performance. 2. 	Hypothesis testing of if this manager always have a poor performance. 3. 	Examination of how managers are influenced by being evaluated...|$|E
50|$|The {{conventional}} {{method of}} Surveillance {{does not apply}} for a partially visible system. Therefore, a new method is required to observe such complex system at any time given. The Surveillance of a partially observable system offers a mathematical model that uses Markov Chains, paired with a Bayesian updating function. The method estimates the <b>statistical</b> impacts of <b>surveillance</b> observations and modified surveillance policies.|$|R
40|$|RNV 3 P is {{indexed in}} PubMed 1 SUMMARY In {{the context of}} {{surveillance}} of health problems, the research {{carried out by the}} French national occupational disease surveillance and prevention network (Réseau National de Vigilance et de Prévention des Pathologies Professionnelles, RNV 3 P) aims to develop, among other approaches, methods of <b>surveillance,</b> <b>statistical</b> analysis and modeling in order to study the structure and change over time of relationships between disease and exposure, and to detect emerging diseaseexposure associations. In this perspective, this paper aims to present the concept of th...|$|R
40|$|A {{declared}} need {{is around}} for geoinformatic <b>surveillance</b> <b>statistical</b> science and software infrastructure for spatial and spatiotemporal hotspot detection. Hotspot means something unusual, anomaly, aberration, outbreak, elevated cluster, critical resource area, etc. The declared need {{may be for}} monitoring, etiology, management, or early warning. The responsible factors may be natural, accidental, or intentional. This paper suggests methods and tools for hotspot detection across geographic regions and across networks. The investigation proposes development of statistical methods and tools that have immediate potential for use in critical societal areas, such as public health and disease surveillance, ecosystem health, water resources and water services, transportation networks, persistent poverty typologies and trajectories, environmental justice, biosurveillance an...|$|R
40|$|In many areas, it is {{important}} to detect turning points in time series early and without faults. Turns in business cycles and financial time series are discussed here. A variety of approaches for analyzing the turns in cyclical processes has been proposed. Some of the proposed techniques aim at point prediction of all values of the process. These techniques often give very low accuracy near turns. Other approaches concentrate on predicting the time of the turn. In this thesis we consider prospective monitoring of a stochastic process in order to call an alarm as soon as we have enough evidence that the critical event of interest has occurred. <b>Statistical</b> <b>surveillance</b> deals with the theory and methodology of online detection of an important change in the underlying process of a time series as soon as possible after it has occurred. The theory of <b>statistical</b> <b>surveillance</b> is used in this thesis to construct and compare methods for two important applications. In the first paper, three likelihood-based methods for detection of a tum are compared. The problem is being addressed in a business cycle context. We aim at timely detecting a turning point in a leading indicator of the business cycle. By detecting the turns in a leading indicator we have an instrument for predicting the turns in the business cycle. One of the methods is based on a hidden Markov model. The two others are based on the theory of <b>statistical</b> <b>surveillance.</b> One of these is free from parametric assumptions of the curve shape. Evaluations are made of e. g. the effects of different specifications of the curve and of the transition probabilities. The second paper investigates inferential differences and similarities between <b>statistical</b> <b>surveillance</b> and some prospective decision rules suggested for trading in financial markets. It is found that the proposed rules can be seen as special cases of classical methods of surveillance and can hence be discussed in the context of optimality properties of surveillance methods. Evaluation measures and utility functions commonly used in <b>statistical</b> <b>surveillance</b> are compared with those generally used in financial settings. Some of the methods are evaluated by case studies. The relative merits of case studies and Monte Carlo studies are discussed...|$|E
40|$|<b>Statistical</b> <b>surveillance</b> are {{methods for}} {{repeated}} analysis of stochastic processes, aiming {{to detect a}} change in the underlying distribution. Such methods are widely used for industrial, medical, economical and other applications. By applying these general methods on data collected for environmetrical purposes, {{it might be possible to}} detect important changes fast and reliable. We exemplify the use of <b>statistical</b> <b>surveillance</b> on a data set of fish catches in Lake MaIaren, Sweden, 1964 - 1993. A model for the in-control process of one species, vendace (Coregonus albula), is constructed and used for univariate monitoring. Further, we demonstrate the application of Hotelling's T 2 and the Shannon-Wiener index for monitoring biodiversity, where a set of five economically interesting species serve as bioindicators for the lake. Keywords...|$|E
40|$|Introduction and {{motivation}} The need for <b>statistical</b> <b>surveillance</b> {{has been noted}} in many different areas, including quality control (see for example [2]), epidemiology (see for example [13]), medicine (see for example [4]), machinery monitoring, seismology, finance (see for example [1]) etc. In this work, we {{address the problem of}} the detection of two-sided alternatives in a Brownian motion model. This model is th...|$|E
40|$|Since the {{intentional}} dissemination of anthrax through the US postal {{system in the}} fall of 2001, there has been increased interest in surveillance for detection of biological terrorism. More generally, this could be described as the detection of incident disease clusters. In addition, the advent of affordable and quick geocoding allows for surveillance on a finer spatial scale than has been possible in the past. Surveillance for incident clusters of disease in both time and space is a relatively undeveloped arena of <b>statistical</b> methodology. <b>Surveillance</b> for bioterrorism detection, in particular, raises unique issues with methodological relevance. For example, the bioterrorism agents of greatest concern cause initial symptoms that may be difficult to distinguish from those of naturally occurring disease. In this paper, the authors propose a general approach to evaluating whether observed counts in relatively small areas are larger than would be expected {{on the basis of a}} history of naturally occurring disease. They implement the approach using generalized linear mixed models. The approach is illustrated using data on health-care visits (1996 – 1999) from a large Massachusetts managed care organization/multispecialty practice group in the context of syndromic surveillance for anthrax. The authors argue that there is great value in using the geographic data...|$|R
40|$|In the {{management}} of emerging infectious disease epidemics, precise and accurate estimation of severity indi-ces, such as the probability of death after developing symptoms—the symptomatic case fatality ratio (sCFR) —is essential. Estimation of the sCFR may require merging data gathered through different surveillance systems and surveys. Since different surveillance strategies provide different levels of precision and accuracy, there is need for a theory to help investigators select the strategy that maximizes these properties. Here, we study the precision of sCFR estimators that combine data from several levels of the severity pyramid. We derive a formula for the standard error, which helps us find the estimator with the best precision given fixed resources. We further propose rules of thumb for guiding the choice of strategy: For example, should surveillance of a particular severity level be started? Which level should be preferred? We derive a formula for the optimal allocation of resources between chosen sur-veillance levels and provide a simple approximation {{that can be used}} in thinking more heuristically about planning surveillance. We illustrate these concepts with numerical examples corresponding to 3 influenza pandemic scenar-ios. Finally, we review the equally important issue of accuracy. case fatality ratio; emerging infectious diseases; influenza; pandemics; <b>statistical</b> planning; <b>surveillance</b> protocol Abbreviations: sCFR, symptomatic case fatality ratio; SE, standard error. During outbreaks of emerging infectious diseases, th...|$|R
40|$|In {{the context}} of {{surveillance}} of health problems, the research {{carried out by the}} French national occupational disease surveillance and prevention network (Réseau National de Vigilance et de Prévention des Pathologies Professionnelles, RNV 3 P) aims to develop, among other approaches, methods of <b>surveillance,</b> <b>statistical</b> analysis and modeling in order to study the structure and change over time of relationships between disease and exposure, and to detect emerging disease-exposure associations. In this perspective, this paper aims to present the concept of the "exposome" and to explain on what bases it is constructed. The exposome is defined as a network of relationships between occupational health problems that have in common one or several elements of occupational exposure (exposures, occupation and/or activity sector). The paper also aims to outline its potential for the study and programmed surveillance of composite disease-occupational exposure associations. We illustrate this approach by applying it to a sample from the RNV 3 P data, taking malignant tumours and focusing on the subgroup of non-Hodgkin lymphomas...|$|R
40|$|The {{need for}} <b>statistical</b> <b>surveillance</b> has been noticed in many {{different}} areas and examples of applications include the detection of an increased incidence of a disease, the detection of an increased radiation level and the detection of {{a turning point in}} a leading index for a business cycle. In all cases, preventive actions are possible if the alarm is made early. If the change is detected too late, this can have severe consequences both at a personal level for affected individuals and to society as a whole. In these important situations we must evaluate the evidence value of the information we have about the process in order to guide us in the choice of making an alarm or not. The aim is to detect the change as quickly as possible {{and at the same time}} control the rate of false alarms. To do this efficiently we construct alarm systems using the available observations of the process, which are taken sequentially in time. (Note the difference from a test of one hypothesis.) The theory of <b>statistical</b> <b>surveillance</b> deals with the construction of alarm systems and the evaluation of such systems. This licentiate thesis consists of two papers with this common subject. The first paper (1) deals with the properties of a special type of surveillance methods called EWMA methods. One attractive feature of EWMA methods is the easily interpretable alarms statistic, which is an exponentially weighted moving average of all available observations of the process. Several ways of constructing alarm limits to this statistic have previously been suggested in the literature. In this paper new types of evaluations of the performance of suggested variants are made and the results cast new light on both the merits of the variants and the optimality criteria commonly used. Methodological issues of general interest in the area of <b>statistical</b> <b>surveillance</b> are also treated, such as the definition of comparability between methods. The second paper (2) deals with <b>statistical</b> <b>surveillance</b> in the area of public health. A critical review with emphasis on the inferential issues is made. The merits of different approaches are discussed and a new method is derived. Especially noticeable from the review is the lack of methods of surveillance of a spatial pattern, an area that includes many important applications, not only in public health...|$|E
40|$|Many <b>statistical</b> <b>surveillance</b> {{systems for}} the timely {{detection}} of outbreaks of infectious disease operate on laboratory data. Such data typically incur reporting delays between the time at which a specimen is collected for diagnostic purposes, and the time at which {{the results of the}} laboratory analysis become available. <b>Statistical</b> <b>surveillance</b> systems currently in use usually make some ad hoc adjustment for such delays, or use counts by time of report. We propose a new statistical approach that takes account of the delays explicitly, by monitoring the number of specimens identified in the current and past m time units, where m is a tuning parameter. Values expected {{in the absence of an}} outbreak are estimated from counts observed in recent years (typically 5 years). We study the method in the context of an outbreak detection system used in the United Kingdom and several other European countries. We propose a suitable test statistic for the null hypothesis that no outbreak is currently occurring. We derive its null variance, incorporating uncertainty about the estimated delay distribution. Simulations and applications to some test datasets suggest the method works well, and can improve performance over ad hoc methods in current use. Supplementary materials for this article are available online...|$|E
40|$|In {{many areas}} {{there is a}} need to monitor {{observations}} in order to detect changes in the underlying processes as quickly as possible. The theory of <b>statistical</b> <b>surveillance</b> provides the possibility of making optimal decisions about whether a change has occurred or not based on the data available at the time of the decision. Surveillance can be used in many different situations. It is important that the relevant characteristics of the change are identified and that the relevant optimality criterion is used. There is a need to further develop the theory of <b>statistical</b> <b>surveillance.</b> One area where surveillance is of special interest is the detection of outbreaks of epidemic diseases. New strains of influenza virus like avian flu and swine flu have drawn much attention, but it is also important to detect the varying onset of the seasonal influenza. Outbreaks are characterized by a change from a constant incidence to an increasing one. A quick and reliable detection of epidemic outbreaks can be beneficial to society as it has the potential to prevent loss of lives and severe economic consequences. The detection of a change from a constant level to a monotonically increasing (or decreasing) regression is of interest also in other areas, for example in finance. This thesis considers outbreak detection i...|$|E
40|$|Monitoring ongoing {{processes}} of illness to detect sudden changes {{is an important}} aspect of practical epidemiology and medicine more generally. Most commonly, the monitoring has been restricted to a unidimensional stream of data over time. In such situations, analytic results from the industrial process monitoring have suggested optimal approaches to monitor the data streams. Data streams including spatial location as well as temporal sequence are becoming available. Monitoring methods that incorporate spatial data may prove superior to those that ignore it. However, analytically, optimal methods for spatial surveil-lance data may not exist. In the present article, we introduce and discuss evaluation metrics {{that can be used to}} compare the performance of <b>statistical</b> methods of <b>surveillance.</b> Our general approach is to gen-eralize receiver operating characteristic (ROC) curves to incorporate the time of detection in addition to the usual test characteristics of sensitivity and specificity. In addition to weighting ordinary ROC curves by two measures of timeliness, we describe three three-dimensional generalizations of ROC curves that result in timeliness-ROC surfaces. Working in the context of surveillance of cases of disease to detect a sudden outbreak, we demonstrate these in an artificial example and in a previously described simulation context and show how the metrics differ. We also discuss the differences and under which circumstances one might prefer a given method. ...|$|R
40|$|A {{declared}} need {{is around}} for geoinformatic <b>surveillance</b> <b>statistical</b> science and software infrastructure for spatial and spatiotemporal hotspot detection. Hotspot means something unusual, anomaly, aberration, outbreak, elevated cluster, critical resource area, etc. The declared need {{may be for}} monitoring, etiology, management, or early warning. The responsible factors may be natural, accidental, or intentional. This proof-of-concept paper suggests methods and tools for hotspot detection across geographic regions and across networks. The investigation proposes development of statistical methods and tools that have immediate potential for use in critical societal areas, such as public health and disease surveillance, ecosystem health, water resources and water services, transportation networks, persistent poverty typologies and trajectories, environmental justice, biosurveillance and biosecurity, among others. We introduce, for multidisciplinary use, an innovation of the health-area-popular circle-based spatial and spatiotemporal scan statistic. Our innovation employs {{the notion of an}} upper level set, and is accordingly called the upper level set scan statistic, pointing to a sophisticated analytical and computational system as the next generation of the present day popular SaTScan. Success of surveillance rests on potential elevated cluster detection capability. But the clusters can be of any shape, and cannot be captured only by circles. This is likely to give more of false alarms and more of false sense of security. What we need is capability to detect arbitrarily shaped clusters. The proposed upper level set scan statistic innovation is expected to ®ll this nee...|$|R
40|$|Objective: There is a {{possible}} correlation between endometriosis and {{an increased risk of}} epithelial ovarian cancer (EOC), but many uncertainties remain, including race, exposure or surveillance time, and surgical confirmation. Therefore, we carried out a large-scale, nationwide, controlled cohort study in the Taiwanese women to respond to these uncertainties. Materials and methods: A historical cohort study was performed by linking the National Health Insurance Research Database of Taiwan. Each patient diagnosed with endometriosis (n =  7537) between 2000 and 2009 was background matched with up to two women without endometriosis (n =  15, 074). The total was 136, 643 person-years of follow-up and 24 women having new EOC. Cox regression analysis was used to determine the relationship between the EOC incidence rate and an endometriosis status. Results: The EOC incidence rate of the endometriosis and non-endometriosis women was 3. 31 per 10, 000 person-years and 0. 99 per 10, 000 person-years, respectively, contributing to an adjusted hazard ratio (HR) of 3. 28 (95 % confidence interval, 1. 37 – 7. 85). The women with surgical confirmation had a much higher adjusted HR (3. 87; 95 % confidence interval, 1. 58 – 9. 47). No significantly <b>statistical</b> difference of <b>surveillance</b> time between women with and without endometriosis (3. 87  years vs. 3. 73  years). The occurrence of EOC was not also affected by exposure time of women with endometriosis. Conclusion: Taiwanese women with endometriosis really had a risk of newly developed EOC, especially those who had a surgical diagnosis, and this three-fold increase of risk was neither influenced by exposure time nor biased by surveillance...|$|R
40|$|A {{computer}} program which simultaneously gives graphical information on important characteristics of <b>statistical</b> <b>surveillance</b> methods is presented. Surveillance, that is continual observation {{of a time}} series {{with the goal of}} timely detection of possible important changes in the underlying process, is used in quality control, economics, medicine and other fields. When surveillance is used in practice it is necessary to evaluate the method in order to know which action is appropriate at an alarm. The probability of a false alarm, the probability of successful detection and the predictive value are three measures (besides the usual ARL), which are illustrated by the program...|$|E
40|$|Monitoring {{medical errors}} has a proven {{positive}} impact on improving health-care quality. This study is designed to discuss the <b>statistical</b> <b>surveillance</b> methods and propose useful procedures to medical practitioners for monitoring time between medical errors. Variable time between events (TBE) control charts are constructed in order to monitor time betwe en dosing errors. Results indicate that the variable TBE control charts can easily be integrated to the medical error monitoring processes. Several methods are considered to analyze the dozing error data and discussion is provided for medical decision make rs to apply a better medical error monitoring program...|$|E
40|$|International audienceSystems for multivariate on-line {{surveillance}} (e. g. outbreak detection), are investigated. Optimal {{systems for}} <b>statistical</b> <b>surveillance</b> {{are based on}} likelihood ratios. Three systems are compared; based on each marginal density, based on the joint density {{and based on the}} Hotelling's T 2. The effect of dependency between the monitored processes is investigated, and the effect of correlation between the change times. When the first change occurs immediately, the three methods give similar delay of an alarm, in the situation with independency. For late changes, T 2 has the longest delay, both for independent processes and for processes with a positive covariance...|$|E
50|$|There {{are many}} systems {{that can be}} {{considered}} as partially observable due to their unknown or partially known structures or the nature of their unknown products and/or partially known results. The impacts of the consumption of genetically modified food (GM) are an example of a system that is only partially observable. The safety of genetically modified foods (GM) products has caused much controversy. Absence of sufficient and reliable information prevents neither certain confidence about the harmlessness of product consumption, nor any certain conclusion to merit a ban for fear of harm. The lack of any reliable or conclusive post-market observation and consumption effects information, make it difficult to establish a global protocol for such products. This paper introduces a model for the analysis of partially observable information from the surveillance of post-market consumption of systems such as genetically modified foods (GM) products. This model uses Markov Chains, paired with a Bayesian updating function to estimate the <b>statistical</b> impacts of <b>surveillance</b> observations and modified surveillance policies. A case study on population health status is used as an illustrative example, which is modeled to demonstrate the impact of policy interventions on simulated data. A cost decision analysis model is also applied to illustrate the impact of policy intervention costs. The model uses a first order Markov chain to estimate the period-over-period change in health status and a Bayesian updating procedure to estimate the population health status based on observations from post-market surveillance. The results show how observation samples can be used to provide information on system changes and improvements.|$|R
40|$|Background: Cholera {{outbreaks}} are {{a continuing}} problem in Bangladesh, and the timely detection of an outbreak {{is important for}} reducing morbidity and mortality. In Matlab, the ongoing Health and Demographic Surveillance System (HDSS) data records symptoms of diarrhea in {{children under the age}} of 5 years at the community level. Cholera surveillance in Matlab currently uses hospital-based data. Objective: The objective of this study is to determine whether increases in cholera in Matlab can be detected earlier by using HDSS diarrhea symptom data in a syndromic surveillance analysis, when compared to hospital admissions for cholera. Methods: HDSS diarrhea symptom data and hospital admissions for cholera in children under 5 years of age over a 2 -year period were analyzed with the syndromic <b>surveillance</b> <b>statistical</b> program EARS (Early Aberration Reporting System). Dates when significant increases in either symptoms or cholera cases occurred were compared to one another. Results: The analysis revealed that there were 43 days over 16 months when the cholera cases or diarrhea symptoms increased significantly. There were 8 months when both data sets detected days with significant increases. In 5 of the 8 months, increases in diarrheal symptoms occurred before increases of cholera cases. The increases in symptoms occurred between 1 and 15 days before the increases in cholera cases. Conclusions: The results suggest that the HDSS survey data may be able to detect an increase in cholera before an increase in hospital admissions is seen. However, there was no direct link between diarrheal symptom increases and cholera cases, and this, as well as other methodological weaknesses, should be taken into consideration...|$|R
40|$|During 2000, a {{total of}} 16, 377 TB cases (5. 8 cases per 100, 000 population) were {{reported}} to CDC from the 50 states and the District of Columbia, representing a 7 % decrease from 1999 and a 39 % decrease from 1992, {{when the number of}} cases peaked during the resurgence of TB in the United States. The national TB case rate also steadily decreased during this period (Table 1). In 2000, 6 % of cases were reported in children under 15 years old, 10 % in persons aged 15 - 24 years, 34 % in persons aged 25 - 44 years, 28 % in persons aged 45 - 64 years, and 22 % in persons aged 65 years and older (Table 2). During 1992 - 2000, there was a decline in both the number of cases reported in each of these age groups and the respective TB case rates. " - p. 2 NPIN 30570 : This report presents summary data for tuberculosis (TB) cases reported to the Division of TB Elimination, CDC during 2000. Morbidity Trend Tables, United States [...] Morbidity Tables, United States [...] Morbidity Tables, States [...] Morbidity Tables, Reporting Areas [...] Morbidity Tables, Cities and Metropolitan <b>Statistical</b> Areas [...] <b>Surveillance</b> Slide Set [...] Appendix A: Technical notes [...] Appendix B: Tuberculosis Case Definition for Public Health Surveillance [...] Appendix C: Recommendations for Counting Reported Tuberculosis Cases"August 2001 " [...] P. [i]. Also available via the World Wide Web. Includes bibliographical references. CDC. Reported Tuberculosis in the United States, 2000. Atlanta, GA: U. S. Department of Health and Human Services, CDC, August, 2001...|$|R
40|$|The {{results of}} {{subsystem}} of the Integrated System of Individual Dose Control (ESKID) functioning {{on the base}} of the Federal State <b>Statistical</b> <b>Surveillance</b> Form № 4 -DOZ in 2001 - 2009, dynamics of the population exposure from natural ionizing sources dose control and accounting system as well as the software for the regional data banks introduction are considered in the article. The article presents exposure levels and doses from the natural ionizing sources for the population of certain regions and Russia as a whole, obtained by integration of all data presented in 2001 - 2009 in the framework of ESKID system. </span...|$|E
40|$|Using Markov chain representations, we {{evaluate}} {{and compare the}} performance of cumulative sum (CUSUM) and Shiryayev-Roberts methods {{in terms of the}} zero- and steady-state average run length and worst-case signal resistance measures. We also calculate the signal resistance values from the worst- to the best-case scenarios for both the methods. Our results support the recommendation that Shewhart limits be used with CUSUM and Shiryayev-Roberts methods, especially for low values {{of the size of the}} shift in the process mean for which the methods are designed to detect optimally. CUSUM chart, likelihood ratio, Shiryayev-Roberts chart, Shewhart chart, statistical process control, <b>statistical</b> <b>surveillance,...</b>|$|E
40|$|As part of {{an effort}} to develop tools to help monitor and manage the Air Force's world-wide health {{delivery}} systems, the USAF Surgeon General's Office has been developing a web-based <b>statistical</b> <b>surveillance</b> information system in order to provide near real-time management and clinical information. The intent is to incorporate statistical and related scientific methods that can help identify unusual events and patterns of concern in large, highly distributed, and near continual data flows. This system combines graphical analysis, query ability, and embedded semi-automated intelligence for examining process performance at any level of aggregation or stratification. The objectives, current status, latest version, successes to-date, future directions, and barriers are discussed and demonstrated...|$|E
40|$|Surveillance of a {{partially}} observable system is complex. There are many systems {{that can be}} considered as partially observable due to their unknown or partially known structures or the nature of their unknown products and/or partially known results. The impacts of the consumption of genetically modified food (GM) are an example of a system that is only partially observable. The safety of genetically modified foods (GM) products has caused much controversy. Absence of sufficient and reliable information prevents neither certain confidence about the harmlessness of product consumption, nor any certain conclusion to merit a ban for fear of harm. The lack of any reliable or conclusive post-market observation and consumption effects information, make it difficult to establish a global protocol for such products. This paper introduces a model for the analysis of partially observable information from the surveillance of post-market consumption of systems such as genetically modified foods (GM) products. This model uses Markov Chains, paired with a Bayesian updating function to estimate the <b>statistical</b> impacts of <b>surveillance</b> observations and modified surveillance policies. A case study on population health status is used as an illustrative example, which is modeled to demonstrate the impact of policy interventions on simulated data. A cost decision analysis model is also applied to illustrate the impact of policy intervention costs. The model uses a first order Markov chain to estimate the period-over-period change in health status and a Bayesian updating procedure to estimate the population health status based on observations from post-market surveillance. The results show how observation samples can be used to provide information on system changes and improvements...|$|R
40|$|Within {{the current}} context that favours the {{emergence}} of new diseases, syndromic surveillance (SyS) appears increasingly more relevant tool for the early detection of unexpected health events. The Triple-S project (Syndromic Surveillance Systems in Europe), co-financed by the European Commission, was launched in September 2010 for a three year period to promote both human and animal health SyS in European countries. Objectives of the project included performing an inventory of current and planned European animal health SyS systems and promoting knowledge transfer between SyS experts. This study presents and discusses the results of the Triple-S inventory of European veterinary SyS initiatives. European SyS systems were identified through an active process based on a questionnaire sent to animal health experts involved in SyS in Europe. Results were analyzed through a descriptive analysis and a multiple factor analysis (MFA) in order to establish a typology of the European SyS initiatives. Twenty seven European SyS systems were identified from twelve countries, at different levels of development, from project phase to active systems. Results of this inventory showed a real interest of European countries for SyS but also highlighted the novelty of this field. This survey highlighted the diversity of SyS systems in Europe in terms of objectives, population targeted, data providers, indicators monitored. For most SyS initiatives, <b>statistical</b> analysis of <b>surveillance</b> results was identified as a limitation in using the data. MFA results distinguished two types of systems. The first one belonged to the private sector, focused on companion animals and had reached a higher degree of achievement. The second one was based on mandatory collected data, targeted livestock species and is still in an early project phase. The exchange of knowledge between human and animal health sectors was considered useful to enhance SyS. In the same way that SyS is complementary to traditional surveillance, synergies between human and animal health SyS could be an added value, most notably to enhance timeliness, sensitivity and help interpreting non-specific signals...|$|R
40|$|Network {{intrusion}} {{starts off}} {{with a series of}} unsuccessful breakin attempts and results eventually with the permanent or transient failure of an authentication or authorization system. Due to the current complexity of authentication systems, clandestine attempts at intrusion generally take considerable time before the system gets compromised or damaging change is affected to the system giving administrators a window of opportunity to proactively detect and prevent intrusion. Therefore maintaining a high level of sensitivity to abnormal access patterns is a very effective way of preventing possible break-ins. Under normal circumstances, gross errors {{on the part of the}} user can cause authentication and authorization failures on all systems. A normal distribution of failed attempts should be tolerated while abnormal attempts should be recognized as such and flagged. But one cannot manage what one cannot measure. This paper proposes a method that can efficiently quantify the behaviour of users on a network so that transient changes in usage can be detected, categorized based on severity, and closely investigated for possible intrusion. The author proposes the identification of patterns in protocol usage within a network to categorize it for <b>surveillance.</b> <b>Statistical</b> anomaly detection, under which category this approach falls, generally uses simple statistical tests such as mean and standard deviation to detect behavioural changes. The author proposes a novel approach using spectral density as opposed to using time domain data, allowing a clear separation or access patterns based on periodicity. Once a spectral profile has been identified for network, deviations from this profile can be used as an indication of a destabilized or compromised network. Spectral analysis of access patterns is done using the Fast Fourier Transform (FFT), which can be computed in Θ(N log N) operations. The paper justifies the use of this approach and presents preliminary results of studies the author has conducted on a restricted campus network. The paper also discusses how profile deviations of the network can be used to trigger a more exhaustive diagnostic setup that can be a very effective first-line of defense for any network...|$|R
40|$|In {{this report}} {{a method for}} {{monitoring}} time series with cycles is presented. It is a nonparametric approach for detecting the turning point of the cycles. Time series of business indicators often exhibit cycles that can not easily be modelled with a parametric function. Forecasting the turning points is important to economic and political decisions. One approach to forecasting the business cycles {{is to use a}} leading indicator. The method presented in this report uses <b>statistical</b> <b>surveillance</b> to detect the turning points of a leading indicator. <b>Statistical</b> <b>surveillance</b> is a methodology for detecting a change in the underlying process as soon as possible. Observations on the leading indicator are gathered once a month and the change in the process is a turning point. Only a part of a series that contains one turning point at most will be investigated. The time series is assumed to consist of two additive components: a trend cycle part and a stochastic error part. No parametric model is assumed for the trend cycle, estimation is instead made by robust regression under different monotonicity restrictions. The aim is to detect a turning point as soon as possible, not to predict the value of the time series at the turning point. Evaluation of this surveillance method is done by means of simulation. The number of false alarms and the delay time are analysed. The evaluation shows that if there is no turning point then the median time to the first false alarm is five years, whereas if there is a turning point after three years, the median time to an alarm is 3 months...|$|E
40|$|<b>Statistical</b> <b>surveillance</b> is {{used for}} fast and secure {{detection}} of a critical event in a monitored process. This paper studies the performance for AR(l) processes. Two often suggested methods for detection of {{a shift in the}} mean, the modified Shewhart and the residual method, are compared and evaluated. Further, comparisons are made with direct Shewhart and a likelihood ratio method. New evaluation measures, the probability for successful detection and the predictive value, are also applied together with the average run length and run length distributions. We conclude that neither the modified nor the residual methods is uniformly optimal. The residual method is, however, optimal for immediate detection, but has inferior properties otherwise. For many parameter setups, the modified method will give the better performance...|$|E
40|$|<b>Statistical</b> <b>surveillance</b> is used {{to detect}} a change in a process. It might for example be a change {{of the level of}} a {{characteristic}} of an economic time series or a change of heart rate in intensive care. An alarm is triggered when there is enough evidence of a change. When surveillance is used in practice it is necessary to know the characteristics of the method, in order to know which action that is appropriate at an alarm. The average run length, the probability of a false alarm, the probability of successful detection and the predictive value of an alarm are measures that are used when comparing the performance of different methods for <b>statistical</b> <b>surveillance.</b> In the first paper a detailed comparison between two important methods, the Exponentially Weighted Moving Average and the CUSUM, is made. Some consequences of using only the average run length as the measure of performance are demonstrated. Differences between the methods are discussed in regard to the measures mentioned above. The second paper is focused on the predictive value of an alarm, that is the relative frequency of motivated alarms among all alarms. The interpretation of an alarm is difficult to make if the predictive value of an alarm varies with time. Thus conditions for a constant predictive value of an alarm are studied. The Shewhart methods and some Moving Average methods are discussed and some general differences in performance are pointed out. Three different types of Exponentially Weighted Average are discussed and some differences established. It is further stated that if a Fast Initial Response feature is added to a method, this will in general lower the level of the predictive value of an alarm {{in the beginning of the}} surveillance. The increased probability of alarm in the beginning might thus be useless...|$|E
40|$|Current situation: Standardized {{terminology}} of the Omaha system brings {{new opportunities}} for nurses providing community care. Objectives: Four objectives have been defined for this study: to implement the Omaha system in the nursing documentation {{for the purpose of}} verification of nursing care in community practice - to characterize the problems of patients with gastrointestinal diseases as per the Problem Classification Scheme - to map the links between problems and interventions as per the Intervention Scheme - to evaluate the problem results using the Problem Rating Scale for Outcomes of the Omaha system. Methodology: The research method used was based on the study by Kathryn H. Bowles (2000) addressing problems of patients and nurse interventions. The quantitative research method was implemented by inquiry using nursing documentation with the Omaha system implemented. The data acquired was tested using the software tools SPSS 22. 0 and MS Excel. Research set: It consisted of 103 patients with gastrointestinal diseases. The inquiry was carried out by contact persons - nurses of the gastroenterology department. Results: Patients with gastrointestinal diseases suffering {{from a wide range of}} problems corresponding to given character of diseases, which predominantly occur in the Physiological Domain and Health-Related Behavior Domain. In terms of mapping of links between problems and interventions, the results have proven a high usability of the objectives defined in the Intervention Scheme in community practice. The interventions were mostly determined in the categories Treatments and Procedures and <b>Surveillance.</b> <b>Statistical</b> differences have been confirmed in the documentation of interventions in individual categories for patient problems as well as for the selection of interventions in individual categories with respect to the type of given gastrointestinal disease. The study results have proven a significant improvement of knowledge, behaviour and status for individual problems, which is shown in comparison of final values with the initial values of problem results. The implementation of the selected interventions as per the Intervention Scheme resulted in a reduction or elimination of ? of problems. Conclusions: The results of the research inquiry clearly show that the Omaha system is an appropriate tool for documentation of all stages of the nursing process in the community care. Implementation of the functional documentation in community practice requires the Czech version of the Omaha system and electronic form of the documentation, which are absent at present...|$|R
40|$|The {{rising level}} of {{antibiotic}} resistance {{is a serious}} public health issue, posing a global threat to human health. The common practice of applying sub-therapeutic dosages of antibiotics to livestock {{has been shown to}} foster the development of antibiotic resistant bacteria (ARB). ARB originating in livestock can reach the general population via multiple pathways: air downwind of animal feeding operations and transportation vehicles, indoor air, soil following land application, surface and groundwater, and retail meat and poultry. Likewise, antibiotic resistance genes (ARGs) confer antibiotic resistance through various mechanisms, and are themselves considered to be emerging contaminants. In Chapter 2, we demonstrate the presence of both ARB and ARGs emanating from cattle production in air. This study is unique in that it is the first to compare resistance in airborne bacteria near conventional and organic beef farms. We used two methods to assess antibiotic resistance: our newly developed high throughput method (HT) for liquid cultures (n= 1295), and a common method, disk diffusion (DD), which involves culturing on solid media. By the HT method, conventional beef production sites showed a greater average fraction of ARB than organic production for most of the six antibiotics at the low and high concentrations, some with <b>statistical</b> significance. Regular <b>surveillance</b> of ARB and ARGs from beef cattle farms is suggested to detect the spread of ARB and ARG to the community via air trajectories. 	This body of work also demonstrates ARB originating from retail meat. In Chapter 3, we present results that suggest that the presence of antibiotic resistant E. coli differs {{depending on the type of}} poultry production system. In this study, we cultured Escherichia coli from retail poultry falling into three categories of farming practices: Conventional, No Antibiotics, and Pristine Organic. We examined the antibiotic resistance of the E. coli isolates (n = 424) by exposing them to seven common antibiotics via a high-throughput, liquid culture-based method. Our findings were that the fraction resistant of the E. coli bacteria from the Pristine Organic was significantly lower than the Conventional and No Antibiotic categories, while the latter categories had similar fractions of isolates resistant. It is the first study to suggest that a particular type of organic meat production shows a significant improvement in antibiotic resistance over typical organic brands. In Chapter 4, the study addresses the gap in literature where there are few studies that address the release of ARGs specifically from poultry CAFOs. To investigate this, upwind and downwind of the farms, bioaerosols were collected and examined for antibiotic resistant genes. To our knowledge, this is the first study to examine air samples collected upwind and downwind of CAFOs for elevated levels of ARGs. Our findings show that there are higher frequency of blaSHV downwind than upwind in two poultry CAFO sites and also, a higher frequency of erm(F) downwind than upwind in three poultry CAFO sites. The higher frequency of ARG detection near downwind versus upwind of poultry farms collectively show the occurrence of antibiotic resistance in the environment, which may imply potential exposure to ARGs via an air pathway downwind of poultry farm. Regular monitoring and surveillance of ARG from poultry sources is suggested to detect development of airborne antimicrobial resistance that can be spread to surrounding community, especially humans living near farms and workers, via wind air trajectories. The microbiome of family workers can be impacted by these ARG’s and research into the public health should be investigated...|$|R
40|$|The {{need for}} <b>statistical</b> <b>surveillance</b> has been noticed in many areas. The Swedish Institute for Infectious Disease Control (SMI), {{managing}} statutory notifications of some 50 communicable infectious diseases, has identified {{the need for}} an automated “early warning ” system, monitoring new notifications for possible disease outbreaks. A double reporting system is used for each case of an infectious disease. The two reports emanate from the doctor treating the patient and from the laboratory where the causative agent is identified. The aim of the report is to evaluate three well known statistical modelling tech-niques, namely; counted data Poisson CuSums, the English model (used in England and Wales) and SPOTv 2. In order to conduct the evaluation I use retrospective epi-demiological information, assembled by the SMI, between 1992 and 2003 of Campy...|$|E
