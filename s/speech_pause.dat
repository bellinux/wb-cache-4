27|168|Public
30|$|Following the {{suggestion}} of Huang [72], before and after each prosodic variation, we used a <b>speech</b> <b>pause</b> of 150 Â ms, as utterance breaks of such duration seem to be preferable to listeners.|$|E
3000|$|... {{determine}} how strict {{the requirements for}} detecting a <b>speech</b> <b>pause</b> are, {{and they can be}} adjusted to make the VAD more or less sensitive to detecting speech pauses. By increasing {{one or both of the}} parameters, the algorithm will detect more speech pauses, but at the same time, it will also detect more speech periods as noise.|$|E
40|$|This paper {{presents}} a speaker localization system using a microphone array. The array is operated as a steered lter-and-sum beamformer implemented as a summed correlator. In particular, we emphasize {{the use of}} a <b>speech</b> <b>pause</b> detector to improve the robustness of the speaker localization system by avoiding erroneous position estimates when no speech signal is present. Simulation results and measurements show that <b>speech</b> <b>pause</b> detection improves the overall system performance considerably. 1 Introduction For acoustic speaker localization, we use a number of spatially distributed microphone sensors to capture incoming sound waves. If the amplitude gradient across the microphone array is negligible, the time delays between dierent microphone signals can be expressed in terms of the unknown source location parameters. Speaker localization is related to the classical sonar target detection problem already described in [1, 2]. However, these early references are not directly applicable to [...] ...|$|E
30|$|Commonly {{the noise}} PSD is {{estimated}} in <b>speech</b> <b>pauses</b> where the pauses are detected using {{voice activity detection}} (VAD, see e.g., [24, 26]). VAD-based methods provide good estimates for stationary noise. However, they may suffer from error propagation if subsequent decisions are not independent. Other methods, like the minimum statistics approach introduced by Martin [23, 27], use a continuous estimation that does not explicitly differentiate between <b>speech</b> <b>pauses</b> and <b>speech</b> active segments.|$|R
3000|$|... is to {{be altered}} {{depending}} on how fast the additional noise damping g 2 (n) should start to affect the signal. A short hold time would imply noticeable additional noise reduction in short <b>speech</b> <b>pauses</b> but could {{on the other hand}} cause annoying pumping of the noise level. A longer hold time lessens this noise level pumping effect, but would not cause any noticeable additional noise damping in short <b>speech</b> <b>pauses.</b> Further, the threshold T [...]...|$|R
30|$|Speech transmission, e.g., via mobile networks, is {{primarily}} focused on speech segments. During <b>speech</b> <b>pauses,</b> less information is transmitted and comfort noise is inserted instead.|$|R
3000|$|... [...]. The {{envelopes}} of band-limited {{signals are}} considered since some noise types have stronger low- (or high-) frequency components. In that case, {{one of the}} band-limited envelopes may be less disturbed by the noise and provide more reliable information for <b>speech</b> <b>pause</b> decision. The envelopes are smoothed slightly using a first-order recursive low-pass filter with a release time constant [...]...|$|E
3000|$|The {{decision}} for a <b>speech</b> <b>pause</b> {{is based on}} several requirements regarding the dynamic range of the signal and the current envelope values for the three bands. As the complete decision process is described in [19], {{it will not be}} outlined here, that is, only the general concepts are provided. The criterion for the envelope being close enough to its minimum is determined by the free parameters [...]...|$|E
40|$|Speech {{disorders}} are very complicated in individuals suffering from Apraxia of Speech-AOS. In this paper,the pathological cases of speech disabled children affected with AOS are analyzed. The speech signalsamples of childrenSpeech {{disorders are}} very complicated in individuals suffering from Apraxia of Speech-AOS. In this paper,the pathological cases of speech disabled children affected with AOS are analyzed. The speech signalsamples {{of children of}} age between three to eight years are considered for the present study. These speechsignals are digitized and enhanced using the using the <b>Speech</b> <b>Pause</b> Index, Jitter,Skew,Kurtosis analysisThis analysis is conducted on speech data samples which are concerned with both place of articulation andmanner of articulation. The speech disability of pathological subjects was estimated using results of aboveanalysis. of age between three to eight years are considered for the present study. These speechsignals are digitized and enhanced using the using the <b>Speech</b> <b>Pause</b> Index, Jitter,Skew,Kurtosis analysisThis analysis is conducted on speech data samples which are concerned with both place of articulation andmanner of articulation. The speech disability of pathological subjects was estimated using results of aboveanalysis...|$|E
40|$|The {{temporal}} {{organization of}} synthetic speech {{of less than}} optimum quality should be adapted to the temporal course of speech perception. Considerations {{on the process of}} spoken word recognition make it likely that the difficulties encountered by listeners, due to degraded speech quality, can be reduced by regularly inserting <b>speech</b> <b>pauses,</b> even where such pauses are unnecessary in high-quality speech. This hypothesis is confirmed in a listening test comparing the intelligibility of Dutch natural and diphone-concatenated speech, both with and without grammatical <b>speech</b> <b>pauses...</b>|$|R
3000|$|As {{described}} in Section 3, the proposed algorithm incorporates a fullband gain g 2 (n), {{which has the}} purpose of damping noise in longer <b>speech</b> <b>pauses.</b> The gain limitation L [...]...|$|R
30|$|This paper {{demonstrates}} the drawback and proposes a modification {{to avoid this}} drawback. Further, the paper presents additional improvements {{in the form of}} a gain modified to produce less speech distortion and to provide more noise damping in <b>speech</b> <b>pauses.</b>|$|R
30|$|The {{likelihood}} ratio in (24) is calculated {{based on a}} single frame. Especially weak speech tails are difficult to detect given this limited amount of information. In several publications, model-based VADs were discussed that incorporate contextual information from multiple frames. Already Sohn et al. [12] introduced a hangover scheme that considers the dependency between successive frames. For this, speech and <b>speech</b> <b>pause</b> are modeled by two states of a hidden Markov model (HMM). Fixed probabilities are assigned to both state transitions. The forward procedure is applied, resulting in a temporally smoothed {{likelihood ratio}}.|$|E
40|$|This paper {{presents}} a speaker localization system using a microphone array operated as a steered filter-and-sum beamformer. To facilitate an efficient software implementation, we propose a summed correlator technique {{for evaluating the}} beamformer output associated with each hypothesized speaker position. We also describe a sequential search method to reduce the search complexity. Finally, a <b>speech</b> <b>pause</b> detector is introduced. It improves the overall system performance by eliminating erroneous position estimates when no speech signal is present. Simulation results and experiments based on real data support the approach proposed...|$|E
40|$|Based on microcontroller, {{an audio}} source {{switcher}} {{which can be}} used for medium wave transmitter is proposed in this paper. According to the default priority, the proposed switcher can correctly realize the function of switch among other signal channels when the current audio source is in transmission fault. In order to improve the stability and practical value, the switcher can correctly distinguish the statuses of <b>speech</b> <b>pause</b> and transmission fault according to default time. Moreover, the trap filter is also used in switcher to inhabit the high frequency interference of transmitting station. It is expected that this switcher has potential application in future...|$|E
6000|$|... "No, I can't guess, dear," [...] she said, and smiled, noticing {{with emotion}} that his breast was heaving {{as if he}} had been out of breath. Nevertheless, he tried to command his <b>speech,</b> <b>pausing</b> only a little between the words.|$|R
40|$|Previous {{research}} works {{proved the}} existence of a synchronization between speech and holds in adults and in 9 year old children with a rich linguistic vocabulary and advanced language skills. When and how does this synchrony develop during child language acquisition? Could it be observed also in children younger than 9 ? The present work aims to answer the above questions reporting on the analysis of narrations produced by three different age groups of Italian children (9, 5 and 3 year olds). Measurements are provided on the amount of synchronization between <b>speech</b> <b>pauses</b> and holds in the three different groups, {{as a function of the}} duration of the narrations. The results show that, as far as the reported data concerns, in children, as in adults, holds and <b>speech</b> <b>pauses</b> are to a certain extent synchronized and play similar functions, suggesting that they may be considered as a multi-determined phenomenon exploited by the speaker under the guidance of a unified planning process to satisfy a communicative intention. In addition, considering the role that <b>speech</b> <b>pauses</b> play in communication, we speculate on the possibility that holds may serve to similar purposes supporting the hypothesis that gestures as speech are an expressive resource that can take on different functions depending on the communicative demand. While <b>speech</b> <b>pauses</b> are likely to play the role of signalling mental activation processes aimed at replacing the âold spoken contentâ of the communicative plan with a new one, holds may signal mental activation processes aimed at replacing the âold visible bodily actionâ with new ones reflecting the representational and/or propositional contribution of gestures to the new communicative plan...|$|R
50|$|It enables for {{constant}} and variable bitrate. If the signal {{disappears into the}} noise floor in <b>speech</b> <b>pauses</b> and similar cases, the transmission can be limited to signal the output of comfort noise to the decoder. Most settings of the naturally streaming-enabled format can be changed on the fly without interrupting transmission.|$|R
30|$|The {{left and}} right {{microphone}} signals are then generated by convolving the target speech signal with the binaural impulse responses. The clean speech signal is a 60 -s concatenation of the female and male sentences taken from the TIMIT database [60]. A total of 30 % of the total length consists of <b>speech</b> <b>pause.</b> Moreover, no initial noise-only frames have been utilized. Regarding the additive noise, six different binaural noises, including cafeteria noise, kindergarten noise, and Mensa noise, from the ETSI database [61] were used. Moreover, the computer-generated binaural babble noise and binaural white Gaussian noise (WGN) [62] were also considered in our evaluation.|$|E
40|$|Psychomotor {{retardation}} {{is important}} in some depressed patients. We found that <b>speech</b> <b>pause</b> time (SPT) during a counting test correlated with the reaction time of both depressed patients and controls. It also correlated with global psychomotor retardation measured on Widlocher's scale. We demonstrated increased SPT in unipolar depressives, and also in retarded depressives as a group when compared with controls and with non-retarded depressives. SPT varied diurnally in controls, but not in depressed subjects. It did not correlate with biological markers of depression (REM sleep latency and the dexamethasone suppression test). It did, however, shorten during clinical improvement with antidepressant chemotherapy. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|E
40|$|Two {{drawbacks}} {{have to be}} envisaged if {{noise suppression}} with one-channel methods or if noise compensation using a reference channel for the noise is applied: on one hand a <b>speech</b> <b>pause</b> detector is a prerequisite to estimate the noise in speech pauses, {{on the other hand}} only point-sources for the noise can be compensated. The system presented in the paper users a mircophone array to overcome these problems. The speech signal is enhanced realizing a specific charateristic of the array and setting a zero {{in the direction of the}} speaker to estimate the noise. Combining this approach with well-known techniques for speech enhancement deliver a high gain for the signal-to noise ratio. The system can be used for speech enhancement for hand-set free communication from cars and for speech control of machinery...|$|E
50|$|Successful {{treatment}} incorporates {{these various}} treatment programs and approaches to facilitate patient's learning. In {{order to improve}} self-monitoring speech language pathologists will slow their own rate of <b>speech,</b> <b>pausing</b> between meaningful segments and encourage patients to do the same, slowing down their own speech, listening to themselves speak and monitoring their speech output.|$|R
3000|$|... a flat noise {{spectrum}} {{is required in}} order to achieve low values [...] 1 for <b>speech</b> <b>pauses.</b> Therefore, a spectral whitening scheme has to be applied in advance. The spectral envelope of noise is determined by temporally smoothing the spectrum. Whitening is then achieved by normalizing the instantaneous spectrum with this estimated {{noise spectrum}}.|$|R
30|$|The {{proposed}} method prevents error propagation, {{because the}} MS approach {{is independent of}} the VAD. During <b>speech</b> <b>pauses</b> the noise PSD estimation can be enhanced compared with an estimate solely based on minimum statistics. A similar time-frequency dependent VAD was presented by Cohen to enhance the noise power spectral density estimation of minimum statistics [28].|$|R
40|$|A {{computationally}} efficient {{approach to}} the automatic segmentation (labeling) of noise disturbed speech is presented. The segmentation algorithm employs short term spectrum based feature vectors and a subspace representation of the sound classes. The two sound classes of vowels and unvoiced fricatives are trained with the TIMIT acoustic phonetic continuous speech corpus. The sound class detector is applied in a speech enhancement system and for the automatic segment duration measurement. INTRODUCTION Originally this automatic sound class detection algorithm was developed as an improved replacement for the <b>speech</b> <b>pause</b> detector of a speech enhancement system [Wokurek 94]. Clearly this application requires noise robustness. Furthermore, a solution with low computational effort was sought to allow real time implementation. Representing the sound classes by subspaces meets both goals. The speech signal {{is transformed into a}} sequence of feature vectors. This transformation controls which [...] ...|$|E
40|$|ABSTRACT This paper {{describes}} an improved spectral subtraction methodby using the complementary beamforming microphone array to enhance noisy speech signals for speech recognition. The comple-mentary beamforming {{is based on}} two types of beamformers designed to obtain complementary directivity patterns with respectto each other. In this paper, it is shown that the nonlinear subtraction processing with complementary beamforming can result in akind of the spectral subtraction {{without the need for}} <b>speech</b> <b>pause</b> detection. In addition, the design of the optimization algorithmfor the directivity pattern is also described. To evaluate the effectiveness, speech enhancement experiments and speech recogni-tion experiments are performed based on computer simulations. In comparison with the optimized conventional delay-and-sum array,it is shown that the proposed array improves the signal-to-noise ratio of degraded speech by about 2 dB and performs about 10 %better in word recognition rates under heavy noisy conditions...|$|E
40|$|Speech has a {{property}} that the speech unit preceding a <b>speech</b> <b>pause</b> tends to lengthen. This work presents {{the use of}} a dynamic Bayesian network to model the prepausal lengthening effect for robust speech recognition. Specifically, we introduce two distributions to model inter-state transitions in prepausal and non-prepausal words, respectively. The selection of the transition distributions depends on a random variable whose value is influenced by whether a pause will appear between the current and the following word. Two experiments are presented here. The first one considers pauses hypothesised during speech decoding. The second one employs an extra component for speech/non-speech determination. By modelling the prepausal lengthening effect we achieve a 5. 5 % relative reduction in word error rate on the 500 -word task of the SVitchboard corpus. Index Terms â Prepausal lengthening, duration, prosody, robust speech recognition, dynamic Bayesian network...|$|E
30|$|High-quality speech without {{grammatical}} <b>speech</b> <b>pauses</b> within sentences can {{be highly}} intelligible and acceptable. But, {{as soon as the}} speech quality is less than normal, or the speech is listened in noisy conditions, the introduction of grammatical <b>speech</b> <b>pauses</b> can help to maintain intelligibility [64]. In general, âit can be observed that the contributions of prosody to speech perception become more important when the segmental quality of speech or the listening conditions become less favorableâ [64]. Pauses can be considered as âmarkers of information structureâ [71, 72]. Intonational phrases (clearly signaled by silence, {{as well as by the}} characteristic pitch movement) are required between utterances and at punctuation boundaries. Phonological phrases (signaled by silence rather by the characteristic pitch movement only) âmay be harder to place with certainty and to evaluateâ [72]. Voices with a high speaking rate combined with long utterance breaks seem to be preferable to listeners [73].|$|R
5000|$|VOX uses a [...] "hang" [...] timer, {{typically}} 1-3 seconds, {{to remain}} engagedduring brief <b>speech</b> <b>pauses.</b> This means {{the last several}} seconds ofeach transmission or recorded segment are always silence. AVOX-activated recorder can delete {{the end of each}} segment but the userof a VOX-activated half duplex radio must wait for the timer to expire before he or she can receive again.|$|R
3000|$|The {{algorithm}} behavior {{presented in}} Section 6 only describes the performance in active speech regions. The {{contribution of the}} additional noise reduction, g 2 (n), applied during <b>speech</b> <b>pauses</b> cannot be discerned from these results. However, this additional gain will reduce the noise level even further resulting in a much lower noise level compared to the SBA. In a conference phone application, a typical scenario is that parts on one side [...] "listen in" [...] to an ongoing presentation conducted by talkers on the opposite side. The extra noise reduction by g 2 (n) in <b>speech</b> <b>pauses</b> reduces annoyance from continuous noise in these situations. The modifications in this paper were motivated by artifacts from the SBA algorithm, subjectively perceived by an evaluation panel of product managers and development engineers, in total 6 persons. The improvements proposed in this paper were considered as necessary improvements to the SBA, and the proposed algorithm was implemented in a commercially available product. Especially, {{the inclusion of the}} additional gain g 2 (n) in (11) was perceived as desirable.|$|R
40|$|We {{present an}} {{efficient}} algorithm for {{the enhancement of}} speech signals which are heavily corrupted by short-time stationary, acoustically or electrically added disturbances. The algorithm is based on spectral amplitude estimation using an overlapadd FFT filter bank system. Compared to other systems, the improved performance of our speech enhancement system is achieved by {{the combination of the}} best known spectral amplitude estimators of the noisy speech signal and a new efficient and reliable noise spectrum tracker. As a result, our speech enhancement system requires no <b>speech</b> <b>pause</b> detection for noise estimation and needs only 14 % [...] 23 % of the resources of a commercially available digital signal processor. I. Introduction T HE enhancement of noisy speech is a challenging research field with applications including suppression of environmental noise in machinery halls, mobile radio communications systems, noise suppressors for automatic speech recognition systems, and hearing aids [...] . ...|$|E
40|$|This paper {{describes}} an improved spectral subtraction method {{by using the}} complementary beamforming microphone array to enhance noisy speech signals for speech recognition. The complementary beamforming is based on two types of beamformers designed to obtain complementary directivity patterns with respect to each other. In this paper, it is shown that the nonlinear subtraction processing with complementary beamforming {{can result in a}} kind of the spectral subtraction without the need for <b>speech</b> <b>pause</b> detection. In addition, the design of the optimization algorithm for the directivity pattern is also described. To evaluate the effectiveness, speech enhancement experiments and speech recognition experiments are performed based on computer simulations. In comparison with the optimized conventional delay-and-sum array, it is shown that the proposed array improves the signal-to-noise ratio of degraded speech by about 2 dB and performs about 10 % better in word recognition rates under heavy no [...] ...|$|E
40|$|Comfort noise {{insertion}} during <b>speech</b> <b>pause</b> {{has been}} ap-plied to Voice-over-IP and wireless networks for increas-ing bandwidth efficiency. We present two classified comfort noise generation (CCNG) schemes using Gaussian Mixture classifiers (GMM-C). Our first scheme employs a classified prototype background noise codebook with the prototype noise waveform chosen using a GMM-C. The second scheme utilizes a classified enhanced excitation codebook. The new CCNG algorithms provide better comfort noise during speech pauses {{and a smaller}} misclassification rate. We have retrofitted the scheme into existing speech transmission sys-tem, such as ITU-T G. 711 /Appendix II and G. 723. 1 /Annex A. The perceived quality of a voice conversation of the novel system has been noticeably enhanced for car and babble noise. For the G. 711 system, a large improvement is ob-tained for car noise while the largest amelioration is for babble noise in the G. 723. 1 system. Index Terms: Comfort Noise, Gaussian Mixture classifier, classified prototype codebook, enhanced classified excita-tion codebook, soft-decision Gaussian mixture classifier. ...|$|E
40|$|INTERSPEECH 2010 : 11 th Annual Conference of the International Speech Communication Association, September 26 - 30, 2010, Chiba, Japan. This paper {{presents}} a noise cancellation method {{based on the}} ability to efficiently cancel a close target speaker contribution from the signals observed at a microphone array. The proposed method exploits this specificity {{in the case of the}} hands-free speech interface. This method is in particular able to deal with non-stationary noise. The method can be divided in three steps. First, the steering vector pointing at the target user is estimated from the covariance of the observed signals. Then the noise estimate is obtained by cancelling the user's contribution. During this step the <b>speech</b> <b>pauses</b> are also estimated. Finally a post-filter is used to suppress this estimated noise from the observed signals. The post-filter strength is controlled by using the estimated noise during the <b>speech</b> <b>pauses</b> as reference. A 20 k-words dictation task in presence of non-stationary diffuse background noise at different SNR levels illustrates the effectiveness of the proposed method...|$|R
40|$|We {{study the}} {{prosodic}} features of simultaneous conference interpreting {{in an attempt}} to describe its particular speaking style. We focus on the temporal organisation of the interpreters' <b>speech</b> (<b>pauses,</b> <b>speech</b> rate), as well as global prosodic properties (f 0 range, melodic agitation). We also study similarity and convergence phenomena between the speaker and the interpreter, on prosodic features, and their dynamic evolution over time. The findings indicate that interpreters make longer silent pauses, less frequently than speakers and their speech rate is more variable. In most cases, interpreters had a narrower pitch range than speakers and do not mirror the pitch of their speakers...|$|R
3000|$|Thus, the {{inclusion}} of the proposed additional gain g 2 (n) leads to a reduced noise level during <b>speech</b> <b>pauses,</b> without affecting the quality of the speech. The additional gain will cause no speech distortion as the gain is constant (with value g 2 (n) = 1) during speech. Further, it does not change the spectral characteristics of the noise since all subbands are equally attenuated and the damping is changing slowly. The damping level L [...]...|$|R
