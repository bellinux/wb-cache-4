109|18|Public
5000|$|Holland's <b>schema</b> <b>theorem,</b> or the [...] "fundamental theorem {{of genetic}} algorithms" ...|$|E
5000|$|Holland {{frequently}} lectured {{around the}} world on his own research, and on research and open questions in complex adaptive systems (CAS) studies. In 1975 he wrote the ground-breaking book on genetic algorithms, [...] "Adaptation in Natural and Artificial Systems". He also developed Holland's <b>schema</b> <b>theorem.</b>|$|E
50|$|Holland's <b>schema</b> <b>theorem,</b> {{also called}} the {{fundamental}} theorem of genetic algorithms, is widely taken to be the foundation for explanations {{of the power of}} genetic algorithms. It says that short, low-order schemata with above-average fitness increase exponentially in successive generations. The theorem was proposed by John Holland in the 1970s.|$|E
40|$|Abstract: 2 ̆ 2 In {{this paper}} we {{characterize}} the well-known computational complexity classes of the polynomial time hierarchy as classes of provably recursive functions (with graphs of suitable bounded complexity) of some second order theories with weak comprehension axiom schemas {{but without any}} induction <b>schemas</b> (<b>Theorem</b> 6). We also find a natural relationship between our theories and the theories of bounded arithmetic S[superscript i]/ (Lemmas 4 and 5). Our proofs use a technique which enables us to 2 ̆ 7 speed up 2 ̆ 7 induction without increasing the bounded complexity of the induction formulas. This technique {{is also used to}} obtain an interpretability result for the theories of bounded arithmetic S[superscript i]/ (Theorem 4). 2 ̆...|$|R
5000|$|From these axiom schemas one {{can quickly}} deduce the <b>theorem</b> <b>schema</b> P→P (reflexivity of implication) {{which is used}} below: ...|$|R
40|$|In {{this paper}} a new general GP schema theory for {{headless}} chicken crossover and subtree mutation is presented. The theory gives an exact formulation for the expected number of instances of a schema {{at the next}} generation. The theory includes four main results: microscopic <b>schema</b> <b>theorems</b> for both headless chicken crossover and subtree mutation, and two corresponding macroscopic theorems. The microscopic versions are applicable to headless chicken crossovers and subtree mutation operators. The macroscopic versions are valid for slightly more restricted sets of headless chicken and mutation operators in which the probability of selecting the crossover/mutation point(s) depends only on {{the size and shape}} of the parent program(s). In the paper we provide examples which show how the theory can be specialised to specific operators. 1 Introduction Schema theories often provide information about the properties of a population at the next generation in terms of macroscopic quantitie [...] ...|$|R
50|$|The <b>schema</b> <b>theorem</b> holds {{under the}} {{assumption}} of a genetic algorithm that maintains an infinitely large population, but does not always carry over to (finite) practice: due to sampling error in the initial population, genetic algorithms may converge on schemata that have no selective advantage. This happens in particular in multimodal optimization, where a function can have multiple peaks: the population may drift to prefer one of the peaks, ignoring the others.|$|E
5000|$|... where [...] is {{the order}} of the schema, [...] is the length of the code, [...] is the {{probability}} of mutation and [...] is the probability of crossover. So a schema with a shorter defining length [...] is less likely to be disrupted.An often misunderstood point is why the <b>Schema</b> <b>Theorem</b> is an inequality rather than an equality. The answer is in fact simple: the Theorem neglects the small, yet non-zero, probability that a string belonging to the schema [...] will be created [...] "from scratch" [...] by mutation of a single string (or recombination of two strings) that did not belong to [...] in the previous generation.|$|E
5000|$|John Henry Holland {{was best}} known for his work popularizing genetic {{algorithms}} (GA), through his ground-breaking book [...] "Adaptation in Natural and Artificial Systems" [...] in 1975 and his formalization of Holland's <b>schema</b> <b>theorem.</b> In 1976, Holland conceptualized an extension of the GA concept to what he called a [...] "cognitive system", and provided the first detailed description of what would become known as the first learning classifier system in the paper [...] "Cognitive Systems based on Adaptive Algorithms". [...] This first system, named Cognitive System One (CS-1) was conceived as a modeling tool, designed to model a real system (i.e. environment) with unknown underlying dynamics using a population of human readable rules. The goal was for a set of rules to perform online machine learning to adapt to the environment based on infrequent payoff/reward (i.e. reinforcement learning) and apply these rules to generate a behavior that matched the real system. This early, ambitious implementation was later regarded as overly complex, yielding inconsistent results.|$|E
40|$|Genetic {{programming}} {{is a powerful}} heuristic search technique that is used {{for a number of}} real world applications to solve among others regression, classification, and time-series forecasting problems. A lot of progress towards a theoretic description of genetic programming in form of <b>schema</b> <b>theorems</b> has been made, but the internal dynamics and success factors of genetic programming are still not fully understood. In particular, the effects of different crossover operators in combination with offspring selection are largely unknown. This contribution sheds light on the ability of well-known GP crossover operators to create better offspring when applied to benchmark problems. We conclude that standard (sub-tree swapping) crossover is a good default choice in combination with offspring selection, and that GP with offspring selection and random selection of crossover operators can improve the performance of the algorithm in terms of best solution quality when no solution size constraints are applied. Comment: The final publication is available at [URL]...|$|R
40|$|This paper {{presents}} two new <b>schema</b> <b>theorems</b> {{in which}} expectations are absent. The first theorem provides confidence intervals {{for the number}} of instances of a schema at the next generation. The second gives a lower bound for the same quantity. 1 THEOREMS In (Poli et al. 1998) we noted that the process of propagation of a schema H from generation t to generation t + 1 {{can be seen as a}} Bernoulli trial with success probability ff(H; t), where ff(H; t) is the probability that H either survives or is created after selection, crossover and mutation. Thus, the number of instances of H at generation t + 1, m(H; t + 1), is binomially distributed. So, given ff(H; t) we can calculate exactly the probability, Prfm(H; t+ 1) xg, that the schema H will have at least x instances at generation t + 1, for any given x. Unfortunately, the result of this calculation is difficult to use (Poli 1999). One way to remove this problem is to not fully exploit our knowledge about the probability distributi [...] ...|$|R
40|$|We study if {{and when}} the {{inequality}} dp(H) ≤ rel∆(H) holds for schemas H in chromosomes that are structured as trees. The disruption probability dp(H) is the probability that a random cut of a tree limb will separate two fixed nodes of H. The relative diameter rel∆(H) is the ratio (max distance between two fixed nodes in H) / (max distance between two tree nodes), and measures how close together are the fixed nodes of H. Inequality dp(H) ≤ rel∆(H) is of significance in proving <b>Schema</b> <b>Theorems</b> for non-linear chromosomes, and so bears upon the success we can expect from genetic algorithms. For linear chromosomes, dp(H) = rel∆(H). Our results include the following. There is no constant c such that dp(H) ≤ c · rel∆(H) holds for arbitrary schemas and trees. This is illustrated in trees with eccentric, stringy shapes. Matters improve for dense, ball-like trees, explained herein. Inequality dp(H) ≤ rel∆(H) always holds in such trees, except for certain atypically large schemas. Thus, the more compact are our tree-structured chromosomes, the better we can expect our genetic algorithms to work...|$|R
5000|$|For example, {{consider}} binary {{strings of}} length 6. The schema 1*10*1 describes {{the set of}} all strings of length 6 with 1's at positions 1, 3 and 6 and a 0 at position 4. The * is a wildcard symbol, which means that positions 2 and 5 can have a value of either 1 or 0. The order of a schema [...] {{is defined as the}} number of fixed positions in the template, while the defining length [...] is the distance between the first and last specific positions. The order of 1*10*1 is 4 and its defining length is 5. The fitness of a schema is the average fitness of all strings matching the schema. The fitness of a string {{is a measure of the}} value of the encoded problem solution, as computed by a problem-specific evaluation function. Using the established methods and genetic operators of genetic algorithms, the <b>schema</b> <b>theorem</b> states that short, low-order schemata with above-average fitness increase exponentially in successive generations. Expressed as an equation: ...|$|E
5000|$|Although Barricelli, in work he {{reported}} in 1963, had simulated {{the evolution of}} ability to play a simple game, artificial evolution became a widely recognized optimization method {{as a result of}} the work of Ingo Rechenberg and Hans-Paul Schwefel in the 1960s and early 1970s [...] - [...] Rechenberg's group was able to solve complex engineering problems through evolution strategies. [...] Another approach was the evolutionary programming technique of Lawrence J. Fogel, which was proposed for generating artificial intelligence. Evolutionary programming originally used finite state machines for predicting environments, and used variation and selection to optimize the predictive logics. Genetic algorithms in particular became popular through the work of John Holland in the early 1970s, and particularly his book Adaptation in Natural and Artificial Systems (1975). His work originated with studies of cellular automata, conducted by Holland and his students at the University of Michigan. Holland introduced a formalized framework for predicting the quality of the next generation, known as Holland's <b>Schema</b> <b>Theorem.</b> Research in GAs remained largely theoretical until the mid-1980s, when The First International Conference on Genetic Algorithms was held in Pittsburgh, Pennsylvania.|$|E
40|$|Due to an {{approximation}} error the <b>schema</b> <b>theorem</b> implies a wrong estimate for {{the frequency of}} instances of a schema. In this article an example is given for which the <b>schema</b> <b>theorem</b> gives a wrong estimate. Based on a modeling which allows a mathematical analysis of genetic algorithms, the <b>schema</b> <b>theorem</b> is revised and a corrected estimate is shown. ...|$|E
2500|$|This <b>theorem</b> <b>schema</b> {{is itself}} a {{restricted}} form of comprehension, which avoids Russell's paradox because of the requirement that C be a set. Then specification for sets themselves can be written as a single axiom ...|$|R
2500|$|In von Neumann–Bernays–Gödel set theory, a {{distinction}} is made between sets and classes. A class C {{is a set}} {{if and only if}} it belongs to some class E. In this theory, there is a <b>theorem</b> <b>schema</b> that reads ...|$|R
50|$|Under the Curry-Howard correspondence, {{the above}} {{conversion}} {{process for the}} deduction meta-theorem {{is analogous to the}} conversion process from lambda calculus terms to terms of combinatory logic, where axiom 1 corresponds to the K combinator, and axiom 2 corresponds to the S combinator. Note that the I combinator corresponds to the <b>theorem</b> <b>schema</b> P→P.|$|R
40|$|We {{analyze the}} <b>schema</b> <b>theorem</b> and the {{building}} block hypothesis using a recently derived, exact schemata evolution equation. We derive a new <b>schema</b> <b>theorem</b> based {{on the concept of}} e#ective fitness showing that schemata of higher than average e#ective fitness receive an exponentially increasing number of trials over time...|$|E
40|$|Holland's <b>Schema</b> <b>Theorem</b> {{is widely}} {{taken to be}} the {{foundation}} for explanations {{of the power of}} genetic algorithms (GAs). Yet some dissent has been expressed as to its implications. Here, dissenting arguments are reviewed and elaborated upon, explaining why the <b>Schema</b> <b>Theorem</b> has no implications for how well a GA is performing. Interpretations of the <b>Schema</b> <b>Theorem</b> have implicitly assumed that a correlation exists between parent and offspring fitnesses, and this assumption is made explicit in results based on Price's Covariance and Selection Theorem. Schemata do not play a part in the performance theorems derived for representations and operators in general. However, schemata re-emerge when recombination operators are used. Using Geiringer's recombination distribution representation of recombination operators, a "missing" <b>schema</b> <b>theorem</b> is derived which makes explicit the intuition for when a GA should perform well. Finally, the method of "adaptive landscape" analysis is exa [...] ...|$|E
40|$|This paper {{develops}} {{the theory that}} can enable the design of genetic algorithms and choose the parameters such {{that the proportion of}} the best building blocks grow. A practical <b>schema</b> <b>theorem</b> has been used for this purpose and its ramification for the choice of selection operator and parameterization of the algorithm is explored. In particular stochastic universal selection, tournament selection, and truncation selection schemes are employed to verify the results. Results agree with the <b>schema</b> <b>theorem</b> and indicate that it must be obeyed in order to ascertain sustained growth of good building blocks. The analysis suggests that <b>schema</b> <b>theorem</b> alone is insufficient to guarantee the success of a selectorecombinative genetic algorithm...|$|E
25|$|Thus, one may {{use this}} as an axiom schema {{in the place of}} the axioms of empty set and pairing. Normally, however, one uses the axioms of empty set and pairing separately, and then proves this as a <b>theorem</b> <b>schema.</b> Note that {{adopting}} this as an axiom schema will not replace the axiom of union, which is still needed for other situations.|$|R
50|$|If {{separation}} is not postulated as an axiom schema, but derived as a <b>theorem</b> <b>schema</b> from the schema of replacement (as is sometimes done), {{the situation is}} more complicated, and depends on the exact formulation of the replacement schema. The formulation used in the axiom schema of replacement article only allows to construct the image Fa when a is contained {{in the domain of}} the class function F; then the derivation of separation requires the axiom of empty set. On the other hand, the constraint of totality of F is often dropped from the replacement schema, in which case it implies the separation schema without using the axiom of empty set (or any other axiom for that matter).|$|R
50|$|Suppose that we {{have that}} Γ and H prove C, and we wish to show that Γ proves H→C. For each step S in the {{deduction}} which is a premise in Γ (a reiteration step) or an axiom, we can apply modus ponens to the axiom 1, S→(H→S), to get H→S. If the step is H itself (a hypothesis step), we apply the <b>theorem</b> <b>schema</b> to get H→H. If the step is the result of applying modus ponens to A and A→S, we first make sure that these have been converted to H→A and H→(A→S) and then we take the axiom 2, (H→(A→S))→((H→A)→(H→S)), and apply modus ponens to get (H→A)→(H→S) and then again to get H→S. At the end of the proof we will have H→C as required, except that now it only depends on Γ, not on H. So the deduction step will disappear, consolidated into the previous step which was the conclusion derived from H.|$|R
40|$|Due to an {{approximation}} error the <b>schema</b> <b>theorem</b> implies a wrong estimate for {{the frequency of}} instances of a schema. In this article an example is given for which the <b>schema</b> <b>theorem</b> gives a wrong estimate. Based on a modeling which allows a mathematical analysis of genetic algorithms, the <b>schema</b> <b>theorem</b> is revised and a corrected estimate is shown. 1 Introduction Today evolutionary algorithms and especially genetic algorithms {{are known to be}} efficient in many problem domains. But up to now a gap between theoretical foundations and empirical studies about their behavior and quality exists. This is displeasing since they are based on a solid grounding of mathematical theory which had been suppressed as their practical application indicates their benefit on various problem domains. Even if this gap cannot be filled within the next years it seems to be essential to build a solid grounding on which newer and better results can be achieved. In this article the <b>schema</b> <b>theorem</b> and the derive [...] ...|$|E
40|$|This paper {{presents}} new theoretical {{results on}} GA and GP schemata which allow one to predict with a known probability whether {{the number of}} instances of a given schema at the next generation is going to be above a given threshold. Unlike previous results, the schema theory presented in this paper does not use expected values and accounts for schema creation. 1 Introduction Many people in the evolutionary computation field consider the <b>schema</b> <b>theorem</b> as a useless tool for predicting the behaviour of a GA over multiple generations. One {{reason for this is that}} the <b>schema</b> <b>theorem</b> gives only a lower bound for the expected value of the number of instances of a given schema at the next generation as a function of quantities characterising the schema at the previous generation. Therefore, it is not possible to use the <b>schema</b> <b>theorem</b> recursively to predict accurately the behaviour of a genetic 1 algorithm over multiple generations. In addition, since the <b>schema</b> <b>theorem</b> provides only a l [...] ...|$|E
40|$|For {{years the}} <b>schema</b> <b>theorem</b> {{has been a}} {{principal}} mathematical foundation for Genetic Algorithms. This article briefly describes a trade-off involving crossover and mutation with respect to schema length. The one-max function, a variant of it, and finding snakes in hypercubes are used to demonstrate this trade-off, looking at the impact on solution quality solution and number of function evaluations required when the <b>schema</b> <b>theorem</b> trade-off is violated. 1...|$|E
40|$|In {{a recent}} paper, Timothy Williamson [4] makes the {{following}} passing remark. 2 The principle that every truth is possibly necessary {{can now be}} shown to entail that every truth is necessary by a chain of elementary inferences in a perspicuous notation unavailable to Hegel. In {{the early part of}} this note, I will understand the modal triviality Williamson has in mind as the trivialization of certain propositional modal logics by the addition of �p Ñ ∼□∼□p � as an axiom schema. By trivialization, I mean that the logic (expanded in this way) has as a <b>theorem</b> <b>schema</b> �p Ñ □p�. 3 In the final section of this note, I will look at the case in which we add another modal operator (⋄) to the language, and we consider adding an axiom schema of the form �p Ñ ⋄□p�. This is a distinct problem (in general), since (presumably) some modal logicians out there will be inclined to reject the interchangeability of �⋄p � and �∼□∼p�. We’ll see that without this assumption, much of the triviality’s generality is lost (independently of what other modal axioms are presen...|$|R
40|$|Research has {{demonstrated}} that instruction that relies heavily on studying worked examples is more effective for less experienced learners compared to instruction emphasizing problem solving. However, the guidance associated with studying some worked examples may reduce the performance of more experienced learners. This study investigated categories of guidance using geometry worked examples. Three conditions were used. In the theorem and step guidance condition, students were provided with the solution steps required to reach the answer and the theorems used to justify the steps. In the step guidance condition, learners were only provided with the sequence of steps needed to reach the answer {{but not with the}} theorems explaining the steps. The problem-solving condition required learners to solve problems without any guidance. It was hypothesized that for students who had already learned the relevant theorems, the major task was to learn to recognize problem states and their associated solution moves. The step guidance condition should best facilitate such knowledge, compared to a problem-solving or a theorem and step guidance approach. For students who had not yet fully learned the theorems, the theorem and step guidance approach should be superior. Two geometry instruction experiments supported these hypotheses. Information concerning theorems should only be provided if students have yet to learn and automate <b>theorem</b> <b>schemas...</b>|$|R
40|$|Research has {{demonstrated}} that instruction that relies heavily on worked examples is more effective for novices as opposed to instruction consisting of problem-solving. However, excessive guidance for expert learners may reduce their performance. This study investigated optimal degrees of guidance using geometry worked examples. Three conditions were used. In the Theorem & Step Guidance condition, students were told the steps to find each angle, {{the measure of the}} angle, and the theorem used to justify the answer. In the Step Guidance condition, learners were told the sequence of steps needed to reach the answer, but not told the theorem required to make a step. The problem solving condition required learners to solve problems with no guidance. It was hypothesized that by using Step Guidance, a new concept could be more readily incorporated into existing knowledge held in long-term memory compared to a Problem Solving approach or a Theorem & Step Guidance approach. Problem Solving would impose the heavy cognitive load associated with problem solving search while providing information concerning well-known theorems would be redundant. In other words, as long recognised by cognitive load theory, most students need to learn to recognise problem states and the moves associated with those states and this information is provided by Step Guidance without additional, redundant information. A series of geometry instruction experiments supported these hypotheses. The results of these experiments revealed that for students who already understand the relevant theorems, learning to solve problems primarily consists of learning to recognise problem states and their associated moves. Information concerning theorems only should be provided if students have yet to learn and automatise <b>theorem</b> <b>schemas...</b>|$|R
40|$|Holland’s <b>Schema</b> <b>Theorem</b> {{is widely}} {{taken to be}} the {{foundation}} for explanations {{of the power of}} genetic algorithms (GAs). Yet some dissent has been expressed as to its implications. Here, dissenting arguments are reviewed and elaborated upon, explaining why the <b>Schema</b> <b>Theorem</b> has no implications for how well a GA is performing. Interpretations of the <b>Schema</b> <b>Theorem</b> have implicitly assumed that a correlation exists between parent and offspring fitnesses, and this assumption is made explicit in results based on Price’s Covariance and Selection Theorem. Schemata do not play a part in the performance theorems derived for representations and operators in general. However, schemata re-emerge when recombination operators are used. Using Geiringer’s recombination distribution representation of recombination operators, a “missing ” <b>schema</b> <b>theorem</b> is derived which makes explicit the intuition for when a GA should perform well. Finally, the method of “adaptive landscape ” analysis is examined and counterexamples offered to the commonly used correlation statistic. Instead, an alternative statistic — the transmission function in the fitness domain — is proposed as the optimal statistic for estimating GA performance from limited samples. ...|$|E
40|$|In {{this paper}} we present {{two forms of}} <b>schema</b> <b>theorem</b> in which {{expectations}} are not present. These theorems allow one to predict with a known probability {{whether the number of}} instances of a schema at the next generation will be above a given threshold. Then we clarify that in the presence of stochasticity schema theorems should be interpreted as conditional statements and we use a conditional version of <b>schema</b> <b>theorem</b> backwards to predict the past from the future. Assuming that at least x instances of a schema are present in one generation, this allows us to find the conditions (at the previous generation) under which such x instances will indeed be present with a given probability. This suggests a possible strategy to study GA convergence based on schemata. We use this strategy to obtain a recursive version of the <b>schema</b> <b>theorem.</b> Among other uses, this <b>schema</b> <b>theorem</b> allows one to find under which conditions on the initial generation a GA will converge to a solution on the h [...] ...|$|E
40|$|In {{this paper}} we first {{develop a new}} form of <b>schema</b> <b>theorem</b> in which {{expectations}} are not present. This theorem allows one to predict with a known probability whether the number of instances of a schema at the next generation will be above a given threshold. Then we use this version of the <b>schema</b> <b>theorem</b> backwards, i. e. to predict the past from the future. Assuming that at least one solution is found at one generation, this allows us to find the conditions (at the previous generation) under which such a solution will indeed be found with a given probability. This allows us to obtain a recursive version of the <b>schema</b> <b>theorem.</b> This <b>schema</b> <b>theorem</b> allows one to find under which conditions on the initial generation the GA will converge to a solution on the hypothesis that building block and population fitnesses are known. These results are important because for the first time they make explicit the relation between population size, schema fitness and probability of convergence [...] ...|$|E
40|$|For Artificial Life {{applications}} it {{is useful}} to extend Genetic Algorithms from a finite search space with fixed-length genotypes to open-ended evolution with variable-length genotypes. A new theoretical analysis is required, as Holland's <b>Schema</b> <b>Theorem</b> only applies to fixed lengths. It will be argued, using concepts of epistasis and fitness landscapes drawn from theoretical biology, {{that in the long run}} a population must have genotypes of nearly equal length, and this length can only increase slowly. As the length increases, the population will be nearly converged, and hence evolving as a species. 1 Introduction Genetic algorithms (GAs) are a form of search technique, primarily used for function optimization, modelled on Darwinian evolution. Some basic knowledge of GAs, is assumed for the purposes of this paper; the best introduction is (Goldberg 1989). Holland's <b>Schema</b> <b>Theorem</b> has provided the theoretical underpinning for GAs (Holland 1975, Goldberg 1989); this <b>Schema</b> <b>Theorem</b> assum [...] ...|$|E
40|$|Abstract. By {{applying}} the <b>schema</b> <b>theorem,</b> we study {{the effects of}} crossover in Genetic Algorithms with the multiplicative fitness function. On this landscape, the analytical expression of the exact <b>schema</b> <b>theorem</b> can be obtained, and this makes it possible to carry out the mathematical investigation of genetic operators. We consider the average fitness under the action of selection, mutation and crossover. To do this, we give the expressions for the average and variance of fitness in terms of schema frequencies. The theoretical results are compared with numerical experiments. ...|$|E
40|$|Holland’s <b>schema</b> <b>theorem</b> (an inequality) may {{be viewed}} as an attempt to {{understand}} genetic search in terms of a coarse graining of the state space. Stephens and Wael-broeck developed that perspective, sharpening the <b>schema</b> <b>theorem</b> to an equality. Of particular interest is a “form invariance ” of their equations; the form is unchanged by the degree of coarse graining. This paper establishes a similar form invariance for the more general model of Vose et al. and uses the attendant machinery as a springboard for an interpretation and discussion of implicit parallelism...|$|E
