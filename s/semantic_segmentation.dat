694|185|Public
5000|$|Best Paper Honorable Mention: Fully Convolutional Networks for <b>Semantic</b> <b>Segmentation,</b> Jonathan Long, Evan Shelhamer, Trevor Darrell ...|$|E
40|$|Abstract — Dense <b>semantic</b> <b>segmentation</b> of 3 D point clouds is a {{challenging}} task. Many approaches deal with 2 D <b>semantic</b> <b>segmentation</b> and can obtain impressive results. With {{the availability of}} cheap RGB-D sensors the field of indoor <b>semantic</b> <b>segmentation</b> has {{seen a lot of}} progress. Still it remains unclear how to deal with 3 D <b>semantic</b> <b>segmentation</b> in the best way. We propose a novel 2 D- 3 D label transfer based on Bayesian updates and dense pairwise 3 D Conditional Random Fields. This approach allows us to use 2 D semantic segmentations to create a consistent 3 D semantic reconstruction of indoor scenes. To this end, we also propose a fast 2 D <b>semantic</b> <b>segmentation</b> approach based on Randomized Decision Forests. Furthermore, we show that it is not needed to obtain a <b>semantic</b> <b>segmentation</b> for every frame in a sequence in order to create accurate semantic 3 D reconstructions. We evaluate our approach on both NYU Depth datasets and show that we can obtain a significant speed-up compared to other methods. I...|$|E
40|$|<b>Semantic</b> <b>segmentation</b> {{was seen}} as a {{challenging}} computer vision problem few years ago. Due to recent advancements in deep learning, relatively accurate solutions are now possible for its use in automated driving. In this paper, the <b>semantic</b> <b>segmentation</b> problem is explored from the perspective of automated driving. Most of the current <b>semantic</b> <b>segmentation</b> algorithms are designed for generic images and do not incorporate prior structure and end goal for automated driving. First, the paper begins with a generic taxonomic survey of <b>semantic</b> <b>segmentation</b> algorithms and then discusses how it fits in the context of automated driving. Second, the particular challenges of deploying it into a safety system which needs high level of accuracy and robustness are listed. Third, different alternatives instead of using an independent <b>semantic</b> <b>segmentation</b> module are explored. Finally, an empirical evaluation of various <b>semantic</b> <b>segmentation</b> architectures was performed on CamVid dataset in terms of accuracy and speed. This paper is a preliminary shorter version of a more detailed survey which is work in progress. Comment: To appear in IEEE ITSC 201...|$|E
40|$|It is well {{accepted}} that image segmentation {{can benefit from}} utilizing multilevel cues. The paper focuses on utilizing the FCNN-based dense semantic predictions in the bottom-up image segmentation, arguing to take semantic cues into account from the very beginning. By this we can avoid merging regions of similar appearance but distinct semantic categories as possible. The semantic inefficiency problem is handled. We also propose a straightforward way to use the contour cues to suppress the noise in multilevel cues, thus to improve the segmentation robustness. The evaluation on the BSDS 500 shows that we obtain the competitive region and boundary performance. Furthermore, since all individual regions can be assigned with appropriate semantic labels during the computation, we are capable of extracting the adjusted <b>semantic</b> <b>segmentations.</b> The experiment on Pascal VOC 2012 shows our improvement to the original <b>semantic</b> <b>segmentations</b> which derives directly from the dense predictions...|$|R
40|$|International audienceThe {{ability to}} predict and {{therefore}} to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e. g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we introduce the novel task of predicting <b>semantic</b> <b>segmentations</b> of future frames. Given a sequence of video frames, {{our goal is to}} predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Prediction results up to half a second in the future are visually convincing and are much more accurate than those of a baseline based on warping <b>semantic</b> <b>segmentations</b> using optical flow...|$|R
40|$|This paper {{proposes a}} CNN cascade for <b>semantic</b> part <b>segmentation</b> guided by pose-specifc {{information}} encoded {{in terms of}} a set of landmarks (or keypoints). There is large amount of prior work on each of these tasks separately, yet, {{to the best of our}} knowledge, this is the first time in literature that the interplay between pose estimation and <b>semantic</b> part <b>segmentation</b> is investigated. To address this limitation of prior work, in this paper, we propose a CNN cascade of tasks that firstly performs landmark localisation and then uses this information as input for guiding <b>semantic</b> part <b>segmentation.</b> We applied our architecture to the problem of facial part segmentation and report large performance improvement over the standard unguided network on the most challenging face datasets. Testing code and models will be published online at [URL]...|$|R
40|$|We {{propose a}} novel {{approach}} to <b>semantic</b> <b>segmentation</b> using weakly supervised labels. In traditional fully supervised methods, superpixel labels are available for training; however, {{it is not easy}} to obtain enough labeled superpixels to learn a satisfying model for <b>semantic</b> <b>segmentation.</b> By contrast, only image-level labels are necessary in weakly supervised methods, which makes them more practical in real applications. In this paper we develop a new way of evaluating classification models for <b>semantic</b> <b>segmentation</b> given weekly supervised labels...|$|E
40|$|<b>Semantic</b> <b>segmentation</b> is one {{of classic}} {{computer}} vision problems and strong tool for machine processing {{and understanding of the}} scene. In this thesis we use <b>semantic</b> <b>segmentation</b> in mountainous environment. The main motivation of this work is to use <b>semantic</b> <b>segmentation</b> for automatic location of geographic position, where the picture was taken. In this thesis we evaluated actual methods of <b>semantic</b> <b>segmentation</b> and we chose three of them  that are appropriate for adapting to mountainous environment. We split the dataset with mountainous environment into validation, train and test sets to use for training of chosen <b>semantic</b> <b>segmentation</b> methods. We trained models from chosen methods on mountainous data. We let segments from the best trained models get evaluated in electronic survey by respondents and we evaluated these segments in process of camera orientation estimation. We showed that chosen methods of <b>semantic</b> <b>segmentation</b> are possible to use in mountainous environment. Our models are trained on 11, 5 or 4 mountainous classes and the best of them achieve on 4 class mean IU 57. 4 %. Models are usable in practise. We show it by their deployment as a part of camera orientation estimation process...|$|E
40|$|Augmenting RGB {{data with}} {{measured}} depth {{has been shown}} to improve the performance of a range of tasks in computer vision including object detection and <b>semantic</b> <b>segmentation.</b> Although depth sensors such as the Microsoft Kinect have facilitated easy acquisition of such depth information, the vast majority of images used in vision tasks do not contain depth information. In this paper, we show that augmenting RGB images with estimated depth can also improve the accuracy of both object detection and <b>semantic</b> <b>segmentation.</b> Specifically, we first exploit the recent success of depth estimation from monocular images and learn a deep depth estimation model. Then we learn deep depth features from the estimated depth and combine with RGB features for object detection and <b>semantic</b> <b>segmentation.</b> Additionally, we propose an RGB-D <b>semantic</b> <b>segmentation</b> method which applies a multi-task training scheme: semantic label prediction and depth value regression. We test our methods on several datasets and demonstrate that incorporating information from estimated depth improves the performance of object detection and <b>semantic</b> <b>segmentation</b> remarkably. Comment: 14 pages. Accepted to IEEE T. Image Processin...|$|E
40|$|In {{this paper}} we {{formulate}} {{the task of}} <b>semantic</b> image <b>segmentation</b> as a manifold embedding problem and solve it using graph Laplacian approximation. This allows for unsupervised learning of graph Laplacian parameters individually for each image without using any prior information. We perform experiments on GrabCut, Graz and Pascal datasets. At a low computational cost proposed learning method shows comparable performance to choosing the parameters on the test set. Our framework for <b>semantic</b> image <b>segmentation</b> shows better performance than the standard discrete CRF with graph-cut inference...|$|R
40|$|Video shot {{segmentation}} is {{a preliminary}} process used in video content analysis which requires for content description. Conventional shot segmentation techniques nourished with statistical approaches depend on chromatic distributions of video frames. However, these conventional approaches lagging behind in performing <b>semantic</b> shot <b>segmentation</b> where more intuitive and perceptive for human understanding and search. This paper presents a novel mechanism which capable of perform <b>semantic</b> shot <b>segmentation</b> without additional computational and description cost for semantic video content depiction process. This mechanism bypasses the traditional video shot segmentation which is normally performs {{at the beginning}} of video content depiction and instead it perform meaningful shot boundary identification at the end {{based on the results of}} video visual concept classification. This is a positive starting point to perform structuring of video content using semantic units of shots rather than abrupt discontinued shots. Further, this paper introduces a search mechanism empowered by <b>semantic</b> shot <b>segmentation...</b>|$|R
30|$|An {{efficient}} and compact solution for solving the <b>semantic</b> road <b>segmentation</b> {{problem has been}} presented. By coalescing different types of convolutional layers and stacking them in a deep residual network style, we achieve the high-quality results on the <b>semantic</b> road <b>segmentation</b> with relatively small model size, surpassing the existing state-of-the-art methods. In the future, {{we would like to}} examine the performance of our RCC-Net on the boarder problems, such as medical images and other challenging image segmentation dataset, for understanding its capabilities to solve more general segmentation applications.|$|R
40|$|The {{importance}} and demands of visual scene understanding have been steadily increasing {{along with the}} active development of autonomous systems. Consequently, {{there has been a}} large amount of research dedicated to <b>semantic</b> <b>segmentation</b> and dense motion estimation. In this paper, we propose a method for jointly estimating optical flow and temporally consistent <b>semantic</b> <b>segmentation,</b> which closely connects these two problem domains and leverages each other. <b>Semantic</b> <b>segmentation</b> provides information on plausible physical motion to its associated pixels, and accurate pixel-level temporal correspondences enhance the accuracy of <b>semantic</b> <b>segmentation</b> in the temporal domain. We demonstrate the benefits of our approach on the KITTI benchmark, where we observe performance gains for flow and segmentation. We achieve state-of-the-art optical flow results, and outperform all published algorithms by a large margin on challenging, but crucial dynamic objects. Comment: 14 pages, Accepted for CVRSUAD workshop at ECCV 201...|$|E
40|$|<b>Semantic</b> <b>segmentation</b> for {{synthetic}} aperture radar (SAR) imagery is a rarely touched area, due to the specific image characteristics of SAR images. In this research, we propose a dataset which consists of three data sources: TerraSAR-X images, Google Earth images and OpenStreetMap data, {{with the purpose of}} performing SAR and optical image <b>semantic</b> <b>segmentation.</b> By using fully convolutional networks and deep residual networks with pre-trained weights, we investigate the accuracy and mean IOU values of <b>semantic</b> <b>segmentation</b> for both SAR and optical image patches. The best segmentation accuracy results for SAR and optical data are around 60...|$|E
40|$|Contextual {{information}} {{is crucial for}} <b>semantic</b> <b>segmentation.</b> However, finding the optimal trade-off between keeping desired fine details {{and at the same}} time providing sufficiently large receptive fields is non trivial. This is even more so, when objects or classes present in an image significantly vary in size. Dilated convolutions have proven valuable for <b>semantic</b> <b>segmentation,</b> because they allow to increase the size of the receptive field without sacrificing image resolution. However, in current state-of-the-art methods, dilation parameters are hand-tuned and fixed. In this paper, we present an approach for learning dilation parameters adaptively per channel, consistently improving <b>semantic</b> <b>segmentation</b> results on street-scene datasets like Cityscapes and Camvid. Comment: GCPR 201...|$|E
40|$|In soccer match, presenters or {{commentators}} {{are needed}} to help the audiences understanding the match more clearly. For better visual effect, we design an Augmented Virtual Presenter System which can integrate presenter’s image into the soccer field in match video. In fact, it is a scene composition process. In this paper, we will illustrate the structure and features of this system, and propose several solutions for the key problems. For scene composition, we design an algorithm consisting of automatic matting, localization, and occlusion processing. For occlusion problem, we propose a mixture solution including interactive and <b>semantic</b> <b>segmentations</b> for different scenarios...|$|R
40|$|Abstract. We {{have been}} working on the {{integration}} of video with supplementary documents, such as cooking programs. We propose an integration system that performs <b>semantic</b> <b>segmentations</b> of video and text and associates them together. This association is realized using the ordinal restriction of the recipe, cooccurrences of words in the text and the audio in the video, and the relation between the background in a video and words which describe the situation in a text. In this paper, we will introduce the result of an evaluation experiment and show the effectiveness of the proposed integration method. Through our method, many applications should become possible, such as a cooking navigation software...|$|R
30|$|Moreover, {{one popular}} {{application}} of clustering algorithms is image segmentation. To evaluate {{the performances of}} <b>semantic</b> image <b>segmentation,</b> one widely used measure is the covering rate (CR) (Richardson and Green [1997]), by whcih a larger CR value indicates a better performance.|$|R
40|$|We {{present an}} {{end-to-end}} trainable deep {{convolutional neural network}} (DCNN) for <b>semantic</b> <b>segmentation</b> with built-in awareness of semantically meaningful boundaries. <b>Semantic</b> <b>segmentation</b> is a fundamental remote sensing task, and most state-of-the-art methods rely on DCNNs as their workhorse. A major reason for their success is that deep networks learn to accumulate contextual information over very large windows (receptive fields). However, this success comes at a cost, since the associated loss of effecive spatial resolution washes out high-frequency details and leads to blurry object boundaries. Here, we propose to counter this effect by combining <b>semantic</b> <b>segmentation</b> with semantically informed edge detection, thus making class-boundaries explicit in the model, First, we construct a comparatively simple, memory-efficient model by adding boundary detection to the Segnet encoder-decoder architecture. Second, we also include boundary detection in FCN-type models and set up a high-end classifier ensemble. We show that boundary detection significantly improves <b>semantic</b> <b>segmentation</b> with CNNs. Our high-end ensemble achieves > 90 % overall accuracy on the ISPRS Vaihingen benchmark...|$|E
40|$|Accurate {{semantic}} {{labeling of}} image pixels {{is difficult because}} intra-class variability is often greater than inter-class variability. In turn, fast <b>semantic</b> <b>segmentation</b> is hard because accurate models are usually too complicated to also run quickly at test-time. Our experience with building and running <b>semantic</b> <b>segmentation</b> systems has also shown a reasonably obvious bottleneck on model complexity, imposed by small training datasets. We therefore propose two simple complementary strategies that leverage context to give better <b>semantic</b> <b>segmentation,</b> while scaling up or down to train on different-sized datasets. As easy modifications for existing <b>semantic</b> <b>segmentation</b> algorithms, we introduce Decorrelated Semantic Texton Forests, and the Context Sensitive Image Level Prior. The proposed modifications are tested using a Semantic Texton Forest (STF) system, and the modifications are validated on two standard benchmark datasets, MSRC- 21 and PascalVOC- 2010. In Python based comparisons, our system is insignificantly slower than STF at test-time, yet produces superior semantic segmentations overall, with just push-button training. Comment: Supplementary material {{can be found at}} [URL]...|$|E
40|$|This paper {{proposes a}} {{convolutional}} neural network that can fuse high-level prior for semantic image segmentation. Motivated by humans' vision recognition system, our key design is a three-layer generative structure consisting of high-level coding, middle-level segmentation and low-level image to introduce global prior for <b>semantic</b> <b>segmentation.</b> Based on this structure, we proposed a generative model called conditional variational auto-encoder (CVAE) that can build up the links behind these three layers. These important links include an image encoder that extracts high level info from image, a segmentation encoder that extracts high level info from segmentation, and a hybrid decoder that outputs <b>semantic</b> <b>segmentation</b> from the high level prior and input image. We theoretically derive the <b>semantic</b> <b>segmentation</b> as an optimization problem parameterized by these links. Finally, the optimization problem enables us {{to take advantage of}} state-of-the-art fully convolutional network structure for the implementation of the above encoders and decoder. Experimental results on several representative datasets demonstrate our supreme performance for <b>semantic</b> <b>segmentation.</b> Comment: 9 page...|$|E
40|$|International audienceThis paper {{presents}} an upper-body detection algorithm that extends classical shape-based detectors {{through the use}} of additional <b>semantic</b> colour <b>segmentation</b> cues. More precisely, candidate upper-body image patches produced by a base detector are soft-segmented using a multi-class probabilistic colour segmentation algorithm that leverages spatial as well as colour prior distributions for different semantic object regions (skin, hair, clothing, background). These multi-class soft segmentation maps are then classified as true or false upper-bodies. By further fusing the score of this latter classifier with the base detection score, the method shows a performance improvement on three different public datasets and using two different upper-body base detectors, demonstrating the complementarity of the contextual <b>semantic</b> colour <b>segmentation</b> and the base detector...|$|R
40|$|In this demo, {{a system}} for the {{semantic}} annotation of images is presented. The proposed knowledge-assisted analysis architecture comprises algorithms that perform <b>semantic</b> image <b>segmentation,</b> region-level classification and fuzzy reasoning; hence, it constitutes a complete {{solution to the problem}} of image annotation based on semantic criteria. 1...|$|R
40|$|A key {{requirement}} for leveraging supervised deep learning methods is {{the availability of}} large, labeled datasets. Unfortunately, {{in the context of}} RGB-D scene understanding, very little data is available [...] current datasets cover a small range of scene views and have limited semantic annotations. To address this issue, we introduce ScanNet, an RGB-D video dataset containing 2. 5 M views in 1513 scenes annotated with 3 D camera poses, surface reconstructions, and <b>semantic</b> <b>segmentations.</b> To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. We show that using this data helps achieve state-of-the-art performance on several 3 D scene understanding tasks, including 3 D object classification, semantic voxel labeling, and CAD model retrieval. The dataset is freely available at [URL]...|$|R
40|$|Traditional Scene Understanding {{problems}} such as Object Detection and <b>Semantic</b> <b>Segmentation</b> have made breakthroughs in recent years due {{to the adoption of}} deep learning. However, the former task is not able to localise objects at a pixel level, and the latter task has no notion of different instances of objects of the same class. We focus on the task of Instance Segmentation which recognises and localises objects down to a pixel level. Our model is based on a deep neural network trained for <b>semantic</b> <b>segmentation.</b> This network incorporates a Conditional Random Field with end-to-end trainable higher order potentials based on object detector outputs. This allows us to reason about instances from an initial, category-level <b>semantic</b> <b>segmentation.</b> Our simple method effectively leverages the great progress recently made in <b>semantic</b> <b>segmentation</b> and object detection. The accurate instance-level segmentations that our network produces is reflected by the considerable improvements obtained over previous work. Comment: British Machine Vision Conference (BMVC) 201...|$|E
40|$|We {{propose a}} {{semantic}} scene understanding {{system that is}} suitable for real robotic operations. The system solves different tasks (<b>semantic</b> <b>segmentation</b> and object detections) in an opportunistic and distributed fashion but still allows communication between modules to improve their respective performances. We propose {{the use of the}} semantic space to improve specific out-of-the-box object detectors and an update model to take the evidence from different detection into account in the <b>semantic</b> <b>segmentation</b> process. Our proposal is evaluated with the KITTI dataset, on the object detection benchmark and on five different sequences manually annotated for the <b>semantic</b> <b>segmentation</b> task, demonstrating the efficacy of our approach. Cesar Cadena, Anthony Dick and Ian D. Rei...|$|E
30|$|Recently, {{a number}} of <b>semantic</b> <b>segmentation</b> methods have been {{proposed}} {{that are based on}} FCNs [6, 7].|$|E
40|$|Deep {{convolutional}} {{neural networks}} (DCNNs) trained {{on a large}} number of images with strong pixel-level anno-tations have recently significantly pushed the state-of-art in <b>semantic</b> image <b>segmentation.</b> We study the more challeng-ing problem of learning DCNNs for semantic image seg-mentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a com-bination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets. We develop Expectation-Maximization (EM) methods for <b>semantic</b> im-age <b>segmentation</b> model training under these weakly super-vised and semi-supervised settings. Extensive experimental evaluation shows that the proposed techniques can learn models delivering competitive results on the challenging PASCAL VOC 2012 image segmentation benchmark, while requiring significantly less annotation effort. We share source code implementing the proposed system a...|$|R
40|$|We {{trained and}} applied an encoder-decoder model to semantically segment breast biopsy images into biologically {{meaningful}} tissue labels. Since conventional encoder-decoder networks cannot be applied directly on large biopsy images {{and the different}} sized structures in biopsies present novel challenges, we propose four modifications: (1) an input-aware encoding block to compensate for information loss, (2) a new dense connection pattern between encoder and decoder, (3) dense and sparse decoders to combine multi-level features, (4) a multi-resolution network that fuses the results of encoder-decoders run on different resolutions. Our model outperforms a feature-based approach and conventional encoder-decoders from the literature. We use <b>semantic</b> <b>segmentations</b> produced with our model in an automated diagnosis task and obtain higher accuracies than a baseline approach that employs an SVM for feature-based segmentation, both using the same segmentation-based diagnostic features. Comment: Added more WSI images in appendi...|$|R
40|$|International audienceThis paper {{presents}} GridNet, a new Convolutional Neural Network (CNN) architecture for <b>semantic</b> image <b>segmentation</b> (full scene labelling). Classical {{neural networks}} are implemented as one stream from the input to the output with subsampling operators {{applied in the}} stream {{in order to reduce}} the feature maps size and to increase the receptive field for the final prediction. However, for <b>semantic</b> image <b>segmentation,</b> where the task consists in providing a semantic class to each pixel of an image, feature maps reduction is harmful because it leads to a resolution loss in the output prediction. To tackle this problem, our GridNet follows a grid pattern allowing multiple interconnected streams to work at different resolutions. We show that our network generalizes many well known networks such as conv-deconv, residual or U-Net networks. GridNet is trained from scratch and achieves competitive results on the Cityscapes dataset...|$|R
40|$|We are {{interested}} in automatic scene understanding from geometric cues. To this end, we aim to bring <b>semantic</b> <b>segmentation</b> in the loop of real-time reconstruction. Our <b>semantic</b> <b>segmentation</b> is built on a deep autoencoder stack trained exclusively on synthetic depth data generated from our novel 3 D scene library, SynthCam 3 D. Importantly, our network is able to segment real world scenes without any noise modelling. We present encouraging preliminary results...|$|E
40|$|The aim of <b>{{semantic}}</b> <b>segmentation</b> is {{to assign}} each pixel a semantic label. Numerous methods for semantic segmen-tation {{have been proposed}} {{in recent years and}} most of them chose pixel or superpixel as the processing primitives. However, as the information contained in a pixel or a superpixel is not discriminative enough, the outputs of these algorithms are usually not object consistent. To tackle this problem, we introduce the concept of object-like regions as a new and higher level processing primitive. We first experimentally showed that using groundtruth segments as pro-cessing primitives can boost <b>semantic</b> <b>segmentation</b> accuracy, and then proposed a novel method to produce regions that resemble the groundtruth regions, which we named them as ‘object-like regions’. We achieve this by integrating state of the art low-level segmentation algorithms with typical <b>semantic</b> <b>segmentation</b> algorithms through a novel se-mantic feature feedback mechanism. We present experimental results on the publicly available image understanding dataset MSRC 21 and stanford background dataset, showing that the new method can achieve relatively good <b>semantic</b> <b>segmentation</b> results with far fewer processing primitives...|$|E
40|$|We {{present a}} two-module {{approach}} to <b>semantic</b> <b>segmentation</b> that incorporates Convolutional Networks (CNNs) and Graphical Models. Graphical models {{are used to}} generate a small (5 - 30) set of diverse segmentations proposals, such that this set has high recall. Since the number of required proposals is so low, we can extract fairly complex features to rank them. Our complex feature of choice is a novel CNN called SegNet, which directly outputs a (coarse) <b>semantic</b> <b>segmentation.</b> Importantly, SegNet is specifically trained to optimize the corpus-level PASCAL IOU loss function. To {{the best of our}} knowledge, this is the first CNN specifically designed for <b>semantic</b> <b>segmentation.</b> This two-module approach achieves 52. 5 % on the PASCAL 2012 segmentation challenge. Comment: 13 pages, 6 figure...|$|E
30|$|Our {{contributions}} are twofold. First, we introduce a coalesced {{style of the}} convolutional layers with the residual-flavored network to build {{an efficient model for}} the <b>semantic</b> road <b>segmentation.</b> Subsequently, we exhibit an asymmetric encoder-decoder network for reducing the model size even more, unlike the conventional symmetric approach used by the previous methods, e.g., SegNet [8].|$|R
40|$|Many {{interactive}} 3 D games utilize {{motion capture}} for both character animation and user input. These applications require short, meaningful sequences of data. Manually producing these segments of motion capture data is a laborious, time-consuming {{process that is}} impractical for real-time applications. We present a method to automatically produce <b>semantic</b> <b>segmentations</b> of general motion capture data by examining the qualitative properties that are intrinsic to all motions, using Laban Movement Analysis (LMA). LMA provides a good compromise between high-level semantic features, which are difficult to extract for general motions, and lowlevel kinematic features, which often yield unsophisticated segmentations. Our method finds motion sequences which exhibit high output similarity from a collection of neural networks trained with temporal variance. We show that segmentations produced using LMA features are more similar to manual segmentations, both at the frame and the segment level, than several other automatic segmentation methods...|$|R
40|$|This paper {{presents}} GridNet, a new Convolutional Neural Network (CNN) architecture for <b>semantic</b> image <b>segmentation</b> (full scene labelling). Classical {{neural networks}} are implemented as one stream from the input to the output with subsampling operators {{applied in the}} stream {{in order to reduce}} the feature maps size and to increase the receptive field for the final prediction. However, for <b>semantic</b> image <b>segmentation,</b> where the task consists in providing a semantic class to each pixel of an image, feature maps reduction is harmful because it leads to a resolution loss in the output prediction. To tackle this problem, our GridNet follows a grid pattern allowing multiple interconnected streams to work at different resolutions. We show that our network generalizes many well known networks such as conv-deconv, residual or U-Net networks. GridNet is trained from scratch and achieves competitive results on the Cityscapes dataset. Comment: Accepted for publication at BMVC 201...|$|R
