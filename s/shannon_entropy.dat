1998|326|Public
25|$|In {{the case}} of {{transmitted}} messages, these probabilities were the probabilities that a particular message was actually transmitted, and the entropy of the message system was {{a measure of the}} average amount of information in a message. For {{the case of}} equal probabilities (i.e. each message is equally probable), the <b>Shannon</b> <b>entropy</b> (in bits) is just the number of yes/no questions needed to determine the content of the message.|$|E
25|$|QDMR (Quantitative Differentially Methylated Regions) is a {{quantitative}} approach to quantify methylation difference and identify DMRs from genome-wide methylation profiles by adapting <b>Shannon</b> <b>entropy</b> /bioinfo.hrbmu.edu.cn/qdmr>. The platform-free and species-free nature of QDMR makes it potentially applicable to various methylation data. This approach provides an effective {{tool for the}} high-throughput identification of the functional regions involved in epigenetic regulation. QDMR {{can be used as}} an effective tool for the quantification of methylation difference and identification of DMRs across multiple samples.|$|E
25|$|Multi-Resolution {{reconstruction}} using single 2D File: High-quality 3D imaging may be {{an ultimate}} solution for revealing the complexities of any porous media, but acquiring them is costly and time consuming. High-quality 2D SEM images, on the other hand, are widely available. Recently, a novel three-step, multiscale, multiresolution reconstruction method is presented that directly uses 2D images {{in order to develop}} 3D models. This method, based on a <b>Shannon</b> <b>Entropy</b> and conditional simulation, can be used for most of the available stationary materials and can build various stochastic 3D models just using a few thin sections.|$|E
40|$|This letter {{demonstrates}} {{the use of}} <b>Shannon</b> <b>entropies</b> to detect chaos exhibited in some local regions on the phase portraits. When both eigenvalues of the second-order digital filters with two's complement arithmetic are outside the unit circle, the <b>Shannon</b> <b>entropies</b> of the state variables are independent of the initial conditions and the filter parameters, except for some special values of the filter parameters. At these special values, the <b>Shannon</b> <b>entropies</b> of the state variables are relatively small. The state trajectories corresponding to these filter parameters either exhibit random-like chaotic behaviors in some local regions or converge to some fixed points on the phase portraits. Hence, by measuring the <b>Shannon</b> <b>entropies</b> of the state variables, these special state trajectory patterns can be detected. For completeness, we extend the investigation to the case when the eigenvalues of the second-order digital filters with two's complement arithmetic are complex and are inside or on the unit circle. It is found that the <b>Shannon</b> <b>entropies</b> of the symbolic sequences for the type II trajectories may be higher than that for the type III trajectories, even though the symbolic sequences of the type II trajectories are periodic and have limit cycle behaviors, while that of the type III trajectories are aperiodic and have chaotic behaviors. Department of Electronic and Information Engineerin...|$|R
5000|$|He {{showed that}} for any such {{functions}} {{the sum of}} the <b>Shannon</b> <b>entropies</b> is non-negative, ...|$|R
5000|$|In the {{discrete}} case, the <b>Shannon</b> <b>entropies</b> {{are defined}} asandand the entropic uncertainty principle becomes ...|$|R
25|$|In {{mathematical}} statistics, the Kullback–Leibler divergence (also called relative entropy) is {{a measure}} of how one probability distribution diverges from a second, expected probability distribution. Applications include characterizing the relative (<b>Shannon)</b> <b>entropy</b> in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference. In contrast to variation of information, it is a distribution-wise asymmetric measure and thus does not qualify as a statistical metric of spread. In the simple case, a Kullback–Leibler divergence of 0 indicates that we can expect similar, if not the same, behavior of two different distributions, while a Kullback–Leibler divergence of 1 indicates that the two distributions behave in such a different manner that the expectation given the first distribution approaches zero. In simplified terms, it {{is a measure}} of surprise, with diverse applications such as applied statistics, fluid mechanics, neuroscience and machine learning.|$|E
500|$|The <b>Shannon</b> <b>entropy</b> of the Cauchy {{distribution}} {{is equal to}} , which also involves [...]|$|E
2500|$|... where H is the <b>Shannon</b> <b>entropy,</b> E(H) is the {{expected}} <b>Shannon</b> <b>entropy</b> for a neutral model of distribution and SD(H) {{is the standard}} deviation of the entropy. The standard deviation is estimated from the formula derived by Pielou ...|$|E
5000|$|Hirschman uncertainty, {{in quantum}} mechanics, {{information}} theory, and Fourier analysis, the entropic uncertainty or Hirschman uncertainty {{is the sum}} of the temporal and spectral <b>Shannon</b> <b>entropies</b> ...|$|R
40|$|This {{paper is}} {{prepared}} as {{a contribution to}} the proceedings after the 12 th ICSSUR/Feynfest Conference held in Foz do Iguaçu (Brazil) from 2 to 6 May 2011. In the first part I briefly report the topic of entropic uncertainty relations for position and momentum variables. Then I investigate the discrete <b>Shannon</b> <b>entropies</b> related to the case of finite number of detectors set to measure probability distributions in position and momentum spaces. I derive an uncertainty relation for the sum of the <b>Shannon</b> <b>entropies</b> which generalizes previous approaches [Phys. Lett. 103 A, 253 (1984) ] based on an infinite number of detectors (bins) ...|$|R
40|$|Entropic {{uncertainty}} {{relations for}} <b>Shannon</b> <b>entropies</b> associated with tomographic probability distributions of continuous quadratures are reviewed. New entropic uncertainty {{relations in the}} form of inequalities for integrals containing the tomograms of quantum states and deformation parameter are obtained...|$|R
2500|$|... which is, as said before, {{referred}} to as the differential entropy. This means that the differential entropy is not a limit of the <b>Shannon</b> <b>entropy</b> for [...] Rather, it differs from the limit of the <b>Shannon</b> <b>entropy</b> by an infinite offset (see also the article on information dimension) ...|$|E
2500|$|... where H is the <b>Shannon</b> <b>entropy</b> and K is {{the number}} of categories.|$|E
2500|$|Bekenstein's topical {{overview}} [...] "A Tale of Two Entropies" [...] describes potentially profound {{implications of}} Wheeler's trend, {{in part by}} noting a previously unexpected connection between the world of information theory and classical physics. This connection was first described shortly after the seminal 1948 papers of American applied mathematician Claude E. Shannon introduced today's most widely used measure of information content, now known as <b>Shannon</b> <b>entropy.</b> As an objective measure of the quantity of information, <b>Shannon</b> <b>entropy</b> has been enormously useful, as the design of all modern communications and data storage devices, from cellular phones to modems to hard disk drives and DVDs, rely on <b>Shannon</b> <b>entropy.</b>|$|E
50|$|Team entropy - {{a measure}} of unpredictability and {{variation}} in teams offense, higher entropy meaning more variation. It is calculated as aggregated individual <b>Shannon</b> <b>entropies,</b> where unpredictability is measured as uncertainty of the ball movement between any two nodes.|$|R
40|$|This report compares {{a variety}} of computable {{information}} measures for finite strings. These include Shannon’s n-block entropy, the three best known versions of the Lempel-Ziv production complexity (LZ- 76, LZ- 77, and LZ- 78), and the lesser known T-entropy. We apply these measures to strings of known entropy, each derived from the logistic map. Pesin’s identity allows us to deduce corresponding <b>Shannon</b> <b>entropies</b> (Kolmogorov-Sinai entropies) for the sample strings, without resorting to probabilistic methods. I...|$|R
50|$|From this norm, one {{is able to}} {{establish}} a lower bound on {{the sum of the}} (differential) Rényi entropies, , where , which generalize the <b>Shannon</b> <b>entropies.</b> For simplicity, we consider this inequality only in one dimension; the extension to multiple dimensions is straightforward and {{can be found in the}} literature cited.|$|R
2500|$|In {{classical}} information theory, the <b>Shannon</b> <b>entropy,</b> [...] {{is associated}} to a probability distribution,, {{in the following}} way: ...|$|E
2500|$|The Rényi entropy is a {{generalization}} of the <b>Shannon</b> <b>entropy</b> to other values of q than unity. It can be expressed: ...|$|E
2500|$|To {{account for}} this discretization, we can define the <b>Shannon</b> <b>entropy</b> of the wave {{function}} for a given measurement apparatus as ...|$|E
40|$|We {{show that}} for any von Neumann measurement, we can {{construct}} a logically reversible measurement such that <b>Shannon</b> <b>entropies</b> and quantum discords {{induced by the}} two measurements have compact connections. In particular, we prove that quantum discord for the logically reversible measurement is never less than that for the von Neumann measurement...|$|R
40|$|In a {{previous}} paper, the author {{proved that the}} difference between the <b>Shannon</b> <b>entropies</b> of the original and the q-deleted point process is bounded above by a function of q. This note strengthens the result by showing that the bound cannot be improved and is in fact the least upper bound for the difference...|$|R
25|$|Although {{the analogy}} between both {{functions}} is suggestive, {{the following question}} must be set: is the differential entropy a valid extension of the <b>Shannon</b> discrete <b>entropy?</b> Differential entropy lacks a number of properties that the <b>Shannon</b> discrete <b>entropy</b> hasnbsp&– it can even be negativenbsp&– and thus corrections have been suggested, notably limiting density of discrete points.|$|R
2500|$|<b>Shannon</b> <b>entropy</b> is {{characterized}} by a small number of criteria, listed below. Any definition of entropy satisfying these assumptions has the form ...|$|E
2500|$|Based on the {{probability}} mass function of each source symbol to be communicated, the <b>Shannon</b> <b>entropy</b> , in units of bits (per symbol), is given by ...|$|E
2500|$|Entropy is one {{of several}} ways to measure diversity. [...] Specifically, <b>Shannon</b> <b>entropy</b> is the {{logarithm}} of , the true diversity index with parameter equal to 1.|$|E
50|$|In quantum mechanics, {{information}} theory, and Fourier analysis, the entropic uncertainty or Hirschman {{uncertainty is}} defined as the sum of the temporal and spectral <b>Shannon</b> <b>entropies.</b> It turns out that Heisenberg's uncertainty principle can be expressed as a lower bound on the sum of these entropies. This is stronger than the usual statement of the uncertainty principle in terms of the product of standard deviations.|$|R
40|$|Uncertainty {{relations}} {{have become the}} trademark of quantum theory since they were formulated by Bohr and Heisenberg. This review covers various generalizations and extensions of the uncertainty relations in quantum theory that involve the Rényi and the <b>Shannon</b> <b>entropies.</b> The advantages of these entropic uncertainty relations are pointed out and their more direct connection to the observed phenomena is emphasized. Several remaining open problems are mentionedComment: 35 pages, review pape...|$|R
40|$|Abstract. We {{calculate}} explicit formulae for the <b>Shannon</b> <b>entropies</b> {{of several}} families of tailored random graph ensembles {{for which no}} such formulae were as yet available, in leading orders in the system size. These include bipartite graph ensembles with imposed (and possibly distinct) degree distributions for the two node sets, graph ensembles constrained by specified node neighbourhood distributions, and graph ensembles constrained by specified generalised degree distributions. 1...|$|R
2500|$|In {{information}} theory, entropy is {{the measure}} of the amount of information that is missing before reception and is sometimes referred to as <b>Shannon</b> <b>entropy.</b> <b>Shannon</b> <b>entropy</b> is a broad and general concept which finds applications in information theory as well as thermodynamics. It was originally devised by Claude Shannon in 1948 to study the amount of information in a transmitted message. The definition of the information entropy is, however, quite general, and is expressed in terms of a discrete set of probabilities pi so that ...|$|E
2500|$|The Kullback–Leibler {{divergence}} is additive {{for independent}} distributions {{in much the}} same way as <b>Shannon</b> <b>entropy.</b> If [...] are independent distributions, with the joint distribution , and [...] likewise, then ...|$|E
2500|$|The <b>Shannon</b> <b>entropy</b> {{satisfies}} the following properties, {{for some of}} which it is useful to interpret entropy as the amount of information learned (or uncertainty eliminated) by revealing the value of a random variable : ...|$|E
40|$|The {{uncertainty}} relation {{based on}} the <b>Shannon</b> <b>entropies</b> of the probability densities in position and momentum spaces is improved for quantum systems in arbitrary $D$-dimensional spherically symmetric potentials. To find it, we have used the $L^p$ [...] $L^q$ norm inequality of L. De Carli and the logarithmic uncertainty relation for the Hankel transform of S. Omri. Applications to some relevant three-dimensional central potentials are shown. Comment: 13 pages, 3 figure...|$|R
40|$|Using entropic inequalities for <b>Shannon</b> <b>entropies</b> new inequalities {{for some}} {{classical}} polynomials are obtained. To this end, photon distribution functions for one-, two- and multi-mode squeezed states {{in terms of}} Hermite, Laguerre, Legendre polynomials and Gauss' hypergeometric functions are used. The dependence between the violation of the quadrature uncertainty relation, the sign {{and the existence of}} the distribution function of such states is considered. Comment: 16 pages, 4 figure...|$|R
40|$|It is {{well-known}} that the <b>Shannon</b> <b>entropies</b> of some parameterized probability distributions are concave functions {{with respect to}} the parameter. In this paper we consider a family of such distributions (including the binomial, Poisson, and negative binomial distributions) and investigate the concavity of the Shannon, Rényi, and Tsallis entropies of them. Comment: 8 pages; an oral presentation based on this work was delivered at ICMA 2015 (International Conference on Mathematics and its Applications...|$|R
