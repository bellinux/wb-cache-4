3|10000|Public
40|$|The authors ' earlier {{model for}} the {{vulnerability}} of aircraft where aircraft was considered as a combination of cylinder, cones and wedges has been extended to the case when structural data of airoraft {{as well as its}} vital parts are given in the form of three-dimensional curvilinear triangles. In the case of VT-fused ammunition, <b>spherical</b> <b>normal</b> <b>distribution</b> has been used to estimate the landing probability of the shell in a. cylindrical vicinity region around the aircraft. Kill criteria of vital parts have been redefined. ...|$|E
40|$|Abstract-The entropy power {{inequality}} {{states that}} the effective vari-ance (entropy power) of the sum of two independent random variables {{is greater than the}} sum of their effective variances. The Brunn-Minkowski inequality {{states that the}} effective radius of the set sum of two sets is greater than the sum of their effective radii. Both these inequalities are recast in a form that enhances their similarity. In spite of this similarity, there is as yet no common proof of the inequalities. Nevertheless, their intriguing similarity suggests that new results relating to entropies from known results in geometry and vice versa may be found. Two applications of this reasoning are presented. First, an isoperimetric inequality for entropy is proved that shows that the <b>spherical</b> <b>normal</b> <b>distribution</b> mini-mizes the trace of the Fisher information matrix given an entropy con-straint-just as a sphere minimizes the surface area given a volume constraint. Second, a theorem involving the effective radii of growing convex sets is proved...|$|E
40|$|Chebyshev-inequality-based convex relaxations of Chance-Constrained Programs (CCPs) {{are shown}} to be useful for {{learning}} classifiers on massive datasets. In particular, an algorithm that integrates efficient clustering procedures and CCP approaches for computing classifiers on large datasets is proposed. The key idea is to identify high density regions or clusters from individual class conditional densities and then use a CCP formulation to learn a classifier on the clusters. The CCP formulation ensures {{that most of the}} data points in a cluster are correctly classified by employing a Chebyshev-inequality-based convex relaxation. This relaxation is heavily dependent on the second-order statistics. However, this formulation and in general such relaxations that depend on the second-order moments are susceptible to moment estimation errors. One of the contributions of the paper is to propose several formulations that are robust to such errors. In particular a generic way of making such formulations robust to moment estimation errors is illustrated using two novel confidence sets. An important contribution is to show that when either of the confidence sets is employed, for the special case of a <b>spherical</b> <b>normal</b> <b>distribution</b> of clusters, the robust variant of the formulation can be posed as a second-order cone program. Empirical results show that the robust formulations achieve accuracies comparable to that with true moments, even when moment estimates are erroneous. Results also illustrate the benefits of employing the proposed methodology for robust classification of large-scale datasets...|$|E
40|$|Projection pursuit {{indices are}} {{intended}} to give 'interesting' one- or two-dimensional projections of data points in higher-dimensional spaces. This paper compares the performance of seven such indices. They are all minimum for a <b>normal</b> <b>distribution,</b> and are standardized to be zero for that case. They are compared by considering projections of a mixture of four trivariate <b>spherical</b> <b>normal</b> <b>distributions</b> with means at the vertices of a regular tetrahedron. Clustering Grouping Projection pursuit indices Multinormal mixtures Sphering...|$|R
40|$|Two optimal {{characteristic}} {{properties of the}} <b>normal</b> <b>distribution</b> are shown: (a) Of all the SNM (<b>spherical</b> scale <b>normal</b> mixtures) the normal with the same Mahalanobis distances between [Pi]i:SNM([mu]i) and [Pi]j:SNM([mu]j), i [not equal to] j, maximizes the probabilities of correct classification determined by a certain subclass of the LDF classification rules; (b) The class of LDF (linear discriminant function) rules is the admissible class for the discrimination problem with spherical population alternatives iff the <b>spherical</b> <b>distribution</b> is <b>normal.</b> <b>Spherical</b> <b>distributions</b> linear discriminant functions characterizations of normality <b>spherical</b> <b>normal</b> mixtures discriminatory power...|$|R
40|$|AbstractTwo optimal {{characteristic}} {{properties of the}} <b>normal</b> <b>distribution</b> are shown: (a) Of all the SNM (<b>spherical</b> scale <b>normal</b> mixtures) the normal with the same Mahalanobis distances between Πi:SNM(μi) and Πj:SNM(μj), i ≠ j, maximizes the probabilities of correct classification determined by a certain subclass of the LDF classification rules; (b) The class of LDF (linear discriminant function) rules is the admissible class for the discrimination problem with spherical population alternatives iff the <b>spherical</b> <b>distribution</b> is <b>normal...</b>|$|R
5000|$|Multivariate <b>normal</b> <b>distribution</b> (a complex <b>normal</b> <b>distribution</b> is a bivariate <b>normal</b> <b>distribution)</b> ...|$|R
50|$|The <b>normal</b> <b>distribution</b> {{and multivariate}} <b>normal</b> <b>distributions.</b>|$|R
5000|$|Complex <b>normal</b> <b>distribution,</b> an {{application}} of bivariate <b>normal</b> <b>distribution</b> ...|$|R
5000|$|Combining {{the above}} for a <b>normal</b> <b>distribution</b> [...] with both μ and σ2 unknown yields the {{following}} ancillary statistic:This simple combination is possible because the sample mean and sample variance of the <b>normal</b> <b>distribution</b> are independent statistics; {{this is only}} true for the <b>normal</b> <b>distribution,</b> and in fact characterizes the <b>normal</b> <b>distribution.</b>|$|R
40|$|The {{mixture of}} <b>normal</b> <b>distributions</b> {{provides}} a useful extension of the <b>normal</b> <b>distribution</b> for modeling of daily changes in market variables with fatter-than-normal tails and skewness. An efficient analytical Monte Carlo method is proposed for generating daily changes using a multivariate mixture of <b>normal</b> <b>distributions</b> with arbitrary covariance matrix. The main purpose of this method is to transform (linearly) a multivariate normal with an input covariance matrix into the desired multivariate mixture of <b>normal</b> <b>distributions.</b> This input covariance matrix can be derived analytically. Any linear combination of mixtures of <b>normal</b> <b>distributions</b> can {{be shown to be}} a mixture of <b>normal</b> <b>distributions...</b>|$|R
5000|$|An {{alternative}} parametric {{approach is}} {{to assume that the}} residuals follow a mixture of normal distributions; in particular, a contaminated <b>normal</b> <b>distribution</b> in which the majority of observations are from a specified <b>normal</b> <b>distribution,</b> but a small proportion are from a <b>normal</b> <b>distribution</b> with much higher variance. That is, residuals have probability [...] of coming from a <b>normal</b> <b>distribution</b> with variance , where [...] is small, and probability [...] of coming from a <b>normal</b> <b>distribution</b> with variance [...] for some ...|$|R
5000|$|In general, noncentrality {{parameters}} {{occur in}} distributions that are transformations of a <b>normal</b> <b>distribution.</b> The [...] "central" [...] versions {{are derived from}} <b>normal</b> <b>distributions</b> that have a mean of zero; the noncentral versions generalize to arbitrary means. For example, the standard (central) chi-squared distribution is the distribution of a sum of squared independent standard <b>normal</b> <b>distributions,</b> i.e., <b>normal</b> <b>distributions</b> with mean 0, variance 1. The noncentral chi-squared distribution generalizes this to <b>normal</b> <b>distributions</b> with arbitrary mean and variance.|$|R
25|$|This simple {{combination}} is possible because the sample mean and sample variance of the <b>normal</b> <b>distribution</b> are independent statistics; {{this is only}} true for the <b>normal</b> <b>distribution,</b> and in fact characterizes the <b>normal</b> <b>distribution.</b>|$|R
30|$|The {{white noise}} with the <b>normal</b> <b>distribution</b> {{characteristics}} is {{the premise of}} the time series model, that is, the multi-element fitting residual errors δ (t) should obey the <b>normal</b> <b>distribution.</b> The existing function of the <b>normal</b> <b>distribution</b> test in MATLAB was used to get that the multi-element fitting residual errors δ (t) obeyed the <b>normal</b> <b>distribution.</b>|$|R
5000|$|The two {{generalized}} normal families described here, {{like the}} skew normal family, are parametric families that extends the <b>normal</b> <b>distribution</b> {{by adding a}} shape parameter. Due to {{the central role of}} the <b>normal</b> <b>distribution</b> in probability and statistics, many distributions can be characterized in terms of their relationship to the <b>normal</b> <b>distribution.</b> For example, the lognormal, folded normal, and inverse <b>normal</b> <b>distributions</b> are defined as transformations of a normally-distributed value, but unlike the generalized normal and skew-normal families, these do not include the <b>normal</b> <b>distributions</b> as special cases.Actually all distributions with finite variance are in the limit highly related to the <b>normal</b> <b>distribution.</b> The Student-t distribution, the Irwin-Hall distribution and the Bates distribution also extend the <b>normal</b> <b>distribution,</b> and include in the limit the <b>normal</b> <b>distribution.</b> So there is no strong reason to prefer the [...] "generalized" [...] <b>normal</b> <b>distribution</b> of type 1, e.g. over a combination of Student-t and a normalized extended Irwin-Hall - this would include e.g. the triangular distribution (which cannot be modeled by the generalized Gaussian type 1).|$|R
2500|$|Multivariate <b>normal</b> <b>distribution</b> — a {{generalization}} of the <b>normal</b> <b>distribution</b> in multiple dimensions ...|$|R
50|$|<b>Normal</b> <b>distributions</b> are symmetrical, {{bell-shaped}} distributions {{that are}} useful in describing real-world data. The standard <b>normal</b> <b>distribution,</b> represented by the letter Z, is the <b>normal</b> <b>distribution</b> having a mean of 0 and {{a standard deviation of}} 1.|$|R
5000|$|Each {{component}} of the velocity vector has a <b>normal</b> <b>distribution</b> with mean [...] and standard deviation , so the vector has a 3-dimensional <b>normal</b> <b>distribution,</b> {{a particular kind of}} multivariate <b>normal</b> <b>distribution,</b> with mean [...] and standard deviation [...]|$|R
5000|$|Sampling {{from the}} matrix <b>normal</b> <b>distribution</b> {{is a special}} case of the {{sampling}} procedure for the multivariate <b>normal</b> <b>distribution.</b> Let [...] be an n by p matrix of np independent samples from the standard <b>normal</b> <b>distribution,</b> so that ...|$|R
40|$|In searching a {{opportunity}} distribution {{a function}} {{from one or}} more random usually in finishing with selected method, to be obtained by data is correctness. method of Transformation variable {{is one of the}} technique all important in determining probability functions. Determination of function of densities the probability in pass Jacobian and process integrate from <b>Normal</b> <b>distribution</b> Lean. <b>Normal</b> <b>Distribution</b> of Standard is one of the distribution function which used many from at other distribution functions. At the writing of this thesis aim to show probability functions to be alighted from by <b>Normal</b> <b>distribution</b> of Standard by 2 random variable and 3 random variable in the form of variable transformation. The <b>Normal</b> <b>distribution</b> of Standard with function of transformation selected variable at 2 obtained by random variable of <b>normal</b> <b>distribution</b> form, distribution of exponential-(), distribution of Cauchy-(1, 0), distribution of beta 2 -, distribution of gamma-(1, 1), and distribution of beta 1 -. While from <b>Normal</b> <b>distribution</b> of Standard with function of transformation selected variable at 3 obtained by random variable of <b>normal</b> <b>distribution</b> for-, <b>normal</b> <b>distribution</b> -, and distribution of beta 2...|$|R
5000|$|In {{a special}} case when [...] the split <b>normal</b> <b>distribution</b> reduces to <b>normal</b> <b>distribution</b> with {{variance}} [...]|$|R
5000|$|It also {{coincides with}} a zero-mean <b>normal</b> <b>distribution</b> {{truncated}} from below at zero (see truncated <b>normal</b> <b>distribution)</b> ...|$|R
30|$|<b>Normal</b> <b>distribution</b> of the {{variables}} was evaluated with Kolmogorov-Smirnov tests. The results showed <b>normal</b> <b>distribution</b> for all variables.|$|R
40|$|Abstract: The log <b>normal</b> <b>distribution</b> is {{as popular}} in {{engineering}} as the <b>normal</b> <b>distribution</b> is in statistics. However, {{there has been}} little work relating to order statistics from the log <b>normal</b> <b>distribution.</b> In this note, explicit expressions are derived for the moments of order statistics from the log <b>normal</b> <b>distribution</b> by using a formula due to Withers. The usefulness of the result is illustrated through two data sets...|$|R
2500|$|The split <b>normal</b> <b>distribution</b> is most {{directly}} {{defined in terms}} of joining scaled sections of the density functions of different <b>normal</b> <b>distributions</b> and rescaling the density to integrate to one. [...] The truncated <b>normal</b> <b>distribution</b> results from rescaling a section of a single density function.|$|R
5000|$|Cramér's theorem {{shows that}} while the <b>normal</b> <b>distribution</b> is {{infinitely}} divisible, {{it can only be}} decomposed into <b>normal</b> <b>distributions.</b>|$|R
30|$|The <b>normal</b> <b>distribution</b> {{is perhaps}} the most {{commonly}} used probability distribution in both statistical theory and applications. The <b>normal</b> <b>distribution</b> was first used by de Moivre (1733) in the literature as an approximation to the binomial distribution. However, the development of the <b>normal</b> <b>distribution</b> by Gauss (1809, 1816) became the standard used in the modern statistics. Hence, the <b>normal</b> <b>distribution</b> is also commonly known as the Gaussian distribution. Properties of the <b>normal</b> <b>distribution</b> have been well developed (e.g., see Johnson et al. 1994; Patel and Read 1996). The distribution also {{plays an important role in}} generating new distributions.|$|R
500|$|The {{simplest}} {{case of a}} <b>normal</b> <b>distribution</b> {{is known}} as the standard <b>normal</b> <b>distribution,</b> described by this probability density function: ...|$|R
50|$|Thus, {{while the}} <b>normal</b> <b>distribution</b> is {{infinitely}} divisible, {{it can only}} be decomposed into <b>normal</b> <b>distributions</b> (if the summands are independent).|$|R
5000|$|... (since X and &minus;X {{both have}} the same <b>normal</b> <b>distribution),</b> where [...] is the {{cumulative}} distribution function of the <b>normal</b> <b>distribution..</b>|$|R
40|$|We {{introduce}} the standard two-sided power <b>normal</b> <b>distribution</b> {{and consider the}} relation between the probability in standard two-sided power distribution and the probability in standard two-sided power <b>normal</b> <b>distribution</b> and obtain the even moment of the special two-sided power <b>normal</b> <b>distribution</b> including the cases considered by Gupta and Nadarajah(2004...|$|R
40|$|Hermite polynomials {{are used}} to derive {{expressions}} for the moments {{about the origin of}} univariate and multivariate <b>normal</b> <b>distributions.</b> A recurrence relation derived for multivariate Hermite polynomials leads to a recurrence relation for the multivariate normal moments. Bivariate <b>normal</b> <b>distribution</b> Chebyshev-Hermite polynomial Multivariate Hermite polynomial Multivariate <b>normal</b> <b>distribution...</b>|$|R
50|$|The split <b>normal</b> <b>distribution</b> {{results from}} merging {{two halves of}} <b>normal</b> <b>distributions.</b> In a general case the 'parent' <b>normal</b> <b>distributions</b> can have {{different}} variances which implies that the joined PDF would not be continuous. To ensure that the resulting PDF integrates to 1, the normalizing constant A is used.|$|R
50|$|The {{primary reason}} that the chi-squared {{distribution}} is used extensively in hypothesis testing is {{its relationship to the}} <b>normal</b> <b>distribution.</b> Many hypothesis tests use a test statistic, such as the t statistic in a t-test. For these hypothesis tests, as the sample size, n, increases, the sampling distribution of the test statistic approaches the <b>normal</b> <b>distribution</b> (Central Limit Theorem). Because the test statistic (such as t) is asymptotically normally distributed, provided the sample size is sufficiently large, the distribution used for hypothesis testing may be approximated by a <b>normal</b> <b>distribution.</b> Testing hypotheses using a <b>normal</b> <b>distribution</b> is well understood and relatively easy. The simplest chi-squared distribution is the square of a standard <b>normal</b> <b>distribution.</b> So wherever a <b>normal</b> <b>distribution</b> could be used for a hypothesis test, a chi-squared distribution could be used.|$|R
5000|$|If [...] {{follows a}} <b>normal</b> <b>distribution,</b> the rate {{function}} becomes a parabola with its apex at {{the mean of}} the <b>normal</b> <b>distribution.</b>|$|R
