13|107|Public
50|$|Because of {{the slow}} {{processor}} {{speed of the}} C128 combined with <b>slow</b> <b>storage</b> access, when a HIRES image loaded it was displayed in 25 passes of 40 chunks creating a choppy effect. If the HIRES image had colors it was often blurry and not recognizable since the colors loaded after the image was already being displayed leaving the user {{with nothing to do}} but wait until it was finished.|$|E
40|$|Caches in FPGAs {{can improve}} the {{performance}} of soft processors and other applications beset by <b>slow</b> <b>storage</b> components. In this paper we present a cache generator which can produce caches {{with a variety of}} associativities, latencies, and dimensions. This tool allows system designers to effortlessly create, and investigate different caches in order to better meet the needs of their target system. The effect of these three parameters on the area and speed of the caches is also examined and we show that the designs can meet a wide range of specifications and are in general fast and compact. 1...|$|E
40|$|Many {{large storage}} systems use approximatemembership-query (AMQ) data {{structures}} {{to deal with}} the massive amounts of data that they process. An AMQ data structure is a dictionary that trades off space for a false positive rate on membership queries. It is designed to fit into small, fast storage, and it is used to avoid I/Os on <b>slow</b> <b>storage.</b> The Bloom filter is a well-known example of an AMQ data structure. Bloom filters, however, do not scale outside of main memory. This paper describes the Cascade Filter TM, an AMQ data structure that scales beyond main memory, supporting over half a million insertions/deletions per second and over 500 lookups per second on a commodity flashbased SSD. ...|$|E
5000|$|... it {{balances}} {{resistance against}} side-channel threats and attacks relying on cheaper (and, hence, <b>slower)</b> <b>storage</b> devices ...|$|R
5000|$|... bcache (abbreviated from block cache) is a cache in the Linux kernel's block layer, {{which is}} used for {{accessing}} secondary storage devices. It allows one or more fast storage devices, such as flash-based solid-state drives (SSDs), {{to act as a}} cache for one or more <b>slower</b> <b>storage</b> devices, such as hard disk drives (HDDs); this effectively creates hybrid volumes and provides performance improvements.|$|R
5000|$|... dm-cache is a {{component}} (more specifically, a target) of the Linux kernel's device mapper, {{which is a}} framework for mapping block devices onto higher-level virtual block devices. It allows one or more fast storage devices, such as flash-based solid-state drives (SSDs), {{to act as a}} cache for one or more <b>slower</b> <b>storage</b> devices such as hard disk drives (HDDs); this effectively creates hybrid volumes and provides secondary storage performance improvements.|$|R
40|$|Caches {{have been}} used to {{successfully}} alleviate the degradation in performance caused by accessing <b>slow</b> <b>storage</b> components, and hence have become a prominent part of memory hierarchy. In this thesis, a cache generator is proposed which can produce a variety of different caches with different sizes. This allows designers to effortlessly create, alter, and examine different caches in order to best meet the needs of their target system. The target of these cache designs is for FPGA designs, specifically, Altera’s Stratix FPGA. Analysis of the area and speed of the generated designs demonstrated that the designs can meet a wide range of design specifications and are in general fast and low-cost cache designs. i...|$|E
40|$|This is {{the author}} {{accepted}} manuscript. The final version is available from IEEE via the DOI in this record. One of the emerging candidates {{to bridge the gap}} between fast but volatile DRAM and non-volatile but <b>slow</b> <b>storage</b> devices is tetrahedral amorphous carbon (ta-C) based memory [1]-[3]. This offers a very good scalability, data retention and sub- 5 ns switching [2], [3]. Amorphous carbon memory devices can be electrically and optically switched from a high resistance state (HRS) to a low resistance state (LRS) [4]. The electrical conduction in the LRS is thought to be through sp 2 clusters that form a conductive filament [4]. This work was funded by the EU research & innovation project CareRAMM, no. 30998...|$|E
40|$|Abstract INTRODUCTION: Mucocele {{is a rare}} {{pathology}} of {{the appendix}} characterized by expansion of the lumen for a <b>slow</b> <b>storage</b> of mucous. Represents the 0. 2 - 0. 3 % of all the appendectomies. Clinical signs and the symptoms {{are similar to those}} of the acute appendicitis, while the disease is occasionally recorded and the diagnosis is essentially histological. AIM OF STUDY: To recall the clinical and anatomopathological features of mucocele evaluating the possible evolutions of this rare appendicular pathology according to the updating reports of the literature. MATERIALS AND METHODS: Study of a clinical case. DISCUSSION: In this study diagnostic chriteria and prognostic factors are revised. Authors evaluate anatomopathological classification, possibility of evolution in a preneoplastic and neoplastic lesion and association with other colon cancers. The surgical treatment is evaluated too. CONCLUSIONS: A correct preoperative mucocele diagnosis is emphasized as indispensable in the choice of the proper surgical treatment since a good prognosis is consequent to a radical treatment...|$|E
50|$|Teradata Active Enterprise Data Warehouse, {{often called}} Teradata Database, {{includes}} data management tools and data mining software.Teradata's product {{can be used}} for business analysis. Data warehouses can track company data, such as sales, customer preferences, product placement, etc.The data warehouse differentiates between “hot and cold” data - meaning that the warehouse puts data that is not often used in a <b>slower</b> <b>storage</b> section.The original system used a proprietary network technology called BYNet.|$|R
5000|$|PCI Express SSDs can be {{interfaced}} {{through the}} AHCI driver, providing backward compatibility older operating systems {{at the cost}} of not delivering optimal performance. AHCI was developed when the purpose of a host bus adapter (HBA) in a system was to connect the CPU/memory subsystem with a much <b>slower</b> <b>storage</b> subsystem based on rotating magnetic media. As a result, AHCI has some inherent inefficiencies when applied to SSD devices, which behave much more like DRAM than like spinning media.|$|R
50|$|A cache often also {{acts as a}} buffer, {{and vice}} versa. However, caches operate {{on the premise that}} the same data will be read from them {{multiple}} times, that written data will soon be read, or that {{there is a good chance}} of multiple reads or writes to combine to form a single larger block. Their sole purpose is to reduce accesses to the underlying <b>slower</b> <b>storage.</b> A cache is also usually an abstraction layer that is designed to be invisible.|$|R
40|$|The {{trend of}} UK {{logistics}} warehouses is reported towards {{larger and more}} centralized systems on which automation is heavily invested in recent years. This is partially because of the disadvantage of manual operation resulting in the <b>slow</b> <b>storage</b> and retrieval speed, the high labor cost, and the high frequency of human errors. On the other hand, there is the growing demand for online shopping; more and more customers are using this method to find low-priced goods while requesting a fast delivery. Thus, retailers require more efficient distribution centers in which improved performances of warehousing systems through automation are increasingly desirable for these companies. The paper presents a conceptual design of a RFID-based automated warehousing system with its inherent feature of scalability and reconfigurability. Physical infrastructure and operational control events within the system are illustrated in the paper. Such a warehousing system was also modeled to indicate the level of capability that the system can provide {{in terms of the}} desired coordinated functionality of various operations that take place within the proposed system...|$|E
40|$|Cloud object {{stores are}} {{increasingly}} becoming {{the de facto}} storage choice for big data analytics platforms, mainly because they simplify the management of large blocks of data at scale. To ensure cost-effectiveness of the storage service, the object stores use hard disk drives (HDDs). However, the lower performance of HDDs af-fect tenants who have strict performance requirements for their big data applications. The use of faster storage devices such as solid state drives (SSDs) is thus desir-able by the tenants, but incurs significant maintenance costs to the provider. We design a tiered object store for the cloud, which comprises both fast and <b>slow</b> <b>storage</b> devices. The resulting hybrid store exposes the tiering to tenants with a dynamic pricing model {{that is based on}} the tenants ’ usage and the provider’s desire to maximize profits. The tenants leverage knowledge of their work-loads and current pricing information to select a data placement strategy that would meet the application re-quirements at the lowest cost. Our approach allows both a service provider and its tenants to engage in a pricing game, which our results show yields a win–win situation. ...|$|E
40|$|The paper aims {{to study}} the {{shelf-life}} of the housing, cut up pieces and chicken organs in a company slaughterhouseduring the summer. The hybrid is Ross 308. Quality samples, taken randomly during the production process fromvarious parts, was studied by means of specific laboratory from microbiological point of view (determination ofSalmonella and E. coli, NTG), organoleptical and physico-chemical (easily hydrolyzable nitrogen and Kreissreaction) during the eight days, until they exceeded the admited limits. Temperatures measured in depth of theproducts of the experiment exceeded the upper limit allowed by 3 - 4 degrees in the carcasse and 8 - 10 degrees inorgans. In terms of NTG in the liver they started to decrease in {{the third day of}} storage,in the gizzard and heart,there is a continuing increase in values during the entire experiment. This, like the gizzard increase in nitrogen value,although values range, may be a consequence of high temperatures packing and refrigeration products very <b>slow.</b> <b>Storage</b> conditions experienced cyclical deviations from the optimum temperature due to icing phenomenon ofevaporators. As a technical solution for evenly temperature during storage, it may be suggested purchasing a backuprefrigerating space. From the organoleptical point of view, all products undergo changes untill the sixth day, whichrecommends the establishment of shelf-life to this day. Microbiological and physico-chemical exceeding limitsoccurs on days seven and eight. The results, in addition to purely commercial usefulness can be considered abarometer of the correct application of technology to slaughter, hygienic production and the storage conditions ofgoods...|$|E
40|$|Managing large {{objects with}} high data-rate {{requirements}} {{is difficult for}} current computing systems. We describe an Input/Output architecture, called Swift, that addresses the problem of storing and retrieving very large data objects from <b>slow</b> secondary <b>storage</b> at very high data-rates. Applications that require this capability are poorly supported in current systems, {{even though they are}} made possible by highspeed networks. These range from storage and visualization of scientific computations to recodring and play-back of color video in real-time. Swift addresses the problem of providing the data rates required by digital video by exploiting the available interconnection capacity and by using several <b>slower</b> <b>storage</b> devices in parallel. We have done two studies to validate the Swift architecture: a simulation study and an Ethernet-based proof-of-concept implementation. Both studies indicate that the aggregation principle proposed in Swift can yield very high data-rates. We present a bri [...] ...|$|R
50|$|A cache also {{increases}} transfer performance. A {{part of the}} increase similarly comes from the possibility that multiple small transfers will combine into one large block. But the main performance-gain occurs {{because there is a}} good chance that the same data will be read from cache multiple times, or that written data will soon be read. A cache's sole purpose is to reduce accesses to the underlying <b>slower</b> <b>storage.</b> Cache is also usually an abstraction layer that is designed to be invisible from the perspective of neighboring layers.|$|R
50|$|In an {{implicit}} data structure, everything is accessed in-place, by reading bits at various {{positions in the}} data. To achieve memory-optimal coding, appropriate data items use bits instead of bytes. Implicit data structures are designed to improve main memory utilization, concomitantly reducing access to <b>slower</b> <b>storage.</b> A greater fraction of data in {{an implicit}} data structure can fit in main memory, reducing administrative processing. Implicit data structures can improve time efficiency due to improving cache efficiency thanks to locality of reference, due to avoiding the indirection introduced by pointers.|$|R
40|$|HPC {{applications}} pose high {{demands on}} I/O performance and storage capability. The emerging non-volatile memory (NVM) techniques offer low-latency, high bandwidth, and persistence for HPC applications. However, the existing I/O stack are designed and optimized {{based on an}} assumption of disk-based storage. To effectively use NVM, we must re-examine the existing high performance computing (HPC) I/O sub-system to properly integrate NVM into it. Using NVM as a fast storage, the previous assumption on the inferior performance of storage (e. g., hard drive) is not valid any more. The performance problem caused by <b>slow</b> <b>storage</b> may be mitigated; the existing mechanisms to narrow the performance gap between storage and CPU may be unnecessary and result in large overhead. Thus fully understanding the impact of introducing NVM into the HPC software stack demands a thorough performance study. In this paper, we analyze and model the performance of I/O intensive HPC applications with NVM as a block device. We study the performance from three perspectives: (1) the impact of NVM {{on the performance of}} traditional page cache; (2) a performance comparison between MPI individual I/O and POSIX I/O; and (3) the impact of NVM on the performance of collective I/O. We reveal the diminishing effects of page cache, minor performance difference between MPI individual I/O and POSIX I/O, and performance disadvantage of collective I/O on NVM due to unnecessary data shuffling. We also model the performance of MPI collective I/O and study the complex interaction between data shuffling, storage performance, and I/O access patterns. Comment: 10 page...|$|E
40|$|The thesis aims at {{verification}} of using emerging Solid-State drives in disk arrays. The advent of SSD disks caused a small revolution in area of data storage, because the growth performance of hard drives {{has been slow}} compared to other PC components. But an entirely different principle of operation could mean compatibility problems between SSD and related technologies, such as RAID. This thesis aims at analyzing all the relevant technologies, mainly HDD, SSD and RAID. To achieve this objective, information from literature, articles and other appropriate sources will be used. Other objectives of this thesis are {{to determine how much}} are the SSDs suitable for use in the disk array, because low performance RAID controllers or different principles of operation could limit their efficiency. This question should be answered by submission of selected types of storage arrays to synthetic and practical tests of performance. The final goal is to use financial analysis of the test solutions as a shared file storage. Today, remote access to data is used by a wide range of job positions. <b>Slow</b> <b>storage</b> could mean inefficient use of working time and therefore unnecessary financial costs. The goal of my work is primarily to provide answers to the questions mentioned above. Currently {{it is very hard to}} find tests of more complex forms of disk arrays based on solid-state drives. This article can be also very useful for companies where fileservers are used to share user data. Based on the result of cost analysis, the company can then decide what type of storage is best for its purpose...|$|E
40|$|The {{world itself}} is continuous, and continuously, {{dynamically}} changing. • Perceptual {{input from the}} world is also extremely complex: – “blooming, buzzing, confusion” • An agent’s representation and inference resources are finite, and quite limited. • An intelligent agent (human or robot) must cope with this challenge 2 Spatial Representations • I work on representing spatial knowledge. – The importance of multiple representations for incomplete knowledge of large-scale space. – How to combine rich sensory input about local space, to build useful representations of global space. • Much of my work has used laser range sensors. – But the lessons are still useful for vision. – My students and I are beginning to use vision. Incomplete Knowledge • The ability to represent incomplete knowledge is important due to: – Sensor errors and imprecision – Limited processing, <b>slow</b> <b>storage</b> and retrieval – Unexpected types of environments • Humans are far more robust than any AI – In spite of fixed and sudden limitations – In surprising environments • Incomplete knowledge is a relevant factor. 3 The Place Abstraction • Focus first on representing space. – Large-scale space is space whose structure {{is larger than the}} perceptual horizon. – Small-scale space has structure within the sensory horizon. • What are places? – In LSS, places are decision points. – In SSS, places are regions with gateways. • Places are made up of distinctive states. Learn Distinctive States • A distinctive state (location plus orientation) is the isolated fixed-point of a hill-climbing control law. • Reliable motion abstracts to a schema 〈x, a, x′〉. • Hill-climbing to a dstate reduces image variability due to pose variation. x x...|$|E
50|$|The Advanced Host Controller Interface (AHCI) {{comes with}} the benefit of wide {{software}} compatibility, but as a downside does not deliver optimal performance when used with SSDs connected via the PCI Express bus. As a logical interface, AHCI was developed when the purpose of a host bus adapter (HBA) in a system was to connect the CPU/memory subsystem with a much <b>slower</b> <b>storage</b> subsystem based on rotating magnetic media. As a result, AHCI introduces certain inefficiencies when used with SSD devices, which behave much more like DRAM than like spinning media.|$|R
5000|$|Used for PCI Express SSDs and {{interfaced}} {{through the}} AHCI driver and provided PCI Express lanes, providing backward compatibility with widespread SATA support in operating systems {{at the cost}} of not delivering optimal performance by using AHCI for accessing PCI Express SSDs. AHCI was developed when the purpose of a host bus adapter (HBA) in a system was to connect the CPU/memory subsystem with a much <b>slower</b> <b>storage</b> subsystem based on rotating magnetic media; as a result, AHCI has some inherent inefficiencies when applied to SSD devices, which behave much more like DRAM than like spinning media.|$|R
50|$|GDDR (Graphical Double Data Rate) {{memory is}} {{required}} for the operation of any PCIe graphic card and is built directly onto the card itself. The amount of RAM built onto a graphic card allows the GPU to quickly access data such as textures instead of reading off of a much <b>slower</b> <b>storage</b> device. Having more GDDR memory allows the system to handle higher levels of Anti-Aliasing and more complex textures. GDDR memory has a much higher latency when compared to DDR memory but also has a much larger bandwidth thus allowing the GPU to deal with larger amounts of data at a slower rate when compared to a CPU. The latest revision of GDDR memory is GDDR5x.|$|R
40|$|Modern HPC {{applications}} pose high {{demands on}} I/O performance and storage capability. The emerging non-volatile memory (NVM) techniques, such as Phase Change Memory and STT-RAM, offer low-latency, high bandwidth, and persistence for HPC applications. However, the existing I/O stack, including OS, high level library, I/O middleware, and applications, are designed and optimized {{based on an}} assumption of disk-based storage. To effectively use NVM, we must re-examine the existing I/O sub-system to properly integrate NVM into it. Using NVM as a fast storage, the previous assumption on the inferior performance of storage (e. g., hard drive) is not valid any more. The performance problem caused by <b>slow</b> <b>storage</b> may be mitigated; the existing mechanisms to narrow the performance gap between storage and CPU may be unnecessary and result in large overhead. Thus fully understanding {{of the impact of}} introducing NVM into the HPC software stack demands a thorough performance study. In this paper, we analyze and model the performance of I/O intensive HPC applications with NVM as a block device. We study the performance from three perspectives: (1) the impact of NVM on the performance of traditional page caches; (2) a performance comparison between MPI individual I/O and POSIX I/O; and (3) the impact of NVM on the performance of collective I/O. We reveal the diminishing effects of page caches, ignorable performance difference between MPI individual I/O and POSIX I/O, and performance disadvantage of collective I/O on NVM due to unnecessary data shuffling. We model the performance of MPI collective I/O and study the complex interaction between data shuffling, storage performance, and I/O access patterns. Extensive experiments have been conducted to verify our analysis. Keywords: NVM, page cache, MPI I/O, Collective I/...|$|E
50|$|Beginning 5 November, rolling blackouts {{took place}} across Ecuador {{for two to}} six hours per day. Government {{officials}} also urged citizens to conserve energy. Economic losses from the blackouts {{are estimated to be}} in the tens of millions of dollars; factory output <b>slowed,</b> and <b>storage</b> of perishables was disrupted.|$|R
40|$|We {{present an}} I/O architecture, called Swift, that {{addresses}} the problem of data-rate mismatches between the requirements of an application, the maximum data-rate of the storage devices, and the data-rate of the interconnection medium. The goal of Swift is to support integrated continuous multimedia in general purpose distributed systems. In installations with a high-speed interconnection medium, Swift will provide high data-rate transfers by using multiple <b>slower</b> <b>storage</b> devices in parallel. The data-rates obtained with this approach scale well when using multiple storage devices and multiple interconnections. Swift has the flexibility to use any appropriate storage technology, including disk arrays. The ability to adapt to technological advances will allow Swift to provide for ever increasing I/O demands. To {{address the problem of}} partial failures, Swift stores data redundantly. Using the Unix operating system, we have constructed a simplified prototype of the Swift archi [...] ...|$|R
40|$|ABSTRACT We {{present an}} I/O architecture, called Swift, that {{addresses}} the problem of data rate mismatches between the requirements of an application, storage devices, and the interconnection medium. The goal of Swift is to support high data rates in general purpose distributed systems. Swift uses a high-speed interconnection medium to provide high data rate transfers by using multiple <b>slower</b> <b>storage</b> devices in parallel. It scales well when using multiple storage devices and interconnections, and can use any appropriate storage technology, including high-performance devices such as disk arrays. To {{address the problem of}} partial failures, Swift stores data redundantly. Using the UNx operating system, we have constructed a simplified prototype of the Swift architecture. The prototype provides data rates that are significantly faster than access to the local SCSI disk, limited by the capacity of a single Ethernet segment, or in the case o...|$|R
5000|$|However, {{most modern}} devices will report write {{operations}} as complete once {{the data is}} stored in its onboard cache memory, before the data is written to the (<b>slow)</b> magnetic <b>storage.</b> This allows commands {{to be sent to}} the other device on the cable, reducing the impact of the [...] "one operation at a time" [...] limit.|$|R
40|$|The paper {{argues that}} recent {{developments}} in solid-state store allow a new implementation paradigm for databases. This involves replacing <b>slow</b> rotating <b>storage</b> with all-semiconductor data stores. The relatively higher costs of semiconductor store make data compression advantageous. It is argued that relational databases are well suited to this. Empirical measures of the potential performance gains from compression in commercial databases are presented...|$|R
30|$|One {{important}} information retrieval task is {{the search for}} patterns in the huge collection of time series data. This procedure is very I/O intensive, {{because we need to}} read the full respective data files for computing occurrences or similarities to the given pattern. Finding the most similar time series, matching a search pattern in millions of large files could take unacceptable duration to complete. Processing huge time series data with a standard PC architecture has many limitations. One is the decrease of processing speed whenever data to be processed does not fit in local RAM, which has a typical size of about 16 GB in current personal computers. Then, the next <b>slower</b> <b>storage</b> layer must be used, which is the hard disk. Again, as soon as the hard drive’s storage limit is reached, data must be fetched from archive servers over the Ethernet, which is again slower. Data I/O becomes the main bottleneck and not - like in other fields - computing effort.|$|R
40|$|The study {{involved}} model {{evaluation of the}} fate and utilization of starch by microbial culture acclimated to different growth conditions and feeding regimes. For this purpose, parallel sequencing batch reactors were operated with pulse and continuous feeding of soluble starch at sludge ages of 8 and 2 days. High-rate adsorption was identified as the initial process for starch utilization under all operating conditions. Hydrolysis mechanism acted as the rate limiting mechanism for different substrate removal/storage modes sustained under pulse and continuous feeding at different sludge ages. Together with variable growth kinetics, faster growth conditions also triggered high-rate hydrolysis and relatively <b>slower</b> <b>storage</b> kinetics to ensure the level of substrate supply for faster microbial growth. Model evaluation indicated the presence of particulate sugar adsorbed, especially under continuous feeding. It enabled accurate interpretation of observed particulate sugar values and this way, differentiating glycogen from the adsorbed starch remaining on the biomass. (C) 2013 Elsevier Ltd. All rights reserved...|$|R
50|$|Finally, a fast local {{hard disk}} drive can also cache {{information}} held on even <b>slower</b> data <b>storage</b> devices, such as remote servers (web cache) or local tape drives or optical jukeboxes; such a scheme is the main concept of hierarchical storage management. Also, fast flash-based solid-state drives (SSDs) {{can be used as}} caches for slower rotational-media {{hard disk drive}}s, working together as hybrid drives or solid-state hybrid drives (SSHDs).|$|R
40|$|Aspergillus flavus and A. versicolor {{were both}} {{shown to be}} weak {{pathogens}} of developing pistachio fruits, producing aflatoxin and sterigmatocystin, respectively. Aflatoxin concentrations approached those reported in cereal and legume seeds. Fungus lesions on the first hulls were followed by invasion of seeds despite the sclerified shell. Infections and mycotoxins present before harvest would presumably lead to further build-up after harvest if drying was <b>slow</b> or <b>storage</b> was under high humidity...|$|R
50|$|Most {{computer}} {{operating systems}} use {{the concept of}} virtual memory, allowing utilization of more primary storage capacity than is physically available in the system. As the primary memory fills up, the system moves the least-used chunks (pages) to secondary storage devices (to a swap file or page file), retrieving them later when they are needed. As more of these retrievals from <b>slower</b> secondary <b>storage</b> are necessary, the more the overall system performance is degraded.|$|R
40|$|Abstract. A buffer cache {{mechanism}} is usually employed in modern operating system {{to enhance the}} performance that is limited by <b>slow</b> secondary <b>storage.</b> In this paper, we present {{the implementation of a}} trace-driven simulator for buffer cache schemes that consider DRAM/PRAM hybrid main memory and flash memory based storages. The goal of simulator is to analyze the legacy buffer cache schemes by measuring the number of write operations on PRAM and the number of erase operations on flash memory...|$|R
