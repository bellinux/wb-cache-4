15|424|Public
50|$|Linguists {{disagree}} as {{to whether}} the unrounding of the lot vowel occurred independently in North America (probably occurring around the end of the 17th century) or was imported from certain types of <b>speech</b> <b>current</b> in Britain at that time.|$|E
5000|$|Walsh {{has become}} {{best known for}} his paper [...] "Optimal Contracts for Central Bankers," [...] {{published}} in 1995 in the American Economic Review. His theory is commonly referred to as the [...] "Walsh Contract." [...] In a 2004 <b>speech,</b> <b>current</b> Chairman of the Federal Reserve Ben S. Bernanke included [...] "Optimal Contracts for Central Bankers" [...] as one of the three most influential papers in macroeconomics over the past 25 years. His theory has also been questioned by Francisco Candel-Sánchez and Juan Cristóbal Campoy-Miñarro. In addition, Haizhou Huang and A. Jorge Padilla have commented on the concept.|$|E
50|$|Liberman's main {{ideas in}} {{phonology}} are as follows: 1) A non-contradictory theory of phonology is probably unattainable, because we lack {{the means of}} segmenting the <b>speech</b> <b>current</b> into phonemes. 2) The most adequate model of a phonetic change continuing for centuries, such as apocope and consonant shifts, {{is that of a}} change is caused by some event and is over once the potential of the initial impulse has been used up. The cause of every major change is another change. 3) Stress is not a force but a privileged position in a word, a position in which some oppositions occur that are not allowed in any other syllable. 4) The greatest phonetic changes in the history of Germanic were the concentration of all distinctive features in the root syllable and consonantal lenition as its consequence. 5) Allophones, that is, the phonetic variants of a phoneme, cannot be phonologized (by definition). 6) In Germanic, systemic changes of short vowels are reactions to changes in long vowels. Likewise, changes of voiced consonants are triggered by changes in voiceless consonants; what appears as voicing is really weakening.|$|E
40|$|The {{issue of}} {{representing}} speech rhythm is understood {{in this paper}} as the search for relevant primary parameters {{that will allow the}} formalisation of <b>speech</b> rhythm. <b>Current</b> <b>speech</b> synthesisers show that phonological models are not satisfactory with respect to the modelling of speech rhythmicity. Our analysis indicates that this may be in part related to the formalisation of rhythmic representation. Based on the observation of other communicative systems facing the problem of representing rhythm, parameters are described for representing speech rhythmic structures...|$|R
50|$|In February 2008, {{he gave a}} <b>speech</b> {{about his}} <b>current</b> work at the TED conference.|$|R
40|$|This study {{attempts}} {{to determine whether}} available ministerial speech training meets the <b>current</b> <b>speech</b> needs of active Protestant ministers by asking: 1. In the opinions of ministers, how comprehensive was their speech training? 2. In active ministers' opinions, what are the <b>current</b> <b>speech</b> needs of ministers? 3. According to the information obtained from school catalogs and the speech departments of the schools from which the ministers graduated, what speech training is available at these institutions? 4. According to {{a comparison of the}} speech needs (given by ministers) with the available speech training (indicated by the schools) : A. Does the available speech training meet the <b>current</b> <b>speech</b> needs? B. If not, what recommendations are needed...|$|R
5000|$|As a researcher, {{lecturer}} {{and author}} {{he has been}} active in the fields of English and general linguistics, contrastive and applied linguistics, sociolinguistics, language policy and planning, language in relation to identity, culture, ethnicity and nationalism, language attitudes, written language and literacy, terminology and bibliography, translation theory, history of linguistics, the status of Serbo-Croatian, political manipulations of language and wartime hate <b>speech,</b> <b>current</b> Serbian slang, etc. He is the author of some twenty books, ten edited volumes, and scores of articles, book chapters and reviews in international journals, congress proceedings and other collective volumes, mostly in English or Serbo-Croatian but some in German, French, Spanish and several other languages. (For a selective list see below). Among his noted contributions are his early research on English prepositions, acknowledged as a thematic and methodological precursor of cognitive linguistics, his notion of graphic relativity {{as an extension of the}} Sapir-Whorf hypothesis of linguistic relativity, his role in defining and organising applied linguistics as an academic discipline in Yugoslavia and internationally, the conception of Serbo-Croatian as one polycentric standard language linguistically but several national languages politically, and his recent pioneering work on lexical blends in Serbian.He has participated in several dozen Yugoslav, European and world congresses, symposia and other conferences, often also as an organiser or invited plenary speaker. He was the academic organiser of two international conferences at the University of London and editor (with Celia Hawkesworth) of their proceedings: Language Planning in Yugoslavia (Columbus, OH: Slavica, 1992. Pp. 233. [...] ) and Language in the Former Yugoslav Lands (Bloomington, IN: Slavica, 2004. Pp. 325. [...] ). He has edited scholarly journals and, as translator and editor, introduced to the Yugoslav public the works of some leading linguists (Chomsky, Sapir, Whorf) and modern linguistic disciplines (transformational-generative grammar, sociolinguistics, psycholinguistics). At the same time, he has kept international audiences informed about the changing language situation in Yugoslavia and its successor states, with a special focus on the official dissolution of Serbo-Croatian.|$|E
40|$|Recent work on {{evaluation}} of spoken dialogue systems indicates that better algorithms {{are needed for}} the presentation of complex information in <b>speech.</b> <b>Current</b> dialogue systems often rely on presenting sets of options and their attributes sequentially. This places a large memory burden on users, who have to remember complex trade-offs between multiple options and their attributes. To address these problems we build on previous work using multiattribute decision theory to devise speech-planning algorithms that present usertailored summaries, comparisons and recommendations that allow users to focus on critical differences between options and their attributes. We discuss the differences between speech and text planning that result from the particular demands of the speech situation. ...|$|E
40|$|Word-final /t/-deletion {{refers to}} a common {{phenomenon}} in spoken English where words such as /wEst / “west ” are pronounced as [wEs] “wes ” in certain contexts. Phonological variation like this is common in naturally occurring <b>speech.</b> <b>Current</b> computational models of unsupervised word segmentation usually assume idealized input that is devoid {{of these kinds of}} variation. We extend a non-parametric model of word segmentation by adding phonological rules that map from underlying forms to surface forms to produce a mathematically well-defined joint model as a first step towards handling variation and segmentation in a single model. We analyse how our model handles /t/-deletion on a large corpus of transcribed speech, and show that the joint model can perform word segmentation and recover underlying /t/s. We find that Bigram dependencies are important for performing well on real data and for learning appropriate deletion probabilities for different contexts. 1...|$|E
3000|$|Perceptual {{evaluation}} of <b>speech</b> quality, the <b>current</b> ITU-t standard for intrusive objective measurement of speech quality [...]...|$|R
50|$|ASHA {{was founded}} in 1925 as the American Academy of <b>Speech</b> Correction. The <b>current</b> name was adopted in 1978.|$|R
50|$|Malone, Kemp. Studies in Heroic Legend and in <b>Current</b> <b>Speech.</b> S. Einarsson & N.E. Eliason, eds. Copenhagen: Rosenkilde & Bagger, 1959.|$|R
40|$|Abstract—Signal {{processing}} methods {{can improve the}} qual-ity and intelligibility of oesophageal <b>speech.</b> <b>Current</b> methods show only moderate improvement leaving potential for better results. Quantifying parameters of oesophageal speech relative to laryngeal (normal) speech would help {{in the design of}} future enhancement methods for oesophageal speech. We quantified parameters of a source–filter model on a database of sustained vowels in Spanish from 4 oesophageal and 4 normal speakers. A ten-parameter glottal waveform model was used as the source and an autoregressive model was used as the filter. Classification, us-ing a log-spectral distance measure, showed that all oesophageal speech samples were classified as whisper voice types; a voice type with a signal to noise ratio of- 20 dB. Filter parameters representing spectral amplitudes and bandwidths had a large degree of variation for oesophageal speech comparative to the degree of variation for normal speech (Brown–Forsythe test, F < 0. 001). Source metrics, noise to harmonic ratio (NHR) and variation in fundamental frequency, were also significantly greater for oesophageal speech (t-test, P < 0. 001). These results show a greater degree of nonstationarity, and a noisier glottal waveform, for oesophageal speech comparative to normal speech. I...|$|E
40|$|Hermus K., Wambacq P., Van Compernolle D., "Fully {{adaptive}} SVD-based noise removal for robust speech recognition", Proceedings Workshop on robust {{methods for}} speech recognition in adverse conditions, pp. 223 - 226, May 25 - 26, 1999, Tampere, Finland. This paper {{deals with the}} problem of the recognition of speech corrupted by additive noise at moderate SNR ratios. The proposed technique - based on Singular Value Decomposition (SVD), and fully adaptive - outperforms well-known approaches as Nonlinear Spectral Estimation and SNR-Normalisation for the recognition of large vocabulary continuous <b>speech.</b> <b>Current</b> techniques for robust speech recognition take advantage of slowly varying and/or accurately modeled environments. Deviations from these prior assumptions greatly compromise the performance. Our new approach is based on SVD and tries to overcome these limitations. This technique automatically removes additive noise by suppressing low energetic, noise related, singular components of the Hankel matrix constructed from the original signal. Providing the SVD-algorithm with prior knowledge about the noise highly improves the efficiency. The algorithm is fully adaptive, and works in real-time. Recognition experiments on a database with large vocabulary, continuous speech (Resource Management) show that the WER is more than halved. status: publishe...|$|E
40|$|The SPECS project aims {{to develop}} a speech-driven device {{that will allow the}} home {{environment}} to be controlled (for example turning on or off the lights or television). The device developed will be targeted at older people and people with disabilities and will be sensitive to disordered <b>speech.</b> <b>Current</b> environmental control systems (ECS) work using either a switch interface or speech recognition software that does not comprehend disordered speech well. Switch-interface systems are often slow and complicated to use and the uptake of the available speech recognition system has been poor. A significant proportion of people requiring electronic assistive technology (EAT) have dysarthria, a motor speech disorder, associated with their physical disability. Speech control of EAT is seen as desirable for such people but machine recognition of dysarthric speech is a difficult problem due to the variability of their articulatory output. Other work on large vocabulary adaptive speech recognition systems and speaker dependent recognisers has not provided a solution for severely dysarthric speech. Building on the work of the STARDUST project our goal is {{to develop a}}nd implement speech recognition as a viable control interface for people with severe physical disability and severe dysarthria. The SPECS project is funded by the Health Technology Devices Programme of the Department of Health...|$|E
5000|$|She is a [...] "yearly {{fundraising}} participant," [...] {{winner of}} the 2004 Rotary Club <b>Speech</b> Contest, <b>current</b> coordinator of the Neighborhood Watch program for her area, a math tutor, a County Chamber Singers, an honorary member of the local Historical Society, and one point worked as a batgirl for the New York Mets [...]|$|R
2500|$|Sidney Hillman, Доклад о настоящем положении России. (<b>Speech</b> on the <b>current</b> {{situation}} in Russia) [...] New York: Board of Directors of the Russian-American Industrial Corporation, n.d[...]|$|R
50|$|The Alfred M. Landon Lecture Series is {{a series}} of <b>speeches</b> on <b>current</b> public affairs, which is {{organized}} and hosted by Kansas State University in Manhattan, Kansas. It is named after Kansas politician Alf Landon, former Governor of Kansas and Republican presidential candidate. The first lecture in the series was given by Landon on December 13, 1966.|$|R
40|$|The main aim of a text-to-speech {{synthesis}} {{system is}} to convert ordinary text into an acoustic signal that is indistinguishable from human speech. This thesis presents an architecture to implement a concatenative speech synthesis algorithm targeted to FPGAs. Many current text-to-speech systems {{are based on the}} concatenation of acoustic units of recorded <b>speech.</b> <b>Current</b> concatenative speech synthesizers are capable of producing highly intelligible speech. However, the quality of speech often suffers from discontinuities between the acoustic units, due to contextual differences. This is the easiest method to produce synthetic speech. It concatenates prerecorded acoustic elements and forms a continuous speech element. The software implementation of the algorithm is performed in C whereas the hardware implementation is done in structural VHDL. A database of acoustic elements is formed first with recording sounds for different phones. The architecture is designed to concatenate acoustic elements corresponding to the phones that form the target word. Target word corresponds to the word that has to be synthesized. This architecture doesn 2 ̆ 7 t address the form discontinuities between the acoustic elements as its ultimate goal is the synthesis of speech. The Hardware implementation is verified on a Virtex (v 800 hq 240 - 4) FPGA device...|$|E
40|$|At many office {{workplaces}} employees must perform verbal tasks, such {{as reading}} or writing text. Often they must {{do so in the}} presence of background speech due to conversations or phone calls among colleagues. Background speech has been shown to impair cognitive performance in laboratory studies. It is assumed that these negative effects can be generalized to real life office tasks and, therefore, a decline in employees' efficiency is expected to occur in ambient <b>speech.</b> <b>Current</b> approaches to predicting performance effects of background speech focus on its physical characteristics and consider speech as a complex time-varying acoustic signal with a certain frequency characteristic. Here, impairments of performance have been mostly tested using simple short term-memory tasks, such as remembering a series of digits. However, verbal office tasks, like reading, are far more than remembering single items in their correct order, as words need to be stored and connected to understand and derive information. Analogously, background speech is defined by more than its physical characteristics since coherent narration comprises informational content based on semantics, syntax, and pragmatics that might contribute to performance decrements in office tasks. In the current paper, we present two experiments that explored the interrelation between background speech coherence and its impact on reading comprehension as a verbal task. The results are discussed with respect to noise abatement measures in office environments...|$|E
40|$|Background A {{hallmark}} of Parkinson’s disease (PD) is a mismatch between the perceived effort and actual forces exerted during functional {{activities such as}} <b>speech.</b> <b>Current</b> evidence supports therapy to help reset this perception of effort, but the neurological underpinnings of such treatments are unclear. This study examined brain activity during tongue movements performed at varying levels of effort to determine (1) which brain areas are involved in each task, and (2) which areas, if any, scale in activation according to effort level. These results, considered with the neurological changes associated with PD, {{can be used to}} develop and refine treatment techniques for PD. Methods The structural and functional magnetic resonance (MR) data were previously collected from 20 healthy 40 - 60 year-old adults. Participants performed phoneme (speech sound) repetition and isometric tongue-to-palate presses while MR images were obtained. Ten datasets underwent whole brain analysis via SPM software to create a mask of shared activation. This mask was applied to the remaining 10 datasets to extract scaling data. Results/Conclusion Multiple areas including sensory, motor, and insular cortices were active during study tasks. The only area exhibiting statistically significant scaling was the left secondary sensorimotor cortex during the isometric tongue press. This area has been linked to processing of light touch, tactile attention, and somatosensory integration for voluntary skeletal movements. Additional activations were noted in the right insula, which is associated with motor control of speech and swallowing movements, as well as self-awareness...|$|E
40|$|The Spoken Web, an {{interconnected}} {{collection of}} spoken content accessed through audio-only cell phones, holds {{the promise of}} transforming information access for users in developing regions. The scale of the Spoken Web is, however, limited because <b>current</b> <b>speech</b> retrieval technology is only affordably deployable {{for a handful of}} languages. This paper proposes rethinking the conventional keyword query paradigm to instead develop systems that support a longer, richer, and more fluid interaction style that is better suited to both the affordances of spoken interaction and to the limitations of <b>current</b> <b>speech</b> technology...|$|R
50|$|As {{more and}} more of the English {{population}} moved into towns and cities during the 20th century, non-regional, Standard English accents increasingly became a marker of personal social mobility. Universal primary education was also an important factor as it made it possible for some to move out of their rural environments into situations where other modes of <b>speech</b> were <b>current.</b>|$|R
40|$|Being {{confronted with}} {{spontaneous}} <b>speech,</b> our <b>current</b> annotation scheme requires alterations that would reflect the abundant use of non-sentential fragments with clausal meaning tightly connected to their context, {{which do not}} systematically occur in written texts. The {{purpose of this paper}} is to list the common patterns of non-sentential fragments and their contexts and to find a smooth resolution of their semantic annotation. ...|$|R
40|$|Disfluency is the {{interruption}} {{of an otherwise}} continuous flow of <b>speech.</b> <b>Current</b> views explain speech disfluency {{in terms of both}} an epiphenomenon of cognitive overload, and as an intentional function for easing social interaction to convey non-explicit thought processes. This study looked at both of these hypotheses, with main focus upon disfluency as a form of social communication. The disfluencies focused upon were: ‘uh’, ‘um’, ‘hmm’, ‘oh’, laughter and silences. The Autism Spectrum Disorder is partially defined by a lack of social awareness. The Autism Quotient (AQ) test is used for determining where any individual lies on the continuum from typical development (TD) to Autism Spectrum Disorder (ASD). This study used the AQ as a measure of meta-cognitive awareness. TD students at the University of Edinburgh (N= 50) undertook both a written AQ test and a verbal general knowledge test. Disfluency use during the general knowledge test was analyzed and compared to: utterance length, question answer confidence ratings, gender and AQ scores. All modeled disfluencies were found to increase with utterance length, which has been related to cognitive load (Oviatt, 1995; Shriberg, 1996). The use of ‘um’, laughter, and silence increased during moments of uncertainty, as shown by the individual confidence ratings. However, this does not distinguish whether participants were intentionally communicating uncertainty or whether it was accidental. Conversely, the use of ‘uh’ increased with confidence, insinuating a distinction between the uses of ‘uh’ and ‘um’ consistent with findings by Clark and Fox Tree (2002). Laughter was predicted significantly by uncertainty and gender (more common in females) consistent with Provine (1996), who theorized laughter as a social buffer rather than a communication tool. The most noteworthy finding was that an increased AQ score predicts a decreased use of fillers; ‘uh’, ‘um’, ‘oh’ and ‘hmm’. This suggests that filler use is significantly related to meta-cognitive interaction and thus may serve as an intentional function for communication. These results indicate that different disfluencies serve different functions. Furthering this, the use of fillers (‘uh’, ‘um’, ‘oh’ and ‘hmm’) can be considered as words rather than speech errors...|$|E
40|$|Pitch {{discrimination}} {{is a fundamental}} property of the human auditory system. Our understanding of pitch-discrimination mechanisms is important from both theoretical and clinical perspectives. The discrimination of spectrally complex sounds is crucial in the processing of music and <b>speech.</b> <b>Current</b> methods of cognitive neuroscience can track the brain processes underlying sound processing either with precise temporal (EEG and MEG) or spatial resolution (PET and fMRI). A combination of different techniques is therefore required in contemporary auditory research. One {{of the problems in}} comparing the EEG/MEG and fMRI methods, however, is the fMRI acoustic noise. In the present thesis, EEG and MEG in combination with behavioral techniques were used, first, to define the ERP correlates of automatic pitch discrimination across a wide frequency range in adults and neonates and, second, they were used to determine the effect of recorded acoustic fMRI noise on those adult ERP and ERF correlates during passive and active pitch discrimination. Pure tones and complex 3 -harmonic sounds served as stimuli in the oddball and matching-to-sample paradigms. The results suggest that pitch discrimination in adults, as reflected by MMN latency, is most accurate in the 1000 - 2000 Hz frequency range, and that pitch {{discrimination is}} facilitated further by adding harmonics to the fundamental frequency. Newborn infants are able to discriminate a 20 % frequency change in the 250 - 4000 Hz frequency range, whereas the discrimination of a 5 % frequency change was unconfirmed. Furthermore, the effect of the fMRI gradient noise on the automatic processing of pitch change was more prominent for tones with frequencies exceeding 500 Hz, overlapping with the spectral maximum of the noise. When the fundamental frequency of the tones was lower than the spectral maximum of the noise, fMRI noise had no effect on MMN and P 3 a, whereas the noise delayed and suppressed N 1 and exogenous N 2. Noise also suppressed the N 1 amplitude in a matching-to-sample working memory task. However, the task-related difference observed in the N 1 component, suggesting a functional dissociation between the processing of spatial and non-spatial auditory information, was partially preserved in the noise condition. Noise hampered feature coding mechanisms more than it hampered the mechanisms of change detection, involuntary attention, and the segregation of the spatial and non-spatial domains of working-memory. The data presented in the thesis can be used to develop clinical ERP-based frequency-discrimination protocols and combined EEG and fMRI experimental paradigms. nrpages: 69 status: publishe...|$|E
40|$|In British English {{the term}} 'accent' refers to {{systematic}} variations in pronunciation, {{often associated with}} particular geographic regions. Accent {{is one of the}} most frequently cited causes of variability in speech. The problem of accents is becoming more important with the advancement of computerised services and Automatic Speech Recognition (ASR) systems. Speech recognition technology is used in a wide range of applications and services such as health care, education, automated call centres, authentication and information services. In many of these applications accents and foreign languages posed a problem for speech recognition developers because, if the people using the system could not be understood, they might become frustrated and stop using the system. For example, a recent news story reported that an automated phone system deployed by Birmingham City Council could not cope with ‘Brummie’ accents [1]. This research investigates how we can exploit the knowledge of accents to obtain both rapid and robust ASR systems for British accented utterances using only 30 seconds of speech. When we hear another person’s speech for the first time, we quickly establish a 'profile' of that person based on his or her speech, in terms of factors such as gender, age, accent, and social group. It is possible that we use this characterization to adapt very quickly to that person’s <b>speech.</b> <b>Current</b> ASR systems typically differentiate between genders, but otherwise tend to ignore important factors including accents. Additionally, conventional adaptation techniques for ASR require a substantial amount of training material from each individual to be able to adapt the system in order to have a more user specific system. Now the question is whether the concept of ‘regional accent’ is useful to overcome these two problems. The first section describes the techniques used to visualise the space where accent recognition is performed, otherwise known as the ‘accent recognition’ space, and show the extent to which the emergent structure is consistent with subjective notions of accent. The two techniques used are Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA). We then interpret speech recognition results on accented speech from 14 different British regions, analysed in terms of their structure in the ‘accent recognition’ space. Following this we present results on accent adaptation using conventional adaptation techniques such as MAP (Maximum A Posteriori Adaptation) and MLLR (Maximum Likelihood Linear Regression). We use these accent adaptation techniques along with the knowledge from accent recognition techniques, such as the ACCDIST accent recognition measure, to provide an accent robust, rapid speaker adaptation system. 1. 	“Brummie accents baffle automated phone system [...] . at Birmingham City Council”, Daily Mail, 5 th November 2012. ...|$|E
50|$|The Clarisse House is the {{official}} residence of the Prime Minister of Mauritius at 37, Riverwalk Vacoas, Plaines Wilhems. It is frequently used for governmental conferences, summits and other official purposes, including the Prime Minister's New Year <b>Speech.</b> The <b>current</b> occupant of this house is the present Prime Minister, Pravind Jugnauth. The Prime Minister's Office is located at Port Louis.|$|R
50|$|The {{demonstration}} {{that a similar}} motor resonance is activated during speech listening and involves tongue-related motor centers (European Journal of Neuroscience, 2002). He recently further demonstrates that this motor activation evoked by speech listening is functional to speech perception. This result shows {{for the first time}} a causal relationship between action representation and perception (The motor somatotopy of <b>speech</b> perception, <b>Current</b> Biology, 2009).|$|R
3000|$|The {{a priori}} {{knowledge}} about the speech includes the following autoregressive model [14]: The <b>current</b> <b>speech</b> sample s(n) {{is assumed to be}} a sum of the predicted speech s [...]...|$|R
3000|$|... 0 of speaker A and B in the <b>current</b> <b>speech</b> segment, respectively. I {{represents}} {{the number of}} frames in the aligned sequence, and w(i) is the weighting factor, based on the frame signal power 8.|$|R
40|$|We {{present a}} speech {{retrieval}} system aimed at retrieving information from audio recordings containing <b>speech.</b> The <b>current</b> system contains 4. 5 hours of radio news and accepts textual queries. The fully automatic indexing was done using speech recognition techniques. Indexing speech documents is challenging, because word boundaries {{are difficult to}} detect, and recognition errors influence the retrieval effectiveness. Our indexing vocabulary consists of 5000 phone sequences. To obtain such a set of suitable indexing features, we developed an efficient selection algorithm which uses statistical information derived from the audio collection. 1 Introduction We will demonstrate {{one of the first}} speech retrieval systems and we will report on our first experiences with this system. A speech retrieval system accepts vague queries and it performs a best-match search to find those speech recordings that are relevant to the query. The database of our <b>current</b> <b>speech</b> retrieval system consis [...] ...|$|R
40|$|Spoken {{language}} {{is the most}} convenient and natural means by which people {{interact with each other}} and is, therefore, a promising candidate for human-machine interactions. Speech also offers an additional channel for hands-busy applications, complementing the use of motor output channels for control. <b>Current</b> <b>speech</b> recognition systems vary considerably across a number of important characteristics, including vocabulary size, speaking mode, training requirements for new speakers, robustness to acoustic environments, and accuracy. Algorithmically, these systems range from rule-based techniques through more probabilistic or self-learning approaches such as hidden Markov modeling and neural networks. This tutorial begins with a brief summary of the relevant features of <b>current</b> <b>speech</b> recognition systems and {{the strengths and weaknesses of}} the various algorithmic approaches...|$|R
50|$|Robots may {{interpret}} strayed {{noise as}} <b>speech</b> instructions. <b>Current</b> {{voice activity detection}} (VAD) system uses the complex spectrum circle centroid (CSCC) method and a maximum signal-to-noise ratio (SNR) beamformer. Because humans usually look at their partners when conducting conversations, the VAD system with two microphones enable the robot to locate the instructional speech by comparing the signal strengths of the two microphones. Current system is {{able to cope with}} background noise generated by televisions and sounding devices that come from the sides.|$|R
40|$|This paper {{introduces}} a new method for eliminating {{high frequency noise}} from audio <b>speech</b> signals. <b>Current</b> noise reduction techniques have generally proven to be effective, yet these typically exhibit certain undesirable characteristics. Distortion and/or alteration of the audio characteristics of primary audio sound is a common problem. Also user intervention in identifying the noise profile is sometimes necessary. The proposed technique is centered on the MAC FIR filtering technique for noise removal but uses a novel architecture whereby advanced signal processing techniques are used to identify an...|$|R
40|$|In {{the face}} of growing race/gender labor market inequality, women-of-color unionists are calling for greater {{responsiveness}} of the labor movement {{to the needs of}} workers of color. This article details how women of color within the labor movement are pushing for structural change within the institution to increase the voice of women and people of color in the labor movement and explores the significant impact these women are having on the institution of labor. In noting the emergence of black and Latina female leaders, the study explores what factors propelled and continue to motivate their careers, and asks what effect, if any, their leadership has on the labor movement’s agenda with respect to diversity. To address these questions, the study relies on interviews with key actors affecting institutional change: black and Latina female union leaders, survey data, resolutions, <b>speeches,</b> <b>current</b> labor force data on black and Latina workers, and data on union involvement by race and gender. Clayola Brown, head of the Labor Coalition for Community Action, addressed an assembly of minority unionists who gathered at a summit held before the 2005 American Federation of Labor–Congress of Industrial Organi-zations (AFL–CIO) convention and declared, “We are {{a force to be reckoned}} with. ” In {{the face of}} persistent and growing race/gender labor market inequality...|$|R
