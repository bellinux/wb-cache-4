10|13|Public
5000|$|Partitioning off several {{gigabytes}} of {{hard drive}} and leaving that space empty will ensure a reliable <b>scratch</b> <b>disk.</b> Hard drive space, on a per-gigabyte basis, is far cheaper than RAM, though performs far slower. Though dedicating a separate physical {{drive from the}} main operating system and software can improve performance, a <b>scratch</b> <b>disk</b> often will not match RAM for speed.|$|E
5000|$|Scratch {{space is}} {{space on the}} hard disk drive that is {{dedicated}} for only temporary storage. It cannot be used to permanently back up files. Scratch disks can be set to erase all data at regular intervals so that the disk space is left free for future use. The management of <b>scratch</b> <b>disk</b> space is typically dynamic, occurring when needed.|$|E
50|$|The Rossmann cluster {{consists}} of HP ProLiant DL165 G7 compute nodes with 64-bit, dual 12-core AMD Opteron 6172 processors (24 cores per node), either 48 gigbytes or 96 GB {{of memory and}} 250 GB of local disk for system software and scratch storage. Nodes with 192 GB of memory and either 1 terabyte or 2 TB of local <b>scratch</b> <b>disk</b> also are available. Rossmann {{consists of}} five logical sub-clusters, each with a different memory and storage configuration. All nodes have 10 Gigabit Ethernet interconnects.|$|E
40|$|In the ATLAS Online {{computing}} farm, {{the majority}} of the systems are network booted - they run an operating system image provided via network by a Local File Server. This method guarantees the uniformity of the farm and allows very fast recovery in case of issues to the local <b>scratch</b> <b>disks.</b> The farm is not homogeneous and in order to manage the diversity of roles, functionality and hardware of different nodes we developed a dedicated central configuration system, ConfDB v 2. We describe the design, functionality and performance of this system and its web-based interface, including its integration with CERN and ATLAS databases and with the monitoring infrastructure...|$|R
50|$|These {{assumptions}} may {{be reasonable}} for transmissions over a binary symmetric channel. They may be unreasonable for other media, {{such as a}} DVD, where a single <b>scratch</b> on the <b>disk</b> can cause an error in many neighbouring symbols or codewords.|$|R
5000|$|A {{wide variety}} of {{failures}} can cause physical damage to storage media, which may result from human errors and natural disasters. CD-ROMs can have their metallic substrate or dye layer <b>scratched</b> off; hard <b>disks</b> can suffer any of several mechanical failures, such as head crashes and failed motors; tapes can simply break.|$|R
5000|$|Scratch {{space is}} {{commonly}} used in graphic design programs, such as Adobe Photoshop. It is used when the program needs more memory, {{and the amount of}} available system RAM is insufficient. A common error in that program is [...] "scratch disks full", which occurs when one has left the scratch disks configured to the default setting, being the boot drive. Many computer users gradually fill up their primary hard drive with permanent data, slowly reducing the amount of space the <b>scratch</b> <b>disk</b> may take up.|$|E
50|$|Calling All Cars on the Vegas Strip is {{the debut}} album by the band Jucifer first {{released}} in 1998 through the independent label Crack Rock records {{and then in}} 2000 after have signed to Capricorn Records label. The album contains an unusual mixture of metal, punk, hardcore, doom, sludge, alternative elements and <b>scratch</b> <b>disk</b> sound effects between track to track. This distinctive and unique style wasn't explored by many bands in the late 1990s, was part of their sound during the 2000s decade, until the release of Throned in Blood in 2010.|$|E
50|$|Mac OS 8.5 and Mac OS 9 {{use only}} BOOTP/DHCP to get IP information, {{followed}} by a TFTP transfer of the Mac OS ROM file. Next, two volumes are mounted via AppleTalk over TCP on which the client disk images reside. All in all, the Classic Mac OS uses three images; a System image which contains the operating system and may contain applications. Next a private image (or <b>scratch</b> <b>disk)</b> is mounted in an overlay over the read-only System image. Finally, an applications image is mounted. This image, however, may be empty.|$|E
5000|$|Mechanical: A {{metal plate}} is {{designed}} to <b>scratch</b> the brake <b>disk</b> causing a noise when the pad has worn down to the desired level. Some cars, such as the 2004 Honda Civic, use a plate that has been tuned specifically to sound like fingernails on a chalkboard, in order to manipulate the owner into replacing the worn brake pads as soon as possible.|$|R
40|$|Networking at an ATLAS Tier- 1 (T 1) {{facility}} is a demanding aspect which {{is vital to}} the overall performance and efficiency of the facility. External connectivity of the facility to other tiers of the Large Hadron Collider Optical Private Network (LHCOPN) is largely via dedicated lightpaths as required to meet Memorandum of Understanding (MOU) commitments. Our primary dedicated link to CERN has an independent, although smaller capacity, dedicated backup link for redundancy. Dedicated lightpaths to the Canadian Tier- 2 facilities, and the international partner Tier- 1 facilities failover to national and international research networks in the event of failure. The distance between TRIUMF and CERN, and even TRIUMF to some of it's Tier- 2 facilities in Canada is thousands of kilometers. Transferring data at the hundreds of terabytes of data level (per year) over such distances and complex networks requires both dedicated bandwidth and network resiliency. Failure scenario, including both failover and fail-back, must be handled efficiently {{and for the most part}} automatically. Although modern network routing protocols handle this well, monitoring processes become key to management of the infrastructure as the complexity of our Tier- 1 site connectivity grows. Internal networking efficiency is also vital to the ATLAS computing model. Large data sets are moved from onsite storage to local <b>scratch</b> <b>disks</b> before analysis, and proper network scaling is vital for efficient use of compute nodes. Although 10 Gigabit network infrastructure (routers) is well established, server 10 Gigabit network components and drivers are not as mature as their 1 Gigabit counterparts, and resource issues have been observed during extreme load testing. In this paper, internal facility networking, as well as external connectivity issues will be discussed...|$|R
50|$|Optical media disks {{often require}} {{professional}} preventative or routine cleaning to ensure data accuracy and accessibility; {{those with no}} professional experience may <b>scratch</b> the <b>disk</b> surface {{in their attempt to}} clean the disk. Build up of dust and oily contaminants on the disk surface, and fingerprints can typically impede the laser beam’s ability to penetrate the substrate to read the data layer, and more often impede writing. Minor interferences with reading is handled by error correction technology. If an audio CD (with a much lower accuracy threshold than a data disk) becomes dirty, it can be cleaned safely with a dry, soft lint-free cloth, holding the disc by the edges or by the center hole. Light dirt that is not removed by this method can be removed with a cloth dampened with water or a suitable optical disc-cleaning fluid. It has been advised that excess dust be blown off an optical disc before reading, to avoid buildup of dust in the reader, particularly on the laser.|$|R
50|$|It uses Long-GOP MPEG2 {{compression}} {{to achieve}} a maximum data rate of 25mps, {{which is the same}} as normal DV video. This standard yields extremely High Definition images in shorter space. The term long refers to the fact that P- and B-frames are used between I-frame intervals. At {{the other end of the}} spectrum, the opposite of long-GOP MPEG is I-frame-only MPEG, in which only I-frames are used. Formats such as IMX use I-frame-only MPEG, which reduces temporal artifacts and improves editing performance. However, I-frame-only formats have a significantly higher data rate because each frame must store enough data to be completely self-contained. Therefore, although the decoding demands on your computer are decreased, there is a greater demand for <b>scratch</b> <b>disk</b> speed and capacity.|$|E
30|$|As hydroclimatic {{simulations}} involve huge sized data, {{the high}} volume of storage is required. In order to run many simulations {{in a limited}} working space or to save the output data for future analysis, users generally move output data, which are stored in a <b>scratch</b> <b>disk</b> space, to storage such as tape storages or data grid for example, iRODS [10].|$|E
30|$|A typical Nyx {{simulation}} evolves a cosmology {{from high}} redshift (z > 100) {{to the present}} (z = 0) in many thousands of time steps. The size of each time step {{is limited by the}} standard CFL condition for the baryonic fluid, as well as by a quasi-CFL constraint imposed on the dark matter particles, and additionally by the evolution of the scale factor a in the Friedmann equation; details of these constraints are described in Almgren et al. (2013). Currently the largest Nyx simulations are run on a 4, 096 ^ 3 grid, and at this scale each plotfile at a single time step is ∼ 4 TiB, with checkpoint files being even larger; a complete data set for a single simulation therefore reaches well into the petascale regime. A single simulation can therefore fill up a typical user allocation of <b>scratch</b> <b>disk</b> space (O(10) TiB) in just a few time steps. We see then that modern simulation data sets represent a daunting challenge for both analysis and storage using current supercomputing technologies.|$|E
5000|$|In {{computer}} security, a sandbox is {{a security}} mechanism for separating running programs, usually {{in an effort}} to mitigate system failures or software vulnerabilities from spreading. It is often used to execute untested or untrusted programs or code, possibly from unverified or untrusted third parties, suppliers, users or websites, without risking harm to the host machine or operating system. [...] A sandbox typically provides a tightly controlled set of resources for guest programs to run in, such as <b>scratch</b> space on <b>disk</b> and memory. Network access, the ability to inspect the host system or read from input devices are usually disallowed or heavily restricted.|$|R
5000|$|The crab scratch {{was invented}} by DJ Qbert while in Japan. Qbert then took the idea back to San Francisco and after showing the <b>scratch</b> to DJ <b>Disk,</b> he created a move that used 3 or 4 fingers: the crab. Later in 1995, while the DMC USA finals were being held in San Francisco, a group of DJs and judges which {{included}} The Beat Junkies, The X-Men (now called the X-ecutioners), {{and the rest of}} ISP among others got together for what would later be known as the [...] "Famous Warehouse Session" [...] at Yoga Frog's old mobile DJ warehouse. It was at this session that Qbert publicized the new scratch.|$|R
40|$|In {{this paper}} we {{introduce}} error-control techniques {{for improving the}} error-rate performance that is delivered to an application in situations where the inherent error rate of a digital transmission system is unacceptable. The acceptability of a given level of bit error rate depends on the particular application. For examples, certain types of digital speech transmission are tolerant to fairly high bit error rates. Other types of applications such as electronic funds transfer require essentially error-free transmission. For example, FEC {{is used in the}} satellite and deep-space communications. A recent application is in audio CD recordings where FEC is used to provide tremendous robustness to errors so that clear sound reproduction is possible even in the presence of smudges and <b>scratches</b> on the <b>disk</b> surface...|$|R
40|$|With {{the advent}} of {{computers}} with many processors, it becomes unclear how to best exploit this advantage. For example, matrices can be inverted by applying several processors to each vector operation, or one processor {{can be applied to}} each matrix. The former approach has diminishing returns beyond a handful of processors, but how many processors depends on the computer architecture. Applying one processor to each matrix is feasible with enough ram memory and <b>scratch</b> <b>disk</b> space, but the speed at which this is done is found to vary by a factor of three depending on how it is done. The cost of the computer must also be taken into account. A computer with many processors and fast interprocessor communication is much more expensive than the same computer and processors with slow interprocessor communication. Consequently, for problems that require several matrices to be inverted, the best speed per dollar for computers is found to be several small workstations that are networked together, such as in a Beowulf cluster. Since these machines typically have two processors per node, each matrix is most efficiently inverted with no more than two processors assigned to it...|$|E
40|$|For large {{scientific}} visualization applications, it {{is often}} impossible to hold the entire datasets in main memory, even on supercomputers. Previously, we proposed the I/O-filter technique, {{which is the first}} I/O-optimal method for the problem of isosurface extraction in scientific visualization. I/O-filter works by indexing and re-organizing the datasets in disk, so that isosurface can be extracted with a very small amount of disk I/O's. The main advantage of this approach is that datasets much larger than main memory can be visualized very efficiently, possibly even on low-end machines. The original I/O-filter technique uses the I/O-optimal interval tree of Arge and Vitter as the indexing data structure, together with the isosurface engine from Vtk (one of the currently best visualization packages). The main shortcoming of this approach was the overheads of the <b>disk</b> <b>scratch</b> space and the preprocessing time necessary to build the data structure, and of the disk space needed to hold th [...] ...|$|R
40|$|To {{compete with}} solid state drives (SSDs), hard disk drives (HDDs) must improve their {{performance}} in capacity, speed and reliability, which requires the spacing between the magnetic disk, used to store information, and the magnetic transducer, {{used to read}} information from and write information onto the disk, to decrease. This distance is now approaching 5 nm, and, accordingly, the distance between a slider, embedding the transducer, and the disk ranges from several nanometers to several micrometers, which makes the gas flowing between the slider and the disk rarefied. This dissertation applies rarefied gas dynamics to investigate several issues related to HDDs' performance. Particle contamination on the slider may <b>scratch</b> the <b>disk</b> and induce loss of data. An improved model is proposed to numerically study particle contamination on a thermal flying-height control (TFC) slider, which adjusts the transducer-disk spacing by use of a small heater embedded in the slider near the transducer. It is found that the currently used model is sufficiently accurate despite its simple form. The temperature increase inside HDDs during operation may affect their reliability. This dissertation derives an analytical formula for the gas-flow induced shear force in the head-disk interface (HDI) and uses it to investigate how the raised temperature affects the slider's flying attitude and the shear forces on the slider and the disk. Numerical prediction of a TFC slider's flying performance lays the foundation for commercial designs of TFC sliders. An improved model is proposed to calculate the heat flux on the TFC slider and {{it is found that}} the currently used model is accurate enough for this purpose. Finally, a general approach is proposed to numerically investigate a TFC slider flying in gas mixtures...|$|R
40|$|Abstract. Nowadays, {{hard disk}} drives (HDD) {{technology}} {{are being developed}} continuously {{in order to increase}} the capacity, and reduce the size of HDD to meet user requirements. To increase the capacity which is equivalent to increasing read/write ability, the flying clearance must be reduced. Current new HDD models show that the fly height is lower than 0. 3 µm. If the height of a particle or contamination is higher than 0. 3 µm, the magnetic head will <b>scratch</b> the magnetic <b>disk</b> surface. However the process of cleaning in the HDD industry cannot remove particles with size smaller than 0. 3 µm[1]. Therefore laser cleaning is selected first because this method can remove small particles [2]. and it does not damage the magnetic head. This research compares the range of temperature needed for cleaning the magnetic head between two types of heat source’s profile. The technique used is the heat transfer by finite element: FEM[3]. This technique provides an important factor of the laser cleaning method that increases the efficiency of particle removal. It is also a non-destructive method for cleaning the surface of the magnetic head slider...|$|R
5000|$|The {{improvement}} of the Orchestron over the Mellotron was overcoming the eight second limitation inherent in the Mellotron and Chamberlin designs. Although the sound quality was of lower fidelity, this was made up for in reliability as there were no tapes to potentially foul as in the Mellotron and Chamberlin. Although <b>scratches</b> on the <b>disk</b> could be audible, one could hold notes {{for as long as}} a key was pressed, and not worry about running out of sound when holding a note or chord. The downside of this was that the attack transient was lost and occasionally an audible thump could be heard on the discs when the loop point came around. This is usually hidden or masked in recordings through effects. The problem of audible loop thumps was addressed in another Mellotron related instrument called a Birotron. In the Orchestron, the most commonly used sounds were the [...] "violins" [...] and [...] "vocal choir" [...] sounds - the choir being taken from the Optigan's [...] "Vox Humana" [...] disk. An estimated 40 Orchestrons still exist today and replacement discs and new discs are being produced for the instrument. While not as popular or well known as the Mellotron, the Orchestron is still revered and sought after by musicians for the low fidelity and murky atmospheres it provides.|$|R

