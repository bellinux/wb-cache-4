1632|1982|Public
5|$|The entries a'ii {{form the}} main {{diagonal}} of a <b>square</b> <b>matrix.</b> They {{lie on the}} imaginary line which runs from the top left corner to the bottom {{right corner of the}} matrix.|$|E
5|$|Matrix {{calculations}} can be often {{performed with}} different techniques. Many {{problems can be}} solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a <b>square</b> <b>matrix</b> {{can be obtained by}} finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.|$|E
5|$|Matrices {{which have}} a single row are called row vectors, and those {{which have a}} single column are called column vectors. A matrix which has {{the same number of}} rows and columns is called a <b>square</b> <b>matrix.</b> A matrix with an {{infinite}} number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.|$|E
30|$|For brevity we only {{consider}} <b>square</b> <b>matrices.</b> The generalizations from <b>square</b> <b>matrices</b> to rectangular matrices are obvious, {{and usually}} problems on singular values of rectangular matrices {{can be converted}} to the case of <b>square</b> <b>matrices</b> by adding zero rows or zero columns.|$|R
40|$|Abstract — In {{this work}} we present some matrix {{operators}} named circulant operators and their action on <b>square</b> <b>matrices.</b> This study on <b>square</b> <b>matrices</b> provides {{new insights into}} the structure of the space of <b>square</b> <b>matrices.</b> Moreover it can be useful in various fields as in agents networking on Grid or large-scale distributed self-organizing grid systems...|$|R
5000|$|... and {{similarly}} for J(n). The three J(m) matrices are each (2m + 1)×(2m + 1) <b>square</b> <b>matrices,</b> {{and the three}} J(n) are each (2n + 1)×(2n + 1) <b>square</b> <b>matrices.</b> The integers or half-integers m and n numerate all the irreducible representations by, in equivalent notations used by authors: D(m, n) ≡ (m, n) ≡ D(m) &otimes; D(n), which are each + 1)(2n×+ 1)(2n <b>square</b> <b>matrices.</b>|$|R
5|$|The {{determinant}} det(A) or |A| of a <b>square</b> <b>matrix</b> A is {{a number}} encoding certain properties of the matrix. A matrix is invertible {{if and only if}} its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.|$|E
5|$|A {{common and}} popular {{structural}} alignment method is the DALI, or distance alignment matrix method, which breaks the input structures into hexapeptide fragments and calculates a distance matrix by evaluating the contact patterns between successive fragments. Secondary structure features that involve residues that are contiguous in sequence {{appear on the}} matrix's main diagonal; other diagonals in the matrix reflect spatial contacts between residues that are not near {{each other in the}} sequence. When these diagonals are parallel to the main diagonal, the features they represent are parallel; when they are perpendicular, their features are antiparallel. This representation is memory-intensive because the features in the <b>square</b> <b>matrix</b> are symmetrical (and thus redundant) about the main diagonal.|$|E
5|$|This {{article focuses}} on {{matrices}} whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, that is, a set where addition, subtraction, multiplication and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field {{than that of the}} entries of the matrix; for instance they may be complex in case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (for example, to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each <b>square</b> <b>matrix</b> to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.|$|E
50|$|For {{arbitrary}} <b>square</b> <b>matrices</b> M, N {{we write}} M ≥ N if M − N ≥ 0; i.e., M − N is positive semi-definite. This defines a partial ordering {{on the set}} of all <b>square</b> <b>matrices.</b> One can similarly define a strict partial ordering M > N.|$|R
40|$|In {{this paper}} we {{consider}} divisibility sequences obtained from <b>square</b> <b>matrices.</b> We work with of matrix divisibility sequences associated to a semigroup and arising from endomorphisms of an affine space. We prove that determinant divisibility sequences originated from powers of <b>square</b> <b>matrices</b> are generalized Lucas sequences...|$|R
50|$|Together {{these facts}} {{mean that the}} upper {{triangular}} matrices form a subalgebra of the associative algebra of <b>square</b> <b>matrices</b> for a given size. Additionally, this also shows that the upper triangular matrices {{can be viewed as}} a Lie subalgebra of the Lie algebra of <b>square</b> <b>matrices</b> of a fixed size, where the Lie bracket a,b given by the commutator ab-ba. The Lie algebra of all upper triangular matrices is a solvable Lie algebra. It is often referred to as a Borel subalgebra of the Lie algebra of all <b>square</b> <b>matrices.</b>|$|R
25|$|A <b>square</b> <b>matrix</b> {{that is not}} {{invertible}} {{is called}} singular or degenerate. A <b>square</b> <b>matrix</b> is singular {{if and only if}} its determinant is 0. Singular matrices are rare in the sense that a <b>square</b> <b>matrix</b> randomly selected from a continuous uniform distribution on its entries will almost never be singular.|$|E
25|$|In linear algebra, the adjugate, {{classical}} adjoint, or adjunct of a <b>square</b> <b>matrix</b> is the transpose of its cofactor matrix.|$|E
25|$|Because of this fact, theorems {{that hold}} for any algebraically closed field, apply to C. For example, any {{non-empty}} complex <b>square</b> <b>matrix</b> {{has at least}} one (complex) eigenvalue.|$|E
30|$|C′is {{the first}} four-order <b>square</b> <b>matrices</b> of each {{transfer}} matrix.|$|R
50|$|Similarity is an {{equivalence}} relation {{on the space}} of <b>square</b> <b>matrices.</b>|$|R
5000|$|... {{where the}} blocks {{along the main}} {{diagonal}} are zero <b>square</b> <b>matrices.</b>|$|R
25|$|In linear algebra, an {{orthogonal}} matrix or real orthogonal matrix is a <b>square</b> <b>matrix</b> {{with real}} entries whose columns and rows are orthogonal unit vectors (i.e., orthonormal vectors), i.e.|$|E
25|$|Therefore, the {{statement}} that every <b>square</b> <b>matrix</b> A can be put in Jordan normal form {{is equivalent to the}} claim that there exists a basis consisting only of eigenvectors and generalized eigenvectors of A.|$|E
25|$|If Gaussian {{elimination}} {{applied to}} a <b>square</b> <b>matrix</b> A produces a row echelon matrix B, let d be {{the product of the}} scalars by which the determinant has been multiplied, using the above rules.|$|E
5000|$|... #Subtitle level 3: <b>Square</b> <b>matrices</b> over {{commutative}} {{rings and}} abstract properties ...|$|R
50|$|Because equal {{matrices}} {{have equal}} dimensions, only <b>square</b> <b>matrices</b> can be symmetric.|$|R
50|$|In the {{remainder}} of this article we will consider only <b>square</b> <b>matrices.</b>|$|R
25|$|In linear algebra, the Cayley–Hamilton theorem (named {{after the}} mathematicians Arthur Cayley and William Rowan Hamilton) states that every <b>square</b> <b>matrix</b> over a {{commutative}} ring (such {{as the real}} or complex field) satisfies its own characteristic equation.|$|E
25|$|If the {{operator}} is originally {{given by a}} <b>square</b> <b>matrix</b> M, then its Jordan normal form is also called the Jordan normal form of M. Any <b>square</b> <b>matrix</b> has a Jordan normal form if the field of coefficients is extended to one containing all the eigenvalues of the matrix. In spite of its name, the normal form for a given M is not entirely unique, {{as it is a}} block diagonal matrix formed of Jordan blocks, the order of which is not fixed; it is conventional to group blocks for the same eigenvalue together, but no ordering is imposed among the eigenvalues, nor among the blocks for a given eigenvalue, although the latter could for instance be ordered by weakly decreasing size.|$|E
25|$|Still another complex-analytic proof can {{be given}} by {{combining}} linear algebra with the Cauchy theorem. To establish that every complex polynomial of degree n>0 has a zero, it suffices to show that every complex <b>square</b> <b>matrix</b> of size n>0 has a (complex) eigenvalue. The proof of the latter statement is by contradiction.|$|E
5000|$|This {{definition}} can {{be applied}} in particular to <b>square</b> <b>matrices.</b> The matrix ...|$|R
40|$|We {{describe}} {{families of}} complete orthogonal bases of full rank matrices which span the vector spaces of <b>square</b> <b>matrices.</b> The proposed bases generalise non-trivially the Pauli matrice while shedding light on their algebraic properties. Finally we introduce the notion k -pseudo-closure for orthogonal bases spanning vector subspaces of <b>square</b> <b>matrices</b> and discuss their connections with hadamard matrices...|$|R
50|$|The {{identity}} matrix {{also has the}} property that, when it {{is the product of}} two <b>square</b> <b>matrices,</b> the matrices can be said to be the inverse of one another.|$|R
25|$|While the Jordan {{normal form}} determines the minimal polynomial, the {{converse}} is not true. This {{leads to the}} notion of elementary divisors. The elementary divisors of a <b>square</b> <b>matrix</b> A are the characteristic polynomials of its Jordan blocks. The factors of the minimal polynomial m are the elementary divisors of the largest degree corresponding to distinct eigenvalues.|$|E
25|$|The minimal {{polynomial}} P of a <b>square</b> <b>matrix</b> A is {{the unique}} monic polynomial of least degree, m, such that P(A) = 0. Alternatively, {{the set of}} polynomials that annihilate a given A form an ideal I in C, the principal ideal domain of polynomials with complex coefficients. The monic element that generates I is precisely P.|$|E
25|$|In linear algebra, a minor of {{a matrix}} A is the {{determinant}} of some smaller <b>square</b> <b>matrix,</b> cut down from A by removing {{one or more}} of its rows or columns. Minors obtained by removing just one row and one column from square matrices (first minors) are required for calculating matrix cofactors, which in turn are useful for computing both the determinant and inverse of square matrices.|$|E
500|$|The {{determinant}} {{of a product}} of <b>square</b> <b>matrices</b> equals the product of their determinants: ...|$|R
50|$|It {{is defined}} for {{matrices}} of any dimension (i.e. no restriction to <b>square</b> <b>matrices).</b>|$|R
5000|$|The {{determinant}} {{of a product}} of <b>square</b> <b>matrices</b> equals the product of their determinants: ...|$|R
