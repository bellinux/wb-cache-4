2|15|Public
40|$|The route-sampling {{method of}} {{estimating}} crop production has been extended to soybeans in a preliminary survey which is reported here. In 1941, just prior to harvest, 67 fields in eight east central Illinois counties were sampled for yield, percent protein, percent oil and iodine number of the oil. Protein percent, oil percent and iodine number of the oil can be estimated satisfactorily, but estimating yield is more uncertain pending the accumulation of information on adjusting for harvesting losses and other factors which cause the sample average yield to be too large. The yield of seed per acre differed with the method of planting (width of rows), indicating for the season studied that soybeans should have been planted in rows about 2 feet apart. The iodine number of the oil was lower for fields with wide rows than for drilled fields. This was attributed to difference in date of planting rather than method of planting. It was concluded that two subsampling units should be taken per field and that the optimum size of <b>subsampling</b> <b>unit</b> is approximately 7 square feet. Other investigations have shown that after the pods are fully distended {{there is little or}} no change in yield or chemical composition, indicating that production and quality can be estimated well in advance of harvest...|$|E
40|$|Graduation date: 1988 The {{major focus}} of this {{research}} was to examine the factors, both instructional factors and student factors, that influence student gains in economic knowledge in economic education. This study sampled twelfth grade students from randomly selected high schools in the Williamette Valley of Oregon. The stepwise regression analysis was used to select the most significant factors influencing student learning of economics. The dependent variable was economics learning. The independent variables were student factors and instructional factors. The dependent variables were measured by the difference between pretest and posttest on the Test of Economic Literacy (TEL). The independent variables were measured by information students and teachers were asked to give on their respective questionnaires. A two-stage sampling technique was utilized to select the sample. Six Willamette Valley high schools, selected at random, participated. The <b>subsampling</b> <b>unit</b> included 113 male and 106 female senior students. All were enrolled in economics courses at the randomly selected high schools. Data were also collected from the six teachers, one per high school, instructing the students in economics. Three models of economic education were developed: (1) a student model using student factors alone, based on student grade point average (GPA) and socioeconomic status (SES); (2) a student model using only instructional factors, based on previous economics courses taken by teacher (EET); and (3) a model using all variables, based on EET and GPA. This study showed that EET was more powerful than GPA in economics learning. Considering the importance of the findings of this study, it is suggested that replication should be conducted, using large samples and more variables. Research is needed which deals with the number of economics courses and level of Economics courses appropriate for economics teacher preparation. Research is also needed which shows the impact of economics on cognitive development attainment and the comparative study on different teaching methods in economics. Such research could well yield more precise results for improving economic education...|$|E
40|$|An {{important}} {{problem in}} studying the etiology of colon cancer is understanding the relationship between DNA adduct levels (broadly, DNA damage) [...] . The methodology used in this paper {{can be applied to}} other settings when the goal of the study is to model the correlation of two continuous repeated measurement responses as a function of a covariate, while the two responses of interest can be measured on the same experimental units but not on the same <b>subsampling</b> <b>units.</b> In our example, the two responses were measured in two different regions of the colon...|$|R
40|$|An {{important}} {{problem in}} studying the etiology of colon cancer is understanding the relationship between DNA adduct levels (broadly, DNA damage) in cells within colonic crypts in distal and proximal parts of the colon, following treatment with a carcinogen and different types of diet. In particular, {{it is important to}} understand whether rats who have elevated adduct levels in particular positions in distal region crypts also have elevated levels in the same positions of the crypts in proximal regions, and whether this relationship depends on diet. We cast this problem as estimating the correlation function of two responses as a function of a covariate for studies where both responses are measured on the same experimental units but not the same <b>subsampling</b> <b>units.</b> Parametric and nonparametric methods are developed and applied to a dataset from an ongoing study, leading to potentially important and surprising biological results. Theoretical calculations suggest that the nonparametric method, based on nonparametric regression, should in fact have statistical properties nearly the same as if the functions nonparametrically estimated were known. The methodology used in this article can be applied to other settings when the goal of the study is to model the correlation of two continuous repeated measurement responses as a function of a covariate, whereas the two responses of interest can be measured on the same experimental units but not on the same <b>subsampling</b> <b>units.</b> In our example, the two responses were measured in two different regions of the colon...|$|R
40|$|Various {{approaches}} to obtaining estimates based on preliminary data are outlined. A case is then considered which frequently arises when selecting a <b>subsample</b> of <b>units,</b> the information for which is collected within a deadline that allows preliminary estimates to be produced. At {{the moment when}} these estimates have to be produced it often occurs that, although the collection of data on <b>subsample</b> <b>units</b> is still not complete, information is available {{on a set of}} units which does not belong to the sample selected for the production of the preliminary estimates. An estimation method is proposed which allows all the data available on a given date to be used to the full - and the expression of the expectation and variance are derived. The proposal is based on two-phase sampling theory and on the hypothesis that the response mechanism is the result of random processes whose parameters can be suitably estimated. An empirical analysis of the performance of the estimator on the Italian Survey on building permits concludes the work. © Springer-Verlag 2005...|$|R
40|$|Resource {{considerations}} {{in relationship to}} data quality often contribute to the decision to subsample nonrespondents for follow-up. For example, the resources saved by {{limiting the number of}} nonrespondents to follow may allow for a more effective, and often more expensive, mode of data collection for those subsampled. We consider weighted response rates under subsampling schemes as a measure of data quality. When subsampling is used to follow-up nonresponse, the American Association for Public Opinion Research (AAPOR) guidelines suggest a weighted response rate that sets the nonsampled unit weights to zero and weights the <b>subsampled</b> <b>unit</b> weights by the inverse of the subsampling fraction. We provide simple examples of the calculation of weighted rates using AAPOR guidelines under different subsampling scenarios to illustrate the complexity involved in defining the quality of the data in terms of response rates and their interpretation...|$|R
40|$|In complex designs, {{classical}} bootstrap methods {{result in}} a biased variance estimator when the sampling design is not taken into account. Resampled units are usually rescaled or weighted {{in order to achieve}} unbiasedness in the linear case. In the present article, we propose novel resampling methods that may be directly applied to variance estimation. These methods consist of selecting subsamples under a completely different sampling scheme from that which generated the original sample, which is composed of several sampling designs. In particular, a portion of the <b>subsampled</b> <b>units</b> is selected without replacement, while another is selected with replacement, thereby adjusting for the finite population setting. We show that these bootstrap estimators directly and precisely reproduce unbiased estimators {{of the variance in the}} linear case in a time-efficient manner, and eliminate the need for classical adjustment methods such as rescaling, correction factors, or artificial populations. Moreover, we show via simulation studies that our method is at least as efficient as those currently existing, which call for additional adjustment. This methodology can be applied to classical sampling designs, including simple random sampling with and without replacement, Poisson sampling, and unequal probability sampling with and without replacement...|$|R
30|$|Two {{community}} diversity statistics (species {{richness and}} the Shannon-Wiener Index (H’)) were calculated at two scales based on trapping point totals nested within units (alpha diversity) {{and for each}} experimental unit (beta diversity). The effect of treatment on alpha diversity measures was evaluated using a nested ANOVA (trapping points as <b>subsamples</b> within <b>units).</b> Treatment effects on beta diversity were analyzed as a completely randomized design with ANOVA. Comparisons among treatment and control means of species richness and H’ at both diversity scales were made using Tukey’s HSD.|$|R
40|$|We {{present a}} {{multi-phase}} variant of adaptive cluster sampling {{which allows the}} sampler to control the number of measurements of the variable of interest. A first-phase sample is selected using an adaptive cluster sampling design based on an inexpensive auxiliary variable associated with the survey variable. Then the network structure of the adaptive cluster sample is used to select an ordinary one-phase or two-phase <b>subsample</b> of <b>units</b> and {{the values of the}} survey variable associated with those units are recorded. The population mean is estimated by either a regression-type estimator or a Horvitz [...] Thompson-type estimator. The results of a simulation study show good performance of the proposed design, and suggest that in many real situations this design might be preferred to the ordinary adaptive cluster sampling design. Copyright 2004, Oxford University Press. ...|$|R
40|$|This paper {{introduces}} a new confidence interval (CI) for the autoregressive parameter (AR) in an AR(1) model {{that allows for}} conditional heteroskedasticity of general form and AR parameters that are {{less than or equal}} to unity. The CI is a modification of Mikusheva's (2007 a) modification of Stock's (1991) CI that employs the least squares estimator and a heteroskedasticity-robust variance estimator. The CI is shown to have correct asymptotic size and to be asymptotically similar (in a uniform sense). It does not require any tuning parameters. No existing procedures have these properties. Monte Carlo simulations show that the CI performs well in finite samples in terms of coverage probability and average length, for innovations with and without conditional heteroskedasticity. Asymptotically similar, Asymptotic size, Autoregressive model, Conditional heteroskedasticity, Confidence interval, Hybrid test, <b>Subsampling</b> test, <b>Unit</b> root...|$|R
40|$|When {{data are}} not missing at random, {{approaches}} to reduce nonresponse bias include <b>subsampling</b> nonresponding <b>units</b> and modeling. The objective of this thesis is to develop unbiased and precise model-assisted estimators of the population total that are applicable to data from a complex survey design with nonignorable nonresponse. When information from a nonrespondent subsample is available, weighting methods for missing-at-random data may be modified to reduce bias from nonignorable missingness in estimates of population totals. Propensity score methodology for nonignorable missingness is developed for use with the weighting class adjustment and with the Horvitz-Thompson estimator {{to account for the}} dependence between the outcome of interest and the response mechanism. The novel propensity score techniques for nonignorable nonresponse are applied to a binary outcome subject to nonignorable missingness from a complex survey of elk hunters and are also examined with simulation...|$|R
40|$|Most {{research}} studies in forestry <b>subsample</b> experimental <b>units</b> to obtain response measures. A common example occurs when rows or plots of trees are randomly assigned {{one of several}} treatments. While the row is the experimental unit, the response variables of height, diameter, etc. must be measured on individual trees. These subsamples (trees) provide {{an estimate of the}} response of that experimental unit to the treatment (most commonly by averaging the individual responses). Increasing the number of <b>subsamples</b> per experimental <b>unit</b> is often cheaper than including more experimental units, and is desirable if this increases the power of the experiment. On the other hand, collecting many subsamples may not increase the power enough to be worth the effort because power is typically increased more by increasing the number of experimental units. To balance these considerations it is helpful to calculate the power for a range of numbers of experimental <b>units</b> and <b>subsamples.</b> This pamphlet will graphically demonstrate the above statements for the Completely Randomized Design and will discuss some of the considerations in using these graphs to choose suitable sample sizes. The next pamphlet (BI # 50) will extend the results to the Randomized Block Design, while the following pamphlet (BI # 51) will briefly describe how to create these graphs in general and provide an example SAS program that produces the plots shown in this pamphlet. For discussion purposes, let us assume a study with four treatments (factor T with t = 4 levels) that will each be randomly assigned to p plots (factor P with p levels). Each plot is the experimental unit and will be subsampled e times to obtain an estimate of the plot response (subsamples will be factor E and if there were 10 subsamples per plot then e = 10). The ANOVA table for this one-way completely randomized design with subsamples 1 is...|$|R
40|$|Three {{stagewise}} discrimination methods, {{one with}} control of Type I error, are given {{for use in}} the selection of a subset of grouped variables. Wilks's lambda is used in Rao's test at each step to decide which groups to add or drop, if any. The cases of no <b>subsampling</b> of sampling <b>units</b> and of <b>subsampling</b> are both considered. A multinormal example of the implementation and the performance of these methods is given. A minimal-best-subset algorithm, which selects from the best subsets of all sizes the smallest subset that retains most of the discrimination, is better than stepwise and simultaneous stepdown algorithms...|$|R
40|$|This paper studies {{subsampling}} hypothesis {{tests for}} panel data {{that may be}} nonstationary, cross-sectionally correlated, and cross-sectionally cointegrated. The subsampling approach provides approximations to the finite sample distributions of the tests without estimating nuisance parameters. The tests include panel unit root and cointegration tests as special cases. The number of cross-sectional units {{is assumed to be}} finite and that of time-series observations infinite. It is shown that subsampling provides asymptotic distributions that are equivalent to the asymptotic distributions of the panel tests. In addition, the tests using critical values from subsampling are shown to be consistent. The subsampling methods are applied to panel unit root tests. The panel unit root tests considered are Levin, Lin, and Chu's (2002) t-test; Im, Pesaran, and Shin's (2003) averaged t-test; and Choi's (2001) inverse normal test. Simulation results regarding the <b>subsampling</b> panel <b>unit</b> root tests and some existing unit root tests for cross-sectionally correlated panels are reported. In using the subsampling approach to examine the real exchange rates of the G 7 countries and a group of 26 OECD countries, we find only mixed support for the purchasing power parity (PPP) hypothesis. We then examine a panel of 17 developed stock market indexes, and also find only mixed empirical support for them exhibiting relative mean reversion with respect to the US stock market index. Copyright (c) 2007 John Wiley & Sons, Ltd...|$|R
40|$|ABSTRACT Pooling excreta, within treatments, {{prior to}} {{calorimetry}} reduces {{the time and}} cost of the bioassay for true metabolizable energy (TME) and does not alter mean TME values. How-ever, by ignoring the among-bird variance of the gross energy per unit weight of excreta (G) and its covariance {{with the weight of}} excreta (D), the standard errors of mean TME values are subject to bias. An additional factor for consideration is the effect of the sampling error from pooled aggre-gates. These factors were examined using data from TME bioassays of 20 feedingstuffs and by measuring the standard deviations of gross energy values for subsamples from pooled excreta. The standard deviation of G for each <b>unit</b> <b>subsample</b> from pooled excreta samples was. 0381 kj/g with 20 degrees of freedom. The resulting contribution to the standard errors of mean TME values was so small that replicate subsampling from the bulked excreta is of minor importance, although a useful precaution. However, ignoring the among-bird variation in G as a consequence of pooling excreta caused the standard errors of mean TME values to be incorrectly estimated by amounts ranging from — 62 to + 64 %. Although for these particular feedingstuffs the average discrepancy was small, the potential magnitude of the bias for an individual feedingstuff is too large to be discounted in a research laboratory. (Key words: true metabolizable energy, bioassay, pooling samples, calorimetry, sampling...|$|R
40|$|The paper {{addresses}} two questions. First, is item nonresponse (INR) {{a precursor}} of panel attrition (UNR), as {{predicted by the}} theory of a latent cooperation continuum, or is the interrelation of another type? Second, are the results in models of item nonresponse behavior affected by a selectivity bias due to panel attrition? We test the hypothesis of a latent cooperation continuum with data taken from the German Socio-Economic Panel (GSOEP) - and can not find evidence for it. In contrast, we hypothesize that the relationship of both nonresponse types may be inverse in principle (the reverse cooperation continuum) and both types of cooperation may coexist. Besides unit nonresponse we analyze questionnaire nonresponse, i. e. participating but refusing a whole questionnaire with specific items in a multi-questionnaire survey. We find evidence for negative correlation of INR with questionnaire nonresponse. The correlation between item and unit nonresponse is inverse U-shaped which supports the hypothesis of coexistence of both types of cooperation. Addressing the second question we test whether panel attrition causes endogenous sample selection in regressions of INR {{by means of a}} bivariate probit model for selection correction. Additionally we use Monte Carlo simulations to test the influence of alternative assumptions for INR-behavior of attriters. The results show that attrition bias is item-specific. Existence and magnitude of the bias differs with the analyzed <b>subsample.</b> item nonresponse, <b>unit</b> nonresponse, cooperation continuum, attrition bias, income and wealth. ...|$|R

