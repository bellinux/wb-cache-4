24|38|Public
50|$|Four-die forging {{device is}} a special forging tool {{designed}} for manufacturing forgings with long axis by four-side radial forging method in conventional open-die hydraulic forging press. A similar <b>stand-alone</b> <b>machine</b> {{is known as a}} radial forging machine.|$|E
50|$|Thus, both compilers and interpreters {{generally}} turn {{source code}} (text files) into tokens, both may (or may not) generate a parse tree, and both may generate immediate instructions (for a stack machine, quadruple code, or by other means). The basic {{difference is that}} a compiler system, including a (built in or separate) linker, generates a <b>stand-alone</b> <b>machine</b> code program, while an interpreter system instead performs the actions described by the high level program.|$|E
50|$|Neo is {{the first}} user {{configurable}} smart camera fully designed in India. It is a compact <b>stand-alone</b> <b>machine</b> vision product with built-in image sensor and processor with integrated LED lights. This camera is completely configurable {{for a range of}} machine vision based poka-yoke applications. The heart of this system is the Soliton Vision library which consists of machine vision algorithms optimised for its processor architecture. Neo was showcased in the TI Developer's Conference 2008. At that point of time, Soliton Technologies was the only Indian company to present in TI Developer's Conference.|$|E
40|$|High-performance {{scientific}} applications {{require more}} and more compute power. The concurrent use of multiple distributed compute resources is vital for making scien-tific progress. The resulting distributed system, a so-called Jungle Computing System, is both highly hetero-geneous and hierarchical, potentially consisting of grids, clouds, <b>stand-alone</b> <b>machines,</b> clusters, desktop grids, mo-bile devices, and supercomputers, possibly with accelera-tors such as GPUs. One striking example of applications that can benefit greatly of Jungle Computing Systems are Multi-Model / Multi-Kernel simulations. In these simulations, multiple models, possibly implemented using different techniques and programming models, are coupled into a single simu-lation of a physical system. Examples include the domain of computational astrophysics and climate modeling. In this paper we investigate the use of Jungle Comput-ing Systems for such Multi-Model / Multi-Kernel sim-ulations. We {{make use of the}} software developed in the Ibis project, which addresses many of the problems faced when running applications on Jungle Computing Systems. We create a prototype Jungle-aware version of AMUSE, an astrophysical simulation framework. We show preliminary experiments with the resulting system, using clusters, grids, <b>stand-alone</b> <b>machines,</b> and GPUs. ...|$|R
40|$|Abstract- The advent {{and wide}} {{adoption}} of cloud computing {{has brought a}} new revolution {{in the field of}} IT. As consumers using cloud for data storage either in the SaaS, PaaS or IaaS deployment model are increasing-they realize the necessity of file compression. It becomes imperative to understand the scenarios where transition to the Cloud is beneficial. In our research, we have demonstrated that for small businesses – making a move to the cloud is not a good approach as less demanding applications can run well even on <b>stand-alone</b> <b>machines,</b> however as the data size grows – making a move towards cloud can yield higher availability. In this paper we evaluate and compare the performances of virtual machines running in cloud with <b>stand-alone</b> (unvirtualized) <b>machine...</b>|$|R
5|$|WinFax enabled {{computers}} {{equipped with}} fax-modems to send faxes directly to <b>stand-alone</b> fax <b>machines</b> or other similarly equipped computers.|$|R
5000|$|CP-40 was {{the first}} {{operating}} system that implemented complete virtualization, i.e. it provided a virtual machine environment supporting all aspects of its target computer system (a S/360-40), such that other S/360 operating systems could be installed, tested, and used {{as if on a}} <b>stand-alone</b> <b>machine.</b> CP-40 supported fourteen simultaneous virtual machines. Each virtual machine ran in [...] "problem state" [...] - privileged instructions such as I/O operations caused exceptions, which were then caught by the control program and simulated. Similarly, references to virtual memory locations not present in main memory cause page faults, which again were handled by control program rather than reflected to the virtual machine. Further details on this implementation are found in CP/CMS (architecture).|$|E
50|$|The {{heart of}} the VM {{architecture}} is a control program or hypervisor called VM-CP (usually: CP; sometimes, ambiguously: VM). It runs on the physical hardware, and creates the virtual machine environment. VM-CP provides full virtualization of the physical machine - including all I/O and other privileged operations. It performs the system's resource-sharing, including device management, dispatching, virtual storage management, and other traditional operating system tasks. Each VM user is provided with a separate virtual machine having its own address space, virtual devices, etc., and which is capable of running any software that could be run on a <b>stand-alone</b> <b>machine.</b> A given VM mainframe typically runs {{hundreds or thousands of}} virtual machine instances. VM-CP began life as CP-370, a reimplementation of CP-67, itself a reimplementation of CP-40.|$|E
5000|$|When IBM {{initially}} {{moved into}} the Data Processing world, each computer was a <b>stand-alone</b> <b>machine</b> with limited stored program capability with little interaction with other devices or software. The only support documentation was a manual of operation outlining the basic capabilities of the computer. The technical specifications were restricted in their distribution and rarely available to users. With the advent of System/360 in 1964 with their connectability to various I/O components and their associated software, their installation and their very usage rapidly became a challenge. Hands-on training was no longer just for the installation and maintenance professionals (Customer Engineers) but a necessity for the IBM System Engineers who ran and coded the systems. While the system engineers in the US Company had some access to the technical skills available in the Poughkeepsie Plant where the systems were built, the World Trade Division (the 131 countries outside of the US where IBM did business) had no such fall-back capability.|$|E
50|$|Washer dryer combo units {{not using}} a heat pump {{have also been}} criticized {{because they are not}} as {{efficient}} as some of the <b>stand-alone</b> <b>machines.</b> For these machines, longer drying times of washer dryer combos also make it difficult to increase efficiency, because the machine has to stay in operation for much longer than a stand-alone dryer does. On the other hand, in heat-pump washer dryers energy is recovered, and it enables energy saving of about 50%.|$|R
5000|$|Minimal {{hardware}} requirements: it runs {{useful applications}} <b>stand-alone</b> on <b>machines</b> {{with as little}} as 1 MiB of memory, and does not require memory-mapping hardware.|$|R
50|$|WinFax (also {{known as}} WinFax PRO) is a Microsoft Windows-based {{software}} {{product designed to}} let computers equipped with fax-modems to communicate directly to <b>stand-alone</b> fax <b>machines,</b> or other similarly equipped computers.|$|R
40|$|This letter {{presents}} an alternate proof for the steady-state equivalent circuit of a doubly fed induction machine operating at supersynchronous speeds. The spatial orientation of rotating magnetic fields {{is used to}} validate the conjugation of rotor side quantities arising in supersynchronous mode. The equivalent circuit is further validated using dynamic simulations of a <b>stand-alone</b> <b>machine...</b>|$|E
40|$|Nanoimprint {{lithography}} (NIL) is a high-resolution, high-throughput {{and cost-effective}} nano-patterning technology. However, the overlay accuracy is lagging behind the resolution {{because of the}} high cost of mechanical precision. We have built an inexpensive <b>stand-alone</b> <b>machine</b> based on the wafer bowing nanoimprint process, and demonstrated single-point overlay of two transferred pattern layers with an accuracy of ≤ 60 nm. © Copyright 2011 Hewlett-Packard Development Company, L. P. link_to_subscribed_fulltex...|$|E
40|$|This chapter {{discusses}} {{two main}} topics: distributing the functionality of an arbitrary {{trusted third party}} (TTP) from a <b>stand-alone</b> <b>machine</b> to an application running on a secure execution environment on a user’s machine, and the specific example of distributing the functionality of a certification authority (CA) in this way. TTP services that are distributed in this way have potential advantages in terms of easing the computational load on the central TTP server and in the flexibility of placement of the service within a network. ...|$|E
25|$|Another <b>stand-alone</b> drum <b>machine</b> {{released}} in 1975, the PAiA Programmable Drum Set {{was also one}} of the first programmable drum machines, and was sold as a kit with parts and instructions which the buyer would use to build the machine.|$|R
40|$|When {{you first}} heard people speak of Piles of PCs, {{the first thing}} that came to mind may have been a {{cluttered}} computer room with processors, monitors, and snarls of cables all around. Collections of computers have undoubtedly become more sophisticated than {{in the early days of}} shared drives and modem connections. No matter what you call them, Clusters of Workstations (COW), Networks of Workstations (NOW), Workstation Clusters (WCs), Clusters of PCs (CoPs), clusters of computers are now filling the processing niche once occupied by more powerful <b>stand-alone</b> <b>machines.</b> This article discusses the need for cluster computing technology, Technologies, Components, and Applications, Supercluster Systems and Issues, The Need for a New Task Force, and Cluster Computing Educational Resources...|$|R
50|$|Axial inserters used {{to consist}} of a <b>stand-alone</b> {{sequencer}} <b>machine</b> which cut and sequenced the parts onto a reel. That reel was then transferred over to a standalone axial inserter to insert the components. This is all done on one machine today.|$|R
30|$|Veetil and Gao [48] {{conducted}} an experiment and created Hadoop clusters {{to implement the}} Naïve Bayes algorithm in a distributed fashion. With a 6 node “homogeneous” Hadoop-based cluster where the nodes had similar hardware, they were able perform classification 37 % quicker than a <b>stand-alone</b> <b>machine</b> could. While the experiment was successful as a proof of concept to use a distributed Hadoop-based cluster to implement Naïve Bayes classifier, it could only classify an average of 434 packets per minute. Much more research and experimentation {{can be done to}} implement Hadoop technologies to improve Intrusion Detection efficiency and classification accuracy.|$|E
40|$|Abstract: This article {{describes}} the structure and the working principle of the blind hole drilling device, by application in the production practice proves its feasibility. At present, we commonly used electrical discharge machining and welding method to solve the problem. However, these methods have the low processing efficiency, low manufacturing precision and high cost defect. This paper developed an inner wall of the blind hole drilling device, similar to a <b>stand-alone</b> <b>machine</b> accessories, adopted the bevel gear transmission. which can directly mounted on vertical or horizontal milling machine, with {{a few minutes to}} complete the processing operations. This device not only has high processing efficiency and accuracy, but also greatly reducing the production cost...|$|E
40|$|This paper reports {{results from}} a project that {{combined}} the visualisation of streetscapes with environmental economics. The particular economic methodology employed was choice experimentation, which has previously tended to use text to convey scenarios between which respondents make choices. The research investigated how the visual impact of streetscapes could be incorporated in choice experiments {{through the use of}} 3 -dimensional computer generated visualisations. Data was gathered initially through an Internet based survey, and replicated later using a high specification <b>stand-alone</b> <b>machine.</b> This paper identifies strengths and weaknesses in the approach, and reports results pertaining to effectiveness and practicality of the methodology. Using visualisation offers many potential benefits to the field of choice experimentation, and application within the built environment merits further development...|$|E
5000|$|Rapidly {{advancing}} technology revolutionized typography in {{the latter}} twentieth century. During the 1960s some camera-ready typesetting could be produced in any office or workshop with <b>stand-alone</b> <b>machines</b> such as those introduced by IBM. During the mid-1980s personal computers such as the Macintosh allowed type designers to create typefaces digitally using commercial graphic design software. Digital technology also enabled designers to create more experimental typefaces {{as well as the}} practical typefaces of traditional typography. Designs for typefaces could be created faster with the new technology, and for more specific functions. The cost for developing typefaces was drastically lowered, becoming widely available to the masses. The change has been called the [...] "democratization of type" [...] and has given new designers more opportunities to enter the field.|$|R
50|$|Pegasus was {{initially}} a text-mode application for networks, handling {{both internal and}} Internet mail, often operating {{in conjunction with the}} Mercury mail transport. Pegasus Mail for <b>stand-alone</b> <b>machines</b> {{was initially}} developed {{at a time when the}} typical email user had to be somewhat more knowledgeable of the way computers, the Internet and particularly email operate than most of today's users have to be, as PCs and the Internet have become more widespread, reached a broader audience and adapted themselves to those new users' needs. At the time Pegasus Mail was first conceived, its extensive array of features coupled with a simple user interface provided an ideal mix for most users' needs. As years went by it was seen as departing from the de facto standard, and lacking features expected by the typical user.|$|R
40|$|Linac 4 {{is a new}} H- linear {{accelerator}} presently studied at CERN. This machine consists of normal-conducting structures operating at 352. 2 MHz and 704. 4 MHz re-using the RF equipment from the decommissioned LEP collider. It consists of a 95 keV H- source, a 352 MHz RFQ bringing the energy the energy to 3 MeV, a Chopper line, a 352 MHz Drift Tube Linac bringing the energy to 40 MeV, a 352 MHz Coupled Cavity Drift Tube Linac bringing the energy to 90 MeV and a 704 MHz Side Coupled Linac bringing the energy to 160 MeV. Each section is designed and optimized as <b>stand-alone</b> <b>machines</b> for a good transmission and minimum possible emittance growth. End-to-end simulations starting from the RFQ {{have been carried out}} in order to validate and compare the multiparticle simulation codes PATH Manager and TRACEWIN used for beam dynamics calculations as well as to perform a global optimization of the structures {{in the context of a}} complex machine...|$|R
40|$|In June of 2008, the University of Michigan Library {{became the}} first {{university}} library in the U. S. to purchase an Espresso Book Machine?? (EBM), a <b>stand-alone</b> <b>machine</b> that automates the printing, binding, and trimming of a softcover book directly from a digital file. With {{hundreds of thousands of}} public domain titles in its online repository, and the promise of access to thousands more through the EBM???s networked catalog, an Espresso Book Machine?? held great appeal for the Library as a new tool for providing quick, affordable print access to the campus community and beyond. This article describes the University of Michigan???s experience with the version 1. 5 machine, including the reasons for purchasing the EBM, {{the development and implementation of}} current services, plans and potential for future uses of the machine, as well as the benefits realized and drawbacks experienced thus far. ...|$|E
40|$|This paper {{reports on}} the {{development}} of a realistic knowledge-based application using the MOBAL system. Some problems and requirements resulting from industrial-caliber tasks are formulated. A step-by-step account of the construction of a knowledge base for such a task demonstrates how the interleaved use of several learning algorithms in concert with an inference engine and a graphical interface can fulfill those requirements. Design, analysis, revision, refinement and extension of a working model are combined in one incremental process. This illustrates the balanced cooperative modeling approach. The case study is taken from the telecommunications domain and more precisely deals with security management in telecommunications networks. MOBAL would be used as part of a security management tool for acquiring, validating and refining a security policy. The modeling approach is compared with other approaches, such as KADS and <b>stand-alone</b> <b>machine</b> learning. What online ML can do for KA - [...] ...|$|E
40|$|ISIS is a toolkit for {{building}} applications consisting of cooperating processes in a distributed system. Group managementand group communication {{are two basic}} building blocks provided by ISIS. This approach has proven successful, and ISIS' large user community is putting substantial demands on these mechanisms. To accommodate these demands a complete redesign of the system, called HORUS, {{is being done to}} build a simpler and faster system that scales well. Of particular concern is the support and management of hundreds of thousands or more process groups. This paper describes a component of HORUS known as light-weight process groups that addresses this scaling issue. 1 Introduction Much of yesterday's centralized mainframe computing has evolved into the large local area networks of today. This trend has made reliability in the computing environment more complex. Failures within large LANs are quite commonplace. Users treat a networked PC much like a <b>stand-alone</b> <b>machine,</b> turning the mac [...] ...|$|E
40|$|Students {{currently}} {{in higher education}} in the industrialised world have unprecedented access to web-based technologies and tools, and are likely to have engaged with online activities throughout their educational experiences. More widely, there is increasing pressure on universities to provide flexible learning environments and access to resources. This is keenly felt in the computer laboratory, where once dedicated, <b>stand-alone</b> <b>machines</b> provided software packages for students to work on during timetabled sessions. In recognising the move away from such patterns, Macquarie University is developing software and infrastructure to enable distributed access at any time to students, thus making a conceptual and physical shift from so-called ‘Local Area Networking’ to ‘Wide Area Networking’ and enabling greater freedom of access. The initiative is from within the Division of Economics and Financial Studies (EFS), but is applicable to students of any discipline in any university. This paper describes the development and discusses some of the implications for learning and teaching. 8 page(s...|$|R
5000|$|Many {{consumers}} {{confuse the}} term [...] "washer dryer combo" [...] with similar {{washer and dryer}} configurations like stackable machines and laundry centers. The main design factor that distinguishes washer dryer combos from other configurations {{is the fact that}} the washer dryer combo is a single machine (typically the size of a <b>stand-alone</b> washing <b>machine)</b> that can do both washing and drying tasks in the single combo machine.|$|R
40|$|Includes bibliographical references. This report {{describes}} how the latest technology in computing and interfacing {{has been used in}} a traditional manufacturing environment to integrate old <b>stand-alone</b> CNC <b>machines,</b> a FANUC robot, a conveyor and a Robot Positioning Device (RPD) into a Flexible Manufacturing System (FMS). Also described are the benefits that South African manufacturers still using old CNC machines could achieve if they implemented a similar process in their organisation...|$|R
40|$|Grid {{computing}} is a {{wide area}} distributed parallel computing environment where it focuses on communication among different heterogeneous devices and harnesses unused processor cycles of all computers in a network for solving critical problems too intensive for any <b>stand-alone</b> <b>machine.</b> In this scenario security is vital concern because of dynamic allocation of users and resources. Among all other security issues trust establishment is given first priority. In this paper, we propose a novel model for resource selection in Grid computing environment. This model is truly unique model where decision is based on computed total trust which includes transaction, feedbacks, reputation, similarity, activity, popularity and obviously the total trust of the resources. The final decision is depend on the total trust of each resource. So the main motto of our project is to retrieve the best resource among the resources in grid environment. This paper provides a model which will allow only dependable transactions in grid by using total trust as a measure for the resources...|$|E
40|$|Computer tools offer {{enormous}} {{benefits for}} the early design process such as remote collaboration, advanced visualiza- tion, {{and the ability to}} run a design. However, current tools fail to support many elements of creative problem solving, inhibiting the early design process. From the literature on design theory and creativity, and extensive low-fidelity pro- totyping, we developed SCWID: a tool for Supporting Creative Work In Design. SCWID uses a large display to provide a shared visual context for alternative design ideas and uses multiple local displays for sketching details, navi-gating a particular idea, and manipulating alternatives. Grounded in creativity theory, the use of our tool facilitates creative thinking {{in the early stages of}} design for individual and groups of designers. Our tool can run either on a <b>stand-alone</b> <b>machine</b> or as part of a distributed workspace formed by connecting multiple clients to a server running on a ma-chine that drives any large display...|$|E
40|$|Industrial robots are automation, {{but with}} a difference. Other machine tools are {{extensions}} of human capabilities, while robots are seen mainly as substitutes for human workers. Robots will find most of their industrial applications {{during the next decade}} or two in the metal-working sectors, where they will begin to displace semiskilled machine operatives in medium to large batch production operations. They cannot substitute for skilled machinists or other workers doing nonroutine jobs, or specialized, dedicated hard automation used in mass production. The current generation of robots, lacking sensory data processing and interpretation capabilities, can potentially replace up to 1. 3 million manufacturing jobs. The next generation, with crude vision or tactile senses; will potentially displace about 3 million more. However, only relatively large firms can profitably utilize many robots at present; it may be 20 years or more before these usage rates are achieved in practice. A shift from <b>stand-alone</b> <b>machine</b> tools, to manufacturing cells consisting of several machine tools served by a robot and controlled by a computer, will accelerate the practical use of robots in the 1990 s...|$|E
50|$|It is {{generally}} {{thought to be}} the first CD player to be widely adopted in club use. Until this point few clubs bothered with CD machines in them, either due to their lack of DJ functionality and overall robustness, or {{due to the fact that}} DJs still liked to use the vinyl format as most of the upfront music they required to play was still much more prevalent on vinyl over CD media. The other reason this machine took off in popularity was the release of recordable CD-R and then CD-RW media discs and <b>stand-alone</b> <b>machines</b> which could record music onto them. Before this, DJs who wanted to test in either a club or as early promotional items to radio DJs, a new piece of music they might have made themselves in a studio, often had to rely on getting acetate discs pressed up. These were both expensive to do and had inherent short lifespan; as after a few plays the disc would wear-out and thus be completely unplayable.|$|R
40|$|An {{integrated}} machine comprising electric motors, hopper, shelling, separating and pressing/grinding {{units for}} processing melon seed to either melon kernel, melon flour or melon oil was developed. This machine which was fabricated using locally sourced standard materials eliminated the drudgery in the loading and discharging of intermediate materials among <b>stand-alone</b> <b>machines</b> for melon shelling, melon shell and kernel separation, melon grinding and melon oil extraction. Performance Analysis of this machine {{indicated that it}} performed best at 9. 7 % melon seed moisture content, press temperature of 100 o C, blow-dryer heat rating of 1500 W and shelling, conveyor, blower speeds of 950 rpm, 24 rpm and 1200 rpm respectively. Its melon kernel, flour and oil extraction capacities are 94. 4 kg/hr, 10. 42 kg/hr and 4. 67 kg/hr respectively while 93 %. 89. 3 % and 91. 5 % constitute the respective melon seed shelling, kernel/shell separation and oil extraction efficiencies of the integrated machine. Adoption of this innovation is recommended because it reduced seed breakage during shelling, improved hygiene (as human contact with the product during processing is reduced) and also energy saving...|$|R
40|$|It {{has been}} tacitly {{acknowledged}} {{for many years}} that personalized interaction and user modeling bear significant implications on privacy, {{due to the fact}} that personal information about users needs to be collected to perform personalization. [11] was arguably the first to discuss this problem in a 1990 article, but the paper had little impact. The only privacy "solution " of these days was to ascertain that users could store their models on a diskette (R. Oppermann, GMD) or PCMCIA card (J. Orwant, MIT [14]) and carry them with. The situation changed completely in the late 1990 s, for three main reasons: • Personalized systems have moved to the web Web retailers quickly realized the enormous potential of personalization for customer relationship management and made their websites user-adaptive. This had significant privacy implications. While user models were previously confined to <b>stand-alone</b> <b>machines</b> or local networks, people's profiles are now being collected by dozens if not hundreds of personalized websites. Widely publicized security glitches and privacy breaches as well as aggressive telemarketing have led to a wide-spread (~ 60 - 80 %) reluctance of Internet users to disclosing their personal data and being tracked online. This however jeoparidizes the basic foundations of personalization [16]. • Personalized systems are moving to mobile device...|$|R
