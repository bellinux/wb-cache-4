7|20|Public
50|$|Sirens {{attached}} on appropriate rooftops {{of buildings}} {{can have a}} custom-made mini <b>server</b> <b>box</b> (which is white) beside the siren, and in on-ground sirens, to eliminate remote control {{of them from the}} nearby fire stations turning them into sourcing sirens.|$|E
50|$|The name {{blade server}} {{appeared}} when a card included the processor, memory, I/O and non-volatile program storage (flash memory or small hard disk(s)). This allowed manufacturers to package a complete server, with its operating system and applications, {{on a single}} card / board / blade. These blades could then operate independently within a common chassis, doing the work of multiple separate server boxes more efficiently. In addition to the most obvious benefit of this packaging (less space consumption), additional efficiency benefits have become clear in power, cooling, management, and networking due to the pooling or sharing of common infrastructure to support the entire chassis, rather than providing each of these on a per <b>server</b> <b>box</b> basis.|$|E
40|$|In this paper, we {{describe}} a simulation-based inventory management tool {{developed for the}} IBM Enterprise Server Group. Through the Web interface of the tool, an inventory manager is able to visualize Days of Supply (DOS) levels [...] current and projected, and to carry out what-if scenario analysis to identify potential opportunities for improvement. The highly complicated server manufacturing environment poses simulation modeling challenges such as two-stage fabrication /fulfillment process, multi-echelon bills-of-materials, complex <b>server</b> <b>box</b> configurations, part tests with random yields, stochastic lead times and so on. In the following sections, we will introduce the common characteristics of the server manufacturing environment, present the overall architecture of our tool, and describe the simulation model design and how we addressed those challenges. At {{the end of the}} paper, we will show some results collected from the tool and point out our future research directions...|$|E
50|$|Playout {{is one of}} {{the basic}} {{infrastructure}} of a playout center. Mostly called as channel in a <b>box</b> <b>server,</b> but basically composed of playout servers with integrated graphics and IP or ASI output. Aim of playout centers is mostly to serve customers a simple file based television facility. Up-link and Channel in a <b>box</b> <b>servers</b> simply provide the facility.|$|R
5000|$|Server - Memory {{for both}} branded (i.e. IBM, HP, etc.) and non-branded white <b>box</b> <b>servers</b> (ValueRAM, Server Premier) ...|$|R
40|$|Dynamic {{provisioning}} of <b>server</b> <b>boxes</b> to applications entails {{an inherent}} performancepower trade-off {{for the service}} provider, a trade-off {{that has not been}} studied in detail. The optimal number of replicas to be dynamically provisioned to an application is ultimately the configuration that results in the highest revenue. The service provider should thus dynamically provision resources for an application only as long as the resulting reward from hosting more clients exceeds its operational costs for power and cooling. We introduce a novel cost-aware dynamic provisioning approach for the database tier of a dynamic content site. Our approach employs Support Vector Machine regression for learning a dynamically adaptive system model. We leverage this lightweight on-line learning approach for two cost-aware dynamic provisioning techniques. The first is a temperature-aware scheme which avoids temperature hot-spots within the set of provisioned machines, and hence reduces cooling costs. The second is a more general cost-aware provisioning technique using a utility function expressing monetary costs for both performance and power...|$|R
40|$|Abstract—High power densities and the {{implications}} of high operating temperatures on the failure rates of components are key driving factors of temperature-aware computing. Computer architects and system software designers need to understand the thermal consequences of their proposals, and develop techniques to lower operating temperatures to reduce both transient and permanent component failures. Until recently, tools for understanding temperature ramifications of designs have been mainly restricted to industry for studying packaging and cooling mechanisms, with little access to such toolsets for academic researchers. Developing such tools is an arduous task since it usually requires cross-cutting areas of expertise spanning architecture, systems software, thermodynamics, and cooling systems. Recognizing the need for such tools, there has been recent work on modelling temperatures of processors at the micro-architectural level which can be easily understood and employed by computer architects for processor designs. However, there is a dearth of such tools in the academic/research community for undertaking architectural/systems studies beyond a processor- a <b>server</b> <b>box,</b> rack or even a machine. With its vast experience in development of cooling systems, Applied Thermal Technologies understands the need of current generation microprocessors. With many components being installed around the processor for various capabilities, space has been a main constraintroom. This paper consists of carrying out detailed thermal analysis of 1 -U Server and identifies the problematic and thermally susceptible areas...|$|E
40|$|Abstract—Temperature-aware {{computing}} {{is becoming}} {{more important in the}} design of computer systems as power densities are increasing and the implications of high operating temperatures result in higher failure rates of components and increased demand for cooling capability. Computer architects and system software designers need to understand the thermal consequences of their proposals and develop techniques for lowering operating temperatures to reduce both transient and permanent component failures. Until recently, tools for understanding the temperature ramifications of the designs of the server and the rack have been mainly restricted to the industry for studying packaging and cooling mechanisms and they have been mainly concerned with the static thermal characteristics of computer systems. Recognizing the need for such tools, there has been recent work on modeling temperatures of processors at the microarchitectural level, which can be easily understood and employed by computer architects for processor designs. However, there is a dearth of such tools in the academic/research community for undertaking architectural/systems studies beyond a processor—a <b>server</b> <b>box,</b> rack, or even a machine room. In this paper, we present a detailed three-dimensional Computational Fluid Dynamics-based thermal modeling tool, called ThermoStat, for rack-mounted server systems. We conduct several experiments with this tool to show how different load conditions affect the thermal profile and to also illustrate how this tool can help design dynamic thermal management techniques. We propose reactive and proactive thermal management for rack-mounted server and isothermal workload distribution for rack. Index Terms—Simulation, energy-aware systems, power management, thermal modeling. Ç...|$|E
40|$|High power densities and the {{implications}} of high operating temperatures on the failure rates of components are key driving factors of temperature-aware computing. Computer architects and system software designers need to understand the thermal consequences of their proposals, and develop techniques to lower operating temperatures to reduce both transient and permanent component failures. Until recently, tools for understanding temperature ramifications of designs have been mainly restricted to industry for studying packaging and cooling mechanisms, with little access to such toolsets for academic researchers. Developing such tools is an arduous task since it usually requires cross-cutting areas of expertise spanning architecture, systems software, thermodynamics, and cooling systems. Recognizing the need for such tools, there has been recent work on modeling temperatures of processors at the microarchitectural level which can be easily understood and employed by computer architects for processor designs. However, there is a dearth of such tools in the academic/research community for undertaking architectural/systems studies beyond a processor- a <b>server</b> <b>box,</b> rack or even a machine room. This paper presents a detailed 3 -dimensional Computational Fluid Dynamics based thermal modeling tool, called ThermoStat, for rack-mounted server systems. Using this tool, we model a 20 (each with dual Xeon processors) node rack-mounted server system, and validate it with over 30 temperature sensor measurements at different points in the servers/rack. We conduct several experiments with this tool to show how different load conditions affect the thermal profile, and also illustrate how this tool can help design dynamic thermal management techniques...|$|E
40|$|Recent {{industry}} trends towards {{reducing the}} costs of ownership in large data centers emphasize the need for database system techniques for both automatic performance tuning and efficient resource usage. The goal is to host several database applications on a shared server farm, including scheduling multiple applications on the same physical server or even within a single database engine, while meeting each application’s service level agreement. Automatic provisioning of database servers to applications and virtualization techniques, such as, live virtual machine migration have been proposed as useful tools to address this problem. In this paper we argue that by allocating entire <b>server</b> <b>boxes</b> and migrating entire application stacks in cases of server overload, these solutions are too coarse-grained for many overload situations. Hence, they may result in resource usage inefficiency, performance penalties, or both. We introduce an outlier detection algorithm which zooms in to the fine-grained query contexts which are most affected by an environment change and/or where a perceived overload problem is likely to originate from. We show that isolating these query contexts through either memory quota enforcements or fine-grained load balancing across different database replicas of their respective applications allows us to alleviate resource interference in many cases of overload. ...|$|R
40|$|Made {{the system}} server {{that could be}} {{undertake}} and access from the other computer used between the face webmin and phpvirtualbox. Webmin was software based web {{that was used to}} arrange computer services server and could be accessed from the other computer. Inside there is a menu webmin server software that can be directly installed, including mysql,ssh server,samba,apache server and dhcp <b>server</b> Virtual <b>box</b> was software virtualisasi {{that could be used to}} undertake the system os in the main computer. whereas phpVirtualBox is a web-based front-end for VirtualBox that allows to control the environment VirtualBox. Because communication is done over the network, phpVirtualBox and VirtualBox does not have to be on the same physical machine...|$|R
40|$|Today’s {{interactive}} television systems are using proprietary communication protocols and interchange formats. To provide interoperability at the application level {{the next generation}} of {{interactive television}} system will be based on standardized communication protocols, monomedia and multimedia formats. This paper presents the Globally Accessible Services (GLASS) system which is a prototype interactive television system based on the Multimedia and Hypermedia Expert Group (MHEG) standard. After a brief introduction to MHEG as the multimedia interchange format between application <b>server</b> and set-top <b>box</b> in interactive television systems, the GLASS clients and servers are described, and an example scenario for navigation in the GLASS system is provided...|$|R
40|$|The {{popularity}} of the Internet {{and the demand for}} 24 / 7 services uptime is driving system performance and reliability requirements to levels that today’s data centers can no longer support. This article examines the traditional monolithic conventional server (CS) design and compares it to a new design paradigm: the disaggregated server (DS) data center design. The DS design arranges data centers resources in physical pools, such as processing, memory, and IO module pools, rather than packing each subset of such resources into a single <b>server</b> <b>box.</b> In this work, we study energy efficient resource provisioning and virtual machine (VM) allocation in DS-based data centers compared to CS-based data centers. First, we present our new design for the photonic DS-based data center architecture, supplemented with a complete description of the architectural components. Second, we develop a mixed integer linear programming (MILP) model to optimize VM allocation for the DS-based data center, including the data center communication fabric power consumption. Our results indicate that, in DS data centers, the optimum allocation of pooled resources and their communication power yields up to 42 % average savings in total power consumption when compared with the CS approach. Due to the MILP high computational complexity, we developed an energy efficient resource provisioning heuristic for DS with communication fabric (EERP-DSCF), based on the MILP model insights, with comparable power efficiency to the MILP model. With EERP-DSCF, we can extend the number of served VMs where the MILP model scalability for a large number of VMs is challenging. Furthermore, we assess the energy efficiency of the DS design under stringent conditions by increasing the CPU to memory traffic and by including high non-communication power consumption to determine the conditions at which the DS and CS designs become comparable in power consumption. Finally, we present a complete analysis of the communication patterns in our new DS design and some recommendations for design and implementation challenges...|$|E
5000|$|The Bloom Energy <b>Server</b> (the Bloom <b>Box)</b> {{is a solid}} {{oxide fuel}} cell (SOFC) power {{generator}} made by Bloom Energy, of Sunnyvale, California, that takes a variety of input fuels, including liquid or gaseous hydrocarbons produced from biological sources, to produce electricity {{at or near the}} site where it will be used. This new class of distributed power generator produce clean, reliable, affordable electricity at the customer site. It can withstand temperatures of up to [...] According to the company, a single cell (one 100 mm × 100 mm plate consisting of three ceramic layers) generates 25 watts.|$|R
5000|$|As {{an example}} {{consider}} the network shown above, in here [...] and [...] are users (senders), , and [...] are <b>servers</b> (receivers), the <b>boxes</b> are mixes, and , [...] and [...] where [...] denotes the anonymity set. Now {{as there are}} pool mixes let the cap {{on the number of}} incoming messages to wait before sending be as such if , or [...] is communicating with [...] and [...] receives a message then [...] knows that it must have come from ???? (as the links between the mixes can only have [...] message at a time). This is in no way reflected in 's anonymity set, but should be taken into account in the analysis of the network.|$|R
40|$|AbstractÐDesigning complex {{distributed}} client/server {{applications that}} meet performance requirements may prove extremely difficult in practice if software developers {{are not willing}} or {{do not have the}} time to help software performance analysts. This paper advocates the need to integrate both design and performance modeling activities so that one can help the other. We present a method developed and used by the authors in the design of a fairly large and complex client/server application. The method is based on a software performance engineering language developed by one of the authors. Use cases were developed and mapped to a performance modeling specification using the language. A compiler for the language generates an analytic performance model for the system. Service demand parameters at <b>servers,</b> storage <b>boxes,</b> and networks are derived by the compiler from the system specification. A detailed model of DBMS query optimizers allows the compiler to estimate the number of I/Os and CPU time for SQL statements. The paper concludes with some results of the application that prompted the development of the method and language. Index TermsÐSoftware performance engineering, performance models, client/server systems, queuing networks, database quer...|$|R
40|$|Data centre {{applications}} for batch processing (e. g. map/reduce frameworks) and online services (e. g. search engines) scale by distributing data and computation across many servers. They typically follow a partition/aggregation pattern: tasks are first partitioned across servers that process data locally, and then those partial results are aggregated. This data aggregation step, however, shifts the performance bottleneck to the network, which typically struggles to support many-to-few, high-bandwidth traffic between servers. Instead of performing data aggregation at edge servers, {{we show that}} it can be done more efficiently along network paths. We describe NETAGG, a software platform that supports on-path aggregation for network-bound partition/aggregation applications. NETAGG exploits a middlebox-like design, in which dedicated <b>servers</b> (agg <b>boxes)</b> are connected by high-bandwidth links to network switches. Agg boxes execute aggregation functions provided by applications, which alleviates network hotspots because {{only a fraction of the}} incoming traffic is forwarded at each hop. NETAGG requires only minimal application changes: it uses shim layers on edge servers to redirect application traffic transparently to the agg boxes. Our experimental results show that NETAGG improves substantially the throughput of two sample applications, the Solr distributed search engine and the Hadoop batch processing framework. Its design allows for incremental deployment in existing data centres and incurs only a modest investment cost...|$|R
40|$|Today's {{interactive}} television systems are using proprietary communication protocols and interchange formats. To provide inter-operability at the application level {{the next generation}} of {{interactive television}} system will be based on standardized communication protocols, monomedia and multimedia formats. This paper presents the Globally Accessible Services (GLASS) system which is a prototype interactive television system based on the Multimedia and Hypermedia Expert Group (MHEG) standard. After a brief introduction to MHEG as the multimedia interchange format between application <b>server</b> and set-top <b>box</b> in interactive television systems, the GLASS clients and servers are described, and an example scenario for navigation in the GLASS system is provided. Keywords: interactive television, multimedia systems, multimedia applications, standards, portable application 1. INTRODUCTION Global interactive television (iTV) systems have to be inter-operable systems. To achieve this inter-operabil [...] ...|$|R
40|$|We {{have been}} {{investigating}} {{the use of}} low-cost, commodity components for multi-terabyte SQL Server databases [SQL]. Dubbed storage bricks, these <b>servers</b> are white <b>box</b> PCs containing the largest ATA drives, value-priced AMD or Intel processors, and inexpensive ECC memory. One issue has been the wiring mess, air flow problems, length restrictions, and connector failures created by seven or more parallel ATA (PATA) ribbon cables and drives in]a tower or 3 U rack-mount chassis. Large capacity Serial ATA (SATA) drives have recently become widely available for the PC environment at a reasonable price. In addition to being faster, the SATA connectors seem more reliable, have a more reasonable length restriction (1 m) and allow better airflow. We tested two drive brands along with two RAID controllers to evaluate SATA drive performance and reliablility. This paper documents our results so far...|$|R
50|$|Unicast {{protocols}} send {{a separate}} {{copy of the}} media stream from the server to each recipient. Unicast is the norm for most Internet connections, but does not scale well when many users want to view the same television program concurrently. Multicast protocols were developed to reduce the server/network loads resulting from duplicate data streams that occur when many recipients receive unicast content streams independently. These protocols send a single stream from the source {{to a group of}} recipients. Depending on the network infrastructure and type, multicast transmission {{may or may not be}} feasible. One potential disadvantage of multicasting is the loss of video on demand functionality. Continuous streaming of radio or television material usually precludes the recipient's ability to control playback. However, this problem can be mitigated by elements such as caching <b>servers,</b> digital set-top <b>boxes,</b> and buffered media players.|$|R
40|$|National audienceThe {{alliance}} between the Smart Home embedded devices e. g., gateways, tablets, media <b>server,</b> home automation <b>boxes,</b> and the Cloud for the Smart Home Service hosting becomes a proven reality. Taking into account Cloud to overcome the embedded resources limitation e. g. for reducing the bill of material for a new customer, or to offer service flexibility and scalability is a very appealing goal. In order to take benefit of these opportunities, selecting an optimal deployment of Service applications in this distributed platform is required. We introduce a new approach based on feature modelling which has been originally introduced for SPL for expressing the deployment constraints {{in spite of the}} Smart Home wide variability. As a first step we show how to get an optimized first deployment of any given set of Home Services. In a second step we will address the deployment adaptation at runtime in response to environment or software evolution...|$|R
40|$|An {{important}} {{development in}} the television environment, and one that has strong resonance for iTV applications is the introduction of Personal Video Recorders (PVRs). Many of today’s researchers forecast that {{a significant portion of}} future television viewing will be from PVRs rather than from realtime broadcast sources. Some current PVRs, and most future units, also store iTV elements of a program {{so that they can be}} selectively accessed upon later for playback. While today’s PVRs are standalone devices, those of the future may be part of a PC, a home <b>server,</b> set-top <b>box,</b> or even a remote storage system in which the actual physical storage is on a server at some service provider’s location supporting the required personal access. In this paper, we propose a dynamic channel allocation on a Fiber Cable (FC) network to realize a Remote PVR (RPVR) at the Multiple System Operator (MSO) end. The VCR controls exercised by the user are carried across the cable network to the cable head-end. The request is processed at the head-end and results are communicated back to the user device. Key features of the proposed RPVR approach are allocation of PVR channels for userspecific PVR operations, initiation and closing of PVR sessions, maintaining data related to PVR sessions in user-specific storage at the head-end, and supporting Digital Rights Management (DRM) based VCR functionality. In the proposed system, we use the “assign and reassign ” spectrum of FC architecture, and have proposed a “split and merge ” signal handling mechanisms to support the RPVR application. The proposed approach uses the available spectrum on FC to realize RPVR functionality...|$|R
40|$|A Video-on-Demand (VoD) {{system is}} a video rental system, which {{delivers}} videos on demand. Due to the large size of digitalized videos, expensive and high IO processing power video servers are {{needed in order to}} provide VoD services in metropolitan area. Due to the low scalability, the classical unicast VoD system is not suitable for large-scale deployments. In this thesis, a highly scalable VoD system of the lower per-user cost is described. In chapter 2, the system performance degradation problems that occur during the handlings of interactions in batching VoD systems and in the centralized buffer VoD system, the Split and Merge (SAM) system, are discussed. Then a new system called the Multi-Batch Buffer (MBB) system, which attempts to solve these problems, is proposed. The proposed system handles a majority of interaction requests by the scalable buffering techniques employed in the buffer of the local <b>servers</b> and set-top <b>boxes</b> (STBs). The comparisons of the simulation results with the other systems show that the proposed system can tackle the system degradation problems...|$|R
40|$|On-board bias {{regulator}} Minimum output voltage: 0. 6 V 0. 6 V {{reference voltage}} with ± 1. 0 % accuracy Supports all N-channel MOSFET power stages Available in 300 kHz, 600 kHz, and 1. 0 MHz options No current sense resistor required Power saving mode (PSM) for light loads (ADP 1879 only) Resistor programmable current limit Power good with internal pull-up resistor Externally programmable soft start Thermal overload protection Short-circuit protection Standalone precision enable input Integrated bootstrap diode for high-side drive Starts into a precharged output Available in a 14 -lead LFCSP_WD package APPLICATIONS Telecommunications and networking systems Mid-to-high end <b>servers</b> Set-top <b>boxes</b> DSP core power supplies GENERAL DESCRIPTION The ADP 1878 /ADP 1879 are versatile current-mode, synchronous step-down controllers. They provide superior transient response, optimal stability, and current-limit protection {{by using a}} constant on time, pseudo fixed frequency with a programmable current-limit, current control scheme. These devices offer optimum performance at low duty cycles by using a valley, current-mode control architecture allowing the ADP 1878 /ADP 1879 to drive all N-channel power stages to regulate output voltages to as low as 0. 6 V. The ADP 1879 is the power saving mode (PSM) version of the device and is capable of pulse skipping to maintain output regulation while achieving improved system efficiency at light loads (see the ADP 1879 Power Saving Mode (PSM) section for more information). Available in three frequency options (300 kHz, 600 kHz, and 1. 0 MHz) plus the PSM option, the ADP 1878 /ADP 1879 are well suited {{for a wide range}} of applications that require a single input power supply range from 2. 95 V to 20 V. Low voltage biasing is supplied via a 5 V internal low dropout regulator (LDO). In addition, soft start programmability is included to limit input inrush current from the input supply during startup and to provide reverse current protection during precharged outpu...|$|R
40|$|The content {{downloading}} is internet based {{service and}} theexpectation of this services are highly popular in wireless communication {{it will be}} supporting for road side communication. We are focusing in content downloading system for both infrastructure-to-vehicle and vehicle-to-vehicle communication. The goal to improving system throughput and formulating a max-flow problem including the channel contention and data transfer paradigm. A system communication while transferring the files or downloading some application in road side environment {{there is the possibility}} of getting disconnected. The purpose of this study used to avoid the in conventional connection at the road side environment while using system or mobile based internet connection used for content or file downloader using MILP(Mixed Integer Linear programming) for max flow problem. The bounding box technique will be used to get the proper signal from base station. To avoid the traffic and access the quick response from the <b>server</b> the bounding <b>box</b> will used. The mail goal of the mobility management service is to trace the location where the subscribers are, allowing calls, SMS and other mobile phone services to be delivered to them. First we can analysing the data and select for correct location [...] It will be provide challenging in vehicular networks, that is the transmission speed of the nodes will even more efficient though the area surrounded of buildings and many other architectural infrastructures of the radio signal...|$|R
500|$|The {{video was}} filmed {{in front of}} a live {{audience}} at the United Nations General Assembly in New York on August 10, 2012. SuperUber worked on integration between animations and technology, the structure design and projection mapping that took over the General Assembly Hall after it was invited by Kenzo Digital. 10 20K HLM Barco projectors were placed and mapping was done using Pandora's widget designer and 5 Pandora's Dual <b>Box</b> <b>servers.</b> The final video had an 8856 x 1664 resolution and was split into 10 slices of 1080 x 1664 pixels, since each projector was turned on its side and the images were projected vertically in order to cover the whole structure. Two slices were fed into each system and SuperUber did the warping and edge blending inside Pandora. The screen which was used was over 12,000 square feet; Lucas Werthein, a representative for SuperUber commented that it was the largest indoor temporary screen that was a tilted compound curve adding that from an engineering standpoint, it is [...] "extremely complex". It was custom-designed to mold {{to the inside of the}} General Assembly Hall. It spanned 68m X 15m and surrounded the audience with a 240 degree immersive projection. Ten synchronized and mapped projections covered the screen with 200,000 lumens, creating one continuous giant image. The screen which weighed 1000 pounds was sewn by forty five people. The whole structure, which was hanging off the UN ceiling weighed 6000 pounds. Werthein revealed, [...] "In order to create the proper tension for the screen to hold its shape we had to weld permanent rigging points into the dome of the general assembly." [...] Peter Kirn of Create Digital Motion wrote that the projection mapping was performed onto a fairly simple surface: [...] "What makes the mapping so effective is the way in which it can fill the space, making those pictures immersive on a grand architectural scale. It makes the image a real volume in which the performance can take place." [...] Russ Rive, SuperUber’s director said, [...] "It’s an ambitious project that has completely transformed an iconic location – seen as immutable and unchanging, with its goldplated panel and grand volume. By adding a virtual layer to it, we could digitally rebuild it, playing with the architectural elements, and therefore changing the notion of something static. The technology made the integration between animation and architecture. We used projectors to 'paint with light' the UN General Assembly’s Hall – a unique opportunity to transform such an emblematic place." ...|$|R

