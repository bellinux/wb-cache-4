19|10000|Public
2500|$|We take a [...] <b>sample</b> <b>with</b> <b>replacement</b> of n values y1,...,y'n {{from the}} population, where n<nbsp&N, and {{estimate}} the variance {{on the basis}} of this sample. Directly taking the variance of the sample data gives the average of the squared deviations: ...|$|E
5000|$|<b>Sample,</b> <b>with</b> <b>replacement,</b> [...] {{training}} {{examples from}} , call these , [...]|$|E
5000|$|<b>Sample</b> <b>with</b> <b>replacement</b> N {{times from}} the set [...] with {{probability}} [...] to generate a realization of [...]|$|E
3000|$|... has {{the same}} number of tuples as D that are <b>sampled</b> <b>with</b> <b>replacement</b> from D. By <b>sampling</b> <b>with</b> <b>replacement,</b> it means that some of the {{original}} tuples of D may not be included in D [...]...|$|R
5000|$|Partition [...] into [...] subsets [...] {{with each}} element of [...] {{belonging}} {{to one of}} the [...] <b>with</b> equal probability (<b>sampling</b> <b>with</b> <b>replacement)</b> ...|$|R
30|$|There are two {{challenges}} in generalizing the random-sampling technique of [7], namely, (i) <b>sampling</b> <b>with</b> <b>replacement</b> cannot be used, and (ii) weights {{must be part}} of the sampling process.|$|R
5000|$|The {{training}} algorithm for random forests {{applies the}} general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set [...] = , ..., [...] with responses [...] = , ..., , bagging repeatedly (B times) selects a random <b>sample</b> <b>with</b> <b>replacement</b> {{of the training}} set and fits trees to these samples: ...|$|E
5000|$|If {{the members}} of the {{population}} come in three kinds, say [...] "blue" [...] "red" [...] and [...] "black", the number of red elements in a sample of given size will vary by sample and hence is a random variable whose distribution can be studied. That distribution depends on the numbers of red and black elements in the full population. For a simple random <b>sample</b> <b>with</b> <b>replacement,</b> the distribution is a binomial distribution. For a simple random sample without replacement, one obtains a hypergeometric distribution.|$|E
40|$|Resampling (a. k. a. bootstrapping) is a computationallyintensive {{statistical}} {{technique for}} estimating the sampling distribution of an estimator. Resampling {{is used in}} many machine learning algorithms, including ensemble methods, active learning, and feature selection. Resampling techniques generate pseudosamples from an underlying population by sampling with replacement from a single sample dataset. It is straightforward to <b>sample</b> <b>with</b> <b>replacement</b> from propositional data that are independent and identically distributed (i. i. d.). However, {{it is not clear}} how to <b>sample</b> <b>with</b> <b>replacement</b> from an interconnected relational data graph with dependencies among related instances. In this paper, we develop a novel method for resampling from relational data that uses a subgraph sampling approach to preserve the local relational dependencies while generating a pseudosample with sufficient global variance. We evaluate our approach on synthetic data, showing that compared to an i. i. d. resampling approach it results in significantly lower error when used to estimate the variance of feature scores. We also evaluate our approach on a real-world relational classification task, showing that it improves the accuracy of bagging when compared with i. i. d. resampling. 1...|$|E
40|$|For {{the mean}} of a finite population, a bounded risk {{estimation}} problem is considered for both the situations Where the population variance mayor may not be known. In this context, three popular (equal probability) sampling strategies are considered. These are the analogues of (i) simple random <b>sampling</b> <b>with</b> <b>replacement,</b> mean per unit estimation, (ii) simple random <b>sampling</b> <b>with</b> <b>replacement,</b> mean per distinct unit estimation, and (iii) simple random sampling without replacement, mean per unit estimation. It {{is well known that}} in the conventional fixedsample size scheme, (iii) fares better than (ii) and (ii) better than (i) ...|$|R
5000|$|From this perspective, {{the case}} labeled [...] "Surjective f" [...] is {{somewhat}} strange: Essentially, we keep <b>sampling</b> <b>with</b> <b>replacement</b> until we've chosen each item at least once. Then, we count how many choices we've made, and if it's not equal to N, {{throw out the}} entire set and repeat. This is vaguely comparable to the coupon collector's problem, where the process involves [...] "collecting" [...] (by <b>sampling</b> <b>with</b> <b>replacement)</b> a set of X coupons until each coupon has been seen at least once. Note that in all [...] "surjective" [...] cases, the number of sets of choices is zero unless N ≥ X.|$|R
30|$|Bagging (also {{known as}} {{bootstrap}} aggregation) is an ensemble learner {{that attempts to}} improve {{the stability of the}} classifiers and reduce variance. Bagging works well with unstable classifiers such as decision trees [31]. The algorithm works by <b>sampling</b> <b>with</b> <b>replacement,</b> thus creating a number of so-called bags. Each bag contains a sample representative of the original dataset and the <b>sampling</b> is done <b>with</b> <b>replacement.</b>|$|R
40|$|AbstractGiven a <b>sample</b> <b>with</b> <b>replacement</b> from {{a finite}} set M, we show simply how to {{generate}} a maximal sequence of functions of the sample, all uniform on M, such that these functions are pairwise independent. We also consider the problem of generating a sequence of k-wise independent functions of the sample. To this end {{we set up a}} geometrical framework in which this problem is intimately related to that of finding, in a projective geometry of dimension r, a set of points such that no one subset with k points belongs to a hyperplane of dimension k − 1...|$|E
40|$|We {{consider}} the simplest split-merge Markov operator T on the infinite-dimensional simplex Σ 1 of monotone non-negative sequences with unit sum. For a sequence x ∈ Σ 1, it picks a size-biased <b>sample</b> (<b>with</b> <b>replacement)</b> of two elements of x; if these elements are distinct, it merges them and reorders the sequence, {{and if the}} same element is picked twice, it splits this element uniformly into two parts and reorders the sequence. We prove that the means along the T-trajectory of the δ-measure at the vector (1, 0, 0, [...] .) converge to the Poisson–Dirichlet distribution PD(1). 1 Partially supported by RFBR grant 00 – 15 – 96060...|$|E
40|$|Let e, e; e 2, e; [...] .; el, el; [...] . be a {{sequence}} of ordered pairs of edges chosen uniformly at random from the edge set of the complete graph Ks (i. e. we <b>sample</b> <b>with</b> <b>replacement).</b> This sequence is used to form a graph by choosing at stage i, i [...] 1, [...] ., one edge from el, eti to be an edge in the graph, where the choice at stage i is based only on the observation of the edges that have appeared by stage i. We show that these choices can be made so that whp {{the size of the}} largest component of the graph formed at stage. 535 n is polylogarithmic in n. This resolves a question of Achlioptas...|$|E
3000|$|... nThis serves {{much the}} same purpose as the <b>sampling</b> <b>with</b> <b>replacement</b> used in random forests. A smaller sample is {{adequate}} because when sampling without replacement, no case is selected more than once; there are no “duplicates.” [...]...|$|R
30|$|We {{produced}} {{logistic regression}} models for 1000 bootstrapped <b>samples,</b> <b>sampled</b> <b>with</b> <b>replacement,</b> from our dataset (Fox 2002). We averaged these models {{to produce a}} final validation model to compare the ORs, 95  % CIs, p values, and the AIC with our prediction model.|$|R
30|$|As {{described}} here, {{this process}} involves <b>sampling</b> <b>with</b> <b>replacement.</b> If, {{as is often}} the case, sampling without replacement is required, then no individual is permitted to enter the sample more than once. This may then require some additional plot selection to achieve the desired sample size.|$|R
40|$|This paper {{describes}} {{the use of}} resampling technique in validity testing and reliability testing that are widely used {{in order to make}} the measurement tool in the field of psychology and education research. Resampling technique is the technique of resampling <b>sample</b> <b>with</b> <b>replacement</b> or without replacement. Resampling technique can be used to determine whether the item is valid or not by using the percentile confidence interval. The same technique can also be used to determine the significance of reliability coefficients in order to obtain a reliable measurement tool. This technique can also be used to obtain a higher reliability coefficient by reducing the sample size or by reducing the number of items used in the calculation. In this paper, the technique/method is described in the mini-data and case studies using real data that has 40 items and 48 respondents...|$|E
40|$|While {{majority}} cycles {{may pose}} a threat to democratic decision making, actual decisions based inadvertently upon an incorrect majority preference relation may be far more expensive to society. We study majority rule both in a statistical sampling and a Bayesian inference framework. Based on any given paired comparison probabilities or ranking probabilities in a population (i. e., culture) of reference, we derive upper and lower bounds on the probability of a correct or incorrect majority social welfare relation in a random <b>sample</b> (<b>with</b> <b>replacement).</b> We also present upper and lower bounds on the probabilities of majority preference relations in the population given a sample, using Bayesian updating. These bounds permit to map quite precisely the entire picture of possible majority preference relations as well as their probabilities. We illustrate our results using survey data. Copyright Springer-Verlag Berlin Heidelberg 2003...|$|E
30|$|In a {{simulation}} without transfer steps, the Gillespie algorithm is simply {{run for the}} given number of time units. In {{a simulation}} with transfer steps, {{at the end of}} each time unit first the solution is diluted to 10 %. Rather than simply reducing the concentration of each molecule type to 10 %, this dilution is done by randomly drawing molecules (without replacement) from the solution with probabilities according to their relative current concentrations, until 10 % of the total concentration is reached. Next, from this diluted solution a random <b>sample</b> (<b>with</b> <b>replacement)</b> of 75 molecules is taken from among the 16 EMN types. Each EMN type that occurs twice or more in this random sample is reported. Finally, the diluted solution is replenished with the 8 food molecule types (in equal concentrations) until the initial total concentration of 16, 000 is reached again. These transfer steps are then repeated for the given number of time units. This provides a detailed simulation of the original laboratory serial transfer experiments [10], which can be repeated an arbitrary number of times.|$|E
40|$|Classifier {{ensembles}} is {{an active}} area of research within the machine learning community. One {{of the most successful}} techniques is bagging, where an algorithm (typically a decision tree inducer) is applied over several di#erent training sets, obtained applying <b>sampling</b> <b>with</b> <b>replacement</b> to the original database. In this paper we define a framework where <b>sampling</b> <b>with</b> and without <b>replacement</b> can be viewed as the extreme cases of a more general process, and analyze the performance of the extension of bagging to such framework...|$|R
3000|$|... -net theorem of Haussler and Welz [7]. As {{explained}} in Section 3.2, the two challenges in generalizing the random-sampling technique are that (i) <b>sampling</b> <b>with</b> <b>replacement</b> cannot be used, and (ii) weights {{must be part}} of the sampling process. Our contribution is a new method to obtain weighted [...]...|$|R
40|$|The {{problem of}} {{estimating}} the population proportion possessing a sensitive attribute using simple random <b>sampling</b> <b>with</b> <b>replacement</b> (SRSWR) is advocated. Two new procedures are proposed. The suggested models are {{more efficient than}} the Huang (2004) randomized response technique under some realistic conditions. Numerical and graphic illustrations are given...|$|R
30|$|First {{argument}} Model {{defines the}} model to be implemented, which contains specification of likelihood and prior. LaplaceApproximation passes two argument to the model function, parm and Data, and receives five arguments from the model function: LP (the logarithm of the unnormalized joined posterior density), Dev (the deviance), Monitor (the monitored variables), yhat (the variables for the posterior predictive checks), and parm, the vector of parameters, which may be constrained in the model function. The argument parm requires a vector of initial values equal in length {{to the number of}} parameters, and LaplaceApproximation will attempt to optimize these initial values for the parameters, where the optimized values are the posterior modes. The Data argument requires a listed data which must be include variable names and parameter names. The argument sir=TRUE stands for implementation of sampling importance resampling algorithm, which is a bootstrap procedure to draw independent <b>sample</b> <b>with</b> <b>replacement</b> from the posterior sample with unequal sampling probabilities. Contrary to sir of LearnBayes package, here proposal density is multivariate normal and not t.|$|E
30|$|Assuming {{that the}} error {{structure}} of residual values was not appropriate to generate reliable variances for model coefficients, we implemented random-x resampling (Fox 2002) in a bootstrapping framework to assess coefficient variability (Efron 1979, Khurshid et al. 2005). Bootstrapping methods reflected Type- 2 resource selection (Thomas and Taylor 2006), in which discrete animals were the experimental units used {{to account for}} individual variation in RSF models. We randomly selected {{the same number of}} animals present in the original <b>sample</b> (<b>with</b> <b>replacement),</b> and used locations from these animals to populate response variables within the best models and generate new variable coefficient point estimates. We then estimated respective standard errors and 95 % confidence intervals by encapsulating the mid-ninety-fifth percentile of each coefficient under 1000 bootstrap iterations. We validated RSFs by populating top model response variables using supplemental location data acquired from bighorns that collected data either before or after habitat alterations. We then assessed whether coefficient point estimates fell within bootstrapped confidence intervals generated from paired data. Finally, we determined significant habitat selection of fire-altered habitats if bootstrapped confidence intervals for the interaction term did not include zero, and we created coefficient plots for visual comparisons among other predictor variables.|$|E
40|$|Let e 1, e 2, [...] . be a {{sequence}} of edges chosen uniformly at random from the edge set of the complete graph Kn (i. e. we <b>sample</b> <b>with</b> <b>replacement).</b> Our goal is to choose, for m as large as possible, a subset E ⊆ {e 1, e 2, [...] ., e 2 m}, |E | = m, such {{that the size of}} the largest component in G = ([n], E) is o(n) (i. e. G does not contain a giant component). Furthermore, the selection process must take place on-line; that is, we must choose to accept or reject an ei based on the previously seen edges e 1, [...] ., ei− 1. We describe an on-line algorithm that succeeds whp 1 for m =. 9668 n. Furthermore, we find a tight threshold for the off-line version of this question; that is, we find the threshold for the existence of m out of 2 m random edges without a giant component. This threshold is m = c ∗ n where c ∗ satisfies a certain transcendental equation, c ∗ ∈ [. 9792,. 9793]. We also establish new upper bounds for more restricted Achlioptas processes. ...|$|E
40|$|In 1990, Frank Wright {{introduced}} {{a method for}} measuring synonymous codon usage bias in a gene by estimation of the “effective number of codons,” Nc. Several {{attempts have been made}} recently to improve Wright's estimate of Nc, but the methods that work in cases where a gene encodes a protein not containing all amino acids with degenerate codons have not been tested against each other. In this article I derive five new estimators of Nc and test them together with the two published estimators, using resampling under rigorous testing conditions. Estimation of codon homozygosity, F, {{turns out to be a}} key to the estimation of Nc. F can be estimated in two closely related ways, corresponding to <b>sampling</b> <b>with</b> or without <b>replacement,</b> the latter being what Wright used. The Nc methods that are based on sampling without replacement showed much better accuracy at short gene lengths than those based on <b>sampling</b> <b>with</b> <b>replacement,</b> indicating that Wright's homozygosity method is superior. Surprisingly, the methods based on <b>sampling</b> <b>with</b> <b>replacement</b> displayed a superior correlation with mRNA levels in Escherichia coli...|$|R
40|$|Equiprobable <b>samples</b> <b>with</b> <b>replacements</b> from Їnite Abelian {{groups are}} con- sidered. Limit theorems are proved {{describing}} convergence {{of the distribution}} of the number of ordered subsamples meeting speciЇed linear relations to Poisson distributions. Basing on these theorems we construct a goodness-of-Їt test which checks the hypothesis on the uniform distribution of sample elements...|$|R
5000|$|The {{conditional}} {{density of}} the object at the current time [...] is estimated as a weighted, time-indexed <b>sample</b> set [...] <b>with</b> weights [...] N is a parameter determining the number of sample sets chosen. A realization of [...] is obtained by <b>sampling</b> <b>with</b> <b>replacement</b> from the set [...] with probability equal to the corresponding element of [...]|$|R
40|$|Let e 1; e 0 1; e 2; e 0 2; : : :; e i; e 0 i; : : : be a {{sequence}} of ordered pairs of edges chosen uniformly at random from the edge set of the complete graph Kn (i. e. we <b>sample</b> <b>with</b> <b>replacement).</b> This sequence is used to form a graph by choosing at stage i, i = 1; : : : one edge from e i; e 0 i to be an edge in the graph, where the choice at stage i is based only on the observation of the edges that have appeared by stage i. We show that these choices can be made so that whp {{the size of the}} largest component of the graph formed at stage : 535 n is polylogarithmic in n. This resolves a question of Achlioptas. 1 Introduction Let e 1; e 2; : : : be {{a sequence}} of edges from the edge set of the complete graph K n where e i is chosen uniformly at random from the collection of edges that have not yet appeared. Let Em = fe 1; e 2; : : :; e m g and Gm = ([n]; Em) be the mth random graph in this process. It is a classical theorem, due to Erd}os and Renyi [3], that if m = [...] ...|$|E
40|$|Background: Delivery of {{maternal}} {{health care services}} is a major challenge to the health system in developing countries. Provision of antenatal care (ANC) services is the major function of public health delivery system in India to improve maternal health outcomes {{and its impact on}} maternal morbidity and mortality. Studies are lack in documenting variation in utilization of ANC services between geographical regions of Andhra Pradesh (AP). Objective: The objective {{of this study is to}} assess variation in utilization of ANC services stratified by geographical region, type of delivery and determinants of utilization of ANC services in AP. Methodology: It is a cross-sectional study of District Level Household and Facility Survey- 4 of the state of AP. Multistage, stratified and probability proportional to size <b>sample</b> <b>with</b> <b>replacement</b> was used. Around 3982 women who delivered after the year 2007 were considered for analysis. Binomial logistic regression was carried out to determine association of demographic, system level variables with adequate ANC. Results: Study reveals wide variation across four regions of AP in utilization of ANC services. Reception of adequate ANC was low in Rayalaseema region (27. 9 %) and high in North-coastal region (42. 4 %). The utilization of private health facilities for ANC services were highest in South-coastal region (73. 2 %) and lowest in North-coastal region (43. 2 %). Conclusion: Policy measures are to be adopted and implemented by government to address the demand-supply imbalance such as public health infrastructure and quality of services in underperforming districts of AP and to increase outreach of current programs by engaging communities...|$|E
40|$|The {{availability}} of high-throughput parallel methods for sequencing microbial communities is increasing {{our knowledge of}} the microbial world at an unprecedented rate. Though most attention has focused on determining lower-bounds on the a-diversity i. e. the total number of different species present in the environment, tight bounds on this quantity may be highly uncertain because {{a small fraction of the}} environment could be composed of a vast number of different species. To better assess what remains unknown, we propose instead to predict the fraction of the environment that belongs to unsampled classes. Modeling samples as draws with replacement of colored balls from an urn with an unknown composition, and under the sole assumption that there are still undiscovered species, we show that conditionally unbiased predictors and exact prediction intervals (of constant length in logarithmic scale) are possible for the fraction of the environment that belongs to unsampled classes. Our predictions are based on a Poissonization argument, which we have implemented in what we call the Embedding algorithm. In fixed i. e. non-randomized sample sizes, the algorithm leads to very accurate predictions on a sub-sample of the original sample. We quantify the effect of fixed sample sizes on our prediction intervals and test our methods and others found in the literature against simulated environments, which we devise taking into account datasets from a human-gut and-hand microbiota. Our methodology applies to any dataset that can be conceptualized as a <b>sample</b> <b>with</b> <b>replacement</b> from an urn. In particular, it could be applied, for example, to quantify th...|$|E
30|$|The {{procedure}} of bootstrapping is a re-sampling method which relies on random <b>sampling</b> <b>with</b> <b>replacement</b> {{of the available}} observations. This procedure allows evaluation {{of the characteristics of}} an estimator (such as its variance) by measuring those properties when obtaining multiple samples from the original dataset (and of size equal to the observed dataset) [15, 16].|$|R
40|$|This {{paper is}} devoted {{to the study of the}} {{estimation}} of the population mean using of product estimator under ranked set. Strategies for different alternative estimators are developed and compared with theirs simple random <b>sampling</b> <b>with</b> <b>replacement</b> counterparts. The <b>sample</b> errors of them are compared analytically. KEY WORDS: product estimators, gain in accuracy, order statistic...|$|R
3000|$|Bootstrap is a {{strategy}} introduced by Efron and Tibshirani [176, 177]. Bootstrap is commonly used when working on a small dataset [159]. In this strategy, a bootstrap set is created by uniformly <b>sampling,</b> <b>with</b> <b>replacement,</b> n instances from the original data to make a training set. The remaining samples not selected are used as testing set. The value n of selected samples is likely to change from fold to fold. Since data is <b>sampled</b> <b>with</b> <b>replacement,</b> the probability of any data sample not being selected is given by [...] (1 - 1 /n)^n≈ e^- 1 ≈ 0.368. Chances of a data sample being selected into a train set is (1 − 0.368) = 0.632. Therefore, the expected number of distinct samples appearing in the train set is 0.632 × n. Since error estimate obtained by using test data will be too pessimistic (since only 62.3 [...]...|$|R
