1063|3550|Public
25|$|Any time-invariant {{operations}} also preserves AEP, stationarity and ergodicity {{and we may}} easily turn a <b>stationary</b> <b>process</b> to non-stationary {{without losing}} AEP by nulling out {{a finite number of}} time samples in the process.|$|E
500|$|When {{the index}} set [...] can be {{interpreted}} as time, a stochastic process is said to be stationary if its finite-dimensional distributions are invariant under translations of time. This type of stochastic process can be used to describe a physical system that is in steady state, but still experiences random fluctuations. The intuition behind such stationarity is that as time passes the distribution of the stationary stochastic process remains the same. A sequence of random variables forms a <b>stationary</b> <b>process</b> if and only if the [...] random variables are identically distributed.|$|E
2500|$|Discrete-time {{functions}} can be interpolated to continuous-time functions. If such interpolation f is measurable, we may {{define the}} continuous-time <b>stationary</b> <b>process</b> accordingly as [...] If AEP holds for the discrete-time process, {{as in the}} i.i.d. or finite-valued stationary ergodic cases shown above, it automatically holds for the continuous-time <b>stationary</b> <b>process</b> derived from it by some measurable interpolation. i.e.|$|E
40|$|In {{this paper}} we discuss set-valued <b>{{stationary}}</b> <b>processes.</b> First, we prove a stationary selection and representation theorem, then we study the laws of large numbers and ergodicities of set-valued <b>stationary</b> <b>processes.</b> set-valued <b>stationary</b> <b>processes</b> <b>stationary</b> selection and representation law of large number ergodicity. ...|$|R
40|$|Many of the proofs {{of various}} central limit theorems and {{laws of the}} {{iterated}} logarithm for strictly <b>stationary</b> <b>processes</b> are based on approximating martingales. Here we study on this line the functional central limit theorem and law of the iterated logarithm for <b>stationary</b> <b>processes,</b> not necessarily possessing a coboundary decomposition, with applications to <b>stationary</b> linear <b>processes.</b> Central limit theorem Law of the iterated logarithm Strictly <b>stationary</b> <b>processes</b> <b>Stationary</b> linear <b>processes</b> Martingale difference sequences...|$|R
40|$|Intended for {{a second}} course in <b>stationary</b> <b>processes,</b> <b>Stationary</b> Stochastic Processes: Theory and Applications {{presents}} the theory behind the field’s widely scattered applications in engineering and science. In addition, it reviews sample function properties and spectral representations for <b>stationary</b> <b>processes</b> and fields, including a portion on <b>stationary</b> point <b>processes...</b>|$|R
2500|$|... {{exist and}} are equal for any <b>stationary</b> <b>process</b> {{including}} the stationary ergodic process X. Denote it as H.|$|E
2500|$|... where, [...] is {{the rate}} matrix. [...] Note that by definition, the columns of [...] sum to zero. [...] For a <b>stationary</b> <b>process,</b> where [...] {{does not depend}} upon time t, this {{differential}} equation is solvable using matrix exponentiation: ...|$|E
2500|$|... (The [...] "rate of self-information" [...] {{can also}} be defined for a {{particular}} sequence of messages or symbols generated by a given stochastic process: this will always be equal to the entropy rate {{in the case of}} a <b>stationary</b> <b>process.)</b> Other quantities of information are also used to compare or relate different sources of information.|$|E
40|$|On {{replacing}} the usual arithmetic addition by the dyadic addition and trigonometric functions by Walsh functions, daydic <b>stationary</b> <b>processes</b> {{defined in the}} sense that their covariance functions are invariant under the shift by the dyadic addition have almost the same structures as those of ordinary <b>stationary</b> <b>processes.</b> However, there are some differences between finite parametric linear models of dyadic <b>stationary</b> <b>processes</b> and those of ordinary <b>stationary</b> <b>processes.</b> It is shown in theorem 2 that a dyadic autoregressive process of finite order analogously defined to the ordinary autoregressive models is always inverted into a dyadic moving average process of finite order, and vice versa. As is well known, these results are not true for the ordinary <b>stationary</b> <b>processes</b> (see, e. g., Box and Jenkins [1]) ...|$|R
40|$|We {{prove the}} almost sure {{invariance}} principle for martingales with stationary ergodic differences taking values in a separable 2 -smooth Banach space (for instance a Hilbert space). A compact {{law of the}} iterated logarithm is established {{in the case of}} stationary differences of reverse martingales. Then, we deduce the almost sure invariance principle for <b>stationary</b> <b>processes</b> under the Hannan condition; and a compact law of the iterated logarithm for <b>stationary</b> <b>processes</b> arising from non-invertible dynamical systems. Those results for <b>stationary</b> <b>processes</b> are new, even in the real valued case. We also obtain the Marcinkiewicz-Zygmund strong law of large numbers for <b>stationary</b> <b>processes</b> with values in some smooth Banach spaces...|$|R
40|$|AbstractMany of the proofs {{of various}} central limit theorems and {{laws of the}} {{iterated}} logarithm for strictly <b>stationary</b> <b>processes</b> are based on approximating martingales. Here we study on this line the functional central limit theorem and law of the iterated logarithm for <b>stationary</b> <b>processes,</b> not necessarily possessing a coboundary decomposition, with applications to <b>stationary</b> linear <b>processes...</b>|$|R
2500|$|Homogeneous Poisson point {{processes}} do {{not depend on}} the position of the underlying space through its parameter , which implies it is both a <b>stationary</b> <b>process</b> (invariant to translation) and an isotropic (invariant to rotation) [...] stochastic process. [...] Similarly to the one-dimensional case, the homogeneous point process is restricted to some bounded subset of , then depending on some definitions of stationarity, the process is no longer stationary.|$|E
2500|$|An {{important}} {{class of}} such continuous-time <b>stationary</b> <b>process</b> is the bandlimited stationary ergodic process with the sample space being {{a subset of}} the continuous [...] functions. AEP holds if the process is white, in which case the time samples are i.i.d., or there exists T > 1/2W, where W is the nominal bandwidth, such that the T-spaced time samples take values in a finite set, in which case we have the discrete-time finite-valued stationary ergodic process.|$|E
2500|$|A {{number of}} point {{processes}} {{have been suggested}} to model the positioning of wireless network nodes. Among these, {{the most frequently used}} is the Poisson process, which gives a Poisson network model. The Poisson process in general is commonly used as a mathematical model across numerous disciplines due to its highly tractable and well-studied nature. It is often assumed that the Poisson process is homogeneous (implying it is a <b>stationary</b> <b>process)</b> with some constant node density λ. For a Poisson process in the plane, this implies that the probability of having n points or nodes in a bounded region B is given by ...|$|E
40|$|Abstract A fixed {{transformation}} {{are given}} for one-dimensional <b>stationary</b> <b>processes</b> in this paper. Based on this, we propose a general filtering problem of <b>stationary</b> <b>processes</b> with fixed transformation. Finally, on a <b>stationary</b> <b>processes</b> with no any additional conditions, {{we get the}} spectral characteristics of P H &# 951; (t) &# 958; in the space L 2 (F X (d&# 955;)), and then we calculate {{the value of the}} best predict quantity Q of the general filtering problem. </p...|$|R
40|$|The article {{contains}} an overview over locally <b>stationary</b> <b>processes.</b> At the beginning time varying autoregressive processes {{are discussed in}} detail - both as as a deep example and an important class of locally <b>stationary</b> <b>processes.</b> In the next section a general framework for time series with time varying finite dimensional parameters is discussed with special emphasis on nonlinear locally <b>stationary</b> <b>processes.</b> Then the paper focusses on linear processes where a more general theory is possible. First a general definition for linear processes is given and time varying spectral densities are discussed in detail. Then the Gaussian likelihood theory is presented for locally <b>stationary</b> <b>processes.</b> In the next section the relevance of empirical spectral <b>processes</b> for locally <b>stationary</b> time series is discussed. Empirical spectral processes {{play a major role}} in proving theoretical results and provide a deeper understanding of many techniques. The article concludes with an overview of other results for locally <b>stationary</b> <b>processes.</b> Comment: 65 pages, 4 Figures, 1 Tabl...|$|R
40|$|Algorithms for {{determining}} the generating function, the prediction error matrix, and the best linear predictor for non-full-rank bivariate <b>stationary</b> stochastic <b>processes</b> are obtained. generating function prediction error matrix best linear predictor bivariate <b>stationary</b> <b>processes</b> non-full-rank processes...|$|R
2500|$|In Niels Bohr's mature view, quantum {{mechanical}} phenomena {{are required}} to be experiments, with complete descriptions of all the devices for the system, preparative, intermediary, and finally measuring. The descriptions are in macroscopic terms, expressed in ordinary language, supplemented with the concepts of classical mechanics. The initial condition and the final condition of the system are respectively described by values in a configuration space, for example a position space, or some equivalent space such as a momentum space. Quantum mechanics does not admit a completely precise description, {{in terms of both}} position and momentum, of an initial condition or [...] "state" [...] (in the classical sense of the word) that would support a precisely deterministic and causal prediction of a final condition. In this sense, advocated by Bohr in his mature writings, a quantum phenomenon is a process, a passage from initial to final condition, not an instantaneous [...] "state" [...] in the classical sense of that word. Thus {{there are two kinds of}} processes in quantum mechanics: stationary and transitional. For a <b>stationary</b> <b>process,</b> the initial and final condition are the same. For a transition, they are different. Obviously by definition, if only the initial condition is given, the process is not determined. Given its initial condition, prediction of its final condition is possible, causally but only probabilistically, because the Schrödinger equation is deterministic for wave function evolution, but the wave function describes the system only probabilistically.|$|E
5000|$|In {{mathematics}} and statistics, a <b>stationary</b> <b>process</b> ( [...] a strict(ly) <b>stationary</b> <b>process</b> or strong(ly) <b>stationary</b> <b>process)</b> is a stochastic process whose joint probability distribution {{does not change}} when shifted in time. Consequently, parameters such as mean and variance, if they are present, also do not change over time.|$|E
5000|$|Suppose [...] be a weakly {{stationary}} (2nd-order <b>stationary)</b> <b>process</b> with mean [...] and variance [...] The Autocorrelation Function (ACF) of lag [...] {{is given}} by Definition:A weakly <b>stationary</b> <b>process</b> {{is said to be}} [...] "Long-Range-Dependence" [...] if ...|$|E
50|$|Unit root <b>{{processes}},</b> trend <b>stationary</b> <b>processes,</b> autoregressive processes, {{and moving}} average processes are specific forms of processes with autocorrelation.|$|R
40|$|For the {{variance}} of stationary renewal and alternating renewal processes Nn(Â·) the paper establishes {{upper and lower bounds}} of the form -B 1 [less-than-or-equals, slant]varN 8 (0,x-A[lambda]x[less-than-or-equals, slant]B 2 (0 <b>Stationary</b> renewal <b>process</b> <b>stationary</b> alternating renewal <b>process</b> <b>stationary</b> point <b>process</b> variance function Palm-Khinchin equations variance function bounds...|$|R
40|$|We {{consider}} the limit distribution of maxima of periodograms for <b>stationary</b> <b>processes.</b> Our method {{is based on}} $m$-dependent approximation for <b>stationary</b> <b>processes</b> and a moderate deviation result. Comment: The constant A_n in Lemma 4. 2 on page 23 (Supplementary Material) is corrected. Other context remains the sam...|$|R
50|$|Stationarity is the {{property}} of a random process which guarantees that its statistical properties, such as the mean value, its moments and variance, will not change over time. A <b>stationary</b> <b>process</b> is one whose probability distribution is the same at all times. For more information see <b>stationary</b> <b>process.</b>|$|E
5000|$|If X(t) is a weakly <b>stationary</b> <b>process,</b> {{then the}} {{following}} are true: ...|$|E
5000|$|... the {{assumption}} of a <b>stationary</b> <b>process,</b> yielding volatility risk, which can be hedged with volatility hedging; ...|$|E
40|$|<b>Stationary</b> <b>processes</b> {{have been}} {{extensively}} {{studied in the}} literature. Their applications include modeling and forecasting numerous real life phenomena such as natural disasters, sales and market movements. When <b>stationary</b> <b>processes</b> are considered, modeling is traditionally based on fitting an autoregressive moving average (ARMA) process. However, we challenge this conventional approach. Instead of fitting an ARMA model, we apply an AR(1) characterization in modeling any strictly <b>stationary</b> <b>processes.</b> Moreover, we derive consistent and asymptotically normal estimators of the corresponding model parameter. Comment: Published at [URL] in the Modern Stochastics: Theory and Applications ([URL] by VTeX ([URL]...|$|R
40|$|We derive {{spectral}} {{necessary and}} sufficient conditions for <b>stationary</b> symmetric stable <b>processes</b> to be metrically transitive and mixing. We then consider some important classes of stationary stable processes: Sub-Gaussian <b>stationary</b> <b>processes</b> and <b>stationary</b> stable <b>processes</b> with a harmonic spectral representation are never metrically transitive, {{the latter in}} sharp contrast with the Gaussian case. Stable processes with a harmonic spectral representation satisfy a strong law of large numbers {{even though they are}} not generally stationary. For doubly <b>stationary</b> stable <b>processes,</b> sufficient conditions are derived for metric transitivity and mixing, and necessary and sufficient conditions for a strong law of large numbers. stable <b>processes</b> ergodic theory <b>stationary</b> <b>processes</b> spectral representations...|$|R
40|$|We {{prove the}} compact {{law of the}} {{iterated}} logarithm for stationary and ergodic differences of (reverse or not) martingales taking values in a separable $ 2 $-smooth Banach space (for instance a Hilbert space). Then, in the martingale case, the almost sure invariance principle is derived from a result of Berger. From those results, we deduce the almost sure invariance principle for <b>stationary</b> <b>processes</b> under the Hannan condition and the compact law of the iterated logarithm for <b>stationary</b> <b>processes</b> arising from non-invertible dynamical systems. Those results for <b>stationary</b> <b>processes</b> are new, even in the real valued case. We also obtain the Marcinkiewicz-Zygmund strong law of large numbers for <b>stationary</b> <b>processes</b> with values in some smooth Banach spaces. Applications to several situations are given. Comment: Published at [URL] in the Bernoulli ([URL] by the International Statistical Institute/Bernoulli Society ([URL]...|$|R
5000|$|... is a <b>stationary</b> <b>process,</b> where [...] is the lag {{operator}} and [...] {{is the first}} difference, i.e.|$|E
5000|$|The {{probability}} that the <b>stationary</b> <b>process</b> is in state i (contains i customers, including those in service) is ...|$|E
5000|$|... {{exist and}} are equal for any <b>stationary</b> <b>process</b> {{including}} the stationary ergodic process X. Denote it as H.|$|E
40|$|Weakly and {{strongly}} consistent nonparametric estimates, along with rates of convergence, are {{established for the}} spectral density of certain <b>stationary</b> stable <b>processes.</b> This spectral density plays a role, in linear inference problems, analogous to that played by the usual power spectral density of second order <b>stationary</b> <b>processes.</b> <b>stationary</b> stable <b>processes</b> nonparametric spectral density estimation...|$|R
40|$|In {{this paper}} we {{investigate}} an optimal {{property of the}} maximum likelihood estimator of Gaussian locally <b>stationary</b> <b>processes</b> by the second order approximation. In the case where the model is correctly specified, it is shown that appropriate modifications of the maximum likelihood estimator for Gaussian locally <b>stationary</b> <b>processes</b> is second order asymptotically efficient. We discuss second order robustness properties...|$|R
40|$|The paper {{identifies}} {{the class of}} <b>stationary</b> <b>processes</b> on an interval which share a given <b>stationary</b> Gaussian <b>process</b> as kth derivative. The membership requirement involves the norm in the reproducing kernel space associated with the process sought as derivative. Some explicit results are obtained when working with the Ornstein-Uhlenbeck and the linear kernels. These latter facts are useful in an adaptive Bayesian modelling of computer experiments; some remarks are given about this type of analysis. <b>stationary</b> <b>processes</b> the Ornstein-Uhlenbeck process Bayesian modelling smoothing...|$|R
