20|10|Public
40|$|There {{is a need}} to empirically {{validate}} theoretical Machine Learning research. This can {{be difficult}} because the algorithms being tested often require large amounts of training data. Softbots provide a unique solution to this problem. Softbots or software robots are intelligent agents that interact with software environments. By attaching the agent directly to the training environment, it is possible for softbots to gather their own training data. St. Bernard is a softbot that locates files within the UNIX 1 file system. It is based on the theory of <b>satisficing</b> <b>search.</b> The training data used by <b>satisficing</b> <b>search</b> is gathered directly from its environment. St. Bernard demonstrates the usefulness of <b>satisficing</b> <b>search</b> and addresses some of the practical issues of implementing this algorithm. 1 UNIX is a trademark of AT&T Bell Labs. 1 Introduction There is always the danger when doing theoretical Machine Learning research that the results will not prove to be useful in the real worl [...] ...|$|E
40|$|Recently, several {{researchers}} have found that cost-based <b>satisficing</b> <b>search</b> with A* often runs into problems. Although some "work arounds" have been proposed to ameliorate the problem, there has not been any concerted effort to pinpoint its origin. In this paper, we argue that the origins {{can be traced back to}} the wide variance in action costs that is observed in most planning domains. We show that such cost variance misleads A* search, and that this is no trifling detail or accidental phenomenon, but a systemic weakness of the very concept of "cost-based evaluation functions + systematic search + combinatorial graphs". We show that <b>satisficing</b> <b>search</b> with sized-based evaluation functions is largely immune to this problem. Comment: Longer version of an extended abstract from SOCS 201...|$|E
40|$|<b>Satisficing</b> <b>search</b> {{algorithms}} {{are proposed}} for adaptively selecting near-best basis and near-best frame decompositions in redundant tree-structured wavelet transforms. Any {{of a variety}} of additive or non-additive information cost functions can be used as the decision criterion for comparing and selecting nodes when searching through the tree. The algorithms are applicable to tree-structured transforms generated by any kind of wavelet whether orthogonal, biorthogonal, or non-orthogonal. These <b>satisficing</b> <b>search</b> algorithms implement sub-optimizing rather than optimizing principles, and acquire the important advantage of reduced computational complexity with significant savings in memory, flops, and time. Despite the sub-optimal approach, top-down tree-search algorithms with additive or non-additive costs that yield near-best bases can be considered, in certain important and practical situations, better than bottom-up tree-search algorithms with additive costs that yield best bases. Here [...] ...|$|E
40|$|Based on the CanMEDS {{framework}} and the European Training Charter for Clinical Radiology a new radiology curriculum was {{designed in the}} Netherlands. Both the development process and the resulting new curriculum are presented in this paper. The new curriculum was developed according to four systematic design principles: discursiveness, hierarchical decomposition, systematic variation and satisficing (satisficing is different from satisfying; in this context, <b>satisficing</b> means <b>searching</b> for an acceptable solution instead of searching for an optimal solution). The new curriculum is organ based with integration of radiological diagnostic techniques, comprises a uniform national common trunk followed by a 2 -year subspecialisation, is competency outcome based with appropriate assessment tools and techniques, {{and is based on}} regional collaboration among radiology departments. The application of the systematic design principles proved successful in producing a new curriculum approved by all authorities. The principles led to a structured, yet flexible, development process in which creative solutions could be generated and adopters (programme directors, supervisors and residents) were highly involved. Further research is needed to empirically test the components of the new curriculum...|$|R
40|$|Abstract. AI Planning is {{concerned}} with the selection of actions towards achieving a goal. Research on cellular automata (CA) is con-cerned with the question how global behaviours arise from local up-dating rules relating a cell to its direct neighbours. While these two areas are disparate at first glance, we herein identify a problem that is interesting to both: How to reach a fixed point in an asynchronous CA where cells are updated one-by-one? Considering a particular local updating rule, we encode this problem into PDDL and show that the resulting benchmark is an interesting challenge for AI Planning. For example, our experiments determine that, very atypically, an optimal SAT-based planner outperforms state-of-the-art <b>satisficing</b> heuristic <b>search</b> planners. This points to a severe weakness of current heuris-tics because, as we prove herein, plans for this problem can always be constructed in time linear {{in the size of the}} automaton. Our proof of this starts from a high-level argument and then relies on using a planner for flexible case enumeration within localised parts of the ar-gument. Besides the formal result itself, this establishes a new proof technique for CAs and thus demonstrates that the potential benefit of research crossing the two fields is mutual. ...|$|R
40|$|The {{theory of}} bounded {{rationality}} takes {{into consideration the}} cognitive limitations of decision makers in accomplishing their goals and emphasizes on <b>satisficing</b> behaviors when <b>searching</b> for solutions. One {{of the hallmarks of}} rational behaviors in decision-making process is embodied in how to cope with inconsistency. Building cognitive computing systems for real world applications amounts to developing systems that possess bounded rationality. In this paper, we examine how bounded rationality is exhibited in the human cognitive capabilities in handling inconsistency and propose an initial theory on how to incorporate those capabilities into cognitive computing systems. In particular, we focus our attention on two important phenomena: rational-but-inconsistent circumstances, and irrational-but-consistent circumstances. The main contribution of the work {{lies in the fact that}} we shed some new light on the interplay between bounded rationality and inconsistency. 9 page(s...|$|R
40|$|Many {{real world}} {{planning}} problems require goals with deadlines and durative actions that consume resources. In this paper, we present Sapa, a domain-independent heuristic forward chaining planner {{that can handle}} durative actions, metric resource constraints, and deadline goals. The main innovation of Sapa is the set of distance based heuristics it employs to control its search. We consider both optimizing and <b>satisficing</b> <b>search.</b> For the former, we identify admissible heuristics for objective functions based on makespan and slack. For <b>satisficing</b> <b>search,</b> our heuristics are aimed at scalability with reasonable plan quality. Our heuristics are derived from the "relaxed temporal planning graph" structure, which is a generalization of planning graphs to temporal domains. We also provide techniques for adjusting the heuristic values to account for resource constraints. Our experimental results indicate that Sapa returns good quality solutions for complex planning problems in reasonable time...|$|E
40|$|We empirically examine {{several ways}} of {{exploiting}} the information of multiple heuristics in a satisficing best-first search algorithm, comparing their performance {{in terms of}} coverage, plan quality, speed, and search guidance. Our results indicate that using multiple heuristics for <b>satisficing</b> <b>search</b> is indeed useful. Among the combination methods we consider, the best results are obtained by the alternation method of the “Fast Diagonally Downward ” planner. runtim...|$|E
40|$|We present a. {{theory of}} a modeler’s problem de-composition {{skills in the}} context of opthal Tea-sowing- the use of {{qualitative}} modeling to strategically guide numerical explorations of ob-jective space. Our technique, called activity annl-ysl:s, applies to the pervasive family of linear and non-linear, constrained optimization problems, and easily integrates with any existing numeri-cal approach. Activity analysis draws from the power of two seemingly divergent perspectives-the global conflict-based approaches of combina-torial <b>satisficing</b> <b>search,</b> and the local gradient-based approaches of continuous optimization-combined with the underlying insights of engi...|$|E
40|$|The Author(s) 2012. This {{article is}} {{published}} with open access at Springerlink. com Abstract The {{purpose of this}} study is threefold: (1) to develop an automated, computer-based method to detect heuristics and biases as pathologists examine virtual slide cases, (2) to measure the frequency and distribution of heuristics and errors across three levels of training, and (3) to examine relationships of heuristics to biases, and biases to diagnostic errors. The authors conducted the study using a computer-based system to view and diagnose virtual slide cases. The software recorded participant responses throughout the diagnostic process, and automatically classified participant actions based on definitions of eight common heuristics and/or biases. The authors measured frequency of heuristic use and bias across three levels of training. Biases studied were detected at varying frequen-cies, with availability and <b>search</b> <b>satisficing</b> observed most frequently. There were few significant differences by level of training. For representativeness and anchoring, the heuristic was used appropriately as often or more often than it was used in biased judg-ment. Approximately half of the diagnostic errors were associated with one or more biases. We conclude that heuristic use and biases were observed among physicians at all levels o...|$|R
40|$|Simon {{proposed}} that human rationality is bounded by both internal (mental) and external (environmental) constraints. Traditionally, these constraints {{have been seen}} as independent, leading to a notion of bounded rationality that is either the attempt to do as well as possible given the demands of the world – the notion of optimization under constraints – or as the suboptimal outcome of the limited cognitive system – the realm of cognitive illusions. But there is a third possibility, following Simon's original conception: rather than being unrelated, the two sets of bounds may fit together like the blades in a pair of scissors. The mind can take advantage of this fit to make good decisions, by using mental mechanisms whose internal structure exploits the external information structures available in the environment. In this paper we lay out a research program for studying simple decision heuristics of this sort that expands on Simon's own search for mechanisms of bounded rationality. We then illustrate how these heuristics can make accurate decisions in appropriate environments, and present detailed examples of two heuristics inspired by Simon's ideas on recognition-based processing and <b>satisficing</b> in sequential <b>search...</b>|$|R
40|$|Review A {{coherent}} {{alternative to}} an economic approach of corporate governance is missing. In this paper we take steps towards developing a behavioral theory of boards and corporate governance. Building upon {{concepts such as}} political bargaining, routinization of decision making, <b>satisficing,</b> and problemistic <b>search,</b> a behavioral theory of boards and corporate governance will focus more on (1) interactions and processes {{inside and outside the}} boardroom; (2) the fact that decision making is made by coalitions of actors and objectives are results of political bargaining; and (3) the notion that not only conflicting, but also cooperating, interests are parts of the boards' decision making and control over firm resources. The consequences are a new research agenda for boards and corporate governance. The agenda will focus on actual instead of stylized descriptions of board behavior. In a behavioral perspective the emphasis on problems of coordination, exploration, and knowledge creation may dominate over problems of conflict of interest, exploitation, and the distribution of value. A future research agenda based on a behavioral framework calls for novel and adventurous research designs. A behavioral theory of boards and corporate governance will be closer to actual board behavior than the traditional economic approach and research about boards and corporate governance may thus become more actionable for practitioners...|$|R
40|$|The {{problem of}} {{effectively}} combining multiple heuristic estimators {{has been studied}} extensively {{in the context of}} optimal planning, but not in the context of satisficing planning. To narrow this gap, we empirically examine several ways of exploiting the information of multiple heuristics in a satisficing best-first search algorithm, comparing their performance in terms of coverage, plan quality and runtime. Our empirical results indicate that using multiple heuristics for <b>satisficing</b> <b>search</b> is indeed useful and that the best results are not obtained by the most obvious combination methods...|$|E
40|$|We {{present a}} theory of a modeler’s problem {{decomposition}} skills {{in the context of}} optimal reasonzng — the use of qualitative modeling to strategically guide numerical explorations of objective space. Our technique, called activity analysis, applies to the pervasive family of linear and non-linear, constrained optimization problems, and easily integrates with any existing numerical approach. Activity analysis draws from the power of two seemingly divergent perspectives — the global conflict-based approaches of combinatorial <b>satisficing</b> <b>search,</b> and the local gradientbased approaches of continuous optimization — combined with the underlying insights of engineerin...|$|E
40|$|Herbert Simon {{introduced}} {{the notion of}} satisficing to explain how boundedly rational agents might approach difficult sequential decision problems. His satisficing decision makers were offered {{as an alternative to}} opti-mizers, who have impressive computational capacities which allow them to maximize. There is no reason, however, why satisficers can not do their task optimally. In this paper, we present a simplified sequential search problem for a satisficing decision maker, and show how to compute its optimal <b>satisficing</b> <b>search</b> policies. Our findings demonstrate that satisficing, when done properly, can be a quite effective search policy. Key words: satisficing; sequential search; dynamic programming 1...|$|E
40|$|The {{purpose of}} this study is threefold: (1) to develop an automated, {{computer-based}} method to detect heuristics and biases as pathologists examine virtual slide cases, (2) to measure the frequency and distribution of heuristics and errors across three levels of training, and (3) to examine relationships of heuristics to biases, and biases to diagnostic errors. The authors conducted the study using a computer-based system to view and diagnose virtual slide cases. The software recorded participant responses throughout the diagnostic process, and automatically classified participant actions based on definitions of eight common heuristics and/or biases. The authors measured frequency of heuristic use and bias across three levels of training. Biases studied were detected at varying frequencies, with availability and <b>search</b> <b>satisficing</b> observed most frequently. There were few significant differences by level of training. For representativeness and anchoring, the heuristic was used appropriately as often or more often than it was used in biased judgment. Approximately half of the diagnostic errors were associated with one or more biases. We conclude that heuristic use and biases were observed among physicians at all levels of training using the virtual slide system, although their frequencies varied. The system can be employed to detect heuristic use and to test methods for decreasing diagnostic errors resulting from cognitive biases. © 2012 The Author(s) ...|$|R
40|$|Purpose – The {{purpose of}} this paper is to gain insight into managers' {{decision-making}} practices when challenged by inappropriate information quality, and to test frameworks developed from research to see whether they apply to these managers. Design/methodology/approach – This exploratory, multiple case study used the critical incident technique in 19 semi-structured interviews. Responses were analyzed using framework analysis, a matrix-based content analysis technique, and then considered with respect to the research literature on information overload, information poverty and satisficing. Findings – The managers in this study tended to <b>satisfice</b> (terminate the <b>search</b> process and make a good enough decision, while recognizing that information gaps remain). Those challenged by too little information appear to fit descriptions of information poverty, while others described aspects of information overload. Research limitations/implications – A shortage of information behavior research on managers makes it difficult to conclude whether these results are typical of managers in general or of healthcare services managers specifically. Further research is needed to confirm initial findings and address questions suggested by this paper. Practical implications – This paper suggests that existing definitions for the concepts of information poverty and information overload can be used to describe managers' experiences. Originality/value – This paper contributes to what is known about information behavior in managers in general and healthcare services managers specifically. It may serve as an example of how to consider new research findings within existing frameworks. ...|$|R
5000|$|It is {{commonly}} believed that good information management {{is crucial to}} the smooth working of organisations, and although there is no commonly accepted theory of information management per se, behavioural and organisational theories help. Following the behavioural science theory of management, mainly developed at Carnegie Mellon University and prominently supported by March and Simon, most of what goes on in modern organizations is actually information handling and decision making. One crucial factor in information handling and decision making is an individual's ability to process information and to make decisions under limitations that might derive from the context: a person's age, the situational complexity, or a lack of requisite quality in the information that is at hand - all of which is exacerbated by the rapid advance of technology and the new kinds of system that it enables, especially as the social web emerges as a phenomenon that business cannot ignore. And yet, well before there was any general recognition of the importance of information management in organisations, March and Simon [...] argued that organizations have to be considered as cooperative systems, {{with a high level of}} information processing and a vast need for decision making at various levels. Instead of using the model of the [...] "economic man", as advocated in classical theory [...] they proposed [...] "administrative man" [...] as an alternative, based on their argumentation about the cognitive limits of rationality. Additionally they proposed the notion of <b>satisficing,</b> which entails <b>searching</b> through the available alternatives until an acceptability threshold is met - another idea that still has currency.|$|R
40|$|We {{present a}} theory of a modeler's problem {{decomposition}} skills {{in the context of}} optimal reasoning [...] - the use of qualitative modeling to strategically guide numerical explorations of objective space. Our technique, called activity analysis, applies to the pervasive family of linear and non-linear, constrained optimization problems, and easily integrates with any existing numerical approach. Activity analysis draws from the power of two seemingly divergent perspectives [...] the global conflict-based approaches of combinatorial <b>satisficing</b> <b>search,</b> and the local gradientbased approaches of continuous optimization [...] combined with the underlying insights of engineering monotonicity analysis. The result is an approach that strategically cuts away subspaces that it can quickly rule out as suboptimal, and then guides the numerical methods to the remaining subspaces. Introduction and Example Our goal is to capture a modeler's tacit skill at decomposing physical models and its application to [...] ...|$|E
40|$|Bounded {{rationality}} and, more specifically, satisficing in game playing assumes choosing strategies by anticipating their likely consequences. Unlike orthodox game theory, {{one does}} not require optimality and rational expectations but views satisficing as a reasoning process with several possible feedback loops. The various stages of such reasoning ask players to• mentally represent the game, typically via simplifying (mental modeling),• generate scenarios, i. e., point expectations concerning others’ choices and chance events (scenario generation),• form payoff aspirations for all scenarios (aspiration formation),• try to satisfy them by successively testing choice alternatives (<b>satisficing</b> <b>search).</b> When repeating this process, players may revise their mental representation, adapt their scenario set and aspiration profile or drop the first, the two first, or {{all three of these}} stages before exploring further strategies. Such satisficing in game playing has been confirmed experimentally by directly observing scenario generation, aspiration formation, and search for satisficing alternatives...|$|E
40|$|Recently, several {{researchers}} have found that cost-based <b>satisficing</b> <b>search</b> with A ∗ often runs into problems. Although some “work arounds ” have been proposed to ameliorate the problem, there has not been any concerted effort to pinpoint its origin. In this paper, we argue that the origins {{can be traced back to}} the wide variance in action costs that is easily observed in planning domains. We show that such cost variance misleads A ∗ search, and that this is a systemic weakness of the very concept: “cost-based evaluation functions + systematic search + combinatorial graphs”. We argue that purely size-based evaluation functions are a reasonable default, as these are trivially immune to cost-induced difficulties. We further show that cost-sensitive versions of size-based evaluation function — where the heuristic estimates the size of cheap paths provides attractive quality vs. speed tradeoffs. ...|$|E
40|$|A <b>satisficing</b> <b>search</b> problem {{consists}} {{of a set of}} probabilistic experiments to be performed in some order, seeking a satisfying configuration of successes and failures. The expected cost of the search depends both on the success probabilities of the individual experiments, and on the search strategy, which specifies the order in which the experiments are to be performed. A strategy that minimizes the expected cost is optimal. Earlier work has provided "optimizing functions" that compute optimal strategies for certain classes of search problems from the success probabilities of the individual experiments. We extend those results by providing a general model of such strategies, and an algorithm pao that identifies an approximately optimal strategy when the probability values are not known. The algorithm first estimates the relevant probabilities from a number of trials of each undetermined experiment, and then uses these estimates, and the proper optimizing function, to identify a stra [...] ...|$|E
40|$|A <b>satisficing</b> <b>search</b> problem {{consists}} {{of a set of}} probabilistic experiments to be performed in some order, without repetitions, until a satisfying configuration of successes and failures has been reached. The cost of performing the experiments depends on the order chosen. Earlier work has concentrated on finding optimal search strategies in special cases of this model, such as search trees and and-or graphs, when the cost function and the success probabilities for the experiments are given. In contrast, we study the complexity of "learning" an approximately optimal search strategy when some of the success probabilities are not known at the outset. Working in the fully general model, we show that if n is the number of unknown probabilities, and C is the maximum cost of performing all the experiments, then 2 (nC ffl) 2 ln 2 n ffi trials of each undetermined experiment are sufficient to identify, with confidence 1 Γ ffi, a search strategy whose cost is within ffl of the optima [...] ...|$|E
40|$|Most of the {{research}} conducted {{in the area of}} deductive learning is experimental. However, many of the experiments reported are far from being systematic and thorough. There are many parameters that are embedded in the system's architecture and {{it is not clear how}} they effect the utility of the learned knowledge. In this paper we describe an attempt to perform systematic experiments in the domain of deductive learning. The part described here explores how the search strategy employed during problem solving and during learning effects the utility of learning process. It was concluded that the utility of deductive learning is negative when the learned knowledge is applied in optimizing search procedure during problem solving, but becomes positive in <b>satisficing</b> <b>search.</b> It was also concluded that for off-line learning it is more beneficial to use optimizing search during the learning process. Knowledge acquired in this method improves both the efficiency and the quality of the problem sol [...] ...|$|E
40|$|Recently, several {{researchers}} have found that cost-based <b>satisficing</b> <b>search</b> with A * often runs into problems. Although some “work arounds ” have been proposed to ameliorate the problem, there has been little concerted effort to pinpoint its origin. In this paper, we argue that the origins of this problem {{can be traced back to}} the fact that most planners that try to optimize cost also use cost-based evaluation functions (i. e., f(n) is a cost estimate). We show that cost-based evaluation functions become ill-behaved whenever there is a wide variance in action costs; something that is all too common in planning domains. The general solution to this malady is what we call a surrogate search, where a surrogate evaluation function that doesn’t directly track the cost objective, and is resistant to cost-variance, is used. We will discuss some compelling choices for surrogate evaluation functions that are based on size rather than cost. Of particular practical interest is a cost-sensitive version of size-based evaluation function where the heuristic estimates the size of cheap paths, as it provides attractive quality vs. speed tradeoffs. ...|$|E
40|$|Pipelined filter {{ordering}} is {{a central}} problem in database query optimization. The problem {{is to determine the}} optimal order in which to apply a given set of commutative filters (predicates) to a set of elements (the tuples of a relation), so as to find, as efficiently as possible, the tuples that satisfy all of the filters. Optimization of pipelined filter ordering has recently received renewed attention in the context of environments such as the web, continuous high-speed data streams, and sensor networks. Pipelined filter ordering problems are also studied in areas such as fault detection and machine learning under names such as learning with attribute costs, minimumsum set cover, and <b>satisficing</b> <b>search.</b> We present algorithms for two natural extensions of the classical pipelined filter ordering problem: (1) a distributional type problem where the filters run in parallel and the goal is to maximize throughput, and (2) an adversarial type problem where the goal is to minimize the expected value of multiplicative regret. We present two related algorithms for solving (1), both running in time O(n²), which improve on the O(n³ log n) algorithm of Kodialam. We use techniques from our algorithms for (1) to obtain an algorithm for (2) ...|$|E
40|$|AbstractA <b>satisficing</b> <b>search</b> problem {{consists}} {{of a set of}} probabilistic experiments to be performed in some order, seeking a satisfying configuration of successes and failures. The expected cost of the search depends both on the success probabilities of the individual experiments, and on the search strategy, which specifies the order in which the experiments are to be performed. A strategy that minimizes the expected cost is optimal. Earlier work has provided “optimizing functions” that compute optimal strategies for certain classes of search problems from the success probabilities of the individual experiments. We extend those results by providing a general model of such strategies, and an algorithm pao that identifies an approximately optimal strategy when the probability values are not known. The algorithm first estimates the relevant probabilities from a number of trials of each undetermined experiment, and then uses these estimates, and the proper optimizing function, to identify a strategy whose cost is, with high probability, close to optimal. We also show that if the search problem can be formulated as an and-or tree, then the pao algorithm can also “learn while doing”, i. e. gather the necessary statistics while performing the search...|$|E
40|$|Recently, several {{researchers}} have found that cost-based <b>satisficing</b> <b>search</b> with A* often runs into problems. Although some "work arounds" have been proposed to ameliorate the problem, there has been little concerted effort to pinpoint its origin. In this paper, we argue that the origins of this problem {{can be traced back to}} the fact that most planners that try to optimize cost also use cost-based evaluation functions (i. e., f(n) is a cost estimate). We show that cost-based evaluation functions become ill-behaved whenever there is a wide variance in action costs; something that is all too common in planning domains. The general solution to this malady is what we call a surrogatesearch, where a surrogate evaluation function that doesn't directly track the cost objective, and is resistant to cost-variance, is used. We will discuss some compelling choices for surrogate evaluation functions that are based on size rather that cost. Of particular practical interest is a cost-sensitive version of size-based evaluation function [...] where the heuristic estimates the size of cheap paths, as it provides attractive quality vs. speed tradeoffsComment: arXiv admin note: substantial text overlap with arXiv: 1103. 368...|$|E
40|$|Abstract. Consider {{the problem}} of {{learning}} how long {{to wait for a}} bus before walking, experimenting each day and assuming that the bus arrival times are independent and identically distributed random variables with an unknown distribution. Similar uncertain optimal stopping problems arise when devising power-saving strategies, e. g., learning the optimal disk spin-down time for mobile computers, or speeding up certain types of <b>satisficing</b> <b>search</b> procedures by switching from a potentially fast search method that is unreliable, to one that is reliable, but slower. Formally, the problem can be described as a repeated game. In each round of the game an agent is waiting for an event to occur. If the event occurs while the agent is waiting, the agent suffers a loss that is the sum of the event’s “arrival time ” and some fixed loss. If the agents decides to give up waiting before the event occurs, he suffers a loss that is the sum of the waiting time and some other fixed loss. It is assumed that the arrival times are independent random quantities with the same distribution, which is unknown, while the agent knows the loss associated with each outcome. Two versions of the game are considered. In the full information case the agent observes the arrival times regardless of its actions, while in the partial information case the arrival time is observed only if it does not exceed the waiting time. After some general structural observations about the problem, we present a number of algorithms for both cases that learn the optimal weighting time with nearly matching minimax upper and lower bounds on their regret. ...|$|E

