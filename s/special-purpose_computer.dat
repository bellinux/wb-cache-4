96|56|Public
2500|$|The four color theorem was proved in 1976 by Kenneth Appel and Wolfgang Haken. It was {{the first}} major theorem to be proved using a computer. Appel and Haken's {{approach}} started by showing that there is a particular set of 1,936 maps, each of which cannot be part of a smallest-sized counterexample to the four color theorem. (If they did appear, there would be a smaller counterexample.) Appel and Haken used a <b>special-purpose</b> <b>computer</b> program to confirm that each of these maps had this property. Additionally, any map that could potentially be a counterexample must have a portion that looks like one of these 1,936 maps. Showing this required hundreds of pages of hand analysis. Appel and Haken concluded that no smallest counterexamples exist because any must contain, yet do not contain, one of these 1,936 maps. This contradiction means there are no counterexamples at all and that the theorem is therefore true. Initially, their proof was not accepted by all mathematicians because the computer-assisted proof was infeasible for a human to check by hand [...] Since then the proof has gained wider acceptance, although doubts remain [...]|$|E
50|$|Ray-tracing {{hardware}} is <b>special-purpose</b> <b>computer</b> hardware {{designed for}} accelerating ray tracing calculations.|$|E
50|$|A graph {{reduction}} machine is a <b>special-purpose</b> <b>computer</b> built to perform combinator calculations by {{graph reduction}}.|$|E
50|$|Most {{computer}} peripherals {{are themselves}} <b>special-purpose</b> <b>computers.</b> Devices such as printers, scanners, cameras and USB flash drives have internally stored firmware; some devices may also permit field upgrading of their firmware.|$|R
40|$|Transformation {{technique}} enables inefficient {{expert systems}} {{to run in}} real time. Paper suggests use of knowledge compiler to transform knowledge base and inference mechanism of expert-system computer program into conventional computer program. Main benefit, faster execution and reduced processing demands. In avionic systems, transformation reduces need for <b>special-purpose</b> <b>computers...</b>|$|R
40|$|A general {{framework}} for special-purpose computing! Special-purpose computational instruments will play an increasing {{role in the}} practice of science and engineering. Although general-purpose supercomputers are becoming more available, there are significant applications for which it is appropriate to construct special-purpose dedicated computing engines. The Supercomputer Toolkit is intended to make the construction and programming of such <b>special-purpose</b> <b>computers</b> routine and inexpensive, in some cases even automatic. The Toolkit is a family of hardware modules (processors, memory, interconnect, and input-output devices) and a collection of software modules (compilers, simulators, scientific libraries, and high-level front ends) from which high-performance <b>special-purpose</b> <b>computers</b> can be easily configured and programmed. The hardware modules are intended to be standard, reusable parts. These are combined by means of a user-reconfigurable, static interconnect technology. The Toolkit's software support, based o...|$|R
50|$|Video cameras can be {{fed into}} {{standard}} or <b>special-purpose</b> <b>computer</b> monitors, {{and the image}} can be zoomed in and magnified. These systems often include a movable table to move the written material.|$|E
50|$|A {{jump server}} or jump host or jumpbox or secure {{administrative}} host is a (<b>special-purpose)</b> <b>computer</b> {{on a network}} typically used to manage devices in a separate security zone. The most common example is managing a host in a DMZ from trusted networks or computers.|$|E
5000|$|Rainer Spurzem is {{a leader}} of the GRACE project, which uses {{reconfigurable}} hardware for astrophysical particle simulations. The GRACE project was funded via grants from the Volkswagen Foundation and from the Ministry of Science, Research and Art of Baden-WÃ¼rttemberg. He also designed gravitySimulator, a <b>special-purpose</b> <b>computer</b> based on GRAPE accelerator boards at the Rochester Institute of Technology.|$|E
40|$|Abstract. Recently, <b>special-purpose</b> <b>{{computers}}</b> have surpassed generalpurpose {{computers in}} {{the speed with which}} large-scale stellar dynamics simulations can be performed. Speeds up to a Teraflops are now available, for simulations in a variety of fields, such as planetary formation, star cluster dynamics, galactic nuclei, galaxy interactions, galaxy formation, large scale structure, and gravitational lensing. Future speed increases for <b>special-purpose</b> <b>computers</b> will be even more dramatic: a Petaflops version, tentatively named the GRAPE- 6, could be built within a few years, whereas general-purpose computers are expected to reach this speed somewhere in the 2010 - 2015 time frame. Boards with a handful of chips from such a machine could be made available to individual astronomers. Such a board, attached to a fast workstation, will then deliver Teraflops speeds on a desktop, around the year 2000. 1...|$|R
40|$|Molecular {{dynamics}} (MD) simulation {{is one of}} {{the most}} exact and powerful method to study protein biophysics and biochemistry. However, its use is still limited by its high requirement of computational power. To solve this problem, several <b>special-purpose</b> <b>computers</b> have been built. The most successful machines are GRAPE family [1], GRAPE- 2 A, MD-GRAPE, and MDM. Especially, MDM [2] with 7...|$|R
40|$|A {{three part}} survey {{is made of}} the {{state-of-the-art}} in digital filtering. Part one presents background material including sampled data transformations and the discrete Fourier transform. Part two, digital filter theory, gives an in-depth coverage of filter categories, transfer function synthesis, quantization and other nonlinear errors, filter structures and computer aided design. Part three presents hardware mechanization techniques. Implementations by general purpose, mini-, and <b>special-purpose</b> <b>computers</b> are presented...|$|R
5000|$|Embedded {{systems and}} other <b>special-purpose</b> <b>computer</b> systems that require very fast and/or very {{consistent}} response times may opt {{not to use}} virtual memory due to decreased determinism; virtual memory systems trigger unpredictable traps that may produce unwanted [...] "jitter" [...] during I/O operations. This is because embedded hardware costs are often kept low by implementing all such operations with software (a technique called bit-banging) rather than with dedicated hardware.|$|E
50|$|Douglas is {{best known}} for the {{development}} of matrix models (the first nonperturbative formulations of string theory), for his work on Dirichlet branes and on noncommutative geometry in string theory, and {{for the development of the}} statistical approach to string phenomenology. He was on the team (led by Gerald J. Sussman) that built the Digital Orrery, a <b>special-purpose</b> <b>computer</b> for computations in celestial mechanics, and maintains an active interest in computer science. He is also very active in organizing schools and workshops, for example at Les Houches, Cargese, and the KITP Santa Barbara.|$|E
5000|$|Palevsky {{began working}} in the {{computer}} industry in 1951 for $100 a week building computers at Northrop Aircraft, building copies of the MADIDDA, a <b>special-purpose</b> <b>computer</b> intended to solve differential equations. The MADIDDA was designed by physicist Floyd Steele, and who left Northrup in 1950, a year after the MADIDDA's completion. [...] Palevsky worked to build copies of Steele's invention between March 1950 and January 1951. MADDIDA was priced from $25,000 to $30,000. MADDIDA would prove to be the last and most sophisticated dedicated differential analyzer ever built, from then on all attention turned to electronic computers.|$|E
40|$|Recently, <b>special-purpose</b> <b>computers</b> have surpassed {{general-purpose}} {{computers in}} {{the speed with which}} large-scale stellar dynamics simulations can be performed. Speeds up to a Teraflops are now available, for simulations in a variety of fields, such as planetary formation, star cluster dynamics, galactic nuclei, galaxy interactions, galaxy formation, large scale structure, and gravitational lensing. Future speed increases for <b>special-purpose</b> <b>computers</b> will be even more dramatic: a Petaflops version, tentatively named the GRAPE- 6, could be built within a few years, whereas general-purpose computers are expected to reach this speed somewhere in the 2010 - 2015 time frame. Boards with a handful of chips from such a machine could be made available to individual astronomers. Such a board, attached to a fast workstation, will then deliver Teraflops speeds on a desktop, around the year 2000. Comment: 12 pages, LaTex, uses paspconf. sty. To appear in `Computational Astrophysics', the Proceedings of the 12 th `Kingston meeting' on Theoretical Astrophysics, eds. D. A. Clarke and M. J. West, ASP Conference Series, Vol. XXX (San Francisco: ASP...|$|R
40|$|AbstractWe {{overview}} our GRAvity PipE (GRAPE) {{project to}} develop <b>special-purpose</b> <b>computers</b> for astrophysical N-body simulations. The basic idea of GRAPE is {{to attach a}} custom-build computer dedicated to the calculation of gravitational interaction between particles to a general-purpose programmable computer. By this hybrid architecture, we can achieve both {{a wide range of}} applications and very high peak performance. Our newest machine, GRAPE- 6, achieved the peak speed of 32 Tflops, and sustained performance of 11. 55 Tflops, for the total budget of about 4 million USD. We also discuss relative advantages of <b>special-purpose</b> and general-purpose <b>computers</b> and the future of high-performance computing for science and technology...|$|R
50|$|Wernher von Braun {{believed}} that the personnel designing the space vehicles should have direct, hands-on participation in the building and testing of the hardware. For this, MSFC had facilities comparable with the best {{to be found in}} private industries. Included were precision machine shops, giant metal-forming and welding machines, and all types of inspection equipment. For every type of Saturn vehicle, one or more prototypes were fabricated in MSFC shops. Large, <b>special-purpose</b> <b>computers</b> were used in the checkout procedures.|$|R
5000|$|In {{computer}} typography, modern outline fonts describe printable characters (glyphs) by cubic or quadratic mathematical curves {{with control}} points. Nevertheless, bitmap fonts {{are still in}} use. Converting outlines requires filling them in; converting to bitmaps is not trivial, because bitmaps often don't have sufficient resolution to avoid [...] "stairstepping" [...] ("aliasing"), especially with smaller visible character sizes. Processing outline character data in sophisticated fashion to create satisfactory bitmaps for rendering is called [...] "hinting". Although the term implies suggestion, the process is deterministic, and done by executable code, essentially a <b>special-purpose</b> <b>computer</b> language. While automatic hinting is possible, results can be inferior to that done by experts.|$|E
5000|$|A Domain-specific {{language}} (DSL) is {{a computer}} language specialized to a particular application domain. This {{is in contrast to}} a general-purpose language (GPL), which is broadly applicable across domains. There is a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software, such as Emacs Lisp for GNU Emacs and XEmacs. DSLs can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. <b>Special-purpose</b> <b>computer</b> languages have always existed in the computer age, but the term [...] "domain-specific language" [...] has become more popular due to the rise of domain-specific modeling. Simpler DSLs, particularly ones used by a single application, are sometimes informally called mini-languages.|$|E
5000|$|Bob O. Evans joined IBM in a {{low level}} {{engineering}} position in1951as it was developing a new range of [...] "computers" [...] based on vacuum tubes (earlier IBM computers used mechanicalswitches). A natural and verycapable manager he moved up the company hierarchy {{to the position of}} vice president (development) inthe Data Systems division in 1962. This was apparently created as a position where he had responsibility for thedevelopment of [...] "System/360", a merger of IBMs separate scientific and business computing systems. In the early 1960s, Evans persuaded IBMâs chairman, Thomas J. Watson Jr., to discontinue the companyâs development of a hodgepodge of incompatible computers and instead to embark on the development of a single product line of general-purpose, compatible computers. Until then, researchers thought that the fields of scientific computing and commercial data processing each required their own type of <b>special-purpose</b> <b>computer.</b> Compatibility would ensure that the same software could run on any model of the product line, avoiding a re-programming of software.|$|E
40|$|Abstract: The {{calculation}} of vortex method has been accelerated by using <b>special-purpose</b> <b>computers,</b> MDGRAPE- 2 and MDGRAPE- 3, respectively. The similar algorithm has been implemented and {{the improvement in}} speed of MDGRAPE- 2 was 100 times while MDGRAPE- 3 was 1000 times faster {{when compared with the}} ordinary PC Xeon 5160 (3. 0 GHz) for N= 10 6. In addition, the speed of MDGRAPE- 3 was 25 times faster compared with MDGRAPE- 2. The round off errors have been investigated in both cases...|$|R
40|$|We {{overview}} our GRAPE (GRAvity PipE) {{project to}} develop <b>special-purpose</b> <b>computers</b> for astrophysical N-body simulations. The basic idea of GRAPE is {{to attach a}} custom-build computer dedicated to the calculation of gravitational interaction between particles to a general-purpose programmable computer. By this hybrid architecture, we can achieve both {{a wide range of}} applications and very high peak performance. Our newest machine, GRAPE- 6, achieved the peak speed of 32 Tfiops, and sustained performance of 11. 55 Tfiops, for the total budget of about 4 million USD...|$|R
5000|$|Mechanical <b>special-purpose</b> <b>computers</b> {{known as}} {{difference}} engines were {{proposed in the}} 19th century to tabulate polynomial approximations of logarithmic functions [...] - [...] i.e. to compute large logarithmic tables. This was motivated mainly by errors in logarithmic tables made by the human computers of the time. Early digital computers were developed during World War II in part to produce specialized mathematical tables for aiming artillery. From 1972 onwards, with the launch and growing use of scientific calculators, most mathematical tables went out of use.|$|R
5000|$|One of CSC's first {{projects}} was {{to submit}} IBM's Project MAC proposal. IBM had received intelligence that MIT was leaning toward the GE proposal, which {{was for a}} modified 600-series computer with virtual memory hardware and other enhancements; this would eventually become the GE 645. IBM proposed a modified S/360 that would include a virtual memory device called the [...] "Blaauw Box" [...] - [...] a component that had been designed for, but not included in, the S/360. The MIT team rejected IBM's proposal. The modified S/360 was seen as too {{different from the rest}} of the S/360 line; MIT did not want to use a customized or <b>special-purpose</b> <b>computer</b> for MULTICS, but sought hardware that would be widely available. GE was prepared to make a large commitment to time-sharing, while IBM was seen as obstructive. Bell Laboratories, another important IBM customer, soon made the same decision, and rejected the S/360 for time-sharing.|$|E
5000|$|The four color theorem was proved in 1976 by Kenneth Appel and Wolfgang Haken. It was {{the first}} major theorem to be proved using a computer. Appel and Haken's {{approach}} started by showing that there is a particular set of 1,936 maps, each of which cannot be part of a smallest-sized counterexample to the four color theorem. (If they did appear, there would be a smaller counterexample.) Appel and Haken used a <b>special-purpose</b> <b>computer</b> program to confirm that each of these maps had this property. Additionally, any map that could potentially be a counterexample must have a portion that looks like one of these 1,936 maps. Showing this required hundreds of pages of hand analysis. Appel and Haken concluded that no smallest counterexamples exist because any must contain, yet do not contain, one of these 1,936 maps. This contradiction means there are no counterexamples at all and that the theorem is therefore true. Initially, their proof was not accepted by all mathematicians because the computer-assisted proof was infeasible for a human to check by hand [...] Since then the proof has gained wider acceptance, although doubts remain [...]|$|E
40|$|There are {{discussed}} implementational {{aspects of the}} <b>special-purpose</b> <b>computer</b> algebra system FELIX designed for computations in constructive algebra. In particular, data types developed for the representation of and computation with commutative and non-commutative polynomials are described. Furthermore, comparisons of time and memory requirements of different polynomial representations are reported...|$|E
40|$|We have {{performed}} a very accurate computation of the non-equilibrium fluctuation- dissipation ratio for the 3 D Edwards-Anderson Ising spin glass, {{by means of}} large-scale simulations on the <b>special-purpose</b> <b>computers</b> Janus and Janus II. This ratio (computed for finite times on very large, effectively infinite, systems) is compared with the equilibrium probability distribution of the spin overlap for finite sizes. Our main result is a quantitative statics-dynamics dictionary, which could allow the experimental exploration of important features of the spin-glass phase without requiring uncontrollable extrapolations to infinite times or system sizes...|$|R
40|$|The Toolkit is {{a family}} of {{hardware}} modules (processors, memory, interconnect, and input-output devices) {{and a collection of}} software modules (compilers, simulators, scientific libraries, and high-level front ends) from which high-performance <b>special-purpose</b> <b>computers</b> can be easily configured and programmed. The hardware modules are intended to be standard, reusable parts. These are combined by means of a user- reconfigurable, static interconnect technology. T he Toolkit's software support, based on n ovel compilation techniques, produces e xtremely high- performance numerical code from high-level language input, and will eventually automatically configure hardware modules for particular applications...|$|R
40|$|During {{the last}} few years, {{advances}} in microprocessor technology have spurred {{a renewed interest in}} <b>special-purpose</b> <b>computers.</b> The microprocessor has become small, inexpensive, and powerful enough to be considered as a building block for special-purpose hardware. A description is presented of the architecture of a prototype 'finite element machine' currently being built. Attention is given to details regarding the finite element analysis problem, the arrangement of the processors as finite element nodes in the structural model, the influence of the architecture on the solution algorithm, interprocessor communication primitives, and the performance of the finite element machine...|$|R
40|$|Qualitative {{simulation}} {{is a new}} {{and challenging}} simulation paradigm. QSim, the widely-used algorithm for qualitative simulation has been developed by Kuipers at UT Austin. A drawback of current QSim-implementations is poor execution speed. In our research project a <b>special-purpose</b> <b>computer</b> architecture for QSim is developed to increase the performance. Two approaches are considered to improve the performance. Complex functions are parallelized and mapped onto a multiprocessor system. Less complex functions are directly implemented in hardware. These functions are executed on specialized coprocessors. The prototype implementation of this computer architecture is based on digital signal processors TMS 320 C 40 and field programmable gate arrays (Xilinx). This paper presents first experimental results of this research project. keywords: <b>special-purpose</b> <b>computer</b> architecture, parallel qualitative simulation, multi-DSP TMS 320 C 40, FPGA specialized coprocessor 1 Introduction QSim, the widely- [...] ...|$|E
40|$|Qualitative {{simulation}} is {{a rather}} new and challenging simulation paradigm. Its major strength is the prediction of all physically possible behaviors of a system given only weak and incomplete information about it. This strength is exploited {{more and more in}} applications like design, monitoring, and fault diagnosis. However, the poor performance of current qualitative simulators complicates or even prevents their application in technical environments. This paper presents the development of a <b>special-purpose</b> <b>computer</b> architecture for the best-known qualitative simulator QSim. Two design methods are considered to improve the performance. Complex functions are parallelized and mapped onto a multiprocessor system. Less complex functions are accelerated by software to hardware migration; they are executed on specialized coprocessors. keywords: qualitative simulator QSim <b>special-purpose</b> <b>computer</b> architecture multi-DSP TMS 320 C 40 FPGA Manuscript submitted to EUROSIM's scientific journal Si [...] ...|$|E
40|$|Digital controllers, one using a <b>special-purpose</b> <b>computer</b> and {{the other}} using a {{combination}} of digital and analog techniques, are designed around / 1 / computers that simulate the transfer function and interface with the system, and / 2 / analog and digital circuits, converters, amplifiers, constant multipliers, and delay lines that form a digital filter...|$|E
50|$|In 1987, Hall founded Linear Integrated Systems, Inc., (LIS) {{a company}} he led {{until his death}} in 2014. LIS {{developed}} advanced custom ICs for specific customers, including an advanced hearing aid device, but has more recently focused on small signal discrete semiconductors, particularly ultra-low-noise junction field effect transistors (JFETs). In 1993, while continuing to lead LIS, Hall founded Integrated Wave Technologies, Inc. (IWT) to combine the talents of former Soviet speech recognition engineers he had met with his experience in developing small, low-power electronic devices. Hall conceived of using the compact, effective speech recognition algorithms developed by these scientists in miniaturized, low-power, <b>special-purpose</b> <b>computers.</b>|$|R
50|$|Sussman {{saw that}} {{artificial}} intelligence ideas {{can be applied}} to computer-aided design. Sussman developed, with his graduate students, sophisticated computer-aided design tools for VLSI. Steele made the first Scheme chips in 1978. These ideas and the AI-based CAD technology to support them were further developed in the Scheme chips of 1979 and 1981. The technique and experience developed were then used to design other <b>special-purpose</b> <b>computers.</b> Sussman was the principal designer of the Digital Orrery, a machine designed to do high-precision integrations for orbital mechanics experiments. The Orrery was designed and built by a few people in a few months, using AI-based simulation and compilation tools.|$|R
40|$|A neural-network {{mathematical}} model that, relative to prior such models, places {{greater emphasis on}} some of the temporal aspects of real neural physical processes, has been proposed as a basis for massively parallel, distributed algorithms that learn dynamic models of possibly complex external processes by means of learning rules that are local in space and time. The algorithms could be made to perform such functions as recognition and prediction of words in speech and of objects depicted in video images. The approach embodied in this model is said to be "hardware-friendly" in the following sense: The algorithms would be amenable to execution by <b>special-purpose</b> <b>computers</b> implemented as very-large-scale integrated (VLSI) circuits that would operate at relatively high speeds and low power demands...|$|R
