754|8324|Public
5|$|The chipset {{brought a}} 1066MHz system bus, use of DDR3 <b>system</b> <b>memory,</b> and {{integrated}} Nvidia GeForce 9400M graphics. Other changes include a display which uses LED backlights (replacing the fluorescent tube backlights {{used in the}} previous model) and arsenic-free glass, a new Mini DisplayPort (replacing the polycarbonate MacBook's mini-DVI port), a multi-touch glass trackpad which also acts as the mouse button, {{and the removal of}} the FireWire 400 port (thus it doesn't support Target Disk Mode, used for data transfers or operating system repairs without booting the system).|$|E
5|$|One {{reviewer}} {{criticized the}} decision to underclock the ATI Mobility Radeon X1600 graphics card by about 30percent its original speed. The notebook was also noted for running hot. Users complained that upgrading <b>system</b> <b>memory</b> was harder than in older Apple notebooks. Since the dimensions for the 15-inch MacBook Pro were tweaked slightly from the 15-inch PowerBook G4, older accessories such as notebook sleeves did not work with the new models. Some users noted a slight flickering when the screen was on lower brightness settings. Apple increased the battery capacity by 10 Wh, going from 50 in the PowerBook G4 to 60, but the more powerful Core Duo CPU required more power. Battery life therefore remained {{about the same as}} in previous models, at three-plus hours.|$|E
5|$|The Saturn had {{technically}} impressive hardware at {{the time}} of its release, but its complexity made harnessing this power difficult for developers accustomed to conventional programming. The greatest disadvantage was that both CPUs shared the same bus and were unable to access <b>system</b> <b>memory</b> at the same time. Making full use of the 4kB of cache memory in each CPU was critical to maintaining performance. For example, Virtua Fighter used one CPU for each character, while Nights used one CPU for 3D environments and the other for 2D objects. The Saturn's Visual Display Processor 2 (VDP2), which can generate and manipulate backgrounds, has also been cited as one of the system's most important features.|$|E
50|$|In {{operating}} <b>systems,</b> <b>memory</b> {{management is}} the function responsible {{for managing the}} computer's primary memory.|$|R
50|$|In Intel <b>systems,</b> <b>memory</b> timings and {{management}} are {{handled by the}} Memory Reference Code (MRC), {{a part of the}} BIOS.|$|R
40|$|The {{performance}} of a vector processor accessing vectors {{is strongly dependent on}} the conflicts produced in the memory subsystem. These conflicts delay the work of the functional units. The concurrent memory access of several vector streams causes inter-conflicts between the references of the different vector streams. In a <b>memory</b> <b>system</b> where several <b>memory</b> modules are mapped in every bus (complex <b>memory</b> <b>system)</b> the number of conflicts increases because the bus must be shared by vector streams. Conflicts named linked conflicts and complex conflicts decrease the {{performance of}} the steady-state phase in complex <b>memory</b> <b>systems</b> (CRAY X-MP). This paper proposes a method that allows a concurrent access to several vector streams reducing the average memory access time in vector processors with complex <b>memory</b> <b>systems.</b> Keywords: Simple <b>Memory</b> <b>System,</b> Complex <b>Memory</b> <b>System,</b> Vector Stream, Inter-conflicts, Inter-Complex Conflicts. 1. Introduction The memory module reservation time is, in general, [...] ...|$|R
5|$|The {{album was}} his first for Starbucks' Hear Music record label, after {{previously}} having a 45-year-old relationship with Capitol/EMI. The recording contract with Capitol/EMI ended a few {{months prior to the}} release of the album, after McCartney had found out that EMI were planning to take six months to set up a promotional plan for the album. McCartney was the first artist to sign to Hear Music. The album was released on 4 June 2007 in the UK, and a day later on the 5th in the US, and with a vinyl edition later in the month on 25 June. In the US, Memory Almost Full debuted at number 3 on the Billboard 200 with about 161,000 copies sold within the first week, making it McCartney's highest-charting album there since 1997's Flaming Pie. 47% of the album sales from the opening week were from Starbucks coffee shops, which were the best sales for any album in the history of Starbucks. While it was announced that copies of the album sold in the Starbucks coffee shops in the UK would not be counted by the Official UK Charts, because they are not registered in the copies counting <b>system,</b> <b>Memory</b> Almost Full, however, still managed to hit number 5 on the UK Album Charts. The album was also peaked at number 1 on Billboard Internet Sales Chart, and number 3 on Billboard Top Internet Albums Downloads.|$|E
5|$|The main {{creative}} {{force behind}} the overall 3D presentation was Kazuyuki Hashimoto, the general supervisor for these sequences. Being experienced in the new technology the team had brought on board, he accepted the post at Square as the team aligned with his own creative spirit. One of the major events in development was when the real-time graphics were synchronized to computer-generated full motion video (FMV) cutscenes for some story sequences, notably an early sequence where a real-time model of Cloud jumps onto an FMV-rendered moving train. The backgrounds were created by overlaying two 2D graphic layers and changing the motion speed of each to simulate depth perception. While {{this was not a}} new technique, the increased power of the PlayStation enabled a more elaborate version of this effect. The biggest issue with the 3D graphics was the large memory storage gap between the development hardware and the console: while the early 3D tech demo had been developed on a machine with over 400 megabytes of total memory, the PlayStation only had two megabytes of <b>system</b> <b>memory</b> and 500 kilobytes for texture memory. The team needed {{to figure out how to}} shrink the amount of data while preserving the desired effects. This was aided with reluctant help from Sony, who had hoped to keep Square's direct involvement limited to a standard API package, but they eventually relented and allowed the team direct access to the hardware specifications.|$|E
5|$|The yearlong {{conversion}} began October 9, 2011. Between October and December, 96 of Jaguar's 200 cabinets, each containing 24 XT5 blades (two 6-core CPUs per node, four nodes per blade), were {{upgraded to}} XK7 blades (one 16-core CPU per node, four nodes per blade) while {{the remainder of}} the machine remained in use. In December, computation was moved to the 96 XK7 cabinets while the remaining 104 cabinets were upgraded to XK7 blades. ORNL's external ESnet connection was upgraded from 10 Gbit/s to 100 Gbit/s and the system interconnect (the network over which CPUs communicate with each other) was updated. The Seastar design used in Jaguar was upgraded to the Gemini interconnect used in Titan which connects the nodes into a direct 3D torus interconnect network. Gemini uses wormhole flow control internally. The <b>system</b> <b>memory</b> was doubled to 584 TiB. 960 of the XK7 nodes (10 cabinets) were fitted with a Fermi based GPU as Kepler GPUs were not then available; these 960 nodes were referred to as TitanDev and used to test code. This first phase of the upgrade increased the peak performance of Jaguar to 3.3 petaFLOPS. Beginning on September 13, 2012, Nvidia K20X GPUs were fitted to all of Jaguar's XK7 compute blades, including the 960 TitanDev nodes. In October, the task was completed and the computer was finally christened Titan.|$|E
40|$|In the {{mammalian}} brain {{newly acquired}} memories {{are dependent on}} the hippocampus for maintenance and recall, but over time these functions are {{taken over by the}} neocortex through a process called <b>systems</b> <b>memory</b> consolidation. Thus, whereas recent memories are likely to be disrupted in the event of hippocampal damage, older memories are less vulnerable. However, if a consolidated memory is reactivated by a reminder, it can temporarily return to a hippocampus-dependent state from which it normally recovers through a process known as <b>systems</b> <b>memory</b> reconsolidation. Here, we present an artificial neural-network model that captures <b>systems</b> <b>memory</b> consolidation and reconsolidation, as well as aspects of the related phenomena of memory extinction, spontaneous recovery and trace dominance (the competition between temporary destabilization and extinction). The model provides a novel explanation of trace dominance, from which we derive predictions that could be tested in future experiments...|$|R
50|$|Memory {{virtualization}} implementations are {{distinguished from}} shared <b>memory</b> <b>systems.</b> Shared <b>memory</b> <b>systems</b> {{do not permit}} abstraction of memory resources, thus requiring implementation with a single operating system instance (i.e. not within a clustered application environment).|$|R
50|$|Competitors include EMC XTremIO, IBM FlashSystem, Nimbus Data, Solidfire, Pure Storage, Texas <b>Memory</b> <b>Systems,</b> Violin <b>Memory</b> and Kaminario.|$|R
25|$|In Windows Vista {{and later}} versions, ReadyBoost feature allows flash drives (from 4GB {{in case of}} Windows Vista) to augment {{operating}} <b>system</b> <b>memory.</b>|$|E
25|$|Internal flash {{uses the}} FAT32 file <b>system.</b> <b>Memory</b> Stick media is {{compatible}} with both FAT and FAT32, although devices measuring 4GB or more must use the FAT32 file system.|$|E
25|$|Techniques {{can also}} be combined. For sorting very large sets of data that vastly exceed <b>system</b> <b>memory,</b> even the index {{may need to be}} sorted using an {{algorithm}} or combination of algorithms designed to perform reasonably with virtual memory, i.e., {{to reduce the amount of}} swapping required.|$|E
50|$|The {{multiple}} <b>memory</b> <b>system</b> theory ascribes {{the differences}} in implicit and explicit memory to {{the differences in}} the underlying structures. The theory says that explicit memories are associated with a declarative <b>memory</b> <b>system</b> responsible for the formation of new representations or data structures. In contrast, implicit memories are associated with a procedural <b>memory</b> <b>system</b> where <b>memories</b> are just modifications of existing procedures or processing operations.|$|R
25|$|Differentiable neural {{computers}} (DNC) are an NTM extension. They out-performed neural Turing machines, long short-term <b>memory</b> <b>systems</b> and <b>memory</b> networks on sequence-processing tasks.|$|R
40|$|The {{efficient}} use of the <b>memory</b> <b>system</b> is a key issue {{for the performance of}} many applications. A benchmark written with OpenMP is presented that measures several aspects of a shared <b>memory</b> <b>system</b> like bandwidth, <b>memory</b> latency and inter-thread latency. Special focus is on revealing and identifying bottlenecks and possible hierarchies in the main <b>memory</b> <b>system...</b>|$|R
25|$|Among other things, a {{multiprogramming}} {{operating system}} kernel must {{be responsible for}} managing all <b>system</b> <b>memory</b> which is currently in use by programs. This ensures that a program does not interfere with memory already in use by another program. Since programs time share, each program must have independent access to memory.|$|E
25|$|The {{hardware}} industry measures <b>system</b> <b>memory</b> (RAM) {{using the}} binary meaning while {{magnetic disk storage}} uses the SI definition. However, many exceptions exist. Labeling of diskettes uses the megabyte to denote 1024×1000 bytes. In the optical disks market, Compact Disks use MB to mean 10242 bytes while DVDs use GB to mean 10003 bytes.|$|E
25|$|Unlike the few Apple {{viruses that}} had come before, such as the Elk Cloner virus, that were {{essentially}} annoying but did no damage, the Festering Hate series of viruses was extremely destructive, spreading to all system files it could find on the host computer (hard drive, floppy and <b>system</b> <b>memory)</b> and then destroying everything when it could no longer find any uninfected files.|$|E
40|$|Situated design agents require {{new types}} of memory. The design of a <b>memory</b> <b>system</b> for use in such a design agent, based {{on the notion of}} {{constructive}} memory is proposed. The specification of the <b>memory</b> <b>system</b> is derived from the architecture of a situated design agent. Cognitive studies of the human <b>memory</b> <b>system</b> provide the functional and behavioural characterization of the <b>memory</b> <b>system...</b>|$|R
40|$|Research and Teaching Interests • Computer {{architecture}} and <b>systems.</b> <b>Memory</b> <b>systems.</b> Multi/many-core systems. Scalable, QoS-aware, latency-tolerant systems. • Computer architectures for secure and robust operating systems (OS). OS/architecture interaction. • Architectural support for safe/managed/parallel programming languages (PL) and programmer productivity. PL/architecture interaction. • Fault tolerant and bug-tolerant architectures. Dependable systems. • System-wide resource management and QoS, especially in multi-core and multithreaded systems...|$|R
40|$|As {{processor}} {{cycle times}} decrease, <b>memory</b> <b>system</b> performance becomes ever more critical to overall performance. Continually changing technology and workloads create {{a moving target}} for computer architects {{in their effort to}} design cost-effective <b>memory</b> <b>systems.</b> Meeting the demands of ever changing workloads and technology requires the following: Efficient techniques for evaluating <b>memory</b> <b>system</b> performance, Tuning programs to better use the <b>memory</b> <b>system,</b> and New <b>memory</b> <b>system</b> designs. This thesis makes contributions in each of these areas. Hardware and software developers rely on simulation to evaluate new ideas. In this thesis, I present a new interface for writing <b>memory</b> <b>system</b> simulators—the active <b>memory</b> abstraction—designed specifically for simulators that process memory references as the application executes and avoids storing them to tape or disk. Active memory allows simulators to optimize for the common case, e. g., cache hits, achieving simulation times only 2 - 6 times slower than the original un-instrumented application. The efficiency of the active memory abstraction can be used by software designers t...|$|R
25|$|On the PlayStation 3, image {{processing}} for PlayStation Move is {{performed in the}} console's Cell microprocessor. According to Sony, use of the motion-tracking library entails some Synergistic Processing Unit (SPU) overhead as well an impact on memory, though the company states that the effects will be minimized. According to Move motion controller co-designer Anton Mikhailov, the library uses 1-2 megabytes of <b>system</b> <b>memory.</b>|$|E
25|$|Programs and {{subsystems}} in {{user mode}} {{are limited in}} terms of to what system resources they have access, while the kernel mode has unrestricted access to the <b>system</b> <b>memory</b> and external devices. The Windows NT kernel {{is known as a}} hybrid kernel. The architecture comprises a simple kernel, hardware abstraction layer (HAL), drivers, and a range of services (collectively named Executive), which all exist in kernel mode.|$|E
25|$|The system {{contains}} {{a total of}} 128MB of <b>system</b> <b>memory</b> consisting of two 64MB (512Mb) FCRAM chips developed by Fujitsu, with a maximum bandwidth of 3.2GB/s. However, 32MB is reserved for the operating system and unavailable to games. Additionally, the system contains 6MB of VRAM. The console also {{contains a}} dedicated hardware audio DSP module capable of outputting mono, stereo or pseudo-surround sound through either its two speakers or headphone jack.|$|E
5000|$|OS/360 and successors use the [...] (for [...] "transfer control") macro for chain loading. Because of the {{operating}} <b>system's</b> <b>memory</b> management this {{may or may not}} result in replacement of the code of the calling program in memory.|$|R
50|$|Moreover, in real systems {{a lot of}} {{parameters}} can {{be taken}} into account in order to give a more realistic description, for example attraction and repulsion between agents (finite size particles), chemotaxis (biological <b>systems),</b> <b>memory,</b> non-identical particles, the surrounding liquid...|$|R
40|$|Background: How {{do people}} sustain a visual {{representation}} of the environment? Currently, many researchers argue that a single visual working <b>memory</b> <b>system</b> sustains non-spatial object information such as colors and shapes. However, previous studies tested visual working memory for two-dimensional objects only. In consequence, the nature of visual working memory for three-dimensional (3 D) object representation remains unknown. Methodology/Principal Findings: Here, I show that when sustaining information about 3 D objects, visual working memory clearly divides into two separate, specialized <b>memory</b> <b>systems,</b> rather than one system, as was previously thought. One <b>memory</b> <b>system</b> gradually accumulates sensory information, forming an increasingly precise view-dependent {{representation of the}} scene {{over the course of}} several seconds. A second <b>memory</b> <b>system</b> sustains view-invariant representations of 3 D objects. The view-dependent <b>memory</b> <b>system</b> has a storage capacity of 3 – 4 representations and the view-invariant <b>memory</b> <b>system</b> has a storage capacity of 1 – 2 representations. These systems can operate independently from one another and do not compete for working memory storage resources. Conclusions/Significance: These results provide evidence that visual working memory sustains object information in two separate, specialized <b>memory</b> <b>systems.</b> One <b>memory</b> <b>system</b> sustains view-dependent representations of the scene, akin to the view-specific representations that guide place recognition during navigation in humans, rodents and insects. Th...|$|R
25|$|Win32/FakeSysdef manifests as {{one or more}} of {{an array}} of {{programs}} that purport to scan one's computer for hardware failures related to <b>system</b> <b>memory,</b> hard drives and system functionality as a whole. They scan the computer, show false hardware issues, and present a remedy to defrag the hard drives and fine-tune the system performance. They then request the user to make a payment in order to activate the program so the user can download the new updates and to repair the hardware issues.|$|E
25|$|The 68EC000 {{was used}} as a {{controller}} in many audio applications, including Ensoniq musical instruments and sound cards, where {{it was part of the}} MIDI synthesizer. On Ensoniq sound boards, the controller provided several advantages compared to competitors without a CPU on board. The processor allowed the board to be configured to perform various audio tasks, such as MPU-401 MIDI synthesis or MT-32 emulation, without the use of a TSR program. This improved software compatibility, lowered CPU usage, and eliminated host <b>system</b> <b>memory</b> usage.|$|E
25|$|Unlike the Mac Mini G4, the Intel-based Mac Mini uses a dual-channel {{architecture}} for memory. The original Intel-based Mac Mini uses 667MHz DDR2 SDRAM, while models {{starting with}} the early 2009 revision use 1066MHz DDR3 SDRAM. From the 2011 revision onwards, the Mac Mini supports up to 16GB of memory. While all versions of the Mac Mini {{up to and including}} the 2012 revision supports user upgradeable memory after purchase, the current model does not, because the memory is soldered to the logic board. Since the integrated graphics processor does not have its own dedicated memory, the system shares some of the main <b>system</b> <b>memory</b> with it.|$|E
50|$|Note {{also that}} the storage {{required}} for the multiple processes came from the <b>system's</b> <b>memory</b> pool as needed. There was no having to do SYSGENs on Burroughs systems as with competing systems in order to preconfigure memory partitions in which to run tasks.|$|R
5000|$|Isolated OS components. It made it {{possible}} to remove most of the operating <b>system's</b> <b>memory</b> footprint from the user's environment, thereby increasing the memory available for application use, and reducing the risk of applications intruding into or corrupting operating system data and programs.|$|R
40|$|Computer {{architecture}} and <b>systems.</b> <b>Memory</b> <b>systems.</b> Multi/many-core systems. Scalable, QoS-aware, latency-tolerant systems. • Computer architectures for secure and robust operating systems (OS). OS/architecture interaction. • Architectural support for safe/managed/parallel programming languages (PL) and programmer productivity. PL/architecture interaction. • Fault tolerant and bug-tolerant architectures. Dependable systems. • System-wide resource management and QoS, especially in multi-core and multithreaded systems. • Novel computer architectures for health, biological, medical, and bioinformatics applications...|$|R
