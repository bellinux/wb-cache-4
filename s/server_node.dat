105|475|Public
50|$|Accounts {{allow for}} {{exclusive}} connections. A <b>server</b> <b>node</b> may establish accounts and then distribute the credentials information. Accounts create an artificial web of trust without exposing the public encryption key and without attaching {{the key to}} an IP address.|$|E
50|$|OpenSSI uses LVS {{to provide}} {{fault-tolerant}} load balanced IP services. Inbound network connections are received by a director node which redirects {{them to the}} least loaded <b>server</b> <b>node.</b> (A node may be both a director and server). In the event of director node failure another director node takes over and the system continues to accept inbound connections.|$|E
50|$|In November 2011 Calxeda {{announced}} the EnergyCore ECX-1000, featuring four 32-bit ARMv7 Cortex-A9 CPU cores operating at 1.1-1.4 GHz, 32 KB L1 I-cache and 32 KB L1 D-cache per core, 4 MB shared L2 cache, 1.5 W per processor, 5 W per <b>server</b> <b>node</b> including 4 GB of DDR3 DRAM, 0.5 W when idle.Each chip included five 10 Gigabit Ethernet ports. Four chips are carried on each EnergyCard.|$|E
30|$|<b>Server</b> <b>nodes</b> may be {{scattered}} {{all around the}} network and may be many hops away. Therefore, communication delay will be increased proportional {{to the number of}} hopes between client and the <b>server</b> <b>nodes.</b>|$|R
50|$|Proxmox VE can be {{clustered}} {{across multiple}} <b>server</b> <b>nodes.</b>|$|R
5000|$|... #Caption: Supermicro SBI-7228R-T2X blade server, {{containing}} two dual-CPU <b>server</b> <b>nodes</b> ...|$|R
5000|$|Beowulf is a multi-computer {{architecture}} {{which can}} be used for parallel computations. It is a system which usually consists of one <b>server</b> <b>node,</b> and one or more client nodes connected via Ethernet or some other network. It is a system built using commodity hardware components, like any PC capable of running a Unix-like operating system, with standard Ethernet adapters, and switches. It does not contain any custom hardware components and is trivially reproducible. Beowulf also uses commodity software like the FreeBSD, Linux or Solaris operating system, Parallel Virtual Machine (PVM) and Message Passing Interface (MPI). The <b>server</b> <b>node</b> controls the whole cluster and serves files to the client nodes. It is also the cluster's console and gateway to the outside world. Large Beowulf machines might have more than one <b>server</b> <b>node,</b> and possibly other nodes dedicated to particular tasks, for example consoles or monitoring stations. In most cases client nodes in a Beowulf system are dumb, the dumber the better. Nodes are configured and controlled by the <b>server</b> <b>node,</b> and do only what they are told to do. In a disk-less client configuration, a client node doesn't even know its IP address or name until the server tells it.One of the main differences between Beowulf and a Cluster of Workstations (COW) is that Beowulf behaves more like a single machine rather than many workstations. In most cases client nodes do not have keyboards or monitors, and are accessed only via remote login or possibly serial terminal. Beowulf nodes {{can be thought of as}} a CPU + memory package {{which can be}} plugged into the cluster, just like a CPU or memory module can be plugged into a motherboard.Beowulf is not a special software package, new network topology, or the latest kernel hack. Beowulf is a technology of clustering computers to form a parallel, virtual supercomputer. Although there are many software packages such as kernel modifications, PVM and MPI libraries, and configuration tools which make the Beowulf architecture faster, easier to configure, and much more usable, one can build a Beowulf class machine using a standard Linux distribution without any additional software. If you have two networked computers which share at least the [...] file system via NFS, and trust each other to execute remote shells (rsh), then it could be argued that you have a simple, two node Beowulf machine.|$|E
50|$|A Helios network {{requires}} {{at least}} one I/O <b>Server</b> <b>node</b> that is able to provide a file system server, console server and reset control for the processing nodes. At power on, the Helios nucleus is bootstrapped from the I/O server into the network. Each node is booted using a small first-stage loader that then downloads and initialises the nucleus proper. Once running, a node communicates with its neighbours, booting them in turn, if required.|$|E
50|$|In 1998, the PARAM 10000 was unveiled. PARAM 10000 used several {{independent}} nodes, each {{based on}} the Sun Enterprise 250 server and each such server contained two 400Mhz UltraSPARC II processors. The base configuration had three compute nodes and a <b>server</b> <b>node.</b> The peak speed of this base system was 6.4 GFLOPS. A typical system would contain 160 CPUs and be capable of 100 GFLOPS But, it was easily scalable to the TFLOP range. Exported to Russia and Singapore.|$|E
50|$|The HP POD 240a was {{launched}} in June 2011. It can be configured with two rows of 44 extra height 50U racks that could house 4,400 <b>server</b> <b>nodes</b> of typical size, or 7,040 <b>server</b> <b>nodes</b> of the densest size. HP claimed that the typical brick and mortar data center required to house this equipment would be 10,000 square feet of floor space.|$|R
50|$|Teradata Database 13.10 was {{announced}} in October 2010. At the time, Teradata used Xeon 5600 processors for the <b>server</b> <b>nodes.</b>|$|R
40|$|This archive {{includes}} the research data associated to the paper: Giuliano Casale. Accelerating Performance Inference over Closed Systems by Asymptotic Methods. Proc. ACM Meas. Anal. Comput. Syst., 1 (1), 2017. The paper is accepted for presentation at ACM SIGMETRICS 2017. The research data requires MATLAB 2015 a or later. Four datasets are included, each corresponding to {{a section of}} the paper: - sec 5. 3. 1 : Small and medium models without infinite <b>server</b> <b>nodes</b> (Section 5. 3. 1) - sec 5. 3. 2 : Large models without infinite <b>server</b> <b>nodes</b> (Section 5. 3. 2) - sec 5. 3. 3 : Models with infinite <b>server</b> <b>nodes</b> (Section 5. 3. 3) - sec 5. 4 : Optimization programs (Section 5. 4) A description of each dataset is included in the README. TXT file inside each folder...|$|R
5000|$|The {{flagship}} Teradata {{product is}} referred to as a [...] "data warehouse system" [...] which stores and manages data. The data warehouses use a [...] "shared nothing architecture", which means that each <b>server</b> <b>node</b> has its own memory and processing power. Adding more servers and nodes increases the amount of data that can be stored. The database software runs on the servers and spreads the workload among them. [...] In 2010, Teradata added text analytics to track unstructured data, such as word processor documents, and semi-structured data, such as spreadsheets.|$|E
50|$|Although RMAs {{are derived}} from RIAs, there are {{fundamental}} dissimilarities between them, particularly task/layer distribution of application, interaction medium, screen size and layout, communication and networking mediums. The logic and data layers in RIAs are initially located in remote back-end servers and only the user interface is located inside the end-user's device. The fundamental principle in forming RIAs is to mitigate the server-side computing cost of the applications by exploiting the computing power of contemporary desktop computers at the user end. Parts of logic and data layers are transferred from the <b>server</b> <b>node</b> to the client node. The rich computing and storage resources in contemporary personal computers (PCs) reduce client-server networking traffic and delay, and shrink ownership and maintenance costs on the server side. This helps service providers reduce operating costs. In return, the end-user benefits from a crisp interaction response from the application since {{some part of the}} data and logic is stored in their local computer.|$|E
50|$|The {{hardware}} {{component of}} the X2-2 appliance consisted of: a group of 1-unit Intel Xeon servers, each equipped with two six-core 2.93 GHz processors and two solid-state drives for operating system and swap space; a common storage area network; {{and a set of}} InfiniBand and Ethernet switches. A full rack contains 30 server nodes, a half rack, 16, a quarter rack, 8, and an eighth rack, 4. Each <b>server</b> <b>node</b> has installed 96 GB of RAM, four 10 Gigabit Ethernet interfaces, and a double InfiniBand port. The storage area network for all configurations is similar, with 40 TB of raw space. The vendor's specifications and advertising content usually indicate the total parameters (360 processor kernels and 2.9 TB RAM for full rack).An X3-2 model was announced in 2012 with newer processors and more memory. Since late 2013 an X4-2 model is commercially available, it has yet more processor cores and four times as large capacity of solid-state drives.|$|E
40|$|Cluster-based {{web servers}} {{have become a}} common so-lution to {{high-traffic}} web hosting in recent years. Such a system usually consists of a dispatcher and a collection ofweb <b>server</b> <b>nodes</b> connected to a local area network. Client requests are distributed to the web <b>server</b> <b>nodes</b> by the dis-patcher to obtain high throughput. Existing solutions for improving cluster-based web server performance either fol-low a dispatcher-based approach to achieve load balancing or use some special caching mechanisms that can avoid ex-cessive disk operations...|$|R
30|$|Aggregator Node: In case of LFTM model, these <b>nodes</b> {{were called}} <b>server</b> <b>nodes.</b> These are the nodes which {{aggregate}} the data coming from different sensor nodes.|$|R
40|$|This article {{addresses}} {{path selection}} problems {{that arise in}} a large-scale media server. In particular, the problem on a distributed server with two tiers of <b>server</b> <b>nodes</b> connected by a VC-enabled interconnection network is considered. To serve each incoming request, a pair of <b>server</b> <b>nodes</b> should be selected to handle the request. In addition, a network path in the interconnection network should be arranged to pipe the program stream between the pair of <b>server</b> <b>nodes</b> for the request. The problem is modeled as a network flow problem. A two-pass flow (TPF) algorithm is proposed to solve the flow problem that involves multiple types of flow. The proposed algorithm allocates system resources effectively to maximize the number of admitted requests without violating resource constraints. Unlike some other works, the algorithm {{does not need to}} rely on assumptions about any particular configuration of the underlying interconnection network...|$|R
5000|$|Lustre 2.9 was {{released}} in December 2016and included a number of features related to security and performance. The Shared Secret Key security flavour uses the same GSSAPI mechanism as Kerberos to provide client and <b>server</b> <b>node</b> authentication, and RPC message integrity and security (encryption). The Nodemap feature allows categorizing client nodes into groups and then mapping the UID/GID for those clients, allowing remotely-administered clients to transparently use a shared filesystem without having a single set of UID/GIDs for all client nodes. The subdirectory mount feature allows clients to mount {{a subset of the}} filesystem namespace from the MDS. This release also added support for up to 16MiB RPCs for more efficient I/O submission to disk, and added the [...] interface to allow clients to provide I/O hints to the servers to prefetch file data into server cache or flush file data from server cache. There was improved support for specifying filesystem-wide default OST pools, and improved inheritance of OST pools in conjunction with other file layout parameters.|$|E
30|$|Like {{the normal}} user nodes, the DCA {{shareholding}} nodes may {{move to the}} other places and be inaccessible to the user nodes. In this condition, a user node may not find the required k DCA <b>server</b> <b>node.</b> Thus, a DCA scheme {{must take into account}} the mobility of DCA server nodes and dynamic nature of a MANET and propose appropriate solutions to solve these problems. For example, in some schemes, this problem is solved by allocating more than one share to each DCA <b>server</b> <b>node.</b>|$|E
40|$|Abstract. The {{urban area}} of the server can be a central control node switch and <b>server</b> <b>node,</b> the node {{switches}} and a <b>server</b> <b>node</b> can know the porttraffic information in real time. When the connection existed over between 2 of these devices, a connection can be selected according to the real-time traffic, real-time routing function to avoid network congestion and flow of the uneven distribution of resources. Similarly, node server can central control node switch can know the real-time node switch port flow information, therefore, the node switches can obtain the terminal flow control information from the server to the terminal node data transmission flow control...|$|E
5000|$|The {{profiler}} uses adaptive {{sampling methods}} to identify process counters and activities, and combines data from multiple processes {{that may be}} running on multiple compute <b>server</b> <b>nodes.</b> It analyzes performance and causes of bottlenecks including: ...|$|R
30|$|To summarize, {{between the}} MMT and AR nodes, Peer-Key is {{generated}} and between MMT and IS <b>server</b> <b>nodes</b> IS-Key is generated. Peer-Key {{is the key}} hierarchy between MMT and AR and IS-Key is the key hierarchy between MMT and IS server.|$|R
30|$|First, a {{user node}} {{has to find}} t, CA <b>server</b> <b>nodes</b> in MANET that is more {{difficult}} to find than finding one CA node. Schemes such as flooding for finding CA will not work since they consume too much network resource.|$|R
40|$|Energy-saving {{optimization}} is {{very important}} for various engineering problems related to modern distributed systems. We consider here a control problem for a wireless sensor network with a single time <b>server</b> <b>node</b> and a large number of client nodes. The problem is to minimize a functional which accumulates clock synchronization errors in the clients nodes and the energy consumption of the server over some time interval [0, T]. The control function u = u(t), 0 ≤ u(t) ≤ u 1, corresponds {{to the power of the}} <b>server</b> <b>node</b> transmitting synchronization signals to the clients. For all possible parameter values we find the structure of optimal trajectories. We show that for sufficiently large u 1 the solutions contain singular arcs...|$|E
40|$|We {{examine the}} {{problems}} of routing and server assignment in networks with random connectivities. In such a network the basic topology is fixed, but during each time slot and for each of tis input queues, each <b>server</b> (<b>node)</b> is either connected to or disconnected from each of its queues with some probability...|$|E
30|$|According to Qiang et al. (2011), we set the {{training}} parameters of RBF 1 neural networks by the HHGA. Similarly, RBFNN 2 should be trained. Finally, {{we can take}} the prediction value of RBFNN 1 and RBFNN 2 and input it in RBFNN 3, and the <b>server</b> <b>node</b> load is predicted by RBFNN 3.|$|E
40|$|In this paper, {{an attempt}} {{has been made}} to develop a compartmental Epidemic model on the {{transmission}} of malicious objects in a computer network with natural death (that is, crashing of nodes due to the reason other than the attack of malicious objects) and vertical transmission. In this model there are two possibilities of computer network: one of high infectious <b>server</b> <b>nodes</b> having high intensity rate of malicious attack while another is low infectious client nodes having low intensity rate of malicious attack. The high infectious <b>server</b> <b>nodes</b> transfer the malicious objects to the low infectious client nodes in the computer network. The low infectious client nodes recover with temporary immunity and becomes again susceptible to the high infectious <b>server</b> <b>nodes</b> but at a lower rate. This needs to develop effective immunity after a period of time and a fully automatic defense mechanism to protect computer networks from modern and fast propagating worms. The stability of the result is stated in terms of the Jacobian of the system and the basic reproduction number is also well-defined. Numerical methods and MATLAB are employed to solve and simulate the system of equations developed and analysis of the model gives remarkable exposure...|$|R
3000|$|... ○ Server-to-server connections: {{because many}} NoSQL and NewSQL data stores include {{some sort of}} {{replication}} and distributed processing functionalities, communications among the <b>server</b> <b>nodes</b> can also be eavesdropped to obtain unauthorized access to data. A server-to-server encryption mechanism guarantees that these flows cannot be read.|$|R
40|$|A {{geographically}} distributed {{web server}} (GDWS) system, consisting of multiple <b>server</b> <b>nodes</b> interconnected by a MAN or a WAN, can achieve better efficiency in handling the ever-increasing web requests than centralized web servers {{because of the}} proximity of <b>server</b> <b>nodes</b> to clients. It is also more scalable since the throughput will not be limited by available bandwidth connecting to a central server. The key research issue {{in the design of}} GDWS is how to replicate and distribute the documents of a web site among the <b>server</b> <b>nodes.</b> This paper proposes a density-based replication scheme and applies it to our proposed Extensible GDWS (EGDWS) architecture. Its document distribution scheme supports partial replication targeting only at hot objects among the documents. To distribute the replicas generated via the density-based replication scheme, we propose four different document distribution algorithms: Greedy-cost, Maximal-density, Greedypenalty, and Proximity-aware. A proximity-based routing mechanism is designed to incorporate these algorithms for achieving better web server performance in a WAN environment. Simulation results show that our document replication and distribution algorithms achieve better response times and load balancing than existing dynamic schemes. To further reduce user's response time, we propose two document grouping algorithms that can cut down on the request redirection overheads...|$|R
40|$|The Lister Hill National Center for Biomedical Communications is {{investigating}} a {{local area network}} approach to a system for the distribution of digital document images among a set of image retrieval and display workstations, each based on an AT-class computer. The goal {{of the study is}} to investigate rapid, online, multi-user access to a database of document images stored on optical disks. The display workstations have access to the document image <b>server</b> <b>node</b> via Ethernet physical and data link layers and inhouse-developed upper layer protocols, The demonstrated reliability of the Ethernet, combined with the restricted functionality of the application, allowed the development of upper layers involving a minimum of control and processing overhead. In addition, certain protocol tasks are performed by an 80186 coprocessor located on the Ethernet controller board, thereby improving the performance of the <b>server</b> <b>node</b> to a level beyond the constraints imposed by the AT-class architecture [...] ...|$|E
40|$|Contents: FEATURE ARTICLES * Mountains of the World: Water Towers of the 21 st Century * Where does Macro meet Micro? REGIONAL UPDATES * Africa * Asia Pacific * Europe * Latin America * North America * Global WHAT'S NEW-Global Information <b>Server</b> <b>Node</b> MOUNTAIN CALENDAR * Mountain Forum IFC Meeting Report * Membership Directory * Documents Available in On-line Librar...|$|E
40|$|Abstract. In {{order to}} realize the {{parameters}} estimation of shared plants by the <b>server</b> <b>node</b> in the Internet Based Shared Control System. We propose a new method that can online identify the model parameters through Internet Based Shared Control System under the time-varying and time-delay environment. By applying the Sampling Information Processor to {{the course of the}} shared plant online identification, we can decrease the influence of network factor in the online identification system. Taking out all ranks state variable of shared plant’s model by identification algorithm processor in <b>server</b> <b>node,</b> we realize the online identifying the model parameters of the remote terminal. Numerical computer simulations are performed to compare our system ’ result with the existing system ’ result without influence of network factor. Results show that our method is valid. Lastly, our method realizes to identify the model parameters of the controlled object in different region under the network environment, and extends the applications of the online identification system...|$|E
5000|$|Technology {{architecture}}/Technical architecture or infrastructure architecture: The {{structure and}} behaviour {{of the technology}} infrastructure. Covers the client and <b>server</b> <b>nodes</b> of the hardware configuration, the infrastructure applications that run on them, the infrastructure services they offer to applications, the protocols and networks that connect applications and nodes.|$|R
40|$|The server array {{is a novel}} {{video server}} {{architecture}} based on partitioning each video over multiple <b>server</b> <b>nodes,</b> thereby achieving perfect load balancing for any demand distribution. We discuss the main design issues, compute the buffer requirements at the client, and compare the reliability of different video server architectures...|$|R
40|$|Distribution {{of large}} data objects among several storage servers {{is a common}} {{technique}} to speed up access rates. In combination with parity schemes, failures of single <b>server</b> <b>nodes</b> can be tolerated, so that such systems reach {{a certain degree of}} fault tolerance. In this paper such a distributed server system is analyzed. Data objects are stored in a data layout according to RAID level 3 among disk subsystems of different computers. An access control provides concurrent up- and down-streaming of data objects to/from the distributed storage system with ensured data consistency. This consistency control is described in combination with the handling of faulty <b>server</b> <b>nodes</b> and faulty clients. Furthermore, performance is measured with several access patterns. An application of that technique is for instance a distributed video server, allowing permanently updates without interrupting access...|$|R
