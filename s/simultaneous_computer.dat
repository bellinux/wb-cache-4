9|13|Public
5000|$|Wars - A frenzied {{multiplayer}} mode, supporting {{up to four}} <b>simultaneous</b> <b>computer</b> opponents, or five human players {{when using}} a multitap. Players damage {{each other as they}} complete chains, with the [...] "last devil standing" [...] becoming the winner.|$|E
5000|$|Apocalypticism was {{especially}} evident with {{the approach of}} the millennial year 2000, in which <b>simultaneous</b> <b>computer</b> crashes caused by uncorrected instances of the Y2k bug were expected to throw global commerce and financial systems into chaos. Piggy-backing on these issues, and probably driven by the [...] "interesting date" [...] unsupported allegations of an apocalypse were common.|$|E
40|$|Rapid, gravity-driven {{flows of}} {{granular}} materials down inclines pose {{a challenge to}} our understanding. Even in situations in which the flow is steady and two-dimensional, {{the details of how}} momentum ad energy are balanced within the flow and at the bottom boundary are not well understood. Thus we have undertaken a research program integrating theory, computer simulation, and experiment that focuses on such flows. the effort involves the development of theory informed by the results of <b>simultaneous</b> <b>computer</b> simulations and the construction, instrumentation, and use of an experimental facility in which the variables necessary to assess {{the success or failure of}} the theory can be measured. A goal of the project is to provide a sound theoretical and experimental base for a better understanding of the behavior and properties of multiphase flow and solid transport...|$|E
5000|$|... #Caption: A 50-port {{plug-and-play}} Ethernet switch, {{which can}} provide network and internet access {{to up to}} 50 <b>simultaneous</b> <b>computers</b> or consoles.|$|R
5000|$|The 8750 {{had from}} 91 to 3000 {{telephone}} extensions; up to 1000 <b>simultaneous</b> conversations; a <b>computer</b> {{based on a}} Motorola 68020; up to 16 IBM 8755 Operator Consoles; a 30MB fixed disk; main/satellite working with IBM 3750 and 1750 Switching Systems; digital trunks in Belgium, Italy and the UK; and ISDN and IBM Systems Network Architecture (SNA) networks.|$|R
40|$|In {{the article}} {{the method of}} {{acceleration}} of calculations {{at the level of}} the <b>simultaneous</b> processing of <b>computer</b> words in the systems, guided the flow of data is offered. The structure of the system, allowing the simultaneous forming and execution of a few instructions, is considered. Possibility of automatic identification of words of actors and dates on the basis of graph of task is shown. ? ?????? ????????? ????? ????????? ?????????? ?? ?????? ???????????? ????????? ???????? ???? ? ????????, ??????????? ??????? ??????. ??????????? ????????? ???????, ??????????? ????????????? ???????????? ? ?????????? ?????????? ??????. ???????? ??????????? ?????????????? ????????????? ???? ??????? ? ?????? ?? ?????? ????? ??????...|$|R
40|$|Rapid, gravity-driven {{flows of}} {{granular}} materials down inclines pose {{a challenge to}} our understanding. Even in situations in which the flow is steady and two-dimensional, {{the details of how}} momentum and energy are balanced within the flow and at the bottom boundary are not well understood. Thus we have undertaken a research program integrating theory, computer simulation, and experiment that will focus on dense entry flows down inclines. The effort involves the development of theory informed by the results of <b>simultaneous</b> <b>computer</b> simulations and the construction, instrumentation, and use of an experimental facility in which the variables necessary to assess {{the success or failure of}} the theory can be measured. In the present reporting period, we have completed the experiments with the bumpy base consisting of random two-dimensional packings of l nun glass spheres; we have derived boundary conditions for a bumpy frictionless boundary for other than small slip velocities; and we have made numerical studies of hydraulic equations using a simple Lagrangian code that we have developed. We are now in the process of writing the final report and a journal article summarizing our findings...|$|E
40|$|Rapid, gravity-driven {{flows of}} {{granular}} materials down inclines pose {{a challenge to}} our understanding. Even in situations in which the flow is steady and two-dimensional, {{the details of how}} momentum and energy are balanced within the flow and at the bottom boundary are not well understood. Thus we have undertaken a research program integrating theory, computer simulation, and experiment that will focus on dense entry flows down inclines. The effort involves the development of theory informed by the results of <b>simultaneous</b> <b>computer</b> simulations and the construction, instrumentation, and use of an experimental facility in which the variables necessary to assess {{the success or failure of}} the theory can be measured. In the present reporting period, we have continued a series of measurements in the chute facility with a flat, frictional boundary. At several inclinations between 15. 5 {degrees} and 20 {degrees}, and at several gate openings for each angle, we have measured mass flow rate and mass holdup, as well as granular temperature and collision frequency at the bottom wall of the chute. By recording simultaneously the collisional normal stress at the bottom wall and the mass holdup above it, the experiments reveal the fraction of the weight of particles that is supported by direct impact...|$|E
40|$|Extended truss {{structures}} are currently {{used as the}} load-carrying frame in development of orbital stations. These are unique and expensive objects with a guaranteed operating period of several decades. In the flight mode such a structure is usually exposed {{to a range of}} dynamic and thermal factors, namely thermal cycling, shocks at docking with manned vehicles, bending moments and torques at maneouvring, collision with space particle, moving at a high velocity, pipeline corrosion, etc. World experience of operation of various unique structures, operating under extreme conditions, assumes the availability of modern highly efficient and reliable NDT methods of quality control of welded elements during their fabrication, service and after performance of repair work [1, 2]. Reference [3] proposes going over from individual methods of inspection to integrated application of several methods of inspection of the same zone of the object. As a result, the inspection validity and recognition of the kinds of defects and accuracy of measurement of defect sizes, are improved. Combinations of different physical methods of control were selected, allowing for the specifics of space objects. A non-destructive testing system is proposed, which includes the eddy current and visual-optical methods with <b>simultaneous</b> <b>computer</b> processing of testing results, produced both by each of the methods and by the system as a whole. Such a computer system, which combined the above methods, was implemented at the E. O. Paton Electri...|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThis work investigates {{the viability of}} Gold code phase modulation in acoustic tomography, a technique for large scale measurement of ocean characteristics. Maximal-length sequences are currently used for modulation, requiring time division multiplexing of tomographic signals to avoid interference. The proposed alternative scheme of code division multiplexing Gold code modulated signals promises more rapid, <b>simultaneous</b> ocean projections. <b>Computer</b> simulation enables side-by-side comparison of the Gold code and maximal-length sequence modulating methods. Based on favorable results, {{a specific set of}} Gold codes is recommended for future use in a tomography experiment. [URL] United States Nav...|$|R
30|$|Deep {{learning}} [33] based {{methods are}} widely used in machine learning and make machine learning approach its original goal: artificial intelligence (AI). This is because deep learning takes an example by neocortex, {{which is the most}} powerful learning machine as far as we know. Deep learning learns hierarchical representation, and the higher layer represents increasingly abstract concepts and is increasingly invariant to transformations and scales. Recent advances [26] in deep learning application, such as <b>computer</b> <b>simultaneous</b> interpretation by Microsoft, have proven that deep learning is feasible in many machine learning tasks and can achieve great performance, so that more and more experts from the AI area believe that deep learning gives machine learning the second wave.|$|R
40|$|This {{article is}} a {{continuation}} on the sensitive scale of cantonal analysis, of research on statistics for recruitment in France during the Restoration. The <b>simultaneous</b> use of <b>computer</b> mapping and factorial analysis {{makes it possible to}} reproduce two essential axes which concern the anthropological and socio-economic characteristics of French male youth : indeed, an opposition emerges between the developed region of the North-East and the less developed zones of the Center, West and South. Moreover, the predictable contrast between the seaboard and the interior turns out to be completely independent of the first opposition. Le Roy Ladurie Emmanuel, Demonet Michel, Dumont Paul. Anthropologie de la jeunesse masculine en France au niveau d'une cartographie cantonale (1819 - 1830). In: Annales. Économies, Sociétés, Civilisations. 31 ᵉ année, N. 4, 1976. pp. 700 - 760...|$|R
40|$|The paper {{deals with}} {{questions}} of protection against unauthorized access to the data transmitted between unmanned aircraft vehicle (UAV) and ground control station (GCS). This {{is due to the}} fact that standard instruments of network security sometimes provide security of not enough proper level or do not satisfy the restrictions connected with the features of UAV: limited computing resources of the UAV on-board computer and real-time operation. We have offered to use Vernam cipher (one-time pad) as an additional measure for data protection. Vernam algorithm combines such advantages as theoretically proved absolute cryptographic security, ease of implementation, high speed of encryption and low processor load. It is especially important for large volumes of data encryption (e. g. video information). Within the bounds of experimental researches the technique is used based on cryptographic gamma of block cipher GOST 28147 - 89 in the Cipher Feedback Mode for a one-time pad generation. Application of Vernam cipher means the deletion of used one-time pad pages. The replacement of used one-time pad pages by cipher text is proposed for assurance of the above-named requirement and for <b>simultaneous</b> <b>computer</b> memory saving. It gives additionally the opportunity not only to save key sequences but also to accumulate encrypted data in the on-board computer memory. Realization of the offered method allows increasing the data protection level without engaging large computing power and memory capacity...|$|E
40|$|On-line {{monitoring}} {{devices to}} control {{functions such as}} volume, body temperature, and ultrafiltration, were considered more toys than real tools for routine clinical application. However, bio-feedback blood volume controlled hemodialysis (HD) is now possible in routine dialysis, allowing the delivery of a more physiologically acceptable treatment. This system has proved to {{reduce the incidence of}} intra-HD hypotension episodes significantly. Ionic dialysance and the patient&#x 2032;s plasma conductivity can be calculated easily from on-line measurements at two different steps of dialysate conductivity. A bio-feedback system has been devised to calculate the patient&#x 2032;s plasma conductivity and modulate the conductivity of the dialysate continuously in order to achieve a desired end-dialysis patient plasma conductivity corresponding to a desired end-dialysis plasma sodium concentration. Another bio-feedback system can control the body tempe-rature by measuring it at the arterial and venous lines of the extra-corporeal circuit, and then modulating the dialysate temperature in order to stabilize the patients&#x 2032; temperature at constant values that result in improved intra-HD cardiovascular stability. The module {{can also be used to}} quantify vascular access recirculation. Finally, the <b>simultaneous</b> <b>computer</b> control of ultrafiltration has proven the most effective means for automatic blood pressure stabilization during hemo-dialysis treatment. The application of fuzzy logic in the blood-pressure-guided biofeedback con-trol of ultrafiltration during hemodialysis is able to minimize HD-induced hypotension. In con-clusion, online monitoring and adaptive control of the patient during the dialysis session using the bio-feedback systems is expected to render the process of renal replacement therapy more physiological and less eventful...|$|E
40|$|In {{recent years}} {{significant}} {{research has been}} carried out aimed at developing a fundamental understanding of the phenomena involved in the transport of mixtures in nanoporous systems, such as adsorbents and membranes, which are crucial to many industrial separation processes. Carbon molecular-sieve membranes (CMSM) were the key focus early on in our DOE/BES-supported investigations. They are thought to be more stable and versatile than polymeric membranes, and capable of operating at higher temperatures, up to 300 C. In our research the emphasis was on understanding the factors determining the ability of the CMSM to separate mixtures based on differences in molecular mobility, and in affinity to the pore surface. Our study involved: (1) the preparation and characterization of the CMSM; (2) the computational modeling of their structure, and (3) the measurement and computer simulations of sorption and transport of mixtures through the membranes. The membranes developed are currently undergoing field-testing by Media & Process Technology (M & PT), our industrial collaborators in the project. In this research project we adopted the methodology and tools developed with the nanoporous CMSM to the preparation of novel membranes and films made of SiC. Our efforts were motivated here by the growing interest in the hydrogen economy, which has necessitated the development of robust nanoporous films that can be used as membranes and sensors in high-temperature and pressure processes related to H{sub 2 } production. SiC is a promising material for these applications due to its many unique properties, such as high thermal conductivity, thermal shock resistance, biocompatibility, resistance in acidic and alkali environments, chemical inertness (e. g., towards steam, H{sub 2 }S, NH{sub 3 }, and HCl, of particular concern for H{sub 2 } production from biomass and coal), and high mechanical strength. Though the CMSM exhibit many similar good properties, they are themselves unstable in the presence of O{sub 2 } and steam at temperatures higher than 300 C (conditions typically encountered in reactive separations for H{sub 2 } production). Other inorganic membranes, like ceramic (e. g., alumina, silica, and zeolite) and metal (Pd, Ag, and their alloys) membranes have, so far, also proven unstable, in such high-temperature applications in the presence of steam and H{sub 2 }S. The preparation of SiC nanoporous membranes involves two important steps. First, the preparation of appropriate SiC porous supports, and second the deposition on these supports of crack- and pinhole-free, thin nanoporous SiC films. Our early research, in collaboration with M & PT, focused on the preparation of quality porous SiC substrates. Our recent efforts involved the deposition of thin nanoporous films on these substrates by the pyrolysis of pre-ceramic polymeric precursors. We have made substantial strides in this area (as discussed further in Section II) preparing hydrogen-selective membranes and films. The objective of the project was not only to advance the 'state-of-the-art' of preparing the SiC membranes and films, but also to significantly broaden our understanding of factors that determine the ability of the SiC materials to separate gas mixtures, based on differences in molecular mobility and molecule-pore surface interactions. It is only such an improved fundamental understanding that will lead to further substantial improvements in the techniques for preparing such materials. In our studies we proceeded along two paths: (1) the preparation and characterization of SiC membranes, and the computational modeling of their molecular structure, and (2) the measurement and <b>simultaneous</b> <b>computer</b> simulation of sorption and transport of mixtures through the membranes. Coupling experiments and simulations facilitated our efforts to relate the membrane's structure with its transport properties, and separation efficacy. This, in turn, enabled progress towards the long-term goal of first-principle molecular engineering and design of improved materials for adsorption and separation. Understanding the transport characteristics of gas mixtures in SiC membranes is a problem of technological significance, as well as challenging technical complexity. A number of important issues that our study addressed, which are generic to the area of transport of gas mixtures through nanoporous systems are given...|$|E
40|$|Broadband {{communications}} {{consists of}} the technologies and equipment required to deliver packet-based digital voice, video, and data services to end users. Broadband affords end users high-speed, always-on access to the Internet while affording service providers the ability to offer value-added services to increase revenues. Due {{to the growth of}} the Internet, there has been tremendous buildout of high-speed, inter-city communications links that connect population centers and Internet service providers (ISPs) points of presence (PoPs) around the world. This build out of the backbone infrastructure or core network has occurred primarily via optical transport technology. Broadband access technologies are being deployed to address the bandwidth bottleneck for the "last mile," the connection of homes and small businesses to this infrastructure. One important aspect of broadband access to the home is that it allows people to telecommute effectively by providing a similar environment as when they are physically present in their office: <b>simultaneous</b> telephone and <b>computer</b> access, high-speed Internet and intranet access for e-mail, file sharing, and access to corporate servers. Comment: 16 page...|$|R
5000|$|John Cullinane mentored {{a series}} of future {{entrepreneurs}} and software industry executives. One of the early executives was Andrew 'Flip' Filipowski, who later founded Platinum Technology, Inc.. Another was Robert Goldman who became the CEO of several public software companies including AICorp. Jon Nackerud was a co-founder of Relational Technology, Inc., formed to commercialize the Ingres database management system. Prior to becoming a public company in 1978, the company's name was changed to Cullinane Database Systems, Inc. The company changed its name again to Cullinet Software in 1983, partly because John Cullinane wanted to distance his name from the personal connection to the business when he turned the company over to Bob Goldman, and also in {{a nod to the}} importance of computer networking (as evidenced by the company's <b>simultaneous</b> acquisition of <b>Computer</b> Pictures, whose microcomputer-based desktop system linked to IDMS data). Joe McNay, a board member, was particularly important regarding the company's IPO, the first ever in the software products industry. Greylock purchased some shares from John Cullinane in 1977, less than a year before the company was to go public. It was to be the early foundation on which their Greylock's software technology investment prowess rested. It was Greylock’s first investment in a software company.|$|R
40|$|The lamella {{structural}} systems {{consists of}} a number of interconnecting units forming a lozenge-shaped grid pattern. Each unit which is twice the length of the side of a diamond is called a "lamella". Theoretically lamella construction may have any geometric surface. The lamella roofs have been used frequently for structures where a wide clear span, freedom of movement, and attractive ceiling are required. Lamella roofs are ideal for prefabricated construction, as all the units are of standard size. Lamella structures can be constructed in steel, timber, aluminium, and reinforced concrete. The stress distribution in a lamella system is remarkably consistent, since the numerous lamella units mutually brace each other at the joints. In the theoretical analysis six degrees of freedom are assumed for each joint. The method which was selected for the theoretical analysis was the "Stiffness Method"; employing the "F. T. Compiler Technique" for the solution {{of a large number of}} <b>simultaneous</b> equations. <b>Computer</b> programs were written for the most general case of a space structure (pin or rigidly connected) in ALGOL 60. The lamella barrel vault, Fig. (1), is considered as a rigidly connected structure, and theoretical analysis of its stress distribution was carried out to investigate: 1. The influence of longitudinal supports: for this purpose two cases of longitudinal support were taken into accounts a) Boundary joints fixed in position, b) Boundary joints fixed in position and direction and for both cases (a & b) two systems of symmetrical and anti-symmetrical loading were considered; 2. The influence of gable-stiffening: for this investigation six types of gable-stiffening, which represent the different kinds of gable-boundaries were studied; 3. A short-cut solution. For a long barrel vault two approaches were considered, which gave similar results: (i) - Analysis of central arch along the lamellas? (ii) - Analysis of half diamonds between two transverse sections over the entire span. Finally, an approximate method was used. The results for each of the above cases are discussed individually. Experimental investigations were carried out on the lamella barrel vault model, Fig (1), employing the "direct method of testing". The theoretical and experimental results were compared and certain conclusions drawn. Finally, general conclusions and the possibilities for future research were expounded...|$|R
40|$|Students with {{significant}} cognitive disabilities often struggle {{to express themselves}} with written language. This project {{examined the effects of}} combining <b>simultaneous</b> prompting and <b>computer</b> assisted instruction, including picture icons, to teach students {{with significant}} cognitive disabilities to create narrative stories. Participants included seven students with significant disabilities currently being taught in a self-contained special education classroom located in a public elementary school. Participants had educational classifications of autism spectrum disorder, intellectual disability, or multiple disabilities. A multiple probe across behaviors design was used and instructors measured the number of sentences written by participants in each computer session. Sentences were defined as a group of words that contain at least a subject and a verb and were linked in a logical way. First, a simultaneous prompting intervention was used to teach participants to copy simple three-sentence stories using the PixwriterTM assisted writing program on a preferred topic. When the participants could do this, they were given a two-sentence story to copy and then asked to generate a final sentence that would be cohesive with the story. This process was to be repeated across three preferred topics. Participants in this study had varying results with this intervention. Six of the participants with significant cognitive skills got to practice their own narrative story writing skills. Only one participant completed the full intervention. One participant did not get past copying the prepared stories. All of the participants enjoyed listening to the computer as it read the stories that they copied or helped to create...|$|R
40|$|Graduation date: 1979 Best scan {{available}} for figures 1 & 2. Original {{is a black}} and white photocopy. Field observations are made of the formation of backwash ripples on the beachface, formed by undular hydraulic jumps generated by backwash down the beach face colliding with wave bores. Measured ripple wavelengths range from set averages of 48 to 70 cm. Within a particular set of ripples there is a tendency for the spacing to decrease in the offshore direction. These field observations are compared with laboratory experiments where undular jumps are generated in a flume, and with a computer simulation model which models both the flow within an undular hydraulic jump and the resulting sediment transport which gives rise to the backwash ripples. The computer model involves a numerical solution of the Boussinesq equations which govern the fluid flow, and sediment transport equations which relate the sand transport rate to the local mean flow velocity. The computer model permits a study of the detailed time-history of the undular jump development and the formation of the backwash ripples. This model shows good agreement with the field observations of backwash ripples, predicting an offshore decrease in their spacing as observed. The laboratory experiments showed a similar result so long as the Froude number of the supercritical flow before the jump occurs is small, on the order of 1. 4. Differences between the computer model and experiments were small and arose principally from the neglect of internal friction and surface tension in the model. The study demonstrates the usefulness of the <b>simultaneous</b> application of <b>computer</b> simulation models and laboratory experiments to understand complex flow and sediment transport conditions such as occur on beaches...|$|R
40|$|When the Supreme Court {{made one}} person, one vote, {{the law of}} the land, the art of ger-rymandering enjoyed a renaissance. The <b>simultaneous</b> advent of <b>computers</b> turned an old art into a new technology; yet the same technology, guided by an {{understanding}} of the complicated legal, historical, statistical, technical, methodological, and normative issues, may be employed to detect gerrymanders. Here we discuss the capabilities and limitations of common software and hardware in gerrymander construction and detec-tion, and present the framework of a general strategy for using computers in the field of apportionment. Keywords: artificial intelligence, BASIC, gerrymander, malappor-tionment, mapping, PROLOG vote dilution, microcomputers. As the 1980 s draw to a close, the decennial specter of reapportionment begins to stalk the land. Since the earliest years of the Republic, politi-cians have had a love-hate relationship with the opportunities for re-warding friends and baffling foes that American electoral law poses. In the 1960 s, a series of Supreme Court decisions (e. g., Baker v. Carr, 1962; Reynolds v. Simms, 1964) outlawed malapportionment; the basic rule in the United States now is one person, one vote. However, historical and technological developments have led to a new focus on gerryman-dering, the artful practice of geographically &dquo;stacking&dquo; or &dquo;cracking&dquo; one’s political foes and, in the absence of changes to malapportion, the last refuge of manipulators of the electoral process. Formally, Gerry-mandering is allocation of voters to districts such that the political con-sequences, by their statistical improbability, demonstrate bias or self-serving allocation. Legally, gerrymandering is strategic use of space to dilute votes. The manipulators are an unlikely coalition of incumbents and chal-lengers, unified by a common desire to create seats that are safe from swings of opinion in the electorate. Their actions place an enormous burden on the courts and cast in doubt the legitimacy of the electoral system; more and more voters see the outcome of elections determine...|$|R
40|$|The past 35 {{years in}} {{biological}} systematics {{have been a}} time of remarkable philosophical and methodological developments. For nearly a century after Darwin 2 ̆ 7 s Origin of Species, systematists worked to understand the diversity of nature based on evolutionary relationships. Numerous concepts were presented and elaborated upon, such as homology, parallelism, divergence, primitiveness and advancedness, cladogenesis and anagenesis. Classifications were based solidly on phylogenetic concepts; they were avowedly monophyletic. Phenetics emphasized the immense challenges represented by phylogeny reconstruction and advised against basing classifications upon it. Pheneticists forced reevaluation of all previous classificatory efforts, and objectivity and repeatability in both grouping and ranking were stressed. The concept of character state was developed, and numerous debates focused on other concepts, such as unit character, homology, similarity, and distance. The <b>simultaneous</b> availability of <b>computers</b> allowed phenetics to explore new limits. Despite numerous positive aspects of phenetics, the near absence of evolutionary insights led eventually to cladistics. Drawing directly from phenetics and from the Hennigian philosophical school, cladistics evolved as an explicit means of deriving branching patterns of phylogeny and upon which classifications might be based. Two decades of cladistics have given us: refined arguments on homology and the evolutionary content of characters and states, views of classifications as testable hypotheses, and computer algorithms for constructing branching patterns of evolution. In contrast to the phenetic movement, which was noteworthy for seeking newer concepts and methods, even including determining evolutionary relationships (which led eventually to numerical cladistics), many cladists have solidified their approaches based on parsimony, outgroups, and holophyly. Instead of looking for newer ways to represent phylogeny, some cladists have attempted to use branching patterns: (1) as a strict basis for biological classification and nomenclature and (2) to explain the origin of biological diversity even down to the populational level. This paper argues that cladistics is inappropriate to both these goals due to: (1) inability of branching patterns to reveal all significant dimensions of phylogeny; (2) acknowledged patterns of reticulate evolution, especially in flowering plants; (3) documented nonparsimonious pathways of evolution: and (4) nondichotomous distribution of genetic variation within populations. New concepts and methods of reconstructing phylogeny and developing classifications must be sought. Most important is incorporation of genetic-based evolutionary divergence within lineages for purposes of grouping and ranking...|$|R

