3|51|Public
40|$|Until the {{experiments}} conducted on Single Sideband <b>speech</b> <b>clipping</b> {{in the early}} 1960 's, very little work had {{been done in the}} field of conventional speech processing or clipping since the late 1940 's. Although the SSB experiments were mainly motivated by the unique repeaking problem associated with clipped SSB signals, they pointed out that further improvements in intelligibility of clipped speech were still possible. Perhaps of more importance, these experiments offered experimental evidence that the problem of intelligibility of clipped speech was closely related to the intermodulation distortion produced in the clipping process. This paper is a study of clipped speech as viewed from this point. The relation of the intermodulation distortion to the intelligibility of clipped speech is developed, and it is shown that intelligibility can be enhanced by reduction of the intermodulation products. [URL] United States Marine Corp...|$|E
40|$|Speech {{intelligibility}} of {{two types}} of vocoders was measured using the modified rhyme test. One type of vocoder, a continuous variable slope delta (CVSD), was a waveform encoder. The other type, an advanced multi-band excitation (AMBE), was a parametric encoder. In the first experiment, clear speech was processed through the vocoders. Intelligibility was measured in a control condition, i. e. without vocoding, with each type alone and with two vocoders in tandem. AMBE and CVSD performed similarly, 92. 6 and 90. 4 %, respectively. CVSD-to-AMBE {{had little effect on}} intelligibility, measured at 89. 2 %. However, AMBEto-CVSD had a large degrading effect on intelligibility. The AMBE-to-CVSD direction scored about 81. 7 % intelligibility with clear, unaltered speech signals. The asymmetry between waveform-to-parametric and parametric-to-waveform encoders underscores the non-linear nature of tandem vocoders on intelligibility. When vocoders of the same type were in tandem, there was no additional effect on intelligibility. The double CVSD condition yielded 92. 2 % intelligibility and the double AMBE condition yielded 91 %. The deleterious effects of <b>speech</b> <b>clipping</b> were measured in a second experiment, as these are ubiquitous in military radio transmission systems. The AMBE parametric vocoder performed at the 88 % level in isolation and at 84 % when tandemed with the CVSD waveform vocoder. Alternative methods of encoding speech signals are being explored to improve speech intelligibility performance in military communication systems...|$|E
40|$|In speech coding one {{can make}} use of the speech {{inactivity}} to reduce the average bit-rate of the encoded signal. This demands a process commonly referred to as Voice Activity Detection (VAD) that separates the speech frames from the frames that only contains background noise. The purpose of the VAD is to tell the speech encoder to stop or reduce the data flow when no speech is present. The goal with such a process is to lower the average bit-rate without affecting the perceived speech quality. This work is an investigation and evaluation of possible improvements of the voice activity detector in the Adaptive Multirate Wideband (AMR-WB) speech coder. The purpose of the work was to reduce the sensitivity to babble background noise and improve the performance for detection of music. In the report there is a brief introduction to the theory of speech coding and VAD followed by the outline of the AMR-WB speech coder. The main part of this thesis discusses possible improvements of the detector starting with recent findings in the Adaptive Multirate Narrowband (AMR-NB) algorithm. Based on the limited material used for evaluation in this work the modifications proposed for the AMR-NB VAD showed good results also for AMR- WB. It turned out however that additional modifications should be done in order to ensure reliable detection of high level non-stationary noises. A music hangover solution was also suggested for better handling of music when the suggested modifications are implemented. The solution suggested for reduction of the sensitivity to babble noises offers a compromise between voice activity and <b>speech</b> <b>clipping</b> that can be tuned to desired performance. The results and conclusions in this thesis are based on objective tests of limited material and contain no formal subjective testing. The conclusions should therefore be treated as guidance for further studies but indicates that the solutions proposed will reduce the AMR-WB VADs sensitivity to non- stationary background noises. Validerat; 20101217 (root...|$|E
5000|$|MSC (Mid <b>Speech</b> Clipping): <b>clipping</b> due to <b>speech</b> misclassified as noise; ...|$|R
2500|$|An {{important}} aspect of Klingon grammar is its [...] "ungrammaticality". As with for example Japanese, shortening of communicative statements is common, and is called [...] "Clipped Klingon" [...] (tlhIngan Hol poD or, more simply, Hol poD) and Ritualized <b>Speech.</b> <b>Clipped</b> Klingon is especially useful in situations where speed is a decisive factor. Grammar is abbreviated, and sentence parts deemed to be superfluous are dropped. Intentional ungrammaticality is widespread, and it takes many forms. It is exemplified by the practice of pabHa’, which Marc Okrand translates as [...] "to misfollow the rules" [...] or [...] "to follow the rules wrongly".|$|R
2500|$|Chef {{makes an}} {{appearance}} in the 2014 video game [...] as the reanimated Nazi Zombie penultimate boss later in the game. Once again, voice clips of Isaac Hayes as Chef are recycled from past episodes {{with the exception of}} recorded <b>speech</b> <b>clips</b> from Adolf Hitler. During his battle with the player and the other boys, Chef will express remorse by saying things like [...] "I'm sorry, boys" [...] and [...] "what have I done?" [...] While there were vague references to the Super Adventure Club and his death, there were no references to the Darth Chef suit.|$|R
5000|$|An {{important}} aspect of Klingon grammar is its [...] "ungrammaticality". As with for example Japanese, shortening of communicative statements is common, and is called [...] "Clipped Klingon" [...] ('''' or, more simply, '''') and Ritualized <b>Speech.</b> <b>Clipped</b> Klingon is especially useful in situations where speed is a decisive factor. Grammar is abbreviated, and sentence parts deemed to be superfluous are dropped. Intentional ungrammaticality is widespread, and it takes many forms. It is exemplified by the practice of '''', which Marc Okrand translates as [...] "to misfollow the rules" [...] or [...] "to follow the rules wrongly".|$|R
5000|$|Chef {{makes an}} {{appearance}} in the 2014 video game South Park: The Stick of Truth as the reanimated Nazi Zombie penultimate boss later in the game. Once again, voice clips of Isaac Hayes as Chef are recycled from past episodes {{with the exception of}} recorded <b>speech</b> <b>clips</b> from Adolf Hitler. During his battle with the player and the other boys, Chef will express remorse by saying things like [...] "I'm sorry, boys" [...] and [...] "what have I done?" [...] While there were vague references to the Super Adventure Club and his death, there were no references to the Darth Chef suit.|$|R
5000|$|The game {{uses the}} 192x64 [...] "supersize" [...] dot matrix display with a Motorola 68000-based 16-bit controller. It {{features}} several electric-green wireform ramps with the [...] "Batcave" [...] escape ramp extending {{down behind the}} flippers and over the playfield apron, releasing balls up the playfield during multi ball. The [...] "Batwing" [...] cannon rotates & aims across the playfield and fires the ball with a pistol grip on front of the machine. <b>Speech</b> <b>clips</b> from the film are used. Additionally, the game had a video mode in which the Batwing is guided over rooftops, dodging obstacles to earn bonus points.|$|R
40|$|This paper {{deals with}} the problem of peak <b>clipped</b> <b>speech.</b> Our basic {{assumption}} is that the <b>clipped</b> <b>speech</b> is voiced and can be linearly predicted with a high accuracy. The coefficients of linear prediction are computed using two different algorithms: a least square direct method and a recursive Kalman filter. The speech reconstruction is accomplished using backward predictio...|$|R
40|$|Abstract—We {{describe}} {{a database of}} static images and video clips of human faces and people that is useful for testing algorithms for face and person recognition, head/eye tracking, and computer graphics modeling of natural human motions. For each person there are nine static “facial mug shots ” {{and a series of}} video streams. The videos include a “moving facial mug shot, ” a facial <b>speech</b> <b>clip,</b> one or more dynamic facial expression clips, two gait videos, and a conversation video taken at a moderate distance from the camera. Complete data sets are available for 284 subjects and duplicate data sets, taken subsequent to the original set, are available for 229 subjects. Index Terms—Face database, face recognition, face tracking, digital video. ...|$|R
40|$|The {{transmission}} of speech through difficult channels is more easily accomplished if the complicated speech waveform can be simplified. One method of simplification is to "infinitely clip" the {{speech to a}} rectangular signal which preserves only the original zero-crossings. Early research with <b>clipped</b> <b>speech</b> and <b>clipped,</b> differentiated <b>speech</b> is extended to <b>clipped,</b> twice differentiated <b>speech</b> and an "enriched" mixture of two clipped derivatives of different order. The two new systems produce a higher quality signal than the earlier methods, the "enriched" speech approaching typical communications quality. Data is presented comparing the intelligibility of these four systems. A scheme for preserving simultaneously the exact times of both zeros and extremals is discussed in detail and experimentally investigated. The intelligibility of time-quantized, second-order <b>clipped.</b> <b>speech</b> is studied, and certain statistical parameters of the clipped signals measured. These data indicate: that a three-to-one reduction in bit rate over telephonic PCM is possible while still preserving tolerable intelligibility. ...|$|R
40|$|This paper {{presents}} a gender identification {{system to be}} used for call forwarding in health related communications. The system listens to the caller then using speech synthesis, image processing, and linear support vector machine SVM identifies either he or she is a male or a female. This solution is imperative in a conservative country such as the Kingdom of Saudi Arabia in order to forward the call to a male or female practitioner. The originality of the approach is that no transcription is used to learn SVM models. To identify the gender of the caller, the trained SVM model of the reference pieces are compared to transcripts of the audio frequency record and are using the Levenshtein distance. For the identification of gender, we obtain an accuracy rate of 94 % on a test flow containing 449 pieces of <b>speech</b> <b>clips.</b> </p...|$|R
40|$|We {{investigated}} whether {{changing the}} frame {{rate at which}} <b>speech</b> video <b>clips</b> were presented (6 - 30 frames per second, fps) would affect audiovisual temporal perception. Participants made unspeeded temporal order judgments (TOJs) regarding which signal (auditory or visual) was presented first for video clips presented at a range of different stimulus onset asynchronies (SOAs) using the method of constant stimuli. Temporal discrimination accuracy was unaffected by changes in frame rate, while lower frame rate <b>speech</b> video <b>clips</b> required larger visual-speech leads for the point of subjective simultaneity (PSS) to be achieved than did higher frame rate video clips. The significant effect of frame rate on temporal perception demonstrated here has not been controlled for in previous studies of audiovisual synchrony perception using video stimuli and is potentially important given the rapid increase {{in the use of}} audiovisual videos in cognitive neuroscience research in recent years...|$|R
40|$|We {{present a}} novel {{algorithm}} for structural analysis of audio to detect repetitive patterns that {{are suitable for}} content-based audio information retrieval systems, since repetitive patterns can provide valuable information {{about the content of}} audio, such as a chorus or a concept. The Audio Spectrum Flatness (ASF) feature of the MPEG- 7 standard, although not having been considered as much as other feature types, has been utilized and evaluated as the underlying feature set. Expressive summaries are chosen as the longest patterns by the k-means clustering algorithm. Proposed approach is evaluated on a test bed consisting of popular song and <b>speech</b> <b>clips</b> based on the ASF feature. The well known Mel Frequency Cepstral Coefficients (MFCCs) are also considered in the experiments for the evaluation of features. Experiments show that, all the repetitive patterns and their locations are obtained with the accuracy of 93 % and 78 % for music and speech, respectively. 1...|$|R
40|$|There {{are many}} types of {{degradation}} which can occur in Voice over IP (VoIP) calls. Of interest in this work are degradations which occur independently of the codec, hardware or network in use. Specifically, their effect on the subjective and objec- tive quality of the speech is examined. Since no dataset suit- able for this purpose exists, a new dataset (TCD-VoIP) has been created and has been made publicly available. The dataset con- tains <b>speech</b> <b>clips</b> suffering {{from a range of}} common call qual- ity degradations, as well as a set of subjective opinion scores on the clips from 24 listeners. The performances of three ob- jective quality metrics: POLQA, ViSQOL and P. 563, have been evaluated using the dataset. The results show that full reference metrics are capable of accurately predicting a variety of com- mon VoIP degradations. They also highlight the outstanding need for a wideband, single-ended, no-reference metric to mon- itor accurately speech quality for degradations common in VoIP scenarios...|$|R
40|$|Spatial {{release from}} masking is {{traditionally}} measured with speech in front. The effect of head-orientation {{with respect to}} the speech direction has rarely been studied. Speech-reception thresholds (SRTs) were measured for eight head orientations and four spatial configurations. Benefits of head orientation away from the speech source of up to 8 [*]dB were measured. These correlated with predictions of a model based on better-ear listening and binaural unmasking (r[*]=[*] 0. 96). Use of spontaneous head orientations was measured when listeners attended to long <b>speech</b> <b>clips</b> of gradually diminishing speech-to-noise ratio in a sound-deadened room. Speech was presented from the loudspeaker that initially faced the listener and noise from one of four other locations. In an undirected paradigm, listeners spontaneously turned their heads away from the speech in 56 % of trials. When instructed to rotate their heads in the diminishing speech-to-noise ratio, all listeners turned away from the speech and reached head orientations associated with lower SRTs. Head orientation may prove valuable for hearing-impaired listeners...|$|R
40|$|Whenever {{we listen}} to a voice for the first time, we {{attribute}} personality traits to the speaker. The process {{takes place in a}} few seconds and it is spontaneous and unaware. While the process is not necessarily accurate (attributed traits do not necessarily correspond to the actual traits of the speaker), still it significantly influences our behavior toward others, {{especially when it comes to}} social interaction. This paper proposes an approach for the automatic prediction of the traits the listeners attribute to a speaker they never heard before. The experiments are performed over a corpus of 640 <b>speech</b> <b>clips</b> (322 identities in total) annotated in terms of personality traits by 11 assessors. The results show that it is possible to predict with high accuracy (more than 70 percent depending on the particular trait) whether a person is perceived to be in the upper or lower part of the scales corresponding to each of the Big -Five, the personality dimensions known to capture most of the individual differences...|$|R
40|$|Abstract—Whenever {{we listen}} to a voice for the first time, we {{attribute}} personality traits to the speaker. The process {{takes place in a}} few seconds and it is spontaneous and unaware. While the process is not necessarily accurate (attributed traits do not necessarily correspond to the actual traits of the speaker), still it significantly influences our behavior toward others, {{especially when it comes to}} social interaction. This paper proposes an approach for the automatic prediction of the traits the listeners attribute to a speaker they never heard before. The experiments are performed over a corpus of 640 <b>speech</b> <b>clips</b> (322 identities in total) annotated in terms of personality traits by 11 assessors. The results show that it is possible to predict with high accuracy (more than 70 percent depending on the particular trait) whether a person is perceived to be in the upper or lower part of the scales corresponding to each of the Big Five, the personality dimensions known to capture most of the individual differences. Index Terms—Personality traits, prosody, Big Five, social signal processing, automatic personality perception Ç...|$|R
5000|$|The Game Boy Color {{version of}} MK4 was {{developed}} by Digital Eclipse and released by Midway. It is in 2D instead of the others' 3D. It features nine selectable characters: Raiden, Quan Chi, Fujin, Liu Kang, Sub-Zero, Reiko, Tanya, Scorpion, and the hidden character Reptile; Shinnok is still the final opponent. In addition, {{there are a few}} <b>speech</b> <b>clips,</b> and instead of using the in-game graphics for the Fatalities, the game uses short FMV clips. The Game Boy Color port's 2D engine reuses the game engine used in the Game Boy port of MK3, including the same character select screen, [...] "Choose Your Destiny" [...] screen, and how the characters move and interact. The background music was replaced with repetitive songs that bore instrumentation befitting a Game Boy release, and the port does not contain any blood outside of the Fatality videos. The combo system and weapons were also removed. However, the graphics for the port were above average for a Game Boy fighting title, and it contains many of the arcade's backgrounds in vivid clarity.|$|R
50|$|It {{was found}} that the analog ear with its {{asymmetric}} overlapping bands was more reliable in identifying speech sounds than is a conventional frequency spectrum. The second formant is the most significant single measure. Speech sounds of interest include whispered and <b>clipped</b> <b>speech.</b>|$|R
5|$|The Season Seven DVD {{contains}} {{a number of}} deleted scenes from this episode. Notable cut scenes include Michael talking about his last Dundies using comedians as metaphors, Dwight booing Deangelo in order to adequately prepare him for his actual <b>speech,</b> various <b>clips</b> from the actual Dundies awards and of people accepting their awards, a cut scene featuring a video of Michael interviewing Danny Cordray (Timothy Olyphant) at his house, and Andy talking to Erin about loading the printer {{so that she can}} take her mind off of Gabe.|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedPeak clipping {{is a well}} known method of increasing the average power of a peak power limited voice communication transmitter. Although the clipping process introduces distortion, articulation tests have shown that <b>clipped</b> <b>speech</b> remains highly intelligible. Using idealizations of vowel sounds based on the mechanism of speech production, calculations were made of the spectra resulting from clipping these speech-like signals. The results indicate {{a high degree of}} similarity between the spectra before and after clipping. The power gained by clipping at audio frequency and at narrowband was calculated and compared with previously published data. Repeaking due to component rejection was investigated for clipping at audio and narrowband. Calculations of the effect of varying the phase characteristic of the signals before clipping indicate that such variation may improve the intelligibility of <b>clipped</b> <b>speech.</b> [URL] United States Nav...|$|R
5000|$|Although {{the method}} {{described}} above provides useful objective {{information concerning the}} performance of a VAD, it is only an approximate measure of the subjective effect. For example, the effects of <b>speech</b> signal <b>clipping</b> can at times be hidden by the presence of background noise, depending on the model chosen for the comfort noise synthesis, so some of the clipping measured with objective tests is in reality not audible. It is therefore important to carry out subjective tests on VADs, the main aim of which is to ensure that the clipping perceived is acceptable. This kind of test requires a certain number of listeners to judge recordings containing the processing results of the VADs being tested. The listeners have to give marks on the following features: ...|$|R
5000|$|Indiana Jones and The Temple of Doom is a 1985 action arcade game {{developed}} {{and published by}} Atari Games, based on the 1984 film of the same name, the second film in the Indiana Jones franchise. [...] It is also the first Atari System 1 arcade game to include digitized <b>speech,</b> including voice <b>clips</b> of Harrison Ford as Indiana Jones and Amrish Puri as Mola Ram, as well as John Williams' music from the film.|$|R
40|$|Isolated {{cerebellar}} mass lesion is {{an uncommon}} presentation of toxoplasmosis. the authors report one rare {{case in a}} 50 -year-old HIV-infected male patient who presented with <b>clipped</b> <b>speech,</b> gait ataxia and incoordination. the cerebellar toxoplasmosis was suspected based on imaging findings, despite the atypical location. This case highlights {{the need for a}} high index of clinical suspicion among HIV-infected patients with neurological manifestations and suspicious neuroimaging findings. Fed Univ São Paulo UNIFESP, Div Infect Dis, São Paulo, BrazilFed Univ São Paulo UNIFESP, Div Infect Dis, São Paulo, BrazilWeb of Scienc...|$|R
40|$|In this paper, we {{describe}} an application that helps hearing-impaired {{students learn to}} read and form words by translating <b>speech</b> into video <b>clips</b> of American Sign Language (ASL). The narrator, either a parent or teacher, reads aloud to the student, and the application displays the ASL clip(s) along with the written word(s). The student is thus able to observe the speaker form the words and then see a sign they recognize, the written word, and an illustration {{of the meaning of}} the word. We have developed a prototype system along with several “albums ” containing the supporting multimedia...|$|R
50|$|One of {{the issues}} with using this type of {{technology}} was that the users listening on an idled channel can sometimes hear the conversation that has been switched onto it. Generally the sound heard was of very low volume and individual words are not distinguishable. See also crosstalk for a similar phenomenon in telecommunications. Another potential issue was ensuring that non-voice type circuits (e.g. Music or radio type circuits where pauses would occur infrequently) were not routed via TASI speech channels since these could seriously degrade the level of service where callers would encounter frequent <b>clipped</b> <b>speech</b> and breaks in the conversation.|$|R
40|$|We {{investigated}} {{the consequences of}} monitoring an asynchronous audiovisual speech stream on the temporal perception of simultaneously presented vowel-consonant-vowel (VCV) audiovisual <b>speech</b> video <b>clips.</b> Participants made temporal order judgments (TOJs) regarding whether the speech-sound or the visual-speech gesture occurred first, for video clips presented at various different stimulus onset asynchronies. Throughout the experiment, {{half of the participants}} also monitored a continuous stream of words presented audiovisually, superimposed over the VCV video clips. The continuous (adapting) speech stream could either be presented in synchrony, or else with the auditory stream lagging by 300 ms. A significant shift (13 ms {{in the direction of the}} adapting stimulus in the point of subjective simultaneity) was observed in the TOJ task when participants monitored the asynchronous speech stream. This result suggests that the consequences of adapting to asynchronous speech extends beyond the case of simple audiovisual stimuli (as has recently been demonstrated by Navarra et al. in Cogn Brain Res 25 : 499 - 507, 2005) and can even affect the perception of more complex speech stimuli...|$|R
40|$|International audienceWe {{propose a}} {{framework}} called audio inpainting {{for the general}} problem of estimating missing samples in audio. It extends {{the problem of the}} interpolation and extrapolation of signals to the cases where possibly-large blocks of consecutive samples must be estimated from the remaining, known samples. We relate this framework to a number of applications including declicking, declipping and audio packet loss in voice over IP. By considering audio inpainting as an inverse problem, we show that sparse representations are an appropriate scheme to develop new approaches for the audio inpainting problem. We will present some experiments, with a particular focus on restoration of <b>clipped</b> <b>speech</b> or music signals...|$|R
40|$|CONCLUSION ILAH, LSR and LILAH from [4] {{exceed the}} {{performance}} of the optimized baseline on unencoded <b>speech,</b> and at <b>clipping</b> levels between- 3 and- 14 dBFS on decoded speech, an important range in mobile telephony applications. Performance is maintained at SNRs down to 5 dB. The averaging nature of histogram approach in ILAH, and the spectral approach of LSR both provide robustness to noise. The baseline method with ε = 0. 71 (optimized) in decoded speech generates many FPs, and loses accuracy at high ODF because it does not adapt to each speaker, whilst the F 1 score improves at high ODF because it crudely labels all samples of significant amplitude as clipped...|$|R
50|$|Outside Dogpatch, {{characters}} used {{a variety}} of stock Vaudevillian dialects. Mobsters and criminal-types invariably spoke slangy Brooklynese, and residents of Lower Slobbovia spoke pidgin-Russian, with a smattering of Yinglish. Comic dialects were also devised for offbeat British characters — like H'Inspector Blugstone of Scotland Yard (who had a Cockney accent) and Sir Cecil Cesspool (whose <b>speech</b> was a <b>clipped,</b> uppercrust King's English). Various Asian, Latin, Native American and European characters spoke {{in a wide range of}} specific, broadly caricatured dialects as well. Capp has credited his inspiration for vividly stylized language to early literary influences like Charles Dickens, Mark Twain and Damon Runyon, as well as Old-time radio and the Burlesque stage.|$|R
50|$|Unfortunately, {{their design}} specs {{proved to be}} a little too {{accurate}} to the creatures they were modeled on, as their primitive brains went out of control, and Grimlock almost destroyed Teletraan I, before the trio was stopped. Optimus Prime deemed them too dangerous to use again, and had them sealed back up in the cavern, but when the majority of the Autobot force was captured by the Decepticons, Wheeljack freed them to go to their rescue. Equipped with new devices that enhanced their brainpower to functional, yet still impaired, levels (resulting in slow thought processes and simple, <b>clipped</b> <b>speech),</b> the three Dinobots successfully rescued their Autobot comrades, and Optimus Prime admitted his error.|$|R
40|$|We {{investigated}} {{whether the}} "unity assumption," {{according to which}} an observer assumes that two different sensory signals refer to the same underlying multisensory event, influences the multisensory integration of audiovisual speech stimuli. Syllables (Experiments 1, 3, and 4) or words (Experiment 2) were presented to participants at a range of different stimulus onset asynchronies using the method of constant stimuli. Participants made unspeeded temporal order judgments regarding which stream (either auditory or visual) had been presented first. The auditory and visual speech stimuli in Experiments 1 - 3 were either gender matched (i. e., a female face presented together with a female voice) or else gender mismatched (i. e., a female face presented together with a male voice). In Experiment 4, different utterances from the same female speaker were used to generate the matched and mismatched <b>speech</b> video <b>clips.</b> Measuring {{in terms of the}} just noticeable difference the participants in all four experiments found it easier to judge which sensory modality had been presented first when evaluating mismatched stimuli than when evaluating the matched-speech stimuli. These results therefore provide the first empirical support for the "unity assumption" in the domain of the multisensory temporal integration of audiovisual speech stimuli...|$|R
2500|$|With his <b>clipped</b> <b>speech,</b> cool {{smile and}} a {{cigarette}} dangling impudently from his lips, Laurence Harvey established himself as the screen's perfect pin-striped cad. He could project such utter boredom that willowy debutantes would shrivel in his presence. He could also exude such charm that the same young ladies would gladly lend him their hearts, which were usually returned utterly broken... The image Mr Harvey carefully fostered for himself off screen was not far removed {{from some of the}} roles he played. [...] "I'm a flamboyant character, an extrovert who doesn't want to reveal his feelings", he once said. [...] "To bare your soul to the world, I find unutterably boring. I think part of our profession is to have a quixotic personality." ...|$|R
50|$|The {{first episode}} {{begins with a}} cold open, the second serial to have a specially-shot pretitles {{sequence}} after Time and the Rani (1987), though Castrovalva (1982) began with a reprise of Logopolis (1981), and The Five Doctors (1983) featured a clip from The Dalek Invasion of Earth (1964) before the title sequence. Remembrance cold open features {{a shot of the}} Earth with audio clips from 1963, including excerpts of John F. Kennedy's American University speech and Martin Luther King, Jr.'s I Have a Dream <b>speech.</b> Many other <b>clips</b> from the early 60s were planned but did not make the final cut. Many songs from the time period can also be heard in the background during several scenes in the serial.|$|R
