6|21|Public
40|$|The Report is {{comprised}} of two parts, corresponding to two separate studies carried forth on the contract. One studyp treated in Part I, is concerned with a theoretical examination of the properties of a specific unsupervised learning technique (the Haralick-Darling technique) of interest to NASA-ERC. The other study, treated in Bart 11, is concerned with further examination of moment rela-tions in nonsupervised learning for parameterized distributions. Parts 1 and I 1 are each self-contained with no consistency between them in use of mathematical symbols {{or in any other}} way. ihi This Report is concerned with two separate topics investigated under this study on unsupervised machine learning and clustering, The first topic, the larger portion of the study, provides theoretical analysis of a particular <b>self-learning</b> <b>technique</b> [...] - the Haralick-Darling technique, Various characteristics of the technique are examined [...] - the decision procedure, behavioral properties based upon incremental per cycle analysis and decision constraints, and conver...|$|E
40|$|This paper {{deals with}} the {{modelling}} of a continuous cooling column crystallizer. An accurate model of this system is needed for complex process control. The investigated system consists on dextrose monohydrate in aqueous solution. An adaptive hybrid model is presented. The model consists of two parts: the phenomenological model, expressed {{by a set of}} differential algebraic equations, and a neural network, based on historical data, developed by the fuzzy ARMAP technique. The empirical part of the hybrid model is aimed at eliminating the deviations of the prediction of the phenomenological model caused mainly by incrustations over the surface of the cooling coils located along the column crystallizer. The model is adaptive since neural network parameters are updated by a self-learning system (SLS) based on the acquired process data storage of the DCS of an industrial plant. Firstly, the hybrid model was implemented by using the data of a three-month campaign of the crystallizer, then the <b>self-learning</b> <b>technique</b> was checked on site in a subsequent campaign. © 2010 The Institution of Chemical Engineers...|$|E
40|$|Traditional {{teaching}} and learning paradigms have been shaken by {{the impact of the}} integration of Information and Communication Technologies (ICT). Computer programmes are interactive and can illustrate a concept through attractive animation, sound and demonstration. They allow students to progress at their own pace and work individually or solve problems in a group. “Computer-assisted instruction” (CAI) refers to instruction or remediation presented on a computer. Many educational computer programmes are available online and from computer stores and textbook companies. They enhance teacher instruction in several ways. A <b>self-learning</b> <b>technique,</b> usually offline/online, involving interaction of the student with programmed instructional materials. Computer-assisted instruction (CAI) is an interactive instructional technique whereby a computer is used to present the instructional material and monitor the learning that takes place. CAI uses a combination of text, graphics, sound and video in enhancing the learning process. The computer has many purposes in the classroom and it can be utilised to help a student in all areas of the curriculum. Reading computer programmes demonstrate concepts, instruct and remediate student errors and misunderstandings from pre-school through college. In this article researchers try to define and show usage and capabilities of CAI in learning and teaching process with reference to the Canadian context...|$|E
40|$|In this article, {{we present}} a {{comprehensive}} review of various experiences with different <b>self-learning</b> <b>techniques</b> applied to the task of converting a graphemic string into the corresponding phonemic sequence. We also report some experiments carried out both with English words and French proper names. These experiments support the view that taking full advantage of the huge pronunciation dictionaries that we have been developing during the ONOMASTICA project is possible only if the traditional understanding of grapheme-tophoneme conversion as a classification problem is questioned...|$|R
40|$|Electronic {{learning}} (e-learning) {{systems have}} gained widespread acceptance {{as one of}} the valuable <b>self-learning</b> <b>techniques</b> for business organisations. The main objective of this research study is to elicit the factors affecting e-learning system use and to determine the difference between adopters and non-adopters of e-learning systems. Using a sample of 302 Hong Kong business organisations, results indicated that adopters have higher levels of technical support and individual information technology capabilities of e-learning systems rather than non-adopters. Adopters {{were more likely to have}} top management support as a champion than non-adopters for e-learning systems. Department of Management and Marketin...|$|R
40|$|The {{usual way}} of {{learning}} technical knowledge about a microcontroller (MCU) is by reading relevant handbooks and textbooks. This method {{requires students to}} memorize many technical terms and usually ignores its actual application of the data. In this paper, a new approach is proposed using the problem-based learning technique to convey such engineering knowledge. A creative group project was designed for the class whereby students were required {{to develop their own}} way of designing a calculator using MCU. Throughout the project, they acquired <b>self-learning</b> <b>techniques</b> to tackle new problems. Moreover, problem-based learning provided students with a cooperative learning environment to enhance their learning capabilities. Department of Electrical Engineerin...|$|R
40|$|This paper's main {{aim is to}} {{identify}} and discuss the areas of internet based elearning and teaching. Internet applications and model strategies for integrating ICT'S into teaching learning process more effective and meaningful. Internet applications consist of database, email, e-journal, World Wide Web, streaming audio and video, audio and video conferencing. A model strategy elaborates web-based lessons which are web quests, multimedia presentations, Cyber Guides, Telecommuting Projects, and Online Discussions. Teaching learning process will be live and advanced by {{the best use of}} internet in the classroom. The paper further discusses the place of internet, impacts of internet on language education, also emphasizes advantages of using internet and problem facing in implementation and finally gave some suggestions for effective implementation of web based lessons and model strategies for real teaching and learning. The Internet evolves continuously with new developments in tools and capacities. Our understanding of how pupils learn continues to change, it is being considered as a unique <b>self-learning</b> <b>technique</b> which enabled the individual to access information on any issue under the sun within no time and less cost. Especially internet is one of the prominent media of informal learning which is so useful for the students of distance education and higher education in the context of globalization. This paper proves Internet is the main tool used for revamping in teaching and learning process...|$|E
40|$|Artificial neural networks, {{that try}} to mimic the brain, are a very active area of {{research}} today. Such networks can potentially solve difficult problems such as image recognition, video analytics, lot more energy efficiently than when implemented in standard von-Neumann computing machines. New algorithms for neural computing with high bio-fidelity are being developed today to solve hard machine learning problems. In this work, we used a spiking network model, and implemented a <b>self-learning</b> <b>technique</b> using a Spike Timing Dependent Plasticity (STDP) algorithm, that closely mimics the neural activity of the brain. The basic STDP algorithm modulates the synaptic weights interconnecting the neurons based on pairs of pre- and post-synaptic spikes. This ignores the timing information embedded {{in the frequency of}} the post-synaptic spikes. We calculated the average of the membrane potential of each column of neurons to give an idea of how it behaved and spiked for the particular output neuron for a particular image in the past. The update of the weights or the synapses are done {{on the basis of the}} frequency obtained. The resultant synaptic updates are less frequent and made wisely making the learning process better. With the present algorithm, we are able to achieve an accuracy of 79 % for classifying images from the MNIST data set for a network of 400 output neurons. So the model was able to identify 79 % of the total images correctly which is greater than the original STDP signifying that slow and sensible updates are definitely having a better impact on the learning process...|$|E
40|$|Aerodynamic design, {{which aims}} at {{developing}} the outer {{shape of the}} aircraft while meeting several contrasting requirements, demands an accurate and reliable aerodynamic database. Computing forces and moments with {{the highest level of}} ?fidelity is a prerequisite, but practically limited by wall clock time and available computing resources. An e?fficient and robust approach is therefore sought after. This study investigates two design of experiments algorithms in combination with surrogate modelling. In traditional design of experiments, the samples are selected a priori before running the numerical explorative campaign. It is well-?known that this may result in either poor prediction capabilities or high computational costs. The second strategy employs an adaptive design of experiments algorithm. As opposed to the former, this is a <b>self?-learning</b> <b>technique</b> that iteratively: i) identi?fies the regions of the design space that are characterised by stronger non?linearities; and ii) select the new samples in order to maximise the information content associated with the simulations to be performed during the next iteration. In this work, the Reynolds?-averaged Navier-?Stokes equations are solved around a complete aircraft confi?guration. A representative ?flight envelope is created taking the angle of attack and Mach number as design parameters. The adaptive strategy is found to perform better than the traditional counterpart. This is quantifi?ed in terms of the sum of the squared error between the surrogate model predictions and CFD results. For the pitch moment coe?fficient, which shows strong non?linearities, the error metric using the adaptive strategy is reduced by about one order of magnitude compared to the traditional approach. Furthermore, the proposed adaptive methodology, which is employed on a high performance computing facility, requires no extra costs or complications than a traditional methodolog...|$|E
40|$|This paper {{addresses}} {{the task of}} readability assessment for the texts aimed at second language (L 2) learners. One of the major challenges in this task {{is the lack of}} significantly sized level-annotated data. For the present work, we collected a dataset of CEFR-graded texts tailored for learners of English as an L 2 and investigated text readability assessment for both native and L 2 learners. We applied a generalization method to adapt models trained on larger native corpora to estimate text readability for learners, and explored domain adaptation and <b>self-learning</b> <b>techniques</b> {{to make use of the}} native data to improve system performance on the limited L 2 data. In our experiments, the best performing model for readability on learner texts achieves an accuracy of 0. 797 and PCC of 0. 938...|$|R
40|$|This paper reports our {{experiences}} using the Building Information Modeling (BIM) approach within the FP 7 project SEEDS (Self Learning Energy Efficient Buildings and open Spaces). SEEDS {{focuses on the}} development of an optimized Building Energy Management System (BEMS) that reduces the energy consumption and the CO 2 emission of the building services during its operation phase based on <b>self-learning</b> <b>techniques.</b> One of the core components of the SEEDS architecture is the BEMS Energy Calculator which calculates the energy consumption of the building by means of a Facility Model at building operation time. In this paper, we introduce a new procedure for the automatic creation of BEMS Energy Calculator using a Multi-Model Approach which solves the drawbacks of the existing IFC models in relation to HVAC modeling. The ability of the purposed BIM-based approach for the design and operation phases is demonstrated in an application case...|$|R
40|$|When {{looked at}} from a {{multilingual}} perspective, grapheme-to-phoneme conversion is a challenging task, fraught {{with most of}} the classical NLP "vexed questions": bottle-neck problem of data acquisition, pervasiveness of exceptions, difficulty to state range and order of rule application, proper treatment of context-sensitive phenomena and long-distance dependencies, and so on. The hand-crafting of transcription rules by a human expert is onerous and time-consuming, and yet, for some European languages, still stops short of a level of correctness and accuracy acceptable for practical applications. We illustrate here a self-learning multilingual system for analogy-based pronunciation which was tested on Italian, English and French, and whose performances are assessed against the output of both statistically and rule-based transcribers. The general point is made that analogy-based <b>self-learning</b> <b>techniques</b> are no longer just psycholinguistically-plausible models, but competitive tools, combining the advantages of using language-independent, self-learning, tractable algorithms, with the welcome bonus of being more reliable for applications than traditional text-to-speech systems...|$|R
40|$|Network Service Providers are {{struggling}} to re- duce cost and still improve customer satisfaction. We have looked at three underlying challenges to achieve these goals; an overwhelming flow of low-quality alarms, understanding the structure {{and quality of the}} delivered services, and automation of service configuration. This thesis proposes solutions in these areas based on domain-specific languages, data-mining and self- learning. Most of the solutions have been validated based on data from a large service provider. We look at how domain-models can be used to capture explicit knowledge for alarms and services. In addition, we apply data- mining and <b>self-learning</b> <b>techniques</b> to capture tacit knowledge. The validation shows that models improve the quality of alarm and service models, and automatically render functions like root cause correlation, service and SLA status, as well as service configuration automation. The data-mining and self-learning solutions show that we can learn from available decisions made by experts and automatically assign alarm priorities. Godkänd; 2012; 20111223 (stewal...|$|R
30|$|Motivated by {{the above}} discussion, we propose a new SC {{resource}} sharing {{scheme based on}} the TS game model. TS game model is a useful framework for designing decentralized mechanisms, such that users in SC systems can self-organize into the mutually satisfactory resource sharing process. This self-organizing feature can add autonomics into SC systems and help to ease the heavy burden of complex centralized control algorithms. Especially, we pay serious attention to trust evaluation, repeated interactions, and iterative <b>self-learning</b> <b>techniques</b> to effectively implement our resource sharing process. In the proposed scheme, such techniques have been incorporated into the TS game model and work together toward an effective system performance. Therefore, we can induce all users to share their resources adaptively. The major contributions of the proposed scheme are (i) the adjustable dynamics considering the current SC environments, (ii) the interactive learning process based on the iterative feedback mechanism, (iii) the sophisticated combination of the reciprocal relationship and incentive mechanism, and (iv) practical approach to effectively reach a desirable solution. Other existing schemes [8, 10, 12 – 16] cannot offer these attractive features.|$|R
40|$|AbstractThe {{next big}} thing in {{computing}} is the Internet of Things (IoT), referring to dynamic and ever-evolving environments, generating high-volume streams of heterogeneous yet correlated contextual information of varying quality and complexity. Moreover, the increase in user mobility and unreliable sensor availability in IoT, necessitates the context-aware applications to dynamically adapt their behavior at run time. In this paper, we elicit the need for different <b>self-learning</b> <b>techniques</b> to tackle the openness of the IoT environments and propose enabling algorithms to achieve them. First, we present possible application scenarios which can benefit from both supervised and unsupervised self-learning. Later, we propose correlation mining algorithms based on Kullback-Leibler (KL) divergence and frequent set mining that exploits correlated contexts to enable unsupervised self-learning. These algorithms help to identify alternate sources for a context and semantically describe the previously unseen contexts in terms of already known contexts. We have realized the proposed algorithms {{on top of a}} Bayesian framework (HARD-BN) which supports autonomous learning. Our experiments demonstrate the applicability of the proposed correlation mining algorithms and their feasibility to enable self-learning in open and ever-evolving IoT environments...|$|R
40|$|Due {{to their}} {{constrained}} nature, {{wireless sensor networks}} (WSNs) are often optimised for a specific application domain, for example by designing a custom medium access control protocol. However, when several WSNs are located {{in close proximity to}} one another, the performance of the individual networks can be negatively affected as a result of unexpected protocol interactions. The performance impact of this 'protocol interference' depends on the exact set of protocols and (network) services used. This paper therefore proposes an optimisation approach that uses <b>self-learning</b> <b>techniques</b> to automatically learn the optimal combination of services and/or protocols in each individual network. We introduce tools capable of discovering this optimal set of services and protocols for any given set of co-located heterogeneous sensor networks. These tools eliminate the need for manual reconfiguration while only requiring minimal a priori knowledge about the network. A continuous re-evaluation of the decision process provides resilience to volatile networking conditions in case of highly dynamic environments. The methodology is experimentally evaluated in a large scale testbed using both single- and multihop scenarios, showing a clear decrease in end-to-end delay and an increase in reliability of almost 25 %...|$|R
40|$|We {{introduce}} a <b>self-learning</b> tomographic <b>technique</b> {{in which the}} experiment guides itself to an estimate of its own state. Self-guided quantum tomography (SGQT) uses measurements to directly test hypotheses in an iterative algorithm which converges to the true state. We demonstrate through simulation on many qubits that SGQT is a more efficient and robust alternative to the usual paradigm of taking {{a large amount of}} informationally complete data and solving the inverse problem of post-processed state estimation. Comment: v 2 : published versio...|$|R
40|$|This thesis {{presents}} a fuzzy logic controller aimed at maintaining constant tension between two adjacent stands in tandem rolling mills. The fuzzy tension controller monitors tension variation by resorting to electric current comparison of different operation modes {{and sets the}} reference for speed controller of the upstream stand. Based on modeling the rolling stand as a single input single output linear discrete system, which works in the normal mode and is subject to internal and external noise, the element settings and parameter selections {{in the design of}} the fuzzy controller are discussed. To improve the performance of the fuzzy controller, a dynamic fuzzy controller is proposed. By switching the fuzzy controller elements in relation to the step response, both transient and stationary performances are enhanced. To endow the fuzzy controller with intelligence of generalization, flexibility and adaptivity, <b>self-learning</b> <b>techniques</b> are introduced to obtain fuzzy controller parameters. With the inclusion of supervision and concern for conventional control criteria, the parameters of the fuzzy inference system are tuned by a backward propagation algorithm or their optimal values are located by means of a genetic algorithm. In simulations, the neuro-fuzzy tension controller exhibits the real-time applicability, while the genetic fuzzy tension controller reveals an outstanding global optimization ability...|$|R
40|$|The {{number of}} {{wirelessly}} communicating devices increases every day, {{along with the}} number of communication standards and technologies that they use to exchange data. A relatively new form of research is {{trying to find a way}} to make all these co-located devices not only capable of detecting each other's presence, but to go one step further - to make them cooperate. One recently proposed way to tackle this problem is to engage into cooperation by activating 'network services' (such as internet sharing, interference avoidance, etc.) that offer benefits for other co-located networks. This approach reduces the problem to the following research topic: how to determine which network services would be beneficial for all the cooperating networks. In this paper we analyze and propose a conceptual solution for this problem using the reinforcement learning technique known as the Least Square Policy Iteration (LSPI). The proposes solution uses a self-learning entity that negotiates between different independent and co-located networks. First, the reasoning entity uses <b>self-learning</b> <b>techniques</b> to determine which service configuration should be used to optimize the network performance of each single network. Afterwards, this performance is used as a reference point and LSPI is used to deduce if cooperating with other co-located networks can lead to even further performance improvements...|$|R
50|$|Automatic {{classification}} {{programs can}} extract index, category, and transfer data autonomously. Automatic classification or categorizing, {{based on the}} information contained in electronic information objects, can evaluate information based on predefined criteria or in a <b>self-learning</b> process. This <b>technique</b> can be used with OCR-converted faxes, office files, or output files.|$|R
40|$|This paper {{suggests}} {{two ways}} of improving semantic role labeling (SRL). First, we introduce a novel transition-based SRL algorithm that gives a quite different approach to SRL. Our algorithm is inspired by shift-reduce parsing and brings {{the advantages of the}} transitionbased approach to SRL. Second, we present a <b>self-learning</b> clustering <b>technique</b> that effectively improves labeling accuracy in the test domain. For better generalization of the statistical models, we cluster verb predicates by comparing their predicate argument structures and apply the clustering information to the final labeling decisions. All approaches are evaluated on the CoNLL’ 09 English data. The new algorithm shows comparable results to another state-of-the-art system. The clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks...|$|R
40|$|Very {{important}} {{breakthroughs in}} data-centric {{machine learning algorithms}} led to impressive performance in transactional point applications such as detecting anger in speech, alerts from a Face Recognition system, or EKG interpretation. Non-transactional applications, e. g. medical diagnosis beyond the EKG results, require AI algorithms that integrate deeper and broader knowledge in their problem-solving capabilities, e. g. integrating knowledge about anatomy and physiology of the heart with EKG results and additional patient findings. Similarly, for military aerial interpretation, where knowledge about enemy doctrines on force composition and spread helps immensely in situation assessment beyond image recognition of individual objects. The Double Deep Learning approach advocates integrating data-centric machine <b>self-learning</b> <b>techniques</b> with machine-teaching techniques to leverage the power of both and overcome their corresponding limitations. To take AI to the next level, {{it is essential that}} we rebalance the roles of data and knowledge. Data is important but knowledge- deep and commonsense- are equally important. An initiative is proposed to build Wikipedia for Smart Machines, meaning target readers are not human, but rather smart machines. Named ReKopedia, the goal is to develop methodologies, tools, and automatic algorithms to convert humanity knowledge that we all learn in schools, universities and during our professional life into Reusable Knowledge structures that smart machines can use in their inference algorithms. Ideally, ReKopedia would be an open source shared knowledge repository similar to the well-known shared open source software code repositories. Examples in the article are based on- or inspired by- real-life non-transactional AI systems I deployed over decades of AI career that benefit {{hundreds of millions of people}} around the globe. Comment: 10 pages, 2 Figure...|$|R
40|$|Edge linking is a {{fundamental}} computer-vision task, yet presents difficulties arising {{from the lack of}} information in the image. Viewed as a constrained optimisation problem, it is NP hard [...] being isomorphic to the classical travelling salesman problem. <b>Self-learning</b> neural <b>techniques</b> boast the ability to solve hard, ill-defined problems and, hence, offer promise for such an application. This paper examines the suitability of four well-known unsupervised techniques for the task of edge linking, by applying them to a test bed of edge point images and then evaluating their performance. Techniques studied are the elastic net, active contours (`snakes'), Kohonen map and Burr's modified elastic net. Of these, only the elastic net and the Kohonen map are realistic contenders for general edgelinking tasks, although special treatment of noise in the image is required...|$|R
40|$|In {{this paper}} we discuss ongoing efforts at Scientific Systems towards the {{development}} of e$ective strategies for trafic management of ATM networks using <b>Self-Learning</b> Adaptive (SLA) <b>techniques.</b> We extended our previous SLA techniques to bursty trafic patterns and show how an approximation to SLA, called proportional feedback, {{can be used to}} manage real-time variable bit rate ATM trafic. Finally, we present results on the dynamic allocation of bandwidth in order to eficiently multiplex several variable rate MPEG- 1 streams over a constant-rate link. ...|$|R
40|$|Selecting {{the most}} {{appropriate}} construction technology for a given project scenario {{is one of the}} most effective approaches for constructability improvement. Experienced construction engineers and managers are frequently involved in applying their constructability knowledge to technology selection during the early phase of a project 2 ̆ 7 s lifecycle in order to achieve cost effective construction. Due to natural attrition and other causes, the constructability knowledge of experienced personnel is diminishing in many construction firms. Moreover, because of the complex nature of construction operations, it is extremely difficult for construction experts to express their constructability knowledge while considering many variable attributes simultaneously. A methodology for automated constructability analysis and knowledge acquisition has yet to be developed. Without such a methodology, timely selection of {{the most appropriate}} construction technology and accumulation of knowledge for technology improvement will remain difficult. ^ This research is the first work on both quantifying the conventional descriptive definition of constructability and on exploring the learning ability of neuro-fuzzy networks for automatic constructability knowledge acquisition. The developed methodology differentiates itself from the traditional constructability analysis and improvement approaches in two aspects: 1) the quantitative definition of constructability is adopted instead of the traditional descriptive definition, so that constructability can be measured, estimated, and improved; (2) the <b>self-learning</b> <b>techniques</b> for constructability knowledge acquisition are adopted instead of the traditional manual human-input approaches, so that the automation of constructability knowledge acquisition and accumulation becomes possible. With this generic methodology, construction firms can develop their own decision support systems to analyze and solve specific constructability problems according to their specialized fields. The result of this research is a tool for continuous constructability improvement of construction projects and technologies. ^ A prototype computer implementation of the proposed methodology named COnstrUction techNology SELectOR (COUNSELOR) is developed for constructability analysis and improvement of concrete formwork technologies. The COUNSELOR system has demonstrated its abilities to quantify constructability according to the constructor 2 ̆ 7 s characteristics, detect potential constructability problems before the construction phase, and propose solutions for constructability improvement. The result of the research indicates a promising solution for barriers encountered in the implementation of conventional constructability approaches. ...|$|R
40|$|Network service {{providers}} {{are struggling to}} reduce cost {{while at the same}} time improving customer satisfaction. This thesis addresses three relevant underlying challenges to achievieng these goals: - managing an overwhelming flow of low-quality alarms - understanding the structure and quality of the delivered services - automation of service configurationAll of the these add to an operator's operational costs since manual work is required in order to understand the alarm and service status as well as for configuring new services. We propose solutions based on domain-specific languages, data-mining and self-learning. We look at how domain-models can be used to capture explicit knowledge for alarms and services. In addition, we apply data-mining and <b>self-learning</b> <b>techniques</b> to understand the alarm semantics. The alarm solution is validated with a quantitative analysis based on real alarm documentation and an alarm database from a large mobile service provider. A qualitative analysis of the service management solutions is given based on prototypes and input from {{service providers}}. We present an approach to alarm interfaces by providing a formal alarm model together with a domain-specic language, BASS. This means that we can verify the consistency of an alarm interface and automatically generate artifacts such as alarm correlation rules or alarm documentation based directly on the model. From a baseline without any correlation, our alarm domain-model based on vendor documentation could automatically find the root-cause alarms in 40 % of the cases. In the process of producing an alarm model from pre-existing alarm documentation for a commercial product, we found over 150 semantic warnings. We also propose a domain specific language called SALmon, which allows for efficient representation of service models, along with a computational engine for calculation of service status. Furthermore, this thesis illustrates how we can achieve automatic service configuration based on YANG, the domain-specific language standardized in the IETF. Prototypes show that the service domain-models can capture the semantics of service models, and automatically render parts of the service management solution. It is not always possible to capture expert knowledge in models. Therefore, we propose a data-mining and self-learning solution that learns alarm priorities from the decisions taken by the network administrators. The solution assigns the same severity as a human expert in 50 % of the cases compared to 17 % for the original alarm severity. Godkänd; 2012; 20120116 (stewal); DISPUTATION Namn: Stefan Wallin Ämnesområde: Mobila system/Mobile Systems Opponent: Professor Rolf Stadler, Skolan för elektro- och systemteknik, KTH, Stockholm, Ordförande: Professor Christer Åhlund, Institutionen för system- och rymdteknik, Luleå tekniska universitet Tid: Torsdag den 23 februari 2012, kl 10. 00 Plats: D 770, Luleå tekniska universite...|$|R
40|$|Limited {{training}} {{in the use of}} Information Resources has been introduced in the curricula of the Faculty of Medicine and Health Sciences since 2003. In the academic year 2008 - 2009 a structured course in Library and Information Sciences has been integrated in the curriculum of the first bachelor in Biomedical Sciences, preparing a research career. The aim of the course is digital literacy. Beside the theoretical teaching, e-learning and <b>self-learning</b> <b>techniques</b> (portfolio, chat box, wiki), were used to learn technical competences in word processing, presentation software and spreadsheet. Search strategies were intensively trained in smaller groups in the PC-room. This paper summarizes the examination results of 138 students coming directly from middle school (K 12 -level, after secondary school). Three components were separately graded: theoretical knowledge by oral interrogation; technical competences using portfolio and a spreadsheet application; search strategy by a literature task, almost identical to the search exercises instructed during the semester. For the search strategy a paragraph of a text was chosen to formulate context-sensitive research questions, one to be selected and answered using a factual database, another to be answered using three different bibliographic databases. The student had full access on his PC to lecture notes, digital library and the internet. Strict instructions were written on the first page of the examination folder. 66 % successfully passed the theoretical oral part, 81 % the spreadsheet exercise. Portfolio was satisfactory in 92 %. Literature searching, however, was less successful: 43 % of the students were unable to distil correct research questions from the text. The choice of index terms from a standard thesaurus (MeSH, Medical Subject Headings, which is the structured thesaurus from NLM - National (US) Library of Medicine) was successful in 68 %. Only 25 % of the students made the correct choice for factual or bibliographic databases, depending on the question, and 25 % of the students correctly used the three bibliographic databases, exactly as written in the instructions. Limits were poorly used to make a selection of the best possible hits, mainly language, recent years of publication and subject. Limitation by publication type was selected by as few as 19 %. The bad performance in the search tasks can be explained by poor basic literacy and by lack of critical reading training. The low number of students capable to phrase correct questions, and the failure in choosing the right database, despite the access to teaching material and explicit instructions, reflects poor reading capability and lack of critical and integrative senses. To become digital literate and to take profit of the power of digital technology in learning, basic literacy is more important than technical competences. The overemphasis on digital software in middle school programs, neglecting basic literacy and critical text analysis skills, might become a negative factor in the use of present powerful data and information resources and explain the sometimes disappointing results of digital learning...|$|R

