6|509|Public
40|$|Connectomics is an {{emerging}} field of neurobiology that uses cutting edge machine learning and image processing to extract brain connectivity graphs from electron microscopy images. It {{has long been}} assumed that the processing of connectomics data will require mass storage and farms of CPUs and GPUs and will take months if not years. This talk will discuss the feasibility of designing a high-throughput connectomics-on-demand system that runs on a multicore machine with less than 100 cores and extracts connectomes at the terabyte per hour pace of modern electron microscopes. Building this system required solving algorithmic and performance engineering issues related to <b>scaling</b> <b>machine</b> learning on multicore architectures, and may have important lessons for other problem spaces in the natural sciences, where until now large distributed server or GPU farms {{seemed to be the}} only way to go...|$|E
40|$|Mechanical scaling {{is one of}} {{the most}} complex {{operations}} of all in the mining process. The operators are using both hearing, sight and feel to read the rock and to know where to scale and where to leave. They say they go on gut feeling in great extent, something that comes with experience. The <b>scaling</b> <b>machine</b> is placed into the edge of unsecured and newly blasted rock to hammer off the blocks that has become loose after the blast, but not enough to fall down. It goes without sayingthat it is a risky job. Boliden Mineral has a desire to remotely control this operation from above ground and thus became this challenge a degree project in industrial design. With a focus on ease of use and the sensory feedback, the project aims to develop a proposal on the driver situation that allows the operator to do an as equally good job above as below ground...|$|E
40|$|Much of the {{research}} in inductive learning concentrates on problems with relatively small amounts of training data. With the steady progress of the Human Genome Project, {{it is likely that}} orders of magnitude more data in sequence databases will be available in the near future for various learning problems of biological importance. Thus, techniques that provide the means of <b>scaling</b> <b>machine</b> learning algorithms requires considerable attention. Meta-learning is proposed as a general technique to integrate a number of distinct learning processes that aims to provide a means of scaling to large problems. This paper details several meta-learning strategies for integrating independently learned classifiers on subsets of training data by the same learner in a parallel and distributed computing environment. Our strategies are particularly suited for massive amounts of data that main-memory-based learning algorithms cannot handle efficiently. The strategies are also independent of the particular learning algorithm used and the underlying parallel and distributed platform. Preliminary experiments using different learning algorithms in a simulated parallel environment demonstrate encouraging results: parallel learning by meta-learning can achieve comparable prediction accuracy in less space and time than serial learning...|$|E
5000|$|... #Caption: Microsoft and Google {{presented}} at the 2017 <b>Scaled</b> <b>Machine</b> Learning Conference held at Stanford University.|$|R
50|$|These {{algorithms}} {{are designed}} for large <b>scale</b> <b>machine</b> learning, dealing with concept drift, and big data streams in real time.|$|R
5000|$|Matroid {{holds the}} <b>Scaled</b> <b>Machine</b> Learning Conference (ScaledML) every year. The {{previous}} two conferences were {{held on the}} Stanford University campus.|$|R
40|$|<b>Scaling</b> <b>machine</b> {{learning}} {{methods to}} very large datasets has attracted considerable attention in recent years, thanks to {{easy access to}} ubiquitous sensing and data from the web. We study face recognition and show that three dis-tinct properties have surprising effects on the transferabil-ity of deep convolutional networks (CNN) : (1) The bottle-neck of the network serves as an important transfer learn-ing regularizer, and (2) {{in contrast to the}} common wisdom, performance saturation may exist in CNN’s (as the number of training samples grows); we propose a solution for al-leviating this by replacing the naive random subsampling of the training set with a bootstrapping process. More-over, (3) we find a link between the representation norm and the ability to discriminate in a target domain, which sheds lights on how such networks represent faces. Based on these discoveries, we are able to improve face recog-nition accuracy on the widely used LFW benchmark, both in the verification (1 : 1) and identification (1 :N) protocols, and directly compare, for the first time, with {{the state of the art}} Commercially-Off-The-Shelf system and show a sizable leap in performance. 1...|$|E
40|$|<b>Scaling</b> <b>machine</b> {{learning}} {{methods to}} massive datasets has attracted considerable attention in recent years, thanks to {{easy access to}} ubiquitous sensing and data from the web. Face recognition is a task of great practical interest for which (i) very large labeled datasets exist, containing billions of images; (ii) the number of classes can reach tens of millions or more; and (iii) complex features are nec-essary in order to encode subtle differences between subjects, while maintaining invariance to factors such as pose, illumination, and aging. We present an elab-orate pipeline that consists of a crucial network compression step followed by a new bootstrapping scheme for selecting a challenging subset of the dataset for efficient training of a higher capacity network. By using this approach, {{we are able to}} greatly improve face recognition accuracy on the widely used LFW bench-mark. Moreover, as performance on supervised face verification (1 : 1) benchmarks saturates, we propose to shift the attention of the research community to the unsu-pervised Probe-Gallery (1 :N) identification benchmarks. On this task, we bridge between the literature and the industry, for the first time, by directly comparing with {{the state of the art}} Commercially-Off-The-Shelf system and show a sizable leap in performance. Lastly, we demonstrate an intriguing trade-off between the number of training samples and the optimal size of the network. ...|$|E
40|$|During {{the past}} few years, the Pittsburgh Research Laboratory of the National Institute for Occupational Safety and Health (NIOSH) {{examined}} and characterized conditions at {{a majority of the}} underground stone mines in the United States. Observations at these mines revealed a limited degree of roof monitoring beyond visual inspection and sounding. When monitors are used, they typically require the miner to measure movement at the roof. If conditions are unstable, the mine rmay be in a hazardous situation while recording data. Based on this scenario, researchers surmised that a simple, inexpensive monitor with the capability of recording data at a distance from the mine roof would be a safer way to gain the information. Additionally, more widespread use of monitors could potentially lead to better understanding of roof movement in general. A monitor to meet this need was designed, tested, and subsequently improved as experience was gained in its 22 ̆ 0 ac 2 ̆ 122 use at a number of underground stone mines. The Roof Monitoring Safety System (RMSS) can provide an initial indication of movement in roof beams. By understanding and measuring roof movement in underground mines, the potential for injuries and fatalities to mine workers from falls of ground can be reduced. Also, officials at a mine with a history of data are better prepared to make a decision on remedial actions in the event of ground falls. This paper will outline the evolution of the RMSS and how it {{can be used in a}} comprehensive proactive ground control safety program. Also included is a case history describing how the RMSS was used in an evaluation of the effectiveness of a mechanical impact <b>scaling</b> <b>machine</b> at an operating limestone mine...|$|E
50|$|Ingersoll Machine Tools is a {{manufacturer}} located in Rockford, Illinois that produces large <b>scale</b> <b>machine</b> tools {{for use in}} metal cutting and automated fiber placement.|$|R
5000|$|It {{serves as}} a main data source for the STReP's [...] "transLectures" [...] project the goal {{of which is to}} develop {{automatic}} large <b>scale</b> <b>machine</b> translations.|$|R
50|$|Third {{process in}} textile {{manufacturing}} is dyeing, printing and finishing. The {{state of the}} art laboratory <b>scale</b> <b>machines</b> are installed in the Wet Processing Laboratory. This includes all sort of dyeing and printing machines.|$|R
30|$|Locality-based data {{selection}} and classification for limiting the latency of basic data analysis operations {{running in parallel}} on large <b>scale</b> <b>machines</b> {{in a way that}} the subset of data needed together in a given phase are locally available (in a subset of nearby cores).|$|R
40|$|ECNet is an {{open source}} Python package for {{creating}} large <b>scale</b> <b>machine</b> learning projects {{with a focus on}} fuel property prediction. ECNet can predict a variety of fuel properties including cetane number, octane number, and yield sooting index using quantitative structure-property relationship (QSPR) input parameters...|$|R
40|$|Stochastic {{optimization}} methods (Robbins-Monro algorithms) {{have been}} very successful for large <b>scale</b> <b>machine</b> learning by using batches A typical MCMC algorithm requires computations over the whole dataset and cannot use batches The authors propose combining a stochastic optimization method with Langevin dynamics for Bayesian learning from large scale dataset...|$|R
5000|$|... #Subtitle level 4: Autonomous Small <b>Scale</b> Construction <b>Machine</b> (ASSCM) ...|$|R
40|$|International audiencehe {{advent of}} extreme <b>scale</b> <b>machines</b> {{will require the}} use of {{parallel}} resources at an unprecedented scale, possibly leading to a high rate of hardware faults. In this presentation, we investigate recovery followed by restarting strategies for the resilience of Krylov subspace linear solvers as well as of eigensolvers...|$|R
5000|$|In 2009 Ellery {{held his}} fourth solo {{entitled}} [...] "Ellery’s Theory of Neo-conservative Creationism". It consisted of large <b>scale,</b> <b>machined,</b> solid brass suspended sculptures, sound and moving image. To commemorate the show, {{a black and}} white catalogue was also published highlighting artworks that had been machined by Ellery into brass.|$|R
50|$|Claytronics is an {{emerging}} field of engineering concerning reconfigurable nanoscale robots ('claytronic atoms', or catoms) designed to form much larger <b>scale</b> <b>machines</b> or mechanisms. The catoms will be sub-millimeter computers {{that will eventually}} {{have the ability to}} move around, communicate with other computers, change color, and electrostatically connect to other catoms to form different shapes.|$|R
40|$|Partitioned global {{address space}} (PGAS) {{languages}} provide a unique programming model that can span shared-memory multiprocessor (SMP) architectures, distributed memory machines, or cluster ofSMPs. Users can program large <b>scale</b> <b>machines</b> with easy-to-use, shared memory paradigms. In order to exploit large <b>scale</b> <b>machines</b> efficiently, PGAS language implementations and their runtime {{system must be}} designed for scalability and performance. The IBM XLUPC compiler and runtime system provide a scalable design {{through the use of}} the shared variable directory (SVD). The SVD stores meta-information needed to access shared data. It is dereferenced, in the worst case, for every shared memory access, thus exposing a potential performance problem. In this paper we present a cache of remote addresses as an optimization that will reduce the SVD access overhead and allow the exploitation of native (remote) direct memory accesses. It results in a significant performance improvement while maintaining the run-time portability and scalability. Postprint (published version...|$|R
50|$|After {{foundation}} in 1904 the company expanded {{in the next}} decade, establishing several new factories including ones for electrical cables, machine and tool making, and large <b>scale</b> <b>machines.</b> In 1914 the company began manufacturing motor vehicles, with an electric transmission system, {{to the design of}} Balachowsky & Caire. During World War I the factory was stripped of machines by occupying German forces.|$|R
40|$|Design and {{construction}} of a machine to desalinate and sieve sea sand is discussed. Initially, a full <b>scale</b> <b>machine</b> was designed according to the requirement. Then, the device was modelled using Pro/Engineer to depict the features and {{the operation of the}} desalination device. Finally, a miniature physical model of the proposed design was constructed to demonstrate the desalination and the sieving process...|$|R
40|$|We give {{a general}} {{constructive}} proof for hierarchical coordinatizations (Lagrange Decompositions) of permutation groups. The generalization originates from {{the investigation of}} how the subgroup chains of finite permutation groups yield different coordinate systems. The study is motivated by the practical needs and the verification of an existing computational implementation. Large <b>scale</b> <b>machine</b> calculated examples are also presented. Comment: 10 pages, 1 figur...|$|R
5000|$|<b>Scale</b> type <b>machines</b> are {{available}} from manufactures such as Ice Systems (United Kingdom), Geneglace (France), Ice Power (China) and Snowkey ...|$|R
40|$|We {{analyze and}} {{evaluate}} an online gradient descent algorithm with adaptive per-coordinate adjustment of learning rates. Our algorithm {{can be thought}} of as an online version of batch gradient descent with a diagonal preconditioner. This approach leads to regret bounds that are stronger than those of standard online gradient descent for general online convex optimization problems. Experimentally, we show that our algorithm is competitive with state-of-the-art algorithms for large <b>scale</b> <b>machine</b> learning problems...|$|R
5000|$|Toutiao’s {{research}} arm, Toutiao AI Lab, {{was founded}} in March 2016 and is headed by Ma Wei-Ying, former assistant managing director of Microsoft Research Asia. The lab’s research is centered around the theme of creating intelligent machines that understand information (text, images, videos) in depth and developing large <b>scale</b> <b>machine</b> learning algorithms for personalized information recommendation. [...] It's main research areas include Natural Language Processing, Machine Learning, Computer Vision and Human-Computer Interaction.|$|R
50|$|The Rock-Ola Manufacturing Corporation {{was founded}} in 1927 by Coin-Op pioneer David Cullen Rockola to {{manufacture}} slot <b>machines,</b> <b>scales</b> and pinball <b>machines.</b> The firm later produced parking meters, furniture, and firearms, but became {{best known for its}} jukeboxes.|$|R
5000|$|Vertical Carousels {{based on}} the {{paternoster}} system or with space optimization, these {{can be thought of}} as large <b>scale</b> vending <b>machines.</b>|$|R
40|$|This paper {{presents}} a simulation-based performance prediction framework for large scale data-intensive applications on large <b>scale</b> <b>machines.</b> Our framework {{consists of two}} components: application emulators and a suite of simulators. Application emulators provide a parameterized model of data access and computation patterns of the applications and enable changing of critical application components #input data partitioning, data declustering, processing structure, etc. # easily and #exibly. Our suite of simulators model the I#O and communication subsystems with good accuracy and execute quickly on a high-performance workstation to allow performance prediction of large <b>scale</b> parallel <b>machine</b> con#gurations. The key to e#cient simulation of very large scale con#gurations is a technique called loosely-coupled simulation where the processing structure of the application {{is embedded in the}} simulator, while preserving data dependencies and data distributions. Weevaluate our performance [...] ...|$|R
50|$|The Water {{frame was}} {{developed}} and patented by Arkwright in the 1770s. The roving was attenuated (stretched) by drafting rollers and twisted by winding it onto a spindle. It was heavy large <b>scale</b> <b>machine</b> {{that needed to be}} driven by power, which in the late 18th century meant by a water wheel. Cotton mills were designed for the purpose by Arkwright, Jedediah Strutt and others along the River Derwent in Derbyshire. Water frames could only spin weft.|$|R
5000|$|After the sale, Collier {{established}} an investment office with Chula Reynolds {{as well as}} an art studio in Sausalito, CA using software and large <b>scale</b> <b>machine</b> tools to making sculptural reliefs of water surfaces. In her artist statement she describes her inspiration as [...] "One day I was walking across a bridge and thought I wish I could just reach down into the water and pick up a piece of that shining surface and keep it forever." ...|$|R
2500|$|Bussard {{believed}} that the WB-6 machine had demonstrated progress and that no intermediate-scale models would be needed. He noted, [...] "We are probably the only {{people on the planet}} who know how to make a real net power clean fusion system" [...] He proposed to rebuild WB-6 more robustly to verify its performance. After publishing the results, he planned to convene a conference of experts in the field in an attempt to get them behind his design. The first step in that plan was to design and build two more small scale designs (WB-7 and WB-8) to determine which full <b>scale</b> <b>machine</b> would be best. He wrote [...] "The only small <b>scale</b> <b>machine</b> work remaining, which can yet give further improvements in performance, is test of one or two WB-6-scale devices but with [...] "square" [...] or polygonal coils aligned approximately (but slightly offset on the main faces) along the edges of the vertices of the polyhedron. If this is built around a truncated dodecahedron, near-optimum performance is expected; about 3–5 times better than WB-6." [...] Bussard died on October 6, 2007 from multiple myeloma at age 79.|$|R
50|$|Constructed in 1866 {{to house}} Pioneer Iron Works, the {{building}} was originally a factory for constructing large <b>scale</b> <b>machines</b> required by industry, including railroad tracks and machinery for sugar plantations. With a footprint of half an acre, {{it was one of}} the largest machine manufacturers in the United States. The building was burned to the ground by a devastating fire in 1881 and rebuilt shortly thereafter, operating until the end of WWII. The factory was a landmark that ultimately gave Pioneer Street its name.|$|R
5000|$|Pauline founded SRL in 1978 {{and it is}} {{considered}} the premier practitioner of [...] "industrial performing arts", and the forerunner of large <b>scale</b> <b>machine</b> performance. SRL is known for producing the most dangerous shows on earth. Although acknowledged as {{a major influence on}} popular competitions pitting remote-controlled robots and machines against each other, such as BattleBots and Robot Wars, Pauline shies away from rules-bound competition preferring a more anarchic approach. Machines are liberated and re-configured away from the functions they were originally meant to perform.|$|R
40|$|Machining is an {{important}} excess-metal removal process in manufacturing. In spite of the considerable effort on metal cutting research, the mechanics of this process at a nano scale are not fully understood. Hence metal cutting research is a challenging field and is currently attracting the attention of workers {{in the field of}} plasticity of metals as well as those concerned with the more immediate problems in practice. Nano scale grooves such as nano channels and nano slots have many applications in semiconductor and biomedical industries, which are growing to meet the increasing demands of nano technology. Whenever nanomachining is performed to produce a nano scale groove on a metal surface, abrasive wear occurs. The ultimate aim of the analysis of the mechanics of metal machining at a nano scale is to understand the basic phenomena, to predict how the materials deforms, which mechanism is dominant and what machining parameters (depth of cut, machining velocity) are required for a given material and for a given machining condition to produce a nano scale groove. In this research, nanomachining was utilized to investigate the abrasive wear mechanism that produces a nano scale groove on metal surfaces. Two different tools (Berkovich and Conical) with the same tip radius of 100 nm but of different edge geometries were used to machine both polycrystalline materials and single crystal materials with a nano indenter equipped with a nano scratching attachment. During the machining operations, the generated normal and cutting forces were measured as a function of nano <b>scale</b> <b>machining</b> parameters. It was found that the generated forces (normal and cutting) increased with an increase in depth of cut; however, there was no significant effect on the generated forces due to the variation in machining velocities. In nano <b>scale</b> <b>machining,</b> the percentage of elastic recovery revealed that the deformation mechanisms were identified as elastoplastic in nature as opposed to the well-established completely plastic mode of traditional machining operations. The pile up volume due to elastoplastic deformation was utilized to distinguish between the ploughing and the cutting modes of abrasive wear mechanism. The percentage values of these two mechanisms were determined and utilized to obtain the dominant mode of an abrasive wear mechanism for producing a nano scale groove and to correlate this abrasive wear mechanism with the co-efficient of friction (µ) in different machining conditions. As a result of machining operations, the machined surface retains residual stress and strain. The type of residual stress, whether tensile or compressive, depends on the used machining parameters. It is however difficult to measure residual stress and strain at a nano <b>scale</b> <b>machining</b> level. Therefore, to understand the material deformation behaviour and their effect on the machined surface, numerical modelling of nano <b>scale</b> <b>machining</b> process was developed by using a novel mesh free modelling tool of Smoothed Particle Hydrodynamics (SPH) ...|$|R
50|$|ANKA (abbreviation for „Angströmquelle Karlsruhe“) is a {{synchrotron}} {{light source}} facility at the Karlsruhe Institute of Technology (KIT). The KIT runs ANKA {{as a national}} synchrotron light source and as a large scale user facility for the international science community. Being a large <b>scale</b> <b>machine</b> of the performance category LK II of the Helmholtz Association (Helmholtz Association of German Research Centres), ANKA {{is part of a}} national and European infrastructure offering research services to scientific and commercial users for their purposes in research and development. The facility was opened to external users in 2003.|$|R
