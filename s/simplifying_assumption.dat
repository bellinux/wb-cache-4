822|3986|Public
25|$|Unique {{distributions}} of compounds across one or many plates {{can be employed}} either {{to increase the number}} of assays per plate or to reduce the variance of assay results, or both. The <b>simplifying</b> <b>assumption</b> made in this approach is that any N compounds in the same well will not typically interact with each other, or the assay target, in a manner that fundamentally changes the ability of the assay to detect true hits.|$|E
25|$|Once in {{thermodynamic}} equilibrium, a system's properties are, by definition, unchanging in time. Systems in equilibrium {{are much}} simpler {{and easier to}} understand than are systems which are not in equilibrium. Often, when analysing a dynamic thermodynamic process, the <b>simplifying</b> <b>assumption</b> is made that each intermediate state in the process is at equilibrium, producing thermodynamic processes which develop so slowly as to allow each intermediate step to be an equilibrium state and {{are said to be}} reversible processes.|$|E
25|$|The {{solutions}} {{are based on}} linear isotropic infinitesimal elasticity and Euler–Bernoulli beam theory. In other words, they contain the assumptions (among others) that the materials in question are elastic, that stress is related linearly to strain, that the material (but not the structure) behaves identically regardless of direction of the applied load, that all deformations are small, and that beams are long relative to their depth. As with any <b>simplifying</b> <b>assumption</b> in engineering, the more the model strays from reality, the less useful (and more dangerous) the result.|$|E
40|$|The {{goal-directed}} {{forward reasoning}} is a two step procedure {{in which the}} first step generates a <b>simplified</b> <b>assumption</b> clause set (or a single assumption clause) that semantically conflicts with the negated conclusion, and the second step searches for a refutation between the <b>simplified</b> <b>assumption</b> and the negated conclusion. In order to form a <b>simplified</b> <b>assumption,</b> the theorem is divided into axioms and a conclusion, but unlike traditional approaches like set-of-support, only clauses from axioms are resolved. When a <b>simplified</b> <b>assumption</b> has been made, the clause set containing the <b>simplified</b> <b>assumption</b> and the negated conclusion is likely smaller than the original theorem. ...|$|R
40|$|<b>Simplifying</b> <b>assumptions</b> — {{everyone}} {{uses them}} but no one's programming tool explicitly supports them. In programming, {{as in other}} kinds of engineering design, <b>simplifying</b> <b>assumptions</b> are an important method for dealing with complexity. Given a complex programming problem, expert programmers typically choose <b>simplifying</b> <b>assumptions</b> which, though false, allow them to arrive rapidly at a program which addresses the important features of the problem without being distracted by all of its details. The <b>simplifying</b> <b>assumptions</b> are then incrementally retracted with corresponding modifications to the initial program. This methodology is particularly applicable to rapid prototyping because the main questions of interest can often be answered using only the initial program. <b>Simplifying</b> <b>assumptions</b> can easily be misused. In order to use them effectively two key issues must be addressed. First, <b>simplifying</b> <b>assumptions</b> should be chosen which simplify the design problems significantly without changing the essential character of the program which needs to be implemented. Second, the designer must keep track of all the assumptions he is making {{so that he can}} later retract them in an orderly manner. By explicitly dealing with these issues, a programming assistant system could directly support the use of <b>simplifying</b> <b>assumptions</b> as a disciplined part of the software development process. Submitted to the ACM SIGSOFT Second Software Engineering Symposium: Workshop on Rapid Prototyping. Columbia, Maryland, April 19 - 21, 1982. MIT Artificial Intelligence Laborator...|$|R
40|$|Simplifying assumptions- {{everyone}} {{uses them}} but no one's programming tool explicitly supports them. In programming, {{as in other}} kinds of engineering design, <b>simplifying</b> <b>assumptions</b> are an important method for dealing with complexity. Given a complex programming problem, expert programmers typically choose <b>simplifying</b> <b>assumptions</b> which, though false, allow them to arrive rapidly at a program which addresses the important features of the problem without being distracted by all of its details. The <b>simplifying</b> <b>assumptions</b> are then incrementally retracted with corresponding modifications to the initial program. This methodology is particularly applicable to rapid prototyping because the main questions of interest can often be answered using only the initial program. <b>Simplifying</b> <b>assumptions</b> can easily be misused. In order to use them effectively two key issues must be addressed. First, <b>simplifying</b> <b>assumptions</b> should be chosen which simplify the design problem significantly without changing the essential character of the program which needs to be implemented. Second, the designer must keep track of all the assumptions he is making {{so that he can}} later retract them in an orderly manner. By explicitly dealing with these issues, a programming assistant system could directly support the use of <b>simplifying</b> <b>assumptions</b> as a disciplined part of the software development process...|$|R
25|$|The models {{described}} {{on this page}} describe {{the evolution of a}} single site within a set of sequences. They are often used for analyzing the evolution of an entire locus by making the <b>simplifying</b> <b>assumption</b> that different sites evolve independently and are identically distributed. This assumption may be justifiable if the sites can be assumed to be evolving neutrally. If the primary effect of natural selection on the evolution of the sequences is to constrain some sites, then models of among-site rate-heterogeneity can be used. This approach allows one to estimate only one matrix of relative rates of substitution, and another set of parameters describing the variance in the total rate of substitution across sites.|$|E
500|$|Leavitt {{used the}} <b>simplifying</b> <b>assumption</b> {{that all of}} the Cepheids within each Magellanic Cloud were at {{approximately}} the same distance from Earth, so that their intrinsic brightness could be deduced from their apparent brightness (as measured from the photographic plates) and from the distance to each of the clouds. [...] "Since the variables are probably at nearly the same distance from the Earth, their periods are apparently associated with their actual emission of light, as determined by their mass, density, and surface brightness." ...|$|E
500|$|The {{handicap}} principle {{has proven}} hard to test empirically, {{partly because of}} inconsistent interpretations of Zahavi’s metaphor and Grafen’s marginal fitness model, {{and partly because of}} conflicting empirical results: in some studies individuals with bigger signals seem to pay higher costs, in other studies they seem to be paying lower costs. [...] A possible explanation for the inconsistent empirical results is given in a series of papers by Getty, who shows that Grafen’s proof of the handicap principle is based on the critical <b>simplifying</b> <b>assumption</b> that signallers trade off costs for benefits in an additive fashion, the way humans invest money to increase income in the same currency. But the assumption that costs and benefits trade off in an additive fashion is true only on a logarithmic scale; for the survival cost – reproduction benefit tradeoff is assumed to mediate the evolution of sexually selected signals. Fitness depends on producing offspring, which is a multiplicative function of reproductive success given an individual is still alive times the probability of still being alive, given investment in signals.|$|E
5000|$|The <b>simplifying</b> <b>assumptions</b> of {{geometrical}} optics include that light rays: ...|$|R
5000|$|During the {{feasibility}} studies, the following basic <b>simplifying</b> <b>assumptions</b> are made: ...|$|R
40|$|<b>Simplifying</b> <b>assumptions</b> lead to {{approximate}} closed-form solution. Theoretical paper discusses melting of solid sphere in spherical container. Develops mathematical model of melting process, {{based in part}} on <b>simplifying</b> <b>assumptions</b> like those used in theories of lubrication and film condensation. Resulting equation for melting speed as function of melting distance solved approximately in closed form...|$|R
2500|$|A {{fundamental}} <b>simplifying</b> <b>assumption</b> of Unix was {{its focus}} on newline-delimited text for nearly all file formats. There were no [...] "binary" [...] editors in the original version of Unix– the entire system was configured using textual shell command scripts. The common denominator in the I/O system was the byte– unlike [...] "record-based" [...] file systems. The focus on text for representing nearly everything made Unix pipes especially useful, and encouraged the development of simple, general tools that could be easily combined to perform more complicated ad hoc tasks. The focus on text and bytes made the system far more scalable and portable than other systems. Over time, text-based applications have also proven popular in application areas, such as printing languages (PostScript, ODF), and at the application layer of the Internet protocols, e.g., FTP, SMTP, HTTP, SOAP, and SIP.|$|E
2500|$|Any {{attempt at}} ancestral {{reconstruction}} {{begins with a}} phylogeny. [...] In general, a phylogeny is a tree-based hypothesis about {{the order in which}} populations (referred to as taxa) are related by descent from common ancestors. [...] Observed taxa are represented by the tips or terminal nodes of the tree that are progressively connected by branches to their common ancestors, which are represented by the branching points of the tree that are usually referred to as the ancestral or internal nodes. [...] Eventually, all lineages converge to the most recent common ancestor of the entire sample of taxa. [...] In the context of ancestral reconstruction, a phylogeny is often treated {{as though it were a}} known quantity (with Bayesian approaches being an important exception). [...] Because there can be an enormous number of phylogenies that are nearly equally effective at explaining the data, reducing the subset of phylogenies supported by the data to a single representative, or point estimate, can be a convenient and sometimes necessary <b>simplifying</b> <b>assumption.</b>|$|E
2500|$|A common {{variant of}} the problem, assumed by several {{academic}} authors as the canonical problem, {{does not make the}} <b>simplifying</b> <b>assumption</b> that the host must uniformly choose the door to open, but instead that he uses some other strategy. The confusion as to which formalization is authoritative has led to considerable acrimony, particularly because this variant makes proofs more involved without altering the optimality of the always-switch strategy for the player. In this variant, the player can have different probabilities of winning depending on the observed choice of the host, but in any case the probability of winning by switching is at least [...] (and can be as high as 1), while the overall probability of winning by switching is still exactly [...] The variants are sometimes presented in succession in textbooks and articles intended to teach the basics of probability theory and game theory. A considerable number of other generalizations have also been studied.|$|E
5000|$|Retention models {{make several}} <b>simplifying</b> <b>assumptions</b> and often involve the {{following}} inputs: ...|$|R
25|$|This section relates a {{historically}} important -body problem solution after <b>simplifying</b> <b>assumptions</b> were made.|$|R
5000|$|Riccati-based {{approaches}} solve 2 Riccati equations to {{find the}} controller, but require several <b>simplifying</b> <b>assumptions.</b>|$|R
2500|$|The first {{ray tracing}} {{algorithm}} used for rendering was presented by Arthur Appel in 1968. This algorithm {{has since been}} termed [...] "ray casting". The idea behind ray casting is to shoot rays from the eye, one per pixel, and find the closest object blocking the path of that ray. Think of an image as a screen-door, with each square in the screen being a pixel. [...] This is then the object the eye sees through that pixel. [...] Using the material properties {{and the effect of}} the lights in the scene, this algorithm can determine the shading of this object. [...] The <b>simplifying</b> <b>assumption</b> is made that if a surface faces a light, the light will reach that surface and not be blocked or in shadow. [...] The shading of the surface is computed using traditional 3D computer graphics shading models. [...] One important advantage ray casting offered over older scanline algorithms was its ability to easily deal with non-planar surfaces and solids, such as cones and spheres. If a mathematical surface can be intersected by a ray, it can be rendered using ray casting. Elaborate objects can be created by using solid modeling techniques and easily rendered.|$|E
5000|$|In many applications, a third <b>simplifying</b> <b>assumption</b> can be made: ...|$|E
5000|$|Removing the <b>simplifying</b> <b>assumption</b> {{of uniform}} {{gravitational}} acceleration provides more accurate results. We find from {{the formula for}} radial elliptic trajectories: ...|$|E
5000|$|Several <b>simplifying</b> <b>assumptions</b> {{are made}} while {{considering}} {{the development of}} algorithms for PRAM. They are: ...|$|R
50|$|The {{model and}} its {{variants}} {{have a number}} of <b>simplifying</b> <b>assumptions.</b> Three of them are listing below.|$|R
5000|$|... {{the role}} of <b>simplifying</b> <b>assumptions</b> such as {{rational}} choice and profit maximizing in explaining or predicting phenomena ...|$|R
50|$|A <b>simplifying</b> <b>assumption</b> {{sometimes}} made is for {{the addition}} of fuel flow to be exactly offset by an overboard compressor bleed, so mass flow remains constant throughout the cycle.|$|E
50|$|Because of the <b>simplifying</b> <b>assumption</b> {{that some}} of the metric {{coefficients}} are zero, some of our results in this motivational treatment will not be as general as they could be.|$|E
50|$|For {{options on}} indices, it is {{reasonable}} to make the <b>simplifying</b> <b>assumption</b> that dividends are paid continuously, and that the dividend amount is proportional {{to the level of the}} index.|$|E
50|$|The phrase {{comes from}} a joke that spoofs the <b>simplifying</b> <b>assumptions</b> that are {{sometimes}} used in theoretical physics.|$|R
40|$|An {{approach}} of structural optimization {{has been used}} to optimize the weight of a simply supported, corrugated hat stiffened composite panel under uniaxial compression. The approach consists of the employment of nonlinear mathematical programming techniques to reach an optimum solution. Some <b>simplifying</b> <b>assumptions</b> are made in the stress analysis to obtain faster convergence to an optimum solution. With these <b>simplifying</b> <b>assumptions</b> the number of unknown design parameters is reduced to twelve...|$|R
40|$|When {{analyzing}} {{the performance of}} an aircraft, certain <b>simplifying</b> <b>assumptions,</b> which decrease {{the complexity of the}} problem, can often be made. The degree of accuracy required in the solution may {{determine the extent to which}} these <b>simplifying</b> <b>assumptions</b> are incorporated. A complex model may yield more accurate results if it describes the real situation more thoroughly. However, a complex model usually involves more computation time, makes the analysis more difficult, and often requires more information to do the analysis. Therefore, to choose the <b>simplifying</b> <b>assumptions</b> intelligently, it is important to know what effects the assumptions may have on the calculated performance of a vehicle. Several <b>simplifying</b> <b>assumptions</b> are examined, the effects of simplified models to those of the more complex ones are compared, and conclusions are drawn about the impact of these assumptions on flight envelope generation and optimal trajectory calculation. Models which affect an aircraft are analyzed, but the implications of simplifying the model of the aircraft itself are not studied. The examples are atmospheric models, gravitational models, different models for equations of motion, and constraint conditions...|$|R
50|$|Gurney made a <b>simplifying</b> <b>assumption</b> {{that there}} is a linear {{velocity}} gradient in the explosive detonation product gases. This works well for most configurations, but see the section Anomalous predictions below.|$|E
5000|$|... #Caption: The {{terms of}} the Landau-Lifshitz-Gilbert equation: {{precession}} (red) and damping (blue). The trajectory of the magnetization (dotted spiral) is drawn under the <b>simplifying</b> <b>assumption</b> that the effective field Heff is constant.|$|E
50|$|The unique name {{assumption}} is a <b>simplifying</b> <b>assumption</b> made in some ontology languages and description logics. In logics with the unique name assumption, different names always refer to different entities in the world.|$|E
5000|$|To {{solve for}} {{the motion of}} an object in a two body system, two <b>simplifying</b> <b>assumptions</b> can be made: ...|$|R
50|$|Using some <b>simplifying</b> <b>assumptions,</b> the Moens-Korteweg {{equation}} can be derived, {{an equation}} that directly relates PWV and artery wall stiffness.|$|R
40|$|We {{develop a}} Stochastic Dominance {{methodology}} to analyze if new assets expand the investment possibilities for rational nonsatiable and risk-averse investors. This methodology avoids the <b>simplifying</b> <b>assumptions</b> underlying the traditional mean-variance approach to spanning. The methodology {{is applied to}} analyze the stock market behavior of small firms {{in the month of}} January. Our findings suggest that the previously observed January effect is remarkably robust with respect to <b>simplifying</b> <b>assumptions</b> regarding the return distribution...|$|R
