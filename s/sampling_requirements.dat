206|962|Public
2500|$|... (see {{the section}} Beyond Nyquist below) using {{compressed}} sensing. In particular, the theory, using signal processing language, {{is described in}} this 2009 paper. They show, among other things, that if the frequency locations are unknown, then {{it is necessary to}} sample at least at twice the Nyquist criteria; in other words, you must pay at least a factor of 2 for not knowing the location of the spectrum. Note that minimum <b>sampling</b> <b>requirements</b> do not necessarily guarantee stability.|$|E
5000|$|Ambiguous azimuth {{aliasing}} usually {{occurs when}} the Nyquist spatial <b>sampling</b> <b>requirements</b> are exceeded by frequencies. Unambiguous aliasing occurs in squinted geometries where the signal bandwidth does not exceed the sampling limits, but has undergone [...] "spectral wrapping." [...] Backprojection Algorithm does not get affected by any such kind of aliasing effects.|$|E
50|$|Composite {{sampling}} can {{be performed}} by combining soil from several locations prior to analysis. This is a common procedure, but should be used judiciously to avoid skewing results. This procedure must be done so that government <b>sampling</b> <b>requirements</b> are met. A reference map should be created to record the location and quantity of field samples in order to properly interpret test results.|$|E
30|$|Norway did {{not meet}} the <b>sample</b> <b>requirements</b> of TEDS-M, the {{response}} rate was less than 60  %.|$|R
50|$|The {{selection}} of microtome knife blade profile {{depends upon the}} material and preparation of the samples, {{as well as the}} final <b>sample</b> <b>requirements</b> (e.g. cut thickness and quality).|$|R
30|$|Airborne {{early warning}} (AEW) radar needs to {{instantly}} detect moving targets, which are saturated {{in the presence}} of dense ground clutter. It is crucial to suppress the ground clutter. As an effective clutter suppression method in AEW radar, STAP undertakes huge computational cost and excessive training <b>sample</b> <b>requirement,</b> which cannot be fully satisfied in the practical battlefield environment. Moreover, the computational cost and training <b>sample</b> <b>requirement</b> are even huger in MIMO-STAP. Therefore, in this paper, an improving STAP method with tractable computational cost and training <b>sample</b> <b>requirement</b> that applies into the airborne MIMO radar is proposed. The proposed method consists of two parts. The first part is CCM estimation, which is computed according to the original data and the corresponding persymmetry data. The second part is weight vector computation, which is iteratively obtained. We firstly establish the signal model, which will be a foundation for the development and performance analysis of the proposed method.|$|R
50|$|The {{general theory}} for non-baseband and {{nonuniform}} samples {{was developed in}} 1967 by Landau. He proved that the average sampling rate (uniform or otherwise) must be twice the occupied bandwidth of the signal, assuming it is a priori known what portion of the spectrum was occupied.In the late 1990s, this work was partially extended to cover signals of when the amount of occupied bandwidth was known, but the actual occupied portion of the spectrum was unknown. In the 2000s, a complete theory was developed(see the section Beyond Nyquist below) using compressed sensing. In particular, the theory, using signal processing language, is described in this 2009 paper. They show, among other things, that if the frequency locations are unknown, then {{it is necessary to}} sample at least at twice the Nyquist criteria; in other words, you must pay at least a factor of 2 for not knowing the location of the spectrum. Note that minimum <b>sampling</b> <b>requirements</b> do not necessarily guarantee stability.|$|E
5000|$|The {{general theory}} for non-baseband and {{nonuniform}} samples {{was developed in}} 1967 by Henry Landau. [...] He proved that the average sampling rate (uniform or otherwise) must be twice the occupied bandwidth of the signal, assuming it is a priori known what portion of the spectrum was occupied.In the late 1990s, this work was partially extended to cover signals for which the amount of occupied bandwidth was known, but the actual occupied portion of the spectrum was unknown. [...] In the 2000s, a complete theory was developed(see the section Beyond Nyquist below) using compressed sensing. In particular, the theory, using signal processing language, is described in this 2009 paper. [...] They show, among other things, that if the frequency locations are unknown, then {{it is necessary to}} sample at least at twice the Nyquist criteria; in other words, you must pay at least a factor of 2 for not knowing the location of the spectrum. Note that minimum <b>sampling</b> <b>requirements</b> do not necessarily guarantee numerical stability.|$|E
3000|$|... and z trajectories is analyzed. The {{corresponding}} resolution {{values in}} both slow-time trajectories are calculated {{and the corresponding}} <b>sampling</b> <b>requirements</b> are determined.|$|E
30|$|A {{method that}} {{decreases}} the training <b>sample</b> <b>requirement</b> and computational cost in the airborne MIMO radar for clutter suppression has been proposed. It estimates CCM by the persymmetry property. Then, the weight vector is constructed as the Kronecker product of two short weight vectors. The bi-iterative method {{is applied to}} find the desired result. Simulation {{results show that the}} proposed method is advantageous in convergence rate and training <b>sample</b> <b>requirement.</b> The proposed method has greater clutter suppression ability compared with the previously proposed post-Doppler adaptive processing methods especially in the airborne MIMO radar with limited homogeneous training sample support.|$|R
30|$|In addition, the {{advantage}} of MALDI-TOF-MS is the direct use of crude sample, large-scale, high-throughput, automated, and minimal <b>sample</b> <b>requirements.</b> MALDI-TOF-MS not only can find a protein or biological markers, it also detected the combination of proteins existing in different forms (Gould et al. 2004).|$|R
5000|$|In {{the digital}} {{implementation}} of AST (DAST) that is performed in 2D {{and applied to}} digital images, an appropriately designed warp kernel stretches the input {{in a way that}} reduces the overall spatial bandwidth and hence the <b>sampling</b> <b>requirement.</b> The previous equation for AST can be rewritten in discrete form for DAST as: ...|$|R
40|$|We {{study the}} problem of synthesizing the sound field at {{arbitrary}} locations and times from the recordings of an array of audio sensors. Given prior estimates of the locations and frequencies of the sound sources, such as those obtained using adaptive source localization, we characterize the spatio-temporal support of the sound field spectrum. This characterization allows the spatial <b>sampling</b> <b>requirements</b> to be reduced in comparison to when no prior estimates of the sources are utilized. We derive an adaptive interpolation kernel, based on the estimated spectral support, to reconstruct the sound-field function using measurements from sensors on a coarse spatial-sampling grid. Simulation results demonstrate the gain achieved in reduced <b>sampling</b> <b>requirements</b> by using the proposed adaptive interpolation approach. 1...|$|E
40|$|Problems {{associated}} with achieving precision in photometric measurements of stars are examined. The thermal stabilization {{of glass and}} interference filters and the determination of correct analytic representations of bandwidth effects in data reduction are particularly discussed. Spectral <b>sampling</b> <b>requirements</b> are also addressed...|$|E
40|$|The US DOE {{requires}} its {{contractors to}} conduct air sampling {{to detect and}} evaluate airborne radioactive material in the workplace. Hanford Reservation T Plant compliance with workplace air <b>sampling</b> <b>requirements</b> has been assessed. Requirements, basis for determining compliance and recommendations are included...|$|E
40|$|Circular dichroism (CD) {{has become}} an {{increasingly}} important tool {{in the study of}} biological molecules as it enables structural information to be obtained nondestructively on solution-phase <b>samples.</b> However, <b>sample</b> <b>requirements</b> for CD are often seen as being too high with protein backbone measurements in standard cuvettes typically requiring ~ 100 – 300 μL of 0. 1 mg/ml protein. To address this issue, we have designed a new form of CD sample holder, which reduces the <b>sample</b> <b>requirements</b> of the technique by two orders of magnitude, with a <b>sample</b> <b>requirement</b> of less than 3 μl. This sample saving has been achieved through the use of extruded quartz capillaries, the sample being held within the internal diameter of the quartz capillary through capillary action. The extruded quartz capillaries exhibit remarkably little birefringence, although still transmitting high energy UV circularly polarized light. The optics associated with capillaries were investigated. A configuration has been adopted with the light beam of the spectrophotometer being focused in front of the front face of the capillary using a biconvex lens and advantage being taken of the additional focusing effect of the capillary itself. The focusing is vital to the low wavelength performance of the cell, where we have acquired reliable data down to 180 nm using a Jasco J- 815 spectrophotometer. The system performance was validated with Na[Co(EDDS) ]. H₂O (EDDS = N,N-ethylenediaminedisuccinic acid), concanavalin A, lysozyme, and progesterone. Chirality 22 :E 136 –E 141, 2010. 6 page(s...|$|R
40|$|In the {{continuous}} drive to increase screening throughput and reduce <b>sample</b> <b>requirement,</b> microarray-based technologies {{have risen to}} the occasion. In the past 7 years, {{a number of new}} methodologies have been developed for preparing small molecule microarrays from combinatorial and natural product libraries with the goal of identifying new interactions or enzymatic activities. Recent advances and applications of small molecule microarrays are reviewed...|$|R
30|$|And many {{efforts have}} been made to develop {{suitable}} compression techniques for Big Data. However, traditional compression methods [5, 6] are all based on Nyquist rate, which will have poor efficiency in terms of both sampling rate and computational complexity. Unlike traditional compression techniques, some sparse sampling algorithms have been proposed to overcome Nyquist <b>sampling</b> <b>requirement,</b> like compressive sensing, nested sampling, and coprime sampling.|$|R
40|$|A trade {{study was}} {{performed}} on twenty-one digital output interface schemes for gas turbine electronic controls to select the most promising scheme based on criteria of reliability, performance, cost, and <b>sampling</b> <b>requirements.</b> The most promising scheme, a digital effector with optical feedback of the fuel metering valve position, was designed...|$|E
40|$|We {{present a}} general class of {{compressed}} sensing matrices {{which are then}} demonstrated to have associated sublinear-time sparse approximation algorithms. We then develop methods for constructing specialized matrices from this class which are sparse when multiplied with a discrete Fourier transform matrix. Ultimately, these considerations improve previous <b>sampling</b> <b>requirements</b> for deterministic sparse Fourier transform methods. ...|$|E
40|$|A {{method is}} {{described}} {{to assess the}} reproductive status of male Hippocampus abdominalis {{on the basis of}} behavioural traits. The non-invasive nature of this technique minimizes handling stress and reduces <b>sampling</b> <b>requirements</b> for experimental work. It represents a useful tool to assist researchers in sample collection for studies of reproduction and development in viviparous syngnathids, which are emerging as important model species...|$|E
40|$|Conclusion: These {{groups of}} Irish EMTs {{appeared}} keen {{to participate in}} continuous professional competence activities. In addition, these EMTs identified areas that, in their opinion, required clarification by the Regulator related to the practicalities of CPC and the governance and administration of CPC. More information, dissemination of <b>sample</b> <b>requirements</b> and further effective engagement with the Regulator {{could be used to}} refine the current CPC requirements for EMTs...|$|R
40|$|In this letter, we {{proposed}} a ghost imaging (GI) and distributed antennas based microwave surveillance scheme. By analyzing its imaging resolution and <b>sampling</b> <b>requirement,</b> the potential of employing microwave GI to achieve high-quality surveillance performance with low system complexity has been demonstrated. The theoretical analysis and effectiveness of the proposed microwave surveillance method are also validated via simulations. Comment: 4 pages, 11 figures, submitted for possible journal publicatio...|$|R
40|$|This paper explores <b>sample</b> size <b>requirements</b> for the {{estimation}} of SUR models by (two-stage) feasible generalized least squares, maximum likelihood and Bayesian methods. It is found that the <b>sample</b> size <b>requirements</b> presented in standard treatments of SUR models are incomplete and potentially misleading. It is also demonstrated that likelihood-based methods potentially require larger sample sizes than does the two-stage estimator considered in this paper. ECONOMETRIC MODELS; ECONOMETRICS; ESTIMATORS...|$|R
40|$|Science {{requirements}} for a Global Change Technology Initiative (GCTI) Architecture Trade Study were established by reviewing and synthesizing results from recent studies. A scientific rationale was adopted {{and used to}} identify a comprehensive set of measurables and their priorities. Spatial and temporal {{requirements for}} a number of measurement parameters were evaluated based on results from several working group studies. Science requirements were defined using these study results in conjunction with guidelines for investigating global changes over a time scale of decades to centuries. Requirements are given separately for global studies and regional process studies. For global studies, temporal requirements are for sampling every 1 to 12 hours for atmospheric and radiation parameters and 1 day or more for most Earth surface measurements. Therefore, the atmospheric measurables provide the most critical drivers for temporal sampling. Spatial <b>sampling</b> <b>requirements</b> vary from 1 km for land and ocean surface characteristics to 50 km for some atmospheric parameters. Thus, the land and ocean surface parameters have the more significant spatial variations and provide the most challenging spatial <b>sampling</b> <b>requirements...</b>|$|E
40|$|We {{demonstrate}} {{an application}} of spherical harmonic decomposition to analysis of the human electroencephalogram (EEG). We implement two methods and discuss issues specific to analysis of hemispherical, irregularly sampled data. Performance of the methods and spatial <b>sampling</b> <b>requirements</b> are quantified using simulated data. The analysis is applied to experimental EEG data, confirming earlier reports of an approximate frequency-wavenumber relationship in some bands. Comment: 12 pages, 8 figures, submitted to Phys. Rev. E, uses APS RevTeX style...|$|E
40|$|This report {{defines the}} Double-Shell Tank (DST) Process Waste Sampling Subsystem (PWSS). This {{subsystem}} definition report fully describes and identifies the system {{boundaries of the}} PWSS. This definition provides a basis for developing functional, performance, and test requirements (i. e., subsystem specification), as necessary, for the PWSS. The resultant PWSS specification will include the <b>sampling</b> <b>requirements</b> to support the transfer of waste from the DSTs to the Privatization Contractor during Phase 1 of Waste Feed Delivery...|$|E
30|$|In general, the space-time {{adaptive}} processing (STAP) {{can achieve}} excellent clutter suppression and moving target detection {{performance in the}} airborne multiple-input multiple-output (MIMO) radar for the increasing system degrees of freedom (DoFs). However, the performance improvement {{is accompanied by a}} dramatic increase in computational cost and training <b>sample</b> <b>requirement.</b> As one of the most efficient dimension-reduced STAP methods, the extended factored approach (EFA) transforms the full-dimension STAP problem into several small-scale adaptive processing problems, and therefore alleviates the computational cost and training <b>sample</b> <b>requirement.</b> However, it cannot effectively work in the airborne MIMO radar since sufficient training samples are unavailable. Aiming at the problem, a fast iterative method using persymmetry covariance matrix estimation in the airborne MIMO radar is proposed. In this method, the clutter covariance matrix is estimated by the original data and the constructed data. Then, the spatial weight vector in EFA is decomposed into the Kronecker product of two short-weight vectors. The bi-iterative algorithm is exploited to obtain the desired weight vectors. Simulation results demonstrate the effectiveness of our proposed method.|$|R
40|$|Mechanistic crop growth {{models are}} {{becoming}} increasingly important in agricultural research and are extensively used in climate change impact assessments. In such studies, statistics of crop yields are usually evaluated without the explicit consideration of <b>sample</b> size <b>requirements.</b> The {{purpose of this paper}} was to identify minimum sample sizes for the estimation of average, standard deviation and skewness of maize and winterwheat yields based on simulations carried out under a range of climate and soil conditions. Our results indicate that 15 years of simulated crop yields are sufficient to estimate average crop yields with a relative error of less than 10 % at 95 % confidence. Regarding standard deviation and skewness, <b>sample</b> size <b>requirements</b> depend on the degree of symmetry of the underlying population’s distribution. For symmetric distributions, samples of 200 and 1500 yield observations are needed to estimate the crop yields’ standard deviation and skewness coefficient, respectively. Higher degrees of asymmetry increase the <b>sample</b> size <b>requirements</b> relative to the estimation of the standard deviation, {{while at the same time}} the <b>sample</b> size <b>requirements</b> relative to the skewness coefficient are decreased...|$|R
40|$|The {{development}} of serial crystallography {{has been driven}} by the <b>sample</b> <b>requirements</b> imposed by X-ray free-electron lasers. Serial techniques are now being exploited at synchrotrons. Using a fixed-target approach to high-throughput serial sampling, it is demonstrated that high-quality data can be collected from myoglobin crystals, allowing room-temperature, low-dose structure determination. The combination of fixed-target arrays and a fast, accurate translation system allows high-throughput serial data collection at high hit rates and with low sample consumption...|$|R
40|$|The high {{probability}} of degenerate frequencies in NMR spectra of complex biopolymers such as proteins presented a great barrier to detailed analysis. The combination of multidimensional NMR spectroscopy and high magnetic field strengths has overcome the resulting resonance assignment problem for proteins less than 50 kDa. However, as protein size increases the sampling and sensitivity limited regimes become apparent. As a consequence, the orthogonal linear <b>sampling</b> <b>requirements</b> of conventional multidimensional NMR spectroscopy, combined with increased signal averaging require a longer acquisition time than is feasible. To overcome these limitations, radial {{sampling of the}} indirect dimensions of multidimensional experiments is utilized. It is demonstrated here, that through optimization of radial sampling acquisition parameters, {{it is possible to}} escape the linear sequential <b>sampling</b> <b>requirements</b> of Cartesian sampling, which allows for the collection of a high resolution spectrum in reduced acquisition time. Further, by exploiting a fundamental statistical advantage of radial sampling, it is possible to obtain a signal-to-noise advantage, over the traditional methodology. The approach is generalized by developing an all inclusive NMR data processing package and associated programs to optimize radial sampling acquisition parameters. An example, which utilizes th...|$|E
40|$|This {{sampling}} analysis {{sets forth}} the effluent <b>sampling</b> <b>requirements,</b> analytical methods, statistical analyses, and reporting requirements {{to satisfy the}} State Waste Discharge Permit No. ST 4502 for the Treated Effluent Disposal Facility. These requirements are listed below: Determine the variability in the effluent of all constituents for which enforcement limits, early warning values and monitoring requirements; demonstrate compliance with the permit; and verify that BAT/AKART (Best Available Technology/All know and Reasonable Treatment) source, treatment, and technology controls are being met...|$|E
40|$|With {{the rapid}} {{development}} of the one-stationary bistatic forward-looking synthetic aperture radar (OS-BFSAR) technology, the huge amount of the remote sensing data presents challenges for real-time imaging processing. In this paper, an efficient time-domain algorithm (ETDA) considering the motion errors for the OS-BFSAR imaging processing, is presented. This method can not only precisely handle the large spatial variances, serious range-azimuth coupling and motion errors, but can also greatly improve the imaging efficiency compared with the direct time-domain algorithm (DTDA). Besides, it represents the subimages on polar grids in the ground plane instead of the slant-range plane, and derives the <b>sampling</b> <b>requirements</b> considering motion errors for the polar grids to offer a near-optimum tradeoff between the imaging precision and efficiency. First, OS-BFSAR imaging geometry is built, and the DTDA for the OS-BFSAR imaging is provided. Second, the polar grids of subimages are defined, and the subaperture imaging in the ETDA is derived. The <b>sampling</b> <b>requirements</b> for polar grids are derived {{from the point of}} view of the bandwidth. Finally, the implementation and computational load of the proposed ETDA are analyzed. Experimental results based on simulated and measured data validate that the proposed ETDA outperforms the DTDA in terms of the efficiency improvement...|$|E
40|$|One of {{the most}} popular {{algorithms}} for clustering in Euclidean space is the k-means algorithm; k-means is difficult to analyze mathematically, and few theoretical guarantees are known about it, particularly when the data is well-clustered. In this paper, we attempt to fill this gap in the literature by analyzing the behavior of k-means on well-clustered data. In particular, we study the case when each cluster is distributed as a different Gaussian [...] or, in other words, when the input comes from a mixture of Gaussians. We analyze three aspects of the k-means algorithm under this assumption. First, we show that when the input comes from a mixture of two spherical Gaussians, a variant of the 2 -means algorithm successfully isolates the subspace containing the means of the mixture components. Second, we show an exact expression for the convergence of our variant of the 2 -means algorithm, when the input is {{a very large number of}} samples from a mixture of spherical Gaussians. Our analysis does not require any lower bound on the separation between the mixture components. Finally, we study the <b>sample</b> <b>requirement</b> of k-means; for a mixture of 2 spherical Gaussians, we show an upper bound on the number of samples required by a variant of 2 -means to get close to the true solution. The <b>sample</b> <b>requirement</b> grows with increasing dimensionality of the data, and decreasing separation between the means of the Gaussians. To match our upper bound, we show an information-theoretic lower bound on any algorithm that learns mixtures of two spherical Gaussians; our lower bound indicates that in the case when the overlap between the probability masses of the two distributions is small, the <b>sample</b> <b>requirement</b> of k-means is near-optimal...|$|R
3000|$|Compared to {{classical}} Monte Carlo method, the importance sampling technique, reduces the <b>sample</b> size <b>requirement</b> {{by a factor}} ranging from [...]...|$|R
40|$|Jugular venous blood {{should be}} {{collected}} into EDTA tubes for haematology and tubes without anticoagu-lant for most routine biochemistry tests. For testing for fibrinogen, a sample tube containing citrate is usu-ally used and, for blood glucose, fluoride oxalate is the sample tube of choice. <b>Sample</b> <b>requirements</b> for some tests (eg, fibrinogen) differ between laboratories, so if in doubt, contact the laboratory {{to which you}} intend to submit the samples. If a flock problem is being investi-F...|$|R
