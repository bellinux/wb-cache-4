4|15|Public
40|$|An {{embedded}} English synthesis approach {{based on}} <b>speech</b> <b>concatenation</b> and smoothing is described. This approach adopts phonetic sub-words as carrier of variable-length units. We define 5 -class units {{to cover all}} English phonetic phenomena. The corresponding cost function and search procedure based on dynamic programming are addressed in the unit-selection stage. Vocal tract response, pitch value and phase are interpolated and merged at concatenating points for smoothing speech in the synthesis stage. The preliminary test shows that this approach can reach a good balance of naturalness, intelligibility and data footprint. 1...|$|E
40|$|In this paper, a corpus-based speech {{synthesis}} system KB 2006 was developed using the speech database provided by Blizzard Challenge 2006. We proposed a novel unit selection method called multi-tier non-uniform unit selection in our corpus-base {{speech synthesis}} system. Non-uniform unit (NUU) {{in our system}} {{was defined as a}} unit sequences that contains one or more joint phoneme units. By using CART algorithm, NUUs with the same phoneme sequence in the inventory were clustered into different classes according to their prosody and acoustic difference. In the unit selection stage, a multi-tier NUUs selection algorithm was adopted by treating different NUUs with several criterions. With the discrimination, proper candidate units that close to the target unit can be selected for <b>speech</b> <b>concatenation.</b> 1...|$|E
40|$|Prolonged {{speech and}} its {{variants}} {{are widely used}} in the behavioral treatment of stuttering. Unlike these approaches, which depend on clinician-prescribed speech pattern changes, two behavioral treatment regimens, one for children and another for adults, recently developed at the Australian Stuttering Research Center, promote self-monitoring of speech {{as a means of}} controlling stuttering. In these programs, the clients themselves modify their speech in subtle and variable ways to gain control over stuttering and, in that, they appear to be similar to a well-known experimental technique for suppressing stutters known as response contingent stimulation. The present paper provides an integrated explanation for the effectiveness of both clinician-directed as well as client-initiated speech pattern modifications and, in the process, develops a new model of stuttering. It also shows why client-generated speech patterns changes potentially produce faster and more lasting improve-ment than those changes prescribed by a clinician. Learning outcomes: The reader will learn about: (1) two hypothesized methods of preparing utterance motor plans—speech concatenation and speech construction; (2) how behavioral treatment programs make use of speech construction to promote fluency in persons who stutter; (3) why therapy procedures based on cognitively driven speech construction produce faster and superior results than those based on motorically driven speech construction; and (4) the empirical evidence that suggests that <b>speech</b> <b>concatenation</b> is the source of stuttering...|$|E
40|$|This report {{describes}} {{a novel approach}} to segmental re-synthesis of child <b>speech,</b> by <b>concatenation</b> of <b>speech</b> from different speakers. The re-synthesis builds upon standard methods of unit selection, but instead of using speech from only one speaker, target segments are selected from a speech database of many child speakers. Results from a listener evaluation suggest that the method {{can be used to}} generate intelligible speech that is difficult to distinguish from original recordings...|$|R
40|$|Synthesized {{speech from}} text-to-speech systems is {{generally}} produced from the concatenation of small units of <b>speech.</b> The <b>concatenation</b> {{process can be}} complex, involving smoothing and context dependent adjustments to the speech. The overall quality of the speech produced will depend {{in large part on}} the quality of the elements used for concatenation. Selection and evaluation of these elements has been done entirely by hand. The proposed work addresses the process by which these concatenative elements are created from a natural voice and optimized. The optimization uses distance measures which exploit detailed information on the structure of the speech signals...|$|R
40|$|If a concatenative speech {{synthesis}} system uses more short speech segments, {{it increases the}} potential to generate natural <b>speech</b> because the <b>concatenation</b> variation becomes greater. Recently, a synthesis approach {{was proposed in which}} very short(5 ms) segmentsare used. In this paper, an implementation of an HMM-based feature generation module into a very short segment concatenative synthesis system that has the advantage of modularityand a synthesisexperimentare described. 1...|$|R
40|$|Text-to-Speech (TTS) {{system is}} {{designed}} to generate natural and intelligible sounding speech from any arbitrary input Odia digital text. The arbitrary text can be generated from a corresponding image file that undergoes processing through an Optical Character Recognition (OCR) scheme. This project is a research work on various techniques viable to develop the segmentation phase of the OCR specifically and a corresponding TTS for Odia language. The pre-processing and segmentation phases of the OCR are thoroughly explored in our work. A scheme is contrived for extracting the atomic Odia characters from a given string of text consisting of both characters and matras. The concept of histograms has come handy in our research. An altogether new L-corner scheme is formulated to handle the exceptional cases. Extensive simulations are carried out to validate {{the efficacy of the}} proposed algorithm. Results show that the proposed scheme attains a high level of accuracy. The methodology used in TTS is to exploit acoustic representations of speech for synthesis, together with linguistic analyses of text to extract correct pronunciations and prosody in context of Odia language. Phonetic analysis to convert grapheme to phoneme is achieved using an array numbering system for the audio files comprising of pre-recorded natural human <b>speech.</b> <b>Concatenation</b> of phones and diphones generates the speech. The phoneme database creation studying the prosody of Odia language is our primary focus in this project apart from accurate and intelligible speech generation...|$|E
40|$|Abstract — This paper {{describes}} the first Text-to-Speech (TTS) {{system for the}} Mongolian language, using the general speech synthesis architecture of Festival. The TTS is based on diphone concatenative synthesis, applying TD-PSOLA technique. The conversion process from input text into acoustic waveform is performed {{in a number of}} steps consisting of functional components. Procedures and functions for the steps and their components are discussed in detail. Finally, the quality of synthesised speech is assessed in terms of acceptability and intelligibility. future work are mentioned. Index Terms—Diphone <b>concatenation,</b> <b>Speech</b> synthesis. I...|$|R
40|$|In {{the paper}} the {{development}} of Slovenian speech corpus for use in concatenative speech synthesis system being developed at University of Maribor, Slovenia, will be presented. The emphasis in {{the paper is the}} issue of maximising the usefulness of the defined <b>speech</b> corpus for <b>concatenation</b> purposes. Usefulness of the speech corpus very much depends on the corresponding text and can be increased if the appropriate text is chosen. In the approach we used, detailed statistics of the text corpora has been done, to be able to define the sentences, rich with non-uniform units like monophones, diphones and triphones. 1...|$|R
40|$|This paper {{describes}} and applies a new algorithm for decomposing pitch curves into component curves, {{in accordance}} with the General Superpositional Model of Intonation. According to this model, which is a generalization of the Fujisaki model [3], a pitch contour can be described as the sum of component curves that are each associated with different phonological levels, including the phrase, foot, and phoneme. The algorithm assumes that the phrase curve is locally linear during intervals spanned by a foot. The algorithm was evaluated using synthetically generated curves, and was found to accurately recover the synthetic component curves. The algorithm was also evaluated in a perceptual experiment, where <b>speech</b> generated by <b>concatenation</b> of accent curves was shown to produce better speech quality than speech based on direct concatenation of “raw ” pitch curve fragments. 1...|$|R
50|$|CereProc's unit {{selection}} {{voices are}} built from large databases of recorded speech. During database creation, each recorded utterance is segmented into {{some or all}} of the following: individual phones, syllables, morphemes, words, phrases, and sentences. The division into segments is done using a specially modified speech recognizer. An index of the units in the speech database is then created based on the segmentation and acoustic parameters like the fundamental frequency (pitch), duration, position in the syllable, and neighbouring phones. At runtime, the desired target utterance is created by determining the best chain of candidate units from the database (unit selection). Unit selection provides the greatest naturalness, because it applies digital signal processing (DSP) to the recorded <b>speech</b> only at <b>concatenation</b> points. DSP often makes recorded speech sound less natural.|$|R
40|$|In concatenative text-to-speech (TTS) {{synthesis}} systems unit selection aims {{to reduce}} the number of concatenation points in the synthesized <b>speech</b> and make <b>concatenation</b> joins as smooth as possible. This research considers synthesis of completely new utterances from non-uniform units, whereby the most appropriate units, according to acoustic and phonetic criteria, are selected from a myriad of similar speech database candidates. A Viterbi-style algorithm dynamically selects the most suitable database units from a large speech database by considering concatenation and target costs. Concatenation costs are derived from mel filter bank amplitudes, whereas target costs are considered in terms of the phonemic and phonetic properties of required units. Within subjects and between subjects ANOVA [9] evaluation of listeners' scores showed that the TTS system with this method of unit selection was preferred in 52 % of test sentences...|$|R
40|$|Abstract: The {{objective}} {{of this paper is}} to convert the english text into speech. The conversion of english text into speech is done by using a stored speech signal data. Text to speech conversion module is designed by the use of matlab. By the use of microphone the phonemes (alphabets, numbers, words) are recorded using a goldwave software. The recorded. wav (sounds) files are saved as a database separately. The phonemes are extracted from the text file. For text to <b>speech</b> conversion the <b>concatenation</b> method is proposed. The recorded speech are concatenated together to produce the synthesized speech. The resulting speech output is assessed by listening test. The Mean Opinion Score (MOS) value is calculated for the synthesized speech output as the performance measure. In future work, a miniaturized hardware implementation will be developed for helping visually impaired persons in understanding text they come across in day today life...|$|R
40|$|This paper {{describes}} {{a method for}} generating natural sounding <b>speech,</b> called phrase <b>concatenation,</b> which is used in a telephone inquiry system that provides train timetable information. The concatenation technique used combines pre-recorded words and phrases, but is new in that it involves the recording of several prosodically different versions of otherwise identical phrases. Although no formal evaluation has taken place yet, we feel confident in saying that the output meets high quality standards and approaches the quality of natural speech. 1. INTRODUCTION During the last decade, the performance of spoken dialogue systems has improved substantially. At the moment {{it is possible to}} support a number of simple practical tasks in limited domains. As a result, many telephone -based information systems are being developed in different countries. The practical goal of the NWOTST Priority Programme is to build a prototype of a Dutch train timetable information system. The system is call [...] ...|$|R
40|$|The {{research}} paper briefs about {{the implementation of}} screen readers for Marathi in Windows and Linux platform using unrestricted domain Marathi Text To Speech with Indian English support. The application is an integration of MTTS with open source Screen readers NVDA and ORCA. MTTS is a syllable based unit selection concatenative system, built around open source festival engine. IE support is provided for the smooth navigation and handling the English words occurring while accessing internet and other applications. The TTS is a concatenative based system in which syllable is the highest unit for concatenation. The TTS output resembles natural human voice since it uses the original <b>speech</b> segments for <b>concatenation.</b> Testing has been done with normal and differently abled users. Tuning of the system for improving the user friendliness has been done based on the feedback from the DA The system gets a Mean Opinion Score of 86. 4 % when evaluated {{by a group of}} DA...|$|R
40|$|Problem statement: Speech corpus {{is one of}} {{the major}} {{components}} in corpus-based synthesis. The quality and coverage in speech corpus will affect the quality of synthesis speech sound. Approach: This study proposes a corpus design for Malay corpus-based speech synthesis system. This includes the study of design criteria in corpus-based speech synthesis, Malay corpus based database design and the concatenation engine in Malay corpus-based synthesis system. A set of 10 millions digital text corpuses for Malay language has been collected from Malay internet news. This text corpus had been analyzed using word frequency count to find out all high frequency words to be used for designing the sentences for speech corpus. Results: Altogether 381 sentences for speech corpus had been designed using 70 % of high frequency words from 10 million text corpus. It consists of 16826 phoneme units and the total storage size is 37. 6 Mb. All the phone units are phonetically transcribed to preserve the phonetic context of its origin that will be used for phonetic context unit. This speech corpus had been labeled at phoneme level and used for variable length continuous phoneme based <b>concatenation.</b> <b>Speech</b> corpus {{is one of the}} major components in corpus-based synthesis. The quality and coverage in speech corpus will affect the quality of synthesized speech sound. Conclusion/Recommendation: This study has proposed a platform for designing speech corpus especially for Malay Text to Speech which can be further enhanced to support more coverage and higher naturalness of synthetic speech...|$|R
40|$|Abstract — One of {{the speech}} {{synthesizer}} problems is the unnaturalness and unintelligible production of speech. It is believed that wave modification could {{also contribute to the}} distortion of synthesized speech. The objective of a unit selection speech corpus is to provide a few possible instance of unit in order to produce synthesized speech as close to human speech production without the need (or slight need) to perform wave modification. Different from a standard pre-recorded speech database, unit selection allow multiple instances for same unit to be presented in a corpus. These instances are differentiated by a unique combination of prosodic and phonemic context. These prosodic and phonemic contexts form a set of speech corpus parameter. To minimize distortion in concatenative speech synthesizer, parameters of selected segments need to reflect the quality of natural speech. The selection process depends on the priority level of each parameter in the corpus. These parameters design unit selection model. One of the proposed parameter is adjacent phoneme of the target unit. This paper will describe the analysis on pitch behaviour using MOMEL/INTSINT, an algorithm used to model speech prosody. [2]. Having a TTS system, we try to improve our synthesizer by implementing unit selection speech synthesizer in our own framework. The vast difference between our proposed system and the previously mentioned systems is, we plan to implement an offline corpus. Which means our corpus is readily segmented with properly labelled attribute 2. The goal of our idea is that, we want to have as straightforward technique of synthesizer as possible {{and at the same time}} able to decrease the computational complexity of the performance although we are going to have a very big speech corpus. Index Terms—Natural Language Processing, <b>Speech</b> Synthesis, syllable <b>concatenation,</b> unit selection...|$|R
40|$|This PhD thesis {{tries to}} {{understand}} how to analyse, decompose, model and transform the vocal identity of a human when seen through an automatic speaker recognition application. It starts with an introduction explaining the properties of the speech signal and the basis of the automatic speaker recognition. Then, the errors of an operating speaker recognition application are analysed. From the deficiencies and mistakes noticed in the running application, some observations cm be made which will imply a re-evaluation of the characteristic parameters of a speaker, and to reconsider some parts of the automatic speaker recognition chain. In order to determine what are the characterising parameters of a speaker, these are extracted from the speech signal with an analysis and synthesis harmonic plus noise model (H+N). The analysis and re-synthesis of the harmonic and noise parts indicate those which are speech or speaker dependent. It is then shown that the speaker discriminating information {{can be found in the}} residual of the subtraction from the original signal of the H+N modeled signal. Then, a study of the impostors phenomenon, essential in the tuning of a speaker recognition system, is carried out. The impostors are simulated in two ways: first by a transformation of the speech of a source speaker (the impostor) to the speech of a target speaker (the client) using the parameters extracted from the H+N model. This way of transforming the parameters is efficient as the false acceptance rate grows from 4 % to 23 %. Second, an automatic imposture by <b>speech</b> sepent <b>concatenation</b> is carried out. In this case the false acceptance rate grows to 30 %. A way to become less sensitive to the spectral modification impostures is to remove the harmonic part or even the noise part modeled by the H+N from the original signal. Using such a subtraction decreases the false acceptance rate to 8 % even if transformed impostors are used. To overcome the lack of training data — one of the main cause of modeling errors in speaker recognition — a decomposition of the recognition task into a set of binary classifiers is proposed. A classifier matrix is built and each of its elements has to classify word by word the data coming from the client and another speaker (named here an anti-speaker, randomly chosen from an extemal database). With such an approach it is possible to weight the results according to the vocabulary or the neighbours of the client in the parameter (acoustic) space. The output of the mamx classifiers are then weighted and mixed in order to produce a single output score. The weights are estimated on validation data, and if the weighting is done properly, the binary pair speaker recognition system gives better results than a state of the an HMM based system. In order to set a point of operation (i. e. a point on the COR cuwe) for the speaker recognition application, an a priori threshold has to be determined. Theoretically the threshold should be speaker independent when stochastic models are used. However, practical experiments show that this is not the case, as due to modeling mismatch the threshold becomes speaker and utterance length dependant. A theoretical framework showing how to adjust the threshold using the local likelihood ratio is then developed. Finally, a last modeling error correction method using decision fusion is proposed. Some practical experiments show the advantages and drawbacks of the fusion approach in speaker recognition applications...|$|R
40|$|This thesis {{explores the}} problem of {{determining}} an objective measure to represent human perception of spectral discontinuity in concatenative speech synthesis. Such measures are used as join costs to quantify the compatibility of <b>speech</b> units for <b>concatenation</b> in unit selection synthesis. No previous study has reported a spectral measure that satisfactorily correlates with human perception of discontinuity. An analysis {{of the limitations of}} existing measures and our understanding of the human auditory system were used to guide the strategies adopted to advance a solution to this problem. A listening experiment was conducted using a database of concatenated speech with results indicating the perceived continuity of each concatenation. The results of this experiment were used to correlate proposed measures of spectral continuity with the perceptual results. A number of standard speech parametrisations and distance measures were tested as measures of spectral continuity and analysed to identify their limitations. Time-frequency resolution was found to limit the performance of standard speech parametrisations. As a solution to this problem, measures of continuity based on the wavelet transform were proposed and tested, as wavelets offer superior time-frequency resolution to standard spectral measures. A further limitation of standard speech parametrisations is that they are typically computed from the magnitude spectrum. However, the auditory system combines information relating to the magnitude spectrum, phase spectrum and spectral dynamics. The potential of phase and spectral dynamics as measures of spectral continuity were investigated. One widely adopted approach to detecting discontinuities is to compute the Euclidean distance between feature vectors about the join in concatenated speech. The detection of an auditory event, such as the detection of a discontinuity, involves processing high up the auditory pathway in the central auditory system. The basic Euclidean distance cannot model such behaviour. A study was conducted to investigate feature transformations with sufficient processing complexity to mimic high level auditory processing. Neural networks and principal component analysis were investigated as feature transformations. Wavelet based measures were found to outperform all measures of continuity based on standard speech parametrisations. Phase and spectral dynamics based measures were found to correlate with human perception of discontinuity in the test database, although neither measure was found to contribute a significant increase in performance when combined with standard measures of continuity. Neural network feature transformations were found to significantly outperform all other measures tested in this study, producing correlations with perceptual results in excess of 90 %...|$|R

