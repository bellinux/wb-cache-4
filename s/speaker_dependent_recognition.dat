13|501|Public
40|$|Abstract. In {{this article}} we {{describe}} methods for improving the RWTH German speech recognizer used within the VERBMOBIL project. In particular, we present acceleration methods for the search based on both within-word and across-word phoneme models. We also study incremental methods to reduce the response time of the online speech recognizer. Finally, we present experimental off-line results for the three VERBMOBIL scenarios. We report on word error rates and real-time factors for both speaker independent and <b>speaker</b> <b>dependent</b> <b>recognition.</b> ...|$|E
40|$|A new {{approach}} to continuous speech recognition (CSR) for German is presented, which integrates both statistical knowledge (at the acoustic-phonetic level) and rule-based knowledge (at the word and sentence levels). We introduce a flexible framework allowing bidirectional processing and virtually any search strategy given an acoustic model and a context-free grammar. An implementation of this class of recognizers {{by means of a}} word spotter and an island chart parser is presented. A word recognition accuracy of 93. 5 % is reported on a <b>speaker</b> <b>dependent</b> <b>recognition</b> task with a 4 k words dictionary. 1...|$|E
40|$|In our {{previous}} publications [1 - 6], {{we proposed a}} novel method to represent signals in terms of, so called, "Signature Base Functions-SBF' which were extracted from the physical features of the waveform under consideration. In [1 - 6], SBF were determined in ad-hoc manner, which requires tedious search process, {{and they were not}} orthogonal. Furthermore, optimality of SBF was in question. In this work however, we suggest a well-organised procedure to generate "Optimum Orthogonal Signature Base Functions-OSBF' for selected waveforms, which in turn provides excellent means for signal representations. II is shown that the new method of signal representation, which is based on OSBF, requires less computation time with substantial signal compression and results in efficient <b>speaker</b> <b>dependent</b> <b>recognition.</b> Publisher's Versio...|$|E
40|$|This paper {{describes}} a feasibility study into automatic recognition of Dutch dysarthric speech. Recognition experiments with speaker independent and <b>speaker</b> <b>dependent</b> models are compared, for tasks with different perplexities. The {{results show that}} <b>speaker</b> <b>dependent</b> speech <b>recognition</b> for dysarthric speakers is very well possible, even for higher perplexity tasks...|$|R
40|$|Speech {{recognition}} {{can potentially}} aid in producing transcripts of discussions, interviews, meetings, and other collaborative efforts. This report presents a concept demonstrator of an automatic transcription system that produces text and audio records using <b>speaker</b> <b>dependent</b> speech <b>recognition.</b> It is defined and discussed {{in terms of}} how it works, how users operate it, and its similarities and differences with other transcription systems...|$|R
40|$|This report {{discusses}} {{the results of}} an experiment to determine the possibilities of obtaining some speaker independence using <b>speaker</b> <b>dependent</b> voice <b>recognition</b> equipment. The results revealed about 99 % accuracy when the user's speech templates were in memory along with those of four other users. If the user's voice patterns were not in memory but those of the four other users still were in memory, recognition accuracy still hovered around 95 %. (Author) [URL]...|$|R
40|$|Abstract- We {{consider}} {{the problem of}} combining visual cues with audio signals {{for the purpose of}} improved automatic machine recognition of speech. Although signi cant {{progress has been made in}} machine transcription of large vocabulary continuous speech (LVCSR) over the last few years, the technology to date is most e ective only under controlled conditions such aslow noise, <b>speaker</b> <b>dependent</b> <b>recognition</b> and read speech (as opposed to conversational speech) etc. On the otherhand, while augmenting the recognition of speech utterances with visual cues has attracted the attention of researchers over the last couple of years, most e orts in this domain can be considered to be only preliminary in the sense that unlike LVCSR e orts, tasks have been limited to small vocabulary (e. g., command, digits) and often to speaker dependent training or isolated word speech where word boundaries are arti cially well de ned...|$|E
40|$|In {{this article}} we {{describe}} methods for improving the RWTH German speech recognizer used within the VERBMOBIL project. In particular, we present acceleration methods for the search based on both within-word and across-word phoneme models. We also study incremental methods to reduce the response time of the online speech recognizer. Finally, we present experimental off-line results for the three VERBMOBIL scenarios. We report on word error rates and real-time factors for both speaker independent and <b>speaker</b> <b>dependent</b> <b>recognition.</b> 1 Introduction The goal of the VERBMOBIL project {{is to develop a}} speech-to-speech translation system that performs close to real-time. In this system, speech recognition is followed by subsequent VERBMOBIL modules (like syntactic analysis and translation) which depend on the recognition result. Therefore, in this application it is particularly important to keep the recognition time as short as possible. There are VERBMOBIL modules which are capable to work [...] ...|$|E
40|$|This paper {{presents}} {{a study of}} a Malay <b>speaker</b> <b>dependent</b> <b>recognition</b> using improved Neural Network (NN). The performances are evaluated for recog-nition of the isolated Malay digits of " 0 " through " 9 ". The Error Backpropagation (BP) and an improved error signal of the BP are used in this study. Experiments are carried out by comparing the recognition rates and convergence time of the standard BP and improved BP, {{as well as the}} effects of normalisation techniques on Malay speaker dependent data. The utter-ances are represented using the Linear Prediction Coding (LPC) method. The results show that the improved BP outperforms the standard BP in terms of its convergence with better recognition rates for unnormalised data. For the effects of normalisation data, the unit simple method gives better result compared to unit range and unit variance with improved BP gives faster convergence and higher recognition rate...|$|E
40|$|Abstract: Whole-word based speech {{recognition}} has proven successful for {{the recognition of}} small and medium-sized vocabularies. For large vocabularies and/or continuous speech, the use of sub-word reference units is a promising and efficient alternative. We describe a system for <b>speaker</b> <b>dependent</b> speech <b>recognition</b> based on acoustic sub-word units. Several strategies for automatic generation of an acoustic lexicon are outlined. Preliminary tests have been performed on small vocabulary. In these tests, the proposed system showed comparable results to whole-word based systems. 1...|$|R
40|$|Abstract—This paper {{presents}} the effective and robust method for the feature extraction of the <b>speaker</b> <b>dependent</b> voice <b>recognition.</b> The authors developed a simple Matlab program {{for this purpose}} where the article discrete wavelet transform theory had been used. The voice of set of speakers had been inputted on the database and the discrete wavelet transform calculates the properties and variables {{needed in order to}} verify correctly the speaker. Experimental results show that our method is very effective and the results are satisfactory and finally, the wavelet-based voice recognition system and its performance are discussed and highlighted. Keywords—Speaker <b>dependent,</b> Voice <b>recognition,</b> Discrete wavelet transform I...|$|R
40|$|This paper {{describes}} an efficient method for learning {{the parameters of}} a Gaussian process (GP). The parameters are learned from multiple tasks which are assumed to have been drawn independently from the same GP prior. An efficient algorithm is obtained by extending the informative vector machine (IVM) algorithm to handle the multi-task learning case. The multi-task IVM (MTIVM) saves computation by greedily selecting the most informative examples from the separate tasks. The MT-IVM is also shown to be more efficient than random sub-sampling on an artificial data-set and {{more effective than the}} traditional IVM in a <b>speaker</b> <b>dependent</b> phoneme <b>recognition</b> task...|$|R
40|$|Determination of {{the error}} rate In the speech {{database}} collected here mostly in 1995, {{there are currently}} data of 20 speakers and at least four recording sessions of 350 Finnish words for each speaker. The <b>speaker</b> <b>dependent</b> <b>recognition</b> models are trained using three word sets and tested by the remaining one. The error rate given as {{the result is the}} number of all phoneme errors (inserted,deleted and changed phonemes) divided {{by the total number of}} phonemes. To gain statistical significance for the model comparisons, the tests are normally made for seven different speakers and the error rates are averaged. For verifying the robustness of the models for slightly different speech data also an older database (from 1990) is sometimes used. In general, the older database gives lower average error rates, probably because of the more experienced speakers. For comparisons of the models the post-processing by the Dynamically Expanding Context (DEC) [1]is not applied in order...|$|E
40|$|We {{consider}} {{the problem of}} combining visual cues with audio signals {{for the purpose of}} improved automatic machine recognition of speech. Although signifcant {{progress has been made in}} machine transcription of large vocabulary continuous speech (LVCSR) over the last few years, the technology to date is most eective only under controlled conditions such as low noise, <b>speaker</b> <b>dependent</b> <b>recognition</b> and read speech (as opposed to conversational speech) etc. On the otherhand, while augmenting the recognition of speech utterances with visual cues has attracted the attention of researchers over the last couple of years, most eorts in this domain can be considered to be only preliminary in the sense that unlike LVCSR eorts, tasks have been limited to small vocabulary (e. g., command, digits) and often to speaker dependent training or isolated word speech where word boundaries are arti cially well de ned. INTRODUCTION The potential for joint audio-visual-based speech recognition is well estab [...] ...|$|E
40|$|Speech {{recognition}} solely {{based on}} visual {{information such as}} the lip shape and its movement {{is referred to as}} lipreading. This paper presents an automatic lipreading technique for speaker dependent (SD) and speaker independent (SI) speech recognition tasks. Since the visual features are derived according to the frame rate of the video sequence, spline representation is then employed to translate the discrete-time sampled visual features into continuous domain. The spline coefficients in the same word class are constrained to have similar expression and can be estimated from the training data by the EM algorithm. In addition, an adaptive multi-model approach is proposed to overcome the variation caused by different speaking style in speaker-independent recognition task. The experiments are carried out to recognize the ten English digits and an accuracy of 96 % for <b>speaker</b> <b>dependent</b> <b>recognition</b> and 88 % for speaker independent recognition have been achieved, which shows the superiority of our approach compared with other classifiers investigated. 1...|$|E
40|$|Conventional speaker {{independent}} {{speech recognition}} systems are trained {{using data from}} many different speakers. Inter-speaker variability {{is a major problem}} because parametric representations of speech are highly <b>speaker</b> <b>dependent.</b> This paper describes a technique which allows <b>speaker</b> <b>dependent</b> parameters to be considered when building a speaker independent speech recognition system. The technique is based on utterance clustering, where subsets of the training data are formed and the variability within each subset minimized. Cluster dependent connectionist models are then used to estimate phone probabilities as part of a hybrid connectionist hidden Markov model based large vocabulary talker independent speech recognition system. The system has been evaluated on the ARPA Wall Street Journal continuous speech recognition task. 1. INTRODUCTION <b>Speaker</b> <b>dependent</b> speech <b>recognition</b> systems are generated using training utterances from a single speaker, resulting in a system tuned to a spec [...] ...|$|R
40|$|Mel-Frequency Cepstral Coefficients are {{spectral}} feature {{which are}} widely used for <b>speaker</b> <b>recognition</b> and text <b>dependent</b> <b>speaker</b> <b>recognition</b> systems are the most accurate in voice based authentication systems. In this paper, a text <b>dependent</b> <b>speaker</b> <b>recognition</b> method is developed. MFCCs are computed for a selected sentence. The first 13 MFCCs are considered for each frames of duration 26 ms and each coefficient is clustered to a 5 element cluster centres and finally to a form a 65 element speech code vector for the entire speech. The speech code is trained using a multi-layer perceptron backpropagation gradient descent network and the network is tested for various test patterns. The performance is measured using FAR, FRR and EER parameters. The recognition rate achieved is 96. 18 % for a cluster size of 5 in each coefficient...|$|R
40|$|This paper {{introduces}} a first approach to {{emotion recognition using}} RAMSES, the UPC’s speech recognition system. The approach is based on standard speech recognition technology using hidden semi-continuous Markov models. Both the selection of low level features and {{the design of the}} recognition system are addressed. Results are given on <b>speaker</b> <b>dependent</b> emotion <b>recognition</b> using the Spanish corpus of INTERFACE Emotional Speech Synthesis Database. The accuracy recognising seven different emotions—the six ones defined in MPEG- 4 plus neutral style—exceeds 80 % using the best combination of low level features and HMM structure. This result is very similar to that obtained with the same database in subjective evaluation by human judges. 1...|$|R
40|$|This paper {{presents}} the preliminary results of experimental research of whispered speech recognition {{that was based}} on the application of artificial neural networks (ANN). The paper also describes a speech database of words that were spoken in a whisper and normal manner, which was created especially for this study. A part of this database was used for preliminary training and testing of the ANN. Mel Frequency Cepstral Coefficients (MFCC) in normal speech and whispered speech were used as an input to the ANN. The case of <b>speaker</b> <b>dependent</b> <b>recognition</b> was tested and ANNs with optimal topologies have a 97. 98 % accuracy in speech recognition and 96. 21 % in whisper recognition for a male speaker. The results for a female speaker were very similar. In the case of whisper recognition, when ANN was trained for normal speech the score of whisper recognition was 75. 71 % for a male speaker (82. 14 % for female), and vice versa, when ANN was trained for whisper, normal speech recognition was 82. 14 % for a male speaker (90 % for female) ...|$|E
40|$|Hindi is {{very complex}} {{language}} with {{large number of}} phonemes and being used with various ascents in different regions in India. In this manuscript, speaker dependent and independent isolated Hindi word recognizers using the Hidden Markov Model (HMM) is implemented, under noisy environment. For this study, a set of 10 Hindi names has been chosen as a test set for which the training and testing is performed. The scheme instigated here implements the Mel Frequency Cepstral Coefficients (MFCC) in order to compute the acoustic features of the speech signal. Then, K-means algorithm {{is used for the}} codebook generation by performing clustering over the obtained feature space. Baum Welch algorithm is used for re-estimating the parameters, and finally for deciding the recognized Hindi word whose model likelihood is highest, Viterbi algorithm has been implemented; for the given HMM. This work resulted in successful recognition with 98. 6 % recognition rate for <b>speaker</b> <b>dependent</b> <b>recognition,</b> for total of 10 speakers (6 male, 4 female) and 97. 5 % for speaker independent isolated word recognizer for 10 speakers (male) ...|$|E
40|$|This report {{gives an}} answer to the {{question}} "Is it possible to make a simple algorithm of acceptable quality for speech recognition?" The report is for programmers without prior knowledge of the subject of speech recognition, who {{want to know more about}} the subject and the methods used in simple algorithms for speech recognition. The scope of the report is limited to simple algorithms capable of performing so called <b>speaker</b> <b>dependent</b> <b>recognition</b> of the words in a small-size vocabulary, if these are pronounced as isolated words (ie. between pauses). The central question is answered through an investigation, which is based on the so called pattern-recognition-based approach to speech recognition. This approach was chosen, because it is described in the litterature as a method that can be used to recognize many different types of speech, that has a proven track record of high succes for speech recognition and which is simple and easy to understand. The report includes a description of different methods typically used in simple algorithms for speech recognition. Based on these descriptions a list of preferred methods are made and a simple algorithm for speech recognition is designed and implemented. Through experiments with the implementation it is decided whether or not the algorithm is of an acceptable quality for speech recognition. The report concludes that it is possible to create a simple algorithm for speech recognition and that the quality is acceptable in specific circumstances...|$|E
40|$|This paper {{presents}} a <b>speaker</b> <b>dependent</b> speech <b>recognition</b> with application to voice dialing. This {{work has been}} developed under the constraints imposed by voice dialing applications, i. e., low memory requirements and limited training material. Two methods for producing <b>speaker</b> <b>dependent</b> word baseforms based on Phone Like Units (PLU) are presented and compared: (1) a classical vector quantizer is used to divide the space into regions associated with PLUs; (2) a speaker independent hybrid HMM/MLP recognizer is used to generate <b>speaker</b> <b>dependent</b> PLU based models. This work shows that very low error rates can be achieved even with very simple systems, namely a DTW-based recognizer. However, best results are achieved when using the hybrid HMM/MLP system to generate the word baseforms. Finally, a realtime demonstration simulating voice dialing functions and including keyword spotting and rejection capabilities {{has been set up}} and can be tested online. 2 IDIAP [...] RR 96 - 09 1 Introduction V [...] ...|$|R
40|$|This {{document}} presents VoicePaint {{along the}} lines defined for the description of common exemplars: overview, reference material available, hardware and software platforms, usage, and future plans. VoicePaint supports the simultaneous use of voice commands to modify brush attributes and color while drawing with the mouse. It illustrates the synergistic use of multiple modalities for input (mouse and voice). It has been implemented on the Macintosh using VoiceNavigator, a discrete, <b>speaker</b> <b>dependent</b> speech <b>recognition</b> system. Table of contents 1. Overview of VoicePaint [...] . 1 2. Reference material available [...] 2 3. VoicePaint and the criteria for exemplar selection [...] . 2 4. Hardware and Software platforms [...] 4 5. What we have done with VoicePaint and the future [...] . ...|$|R
40|$|In {{this paper}} {{we show that}} there is {{measurable}} information in the articulatory system which can help to disambiguate the acoustic signal. We measure directly the movement of the lips, tongue, jaw, velum and larynx and parameterise this articulatory feature space using principal components analysis. The parameterisation is developed and evaluated using a <b>speaker</b> <b>dependent</b> phone <b>recognition</b> task on a specially recorded TIMIT corpus of 460 sentences. The results {{show that there is}} useful supplementary information contained in the articulatory data which yields a small but significant improvement in phone recognition accuracy of 2 %. However, preliminary attempts to estimate the articulatory data from the acoustic signal and use this to supplement the acoustic input have not yielded any significant improvement in phone accuracy. 1...|$|R
40|$|M. Comm. In {{recent times}} access control {{has become more}} and more important, {{largely as a result of}} changes in society and an {{increase}} in the quantity and sensitivity of information being stored on computers. Speech recognition is nothing but communication which occurs when two persons have a conversation and one understands what the other says and means. This process consists of sound waves (analogue signals) that are carried through the air. The sound is converted (digitized) by the ear to impulses. The brain matches these impulses to a meaning (template) to which the person responds by an action. Speaker independent recognition involves converting the spoken word into an electronic signal. The signal is then compared to the computer's vocabulary, which consists of a set of templates which have been chosen to represent the average speaker. <b>Speaker</b> <b>dependent</b> <b>recognition</b> consists of training the computer to recognize a specific word spoken by an individual. This is done by having the speaker say the word several times. The computer then creates an average template for that word for that speaker which is then used for reference. For any speech recognition system that an auditor needs to audit, the following have to be established: What does the system reside on? A mainframe, Mini, PC or LAN. Is the system speaker independent, speaker dependent or both? Is the system used for control of physical access, logical access or both? Is the system used for control of access to high security area/data, low security area/data or both? The answers to the above will place the system in one of the categories of the following risk matrix. At the moment the auditor need not be excessively concerned about speech recognition, as it is mainly confined to access control. Both physical and logical access control can easily be audited using normal audit techniques, with a basic knowledge of speech recognition. The future promises exciting applications for speech recognition, which may even include the ability to communicate with the computer in the same way as one speaks to another human being. The auditor will have to grow with technology and keep up to date with developments...|$|E
40|$|Abstract:- In {{this paper}} we propose a new {{approach}} to short-time <b>speaker</b> <b>dependent</b> word <b>recognition</b> based on Dempster-Shafer theory using Linear Predictive Coding (LPC) coefficients. For this we used a database of ten pre-determined signals and one-incoming signal, these signals are generated from ten different words of ten-different persons by using LPC techniques. Now by measuring similarity between incoming signal and predetermined signals from the database, the recognition of a particular word is done. Correlation and monogenic signatures are two theories used to study the similarity of the two signals using discrimination factor. However mutual information theory predicts the probability of occurrence of a signal by measuring the information obtained in the signal. The values from these three theories are changed into mass functions and these are used as evidences in the Dempster-Shafer theory of evidence. All these evidences are combined using the Dempster’s rule and finally the best matching <b>speaker</b> <b>dependent</b> word is identified...|$|R
40|$|Speech {{recognition}} {{is about what}} is being said, irrespective of who is saying. Speech {{recognition is}} a growing field. Major progress is taking place on the technology of automatic speech recognition (ASR). Still, {{there are lots of}} barriers in this field in terms of recognition rate, background noise, speaker variability, speaking rate, accent etc. Speech recognition rate mainly depends on the selection of features and feature extraction methods. This paper outlines the feature extraction techniques for <b>speaker</b> <b>dependent</b> speech <b>recognition</b> for isolated words. A brief survey of different feature extraction techniques like Mel-Frequency Cepstral Coefficients (MFCC), Linear Predictive Coding Coefficients (LPCC), Perceptual Linear Prediction (PLP), Relative Spectra Perceptual linear Predictive (RASTA-PLP) analysis are presented and evaluation is done. Speech recognition has various applications from daily use to commercial use. We have made a <b>speaker</b> <b>dependent</b> system and this system can be useful in many areas like controlling a patient vehicle using simple commands. </p...|$|R
40|$|This paper {{presents}} the ALIZE/SpkDet {{open source software}} for text independent speaker recognition. This software {{is based on the}} well-known UBM/GMM approach. It includes also the latest speaker recognition developments such as latent fac-tor analysis and unsupervised adaptation. Discriminant classi-fiers such as SVM supervectors are also provided, linked with the nuisance attribute projection. The software performance is demonstrated {{within the framework of the}} NIST’ 06 SRE eval-uation campaign. Several other applications like speaker di-arization, embedded <b>speaker</b> <b>recognition,</b> password <b>dependent</b> <b>speaker</b> <b>recognition</b> and pathological voice assessment are also presented. 1...|$|R
40|$|International audienceThis paper {{presents}} the ALIZE/SpkDet {{open source software}} packages for text independent speaker recognition. This software {{is based on the}} well-known UBM/GMM approach. It includes also the latest speaker recognition developments such as Latent Factor Analysis (LFA) and unsupervised adaptation. Discriminant classifiers such as SVM supervectors are also provided, linked with the Nuisance Attribute Projection (NAP). The software performance is demonstrated {{within the framework of the}} NIST' 06 SRE evaluation campaign. Several other applications like speaker diarization, embedded <b>speaker</b> <b>recognition,</b> password <b>dependent</b> <b>speaker</b> <b>recognition</b> and pathological voice assessment are also presented...|$|R
40|$|This study {{investigated}} {{whether it is}} possible for people with chronic dysarthria to adjust their articulation in three practice conditions. A <b>speaker</b> <b>dependent,</b> speech <b>recognition</b> system was used to compare participants' practice attempts with a model of a word made from previous recordings to give a recognition score. This score was used to indicate changes in production of practice words with different conditions. The three conditions were reading of written target words, visual feedback, and an auditory model followed by visual feedback. For eight participants with dysarthria, the ability to alter speech production was shown, together with a differential effect of the three conditions. Copying an auditory target gave significantly better recognition scores than just repeating the word. Visual feedback was no more effective than repetition alone. For four control participants, visual feedback did produce significantly better recognition scores than just repetition of written words, and the presence of an auditory model was Significantly more effective than visual feedback. Possible reasons for differences between conditions are discussed...|$|R
40|$|The first {{prototype}} {{of a low}} cost dictation machine for Spanish is described (DIVO). The main characteristics of our recognition approach are: bottomup, hypothesis-verification strategy; large vocabulary, <b>speaker</b> <b>dependent,</b> isolated word <b>recognition.</b> Its modular structure is the cue for quick development and testing of different implementation alternatives. Two of them are presented: one is based in Static phoneme Modeling (SM) and the other uses Discrete Hidden Markov Modeling (DHMM). The system runs on a standard PC compatible (286 or higher) equipped with a DSP board and is fully voice controlled. This first version of the system can address multiple vocabulary sets of up to 2000 words each, with immediate response and reasonable performance. Modules for increasing vocabulary and performance are being developed. I...|$|R
40|$|Abstract: Speaker {{recognition}} {{is used to}} recognize persons from their voice. It has many applications such as in security systems, database access services, forensics etc. In most of the today’s literatures for improvement of speaker recognition system are limited {{to the study of}} feature matching techniques. This paper deals with a text <b>dependent</b> <b>speaker</b> <b>recognition</b> system using neural network and also proposing a method to improve the accuracy of recognition by changing the number of Mel Frequency Cepstral coefficients (MFCC) used in training stage. Voice Activity Detection (VAD) is used as a preprocessing step to further improve the accuracy...|$|R
40|$|It is a {{challenge}} for many years that how to fix the no. of states and no. of mixtures when HMM models are used for speech recognition. In this paper we have analysed that for hearing impaired speech that is partially intelligible {{to people who are}} speaking to them frequently and it is not understandable by the unfamiliar listeners. They suffer in many aspects like education and in public places to converse with the normal speakers. Since speech is unique most of the time normal speech itself could not be understand by others. If we develop the speech recognizer for their speech it will convert their unintelligible speech into intelligible speech. <b>Speaker</b> <b>dependent</b> connected digit <b>recognition</b> for this task using HTK tool kit is done and the average recognition accuracy obtained is 93...|$|R
40|$|Spoken words {{recognition}} provides applications like spoken commands recognitions in robotics command, speech based number dialing for {{phones and}} mobiles, etc. It also provides applications in railway and banking areas. This work aims at designing of optimal Multilayer Perceptron Neural Network (MLP NN) based classifiers for <b>speaker</b> <b>dependent</b> spoken digits <b>recognition.</b> The classifier attempted as optimal leading to less number of computations and few components requirement for its future implementation in hardware {{leading to a}} low cost speech recognition system. Isolated spoken digits were used as an input data to the neural networks based classifiers. Each spoken word was analyzed for the feature like Mel Frequency Cepstral Coefficients (MFCC). The MLP NN based classifier was designed meticulously with the condition of minimum components and attempting maximum classification accuracy...|$|R
40|$|In this paper, a text <b>dependent</b> <b>speaker</b> <b>recognition</b> {{algorithm}} {{based on}} spectrogram is proposed. The spectrograms have been generated using Discrete Fourier Transform for varying frame sizes with 25 % and 50 % overlap between speech frames. Feature vector extraction {{has been done}} by using the row mean vector of the spectrograms. For feature matching, two distance measures, namely Euclidean distance and Manhattan distance have been used. The results have been computed using two databases: a locally created database and CSLU speaker recognition database. The maximum accuracy is 92. 52 % for an overlap of 50 % between speech frames with Manhattan distance as similarity measure...|$|R
40|$|The {{goal of this}} Bachelor's thesis was {{to design}} text <b>dependent</b> <b>speaker</b> <b>recognition</b> system. There were few systems tested for MIT {{database}}. This database contains recordings of 0. 46 s average length. Best case for recognition {{is to use a}} combination of DTW system using posterior probability estimation (posteriograms) as an output of Phoneme recognizer and acoustic SID system based on iVectors and PLDA (Probabilistic Linear Component Analysis). Fusion with Neural network gives the best results (EER). These are 17. 84 % EER for women and 16. 38 % for men. It's 49. 9 % relative improvement for women and 54. 2 % for men against acoustic recognition alone...|$|R
