2465|10000|Public
25|$|The AEP for non-stationary discrete-time {{independent}} process {{leads us}} to (among other results) <b>source</b> <b>coding</b> theorem for non-stationary source (with independent output symbols) and channel coding theorem for non-stationary memoryless channels.|$|E
25|$|The term average bitrate {{is used in}} case of {{variable}} bitrate multimedia <b>source</b> <b>coding</b> schemes. In this context, the peak bit rate is {{the maximum number of}} bits required for any short-term block of compressed data.|$|E
25|$|Coding {{theory is}} one of the most {{important}} and direct applications of information theory. It can be subdivided into <b>source</b> <b>coding</b> theory and channel coding theory. Using a statistical description for data, information theory quantifies the number of bits needed to describe the data, which is the information entropy of the source.|$|E
40|$|The {{exploration}} of <b>source</b> <b>code</b> in modern integrated development environments {{can lead to}} disorientation problems {{due to a lack}} of visible exploration context as the programmer moves between successive <b>source</b> <b>code</b> displays. Inline <b>source</b> <b>code</b> exploration is a technology which facilitates the {{exploration of}} <b>source</b> <b>code</b> in context. In contrast to explicitly navigating between isolated displays of <b>source</b> <b>code,</b> the programmer fluidly introduces related <b>source</b> <b>code</b> declarations into the context of a primary or focal <b>source</b> <b>code</b> document. The inline approach provides an explicit representation of exploration context between successive <b>source</b> <b>code</b> locations, provides support for the pursuit of exploratory digressions, and allows the programmer to view multiple related <b>source</b> <b>code</b> locations simultaneously with minimal interface adjustment. In this paper we introduce inline <b>source</b> <b>code</b> exploration and describe a user experiment designed to evaluate the effectiveness of the technique at reducing the level of disorientation experienced by programmers during <b>source</b> <b>code</b> exploration activities. 1...|$|R
40|$|Querying and {{analyzing}} <b>source</b> <b>code</b> interactively {{is a critical}} task in reverse engineering and program understanding. Current <b>source</b> <b>code</b> query systems lack sufficient formalism and offer limited query capabilities. In this paper, we introduce the formal framework of <b>Source</b> <b>Code</b> Algebra (SCA), and outline a <b>source</b> <b>code</b> query system based on it. SCA provides a formal data model for <b>source</b> <b>code,</b> an algebraic expression-based query language, and opportunities for query optimization. An algebraic model of <b>source</b> <b>code</b> addresses the issues of conceptual integrity, expressive power, and performance of a <b>source</b> <b>code</b> query system within a unified framework. Keywords: Reverse engineering, <b>source</b> <b>code</b> query, query language, many-sorted algebra. 1 Overview The answers are in the <b>source</b> <b>code.</b> Mark Weiser, in <b>Source</b> <b>Code</b> [28]. Software reverse engineering, code re-engineering, and program understanding have begun to emerge as the latest challenges {{in the field of}} software engineering. Interest in [...] ...|$|R
40|$|Abstract. This paper {{discusses}} {{a method}} for measuring the similarity between program <b>source</b> <b>codes.</b> Unlike many existing methods, our method compares the <b>source</b> <b>codes</b> indirectly using reference vectors instead of using the original <b>source</b> <b>codes.</b> Elements of the vectors are textural features of token-cooccurrence matrices generated from the <b>source</b> <b>codes.</b> Since similarity is simply defined as a distance between two reference vectors, it can be calculated {{in a very short}} time. Another advantage is that we do not need to retain a huge storage area for <b>source</b> <b>codes</b> when applying this method to a <b>source</b> <b>code</b> retrieval system. We implemented the proposed method on a simple <b>source</b> <b>code</b> retrieval system and made experiments with Java program <b>source</b> <b>codes</b> submitted as assignments for a programming class. Results confirmed that our method can effectively find reasonable similarity between pairs of <b>source</b> <b>codes</b> in a short time...|$|R
25|$|In digital multimedia, {{bit rate}} often {{refers to the}} number of bits used per unit of {{playback}} time to represent a continuous medium such as audio or video after <b>source</b> <b>coding</b> (data compression). The encoding bit rate of a multimedia file {{is the size of a}} multimedia file in bytes divided by the playback time of the recording (in seconds), multiplied by eight.|$|E
25|$|The {{field is}} at the {{intersection}} of mathematics, statistics, computer science, physics, neurobiology, and electrical engineering. The theory has also found applications in other areas, including statistical inference, natural language processing, cryptography, neurobiology, the evolution and function of molecular codes (bioinformatics), model selection in statistics, thermal physics, quantum computing, linguistics, plagiarism detection, pattern recognition, and anomaly detection. Important sub-fields of information theory include <b>source</b> <b>coding,</b> channel coding, algorithmic complexity theory, algorithmic information theory, information-theoretic security, and measures of information.|$|E
25|$|Coding {{theory is}} {{concerned}} with finding explicit methods, called codes, for increasing the efficiency and reducing the error rate of data communication over noisy channels to near the channel capacity. These codes can be roughly subdivided into data compression (<b>source</b> <b>coding)</b> and error-correction (channel coding) techniques. In the latter case, it took many years to find the methods Shannon's work proved were possible. A third class of information theory codes are cryptographic algorithms (both codes and ciphers). Concepts, methods and results from coding theory and information theory are widely used in cryptography and cryptanalysis. See the article ban (unit) for a historical application.|$|E
50|$|A <b>source</b> <b>code</b> editor {{can check}} syntax while code is being entered and {{immediately}} warn of syntax problems. A few <b>source</b> <b>code</b> editors compress <b>source</b> <b>code,</b> typically converting common keywords into single-byte tokens, removing unnecessary whitespace, and converting numbers to a binary form. Such tokenizing editors later uncompress the <b>source</b> <b>code</b> when viewing it, possibly prettyprinting it with consistent capitalizing and spacing. A few <b>source</b> <b>code</b> editors do both.|$|R
50|$|<b>Source</b> <b>code</b> viruses are {{a subset}} of {{computer}} viruses that make modifications to <b>source</b> <b>code</b> located on an infected machine. A source file can be overwritten such that it includes a call to some malicious code. By targeting a generic programming language, such as C, <b>source</b> <b>code</b> viruses can be very portable. <b>Source</b> <b>code</b> viruses are rare, {{partly due to the}} difficulty of parsing <b>source</b> <b>code</b> programmatically, but have been reported to exist.|$|R
5000|$|... is a {{platform}} for <b>source</b> <b>code</b> analysis and review based on <b>source</b> <b>code</b> metrics. With the help of codeNforcer, software company can decrease its time for <b>source</b> <b>code</b> review during refactoring, reengineering and white box testing. codeNforcer provides software developers with clear and concrete recommendations for improving their <b>source</b> <b>code</b> on different levels.|$|R
25|$|If a {{compression}} {{scheme is}} lossless—that is, {{you can always}} recover the entire original message by decompressing—then a compressed message has the same quantity of information as the original, but communicated in fewer characters. That is, it has more information, or a higher entropy, per character. This means a compressed message has less redundancy. Roughly speaking, Shannon's <b>source</b> <b>coding</b> theorem says that a lossless compression scheme cannot compress messages, on average, {{to have more than}} one bit of information per bit of message, but that any value less than one bit of information per bit of message can be attained by employing a suitable coding scheme. The entropy of a message per bit multiplied by the length of that message is a measure of how much total information the message contains.|$|E
2500|$|The <b>source</b> <b>coding</b> theorem for {{discrete}} time non-stationary independent sources can {{be found}} here: <b>source</b> <b>coding</b> theorem ...|$|E
2500|$|... the {{information}} entropy and redundancy of a source, and its relevance through the <b>source</b> <b>coding</b> theorem; ...|$|E
40|$|Understanding or comprehending <b>source</b> <b>code</b> {{is one of}} {{the core}} {{activities}} of software engineering. Understanding object-oriented <b>source</b> <b>code</b> is essential and required when a programmer maintains, migrates, reuses, documents or enhances <b>source</b> <b>code.</b> The <b>source</b> <b>code</b> that is not comprehended cannot be changed. The comprehension of object-oriented <b>source</b> <b>code</b> is a difficult problem solving process. In order to document object-oriented software system there are needs to understand its <b>source</b> <b>code.</b> To do so, it is necessary to mine <b>source</b> <b>code</b> dependencies in addition to quantitative information in <b>source</b> <b>code</b> such as the number of classes. This paper proposes an automatic approach, which aims to document object-oriented software by visualizing its <b>source</b> <b>code.</b> The design of the object-oriented <b>source</b> <b>code</b> and its main characteristics are represented in the visualization. Package content, class information, relationships between classes, dependencies between methods and software metrics is displayed. The extracted views are very helpful to understand and document the object-oriented software. The novelty of this approach is the exploiting of code dependencies and quantitative information in <b>source</b> <b>code</b> to document object-oriented software efficiently by means of a set of graphs. To validate the approach, it has been applied to several case studies. The results of this evaluation showed that most of the object-oriented software systems have been documented correctly...|$|R
40|$|Attribute grammars {{have been}} used in {{defining}} programming languages and constructing compilers. Since these are concerned with the syntax and static semantics of the <b>source</b> <b>code</b> of the language, attribute grammars can be effectively used to define <b>source</b> <b>code</b> metrics on it. Most of the <b>source</b> <b>code</b> metrics are based on measuring models of the <b>source</b> <b>code.</b> However, there is no formal way of specifying the mapping of the <b>source</b> <b>code</b> onto the models. This paper attempts to provide an approach using an attribute grammar to demonstrate how Halstead's metrics may be specified in an unambiguous manner on the <b>source</b> <b>code</b> itself...|$|R
5000|$|<b>Source</b> <b>code</b> size: Gives {{the size}} of the <b>source</b> <b>code</b> in megabytes.|$|R
2500|$|It {{is common}} in {{information}} theory {{to speak of the}} [...] "rate" [...] or [...] "entropy" [...] of a language. [...] This is appropriate, for example, when the source of information is English prose. [...] The rate of a source of information is related to its redundancy and how well it can be compressed, the subject of <b>source</b> <b>coding.</b>|$|E
2500|$|Applications of {{fundamental}} topics of information theory include lossless data compression (e.g. ZIP files), lossy data compression (e.g. MP3s and JPEGs), and channel coding (e.g. for Digital Subscriber Line (DSL)). [...] The field {{is at the}} intersection of mathematics, statistics, computer science, physics, neurobiology, and electrical engineering. Its impact has been crucial {{to the success of the}} Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones, the development of the Internet, the study of linguistics and of human perception, the understanding of black holes, and numerous other fields. Important sub-fields of information theory are <b>source</b> <b>coding,</b> channel coding, algorithmic complexity theory, algorithmic information theory, information-theoretic security, and measures of information.|$|E
2500|$|Information {{reconciliation}} {{is a form}} {{of error}} correction carried out between Alice and Bob's keys, in order to ensure both keys are identical. It is conducted over the public channel and as such it is vital to minimise the information sent about each key, as this can be read by Eve. A common protocol used for information reconciliation is the cascade protocol, proposed in 1994. This operates in several rounds, with both keys divided into blocks in each round and the parity of those blocks compared. If a difference in parity is found then a binary search is performed to find and correct the error. If an error is found in a block from a previous round that had correct parity then another error must be contained in that block; this error is found and corrected as before. This process is repeated recursively, which {{is the source of the}} cascade name. After all blocks have been compared, Alice and Bob both reorder their keys in the same random way, and a new round begins. At the end of multiple rounds Alice and Bob have identical keys with high probability; however, Eve has additional information about the key from the parity information exchanged. However, from a coding theory point of view information reconciliation is essentially <b>source</b> <b>coding</b> with side information, in consequence any coding scheme that works for [...] this problem can be used for information reconciliation. Lately turbocodes, LDPC codes [...] and polar codes [...] have been used for this purpose improving the efficiency of the cascade protocol.|$|E
40|$|The {{on demand}} {{generation}} of <b>source</b> <b>code</b> and its execution is essential if computers {{are expected to}} {{play an active role}} in information discovery and retrieval. This paper presents a model of implementation of a <b>source</b> <b>code</b> generator, whose purpose is to generate <b>source</b> <b>code</b> on demand. The implementation of the <b>source</b> <b>code</b> generator is fully configurable and its adoption to a new application is done by changing the generator configuration and not the generator itself. The advantage of using the <b>source</b> <b>code</b> generator is rapid and automatic development of a family of application once necessary program templates and generator configuration are made. The model of implementation of the <b>source</b> <b>code</b> generator is general and implemented <b>source</b> <b>code</b> generator can be used in different areas. We use a <b>source</b> <b>code</b> generator for dynamic generation of ontology supported Web services for data retrieval and for building of different kind of web application...|$|R
50|$|<b>Source</b> <b>code</b> {{generation}} {{is the process}} of generating <b>source</b> <b>code</b> based on an ontological model such as a template and is accomplished with a programming tool such as a template processor or an integrated development environment (IDE). These tools allow the generation of <b>source</b> <b>code</b> through any of various means. A macro processor, such as the C preprocessor, which replaces patterns in <b>source</b> <b>code</b> according to relatively simple rules, is a simple form of <b>source</b> <b>code</b> generator. Source-to-source code generation tools also exist.|$|R
40|$|The {{validation}} of static program analysis tools {{is an extremely}} hard and time consuming process since those tools process <b>source</b> <b>code</b> of computer programs that are usually extremely large and complex. In this paper we argue that static program analysis tools can be validated by self-application, i. e. by applying a <b>source</b> <b>code</b> analysis tool to its own <b>source</b> <b>code.</b> Namely, developers of a complex <b>source</b> <b>code</b> analysis tool are familiar with its <b>source</b> <b>code</b> and consequently in position to more quickly examine whether obtained results are correct. The idea is demonstrated by the application of SNEIPL, a language-independent extractor of dependencies between <b>source</b> <b>code</b> entities, to itself...|$|R
5000|$|The <b>source</b> <b>coding</b> theorem for {{discrete}} time non-stationary independent sources can {{be found}} here: <b>source</b> <b>coding</b> theorem ...|$|E
50|$|<b>Source</b> <b>coding</b> is {{a mapping}} from (a {{sequence}} of) symbols from an information source to {{a sequence of}} alphabet symbols (usually bits) such that the source symbols can be exactly recovered from the binary bits (lossless <b>source</b> <b>coding)</b> or recovered within some distortion (lossy <b>source</b> <b>coding).</b> This is the concept behind data compression.|$|E
50|$|DISCUS, or {{distributed}} <b>source</b> <b>coding</b> using syndromes, is {{a method}} for distributed <b>source</b> <b>coding.</b> It is a compression algorithm used to compress correlated data sources. The method is designed to achieve the Slepian-Wolf bound by using channel codes.|$|E
50|$|A <b>source</b> <b>code</b> editor is a {{text editor}} program {{designed}} specifically for editing <b>source</b> <b>code</b> of computer programs by programmers. It may be a standalone application {{or it may be}} built into an integrated development environment (IDE) or web browser. <b>Source</b> <b>code</b> editors are the most fundamental programming tool, as the fundamental job of programmers is to write and edit <b>source</b> <b>code.</b>|$|R
40|$|Querying <b>source</b> <b>code</b> interactively for {{information}} {{is a critical}} task in reverse engineering of software. However, current <b>source</b> <b>code</b> query systems succeed in handling only small subsets of {{the wide range of}} queries possible on code, trading generality and expressive power for ease of implementation and practicality. We attribute this to the absence of clean formalisms for modeling and querying <b>source</b> <b>code.</b> In this paper, we present an algebraic framework (<b>Source</b> <b>Code</b> Algebra or SCA) for modeling and querying <b>source</b> <b>code.</b> The framework forms the basis of our query system for C <b>source</b> <b>code.</b> An analogy can be drawn with relational algebra, which forms the basis for relational databases. The benefits of using SCA include the integration of structural and flow information into a single <b>source</b> <b>code</b> data model, the ability to process high-level <b>source</b> <b>code</b> queries (command-line, graphical, relational, or pattern-based) by translating them into SCA expressions which can be evaluated using th [...] ...|$|R
50|$|Structure editing {{has often}} been {{employed}} in <b>source</b> <b>code</b> editors, as <b>source</b> <b>code</b> is naturally structured by the syntax of the computer language. However, most <b>source</b> <b>code</b> editors are instead text editors with additional features such as syntax highlighting and code folding, rather than structure editors. The editors in some integrated development environments parse the <b>source</b> <b>code</b> and generate a parse tree, allowing the same analysis as by a structure editor, but the actual editing of the <b>source</b> <b>code</b> is generally done as raw text.|$|R
50|$|Various {{techniques}} used by <b>source</b> <b>coding</b> schemes try {{to achieve the}} limit of Entropy of the source. C(x) ≥ H(x), where H(x) is entropy of source (bitrate), and C(x) is the bitrate after compression. In particular, no <b>source</b> <b>coding</b> scheme can {{be better than the}} entropy of the source.|$|E
5000|$|<b>Source</b> <b>coding</b> (digitization {{and data}} compression), and {{information}} theory.|$|E
5000|$|... (<b>Source</b> <b>coding),</b> {{including}} audio compression, image compression, {{and video}} compression.|$|E
5000|$|There {{has been}} debate {{on whether it}} is illegal to skirt {{copyleft}} software licenses by releasing <b>source</b> <b>code</b> in obfuscated form, such as in cases in which the author is less willing to make the <b>source</b> <b>code</b> available. The issue is addressed in the GNU General Public License by defining <b>source</b> <b>code</b> as the [...] "preferred" [...] version of the <b>source</b> <b>code</b> be made available. The GNU website states [...] "Obfuscated 'source code' is not real <b>source</b> <b>code</b> and does not count as source code." ...|$|R
40|$|Comprehending Object-Oriented Programming (OOP) {{is not an}} {{easy task}} {{especially}} by novice students. The problem occurs during the transition from learning fundamental programming language concept to OOP concept. It is very important to handle this problem from the beginning before novices learn more advanced OOP concepts like encapsulation, inheritance, and polymorphism. Learning programming from <b>source</b> <b>code</b> examples is a common behavior among novices. Novices tend to refer to <b>source</b> <b>codes</b> examples and adapt the <b>source</b> <b>codes</b> to the problem given in their assignments. To cater the problems faced by these novices, a novel agent-based model have been designed to assist them in comprehending OOP concepts through <b>source</b> <b>codes</b> examples. The instructor needs to provide two related <b>source</b> <b>codes</b> that are similar but in different domain. Generally, these <b>source</b> <b>codes</b> go through the preprocessing, comparison, extraction, generate program semantics and classification processes. A formal algorithm that can be applied to any two related Java-based <b>source</b> <b>codes</b> examples is invented to generate the semantics of these <b>source</b> <b>codes.</b> The algorithm requires <b>source</b> <b>codes</b> comparison based on keyword similarity to extract the words that exist in the two related <b>source</b> <b>codes.</b> Three agents namely SemanticAgentGUI, semanticAgent and noviceAgent are designed in the proposed model. The running system shows an OOP semantic knowledge representation by intelligent agents...|$|R
50|$|Technical {{commentators}} have documented varying viewpoints on whether and when comments are appropriate in <b>source</b> <b>code.</b> Some commentators assert that <b>source</b> <b>code</b> should be written with few comments, {{on the basis}} that the <b>source</b> <b>code</b> should be self-explanatory or self-documenting. Others suggest code should be extensively commented (it is not uncommon for over 50% of the non-whitespace characters in <b>source</b> <b>code</b> to be contained within comments).|$|R
