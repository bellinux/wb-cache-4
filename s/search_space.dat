10000|4224|Public
5|$|Because {{the planet}} is {{predicted}} to be visible in the Northern Hemisphere, the primary search {{is expected to be}} carried out using the Subaru Telescope, which has both an aperture large enough to see faint objects and a wide field of view to shorten the search. Two teams of astronomers—Batygin and Brown, as well as Trujillo and Sheppard—are undertaking this search together, and both teams cooperatively expect the search to take up to five years. Brown and Batygin initially narrowed the search for Planet Nine down to roughly 2,000 square degrees of sky near Orion, a swath of space, that in Batygin's opinion, could be covered in about 20 nights by the Subaru Telescope. Subsequent refinements by Batygin and Brown have reduced the <b>search</b> <b>space</b> to 600–800 square degrees of sky.|$|E
5|$|The {{combinatorial}} extension (CE) {{method is}} similar to DALI in that it too breaks each structure in the query set {{into a series of}} fragments that it then attempts to reassemble into a complete alignment. A series of pairwise combinations of fragments called aligned fragment pairs, or AFPs, are used to define a similarity matrix through which an optimal path is generated to identify the final alignment. Only AFPs that meet given criteria for local similarity are included in the matrix as a means of reducing the necessary <b>search</b> <b>space</b> and thereby increasing efficiency. A number of similarity metrics are possible; the original definition of the CE method included only structural superpositions and inter-residue distances but has since been expanded to include local environmental properties such as secondary structure, solvent exposure, hydrogen-bonding patterns, and dihedral angles.|$|E
25|$|Dual-phase {{evolution}} {{is a family}} of algorithms and processes (to which simulated annealing belongs) that mediate between local and global search by exploiting phase changes in the <b>search</b> <b>space.</b>|$|E
40|$|Abstract. In {{order to}} avoid blind searching before {{reducing}} the <b>searching</b> <b>space</b> of optimized variable and enhance searching efficiency in chaos optimization algorithm, a new mutative scale chaos optimization algorithm, Probability Chaos Optimization Algorithm (PCOA) was proposed. The current <b>searching</b> <b>space</b> is <b>searched</b> according to large probability and the origin <b>space</b> is <b>searched</b> according to small probability. Though the <b>searching</b> <b>space</b> is shrunk prematurely, the global optimal point can be found because the origin <b>space</b> is still <b>searched</b> according to small probability, which can overcome the shortcoming of losing the global optimal points owing to prematurely shrinking the <b>searching</b> <b>space</b> of the optimized variables in conventional mutative scale chaos optimization algorithm. The simulation results prove {{the validity of the}} algorithm...|$|R
40|$|Abstract | This paper {{provides}} {{conditions under}} which evolutionary algorithms with an elitist selection rule will converge to the global optimum of some function whose domain may be an arbitrary space. These results generalize the previously developed convergence theory for binary and Euclidean <b>search</b> <b>spaces</b> to general <b>search</b> <b>spaces.</b> I...|$|R
5000|$|Nelder - Mead method aka. the {{simplex method}} conceptually resembles PS in its {{narrowing}} {{of the search}} range for multi-dimensional <b>search</b> <b>spaces</b> but does so by maintaining n + 1 points for n-dimensional <b>search</b> <b>spaces,</b> whereas PS methods computes 2n + 1 points (the central point and 2 points in each dimension).|$|R
25|$|Particle swarm {{optimization}} is an algorithm modelled on swarm {{intelligence that}} finds {{a solution to}} an optimization problem in a <b>search</b> <b>space,</b> or model and predict social behavior {{in the presence of}} objectives.|$|E
25|$|An {{important}} {{application of}} {{divide and conquer}} is in optimization, where if the <b>search</b> <b>space</b> is reduced ("pruned") by a constant factor at each step, the overall algorithm has the same asymptotic complexity as the pruning step, with the constant depending on the pruning factor (by summing the geometric series); {{this is known as}} prune and search.|$|E
25|$|Simulated {{annealing}} (SA) is {{a related}} global optimization technique which traverses the <b>search</b> <b>space</b> by generating neighboring solutions {{of the current}} solution. A superior neighbor is always accepted. An inferior neighbor is accepted probabilistically based on the difference in quality and a temperature parameter. The temperature parameter is modified as the algorithm progresses to alter {{the nature of the}} search.|$|E
30|$|One {{might be}} {{thinking}} of determining the <b>search</b> <b>spaces</b> SS_ΔN_ 1 ^p(k) and SS_ΔN_ 2 ^p(k) {{without the use of}} pseudorange data. The approach is to use polynomial fitting method or high-order epoch-wise differencing method to predict the cycle slips at epoch (k), then the <b>search</b> <b>spaces</b> can be defined. The risk is that it might not get a close enough estimate of the cycle slips when the receiver clock has large variations. It is true that the <b>search</b> <b>spaces</b> can be appropriately expanded to handle the situation. However, how much expansion is appropriate is difficult to calculate.|$|R
30|$|PSO {{algorithm}} {{was developed}} for continuous <b>searching</b> <b>space</b> optimization problems. Since we encode the <b>searching</b> <b>space</b> of this problem continuously, therefore, PSO algorithm is used appropriately in this paper. Moreover, regarding to population based property of the MOPSO, we use two well-known population based algorithms namely NRGA and NSGAII to verify and validate the MOPSO results.|$|R
30|$|In this way, we make ABC {{algorithm}} {{more suitable}} for the discrete problem. However, this method will narrow the <b>searching</b> <b>space,</b> since the <b>searching</b> <b>space</b> just depends on the potential solutions generated from the beginning. To deal with this problem, we will discard the worst solution, whose f(X) is the largest, and re-initialize it in scout bees searching step to let more potential solutions come into exchange.|$|R
25|$|Automated {{theorem proving}} {{refers to the}} {{development}} of computer programs that search and find derivations (formal proofs) of mathematical theorems. Finding derivations is a difficult task because the <b>search</b> <b>space</b> can be very large; an exhaustive search of every possible derivation is theoretically possible but computationally infeasible for many systems of interest in mathematics. Thus complicated heuristic functions are developed to attempt to find a derivation in less time than a blind search.|$|E
25|$|This {{approach}} operates {{under the}} assumption that local interactions play a large role in stabilizing the overall protein conformation. In any short sequence, the molecular forces constrain the structure, leading to {{only a small number of}} possible conformations, which can be modeled by fragments. Indeed, according to Levinthal's paradox, a protein could not possibly sample all possible conformations within a biologically reasonable amount of time. Locally stabilized structures would reduce the <b>search</b> <b>space</b> and allow proteins to fold on the order of milliseconds.|$|E
25|$|In {{order to}} deal with {{problems}} with thousands of variables, it is necessary to use a different approach. One is to first sample one ordering, and then find the optimal BN structure with respect to that ordering. This implies working on the <b>search</b> <b>space</b> of the possible orderings, which is convenient as it is smaller than the space of network structures. Multiple orderings are then sampled and evaluated. This method has been proven to be the best available in literature when the number of variables is huge.|$|E
3000|$|... [...]. This {{modification}} {{leads to}} decreased <b>searching</b> <b>space</b> and computation time, {{as well as}} spurious similar segments.|$|R
5000|$|Golden-section search conceptually resembles PS in its {{narrowing}} {{of the search}} range, only for single-dimensional <b>search</b> <b>spaces.</b>|$|R
40|$|Evolutionary {{computation}} {{has been}} used many times for protein function prediction. In this paper a new approach is taken by constraining the problem to predicting the products of enzyme catalysis. Genetic programming with the Push programming language is used to evolve predictors within multiple <b>search</b> <b>spaces.</b> Predictors are evolved within multiple <b>search</b> <b>spaces</b> to reduce the complexity of solutions and represent sequence analysis, protein domain recognition, protein folding, and informatic approaches...|$|R
25|$|In {{addition}} to expert systems, other researchers developed {{the concept of}} frame based languages in the mid 1980s. A frame is similar to an object class: It is an abstract description of a category describing things in the world, problems, and potential solutions. Frames were originally used on systems geared toward human interaction, e.g. understanding natural language and the social settings in which various default expectations such as ordering food in a restaurant narrow the <b>search</b> <b>space</b> and allow the system to choose appropriate responses to dynamic situations.|$|E
25|$|The Jodrell Bank Centre for Astrophysics {{comprises}} the university's astronomical academic staff in Manchester and Jodrell Bank Observatory on rural land near Goostrey, about ten miles (16km) west of Macclesfield {{away from the}} lights of Greater Manchester. The observatory's Lovell Telescope, named after Sir Bernard Lovell, {{a professor at the}} Victoria University of Manchester who first proposed the telescope. Constructed in the 1950s, it is the third largest fully movable radio telescope in the world. It has {{played an important role in}} the research of quasars, pulsars and gravitational lenses, and in confirming Einstein's theory of General Relativity. A £1.2 billion Square Kilometre Array with 50 times more sensitivity and the ability to <b>search</b> <b>space</b> 10,000 times faster than any other telescope in existence will become operational in 2020.|$|E
500|$|The {{result of}} this {{simplification}} is that, instead of searching for 48 * 47 = 2,256 permutations for the pawns' locations, {{there is only one}} permutation. [...] Reducing the <b>search</b> <b>space</b> by a factor of 2,256 facilitates a much quicker calculation.|$|E
40|$|AbstractThis paper {{advances}} {{the design}} of a unified model for the representation of search in first-order clausal theorem-proving, by extending to tableau-based subgoal-reduction strategies (e. g., model-elimination tableaux), the marked search-graph model, already introduced for ordering-based strategies, those that use (ordered) resolution, paramodulation/superposition, simplification, and subsumption. The resulting analytic marked search-graphs subsume AND–OR graphs, and allow us to represent those dynamic components, such as backtracking and instantiation of rigid variables, that have long been an obstacle to modelling subgoal-reduction strategies properly. The second part of the paper develops for analytic marked search-graphs the bounded <b>search</b> <b>spaces</b> approach to the analysis of infinite <b>search</b> <b>spaces.</b> We analyze how tableau inferences and backtracking affect the bounded <b>search</b> <b>spaces</b> during a derivation. Then, we apply this analysis to measure the effects of regularity and lemmatization by folding-up on search complexity, by comparing the bounded <b>search</b> <b>spaces</b> of strategies with and without these features. We conclude with a discussion comparing the marked search-graphs for tableaux, linear resolution, and ordering-based strategies, showing how this search model applies across these classes of strategies...|$|R
40|$|We {{present a}} novel {{solution}} to the warping recovery problem. Our algorithm has several distinct advantages; it is scalable, it enables effective integration of boundary and continuity constraints, and most importantly it is computationally much less demanding than the previous approaches. In addition, our algorithm accurately detects non-linear warping functions without restricting to the linearity assumptions and 2 -D planar deformations unlike the existing approaches. We achieve to decompose the image warping as an optimization process in 1 -D scan-line <b>search</b> <b>spaces.</b> We construct the <b>search</b> <b>spaces</b> from block-matching based image distances, and then we traverse minimum cost paths into these <b>search</b> <b>spaces</b> using boundary conditions to determine the horizontal and vertical component of warping for each pixel. Our experiments prove {{the performance of the}} proposed algorithm...|$|R
30|$|The <b>searching</b> <b>space</b> {{for every}} {{instance}} is x_k∈{ 0, 1 }, q_k∈ [0.5, 5], s ∈ [1, n - m].|$|R
500|$|According to {{the method}} {{described}} above, the tablebase must allow {{the possibility that}} a given piece might occupy any of the 64 squares. [...] In some positions, it is possible to restrict the <b>search</b> <b>space</b> without affecting the result. [...] This saves computational resources and enables searches which would otherwise be impossible.|$|E
500|$|The {{principal}} difficulty was that, {{for many}} problems, {{the number of}} possible paths through the [...] "maze" [...] was simply astronomical (a situation known as a [...] "combinatorial explosion"). Researchers would reduce the <b>search</b> <b>space</b> by using heuristics or [...] "rules of thumb" [...] that would eliminate those paths that were unlikely {{to lead to a}} solution.|$|E
500|$|For n {{individual}} sequences, {{the naive}} method requires constructing the n-dimensional {{equivalent of the}} matrix formed in standard pairwise sequence alignment. The <b>search</b> <b>space</b> thus increases exponentially with increasing n and is also strongly dependent on sequence length. Expressed with the big O notation commonly used to measure computational complexity, a naïve MSA takes O(LengthNseqs) time to produce. [...] To find the global optimum for n sequences this way {{has been shown to}} be an NP-complete problem. [...] In 1989, based on Carrillo-Lipman Algorithm, Altschul introduced a practical method that uses pairwise alignments to constrain the n-dimensional <b>search</b> <b>space.</b> [...] In this approach pairwise dynamic programming alignments are performed on each pair of sequences in the query set, and only the space near the n-dimensional intersection of these alignments is searched for the n-way alignment. The MSA program optimizes the sum of all of the pairs of characters at each position in the alignment (the so-called sum of pair score) and has been implemented in a software program for constructing multiple sequence alignments.|$|E
5000|$|... {{versioned}} {{snapshots of}} the state of the whole system, or any part, supporting rollback and easy exploration of <b>search</b> <b>spaces</b> ...|$|R
40|$|Translation {{systems are}} complex, and most metrics {{do little to}} {{pinpoint}} causes of error or isolate system differences. We use a simple technique to discover induction errors, which occur when good translations are absent from model <b>search</b> <b>spaces.</b> Our results show that a common pruning heuristic drastically increases induction error, and also strongly suggest that the <b>search</b> <b>spaces</b> of phrase-based and hierarchical phrase-based models are highly overlapping despite the well known structural differences. ...|$|R
40|$|Abstract- Here {{we present}} a rst study of <b>search</b> <b>spaces</b> and tness landscapes {{in the context of}} the {{evolution}} of quantum programs. We consider small instances of the Deutsch-Jozsa problem as a starting point for the explo-ration of <b>search</b> <b>spaces</b> of quantum algorithms and ana-lyze the structure of mutation landscapes using autocor-relation functions and information measures for charac-terizing their behavior. The relationship between land-scape characteristics and quantum algorithm evolution is useful for improving the efciency of the search pro-cess. ...|$|R
500|$|Uniform {{binary search}} stores, {{instead of the}} lower and upper bounds, the index of the middle element and {{the change in the}} middle element from the current {{iteration}} to the next iteration. Each step reduces the change by about half. For example, if the array to be searched was , the middle element would be [...] Uniform binary search works on the basis that the difference between the index of middle element of the array and the left and right subarrays is the same. In this case, the middle element of the left subarray (...) is [...] and the middle element of the right subarray (...) is [...] Uniform binary search would store the value of [...] as both indices differ from [...] by this same amount. To reduce the <b>search</b> <b>space,</b> the algorithm either adds or subtracts this change from the middle element. The main advantage of uniform binary search is that the procedure can store a table of the differences between indices for each iteration of the procedure, which may improve the algorithm's performance on some systems.|$|E
2500|$|... the <b>search</b> <b>space</b> is pruned if {{the partial}} {{permutation}} produces a ...|$|E
2500|$|Given are the <b>search</b> <b>space</b> {{dimension}} [...] and the iteration step [...] The five state {{variables are}} ...|$|E
3000|$|... {{from the}} <b>searching</b> <b>space,</b> {{which is also}} {{referred}} to as pruning and is the reason why the BB algorithm is more efficient than exhaustive search.|$|R
3000|$|... true be known. Unfortunately, {{for more}} complex {{problems}} (with bigger <b>search</b> <b>spaces),</b> as {{the space and}} flex programs, {{it is impossible to}} know P [...]...|$|R
40|$|We {{present a}} new system, Genesis, that {{processes}} sets of human patches to automatically infer code transforms and <b>search</b> <b>spaces</b> for automatic patch generation. We present results {{that characterize the}} effectiveness of the Genesis inference algorithms and the resulting complete Genesis patch generation system working with real-world patches and errors collected from top 1000 github Java software development projects. To the best of our knowledge, Genesis is the first system to automatically infer patch generation transforms or candidate patch <b>search</b> <b>spaces</b> from successful patches...|$|R
