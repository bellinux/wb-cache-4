8|10000|Public
50|$|Magnetic <b>sequential</b> <b>access</b> <b>memory</b> is {{typically}} used for secondary storage in general-purpose computers {{due to their}} higher density at lower cost compared to RAM, as well as resistance to wear and non-volatility. Magnetic tape is the only type of <b>sequential</b> <b>access</b> <b>memory</b> still in use; historically, drum memory has also been used.|$|E
5000|$|<b>Sequential</b> <b>access</b> <b>memory</b> a {{class of}} data storage devices that read stored data in a {{sequence}} ...|$|E
5000|$|While <b>sequential</b> <b>access</b> <b>memory</b> is read in sequence, {{arbitrary}} locations {{can still}} be accessed by [...] "seeking" [...] to the requested location. This operation, however, is often relatively inefficient (see seek time, rotational latency).|$|E
50|$|System level buses such as off-chip buses or long on-chip buses between IP block {{are often}} {{major sources of}} energy {{consumption}} due to their large load capacitance. Experimental results {{have shown that the}} bus activity for <b>memory</b> <b>access</b> can be reduced to 50% by organizing the data. Consider the case of compiling the code written in C language : int A44,B44; for(i=0;i<4;i++){ for(j=0;j<4;j++){ Bij=Aji; } }Most of the existing C compiler places a multidimensional array in row-major form that is row by row, this is shown in unoptimized column. As a result, no <b>memory</b> <b>access</b> while running this code has <b>sequential</b> <b>memory</b> <b>access</b> because elements in columns are accessed sequentially. But if we change the structure of form in which they are placed in memory so that there is maximum number of <b>sequential</b> <b>access</b> from <b>memory.</b> This can be achieved if we place data in the order as shown in optimized column of below table. Such redistribution of data by compiler can reduce energy consumption due to <b>memory</b> <b>access</b> by significant amount.|$|R
40|$|The on-chip memory {{performance}} of embedded systems directly affects the system designers' decision {{about how to}} allocate expensive silicon area. We investigate a novel random <b>access</b> <b>memory</b> (RAM) architecture for embedded systems that allows both random-access and sequential-access for reads and writes. To realize <b>sequential</b> <b>accesses,</b> small "links" are added to each row in the RAM array {{to point to the}} next row to be prefetched. The potential cache pollution caused by prefetching is ameliorated by a small cache structure called a <b>sequential</b> <b>access</b> buffer (SAB). To evaluate the architecture-level {{performance of}} the flexible <b>sequential</b> and random <b>access</b> <b>memory</b> (FSRAM), we run the Mediabench benchmark programs [1] on {{a modified version of the}} Simplescalar simulator [2]. Our results show that the FSRAM improves the performance of a baseline processor with a 16 KB data cache up to 55 % on the benchmark programs tested, with an average improvement of about 9 %. We also designed RTL and SPICE models of the FSRAM to evaluate its potential cost and delay characteristics [3]. Our design shows that the FSRAM significantly improves <b>memory</b> <b>access</b> time, while reducing power consumption, with negligible area overhead...|$|R
50|$|After {{a memory}} word is fetched, the memory is {{typically}} inaccessible {{for an extended}} period of time while the sense amplifiers are charged for access of the next cell. By interleaving the memory (e.g. cells 0, 4, 8, etc. are stored together in one rank), <b>sequential</b> <b>memory</b> <b>accesses</b> can be performed more rapidly because sense amplifiers have 3 cycles of idle time for recharging, between accesses.|$|R
50|$|In computing, <b>sequential</b> <b>access</b> <b>memory</b> (SAM) is a {{class of}} data storage devices that read stored data in a sequence. This is in {{contrast}} to random access memory (RAM) where data can be accessed in any order. Sequential access devices are usually a form of magnetic storage.|$|E
50|$|The {{magnetic}} medium {{is found in}} magnetic tape, hard disk drives, floppy disks, and so on. This medium uses different patterns of magnetization in a magnetizable material to store data and {{is a form of}} non-volatile memory. Magnetic storage media can be classified as either <b>sequential</b> <b>access</b> <b>memory</b> or random access memory although in some cases the distinction is not perfectly clear.|$|E
50|$|Cracovians {{adopted a}} column-row {{convention}} for designating individual elements {{as opposed to}} the standard row-column convention of matrix analysis. This made manual multiplication easier, as one needed to follow two parallel columns (instead of a vertical column and a horizontal row in the matrix notation.) It also sped up computer calculations, because both factors' elements were used in a similar order, which was more compatible with the <b>sequential</b> <b>access</b> <b>memory</b> in computers of those times â€” mostly magnetic tape memory and drum memory. Use of Cracovians in astronomy faded as computers with bigger random access memory came into general use. Any modern reference to them is in connection with their non-associative multiplication.|$|E
30|$|The problem arising {{is how to}} {{deal with}} more than one entry per bucket. A naive {{solution}} is to use E memory backs, one for each possible entry, and query them in parallel. The additional cost is acceptable compared to the saved SRAM. In Section 7 we will discuss this issue in more detail and present techniques that allow multiple entries per bucket but do not require parallel or <b>sequential</b> <b>memory</b> <b>accesses.</b>|$|R
40|$|Two {{issues are}} {{addressed}} in this digital design and analysis investigation: First, the device of <b>sequential</b> <b>memory</b> (<b>accessed</b> using animation) is employed in the construction and in the reading of audio-visual design messages. Second, threedimensional Design Diagrams, audio-visual ideograms, are structured as memory schematics that are intended to organize an architectural digital/physical design schema. The accompanying presentation employs Memory Diagrams in audiovisual sequences demonstrating a methodology of structuring and restructuring memory in animation...|$|R
40|$|This paper {{considers}} smooth principle {{component analysis}} for high dimensional data with very large dimensional observations p and moderate {{number of individuals}} N. Our setting is similar to traditional PCA, but we assume the factors are smooth and design {{a new approach to}} estimate them. By connecting with Singular Value Decomposition subjected to penalized smoothing, our algorithm is linear in the dimensionality of the data, and it also favors block calculations and <b>sequential</b> <b>access</b> to <b>memory.</b> Different from most existing methods, we avoid extracting eignefunctions via smoothing a huge dimensional covariance operator. Under regularity assumptions, the results indicate that we may enjoy faster convergence rate by employing smoothness assumption. We also extend our methods when each subject is given multiple tasks by adopting the two way ANOVA approach to further demonstrate the advantages of our approach...|$|R
50|$|Magnetic {{storage media}} can be {{classified}} as either <b>sequential</b> <b>access</b> <b>memory</b> or random access memory, although {{in some cases the}} distinction is not perfectly clear. The access time can be defined as the average time needed to gain access to stored records. In the case of magnetic wire, the read/write head only covers a very small part of the recording surface at any given time. Accessing different parts of the wire involves winding the wire forward or backward until the point of interest is found. The time to access this point depends on how far away it is from the starting point. The case of ferrite-core memory is the opposite. Every core location is immediately accessible at any given time.|$|E
40|$|This paper {{presents}} {{the design and}} evaluation of a <b>sequential</b> <b>access</b> <b>memory</b> (SAM) that provides low power and high performance by replacing address decoders with special locally-communicating sequencers. A test chip containing one 16 x 16 -b SAM and one 64 x 16 -b SAM (consisting of four 16 x 16 -b banks) has been designed, fabricated, and evaluated using a 0. 25 -m CMOS process. With a clock frequency of 40 MHz at 1. 2 V, the measured worst-case read power dissipations for the 16 x 16 -b SAM and the 64 x 16 -b SAM are 344 W and 358 W respectively, demonstrating power dissipation that is largely independent of SAM size...|$|E
40|$|We {{implemented}} POPeye (Probe of Performance + eye), {{a system}} analysis simulator for DRAM performance evaluation {{in a personal}} computer environment. When running real-life application programs such as Microsoft Office and Paint Shop Pro on Windows 95, POPeye simulates detailed transactions between a CPU and a memory system. Using this tool, we comparatively analyzed {{the performance of a}} DDR-SDRAM and a D-RDRAM. The simulation results show that the D-RDRAM is faster than the DDR-SDRAM for a <b>sequential</b> <b>memory</b> <b>access</b> pattern in a 128 Mbyte memory system. But the DDR-SDRAM system shows higher performance for a random <b>memory</b> <b>access</b> pattern. I...|$|R
40|$|Pointer-based array traversals {{are often}} used by C programmers to {{optimize}} <b>sequential</b> <b>memory</b> <b>access.</b> However, the use of pointer arithmetic prevents compilers from effectively applying dependence testing and loop analysis for optimization and parallelization. Because pointer variables {{can be viewed as}} reference-type induction variables, conventional induction variable recognition and substitution methods can be used to translate pointer-based code to array-based code, thereby enabling the application of traditional loop restructuring compiler optimizations for parallelization. However, conditionally updated induction variables and pointers are prohibiting the application of these optimizations, due to the inability to determine closed-form function equivalents. This paper presents a method for value range analysis on conditionally updated induction variables and pointers which have no closed-form functions...|$|R
40|$|This paper {{presents}} new {{results on}} an approach for solving satisfiability problems (SAT), i. e. creating a logic circuit that is specialized to solve each problem instance on Field Programmable Gate Arrays (FPGAs). This approach becomes feasible {{due to the}} recent advances in FPGAs and high-level logic synthesis. In this approach, each SAT problem is automatically analyzed and implemented on FPGAs. We have developed an algorithm which is suitable for implementing on a logic circuit. This algorithm {{is equivalent to the}} Davis-Putnam procedure with a powerful dynamic variable ordering heuristic. The algorithm does not have a large memory structure like a stack; thus <b>sequential</b> <b>accesses</b> to the <b>memory</b> do not become a bottleneck in algorithm execution. Simulation results show that this method can solve a hard random 3 -SAT problem with 400 variables within 20 minutes at a clock rate of 1 MHz. 1...|$|R
40|$|The {{advantages}} of using Geometry Images as surface representations largely {{depend on their}} regular sampling distribution and strictly ordered 2 D storage arrangement. Traditional 3 D spatial partitioning techniques often compromise these attractive properties when building hierarchical data structures. We present a modification to traditional partitioning methods using locality masks, which maintain the original sampling and storage structure of Geometry Images. Applications using spatial hierarchies can then {{take advantage of the}} <b>sequential</b> <b>memory</b> <b>access</b> and simplified sampling neighbourhoods associated with Geometry Images without an intermediate sorting phase. The method uses traditional principles for creation, storage and processing of internal hierarchy nodes, but treats the referencing of primitives at leaf nodes differently. Locality masks are presented with future Geometry Image processing techniques in mind and handle both single and multi-chart Geometry Images...|$|R
5000|$|Furthermore, {{independent}} storage sections provided two-way (H75) or four-way (I75, J75) interleaving of <b>memory</b> <b>access.</b> Even {{with only}} two-way interleaving, [...] "an effective <b>sequential</b> <b>access</b> rate of 400 nanoseconds per double word (eight bytes) is possible." ...|$|R
40|$|As {{processor}} speeds {{continue to}} increase the primary assumption of the RAM model: that all memory locations may be accessed in unit time, becomes unrealistic. In the following we consider an alternate model, called the Limiting Technology or LT-RAM, where the cost of accessing a given memory location {{is dependent on the}} size of the memory module. In general, computations which are performed on an LT-RAM with a memory of size n Î˜ n will result in execution times which are n times slower than a comparable RAM, if no special precautions are taken. Here we provide a general technique by which, for a class of algorithms, this slow-down can be reduced to O(n 6 = log 1 = 2 n) for <b>sequential</b> <b>memory</b> <b>access,</b> or to just O(1) if the <b>memory</b> <b>access</b> can be pipelined. 1 Introduction The past twenty years the speed of computer systems has constantly increased. Through progress in chip fabrication, memory chips have become both smaller and faster, even as the storage capacity of the chips has i [...] ...|$|R
40|$|Facility {{location}} {{problems are}} captivating both from {{theoretical and practical}} point of view. In this paper, we study some fundamental facility location problems from the space-efficient perspective. Here the input {{is considered to be}} given in a read-only memory and only constant amount of work-space is available during the computation. This constant-work-space model is well-motivated for handling big-data as well as for computing in smart portable devices with small amount of extra-space. First, we propose a strategy to implement prune-and-search in this model. As a warm up, we illustrate this technique for finding the Euclidean 1 -center constrained on a line for a set of points in ^ 2. This method works even if the input is given in a <b>sequential</b> <b>access</b> read-only <b>memory.</b> Using this we show how to compute (i) the Euclidean 1 -center of a set of points in ^ 2, and (ii) the weighted 1 -center and weighted 2 -center of a tree network. The running time of all these algorithms are O(n poly(n)). While the result of (i) gives a positive answer to an open question asked by Asano, Mulzer, Rote and Wang in 2011, the technique used can be applied to other problems which admit solutions by prune-and-search paradigm. For example, we can apply the technique to solve two and three dimensional linear programming in O(n poly(n)) time in this model. To the best of our knowledge, these are the first sub-quadratic time algorithms for all the above mentioned problems in the constant-work-space model. We also present optimal linear time algorithms for finding the centroid and weighted median of a tree in this model...|$|R
40|$|We {{propose a}} novel RAM {{architecture}} for embedded systems that allows both random-access and <b>sequential</b> <b>access</b> for reads and writes. Using small "links" in each row that {{points to the}} next row to be prefetched, our design significantly improves <b>memory</b> <b>access</b> time, while reducing power consumption {{at the expense of}} negligible area overhead...|$|R
40|$|Field-programmable gate arrays (FPGAs) {{and other}} {{reconfigurable}} computing (RC) devices {{have been widely}} shown to have numerous advantages including order of magnitude performance and power improvements compared to microprocessors for some applications. Unfortunately, FPGA usage has largely been limited to applications exhibiting <b>sequential</b> <b>memory</b> <b>access</b> patterns, thereby prohibiting acceleration of important applications with irregular patterns (e. g., pointer-based data structures). In this paper, we present a design pattern for RC application development that serializes irregular data structure traversals online into a traversal cache, which allows the corresponding data to be efficiently streamed to the FPGA. The paper presents a generalized framework that benefits applications with repeated traversals, which we show can achieve between 7 x and 29 x speedup over pointer-based software. For applications without strictly repeated traversals, we present application-specialized extensions that benefit applications with highly similar traversals by exploiting similarity to improve memory bandwidth and execute multiple traversals in parallel. We show that these extensions can achieve a speedup between 11 x and 70 x on a Virtex 4 LX 100 for Barnes-Hut n-body simulation...|$|R
40|$|This thesis {{investigates the}} {{performance}} of memory resident spatial search, focusing on the R-tree. The characteristics of modern computer architectures are first visited to understand how they have changed since {{the invention of the}} R-tree, and what difference moving the database from disk to memory can be expected to make. The design of four well known R-trees are introduced, implemented and used to reproduce the results of Beckmann and Seeger. Next, four optimizations for the R-tree search are suggested in an attempt to speed up search by improving the memory layout, through explicit parallelization using SIMD instruction, and by applying pruning at a lower level than classical search in R-tree does. The results show that the optimizations often have the intended effects and yield speedups in excess of 1. 3 for all tested data sets, and almost reach 1. 8 in specific cases. More importantly, it is found during the analysis that there exists a tradeoff between <b>sequential</b> <b>memory</b> <b>access,</b> with an associated computational cost, and random <b>memory</b> <b>access</b> arising due to the tree structure. Improving one aspect alone may offsets this balance and improve performance, but the true challenge is to create the combined approach needed to fully exploit modern computer architectures during search in memory resident R-trees...|$|R
50|$|<b>Sequential</b> <b>access</b> {{assumes that}} records can be {{processed}} only sequentially, {{as opposed to}} direct (or random) access. Some devices, such as magnetic tape, naturally enforce <b>sequential</b> <b>access,</b> {{but it can be}} used as well on direct access storage devices (DASD), such as disk drives. In the latter case, a data set written with <b>sequential</b> <b>access</b> can be later processed in a direct manner.|$|R
40|$|Web usage mining discovers {{interesting}} and frequent user access patterns from web logs. Most {{of the previous}} works have focused on mining common <b>sequential</b> <b>access</b> patterns of web access events that occurred within the entire duration of all web access transactions. However, many useful <b>sequential</b> <b>access</b> patterns occur frequently only during a particular periodic time interval due to user browsing behaviors and habits. It is therefore important to mine periodic <b>sequential</b> <b>access</b> patterns with periodic time constraints. In this paper, we propose an efficient approach, known as TCS-mine (Temporal Conditional Sequence mining algorithm), for mining periodic <b>sequential</b> <b>access</b> patterns based on calendar-based periodic time constraints. The calendar-based periodic time constraints are used for describing real-life periodic time concepts such as the morning of every weekend. The mined periodic <b>sequential</b> <b>access</b> patterns {{can be used for}} temporal-based personalized web recommendations. The performance of the proposed TCS-mine algorithm is evaluated and compared with a modified version of WAP-mine for mining periodic <b>sequential</b> <b>access</b> patterns...|$|R
40|$|The {{proliferation}} of computational problems involving massive data sets has necessi-tated {{the design of}} computational paradigms that model the extra constraints placed on systems processing very large inputs. Among these algorithmic paradigms, the pass-efficient model captures the constraints that the input may be much too large to fit in main memory for processing, and that system performance is optimized by <b>sequential</b> <b>access</b> to the data in storage. Thus, in the pass-efficient model of compu-tation, an algorithm may make a constant number of sequential passes over read-only input while using {{a small amount of}} random <b>access</b> <b>memory.</b> The resources to be optimized are memory, number of passes, and per element processing time. We give pass-efficient algorithms for clustering and finding structure in large amounts of data. Our algorithms have the property that the number of passes allotted is an input parameter to the algorithm. We answer questions regarding the intrinsic tradeoffs between the number of passes used by a pass-efficient algorithm and the amount of random <b>access</b> <b>memory</b> required. Our algorithms use adaptive sampling techniques that are quite general and can be used to solve many massiv...|$|R
40|$|Abstract Surfing the Web {{has become}} an {{important}} daily activity for many users. Discovering and understanding web usersâ€™ surfing behavior are essential {{for the development of}} successful web monitoring and recommendation systems. To capture usersâ€™ web access behavior, one promising approach is web usage mining which discovers interesting and frequent user access patterns from web usage logs. Web usage mining discovers interesting and frequent user access patterns from web logs. Most of the previous works have focused on mining common <b>sequential</b> <b>access</b> patterns of web access events that occurred within the entire duration of all web access transactions. However, many useful <b>sequential</b> <b>access</b> patterns occur frequently only during a particular periodic time interval due to user browsing behaviors and habits. It is therefore important to mine periodic <b>sequential</b> <b>access</b> patterns with periodic time constraints. In this paper, we propose an efficient approach, known as TCSMA (Temporal Conditional Sequence Mining Algorithm), for mining periodic <b>sequential</b> <b>access</b> patterns based on calamander-based periodic time constraint. The calamander-based periodic time constraints are used for describing real-life periodic time concepts such as the morning of every weekend. The mined periodic <b>sequential</b> <b>access</b> patterns can be used for temporal-based personalized web recommendations. The performance of the proposed TCSMA is evaluated and compared with a modified version of Web Access Pattern Mine for mining periodic <b>sequential</b> <b>access</b> patterns. Keywords: Periodic <b>Sequential</b> <b>Access</b> Patterns, Web Access Patterns, Association Rule, Web Log Mining, TCSM&WAPM Algorith...|$|R
5000|$|F# {{provides}} generators via sequence expressions, since version 1.9.1. These {{can define}} a sequence (lazily evaluated, <b>sequential</b> <b>access)</b> via , a list (eagerly evaluated, <b>sequential</b> <b>access)</b> via [...] or an array (eagerly evaluated, indexed access) via [...] that contain code that generates values. For example, ...|$|R
50|$|Just as earlier {{machines}} {{had ignored}} {{the fact that}} most operations were being applied to many data points, the STAR ignored the fact that those same data points would be repeatedly operated on. Whereas the STAR would read and process the same memory five times to apply five vector operations on a set of data, it would be much faster to read the data into the CPU's registers once, and then apply the five operations. However, there were limitations with this approach. Registers were significantly more expensive in terms of circuitry, so only a limited number could be provided. This implied that Cray's design would have less flexibility in terms of vector sizes. Instead of reading any sized vector several times as in the STAR, the Cray-1 would have to read only a portion of the vector at a time, but it could then run several operations on that data prior to writing the results back to memory. Given typical workloads, Cray felt that the small cost incurred by being required to break large <b>sequential</b> <b>memory</b> <b>accesses</b> into segments was a cost well worth paying.|$|R
5000|$|Simple Hierarchical Indexed <b>Sequential</b> <b>Access</b> Method (SHISAM).|$|R
5000|$|Alongside Nicolas Jaar, Harrington remixed Daft Punkâ€™s Random <b>Access</b> <b>Memories</b> {{under the}} {{pseudonym}} Daftside and released it on June 21, 2013 as Random <b>Access</b> <b>Memories</b> Memories on Darksideâ€™s SoundCloud ...|$|R
50|$|In data {{structure}}s, a {{data structure}} {{is said to}} have <b>sequential</b> <b>access</b> if one can only visit the values it contains in one particular order. The canonical example is the linked list. Indexing into a list that has <b>sequential</b> <b>access</b> requires O(n) time, where n is the index. As a result, many algorithms such as quicksort and binary search degenerate into bad algorithms that are even less efficient than their naive alternatives; these algorithms are impractical without random access. On the other hand, some algorithms, typically those that do not have index, require only <b>sequential</b> <b>access,</b> such as mergesort, and face no penalty.|$|R
50|$|BPAM {{provides}} an {{application program interface}} (API) to allow programmers to access libraries directly. The BPAM API is similar to basic <b>sequential</b> <b>access</b> method (BSAM), but it adds functionality to process directories. Individual members of a PDS can also be processed using <b>sequential</b> <b>access</b> methods by specifying the member name on the job control DD statement.|$|R
5000|$|... lbzip2: Parallel pthreads-based bzip2/bunzip2 (bzip2 compressor/decompressor) filter for <b>sequential</b> <b>access</b> input/output, by LÃ¡szlÃ³ Ã‰rsek.|$|R
40|$|The {{purpose of}} a model of {{computation}} is to provide the algorithm designer with a device for running algorithms. It should be conceptually clear to let him or her concentrate at the algorithmic ideas for solving the problem. At {{the same time it}} should be concrete enough to give a realistic estimate on the use resources when the algorithm is executed on a real computer. In this paper we analyze some weaknesses of existing models of computation, namely <b>sequential</b> <b>access</b> machine and random access machine, and propose a new cost model, called relative cost random access machine, which solves some contradictions between these models. The new model actually only generalizes the way of counting the complexity, and includes <b>sequential</b> <b>access</b> machines and random access machines as special cases. At the same time, it is exible enough to characterize the cost of <b>memory</b> <b>access</b> in current computers...|$|R
