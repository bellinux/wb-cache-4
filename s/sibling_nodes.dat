58|31|Public
25|$|Informally speaking, if a node has any <b>sibling</b> <b>node(s),</b> then it c-commands its {{siblings}} {{and all of}} their descendants; however, if a node doesn't have any siblings, then it c-commands everything that its parent c-commands.|$|E
5000|$|The {{adjacent}} sibling and {{the parent}} key overlooking the two <b>sibling</b> <b>nodes</b> {{come together to}} form a 4-node.|$|E
50|$|Informally speaking, a node {{in a tree}} c-commands its <b>sibling</b> <b>node(s)</b> {{and all of its}} {{siblings}}' descendants; however, a node without siblings c-commands {{everything that}} its parent c-commands.|$|E
5000|$|We {{know that}} [...] {{is a child}} of [...] To let the {{verifier}} calculate the next node [...] given the previous, {{they need to know}} the other child of , the <b>sibling</b> <b>node</b> of [...] We call this node , so that [...] Hence, [...] nodes [...] are needed, to reconstruct [...] from [...] An example of an authentication path is illustrated in the figure on the right.|$|R
5000|$|If <b>sibling</b> (adjacent <b>node</b> with same parent as L) is {{more than}} half-full, re-distribute, {{borrowing}} an entry from it.|$|R
30|$|Given an {{equivalence}} class [P], we obtain its successor classes by expanding patterns in [P]. Specifically, candidates are generated by joining each pattern P_x^i∈ [P] {{with any other}} pattern P_y^j in [P], including itself, to produce the patterns of the {{equivalence class}} [P_x^i]. We denote the above join operation by P_x^i⊗ P_y^j. There are two possible outcomes for each P_x^i⊗ P_y^j: One is obtained by making y a <b>sibling</b> <b>node</b> of x in P_x^i (cousin expansion), the other is obtained by making y a child node of x in P_x^i (child expansion). We call patterns P_x^i and P_y^j the left parent and right parent of a join outcome, respectively.|$|R
50|$|Note {{that every}} node {{in a weak}} heap can be {{considered}} the root of a smaller weak heap by ignoring its next <b>sibling.</b> <b>Nodes</b> with no first child are automatically valid weak heaps.|$|E
5000|$|AVL {{trees are}} often {{compared}} with red-black trees because both support {{the same set}} of operations and take [...] time for the basic operations. For lookup-intensive applications, AVL trees are faster than red-black trees because they are more strictly balanced. Similar to red-black trees, AVL trees are height-balanced. Both are, in general, neither weight-balanced nor μ-balanced for any μ&le;1&frasl;2; that is, <b>sibling</b> <b>nodes</b> can have hugely differing numbers of descendants.|$|E
5000|$|Tree {{diagrams}} {{may represent}} a series of independent events (such {{as a set of}} coin flips) or conditional probabilities (such as drawing cards from a deck, without replacing the cards). [...] Each node on the diagram represents an event and is associated with the probability of that event. The root node represents the certain event and therefore has probability 1. Each set of <b>sibling</b> <b>nodes</b> represents an exclusive and exhaustive partition of the parent event.|$|E
2500|$|If nodes {{are assumed}} to contain sets of formulae, this rule is {{replaced}} by: if a node is labeled , a leaf of the branch this node is in can be appended two <b>sibling</b> child <b>nodes</b> labeled [...] and , respectively.|$|R
3000|$|... [...]. For the node under checking, {{the partial}} {{distance}} resulted {{by the current}} path is compared with the radius. If the partial distance is smaller than the radius, the search moves on to the children nodes on the next level. Otherwise, the search jumps to another <b>sibling</b> <b>node</b> on the current level. When all the nodes of the level have already been checked, the search {{goes back to the}} upper level. The radius is initially set to infinity and is adaptively decreased according to the best solution already found in the search. Specifically, the radius is updated, taking into account the best combination of [c,d], and [a,b] (line 16 of Algorithm 2). The latter is obtained from the parallel decisions phase. The tree search is terminated when all the nodes within the hypersphere have been checked. The best solution is the ML solution.|$|R
2500|$|Rebalancing {{starts from}} a leaf and {{proceeds}} toward the root until {{the tree is}} balanced. If deleting an element from a node has brought it under the minimum size, then some elements must be redistributed to bring all nodes up to the minimum. Usually, the redistribution involves moving an element from a <b>sibling</b> <b>node</b> that has more than the minimum number of nodes. That redistribution operation is called a rotation. If no sibling can spare an element, then the deficient node must be merged with a sibling. The merge causes the parent to lose a separator element, so the parent may become deficient and need rebalancing. The merging and rebalancing may continue {{all the way to}} the root. Since the minimum element count doesn't apply to the root, making the root be the only deficient node is not a problem. The algorithm to rebalance the tree is as follows: ...|$|R
50|$|In the Hilbert R-tree, {{there is}} no need to re-insert {{orphaned}} nodes whenever a father node underflows. Instead, keys can be borrowed from the siblings or the underflowing node is merged with its siblings. This is possible because the nodes have a clear ordering (according to Largest Hilbert Value, LHV); in contrast, in R-trees {{there is no}} such concept concerning <b>sibling</b> <b>nodes.</b> Notice that deletion operations require s cooperating siblings, while insertion operations require s - 1 siblings.|$|E
50|$|The dynamic Hilbert R-tree is {{suitable}} for dynamic databases where insertions, deletions, or updates may occur in real time. Moreover, dynamic Hilbert R-trees employ flexible deferred splitting mechanism to increase the space utilization. Every node has a well defined set of <b>sibling</b> <b>nodes.</b> By adjusting the split policy the Hilbert R-tree can achieve a degree of space utilization as high as is desired. This is done by proposing an ordering on the R-tree nodes. The Hilbert R-tree sorts rectangles according to the Hilbert value {{of the center of}} the rectangles (i.e., MBR). (The Hilbert value of a point is the length of the Hilbert curve from the origin to the point.) Given the ordering, every node has a well-defined set of sibling nodes; thus, deferred splitting can be used. By adjusting the split policy, the Hilbert R-tree can achieve as high utilization as desired. To the contrary, other R-tree variants have no control over the space utilization.|$|E
40|$|The {{software}} includes twig query processing techniques allowing flexible {{methods of}} structural interrogation, both for ordered (the {{order of the}} <b>sibling</b> <b>nodes</b> of a query is important) and unordered (the order of the <b>sibling</b> <b>nodes</b> is not influential). Further, it includes algorithms and structures allowing an efficient execution also on remarkable amounts of data...|$|E
2500|$|Replace the {{separator}} in {{the parent}} {{with the first}} element of the right sibling (right <b>sibling</b> loses one <b>node</b> but still has at least the minimum number of elements) ...|$|R
30|$|In an XML tree, node A with label[*]<LevelA, ParentIDA, SelfIDA> and node B with label <LevelB, ParentIDB, SelfIDB>[*]should be in {{the same}} level to have the sibling relationship. That is, ParentIDA[*]=[*]ParentIDB and LevelA[*]=[*]LevelB. For example, node A with label “ 2, 00, 01.011 ” is the <b>sibling</b> of <b>node</b> B with label “ 2, 00, 01.001 ” since ParentIDA[*]=[*]ParentIDB[*]=[*] 00 and LevelA[*]=[*]LevelB[*]=[*] 2.|$|R
5000|$|What {{happens next}} {{depends on the}} color of other nearby nodes. The term uncle node {{will be used to}} refer to the <b>sibling</b> of a <b>node's</b> parent, as in human family trees. Note that: ...|$|R
3000|$|... [...]. In {{the event}} of an underflow, <b>sibling</b> <b>nodes</b> can be merged together. A functional-level {{algorithm}} is listed in Algorithm 4.|$|E
3000|$|... is {{allocated}} to node n^l_i_k, no other <b>sibling</b> <b>nodes</b> of n^l_i_k shall be assigned {{the same time}} slot for transmission to their parent node, n^l_p_k.|$|E
30|$|The space {{allocated}} for each leaf is calculated {{according to its}} weight and the space available from its branch. Each node is placed next to its siblings to achieve a high space utilization and also high proximity of <b>sibling</b> <b>nodes</b> [27].|$|E
50|$|For {{any given}} object O, {{it is clear}} that the method chooses each cluster, and hence each of the n sites, with equal probability. If the site finally {{selected}} is unavailable, we can select a different site within the same cluster, in the usual manner. Alternatively, we could go up one or more tiers in the skeleton and select an alternate from among the <b>sibling</b> virtual <b>nodes</b> at that tier, and once again descend the hierarchy to the real nodes, as above.|$|R
5000|$|The {{first and}} next {{procedures}} {{are used by}} the backtracking algorithm to enumerate the children of a node c of the tree, that is, the candidates that differ from c by a single extension step. The call first(P,c) should yield the first child of c, in some order; and the call next(P,s) should return the next <b>sibling</b> of <b>node</b> s, in that order. Both functions should return a distinctive [...] "null" [...] candidate, denoted here by 'Λ', if the requested child does not exist.|$|R
50|$|As {{an example}} of this type of argument, {{consider}} the set of all binary trees. We will show that the number of leaves in a full binary tree is one more than the number of interior nodes. Suppose there is a counterexample; then there must exist one with the minimal possible number of interior nodes. This counterexample, C, has n interior nodes and l leaves, where n + 1 ≠ l. Moreover, C must be nontrivial, because the trivial tree has n = 0 and l = 1 and is therefore not a counterexample. C therefore has at least one leaf whose parent node is an interior node. Delete this leaf and its parent from the tree, promoting the leaf's <b>sibling</b> <b>node</b> to the position formerly occupied by its parent. This reduces both n and l by 1, so the new tree also has n + 1 ≠ l and is therefore a smaller counterexample. But by hypothesis, C was already the smallest counterexample; therefore, the supposition that there were any counterexamples to begin with must have been false. The partial ordering implied by 'smaller' here is the one that says that S < T whenever S has fewer nodes than T.|$|R
40|$|Flooding is the {{simplest}} and {{most effective way to}} disseminate a packet to all nodes in a wireless sensor network (WSN). However, basic flooding makes all nodes transmit the packet at least once, resulting in the broadcast storm problem in a worst case, and in turn, network resources are severely wasted. Particularly, power is the most valuable resource of WSNs as nodes are powered by batteries, then the waste of energy by the basic flooding lessens the lifetime of WSNs. In order to solve the broadcast storm problem, this paper proposes a dynamic probabilistic flooding that utilizes the neighbor information like the numbers of child and <b>sibling</b> <b>nodes.</b> In general, the more <b>sibling</b> <b>nodes</b> there are, the higher is the probability that a broadcast packet may be sent by one of the <b>sibling</b> <b>nodes.</b> The packet is not retransmitted by itself, though. Meanwhile, if a node has many child nodes its retransmission probability should be high to achieve the high packet delivery ratio. Therefore, these two terms—the numbers of child and sibling nodes—are adopted in the proposed method in order to attain more reliable flooding. The proposed method also adopts the back-off delay scheme to avoid collisions between close neighbors. Simulation results prove that the proposed method outperforms previous flooding methods in respect of the number of duplicate packets and packet delivery ratio...|$|E
3000|$|... 2). Then the {{intersection}} of a line that connects this node to its sibling node and the sibling node circle is obtained, and the time complexity is O(n). Finally, check whether {{the intersection}} is within the communication range of all the <b>sibling</b> <b>nodes</b> and find the minimum travel distance. The time complexity is also O(n [...]...|$|E
40|$|Abstract: This article {{presents}} the results of an empirical experiment designed to gain insight into what is the effect of the minimax on the evaluation function. The experiment’s simulations were performed upon the KRK chess endgame. Main result is that dependencies between evaluations of <b>sibling</b> <b>nodes</b> in a game tree and an abundance of possibilities to commit blunders present in the KRK endgame are not enough to explain the success of the minimax in practical game-playing as was previously believed. The article argues that correlation between a high branching factor in the game tree and having a winning move available is something that minimax implicitly takes into account and that this is at least partially responsible for its success. Second part of the {{article presents}} ‘weighted minimax’, an attempt at exploiting dependencies between evaluations of <b>sibling</b> <b>nodes</b> in a game tree and shows that its results could be promising. Key words: minimax principle, KRK chess endgame, weighted minimax 1...|$|E
30|$|The {{labeling}} method {{supports the}} structural relationships, AD (Ancestor–Descendant), PC (Parent–Child), DO (Document Order) 1 and <b>Sibling</b> relations, between <b>nodes.</b> The method uses a simple algorithm to produce small-size labels. Experimental {{results show that}} the proposed method is efficient in terms of the label size, labeling time, querying time and update/insertion time. The results are compared against state-of-the-art labeling methods.|$|R
5000|$|An {{important}} {{choice when}} making a suffix tree implementation is the parent-child relationships between nodes. The most common is using linked lists called <b>sibling</b> lists. Each <b>node</b> has a pointer {{to its first}} child, and to the next node in the child list {{it is a part}} of. Other implementations with efficient running time properties use hash maps, sorted or unsorted arrays (with array doubling), or balanced search trees. We are interested in: ...|$|R
30|$|Without {{properly}} {{securing the}} virtualised infrastructure of Fog nodes in a 5 G network, providers risk {{not being able}} to achieve the desired performance. A single compromised Fog node in the 5 G mobile network can generate the potential entry point for a Man-in-the-Middle (MITM) attack and interrupt all connected users, leak data, abuse the service by exceeding the limit of data plan and damage <b>sibling</b> Fog <b>nodes.</b> A MITM attack can be launched by a malicious internal user and can exploit the Fog platform by sniffing, hijacking, injecting and filtering data incoming from the end-user [48]. This will consequently affect the data communication of the underlying network (E.g. the 5 G network). The most common way of eliminating such issues is to encrypt communication with either symmetric or asymmetric algorithms, mutual authentication, using the OAuth 2 protocol, and ensuring the isolation of compromised nodes and certificate pinning as discussed by [49].|$|R
40|$|For an XML database, it is {{important}} to automatically reason meaningful answers for a given keyword query. Among all relevant research efforts, the MaxMatch algorithm proposed to output contributors, which represent more (or equal number of) keywords than those contained by their <b>sibling</b> <b>nodes.</b> Such concept can distinguish the importance of <b>sibling</b> <b>nodes</b> and has attracted a lot of attention. In this paper, we intend to improve the time and space performance of the original approach. We first propose the LevelPrune algorithm, which only needs to construct the smallest necessary intermediate tree and can also reduce the times of processing nodes. We then extend the idea of constructing trees level by level to optimize physical representations, and design the corresponding algorithm LevelPrune+. According to the experimental results, our proposed algorithms outperform other state-of-the-art approaches by an order of magnitude in some cases. Moreover, the index size of LevelPrune+ is significantly reduced compared with that of the original approach...|$|E
40|$|In this paper, {{we focus}} on {{efficient}} construction of tightest matched subtree (TMSubtree) results, for keyword queries on extensible markup language (XML) data, based on smallest lowest common ancestor (SLCA) semantics. Here, "matched" means that all nodes in a returned subtree satisfy the constraint that the set of distinct keywords of the subtree rooted at each node is not subsumed by that of any of its <b>sibling</b> <b>nodes,</b> while "tightest" means that no two subtrees rooted at two <b>sibling</b> <b>nodes</b> can contain {{the same set of}} keywords. Assume that d is the depth of a given TMSubtree, m is the number of keywords of a given query Q. We proved that if d ≤ m, a matched subtree result has at most 2 m! nodes; otherwise, the size of a matched subtree result is bounded by (d - m + 2) m!. Based on this theoretical result, we propose a pipelined algorithm to construct TMSubtree results without rescanning all node labels. Experiments verify the benefits of our algorithm in aiding keyword search over XML data...|$|E
30|$|Though reconnecting two <b>sibling</b> <b>nodes</b> {{just needs}} to move one node for less than Rc, it may trigger another failure, which causes {{cascading}} failures. Even if the motion does not trigger other node failures, it may consume a lot of energy. Hence, load balancing should also be taken into consideration. In addition, a node motion may generate some other critical nodes, which {{increases the risk of}} node failure in the future. Considering all of these factors, we formulate the following problem.|$|E
40|$|In {{order to}} {{facilitate}} query processing for XML data, several path indexing, labelling and numbering scheme have been proposed. However, if XML data need to be updated frequently, most of these approaches will need to re-compute existing labels which is rather time consuming. In this paper, we propose a new Labelling Scheme for Dynamic XML data (LSDX) that supports {{the representation of the}} ancestor – descendant relationship and <b>sibling</b> relationship between <b>nodes.</b> Moreover, LSDX supports the process of updating XML data without the need of re-labelling existing labels, hence facilitating fast update. Some experimental works have been conducted to show its effectiveness...|$|R
40|$|International audienceWith {{the growing}} {{importance}} of XML in data exchange, much {{research has been}} done in providing flexible query facilities to extract data from structured XML documents. Thereby, several path indexing, labelling and numbering scheme have been proposed. However, if XML data need to be updated frequently, most of these approaches will need to re-compute existing labels which is rather time consuming. The goal of the research reported in this paper is to design a persistent structural labelling scheme, namely a labelling scheme where labels encode ancestor-descendant relationships and <b>sibling</b> relationship between <b>nodes</b> but need not to be changed when the document is updated...|$|R
40|$|Abstract. XML query {{processing}} {{needs to}} search an XML document and return results that satisfy a request. In this paper, we proposed an effective pattern match algorithm based on structural index. This algorithm uses a suitable labeling scheme and an efficient structural index by identifying parent-child, ancestor-descendant and <b>sibling</b> relationships among <b>nodes.</b> Due to the algorithm avoids invalid immoderate results {{to be pushed}} stacks, So, It reduces the amount of joint and improves the performance of Query. Experiment results proved that our algorithm, TwigM performs about 15 % better compared to TwigStack[1] and 10 % better than TwigINLAB[2] {{for all types of}} queries. I...|$|R
