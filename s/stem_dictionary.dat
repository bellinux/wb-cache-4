2|14|Public
40|$|An {{algorithm}} for suffix stripping Many {{strategies for}} suffix stripping {{have been reported}} in the literature (see for example [1] to [6]). The nature of the task will vary considerably depending on whether a <b>stem</b> <b>dictionary</b> is being used, whether a suffix list is being used, and of course on the purpose for which the suffix stripping is being done. Assuming that one is not making use of a <b>stem</b> <b>dictionary,</b> and that the purpose of the task is to improve IR performance, the suffix stripping program will usually be given an explicit list of suffixes, and, with each suffix, the criterion under which it may be removed from a word to leave a valid stem. This is the approach adopted here. The main merits of the present program are that it is small (less than 400 lines of BCPL), fast (it will proces a vocabulary of 10, 000 different words in about 8. 1 seconds on the IBM 370 / 165 at Cambridge University), and reasonably simple. At any rate, it is simple enough to be described in full as an algorithm in this paper. Given the speed of the program, it would be quite realistic to appl...|$|E
40|$|Representations of the Cranfield {{collection}} by both a {{thesaurus dictionary}} and a null, or word-stem dictionary, {{are subjected to}} Rocchio f s clustering algorithm. The clusters generated are then evaluated using ^ 2 search requests, and the results axe compared to find optimum cluster sizes for both null and thesaurus dictionary representations of the collection. The thesaurus dictionary produces the highest recall and precision when the cluster size is approximately 5 $ of the collection size. The word <b>stem</b> <b>dictionary</b> produces comparable results only if the cluster size is increased to approximately 10 $ of the collection size. Deletion of low-weighted concepts from the cluster centroids of either dictionary representation does not affect appreciably the recall and precision values obtained. 1...|$|E
40|$|Stemming {{has been}} an {{influential}} part in Information retrieval and search engines. There have been tremendous endeavours in making stemmer that are both efficient and accurate. Stemmers can have three method in <b>stemming,</b> <b>Dictionary</b> based stemmer, statistical-based stemmers, and rule-based stemmers. This paper aims at building a hybrid stemmer that uses both Dictionary based method and rule-based method for stemming. This ultimately helps the efficacy and accurateness of the stemmer. Comment: 8 pages, 5 tables, 1 figur...|$|R
40|$|Information {{retrieval}} {{focuses on}} content-based searching in text documents. For this purpose, first text content must be represented, {{by using a}} representation language (like thesauri or classification schemes) or by performing free-text search. The latter approach uses either string-based or computer-linguistic methods (<b>stemming,</b> <b>dictionary</b> lookup, syntax analysis). For retrieval, weighting and ranking methods give better results than Boolean retrieval, {{and some of them}} also allow for relevance feedback. Retrieval of XML documents requires new methods for support weighting and ranking, specificity-oriented search, data types with vague predicates and vague structural conditions. 1...|$|R
40|$|Abstract—In this paper, {{we propose}} a new {{approach}} for spellchecking errors committed in Arabic language. This approach is almost independent of the used dictionary, {{of the fact that}} we introduced the concept of morphological analysis in the process of spell-checking. Hence, our new system uses a <b>stems</b> <b>dictionary</b> of reduced size rather than exploiting a large dictionary not covering the all Arabic words. The obtained results are highly positive and satisfactory; this has allowed us to appreciate the validity of our concept and shows the importance of our new approach...|$|R
25|$|Many <b>stems</b> in the <b>dictionary</b> are indivisible; others {{consist of}} endings {{attached}} to a root.|$|R
50|$|Different tokens might {{carry out}} similar {{information}} (e.g. tokenization and tokenizing). And {{we can avoid}} calculating similar information repeatedly by reducing all tokens to its base form using various <b>stemming</b> and lemmatization <b>dictionaries.</b>|$|R
40|$|The lexical {{database}} of the humanistic and baroque Czech MADLA covers vocabulary from 1500 – 1780. It contains about 750 000 hand-excerpted documents <b>stemming</b> from <b>dictionaries,</b> herbaria, chronicles and other literary documents from this historical period. Using this database, {{it is possible}} to monitor the development of meaning, word formation, paradigm and other grammatical categories, and it serves as a basis for further research on the vocabulary from the 16 th– 18 th centuries. The text describes a representative sample of change in the meaning of the substantive kredenc („cupboard“), the meaning of which has shifted from the original „tasting“ to the contemporary „dresser“. Emphasis is placed on the proof and comparison of different meanings of this word...|$|R
50|$|Persian verbs {{are very}} regular {{compared}} with those of most European languages. From the two <b>stems</b> given in <b>dictionaries</b> (e.g. gir, gereft 'take, took', nevis, nevešt 'write, wrote', deh, dād 'give, gave' etc.) it is possible to make all the other forms of almost any verb. The main irregularity is that given one stem it is not usually possible to predict the other. Another irregularity is that the verb 'to be' has no stem in the present tense.|$|R
40|$|Introduction The University of Massachusetts {{took on the}} TREC 10 cross-language {{track with}} no prior {{experience}} with Arabic, and no Arabic speakers among any of our researchers or students. We intended to implement some standard approaches, and to extend a language modeling approach to handle co-occurrences. Given the lack of resources [...] training data, electronic bilingual dictionaries, and stemmers, and our unfamiliarity with Arabic, we had our hands full carrying out some standard approaches to monolingual and cross-language Arabic retrieval, and did not submit any runs based on novel approaches. We submitted three monolingual runs and one cross-language run. We first describe the models, techniques, and resources we used, then we describe each run in detail. Our official runs performed moderately well, in the second tier (3 rd or 4 th place). Since submitting these results, we have improved normalization and <b>stemming,</b> improved <b>dictionary</b> construction, expanded Arabic queries,...|$|R
40|$|In {{this paper}} we {{describe}} our approaches and results of evaluating the metadata in tagged user-generated videos {{as well as}} their visual features in order to extrapolate geographical relevance. The evaluation was done {{in the context of the}} MediaEval 2011 Placing Task in which we had to determine and to assign the best fitting geographical coordinates to each video. Our main goal was to realize this task with a retrieval framework developed by the Chemnitz University using the bag-of-words model to compare parts of metadata. This framework is used for indexing and comparing purposes. Particularly, it incorporates multiple lists of stop words, <b>stemming</b> lists and <b>dictionaries.</b> For enhancement purposes, we also used the GeoNames gazetteer despite noticing that the overall results seem to be slightly better using sole metadata comparisons...|$|R
40|$|This paper {{investigates the}} use of {{stemming}} for classification of Dutch (email) texts. We introduce a stemmer, which combines dictionary lookup (implemented efficiently as a finite state automaton) with a rule-based backup strategy and,how, that it outperforms the Dutch Porter stemmer in terms of accuracy. while not being substantially slower. For text classification, the most important property of a stemmer {{is the number of}} words it (correctly) reduces to the same <b>stem.</b> Here the <b>dictionary</b> - based system also outperforms Porter. However, evaluation of a Bayesian text classification system with either no stemming or the Porter or dictionary-based stemmer on an email classification and a newspaper topic classification task does not lead to significant differences in accuracy. We conclude with an analysis of why this is the case...|$|R
40|$|I {{have been}} a {{language}} activist and language teacher in my community for over 20 years. Even though I am a speaker, I never thought I had a deep understanding of my Isga language, the way that many of my Elders do. But after taking the Community Linguist Certificate program at CILLDI, I am using what I learned about linguistics to gain a deeper understanding about the language. I used to be afraid to talk in front of certain speakers because I didn’t pronounce things {{the same way as}} them or I used different words from them, but now I understand that the Isga language is changing, from the Elders, to my generation, to the kids in my class. This doesn’t mean that the language is dying. It’s how we know that the language is still alive. It is okay to invent new words. It’s even okay to use slang and translate song lyrics into Isga. In this presentation, I will describe some of the activities I use with my middle-school students in Nakota language classes. The language we use in the classroom doesn’t come from a book. I can teach students useful expressions and what certain things mean even if I don’t write things down and even if their relatives speak a little differently. The language can take it. My goal is to make them “Jedi warriors” in the language and to help them learn to figure out things on their own when they hear their relatives talking Nakota. I also work on a language documentation project in my community. We are building a dictionary where we include everyone’s way of pronouncing words. We are also trying to include morphemes and <b>stems</b> in our <b>dictionary,</b> not just whole words. Learners won’t ever be able to figure out the language if they can’t see the patterns inside the words. Before the CLC, I never saw inside a word, but now I always look. We are having CILLDI courses in our community now, and many other people from our language program are earning their CLCs. When we all learn together, it is more powerful...|$|R
40|$|The thesis {{deals with}} searching and proving the {{connections}} between the quality of organization and levels of economic outcomes of formal social units (firms). Therefore, it first theoretically defines and distinguishes the basic concepts of organizations and formal social units (firms) according to Lipovec’s developed theory of organization and its further development regarding the Mihelčič’s identification and segmentation of critical target organizational relationships. From there, the original Slovenian method of assessing the quality of organization of formal social unit MUKOZ is derived, which {{is used as a}} basis for assessing the organizational impacts on the economic results of the firm. To assess these effects, approach of data mining is used and initially theoretically briefly presented, from the regression, classification, clustering and attribute quality measures. The latest general method for explanation of regressors and classifiers is also used, which is independent of the applied methods of machine learning and the experiments raise expectations for very good results. Also, the thesis presents the approach of text mining, from the essential differences between text and numeric data, transformations between them, creating words in a <b>dictionary,</b> <b>stemming</b> to a basic form as well as classifying and clustering texts and its performance assessment, all in functional connection with voluminous text of the method for assessing the quality of organization. On the collected data from the formal social units in Slovenia the links between organizational indicators and economic outcomes are presented, separately for the economy and for the public sector. The findings of classical data mining methods and new general method for model's interpretation are cross compared, both confronted with standard statistic approach of Spearman's rank correlations in order to substantiate the links between indicators of the quality of organization and appropriate economic outcomes. On the model of assessment of the quality of organization MUKOZ the comparative analysis of classification algorithms for organizational problems is accomplished in order to verify the consistency, reliability and validity of this model. With the same purpose on the extensive questionnaire of this method text analysis is performed to further confirm its logical structure. On the elements of this model, three different clustering approaches are applied, based on the frequency of words, the binary presence / absence of words and TF-IDF measure, to prove throug data mining methods the substantial congruity with the current clustering model. At the same time, some new concepts are exposed to be considered in the future development of this method. ...|$|R
40|$|In 2010 Chang {{presented}} the T APS Checklist, “intended to help depositors of language materials assess digital language archives based on (1) areas of special concern to linguists and language communities (Target and Access) and (2) recommended best practices {{for the long-term}} preservation of digital information (Preservation and Sustainability) ” (2010 : iv). In 2009, two volunteers with no webpage creation experience created an online resource for the Coeur d’Alene community and scholars which included a searchable root <b>dictionary,</b> <b>stem</b> list, and affix list along with over 1, 200 pages of previously unpublished fieldnotes and typed manuscripts. Additionally, other resources such as links to original publications in various digital archives and information regarding the resources creation and history {{were included in the}} website. I n 2011, linguists, community members, and an engineer decided to use the TAPS Checklist to update the 2009 website. The goal was to create online language resources at the grassroots level in line with the best practices outlined in Chang 2010. Although the TAPS Checklist was designed for assessment by depositors, we decided to use it to inform the updating of the 2009 website. To this end, we treated the directory containing the resources accessed via the internet browser interface as an “archive”. The goal has been to develop and maintain this directory as nearly as possible with the checklist outlined in Chang 2010. This presentation highlights advantages of using the TAPS Checklist as a guide for the development of community-based digital language projects. We argue that using the best practices outlined in Chang 2010 can ensure longevity and accessibility of important digital language resources. Further, following such practices can ensure materials that would not necessarily be appropriate for archives (e. g. interactive digital dictionaries) can be created and maintained in a similar fashion and thus be available for use in the future. We compare the 2009 website with the current updated site, which we call the Coeur d’Alene Online Language Resource Center (COLRC). We present some necessary changes made to bring it in line with the TAPS Checklist, changes such as the development of the following: metadata grounded in the Dublin Core and extended elements; Mission Statement including the succession plan; disaster plan; backup and storage plan; history of resources and their origin; maintenance plan; among other elements. We believe the COLRC can be an example of how existing online language resources can be updated to conform to best practices. References Chang, Debbie. 2010. TAPS: Checklist for Responsible Archiving of Digital Language Resources. MA thesis Graduate Institute of Applied Linguistics...|$|R

