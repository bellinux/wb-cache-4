0|40|Public
40|$|In this paper, we {{comparatively}} evaluate two photonic {{packet switch}} architectures with WDM-FDL buffers for <b>synchronized</b> <b>variable</b> length packets. The {{first one is}} an output buffer type switch, which stores packets in the FDL buffer attached to each output port. Another is a shared buffer type switch, which stores packets in the shared FDL buffer. The performance of a switch is greatly influenced by its architecture and the packet scheduling algorithm...|$|R
40|$|This paper {{studies the}} design and {{implementation}} of the Multi-Threading Architecture (MTA) supercomputer produced by the Tera Computer Company. It describes the most salient hardware and software features of this architecture, including light-weight synchronization and hardware/software cooperation. Special emphasis is placed on the available programming methodolgies (futures, <b>synchronized</b> <b>variables,</b> and implicit parallelism) and their implementation on the Tera hardware. Programs from two very different problem domains, matrix multiplication and dynamic programming, are used to study Tera's development environment. Di erent versions of the same program are compared in order to evaluate the suitability of varying parallelizing techniques...|$|R
50|$|Sequential {{consistency}} can {{be achieved}} simply by hardware implementation, while release consistency is also based on an observation {{that most of the}} parallel programs are properly synchronized. In programming level, synchronization is applied to clearly schedule a certain memory access in one thread to occur after another. When a <b>synchronized</b> <b>variable</b> is accessed, hardware would make sure that all writes local to a processor have been propagated to all other processors and all writes from other processors are seen and gathered. In release consistency model, the action of entering and leaving a critical section are classified as acquire and release and for either case, explicit code should be put in the program showing when to do these operations.|$|R
40|$|In {{this paper}} we analyze the {{semantics}} of a higher-order functional language with concurrent threads, monadic IO and <b>synchronizing</b> <b>variables</b> as in Concurrent Haskell. To assure declarativeness of concurrent programming we extend the language by implicit, monadic, and concurrent futures. As semantic model we introduce {{and analyze the}} process calculus CHF, which represents a typed core language of Concurrent Haskell extended by concurrent futures. Evaluation in CHF is defined by a small-step reduction relation. Using contextual equivalence based on may- and should-convergence as program equivalence, we show that various transformations preserve program equivalence. We establish a context lemma easing those correctness proofs. An important result is that call-by-need and call-by-name evaluation are equivalent in CHF, since they induce the same program equivalence. Finally we show that the monad laws hold in CHF under mild restrictions on Haskell’s seq-operator, which for instance justifies {{the use of the}} do-notation...|$|R
40|$|In this paper, we {{comparatively}} evaluate two photonic {{packet switch}} architectures with WDM-FDL buffers for <b>synchronized</b> <b>variable</b> length packets. The {{first one is}} an output buffer type switch, which stores packets in the FDL buffer attached to each output port. Another is a shared buffer type switch, which stores packets in the shared FDL buffer. The performance of a switch is greatly influenced by its architecture and the packet scheduling algorithm. We compare the performance of these two packet switches by applying different packet scheduling algorithms. Through simulation experiments, we show that each architecture has a parameter region for achieving a better performance. For the shared buffer type switch, we found that void space introduces unacceptable performance degradation when the traffic load is high. Accordingly, we propose a void space reduction method. Our simulation results show that our proposed method enables to the shared buffer type switch to outperform the output buffer type switch even under high traffic load condition...|$|R
40|$|Condition {{variables}} {{are a common}} synchronization mechanism present in many programming languages. Still, due to the combinatorial complexity of the behaviours the mechanism induces, {{it has not been}} addressed sufficiently with formal techniques. In this paper we propose a fully automated technique to prove the correct synchronization of concurrent programs <b>synchronizing</b> via condition <b>variables,</b> where under correctness we mean the liveness property: "If for every set of condition <b>variables,</b> every thread <b>synchronizing</b> under the <b>variables</b> of the set eventually enters its synchronization block, then every thread will eventually exit the synchronization". First, we introduce SyncTask, a simple imperative language to specify parallel computations that <b>synchronize</b> via condition <b>variables.</b> Next, we model the constructs of the language as Petri Net components, and define rules to extract and compose nets from a SyncTask program. We show that a SyncTask program terminates if and only if the corresponding Petri Net always reaches a particular final marking. We thus transform the verification of termination into a reachability problem on the net, which can be solved efficiently by existing Petri Net analysis tools. Further, to relieve the programmer from the burden of having to provide specifications in SyncTask, we introduce an economic annotation scheme for Java programs to assist the automated extraction of SyncTask programs capturing the synchronization behaviour of the underlying program. We show that, for the Java programs that can be annotated according to the scheme, the above-mentioned liveness property holds if and only if the corresponding SyncTask program terminates. Both the SyncTask program extraction and the generation of Petri Nets are implemented in the STaVe tool. We evaluate the proposed verification framework on a number of test cases QC 20160517 </p...|$|R
40|$|A {{technique}} is introduced for estimating unknown parameters when time series {{of only one}} variable from a multivariate nonlinear dynamical system is given. The technique employs a combination of two different control methods, a linear feedback for <b>synchronizing</b> system <b>variables</b> and an adaptive control. The {{technique is}} shown to work even when the unknown parameter appears in the evolution equations of the variables {{other than the one}} for which the time series is given. The technique not only esablishes that explicit detailed information about all system variables and parameters is contained in a scalar time series, but also gives a way to exract it out under suitable conditions. Illustrations are presented and effect of noise is studied. Comment: Revised for simultaneous estimation of many parameters. 24 pages of RevTex, 12 figures in postscript files. To appear in PR(E...|$|R
40|$|We {{investigate}} {{the effects of}} heterogeneous delays in the coupling of two excitable neural systems. Depending upon the coupling strengths and the time delays in the mutual and self-coupling, the compound system exhibits different types of <b>synchronized</b> oscillations of <b>variable</b> period. We analyze this synchronization based on the interplay of the different time delays and support the numerical results by analytical findings. In addition, we elaborate on bursting-like dynamics with two competing timescales {{on the basis of}} the autocorrelation function. Comment: 18 pages, 14 figure...|$|R
40|$|Synchronization {{has been}} {{suggested}} as a mechanism of binding distributed feature representations facilitating segmentation of visual stimuli. Here we investigate this concept based on unsupervised learning using natural visual stimuli. We simulate dual-variable neural oscillators with separate activation and phase variables. The binding {{of a set of}} neurons is coded by <b>synchronized</b> phase <b>variables.</b> The network of tangential synchronizing connections learned from the induced activations exhibits small-world properties and allows binding even over larger distances. We evaluate the resulting dynamic phase maps using segmentation masks labeled by human experts. Our simulation results show a continuously increasing phase synchrony between neurons within the labeled segmentation masks. The evaluation of the network dynamics shows that the synchrony between network nodes establishes a relational coding of the natural image inputs. This demonstrates that the concept of binding by synchrony is applicable in the context of unsupervised learning using natural visual stimuli...|$|R
40|$|A new {{synchronization}} method is investigated for node of complex networks consists of complex chaotic system. When complex networks realize synchronization, different component of complex state <b>variable</b> <b>synchronize</b> up to different scaling complex function by a designed complex feedback controller. This paper change synchronization scaling function from real field to complex field for synchronization in node of complex networks with complex chaotic system. Synchronization in constant delay and time-varying coupling delay complex networks are investigated, respectively. Numerical simulations are provided {{to show the}} effectiveness of the proposed method...|$|R
40|$|This paper {{concerns}} the optimal control of modular hybrid systems <b>synchronized</b> by shared <b>variables.</b> Instead of <b>synchronizing</b> the discrete {{dynamics of the}} system into one global mode before optimization, Constraint Programming (CP) is used to model the discrete dynamics of each modular system separately. Integrated in the CP solver are also classic Operations Research (OR) models {{in the form of}} Nonlinear Programs (NLPs) approximating the continuous dynamics of the system. Using CP considerably decreases the number of NLPs which must be solved, compared to that of using a traditional mixed integer nonlinear programming approach...|$|R
50|$|Although a VPR has a {{drawback}} for {{not being}} able to identify plankton into species level, the advantages to use this instrument has gone beyond its limitation to provide us with convenient and accurate result. The high resolution result on plankton taxa observation along with <b>synchronized</b> measurement environmental <b>variables</b> from another oceanographic sensor attached in VPR body can be considered as the milestone of this device. In addition, since the observation is conducted visually by photographing the sample, the observation of delicate plankton and gelatinous species can be done accurately without having them destroyed in the net.|$|R
40|$|Abstract. Asynchronous automata are {{parallel}} compositions of finite-state processes <b>synchronizing</b> over shared <b>variables.</b> A deep theorem due to Zielonka says that every regular trace language {{can be represented}} by a deterministic asynchronous automaton. In this paper we improve the construction, in {{that the size of}} the obtained asynchronous automaton is polynomial in the size of a given DFA and simply exponential in the number of processes. We show that our construction is optimal within the class of automata produced by Zielonka-type constructions. In particular, we provide the first non trivial lower bound on the size of asynchronous automata. ...|$|R
40|$|Curry is a multi-paradigm {{declarative}} language aiming to amalgamate functional, logic, and concurrent programming paradigms. Curry combines in a seamless way features from functional programming and (concurrent) logic programming. Curry's operational semantics {{is based on}} the combination of lazy reduction of expressions together with a possibly non-deterministic binding of free variables occurring in expressions. Moreover, (equational) constraints can be executed concurrently which provides for passive constraints and concurrent computation threads that are <b>synchronized</b> on logical <b>variables.</b> This paper sketches a first prototype implementation of Curry in Java. The main emphasis of this implementation is the exploitation of Java threads to implement the concurrent and non-deterministic features of Curry...|$|R
40|$|The primary {{objective}} {{of this research was}} to investigate and develop an electrostatic energy-harvesting voltage-constrained CMOS/BiCMOS integrated circuit (IC) that harnesses ambient kinetic energy from vibrations with a vibration-sensitive variable capacitor and channels the extracted energy to charge an energy-storage device (e. g., battery). The proposed harvester charges and holds the voltage across the vibration-sensitive variable capacitor so that vibrations can induce it to generate current into the battery when capacitance decreases (as its plates separate). To that end, the research developed an energy-harvesting system that <b>synchronizes</b> to <b>variable</b> capacitor's state as it cycles between maximum and minimum capacitance by controlling each functional phase of the harvester and adjusting to different voltages of the on-board battery. One of the major challenges of the system was performing all of these duties without dissipating the energy harnessed and gained from the environment. Consequently, the system reduces losses by time-managing and biasing its circuits to operate only when needed and with just enough energy while charging the capacitor through an efficient inductor-based precharger. As result, the proposed energy harvester stores a net energy gain in the battery during each vibration cycle. Ph. D. Committee Chair: Rincon-Mora, Gabriel A.; Committee Member: Ayazi, Farrokh; Committee Member: Divan, Deepakraj; Committee Member: Leach, W. Marshall; Committee Member: Morley, Thoma...|$|R
40|$|Curry is a multi-paradigm {{declarative}} language covering functional, logic, and concurrent programming paradigms. Curry's operational semantics {{is based on}} lazy reduction of expressions extended by a possibly non-deterministic binding of free variables occurring in expressions. Moreover, constraints can be executed concurrently which provides for concurrent computation threads that are <b>synchronized</b> on logical <b>variables.</b> In this paper, we extend Curry's basic computational model by a few primitives to support distributed applications where a dynamically changing number of di#erent program units must be coordinated. We develop these primitives as a special case of the existing basic model so that the new primitives interact smoothly with the existing features for search and concurrent computations. Moreover, programs with local concurrency can be easily transformed into distributed applications. This supports a simple development of distributed systems that are executable on local netwo [...] ...|$|R
40|$|AbstractBatch {{processes}} are commonly characterized by uneven trajectories {{due to the}} existence of batch-to-batch variations. The batch end-product quality is usually measured at the end of these uneven trajectories. It is necessary to align the time differences for both the measured trajectories and the batch end-product quality in order to implement statistical process monitoring and control schemes. Apart from <b>synchronizing</b> trajectories with <b>variable</b> lengths using an indicator variable or dynamic time warping, this paper proposes a novel approach to align uneven batch data by identifying short-window PCA&PLS models at first and then applying these identified models to extend shorter trajectories and predict future batch end-product quality. Furthermore, uneven batch data can also be aligned to be a specified batch length using moving window estimation. The proposed approach and its application to the control of batch end-product quality are demonstrated with a simulated example of fed-batch fermentation for penicillin production...|$|R
40|$|This paper {{presents}} a top-down mechanism for coordinating Distributed Discrete Event Simulation (DDES) models using an MRP/ERP system as the federation coordinator. The same MRP/ERP system, which is typically {{used as a}} coordination tool for interactions between complex highly variable manufacturing systems, serves to coordinate and <b>synchronize</b> complex highly <b>variable</b> simulation models of these same systems. This research focuses on enabling each system entity modeled by DDES models to constantly correct its performance with respect to reference trajectories which consist of planned orders {{and the size of}} a time bucket generated by an MRP/ERP system, and trigger a global coordinator which consists of the MRP/ERP system and adapter if necessitated by any discrepancies observed by the entity through simulation models. A global coordinator can synchronize timing of DDES models and provide adaptive time buckets using the cost-based mathematical model and corrected plans using the updated time bucket...|$|R
40|$|Abstract — An {{important}} {{challenge in}} multiagent systems is consensus, {{in which the}} agents are required to <b>synchronize</b> certain controlled <b>variables</b> of interest, often using only an incomplete and time-varying communication graph. We propose a consensus approach based on optimistic planning (OP), a predictive control algorithm that finds near-optimal control actions for any nonlinear dynamics and reward (cost) function. At every step, each agent uses OP to solve a local control problem with rewards that express the consensus objectives. Neighboring agents coordinate by exchanging their predicted behaviors in a predefined order. Due to its generality, OP consensus can adapt to any agent dynamics and, by changing the reward function, {{to a variety of}} consensus objectives. OP consensus is demonstrated for velocity consensus (flocking) with a time-varying communication graph, where it preserves connectivity better than a classical algorithm; and for leaderless and leader-based consensus of robotic arms, where OP easily deals with the nonlinear dynamics. I...|$|R
40|$|Concurrent logic {{languages}} are high-level programming languages for parallel and distributed systems {{that offer a}} wide range of both known and novel concurrent programming techniques. Being logic programming languages, they preserve many advantages of the abstract logic programming model, including the logical reading of programs and computations, the convenience of representing data structures with logical terms and manipulating them using unification, and the amenability to metaprogramming. Operationally, their model of computation consists of a dynamic set of concurrent processes, communicating by instantiating shared logical <b>variables,</b> <b>synchronizing</b> by waiting for variables to be instantiated, and making nondeterministic choices, possibly based on the availability of values of variables. This paper surveys the family of concurrent logic programming languages within a uniform operational framework. It demonstrates the expressive power of even the simplest language in the family and investigates how varying the basic synchronization and control constructs affect the expressiveness and efficiency of the resulting languages. In addition, the paper reports on techniques for sequential and parallel implementatio...|$|R
40|$|With {{the rise}} of chip multiprocessors (CMPs), it is {{necessary}} to use parallel programming to exploit computational power of CMPs. Traditionally, lock-based mechanisms have been used to <b>synchronize</b> shared <b>variables</b> in parallel programs. However, with the complexity associated with locks, writing a correct parallel program is a huge burden for programmers. As an alternative, Transactional Memory (TM) is gaining momentum as a parallel programming model for multi [...] ?core processors. TM provides programmers with an atomic construct (transaction), which can be used to guarantee atomicity of accesses to shared variables, as the synchronization is handled through the underlying system. Transactional memory comes in two variants: Software transaction memory (STM) and Hardware transaction memory (HTM). Both STM and HTM systems have advantages and disadvantages that either enhance or penalize performance in transactional applications. In this thesis, the focus is on implementing an adaptive system that exploits both STM and HTM at transaction granularity. The goal is to achieve performance gain by incorporating the benefits of both TM systems. A synchronization technique is developed to seamlessly switch between HTM and STM based on the characteristics of a transaction. We exploit decision tree to predict the optimum system for each transaction in a given application. The decision tree is a form of supervised machine learning to classify transactions based on parameters such as transaction size, transaction write ratio, etc. From the evaluations using STAMP, NAS, and DiscoPoP benchmark suites, the proposed adaptive system is able to improve speed of transactional applications by 20. 82 % on average...|$|R
40|$|The 4 th Ninapro {{database}} includes 10 intact subjects {{recorded with}} "Cometa" electrodes ([URL] The database is thoroughly {{described in the}} paper: "Pizzolato et al., Comparison of Six Electromyography Acquisition Setups on Hand Movement Classification Tasks, Plos One 2017 (accepted). ". Please, cite this paper for any work related to the 5 th Ninapro database. The dataset {{is part of the}} Ninapro database ([URL] Please, look at the database for more information. Acquisition Protocol The subjects repeat several movements represented by movies that are shown on the screen of a laptop. The experiment is divided in three exercises: 1. Basic movements of the fingers 2. Isometric, isotonic hand configurations and basic wrist movements 3. Grasping and functional movements During the acquisition, the subjects were asked to repeat the movements with the right hand. Each movement repetition lasted 5 seconds and was followed by 3 seconds of rest. The protocol includes 6 repetitions of 52 different movements (plus rest) performed by 10 intact subjects. The movements were selected from the hand taxonomy as well as from hand robotics literature. Acquisition Setup The muscular activity is gathered using 12 active single–differential wireless electrodes from Cometa. The electrodes are positioned as shown in the figure: eight electrodes are equally spaced around the forearm in correspondence to the radio humeral joint; two electrodes are placed on the main activity spots of the flexor digitorum and of the extensor digitorum; two electrodes are placed on the main activity spots of the biceps and of the triceps. The described locations have been chosen in order to combine a dense sampling approach with a precise anatomical positioning strategy. The electrodes were fixed on the forearm using their standard adhesive bands. The sEMG signals are sampled at a rate of 2 kHz. During the acquisition, the subjects were asked to repeat the movements with the right hand. Each movement repetition lasted 5 seconds and was followed by 3 seconds of rest. The protocol includes 6 repetitions of 52 different movements (plus rest) performed by 10 intact subjects. The movements were selected from the hand taxonomy as well as from hand robotics literature. Data Sets For each exercise, for each subject, the database contains one matlab file with <b>synchronized</b> <b>variables.</b> The variables included in the matlab files are: • subject: the subject number; • sensor: the name of the sEMG sensor; • frequency: the frequency in Hertz of the recorded data • exercise: exercise number; • emg: sEMG signal. Columns 1 - 8 are the electrodes equally spaced around the forearm {{at the height of the}} radio humeral joint. Columns 9 and 10 contain signals from the main activity spot of the muscles flexor and extensor digitorum superficialis, while columns 11 and 12 contain signals from the main activity spot of the muscles biceps brachii and triceps brachii. • stimulus: the original label of the movement repeated by the subject; • restimulus: the corrected stimulus, processed with movement detection algorithms; • repetition: stimulus repetition index; • rerepetition: restimulus repetition index; • age: subject’s age; • gender: subject’s gender, ”m” for male ”f” for female; • weight: subject’s weight in kilograms; • height: subject’s height in centimeters; • laterality: subject’s laterality, ”r” for right-handed, ”l” for left-handed; • circumference: circumference of the subject’s forearm at the radio-humeral joint height, measured in centimeters...|$|R
40|$|A co-simulation {{methodology}} is explored whereby {{a finite}} element code and a multi-body dynamics code featuring flexible cantilevered beams can be coupled and interactively executed. The floating {{frame of reference}} formulation is used to develop the equations of motion. The floating frame is fixed at the blade root. Such a formulation results in ordinary differential equations without added algebraic constraints. A variety of loose coupling and tight coupling schemes are examined for this problem. To <b>synchronize</b> the coupling <b>variables,</b> a Gauss-Seidel type iterative algorithm is used. The resulting fixed-point iterations are accelerated using Aitken?s adaptive relaxation technique. The methodology is evaluated for FAST, a wind turbine aeroelastic simulation code developed by NREL. As with FAST, many multi-body codes which can model flexibility employ modal methods. A proposed addition for FAST to simulate flexible effects using a finite element method module offers a potential to include a variety of non-linearities and also provides possibilities for using a high-fidelity aerodynamics module. The coupling schemes are compared and their applicability and limitations for different scenarios are pointed out. Results validating the approach are provided...|$|R
40|$|Abstract. Curry is a multi-paradigm {{declarative}} language covering functional, logic, and concurrent programming paradigms. Curry’s operational semantics {{is based on}} lazy reduction of expressions extended by a possibly non-deterministic binding of free variables occurring in expressions. Moreover, constraints can be executed concurrently which provides for concurrent computation threads that are <b>synchronized</b> on logical <b>variables.</b> In this paper, we extend Curry’s basic computational model by a few primitives to support distributed applications where a dynamically changing number of different program units must be coordinated. We develop these primitives as a special case of the existing basic model so that the new primitives interact smoothly with the existing features for search and concurrent computations. Moreover, programs with local concurrency can be easily transformed into distributed applications. This supports a simple development of distributed systems that are executable on local networks {{as well as on}} the Internet. In particular, sending partially instantiated messages containing logical variables is quite useful to implement reply messages. We demonstrate the power of these primitives by various programming examples. ...|$|R
40|$|Biological rhythms are {{critical}} in the etiology of mood disorders; therefore, effective mood disorder treatments should address rhythm disturbances. Among the <b>variables</b> <b>synchronized</b> with the light–dark cycle, spontaneous activity in rodents is useful for investigating circadian rhythms. However, previous studies have focused only on the increase of wheel-running activity under restricted feeding conditions, while little information is available on circadian rhythm of running activity. In this study, chronometrical {{analysis was used to}} assess whether circadian rhythms during wheel-running are altered by restricted feeding and affected by antidepressant drugs. Wheel revolutions were automatically recorded and analyzed using cosinor-rhythmometry in 8 -week old ICR albino mice. When feeding was restricted to 1  h per day (21 : 00 – 22 : 00), wheel-running rhythms were reliably disrupted. Female mice exhibited marked alterations in the pattern and extent of wheel-running beginning on day 1. Subchronic treatment with imipramine or paroxetine, as well as tandospirone and (−) -DOI, prevented wheel-running rhythm disruption. Thus, altering the circadian activity rhythms of female mice on a 1 -h feeding schedule may be useful for investigating disturbances in biological rhythms...|$|R
40|$|We {{address the}} problem of {{automatically}} establishing correctness for programs generating an arbitrary number of concurrent processes and manipulating variables ranging over an infinite domain. The programs we consider can make use of the shared variables to count and synchronize the spawned processes. This allows them to implement intricate synchronization mechanisms, such as barriers. Automatically verifying correctness, and deadlock freedom, of such programs is beyond the capabilities of current techniques. For this purpose, we make use of counting predicates that mix counters referring to the number of processes satisfying certain properties and variables directly manipulated by the concurrent processes. We then combine existing works on counter, predicate, and constrained monotonic abstraction and build a nested counter example based refinement scheme for establishing correctness (expressed as non-reachability of configurations satisfying counting predicates formulas). We have implemented a tool (Pacman, for predicated constrained monotonic abstraction) and used it to perform parameterized verification on several programs whose correctness crucially depends on precisely capturing the number of processes <b>synchronizing</b> using shared <b>variables.</b> Funding agencies: 12. 04 CENIIT project</p...|$|R
40|$|The {{objective}} of sinusoidal PWM technique is to synthesize the motor currents as near to a sinusoid as possible economically. Traditionally, PWM control is accomplished using a natural sampling technique. Though its implementation {{is simple and}} in real time, it cannot give good performance in the entire operating range. Microprocessor-based PWM generation can offer significant advantages but suffer from computational time limitations. Moreover, it is not economical for general-purpose variable speed drives. A novel technique of PWM signal generation is addressed in this paper. While different requirements of high performance SPWM are fulfilled, the circuit configuration is reduced compared with conventional circuits. A regular sampled, asymmetric, <b>synchronized</b> SPWM with <b>variable</b> gear ratio and third harmonic injection at higher modulation depth, is realized with much reduced hardware. Such reduction in complexity, cost and space are essential to boost the changeover from constant speed to variable speed drive for energy saving in general applications. This was the motivation of work presented. The technique is well suited even for dedicated implementation using a microcontroller. © IEE...|$|R
40|$|Abstract—The third {{generation}} (3 G) of cellular system adopted the spread spectrum as {{solution for the}} transmission of the data in the physical layer. Contrary to systems IS- 95 or CDMAOne (systems with spread spectrum of the preceding generation), the new standard, called Universal Mobil Telecommunications System (UMTS), uses long codes in the down link. The system is conceived for the vocal communication and the transmission of the data. In particular, the down link is very important, because of the asymmetrical request of the data, i. e., more remote loading towards the mobiles than towards the basic station. Moreover, the UMTS uses for the down link an orthogonal spreading out with a variable factor of spreading out (OVSF for Orthogonal Variable Spreading Factor). This characteristic {{makes it possible to}} increase the flow of data of one or more users by reducing their factor of spreading out without changing the factor of spreading out of other users. In the current standard of the UMTS, two techniques to increase the performances of the down link were proposed, the diversity of sending antenna and the codes space-time. These two techniques fight only fainding. The receiver proposed for the mobil station is the RAKE, but one can imagine a receiver more sophisticated, able to reduce the interference between users and the impact of the coloured noise and interferences to narrow band. In this context, where the users have long codes <b>synchronized</b> with <b>variable</b> factor of spreading out and ignorance by the mobile of the other active codes/users, the use of the sequences of code pseudo-noises different lengths is presented in the form {{of one of the most}} appropriate solutions. Keywords—DS-CDMA, multiple access interference, ratio Signal / interference + Noise. I...|$|R
40|$|This paper {{describes}} the conception {{and the development}} of a real- time data-acquisition system for prototype detectors of the Tracker being designed for the compact muon solenoid (CMS) experiment at the Large Hadron Collider of CERN, European Laboratory for Particle Physics, Geneva, Switzerland. The rationale for the development of a dedicated data-acquisition system was the need to perform two fundamental beam tests (the "Milestone Barrel 1 " and "Milestone Forward 1 "), with large-scale prototypes of the detectors planned as the baseline design. The number of readout channels, the complexity of the readout electronics, and the stringent requirements of the milestone tests mandated that a thorough understanding of the issues related to the physics of the detectors themselves be coupled with the application of leading-edge electronic and software engineering technologies. The implementation described in this paper is based on a distributed architecture. An event builder CPU handles the two main tasks of <b>synchronizing</b> a <b>variable</b> number of front-end processors and formatting the data in preparation for the transfer to a dedicated high-performance storage system, while the front-end processors handle the hardware and the real-time readout. Additional workstations are used to decouple the actual task of transferring the data files and monitoring the detector performance on-line from the readout farm. The system has been successfully operated during the two aforementioned Milestone tests, allowing the CMS Tracker collaboration to pass them, with the simultaneous readout of up to 40000 detector channels. The results of the two Milestones have led to the compilation of the "Tracker Technical Design Report". Subsequently, the same readout system has been used for a number of other beam tests, and it has formed the basis for the development of further, more advanced data-acquisition systems for the new readout electronic of the CMS Tracker. (28 refs) ...|$|R
40|$|The 5 th Ninapro {{database}} includes 10 intact subjects {{recorded with}} two Thalmic Myo ([URL] armbands. The database {{can be used}} to test the Myo armbands separately as well. The database is thoroughly described in the paper: "Pizzolato et al., Comparison of Six Electromyography Acquisition Setups on Hand Movement Classification Tasks, Plos One 2017 (accepted). ". Please, cite this paper for any work related to the 5 th Ninapro database. The dataset is part of the Ninapro database ([URL] Please, look at the database for more information. Acquisition Protocol The subjects have to repeat several movements represented by movies that are shown on the screen of a laptop. The experiment is divided in three exercises: 1. Basic movements of the fingers 2. Isometric, isotonic hand configurations and basic wrist movements 3. Grasping and functional movements During the acquisition, the subjects were asked to repeat the movements with the right hand. Each movement repetition lasted 5 seconds and was followed by 3 seconds of rest. The protocol includes 6 repetitions of 52 different movements (plus rest) performed by 10 intact subjects. The movements were selected from the hand taxonomy as well as from hand robotics literature. Acquisition Setup The muscular activity is gathered using 2 Thalmic Myo armbands. The database {{can be used to}} test the Myo armbands separately as well. The subjects in this database wore two Myo armbands one next to the other, including 16 active single–differential wireless electrodes. The top Myo armband is placed closed to the elbow with the first sensor placed on the radio humeral joint, as in the standard Ninapro configuration for the equally spaced electrodes; the second Myo armband is placed just after the first, nearer to the hand, tilted of 22. 5 degrees. This configuration provides an extended uniform muscle mapping at an extremely affordable cost. The Myo sensors do not require the arm to be shaved and after few minutes the armband tighten very firmly to the arm of the subject. The sEMG signals are sampled at a rate of 200 Hz. The kinematic information is recorded with a dataglove (22 sensors Cyberglove 2). The cyberglove signal corresponds to raw data from the cyberglove sensors located as shown in the following pictures. The raw data are declared to be proportional to the angles at the joints in the CyberGlove manual. Data Sets For each exercise, for each subject, the database contains one matlab file with <b>synchronized</b> <b>variables.</b> The variables included in the matlab files are: • subject: the subject number; • sensor: the name of the sEMG sensor; • frequency: the frequency in Hertz of the recorded data • exercise: exercise number; • emg: sEMG signal. Columns 1 - 8 are the electrodes equally spaced around the forearm {{at the height of the}} radio humeral joint. Columns 9 - 16 represent the second Myo, tilted by 22. 5 degrees clockwise. • acc (3 columns) : raw signals from the three axis accelerometer of the first Myo, found in the Myo DB • glove (22 columns) : uncalibrated signal from the 22 sensors of the CyberGlove. The raw data are declared to be proportional to the angles of the joints in the CyberGlove manual. • stimulus: the original label of the movement repeated by the subject; • restimulus: the corrected stimulus, processed with movement detection algorithms; • repetition: stimulus repetition index; • rerepetition: restimulus repetition index; • age: subject’s age; • gender: subject’s gender, ”m” for male ”f” for female; • weight: subject’s weight in kilograms; • height: subject’s height in centimeters; • laterality: subject’s laterality, ”r” for right-handed, ”l” for left-handed; • circumference: circumference of the subject’s forearm at the radio-humeral joint height, measured in centimeters...|$|R
30|$|Chaos {{synchronization}} {{refers to}} a process wherein two dynamical systems (master and slave systems, respectively) adjust their motion to achieve a common behavior, mainly due to a coupling or control input [1]. The issue was firstly studied in dynamical systems described by integer-order differential equations [2]. By considering the historical timeline of the topic, it can be observed that a large variety of synchronization types has been proposed [3, 4]. Among the different methods, projective synchronization provides the slave system variables consisting in scaled replicas of the master system variables [5]. Recently, the full-state hybrid projective synchronization (FSHPS) has been introduced, wherein each slave system <b>variable</b> <b>synchronizes</b> with a linear combination of master system variables. On the other hand, when the inverted scheme is implemented, i.e., each master system state synchronizes with a linear combination of slave system states, the inverse full-state hybrid projective synchronization (IFSHPS) is obtained [6]. Moreover, when the scaling factors are replaced by scaling functions, function-based hybrid synchronization schemes are obtained, i.e., the full-state hybrid function projective synchronization (FSHFPS) and the inverse full-state hybrid function projective synchronization (IFSHFPS), respectively. Note that some synchronization types may coexist in chaotic systems.|$|R
40|$|We study {{distributed}} {{methods for}} online prediction and stochastic optimization. Our approach is iterative: in each round nodes first perform local computations and then communicate {{in order to}} aggregate information and <b>synchronize</b> their decision <b>variables.</b> Synchronization is accomplished {{through the use of}} a distributed averaging protocol. When an exact distributed averaging protocol is used, it is known that the optimal regret bound of O(√(m)) can be achieved using the distributed mini-batch algorithm of Dekel et al. (2012), where m is the total number of samples processed across the network. We focus on methods using approximate distributed averaging protocols and show that the optimal regret bound can also be achieved in this setting. In particular, we propose a gossip-based optimization method which achieves the optimal regret bound. The amount of communication required depends on the network topology through the second largest eigenvalue of the transition matrix of a random walk on the network. In the setting of stochastic optimization, the proposed gossip-based approach achieves nearly-linear scaling: the optimization error is guaranteed to be no more than ϵ after O(1 /n ϵ^ 2) rounds, each of which involves O(n) gossip iterations, when nodes communicate over a well-connected graph. This scaling law is also observed in numerical experiments on a cluster. Comment: 30 pages, 2 figure...|$|R
40|$|We {{develop a}} tool to explore the {{behavior}} of parameterized systems (i. e., systems consisting of an arbitrary number of identical processes that <b>synchronize</b> using shared <b>variables</b> or global communications) and to ease user interaction with tools that verify them. The tool includes a user friendly GUI that allows the user to describe a parameterized system and to perform guided, interactive or random simulation. This tool empowers the user to plug in several independent verifiers to perform verification. A mockup verifier is developed {{in order to facilitate}} the development of the tool and testing the required functionalities. The mockup verifier involves parsing descriptions of the parameterized systems to be analyzed. In order to interact with the verifier, the tool is user friendly and flexible {{in the sense that the}} user can plug in a verifier developed in any language as long as it allows to perform a number of basic computations on the parameterized system (such as the set of enabled transitions or the set of successor configurations). In order to plug in a new tool, our tool needs to be able to make use of these operations, for instance using a wrapper written for a verifier particular to a class of parameterized systems. Given these operations, our tool enables the user to carry out various types of simulations like random, interactive or guided simulations. Moreover, our tool can submit verification queries to the underlying verifier and walk the user through the generated counter examples as if it was a simulation session...|$|R
40|$|Experiments such as CHORUS at CERN {{require the}} {{inspection}} {{of a large}} amount of nuclear emulsion plates exposed to particle beams. Rare events need to be found, measured and analyzed. Their features are stored as grains in microscopic dimensions in a 3 D stack of plates. A new, fully automatic immersion microscope system was developed. It features high resolution, small depth of focus, large working distance, large field of view and synchronization of illumination and detector. An additional requirement is given by variations in the refraction index and in the relative thickness of immersion oil and emulsion. The approach used is an imaging system based on a various objective lens with extreme numerical aperture, large working distance and wide field, combined with a matched high-aperture Koehler illuminator. The light source is a mercury arc lamp, combined with a filter package for the g-line. It includes liquid crystal elements for <b>synchronized</b> shuttering and <b>variable</b> attenuation. The theoretical resolution is less than 1 micron in x, y, z within a volume of 0. 5 mm diameter times 1 mm scanning depth in all situations within a predefined index range. Three identical pieces of the system have been built. The experimentally measured resolution confirms the expectations and is better than 1 micron in all three dimensions. This {{is the result of a}} complex process of system design and manufacturing, unifying optical, opto-mechanical and opto- electronical contributions. This process spans from the early stages of feasibility and manufacturing up to the test and adjustment procedures. The three prototypes have been operational since the fall of 1998 in the frame of the CHORUS project. Practical experience and application results are presented. (0 refs) ...|$|R
40|$|Human use of {{the oceans}} is {{increasingly}} in conflict with conservation of endangered species. Methods for managing the spatial and temporal placement of industries such as military, fishing, transportation and offshore energy, have historically been post hoc; i. e. {{the time and place}} of human activity is often already determined before assessment of environmental impacts. In this dissertation, I build robust species distribution models in two case study areas, US Atlantic (Best et al. 2012) and British Columbia (Best et al. 2015), predicting presence and abundance respectively, from scientific surveys. These models are then applied to novel decision frameworks for preemptively suggesting optimal placement of human activities in space and time to minimize ecological impacts: siting for offshore wind energy development, and routing ships to minimize risk of striking whales. Both decision frameworks relate the tradeoff between conservation risk and industry profit with <b>synchronized</b> <b>variable</b> and map views as online spatial decision support systems. For siting offshore wind energy development (OWED) in the U. S. Atlantic (chapter 4), bird density maps are combined across species with weights of OWED sensitivity to collision and displacement and 10 km 2 sites are compared against OWED profitability based on average annual wind speed at 90 m hub heights and distance to transmission grid. A spatial decision support system enables toggling between the map and tradeoff plot views by site. A selected site can be inspected for sensitivity to a cetaceans throughout the year, so as to capture months of the year which minimize episodic impacts of pre-operational activities such as seismic airgun surveying and pile driving. Routing ships to avoid whale strikes (chapter 5) can be similarly viewed as a tradeoff, but is a different problem spatially. A cumulative cost surface is generated from density surface maps and conservation status of cetaceans, before applying as a resistance surface to calculate least-cost routes between start and end locations, i. e. ports and entrance locations to study areas. Varying a multiplier to the cost surface enables calculation of multiple routes with different costs to conservation of cetaceans versus cost to transportation industry, measured as distance. Similar to the siting chapter, a spatial decisions support system enables toggling between the map and tradeoff plot view of proposed routes. The user can also input arbitrary start and end locations to calculate the tradeoff on the fly. Essential to the input of these decision frameworks are distributions of the species. The two preceding chapters comprise species distribution models from two case study areas, U. S. Atlantic (chapter 2) and British Columbia (chapter 3), predicting presence and density, respectively. Although density is preferred to estimate potential biological removal, per Marine Mammal Protection Act requirements in the U. S., all the necessary parameters, especially distance and angle of observation, are less readily available across publicly mined datasets. In the case of predicting cetacean presence in the U. S. Atlantic (chapter 2), I extracted datasets from the online OBIS-SEAMAP geo-database, and integrated scientific surveys conducted by ship (n= 36) and aircraft (n= 16), weighting a Generalized Additive Model by minutes surveyed within space-time grid cells to harmonize effort between the two survey platforms. For each of 16 cetacean species guilds, I predicted the probability of occurrence from static environmental variables (water depth, distance to shore, distance to continental shelf break) and time-varying conditions (monthly sea-surface temperature). To generate maps of presence vs. absence, Receiver Operator Characteristic (ROC) curves were used to define the optimal threshold that minimizes false positive and false negative error rates. I integrated model outputs, including tables (species in guilds, input surveys) and plots (fit of environmental variables, ROC curve), into an online spatial decision support system, allowing for easy navigation of models by taxon, region, season, and data provider. For predicting cetacean density within the inner waters of British Columbia (chapter 3), I calculated density from systematic, line-transect marine mammal surveys over multiple years and seasons (summer 2004, 2005, 2008, and spring/autumn 2007) conducted by Raincoast Conservation Foundation. Abundance estimates were calculated using two different methods: Conventional Distance Sampling (CDS) and Density Surface Modelling (DSM). CDS generates a single density estimate for each stratum, whereas DSM explicitly models spatial variation and offers potential for greater precision by incorporating environmental predictors. Although DSM yields a more relevant product for the purposes of marine spatial planning, CDS has proven to be useful in cases where there are fewer observations available for seasonal and inter-annual comparison, particularly for the scarcely observed elephant seal. Abundance estimates are provided on a stratum-specific basis. Steller sea lions and harbour seals are further differentiated by ‘hauled out’ and ‘in water’. This analysis updates previous estimates (Williams & Thomas 2007) by including additional years of effort, providing greater spatial precision with the DSM method over CDS, novel reporting for spring and autumn seasons (rather than summer alone), and providing new abundance estimates for Steller sea lion and northern elephant seal. In addition to providing a baseline of marine mammal abundance and distribution, against which future changes can be compared, this information offers the opportunity to assess the risks posed to marine mammals by existing and emerging threats, such as fisheries bycatch, ship strikes, and increased oil spill and ocean noise issues associated with increases of container ship and oil tanker traffic in British Columbia’s continental shelf waters. Starting with marine animal observations at specific coordinates and times, I combine these data with environmental data, often satellite derived, to produce seascape predictions generalizable in space and time. These habitat-based models enable prediction of encounter rates and, in the case of density surface models, abundance that can then be applied to management scenarios. Specific human activities, OWED and shipping, are then compared within a tradeoff decision support framework, enabling interchangeable map and tradeoff plot views. These products make complex processes transparent for gaming conservation, industry and stakeholders towards optimal marine spatial management, fundamental to the tenets of marine spatial planning, ecosystem-based management and dynamic ocean management. Dissertatio...|$|R
40|$|As {{technology}} evolves, {{the need}} to use software for critical applications increases. It is then required that this software will always behave correctly. Verification {{is the process of}} formally proving that a program is correct. Model checking is a technique used to perform verification, which has been successful with finite state concurrent programs. In the recent years, there has been progress {{in the area of the}} verification of infinite state concurrent programs. There can be several sources of infiniteness. Relevant to this thesis are recent model checking techniques developed at LiU, that can automatically establish correctness for programs manipulating variables that range over infinite domains, and spawning arbitrary many threads, which can <b>synchronize</b> using shared <b>variables,</b> barriers, semaphores etc. These techniques resulted in the tool PACMAN for the verification of multi-threaded programs. The aim of this thesis is to extract analyzable models from multi-threaded C programs, in order to use them for verifying the program that they describe, by using PACMAN. In addition, we augment the C programming language to allow the possibility of expressing some important concepts of multi-threaded programs, such as non-determinism, atomicity etc, with the use of the traditional C syntax. In a following step, we target PACMAN's input format, in order to verify our extracted models. Such verification engines usually accept as input the description of a multi-threaded program expressed in some modeling language. We, therefore, translate a minimum subset of the C programming language, which has been augmented, to effectively describe a multithreaded program, to PACMAN's input format and then pass the description to the engine. In the context of this thesis, we have successfully defined a set of annotation for the C programming language, in order to assist the description of multi-threaded programs. We have implemented a tool that effectively translates annotated C code into the modeling language of PACMAN. The output of the tool is later passed to the verification engine. As a result, we have contributed to the automation of verifying multi-threaded C programs...|$|R
