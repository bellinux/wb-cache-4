82|184|Public
5000|$|... 1972 - <b>Sequential</b> <b>computer</b> {{drawings}} were introduced; {{began to}} work on fixed structures: the cube ...|$|E
5000|$|Parallel {{scientific}} algorithms can {{be developed}} in an elegant publication language and tested on a <b>sequential</b> <b>computer.</b> When it is established an algorithm works, it can easily be implemented in a parallel implementation language.|$|E
50|$|A <b>sequential</b> <b>computer</b> which {{exploits}} no parallelism {{in either}} the instruction or data streams. Single control unit (CU) fetches single instruction stream (IS) from memory. The CU then generates appropriate control signals to direct single processing element (PE) to operate on single data stream (DS) i.e., one operation at a time.|$|E
40|$|Abstract|This paper {{discusses}} ecient {{solution of}} dense systems of linear equations arising from inte-gral equation formulations. Several preconditioners {{in connection with}} Krylov iterative solvers are exam-ined and compared with LU factorization. Results are shown demonstrating practical aspects and issues we have encountered in implementing iterative solvers on both parallel and <b>sequential</b> <b>computers.</b> I...|$|R
40|$|An {{explicit}} CFD {{computer code}} with parallel processing is {{developed for the}} purpose of simulating internal high-speed reacting flows. The code solves the 3 D Navier-Stokes equations for compressible flows. Preliminary results indicate that parallel processing should solve the resource and speed shortages presently encountered with <b>sequential</b> <b>computers</b> for problems of interest...|$|R
40|$|Computer {{hardware}} {{manufacturers have}} moved decisively to multi-core and are currently experimenting with increasingly advanced many-core architectures. In the long term, writing efficient, portable and correct parallel programs targeting multi- and many-core architectures must become no more challenging than writing the same programs for <b>sequential</b> <b>computers.</b> To date, however, most applications running on multicor...|$|R
50|$|Similarly, {{the class}} L {{contains}} all {{problems that can}} be solved by a <b>sequential</b> <b>computer</b> in logarithmic space. Such machines run in polynomial time because {{they can have a}} polynomial number of configurations. It is suspected that L ≠ P; that is, that some problems that can be solved in polynomial time also require more than logarithmic space.|$|E
50|$|On a <b>sequential</b> <b>computer,</b> it is {{straightforward}} to compute all nearest smaller values using a stack data structure: one processes the values in sequence order, using the stack {{to maintain a}} subsequence of the values that have been processed so far and are smaller than any later value {{that has already been}} processed. In pseudocode, the algorithm is as follows.|$|E
5000|$|The class P, {{typically}} {{taken to}} consist of all the [...] "tractable" [...] problems for a <b>sequential</b> <b>computer,</b> contains the class NC, which consists of those problems which can be efficiently solved on a parallel computer. This is because parallel computers can be simulated on a sequential machine. It is not known whether NC = P. In other words, {{it is not known}} whether there are any tractable problems that are inherently sequential. Just as it is widely suspected that P does not equal NP, so it is widely suspected that NC does not equal P.|$|E
40|$|The {{purpose of}} a {{parallel}} computer is to execute a program faster than on <b>sequential</b> <b>computers.</b> How can performance be characterised? There are three classes of quantitative indices for evaluating the performance of computer systems: most probably {{we are interested in}} response time, i. e. how fast a program can be executed (responsiveness) but i...|$|R
5|$|Michael J. Flynn created {{one of the}} {{earliest}} classification systems for parallel (and <b>sequential)</b> <b>computers</b> and programs, now known as Flynn's taxonomy. Flynn classified programs and computers by whether they were operating using a single set or multiple sets of instructions, and whether or not those instructions were using a single set or multiple sets of data.|$|R
40|$|This paper {{presents}} {{a family of}} parallel thinning algorithms for extracting medial surfaces from 3 D binary pictures. The proposed algorithms are based on sufficient conditions for 3 D parallel reduction operators to preserve topology for (26, 6) pictures. Hence it is self-evident that our algorithms are topology preserving. Their efficient implementation on conventional <b>sequential</b> <b>computers</b> is also presented...|$|R
5000|$|In {{parallel}} algorithms, {{the list}} ranking problem involves determining the position, or rank, of each item in a linked list. That is, {{the first item}} in the list should be assigned the number 1, the second item in the list should be assigned the number 2, etc. Although it is straightforward {{to solve this problem}} efficiently on a <b>sequential</b> <b>computer,</b> by traversing the list in order, it is more complicated to solve in parallel. As [...] wrote, the problem was viewed as important in the parallel algorithms community both for its many applications and because solving it led to many important ideas that could be applied in parallel algorithms more generally.|$|E
50|$|The {{most basic}} P-complete problem is this: given a Turing machine, an input for that machine, {{and a number}} T (written in unary), does that machine halt on that input within the first T steps? It is clear that this problem is P-complete: if we can {{parallelize}} a general simulation of a <b>sequential</b> <b>computer,</b> then {{we will be able}} to parallelize any program that runs on that computer. If this problem is in NC, then so is every other problem in P. If the number of steps is written in binary, the problem is EXPTIME-complete.This problem illustrates a common trick in the theory of P-completeness. We arent really interested in whether a problem can be solved quickly on a parallel machine. Were just interested in whether a parallel machine solves it much more quickly than a sequential machine. Therefore, we have to reword the problem so that the sequential version is in P. That is why this problem required T to be written in unary. If a number T is written as a binary number (a string of n ones and zeros, where n = log T), then the obvious sequential algorithm can take time 2n. On the other hand, if T is written as a unary number (a string of n ones, where n = T), then it only takes time n. By writing T in unary rather than binary, we have reduced the obvious sequential algorithm from exponential time to linear time. That puts the sequential problem in P. Then, it will be in NC if and only if it is parallelizable.|$|E
40|$|This paper {{presents}} an approximate string matching algorithm on the 1023 -processor parallel computer DADO 2. To allow proximity in matching between {{the text and}} search pattern, the dynamic programming method is used as the matching algorithm. This paper includes timing measurements and comparison with a conventional <b>sequential</b> <b>computer</b> (VAX). The results show significant speedup over the <b>sequential</b> <b>computer...</b>|$|E
50|$|Michael J. Flynn created {{one of the}} {{earliest}} classification systems for parallel (and <b>sequential)</b> <b>computers</b> and programs, now known as Flynn's taxonomy. Flynn classified programs and computers by whether they were operating using a single set or multiple sets of instructions, and whether or not those instructions were using a single set or multiple sets of data.|$|R
40|$|The {{subject of}} this chapter is the design and {{analysis}} of parallel algorithms. Most of today’s algorithms are sequential, that is, they specify a sequence of steps in which each step consists of a single operation. These algorithms are well suited to today’s computers, which basically perform operations in a sequential fashion. Although {{the speed at which}} <b>sequential</b> <b>computers</b> operate ha...|$|R
40|$|Adaptive {{multilevel}} {{methods are}} methods for solving partial differential equations that combine adaptive grid refinement with multigrid solution techniques. These {{methods have been}} shown to be very effective on <b>sequential</b> <b>computers.</b> Recently, a technique for parallelizing these methods for cluster computers has been developed. This paper presents an overview of a particular adaptive multilevel method and the parallelization of that method via the full domain partition. ...|$|R
40|$|This {{research}} investigated {{an automated}} approach to re-writing traditional <b>sequential</b> <b>computer</b> programs into parallel programs for networked computers. A tool {{was designed and}} developed for generating parallel programs automatically and also executing these parallel programs on a network of computers. Performance is maximized by utilising all idle resources...|$|E
40|$|International audienceSome arithmetics for {{numerical}} validation {{have been}} developed on <b>sequential</b> <b>computer</b> architectures: interval arithmetic, discrete stochastic arithmetic or multi-precision arithmetic. Today, a lot of powerful computations are performed on vector computer architectures. We present here the first vector version of the CADNA software based on discrete stochastic arithmetic which combines powerful and reliable computations...|$|E
40|$|Measurements to aid user programmers {{in writing}} {{efficient}} programs for a single-instruction-stream, multiple-data-stream (SIMD) computer such as ILLIAC IV, PEPE, or MAP are described. Applications {{of some of}} these measurements to determine when a program could effectively be implemented on a conventional <b>sequential</b> <b>computer</b> are also discussed. An archetype hardware monitor to obtain the desired measurements is also outlined...|$|E
40|$|In {{this paper}} we {{introduce}} a new procedure for stack filter design with respect to MAE optimality criterion. While the procedure is only suboptimal, it differentiates from the existing methods by being extremely fast on <b>sequential</b> <b>computers.</b> Up to window length 19 this {{is by far the}} most efficient method, being an order of magnitude faster, while providing a MAE with only 0 : 1 % greater than other existing methods...|$|R
40|$|The greedy {{algorithm}} {{is a standard}} paradigm for solving matroid optimization problems on <b>sequential</b> <b>computers.</b> This paper presents a {{greedy algorithm}} suitable for a fully-pipelined linear array of processors, a generalization of Huang's algorithm [Hua 90] for minimum spanning trees. Application of the algorithm to uniprocessor scheduling with release times and deadlines is discussed in detail. A key feature of the algorithm is its use of matroid contraction...|$|R
40|$|It is an {{increasingly}} common {{belief that the}} programmability of parallel machines is lacking, and that the high-end computing (HEC) community is suffering {{as a result of}} it. The population of users who can effectively program parallel machines comprises {{only a small fraction of}} those who can effectively program traditional <b>sequential</b> <b>computers,</b> and this gap seems only to be widening as time passes. The parallel computing community’s inability to tap the skill...|$|R
40|$|Although {{available}} (<b>sequential)</b> <b>computer</b> hardware is {{very powerful}} nowadays, {{the implementation of}} artificial neural networks on massively parallel hardware is still undoubtedly of high interest, not only under an academic point of view. This paper presents an implementation of multi-dimensional Self-Organizing Maps on a scalable SIMD structure of a CNAPS computer with up to 512 parallel processors. ...|$|E
40|$|We {{present a}} {{comparison}} between DEVS, Cell-DEVS and a <b>sequential</b> <b>computer</b> simulation of a semi-physical fire spread model. Forest fire is a complex phenomenon, which requires analyzing model evolution and requires high computing capabilities. Hence, we discuss about different environment language implementations (C, Java and C++) for time improvement looking at trade-off between model evolutivity and simulation time provided by the different approaches. Finally, we show how Cell-DEVS allows developing safe and cost-effective simula-tions, reducing significantly the development times for cellular models. ...|$|E
40|$|Use of the Caltech/JPL {{hypercube}} multicomputer {{to solve}} problems in chemical dynamics {{is the subject of}} this paper. The specific application is quantum mechanical atom diatomic molecule reactive scattering. One methodology for solving this dynamics problem on a <b>sequential</b> <b>computer</b> is based on symmetrized hyperspherical coordinates. We will discuss our strategy for implementing the hyperspherical coordinate methodology on the hypercube. In particular, the performance of a parallel integrator for the special system of ordinary differential equations which arises in this application is discussed...|$|E
25|$|Spanning {{trees are}} {{important}} in parallel and distributed computing, {{as a way of}} maintaining communications between a set of processors; see for instance the Spanning Tree Protocol used by OSI link layer devices or the Shout (protocol) for distributed computing. However, the depth-first and breadth-first methods for constructing spanning trees on <b>sequential</b> <b>computers</b> are not well suited for parallel and distributed computers. Instead, researchers have devised several more specialized algorithms for finding spanning trees in these models of computation.|$|R
40|$|AbstractThe {{thinning}} is an iterative layer by layer erosion {{until only}} the “skeletons” {{of the objects}} are left. This paper presents a thinning algorithm for extracting medial surfaces from 3 D binary pictures. The strategy which is used is called fully parallel, {{which means that the}} same parallel operator is applied at each iteration. An efficient implementation of the proposed algorithm on conventional <b>sequential</b> <b>computers</b> is given and the topological correctness for (26, 6) binary pictures is proved...|$|R
50|$|Spanning {{trees are}} {{important}} in parallel and distributed computing, {{as a way of}} maintaining communications between a set of processors; see for instance the Spanning Tree Protocol used by OSI link layer devices or the Shout (protocol) for distributed computing. However, the depth-first and breadth-first methods for constructing spanning trees on <b>sequential</b> <b>computers</b> are not well suited for parallel and distributed computers. Instead, researchers have devised several more specialized algorithms for finding spanning trees in these models of computation.|$|R
40|$|We {{explore the}} {{adaptation}} of a ranking and selection procedure, originally designed for a <b>sequential</b> <b>computer,</b> to a high-performance (parallel) computing setting. We {{pay particular attention to}} screening and explaining why care is required in implementing screening in parallel settings. We develop an algorithm that allows screening at both the master and worker levels, and that apportions work to processors {{in such a way that}} excessive communication is avoided. In doing so we rely on a random number generator with many streams and substreams. 1...|$|E
40|$|Abstract. The B {{method is}} a well known {{approach}} to the formal specification and development of <b>sequential</b> <b>computer</b> programs. Inspired by action systems, the B method has evolved to incorporate system modelling and distributed system development. This extension is called Event-B. Even though several of the structuring mechanisms of the original B method are absent from Event-B, the desire to define and maintain structured data persists. We propose the introduction of records to Event-B for this purpose. Our approach upholds the refinement principles of Event-B by allowing the stepwise development of records too. ...|$|E
40|$|In {{this paper}} we briefly review {{some of the}} {{existing}} domain-decomposition coupling of the finite and boundary element methods. We also summarize a general interface relaxation framework, originally developed for the solution of composite PDEs and extend two interface relaxation algorithms to higher dimensional analysis. We further present a new interface relaxation finite element/boundary element coupling algorithm, which may be implemented on a distributed parallel or <b>sequential</b> <b>computer.</b> The method overcomes some {{of the limitations of}} the existing domain decomposition coupling methods. We also investigate the convergence of the method...|$|E
40|$|AbstractThe use of {{implicit}} {{methods for}} ODEs, e. g. implicit Runge-Kutta schemes, requires {{the solution of}} nonlinear systems of algebraic equations of dimension s · m, where m {{is the size of}} the continuous differential problem to be approximated. Usually, the solution of this system represents the most time-consuming section in the implementation of such methods. Consequently, the efficient solution of this section would improve their performance. In this paper, we propose a new iterative procedure to solve such equations on <b>sequential</b> <b>computers...</b>|$|R
40|$|This paper {{summarizes}} {{our progress}} {{and experience in}} the development of a Computational-Fluid-Dynamics code on parallel computers to simulate three-dimensional spatially-developing mixing layers. In this initial study, the three-dimensional time-dependent Euler equations are solved using a finite-volume explicit time-marching algorithm. The code was first programmed in Fortran 77 for <b>sequential</b> <b>computers.</b> The code was then converted for use on parallel computers using the conventional message-passing technique, while we have not been able to compile the code with the present version of HPF compilers...|$|R
40|$|The use of {{implicit}} {{methods for}} ODEs, e. g. implicit Runge-Kutta schemes, requires {{the solution of}} nonlinear systems of algebraic equations of dimension s • m, where m {{is the size of}} the continuous differential problem to be approximated. Usually, the solution of this system represents he most time-consuming section in the implementation f such methods. Consequently, the efficient solution of this section would improve their performance. In this paper, we propose a new iterative procedure to solve such equations on <b>sequential</b> <b>computers...</b>|$|R
