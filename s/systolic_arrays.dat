347|529|Public
5|$|The single-instruction-single-data (SISD) {{classification}} {{is equivalent}} to an entirely sequential program. The single-instruction-multiple-data (SIMD) classification is analogous to doing the same operation repeatedly over a large data set. This is commonly done in signal processing applications. Multiple-instruction-single-data (MISD) is a rarely used classification. While computer architectures {{to deal with this}} were devised (such as <b>systolic</b> <b>arrays),</b> few applications that fit this class materialized. Multiple-instruction-multiple-data (MIMD) programs are by far {{the most common type of}} parallel programs.|$|E
2500|$|Colossus {{included}} {{the first ever}} use of shift registers and <b>systolic</b> <b>arrays,</b> enabling five simultaneous tests, each involving up to 100 Boolean calculations, {{on each of the}} five channels on the punched tape (although in normal operation only one or two channels were examined in any run). Initially Colossus was only used to determine the initial wheel positions used for a particular message (termed wheel setting). [...] The Mark 2 included mechanisms intended to help determine pin patterns (wheel breaking). Both models were programmable using switches and plug panels in a way their predecessors had not been.|$|E
50|$|<b>Systolic</b> <b>arrays</b> (< {{wavefront}} processors), {{were first}} described by H. T. Kung and Charles E. Leiserson, who published the first paper describing <b>systolic</b> <b>arrays</b> in 1978. However, the first machine {{known to have}} used a similar technique was the Colossus Mark II in 1944.|$|E
40|$|We {{propose a}} <b>systolic</b> <b>array</b> for dynamic {{programming}} {{which is a}} technique for solving combinatorial optimization problems. We derive a <b>systolic</b> <b>array</b> for single-source shortest path problem, SA-SSSP, and then show that the <b>systolic</b> <b>array</b> serves as dynamic programming <b>systolic</b> <b>array</b> which is applicable to any dynamic programming problem by developing a <b>systolic</b> <b>array</b> for 0 - 1 knapsack problem, SA- 01 KS, with SA-SSSP for a basis...|$|R
40|$|Abstract- High-performance {{computation}} {{on a large}} {{array of}} cells {{has been an important}} feature of <b>systolic</b> <b>array.</b> To achieve even higher degree of concurrency, it is desirable to make cells of <b>systolic</b> <b>array</b> themselves <b>systolic</b> <b>array</b> as well. The architecture of <b>systolic</b> <b>array</b> with its cells consisting of another <b>systolic</b> <b>array</b> is to be called super-systolic array. In this paper we propose a scalable super-systolic array architecture which shows high-performance and can be adopted in the VLSI design including regular interconnection and functional primitives that are typical for a systolic architecture. I...|$|R
40|$|Parallel {{computing}} is {{a method}} of computation in which many calculations are carried out concurrently, operating {{on the principle that}} large problems can often be separated into smaller ones, which are then solved simultaneously. <b>Systolic</b> <b>array</b> is a specialized form of parallel computing which is an arrangement of processors in an array where data flows synchronously across the array between neighbours, usually with different data flowing in different directions. The simulation of <b>systolic</b> <b>array</b> for matrix multiplication is the practical application in order to evaluate the performance of <b>systolic</b> <b>array.</b> In this paper, a two-dimensional orthogonal <b>systolic</b> <b>array</b> for matrix multiplication is presented. Perl scripting language is used to simulate a two-dimensional orthogonal <b>systolic</b> <b>array</b> compared to conventional matrix multiplication in terms of average execution time. The comparison is made using matrices of size 5 xM versus Mx 5 which M ranges from 1 to 10, 10 to 100 and 100 to 1000. The orthogonal <b>systolic</b> <b>array</b> results show better average execution time when M is more than 30 compared to conventional matrix multiplication when the size of the matrix multiplication is increased...|$|R
5000|$|MISD - Multiple Instruction Single Data, Example: <b>Systolic</b> <b>Arrays</b> ...|$|E
5000|$|<b>Systolic</b> <b>arrays</b> are {{arrays of}} DPUs which are {{connected}} to {{a small number of}} nearest neighbour DPUs in a mesh-like topology. DPUs perform a sequence of operations on data that flows between them. Because the traditional systolic array synthesis methods have been practiced by algebraic algorithms, only uniform arrays with only linear pipes can be obtained, so that the architectures are the same in all DPUs. The consequence is, that only applications with regular data dependencies can be implemented on classical <b>systolic</b> <b>arrays.</b> Like SIMD machines, clocked <b>systolic</b> <b>arrays</b> compute in [...] "lock-step" [...] with each processor undertaking alternate compute | communicatephases. But <b>systolic</b> <b>arrays</b> with asynchronous handshake between DPUs are called wavefront arrays.One well-known systolic array is Carnegie Mellon University's iWarp processor, which has been manufactured by Intel. An iWarp system has a linear array processor connected by data buses going in both directions.|$|E
50|$|Versions {{exist for}} the Cray X-MP, Y-MP, 2; Sequent, Encore Alliant, DEC VAX-11/784, {{dataflow}} architectures, KSR1, Transputers and <b>systolic</b> <b>arrays.</b>|$|E
40|$|This brief {{presents}} a high-performance curve cryptographic processor for generalnew unified <b>systolic</b> <b>array</b> that efficiently implements addition,el, the control dependencies {{in the operation}} sequencestorage would stall the pipeline in the <b>systolic</b> <b>array.</b> These pipelineSynthesized in 0. 13 - µm standard-cell technology, the processorGF(p) systolic arithmetic unit. We propose a newdivision over GF(p). At the system level, between the <b>systolic</b> <b>array</b> and the separate storageby using two optimization methods. Synthesized inscalar multiplication for general curves over GF...|$|R
40|$|In this paper, we {{show that}} every <b>systolic</b> <b>array</b> executes a Regular Iterative Algorithm with a {{strongly}} separating hyperplane and conversely, that every such algorithm can be implemented on a <b>systolic</b> <b>array.</b> This characterization provides us with an unified framework for describing the contributions of other authors. It also exposes the relevance of many fundamental concepts that were introduced in the sixties by Hennie, Waite and Karp, Miller and Winograd, {{to the present day}} concern of <b>systolic</b> <b>array</b> design. 1...|$|R
40|$|In {{this paper}} a {{bidirectional}} linear <b>systolic</b> <b>array</b> (BLSA) that computes minimum cost spanning tree (MCST) {{of a given}} graph is designed. We present a synthesis procedure, based on data dependencies and space-time transformations of index space, to design BLSA with optimal number of processing elements (PEs) for a given problem size. The execution time is minimized for that number of PEs. Explicit mathematical formulas for <b>systolic</b> <b>array</b> synthesis are derived. We compare performances of the BLSA with the unidirectional linear <b>systolic</b> <b>array</b> (ULSA). ...|$|R
50|$|In {{parallel}} computer architectures, a systolic array is a homogeneous network of tightly coupled data processing units (DPUs) called cells or nodes. Each node or DPU independently computes a partial result {{as a function}} of the data received from its upstream neighbors, stores the result within itself and passes it downstream. <b>Systolic</b> <b>arrays</b> were invented by Richard P. Brent and H. T. Kung, who developed them to compute greatest common divisors of integers and polynomials. They are sometimes classified as multiple-instruction single-data (MISD) architectures under Flynn's taxonomy, but this classification is questionable because a strong argument can be made to distinguish <b>systolic</b> <b>arrays</b> from any of Flynn's four categories: SISD, SIMD, MISD, MIMD, as discussed later in this article.|$|E
5000|$|<b>Systolic</b> <b>arrays</b> {{are often}} {{hard-wired}} for specific operations, such as [...] "multiply and accumulate", to perform massively parallel integration, convolution, correlation, matrix multiplication or data sorting tasks. They {{are also used}} for dynamic programming algorithms, used in DNA and protein sequence analysis.|$|E
50|$|<b>Systolic</b> <b>arrays</b> are {{therefore}} extremely good at artificial intelligence, image processing, pattern recognition, computer vision and other tasks which animal brains do so particularly well. Wavefront processors in general {{can also be}} very good at machine learning by implementing self configuring neural nets in hardware.|$|E
3000|$|When {{the data}} of the first pixel have been written in the Write FIFO, the <b>systolic</b> <b>array</b> and the random {{generation}} module start working. Every clock cycle, a new pixel is read by the control unit {{and sent to the}} <b>systolic</b> <b>array.</b> In parallel, the [...]...|$|R
40|$|Abstract—A <b>systolic</b> <b>array</b> {{provides}} an alternative comput-ing paradigm to the von Neuman architecture. Though its hardware implementation has failed as a paradigm to design integrated circuits in the past, {{we are now}} discovering that the <b>systolic</b> <b>array</b> as a software virtualization layer can lead to an extremely scalable execution paradigm. To demonstrate this scalability, in this paper, we design and implement a 3 D virtual <b>systolic</b> <b>array</b> to compute a tile QR decomposition of a tall-and-skinny dense matrix. Our implementation {{is based on a}} state-of-the-art algorithm that factorizes a panel based on a tree-reduction. Using a runtime developed {{as a part of the}} Parallel Ultra Light <b>Systolic</b> <b>Array</b> Runtime (PULSAR) project, we demonstrate on a Cray-XT 5 machine how our virtual <b>systolic</b> <b>array</b> can be mapped to a large-scale machine and obtain excellent parallel performance. This is an important contribution since such a QR decomposition is used, for example, to compute a least squares solution of an overdetermined system, which arises in many scientific and engineering problems. Keywords-systolic array; QR decomposition; multithreading, message-passing, dataflow; runtime; I...|$|R
50|$|The <b>systolic</b> <b>array</b> {{paradigm}} with data-streams {{driven by}} data counters, is the counterpart of the Von Neumann architecture with instruction-stream {{driven by a}} program counter. Because a <b>systolic</b> <b>array</b> usually sends and receives multiple data streams, and multiple data counters are needed to generate these data streams, it supports data parallelism.|$|R
50|$|All of {{the above}} not withstanding, <b>systolic</b> <b>arrays</b> are often offered as {{a classic example of}} MISD {{architecture}} in textbooks on parallel computing and in the engineering class. If the array is viewed from the outside as atomic it should perhaps be classified as SFMuDMeR = Single Function, Multiple Data, Merged Result(s).|$|E
50|$|<b>Systolic</b> <b>arrays</b> (< {{wavefront}} processors), {{first described}} by H. T. Kung and Charles E. Leiserson are {{an example of}} MISD architecture. In a typical systolic array, parallel input data flows through a network of hard-wired processor nodes, resembling the human brain which combine, process, merge or sort the input data into a derived result.|$|E
5000|$|<b>Systolic</b> <b>arrays,</b> {{proposed}} {{during the}} 1980s are multiprocessors in which data and partial results are rhythmically pumped from processor to processor through a regular, local interconnection network. [...] Systolic architectures use a global clock and explicit timing delays to synchronize data flow from processor to processor. [...] Each processor in a systolic system executes an invariant {{sequence of instructions}} before data and results are pulsed to neighboring processors.|$|E
40|$|The QRD RLS {{algorithm}} {{is one of}} the most promising RLS algorithms, due to its robust numerical stability and suitability for VLSI implementation based on a <b>systolic</b> <b>array</b> architecture. Up to now, among many techniques to implement the QR decomposition, only the Given rotation and modified Gram-Schmidt methods have been successfully applied to the development of the QRD RLS <b>systolic</b> <b>array.</b> It is well-known that Householder transformation (HT) outperforms the Givens rotation method under finite precision computations. Presently, there is no know technique to implement the HT on a <b>systolic</b> <b>array</b> architecture. In this paper, we propose a Systolic Block Householder Transformation (SBHT) approach, to implement the HT on a <b>systolic</b> <b>array</b> as well as its application to the RLS algorithm. Since the data is fetched in a block manner, vector operations are in general required for the vectorized array. However, by using a modified HT algorithm, a two-level pipelined implementation can be used to pipeline the SBHT <b>systolic</b> <b>array</b> both at the vector and word level. The throughput rate can be as fast as that of the Givens rotation method. Our approach makes the HT amenable for VLSI implementation as well as applicable to real-time high throughput applications of modern signal processing. The constrained RLS problem using the SBHT RLS <b>systolic</b> <b>array</b> is also considered in this paper...|$|R
50|$|Cisco PXF network {{processor}} is internally {{organized as}} <b>systolic</b> <b>array.</b>|$|R
40|$|AbstractOne of {{the main}} {{operations}} for the public key cryptosystem is the modular exponentiation. In this paper, we analyze the Montgomery's algorithm and design a linear <b>systolic</b> <b>array</b> for performing both modular multiplication and modular squaring simultaneously. The proposed <b>systolic</b> <b>array</b> with less hardware complexity can be designed by making use of common-multiplicand multiplication in the right-to-left modular exponentiation over GF(2 m). For the fast computation of the modular exponentiation, the proposed <b>systolic</b> <b>array</b> has 1. 25 times improvement in area-time complexity when compared to existing multipliers. The proposed <b>systolic</b> <b>array</b> suffers a little loss in time complexity, but it has 1. 44 times improvement in area complexity since it executes the common parts {{that exist in the}} simultaneous computation of both modular multiplication and squaring only once. It could be designed on VLSI hardware and used in IC cards...|$|R
50|$|A {{major benefit}} of <b>systolic</b> <b>arrays</b> {{is that all}} operand data and partial results are {{contained}} within (passing through) the processor array. There {{is no need to}} access external buses, main memory or internal caches during each operation {{as is the case with}} standard sequential machines. The sequential limits on parallel performance dictated by Amdahl's theorem also do not apply in the same way, because data dependencies are implicitly handled by the programmable node interconnect.|$|E
50|$|In 1991-97, Siemens {{developed}} the MA-16 chip, SYNAPSE-1 and SYNAPSE-3 Neurocomputer. The MA-16 is a fast matrix-matrix multiplier {{that can be}} combined to form <b>systolic</b> <b>arrays.</b> It can process 4 patterns of 16 elements each (16-bit), with 16 neuron values (16-bit) {{at a rate of}} 800 MMAC or 400 MCPS at 50 MHz. The SYNAPSE3-PC PCI card contains 2 MA-16 with a peak performance of 2560 MOPS (1.28 GMAC); 7160 MOPS (3.58 GMAC) when using three boards.|$|E
50|$|The single-instruction-single-data (SISD) {{classification}} {{is equivalent}} to an entirely sequential program. The single-instruction-multiple-data (SIMD) classification is analogous to doing the same operation repeatedly over a large data set. This is commonly done in signal processing applications. Multiple-instruction-single-data (MISD) is a rarely used classification. While computer architectures {{to deal with this}} were devised (such as <b>systolic</b> <b>arrays),</b> few applications that fit this class materialized. Multiple-instruction-multiple-data (MIMD) programs are by far {{the most common type of}} parallel programs.|$|E
40|$|A <b>systolic</b> <b>array</b> for {{recursive}} {{least squares}} estimation by inverse updates is derived {{by means of}} algorithmic engineering. The derivation of this <b>systolic</b> <b>array</b> is highly nontrivial due {{to the presence of}} data contra-flow and feedback loops in the underlying signal flow graph. This would normally prohibit pipelined processing. However, it is shown that suitable delays may be introduced into the signal how graph by performing a simple algorithmic transformation which compensates for the interference of crossing data flows. The pipelined <b>systolic</b> <b>array</b> is then obtained by retiming the signal flow graph and applying the cut theorem. status: publishe...|$|R
40|$|A fully-pipelined <b>systolic</b> <b>array</b> for {{computing}} {{the minimum}} variance distortionless response (MVDR) was first proposed by McWhirter and Shepherd. The fundamental concept is {{to fit the}} MVDR beamforming to the non-contrainted recursive least-squares (RLS) minimization. Until now, their <b>systolic</b> <b>array</b> processor is well-recognized as the most efficient design for MVDR beamforming. In this paper, we first point out the mistake by relating the MVRD beamforming and RLS minimization and then propose a new algorithm for the MVDR beamforming. Moreover, a fully parallel and pipelined <b>systolic</b> <b>array</b> for the newly proposed algorithm is presented and the square-root free implementation is also considered...|$|R
40|$|In {{this paper}} NASH 1 algorithm, a new matrix-based method for {{identification}} of radar pulse train, is implemented by <b>systolic</b> <b>array.</b> NASH {{can be used}} to identify the PRI for constant, staggered, and jittered signal. Previous matrix-based methods can only identify the first type of signal, i. e. constant PRI. The complexity of the computation in NASH is more than the previous matrix-based method. To overcome this drawback a <b>systolic</b> <b>array,</b> the best parallel structure for the matrix operation, is designed. The <b>systolic</b> <b>array</b> helps to parallelize the matrix inversion, which is the most time consuming part of NASH algorithm...|$|R
50|$|A {{systolic}} array typically {{consists of}} a large monolithic network of primitive computing nodes which can be hardwired or software configured for a specific application. The nodes are usually fixed and identical, while the interconnect is programmable. The more general wavefront processors, by contrast, employ sophisticated and individually programmable nodes {{which may or may}} not be monolithic, depending on the array size and design parameters. The other distinction is that <b>systolic</b> <b>arrays</b> rely on synchronous data transfers, while wavefront tend to work ly.|$|E
50|$|While <b>systolic</b> <b>arrays</b> are officially {{classified}} as MISD, their classification is somewhat problematic. Because the input is typically a vectorof independent values, the systolic array {{is definitely not}} SISD. Since these input values are merged and combined into the result(s) and do not maintain their independence as they would in a SIMD vector processing unit, the array cannot be {{classified as}} such. Consequently, the array cannot be classified as a MIMD either, because MIMD {{can be viewed as}} a mere collection of smaller SISD and SIMD machines.|$|E
50|$|A {{major benefit}} of <b>systolic</b> <b>arrays</b> {{is that all}} operand data and partial results are stored within (passing through) the {{processor}} array. There {{is no need to}} access external buses, main memory or internal caches during each operation {{as is the case with}} Von Neumann or Harvard sequential machines. The sequential limits on parallel performance dictated by Amdahl's Law also do not apply in the same way, because data dependencies are implicitly handled by the programmable node interconnect and there are no sequential steps in managing the highly parallel data flow.|$|E
40|$|A {{formal proof}} is {{presented}} for a recently presented <b>systolic</b> <b>array</b> for recursive least squares estimation by inverse updates. The derivation of this <b>systolic</b> <b>array</b> is highly non-trivial {{due to the}} presence of data contra-flow and feedback loops in the underlying signal flow graph. This would normally prohibit pipelined processing. However, it is shown that suitable delays may be introduced into the signal flow graph by performing a simple algorithmic transformation which compensates for the interference of crossing data flows. The pipelined <b>systolic</b> <b>array</b> is then obtained by retiming the signal flow graph and applying the cut theorem. I. Introduction In this paper we derive a novel <b>systolic</b> <b>array</b> for implementing recursive least squares (RLS) computations based on the method of inverse updates. Recursive least squares estimation is required {{in a wide range of}} applications from adaptive beamforming for antenna arrays to data communications, space navigation and system identification. [...] ...|$|R
40|$|A <b>systolic</b> <b>array</b> to {{implement}} lattice-reduction-aided linear detection is proposed for a MIMO receiver. The lattice reduction algorithm {{and the ensuing}} linear detections are operated in the same array, which can be hardware-efficient. All-swap lattice reduction algorithm (ASLR) is considered for the systolic design. ASLR is {{a variant of the}} LLL algorithm, which processes all lattice basis vectors within one iteration. Lattice-reduction-aided linear detection based on ASLR and LLL algorithms have very similar bit-error-rate performance, while ASLR is more time efficient in the <b>systolic</b> <b>array,</b> especially for systems with a large number of antennas. Index Terms — MIMO receivers, <b>systolic</b> <b>array,</b> lattice reduction, wireless communication...|$|R
40|$|Abstract—In this paper, {{we present}} a {{performance}} comparison between linear recursive variable expansion (RVE) and linear <b>systolic</b> <b>array</b> implementations of the Smith‐Waterman (S‐ W) algorithm. The results demonstrate that the temporal performance of linear RVE implementation is 2. 11 to 3 times better than the traditional linear <b>systolic</b> <b>array</b> implementation at the spatial cost of 2. 02 to 2. 54...|$|R
