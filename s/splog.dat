20|15|Public
25|$|By genre: Some blogs {{focus on}} a {{particular}} subject, such as political blogs, journalism blogs, health blogs, travel blogs (also known as travelogs), gardening blogs, house blogs, book blogs, fashion blogs, beauty blogs, lifestyle blogs, party blogs, wedding blogs, photography blogs, project blogs, psychology blogs, sociology blogs, education blogs, niche blogs, classical music blogs, quizzing blogs, legal blogs (often {{referred to as a}} blawgs), or dreamlogs. How-to/Tutorial blogs are becoming increasing popular. Two common types of genre blogs are art blogs and music blogs. A blog featuring discussions especially about home and family is not uncommonly called a mom blog and one made popular is by Erica Diamond who created Womenonthefence.com which is syndicated to over two million readers monthly. While not a legitimate type of blog, one used {{for the sole purpose of}} spamming is known as a <b>splog.</b>|$|E
50|$|The {{purpose of}} a <b>splog</b> can be to {{increase}} the PageRank or backlink portfolio of affiliate websites, to artificially inflate paid ad impressions from visitors (see made for AdSense or MFA-blogs), and/or use the blog as a link outlet to sell links or get new sites indexed. Spam blogs are usually a type of scraper site, where content is often either inauthentic text or merely stolen (see blog scraping) from other websites. These blogs usually contain {{a high number of}} links to sites associated with the <b>splog</b> creator which are often disreputable or otherwise useless websites.|$|E
5000|$|Niche {{blogging}} and splogging {{are often}} hard to distinguish. However, niche blogging's reliance on pay-per-click advertising and other revenue streams usually requires such blogs to have valuable content {{related to their}} chosen niche, unlike a <b>splog.</b>|$|E
40|$|This paper {{focuses on}} {{analyzing}} (Japanese) <b>splogs</b> based on various characteristics of keywords contained in them. We estimate {{the behavior of}} spammers when creating <b>splogs</b> from other sources by analyzing the characteristics of keywords contained in <b>splogs.</b> Since <b>splogs</b> often cause noises in word occurrence statistics in the blogosphere, we assume that we can efficiently (manually) collect <b>splogs</b> by sampling blog homepages containing keywords of a certain type on the date with its most frequent occurrence. We manually examine various features of collected blog homepages regarding whether their text content is excerpt from other sources or not, as well as whether they display affiliate advertisement or out-going links to affiliated sites. Among various informative results, {{it is important to}} note that more than half of the collected <b>splogs</b> are created by a very small number of spammers...|$|R
40|$|Weblogs or blogs collectively {{constitute}} the Blogosphere, forming an influential and interesting subset on the Web. As with most Internet-enabled applications, {{the ease of}} content creation and distribution makes the blogosphere spam prone. Spam blogs or <b>splogs</b> are blogs hosting spam posts, created using machine generated or hijacked content {{for the sole purpose}} of hosting ads or raising the PageRank of target sites. These <b>splogs</b> make up the splogosphere, and are now inundating blog search engines and update ping servers. In this work we characterize <b>splogs</b> by comparing them against authentic blogs. Our analysis is based on a dataset made publicly available by BlogPulse, and employs a machine learning model that detects <b>splogs</b> with an accuracy of 90 %. To round o# this analysis and to better understand <b>splogs,</b> we also present our study of a popular blog update ping server, and show how they are overwhelmed by pings sent by <b>splogs.</b> This overall study will facilitate finding e#ective new techniques to detect and weed out <b>splogs</b> from the blogosphere...|$|R
40|$|Spam blogs or <b>Splogs</b> are blogs {{with either}} auto-generated or plagiarized content {{created for the}} sole purpose of hosting ads, {{promoting}} affiliate sites and getting new pages indexed. <b>Splogs</b> now rival generic web spam and e-mail spam, presenting a major problem to analytics on the blogosphere from basic search and indexing, to opinion, community, influence and correlation detection. This open task submission details how <b>splogs</b> impact Opinion Identification, and proposes an approach to assessment and evaluation for a Spam Blog Classification task in 2007...|$|R
50|$|Additionally, scraped content {{can appear}} on {{literally}} {{any type of}} <b>splog</b> or RSS-fed spam site. This means an unsuspecting individual could find their creative or copyrighted material copied onto a site promoting pornography or similar type of content that may be offensive to the original author and his/her audience. This may be damaging to the original author's reputation.|$|E
50|$|By genre: Some blogs {{focus on}} a {{particular}} subject, such as political blogs, journalism blogs, health blogs, travel blogs (also known as travelogs), gardening blogs, house blogs, book blogs, fashion blogs, beauty blogs, lifestyle blogs, party blogs, wedding blogs, photography blogs, project blogs, psychology blogs, sociology blogs, education blogs, niche blogs, classical music blogs, quizzing blogs, legal blogs (often {{referred to as a}} blawgs), or dreamlogs. How-to/Tutorial blogs are becoming increasing popular. Two common types of genre blogs are art blogs and music blogs. A blog featuring discussions especially about home and family is not uncommonly called a mom blog and one made popular is by Erica Diamond who created Womenonthefence.com which is syndicated to over two million readers monthly. While not a legitimate type of blog, one used {{for the sole purpose of}} spamming is known as a <b>splog.</b>|$|E
5000|$|A blog scraper who gathers {{content that}} is copyrighted {{material}} {{can be considered}} {{in violation of the}} law, depending on the case, data usage and country. [...] Blog scraping can create problems for the individual or business who owns the blog. Blog scraping is particularly worrisome for business owners and business bloggers. Scrapers can copy an entire post from an independent or business blog. The duplicated content will include the author's tag and a link back to the author's site (if that link appears in the author's tag). However, most blog scrapers copy only a portion of the content that is keyword-relevant to their <b>splog</b> topic. By doing this, the keyword relevancy of the scraper's site is increased. Secondly, by not scraping the entire post, any outbound links are eliminated which means their search engine ranking is not reduced.|$|E
5000|$|Spam {{related to}} blogs, {{including}} comment spam, <b>splogs,</b> and ping spam ...|$|R
40|$|Weblogs or blogs are an {{important}} new way to publish information, engage in discussions, and form communities on the Internet. The Blogosphere has unfortunately been infected by several varieties of spam-like content. Blog search engines, for example, are inundated by posts from <b>splogs</b> – false blogs with machine generated or hijacked content whose sole purpose is to host ads or raise the PageRank of target sites. We discuss how SVM models based on local and link-based features {{can be used to}} detect <b>splogs.</b> We present an evaluation of learned models and their utility to blog search engines; systems that employ techniques differing from those of conventional web search engines...|$|R
50|$|Blog {{scraping}} is copying a blog, or blog content, {{that is not}} {{owned by}} the individual initiating the scraping process. If the material is copyrighted it is considered copyright infringement, {{unless there is a}} license relaxing the copyright or the country has fair-use or private use law. The scraped content is often used on spam blogs or <b>splogs,</b> such places are called scraper sites.|$|R
40|$|Spam blogs (splogs) {{have become}} {{a major problem in}} the {{increasingly}} popular blogosphere. Splogs are detrimental in that they corrupt the quality of information retrieved and they waste tremendous network and storage resources. We study several research issues in <b>splog</b> detection. First, in comparison to web spam and email spam, we identify some unique characteristics of <b>splog.</b> Second, we propose a new online task that captures the unique characteristics of <b>splog,</b> in addition to tasks based on the traditional IR evaluation framework. The new task introduces a novel time-sensitive detection evaluation to indicate how quickly a detector can identify splogs. Third, we propose a <b>splog</b> detection algorithm that combines traditional content features with temporal and link features that are unique to blogs. Finally, we develop an annotation tool to generate ground truth on a sampled subset of the TREC-Blog dataset. Initial experiments based on this ground truth set show promising results. 1...|$|E
40|$|This paper {{shows that}} {{in the context of}} {{statistical}} weblog classification for <b>splog</b> filtering based on n-grams of tokens in the URL, further segmenting the URLs beyond the standard punctuation is helpful. Many <b>splog</b> URLs contain phrases in which the words are glued together in order to avoid <b>splog</b> filtering techniques based on punctuation segmentation and unigrams. A technique which segments long tokens into the words forming the phrase is proposed and evaluated. The resulting tokens are used as features for a weblog classifier whose accuracy {{is similar to that of}} humans (78 % vs. 76 %) and reaches 93. 3 % of precision in identifying splogs with recall of 50. 9 %. ...|$|E
40|$|This article {{addresses}} {{the problem of}} spam blog (<b>splog)</b> detection using temporal and structural regularity of content, post time and links. Splogs are undesirable blogs meant to attract search engine traffic, used solely for promoting affiliate sites. Blogs represent popular online media, and splogs not only degrade the quality of search engine results, but also waste network resources. The <b>splog</b> detection problem is made difficult {{due to the lack}} of stable content descriptors. We have developed a new technique for detecting splogs, based on the observation that a blog is a dynamic, growing sequence of entries (or posts) rather than a collection of individual pages. In our approach, splogs are recognized by their temporal characteristics and content. There are three key ideas in our <b>splog</b> detection framework. (a) We represent the blog temporal dynamics using selfsimilarity matrices defined on the histogram intersection similarity measure of the time, content, and link attributes of posts, to investigate the temporal changes of the post sequence. (b) We study the blog temporal characteristics using a visual representation derived from the self-similarity measures. The visual signature reveals correlation between attributes and posts, depending on the type of blogs (normal blogs and splogs). (c) We propose two types of novel temporal features to capture the <b>splog</b> temporal characteristics. In our <b>splog</b> detector, these novel features are combine...|$|E
50|$|On the Web, the {{predominant}} form of such manipulation is search engine spamming (also known as spamdexing), which involves employing various techniques {{to disrupt the}} activity of web search engines, usually for financial gain. Examples of spamdexing are link-bombing, comment or referrer spam, spam blogs (<b>splogs),</b> malicious tagging. Reverse engineering of ranking algorithms, advertisement blocking, click fraud, and web content filtering may also be considered forms of adversarial data manipulation.|$|R
5000|$|There is {{frequent}} confusion {{between the}} terms [...] "splog" [...] and [...] "spam in blogs". <b>Splogs</b> are blogs where the articles are fake, and are only created for search engine spamming. To spam in blogs, conversely, is to include random {{comments on the}} blogs of innocent bystanders, in which spammers {{take advantage of a}} site's ability to allow visitors to post comments that may include links. In fact, one of the earliest uses of the term [...] "splog" [...] referred to the latter.|$|R
40|$|Blogs {{have become}} {{a means by which}} new ideas and {{information}} spreads rapidly on the web. They often discuss the latest trends and echo with reactions to different events in the world. The collective wisdom present on the blogosphere is invaluable for market researchers and companies launching new products. In this paper, we evaluate the effectiveness of some of the influence models on the blogosphere. We also examine the robustness of different heuristics in the presence of <b>splogs</b> or spam blogs. Experiments show PageRank based heuristics could be used to select an influential set of bloggers such that we could maximize the spread of information on the blogosphere. 1...|$|R
40|$|This paper studies how {{to reduce}} the amount of human su-pervision for {{identifying}} splogs / authentic blogs in the con-text of continuously updating <b>splog</b> data sets year by year. Following the previous works on active learning, against the task of <b>splog</b> / authentic blog detection, this paper empir-ically examines several strategies for selective sampling in active learning by Support Vector Machines (SVMs). As a confidence measure of SVMs learning, we employ the dis-tance from the separating hyperplane to each test instance, which have been well studied in active learning for text clas-sification. Unlike those results of applying active learning to text classification tasks, in the task of <b>splog</b> / authentic blog detection of this paper, it is not the case that adding least confident samples peforms best...|$|E
40|$|This paper {{focuses on}} spam blog (<b>splog)</b> detection. Blogs are highly popular, new media social {{communication}} mechanisms. The presence of splogs degrades blog search results {{as well as}} wastes network resources. In our approach we exploit unique blog temporal dynamics to detect splogs. There are three key ideas in our <b>splog</b> detection framework. We first represent the blog temporal dynamics using self-similarity matrices defined on the histogram intersection similarity measure of the time, content, and link attributes of posts. Second, we show via a novel visualization that the blog temporal characteristics reveal attribute correlation, depending on type of the blog (normal blogs and splogs). Third, we propose the use of temporal structural properties computed from self-similarity matrices across different attributes. In a <b>splog</b> detector, these novel features are combined with content based features. We extract a content based feature vector {{from different parts of}} the blog – URLs, post content, etc. The dimensionality of the feature vector is reduced by Fisher linear discriminant analysis. We have tested an SVM based <b>splog</b> detector using proposed features on real world datasets, with excellent results (90 % accuracy) ...|$|E
40|$|In this work, {{we propose}} a novel post-indexing spam-blog (or <b>splog)</b> {{detection}} method, which capitalizes on the re-sults returned by blog search engines. More specifically, we analyze the search {{results of a}} sequence of temporally-ordered queries returned by a blog search engine, and build and maintain blog profiles for those blogs whose posts fre-quently appear in the top-ranked search results. With the blog profiles, 4 <b>splog</b> scoring functions were evaluated us-ing real data collected from a popular blog search engine. Our experiments show that the proposed method could ef-fectively detect splogs with a high accuracy...|$|E
40|$|This paper {{describes}} our {{approaches to}} the opinion retrieval and blog distillation tasks for the Blog Track. For opinion retrieval we employ a two-stage framework consisting of keyword search and opinion classification, where customer reviews collected from Amazon. com are used for feature selection. For the blog distillation task we consider all the blog posts belonging to a blog in order to estimate {{the relevance of the}} blog at large. To accomplish this, we first identify relevant blogs for a given topic by keyword search and then examine all the posts for each identified blog. In addition, we attempt to detect and discard spam blogs (<b>splogs)</b> and non-English blogs to improve system performance. ...|$|R
5000|$|The {{infopreneur}} may attract {{traffic to}} his/her site by manipulating their site to appear higher on search engine results. This may {{be done by}} creating a site that is robust in information, and configuring META keywords and descriptions that accurately describe the web page. More often, infopreneurs who are out to get a [...] "quick buck" [...] will create a mash-together of information by publishing popular, sought after content, often incorporating RSS feeds from more popular sites. The infopreneur then makes money from AdSense ads, affiliate links, referrals and leads, and/or selling ebooks {{that are related to}} the search parameters and keywords. Essentially, these infopreneurs [...] "piggy-back" [...] on already established information. For example, there are many such <b>splogs</b> that copy verbatim the articles from Wikipedia.|$|R
5000|$|Google's “Boston” update in February 2003 saw major {{algorithmic}} {{changes and}} the promise of frequent index updates. [...] “Cassandra” marked a much more aggressive attack on shady SEO techniques like hidden and disguised keyword links, by emphasizing link qualityThis was taken a step further in “Dominic”, which sought to analyze the quality of all backlinks to prevent the then emerging practice of <b>splogging</b> (the practice of creating nonsensical offsite content to boost SERP of another site). [...] To combat practices like “Googlebombing” (putting irrelevant, often negative anchor text linking to popular websites) Dominic tinkered with the weighting of anchor text while stringently scrutinizing back links and internal linking. [...] An exploit where webmasters would link to the same site using different anchor text (thereby allowing both links to unfairly contribute to sites PageRank) was addressed by allowing only one site (given duplicate site links with differing anchor text) link to flow to PageRank.|$|R
40|$|This paper {{focuses on}} spam blog (<b>splog)</b> detection. Blogs are highly popular, new media social {{communication}} mechanisms and splogs corrupt blog search results {{as well as}} waste network resources. In our approach we exploit unique blog temporal dynamics to detect splogs. The key idea is that splogs exhibit high temporal regularity in content and post time, as well as consistent linking patterns. Temporal content regularity is detected using a novel autocorrelation of post content. Temporal structural regularity is determined using the entropy of the post time difference distribution, while the link regularity is computed using a HITS based hub score measure. Experiments based on the annotated ground truth on real world dataset show excellent results on <b>splog</b> detection tasks with 90 % accuracy. ...|$|E
40|$|Weblogs, or blogs {{have become}} an {{important}} new way to publish information, engage in discussions and form communities. The increasing popularity of blogs {{has given rise to}} search and analysis engines focusing on the “blogosphere”. A key requirement of such systems is to identify blogs as they crawl the Web. While this ensures that only blogs are indexed, blog search engines are also often overwhelmed by spam blogs (splogs). Splogs not only incur computational overheads but also reduce user satisfaction. In this paper we first describe experimental results of blog identification using Support Vector Machines (SVM). We compare results of using different feature sets and introduce new features for blog identification. We then report preliminary results on <b>splog</b> detection and identify future work...|$|E
40|$|The BlogVox system {{retrieves}} opinionated blog posts {{specified by}} ad hoc queries. BlogVox {{was developed for}} the 2006 TREC blog track by the University of Maryland, Baltimore County and the Johns Hopkins University Applied Physics Laboratory using a novel system to recognize legitimate posts and discriminate against spam blogs. It also processes posts to eliminate extraneous non-content, including blog-rolls, link-rolls, advertisements and sidebars. After retrieving posts relevant to a topic query, the system processes them to produce a set of independent features estimating {{the likelihood that a}} post expresses an opinion about the topic. These are combined using an SVM-based system and integrated with the relevancy score to rank the results. We evaluate BlogVox’s performance against human assessors. We also evaluate the individual <b>splog</b> filtering and non-content removal components of BlogVox...|$|E
40|$|The {{explosion}} of blogs on the Web {{in recent years}} has fostered research interest in the Information Retrieval (IR) and other communities into the properties of the so-called `blogsphere'. However, without any standard test collection available, research has been restricted to unshared collections collected by individual research groups. With the advent of the Blog Track running at TREC 2006, there was a need to create a test collection of blog data, that could be shared among participants and form the backbone of the experiments. Such a collection should be a realistic snapshot of the blogsphere, of enough blogs as to have recognisable properties of the blogsphere, and over a long enough time period that events should be recognisable. In addition, the collection should exhibit other properties of the blogsphere, such as <b>splogs</b> and comment spam. This paper describes the creation of the Blogs 06 collection by the University of Glasgow, and reports statistics of the collected data. Moreover, we demonstrate how some characteristics of the collection vary across the spam and non-spam components of the collection...|$|R
40|$|We explore several {{different}} document representation models and two query expansion {{models for the}} task of recommending blogs to a user {{in response to a}} query. Blog relevance ranking differs from traditional document ranking in ad-hoc information retrieval in several ways: (1) the unit of output (the blog) is composed of a collection of documents (the blog posts) rather than a single document, (2) the query represents an ongoing – and typically multifaceted – interest in the topic rather than a passing ad-hoc information need and (3) due to the propensity of spam, <b>splogs,</b> and tangential comments, the blogosphere is particularly challenging to use as a source for high-quality query expansion terms. We address these differences at the document representation level, by comparing retrieval models that view either the blog or its constituent posts as the atomic units of retrieval, and at the query expansion level, by making novel use of the links and anchor text in Wikipedia 1 to expand a user’s initial query. We develop two complementary models of blog retrieval that perform at comparable levels of precision and recall. We also show consistent and significant improvement across all models using our Wikipedia expansion strategy...|$|R
5000|$|Some webmasters create {{websites}} {{tailored to}} lure searchers from Google and other engines onto their AdSense website {{to make money}} from clicks. Such websites often contain nothing but {{a large amount of}} interconnected, automated content (e.g., a directory with content from the Open Directory Project, or [...] "scraper" [...] websites relying on RSS feeds for content). Possibly the most popular form of such [...] "AdSense farms" [...] are <b>splogs</b> (spam blogs), which are poorly written content centered around known high-paying keywords. Many of these websites reuse content from other websites, such as Wikipedia, to attract visitors. These and related approaches are considered to be search engine spam and can be reported to Google. A Made for AdSense (MFA) website or webpage has little or no content, but is filled with advertisements so that users {{have no choice but to}} click on advertisements. Such pages were tolerated in the past, but due to complaints, Google now disables such accounts. There have also been reports of Trojan horses engineered to produce counterfeit Google advertisements that are formatted looking like legitimate ones. The Trojan uploads itself onto an unsuspecting user's computer through a webpage and then replaces the original advertisements with its own set of malicious advertisements.|$|R
40|$|Spam blogs, or splogs feature plagiarized or auto-generated content. They create link {{farms to}} promote affiliates, and are {{motivated}} by the profitability of hosting ads. Splogs infiltrate the blogosphere at ping servers, systems that aggregate blog update pings. Over the past year, our work has focused on detecting and eliminating splogs. As techniques used by spammers have evolved, we have learned how <b>splog</b> signatures are tied to tools that create them, that {{they are beginning to}} be a problem across languages, and that they require a much quicker assessment. Though we continue to address these specific challenges, we discuss our larger goal in this work, of developing a scalable meta-ping filter that detects and eliminates update pings from splogs. This will considerably reduce computational requirements and manual efforts at downstream services (search engines) and involve the community in detecting spam blogs. 1...|$|E
40|$|Document {{spanners}} are {{a formal}} framework for information extraction that {{was introduced by}} [Fagin, Kimelfeld, Reiss, and Vansummeren, J. ACM, 2015]. One of the central models in this framework are core spanners, {{which are based on}} regular expressions with variables that are then extended with an algebra. As shown by [Freydenberger and Holldack, ICDT, 2016], there is a connection between core spanners and EC^{reg}, the existential theory of concatenation with regular constraints. The present paper further develops this connection by defining <b>SpLog,</b> a fragment of EC^{reg} that has the same expressive power as core spanners. This equivalence extends beyond equivalence of expressive power, as we show the existence of polynomial time conversions between this fragment and core spanners. This even holds for variants of core spanners that are based on automata instead of regular expressions. Applications of this approach include an alternative way of defining relations for spanners, insights into the relative succinctness of various classes of spanner representations, and a pumping lemma for core spanners...|$|E
40|$|<b>Splog</b> {{is the key}} {{challenge}} in the access of blogosphere. Existing splog-filtering methods are restricted to the way for traditional web spam filtering, without considering the characteristics of blogs. Inspired by the observation that fake writers (writers of splogs) have striking higher consistent writing behavior than real writers (writers of legitimate blogs), we propose to detect splogs by distinguishing fake writers from real writers. To measure how consistent the writing behavior is, we propose the consistency-based features derived from writing interval, writing structure and writing topic. Then we designed a splog-filtering system which can use the consistency-based features effectively and flexibly. The experimental results on Blog 06 data set show that, proposed measure can effectively detect splogs, reaching an accuracy of 90 %. Compared with content-based methods, our approach can get a comparable accuracy with fewer features and smaller train set, indicating that writing consistency represents the essential difference between splogs and blogs. 1...|$|E
40|$|This is an Open Access Article. It is {{published}} by Schloss Dagstuhl under the Creative Commons Attribution 4. 0 Unported Licence (CC BY). Full details of this licence are available at: [URL] spanners are a formal framework for information extraction that was introduced by Fagin, Kimelfeld, Reiss, and Vansummeren (PODS 2013, JACM 2015). One of the central models in this framework are core spanners, {{which are based on}} regular expressions with variables that are then extended with an algebra. As shown by Freydenberger and Holldack (ICDT 2016), there is a connection between core spanners and ECreg, the existential theory of concatenation with regular constraints. The present paper further develops this connection by defining <b>SpLog,</b> a fragment of ECreg that has the same expressive power as core spanners. This equivalence extends beyond equivalence of expressive power, as we show the existence of polynomial time conversions between this fragment and core spanners. This even holds for variants of core spanners that are based on automata instead of regular expressions. Applications of this approach include an alternative way of defining relations for spanners, insights into the relative succinctness of various classes of spanner representations, and a pumping lemma for core spanners...|$|E
40|$|Weblogs, or blogs are an {{important}} new way to publish information, engage in discussions, and form communities on the Internet. Blogs are a global phenomenon, and with numbers well over 100 million they form {{the core of the}} emerging paradigm of Social Media. While the utility of blogs is unquestionable, a serious problem now afflicts them, that of spam. Spam blogs, or splogs are blogs with auto-generated or plagiarized content with the sole purpose of hosting profitable contextual ads and/or inflating importance of linked-to sites. Though estimates vary, splogs account for more than 50 % of blog content, and present a serious threat to their continued utility. Splogs impact search engines that index the entire Web or just the blogosphere by increasing computational overhead and reducing user satisfaction. Hence, search engines try to minimize the influence of spam, both prior to indexing and after indexing, by eliminating splogs, comment spam, social media spam, or generic web spam. In this work we further {{the state of the art}} of <b>splog</b> detection prior to indexing. First, we have identified and developed techniques that are effective for <b>splog</b> detection in a supervised machine learning setting. While some of these are novel, a few others confirm the utility of techniques that have worked well for e-mail and Web spam detection in a new domain i. e. the blogosphere. Specifically, our techniques identify spam blogs using URL, home-page, and syndication feeds. To enable the utility of our techniques prior to indexing, the emphasis of our effort is fast online detection. Second, to effectively utilize identified techniques in a real-world context, we have developed a novel system that filters out spam in a stream of update pings from blogs. Our approach is based on using filters serially in increasing cost of detection that better supports balancing cost and effectiveness. We have used such a system to support multiple blog related projects, both internally and externally. Next, motivated by these experiences, and input room real-world deployments of our techniques for over ayear, we have developed an approach for updating classifiers in an adversarial setting. We show how an ensemble of classifiers can co-evolve and adapt when used on a stream of unlabeled instances susceptible to concept drift. We discuss how our system is amenable to such evolution by discussing approaches that can feed into it. Finally, over the course of this work we have characterized the specific nature of spam blogs along various dimensions, formalized the problem and created general awareness of the issue. We are the first to formalize and address the problem of spam in blogs and identify the general problem of spam in Social Media. We discuss how lessons learned can guide follow-up work on spam in social media, {{an important}} new problem on the Web...|$|E

