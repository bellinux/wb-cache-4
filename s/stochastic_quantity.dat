29|73|Public
40|$|We {{consider}} a one-dimensional "gas" of inelastically colliding particles where kinetic energy is dissipated by the excitation of vibrational degrees of freedom. In our model {{the coefficient of}} restitution is a <b>stochastic</b> <b>quantity</b> whose distribution can be calculated from an exact stochastic equation of motion. We investigate the equipartition properties {{of the system and}} propose a new algorithm for computer simulations, that is a combination of event-driven and Monte-Carlo methods. Comment: 7 pages, Latex, 3 Postscript figures uuencode...|$|E
40|$|The {{simulation}} of {{the expectation of}} a <b>stochastic</b> <b>quantity</b> E[Y] by Monte Carlo methods {{is known to be}} computationally expensive especially if the <b>stochastic</b> <b>quantity</b> or its approximation Y_n is expensive to simulate, e. g., the solution of a stochastic partial differential equation. If the convergence of Y_n to Y in terms of the error |E[Y - Y_n]| is to be simulated, this will typically be done by a Monte Carlo method, i. e., |E[Y] - E_N[Y_n]| is computed. In this article upper and lower bounds for the additional error caused by this are determined and compared to those of |E_N[Y - Y_n]|, which are found to be smaller. Furthermore, the corresponding results for multilevel Monte Carlo estimators, for which the additional sampling error converges with the same rate as |E[Y - Y_n]|, are presented. Simulations of a stochastic heat equation driven by multiplicative Wiener noise and a geometric Brownian motion are performed which confirm the theoretical results and show the consequences of the presented theory for weak error simulations. Comment: 16 pages, 5 figures; formulated Section 2 independently of SPDEs, shortened Section 3, added example of geometric Brownian motion in Section...|$|E
40|$|We {{examine the}} second-best family policy under the {{assumption}} that both the number and the future earning capacities of the children born to a couple are random variables with probability distributions conditional on unobservable parental actions. Potential parents take their decisions without taking into account the effects of these actions on the government's future tax revenue. The second-best policy provides parents with credit and insurance, and allows them to appropriate the external benefits of their actions. <b>stochastic</b> <b>quantity</b> and quality of children, moral hazard, population externalities, family allowances, scholarships, pensions...|$|E
5000|$|However {{the meaning}} of the {{parameters}} is different from Heston model. In this model both, mean reverting and volatility of variance parameters, are <b>stochastic</b> <b>quantities</b> given by [...] and [...] respectively.|$|R
40|$|Stochastic Einstein {{equations}} {{are considered}} when 3 D space metric γ_ij are stochastic functions. The probability density for the <b>stochastic</b> <b>quantities</b> {{is connected with}} the Perelman's entropy functional. As an example, the Friedman Universe is considered. It is shown that for the Friedman Universe the dynamical evolution is not changed. The connection between general relativity and Ricci flows is discussed. Comment: 5 page...|$|R
50|$|The {{mathematical}} {{usage of}} a quantity {{can then be}} varied and so is situationally dependent. Quantities {{can be used as}} being infinitesimal, arguments of a function, variables in an expression (independent or dependent), or probabilistic as in random and <b>stochastic</b> <b>quantities.</b> In mathematics, magnitudes and multitudes are also not only two distinct kinds of quantity but furthermore relatable to each other.|$|R
30|$|The {{ability to}} predict the future return of a {{business}} based on historical data is the biggest challenge for any investment as it is affected by uncertainties from various contributing sources. Sources of uncertainty may be divided into two types: aleatory and epistemic (Oberkampf et al. 2004; Khalaj et al. 2013). Aleatory uncertainty is irreducible. Examples include phenomena that exhibit natural variation such as operating conditions, material properties. In contrast, epistemic uncertainty results from a lack of knowledge about the system, or due to approximations in the system behavior models, or due to limited or subjective data (e.g., expert opinion); it can be reduced as more information about the system is obtained. Epistemic uncertainty regarding model parameters can be viewed in two ways. It can be defined with reference to a <b>stochastic</b> <b>quantity</b> whose distribution type and/or distribution parameters are not precisely known (Baudrit and Dubois 2006), or with reference to a deterministic quantity whose value is not precisely known (Helton et al. 2004). This paper focuses on handling the first definition of epistemic uncertainty, i.e., epistemic uncertainty with reference to a <b>stochastic</b> <b>quantity.</b> In some cases, distribution information of a random variable may only be available as intervals given by experts. The objective {{of this paper is to}} develop an efficient robust portfolio optimization methodology that includes both aleatory uncertainty and epistemic uncertainty described through interval data.|$|E
40|$|Radar {{tracking}} of a projectile {{flying in the}} Earth's atmosphere {{is a very complex}} issue to cope with, due to the need of (suboptimal) nonlinear filtering techniques. Almost all cases found in literature assume that the target trajectory is observable from the firing point to the impact point on the ground, namely the trajectory observation gets under way from the first available measurement. The radar track initiation time is actually a <b>stochastic</b> <b>quantity</b> that has to be treated by means of a statistical procedure. In this paper a preliminary analysis of the effect of a more realistic filter initialization is proposed. © 2010 IEEE...|$|E
40|$|Abstract—In {{data fusion}} theory, {{multiple}} estimates are com-bined to yield an optimal result. In this paper, {{the set of}} all possible results is investigated, when two random variables with unknown correlations are fused. As a first step, recursive processing of the set of estimates is examined. Besides set-theoretic considerations, the {{lack of knowledge about}} the un-known correlation coefficient is modeled as a <b>stochastic</b> <b>quantity.</b> Especially, a uniform model is analyzed, which provides a new optimization criterion for the covariance intersection algorithm in scalar state spaces. This approach is also generalized to multi-dimensional state spaces in an approximative, but fast and scalable way, so that consistent estimates are obtained...|$|E
40|$|A phase space {{theory for}} fermions has been {{developed}} using Grassmann phase space variables {{which can be used}} in numerical calculations for cold Fermi gases and for large fermion numbers. Numerical calculations are feasible because Grassmann stochastic variables at later times are related linearly to such variables at earlier times via c-number <b>stochastic</b> <b>quantities.</b> A Grassmann field version {{has been developed}} making large fermion number applications possible. Applications are shown for few mode and field theory cases...|$|R
40|$|We {{show for}} the first time that the {{stochastic}} variational method can naturally derive the Navier-Stokes equation starting from the action of ideal fluid. In the frame work of the stochastic variational method, the dynamical variables are extended to <b>stochastic</b> <b>quantities.</b> Then the effect of dissipation is realized as the direct consequence of the fluctuation-dissipation theorem. The present result reveals the potential availability of this approach to describe more general dissipative processes. Comment: 5 pages, no figure, discussions and references are added, errors in Sec. IV were correcte...|$|R
40|$|Abstract. The {{purpose of}} this paper is to present the {{catastrophe}} theory method for the optimal design of machine components. A brief description of the cusp catastrophe is presented in the introduction. The statement of optimal design problem is given in the second part of the paper. A single criterion design is presented; the reliability function is used as the objective function. The last part is devoted to probability approach. Manage variables are viewed as <b>stochastic</b> <b>quantities,</b> analytical and statistical linearization methods are used for the reliability function evaluation...|$|R
40|$|The {{stability}} of the Phillips Curve, the negative correlation between unemploy-ment rate and inflation rate, is a controversial issue since it seemed to break in the 1970 s. This paper shows that the {{stability of}} the Phillips Curve depends on sectoral structure of the economy, especially intersectoral mobility of labor. To describe this situation, I employ a multi-sector model with <b>stochastic</b> <b>quantity</b> ad-justment created by Aoki and Yoshikawa (2003), and incorporate prices into it. The simulation analysis with the model shows that externality and mobility in the labor markets are essential for the negative slope of the Phillips Curve. JEL Classification Number: E 24, E 3...|$|E
40|$|In {{this paper}} {{we present a}} dynamic {{macroeconomic}} model with <b>stochastic</b> <b>quantity</b> rationing. Trades take place in each period even when prices are not at their Walrasian level. Moving from one period to the next, prices and wages are adjusted according to the intensity of rationing, a reliable measure of which is obtained by means of stochastic rationing. A complete characterization of the typology of equilibria is given and dynamic adjustment equations are derived. From this {{it is evident that}} structural parameters such as the adjustment speed of prices, as well as government policy parameters, are decisive for the type of dynamics that emerges. In particular there is a tendency for nominal wage stickiness to stabilise the economy whereas high wage ‡exibility favours cyclical and irregular behaviour...|$|E
40|$|It is {{emphasized}} that quantum entanglement determined {{in terms of}} the von Neumann entropy operator is a <b>stochastic</b> <b>quantity</b> and, therefore, can fluctuate. The rms fluctuations of the entanglement entropy of two-qubit systems in both pure and mixed states have been obtained. It has been found that entanglement fluctuations in the maximally entangled states are absent. Regions where the entanglement fluctuations are larger than the entanglement itself (strong fluctuation regions) have been revealed. It has been found that the magnitude of the relative entanglement fluctuations is divergent at the points of the transition of systems from an entangled state to a separable state. It has been shown that entanglement fluctuations vanish in the separable states. Comment: 5 pages, 4 figure...|$|E
40|$|Considering {{the three}} {{components}} of the geomagnetic field as <b>stochastic</b> <b>quantities,</b> we used neural networks to study their time evolution in years. In order {{to find the best}} NN for the time predictions, we tested different kinds of NN and different ways of their training, where the inputs and targets are long hourly time series of geomagnetic field values. The found NN was used to predict the values of the hourly mean values of the geomagnetic field components. KEY WORDS: Geomagnetic field –Geomagnetic components –Neural networks (NN) –Time series –Time prediction...|$|R
40|$|This paper {{addresses}} {{the problem of}} interconnection networks performance modeling of large-scale distributed systems with emphases on multi-cluster computing systems. The study of interconnection networks is important because the overall performance of a distributed system is often critically hinged {{on the effectiveness of}} its interconnection network. We present an analytical model that considers <b>stochastic</b> <b>quantities</b> as well as processor heterogeneity of the target system. The model is validated through comprehensive simulation, which demonstrates that the proposed model exhibits a good degree of accuracy for various system sizes and under different operating conditions. <br /...|$|R
40|$|Abstract — We {{propose a}} Bayesian {{technique}} for blind detection of coded data transmitted over a dispersive channel. The Bayesian maximum likelihood sequence detector views the channel taps as <b>stochastic</b> <b>quantities</b> {{drawn from a}} known distribution and computes the probability of any transmitted sequence by averaging over the tap values. The resulting path metric requires memory of all previous symbols, and hence a tree-based algorithm is employed {{to find the most}} likely transmitted sequence. Simulation results show that the Bayesian detector can achieve bit error rates within 1 / 4 dB of the conventional known-channel maximum likelihood (ML) sequence detector. I...|$|R
40|$|Most {{models of}} {{decision-making}} in neuroscience assume an infinite horizon, which yields an optimal solution that integrates evidence up to a fixed decision threshold; however, under most experimental {{as well as}} naturalistic behavioral settings, the decision {{has to be made}} before some finite deadline, which is often experienced as a <b>stochastic</b> <b>quantity,</b> either due to variable external constraints or internal timing uncertainty. In this work, we formulate this problem as sequential hypothesis testing under a stochastic horizon. We use dynamic programming tools to show that, for a large class of deadline distributions, the Bayes-optimal solution requires integrating evidence up to a threshold that declines monotonically over time. We use numerical simulations to illustrate the optimal policy in the special cases of a fixed deadline and one that is drawn from a gamma distribution. ...|$|E
40|$|Abstract Most {{models of}} {{decision-making}} in neuroscience assume an infinite horizon,which yields an optimal solution that integrates evidence up to a fixed decision threshold; however, under most experimental {{as well as}} naturalistic behavioralsettings, the decision {{has to be made}} before some finite deadline, which is often experienced as a <b>stochastic</b> <b>quantity,</b> either due to variable external constraints orinternal timing uncertainty. In this work, we formulate this problem as sequential hypothesis testing under a stochastic horizon. We use dynamic programming toolsto show that, for a large class of deadline distributions, the Bayes-optimal solution requires integrating evidence up to a threshold that declines monotonically overtime. We use numerical simulations to illustrate the optimal policy in the special cases of a fixed deadline and one that is drawn from a gamma distribution...|$|E
40|$|This paper {{uses the}} random fatigue limit (RFL) model of Pascual and Meeker to address fatigue limits for PVC {{pressure}} pipes. This model differs from previous ways of describing fatigue data for PVC by recognising that the fatigue limit is a <b>stochastic</b> <b>quantity</b> {{rather than a}} single valued stress amplitude below which fatigue failures will not occur. By analysing published fatigue data, it is demonstrated that the RFL model is capable of quantifying fatigue limit variability and its influence on fatigue life variability. Moreover, the RFL model was used to illustrate {{the risks associated with}} defining fatigue limits based on small quantities of high-cycle fatigue data. In particular, it is shown that fatigue failure can occur below the mean fatigue limit and that the RFL model is capable of quantifying the probability of failure at a given level of stress amplitude...|$|E
40|$|ABSTRACT: The aim of {{this work}} {{is to develop a}} {{simulation}} approach to the yield curve evolution in the Heath, Jarrow & Morton (1992) framework. The <b>stochastic</b> <b>quantities</b> considered as affecting the forward rate volatility function are the spot rate and the forward rate. A decomposition of the volatility function into a Hull & White (1990) volatility and a remainder allows us to develop an efficient Control Variate Method that makes use of the closed form solution of the Hull and White call option. This technique considerably speeds up the simulation algorithm to approximate call option values with Monte Carlo simulation. ...|$|R
40|$|This paper {{presents}} a methodology for design optimization of decomposed {{systems in the}} presence of uncertainties. We extend the analytical target cascading (ATC) formulation to probabilistic design by treating <b>stochastic</b> <b>quantities</b> as random variables and parameters and posing reliability-based design constraints. We model the propagation of uncertainty throughout the multilevel hierarchy of elements that comprise the decomposed system by using the advanced mean value (AMV) method to generate the required probability distributions of nonlinear responses. We utilize appropriate metamodeling techniques for simulation-based design problems. A simple yet illustrative hierarchical bi-level engine design problem is used to demonstrate the proposed methodology. ...|$|R
40|$|This paper {{addresses}} {{the problem of}} performance modeling of heterogeneous multi-cluster computing systems. We present an analytical model that can be employed to explore the effectiveness of different design approaches so that one can have an intelligent choice during design and evaluation of a cost effective large-scale heterogeneous distributed computing system. The proposed model considers <b>stochastic</b> <b>quantities</b> as well as processor heterogeneity of the target system. The analysis {{is based on a}} parametric fat-tree network, the m- port n- tree, and a deterministic routing algorithm. The correctness of the proposed model is validated through comprehensive simulation of different types of clusters. <br /...|$|R
40|$|International audienceIn this paper, a {{stochastic}} fluid {{model is}} used to study a manufacturing/ remanufacturing system composed by two parallel machines, a serviceable inventory, a remanufacturing inventory and customers who demand a <b>stochastic</b> <b>quantity</b> of product. Stochastic fluid model is adopted to describe the system and {{to take into account}} machine failure, stochastic demand, stochastic returned products and remanufacturing products. The goal {{of this paper is to}} evaluate the optimal serviceable inventory level which allows minimizing the sum of inventory and lost sales costs. Perturbation analysis is applied to the stochastic fluid model to optimize the considered system. The trajectories of the serviceable inventory level are studied and the perturbation analysis estimates are evaluated. The unbiasedness of these estimates is proved and then they are implemented in an optimization algorithm which determines the optimal serviceable inventory in the presence of stochastic returned products and stochastic demand...|$|E
40|$|Compressive-sensing-based {{uncertainty}} quantification {{methods have}} become a pow- erful tool for problems with limited data. In this work, we use the sliced inverse regression (SIR) method to provide an initial guess for the alternating direction method, {{which is used to}} en- hance sparsity of the Hermite polynomial expansion of <b>stochastic</b> <b>quantity</b> of interest. The sparsity improvement increases both the efficiency and accuracy of the compressive-sensing- based uncertainty quantification method. We demonstrate that the initial guess from SIR is more suitable for cases when the available data are limited (Algorithm 4). We also propose another algorithm (Algorithm 5) that performs dimension reduction first with SIR. Then it constructs a Hermite polynomial expansion of the reduced model. This method affords the ability to approximate the statistics accurately with even less available data. Both methods are non-intrusive and require no a priori information of the sparsity of the system. The effec- tiveness of these two methods (Algorithms 4 and 5) are demonstrated using problems with up to 500 random dimensions...|$|E
40|$|We {{consider}} a one–dimensional ”gas ” of inelastically colliding particles where kinetic energy is dissipated by the excitation of vibrational degrees of freedom. In our model {{the coefficient of}} restitution is a <b>stochastic</b> <b>quantity</b> whose distribution can be calculated from an exact stochastic equation of motion. We investigate the equipartition properties {{of the system and}} propose a new algorithm for computer simulations, that is a combination of event–driven(ED) and Monte–Carlo methods. Numerical and theoretical approaches to the dynamics of granular materials frequently adopt the concept of a coefficient of restitution that determines the energy loss during collisions of granular particles. Event–driven (ED) simulations[1, 2, 3] have shown that model systems with fixed coefficient of restitution evolve into clustered states where a hydrodynamic description ceases to be correct: Fundamental assumptions of hydrodynamics concerning the validity of molecular chaos and local equilibrium are violated[4]. On the other hand, molecular–dynamics simulations[5] have the difficulty that ad hoc assumptions about microscopic interaction laws have to be made. An inadequate choice of th...|$|E
40|$|With {{the current}} {{popularity}} of cluster computing systems, {{it is increasingly}} {{important to understand the}} capabilities and potential performance of various interconnection networks. In this paper, we propose an analytical model for studying the capabilities and potential performance of interconnection networks for multi-cluster systems. The model takes into account <b>stochastic</b> <b>quantities</b> as well as network heterogeneity in bandwidth and latency in each cluster. Also, blocking and non-blocking network architecture model is proposed and are used in performance analysis of the system. The model is validated by constructing a set of simulators to simulate different types of clusters, and by comparing the modeled results with the simulated ones. <br /...|$|R
40|$|Professor Horowitz {{correctly}} {{identifies the}} limitation of my assuming separable utility functions to derive a marginal condition for efficiency under uncertainty. Correction this limitation, he provides a simple but powerful condition that encompasses the nonseparable {{as well as the}} separable case. This condition replaces the dubious Equation (14) derived in Kohn (1999). In a departure from von Neumann-Morgenstern theory, for cases in which the decisions of a risk-averse community are compared with those it would make were it risk-neutral, it is proposed here that the same utility function holds for risk-neutrality as for risk-aversion, but that the <b>stochastic</b> <b>quantities</b> be replaced by their expected value in the former. [Q 25]...|$|R
40|$|We {{investigate}} {{the efficiency of}} an isothermal Brownian work-to-work converter engine, composed of a Brownian particle coupled to a heat bath at a constant temperature. The system is maintained out of equilibrium by using two external time-dependent stochastic Gaussian forces, where one is called load force {{and the other is}} called drive force. Work done by these two forces are <b>stochastic</b> <b>quantities.</b> The efficiency of this small engine is defined as the ratio of stochastic work done against load force to stochastic work done by the drive force. The probability density function as well as large deviation function of the stochastic efficiency are studied analytically and verified by numerical simulations. Comment: 12 pages, 8 figure...|$|R
40|$|The Polynomial Chaos Expansion (PCE) {{technique}} {{allows us}} to recover a finite second-order random variable exploiting suitable linear combinations of orthogonal polynomials which are functions of a given <b>stochastic</b> <b>quantity</b> ξ, hence acting {{as a kind of}} random basis. The PCE methodology has been developed as a mathematically rigorous Uncertainty Quantification (UQ) method which aims at providing reliable numerical estimates for some uncertain physical quantities defining the dynamic of certain engineering models and their related simulations. In the present paper, we use the PCE approach in order to analyze some equity and interest rate models. In particular, we take into consideration those models which are based on, for example, the Geometric Brownian Motion, the Vasicek model, and the CIR model. We present theoretical as well as related concrete numerical approximation results considering, without loss of generality, the one-dimensional case. We also provide both an efficiency study and an accuracy study of our approach by comparing its outputs with the ones obtained adopting the Monte Carlo approach, both in its standard and its enhanced version...|$|E
40|$|Abstract The stride {{interval}} {{in healthy}} human gait fluctuates from step {{to step in}} a random manner and scaling of the interstride interval time series motivated previous investigators to conclude that this time series is fractal. Early studies suggested that gait is a monofractal process, but more recent work indicates the time series is weakly multifractal. Herein we present additional evidence for the weakly multifractal nature of gait. We use the stride interval time series obtained from ten healthy adults walking at a normal relaxed pace for approximately fifteen minutes each as our data set. A fractional Langevin equation is constructed to model the underlying motor control {{system in which the}} order of the fractional derivative is itself a <b>stochastic</b> <b>quantity.</b> Using this model we find the fractal dimension for each of the ten data sets to be in agreement with earlier analyses. However, with the present model we are able to draw additional conclusions regarding the nature of the control system guiding walking. The analysis presented herein suggests that the observed scaling in interstride interval data may not be due to long-term memory alone, but may, in fact, be due partly to the statistics. </p...|$|E
40|$|ANSYS/CARES/PDS is a {{software}} system that combines the ANSYS Probabilistic Design System (PDS) software with {{a modified version}} of the Ceramics Analysis and Reliability Evaluation of Structures Life (CARES/Life) Version 6. 0 software. [A prior version of CARES/Life was reported in Program for Evaluation of Reliability of Ceramic Parts (LEW- 16018), NASA Tech Briefs, Vol. 20, No. 3 (March 1996), page 28. ] CARES/Life models effects of stochastic strength, slow crack growth, and stress distribution on the overall reliability of a ceramic component. The essence of the enhancement in CARES/Life 6. 0 is the capability to predict the probability of failure using results from transient finite-element analysis. ANSYS PDS models the effects of uncertainty in material properties, dimensions, and loading on the stress distribution and deformation. ANSYS/CARES/PDS accounts for the effects of probabilistic strength, probabilistic loads, probabilistic material properties, and probabilistic tolerances on the lifetime and reliability of the component. Even failure probability becomes a <b>stochastic</b> <b>quantity</b> that can be tracked as a response variable. ANSYS/CARES/PDS enables tracking of all stochastic quantities in the design space, thereby enabling more precise probabilistic prediction of lifetimes of ceramic components...|$|E
40|$|This article {{presents}} {{a method to}} characterize stochastic observables defined by induced surface currents and fields in electromagnetic interactions with uncertain configurations. As the covariance operators of the stochastic distributions and fields are not compact, a strict Karhunen-Loeve (KL) approach is not possible. Instead, we apply a point-spectrum regularization by expanding the <b>stochastic</b> <b>quantities</b> on a finite-element-like basis. The coefficients of the KL expansion are approximated analytically in a polynomial-chaos (PC) expansion. The novelty of our approach resides {{in its ability to}} handle multiple PC expansions simultaneously and determine the orders of the KL and PC expansions adaptively. Thismethod is illustrated through the example of the voltage induced at the port of a random thin-wire frame illuminated by random plane waves. The results show the accuracy and computational efficiency of the proposed method, which provides a complete characterization of the randomness of the observable. 3 ̆cbr/ 3 ̆e This {{article presents}} a method to characterize stochastic observables defined by induced surface currents and fields in electromagnetic interactions with uncertain configurations. As the covariance operators of the stochastic distributions and fields are not compact, a strict Karhunen-Loeve (KL) approach is not possible. Instead, we apply a point-spectrum regularization by expanding the <b>stochastic</b> <b>quantities</b> on a finite-element-like basis. The coefficients of the KL expansion are approximated analytically in a polynomial-chaos (PC) expansion. The novelty of our approach resides in its ability to handle multiple PC expansions simultaneously and determine the orders of the KL and PC expansions adaptively. Thismethod is illustrated through the example of the voltage induced at the port of a random thin-wire frame illuminated by random plane waves. The results show the accuracy and computational efficiency of the proposed method, which provides a complete characterization of the randomness of the observable...|$|R
40|$|This paper {{describes}} a methodology {{to evaluate the}} amount of required regulation and load following capability to maintain reliability in the CAISO Control Area. The methodology {{is based on a}} mathematical model of the CAISO 2 ̆ 7 s actual scheduling, real-time dispatch and regulation processes. The forecast load and wind power are <b>stochastic</b> <b>quantities,</b> represented by two series of minute by minute values: the load/wind-power average value and its standard deviation magnitude. The hour-ahead wind generation forecast was assumed {{to be a part of}} the future CAISO/Scheduling Coordinator (SC) scheduling system. CAISO actual 2006 data and simulated 2010 data are analyzed by season. Load following and regulation requirements, including the capacity, ramping, and duration requirements by operating hour within a season of 2006 and 2010 were analyzed simultaneously...|$|R
40|$|Abstract — We {{propose a}} novel scheme for {{detecting}} coded data transmitted over a communication channel that is either partially or entirely unknown. Viewing the unknown channel parameters as <b>stochastic</b> <b>quantities</b> {{drawn from a}} known probability distribution, {{the likelihood of a}} sequence of data is derived using Bayesian techniques. A stack-like tree search algorithm is proposed for implementation of maximum likelihood (ML) sequence detection under the Bayesian metric. We apply the Bayesian scheme to the binary symmetric channel (BSC) with unknown crossover probability. The structure of the resulting metric is compared to both the conventional Fano metric and a universal metric presented in (Lapidoth and Ziv, IEEE Trans. IT 1999). Based on its relationship to the metric developed by Lapidoth and Ziv, the newly-derived metric is shown to be pairwise universal over the ensemble of random uniform codes. I...|$|R
