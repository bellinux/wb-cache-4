16|11|Public
5000|$|... "Environmental Sciences & Pollution Management" [...] {{consists}} of twelve component or <b>sub-file</b> bibliographic databases {{that can be}} accessed independently. These are: ...|$|E
50|$|All files (accounts, dictionaries, files, sub-files) are {{organized}} identically, as are all records. This uniformity is exploited throughout the system, both by system functions, {{and by the}} system administration commands. For example, the 'find' command will find and report the occurrence of a word or phrase in a file, and can operate on any account, dictionary, file or <b>sub-file.</b>|$|E
50|$|Data {{security}} typically goes {{one step}} further than database security and applies control directly to the data element. This {{is often referred to}} as Data-Centric Security. On traditional relational databases, ABAC policies can control access to data at the table, column, field, cell and sub-cell using logical controls with filtering conditions and masking based on attributes. Attributes can be data, user, session or tools based to deliver the greatest level of flexibility in dynamically granting/denying access to a specific data element. On big data, and distributed file systems such as Hadoop, ABAC applied at the data layer control access to folder, sub-folder, file, <b>sub-file</b> and other granular.|$|E
50|$|SolidWorks files (previous to version 2015) use the Microsoft Structured Storage file format. This {{means that}} there are various files {{embedded}} within each SLDDRW (drawing files), SLDPRT (part files), SLDASM (assembly files) file, including preview bitmaps and metadata <b>sub-files.</b> Various third-party tools (see COM Structured Storage) {{can be used to}} extract these <b>sub-files,</b> although the subfiles in many cases use proprietary binary file formats.|$|R
40|$|Multimedia {{confidential}} data uses internet communication channel for transmission. It {{is not very}} safe to transmit data via internet communication channel. So it is desired to secure the data before transmitting. Recently various algorithms have been proposed in this context. But there is problem of wrong reconstruction due to fake player participation. To overcome this problem, the proposed method presents an optimal approach for secret sharing. Proposed approach totally depends upon threshold. A secret image can be split into N small <b>sub-files</b> and combination of any T <b>sub-files</b> the original file can be recovered without errors...|$|R
50|$|Visual Components files use a {{proprietary}} structured file format. This {{means that there}} are various files embedded within each VCM (component model & layout file) and VCP (layout package) file including preview bitmaps, geometry information and metadata <b>sub-files.</b>|$|R
50|$|Groups {{will need}} to {{federate}} with other groups to create scientific dataspaces of regional or national scope. They {{will need to}} easily export their data in standard scientific formats, and at granularities (<b>sub-file</b> or multiple file) that don't necessarily correspond to the partitions they use to store the data. Users of the federated dataspace may want to see collections of data that cut across the groups in the federation, such as all observations and data products related to water velocity, or all data related to a certain stretch of coastline {{for the past two}} months. Such collections may require local copies or additional indices for fast search.|$|E
5000|$|After Respekts report (which {{states that}} Kundera {{did not know}} Dvořáček), Kundera denied turning Dvořáček in to the police, stating {{he did not know}} him at all, and could not even {{recollect}} [...] "Militská". On 14 October 2008, the Czech Security Forces Archive ruled out the possibility that the document could be a fake, but refused to make any interpretation about it. (Vojtech Ripka, of the Institute for the Study of Totalitarian Regimes, said, [...] "There are two pieces of circumstantial evidence police report and its <b>sub-file,</b> but we, of course, cannot be one hundred percent sure. Unless we find all survivors, which is unfortunately impossible, it will not be complete." [...] Ripka added that the signature on the police report matches the name of a man who worked in the corresponding National Security Corps section and that a police protocol is missing.) ...|$|E
50|$|In {{the most}} general definition, Automated Tiered Storage {{is a form}} of Hierarchical Storage Management. However, the term {{automated}} tiered storage has emerged to accommodate newer forms of real-time performance optimized data migration driven by the proliferation of solid state disks and storage class memory. Furthermore, where traditional HSM systems act on files and move data between storage tiers in a batch, scheduled like fashion, automated storage tiered systems are capable of operating at <b>sub-file</b> level both in batch and real-time modes. In the case of the latter, data is moved almost as soon as it enters the storage system or relocated based on its activity levels within seconds of data being accessed, whereas more traditional tiering tends to operate on an hourly, daily or even weekly schedule. Some more background on the relative differences between HSM, ILM and automated tiered storage is available at SNIA web site. A general comparison of different approaches can also be found in this 'comparison article on auto tiered storage'http://searchstorage.techtarget.co.uk/feature/Automated-storage-tiering-product-comparison.|$|E
5000|$|A TaskJuggler project {{consists}} {{of one or}} more plain text documents written in a domain-specific declarative programming language. The documents consist of a root project file and optional <b>sub-files</b> incorporated into the root by means of the [...] keyword (property in TaskJuggler lingo), to one or more levels. The TaskJuggler language reference refers to the project definition source document somewhat loosely as The TJP File. This discussion refers to the project definition source document simply as the TJP.|$|R
40|$|Abstract {{copyright}} UK Data Service {{and data}} collection copyright owner. Main Topics : Variables The results for each general election from 1955 to 1979 form separate <b>sub-files.</b> Each case contains results from the previous election. In addition to number of electors and votes cast each case (constituency) has a regional code and a quartile value based on turnout. Results are also expressed as percentage of electorate voting for party, {{and there is a}} coded variable for size of constituency. Also given are the party of the winner and runner-up...|$|R
40|$|Information {{extraction}} systems {{identify and}} retrieve {{certain types of}} information from natural language text. A recent development {{in the field of}} information extraction is the emergence of ontology-based information extraction as a <b>sub-filed,</b> where ontologies are used to guide the information extraction process and to present the extracted information. One of the challenges faced by fields of ontology-based information extraction and information extraction is the difficulty of reuse of prior work in developing new systems. A component-based approach for information extraction named OBCIE (Ontology-Based Components for Information Extraction) has been previously developed to address this issue. This paper presents the progress in developing an ontology construction component for the OBCIE approach, which identifies classes and relationships for a given domain. It is centered on discovering the information contained within the loose structure of Wikipedia pages...|$|R
30|$|This is <b>sub-file</b> level {{deduplication}} {{where the}} file {{is divided into}} chunks called blocks. Based {{on the size of}} blocks in the chunking process, block-level deduplication is divided into fixed length block-level deduplication and variable length block-level deduplication.|$|E
40|$|AbstractSub-file hashing and {{hash-based}} carving {{are increasingly}} popular methods in digital forensics to detect files on hard drives that are incomplete {{or have been}} partially overwritten/modified. While these techniques {{have been shown to}} be usable in practice and can be implemented efficiently, they face the problem that a-priori specific “target files” need to be available and at hand. While it is always feasible and, in fact, trivial to create case-specific <b>sub-file</b> hash collections, we propose the creation of case-independent <b>sub-file</b> hash databases. For facilitating hash databases which can be publicly shared among investigators, we propose the usage of data from peer-to-peer file sharing networks such as BitTorrent. Most of the file sharing networks in use today rely on large quantities of hash values for integrity checking and chunk identification, and can be leveraged for digital forensics. In this paper we show how these hash values can be of use for identifying possibly vast amounts of data and thus present a feasible solution to cope with the ever-increasing case sizes in digital forensics today. While the methodology used is independent of the used file sharing protocol, we harvested information from the BitTorrent network. In total we collected and analyzed more than 3. 2  billion hash values from 2. 3  million torrent files, and discuss to what extent they can be used to identify otherwise unknown file fragments and data remnants. Using open-source tools like bulk_extractor and hashdb, these hash values can be directly used to enhance the effectiveness of <b>sub-file</b> hashing at scale...|$|E
40|$|Milestone v 0. 5. 4 Fixed Fix {{multiple}} bugs encountered {{when running}} phenology estimates # 49 Changed Metadata from yatsm line runs are now stored in metadata <b>sub-file</b> of NumPy compressed saved files # 53 Algorithm configurations must now declare subsections that match estimator methods (e. g., init and fit) # 52 Refactored yatsm. phenology to make LongTermMeanPhenology estimator follow scikit-learn API # 50 Added Add [...] num_threads option to yatsm CLI. This argument sets various environment variables (e. g., OPENBLAS_NUM_THREADS or MKL_NUM_THREADS) before beginning computation to set or limit multithreaded linear algebra calculations within NumPy # 51 Add a changelog...|$|E
50|$|Engineered Materials Abstracts, {{established}} in 1986, provides in-depth coverage of polymers, ceramics, and composites, including complex and advanced materials. The processes involving these {{materials such as}} research, manufacturing practices, properties and applications are also cited in this database. This electronic database also contains the <b>sub-files</b> named Ceramics, Composites and Polymers. In addition, this database is indexed for more than 3,000 relevant publications of various types, or groupings, which are related to scientific journal content. Dates of coverage span from approximately 1986 to present day. The oldest record in the database has a publication date of 1953. Furthermore, about 50% of its records have publication dates of 1993 or later. This database is updated once per month, and approximately 20,000 new records are added each year. As of June 2010 {{the size of the}} database is more than 796,357 records.|$|R
5000|$|A Pick {{database}} {{is divided}} into one or more accounts, master dictionaries, dictionaries, files and <b>sub-files,</b> {{each of which is}} a hash-table oriented file. These files contain records made up of fields, sub-fields and sub-sub-fields. In Pick, records are called items, fields are called attributes, and sub-fields are called values or sub-values (hence the present-day label [...] "multivalued database"). All elements are variable-length, with field and values marked off by special delimiters, so that any file, record, or field may contain any number of entries of the lower level of entity. As a result, a Pick item (record) can be one complete entity (one entire invoice, purchase order, sales order, etc.), or is like a file on most conventional systems. Entities that are stored as 'files' in other common-place systems (e.g. source programs and text documents) must be stored as records within files on Pick.|$|R
40|$|We {{consider}} {{content delivery}} over fading broadcast channels. A server wants to transmit K files to K users, each {{equipped with a}} cache of finite size. Using the coded caching scheme of Maddah-Ali and Niesen, we design an opportunistic delivery scheme where the long-term sum content delivery rate scales with K the number of users in the system. The proposed delivery scheme combines superposition coding together with appropriate power allocation across <b>sub-files</b> intended to different subsets of users. We analyze the long-term average sum content delivery rate achieved by two special cases of our scheme: a) a selection scheme that chooses the subset of users with the largest weighted rate, and b) a baseline scheme that transmits to K users using the scheme of Maddah-Ali and Niesen. We prove that coded caching with appropriate user selection is scalable since it yields a linear increase of the average sum content delivery rate. Comment: 7 pages, 2 figures, extended version of a paper submitted to ISIT 201...|$|R
40|$|Abstract. In {{order to}} improve the {{objectivity}} of urban rail transit network evaluation and achieve the <b>sub-file</b> of quality level of preliminary planning, a comprehensive evaluation method for urban rail transit network was proposed based on TOPSIS, then the evaluation model of urban rail transit network was constructed. This method was applied to Xi’an urban rail transit network planning. The result shows that case- 4 is best one among the four urban rail transit network planning of Xi’an and consistent with the passenger flow prediction. The evaluation model for urban rail transit network based on TOPSIS is objective, which involves few parameters, simple operation, science evaluation and provides a basis of decision for urban rail transit network planning...|$|E
40|$|Abstract. In {{order to}} improve the {{objectivity}} of urban public transport network evaluation and achieve the <b>sub-file</b> of quality level of preliminary scheme, a comprehensive evaluation method for urban public transport network was proposed based on TOPSIS, and then constructed the evaluation model of urban public transport network. This method was applied to Xi’an urban public transport network planning, which obeyed the basic theory of scheme evaluation and some related requirements. The result showed that planning- 5 was the best one among the five urban public transport network planning in Xi’an and consistent with the passenger flow prediction. The evaluation model for urban public transport network based on TOPSIS was objective, which involved few parameters, simple operation, scientific evaluation and provided a basis for urban public transport network planning decision...|$|E
40|$|Digital {{forensics}} <b>Sub-file</b> forensics a b s t r a c t Over {{the past}} decade, a substantial {{effort has been}} put into developing methods to classify file fragments. Throughout, {{it has been an}} article of faith that data fragments, such as disk blocks, can be attributed to different file types. This work is an attempt to critically examine the underlying assumptions and compare them to empirically collected data. Specifically, we focus most of our effort on surveying several common compressed data formats, and show that the simplistic conceptual framework of prior work is at odds with the realities of actual data. We introduce a new tool, zsniff, which allows us to analyze deflate-encoded data, and we use it to perform an empirical survey of deflate-coded text, images, and executables. The results offer a conceptually new type of classification capa-bilities that cannot be achieved by other means. ª 2013 Vassil Roussev and Candice Quates. Published by Elsevier Ltd. All rights reserved...|$|E
40|$|French {{administrative}} {{data of the}} breeding season 1994 - 2008 concerning gestation lengths of 5 warm-blood breeds: 47789 Thoroughbred gestations, 14336 Arab, 7363 Anglo-Arab, 53832 French Saddle, 130769 French Trotter, and 5 cold-blood breeds: 13528 Percheron's gestations, 7480 for Ardennes, 5181 for Cob Normand, 33335 for Comtois and 36537 for Bretons were analyzed using the ASREML software. After preliminary studies, the model included as fixed effects, year (15), covering month (12), breeding area (10), mare's status (3), sex of the foal (2); age of the mare was accounted for as a linear covariate; direct genetic, maternal genetic, and common maternal environment, were the random variables. The genetic correlation between direct and maternal genetic effects {{was assumed to be}} zero. The different <b>sub-files</b> did not give significantly different results: direct genetic effect was evaluated between 8 and 12 %, maternal genetic effect between 8 and 12 % also and common maternal environment between 5 and 6 %. This confirmed the dissymmetric contribution of sire and dam: 2 - 3 % for the sire and 13 - 18 % for the dam. When direct and maternal effects were confounded they accounted for 16 - 24 %, confirming the possibility of selection for this character...|$|R
40|$|Large api overhaul: (almost) entirely RESTful Old API gone (but not yet deleted {{from the}} repository, needs testing to confirm it isn't being used) Large routes. php file split to <b>sub-files</b> Introduction of app/ {{directory}} Most project files {{are in there}} now, Document root for Apache should be set to the app/ directory now Keeps config files seperated out More consistent with structures of other webapps (but {{still has a long}} way to go) New Music review system Submit music to your station via an embed-able form on your website Volunteers can listen toi and review tracks submitted Tagging the albums as they are submitted Automatic add to the library as submitted music moves through the album review system Administrative functions: manually submit music through DJLand into the review system (for submissions that are still received by snail mail), search archived and rejected submissions, generate lists of newly added music to the library Library bulk-editing: no more tedious single record-editing Centrally managed genres and sub-genres for the Library Genres + subgenres used in the library moved to DB table Song previewing in the Library feature added Previously you could just look up album details Album art added to Library record pages DJLand Scan: import ripped files in bulk to submissions / library record...|$|R
40|$|Aims: To {{study the}} {{relation}} between risk of cerebral palsy and socioeconomic status. Methods: A total of 293 children with a diagnosis of cerebral palsy out of 105 760 live births between 1 January 1982 and 31 December 1997 were identified from the special conditions <b>sub-file</b> of the West Sussex Computerised Child Health System. Results: There was a linear association between risk of cerebral palsy and socioeconomic status (SES) measured by the Registrar General's social class (RGSC) and enumeration district (ED) ranked into quintiles by the Townsend Deprivation Index derived from 1991 census data. Fifty one per cent and 30 % of cases of cerebral palsy were statistically "attributable'' to inequality in SES using the RGSC and ED quintile respectively. A linear association was also noted for singleton live births. The association between risk of cerebral palsy and ED quintile persisted in a logistic regression model that included birth weight and gestational age, although that between RGSC and cerebral palsy no longer reached conventional levels of statistical significance after adjustment. Conclusions: A strong association was observed between socioeconomic status {{and the risk of}} cerebral palsy, which was only partly accounted for by the known social gradients in birth weight and gestational age...|$|E
40|$|With the {{popularity}} {{and expansion of}} Cloud Computing, NoSQL databases (DBs) are becoming the preferred choice of storing data in the Cloud. Because they are highly de-normalized, these DBs tend to store significant amounts of redundant data. Data de-duplication (DD) has {{an important role in}} reducing storage consumption to make it affordable to manage in today’s explosive data growth. Numerous DD methodologies like chunking and, delta encoding are available today to optimize the use of storage. These technologies approach DD at file and/or <b>sub-file</b> level but this approach has never been optimal for NoSQL DBs. This research proposes data De-Duplication in NoSQL Databases (DDNSDB) which makes use of a DD approach at a higher level of abstraction, namely at the DB level. It makes use of the structural information about the data (metadata) exploiting its granularity to identify and remove duplicates. The main goals of this research are: to maximally reduce the amount of duplicates in one type of NoSQL DBs, namely the key-value store, to maximally increase the process performance such that the backup window is marginally affected, and to design with horizontal scaling in mind such that it would run on a Cloud Platform competitively. Additionally, this research presents an analysis of the various types of NoSQL DBs (such as key-value, tabular/columnar, and document DBs) to understand their data model required for the design and implementation of DDNSDB. Primary experiments have demonstrated that DDNSDB can further reduce the NoSQL DB storage space compared with current archiving methods (from 17 % to near 69 % as more structural information is available). Also, by following an optimized adapted MapReduce architecture, DDNSDB proves to have competitive performance advantage in a horizontal scaling cloud environment compared with a vertical scaling environment (from 28. 8 milliseconds to 34. 9 milliseconds as the number of parallel Virtual Machines grows) ...|$|E
40|$|This {{dissertation}} {{introduces the}} Composite Endpoint Protocol (CEP) which solves two related problems: large- scale high performance transfers, and partial content distribution. Achieving high performance in large-scale networks, with speeds above 1 Gbps and latency up to 200 ms, is difficult; individual machines {{can not fully}} exploit overall system capacity, and existing protocols (e. g. TCP) have well-known problems. Similarly, while whole-file content distribution is well studied, when individual clients each desire different parts of a file new techniques are required. The core algorithms and abstractions needed to exploit large scale networks or provide <b>sub-file</b> distribution semantics do not exist. The underlying problem is fundamental: transfer scheduling. Given a set of heterogeneous nodes which have data and nodes which need some subset of that data, perform transfers to best satisfy all nodes' demands. No strong semantics are implied here; subsets of this data may be replicated, missing, not fall on block/word boundaries, etc. The solution is a transfer scheduler which implicitly or explicitly specifies which nodes transfer what data and when. CEP solves the transfer scheduling problem using minimal centralization for metadata/scheduling and infrastructure for fully distributed data transmission. Hybrid centralized/distributed algorithms and heuristics dynamically generate the most desirable transfers as system state evolves. In this way, CEP enables both large- scale high performance transfers and provides rich partial content distribution semantics. The dissertation includes the following contributions. 1. An efficient mechanism for multiple heterogeneous nodes/processes (a composite endpoint) {{to take part in}} a single logical connection, where core algorithms run in O(n log n) for the common case; Simple, flexible interfaces for describing data layouts and composite endpoint communication, backed by a general mathematical abstraction; 3. Multiple transfer scheduling algorithms which produce high performance (over 10 Gbps), high resolution, and when possible provably optimal output, with detailed analysis of each; 4. A scalable and robust composite endpoint architecture which supports tens of thousands of participants and transparently survives server failures. Analysis of the algorithms involved, discuss two implementations of the Composite Endpoint Protocol, as provide an empirical evaluation showing the benefits of CEP under a variety of conditions: over 10 x faster than Apache, BitTorrent, DHTs, or uniform striping technique...|$|E
40|$|ABSTRAC ISCG 2012 Evolution of the Atlas {{data and}} {{computing}} {{model for a}} Tier 2 in the EGI infrastructure During last years the Atlas computing model has moved from a more strict design, where every Tier 2 had a liaison and a network dependence from a Tier 1, to a more meshed approach where every cloud could be connected. Evolution of ATLAS data models requires changes in ATLAS Tier 2 s policy for the data replication, dynamic data caching and remote data access. It also requires rethinking the network infrastructure to enable any Tier 2 and associated Tier 3 to easily connect to any Tier 1 or Tier 2. Tier 2 s {{are becoming more and}} more important in the ATLAS computing model as it allows more data to be readily accessible for analysis jobs to all users, independently of their geographical location. The Tier 2 s disk space has been reserved for real, simulated, calibration and alignment, group, and user data. A buffer disk space is needed for input and output data for simulations jobs. Tier 2 s are going to be used more efficiently. In this way Tier 1 s and Tier 2 s are becoming more equivalent for the network and the Hierarchy of Tier 1, 2 is not longer so important. A number of concepts and challenges are raised in these proposals, and in this contribution we show how these changes affect an Atlas Tier 2 and its co-located Tier 3 that are using the EGI infrastructure. We will present the Tier 2 and Tier 3 facility setup, how do we get the data and the arrangements proposed to fulfil the requirements coming from the new model, like the fact that any site can replicate data from any other site. The approach to Dynamic Data Caching, where analysis sites receive datasets from any other site "on demand" based on usage pattern, and possibly using a dynamic placement of datasets by centrally managed replication of whole datasets, and unused data is removed. Also the Remote Data Access approach, where local jobs could access data stored at remote sites using local caching on a file or <b>sub-file</b> level. We also present how do we enable at the same time grid and local data access for our users, using the EGI infrastructure and procedures, and the middleware glite flavour that is being provided by EMI releases. In this direction an example of a real physics analysis and how the users are working will be presented, to check the readiness of the tools and how they perform with the current and within the changes being adopted coming from the evolution of the model...|$|E

