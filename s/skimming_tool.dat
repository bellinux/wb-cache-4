3|3|Public
50|$|Unlike sieves or strainers, {{which have}} fine mesh screens for {{straining}} away liquids as food is retrieved, the spider {{can be used}} as a strainer for larger pieces of food. However, most often it is used as a <b>skimming</b> <b>tool</b> to add or remove foods from hot liquids such as water or oil. Spiders may be somewhat flat and round or small round spoon-like utensils shaped into the form of an open basket. They may also be referred to as sieves, spoon sieves, spoon skimmers, or basket skimmers.|$|E
40|$|Abstract 1 To achieve more {{efficient}} video indexing and access, we introduce a video content structure and event mining framework. A video shot segmentation and key-frame selection strategy are first utilized to parse the continuous video stream into physical units. Video shot grouping, group merging, and scene clustering schemes are then proposed {{to organize the}} video shots into a hierarchical structure using clustered scenes, scenes, groups, and shots, in increasing granularity from top to bottom. Then, audio and video processing techniques are integrated to mine event information, such as dialog, presentation and clinical operation, among the detected scenes. Finally, the acquired video content structure and events are integrated to construct a scalable video <b>skimming</b> <b>tool</b> {{which can be used}} to visualize the video content hierarchy and event information for efficient access. Experimental results are also presented to evaluate the performance of the proposed algorithms. 1...|$|E
40|$|To achieve more {{efficient}} video indexing and access, we introduce a video database management framework {{and strategies for}} video content structure and events mining. The video shot segmentation and representative frame selection strategy are first utilized to parse the continuous video stream into physical units. Video shot grouping, group merging, and scene clustering schemes are then proposed to organize the video shots into a hierarchical structure using clustered scenes, scenes, groups, and shots, in increasing granularity from top to bottom. Then, audio and video processing techniques are integrated to mine event information, such as dialog, presentation and clinical operation, from the detected scenes. Finally, the acquired video content structure and events are integrated to construct a scalable video <b>skimming</b> <b>tool</b> {{which can be used}} to visualize the video content hierarchy and event information for efficient access. Experimental results are also presented to evaluate the performance of the proposed framework and algorithms...|$|E
40|$|We {{present a}} user-centred, task-oriented, {{comparative}} evaluation of two query-based document <b>skimming</b> <b>tools.</b> ProfileSkim bases within-document retrieval on computing a relevance profile for a document and query; FindSkim provides similar functionality {{to the web}} browser Find-command. A novel simulated work task was devised, where experiment participants are asked to identify (index) relevant pages of an electronic book, given subjects from the existing book index. This subject index provides the ground truth, against which the indexing results can be compared. Our major hypothesis was confirmed, namely ProfileSkim proved significantly more efficient than Find-Skim, as measured by time for task. Moreover, indexing task effectiveness, measured by typical IR measures, demonstrated that ProfileSkim was better than FindSkim in identifying relevant pages, although not significantly so. The experiments confirm the potential of relevance profiling to improve query-based document skimming, which should prove highly beneficial for users trying to identify relevant information within long documents...|$|R
40|$|As more {{audio and}} video {{technical}} presentations go online, it becomes imperative to give users effective summarization and <b>skimming</b> <b>tools</b> {{so that they can}} find the presentation they want and browse through it quickly. In a previous study, we reported three automated methods for generating audio-video summaries and a user evaluation of those methods. An open question remained about how well various text/image only techniques will compare to the audio-video summarizations. This study attempts to fill that gap. This paper reports a user study that compares four possible ways of allowing a user to skim a presentation: 1) PowerPoint slides used by the speaker during the presentation, 2) the text transcript created by professional transcribers from the presentation, 3) the transcript with important points highlighted by the speaker, and 4) a audio-video summary created by the speaker. Results show that although some text-only conditions can match the audio-video summary, users have a marginal preference for audio-video (ANOVA f= 3. 067, p= 0. 087). Furthermore, different styles of slide-authoring (e. g., detailed vs. big-points only) can have a big impact on their effectiveness as summaries, raising a dilemma for some speakers in authoring for on-demand previewing versus that for live audiences...|$|R
40|$|Background. Unlike full reading, ‘skim-reading’ {{involves}} {{the process of}} looking quickly over information {{in an attempt to}} cover more material whilst still being able to retain a superficial view of the underlying content. Within this work, we specifically emulate this natural human activity by providing a dynamic graph-based view of entities automatically extracted from text. For the extraction, we use shallow parsing, co-occurrence analysis and semantic similarity computation techniques. Our main motivation is to assist biomedical researchers and clinicians in coping with increasingly large amounts of potentially relevant articles that are being published ongoingly in life sciences. Methods. To construct the high-level network overview of articles, we extract weighted binary statements from the text. We consider two types of these statements, co-occurrence and similarity, both organised in the same distributional representation (i. e., in a vector-space model). For the co-occurrence weights, we use point-wise mutual information that indicates the degree of non-random association between two co-occurring entities. For computing the similarity statement weights, we use cosine distance based on the relevant co-occurrence vectors. These statements are used to build fuzzy indices of terms, statements and provenance article identifiers, which support fuzzy querying and subsequent result ranking. These indexing and querying processes are then used to construct a graph-based interface for searching and browsing entity networks extracted from articles, as well as articles relevant to the networks being browsed. Last but not least, we describe a methodology for automated experimental evaluation of the presented approach. The method uses formal comparison of the graphs generated by our tool to relevant gold standards based on manually curated PubMed, TREC challenge and MeSH data. Results. We provide a web-based prototype (called ‘SKIMMR’) that generates a network of inter-related entities from a set of documents which a user may explore through our interface. When a particular area of the entity network looks interesting to a user, the tool displays the documents that are the most relevant to those entities of interest currently shown in the network. We present this as a methodology for browsing a collection of research articles. To illustrate the practical applicability of SKIMMR, we present examples of its use in the domains of Spinal Muscular Atrophy and Parkinson’s Disease. Finally, we report on the results of experimental evaluation using the two domains and one additional dataset based on the TREC challenge. The results show how the presented method for machine-aided <b>skim</b> reading outperforms <b>tools</b> like PubMed regarding focused browsing and informativeness of the browsing context...|$|R

