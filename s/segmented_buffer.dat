0|47|Public
40|$|In this study, the Agricultural Nonpoint Source Pollution Model (AGNPS) {{was used}} to {{determine}} locations of vegetative buffer strip effectiveness on reducing sediment load within the East Bad Creek (EBC) watershed, a 690 ha agricultural watershed located mid Michigan. Modeling scenarios consisted of simulating the hydrology and sediment transport throughout the EBC watershed on a baseline scenario (no buffer) and with a 30 -meter vegetative buffer strip placed around each stream <b>segment</b> (<b>buffer</b> strip scenario). The model’s results showed a 17 % decrease in sediment load at the watershed’s outlet for a 10 yr- 24 hr storm. As a result, the placement of buffer strips within the watershed was prioritized on three different scales. The reduction of sediment due to buffer strips was analyzed on a stream segment level, a field boundary level, and on a cell-by cell basis. The stream <b>segment</b> <b>buffers</b> and field buffers were ranked on their overall ability to reduce sediment load into the stream. The reduction in sediment yield from the stream segments and the fields varied from 3. 49 to 58. 54 tons and 0 to 19. 31 tons respectively. The cell results were evaluated by highlighting 0. 5 tons – 3. 63 tons of sediment throughout the watershed, deeming those buffered cells efficient. The cell-by-cell evaluations highlighted specific critical areas of buffer efficiency on a 30 -meter resolution where the stream segment and field evaluation...|$|R
5000|$|Cross Drain - {{to predict}} {{sediment}} yield from a road <b>segment</b> across a <b>buffer</b> ...|$|R
40|$|An {{interconnect}} {{joining a}} source and a sink {{is divided into}} fixed-length uniform-width wire segments, and some adjacent <b>segments</b> have <b>buffers</b> in between. The problem we considered is to simultaneously size the <b>buffers</b> and the <b>segments</b> so that the Elmore delay from the source to the sink is minimized. Previously, no polynomial time algorithm for {{the problem has been}} reported in literature. In this paper, we present a polynomial time algorithm SBWS for the simultaneous buffer and wire sizing problem. SBWS is an iterative algorithm with guaranteed convergence to the optimal solution. It runs in quadratic time and uses constant memory for computation. Also, experimental results show that SBWS is extremely efcient in practice. For example, for an interconnect of 10000 <b>segments</b> and <b>buffers,</b> the CPU time is only 0. 127 second...|$|R
40|$|This paper {{presents}} the design, simulation and performance {{evaluation of a}} novel reordering write buffer for Log-structured File Systems (LFS). While LFS provides good write performance for small files, its biggest problem is the high overhead from cleaning. Previous research concentrated on improving the cleaner's efficiency after files are written to the disk. We propose a new method that reduces {{the amount of work}} the cleaner has to do before the data reaches the disk. Our design sorts active and inactive data in memory into different <b>segment</b> <b>buffers</b> and then writes them to different disk segments. This approach forces data on the disk into a bimodal distribution. Most data in active segments are quickly invalidated, while inactive segments are mostly intact. Simulation results based on both real-world and synthetic traces show that such a reordering write buffer dramatically reduces the cleaning overhead, slashing the system's overall write cost by up to 53 %...|$|R
5000|$|A <b>buffer</b> <b>segment</b> {{in between}} {{commercial}} breaks {{that is a}} tape of a message left by listeners who are [...] "drunk, high, pissed off, crazy and/or have a great accent" [...] This segment {{seemed to have been}} picked up by The Adam Carolla Show on the radio.|$|R
40|$|This letter {{reports a}} {{multiplexed}} optical displacement sensor using a thin core fiber (TCF) exciter. The TCF exciter {{is followed by}} a stripped single mode optical fiber. A small section of buffer is used as the movable component along the single mode fiber. Ultra-weak cladding mode reflection (3 ̆c − 75 dB) was employed to probe the refractive index discontinuity between the air and buffer coating boundary. The position change of the movable <b>buffer</b> <b>segment</b> results in a delay change of the cladding mode reflection. Thus, it {{is a measure of the}} displacement of the <b>buffer</b> <b>segment</b> with respect to the glass fiber. The insertion loss of one sensor was measured to be less than 3 dB. A linear relationship was evaluated between the measurement position and absolute position of the moving actuator. Multiplexed capability was demonstrated and no cross talk was found between the sensors...|$|R
5000|$|The simple naive [...] "one large sieving array" [...] sieves of any {{of these}} sieve types take memory space of about , which means that 1) they are very limited in the sieving ranges they can handle to the amount of RAM memory {{available}} and 2) that they are typically quite slow since RAM memory access speed typically becomes the speed bottleneck more than computational speed once the array size grows beyond the size of the CPU caches. The normally implemented page segmented sieves of both Eratosthenes and Atkin take space [...] plus small sieve <b>segment</b> <b>buffers</b> which are normally sized to fit within the CPU cache sizes; page segmented wheel sieves including special variations of the Sieve of Eratosthenes typically take much more space than this by a significant factor in order to store the required wheel representations; Pritchard's variation of the linear time complexity sieve of Eratosthenes/wheel sieve takes [...] space. The better time complexity special version of the Sieve of Atkin takes space [...] Sorenson shows an improvement to the wheel sieve that takes even less space at [...] for any [...] However, the following is a general observation: the more the amount of memory is reduced, the greater the constant factor increase in the cost in time per operation even though the asymptotic time complexity may remain the same, meaning that the memory-reduced versions may run many times slower than the non-memory-reduced versions by quite a large factor.|$|R
50|$|Some modern {{designs are}} still {{developed}} {{to operate in}} a non-linear fashion. IBM's 3xxx formats are designed to keep the tape moving irrespective of the data buffer—segments are written when data is available, but gaps are written when buffers run empty. When the drive detects an idle period, it re-reads the fragmented <b>segments</b> into a <b>buffer</b> and writes them back over the fragmented sections—a 'virtual backhitch'.|$|R
40|$|A {{large-scale}} full-parallax computer-generated hologram (CGH) {{with four}} billion (216 × 216) pixels is created to reconstruct a fine true 3 D {{image of a}} scene, with occlusions. The polygon-based method numerically generates the object field of a surface object, whose shape is provided {{by a set of}} vertex data of polygonal facets, while the silhouette method makes it possible to reconstruct the occluded scene. A novel technique using the <b>segmented</b> frame <b>buffer</b> is presented for handling and propagating large wave fields even in the case where the whole wave field cannot be stored in memory. We demonstrate that the fullparallax CGH, calculated by the proposed method and fabricated by a laser lithography system, reconstructs a fine 3 D image accompanied by a strong sensation of depth...|$|R
40|$|The new {{experimental}} {{transport protocol}} (NTCP) has been proposed. It can provide higher efficiency of channel usage. It minimizes RTT and excludes buffer overflows. This protocol has three advantages: 1. A local retransmission {{in case of}} a <b>segment</b> loss. 2. <b>Buffer</b> status information along the pass is transmitted in every response, giving a full picture of buffer filling. This excludes any buffer overflow, as sender can easily forecast the situation with buffer. 3. At overloading recovery is much faster, as status information is sent upstream immediately from the overloaded node. 1...|$|R
40|$|Abstract—In this paper, we {{consider}} the problem of interconnect delay minimization by simultaneous buffer and wire sizing under the Elmore delay model. We first present a polynomial time algorithm SBWS to minimize the delay of an interconnect wire. Previously, no polynomial time algorithm for {{the problem has been}} reported in the literature. SBWS is an iterative algorithm with guaranteed convergence to the optimal solution. It runs in quadratic time and uses constant memory for computation. Experimental results show that SBWS is extremely efficient in practice. For example, for an interconnect of 10 000 <b>segments</b> and <b>buffers,</b> the CPU time is only 0. 255 s. We then extend our result to handle interconnect trees. We present an algorithm SBWS-T which always gives the optimal solution. Experimental results show that SBWS-T is faster than the greedy wire sizing algorithm [2] in practice. Index Terms — Buffer sizing, interconnect, performance optimization, physical design, wire sizing...|$|R
40|$|Aged pea Pisum sativum L. var Alaska {{epicotyl}} tissue {{was wounded}} by excising the apical 10 or 20 millimeters and incubating the excised <b>segments</b> upright in <b>buffer.</b> Wounding induced a very rapid formation of polysomes which {{was accompanied by}} minor increases in ribosomes, mRNA, and poly(A) and by a doubling of the in vivo protein synthesizing capacity. This increase in protein synthesis in vivo was matched by a similar increase in polypeptide synthesis in vitro in wheat germ reactions primed by polysomes. However, in vitro reactions primed by total and polysomal RNA from wounded tissue were affected much less...|$|R
50|$|Other {{resources}} are mostly memory and various in-kernel {{objects such as}} Inter-process communication shared memory <b>segments</b> and network <b>buffers.</b> Each resource {{can be seen from}} /proc/user_beancounters and has five values associated with it: current usage, maximum usage (for the lifetime of a container), barrier, limit, and fail counter. The meaning of barrier and limit is parameter-dependent; in short, those {{can be thought of as}} a soft limit and a hard limit. If any resource hits the limit, the fail counter for it is increased. This allows the owner to detect problems by monitoring /proc/user_beancounters in the container.|$|R
40|$|A {{networking}} module that implemented reassembly of segmented packets was designed. The module {{can accept}} up to 1024 flows on an Internet Protocol (IP) network. Packet <b>segments</b> were <b>buffered</b> and queued for each flow until the segment was complete, {{at which time}} the reassembled packet was transmitted. The hardware implementation of this circuit was coded using VHSIC Hardware Description Language (VHDL) and the design was targeted for the Field Programmable Port Extender (FPX) with a target speed of 100 MHz. This implementation had a critical path of 16. 137 ns, which yields a maximum clock frequency of 61. 969 MHz. 2. Project Description For this CS 535 Final Project, a networking module that implemented reassembly of segmented packets from multiple flows on an IP network was designed. The module was designed to support traffic on up to 1024 different IP flows. Furthermore, this module was responsible for queuing packet segments and storing them until complete packets could be assembled. After this was completed, the entire packet was then transmitted along the link. Packet segments were queued using SDRAM, where separat...|$|R
40|$|Wireplanning is an {{approach}} {{in which the}} timing of inputoutput paths is planned before modules are specified, synthesized or sized. If these global wires are optimally <b>segmented</b> and <b>buffered,</b> their delay is linear in the path length and independent {{of the position of}} the modules along these paths. From timing requirements, the total budget left to modules after allocating the appropriate delay to the wires can be determined. This paper describes how this budget can be optimally divided amongst the modules. A novel, static timing-like, mathematical programming formulation is introduced such that the total module area is minimized. Instead of only the worst delay, all pin-to-pin delays are implicitly taken into account. If area-delay tradeo#s are convex, a reasonable approximation in practice, the program can be solved efficiently. Further, efficiency of different formulations is discussed, and a low-cost method of making the budget relatively immune to downstream uncertainties and surprises is presented. The efficiency of the formulation is clear from benchmarks with over 2000 nodes and 5 e 19 paths...|$|R
40|$|Los Angeles is a vast, dense, and notorious {{city that}} overshadows the individualities of its {{outlying}} territories. California is likewise divided between urban center and middle land, with inland acting as producer and collector, and coast as consumer. However, {{there is the}} potential in this middle zone, stuck between the urban and rural, to re-imagine the way that cities develop and function based on infrastructural opportunities. North of Los Angeles over the San Gabriel mountains, Palmdale, Victorville, and Bakersfield operate together as the production and logistics staging grounds for Los Angeles, a collective back of house to the largest city on the west coast. Of these, Palmdale is used as the testing ground for infrastructural opportunism and edge expansion; but while Palmdale acts as producer, staging ground, and dormitory for Los Angeles, {{it will not be}} defined by this adjacency. Instead, Palmdale and its neighbors are re-imagined as a collective of edge cities that signify a new region both in service of and independent from Los Angeles: The High Desert Triangle. To address the edge region, this thesis proposes a new typology for expansion that identifies infrastructural overlaps between road, rail, and water as opportunities to link across fragmented city fabric. This method of aggregation and stitching operates at an urban scale within Palmdale, a territorial scale between cities, and site-specifically in bridging the scalar gap between humans and logistics. By operating opportunistically with infrastructure, this thesis proposes that 1] concentrating infrastructure and logistics development at multi-modal intersections reduces redundancy and de-fragments city fabric, 2] demographic segmentation can be altered by mixing communities and improving access to transit both locally and regionally, and 3] the cost efficiency of bundling infrastructures allows for iteration and experimentation at the architectural scale to address changing programmatic and demographic needs. The aim of this thesis is not to imitate existing city fabric, but instead to design the typological tools for urban edge development and re-imagine how essential logistics spaces can be integrated with living spaces. It does not propose to <b>segment,</b> <b>buffer,</b> or zone out the overlaps between logistics and people, but rather seeks out those intersections as infrastructural opportunities with inherent value. by Laura Williams. Thesis: S. M., Massachusetts Institute of Technology, Department of Architecture, 2016. Cataloged from PDF version of thesis. Includes bibliographical references (pages 164 - 165) ...|$|R
40|$|This thesis {{considers}} {{the analysis and}} study of short and long-term traffic patterns of heterogeneous networks. A large number of traffic profiles from different locations and network environments have been determined. The result of the analysis of these patterns {{has led to a}} new parameter, namely the 'application signature'. It was found that these signatures manifest themselves in various granularities over time, and are usually unique to an application, permanent virtual circuit (PVC), user or service. The differentiation of the application signatures into different categories creates a foundation for short and long-term management of networks. The thesis therefore looks from the micro and macro perspective on traffic management, covering both aspects. The long-term traffic patterns have been used to develop a novel methodology for network planning and design. As the size and complexity of interconnected systems grow steadily, usually covering different time zones, geographical and political areas, a new methodology has been developed as part of this thesis. A part of the methodology is a new overbooking mechanism, which stands in contrast to existing overbooking methods created by companies like Bell Labs. The new overbooking provides companies with cheaper network design and higher average throughput. In addition, new requirements like risk factors have been incorporated into the methodology, which lay historically outside the design process. A large network service provider has implemented the overbooking mechanism into their network planning process, enabling practical evaluation. The other aspect of the thesis looks at short-term traffic patterns, to analyse how congestion can be controlled. Reoccurring short-term traffic patterns, the application signatures, have been used for this research to develop the "packet train model" further. Through this research a new congestion control mechanism was created to investigate how the application signatures and the "extended packet train model" could be used. To validate the results, a software simulation has been written that executes the proprietary congestion mechanism and the new mechanism for comparison. Application signatures for the TCP/IP protocols have been applied in the simulation and the results are displayed and discussed in the thesis. The findings show the effects that frame relay congestion control mechanisms have on TCP/IP, where the re-sending of <b>segments,</b> <b>buffer</b> allocation, delay and throughput are compared. The results prove that application signatures can be used effectively to enhance existing congestion control mechanisms. AT&T (UK) Ltd, Englan...|$|R
40|$|Proceedings of the 2003 Georgia Water Resources Conference, held April 23 - 24, 2003, at the University of Georgia. The {{objective}} of this project is to identify and rank riparian buffers in the ten-mile corridor between Rockdale County’s drinking water intake and the Black Shoals water supply reservoir. Rockdale County is considering several methods for protection including acquisition. The method used to rank the riparian buffer zone include surveying the buffer zone for ecological value and development risk, creating a database of landowners, and creating a GIS of survey results and property parcels. A list of the highest priority <b>buffer</b> <b>segments</b> was developed for Rockdale County to consider. An educational brochure outlining the importance of protecting natural buffers was also produced...|$|R
30|$|In many {{high-speed}} lines, {{especially in}} the process industries, {{it may not be}} possible to store in-process material, because buffers may hurt the quality of the material by allowing it to deteriorate over time (Liberopoulos et al. 2007). For these reasons, high-speed lines generally do not have buffers between workstations. Dogan and Altiok (1998) mentioned that in a pharmaceutical transfer line with several workstations in series connected with conveyor <b>segments</b> and <b>buffers</b> between workstations, when a workstation fails, the part in it is scrapped. Chen and Yuan (2004) presented a method to calculate and estimate the performance indices such as the mean and variance of the transient throughput and the probability that the total outputs will meet the demand on time for a series of unreliable machines with the same production rate and without intermediate buffers. Raissi and Gatmiry (2012) addressed a systematic method for measuring multi-stage service reliability function using failure rate analysis beside a systematic Six Sigma approach to improve total system reliability. Jin et al. (2011) proposed a Six Sigma based framework to deploy high product reliability commitment in distributed subcontractor manufacturing processes. Dhouib et al. (2008) studied the steady-state availability and throughput of production lines without buffers composed of several serial machines subject to random operation-dependent failures.|$|R
40|$|As {{the number}} of mobile devices increases, so do the {{complexity}} of wireless networks and the user's requirements. This tendency makes necessary for Multimedia Services to take the needed actions {{to adapt to the}} upcoming technology. A prominent example of this type of services is HTTP Adaptive Video Streaming Applications. In this research, we have studied how the latest HTTP Adaptive Streaming techniques, mainly developed for standard computers, could be adapted and used in mobile wireless devices. Furthermore, inspired by these solutions, which usually make use of Reinforcement Learning (RL) algorithms to find the suitable streaming rate, we have conceived a novel smart video player client in Java for Android platform using the Dynamic Adaptive Streaming over HTTP (DASH) protocol. We have assessed the performance of our proposed solution in a self-developed wireless test-bed under different network conditions. Thus, we have seen that by including in the reward function contributions regarding the download speed of the video segments, especially needed due to the fluctuating nature of the wireless networks, and the <b>segments</b> already <b>buffered,</b> improves drastically the overall performance of the video client. Besides that, we have discovered that, in a cognitive adaptive approach, bandwidth constraints affect the user's experience more substantially, while impairments such as packet loss can be prevented...|$|R
40|$|Improvements in Light Detection and Ranging (LiDAR) {{technology}} and spatial analysis of high-resolution digital elevation models (DEMs) have advanced the accuracy {{and diversity of}} applications for coastal hazards and natural resources management. This article presents a concise synthesis of LiDAR analysis for coastal flooding and management applications in low-relief coastal plains and a case study demonstration of a new, efficient drainage mapping algorithm. The impetus for these LiDAR applications follows historic flooding from Hurricane Floyd in 1999, after which the State of North Carolina and the Federal Emergency Management Agency (FEMA) undertook extensive LiDAR data acquisition and technological developments for high-resolution floodplain mapping. An efficient algorithm is outlined for hydro-conditioning bare earth (BE) LiDAR DEMs using available US Geological Survey 1 National Hydrography Dataset (NHD) canal and ditch vectors. The methodology is illustrated in Moyock, North Carolina, for refinement of hydro-conditioning by combining pre-existing BE DEMs with spatial analysis of LiDAR point clouds in <b>segmented</b> and <b>buffered</b> ditch and canal networks. The methodology produces improved maps of fine-scale drainage, reduced omission of areal flood inundation, and subwatershed delineations that typify heavily ditched and canalled drainage areas. These preliminary results illustrate the capability of the technique to improve the representation of ditches in DEMs as well as subsequent flow and inundation modeling that could spur further research on low-relief coastal LiDAR applications. ECU Open Access Publishing Support Fun...|$|R
40|$|For Internet based video {{broadcasting}} {{applications such}} as IPTV, the peer-to-peer (P 2 P) streaming scheme {{has been found to}} be an effective solution. An important issue in live broadcasting is to avoid playback buffer underflow. How to utilize the playback buffer and upload bandwidth of peers to minimize the freeze-ups in playback, is the problem we try to solve. In this work, we propose a successive water-filling (SWaF) algorithm for the video transmission scheduling in P 2 P live streaming system, to minimize the playback freeze-ups among peers. SWaF algorithm only needs each peer to optimally transmit (within its uploading bandwidth) part of its available video <b>segments</b> in the <b>buffer</b> to other peers requiring the content and pass small amount message to some other peers. Moreover, SWaF has low complexity and provable optimality. Numerical results demonstrated the effectiveness of the proposed algorithm. Department of ComputingRefereed conference pape...|$|R
40|$|NTRODUCTION: Colonic {{bacteria}} produce short-chain {{fatty acids}} (SCFAs) by fermentation of dietary carbohydrates and fiber. The production of SCFAs is greatest in proximal colon where propulsion {{is likely to}} be highly dependent on chemical/nutrient stimuli. Unabsorbed SCFAs entering the distal colon are likely to modify peristalsis initiated by fecal pellet-induced distension. AIM: To determine the effect of individual SCFAs on propulsive contractions in guinea pig proximal colon and on pellet propulsion in distal colon. METHODS: Proximal colon was excised, cannulated and placed in Krebs buffer in an organ bath. After equilibration, the colon was distended with 1 ml of Krebs buffer alone or containing sodium salts of acetate, butyrate, or propionate at 10 - 100 mM. Motility was video recorded, spatiotemporal maps generated, and the number of full-length propulsive contractions during a 5 min period was determined. The distal colon was removed from guinea pig and placed in an organ bath containing Krebs buffer. Following equilibration, video-tracking software was used to measure the velocity of propulsion of a clay pellet placed in the orad end of a <b>segment.</b> Krebs <b>buffer</b> alone or containing individual SCFAs at 30 mM was perfused caudad to the pellet at 0. 1 ml/min. RESULTS: The basal rate of propulsive contractions in Krebs buffer was 2. 9 ± 0. 7 per 5 min. Butyrate and propionate had concentration-dependent and opposing effects on propulsive contractions; acetate had no effect. Butyrate significantly increased propulsive contractions (maximal increase of 207...|$|R
40|$|HTTP Adaptive Streaming (HAS) is {{becoming}} the de-facto standard for Over-The-Top video streaming services. Video content is temporally split into segments which are offered at multiple qualities to the clients. These clients autonomously select the quality layer matching {{the current state of}} the network through a quality selection heuristic. Recently, academia and industry have begun evaluating the feasibility of adopting layered video coding for HAS. Instead of downloading one file for a certain quality level, scalable video streaming requires downloading several interdependent layers to obtain the same quality. This implies that the base layer is always downloaded and is available for playout, even when throughput fluctuates and enhancement layers can not be downloaded in time. This layered video approach can help in providing better service quality assurance for video streaming. However, adopting scalable video coding for HAS also leads to other issues, since requesting multiple files over HTTP leads to an increased impact of the end-to-end delay and thus on the service provided to the client. This is even worse in a Live TV scenario where the drift on the live signal should be minimized, requiring smaller <b>segment</b> and <b>buffer</b> sizes. In this paper, we characterize the impact of delay on several measurement-based heuristics. Furthermore, we propose several ways to overcome the end-to-end delay issues, such as parallel and pipelined downloading of segment layers, to provide a higher quality for the video service...|$|R
40|$|We {{present a}} study of the {{performance}} of Adaptive HTTP Streaming over different access networks. Adaptive HTTP Streaming {{is on the verge of}} becoming the de-facto standard for multimedia streaming over the Internet and standards exist for its implementation. However, important aspects like media bitrate, <b>segment</b> duration and <b>buffer</b> requirements are not specified. We simulated characteristical HTTP streaming sessions over typical networks like DSL, WLAN and UMTS in order to classify the impact of these aspects on the overall performance of a streaming system. The measurements have shown a frequency of late segments between 7. 88 % for cellular networks and 17. 72 % for wireless LAN connections with more than 50 % of the segments still arriving in less than 50 % of the respective segment duration. The influence of different segment durations was not significant. The results prove the high variability of these access technologies and thus the immediate need for adaptivity in an HTTP streaming system...|$|R
40|$|It is true {{that some}} {{conventional}} algorithms do not fit in Continuous Media (CM) environments because of their different properties. A typical example is the buffer replacement algorithmssuch as least recently used (LRU) and most recently used (MRU) strategies. These replacement policies {{are based on the}} referencing history for replacing buffered page with lower priority because it is not always possible to predict when a buffered page will be re-referred in a traditional DBMS. In contrast, since a CM storage system accesses CM segments sequentially, it can replace <b>buffered</b> <b>segments</b> which will not be re-referred in the near future. The idea of far-future-using (FFU) buffer replacement algorithm is to share the buffer for concurrent transactions and to keep segments in the order of re-referencing plan. In this paper, we analize FFU buffer replacement algorithm for CM playback in terms of buffer hit ratio and required buffer size in comparison with LRU and MRU. 1 Introduction The chara [...] ...|$|R
40|$|Although {{feedback}} loops {{are essential}} in development, their molecular implementation and precise functions remain elusive. Using enhancer knockout in mice, we demonstrate that a direct, positive autoregulatory loop amplifies and maintains {{the expression of}} Krox 20, a transcription factor governing vertebrate hindbrain segmentation. By combining quantitative data collected in the zebrafishwith biophysical modelling that accounts for the intrinsic stochastic molecular dynamics, we dissect the loop at the molecular level. We find that it underpins a bistable switch that turns a transient input signal into cell fate commitment, as we observe in single cell analyses. The stochasticity of the activation process leads to a graded input–output response until saturation is reached. Consequently, the duration and strength of the input signal controls {{the size of the}} hindbrain segments by modulating the distribution between the two cell fates. Moreover, <b>segment</b> formation is <b>buffered</b> from severe variations in input level. Finally, the progressive extinction of Krox 20 expression involves a destabilization of the loop by repressor molecules. These mechanism...|$|R
40|$|In 1981 and 1982 the SWOV Institute for Road Safety Research {{has carried}} out final tests with the RIMOB (Impact {{attenuator}} equipped with crumpling tubes) crash cushion. In those years {{there were no}} test conditions for crash cushions available. In accordance with some experiences in the United States, the following relevant tests were chosen: (1) central impacts; (2) frontal off set impacts; and (3) side impacts. In this report the `old' tests and results are redescribed similar to the CEN/TC 226 /WG 1 standard. Owing to differences between RIMOB and CEN test conditions the given results are not directly comparable. However, there is much agreement. The RIMOB is developed for passenger cars with a mass of approximately 900 kg. passenger cars with a mass of approximately 900 kg. An integrated <b>buffer</b> <b>segment</b> provides for well-functioning with vehicles with a higher mass. Collisions with vehicles with a lower mass give higher values for the Acceleration Severity Index (ASI criterium) as described in this report. (A...|$|R
40|$|In deep {{submicron}} fabrication technology, transistors can now switch much faster, but wire resistances are now larger, {{and delay}} due to wires can exceed gate delay. Consequently, the interconnect delay {{is the dominant}} factor {{in the construction of}} wire routing in very large scale integrated (VLSI) circuits, which today, has feature dimensions in the nanometer range. Today, the state-of-the-art circuit design involves as much the engineering of the wires as the design of transistors. Hence, a successful VLSI design today depends heavily on a successful interconnect design. An effective approach for reducing the interconnect delay is buffer insertion (van Ginneken, 1990). In this method, a wire is divided into <b>segments</b> with a <b>buffer</b> inserted between the segments (Cong et al., 1996). Traditionally, buffer insertion is a post-layout optimization technique, implying that the routing paths are first found, and then buffers are inserted in these paths. However, today?s VLSI designs typically apply some form of design reuse utilizing pre-designed cells, or macro blocks...|$|R
5000|$|The page {{segmented}} version {{implemented by}} the authors has the same O(N) operations but reduces the memory requirement to just that required by the base primes below the square root {{of the range of}} O(N1/2/log N) bits of memory plus a minimal page buffer. This is slightly better performance with the same memory requirement as the page segmented sieve of Eratosthenes which uses O(N log log N) operations and the same O(N1/2/log N) bits of memory plus a minimal page buffer. However, such a sieve does not outperform a Sieve of Eratosthenes with maximum practical wheel factorization (a combination of a 2/3/5/7 sieving wheel and pre-culling composites in the <b>segment</b> page <b>buffers</b> using a 2/3/5/7/11/13/17/19 pattern), which although it has slightly more operations than the Sieve of Atkin for very large but practical ranges, has a constant factor of less complexity per operation by about three times in comparing the per operation time between the algorithms implemented by Bernstein in CPU clock cycles per operation. The main problem with the Page Segmented Sieve of Atkin is the difficulty in implementing the [...] "prime square free" [...] culling sequences due to the span between culls rapidly growing far beyond the page buffer span; the time expended for this operation in Bernstein's implementation rapidly grows to many times the time expended in the actual quadratic equation calculations, meaning that the linear complexity of the part that would otherwise be quite negligible becomes a major consumer of execution time. Thus, even though an optimized implementation may again settle to a O(n) time complexity, this constant factor of increased time per operations means that the Sieve of Atkin is slower.|$|R
40|$|Video-on-demand (VOD) servers {{need to be}} {{efficiently}} {{designed in}} order to support a large number of users viewing the same or different videos at different rates. While considering a disk-array based VOD server, use of a shared buffer at the server end may be more economical than the sole use of dedicated buffers at each user's end. In this paper, we propose a simple buffer sharing architecture that may be used when disk-array based video servers are used. Our aim is to support the maximum number of users for a given number of video server disks while employing a simple scheme requiring less buffer space. The number of video segment retrievals that can occur within a certain time (the service round) is maximum when the scan disk scheduling algorithm is used. Consequently, we shall assume use of the scan algorithm for disk retrieval. The VOD server has a buffer manager that directs retrieved <b>segments</b> to appropriate <b>buffer</b> locations depending on their release and deadlines. The release a [...] ...|$|R
40|$|The Fibre Channel Standard (FCS) is an {{emerging}} standard for providing a common efficient transport vehicle for existing channel protocols like the Intelligent Peripheral Interface (IPI), Small Computer System Interface (SCSI) and High Performance Parallel Interface (HIPPI). This paper {{deals with the}} interconnection issue of FCS-based workstations through an ATM switch. In particular, {{the design of a}} high throughput FCS/ATM InterWorking Unit (IWU) is presented. A specialized segmenting and reassembly protocol, named FAP, has been defined to guarantee the transparency of the end-to-end FCS protocols. The hardware and software characteristics of the IWU are described and its performance evaluated by a simulation study. Performance evaluation has focused on FCS flow control functions with the twofold aim to determine the impact of IWU on the protocol parameters (e. g. timeouts, window size, etc.) and to assume reliable data for a suitable dimensioning criteria for IWU elements (e. g. <b>segmenting</b> and reassembly <b>buffers).</b> Results obtained show that throughput values approaching the available ATM link bandwidth can be achieved depending on the discipline utilized to manage the IWU buffers...|$|R
40|$|Abstract—In recent {{fabrication}} technologies, buffered clock distribution networks {{have become increasingly}} popular due to increasing on-chip wiring delays. Traditionally, clock distribution networks has been optimized to minimize end-to-end skew of the distribution network. However, since most ICs have an on-chip PLL, we argue that the design goal of minimizing end-to-end jitter is more relevant. In this paper, we present a dynamic programming based approach to synthesize a minimum cost buffered H-tree clock distribution network. Our cost functions are a weighted sum of power and jitter, and a weighted sum of power and end-to-end delay of the distribution network. Our approach is based on pre-characterizing the delay, jitter and power of <b>buffered</b> <b>segments</b> of different lengths, topologies, buffer sizes and wire-codes. Using this information, a dynamic programming (DP) engine automatically generates the optimal H-tree that minimizes the appropriate cost function. Compared to a manually constructed buffered H-tree network, our approaches are able to reduce both jitter ({{by as much as}} 28 %, and power by as much as 46 %. When optimizing for minimum jitter, the DP engine generates a H-tree with lower jitter than when optimizing for minimum delay, thereby validating our approach, and proving its usefulness. I...|$|R
30|$|Total RNA was {{extracted}} from rice panicles using Trizol reagent (Invitrogen, Carlsbad, CA, USA) and purified using an RNeasy Plant Mini Kit (Qiagen, Valencia, CA, USA) {{according to the}} manufacturer’s instructions. The quality and integrity of RNA were tested using an Agilent Bioanalyzer 2100 system (Agilent, Santa Clara, CA, USA); RNA Integrity Number (RIN) values were greater than 8.5 for all samples. After total RNA extraction, eukaryotic mRNA was enriched by Oligo (dT) beads, while prokaryotic mRNA was enriched by removing rRNA using the Ribo-Zero TM Magnetic Kit (Epicentre). Then, the enriched mRNA was fragmented into 200 -bp <b>segments</b> using fragmentation <b>buffer</b> and reverse transcribed into cDNA with random primers. Second-strand cDNA synthesis was subsequently performed using DNA polymerase I, RNase H, dNTP and buffer. Then, the cDNA fragments were purified with QIAquick PCR extraction kit, end repaired, poly (A) added, and ligated to Illumina sequencing adapters. The ligation product size was selected by agarose gel electrophoresis, PCR amplified, and sequenced with 100  cycles of paired-end sequencing (2 [*]×[*] 150  bp) using Illumina HiSeq TM 2500 by Gene Denovo Biotechnology Co. (Guangzhou, China). The processing of fluorescent images into sequences, base-calling and quality value calculations were performed using the Illumina data processing pipeline (version 1.8). The sequence reads were submitted to the NCBI Sequence Read Archive (SRA, [URL] under the accession number SRP 127997.|$|R
40|$|Generalization of {{topographic}} maps is a {{very challenging}} problem for map producers. Therefore, NMAs are intensively working on the matter {{in order to make}} it automated as much as possible for their production requirements. In this paper, we present a case study for automated generalization of buildings and settlement areas in Turkish topographic maps from 1 : 25 K to 1 : 50 K, which is implemented in Laser-Scan LAMPS 2 GIS and map production software based on object-oriented database technology. It begins with the issues regarded in their generalization. Then, steps for generalization of surrounding roads of settlement areas are mentioned in a limited focus. After that comprehensive steps for generalization of buildings and settlement areas are given. At the beginning, settlement areas were stored as a whole without any direct interaction with roads. Their independent generalization created some problems and we did not have the possibility of analysing the areas surrounded by roads for the decisions in some building generalization operations. To solve these problems, we create settlement blocks using road <b>segments</b> after creating <b>buffers</b> on roads considering symbology and then partition existing settlement areas according to these blocks. After that voronoi diagrams are created and combined according to building clusters. It enabled us to analyse within the blocks for optimal generalization decisions. First results of this ongoing study are close to solution although some editing is required. It concludes with an evaluation of results, addressing future work. 1...|$|R
40|$|The growth-promoting {{effect of}} xyloglucan-derived {{oligosaccharides}} was investigated using a bioassay with entire pea (Pisum sativum L., var Alaska) shoots. After a 24 -h incubation period at 25 [deg]C, xyloglucan oligosaccharide (XGO) solutions with concentrations of 10 - 6 M notably increased {{the growth rate}} of pea shoots, whereas the same oligosaccharides at 10 - 7 M were less effective. To investigate the possible correlation between growth rate changes in the XGO-treated shoots and changes in the wall mechanical properties of their growing regions (third internodes), we used a short-term creep assay. The promotion of elongation by XGOs was reflected in an enhancement of the viscoelasticity of the growing regions of the shoots. To show whether this effect on wall viscoelastic properties was the cause or a consequence of their growth promotion, we tested the effect of XGOs on the long-term extension of isolated cell walls. We characterized an acid-induced extension in isolated cell walls from pea shoots that was not inhibited by preincubation in neutral buffers. Exogenously added XGOs did not alter the pattern of pea segment extension at any pH tested, indicating that XGOs have no direct effect on cell wall viscoelasticity. Finally, preincubation of pea <b>segments</b> in neutral <b>buffers</b> with XGOs enhanced their capacity to extend under acidic conditions. This finding suggests that XGOs at a neutral pH can act via transglycosylation, weakening the wall matrix and making the wall more responsive to other mechanisms of acid-induced extension as an expansin-mediated extension...|$|R
