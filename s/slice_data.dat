80|464|Public
50|$|In CT, {{a volume}} is formed by {{stacking}} the axial slices. The software cuts the volume {{in a different}} plane(usually orthogonal). Commonly, <b>slice</b> <b>data</b> is generated using an X-ray source that rotates around the object. X-ray sensors are positioned {{on the opposite side}} of the circle from the X-ray source.|$|E
50|$|The list of pointers, {{together}} with {{the address of the}} 1st MB of the slice, the SG, and the mb_allocation_map (stored in the processor’s local memory), will be used to navigate through the slices. The slice length will be used to transfer the <b>slice</b> <b>data</b> from the DRAM to the processor local memory.|$|E
50|$|The DEC 3000 AXP series {{uses the}} 32-bit TURBOchannel bus running at various speeds, 12.5 MHz in the 300 models, 22.5 MHz in the 400 models and 25 MHz in models 500 to 900. The TURBOchannel bus is {{provided}} by an ASIC, which connected it to the <b>SLICE</b> <b>data</b> path ASICs. The number to expansion slots also varied, the 300 models had two slots, except for the 300L, which had none. Models 400, 600 and 700 had three slots, the model 500X featured five, while models 500, 700 and 800 featured six.|$|E
40|$|Many {{techniques}} {{have been designed}} for privacy preserving and micro data publishing, such as generalization and bucketization. Several works showed that generalization loses some amount of information especially for high dimensional data. So it’s not efficient for high dimensional data. In case of Bucketization, it does not prevents membership disclosure and also does not applicable for data {{that do not have}} a clear separation between Quasi-identifying attributes and sensitive attributes. In this paper, we presenting an innovative technique called <b>data</b> <b>slicing</b> which partitions the data. An efficient algorithm is developed for computing <b>sliced</b> <b>data</b> that obeys l-diversity requirement. we also show how <b>data</b> <b>slicing</b> is better than generalization and bucketization. <b>Data</b> <b>slicing</b> preserves better utility than generalization and also does not requires clear separation between Quasi-identifying and sensitive attributes. <b>Data</b> <b>slicing</b> is also used to prevent attribute disclosure and develop an efficient algorithm for computing the <b>sliced</b> <b>data</b> that obeys l-diversity requirement. Experimental results confirm that <b>data</b> <b>slicing</b> preserves <b>data</b> utility than generalization and more effective than bucketization involving sensitive attributes. Experimental results demonstrate the effectiveness of this method...|$|R
40|$|Abstract. In this paper, {{we present}} a formal {{description}} of <b>data</b> <b>slicing,</b> which is a type-directed program transformation technique that separates a program’s heap into several independent regions. Pointers within each region mirror the structure of pointers in the original heap; however, each field whose type is a base type (e. g., the integer type) appears in {{only one of these}} regions. In addition, we discuss several applications of <b>data</b> <b>slicing.</b> First, <b>data</b> <b>slicing</b> can be used to add extra fields to existing data structures without compromising backward compatibility; the CCured project uses <b>data</b> <b>slicing</b> to preserve library compatibility in instrumented programs at a reasonable performance cost. <b>Data</b> <b>slicing</b> {{can also be used to}} improve locality by separating “hot ” and “cold ” fields in an array of data structures, and it can be used to protect sensitive data by separating “public ” and “private ” fields. Finally, <b>data</b> <b>slicing</b> can serve as a refactoring tool, allowing the programmer to split data structures while automatically updating the code that manipulates them. ...|$|R
40|$|Dynamic slicing {{algorithms}} {{are used}} to narrow {{the attention of the}} user or an algorithm to a relevant subset of executed program statements. Although dynamic slicing was first introduced to aid in user level debugging, increasingly applications aimed at improving software quality, reliability, security, and performance are finding opportunities to make automated use of dynamic slicing. In this paper we present the design and evaluation of three precise dynamic <b>data</b> <b>slicing</b> algorithms called the full preprocessing (FP), no preprocessing (NP) and limited preprocessing (LP) algorithms. The algorithms differ in the relative timing of constructing the dynamic data dependence graph and its traversal for computing requested dynamic <b>data</b> <b>slices.</b> Our experiments show that the LP algorithm is a fast and practical precise <b>data</b> <b>slicing</b> algorithm. In fact we show that while precise <b>data</b> <b>slices</b> can be orders of magnitude smaller than imprecise dynamic <b>data</b> <b>slices,</b> for small number of <b>data</b> <b>slicing</b> requests, the LP algorithm is faster than an imprecise dynamic <b>data</b> <b>slicing</b> algorithm proposed by Agrawal and Horgan...|$|R
50|$|The {{architecture}} of the Flamingo- and Sandpiper-based systems is based around a crossbar switch implemented by an ADDR (Address) ASIC, four <b>SLICE</b> (<b>data</b> slice) ASICs and a TC (TURBOchannel) ASIC. These ASICs connect the various different width buses used in the system, allowing data {{to be transferred to}} the different subsystems. PALs were used to implement the control logic. The cache, memory and TURBOchannel controllers, as well as other control logic, is entirely implemented by PALs. Pelican-based systems have an entirely different architecture from the other systems, similar to that of late-model Personal DECstations that they are based on, with a traditional workstation architecture with buses and buffers.|$|E
50|$|For each slice, {{the slice}} length and the {{macroblock}} address (i.e. index {{with respect to}} the raster scan order) of the first macroblock (MB) of the slice are extracted by the slice parser (Figure 2). This information, together with the slice itself, is stored in memory (shown as DRAM). In addition, a list of pointers (Figure 2, a pointer for each slice, and each pointing to the memory location where a slice is stored), should be generated. The list of pointers, together with the address of the first macroblock of the slice, will be used to navigate through the out of order slices. The slice length will be used to transfer the <b>slice</b> <b>data</b> from the DRAM to the decoder’s internal memory.|$|E
5000|$|A set of NAL {{units in}} a {{specified}} form {{is referred to}} as an access unit. The decoding of each access unit results in one decoded picture.Each access unit contains a set of VCL NAL units that together compose a primary coded picture. It may also be prefixed with an access unit delimiter to aid in locating the start of the access unit. Some supplemental enhancement information containing data such as picture timing information may also precede the primary coded picture.The primary coded picture consists of a set of VCL NAL units consisting of slices or [...] <b>slice</b> <b>data</b> partitions that represent the samples of the video picture.Following the primary coded picture may be some additional VCL NAL units that contain redundant representations of areas of the same video picture. These are referred to as redundant coded pictures, and are available for use by a decoder in recovering from loss or corruption of the data in the primary coded pictures. Decoders are not required to decode redundant coded pictures if they are present.Finally, if the coded picture is the last picture of a coded video sequence (a sequence of pictures that is independently decodable and uses only one sequence parameter set), an end of sequence NAL unit may be present to indicate the end of the sequence; and if the coded picture is the last coded picture in the entire NAL unit stream, an end of stream NAL unit may be present to indicate that the stream is ending.|$|E
30|$|FFTW {{distributes}} <b>slices</b> of <b>data,</b> while ScaLAPACK uses a block-cyclic distribution pattern.|$|R
40|$|In this research, we have {{developed}} new methods to improve the accuracy and increase the efficiency of rapid prototyping processes. The major contributions of this research are summarized as follows: • A biarc curve fitting of RP <b>slicing</b> contour <b>data</b> has been developed to smooth cross-sectional contours. The mathematics formulation and a Max-Fit algorithm {{have been developed to}} find the biarc curves of the STL <b>slicing</b> <b>data</b> points. Max-Fit biarc fitting algorithm progresses through all the points on the <b>slicing</b> contour <b>data</b> to find an efficient biarc fitting. The results show that rough cross sectional contours can be smoothed with the newly developed method. Therefore, less strict requirements on the STL triangulation tolerance can be used when STL is generated for rapid prototyping. • Non-uniform offsetting and hollowing by using biarcs fitting has been developed to increase the efficiency of the rapid prototyping process. The developed method can reduce the area that needs to be built so the build time can be reduced significantly. Constant wall thickness is obtained to avoid non...|$|R
40|$|ABSTRACT: Some {{different}} anonymization techniques, such as generalization and bucketization, {{have been}} designed for privacy preserving micro data publishing. Recent work has shown that generalization loses considerable amount of information, especially for high dimensional data. Bucketization, on the other hand, does not prevent membership disclosure and does not apply for data {{that do not have}} a clear separation between quasi-identifying attributes and sensitive attributes. In this paper, we present a novel technique called slicing, which partitions the data both horizontally and vertically. We show that <b>slicing</b> preserves better <b>data</b> utility than generalization and can be used for membership disclosure protection. Another important advantage of slicing is that it can handle high-dimensional data. We show how slicing can be used for attribute disclosure protection and develop an efficient algorithm (An algorithm is a procedure or formula for solving a problem.) for computing the <b>sliced</b> <b>data</b> that obey the diversity requirement. We show how slicing can be used for attribute disclosure (uncover) protection and develop an efficient algorithm for computing the <b>sliced</b> <b>data</b> that obey the ‘-diversity requirement. Our workload experiments confirm that slicing preserves better utility than generalization and is more effective than bucketization in workloads involving the sensitive attribute. Our experiments also demonstrate that slicing can be used to prevent membership disclosure. Slicing gives us a higher security as well as open source environment. i. e. on integration of project...|$|R
40|$|Consider {{a simple}} {{branching}} diffusion process, {{which is a}} branching {{process in which the}} individuals move and live and die in space. The offspring distribution has finite moments of all orders. A parametric estimation theory is presented, using time <b>slice</b> <b>data.</b> This involves the use of third order cumulant spectra to identify and estimate the parameters. simple branching diffusion cumulant cumulant spectra estimation consistency asymptotic normality time <b>slice</b> <b>data...</b>|$|E
30|$|The {{ketamine}} analogues were synthesised {{and provided}} by the Auckland Cancer Research Centre, School of Medical Sciences, University of Auckland, New Zealand. Liisa Andersson and Anna Jadelind collected the propofol and etomidate <b>slice</b> <b>data.</b>|$|E
40|$|AbstractConsider {{a simple}} {{branching}} diffusion process, {{which is a}} branching {{process in which the}} individuals move and live and die in space. The offspring distribution has finite moments of all orders. A parametric estimation theory is presented, using time <b>slice</b> <b>data.</b> This involves the use of third order cumulant spectra to identify and estimate the parameters...|$|E
40|$|Privacy Preserving on {{microdata}} publishing different anonymization techniques {{named as}} generalization and bucketization have been proposed. In generalization {{amount of information}} loses on high-dimensional data. Bucketization does not prevent membership disclosure and {{does not provide a}} clear separation between quasi-identifying attributes and sensitive attributes. In this Paper, we present a new technique called slicing which partitions the data both horizontally and vertically. <b>Slicing</b> preserves better <b>data</b> utility than generalization and can be used for membership disclosure protection. Another important advantage of slicing is that it can handle high-dimensional <b>data.</b> <b>Slicing</b> can be used for attribute disclosure protection and develop an efficient algorithm for computing the <b>sliced</b> <b>data</b> that obey the ℓ-diversity requirement. Not only providing privacy for micro data, mining the micro data also important. This system uses clusters identified in multiple time periods and identifies trends based on similarities between clusters over time. Our experiments shows that slicing gives a better and effective utility better than the existing one. Keywords—Microdata,Bucketization,SAS, Centroids, anonymization...|$|R
40|$|A method, apparatus, system, {{article of}} manufacture, and {{computer}} readable storage medium provide {{the ability to}} measure wind. Data at a first resolution (i. e., low resolution data) is collected by a satellite scatterometer. Thin <b>slices</b> of the <b>data</b> are determined. A collocation of the <b>data</b> <b>slices</b> are determined at each grid cell center to obtain ensembles of collocated <b>data</b> <b>slices.</b> Each ensemble of collocated <b>data</b> <b>slices</b> is decomposed into a mean part and a fluctuating part. The data is reconstructed at a second resolution from the mean part and a residue of the fluctuating part. A wind measurement is determined from the data at the second resolution using a wind model function. A description of the wind measurement is output...|$|R
40|$|AbstractThis paper {{introduces}} a novel feature matching algorithm using both Image interpolating method and edge detecting method to make feature matching more precise. The proposed feature matching {{is composed of}} four elements: inputting data, interpolating method, features extracting and matching. At first, the tasks in human anatomy and the method to input <b>slices</b> <b>data</b> are introduced. Secondly, we propose a novel slices matching using image interpolating method and edge detecting method. Experimental results demonstrate that the proposed approach owns the properties both quickly and exactly...|$|R
3000|$|... 3 D {{construction}} software (VG Studio; Volume Graphics, Heidelberg, Germany) {{was used}} to observe the internal structure of the bone by 3 D construction with volume rendering from the <b>slice</b> <b>data.</b> At the same time, image processing software (Mimics; Materialise, Leuven, Belgium) {{was used to}} separate the IC and other structures, followed by reconstruction with the IC shown in red.|$|E
40|$|In this paper, we {{investigate}} {{a new approach}} to the cooccurrence matrix currently used to extract textural features: co-occurrence matrices for volumetric data. While traditional texture metrics have concentrated on 2 D texture, 3 D imaging modalities {{are becoming more and more}} prevalent, providing the possibility of examining texture as a volumetric phenomenon. Just as computer graphics have used 3 D textures as a more realistic alternative to 2 D texture mapping, we expect that texture derived from volumetric data will have better discriminating power than 2 D texture derived from <b>slice</b> <b>data.</b> An experimental study has been conducted in which the results for textural features derived from 2 D are compared to those results derived from using cooccurrence matrices for volumetric data. Our preliminary experimental results indicate that the volumetric texture features have better discriminating power than 2 D texture derived from <b>slice</b> <b>data.</b> KEY WORDS Imaging and image processing, co-occurrence matrices, volumetric data, volumetric texture 1...|$|E
30|$|Virtual Resource: is an {{abstraction}} added onto compute, storage and network resources. It enables slicing {{of these resources}} into smaller chunks that can be scaled vertically or horizontally. Typically virtualisation is used in a data centre to <b>slice</b> <b>data</b> centre compute resource into Virtual machines, and potentially to present several logical processors by mapping these onto a single physical processor. Network cards and storage are also virtualised and presented as individual devices to VMs.|$|E
40|$|We {{examine the}} {{functional}} cohesion of procedures using a <b>data</b> <b>slice</b> abstraction. Our analysis identifies the data tokens that lie {{on more than}} one slice as the "glue" that binds separate components together. Cohesion is measured in terms of the relative number of glue tokens, tokens that lie {{on more than one}} <b>data</b> <b>slice,</b> and super-glue tokens, tokens that lie on all <b>data</b> <b>slices</b> in a procedure, and the adhesiveness of the tokens. The intuition and measurement scale factors are demonstrated through a set of abstract transformations...|$|R
40|$|Abstract [...] This paper {{reports on}} {{computing}} program slices for languages with pointers. We use {{a variation on}} symbolic execution to assign addresses to pointer variables and to build lists of possible addresses contained by each pointer at each statement. The number of lists is kept small by using a concept similar to control flow based basic blocks called basic pointer blocks, acontiguous set of statements headed by an assignment to a pointer variable. Index Terms [...] Program <b>slicing,</b> <b>data</b> flow analysis, debugging, pointers, software tools, software engineering. I...|$|R
40|$|We {{examine the}} {{functional}} cohesion of procedures using a <b>data</b> <b>slice</b> abstraction. analysis identifies the data tokens that lie {{on more than}} one slice as the "glue" that binds separate components together. Cohesion is measured in terms of the relative number of glue tokens, tokens that lie {{on more than one}} <b>data</b> <b>slice,</b> and super-glue tokens, tokens that lie on all <b>data</b> <b>slices</b> in a procedure, and the adhesiveness of the tokens. The intuition and measurement scale factors are demonstrated through a set of abstract transformations and composition operators...|$|R
40|$|A {{simple and}} {{effective}} compression method is proposed for multiple-scan testing. For a given test set, each test pattern is compressed from {{the view of}} slices. An encoding table exploiting seven types of frequently-occurring pattern is used. Compression is then achieved by mapping <b>slice</b> <b>data</b> into codewords. The decompression logic is small and easy to implement. It is also applicable to schemes adopting a single-scan chain. Experimental results show this method can achieve good compression effect...|$|E
40|$|We {{present a}} tool for {{visualizing}} the wall motion and thickness of the left ventricle. Smooth surface reconstructions of the epicardial and endocardial surfaces are computed from MRI <b>slice</b> <b>data,</b> and used to compute wall thickness along the epicardial surface. The epicardial surface is then displayed color-coded by wall thickness. Wall motion can also be visualized as the epicardial surface is drawn beating in real time. We {{believe that this is}} an intuitive interface for the study of ventricular function and treatment planning. ...|$|E
3000|$|The {{samples were}} imaged with a micro-CT system [...] (HMX- 225 Actis 4; TESCO, Tokyo, Japan). Imaging {{conditions}} were as follows: matrix size 512 × 512, tube voltage 120  kV, and tube current 80  μA. The micro-CT imaging intensifier (I.I.) had a 1 -in. CCD camera with a 4 -in., 16 -bit 1, 024 [*]×[*] 1, 024 scanning line. A total of 1, 200 images of raw data were output with this camera. On {{the basis of}} this raw data, a back projection method was used to produce 2 D <b>slice</b> <b>data.</b>|$|E
40|$|Several anonymization techniques, like {{generalization}} and bucketization, {{have been}} intended for privacy preserving microdata publishing. current work {{has shown that}} generalization loses significant amount of information, particularly for high-dimensional data. on the other hand, Bucketization does not prevent membership disclosure and does not apply for data {{that do not have}} a clear separation between quasi-identifying attributes and sensitive attributes. In this paper, we present a new technique called <b>slicing,</b> in that <b>data</b> is partition into both horizontally and vertically. We demonstrate that <b>slicing</b> preserves better <b>data</b> utility than generalization and can be used for membership disclosure protection. Another main advantage of slicing is that it can handle high-dimensional data. We illustrate how slicing can be used for attribute disclosure protection and build up an efficient algorithm for computing the <b>sliced</b> <b>data</b> that obey the l-diversity requirement. Our workload experiments verify that slicing preserves better utility than generalization and is more effective than bucketization in workloads involving the sensitive attribute. Our experiments also show that slicing can be used to prevent membership disclosure...|$|R
40|$|The {{application}} of loop and data transformations to array and loop intensive programs {{is crucial to}} obtain a good performance. Designers often apply these transformations manually or semi-automatically. For the class of static affine programs, automatic methods exist for proving the correctness of these transformations. Realistic multimedia systems, however, often contain constructs that fall outside of this class. We present an extension of a widening based approach to handle the most relevant of these constructs, viz. accesses to array <b>slices,</b> <b>data</b> dependent accesses and data dependent assignments, and report on some experiments with non-trivial applications. nrpages: 23 status: publishe...|$|R
40|$|ISVAS (interactive System {{for visual}} data Analysis) is an {{end-user}} visualization syste for finite-element analysis and for volume data. The system supports methods that allow an interactive analysis of very large data sets {{as they are}} commen in computational fluid dynamics (CFD). The paper presents visualization strategies {{as well as their}} realization in ISVAS. The concepts of interaction for configuration and control are outlined. Beside this user-interaction for configuration and control are outlined. Beside this user-interaction with application data by means of specification of the view point, positioning of <b>slices,</b> <b>data</b> probing, and intuitive positioning of seed points for particle tracing are some of the main characteristics of ISVAS...|$|R
30|$|Multiplexing and {{demultiplexing}} do {{not require}} deep inspection on different layers of a <b>slice.</b> <b>Data</b> from different slices is marked; slices are grouped and sent through a common link. A switch and a gateway are more complicated according to slices analysis. In some cases, they would require accessing data sent through a slice to transfer {{it to the other}} slice. This means that such devices require special protection according to access to raw slices’ data as well as a proper assurance for interconnecting slices, including preventing wrong interconnection of slices and potential data leakage.|$|E
30|$|It is {{necessary}} {{that the information}} contained in the PSC arrives reliably at the decoder, otherwise the H. 264 codec {{will not be able to}} decode the video. However, the loss of coded slices is tolerable at the decoder. In fact, the H. 264 standard specifies a number of error resilience techniques [14]. One of these techniques, which is in particular interest to network applications, is data partitioning (DP). With DP, each video <b>slice</b> <b>data</b> is partitioned to three groups with different importance, each group delivered in a separate packet. Using this technique, higher-priority data can receive better services from the delivery layer.|$|E
30|$|H. 264 {{baseline}} profile {{was designed}} to minimize the computational complexity and provide high robustness and flexibility for utilization over {{a broad range of}} network environment and conditions. It is typically regarded as the simplest one in the standard, which includes all the H. 264 tools {{with the exception of the}} following tools: B-slices, weighted prediction, field (interlaced) coding, picture/macroblock adaptive switching between the frame and field coding (MB-AFF), context adaptive binary arithmetic coding (CABAC), SP/SI slices and <b>slice</b> <b>data</b> partitioning. This profile normally targets the video applications with low computational complexity and low delay requirements.|$|E
40|$|Modern {{society is}} {{increasingly}} dependent on (and fearful of) massive amounts and availability ofelectronic information. There are numerous everyday scenarios where sensitive data must be sometimesreluctantly or suspiciously shared between entities without mutual trust. This prompts the need formechanisms to enable limited (privacy-preserving) information sharing. A typical scenario involves twoparties: one seeks {{information from the}} other that is either motivated, or compelled to share only therequested information. This paper highlights two main technical challenges:(1) How to enable this type of sharing such that parties learn no information beyond what they areentitled to, and(2) How to do so efficiently, in real-world practical terms. In this paper, it is shown that <b>slicing</b> preserves better <b>data</b> utility than generalization {{and can be used}} formembership disclosure protection. Another important advantage of slicing is that it can handle highdimensionaldata. In this paper, it is shown how slicing can be used for attribute disclosure protection,Membership disclosure protection and develop an efficient algorithm for computing the <b>sliced</b> <b>data</b> that obeythe L-diversity requirement...|$|R
40|$|The {{individual}} {{data may}} be altered, {{for a variety}} of purposes. To overcome these concerns, a number of techniques have recently been proposed. Preserving utility of data and actual data from generalization and bucketization in workload involving the sensitive attributes the new technique introduced ‘Slicing’. Slicing can handle high dimensional data by partitioning the data sets horizontally and vertically. In <b>slicing</b> <b>data</b> can be organized arbitrarily, checking privacy threats is a concern. Due to the large size of the data sources having several hundred millions to several billions records, and continuously growing, efficient techniques and algorithms are needed. <b>Slicing</b> preserves better <b>data</b> utility than generalization and also prevents membership disclosure. One approach to speed up the processing is to use a process, where potential candidate records are grouped together one and each group is further processed and analyzed on overlapping attributes. The record grouping problem is a formal formulation is to be done in step one. The significance of using slicing is that it can handle high dimension <b>data.</b> <b>Slicing</b> technique used random rows and columns which not give better accuracy hence the new technique of grouping, which improve the working efficiency and accuracy. This paper focus on effective method {{that can be used for}} providing better data utility. It can handle high-dimensional data for better security...|$|R
40|$|Abstract-More techniques, such as {{generalization}} and bucketization, {{have been}} introduced for privacy preserving micro data publishing. Recent tasks have cleared that generalization loses some amount of information, especially for large (high-dimensional) data. Bucketization, is the another technique, does not prevent membership disclosure and does not apply for data {{that do not have}} a clear separation between quasi-identifying attributes and sensitive attributes. In this paper, we introduce technique called slicing, which partitions the data both horizontally and vertically. We represent that <b>slicing</b> preserves best <b>data</b> utility than generalization and can be used for membership disclosure protection. Another important advantage of slicing is that it can handle high-dimensional data. We shown how slicing can be used for attribute disclosure protection and develop an efficient algorithm for computing the <b>sliced</b> <b>data</b> that obey the ℓ-diversity requirement. Our workload experiments confirm that slicing preserves better utility than generalization and is more effective than bucketization in workloads involving the sensitive attribute. Our experiments also describes that slicing can be used to prevent membership disclosure...|$|R
