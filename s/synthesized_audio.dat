27|74|Public
500|$|Like its predecessor, Ultima Underworld II was {{produced}} by Warren Spector, who was Looking Glass' main link to publisher Origin Systems. Church later praised Spector's handling of the project: his weekly phone conversations and monthly meetings with Looking Glass helped Church and the team to refocus creatively during development. When the team failed to produce enough art for the game, Spector supplemented the art team with contractors from Origin, which Church believed was [...] "critical" [...] to the game's being released on schedule. As {{a result of the}} art team's distance from Looking Glass, Church had to phone [...] "nine different area codes every couple days to check up on things". He recalled the challenge of describing the enemy designs by phone. Dan Schmidt and the recently hired Seamus Blackley composed the game's score in Blackley's apartment over one week. They attempted to give each world a unique sound, while hiding variations of the main theme in each track. The music system from the original Ultima Underworld was retained with only minor alterations, but the team included digitized sound effects instead of the <b>synthesized</b> <b>audio</b> used in the first game.|$|E
50|$|PySynth is a {{lightweight}} {{open source software}} synthesizer written in Python. It accepts a song as a nested list and outputs a WAV file with the <b>synthesized</b> <b>audio.</b>|$|E
5000|$|Like its predecessor, Ultima Underworld II was {{produced}} by Warren Spector, who was Looking Glass' main link to publisher Origin Systems. Church later praised Spector's handling of the project: his weekly phone conversations and monthly meetings with Looking Glass helped Church and the team to refocus creatively during development. When the team failed to produce enough art for the game, Spector supplemented the art team with contractors from Origin, which Church believed was [...] "critical" [...] to the game's being released on schedule. As {{a result of the}} art team's distance from Looking Glass, Church had to phone [...] "nine different area codes every couple days to check up on things". He recalled the challenge of describing the enemy designs by phone. Dan Schmidt and the recently hired Seamus Blackley composed the game's score in Blackley's apartment over one week. They attempted to give each world a unique sound, while hiding variations of the main theme in each track. The music system from the original Ultima Underworld was retained with only minor alterations, but the team included digitized sound effects instead of the <b>synthesized</b> <b>audio</b> used in the first game.|$|E
5000|$|Web Audio - a {{high-level}} JavaScript API for processing and <b>synthesizing</b> <b>audio</b> in web applications; ...|$|R
40|$|We {{investigate}} {{differences among}} the approaches to the digitization of phonographic discs, using two novel methods developed by the authors: a system for <b>synthesizing</b> <b>audio</b> signals from still images of phonographic discs and a tool for the automatic alignment of audio signals. Results {{point out that this}} combined approach can be used as an effective tool for the preservation of and access to the audio documents...|$|R
40|$|In {{this thesis}} we have {{attempted}} {{to develop a new}} model for sound synthesis based on diffraction theory. Unlike the other methods, the synthesis method proposed in this thesis makes use a physical model which is based on diffraction theory. The main contribution of this thesis is the development of a physical model based on diffraction theory which can be used to analyze and <b>synthesize</b> <b>audio</b> and visual signals...|$|R
30|$|The {{absolute}} {{quality and}} representativeness of the <b>synthesized</b> <b>audio</b> textures were studied in experiment 3. Real audio samples and <b>synthesized</b> <b>audio</b> textures {{produced by the}} proposed method were presented in random order to test subjects.|$|E
40|$|PortAudio is an {{open source}} 'C ' {{language}} API and library for implementing cross-platform real-time audio applications. This paper describes recent additions to the PortAudio API designed to assist in implementing synchronisation between real-time audio and other time-based data such as MIDI and computer graphics. Examples are presented that illustrate synchronisation of a graphical display to real time audio, low jitter MIDI triggering of software <b>synthesized</b> <b>audio,</b> and synchronising software <b>synthesized</b> <b>audio</b> to an external time source such as MIDI clock. Some of the challenges of implementing the PortAudio synchronisation infrastructure on Microsoft ® Windows ® are discussed. ...|$|E
30|$|Three {{listening}} {{tests were}} conducted to study the proposed method for location-specific audio textures. The first listening test studied the benefits of using similarity-controlled segment shuffling over random segment shuffling (later denoted by experiment 1). The second test studied the relative difference in quality between a <b>synthesized</b> <b>audio</b> texture and an original audio sample (later denoted by experiment 2). First and second listening {{tests were conducted}} in a forced choice paired comparison setup. The third listening test studied the absolute quality of the <b>synthesized</b> <b>audio</b> textures and their ability for representing the auditory scene of various locations (later denoted by experiment 3). This test was conducted in an absolute rating test setup.|$|E
40|$|Substantial {{progress}} has been made recently in finding acoustic features that describe perceptually relevant aspects of sound. This paper presents a general framework for <b>synthesizing</b> <b>audio</b> manifesting arbitrary sets of quantifiable acoustic features of the sort used in music information retrieval and other sound analysis applications. The methods described have broad applications to the synthesis of novel musical timbres, processing by analysisresynthesis, efficient audio coding, generation of psychoacoustic stimuli, and music information retrieval research. ...|$|R
50|$|The Web Audio API {{specification}} {{developed by}} W3C describes a high-level JavaScript API for processing and <b>synthesizing</b> <b>audio</b> in web applications. The primary paradigm {{is of an}} audio routing graph, where a number of AudioNode objects are connected together to define the overall audio rendering. The actual processing will primarily {{take place in the}} underlying implementation (typically optimized Assembly / C / C++ code), but direct JavaScript processing and synthesis is also supported.|$|R
5000|$|Christina Kubisch (born January 31, 1948) [...] is a German composer, {{performance}} artist, {{professor and}} flautist. She composes both electronic and acoustic music for multimedia installations. She gained {{recognition in the}} mid-1970s from her early works including concerts, performances and installations. Her work focuses on <b>synthesizing</b> <b>audio</b> and visual arts to create multi-sensory experiences for participants. She focuses on finding sounds and music in unusual places that participants would normally not think of as somewhere to experience sound.|$|R
30|$|Experiment 1 {{studied the}} effect of segment {{shuffling}} method to the perceived fluency and naturalness of the <b>synthesized</b> <b>audio</b> texture. In the test, two samples were presented to test subjects: a reference sample synthesized by random segment shuffling and a sample synthesized by similarity-controlled segment shuffling. The expected outcome of this experiment was that the similarity-based segment shuffling is preferred because the transitions between segments should be concealed by similar audio content {{on either side of}} the cuts.|$|E
40|$|We {{describe}} {{efforts to}} evaluate a new smartphone-based visual programming language, which gives users the ability to map their own musical blob notation to <b>synthesized</b> <b>audio.</b> We use the Cognitive Dimensions of Notations questionnaire (Blackwell & Green, 2000) to gain qualitative feedback, and use traditional observation methods to support our results. Using this methodology, {{we were able to}} find that musical and non-musical users both react positively to the system, with no marked difference between the two groups. 1...|$|E
40|$|We {{describe}} a system called AutoBriefer which automatically generates multimedia briefings from highlevel outlines. The {{author of the}} briefing interacts with the system to specify some of the briefing content. In order to be scalable across domains, AutoBriefer provides a graphical user interface (GUI) that allows the user to create or edit domain knowledge {{for use in the}} selecting of some of the briefing content. The system then uses declarative presentation planning strategies to synthesize a narrated multimedia briefing in various presentation formats. The narration employs <b>synthesized</b> <b>audio</b> as well as, optionally, an agent embodying the narrator. 1...|$|E
40|$|A novel {{approach}} for obtaining labeled training data {{is presented to}} directly estimate the model parameters in a supervised learning algorithm for automatic chord recognition from the raw audio. To this end, harmonic analysis is first performed on symbolic data to generate label files. In parallel, we <b>synthesize</b> <b>audio</b> data from the same symbolic data, which are then provided to a machine learning algorithm along with label files to estimate model parameters. Experimental results show higher performance in frame-level chord recognition than the previous approaches...|$|R
40|$|We have {{developed}} a computer interface that simultaneously renders integrated auditory and haptic stimuli with very low latency. The interface includes a three degree of freedom (DOF) Pantograph haptic device that reads position input and renders force output. We <b>synthesize</b> <b>audio</b> by convolving the force profile generated by user interaction with the impulse response of the virtual surface. Thus, both auditory and haptic modes are tightly coupled. This system should aid in evaluating efficient algorithms for integrated rendering of both sensory modes. We will demonstrate the interface during the conference. Keywords: User Interface, Haptics+Audi...|$|R
40|$|We {{present a}} general {{framework}} for <b>synthesizing</b> <b>audio</b> manifesting arbitrary sets of perceptually motivated, quantifiable acoustic features. Much {{work has been}} done recently on finding acoustic features that describe perceptually relevant aspects of sound. The ability to synthesize sounds defined by arbitrary feature values would allow perception researchers to more directly generate stimuli “to order, ” as well as providing an opportunity to directly test the perceptual relevance and characteristics of such features. The methods we describe also provide a straightforward way of approaching the problem of mapping from data to synthesis control parameters for sonification. 1...|$|R
40|$|In this paper, we use {{mathematical}} tools {{developed for}} chaos theory and {{time series analysis}} and apply them to the analysis and resynthesis of musical instruments. In particular, we can embed a basic one-dimensional audio signal time series within a higher-dimensional space to uncover the underlying generative attractor. Röbel [7],[8] described a neural-net model for audio sound synthesis based on attractor reconstruction. We present a different methodology inspired by Kaplan and Glass [3] to resynthesize the signal based on time-lag embedding in different numbers of dimensions, and suggest techniques for choosing the approximate embedding dimension to optimize {{the quality of the}} <b>synthesized</b> <b>audio...</b>|$|E
40|$|This project {{reflects}} {{an investigation into}} the possibilities of integrating ballroom dance, computerized animated graphics, and <b>synthesized</b> <b>audio</b> sound to create a viable ballroom dance notation system. This system adds the essential dimensions of movement and musical interpretation to the written descriptions and printed foot diagrams currently being used to denote ball room dance step patterns. A system for presenting computer animation and a method of creating a <b>synthesized</b> <b>audio</b> rhythm were discovered and adapted for the purpose of visually displaying ball room dance step patterns on the computer screen through experimentation and exploration of the home personal computer's multi-faceted capabilities. This project examined aspects of the ball room dance technique that can be displayed by the graphic illustration of the feet. Through animated graphics the individual footwork of the gentleman and lady was represented by a series of animated step patterns in the waltz. The interconnection of step patterns and rhythm was displayed by the synchronization of each step within the sequences to a synthesized musical note equal to the time value of that step. The combination of the audio rhythm and the animated step sequences created a moving model of the written descriptions and printed foot diagrams to depict the ballroom dance technique. This project reflects the beginnings of the developmental process for a computerized ballroom dance notation system. The integration of computer technology and ballroom dance is an exciting and viable combination for further research...|$|E
40|$|In {{this paper}} we apply speaker-dependent {{training}} of Hidden Markov Models (HMMs) to audio and visual laughter syn-thesis separately. The two modalities are synthesized with a forced durations approach and are then combined together to render audio-visual laughter on a 3 D avatar. This paper fo-cuses on visual synthesis {{of laughter and}} its perceptive eval-uation when combined with <b>synthesized</b> <b>audio</b> laughter. Pre-vious work on audio and visual synthesis has been success-fully applied to speech. The extrapolation to audio laughter synthesis has already been done. This paper shows {{that it is possible}} to extrapolate to visual laughter synthesis as well. Index Terms — Audio, visual, laughter, synthesis, HMM 1...|$|E
40|$|The digital {{re-recording}} of analogue material, {{in particular}} phonographic discs, {{can be carried}} out using different approaches: mechanical, electro-mechanical, opto-mechanical, and opto-digital. In this paper, we investigate the differences among these approaches, using two novel methods that have been developed on purpose: a system for <b>synthesizing</b> <b>audio</b> signals from still images of phonographic discs and a tool for the automatic alignment of audio signals. The methods have been applied to two case studies, taken from a shellac disc. Results point out that this combined approach can be used as an effective tool for the preservation of and access to the audio documents...|$|R
40|$|This work explores {{nonparametric methods}} which aim at <b>synthesizing</b> <b>audio</b> from low-dimensionnal {{acoustic}} features typically used in MIR frameworks. Several issues prevent this task to be straightforwardly achieved. Such features {{are designed for}} analysis and not for synthesis, thus favoring high-level description over easily inverted acoustic representation. Whereas some previous studies already considered the problem of <b>synthesizing</b> <b>audio</b> from features such as Mel-Frequency Cepstral Coefficients, they mainly relied on the explicit formula used to compute those features in order to inverse them. Here, we instead adopt a simple blind approach, where arbitrary sets of features can be used during synthesis and where reconstruction is exemplar-based. After testing the approach on a speech synthesis from well known features problem, we apply it to the more complex task of inverting songs from the Million Song Dataset. What makes this task harder is twofold. First, that features are irregularly spaced in the temporal domain according to an onset-based segmentation. Second the exact method used to compute these features is unknown, although the features for new audio can be computed using their API as a black-box. In this paper, we detail these difficulties and present a framework to nonetheless attempting such synthesis by concatenating audio samples from a training dataset, whose features have been computed beforehand. Samples are selected at the segment level, in the feature space with a simple nearest neighbor search. Additionnal constraints can then be defined to enhance the synthesis pertinence. Preliminary experiments are presented using RWC and GTZAN <b>audio</b> datasets to <b>synthesize</b> tracks from the Million Song Dataset. Comment: Technical Repor...|$|R
40|$|A {{recently}} published method for audio style transfer has shown how {{to extend the}} process of image style transfer to <b>audio.</b> This method <b>synthesizes</b> <b>audio</b> "content" and "style" independently using the magnitudes of a short time Fourier transform, shallow convolutional networks with randomly initialized filters, and iterative phase reconstruction with Griffin-Lim. In this work, we explore whether {{it is possible to}} directly optimize a time domain audio signal, removing the process of phase reconstruction and opening up possibilities for real-time applications and higher quality syntheses. We explore a variety of style transfer processes on neural networks that operate directly on time domain audio signals and demonstrate one such network capable of audio stylization...|$|R
40|$|This paper {{presents}} an audio coder {{based on the}} combination of the Modulated Lapped Transform (MLT) with the Set Partitioning In Hierarchical Trees (SPIHT) algorithm. SPIHT allows scalable coding by transmitting more important information first in an efficient manner. The results presented reveal that the Modulated Lapped Transform (MLT) based scheme produces a high compression ratio for little or no loss of quality. A modification is introduced to SPIHT which further improves the performance of the algorithm when it is being used with uniform M-band transforms and masking. Further, the MLT-SPIHT scheme is shown to achieve high quality <b>synthesized</b> <b>audio</b> at 54 kbps through subjective listening tests. 1...|$|E
40|$|In this paper, we {{describe}} the syntactic and semantic issues of a voice man machine dialogue system. This system, featuring a voice recognition circuit, allows the dictation of medical analysis reports and automatically generates a written document. An object oriented kernel is designed to represent the linguistic and medical entities. The syntactic parser is adapted to voice input and enables the physician to dictate using natural language. A semantics analyzer manages the medical entities and {{the agenda of the}} report. This analyzer allows to detect certain errors or inconsistencies, and to warn the user, using <b>synthesized</b> <b>audio</b> messages. The audio interface has been merged into a graphical environment to allow keyboard and pointing device controls, as well as graphical alerts...|$|E
40|$|A {{considerable}} number of MIR tasks requires annotations at the note-level {{for the purpose of}} in-depth evaluation. A common means of obtaining accurately annotated data corpora is to start with a symbolic representation of a piece and generate corresponding audio data. This study investigates the effect of audio quality and source on the performance of two representative MIR algorithms – Onset Detection and Audio Alignment. Three kinds of audio material are compared: piano pieces generated using a freely available software synthesizer with its default instrument patches; a commercial high-quality sample library; and audio recordings made on a real (computer-controlled) grand piano. Also, the effect of varying richness of artistic changes in tempo and dynamics or natural asynchronies is examined. We show that the algorithms ’ performance on the different datasets varies considerably, but <b>synthesized</b> <b>audio,</b> does not necessarily yield better results...|$|E
40|$|Presented at the 12 th International Conference on Auditory Display (ICAD), London, UK, June 20 - 23, 2006. We {{present a}} general {{framework}} for <b>synthesizing</b> <b>audio</b> manifesting arbitrary sets of perceptually motivated, quantifiable acoustic features. Much {{work has been}} done recently on finding acoustic features that describe perceptually relevant aspects of sound. The ability to synthesize sounds defined by arbitrary feature values would allow perception researchers to more directly generate stimuli ``to order,'' as well as providing an opportunity to directly test the perceptual relevance and characteristics of such features. The methods we describe also provide a straightforward way of approaching the problem of mapping from data to synthesis control parameters for sonification...|$|R
40|$|DE 102007046020 A 1 UPAB: 20090503 NOVELTY - The device has an {{analyzing}} and synthesizing unit (4) {{for generating}} a combination signal {{based on a}} selected signal, by combining base signals, using an evolutionary algorithm. The synthesizing unit transmits the combination signal for acoustic reproduction at an output section. A selection section (3) selects the combination signal as an analysis signal. The synthesizing unit generates another combination signal based on the analysis signal or a signal derived from the analysis signal. The selection section selects the latter combination signal as a compensation signal. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for analyzing and <b>synthesizing</b> <b>audio</b> signals. USE - Device for analyzing and <b>synthesizing</b> an <b>audio</b> signal and {{for use as a}} mobile device or a stationary laboratory system, for diagnosis and therapy of tinnitus disease (all claimed). ADVANTAGE - The device compensates patient specific tinnitus noises and/or phantom noises in a simple, controlled and optimal manner. The device detects relevant phantom signals and/or compensation signals in a simple manner. The device reduces error-prone and complicated communication between an acoustician and a patient, thus preventing misinterpretation of phantom signals. The device improves quality of life of patients suffering from tinnitus disease in a cost-effective manner, while reducing non-productive time...|$|R
50|$|To {{convert the}} article into a talking video a {{voice-over}} is required. AVR completes this task by <b>synthesizing</b> the <b>audio</b> and converting the article's text into speech. There are over 50 different {{male and female}} voices provided within the AVR interface that can read the text aloud.|$|R
40|$|Linear modal {{synthesis}} (LMS) {{has been}} investigated to synthesize virtual sound of rigid bodies. The existing methods to synthesize realistic sound of virtual objects lack the real-time computations of perceptually satisfactory material parameters. In this study, a novel approach {{is used for the}} estimation of the material parameters that record the intrinsic quality of interacting materials and to extract significant features from the recorded impulse responses. A parameter estimation technique based on psychoacoustic principles is developed that incorporates material features to search the best material parameters for LMS. The <b>synthesized</b> <b>audio</b> produced from LMS shows the same perception of material as a recorded sound. This work was supported by the Global Frontier R&D Program on funded by the National Research Foundation of Korea grant funded by the Korean Goverment (MSIP) (2013 M 3 A 6 A 3079356...|$|E
40|$|We {{present a}} novel {{approach}} to labeling {{a large amount of}} training data for vocal/non-vocal discrimination in musical audio with the minimum amount of human labor. To this end, we use MIDI files for which vocal lines are encoded on a separate channel and synthesize them to create audio files. We then align <b>synthesized</b> <b>audio</b> with real recordings using dynamic time warping (DTW) algorithm. Note onset/offset information encoded in vocal lines in MIDI files provides precise vocal/non-vocal boundaries and we obtain from the minimum-cost alignment path the corresponding boundaries in actual recordings. This near labor-free labeling process allows us to acquire a large training data set, and the experiments show promising results when tested on an independent test set, using hidden Markov models as a classifier. We also demonstrate that the data generated by the proposed system is good data by showing that the overall performance increases with more training data. KEY WORDS singing voice detection, supervised learning, automatic labeling, MIDI, dynamic time warping...|$|E
40|$|Shimmer is a {{classical}} acoustic {{measure of the}} amplitude perturbation of a signal. This kind of variation in the human voice allow to characterize some properties, {{not only of the}} voice itself, but of the person who speaks. During the last years deep learning techniques have become {{the state of the art}} for recognition tasks on the voice. In this work the relationship between shimmer and deep neural networks is analyzed. A deep learning model is created. It is able to approximate shimmer value of a simple <b>synthesized</b> <b>audio</b> signal (stationary and without formants) taking the spectrogram as input feature. It is concluded firstly, that for this kind of synthesized signal, a neural network like the one we proposed can approximate shimmer, and secondly, that the convolution layers can be designed in order to preserve the information of shimmer and transmit it to the following layers. Eje: XVIII Workshop de Agentes y Sistemas Inteligentes (WASI) ...|$|E
25|$|CD-MIDI is {{a format}} used to store music-performance data which upon {{playback}} is performed by electronic instruments that <b>synthesize</b> the <b>audio.</b> Hence, unlike the original Red Book CD-DA, these recordings are not digitally sampled audio recordings. The CD-MIDI format {{is defined as}} an extension to the original Red Book.|$|R
40|$|ABSTRACT Cluster-Weighted Modeling, {{a mixture}} density {{estimator}} around local models, {{is presented as}} a framework for the analysis, prediction and characterization of non-linear time series. First architecture, model estima-tion and characterization formalisms are introduced. The characterization tools include estimator uncertainty, predictor uncertainty and the corre-lation dimension of the data set. In the second part of this chapter the framework is extended to <b>synthesize</b> <b>audio</b> signals and is applied to model a violin in a data-driven input-output approach. The list of time series worthwhile to be forecast is about as long as the rst unsuccessful attempts to do so. It would be most helpful to know beforehand when a heart is about to stop beating, what the weather will be like tomorrow and when the stock market is going to crash. Unfortunately...|$|R
40|$|This paper {{describes}} {{a method for}} creating new music by concatenative synthesis. Given a MIDI score and an audio recording of an example piece of monophonic music, our method <b>synthesizes</b> <b>audio</b> to correspond with a new MIDI score. The algorithm we use is based on concatenative synthesis, commonly used for generating speech. Two versions of our algorithm are explored, one in which individual notes from the example piece are concatenated, and one in which pairs of adjacent notes from the example piece are concatenated. We examine the range of example pieces and target scores for which each version of our algorithm yields good results. Our underlying framework remains general enough to be applicable to other problems, such as rendering a stylized version of the target score, {{and other types of}} sound analogies. 1...|$|R
