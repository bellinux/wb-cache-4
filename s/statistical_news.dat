6|9|Public
50|$|Notable GUS {{publications}} include Rocznik Statystyczny (Statistical Yearbook), Mały Rocznik Statystyczny (Concise Statistical Yearbook), and Wiadomości Statystyczne (<b>Statistical</b> <b>News).</b>|$|E
50|$|Abelen {{was born}} in Osnabrück, Germany. He worked as {{merchant}} in New York City, but returned to Europe due to a chest affliction. A friend of his brother Friedrich, the young Count of Görz-Schliz, urged him to begin his law studies in Göttingen and Berlin. Beginning in autumn 1846 he began to write, influenced by his stay in the United States and <b>statistical</b> <b>news.</b>|$|E
40|$|With so many Indonesian Workers {{who work}} abroad {{showed that the}} {{government}} 2 ̆ 7 s ability to provide jobs is very limited. <b>Statistical</b> <b>news</b> 2016 states {{that the number of}} placement of Indonesian workers abroad in 2016 amounted to 153. 804 with details of Indonesian manpower formal 82. 443 Indonesian workers and informal 71. 361 Indonesian workers. Comparing the two types of workers in the formal and informal sectors, the subject-matter of the author adopted is to provide protection to informal workers thus have a legal position that is better and stronger through a certificate of competence so as {{to protect the rights of}} workers and bargaining power balanced between giver employers and workers...|$|E
40|$|We {{describe}} a visualization of <b>statistical</b> analysis of <b>news</b> data, specifically the headlines of the New York Times. We detail its implementation and its integration with our existing collaborative visualization framework. We then discuss {{the advantages and}} disadvantages of the visualization and outline a number of further directions we wish to take this project. Author Keywords Visualization, collaborative work, large text corpora...|$|R
30|$|The {{paper will}} also {{evaluate}} shopping behavior in various domains {{within the context}} of mobile devices and cloud. The focus on this topic is due to its surging popularity and development within the e-commerce portals. The increasing popularity and developments within the e-commerce portals lead to the need to address requirements for privacy and security. This paper will draw on extensive research from varied areas such as <b>statistical</b> approaches, <b>news</b> reports, academic sources, and databases. The research will assist in understanding the interaction of privacy with other components of a system/business, the factors impacting privacy, the challenges faced by privacy, and the possible ways that the challenges and their impacts can be reduced. This paper will also focus on keyword analysis, and will look at shopping behavior in various domains {{within the context of}} the mobile device.|$|R
40|$|A new multivariate {{stochastic}} volatility {{model is}} developed in this paper. The main {{feature of this}} model is to allow threshold asymmetry in a factor covariance structure. The new model provides a parsimonious characterization of volatility and correlation asymmetry in response to market <b>news.</b> <b>Statistical</b> inferences are drawn from Markov chain Monte Carlo methods. We introduce news impact analysis to analyze volatility asymmetry with a factor structure. This analysis helps us to study different responses of volatility to historical market information in a multivariate volatility framework. Our model is successful when applied to an extensive empirical study of twenty stocks. Copyright (C) 2009 John Wiley & Sons, Ltd...|$|R
40|$|This thesis {{examines}} {{the use of}} peer-reviewed data and statistics in news communication of science through a content analysis and close reading analysis of statistical data in the United Kingdom science news and in-depth interviews with science journalists. The content analysis yields three key insights into the use of science data in the United Kingdom and Brazilian press: (1) statistics are used overwhelmingly to treat science as hard news, (2) there is an immense lack of fundamental background information about how the reported data are produced and (3) science journalists tend to use peer-reviewed data in a unique fashion: their stories include either too few or too many statistics from original sources. The in-depth interviews attempt to explain this content pattern, examining how journalists access and interpret quantitative data when producing stories about science, the nature of <b>statistical</b> <b>news</b> sources that they regularly use, and how they evaluate and treat such sources in articulating science news stories. Overall, this research finds that journalists tend to see and use statistics mainly to maintain the strategic ritual of objectivity in their social construction of science. The findings {{will be discussed in}} relation to a comprehensive body of literature on the use and abuse of statistical information as a key tool in the construction of journalistic objectivity...|$|E
40|$|Abstract. This paper {{describes}} {{the framework of}} the StatCan Daily Translation Extraction System (SDTES), a computer system that maps and compares webbased translation texts of Statistics Canada (StatCan) news releases in the StatCan publication The Daily. The goal is to extract translations for translation memory systems, for translation terminology building, for cross-language information retrieval and for corpus-based machine translation systems. Three years of officially published <b>statistical</b> <b>news</b> release texts at www. statcan. ca were collected to compose the StatCan Daily data bank. The English and French texts in this collection were roughly aligned using the Gale-Church statistical algorithm. After this, boundary markers of text segments and paragraphs were adjusted and the Gale-Church algorithm was run a second time for a more fine-grained text segment alignment. To detect misaligned areas of texts and to prevent mis-matched translation pairs from being selected, key textual and structural properties of the mapped texts were automatically identified and used as anchoring features for comparison and misalignment detection. Results show that SDTES is very efficient in extracting translations from Daily texts, and very accurate in identifying mismatched translations. With parameters tuned, the text-mapping part can be used to align officially published bilingual government web-site materials; and the text-comparing component can be applied in pre-publication translation quality control and in evaluating the results of statistical machine translation systems. Key words: automatic translation extraction, bi-text mapping, machine translation, parallel alignment, translation memory system. ...|$|E
40|$|This paper {{describes}} {{the framework of}} the StatCan Daily Translation Extraction System (SDTES), a computer system that maps and compares web-based translation texts of Statistics Canada (StatCan) news releases in the StatCan publication The Daily. The goal is to extract translations for translation memory systems, for translation terminology building, for cross-language information retrieval and for corpus-based machine translation systems. Three years of officially published <b>statistical</b> <b>news</b> release texts at [URL] were collected to compose the StatCan Daily data bank. The English and French texts in this collection were roughly aligned using the Gale-Church statistical algorithm. After this, boundary markers of text segments and paragraphs were adjusted and the Gale-Church algorithm was run a second time for a more fine-grained text segment alignment. To detect misaligned areas of texts and to prevent mismatched translation pairs from being selected, key textual and structural properties of the mapped texts were automatically identified and used as anchoring features for comparison and misalignment detection. The proposed method has been tested with web-based bilingual materials from five other Canadian government websites. Results show that the SDTES model is very efficient in extracting translations from published government texts, and very accurate in identifying mismatched translations. With parameters tuned, the text-mapping part can be used to align corpus data collected from official government websites; and the text-comparing component can be applied in prepublication translation quality control and in evaluating the results of statistical machine translation systems. © 2008 Springer Science+Business Media B. V...|$|E
30|$|Our {{approach}} {{to measure the}} novelty and topicality of news {{is closely related to}} recent studies on the application of text mining techniques to the analysis of financial market activities. Specifically, {{it has been shown that}} linguistic and <b>statistical</b> characteristics of <b>news</b> articles extracted using text mining techniques contain useful information to predict future stock price changes and trading volumes [28 – 33]. Also, in the context of information filtering, several new methods for detecting and eliminating redundant text in blogs and on twitters have been developed and applied to the identification of the novelty content of social networking service (SNS) texts [34 – 39].|$|R
25|$|Frank Carter Duckworth MBE (born 26 December 1939 in Lytham St Annes, Lancashire) is {{a retired}} English statistician, {{and is one of}} the two statisticians who {{developed}} the Duckworth–Lewis method of resetting targets in interrupted one-day cricket matches. He studied physics (BSc Hons 1961) and earned a PhD (1965) in metallurgy at the University of Liverpool. Prior to his retirement, he worked as a mathematical scientist for the English nuclear power industry. He is a consultant statistician to the International Cricket Council, and the editor of the Royal <b>Statistical</b> Society's monthly <b>news</b> magazine, RSS News. He also served on the editorial board of Significance before stepping down in 2010. In 2004 he delivered the Royal Statistical Society Schools Lecture, entitled Lies and Statistics.|$|R
40|$|This paper {{explores the}} {{relationship}} between daily market volatility {{and the arrival of}} public information in four different financial markets. Public information is measured as the daily number of economic news headlines, divided in six categories of <b>news.</b> <b>Statistical</b> analysis of the news data suggests the presence of particular seasonality effects, as well as a strong degree of autocorrelation. Over the period 1994 - 1998, significant effects of specific news categories on the volatility of US stocks, treasury bills, bonds and dollar were detected. However, the effects - in size and duration - vary by news category and by financial market. It is demonstrated that most of the volatility persistence, as observed by GARCH models, tends to disappear when news is included in the conditional variance equation. news, volatility, persistence, autocorrelation, GARCH,...|$|R
40|$|The {{dynamics}} of online content popularity has attracted {{more and more}} researches in recent years. In this paper, we provide a quantitative, temporal analysis about the {{dynamics of}} online content popularity in a massive system: Sina Microblog. We use time-stamped data to investigate the impact of bursty human comment patterns on the popularity of online microblog <b>news.</b> <b>Statistical</b> {{results indicate that the}} number of news and comments exhibits an exponential growth. The strength of forwarding and comment is characterized by bursts, displaying fat-tailed distribution. In order to characterize the dynamics of popularity, we explore the distribution of the time interval Δt between consecutive comment bursts and find that it also follows a power-law. Bursty patterns of human comment are responsible for the power-law decay of popularity. These results are well supported by both the theoretical analysis and empirical data...|$|R
40|$|The use of data {{is often}} viewed as a {{potentially}} powerful democratic force in journalism, promoting {{the flow of information}} sources and enriching debates in the public sphere. We explore a key feature of the relationship between data and journalism, drawing upon the largest ever study of <b>statistical</b> references in <b>news</b> reporting (N= 4285) commissioned by the BBC Trust to examine how statistics inform coverage {{in a wide range of}} UK television, radio and online media (N= 6916). Overall, our study provides a cautionary tale about the use of data to enlighten democratic debate. While we found that statistics were often referenced in news coverage, their role in storytelling was often vague, patchy and imprecise. Political and business elites were the main actors’ referencing statistics and interpreting them, but most of their claims were neither questioned nor interrogated further by journalists, with statistics often traded by opposing sides of an argument without independent analysis. In order to enhance the independent scrutiny of statistics, we argue a radical shift in newsgathering and journalistic interpretation is needed, which allows reporters to draw on a wider range of statistical sources and to adopt more critical judgements based on the weight of statistical evidence...|$|R

