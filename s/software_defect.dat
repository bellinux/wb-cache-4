225|475|Public
2500|$|I {{am trying}} to ferret out some very {{specific}} information about monitoring <b>software</b> <b>defect</b> trends. Again, it was years ago but I was a software development Program Manager and my team used a tool that tracked the progress of a defect find/fix rate over time. This trend was called a [...] "glide slope". It was not a crystal ball but was reasonably illustrative in forecasting code stability and release date. I can't locate my copy of the algorithm and have been doing an Internet search with no luck. Thanks for any help that you can provide.|$|E
50|$|A <b>Software</b> <b>Defect</b> Indicator is {{a pattern}} {{that can be found}} in source code that is {{strongly}} correlated with a <b>software</b> <b>defect,</b> an error or omission in the source code of a computer program that may cause it to malfunction. When inspecting the source code of computer programs, it is not always possible to identify defects directly, but there are often patterns, sometimes called anti-patterns, indicating that defects are present.|$|E
5000|$|... #Caption: The bug from 1947 {{which is}} at the origin of a popular (but incorrect) {{etymology}} for the common term for a <b>software</b> <b>defect.</b>|$|E
40|$|AbstractSoftware defects {{classification}} is {{the basis}} for effective management of <b>software</b> <b>defects.</b> Current air on-board software testing classification GJB 437 is too easy, coding defects classification in other <b>software</b> <b>defects</b> classification does not meet our air on-board software code review facts. The deficiencies of existing classifications of <b>software</b> <b>defects</b> are analyzed. And a classification for air on-board <b>software</b> code <b>defects</b> is presented. This classification is verified and completed by investigating the historical code defects of air on-board software. The revised classification has covered selected historical defects perfectly. The results of verification show that the revised classification of code defects can guide defects management effectively. At the same time, defects database is established using typical air on-board <b>software</b> code <b>defects.</b> Defects management is implemented based on <b>defects</b> database, guiding <b>software</b> <b>defects</b> detection and prevention...|$|R
40|$|An {{important}} goal during {{the cycle of}} software development is to find and fix existing defects as early as possible. This {{has much to do}} with <b>software</b> <b>defects</b> prediction and management. Nowadays,many  big software development companies have their own development repository, which typically includes a version control system and a bug tracking system. This has no doubt proved useful for <b>software</b> <b>defects</b> prediction. Since the 1990 s researchers have been mining software repository to get a deeper understanding of the data. As a result they have come up with some <b>software</b> <b>defects</b> prediction models the past few years. There are basically two categories among these prediction models. One category is to predict how many defects still exist according to the already captured defects data in the earlier stage of the software life-cycle. The other category is to predict how many defects there will be in the newer version software according to the earlier version of the <b>software</b> <b>defects</b> data. The complexities of software development bring a lot of issues which are related with <b>software</b> <b>defects.</b> We have to consider these issues as much as possible to get precise prediction results, which makes the modeling more complex. This thesis presents the current research status on <b>software</b> <b>defects</b> classification prediction and the key techniques in this area, including: software metrics, classifiers, data pre-processing and the evaluation of the prediction results. We then propose a way to predict <b>software</b> <b>defects</b> classification based on mining software repository. A way to collect all the defects during the development of software from the Eclipse version control systems and map these defects with the defects information containing in <b>software</b> <b>defects</b> tracking system to get the statistical information of <b>software</b> <b>defects,</b> is described. Then the Eclipse metrics plug-in is used to get the software metrics of files and packages which contain defects. After analyzing and preprocessing the dataset, the tool(R) is used to build a prediction models on the training dataset, in order to predict <b>software</b> <b>defects</b> classification on different levels on the testing dataset, evaluate the performance of the model and comparedifferent models’ performance...|$|R
40|$|Some <b>software</b> <b>defects</b> can be {{detected}} only if we consider all possible combinations of three, four, or more inputs. However, empirical data shows {{that the overwhelming majority}} of <b>software</b> <b>defects</b> are detected during pairwise testing, when we only test the software on combinations of pairs of different inputs. In this paper, we provide a possible theoretical explanation for the corresponding empirical data...|$|R
50|$|In {{the context}} of {{software}} quality, defect criticality {{is a measure of}} the impact of a <b>software</b> <b>defect.</b> It is defined as the product of severity, likelihood, and class.|$|E
5000|$|September 9, 2016, General Motors {{recalled}} over 4 million vehicles (brands such as Buick, Chevrolet, GMC and Cadillac) {{after an}} air bag <b>software</b> <b>defect</b> kills {{one person and}} injures three people.|$|E
5000|$|The Fenton and Neil paper [...] "A {{critique}} of <b>software</b> <b>defect</b> prediction models" [...] placed in top 1% most influential papers in its field based on number of citations (according to Essential Science Indicators).|$|E
40|$|This paper {{analyzes}} {{the influence of}} evolution activities such as refactoring on <b>software</b> <b>defects.</b> In {{a case study of}} five open source projects we used attributes of software evolu-tion to predict defects in time periods of six months. We use versioning and issue tracking systems to extract 110 data mining features, which are separated into refactoring and non-refactoring related features. These features are used as input into classification algorithms that create prediction models for <b>software</b> <b>defects.</b> We found out that refactoring related features as well as non-refactoring related features lead to high quality prediction models. Additionally, we discovered that refactorings and defects have an inverse cor-relation: The number of <b>software</b> <b>defects</b> decreases, if the number of refactorings increased in the preceding time pe-riod. As a result, refactoring should be a significant part of both bug fixes and other evolutionary changes to reduce <b>software</b> <b>defects.</b> Categories and Subject Descriptors D. 2. 8 [Software Engineering]: Evolution—software defects, pre-dictio...|$|R
5000|$|Variable capture can {{introduce}} <b>software</b> <b>defects.</b> This happens {{in one of}} the following two ways: ...|$|R
40|$|Abstract—Change {{coupling}} is {{the implicit}} relationship between {{two or more}} software artifacts that have been observed to frequently change together during {{the evolution of a}} software system. Researchers have studied this dependency and have observed that it points to design issues such as architectural decay. It is still unknown whether change coupling correlates with a tangible effect of design issues, i. e., <b>software</b> <b>defects.</b> In this paper we analyze the relationship between change coupling and <b>software</b> <b>defects</b> on three large software systems. We investigate whether change coupling correlates with defects, and if the performance of bug prediction models based on software metrics can be improved with change coupling information. Keywords-Change coupling; <b>Software</b> <b>defects</b> I...|$|R
50|$|Visual Intercept is a Microsoft Windows based <b>software</b> <b>defect</b> {{tracking}} system produced by Elsinore Technologies Inc.. Visual Intercept was actively sold from 1995 until early 2006 {{when it was}} integrated as a single solution in the broader IssueNet issue management system, also produced by Elsinore Technologies Inc..|$|E
5000|$|Rational ClearQuest is an {{enterprise}} level workflow automation tool from the Rational Software division of IBM. Commonly, ClearQuest is configured as a bug tracking system, {{but it can}} be configured to act as a CRM tool or to track a complex manufacturing process. It can also implement these functions together. IBM provides a number of predefined [...] "schemas" [...] for common tasks such as <b>software</b> <b>Defect</b> Tracking which can themselves be further customized if required.|$|E
50|$|Xerox {{subsequently}} {{acknowledged that}} this was a long-standing <b>software</b> <b>defect,</b> and their initial statements in suggesting that only non-factory settings could introduce the substitution were incorrect. Patches that comprehensively address the problem were published later in August, but no attempt has been made to recall or mandate updates to the affected devices - which was acknowledged to affect more than a dozen product families. Documents previously scanned continue to potentially contain errors making their veracity difficult to substantiate. German and Swiss regulators have subsequently (in 2015) disallowed the JBIG2 encoding in archival documents.|$|E
5000|$|Property Damage: <b>Software</b> <b>defects</b> {{can cause}} {{property}} damage. Poor software security allows hackers to steal identities, costing time, money, and reputations.|$|R
40|$|Want {{to improve}} testing and quality {{assurance}} efforts? These authors discuss careful handling and tracking of software bugs {{by using a}} database and a little organization. by Lisa Anderson and Brenda Francis Comparing Lean Six Sigma to the Capability Maturity Model A comparison of differences and commonalties between these two process improvement efforts shows that adding Lean Six Sigma to the mix can further reduce <b>software</b> <b>defects.</b> by Dr. Kenneth D. Shere Managing <b>Software</b> <b>Defects</b> in an Object-Oriented Environmen...|$|R
40|$|Abstract—There are {{numerous}} studies that examine {{whether or not}} cloned code is harmful to software systems. Yet, few of them study which characteristics of cloned code in particular lead to <b>software</b> <b>defects.</b> In our work, we use survival analysis to understand the impact of clones on <b>software</b> <b>defects</b> and to determine the characteristics of cloned code that have the highest impact on <b>software</b> <b>defects.</b> Our survival models express the risk of defects in terms of basic predictors inherent to the code (e. g., LOC) and cloning predictors (e. g., number of clone siblings). We perform a case study using two clone detection tools on two large, long-lived systems using survival analysis. We determine that the defect-proneness of cloned methods is specific to the system under study and that more resources should be directed towards methods with a longer 'commit history'. I...|$|R
5000|$|I {{am trying}} to ferret out some very {{specific}} information about monitoring <b>software</b> <b>defect</b> trends. Again, it was years ago but I was a software development Program Manager and my team used a tool that tracked the progress of a defect find/fix rate over time. This trend was called a [...] "glide slope". It was not a crystal ball but was reasonably illustrative in forecasting code stability and release date. I can't locate my copy of the algorithm and have been doing an Internet search with no luck. Thanks for any help that you can provide.|$|E
5000|$|Orthogonal Defect Classification (ODC) turns {{semantic}} {{information in}} the <b>software</b> <b>defect</b> stream into a measurement on the process. The ideas were developed in the late '80s and early '90s by Ram Chillarege at IBM Research. This {{has led to the}} development of new analytical methods used for software development and test process analysis. ODC is process model, language and domain independent. Applications of ODC have been reported by several corporations on a variety of platforms and development processes, ranging from waterfall, spiral, gated, and agile [...] development processes. One of the popular applications of ODC is software root cause analysis. ODC is known to reduce the time taken to perform root cause analysis by over a factor of 10. The gains come primarily from a different approach to root cause analysis, where the ODC data is generated rapidly (in minutes, as opposed to hours per defect) and analytics used for the cause and effect analysis. This shifts the burden of analysis from a purely human method to one that is more data intensive.|$|E
40|$|Software {{development}} involves {{plenty of}} risks, and errors exist in software modules represent a major kind of risk. <b>Software</b> <b>defect</b> prediction techniques and tools that identify software errors {{play a crucial}} role in software risk management. Among <b>software</b> <b>defect</b> prediction techniques, classification is a commonly used approach. Various types of classifiers have been applied to <b>software</b> <b>defect</b> prediction in recent years. How to select an adequate classifier (or set of classifiers) to identify error prone software modules is an important task for software development organizations. There are many different measures for classifiers and each measure is intended for assessing different aspect of a classifier. This paper developed a performance metric that combines various measures to evaluate the quality of classifiers for <b>software</b> <b>defect</b> prediction. The performance metric is analyzed experimentally using 13 classifiers on 11 public domain <b>software</b> <b>defect</b> datasets. The results of the experiment indicate that support vector machines (SVM), C 4. 5 algorithm, and K-nearest-neighbor algorithm ranked the top three classifiers. Classification, software risk management, <b>software</b> <b>defect</b> prediction, performance metric...|$|E
40|$|Improving {{software}} assurance is {{of paramount}} importance given the impact of software on our lives. Static and dynamic approaches have been proposed over the years to detect security vulnerabilities. These approaches assume that the signature of a defect, for instance the use of a vulnerable library function, is known apriori. A greater challenge is detecting defects with signatures that are not known apriori – unknown <b>software</b> <b>defects.</b> In this paper, we propose a general approach for detection of unknown <b>defects.</b> <b>Software</b> <b>defects</b> are discovered by applying data-mining techniques to pinpoint deviations from common program behavior in the source code and using statistical techniques to assign significance to each such deviation. We discuss the implementation of our tool, FaultMiner, and illustrate the power of the approach by inferring two types of security properties on four widely-used programs. We found two new potential vulnerabilities, four previously known bugs, and several other violations. This suggests that FaultMining is a useful and promising approach to finding unknown <b>software</b> <b>defects.</b> ...|$|R
40|$|This paper {{analyses}} {{the effect}} of the effort distribution along the software development lifecycle on the prevalence of <b>software</b> <b>defects.</b> This analysis is based on data that was collected by the International Software Benchmarking Standards Group (ISBSG) on the development of 4, 106 software projects. Data mining techniques have been applied {{to gain a better understanding}} of the behaviour of the project activities and to identify a link between the effort distribution and the prevalence of <b>software</b> <b>defects.</b> This analysis has been complemented with the use of a hierarchical clustering algorithm with a dissimilarity based on the likelihood ratio statistic, for exploratory purposes. As a result, different behaviours have been identified for this collection of software development projects, allowing for the definition of risk control strategies to diminish the number and impact of the <b>software</b> <b>defects.</b> It is expected that the use of similar estimations might greatly improve the awareness of project managers on the risks at hand...|$|R
40|$|<b>Software</b> <b>defects</b> {{prediction}} aims {{to reduce}} software testing efforts by guiding the testers through the <b>defect</b> classification of <b>software</b> systems. <b>Defect</b> predictors {{are widely used}} in many organizations to predict <b>software</b> <b>defects</b> {{in order to save}} time, improve quality, testing and for better planning of the resources to meet the timelines. The application of statistical <b>software</b> testing <b>defect</b> prediction model in a real life setting is extremely difficult because it requires more number of data variables and metrics and also historical defect data to predict the next releases or new similar type of projects. This paper explains our statistical model, how it will accurately predict the <b>defects</b> for upcoming <b>software</b> releases or projects. We have used 20 past release data points of software project, 5 parameters and build a model by applying descriptive statistics, correlation and multiple linear regression models with 95 % confidence intervals (CI). In this appropriate multiple linear regression model the R-square value was 0. 91 and its Standard Error is 5. 90 %. The <b>Software</b> testing <b>defect</b> prediction model is now being used to predict defects at various testing projects and operational releases. We have found 90. 76 % precision between actual and predicted defects...|$|R
40|$|Due to the {{popularity}} of mobile devices and increasing demands of software applications, more and more individual developers join this industry. However, software defects top {{at the cost of}} software development. Software metrics are able to show some indication of <b>software</b> <b>defect.</b> This paper reviews popular static code and object-oriented metrics and summarizes heuristics for using the metrics. Correlations between <b>software</b> <b>defect</b> and metrics are presented. Finally, advantages and disadvantages of metrics are discussed. According to the summary of correlation analyses, some metricsshow inconsistent relationships with <b>software</b> <b>defect.</b> Implications to practice and research are provided...|$|E
40|$|<b>Software</b> <b>defect</b> curves {{describe}} {{the behavior of}} the estimate of the number of remaining software defects as software testing proceeds. They are of two possible patterns: single-trapezoidal-like curves or multiple-trapezoidal-like curves. In this paper we present some necessary and/or sufficient conditions for <b>software</b> <b>defect</b> curves of the Goel-Okumoto NHPP model. These conditions can be used to predict the effect of the detection and removal of a <b>software</b> <b>defect</b> on the variations of the {{estimates of the number of}} remaining defects. A field software reliability dataset is used to justify the trapezoidal shape of <b>software</b> <b>defect</b> curves and our theoretical analyses. The results presented in this paper may provide useful feedback information for assessing software testing progress and have potentials in the emerging area of software cybernetics that explores the interplay between software and control...|$|E
40|$|<b>Software</b> <b>defect</b> {{predictors}} {{are useful}} {{to maintain the}} high quality of software products effectively. The early prediction of defective software modules can help the software developers to allocate the available resources to deliver high quality software products. The objective of <b>software</b> <b>defect</b> prediction system is to find as many defective software modules as possible without affecting the overall performance. The learning process of a <b>software</b> <b>defect</b> predictor is difficult due to the imbalanced distribution of software modules between defective and nondefective classes. Misclassification cost of defective software modules generally incurs much higher cost than the misclassification of nondefective one. Therefore, on considering the misclassification cost issue, we have developed a <b>software</b> <b>defect</b> prediction system using Weighted Least Squares Twin Support Vector Machine (WLSTSVM). This system assigns higher misclassification cost to the data samples of defective classes and lower cost to the data samples of nondefective classes. The experiments on eight <b>software</b> <b>defect</b> prediction datasets have proved {{the validity of the}} proposed defect prediction system. The significance of the results has been tested via statistical analysis performed by using nonparametric Wilcoxon signed rank test...|$|E
40|$|Abstract — Software Inspection is used {{to enhance}} {{software}} quality by detecting and removing <b>software</b> <b>defects</b> that mar <b>software</b> quality. <b>Software</b> design <b>defects</b> are design flaws which might not stop application from working but will hamper the process of extension and reuse of application. <b>Software</b> design <b>defects</b> manifest themselves {{in the form of}} AntiPatterns and Code Smells. In Software Inspection, we can detect Code Smells and Anti Patterns by using visualization or metrics based solution. But, currently there is no process that defines the steps to detect Code Smells and Anti Patterns by using Rule Based Checklist. In this paper, we propose Process-based Detection of Code Smells and Anti Patterns. We have defined Rule Set and Checklist for <b>software</b> design <b>defects</b> i. e. Code Smells and Anti Patterns and applied and validated the Rules set and Checklists on an industrial project...|$|R
5000|$|Life and Death: <b>Software</b> <b>defects</b> can kill. Some {{embedded}} systems used in radiotherapy machines failed so catastrophically that they administered lethal doses of radiation to patients. The {{most famous of}} these failures is the Therac-25 incident.|$|R
40|$|Design defects, antipatterns, code smells are <b>software</b> <b>defects</b> at the {{architectural}} level {{that must be}} detected and corrected to improve software quality. Automatic detection and correction of these <b>software</b> architectural <b>defects,</b> which suffer {{of a lack of}} tools, are important to ease the maintenance of objectoriented architectures and thus to reduce the cost of maintenance. A clear understanding of the different types of <b>software</b> architectural <b>defects</b> defects and a classification of these defects is necessary before proposing any techniques related to their detection or correction. We introduce a first classification and summarise existing techniques. Then, we introduce some challenges that our community must meet...|$|R
40|$|This {{research}} analysis work {{reports on}} a set of empirical studies tackling the research issues of improving <b>software</b> <b>defect</b> estimation models with Sigma defect measures (e. g., Sigma levels) using the ISBSG data repository with a high ratio of missing data. Three imputation techniques that were selected for this research work: single imputation, regression imputation, and stochastic regression imputation. These imputation techniques were used to impute the missing data within the variable ‘Total Number of Defects’, and were first compared with each other using common verification criteria. A further verification strategy was developed to compare and assess the performance of the selected imputation techniques through verifying the predictive accuracy of the obtained <b>software</b> <b>defect</b> estimation models form the imputed datasets. A Sigma-based classification was carried out on the imputed dataset of the better performance imputation technique on <b>software</b> <b>defect</b> estimation. This classification was used to determine at which levels of Sigma; the software projects can be best used to build <b>software</b> <b>defect</b> estimation models: which has resulted in Sigma-based datasets with Sigma ranging (e. g., dataset of software projects with a range from 3 Sigma to 4 Sigma). Finally, <b>software</b> <b>defect</b> estimation models were built on the Sigma-based datasets...|$|E
40|$|Abstract—Software defect {{prediction}} {{can help to}} allocate testing resources efficiently through ranking software modules according to their defects. Existing <b>software</b> <b>defect</b> prediction models that are optimized to predict explicitly the number of defects in a software module might fail to give an accurate order because it is very dif-ficult to predict {{the exact number of}} defects in a software module due to noisy data. This paper introduces a learning-to-rank ap-proach to construct <b>software</b> <b>defect</b> prediction models by directly optimizing the ranking performance. In this paper, we build on our previous work, and further study whether the idea of directly optimizing the model performance measure can benefit <b>software</b> <b>defect</b> prediction model construction. The work includes two aspects: one is a novel application of the learning-to-rank approach to real-world data sets for <b>software</b> <b>defect</b> prediction, and the other is a comprehensive evaluation and comparison of th...|$|E
40|$|Classification {{algorithms}} {{that help}} to identify software defects or faults {{play a crucial role}} in software risk management. Experimental results have shown that ensemble of classifiers are often more accurate and robust to the effects of noisy data, and achieve lower average error rate than any of the constituent classifiers. However, inconsistencies exist in different studies and the performances of learning algorithms may vary using different performance measures and under different circumstances. Therefore, more research is needed to evaluate the performance of ensemble algorithms in <b>software</b> <b>defect</b> prediction. The goal of this paper is to assess the quality of ensemble methods in <b>software</b> <b>defect</b> prediction with the analytic hierarchy process (AHP), which is a multicriteria decision-making approach that prioritizes decision alternatives based on pairwise comparisons. Through the application of the AHP, this study compares experimentally the performance of several popular ensemble methods using 13 different performance metrics over 10 public-domain <b>software</b> <b>defect</b> datasets from the NASA Metrics Data Program (MDP) repository. The results indicate that ensemble methods can improve the classification results of <b>software</b> <b>defect</b> prediction in general and AdaBoost gives the best results. In addition, tree and rule based classifiers perform better in <b>software</b> <b>defect</b> prediction than other types of classifiers included in the experiment. In terms of single classifier, K-nearest-neighbor, C 4. 5, and Naïve Bayes tree ranked higher than other classifiers. Ensemble, classification, <b>software</b> <b>defect</b> prediction, the analytic hierarchy process (AHP) ...|$|E
50|$|The quality {{assurance}} {{is carried out}} by game testers. A game tester analyzes video games to document <b>software</b> <b>defects</b> {{as part of a}} quality control. Testing is a highly technical field requiring computing expertise, and analytic competence.|$|R
40|$|Abstract — As firms {{increasingly}} rely {{on information}} systems to perform critical functions {{the consequences of}} <b>software</b> <b>defects</b> can be catastrophic. Although the software engineering literature suggests that software process improvement can help to reduce <b>software</b> <b>defects,</b> the actual evidence is equivocal. For example, improved development processes may only remove the “easier ” syntactical defects, while the more critical defects remain. Rigorous empirical analyses of these relationships have been very difficult to conduct due to the difficulties in collecting the appropriate data on real systems from industrial organizations. This field study analyzes a detailed data set consisting of 7, 545 <b>software</b> <b>defects</b> that were collected on software projects completed at a major software firm. Our analyses reveal that higher levels of software process improvement significantly {{reduce the likelihood of}} high severity defects. In addition, we find that higher levels of process improvement are even more beneficial in reducing severe defects when the system developed is large or complex, but are less beneficial when requirements are ambiguous, unclear or incomplete. Our findings reveal the benefits and limitations of software process improvement for the removal of severe defects and suggest where investments in improving development processes may hav...|$|R
40|$|Copyright © 2014 Javier Alfonso-Cendón et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. This paper analyses {{the effect of the}} effort distribution along the software development lifecycle on the prevalence of <b>software</b> <b>defects.</b> This analysis is based on data that was collected by the International Software Benchmarking Standards Group (ISBSG) on the development of 4, 106 software projects. Data mining techniques have been applied {{to gain a better understanding}} of the behaviour of the project activities and to identify a link between the effort distribution and the prevalence of <b>software</b> <b>defects.</b> This analysis has been complemented with the use of a hierarchical clustering algorithmwith a dissimilarity based on the likelihood ratio statistic, for exploratory purposes. As a result, different behaviours have been identified for this collection of software development projects, allowing for the definition of risk control strategies to diminish the number and impact of the <b>software</b> <b>defects.</b> It is expected that the use of similar estimations might greatly improve the awareness of project managers on the risks at hand. 1...|$|R
