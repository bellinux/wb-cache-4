76|43|Public
25|$|A British scheme, QHA Trent Accreditation, is {{an active}} {{independent}} holistic accreditation scheme, as well as GCR.org which monitors the <b>success</b> <b>metrics</b> and standards of almost 500,000 medical clinics worldwide.|$|E
5000|$|While {{generally}} {{athletic directors}} are evaluated {{using a variety}} of <b>success</b> <b>metrics</b> relating to athletic and academic performance of student athletes, Hyman views his role as one of [...] "managing expectations", encouraging decisionmakers to accept a lower level of success.|$|E
50|$|Direct {{marketing}} {{is attractive to}} many marketers because its positive results can be measured directly. For example, if a marketer sends out 1,000 solicitations by mail and 100 respond to the promotion, the marketer can say with confidence that campaign led directly to 10% direct responses. This metric {{is known as the}} 'response rate,' and it is one of many clearly quantifiable <b>success</b> <b>metrics</b> employed by direct marketers. In contrast, general advertising uses indirect measurements, such as awareness or engagement, since there is no direct response from a consumer. Measurement of results is a fundamental element in successful direct marketing.|$|E
40|$|Abstract- The {{main reason}} behind failure {{of lots of}} {{software}} is poor quality thus estimation software quality become an important task in software industry. Late estimation of software quality results in ineffectiveness, late delivery and most important poor quality of software product. An early estimation {{plays an important role}} in shorting the time and by increases probability of project <b>success.</b> Software <b>metrics</b> are used to improve the quality and validity of software systems. This paper represents various metrics for estimation of quality of software product...|$|R
50|$|Third Frontier {{has also}} been able to measure its <b>success</b> through <b>metrics</b> such as job {{creation}} {{and the increase in}} productivity and quality in existing jobs. The most similar program to Ohio’s Third Frontier is the Start-up Brasil program implemented by the Brazilian federal government. Start-up Brasil was created by the Ministry of Science, Technology and Innovation (MCTI) - in order to provide support to emerging technology-based companies and to strengthen their connection to accelerators. The funds made available by the federal government for the program allow up to R$200,000 in direct investment for selected startups.|$|R
40|$|Abstract—Measurement {{programs}} in software organizations {{are an important}} source of control over quality and cost in software development. The findings of this research presented here are based on an industry-wide survey conducted to examine the factors that influence <b>success</b> in software <b>metrics</b> programs. Our approach is to go beyond the anecdotal information on metrics programs that exists in the literature and use the industry-wide survey data to rigorously test for the effects of various factors that affect <b>metrics</b> programs <b>success.</b> We measure <b>success</b> in <b>metrics</b> programs using two variables—use of metrics information in decision-making and improved organizational performance. The various determinants of <b>metrics</b> program <b>success</b> are divided into two sets—organizational variables and technical variables. The influence of these variables on <b>metrics</b> programs <b>success</b> is tested using regression analysis. Our results support some of the factors discussed in the anecdotal literature such as management support, goal alignment, and communication and feedback. Certain other factors such as metrics quality and the ease of data collection are not as strongly influential on success. We conclude the paper with a detailed discussion of our results and suggestions for future work. Index Terms—Software metrics programs, empirical methods, survey, regression analysis, software development, measurement programs, determinants of success, software engineering. ...|$|R
40|$|Abstract—With the Internet {{has come}} the {{phenomenon}} of people volunteering to work on digital public goods such as open source software and online encyclopedia articles. Presumably, the success of individual public goods {{has an effect on}} attracting volunteers. However, the definition of success is ill-defined. This paper explores the impact of different <b>success</b> <b>metrics</b> on a simple public goods model. The findings show that the different <b>success</b> <b>metrics</b> considered do {{have an impact on the}} behavior of the model, with the largest differences being between consumeroriented and producer-oriented metrics. This indicates that many proposed <b>success</b> <b>metrics</b> may be mapped into one of these two categories and within a category, all <b>success</b> <b>metrics</b> measure the same phenomenon. We argue that the characteristics of produceroriented metrics more closely match real world phenomena, indicating that public goods are driven by producer, and not consumer, interests. Index Terms—Digital public goods, <b>success</b> <b>metrics,</b> FLOSS, open source software, Wikipedia...|$|E
40|$|In this paper, DeLone and McLean’s updated {{information}} system model {{was used to}} evaluate the success of an e-Learning system and its courses in a transitional country like Serbia. In order to adapt this model to an e-Learning system, suitable <b>success</b> <b>metrics</b> were chosen for each of the evaluation stages. Furthermore, the <b>success</b> <b>metrics</b> for e-Learning evaluation are expanded by providing several systems for quantifying the given <b>success</b> <b>metrics.</b> The results presented in this paper are based on courses that were taught both online and traditionally in three different subject areas: graphic design, information technology, and management. Of particular interest were <b>success</b> <b>metrics</b> which can be determined using quantifiable data from the e-Learning system itself, in order to evaluate and find the relationship between students’ academic achievement, usage of learning materials, and students’ satisfaction. The results from different courses were used to illustrate the implementation and evaluation of these <b>success</b> <b>metrics</b> for both online and traditional students...|$|E
40|$|Spring 2001 Table of {{contents}} I. Executive Summary [...] 4 II. Business Case [...] 5 A. Business Problems [...] . 5 B. Existing Metrics [...] 5 C. Solutions [...] 5 D. <b>Success</b> <b>Metrics</b> [...] . ...|$|E
40|$|Abstract—The {{new product}} {{development}} (NPD) literature {{emphasizes the importance}} of introducing new products on the market for continuing business success. New products are responsible for employment, economic growth, technological progress, and high standards of living. Therefore, the study of NPD and the processes through which they emerge is important. The goal of our research is to propose a framework of critical <b>success</b> factors, <b>metrics,</b> and tools and techniques for implementing metrics for each stage of the new product development (NPD) process. An extensive literature review was undertaken to investigate decades of studies on NPD success and how it can be achieved. These studies were scanned for common factors for firms that enjoyed success of new products on the market. The paper summarizes NPD <b>success</b> factors, suggests <b>metrics</b> that should be used to measure these factors, and proposes tools and techniques to make use of these metrics. This was done for each stage of the NPD process, and brought together in a framework that the authors propose should be followed for complex NPD projects. While many studies have been conducted on critical success factors for NPD, these studies tend to be fragmented and focus on one or a few phases of the NPD process. Keywords—New product development, performance, critical success factors, framework...|$|R
40|$|A recent {{viewpoint}} article (Improving the plausibility of <b>success</b> with inefficient <b>metrics.</b> ACS Med. Chem. Lett. 2014, 5, 2 - 5) {{argued that}} the standard definition of ligand efficiency (LE) is mathematically invalid. In this viewpoint, we address this criticism and show categorically that the definition of LE is mathematically valid. LE and other metrics such as lipophilic ligand efficiency (LLE) can be useful during the multiparameter optimization challenge faced by medicinal chemists...|$|R
40|$|Purpose: The {{purpose of}} this paper is to propose a {{framework}} of critical <b>success</b> factors, <b>metrics,</b> and tools and techniques for implementing metrics for each stage of the new product development (NPD) process. Design/methodology/approach: To achieve this objective, a literature review was undertaken to investigate decades of studies on NPD success and how it can be achieved. These studies were scanned for common factors for firms that enjoyed success of new products on the market. Findings: The paper summarizes NPD <b>success</b> factors, suggests <b>metrics</b> that should be used to measure these factors, and proposes tools and techniques to make use of these metrics. This was done for each stage of the NPD process, and brought together in a framework that the authors propose should be followed for complex NPD projects. Research limitations/implications: Several different research directions could provide additional useful information both to firms finding critical success factors (CSF) and measuring product development success as well as to academics performing research in this area. The main research opportunity exists in implementing or testing the proposed framework. Practical implications: The framework can be followed by managers of complex NPD projects to ensure success. Originality/value: While many studies have been conducted on critical success factors for NPD, these studies tend to be fragmented and focus on one or a few phases of the NPD process. To the authors’ knowledge, this is the first time a framework that synthesizes these studies into a single framework. Peer Reviewe...|$|R
40|$|The {{purpose of}} this study is to examine how the {{dimensions}} of strategic enrolment management (SEM) tie to the <b>success</b> <b>metrics</b> in the area of enrolment, retention and graduation from senior and programme management perspectives of a self-financed institution in Hong Kong. The literature on SEM has demonstrated that managing enrolment is a global concern and requires institution-wide effort. For successful SEM, it has to be a performance-based, outcome-oriented system which requires significant data to determine its effectiveness, success or failure, growth or decline. Though the focus of most SEM research is about the implementation of SEM in achieving enrolment and retention goals, there are far fewer studies that look critically at the perceptions of success tying SEM to <b>success</b> <b>metrics.</b> This study fills the research gap regarding the perceptions of tying SEM to the success. Thus, <b>success</b> <b>metrics</b> are vital in assessing the achievement of enrolment, retention and graduation goals. This study employs a combination of survey results and a formal content analysis methodology from a series of in-depth face-to-face interviews with both senior and programme managers at a self-financed institution in Hong Kong. The research identifies the perceived importance of different components related to enrolment, retention and graduation successes and examines differences in these perceptions between senior management and programme management. New <b>success</b> <b>metrics</b> (availability of an Honours degree programme, the employability ratio and student learning outcomes) are found in both enrolment and graduation stages which are quantified as perceived SEM success in a self-financed institution in Hong Kong. School of Professional Education and Executive Developmen...|$|E
40|$|This session {{outlines}} {{steps to}} create and to sustain an inclusive campus climate. Presented strategies will focus on recruitment and retention, student and faculty mentoring, and community outreach. Strategy implementation recommendations discussed include gaining executive support, leveraging existing programs, and maximizing technology. Creating <b>success</b> <b>metrics</b> will also be covered...|$|E
30|$|The {{project charter}} {{for the process}} {{capability}} improvement of the stub-end hole boring operation of the crankshaft machining process is depicted in Table  1. The project charter outlines the objectives, deliverables and <b>success</b> <b>metrics</b> of this improvement project. The business impact in terms of monetary benefits is also reflected in the project charter.|$|E
40|$|We {{estimate}} {{the security of}} dictionary-based PINs (Personal Identification Numbers) that a user selects from his/her memory without any additional aids. The estimates {{take into account the}} distribution of words in source language. We use established security metrics, such as entropy, guesswork, marginal guesswork and marginal <b>success</b> rate. The <b>metrics</b> are evaluated for various scenarios – aimed at improving the security of the produced PINs. In general, plain and straightforward construction of memory-only dictionary PINs yields unsatisfactory results and more involved methods must be used to produce secure PINs. ...|$|R
40|$|Abstract- Army vehicle {{software}} {{interacts with}} complex electronics from multiple vendors. The software structure complexity {{is influenced by}} many factors prior to software development. Understanding, predicting and resolving complexity of vehicle software prior to its development is a necessity for Army mission <b>success.</b> Current complexity <b>metrics</b> focus on software and its technical structure with no consideration of its influencing factors. Non-technical metrics related to software complexity are required to address diverse skill set including the management. In this paper, the authors propose five non-technical factor metrics based on the current software development process to predict future Army vehicle software complexity. Fuzzy logic techniques are used for developing, modeling and analyzing the software complexity prediction metric...|$|R
40|$|The {{real-time}} {{collection and}} filtering {{of the enormous}} amounts of data resulting from training simulations involving hundreds of entities is a challenge for training system architects. Traditional approaches have relied on human observer/trainers (O/T) to tag key events and prepare the After Action Reviews (AAR), which identified what happened (particularly events contributing to <b>metrics</b> for mission <b>success),</b> why <b>metrics</b> were not met (precursor events {{that contributed to the}} metric events), how the critical sequences of events arose (identifying decision points), and provide timely learning points. For collective training simulations, the O/Ts are often overwhelmed in terms of tracking individual behaviors and skills. Automated approaches for capturing human performance data are a preferred method that can provide feedback to each member of a team or unit. This paper describes the application of a novel use of XML Stylesheet Language Transformations to process simulation event stream using an Event-Condition-Action (ECA) rule engine. The human performance data capture used the following process: 1. Data Collection from multiple sources, including IEEE 1278 Distributed Interactive Simulation (DIS) data and other network data. 2. Protocol Filtering, which is done to reduce the processing workload in the later stages of the pipeline...|$|R
40|$|Responding to {{the need}} for a better {{understanding}} of the factors that explain ERP systems implementation success, this chapter used a field study to collect data from managers working in Bahraini enterprises that use ERP systems to examine the influence of some selected factors on two perspectives: project and business success of such systems. Results support previous research findings in this area concerning the impact of factors such as project planning, organizational resistance, and ease of use on ERP project <b>success</b> <b>metrics.</b> Also the study results show that project planning; business process reengineering; and organizational fit have significant influence on business <b>success</b> <b>metrics.</b> However, no significant impact was found for some classical success factors such as top management support, technical fit, training, competitive pressure, and strategic fit on both project and business success. The chapter ends with implications for these findings and possible extensions for the study...|$|E
40|$|A {{recognized}} issue {{related to}} the processes in systems engineering is that they vary based on the project and organization. However, understanding how to tailor the systems engineering processes to help ensure project success continues to trouble engineers, and practical tools to help aid engineers in this field are not as readily available as many would like. Moreover, budget and schedule constraints continue to place an additional burden on systems engineering being done well. Many studies on how to help contribute to improving systems engineering exists for different organizations. Yet, most studies are based on case studies from one project, involve a small sample size, or only focus on one organization type. This thesis discusses the results of how systems engineering processes applied to complex projects impact project success based on organization type with a sample size of 180 institutions. The organizations examined are divided into two groups, commercial organizations and government organizations. Within the commercial organization, government-focused projects and commercial-focused projects are examined. Within the government organization, projects from the National Aeronautics and Space Administration (NASA) Agency are examined. NASA is a government organization comprised of 10 Centers located around the country, and {{for the purpose of}} this research, is the primary organization discussed and metric in which the other organizations will be quantitatively compared and contrasted against. For this reason, the standard participants used in this research effort were NASA 2 ̆ 7 s 17 systems engineering processes. Data was gathered through a modified data collection instrument, and a three-level data analysis was performed. First, meaningful correlations were identified between the systems engineering processes and project <b>success</b> <b>metrics,</b> as well as, non-technical variables and project <b>success</b> <b>metrics</b> for the different organizations. Then, a deeper data analysis was conducted to test for statistically significant differences through project description variables, project <b>success</b> <b>metrics,</b> and systems engineering processes across organization types. Finally, statistically significant differences among the project description variables, project <b>success</b> <b>metrics,</b> systems engineering processes, and non-technical variables were examined within each organization type. The results from the data analysis will be delivered to NASA to help aid in the development of a NASA systems engineering practitioners guide...|$|E
40|$|A new {{approach}} is suggested {{to define and}} evaluate key metrics as to autonomous aerial vehicle performance. This approach entails the conceptual definition of a “Turing Test ” for UAVs. Such a “UAV Turing test ” would be conducted by means of mission simulations and/or tailored flight demonstrations of vehicles {{under the guidance of}} their autonomous system software. These autonomous vehicle mission simulations and flight demonstrations would also have to be benchmarked against missions “flown ” with pilots/human-operators in the loop. In turn, scoring criteria for such testing could be based upon both quantitative mission <b>success</b> <b>metrics</b> (unique to each mission) and by turning to analog “handling quality ” metrics similar to the well-known Cooper-Harper pilot ratings used for manned aircraft. Autonomous aerial vehicles would be considered to have successfully passed this “UAV Turing Test ” if the aggregate mission <b>success</b> <b>metrics</b> and handling qualities for the autonomous aerial vehicle matched or exceeded the equivalent metrics for missions conducted with pilots/human-operators in the loop. Alternatively, an independent, knowledgeable observer could provide the “UAV Turing Test ” ratings of whether a vehicle is autonomous or “piloted. ” This observer ideally would – in the more sophisticated missio...|$|E
40|$|Real-time {{communication}} with performance guarantees is becoming {{very important to}} many applications, like computer integrated manufacturing, multimedia, and many embedded systems. Though several real-time protocols have been proposed for the multiple access bus networks, {{there is no guarantee}} based protocol which addresses the problem of integrated scheduling of dynamically arriving periodic and aperiodic messages. In this paper, we propose two guarantee based protocols, EDF and BUS protocols, which address this problem. In the simulation studies, the performance <b>metrics,</b> <b>success</b> ratio (measure of schedulability) and channel utilization are used to study the performance of the two protocols. It is observed that the EDF protocol offers higher schedulability as compared to the BUS protocol for periodic messages, while the BUS protocol offers better channel utilization for aperiodic messages as compared to the EDF protocol...|$|R
40|$|AbstractElectron Backscatter Diffraction (EBSD) {{provides}} a useful means for characterizing microstructure. However, {{it can be difficult}} to obtain index-able diffraction patterns from some samples. This can lead to noisy maps reconstructed from the scan data. Various post-processing methodologies have been developed to improve the scan data generally based on correlating non-indexed or mis-indexed points with the orientations obtained at neighboring points in the scan grid. Two new approaches are introduced (1) a re-scanning approach using local pattern averaging and (2) using the multiple solutions obtained by the triplet indexing method. These methodologies are applied to samples with noise introduced into the patterns artificially and by the operational settings of the EBSD camera. They are also applied to a heavily deformed and a fine-grained sample. In all cases, both techniques provide an improvement in the resulting scan data, the local pattern averaging providing the most improvement of the two. However, the local pattern averaging is most helpful when the noise in the patterns is due to the camera operating conditions as opposed to inherent challenges in the sample itself. A byproduct of this study was insight into the validity of various indexing <b>success</b> rate <b>metrics.</b> A metric based given by the fraction of points with CI values greater than some tolerance value (0. 1 in this case) was confirmed to provide an accurate assessment of the indexing success rate...|$|R
40|$|Zooarchaeologists cannot {{identify}} mammal species {{by their}} stylohyoid bones. Current trends in zooarchaeological research stress {{the need for}} rigorous and accessible identification methodology. I examined the stylohyoids of 15 hooved mammals: cattle, bison, domestic sheep, bighorn sheep, Dall sheep, mountain goat, domestic goat, elk, caribou, white-tailed deer, mule deer, moose, pronghorn antelope, domestic pig, and horse. Objectives included documenting how to side the stylohyoid (left or right), and producing species identification criteria based on large samples. A total of 325 samples were measured from eight repositories. Written descriptions, photographs, and <b>success</b> ratios for <b>metrics</b> and distinct traits are included for each species. Results indicate that stylohyoids can be sided based on longitudinal curvature, and that broad categories such as large vs. small ungulates, medium categories such as family and genus, and several species can be identified with more than 90 % probability using combinations of measurements and ratios...|$|R
40|$|For the {{evaluation}} of web sites a multitude of metrics are available. Apart from general statistical measures, <b>success</b> <b>metrics</b> reflect {{the degree to which}} a web site achieves its defined objectives. Particularly metrics for e-commerce sites based on transaction analysis are commonly available and well understood. In contrast to transaction based sites, the success of web sites geared toward information delivery is harder to quantify since there is no direct feedback of user intent. User feedback is only directly available on transactional web sites...|$|E
30|$|Defining project {{success in}} terms of time, costs, and quality is not only easily {{applicable}} and understandable, but also widespread in project management practice (Jugdev and Müller 2005). Measurement approaches that take a broader scope on critical <b>success</b> <b>metrics</b> may be more sophisticated from a theoretical point of view, but are rarely confirmed by credible empirical research (Nicolai and Kieser 2002). Accordingly, we chose to limit ourselves to the well-established fundamentals. Anticipated project success contains several dimensions, making it a formative measure. Multicollinearity was not present, as the maximum VIF of all items was 3.1.|$|E
40|$|Amazon. com has {{introduced}} the Simple Storage Service (S 3), a commodity-priced storage utility. S 3 aims to provide storage as a low-cost, highly available service, {{with a simple}} ‘pay-as-you-go’ charging model. This article makes three contributions. First, we evaluate S 3 's ability to provide storage support to large-scale science projects from a cost, availability, and performance perspective. Second, we identify a set of additional functionalities that storage services targeting data-intensive science applications should support. Third, we propose unbundling the <b>success</b> <b>metrics</b> for storage utility performance as a solution, to reduce storage costs...|$|E
40|$|ScholarsArchive@OSU (SA@OSU) {{has stood}} as Oregon State University’s {{institutional}} repository {{for nearly a}} decade, and has seen {{a great deal of}} <b>success,</b> by many <b>metrics,</b> as such. Over that time period, the mission, content types, and stakeholders of the repository have changed, as has the ecosystem of available and emerging repository platforms. These changes have sparked interest in OSU Libraries & Press in assessing the technical infrastructures for SA@OSU to determine whether migration to a new system could benefit all of our stakeholders. This document presents the results of an investigation conducted by Center for Digital Scholarship and Services faculty and staff between September 2014 and January 2015 into the requirements of our stakeholders and the suite of practically implementable repository platforms for SA@OSU. We provide an assessment of a number of platforms in the context of our requirements analysis and make recommendations for next step...|$|R
40|$|Two of the {{principal}} lessons to emerge from software measurement over {{the last twenty years}} have been the need for an effective approach to metric modelling (Shepperd and Ince 1993), and the need for automated support of such a modelling approach (Fenton 1992). There are currently no tools which fully support an effective metric modelling approach. This is in many respects responsible for the lack of <b>success</b> in the <b>metrics</b> field, as measurement programs are usually driven by the tools available (Basili and Rombach 1988). We review existing system lifecycle models and metric modelling approaches to elucidate inherent problems of existing metric modelling approaches. We argue that there are various fundamental problems with existing metric modelling approaches, and that a subset of these mirror problems of earlier system lifecycle models. A list of criteria which a higher quality metric modelling approach should support are presented. Underlying concepts of recent object-oriented system [...] ...|$|R
40|$|Detecting and characterizing {{emerging}} {{topics of}} discussion and consumer trends through analysis of Internet data {{is of great}} interest to businesses. This paper considers the problem of monitoring the Web to spot emerging memes - distinctive phrases which act as "tracers" for topics - as a means of early detection of new topics and trends. We present a novel methodology for predicting which memes will propagate widely, appearing in hundreds or thousands of blog posts, and which will not, thereby enabling discovery of significant topics. We begin by identifying measurables which should be predictive of meme <b>success.</b> Interestingly, these <b>metrics</b> are not those traditionally used for such prediction but instead are subtle measures of meme dynamics. These metrics form the basis for learning a classifier which predicts, for a given meme, whether or not it will propagate widely. The utility of the prediction methodology is demonstrated through analysis of memes that emerged online {{during the second half of}} 2008. Comment: AAAI 2011 Spring Symposium, Stanford University, CA. 21 - 23 March 201...|$|R
40|$|Abstract. The {{interest}} among a geographically distributed user base to mine massive collections of scientific data propels {{the need for}} efficient data dissemination solutions. An optimal data distribution scheme will find the delicate and often application-specific balance among conflicting <b>success</b> <b>metrics</b> such as minimizing transfer times, minimizing {{the impact on the}} network, and uniformly distributing load among participants. We use simulations to explore the performance of classes of data-distribution techniques, some of which successfully deployed in large peer-to-peer communities, in the context of today’s data-centric scientific collaborations. Based on these simulations we derive several recommendations for data distribution in real-world science collaborations. ...|$|E
40|$|Abstract Amazon. com has {{introduced}} the Simple Storage Service (S 3), a commodity-priced storage utility. S 3 aims to provide storage as a low-cost, highly available service, {{with a simple}} ‘pay-as-you-go’ charging model. This article makes three contributions. First, we evaluate S 3 's ability to provide storage support to large-scale science projects from a cost, availability, and performance perspective. Second, we identify a set of additional functionalities that storage services targeting data-intensive science applications should support. Third, we propose unbundling the <b>success</b> <b>metrics</b> for storage utility performance as a solution, to reduce storage costs. I...|$|E
40|$|A recent Strength, Weaknesses, Opportunities, and Threats (SWOT) {{analysis}} {{and the subsequent}} development of a strategic plan for the College of Engineering and Petroleum at Kuwait University are presented. The SWOT analysis is based on internal self-studies {{and a number of}} surveys carried out to determine views of various constituencies. Strategic objectives are developed into strategies and actions to address weaknesses and threats by effectively leveraging the strengths and opportunities. An operational plan is currently being developed, which includes specific actions, a timeline for relevant activities, those responsible to implement them, required resources, as well as <b>success</b> <b>metrics</b> to determine the level of achievement...|$|E
40|$|Infusion of {{automation}} technologies into NASA s future missions will {{be essential}} {{because of the}} need to: (1) effectively handle an exponentially increasing volume of scientific data, (2) successfully meet dynamic, opportunistic scientific goals and objectives, and (3) substantially reduce mission operations staff and costs. While much effort has gone into automating routine spacecraft operations to reduce human workload and hence costs, applying intelligent automation to the science side, i. e., science data acquisition, data analysis and reactions to that data analysis in a timely and still scientifically valid manner, has been relatively under-emphasized. In order to introduce science driven automation in missions, we must be able to: capture and interpret the science goals of observing programs, represent those goals in machine interpretable language; and allow spacecrafts onboard systems to autonomously react to the scientist's goals. In short, we must teach our platforms to dynamically understand, recognize, and react to the scientists goals. The Science Goal Monitor (SGM) project at NASA Goddard Space Flight Center is a prototype software tool being developed to determine the best strategies for implementing science goal driven automation in missions. The tools being developed in SGM improve the ability to monitor and react to the changing status of scientific events. The SGM system enables scientists to specify what to look for and how to react in descriptive rather than technical terms. The system monitors streams of science data to identify occurrences of key events previously specified by the scientist. When an event occurs, the system autonomously coordinates the execution of the scientist s desired reactions. Through SGM, we will improve om understanding about the capabilities needed onboard for <b>success,</b> develop <b>metrics</b> to understand the potential increase in science returns, and develop an operational prototype so that the perceived risks associated with increased use of automation can be reduced...|$|R
40|$|The {{selection}} of metrics for ecosystem restoration programs {{is critical for}} improving the quality and utility of design and monitoring programs, informing adaptive management actions, and characterizing project <b>success.</b> The <b>metrics</b> selection process, that in practice is left to the subjective judgment of stakeholders, is often complex and should simultaneously take into account monitoring data, environmental models, socio-economic considerations, and stakeholder interests. With limited funding, it is often very difficult to balance the importance of multiple metrics, often competing, intended to measure different environmental, social, and economic aspects of the system. To help restoration planners and practitioners develop the most useful and informative design and monitoring programs, we propose the use of multi-criteria decision analysis (MCDA) methods, broadly defined, to select optimal ecosystem restoration metric sets. In this paper, we apply and compare two MCDA methods, multi-attribute utility theory (MAUT), and probabilistic multi-criteria acceptability analysis (ProMAA), for a hypothetical river restoration case study involving multiple stakeholders with competing interests. Overall, the MCDA results in a systematic, quantitative, and transparent evaluation and comparison of potential metrics that provides planners and practitioners with a clear basis for selecting the optimal set of metrics to evaluate restoration alternatives and to inform restoration design and monitoring. In our case study, the two MCDA methods provide comparable results in terms of selected metrics. However, because ProMAA can consider probability distributions for weights and utility values of metrics for each criterion, it is most likely the best option for projects with highly uncertain data and significant stakeholder involvement. Despite the increase in complexity in the metrics selection process, MCDA improves upon the current, commonly used ad-hoc decision practice based on consultations with stakeholders by applying and presenting quantitative aggregation of data and judgment, thereby increasing the effectiveness of environmental design and monitoring and the transparency of decision making in restoration projects...|$|R
40|$|Software Model Checkers {{have shown}} {{outstanding}} performance improvements in recent times. Moreover, for specific use cases, formal verification techniques have {{shown to be}} highly effective, leading {{to a number of}} high-profile success stories. However, widespread adoption remains unlikely in the short term and one of the remaining obstacles in that direction is the vast number of instances which software model checkers cannot fully analyze within reasonable memory and CPU bounds. The majority of verification tools fail to provide a measure of progress or any intermediate verification result when such situations occur. Inspired in the <b>success</b> that coverage <b>metrics</b> have achieved in industry, we propose to adapt the definition of coverage to the context of verification. We discuss some of the challenges in pinning down a definition that resembles the deeply rooted semantics of test coverage. Subsequently we propose a definition for a broad family of verification techniques: those based on Abstract Reachability Trees. Moreover, we discuss a general approach to computing an under-approximation of such metric and a specific heuristic to improve the performance. Finally, we conduct an empirical evaluation to assess the viability of our approach...|$|R
