21|14|Public
25|$|The {{first of}} four prototypes flew on 28 March 1931, powered by a 239kW (320hp) Mitsubishi A2 radial engine. <b>Successive</b> <b>prototypes</b> were {{modified}} with more powerful engines, reduced wing area and a shorter fuselage until the fourth prototype was accepted by the Japanese Army, and ordered into service as the Type 92 Reconnaissance Aircraft.|$|E
50|$|Only three {{examples}} (the original plus <b>successive</b> <b>prototypes</b> known as Type 457 and Type 461) were built. This {{was due to}} refinements in the existing Lancaster bomber, rendering it suitable for the role for which the Windsor had been designed. The first prototype flew on 23 October 1943, the second on 15 February 1944, and the third on 11 July 1944. All three were built at Vickers' secret dispersed Foxwarren Experimental Department between Brooklands and nearby Cobham. The two latter prototypes were tested {{till the end of}} the Second World War, when further development and production were cancelled.|$|E
40|$|TT 2 is an {{innovative}} tool for speeding up and facilitating {{the work of}} translators by automatically suggesting translation completions. Different versions of the system are being developed for English, French, Spanish and German by an international team of researchers from Europe and Canada. Two professional translation agencies are currently evaluating <b>successive</b> <b>prototypes.</b> ...|$|E
40|$|Yasuhiro Monden The Japanese {{automotive}} industry has already become a by-word for lean production efficiency. Recently attention {{has shifted to}} its new-product development activities, and in particular to the techniques of simultaneous engineering that generate superior performance when contrasted with the traditional linear and sequential approaches. The paper probes the cost management system that underpins the Japanese automotive product-development system, enabling a network of producers and suppliers to work cooperatively and in parallel through <b>successive</b> <b>prototyping</b> stages of the process. The core concept is the iterative development of a 'target cost' for each component of the process. ...|$|R
40|$|Naval aviators who employ {{night vision}} goggles (NVG) face {{additional}} risks during nighttime operations. In {{an effort to}} reduce these risks, increased training with NVGs is suggested. Our goal was to design a computer-based, interactive multimedia system that would assist in the training of pilots who use NVGs. This thesis details the methods and techniques used {{in the development of the}} NVG multimedia prototype. It describes which hardware components and software applications were utilized as well as how the prototype was developed. Several facets of multimedia technology (sound, animation, video and three dimensional graphics) have been incorporated into the interactive prototype. For a more robust <b>successive</b> <b>prototype,</b> recommendations are submitted for future enhancements that include alternative methodologies as well as expanded interactions. Multimedia, Computer aided instruction, Night vision goggles. NANAU. S. Navy (U. S. N.) authors...|$|R
40|$|The Programming Systems Lab at Columbia University has {{investigated}} software process modeling and enactment {{since its inception}} in the mid- 1980 s, initially in the Marvel project. In the early to mid- 90 s, we extended to cross-organizational processes operating over the Internet, in Oz and OzWeb. The <b>successive</b> <b>prototype</b> frameworks we developed and demonstrated were used on a daily basis in-house to maintain, deploy and monitor their own components, APIs and user interfaces. The new process technology first presented here is broadly based on our decade of research on and experimentation with architecting and using such prototype services and software development processes targeted to Internet/Web middleware and applications, but reflects a major departure from our own (and others') previous directions. In particular, current process and workflow systems, including our own, are often too rigid for open-ended creative intellectual work, unable to rapidly adapt either the models or the enactment to situational context and/or user role. On the other hand, the process/workflow ideal implies a flexible mechanism for composition and coordination of information system components. We now present our in-progress development of rehostable lightweight mobile agents for on-the-fly process construction, adaptation and evolution, system reconfiguration, and knowledge propagation...|$|R
40|$|We {{present the}} design of an {{interactive}} tabletop exhibit intended to engage visitors in free-form computer programming activities at the Computer History Museum in Mountain View, California. We describe our design goals and outline challenges associated with creating this interactive experience for a free-choice learning environment. We review results of testing sessions with users from our target audience across three <b>successive</b> <b>prototypes.</b> Author Keywords Computer programming; Computer science education...|$|E
40|$|Abstract. Recently, {{automatic}} 3 D scanning {{devices are}} commonly researched and developed for better {{productivity of the}} reverse engineering fields. In this paper, a 3 D scanner utilizing a spherical coordinate system was designed and analyzed using FEM analysis. The system was designed for optimal performance, high precision, minimal deflection, and speed of data collection. FEM analysis allowed us to properly design the system to achieve these goals, with focus on the deflection of the cantilever arm. Results of the FEM analysis and figures showing the apparatus design are provided. <b>Successive</b> <b>prototypes</b> are shown to increase in overall performance and reliability through improved design and analysis...|$|E
40|$|This paper {{describes}} a long-term project exploring advanced visual interfaces for antenna design. MERL developed three <b>successive</b> <b>prototypes</b> that embodied an evolution towards larger scales and more concrete semantics for visualization of large sets of candidate designs and then winnowing them down. We experimented with multidimensional scaling and then collective line graphs {{before settling on}} linked scatterplots to visualize performance in a design of up to 10 million antennas at a time. In the end, the scatterplot solution was most successful at balancing intelligibility with visualization of the space as a whole. The design allows for adding more 1 D or 2 D linked feature visualizations if needed, and it smoothly transitions to other ¨details on demand¨views for final tweaking...|$|E
500|$|The {{idea for}} a video game console was thought up by Baer in August 1966, {{and over the next}} three years he, along with Bill Harrison and Bill Rusch, created seven <b>successive</b> <b>prototype</b> consoles. The seventh, known as the Brown Box, was shown to several {{manufacturers}} before Magnavox agreed to produce it in January 1971. After releasing the console in September 1972 through their dealerships, Magnavox sold between 69,000 and 100,000 units by the end of the year, and 350,000 by the time the console was discontinued in 1975. The console spawned the Magnavox Odyssey series of dedicated consoles, as well as the 1978 Magnavox Odyssey². One of the 28 games made for the system, a ping pong game, was an inspiration for Atari's successful Pong arcade game, in turn driving sales of the console. Baer's patents for the system and the games, including what was termed by a judge as [...] "the pioneering patent of the video game art", formed the basis of a series of lawsuits over 20 years, earning Sanders and Magnavox over US$100 million. The release of the Odyssey marked the end of the early history of video games, and the rise of the commercial video game industry along with the start of the first generation of video game consoles.|$|R
50|$|There are six model villages set {{within the}} {{miniature}} landscape. These are entirely fictional towns, but many buildings within them {{are based on}} UK <b>prototypes.</b> <b>Successive</b> generations of modelmakers, gardeners and craftsmen have left their mark on their subjects, which display {{a wide range of}} vernacular architectural styles.|$|R
50|$|Computational fluid {{dynamics}} (CFD) is an invaluable {{tool in the}} development of VADs, enabling new designs to be tested rapidly and undesirable flow characteristics eliminated in <b>successive</b> versions before <b>prototypes</b> are manufactured. CFD is used for predicting pressure-flow characteristics and efficiency curves, revealing the detailed flow field to help eliminate regions of recirculation or stagnation and for calculating fluid dynamic forces. When combined with models of blood damage CFD has been used for predicting haemolysis and platelet activation by VADs. When combined with shape optimization algorithms it can be used in design optimization.|$|R
40|$|Proceedings of the European Conference on Curriculum Studies. Future Directions: Uncertainty and Possibility. Braga: University of Minho, October 18 and 19, 2013. Curriculum Design Research (CDR) is {{a branch}} of {{educational}} design research whereby {{the development of a}} curriculum or part of it is systematically studied, with a strong focus on the evaluation of <b>successive</b> <b>prototypes</b> of the product being developed. This paper discusses how CDR has been used to study the development of CTD-O, which is a course on Curriculum Theory and Development that started to be taught totally online in 2011 / 12 in an institution where the full virtualization of a course is still a rare phenomenon. I will describe and discuss the evaluation of prototype 1 (2011 / 12) and prototype 2 (2012 / 13) of CTD-O, which was very successful. Most students stated, via questionnaires, that, if they could move back in time and decide whether to take the course online or offline, they would take it online...|$|E
40|$|The {{approach}} to satisfying the requirements {{depends on the}} chosen lifecycle. Waterfall takes the specification as a target and attempts to meet it. The spiral model sees <b>successive</b> <b>prototypes</b> as approximations to the ideal specification (the specification may change {{as a result of}} prototyping). Evolutionary delivery attempts to partition the specification into many small sub-specifications and attempts to meet each sub-specification with a product delivered to the customer (the specification may change as a result of earlier delivery). This note is an overview of the design process. We consider what a design should contain, the design process, criteria to judge the quality of a design, an overview of design techniques, the design evaluation process and how structured approaches to design can be supported by programming language structures. These components will play a part in the design process in whatever lifecycle model structures the development process. The distribution of the activities will be different but all will take place...|$|E
40|$|PNNL- 18938 Revision Radiation portal {{monitors}} {{used for}} interdiction of illicit materials at borders include highly sensitive neutron detection systems. The {{main reason for}} having neutron detection capability is to detect fission neutrons from plutonium. The currently deployed radiation portal monitors (RPMs) from Ludlum and Science Applications International Corporation (SAIC) use neutron detectors based upon 3 He-filled gas proportional counters, which {{are the most common}} large neutron detector. There is a declining supply of 3 He in the world, and thus, methods to reduce the use of this gas in RPMs with minimal changes to the current system designs and sensitivity to cargo-borne neutrons are being investigated. Four technologies have been identified as being currently commercially available, potential alternative neutron detectors to replace the use of 3 He in RPMs. Reported here are the results of tests of a newly designed boron-lined proportional counter option. This testing measured the neutron detection efficiency and gamma ray rejection capabilities of two <b>successive</b> <b>prototypes</b> of a system manufactured by GE Reuter Stokes...|$|E
40|$|Many {{doctoral}} {{researchers are}} adopting interpretive epistemologies of inquiry in which research design is emergent and {{is shaped by}} the developing subjectivity of the researcher-as-learner. When interpretive researchers also adopt narrative modes of inquiry and literary genres for representing their unfolding relationships with the participants of their inquiry (including the reader of their thesis), then the question arises as to what might constitute an appropriate thesis structure. We believe that, in succumbing to the structural template of positivism, interpretive researchers {{are in danger of}} creating distorted portrayals of their inquiries as timeless, lacking in contingency and without an emergent nature. In this paper we argue for a diachronic structure that allows the narrative flow of the inquiry to be revealed. Drawing on a recently completed doctoral study, in which a multimedia educational program was designed and implemented by the first author, we illustrate how a screenplay metaphor combined with electronic hyperlinking provided a non-linear thesis structure that allows multiple reading pathways, exploration of rich documentation and viewing of <b>successive</b> multimedia <b>prototype</b> designs...|$|R
40|$|Design {{knowledge}} {{in the form of}} a priori knowledge is put forward as an essential ingredient in knowledge-based design. The concept of the prototype in design is introduced and different strategies for refinement processes are discussed. The concept of a generative prototype is proposed as a way to represent a generative design description in knowledge-based design systems. The refinement process is considered a <b>successive</b> classification of <b>prototypes</b> and subtypes, by executing design operations associated with the type. The application of these approaches is implemented in a system called PRODS: A PROtotype-based Design System. Finally, issues such as the role of prototypes and other forms of reasoning for creative design are discusse...|$|R
40|$|The ParaSight F {{test was}} {{developed}} as a pioneer industry effort in the large-scale, process-controlled production of a device for the rapid diagnosis of malaria. This device performed well in field settings but {{was limited to the}} detection of a single malaria species, Plasmodium falciparum. The ParaSight F+V assay advanced upon the ParaSight F test format by incorporating a monoclonal antibody directed against a proprietary Plasmodium vivax-specific antigen, in addition to the antibody directed against P. falciparum histidine-rich protein 2, which was used in the ParaSight F assay. The modified assay was developed to add the capability to detect P. falciparum and P. vivax in a single-test-strip format. The present study evaluated three distinct ParaSight F+V prototypes with samples from symptomatic patients in regions of Thailand and Peru where malaria is endemic. Over a 2 -year enrollment period (1998 and 1999), a total of 4, 894 patients consented to participation in the study. Compared with the results for duplicate microscopic examinations of Giemsa-stained blood smears as the reference diagnostic standard, each <b>successive</b> <b>prototype</b> showed substantial improvement in performance. The final ParaSight F+V prototype, evaluated in 1999, had an overall sensitivity for detection of asexual P. falciparum parasites of 98 %. The sensitivity of the device was 100 % for P. falciparum densities of > 500 parasites/μl, with a sensitivity of 83 % for parasite densities of ≤ 500 /μl. The specificity for the exclusion of P. falciparum was 93 %. For P. vivax, the overall sensitivity was 87 % for the final 1999 prototype. The sensitivities calculated for different levels of P. vivax parasitemia were 99 % for parasite densities of > 5, 000 /μl, 92 % for parasite densities of 1, 001 to 5, 000 /μl, 94 % for parasite densities of 501 to 1, 000 /μl, and 55 % for parasite densities of 1 to 500 /μl. The specificity for the exclusion of P. vivax was 87 %. The areas under the receiver operating characteristic curves for the diagnostic performance of the assay for the detection of P. falciparum and P. vivax were 0. 8907 and 0. 8522, respectively. These findings indicate that assays for rapid diagnosis have the potential to enhance diagnostic capabilities in those instances in which skilled microscopy is not readily available...|$|R
40|$|This paper {{presents}} a robust fuzzy c-means (FCM) for an automatic effective segmentation of breast and brain magnetic resonance images (MRI). This paper obtains novel objective functions for proposed robust fuzzy c-means by replacing original Euclidean distance with properties of kernel function on feature space and using Tsallis entropy. By minimizing the proposed effective objective functions, this paper gets membership partition matrices and equations for <b>successive</b> <b>prototypes.</b> In {{order to reduce}} the computational complexity and running time, center initialization algorithm is introduced for initializing the initial cluster center. The initial experimental works have done on synthetic image and benchmark dataset to investigate the effectiveness of proposed, and then the proposed method has been implemented to differentiate the different region of real breast and brain magnetic resonance images. In order to identify the validity of proposed fuzzy c-means methods, segmentation accuracy is computed by using silhouette method. The experimental results show that the proposed method is more capable in segmentation of medical images than existed methods. (c) 2011 Elsevier Inc. All rights reserved...|$|E
40|$|OBJECTIVE: To {{develop and}} test the prototypes of a novel post-laryngectomy {{rehabilitation}} tool incorporating an obligatory, disposable heat and moisture exchanger (HME) and a reusable, multi-magnet automatic speaking valve (ASV). MATERIAL AND METHODS: The study subjects comprised 20 laryngectomized individuals (15 males, 5 females), 5 of whom were already using an ASV and 15 who were not. Three <b>successive</b> <b>prototypes</b> were tested. Data were collected by means of structured questionnaires, considering for example patient compliance, skin adhesion, voicing and coughing aspects, and voice and speech quality assessments, assessing for example maximum phonation time and dynamic loudness range. RESULTS: Of the 15 non-ASV users, 5 did not comply with the study due to peristomal skin adhesion problems. Of the remaining 15 patients, all 5 ASV users and 6 / 10 non-users were fully compliant with the new device. The cough-relief valve of the new device functions properly, as does the valve position adjustment for physical exertion. With this new device the maximum phonation time was longer than with a regular ASV (15. 2 vs 11. 6 s; p = 0. 006) and the dynamic range was larger (33. 0 vs 24. 8 dB; p < 0. 001). CONCLUSION: The test results obtained with this new device show that its advanced features (obligatory HME and multi-magnet valve systems) offer additional benefits for further improving vocal and pulmonary rehabilitation after total laryngectom...|$|E
40|$|Conventional {{skateboard}} {{trucks are}} currently {{unable to meet}} the challenges of the modern enthusiast. They are lacking in key performance metrics such as handling, stability, and traction. Longboard enthusiasts, whom rely heavily on handling performance, are hungry for new and innovative technology to help bring the sport to the next level. The aim of this project was to solve these problems by applying specific aspects of automotive steering geometry and best engineering practices. Three <b>successive</b> <b>prototypes</b> were designed, with the first two prototype sets being manufactured and extensively tested. The first prototype served as a proof of concept, but suffered from design and manufacturing complexities and would have been too expensive to be mass produced. Positive and negative feedback was obtained from enthusiasts which was used to design the second prototype set. More testing was done and while the second prototype set showed major improvement across all key metrics, problems still existed. A third prototype design was developed to solve the remaining problems and is currently being manufactured. Overall, the result of the project is a longboard truck system that is superior to current products in terms of stability and handling. Its simple design and ease of manufacture allow for a potentially very competitive price point. Furthermore, the new technology will be a basis for future developments and refinement much like the roller skate truck has been since the 1940 s...|$|E
40|$|Archaeological {{surveying}} undoubtedly belongs {{with the}} fields of research most elaborated in this domain of study. This is inevitably linked both to {{the scale of the}} problems tackled and – in more strictly 'physical' terms – to the fact that this operation is closely and indissolubly related to the continuous evolution of the archaeological excavation. The expectations related to archaeological surveying – to a much higher degree than to other types thereof – are much more complex and larger, ranging from a documentation of various materials found and/or of their traces to a diagnosing and differentiating the type of work to be done; from various types of finishing jobs to the documentation of different stages of destructive excavations. All this usually created ambiguous situations for the technicians in the stage of elaborating collected data and to those who did not participate in the work itself – in the stage of communication. The problems with the interpretation of data and their lack of clarity disturbed considerably the final objective of surveying: a profound knowledge of the object analyzed. The research within the Department of History, Drawing and Restoration of Architecture focussed on the establishment of the so called 'operative guidelines' which would define the methodology of surveying and representing archaeological artefacts making full advantage of new technologies, and integrating them rigorously with traditional techniques and procedures. Taking into consideration numerous survey campaigns carried out for years, the present study seeks to present a modus operandi that seems to be indispensable for standardizing and regulating procedures of data collecting, elaborating and restoring procedures applied by the research team, the aim being to make the final result scientific, i. e. more objective and correct. The guidelines are no means an rigid list of operations to be carried out. This would be futile in surveying where nothing is purely mechanical and where the abilities and the sensibility of the research worker play a significant role. The guidelines include a wide range of cases and make it possible to adjust the procedure to the object of study {{as well as to the}} exigencies of surveying itself, all the time preserving the versatility of survey. Among the campaigns carried out there were the surveys of some Roman theatres at important archaeological sites, like the Roman theatres at Petra and Jerash in Jordan, the Roman theatre and amphitheatre in the town Mérida in Spain, the Roman theatre in Taormina and – on a smaller scale – the Tempio del Divo Claudio and the Janus Arch in Rome. In many cases the objects of study were small and required a high level of detailed data for <b>successive</b> <b>prototyping</b> and so, for the physical reproduction of the artefact...|$|R
40|$|This {{research}} {{deals with}} annotations in scholarly work. Annotations {{have been studied}} by many people. A significant amount of {{research has shown that}} instead of implementing domain specific annotation applications a better approach is to develop general purpose annotation toolkits {{that can be used to}} create domain specific applications. A video annotation toolkit along with toolkits for searching, retrieving, analyzing and presenting videos can help achieve the broader goal of creating integrated work spaces for scholarly work in humanities research similar to existing environments in such fields as mathematics, engineering, statistics, software development and bioinformatics. This research implements a video annotation toolkit and evaluates it by looking at its usefulness in creating applications for different areas. It was found that many areas of study in the arts and sciences can benefit from a video annotation application tailored to their specific needs and that an annotation toolkit can significantly reduce the time for developing such applications. The toolkit was engineered through <b>successive</b> refinements of <b>prototype</b> applications developed for different application areas. The toolkit design was also guided by a set of features identified by the research community for an ideal general purpose annotation toolkit. This research contributes by combining these two different approaches to toolkit design and construction into a hybrid approach. This approach could be useful for similar or related efforts...|$|R
40|$|AbstractThere is {{a growing}} {{consensus}} that 3 D printing technologies {{will be one of}} the next major technological revolutions. While a lot of work has already been carried out as to what these technologies will bring in terms of product and process innovation, little has been done on their impact on business models and business model innovation. Yet, history has shown that technological revolution without adequate business model evolution is a pitfall for many businesses. In the case of 3 D printing, the matter is further complicated by the fact that adoption of these technologies has occurred in four <b>successive</b> phases (rapid <b>prototyping,</b> rapid tooling, digital manufacturing, home fabrication) that correspond to a different level of involvement of 3 D printing in the production process. This article investigates the effect of each phase on the key business model components. While the impact of rapid prototyping and rapid tooling is found to be limited in extent, direct manufacturing and, even more so, home fabrication have the potential to be highly disruptive. While much more value can be created, capturing value can become extremely challenging. Hence, finding a suitable business model is critical. To this respect, this article shows that 3 D printing technologies have the potential to change the way business model innovation is carried out, by enabling adaptive business models and by bringing the ‘rapid prototyping’ paradigm to business model innovation itself...|$|R
40|$|The use of eXtensible Markup Language (XML) schema for geoscience data {{exchange}} is an improvement on non XML data formats because the XML format is partially self-documenting and provides common methods for parsing files, obtaining their structure and transforming them to alternative formats. The British Geological Survey (BGS) {{believes it is}} important to develop some common ML for the exchange of generic geoscience information. If communities share a common data transfer model for their domains of interest, {{data exchange}} becomes even easier and more likely to take place efficiently. To address this need the BGS is proposing the development of GeoSciML, a geoscience information markup language, as an application of the OpenGIS Consortium's (OGC) Geography Markup Language (GML), building upon the applied geoscience domain focused eXploration and Mining Markup Language (XMML). We are aiming for an open standard with the support of bodies such as the International Union of Geological Sciences’ (IUGS) Commission for the Management and Application of Geoscience Information and the OGC. This paper illustrates the proposed approach with some initial development work to cover geoscientific domains of particular interest, such as boreholes, text and structural geology. The development process has been iterative, with <b>successive</b> <b>prototypes</b> incorporating the comments of experts in each geoscientific domain. We propose that development be extended to the wider geoscience community with the developing schemas and documentation available on a collaborative web site. ...|$|E
40|$|In field archaeology, {{analysis}} of an excavated region is a meticulous process requiring exploration {{at a variety}} of levels. While the act of excavation offers the best way for archaeologists to monitor close-range details, they can use empirically based analysis to analyze only the findings to which they themselves have been exposed. With a team of people excavating, synthesizing the wealth of observation from one year to the next is difficult. Archaeologists also use quantitative methods to compare findings throughout the site over time to pinpoint base trends among recorded artifact typologies. 1 Quantitative approaches, unfortunately, are often limited because they lack a spatial component—explicit information about the physical 3 D relationships among the excavated objects and the site. In this article, we chronicle a collaborative effort (from 1997 to the present) with Petra Great Temple archaeologists to augment traditional analysis approaches. We introduce new archaeological analysis tools that combine novel visualization and interaction techniques within a Cave Automatic Virtual Environment (CAVE). 2 New tools give archaeologists access to formerly inaccessible parts of the archaeological record. The result is a demonstrably improved model for inquiry to pose, and answer, important research questions. These tools ■ give archaeologists access to formerly inaccessible parts of the archaeological record; ■ support navigation and interaction with virtual trenches, stratigraphy, artifacts, and architecture; and ■ preserve and display the spatial relationships present before excavation. Using an iterative approach and working with archaeologists from the Brown University-sponsored excavations at Petra, Jordan, we built four <b>successive</b> <b>prototypes...</b>|$|E
40|$|This thesis {{presents}} {{research results}} {{on a new}} fluid-mechatronic brake principle. The Self-energizing Electro-Hydraulic Brake (SEHB) utilizes the effect of instable self-reinforcement in combination with a closed loop control. Background {{for the development of}} the brake concept is a train application. However, SEHB is not limited to any specific application. Main advantages of the concept are its minimal energy consumption, the closed loop control of the true brake torque and its feedback ability due to the decentralized low-power electronic control. This thesis introduces the new brake principle by comparing it to conventional self-reinforcing brakes. A mathematical distinction is given between self-reinforcement and self-energization on the basis of static considerations. The dynamic characteristics are analyzed using a linearized system description which is further simplified using the method of pole dominance analysis. The simplified model is used to calculate a state dependent proportional controller map on the basis of a damping criteria. Besides the theoretic analysis, the thesis presents the basic hydraulic design criteria and gives a systematic overview over different hydraulic-mechanical design solutions. A special focus is given on the valve control, since it is vital for the brake performance. Different automotive valves such as from antilock brake systems (ABS) or electronic stabilization programs (ESP) are applied using electronic power switches and current drivers. The brake test stand and two <b>successive</b> <b>prototypes</b> are outlined at the end of this thesis. Different exemplary measurement results show the performance of the implemented types of valve control and demonstrate the potential of this new brake technology...|$|E
40|$|The Compact Compton Imagers CCI- 1 and CCI- 2 are two <b>successive</b> gamma-ray imaging <b>prototypes</b> {{developed}} by our group. Both systems use the Compton camera concept for imaging, and are based on position sensitive, double sided segmented (DSSD) planar HPGe and Si(Li) detectors. Whereas each electrode of the Ge detectors is segmented in 37 strips, 2 mm pitch size, each electrode of the Si(Li) detectors is segmented in 32 strips, 2 mm pitch size. Both imaging systems provide panoramic images of extended sources in a 4 -pi field of view for modest intensity sources at distances from 3 to 400 cm with an angular resolution as good as 2 degrees. An average energy resolution of 2 keV per channel enables these instruments to accurately identify radioisotopes. The cameras are sensitive to gamma-ray photons of energies between 150 keV to several MeV. We report on measurements using these imagers to create 3 D maps of extended gamma-ray sources distributed in the environment in the medium to far-field distances. For increased image contrast and accuracy, range data from a LIDAR scanner was merged into the 3 D gamma-ray image reconstruction process. This integration not only allows for accurate imaging of extended sources in a large range of distances, but also helps the operator to unambiguously identify the objects containing the radioactive materials. By comparison, a simple projection of a gamma-ray image onto a visual picture can often lead to ambiguous interpretations. The experimental technique will be described along with the data analysis and image reconstruction utilized in the process. The goal of this development work was to demonstrate the potential gain in monitoring installations and nuclear materials simultaneously using a gamma-ray imaging and a LIDAR system. A future dedicated system developed on this concept can potentially increase the sensitivity and automation level for inspection and monitoring of plants processing nuclear materials. Of special interest is the measurement of material hold-up in hard to reach places. JRC. G. 8 -Nuclear securit...|$|R
40|$|Research objectives. 1) To {{create an}} {{original}} and useful software application; 2) {{to investigate the}} utility of dyna-linking for teaching upper limb anatomy. Dyna-linking is an arrangement whereby interaction with one representation automatically drives the behaviour of another representation. Method. An iterative user-centred software development methodology was used to build, test and refine <b>successive</b> <b>prototypes</b> of an upper limb software tutorial. A randomised trial then tested the null hypothesis: There will be {{no significant difference in}} learning outcomes between participants using dyna-linked 2 D and 3 D representations of the upper limb and those using non dyna-linked representations. Data was analysed in SPSS using factorial analysis of variance (ANOVA). Results and analysis. The study failed to reject the null hypothesis as there was no signi cant di fference between experimental conditions. Post-hoc analysis revealed that participants with low prior knowledge performed significantly better (p = 0. 036) without dyna-linking (mean gain = 7. 45) than with dyna-linking (mean gain = 4. 58). Participants with high prior knowledge performed equally well with or without dyna-linking. These findings reveal an aptitude by treatment interaction (ATI) whereby the effectiveness of dyna-linking varies according to learner ability. On average, participants using the non dyna-linked system spent 3 minutes and 4 seconds longer studying the tutorial. Participants using the non dyna-linked system clicked 30 % more on the representations. Dyna-linking had a high perceived value in questionnaire surveys (n= 48) and a focus group (n= 7). Conclusion. Dyna-linking has a high perceived value but may actually over-automate learning by prematurely giving novice learners a fully worked solution. Further research is required to confirm if this finding is repeated in other domains, with different learners and more sophisticated implementations of dyna-linking. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|Information on solar {{radiation}} {{is a critical}} issue in several environmental domains {{as well as for}} sun-powered sys-tems. The present means for accessing information by users present several severe drawbacks. Three major problems were identified and should be solved to supply users with relevant information: improved access to information, improved space and time description / knowledge of the radiation field and related quantities, improved matching to actual user needs. The SoDa project was launched (2000 - 2003, IST programme of the European Commission) to bring solutions by an efficient use of advanced information and communication technologies. An integration of information sources of different natures was initiated by the SoDa Intelligent System (see online at [URL] These sources include databases containing {{solar radiation}} parameters and other relevant information; several of them originate from the processing of images taken by satellites. The sources! also include user-oriented applications. The <b>successive</b> <b>prototypes</b> of the SoDa Service were validated through users trials. The outcomes of the project SoDa represent a significant step forward beyond {{the current state of the}} art and include substantial original work. The main innovations of SoDa are to offer a smart access to diverse networked sources of information that are geographically dispersed, and to supply users with information of high quality. Surveys of users demonstrated that large gains in terms of efficiency, costs, etc. were expected by engineers, companies, agencies and research institutes if relevant information were more easily available for virtually any geographical location at any time. Accordingly, it was decided in 2003 to create the SoDa Service and to operate it. During these past three years, the SoDa Service underwent several improvements, all aiming at consolidating it with respect to access by users. Improvements were made on the SoDa Intellig! ent System, including works on the user interface and on the p! resentation of the services. Promotion efforts were made towards media, including TV. The effective use of the SoDa Service is increasing from year to year. In 2003, 2000 requests for information were satisfied; in 2004, 20 000 requests; 35 000 are expected in 2005. This communication presents the lessons learned from the past and the perspectives of the SoDa Service. We discuss the sustainability of the SoDa Service, the technologies used and the approach to customers, in the perspective of developing a B 2 B merchant site...|$|E
40|$|Abstract__ This paper {{addresses}} {{the challenges of}} Joint Fact Finding (JFF) in spatial planning and design. JFF {{is an important component}} of a deliberative planning practice: The construction of (problematic) realities is fundamental for the formulation of challenges and solutions. Information is often contested in complex planning processes due to different interests, values and perspectives. Carefully designed interaction procedures are needed to negotiate the relevance and validity of information sources. Particularly promising procedures for this are Serious Games: Facilitating joint reality construction through immersive simulations, they are appealing ways to engage not only knowledge-oriented researchers, but also practice-oriented stakeholders and professionals. Their concreteness speaks to spatial planning and design as crafts. Still, the development of such games is not without its challenges and trade-offs. As procedures for reality construction, they cannot escape the power-laden nature of knowledge. We present a case study on developing a spatial design-oriented game, and analyze it {{in the tradition of the}} sociology of translations, aided by literature on serious game development. As indicated, Serious Games could function as JFF procedures in spatial planning and design. Moreover, their architecture can be considered a ‘boundary object’ providing actors an environment that accommodates information sharing, learning and joint reality construction. In this way the game facilitates the building of capacity to generate and integrate knowledge for spatial planning and design. In our project on integrative planning in delta areas, the game architecture accommodated researchers and practitioners in governance, spatial design and geo-information. Striving for interdisciplinary synergies, the game architecture was to be accordingly polyvalent. Its main innovative features would be its generative and integrative capacity, i. e. its capacity to both co-produce and integrate a diversity of information sources and to co-develop/generate spatial designs on this basis. How can joint fact finding in spatial planning and design be organized through a serious game in such a way that it develops integrative and generative capacity, and which challenges and trade-offs are faced in realizing this goal? In this paper we describe and discuss the practical shaping of these two capacities, and the attendant trade-offs. Tracking the ontogenesis of a game design, we describe a struggle over appropriate JFF procedures. As proposed procedures for interactive reality construction, the <b>successive</b> <b>prototypes</b> and game elements reflect meta-visions on the area under study – as a physical formation to shape, a system to represent and display, or an actor constellation to engage. This analysis allows us to distinguish alternative game architectures, with different appropriateness to JFF and other planning-related purposes. Beyond these procedural-methodological insights on JFF in game development, the observed variety of delta realities is also telling for the complexity of these areas: They are not only intersections of marine and fluvial dynamics, but also of multiple reality constructions...|$|E
40|$|The {{quality of}} {{education}} in Botswana is not yet up to standard as there has been emphasis on attainment of Universal Basic Education. Quality in education encompasses {{a number of factors}} such as the development of the relevant curriculum, improvement of teacher preparation, development of appropriate learning materials, and improving the methods of assessing pupils (Grisay&Mählck, 1991, cited in Kellaghan&Geaney, 2003). The quality {{of what is going on}} in the classroom is judged by the processes and outcomes that are defined qualitatively. Assessment in Agriculture in Botswana senior schools comprises performance assessment and standardised paper-and-pencil tests. Performance assessment contributes only 20 % (MoE&SD, 2001. p. 6) yet it is allocated more time than paper-and-pencil tests. The aim of the study therefore was to understand and explore the characteristics and quality processes needed in the performance assessment of Agriculture Form Four students to ensure valid and reliable examinations in Botswana. The study was guided by two research questions. The first research question was: How valid and reliable are the performance assessment processes in Botswana schools? This research question sought to understand how performance assessment was conducted in Botswana schools, and how it compared with the international practice. The second research question was: How can quality assurance processes be developed in order to produce valid and reliable marks for BGCSE Agriculture performance assessment? The intention was to develop quality processes for performance assessment in the context of Form Four Agriculture in Botswana, to ensure valid and reliable marks for certification. A design research was employed in this study in which a baseline survey was conducted and based on the outcome, a quality assurance process was designed which included the development of standard tasks and assessment materials. During the baseline survey, teachers and school administrators completed a questionnaire and were also interviewed. Subsequently, prototypes of exemplar materials were developed iteratively in collaboration with practitioners and formatively evaluated. Feedback from evaluation was incorporated into the redesign and development of <b>successive</b> <b>prototypes.</b> Findings from baseline survey revealed that the conduct of performance assessment in schools was not standardised, primarily due to the absence of assessment policy and procedures to guide its conduct. Implementation of performance assessment was done by teachers who had insufficient training, in large classes with inadequate resources and received very little support from supervisors both internally and externally. Despite all these, insufficient time was allocated for conducting performance assessment, resulting in teachers forming groups most of the time during the conduct of tasks and assigning a single mark for the group based on the quality of the group’s product. However, findings from the intervention study revealed that entrenching quality assurance processes in the system produced valid and reliable performance assessment marks for certification. The characteristics of a quality assurance system for implementation of performance assessment were the presence of an assessment policy; training and accrediting teachers to assess; an efficient internal and external monitoring system; the provision of adequate resources; applying multiple modes of assessment; and multiple rating of the students. Thesis (PhD) [...] University of Pretoria, 2011. Science, Mathematics and Technology Educationunrestricte...|$|E
40|$|In this paper, we {{will present}} {{findings}} from the first twelve months of a research and development project called ‘Making Games’, which is developing a software tool to enable 11 - 14 year olds create their own 3 D computer games using object-oriented programming. The project is a collaboration between the Centre for the Study of Children, Youth and Media (University of London) and Immersive Education, a software development company set up by Elixir Studios and Math Engine. Over a three-year period, Immersive is releasing <b>successive</b> <b>prototypes</b> of a game authoring tool, which researchers are taking into schools and summer camps to research its design, uses and benefits. The research is investigating how game design can be taught and learned, and whether the concept of ‘literacy’ can be extended to the analysis (reading) and production (writing) of computer games. This develops the recent emphasis in education on digital and media literacies (Buckingham 2002, 2003; Kress 2003). In particular, {{we are interested in}} the benefits such a literacy might offer girls, as well as young people with print literacy difficulties. The paper will focus on two questions. Firstly, what are the components of ‘game literacy’? The term ‘literacy’ is traditionally used only in relation to print. However, in recent years it has been extended to apply to the different forms of competence that are required by a range of communicational and representational media, including print, visual images and sound among others. Communication has always taken place through these different modes, but in the wake of new information technologies, traditional definitions of literacy have been widened to encompass not only print-based media but also multimodal forms of expression. The notion of ‘game literacy’ extend this, by attempting to identify how meaning is created within the specific medium of games. It includes elements of signification that relate to all or most media, such as aspects of narrative, mode of address and representation; but it also incorporates elements that are specific to games and game systems, such as rules, goals, economies, exploration and conditionality. Our second question relates to how game literacy is taught and learned. Being able to read and write game texts is the result of pedagogic processes. In this paper, we will briefly present the approach we took to teaching game design in three sites: a media studies classroom in a mixed comprehensive school; an after-school club in a girls’ comprehensive school; and a summer camp. In each site, the approach we took to researching and teaching ‘game literacy’ differed. In the classroom, we used an established model within media studies that involves analysing media as social and cultural phenomena. We adapted an approach often taken to the analysis of film and TV in schools and focused on the experiential dimension of gaming, discussing issues relating to representation, identification, narrative structure, genre, marketing, and audience pleasures. In the after-school club, we focused more tightly on game design as a design practice, starting with board games and then moving on to computer games. This pedagogical approach encouraged students to view design as an enjoyable activity, on a par with playing games, and allowed us to develop an understanding of both the kinds of practices and areas of knowledge that might encompass game literacy. In the summer camp, game playing and game design were much more closely intertwined, allowing us to research how production might fit into young people’s wider gaming culture. The paper will comment on the pedagogical strategies that we deployed in each context and offer reflections on the different manifestations which ‘game literacy’ might take. In particular, we will examine the place of gender in learning and teaching game design. The significance of gender differed across our three sites of research as well as across time within each site, emphasising the need to view gender not simply as socially constructed but also as a form of social action intended to achieve certain ends within specific situations. Judith Butler’s notion of gender as grounded in language and enacted as a performance is useful here and particularly relevant to identifying the relation between gender and literacy (Butler, 1999). Our argument is in part constructed as a reflection on and response to Kafai’s research on gender and young people’s game design (Kafai 1996, 2000). The kinds of gaming knowledge which students chose to display in their game designs, and in particular the way they interpret genre conventions, relates not only to their experience of games but also to how they are positioned, and want to position themselves, in relation to the interpersonal context of design as well as the wider gaming culture and fan community. In our research, gender is not associated with a set of stable preferences or competences, but is rather performed to maintain a certain level of authority and a certain kind of relation to others within a specific pedagogic context. The presentation will include a demonstration of the prototype game authoring tool, {{as well as some of}} the games that young people have built within it. The authoring environment is based on some of the same principles as a level editor, but allows greater flexibility in terms of design and game play. Users select from a range of objects (such as environments, decorative objects, pick-up objects, triggers, etc), assign properties to them (for example, this key unlocks this door; the inventory has X number of slots; reaching point X earns the player Y number of points) and order them within the game space according to the rules of their game. By the time of the DIGRA conference, we will be half way through the development schedule; the prototype will therefore include only s fraction of the functions we hope to include in the final product. However, it will enable us to illustrate our approach to teaching game design, which aims to allow users design their own rules within certain genres (action, adventure and role-playing) as well as deploy and create a broad range of representations. References Buckingham, D. (2003) Media Education: literacy, learning and contemporary culture. Cambridge: Polity Press. Buckingham, D. (2002) 2 ̆ 7 The electronic generation? Children and new media 2 ̆ 7 in Lievrouw, L. A. and Livingstone, S. (eds.) Handbook of New Media. London: Sage. Butler, J (1999) Gender trouble: feminism and the subversion of identity. London: Routledge Kafai, Y. (1996) 2 ̆ 7 Gender differences in children 2 ̆ 7 s constructions of video games 2 ̆ 7 in Greenfield, P. M. and Cocking, R. R. (eds.) Interacting with video. Norwood NJ: Ablex. Kafai, Y. B. (2000) 2 ̆ 7 Video game designs by girls and boys: variability and consistency of gender differences 2 ̆ 7 in Cassell, J. and Jenkins, H. (eds.) From Barbie to Mortal Kombat: gender and computer games. Cambridge: MIT press. Kress, G. (2003) Literacy in the New Media Age. London: Routledge...|$|E

