11|111|Public
30|$|<b>Statistical</b> <b>family</b> - basic {{statistical}} analyses, including correlations and {{the like}} [46].|$|E
40|$|A {{family of}} kernels for {{statistical}} learning is introduced that exploits the geometric structure of statistical models. The kernels {{are based on}} the heat equation on the Riemannian manifold defined by the Fisher information metric associated with a <b>statistical</b> <b>family,</b> and generalize the Gaussian kernel of Euclidean space. As an important special case, kernels based on the geometry of multinomial families are derived, leading to kernel-based learning algorithms that apply naturally to discrete data. Bounds on covering numbers and Rademacher averages for the kernels are proved using bounds on the eigenvalues of the Laplacian on Riemannian manifolds. Experimental results are presented for document classification, for which the use of multinomial geometry is natural and well motivated, and improvements are obtained over the standard use of Gaussian or linear kernels, which have been the standard for text classification...|$|E
40|$|Abstract: 2 ̆ 2 A {{family of}} kernels for {{statistical}} learning is introduced that exploits the geometric structure of statistical models. The kernels {{are based on}} the heat equation on the Riemannian manifold defined by the Fisher information metric associated with a <b>statistical</b> <b>family,</b> and generalize the Gaussian kernel of Euclidean space. As an important special case, kernels based on the geometry of multinomial families are derived, leading to kernel-based learning algorithms that apply naturally to discrete data. Bounds on covering numbers and Rademacher averages for the kernels are proved using bounds on the eigenvalues of the Laplacian on Riemannian manifolds. Experimental results are presented for document classification, for which the use of multinomial geometry is natural and well motivated, and improvements are obtained over the standard use of Gaussian or linear kernels, which have been the standard for text classification. 2 ̆...|$|E
40|$|This chapter {{deals with}} the {{estimation}} of risk neutral distributions for pricing index options resulting from the hypothesis of the risk neutral valuation principle. After justifying this hypothesis, we shall focus on parametric estimation methods for the risk neutral density functions determining the risk neutral distributions. We we shall differentiate between the direct and the indirect way. Following the direct way, parameter vectors are estimated which characterize the distributions from selected <b>statistical</b> <b>families</b> to model the risk neutral distributions. The idea of the indirect approach is to calibrate characteristic parameter vectors for stochastic models of the asset price processes, and then to extract the risk neutral density function via Fourier methods. For every of the reviewed methods the calculation of option prices under hypothetically true risk neutral distributions is a building block. We shall give explicit formula for call and put prices w. r. t. reviewed parametric <b>statistical</b> <b>families</b> used for direct estimation. Additionally, we shall introduce the Fast Fourier Transform method of call option pricing developed in [6]. It is intended to compare the reviewed estimation methods empirically. Risk neutral valuation principle, risk neutral distribution, logprice risk neutral distribution, risk neutral density function, Black Scholes formula, Fast Fourier Transform method, log-normal distributions, mixtures of log-normal distributions, generalized gamma distributions, model calibration, Merton’s jump diffusion model, Heston’s volatility model...|$|R
5000|$|From 2001 to 2005, Henley was a {{founding}} partner and Head Trader of a <b>statistical</b> arbitrage <b>family</b> of hedge funds, with clients such as Bank of America, Gottex Fund of Funds and Aegon USA Insurance Company.|$|R
40|$|Discusses {{about the}} {{different}} poverty measuements. education, countries, poverty, poverties, Indian, <b>statistical,</b> <b>statistical</b> probability, <b>family,</b> consuming, poverty estimates, relative poverty, income, median, china, countries, inequalities, consumes, poverty line, urban residents, rural population, affordability, demographic, consumption, food intake...|$|R
40|$|Transformation-based {{learning}} (TBL) {{and frequent}} pattern discovery (FPD) are two popular research paradigms, {{one from the}} domain of empirical natural language processing, the second {{from the field of}} data mining. This paper describes how Eric Brill's original TBL algorithm can be improved via incorporation of FPD techniques. The algorithm B-Warmr is presented that upgrades TBL to first-order logic and speeds up the search for transformations, also in the original propositional case. We demonstrate some scaling properties of B-Warmr and discuss how the algorithm can be tuned to generate (first-order) decision lists. We also propose a new method, Disjunctive TransformationBased Learning (DTBL) that combines the advantages of TBL and decision lists. 1 Introduction Basic idea. Empirical methods for natural language processing (Brill and Mooney, 1997) are commonly situated in one of two main families according to whether they operate on the sub-symbolic level (the <b>statistical</b> <b>family),</b> or [...] ...|$|E
30|$|Two {{different}} methods are introduced in the article: (1) computation of the spectral {{moments in the}} wavenumber–frequency domain and in the wavevector domain and (2) reconstruction of the spectrum using Gaussian statistics or a power-law distribution. The former is an objective, mathematical exercise and the method itself does not require any specific type of fluctuation data {{as far as the}} fluctuations belong the same <b>statistical</b> <b>family</b> (so, discontinuities should be avoided in the data). The latter is a subjective, interpretation work and the results depend on the choice of physics model of interest. We use a Gaussian shape of the spectrum both in the wavenumber–frequency domain, and an elliptic power-law spectrum in the wavevector domain. The corresponding physics model is the random sweeping model in the wavenumber–frequency domain (Kraichnan 1964; Narita 2017; Wilczek and Narita 2012) and the random spatial variation model (Gaussian statistics) with an elliptic sense of wavevector anisotropy (Carbone et al. 1995) for power-law spectrum of elliptic wavevector anisotropy.|$|E
40|$|International audienceThe {{objective}} of flood frequency analysis (FFA) is to associate flood intensity with a probability of exceedance. Many methods are currently employed for this, ranging from statistical distribution fitting to simulation approaches. In {{many cases the}} site of interest is actually ungauged, and a regionalisation scheme has {{to be associated with}} the FFA method, leading to a multiplication of the number of possible methods available. This paper presents the results of a wide-range comparison of FFA methods from statistical and simulation families associated with different regionalisation schemes based on regression, or spatial or physical proximity. The methods are applied to a set of 1535 French catchments, and a k-fold cross-validation procedure is used to consider the ungauged configuration. The results suggest that FFA from the <b>statistical</b> <b>family</b> largely relies on the regionalisation step, whereas the simulation-based method is more stable regarding regionalisation. This conclusion emphasises the difficulty of the regionalisation process. The results are also contrasted depending on the type of climate: the Mediterranean catchments tend to aggravate the differences between the methods...|$|E
40|$|Certain thermal non-equilibrium situations, {{outside of}} the astrophysical realm, suggest that entropy {{production}} extrema, instead of entropy extrema, are related to stationary states. In an effort {{to better understand the}} evolution of collisionless self-gravitating systems, we investigate the role of entropy production and develop expressions for the entropy production rate in two particular <b>statistical</b> <b>families</b> that describe self-gravitating systems. From these entropy production descriptions, we derive the requirements for extremizing the entropy production rate in terms of specific forms for the relaxation function in the Boltzmann equation. We discuss some implications of these relaxation functions and point to future work that will further test this novel thermodynamic viewpoint of collisionless relaxation. Comment: accepted for publication in Ap...|$|R
40|$|In this paper, {{we discuss}} <b>statistical</b> <b>families</b> P with the {{property}} {{that if the}} distribution of a random variable X is in P, then so is the distribution of ZwBi(X, p) for 0 %p% 1. (Here we take ZwBi(X, p) to mean that given XZx, Z is a draw from the binomial distribution Bi(x, p).) It is said that the family is closed under binomial subsampling. We characterize such families in terms of probability generating functions and for families with finite moments of all orders we give a necessary and sufficient condition for the family to be closed under binomial subsampling. The results are illustrated with power series and other examples, and related to examples from mathematical biology. Finally, some issues concerning inference are discussed...|$|R
40|$|This paper {{describes}} how a competitive tree learning algorithm {{can be derived}} from first principles. The algorithm approximates the Bayesian decision theoretic solution to the learning task. Comparative experiments with the algorithm and the several mature AI and <b>statistical</b> <b>families</b> of tree learning algorithms currently in use show the derived Bayesian algorithm is consistently as good or better, although sometimes at computational cost. Using the same strategy, we can design algorithms for many other supervised and model learning tasks given just a probabilistic representation for the kind of knowledge to be learned. As an illustration, a second learning algorithm is derived for learning Bayesian networks from data. Implications to incremental learning and the use of multiple models are also discussed. ...|$|R
40|$|We {{present an}} {{analysis}} of two thermodynamic techniques for determining equilibria of self-gravitating systems. One is the Lynden-Bell entropy maximization analysis that introduced violent relaxation. Since we do not use the Stirling approximation which is invalid at small occupation numbers, our systems have finite mass, unlike Lynden-Bell's isothermal spheres. (Instead of Stirling, we utilize a very accurate smooth approximation for x!.) The second analysis extends entropy production extremization to self-gravitating systems, also {{without the use of}} the Stirling approximation. In addition to the Lynden-Bell (LB) <b>statistical</b> <b>family</b> characterized by the exclusion principle in phase-space, and designed to treat collisionless systems, we also apply the two approaches to the Maxwell-Boltzmann (MB) families, which have no exclusion principle and hence represent collisional systems. We implicitly assume that all of the phase-space is equally accessible. We derive entropy production expressions for both families, and give the extremum conditions for entropy production. Surprisingly, our analysis indicates that extremizing entropy production rate results in systems that have maximum entropy, in both LB and MB statistics. In other words, both thermodynamic approaches lead to the same equilibrium structures. Comment: accepted for publication in Ap...|$|E
40|$|For {{reasons that}} have been well-rehearsed {{elsewhere}} (e. g., Ragin 1987; Hall 2002; Shalev 2007), the standard statistical methods are ill-suited to the tasks of characterizing and explaining cross-country diversity in systems of social protection. This paper draws on several recent strands of methodological innovation in comparative welfare state research and related fields, {{with a view to}} encouraging more self-conscious and systematic use of Esping-Andersen's welfare state regimes as an alternative epistemological framework for explanatory comparative research. I discuss diverse methodological tools and approaches which can help harness the power of regime-based approach in empirical analysis. I introduce some tools belonging to the <b>statistical</b> <b>family</b> known as multivariate analysis ("methods that simultaneously analyze multiple measurements on each individual or object under investigation", Hair et al. 1995 : 5). Some such tools (factor analysis and cluster analysis) are already staples of the field, but the paper also shows potential usefulness of correspondence analysis, which has some similarities to factor analysis but rests on assumptions more congenial to comparativists. The biplot, a "perceptual map" that makes it possible to simultaneously visualize affinities between cases and their attributes, adds value to these techniques...|$|E
40|$|Many {{parametric}} {{statistical models}} suffer from “intrinsic ambiguities” {{in the sense}} that the distribution of the observation vector is invariant to smooth, structured changes in the model’s parameters. The fact that certain members of the parametric <b>statistical</b> <b>family</b> are locally undistinguishable makes the Fisher information matrix (FIM) associated to the given statistical model singular. We examine such degenerate deterministic parameter estimation problems from a Riemannian geometric perspective. We start by replacing the original (ambiguous) parameter set by a lower-dimensional Riemannian (non-ambiguous) parameter set. The new parameter set comes in the form of a quotient space and is obtained by identifying equivalent family members in the initial parameterization. We specialize recently developed extensions of the Cramér-Rao bound (CRB) for the Riemannian setup to this particular setting. This offers a re-interpretation of the CRB inequality involving the pseudo-inverse of the FIM. Also, we present a lower bound on the variance (computed with respect to the geodesic distance) of unbiased estimators taking values in the quotient space. Geometrically, this corresponds to a fundamental limit on the capability of these estimators in discriminating adjacent parameter equivalence classes in the original problem parameterization. A numerical example involving the blind identification of single-input multipleoutput (SIMO) channels driven by a Gaussian source is worked out. 1...|$|E
40|$|The {{normalized}} {{maximum likelihood}} (NML) distribution plays a fundamental {{role in the}} MDL approach to statistical inference. It is only defined for <b>statistical</b> <b>families</b> with a finite Shtarkov sum. Here we characterize, for 1 -dimensional exponential families, when the Shtarkov sum is finite. This {{turns out to be the}} case if and only if the minimax redundancy is finite, thus extending the reach of our results beyond the individual-sequence setting. In practice, the NML/Shtarkov distribution is often approximated by the Bayesian marginal distribution based on Jeffreys' prior. One serious problem is that in many cases Jeffreys' prior cannot be normalized. It has been conjectured that Jeffreys' prior cannot be normalized in exactly the cases where the Shtarkov sum is infinite, i. e. when the minimax redundancy and regret are infinite. We show that the conjecture is true for a large class of exponential families but that there exist examples where th...|$|R
25|$|Blark. Gregory, et al. The Son Also Rises: Surnames and the History of Social Mobility (Princeton University Press; 2014) 384 pages; uses <b>statistical</b> data on <b>family</b> names over {{generations to}} {{estimate}} social mobility in diverse societies and historical periods.|$|R
40|$|The {{decennial census}} {{has been called}} the <b>statistical</b> {{national}} <b>family</b> portrait that is taken every ten years. The constitutionally mandated census counts heads, families, housing facts, and social, demographic and economic characteristics. The results may alter the boundaries of legislative districts, the apportionment of legislators within a district and allocated government funding. This workshop covers important changes that researchers and information providers must know about the 2010 census. The session includes virtual tours of resources and hands-on practice with web-based information sources for attendees...|$|R
40|$|Space Shuttle Reusable Solid Rocket Motors (RSRM) are static {{tested at}} two ATK Thiokol Propulsion {{facilities}} in Utah, T- 24 and T- 97. The newer T- 97 static test facility was recently upgraded to allow thrust measurement capability. All previous static test motor thrust measurements {{have been taken}} at T- 24; data from these tests were used to characterize thrust parameters and requirement limits for flight motors. Validation of the new T- 97 thrust measurement system is required prior to use for official RSRM performance assessments. Since thrust cannot be measured on RSRM flight motors, flight motor measured chamber pressure and a nominal thrust-to-pressure relationship (based on static test motor thrust and pressure measurements) are used to reconstruct flight motor performance. Historical static test and flight motor performance data are {{used in conjunction with}} production subscale test data to predict RSRM performance. The predicted motor performance is provided to support Space Shuttle trajectory and system loads analyses. Therefore, an accurate nominal thrust-to-pressure (F/P) relationship is critical for accurate RSRM flight motor performance and Space Shuttle analyses. Flight Support Motors (FSM) 7, 8, and 9 provided thrust data for the validation of the T- 97 thrust measurement system. The T- 97 thrust data were analyzed and compared to thrust previously measured at T- 24 to verify measured thrust data and identify any test-stand bias. The T- 97 FIP data were consistent and within the T- 24 static test <b>statistical</b> <b>family</b> expectation. The FSMs 7 - 9 thrust data met all NASA contract requirements, and the test stand is now verified for future thrust measurements...|$|E
40|$|We {{consider}} {{a family of}} estimation problems not admitting conventional analysis because of singularity and measurability issues. We define posterior distributions for the family by a variational technique analogous to that used to define Gibbs measures in <b>statistical</b> mechanics. The <b>family</b> of estimation problems, which arise in the asymptotic analysis of error-control codes, is parametrized by a code rate, R∈(0,∞); this is shown to be analogous to the absolute temperature of <b>statistical</b> mechanics. The <b>family</b> undergoes an (Ehrenfest) first-order phase transition at a critical code rate C (the channel capacity), {{where there is a}} convex set of posterior distributions. At all other code rates, there is only one posterior distribution; if R C it has infinite support. In a result reflecting the Dobrushin construction, we show that these posterior distributions are asymptotically consistent with those of families of finite-sequence error-control codes. © 2012 IOP Publishing Ltd and SISSA Medialab srl...|$|R
40|$|In {{the second}} half of the nineteenth century in the remote farming {{district}} of Dungog in the colony of New South Wales on the Australian continent, widows faced harsh economic realities. Using civil registration records, census data, newspaper reports, <b>statistical</b> returns, <b>family</b> histories and other sources, we have, where possible, reconstructed the lives of these widows, particularly those with dependent children. This paper discusses the range of survival strategies used. It presents statistical evidence from official records, and adds vignettes of the lives of a handful of widows whose strategies can be explored more completely using additional historical sources. Griffith Business School, Dept of Employment Relations and Human ResourcesFull Tex...|$|R
40|$|Neural {{networks}} are <b>family</b> <b>statistical</b> learning algorithms and structures {{and are used}} to estimate or approximate functions and pattern classification. The Neural network system is constructed through interconnected neurons and training weights. The paper will present the improvement of recognition rate, recognition time and hardware overhead through introducing TMR technology into the conventional RBF neural network which is a simple neural network only consisting of three layers. This {{research was supported by}} the Undergraduate Research Opportunities Program (UROP) ...|$|R
40|$|Ninety-six child {{protection}} files were scrutinized {{according to the}} Holland Complex Care Case Review Data Collection Instrument in order to verify {{the applicability of the}} instrument to determined the variables influencing social workers in the determination of bringing an adolescent in need of protection into care. Results show that school related issues seem to influence workers in determining the need to remove the child from the <b>family.</b> <b>Statistical</b> analyses indicated numerous correlations supporting the link between case complexity and the need to bring an adolescent into care...|$|R
40|$|We predict {{theoretically}} and numerically {{the existence}} of incoherent dispersive shock waves. They manifest themselves as an unstable singular behavior of the spectrum of incoherent waves that evolve in a noninstantaneous nonlinear environment. This phenomenon of ‘‘spectral wave breaking’’ develops in the weakly nonlinear regime of the random wave. We elaborate a general theoretical formulation of these incoherent objects {{on the basis of}} a weakly nonlinear <b>statistical</b> approach: a <b>family</b> of singular integrodifferential kinetic equations is derived, which provides a detailed deterministic description of the incoherent dispersive shock wave phenomenon...|$|R
40|$|Contemporary family {{environment}} {{is becoming more}} and more media-rich. According to the latest data from <b>statistical</b> investigations, <b>families</b> with children are equipped by different media tools with the growing tendency. This also brings an increasing tendency to be attacked by different risks and danger from media environment. The paper shows the European situation regarding the on-line risks from last European research and describes the situation in the Czech society. The analysis is based on the data from a research in Czech families with children at the age 6 – 17 and the description of the situation regarding the parental activities and parental monitoring. We use the parental and also children's perspectives. Our data showed that regardless the high level of discussion about risks from media and especially the internet in society, school and family, there is still part of parents who do not pay adequate attention to these matters. According to these findings, more attention should be paid to parental support and education in this field...|$|R
40|$|The {{statistical}} {{modeling of}} social network data is difficult {{due to the}} complex dependence structure of the tie variables. <b>Statistical</b> exponential <b>families</b> of distributions provide a flexible way to model such dependence. They enable the statistical characteristics of the network to be encapsulated within an exponential family random graph (ERG) model. For a long time, however, likelihood-based estimation was only feasible for ERG models assuming dyad independence. For more realistic and complex models inference {{has been based on}} the pseudo-likelihood. Recent advances in computational methods have made likelihood-based inference practical, and comparison of the different estimators possible. In this paper, we compare the bias, standard errors, coverage rates and efficiency of maximum likelihood and maximum pseudo-likelihood estimators. We also propose an improved pseudo-likelihood estimation method aimed at reducing bias. The comparison is performed using simulated social network data based on two versions of an empirically realistic network model, the first representing Lazega’s law firm data and the second a modified versio...|$|R
40|$|International audienceGiven {{the coarse}} spatial {{resolution}} of General Circulation Models, finer scale projections of variables affected by local-scale {{processes such as}} precipitation are often needed to drive impacts models, for example in hydrology or ecology among other fields. This need for high-resolution data leads to apply projection techniques called downscaling. Downscaling can be performed according to two approaches: dynamical and statistical models. The latter approach is constituted by various <b>statistical</b> <b>families</b> conceptually different. If several studies have made some intercomparisons of existing downscaling models, none of them included all those families and approaches {{in a manner that}} all the models are equally considered. To this end, the present study conducts an intercomparison exercise under the EURO- and MED-CORDEX initiative hindcast framework. Six Statistical Downscaling Models (SDMs) and five Regional Climate Models (RCMs) are compared in terms of precipitation outputs. The downscaled simulations are driven by the ERAinterim reanalyses over the 1989 – 2008 period over a common area at 0. 44 ° of resolution. The 11 models are evaluated according to four aspects of the precipitation: occurrence, intensity, as well as spatial and temporal properties. For each aspect, one or several indicators are computed to discriminate the models. The results indicate that marginal properties of rain occurrence and intensity are better modelled by stochastic and resampling-based SDMs, while spatial and temporal variability are better modelled by RCMs and resampling-based SDM. These general conclusions have to be considered with caution because they rely on the chosen indicators and could change when considering other specific criteria. The indicators suit specific purpose and therefore the model evaluation results depend on the end-users point of view and how they intend to use with model outputs. Nevertheless, building on previous intercomparison exercises, this study provides a consistent intercomparison framework, including both SDMs and RCMs, which is designed to be flexible, i. e., other models and indicators can easily be added. More generally, this framework provides a tool to select the downscaling model to be used according to the statistical properties of the local-scale climate data to drive properly specific impact models...|$|R
40|$|Heredity is an {{important}} factor in vulnerability to manic depression. A genetic linkage has been demonstrated between manic depression and coagulation factor IX at Xq 27 with a TaqI polymorphism at the F 9 locus in DNA samples from peripheral leucocytes of manic depressive probands and relatives in 10 informative <b>families.</b> <b>Statistical</b> analysis of the pedigrees gave a maximum lod score of 3 · 10 at a recombination fraction of 0 · 11, demonstrating a linkage between a manic depressive locus and the F 9 locus in the Xq 27 region. © 1987. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
40|$|KEY WORDS. Substantive and <b>statistical</b> hypotheses, <b>family</b> of tests, error probabilities, {{rules for}} adjustment. ABSTRACT. The {{well-known}} problem of cumulating error probabilities is reconsidered from a general epistemological perspective, namely, {{the concepts of}} severity (Popper) and of fairness of tests. Applying these concepts to hypothesis-testing research leads to a reevaluation of {{the relative importance of}} the probabilities of Type 1 and Type 2 errors connected with those statistical hypotheses that have been derived from the substantive ones. It is shown that not only Type 1 but also Type 2 errors can cumulate. This cumulation is discussed for various basic types of empirical situations in which substantive hypotheses are examined by means of statistical ones. A new adjustment strategy based on the Dunn-Bonferroni inequality for planned tests is proposed and applied to some empirical examples. When applying tests of significance, one usually addresses two kinds of statistical hypotheses and deals with two kinds of errors (Hays, 1977; Kendall & Stuart, 1961). The hypothesis actually tested by a particular test of signifi...|$|R
40|$|This {{article has}} three goals. First, it explores {{the effects of}} changes in census {{definitions}} and concepts on the measurement of living arrangements. As part of this analysis, the authors develop new {{estimates of the number}} of households and group quarters in each census year since 1850. Second, they evaluate the existing aggregate <b>statistical</b> series on <b>family</b> and household composition, with particular attention to problems in the measurement of subfamilies. Finally, they describe data and methods for developing a consistent set of statistics for the period since 1850 and offer recommendations for the coherent measurement of family and household composition. Copyright 2003 by The Population Council, Inc [...] ...|$|R
40|$|The {{purpose of}} {{discussed}} optimal valid partitioning (OVP) methods is uncovering of ordinal or continuous explanatory variables effect on outcome variables of different types. The OVP approach {{is based on}} searching partitions of explanatory variables space that {{in the best way}} separate observations with different levels of outcomes. Partitions of single variables ranges or two-dimensional admissible areas for pairs of variables are searched inside corresponding <b>families.</b> <b>Statistical</b> validity associated with revealed regularities is estimated with the help of permutation test repeating search of optimal partition for each permuted dataset. Method for output regularities selection is discussed that is based on validity evaluating with the help of two types of permutation tests...|$|R
40|$|A {{continuous}} infinite {{system of}} point particles with strong superstable interaction is {{considered in the}} framework of classical <b>statistical</b> mechanics. The <b>family</b> of approximated correlation functions is determined in such a way, that they take into account only such configurations of particles in R^d which for a given partition of the configuration space R^d into nonintersecting hyper cubes with a volume a^d contain no more than one particle in every cube. We prove that these functions converge to the proper correlation functions of the initial system if the parameter of approximation a→ 0 for any positive values of an inverse temperature β and a fugacity z. This result is proven both for two-body interaction potentials and for many-body case...|$|R
40|$|Abstract: The {{purpose of}} {{discussed}} optimal valid partitioning (OVP) methods is uncovering of ordinal or continuous explanatory variables effect on outcome variables of different types. The OVP approach {{is based on}} searching partitions of explanatory variables space that {{in the best way}} separate observations with different levels of outcomes. Partitions of single variables ranges or two-dimensional admissible areas for pairs of variables are searched inside corresponding <b>families.</b> <b>Statistical</b> validity associated with revealed regularities is estimated with the help of permutation test repeating search of optimal partition for each permuted dataset. Method for output regularities selection is discussed that is based on validity evaluating with the help of two types of permutation tests...|$|R
40|$|In this work, we {{investigated}} on microwave parameters geometry dependence in Graphene Field Effect Transistors (GFETs). A DC and RF {{characterization of the}} fabricated GFETs has been performed. The parametric analysis was carried out on 24 GFET families fabricated on the same chip and differing only for the channel length (Δ) and the gate length (Lg). In order to obtain a <b>statistical</b> average, each <b>family</b> included ten devices with the same geometry. Our study demonstrates that the output resistance and the cut-off frequency depend on both Δ and Lg. As expected, Rout increases with the graphene channel surface thus confirming the good quality of the fabrication procedures. An optimum region which maximizes the cut-off frequency has been found...|$|R
40|$|Using two volume-limited Main galaxy {{samples of}} the Sloan Digital Sky Survey Data Release 10 (SDSS DR 10), we examine the {{environmental}} dependence of galaxy age at fixed parameters or for different galaxy <b>families.</b> <b>Statistical</b> {{results show that the}} environmental dependence of galaxy age is stronger for late type galaxies, but can be still observed for the early types: the age of galaxies in the densest regime is preferentially older than that in the lowest density regime with the same morphological type. We also find that the environmental dependence of galaxy age for red galaxies and Low Stellar Mass (LSM) galaxies is stronger, while the one for blue galaxies and High Stellar Mass (HSM) galaxies is very weak...|$|R
40|$|Protein kinases {{play key}} roles in oncogenic {{signaling}} {{and are a}} major focus {{in the development of}} targeted cancer therapies. Imatinib, a BCR-Abl tyrosine kinase inhibitor, is a successful front-line treatment for chronic myelogenous leukemia (CML). However, resistance to imatinib may be acquired by BCR-Abl mutations or hyperactivation of Src family kinases such as Lyn. We have used multiplexed kinase inhibitor beads (MIBs) and quantitative mass spectrometry (MS) to compare kinase expression and activity in an imatinib-resistant (MYL-R) and-sensitive (MYL) cell model of CML. Using MIB/MS, expression and activity changes of over 150 kinases were quantitatively measured from various protein kinase <b>families.</b> <b>Statistical</b> analysis of experimental replicates assigned significance to 35 of these kinases, referred to as the MYL-R kinome profile. MIB...|$|R
