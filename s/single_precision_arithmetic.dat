25|1474|Public
5000|$|The same {{approach}} {{can be used to}} calculate digits of the binary expansion of ln(2) starting from an arbitrary nth position. The number of terms in the [...] "head" [...] sum increases linearly with n, but the complexity of each term only increases with the logarithm of n if an efficient method of modular exponentiation is used. The precision of calculations and intermediate results and the number of terms taken from the [...] "tail" [...] sum are all independent of n, and only depend on the number of binary digits that are being calculated - <b>single</b> <b>precision</b> <b>arithmetic</b> can be used to calculate around 12 binary digits, regardless of the starting position.|$|E
40|$|A single {{precision}} preconditioner for Krylov subspace iterative methods on the CELL processor Co-authored by: T. Kihara 1 Hiroto Tadano 2 Tetsuya Sakurai 3 The calculation techniques using the Cell processor have attracted attention for high perfor-mance computing. It is a heterogeneous multicore chip that is {{significantly different from}} conventional multi-processor or multicore architectures. The Cell processor provides extremely high performance single-precision floating operations, however the majority of scientific appli-cations require results with double precision. Large sparse linear systems Ax = b arise in many scientific applications. Krylov subspace iterative methods [2] are often used for solving such linear systems. Preconditioning techniques are efficient {{to reduce the number}} of iterations of Krylov subspace methods ([1]). However, the computational cost of the preconditioning part is sometimes large. In [3], an acceleration technique of preconditioning with <b>single</b> <b>precision</b> <b>arithmetic</b> is considered. We can obtain the approximate solution with double precision by using the Krylov subspace methods with the <b>single</b> <b>precision</b> <b>arithmetic</b> preconditioner. It is effective if the computational cost of the preconditioner is dominant in a iterative method. We have implemented and tested the {{single precision}} preconditioner on the Cell Processor. The computation of preconditioning is performed on Synergistic Processor Elements (SPE) with <b>single</b> <b>precision</b> <b>arithmetic.</b> Several numerical experiments illustrate the performance of this preconditioner...|$|E
40|$|Here is {{a method}} for very fast {{evaluation}} of the inverse of the normal distribution [...] in two versions. The first, given u, rapidly produces the solution x to 2, to within the accuracy available in <b>single</b> <b>precision</b> <b>arithmetic.</b> The second is faster. Using one less term in an expansion, it provides accuracy to within 0. 000002 [...] suitable for generating a normal random variable by direct inversion of its distribution function. ...|$|E
40|$|In {{this paper}} we discuss a {{processor}} architecture for interval arithmetic. Firstly it is shown that double precision FPUs can cheaply be split to support <b>single</b> <b>precision</b> interval addition/subtraction or multiplication, secondly we propose hardware support for double <b>precision</b> interval <b>arithmetic</b> and compare the effort and performance with software implementations on current architectures. 1 Introduction In this paper we present an architecture extension of a double precision floating point unit (FPU) in order to support <b>single</b> <b>precision</b> interval <b>arithmetic</b> very much alike the extensions which have been proposed for multimedia [6] or 3 D graphics support [3, 2]. And indeed, the most promising application of <b>single</b> <b>precision</b> interval <b>arithmetic</b> is in the same area. For rendering images, ray-tracing and other problems in computer graphics or constructive solid geometry interesting algorithms using interval arithmetic have been proposed [10, 4]. The interval algorithms are superior to the [...] ...|$|R
40|$|Recently, {{development}} of embedded processors is toward miniaturization and energy saving for ecology. On the other hand, high performance arithmetic circuits are required {{in a lot}} of application in science and technology. Dynamically reconfigurable processors have been developed to meet these requests. They can change circuit configuration according to instructions in program instantly during operations. This paper describes, a dynamically reconfigurable circuit for floating-point arithmetic is proposed. The arithmetic circuit consists of two <b>single</b> <b>precision</b> floating-point <b>arithmetic</b> circuits. It performs double <b>precision</b> floating-point <b>arithmetic</b> by reconfiguration. Dynamic reconfiguration changes circuit construction at one clock cycle during operation without stopping circuits. It enables reconfiguration of circuits in a few nano seconds. The proposed circuit is reconfigured in two modes. In first mode it performs one double <b>precision</b> floating-point <b>arithmetic</b> or else the circuit will perform two parallel operations of <b>single</b> <b>precision</b> floating-point <b>arithmetic.</b> The new system design reduces implementation area by reconfiguring common parts of each operation. It also increases the processing speed with a very little number of clocks. ...|$|R
40|$|International audienceGiven floating-point {{arithmetic}} with t-digit base-β significands {{in which}} all arithmetic operations are performed as if calculated to infinite precision and rounded to a nearest representable value, we prove that the product of complex values z_ 0 and z_ 1 can be computed with maximum absolute error z_ 0 z_ 11 / 2 β^ 1 - t√(5). In particular, this provides relative error bounds of 2 ^- 24 √(5) and 2 ^- 53 √(5) for IEEE 754 <b>single</b> and double <b>precision</b> <b>arithmetic</b> respectively, provided that overflow, underflow, and denormals do not occur. We also provide the numerical worst cases for IEEE 754 <b>single</b> and double <b>precision</b> <b>arithmetic...</b>|$|R
40|$|DS is a {{dependable}} {{and secure}} data storage for mobile, wireless networks {{based on a}} peer-to-peer paradigm. DS share files under a write-once model, and ensures data confidentiality and dependability by encoding files in a Redundant Residue Number System. The paper analyzes the code efficiency of DS using a set of moduli allowing for efficient encoding and decoding procedures based on <b>single</b> <b>precision</b> <b>arithmetic,</b> with the Information Dispersal Algorithm approach (IDA) shows that DS features security features which are not provided by IDA, while the two approaches are comparable {{from the viewpoint of}} code efficiency and encoding/decoding complexity...|$|E
40|$|We present several {{algorithms}} {{to compute}} the solution of a linear system of equations on a graphics processor (GPU), as well as general techniques to improve their performance, such as padding and hybrid GPU-CPU computation. We compare single and double precision performance of a modern GPU with unified architecture, and show how iterative refinement with mixed precision {{can be used to}} regain full accuracy in the solution of linear systems, exploiting the potential of the processor for <b>single</b> <b>precision</b> <b>arithmetic.</b> Experimental results on a GTX 280 using CUBLAS 2. 0, the implementation of BLAS for NVIDIA® GPUs with unified architecture, illustrate the performance of the different algorithms and techniques proposed...|$|E
40|$|Numerical {{characteristics}} of various Kalman filter algorithms are illustrated with a realistic orbit determination study. The {{case study of}} this paper highlights the numerical deficiencies of the conventional and stabilized Kalman algorithms. Computational errors associated with these algorithms {{are found to be}} so large as to obscure important mismodeling effects and thus cause misleading estimates of filter accuracy. The positive result {{of this study is that}} the U-D covariance factorization algorithm has excellent numerical properties and is computationally efficient, having CPU costs that differ negligibly from the conventional Kalman costs. Accuracies of the U-D filter using <b>single</b> <b>precision</b> <b>arithmetic</b> consistently match the double precision reference results. Numerical stability of the U-D filter is further demonstrated by its insensitivity to variations in the a priori statistics...|$|E
40|$|Abstract. Given floating-point {{arithmetic}} with t-digit base-β significands {{in which}} all arithmetic operations are performed as if calculated to infinite precision and rounded to a nearest representable value, we prove that the product of complex values z 0 and z 1 can be computed with maximum absolute error |z 0 | |z 1 | 1 2 β 1 −t √ 5. In particular, this provides relative error bounds of 2 − 24 √ 5 and 2 − 53 √ 5 for IEEE 754 <b>single</b> and double <b>precision</b> <b>arithmetic</b> respectively, provided that overflow, underflow, and denormals do not occur. We also provide the numerical worst cases for IEEE 754 <b>single</b> and double <b>precision</b> <b>arithmetic.</b> 1...|$|R
50|$|Floating Point Unit (FPU):Implements the new IEEE 754-2008 floating-point standard, {{providing}} the fused multiply-add (FMA) instruction (see Fused Multiply-Add subsection) for both <b>single</b> and double <b>precision</b> <b>arithmetic.</b> Up to 16 double precision fused multiply-add operations {{can be performed}} per SM, per clock.|$|R
40|$|It {{gives the}} {{architecture}} of an optimized complex matrix inversion using GAUSS-JORDAN (GJ) elimination in Verilog with <b>single</b> <b>precision</b> floating-point representation. The GJ-elimination algorithm uses a <b>single</b> <b>precision</b> floating point <b>arithmetic</b> components and control unit for performing necessary arithmetic operations. The proposed architecture implements the GJ-elimination algorithm for complex matrix element sequentially. Matrix inversion using GJ-elimination improves the frequency when compared with QR Decomposition algorithm. The design is targeted on XC 5 VLX 50 T Xilinx FPGA. Key word...|$|R
40|$|In {{recent years}} {{polynomial}} solvers based on algebraic geometry techniques, and specifically the action matrix method, have become popular for solving minimal problems in computer vision. In this paper we {{develop a new}} method for reducing the computational time and improving numerical stability of algorithms using this method. To achieve this, we propose and prove a set of algebraic conditions which allow us {{to reduce the size}} of the elimination template (polynomial coefficient matrix), which leads to faster LU or QR decomposition. Our technique is generic and has potential to improve performance of many solvers that use the action matrix method. We demonstrate the approach on specific examples, including an image stitching algorithm where computation time is halved and <b>single</b> <b>precision</b> <b>arithmetic</b> can be used. 1...|$|E
40|$|Abstract – The {{nonlinear}} analytic nodal method, {{which is}} formulated {{by combining the}} nonlinear itera-tion technique and the analytic nodal method (ANM), requires analytic solutions of the two-node prob-lems. When the method is applied to problems that contain near-critical nodes {{in which there is}} essentially no net leakage, the two-node ANM solution for such nodes results in highly ill-conditioned matrices and potential numerical instabilities, especially in <b>single</b> <b>precision</b> <b>arithmetic.</b> Two stabilization techniques are introduced to resolve the instability problem by employing alternate basis functions for near-critical nodes. The first uses the exact ANM solution for a critical node, and the second employs the nodal expan-sion method. Both techniques are shown to perform well; however, the solution accuracy can be mildly sensitive to the criterion used to invoke the stabilized coupling kernel. I...|$|E
40|$|Abstract—This paper {{describes}} {{an approach that}} allows applications to be developed in a software language, while taking advantage of hardware by facilities that automatically transform such software programs for hardware accelerators. A demonstration of this approach has been built for the C # language. Three case studies in numerical integration show that the automatically generated hardware accelerators can achieve similar speed-ups to manually optimised versions. In particular, the automatically generated accelerator running on an xc 4 vlx 160 FPGA at 83 MHz with <b>single</b> <b>precision</b> <b>arithmetic</b> can be more than 18 times faster and up to 143 times more power efficient than a Pentium 4 processor at 3. 6 GHz, while the double precision accelerator running at 64 MHz is 7 times faster and 77 times more power efficient. I...|$|E
40|$|We {{solve the}} Hubbard model with the exact diagonalization method on a {{graphics}} processing unit (GPU). We benchmark our GPU program against a sequential CPU code by using the Lanczos algorithm to solve the ground state energy in two cases: a one-dimensional ring and a two-dimensional square lattice. In the one-dimensional case, we obtain speedups of over 100 and 60 in <b>single</b> and double <b>precision</b> <b>arithmetic,</b> respectively. In the two-dimensional case, the corresponding speedups are over 110 and 70...|$|R
40|$|PIRO BAND is {{a library}} for bidiagonal {{reduction}} of unsymmetric banded matrices and tridiagonal reduction of symmetric banded matrices. It supports both real and complex matrices stored in packed band format. The software also can handle <b>single</b> and double <b>precision</b> <b>arithmetic</b> with 32 -bit and 64 -bit integers. We provide both a C library and MATLAB callable interfaces. The library is about 2 to 7 {{times faster than}} LAPACK’s band reduction routines. It is about {{twice as fast as}} the SBR toolbox for larger matrices...|$|R
40|$|This paper {{presents}} a C library for the software support of <b>single</b> <b>precision</b> floating-point (FP) <b>arithmetic</b> on processors without FP hardware units such as VLIW or DSP processor cores for embedded applications. This library provides several levels of compliance to the IEEE 754 FP standard. The complete specifications {{of the standard}} can be used or just some relaxed characteristics such as restricted rounding modes or computations without denormal numbers. This library is evaluated on the ST 200 VLIW processors from STMicroelectronics...|$|R
40|$|March, 1997 Abstract We {{describe}} a construction of almost universal hash functions suitable for very fast software implementation and {{applicable to the}} hashing of variable size data and fast cryptographic message authentication. Our construction uses fast <b>single</b> <b>precision</b> <b>arithmetic</b> which is increasingly supported by modern processors due to the growing needs for fast arithmetic posed by multimedia applications. We report on hand-optimized assembly implementations on a 150 MHz PowerPC 604 and a 150 MHz Pentium-Pro, which achieve hashing speeds of 350 to 820 Mbit/sec, depending on the desired level of security (or collision probability), and a {{rate of more than}} 1 Gbit/sec on a 200 MHz Pentium-Pro. This represents a significant speed-up over current software implementations of universal hashing and other message authentication techniques (e. g., MD 5 -based). Moreover, our construction is specifically designed to take advantage of emerging microprocessor technologies (such as Intel's MMX, [...] ...|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimited. 13. Abstract (Maximum 200 words). This article compares both real and complex outputs from sizeable numeric computations using identical code on several computer systems. The {{digital signal processing}} technique known as the modified covariance method {{was used as the}} computational engine. It is a recursive algorithm for solving the covariance equations of a linear predictor that seeks to predict an input signal by a linear combination of past signal samples. Single precision and double precision results are presented but the study focuses primarily on differences between the VAX Fortran 4. 8 and MacFortran/ 020 compilers. Differences in the first digit for <b>single</b> <b>precision</b> <b>arithmetic</b> were found and double precision differences occurred in the eighth digit. Arithmetic with complex data types was found to be less precise than with real data types. Although differences exist among various computer systems...|$|E
40|$|The {{nonlinear}} analytic nodal {{method is}} formulated {{by combining the}} nonlinear iteration technique and the analytic nodal method (ANM). The ANM "two-node" kernel employs analytic solutions of the onedimensional neutron flux at the interface between adjacent nodes. For problems in which the net leakage is small {{and one of the}} nodes is "near critical", the resulting matrices are highly ill-conditioned and numerical instability problems can occur, particularly in <b>single</b> <b>precision</b> <b>arithmetic.</b> Two stabilization techniques were investigated to resolve the instability problem which employ alternate basis functions for near-critical nodes. The first uses the exact ANM solution for a critical node and the second employs the Nodal Expansion Method (NEM) for nodes that are near critical. Both techniques are shown to perform well, however, the solution accuracy can be mildly sensitive to the criterion used to invoke the stabilized coupling kernel. I. INTRODUCTION Nonlinear nodal methods 1 have bee [...] ...|$|E
40|$|The theory {{underlying}} {{a proposed}} {{random number generator}} for numerical simulations in elementary particle physics and statistical mechanics is discussed. The generator {{is based on an}} algorithm introduced by Marsaglia and Zaman, with an important added feature leading to demonstrably good statistical properties. It can be implemented exactly on any computer complying with the IEEE- 754 standard for <b>single</b> <b>precision</b> floating point <b>arithmetic.</b> (orig.) SIGLEAvailable from TIB Hannover: RA 2999 (93 - 133) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|R
40|$|THe Collins-Loos {{algorithm}} for computing isolating intervals for the zeros of {{an integer}} polynomial requires {{the evaluation of}} polynomials at rational points. This implies the use of arbitrary <b>precision</b> integer <b>arithmetic.</b> It is shown how careful use of <b>single</b> <b>precision,</b> floating point <b>arithmetic</b> {{within the context of}} a slightly modified algorithm can make the calculation considerably faster and no less exact. Typically, 95 % or more of the evaluations can be done without exact arithmetic. The precise speedup depends on the relative costs of the arithmetic in a given implementation. The implementation on DEC KL- 10 computer is some 5 to 10 times faster than the original Univac 1110 implementation in SAC-I...|$|R
40|$|We {{present an}} {{efficient}} implementation of 7 –point and 27 –point stencils on high-end Nvidia GPUs. A new method {{of reading the}} data from the global memory to the shared memory of thread blocks is developed. The method avoids conditional statements and requires only two coalesced instructions to load the tile data with the halo. Additional optimizations include storing only one XY tile of data at a time in the shared memory to lower shared memory requirements, Common Subexpression Elimination {{to reduce the number of}} instructions, and software prefetching to overlap arithmetic and mem-ory instructions, and enhance latency hiding. The efficiency of our implementation is analyzed using a simple stencil memory footprint model that takes into account the actual halo overhead due to the minimum memory transaction size on the GPUs. We use the model to demonstrate that in our implementation the memory overhead due to the halos is largely eliminated by good reuse of the halo data in the memory caches, and that our method of reading the data is close to optimal in terms of memory band-width usage. Detailed performance analysis for <b>single</b> <b>precision</b> stencil computations, and performance results for <b>single</b> and double <b>precision</b> <b>arithmetic</b> on two Tesla cards are presented. Our stencil implementations are more efficient than any other imple-mentation described in the literature to date. On Tesla C 2050 with <b>single</b> and double <b>precision</b> <b>arithmetic</b> our 7 –point stencil achieves an average throughput of 12. 3 and 6. 5 Gpts/s, respectively (98 GFLOP/s and 52 GFLOP/s, respectively). The symmetric 27 –point stencil sustains a throughput of 10. 9 and 5. 8 Gpts/s, respectively...|$|R
40|$|Abstract. We {{present an}} {{improved}} matrix-matrix multiplication routine (GEMM) in the MAGMA BLAS library that targets the Fermi GPUs. We show how {{to modify the}} previous MAGMA GEMM kernels {{in order to make}} a more efficient use of the Fermi’s new architectural features, most notably their extended memory hierarchy and sizes. The improved kernels run at up to 300 GFlop/s in double and up to 600 GFlop/s in <b>single</b> <b>precision</b> <b>arithmetic</b> (on a C 2050), which is 58 % of the theoretical peak. We compare the improved kernels with the currently available in CUBLAS 3. 1. Further, we show the effect of the new kernels on higher level dense linear algebra (DLA) routines such as the one-sided matrix factorizations, and compare their performances with corresponding, currently available routines running on homogeneous multicore systems. A general conclusion is that DLA has become a better fit for the new GPU architectures, to the point where DLA can run more efficiently on GPUs than on current, high-end homogeneous multicore-based systems. ...|$|E
40|$|Digital Audio Broadcasting (DAB) is an {{emerging}} technology {{which is currently}} experiencing rapid growth following the advent of ETS 300 - 401 DAB standard in Europe and the ATSC ATV standard in the U. S. Wide-band audio compression is performed by ISO-MPEG layer II for DAB, and by Dolby AC- 3 for ATV. To achieve high quality audio, current DSP implementations for such compression algorithms typically use word-widths in excess of 18 bits. Implementation on industry standard 16 -bit DSP devices has to date required double-precision arithmetic. By suitable formulation of the synthesis transformation to allow dynamic scaling, high quality audio using <b>single</b> <b>precision</b> <b>arithmetic</b> can be achieved on a 16 -bit data path. To illustrate this approach, an implementation of Dolby AC- 3 utilising this technique is described. An alternative is also described: a mixed single-and-double precision direct implementation which exploits the variable precision requirements within the synthesis transform algorithm, thus yielding a low complexity solution. 1...|$|E
40|$|Suppose we {{are given}} a {{polynomial}} P(x 1,…,xr) in r≥ 1 variables, let m bound the degree of P in all variables xi, 1 ≤i≤r, and we wish to raise P to the nth power, n> 1. In a recent paper which compared the iterative versus the binary method it was shown that their respective computing times were O(m 2 rnr+ 1) versus O(mn) 2 r) when using <b>single</b> <b>precision</b> <b>arithmetic.</b> In this paper a new algorithm is given whose computing time is shown to be O((mn) r+ 1). Also if we allow for polynomials with multiprecision integer coefficients, the new algorithm presented here will be faster {{by a factor of}} mr− 1 nr over the binary method and faster by a factor of mr− 1 over the iterative method. Extensive empirical studies of all three methods show that this new algorithm will be superior for polynomials of even relatively small degree, thus guaranteeing a practical as well as a useful result...|$|E
40|$|We {{propose to}} study {{the impact on the}} energy {{footprint}} of two advanced algorithmic strategies in the context of high performance dense linear algebra libraries: (1) mixed precision algorithms with iterative refinement allow to run at the peak performance of <b>single</b> <b>precision</b> floating-point <b>arithmetic</b> while achieving double precision accuracy and (2) tree reduction technique exposes more parallelism when factorizing tall and skinny matrices for solving over determined systems of linear equations or calculating the singular value decomposition. Integrated within the PLASMA library using tile algorithms, which will eventually supersede the block algorithms from LAPACK, both strategies further excel in performance {{in the presence of a}} dynamic task scheduler while targeting multicore architecture. Energy consumption measurements are reported along with parallel performance numbers on a dual-socket quad-core Intel Xeon as well as a quad-socket quad-core Intel Sandy Bridge chip, both providing component-based energy monitoring at all levels of the system, through the Power Pack framework and the Running Average Power Limit model, respectively. © 2012 IEEE...|$|R
40|$|Abstract—We {{propose to}} study {{the impact on the}} energy {{footprint}} of two advanced algorithmic strategies in the context of high performance dense linear algebra libraries: (1) mixed precision algorithms with iterative refinement allow to run at the peak performance of <b>single</b> <b>precision</b> floating-point <b>arithmetic</b> while achieving double precision accuracy and (2) tree reduction technique exposes more parallelism when factorizing tall and skinny matrices for solving overdetermined systems of linear equations or calculating the singular value decomposition. Integrated within the PLASMA library using tile algorithms, which will eventually supersede the block algorithms from LAPACK, both strategies further excel in performance {{in the presence of a}} dynamic task scheduler while targeting multicore architecture. Energy consumption measurements are reported along with parallel performance numbers on a dual-socket quad-core Intel Xeon as well as a quad-socket quad-core Intel Sandy Bridge chip, both providing component-based energy monitoring at all levels of the system, through the PowerPack framework and the Running Averag...|$|R
40|$|Floating point {{arithmetic}} fpga implementation described various arithmetic operations like addition, subtraction, multiplication, division. Floating point fpga {{arithmetic unit}} {{are useful for}} <b>single</b> <b>precision</b> and double precision and quad precision. This precision specify various bit of operation. In science computation this circuit is useful. A general purpose arithmetic unit require for all operations. In this paper <b>single</b> <b>precision</b> floating point <b>arithmetic</b> calculation is described. This paper includes <b>single</b> <b>precision,</b> double precision and quad precision floating point format representation and provide implementation technique of various <b>arithmetic</b> calculation. Double <b>precision</b> and quad precision format specify more bit operation. Low precision custom format is useful for reduce the associated circuit costs and increasing their speed. Fpga implementation of Floating point arithmetic calculation provide various step which are require for calculation. Normalization and alignment are useful for operation and floating point number should be normalized before any calculation. I consider the implementation of 64 / 128 -bit precision circuits...|$|R
40|$|Abstract—This paper {{proposes a}} new sparse matrix storage format which allows an {{efficient}} {{implementation of a}} sparse matrix vector product on a Fermi Graphics Processing Unit (GPU). Unlike previous formats it has both low memory footprint and good throughput. The new format, which we call Sliced ELLR-T has been designed specifically for accelerating the iterative solution of a large sparse and complex-valued system of linear equations arising in computational electromagnetics. Numerical tests {{have shown that the}} performance of the new implementation reaches 69 GFLOPS in complex <b>single</b> <b>precision</b> <b>arithmetic.</b> Compared to the optimized six core Central Processing Unit (CPU) (Intel Xeon 5680) this performance implies a speedup by a factor of six. In terms of speed the new format is as fast as the best format published so far {{and at the same time}} it does not introduce redundant zero elements which have to be stored to ensure fast memory access. Compared to previously published solutions, significantly larger problems can be handled using low cost commodity GPUs with limited amount of on-board memory. 1...|$|E
40|$|URL] audienceWe {{study the}} use of a GPU for the {{numerical}} approximation of the curvature dependent flows of graphs - the mean-curvature flow and the Willmore flow. Both problems are often applied in image processing where fast solvers are required. We approximate these problems using the complementary finite volume method combined with the method of lines. We obtain a system of ordinary differential equations which we solve by the Runge-Kutta-Merson solver. It is a robust solver with an automatic choice of the integration time step. We implement this solver on CPU but also on GPU using the CUDA toolkit. We demonstrate that the mean-curvature flow can be successfully approximated in <b>single</b> <b>precision</b> <b>arithmetic</b> with the speed-up almost 17 on the Nvidia GeForce GTX 280 card compared to Intel Core 2 Quad CPU. On the same card, we obtain the speed-up 7 in double precision arithmetic which is necessary for the fourth order problem - the Willmore flow of graphs. Both speed-ups were achieved without affecting the accuracy of the approximation. The article is structured in such way that the reader interested only in the implementation of the Runge-Kutta-Merson solver on the GPU can skip the sections containing the mathematical formulation of the problems...|$|E
40|$|Abridged] We {{present the}} results of a highly {{parallel}} Kepler equation solver using the Graphics Processing Unit (GPU) on a commercial nVidia GeForce 280 GTX and the "Compute Unified Device Architecture" programming environment. We apply this to evaluate a goodness-of-fit statistic (e. g., chi^ 2) for Doppler observations of stars potentially harboring multiple planetary companions (assuming negligible planet-planet interactions). We tested multiple implementations using single precision, double precision, pairs of single precision, and mixed precision arithmetic. We find {{that the vast majority of}} computations can be performed using <b>single</b> <b>precision</b> <b>arithmetic,</b> with selective use of compensated summation for increased precision. However, standard single precision is not adequate for calculating the mean anomaly from the time of observation and orbital period when evaluating the goodness-of-fit for real planetary systems and observational data sets. Using all double precision, our GPU code outperforms a similar code using a modern CPU by a factor of over 60. Using mixed-precision, our GPU code provides a speed-up factor of over 600, when evaluating N_sys > 1024 models planetary systems each containing N_pl = 4 planets and assuming N_obs = 256 observations of each system. We conclude that modern GPUs also offer a powerful tool for repeatedly evaluating Kepler's equation and a goodness-of-fit statistic for orbital models when presented with a large parameter space. Comment: 19 pages, to appear in New Astronom...|$|E
50|$|The CPU {{consisted}} of an ALU {{that was built}} around the Am2901 bit-sliced microprocessor. To this a byte manipulation unit was added which could perform the shifting, rotating and masking operation required for handling eight and sixteen bit data. Additional logic was provided to support both signed and unsigned two's complement comparisons in a <b>single</b> operation, multiple <b>precision</b> <b>arithmetic</b> and floating point normalization. Most operations could be performed in 150 ns, however the cycle time was variable from 125 ns to 200 ns under microprogram control so that timing could be optimized. A microsequencer, based around the Am2910, directed the control flow though the microprogram. It could perform branches, loops and subroutine calls most {{of which could be}} conditional on any of several CPU status conditions.|$|R
40|$|Michela TauferThe {{advent of}} general purpose {{graphics}} processing units (GPGPU???s) brings about {{a whole new}} platform for running numerically intensive applications at high speeds. Their multi-core architectures enable large degrees of parallelism via a massively multi-threaded environment. Molecular dynamics (MD) simulations are particularly well-suited for GPU???s because their computations are easily parallelizable. Significant performance improvements are observed when <b>single</b> <b>precision</b> floating point <b>arithmetic</b> is used. However, this performance comes {{at the cost of}} accuracy: it is widely acknowledged that constant-energy (NVE) MD simulations accumulate errors as the simulation proceeds due to the inherent errors associated with integrators used for propagating the coordinates. A consequence of this numerical integration is the drift of potential energy as the simulation proceeds. Double <b>precision</b> <b>arithmetic</b> partially corrects this drifting but is significantly slower than <b>single</b> <b>precision</b> and is comparable to CPU performance. To address this problem, we present development of a library of mathematical functions that use fast and efficient algorithms to improve numerical reproducibility and stability of large-scale simulations. We test the library in terms of its performance and accuracy with a synthetic code that emulates the behavior of MD codes on GPU, and then we present results of a first integration of our library in a MD code. These first results show correction of the drifting with a performance much better than double precision. Computer Scienc...|$|R
40|$|Most modern {{processors}} have hardware {{support for}} <b>single</b> <b>precision</b> and double precision floating point multiplication. For many scientific computations like climate modeling, computational physics and computational geometry this support is inadequate. They impose {{the use of}} quadruple <b>precision</b> <b>arithmetic</b> because it provides twice the precision of double precision format. The proposed design performs four <b>single</b> <b>precision</b> multiplications in parallel, or two double precision multiplications in parallel, or one quadruple precision multiplication. The throughput is increased {{by a factor of}} four for <b>single</b> <b>precision</b> multiplication and by two for double precision multiplication...|$|R
