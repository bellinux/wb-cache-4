1|10|Public
40|$|Energy-parity {{objectives}} combine ω-regular with quantitative {{objectives of}} reward MDPs. The controller needs to avoid {{to run out}} of energy while satisfying a parity objective. We refute the common belief that, if an energy-parity objective holds almost-surely, then this can be realised by some finite memory strategy. We provide a surprisingly simple counterexample that only uses coBüchi conditions. We introduce the new class of bounded (energy) storage objectives that, when combined with parity objectives, preserve the finite memory property. Based on these, we show that almost-sure and limit-sure energy-parity objectives, as well as almost-sure and limit-sure <b>storage</b> <b>parity</b> objectives, are in NP∩coNP and can be solved in pseudo-polynomial time for energy-parity MDPs...|$|E
5000|$|A [...] "parity track" [...] {{was present}} on the first {{magnetic}} tape data <b>storage</b> in 1951. <b>Parity</b> in this form, applied across multiple parallel signals, {{is known as a}} transverse redundancy check. This can be combined with parity computed over multiple bits sent on a single signal, a longitudinal redundancy check. In a parallel bus, there is one longitudinal redundancy check bit per parallel signal.|$|R
50|$|In the {{original}} implementation, each main memory module contained 0.5 Mbytes of <b>storage</b> with <b>parity</b> protection constructed using 64K dynamic MOS RAMs. Random access cycle time was 500 ns per 32-bit word but multi-word transfers, for example {{to and from}} the cache, yielded an effective cycle time of 250 ns per 32-bit word (16 Mbytes per second). The memory modules decoded 26-bit physical word addresses and within this limit total memory capacity was restricted only by the number of available system bus slots; depending on the I/O configuration of the system, up to 10 Mbytes of physical memory could be installed. A later implementation of the memory module increased the size to 2 MB using 256K RAMs.|$|R
40|$|We {{evaluate}} {{the reliability of}} storage system schemes consisting of an equal numbers of data disks and parity disks where each parity disk contains the exclusive or (XOR) {{of two or three}} of the data disks. These schemes are instances of Survivable <b>Storage</b> using <b>Parity</b> in Redundant Array Layouts (SSPiRAL). They have the same storage costs as mirrored organizations and use very simple parity schemes. Through a novel dynamic analysis of the likelihood of data losses, we show that these schemes are one hundred thousand to a million times less likely to lose data than a comparable mirrored organization. We also found that schemes where each parity disk contains the exclusive or of three data disks performed much better than schemes where each parity disk contains the exclusive or of only two data disks. 1...|$|R
40|$|We {{propose the}} use of parity-based {{redundant}} data layouts of increasing reliability {{as a means to}} progressively harden data archives. We evaluate the reliability of two such layouts and demonstrate how moving to layouts of higher parity degree offers a mechanism to progressively and dramatically increase the reliability of a multi-device data store. Specifically we propose that a data archive can be migrated to progressively more reliable layouts as the data ages, trading limited (and likely unrealized) increases in update costs for increased reliability. Our parity-based schemes are drawn from SSPiRAL (Survivable <b>Storage</b> using <b>Parity</b> in Redundant Array Layouts) that offer capacity efficiency equivalent to a straightforward mirroring arrangement. Our analysis shows our proposed schemes would utilize no additional physical resources and result in improvements to mean time to data loss of four to seven orders of magnitude. ...|$|R
40|$|Parity {{checking}} comprises a low-redundancy {{method for}} the design of reliable digital systems. While quite effective for detecting single-bit transmission or <b>storage</b> errors, <b>parity</b> encoding has not been widely used for checking the correctness of arithmetic results because parity is not preserved during arithmetic operations and parity prediction requires fairly complex circuits in most cases. We propose a general strategy for designing parity-checked arithmetic circuits that takes advantage of redundant intermediate representations. Because redundancy is often used for high performance anyway, the incremental cost of our proposed method is quite small. Unlike conventional binary numbers, redundant representations can be encoded and manipulated {{in such a way that}} parity is preserved in each step. Additionally, lack of carry propagation ensures that the effect of a fault is localized rather than catastrophic. After establishing the framework for our parity-preserving transformations in computer arithmetic, we illustrate some applications of the proposed strategy to the design of parity-checked adder/subtractors, multipliers, and other arithmetic structures used in signal processing...|$|R
40|$|Big data {{stores are}} {{becoming}} increasingly important {{in a variety of}} domains including scientific computing, internet applications, and business applications. For price and performance reasons, such storage is comprised of magnetic hard drives. To achieve the necessary degree of performance and reliability, the drives are configured into storage subsystems based on RAID (Redundant Array of Independent Disks). Because of their mechanical nature, hard drives are relatively power-hungry and slow compared to most other computer components. Big data centers spend tremendous amounts on power, including cooling, adding significantly to their overall costs. Additionally, drives are orders of magnitude slower than electrical computer components, resulting in significant performance challenges any time disk I/O is required. Recently, SSDs (solid state drives) have emerged based on flash memory technology. Although too expensive to replace magnetic disks altogether, SSDs use less power and are significantly faster for certain operations. This dissertation examines several new architectures that use a limited amount of faster hardware to decrease the power consumption and increase the performance or data redundancy of RAID storage subsystems. We present RAID 4 S, which uses SSDs for <b>parity</b> <b>storage</b> in a disk-SSD hybrid RAID system. Because of itsbetter performance characteristics, SSD <b>parity</b> <b>storage</b> reduces thedisk overhead associated with <b>parity</b> <b>storage</b> and thereby significantly reduces the disk overheads caused by RAID. This decreases the power consumption and can be used to increase the performance of simple RAID schemes or increase redundancy by enabling the use of higher order RAID schemes with less performance penalty. Storing parity on SSDs can reduce the average number of I/Os serviced by the remaining disks by up to 25 - 50 %. By replacing some hard drives with SSDs, we reduce power and improve performance. Our RAID 4 S-modthresh optimization improves performance in certain workloads. The other two architectures expose RAID's inability to handle heterogeneity in workloads and hardware. RAIDE is motivated by a workload imbalance we detected in stripe-unaligned workloads. By placing faster hardware to handle the higher workload {{at the edges of the}} array, we observed higher throughput. RAIDH places parity on faster devices based on device weights. Higher weighted devices are faster and thus store additional parity. The slowest devices may store no parity at all. This technique can be used on any heterogeneous array to enable faster random write throughput...|$|R
40|$|SDDS). An LH*RS file is hash {{partitioned}} {{over the}} distributed RAM of a multicomputer, e. g., {{a network of}} PCs, and supports the unavailability of any of its k ≥ 1 server nodes. The value of k transparently grows with the file to offset the reliability decline. Only {{the number of the}} storage nodes potentially limits the file growth. The high-availability management uses a novel parity calculus that we have developed, based on the Reed-Salomon erasure correcting coding. The resulting <b>parity</b> <b>storage</b> overhead is about the minimal ever possible. The parity encoding and decoding are faster than for any other candidate coding we are aware of. We present our scheme and its performance analysis, including experiments with a prototype implementation on Wintel PCs. The capabilities of LH*RS offer new perspectives to data intensive applications, including the emerging ones of grids and of P 2 P computing...|$|R
40|$|Continuous media such as {{video or}} audio from {{databases}} that are disk resident require real-time disk I/O support. Video on demand {{systems have been}} widely studied and most proposed designs {{take advantage of the}} (largely) predictable nature of the I/O stream to provide both guaranteed upper bounds on delay and reasonably high resource utilizations. For disk based 3 D interactive systems the problem is very di erent since the user's actions determine the future deadlines for model data at the display. The data layout we propose and evaluate in this paper abandons the idea of a careful layout of data for a completely randomized layout. We consider large, multidisk systems in which the 3 D model data is partitioned into granules which are the logical unit of data that gets transferred {{in and out of the}} active scene graph. Granules form parity groups as is familiar from disk arrays. In the proposed system, the use of redundancy in the form of parity is used under normal conditions for load balancing (since for a parity group of size G, any G; 1 blocks can be read to obtain the data). Preliminary simulation results suggest that for moderate increase in <b>storage</b> for <b>parity,</b> one can obtain high degree of disk bandwidth utilization combined with a guaranteed " maximum delay, i. e., with probability approaching 1. The simulations show the nature ofthetradeo s between utilization and the probability that deadlines are met. ...|$|R
40|$|LH*RS is a high-availability {{scalable}} distributed {{data structure}} (SDDS). An LH*RS file is hash partitioned over the distributed RAM of a multicomputer, e. g., {{a network of}} PCs, and supports the unavailability of any k ≥ 1 of its server nodes. The value of k transparently grows with the file to offset the reliability decline. Only {{the number of the}} storage nodes potentially limits the file growth. The high-availability management uses a novel parity calculus that we have developed, based on Reed-Salomon erasure correcting coding. The resulting <b>parity</b> <b>storage</b> overhead is about the lowest possible. The parity encoding and decoding are faster than for any other candidate coding we are aware of. We present our scheme and its performance analysis, including experiments with a prototype implementation on Wintel PCs. The capabilities of LH*RS offer new perspectives to data intensive applications, including the emerging ones of grids and of P 2 P computing...|$|R
40|$|We address {{various issues}} {{dealing with the}} use of disk arrays in {{transaction}} processing environments. We look at the problem of transaction undo recovery and propose a scheme for using the redundancy in disk arrays to support undo recovery. The scheme uses twin page <b>storage</b> for the <b>parity</b> information in the array. It speeds up transaction processing by eliminating the need for undo logging for most transactions. The use of redundant arrays of distributed disks to provide recovery from disasters as well as temporary site failures and disk crashes is also studied. We investigate the problem of assigning the sites of a distributed storage system to redundant arrays {{in such a way that}} a cost of maintaining the redundant parity information is minimized. Heuristic algorithms for solving the site partitioning problem are proposed and their performance is evaluated using simulation. We also develop a heuristic for which an upper bound on the deviation from the optimal solution can be established...|$|R

