3|10000|Public
40|$|A {{computer-assisted}} {{method for}} localizing a rack, including <b>sensing</b> <b>an</b> <b>image</b> of the rack, detecting line segments in the sensed image, recognizing a candidate arrangement of line segments in the sensed image {{indicative of a}} predetermined feature of the rack, generating a matrix of correspondence between the candidate arrangement of line segments and an expected position and orientation of the predetermined feature of the rack, and estimating a position and orientation of the rack based on the matrix of correspondence...|$|E
40|$|An {{apparatus}} (10) {{for determining}} a position for an object (5) {{in relation to}} a presentation of an image (3) that is to be presented comprises a position pattern generator (12) for generating a position pattern (13) which is divided into a multiplicity of pattern sections, wherein each of the pattern sections has a unique bit pattern comprising a multiplicity of bit patterns and wherein the bit patterns are Gray-coded in a generalized manner. The apparatus (10) also comprises a combination unit (14) for combining the position pattern (13) with the at least one image (3) that is to be presented and for providing a corresponding combined image, and an optical sensor (15) for optically <b>sensing</b> <b>an</b> <b>image</b> detail from the combined image, wherein the image detail correlates to the position of the object (5).; In addition, the apparatus (10) comprises a filter (16) for extracting at least one bit pattern, which corresponds to a pattern section from the position pattern (13), from the image detail and for providing at least one corresponding extracted pattern section, and also a determination device for determining the position of the object (5) {{on the basis of the}} at least one extracted bit pattern. A method for determining the position of an object (5) is likewise disclosed...|$|E
40|$|This report {{describes}} a method called Mixed Pixel Allocation Quality (MPAQ) for determining a quantitative {{value for the}} quality of detecting mixed pixels during a automated segmentation process of an image obtained by remote <b>sensing.</b> <b>An</b> <b>image</b> consists of picture elements called pixels. When these elements cover more than one object they are called mixed pixels else they are called pure. For this report we are especially interested in satellite images of agricultural fields, but MPAQ is also usable for other kind of images. The segmentation program used here is the best merge algorithm designed by Ron Schoenmakers [4]. To determine the quality of the resulting segmentation one has to have a correct segmentation for comparison. A segmentation made by human experts is considered to be correct. In spite of the segmentation process being laborious and very costly the resulting segmentation has a high probability of containing errors. The {{reason for this is that}} humans remain fallible and a segmentation made by different people will result in different segmentations making it hard to tell which man-made segmentation is correct. In this thesis a correct segmentation is obtained during the process of creating an artificial image 2 Chapter 1 : Introduction by a method developed by Theo Schouten and Maurice Klein Gebbinck [9]. A possible comparison alone however is not enough, one also has to have a specific method for determining the correctness of the segmentation. The used methods for evaluating the accuracy of an image are the percentage correct and the kappa coefficient [5, 6]. This chapter describes remote sensing and its applications. Information about segmentation of an image and existing segmentation methods is given. Finally some information is given about the quantit [...] ...|$|E
50|$|UV/visible {{light from}} an {{integrating}} sphere (and/or other source {{such as a}} black body) is focused onto a square test target at the focal plane of a collimator (the mirrors in the diagram), such that <b>a</b> virtual <b>image</b> of the test target will be seen infinitely far away by the camera under test. The camera under test <b>senses</b> <b>a</b> real <b>image</b> of the virtual image of the target, and the sensed image is displayed on a monitor.|$|R
40|$|SummaryEdge {{detection}} is {{a signal}} processing algorithm common in artificial intelligence and image recognition programs. We have constructed a genetically encoded edge detection algorithm that programs an isogenic community of E. coli to <b>sense</b> <b>an</b> <b>image</b> of light, communicate {{to identify the}} light-dark edges, and visually present {{the result of the}} computation. The algorithm is implemented using multiple genetic circuits. An engineered light sensor enables cells to distinguish between light and dark regions. In the dark, cells produce a diffusible chemical signal that diffuses into light regions. Genetic logic gates are used so that only cells that sense light and the diffusible signal produce a positive output. A mathematical model constructed from first principles and parameterized with experimental measurements of the component circuits predicts the performance of the complete program. Quantitatively accurate models will facilitate the engineering of more complex biological behaviors and inform bottom-up studies of natural genetic regulatory networks...|$|R
40|$|This paper {{presents}} a new intensity-to-time processing paradigm suitable for very large scale integration (VLSI) computational sensor implementation of global operations over sensed images. Global image quantities usually describe images with fewer data. When computed {{at the point}} of sensing, global quantities result in a low-latency performance due to the reduced data transfer requirements between <b>an</b> <b>image</b> sensor and <b>a</b> processor. The global quantities also help global top-down adaptation: the quantities are continuously computed on-chip, and are readily available to <b>sensing</b> for adaptation. <b>As</b> an example, we have developed <b>a</b> sorting <b>image</b> computational sensor [...] -a VLSI chip which <b>senses</b> <b>an</b> <b>image</b> and sorts all pixel by their intensities. The first sorting sensor prototype is a 21 222 26 array of cells. It receives <b>an</b> <b>image</b> optically, <b>senses</b> it, and computes the image's cumulative histogram [...] -a global quantity which can be quickly routed off chip via one pin. In addition, the global cumulative histogram is used internally on-chip in a top-down fashion to adapt the values in individual pixel so as to reflect the index of the incoming light, thus computing an "image of indices. " The image of indices never saturates and has a uniform histogram. Index Terms [...] - Computational sensors, image sensors, robot perception, smart sensors, vision, VLSI sensors. I...|$|R
40|$|Conventional {{video cameras}} have limited fields {{of view that}} make them {{restrictive}} {{in a variety of}} vision applications. There are several ways to enhance the field of view of an imaging system. However, the entire imaging system must have a single effective viewpoint to enable the generation of pure perspective <b>images</b> from <b>a</b> <b>sensed</b> <b>image.</b> <b>A</b> camera with a hemispherical field of view is presented. Two such cameras can be placed back-to-back, without violating the single viewpoint constraint, to arrive at a truly omnidirectional sensor. The implications of such a sensor for computational vision are explored. Results ar...|$|R
40|$|We seek to {{understand}} and correct shading artifacts caused by varying solar illumination conditions and different viewing angles in remotely <b>sensed</b> <b>images.</b> <b>A</b> synthetic <b>image</b> simulating the shading effects for the sun angle and viewing angle as <b>a</b> real <b>image,</b> is generated using a ter-rain reflectance model and digital eleva-tion data. A good correlation is obtained between the real and synthetic images. The artifact correction is then made, and <b>a</b> resulting <b>image</b> shows useful information that is not apparent in the original image. We also present a three-dimensional perspective representation of a terrain with a corresponding aerial photograph mapped on it, which incidentally may help image interpretation. I...|$|R
5000|$|The neighborhood, Bolvanovka (Болвановка), derives {{its name}} from Russian bolvan (болван), which could mean either a billet or, in {{obsolete}} <b>sense,</b> <b>a</b> non-Orthodox cult <b>image.</b> There have different Bolvanovka neighborhoods in medieval Moscow, a sign of wide Tatar presence (e.g. near Taganka Square).|$|R
40|$|This paper {{conveys a}} {{proof-of-concept}} chip for Gaussian pyramid generation for image feature detectors. Gaussian filtering and image resizing are performed with a switched capacitor (SC) network. The chip is conceived as the mapping of CMOS- 3 D architecture for feature detectors onto a conventional technology, with some functionality removed, {{and the corresponding}} area overhead {{with respect to a}} CMOS- 3 D architecture, but preserving massively parallel Correlated Double Sampling (CDS) and A/D conversion. The chip has been fabricated on a die of 5 x 5 mm 2 with 0. 18 m CMOS technology, achieving an array of 176 x 120 sensing elements (pixels). The pixels are arranged in Processing Elements (PEs). Every PE comprises four photodiodes, four SC nodes, one CDS circuit, and local circuitry for one ADC. Every PE occupies an area of 44 x 44 m 2. The chip <b>senses</b> <b>an</b> <b>image</b> and computes the Gaussian pyramid with an average power consumption lower than 75 nW/pixel at 30 frames/...|$|R
40|$|How {{to greatly}} benefit form {{a variety of}} remote sensing data has become a bottle-neck problem in {{application}} of remote <b>sensing.</b> <b>As</b> <b>an</b> <b>image</b> has high spectral resolution, but relatively lower spatial resolution. Image fusion technology emerged as the times required, can highly enhance image quality by distilling and integrating information of several images, overcoming the limitations and differences of the single one in the geometry, spectrum and spatial resolution, etc. Lots of fusion methods were posed, but they more or less change the spectral information. To ameliorate this problem, this paper addresses a simple, fast, and efficient method SAO (Simple Arithmetic Operation). It just needs to take the multi-spectral and high-resolution panchromatic <b>images</b> carry out <b>a</b> simple arithmetic operation. SPOT 5 XS+P images are {{used to assess the}} effectiveness of SAO method in comparison to IHS transform, Brovey transform, SSVR methods. Compared to other fusion methods, the images generated by SAO method have high spatial resolution while preserving the basic spectral characters of the original multi-spectral image. 1...|$|R
50|$|Thermal imaging detects the {{temperature}} {{difference between the}} background and the foreground objects. Some organisms are able to <b>sense</b> <b>a</b> crude thermal <b>image</b> by means of special organs that function as bolometers. This allows thermal infrared sensing in snakes, which functions by detection of thermal radiation.|$|R
50|$|In 1975, the School {{was divided}} into three departments, Geodesy, Photogrammetry, and Surveying, {{to take account of}} {{emerging}} technologies deriving from developments in electronics and space science, including satellite technology for geopositioning and remote <b>sensing.</b> <b>A</b> major <b>Image</b> Analysis Laboratory was installed in 1977 and the Centre for Remote Sensing (later known as the Centre for Remote Sensing and Geographic Information Systems (GIS)) was established in 1982, jointly with the UNSW Schools of Geography and Electrical Engineering.|$|R
50|$|The noise {{caused by}} {{quantizing}} the pixels of <b>a</b> <b>sensed</b> <b>image</b> to <b>a</b> number of discrete levels {{is known as}} quantization noise. It has an approximately uniform distribution. Though it can be signal dependent, it will be signal independent if other noise sources are big enough to cause dithering, or if dithering is explicitly applied.|$|R
5000|$|In {{the verdict}} the judges {{expressed}} their {{shock of the}} brutality of the murder:"It was enough for one bullet, fired from a sniper rifle, to end {{the life of the}} infant Shalhevet Pass, who up to that event was unknown to the wide public, and just lived her life as all other children, until one day as the evening came she was hit in her head, and she died, and Shalhevet whom was still small and in her infant stage, was sentenced to death by a vile killer whom intentionally, using a Telescopic sight, pulled the trigger. The picture of the shot baby is on our table, is engraved in our minds and does not give peace to our souls. We cannot understand and we cannot accept the unbearable ease with which the killer decided to harm a helpless person... We the judges are only humans and we cannot see anything else but the image which emerges in our <b>senses,</b> <b>an</b> <b>image</b> full of hate, blood and bereavement. We must not accept this image and we need {{to do everything we can}} to condemn it." ...|$|R
30|$|Orfeo ToolBox is an {{open-source}} {{project for}} state-of-the-art remote <b>sensing,</b> including <b>a</b> fast <b>image</b> viewer, applications callable from command-line, Python or QGIS, {{and a powerful}} C++ API. This article is {{an introduction to the}} Orfeo ToolBox’s flagship features {{from the point of view}} of the two communities it brings together: remote sensing and software engineering.|$|R
50|$|There {{is a lot}} of anecdotal {{evidence}} that reading musical notation can cause musicians to <b>sense</b> <b>an</b> auditory <b>image</b> of the notes they are reading which is a phenomenon called notational audiation. Present studies show that only some musicians who can read musical notation can hear an inner voice emulating the melody while reading the notation which has served as an interesting mode of study to understand the way information is encoded in the brain. Musicians have their sense of notational audiation significantly impaired during phonatory distractions due to the conflicting signals induced onto a single sensory modality. Some musicians who are proficient at reading sheet music may experience <b>an</b> auditory <b>image</b> while reading over the excerpt for Symphony No. 40 from Mozart below.|$|R
40|$|Two image {{reconstruction}} methods currently dominate parallel MR imaging: SENSE and GRAPPA. While both seek {{to reconstruct}} images from subsampled multi-channel MRI data, there exist fundamental {{differences between the}} two. In particular, <b>SENSE</b> reconstructs <b>an</b> <b>image</b> of the excited spindensity directly whereas GRAPPA reconstructs estimates of the fully sampled raw coil data and then combines them to obtain <b>an</b> <b>image.</b> In this work we show that these differences can be exploited such that each method can compliment the other. In the case of <b>SENSE,</b> which requires <b>an</b> estimate of the coil sensitivity map before reconstruction, one can use GRAPPA to improve the coil sensitivity estimates. Alternatively, using coil sensitivity estimates and the SENSE reconstruction equations, one can improve the GRAPPA reconstruction parameter estimation. Together, these approaches can provide higher image quality than either method alone. 1...|$|R
40|$|Conventional {{video cameras}} have limited fields {{of view that}} make them {{restrictive}} {{in a variety of}} vision applications. There are several ways to enhance the field of view of an imaging system. However, the entire imaging system must have a single effective viewpoint to enable the generation of pure perspective <b>images</b> from <b>a</b> <b>sensed</b> <b>image.</b> <b>A</b> camera with a hemispherical field of view is presented. Two such cameras can be placed back-toback, without violating the single viewpoint constraint, to arrive at a truly omnidirectional sensor. 1 Introduction Conventional imaging systems are quite limited in their field of view. Is it feasible to devise a video camera that can, at any instant in time, "see" in all directions? Such an omnidirectional camera would have an impact on a variety of applications, including autonomous navigation, video surveillance, video conferencing, virtual reality and site modelling. Our approach to omnidirectional image sensing is to incorporate reflecting surfaces [...] ...|$|R
50|$|A visual {{prosthesis}} {{can create}} <b>a</b> <b>sense</b> of <b>image</b> by electrically stimulating neurons {{in the visual}} system. A camera would wirelessly transmit to an implant, the implant would map the <b>image</b> across <b>an</b> array of electrodes. The array of electrodes has to effectively stimulate 600-1000 locations, stimulating these optic neurons in the retina thus will create <b>an</b> <b>image.</b> The stimulation can also be done anywhere along the optic signal's path way. The optical nerve can be stimulated {{in order to create}} <b>an</b> <b>image,</b> or the visual cortex can be stimulated, although clinical tests have proven most successful for retinal implants.|$|R
5000|$|A dead {{metaphor is}} one in which the <b>sense</b> of <b>a</b> {{transferred}} <b>image</b> has become absent. Examples: [...] "to grasp a concept" [...] and [...] "to gather what you've understood" [...] use physical action as a metaphor for understanding. The audience does not need to visualize the action; dead metaphors normally go unnoticed. Some people distinguish between a dead metaphor and a cliché. Others use [...] "dead metaphor" [...] to denote both.|$|R
30|$|In {{addition}} to autonomous pattern formation, the QS-based communication mechanism {{can also be}} applied to detect complex spatial signals. Tabor et al. recently developed a multi-module gene circuit system for edge detection, a signal processing algorithm common in artificial intelligence and <b>image</b> recognition [99]. <b>As</b> illustrated in Figure  4 B, the biological edge detection algorithm is composed of three modules: a dark sensor (NOT light), cell-cell communication cassette, and an X AND (NOT Y) genetic logic. The darker sensor was engineered based on the light-sensitive protein Cph 8, a chimeric sensor kinase. With the covalent association of chromophore phycocyanobilin produced from heme via ho 1 and pcyA [100],[101], Cph 8 is able to activate the ompC promoter (PompC) by transferring a phosphoryl group to the response regulator OmpR. However, {{in the presence of}} red light, the kinase activity of Cph 8 is inhibited, which precludes the transcription from PompC and causes a NOT light transcriptional logic gate. The cell-cell communication was implemented through the Lux QS system and was used to convert light information into spatial distribution of AHL. With the incorporation of the converter cI and the hybrid promoter Plux-lambda, the state of PompC is converted via an X AND (NOT Y) logical operation into the state of the promoter Plux-lambda, which is displayed via the production of LacZ that produces black pigment. Upon the loading of the programs, a lawn of isogenic E. coli populations was able to <b>sense</b> <b>an</b> <b>image</b> of light, communicate to identify the light-dark edges, and visually present the result of the computation.|$|R
40|$|A map-guided {{approach}} to interpretation of remotely sensed imagery is described, {{with emphasis on}} applications involving continuous monitoring of predetermined ground sites. Geometric correspondence between <b>a</b> <b>sensed</b> <b>image</b> and <b>a</b> symbolic reference map is established in an initial stage of processing by adjusting parameters of a sensor model so that image features predicted from the map optimally match corresponding features extracted from the sensed image. Information in the map is then used to constrain where to look in <b>an</b> <b>image</b> and what to look for. With such constraints, previously intractable remote sensing tasks can become feasible, even easy, to automate. Four illustrative examples are given, involving the monitoring of reservoirs, roads, railroad yards, and harbors...|$|R
5000|$|Kurosawa {{was often}} criticized by his countrymen for {{perceived}} [...] "arrogant" [...] behavior. It was in Japan that the (initially) disparaging nickname [...] "Kurosawa Tennō"—"The Emperor Kurosawa"—was coined. [...] "Like tennō", Yoshimoto claimed, [...] "Kurosawa {{is said to}} cloister himself in his own small world, which is completely {{cut off from the}} everyday reality of the majority of Japanese. The nickname tennō is used in this <b>sense</b> to create <b>an</b> <b>image</b> of Kurosawa as a director who abuses his power solely for the purpose of self-indulgence." ...|$|R
3000|$|At every iteration, {{the neural}} net works by {{simulating}} both {{an approach to}} the degradation process (backward) and to the restoration solution (forward), while refining the results according to a optimization criteria. However, the input to the net is always the image ytru, as no net training is required. Let us remark that we manage [...] "backward" [...] and [...] "forward" [...] concepts in the opposite <b>sense</b> to <b>a</b> standard <b>image</b> restoration problem due to the specific architecture of the net.|$|R
40|$|International audienceIn this paper, {{we present}} a novel {{unsupervised}} algorithm, called CLimate and rEmote sensing Association patteRns Miner, for mining association patterns on heterogeneous time series from climate and remote sensing data integrated in <b>a</b> remote <b>sensing</b> information system developed to improve the monitoring of sugar cane fields. The system, called RemoteAgri, consists of a large database of climate data and low-resolution remote <b>sensing</b> <b>images,</b> <b>an</b> <b>image</b> preprocessing module, <b>a</b> time series extraction module, and time series mining methods. The preprocessing module was projected to perform accurate geometric correction, what is a requirement particularly for land and agriculture applications of satellite images. The time series extraction is accomplished through a graphical interface that allows easy interaction and high flexibility to users. The time series mining method transforms series to symbolic representation {{in order to identify}} patterns in <b>a</b> multitemporal satellite <b>images</b> and associate them with patterns in other series within a temporal sliding window. The validation process was achieved with agroclimatic data and NOAA-AVHRR images of sugar cane fields. Results show a correlation between agroclimatic time series and vegetation index images. Rules generated by our new algorithm show the association patterns in different periods of time in each time series, pointing to a time delay between the occurrences of patterns in the series analyzed, corroborating what specialists usually forecast without having the burden of dealing with many data charts...|$|R
40|$|This work {{is focused}} on methods of {{real-time}} vein imaging using a camera in the near infrared region. The theoretical section describes the dependence of reflectance and absorbance of blood and tissue on the wavelength and suggests appropriate ways of illumination and viewing the bloodstream. The paper describes methods for real-time venous processing and projection. Based on theoretical knowledge <b>a</b> device <b>sensing</b> and projecting <b>an</b> <b>image</b> of real time examination of the venous system and a user-friendly programme for operating this device were created. The created device has been tested {{on a number of}} people to confirm proper operation...|$|R
40|$|The present work {{used the}} remote <b>sensing</b> through <b>a</b> <b>image</b> {{of the high}} {{resolution}} from the QuickBird satellite {{as a tool to}} survey the structure of the urban Iracemápolis, SP. There were made supervised classifications for characterization of intra-urban elements and the proportions obtained, as soil, tree cover, lawns, asphalt and roofs. The Urban Forestry studies have a lot importance to planning, management and research the presence of trees in the city. The trees help to cities to keep a good maintenance of microclimate, aesthetic landscape, flood control, pollution reduction and others. The Iracemápolis city has mid level of urban forestry (0, 57) compared with Maringá-PR (0, 94), which is a model city. This way, the city requires the implementation of public policies to increase afforestation and mitigate the harmful effects of its absence, especially for the microclimate of the city. Pages: 738 - 74...|$|R
40|$|The {{author has}} {{identified}} the following significant results. Geometric correspondance between <b>a</b> <b>sensed</b> <b>image</b> and <b>a</b> symbolic map is established in an initial {{stage of processing}} by adjusting parameters of <b>a</b> <b>sensed</b> model so that the image features predicted from the map optimally match corresponding features extracted from the sensed image. Information in the map is then used to constrain where to look in <b>an</b> <b>image,</b> what to look for, and how to interpret what is seen. For simple monitoring tasks involving multispectral classification, these constraints significantly reduce computation, simplify interpretation, and improve {{the utility of the}} resulting information. Previously intractable tasks requiring spatial and textural analysis may become straightforward in the context established by the map knowledge. The use of map-guided image analysis in monitoring the volume of water in a reservoir, the number of boxcars in a railyard, and the number of ships in a harbor is demonstrated...|$|R
50|$|When Mary Shelley makes {{a choice}} {{regarding}} {{the place on}} which Doctor Frankenstein will finally render to our memory the creature which he procreated, she opts for an icy wasteland. The place {{that looks like a}} perfect description of eternity, where everything is motionless, out of the <b>senses,</b> like <b>a</b> frozen <b>image</b> in which only a wind might be giving a promise of itself - a breath of death, something which remains at the very end, yet testifying that we are still here, {{on this side of the}} life.|$|R
40|$|Abstract Considering of {{the human}} visual system and the {{characteristics}} of remote <b>sensing</b> <b>images,</b> <b>a</b> novel <b>image</b> fusion strategy is presented for panchromatic high resolution image and multispectral image in nonsubsampled contourlet (NSCT) domain. The NSCT can give the asymptotic optimal representation of the edges and contours in image {{by virtue of the}} characteristics of good multiresolution, shift-invariance and highly directionality. An adaptive intensity component addition method based on the LHS transform is introduced into NSCT domain to preserve spatial resolutions and color content simultaneously. Experimentals show that the method proposed in this paper can improve spatial resolution and keep the spectral information, simultaneously. There are improvements both in visual effects and quantitative analysis, which is compared with that of the traditional PCA method, IHS transform technique, wavelet transform weighted fusion method, the corresponding wavelet transform-based fusion method and the contourlet transform-based fusion methods, respectively. Key words Image fusion, nonsubsampled contourlet transform, LHS transform, adaptive intensity component addition <b>Image</b> fusion is <b>a</b> process by combining two or more source images from different modalities or instruments into <b>a</b> single <b>image</b> with more information. The successful fusion is of great importance in many applications, such as remote sensing, computer vision and medical imaging, e...|$|R
40|$|This paper {{studies the}} effect of {{synthetic}} feature vectors on the classification performance of hyperspectral remote <b>sensing</b> <b>images.</b> <b>As</b> feature vectors, it has been chosen to employ morphological attribute profiles, that have proven themselves in this field. At this early stage of our work, the relatively simple Bootstrapping algorithm {{has been used for}} synthetic feature vector generation. Based on experiments conducted on multiple hyperspectral datasets, it has been observed that synthetic feature vectors contribute considerably to classification performance in the case of limited training dataset sizes...|$|R
40|$|Abstract: Studies in {{the area}} of Pattern Recognition have {{indicated}} that a classification model performs differently from one pattern class to another. This observation gave birth to the idea of combining the individual results of different classifiers to derive a consensus decision. This work investigates the potential of combining classifiers to remotely <b>sensed</b> <b>images.</b> <b>A</b> classifier system is built integrating the results of a feed-forward neural network and of a maximal likelihood classifier. Fuzzy Integrals are used as the combining strategy. Experiments carried out to evaluate the system, using <b>a</b> satellite <b>image</b> of <b>an</b> area undergoing a rapid degradation process, has hown that the combination may considerably improve the classification performance of the individual classifiers...|$|R
40|$|International audienceSingle pixel imaging {{opened the}} door to a cheaper camera {{architecture}} able to operate in a wide spectral range. Such an optical setup has been used with compressed <b>sensing</b> to reconstruct <b>an</b> <b>image</b> via l 1 -minimization ruling out real time applications. In order to have a direct restoration of the <b>image,</b> we consider <b>an</b> adaptive approach for which we propose a new acquisition strategy. Our method progressively acquires <b>an</b> <b>image</b> in the wavelet domain by predicting the significant coefficients. For this, we base our technique on the non-linear approximation of the wavelet transform taking advantage of the transformation’s sparsity. This new strategy is shown to offer high performance on simulated and real data that we compare to compressive sensing acquisitions. One possible application of the single pixel camera can be foreseen in fluorescence images of biological structures...|$|R
30|$|The {{second example}} is {{motivated}} by computational photography [35], where one takes a sequence of measurements and each measurement corresponds to the integrated light intensity through a designed mask. We consider <b>a</b> scheme for <b>sensing</b> <b>a</b> high-resolution <b>image</b> that exploits {{the fact that the}} patches of the image can be approximated using a Gaussian mixture model, as demonstrated in Fig.  1. We break the image into 8 × 8 patches, which resulted in 89250 patches. We randomly select 500 patches (0.56 % of the total pixels) to estimate a GMM model with C= 10 components, and then based on the estimated GMM, initialize Info-Greedy Sensing with K= 5 measurements and sense the rest of the patches. This means we can use a compressive imaging system to capture five low-resolution images of size 238 × 275 (this corresponds to compressing the data into 8.32 % of its original dimensionality). With such a small number of measurements, the recovered image from Info-Greedy Sensing measurements has superior quality compared with those with random sensing measurements.|$|R
40|$|A novel {{planar array}} sensor based on {{electrical}} conductivity measurements ispresented {{which may be}} applied to visualize surface fluid distributions. The sensor ismanufactured using printed-circuit board fabrication technology and comprises of 64 x 64 interdigital sensing structures. An associated electronics measures the electricalconductivity of the fluid over each individual <b>sensing</b> structure in <b>a</b> multiplexed manner byapplying a bipolar excitation voltage and by measuring the electrical current flowing from adriver electrode to <b>a</b> <b>sensing</b> electrode. After interrogating all <b>sensing</b> structures, <b>a</b> two-dimensional <b>image</b> of the conductivity distribution over a surface is obtained which in turnrepresents fluid distributions over sensorÃ¢Â€Â™s surface. The employed electronics can acquire upto 2500 frames per second thus being able to monitor fast transient phenomena. The systemhas been evaluated regarding measurement accuracy and depth sensitivity. Furthermore, theapplication of the sensor {{in the investigation of}} two different flow applications is presented...|$|R
