95|129|Public
2500|$|In a {{conventional}} typewriter the typebar reaches {{the end of}} its travel simply by striking the ribbon and paper. A [...] "noiseless" [...] typewriter has a complex lever mechanism that decelerates the typebar mechanically before pressing it against the ribbon and paper in an attempt to dampen the noise. It certainly reduced the high-frequency content of the <b>sound,</b> <b>rendering</b> it more of a [...] "clunk" [...] than a [...] "clack" [...] and arguably less intrusive, but such advertising claims as [...] "A machine that can be operated {{a few feet away from}} your desk — And not be heard" [...] were not true.|$|E
6000|$|Knut {{did nothing}} more this winter. The two Norse kings, quite unequal to attack such an armament, except by ambush and engineering, sailed away; again plundering at {{discretion}} on the Danish coast; carrying into Sweden great booties and many prisoners; but obliged to lie fixed all winter; and indeed {{to leave their}} fleets there {{for a series of}} winters,--Knut's fleet, posted at Elsinore {{on both sides of the}} <b>Sound,</b> <b>rendering</b> all egress from the Baltic impossible, except at his pleasure. Ulf's opportune deliverance of his royal brother-in-law did not much bestead poor Ulf himself. He had been in disfavor before, pardoned with difficulty, by Queen Emma's intercession; an ambitious, officious, pushing, stirring, and, both in England and Denmark, almost dangerous man; and this conspicuous accidental merit only awoke new jealousy in Knut. Knut, finding nothing pass the Sound worth much blockading, went ashore; [...] "and the day before Michaelmas," [...] says Snorro, [...] "rode with a great retinue to Roeskilde." [...] Snorro continues his tragic narrative of what befell there: ...|$|E
5000|$|... #Subtitle level 3: <b>Sound</b> <b>rendering</b> as {{a method}} for 3D sound {{synthesis}} ...|$|E
40|$|We {{present an}} {{overview}} of some principles of synthetic vision and audition for digital autonomous actors in virtual worlds. After a short review of the state-of-the-art we focus on some aspects of synthetic vision and virtual world constraints. Then, we present a simple real time structured <b>sound</b> <b>renderer.</b> This <b>sound</b> <b>renderer</b> is used as audition channel for synthetic and real actors and synchronized sound track generator for video film productions...|$|R
50|$|Note: Declensions {{are named}} {{according}} to their form in Proto-Germanic. Often intervening <b>sound</b> changes <b>render</b> the once transparent stem endings opaque, and the name may no longer make much sense synchronically.|$|R
50|$|In {{practice}} {{two species}} are generally used: the Ashkenazi and Sefardi shofar {{is made from}} the horn of a domestic ram, while a Yemeni shofar is made from the horn of a kudu. A Moroccan Shofar is a flat Shofar with no curves besides the main curve.A crack or hole in the shofar affecting the <b>sound</b> <b>renders</b> it unfit for ceremonial use. A shofar may not be painted in colors, {{but it may be}} carved with artistic designs (Shulkhan Arukh, Orach Chayim, 586, 17). Shofars (especially the Sephardi shofars) are sometimes plated with silver across part of their length for display purposes, although this invalidates them for use in religious practices.|$|R
5000|$|Shock Grenade - A {{reusable}} spherical {{device that}} emits a blinding {{light and a}} deafening <b>sound,</b> <b>rendering</b> all individuals in its vicinity unconscious. Exposure to the device can cause temporary blindness. They are often deployed in advance of an attack force to neutralize opposition.|$|E
5000|$|Graves was {{sometimes}} sarcastic. In {{dealing with a}} colleague's attack {{on the use of}} the stethoscope (the instrument was advocated by himself and Stokes having been invented in France in 1816), he wrote: [...] "We suspect Dr Clutterbuck's sense of hearing must be injured: for him the 'ear trumpet' magnifies but distorts <b>sound,</b> <b>rendering</b> it less distinct than before". Dr. Clutterbuck was Henry Clutterbuck, 1770-1856.|$|E
5000|$|In a {{conventional}} typewriter the typebar reaches {{the end of}} its travel simply by striking the ribbon and paper. A [...] "noiseless" [...] typewriter has a complex lever mechanism that decelerates the typebar mechanically before pressing it against the ribbon and paper in an attempt to dampen the noise. It certainly reduced the high-frequency content of the <b>sound,</b> <b>rendering</b> it more of a [...] "clunk" [...] than a [...] "clack" [...] and arguably less intrusive, but such advertising claims as [...] "A machine that can be operated {{a few feet away from}} your desk — And not be heard" [...] were not true.|$|E
5000|$|The digraph [...] is {{used at the}} end of {{the word}} to {{represent}} the sound [...] (in other positions this <b>sound</b> is <b>rendered</b> by means of the usual Italian orthography rules: [...] before front vowels and [...] before non-front vowels).|$|R
50|$|To {{segregate}} {{the sound}} source, CASA systems mask the cochleagram. This mask, sometimes a Wiener filter, weighs the target source regions and suppresses the rest. The physiological motivation behind the mask {{results from the}} auditory perception where <b>sound</b> is <b>rendered</b> inaudible by a louder sound.|$|R
50|$|The actual game logic {{has to be}} {{implemented}} by some algorithms. It is distinct from any <b>rendering,</b> <b>sound</b> or input work.|$|R
50|$|In July 2000 the {{official}} support ended with patch 2.26f by Epic MegaGames. Therefore, with the awareness and permission of Epic, the fan community started the OldUnreal Community patch project {{based on the}} original source code in 2008. The latest patch iteration, 2.27i, released in November 2012, features new graphics rendering like DirectX 9, updated OpenGL, new <b>sound</b> <b>rendering</b> based on OpenAL and fixes many incompatibilities with modern operating systems and hardware. It also contains the in Unreal Tournament introduced UnrealED2. In 2015 Tim Sweeney announced that he hoped to one day be able to release the engine as open source to the public.|$|E
50|$|The {{processing}} procedure {{consists of four}} parts. Characteristic sound of each object will be generated first. Then the sound is created and attached to the moving objects. The third step is to calculate the convolutions. Finally the calculated convolutions will {{be applied to the}} sound sources we get in step two.The convolution calculations in step three are related to the effect of reverberation. The mathematical description of reverberation is a convolution with a continuous weighting function. This is due to the echos in the environment. The <b>sound</b> <b>rendering</b> method approximates this by using the fact that the wavelength of sound {{is similar to that of}} the object, so it diffuses in its reflections. This provides a smoothing effect of the sound. These facts allow us to use a simplified sound tracking algorithm without making much difference.All in all, this method is much simpler than HRTF, but it is not real-time. However, its similarity to ray-tracing and its unique approach to handling reverberation are noteworthy aspects.|$|E
40|$|This paper {{presents}} {{recent advances}} in Audio Coding and 3 D <b>Sound</b> <b>Rendering.</b> The techniques described are in lines with MPEG- 4 requirements and have already been proposed to the MPEG community. This paper is divided in two parts. On the one hand, a scaleable audio decoder based on an ITU core coder is described. The scheme proposed is particularly attractive since it allows for fine granularity scalability with high coding quality. On the other hand, original and efficient 3 D <b>sound</b> <b>rendering</b> techniques are described. These algorithms combine high quality <b>sound</b> <b>rendering</b> with real time implementation capabilities. ...|$|E
5000|$|Poo-Chi spoke using {{prerecorded}} sounds such as barks, whines, and growls. Due to {{limitations of}} the toy's technology, most of the <b>sounds</b> were <b>rendered</b> as beeps rather than a realistic sound effect. Poo-Chi could also [...] "bark" [...] songs, which could be prompted by pressing the button {{on the top of}} its head.|$|R
50|$|When {{exposed to}} noise, the human ear's {{sensitivity}} to sound is decreased, corresponding {{to an increase}} in the threshold of hearing. This shift is usually temporary but may become permanent. A natural physiological reaction to these threshold shifts is vasoconstriction, which will reduce the amount of blood reaching the hair cells of the organ of Corti in the cochlea. With the resultant oxygen tension and diminished blood supply reaching the outer hair cells, their response to sound levels is lessened when exposed to loud <b>sounds,</b> <b>rendering</b> them less effective and putting more stress on the inner hair cells. This can lead to fatigue and temporary hearing loss if the outer hair cells do not get the opportunity to recover through periods of silence. If these cells do not get this chance to recover, they are vulnerable to death.|$|R
5000|$|Latin FL — The other unique Sicilian {{sound is}} found in those words that have been derived from Latin words {{containing}} -fl-. In standard literary Sicilian, the <b>sound</b> is <b>rendered</b> as ci (representing the voiceless palatal fricative [...] ), e.g. ciumi , but {{can also be found}} in written form as hi, sci, x or çi.|$|R
40|$|A novel room {{acoustic}} simulation system {{capable of}} producing interactive sound environments in dynamic and complex 3 D geometries is introduced. The system is distributed to several modules that share the same 3 D geometry. All changes made by one module are updated in all the other modules in real time. The auralization tools of the system include a geometry reduction tool, a beam tracing algorithm, and a <b>sound</b> <b>rendering</b> application. The geometry reduction simplifies 3 D models for beam tracing module that forwards direct sound and early reflection paths for <b>sound</b> <b>rendering.</b> The <b>sound</b> <b>rendering</b> application contains a automatic estimation of late reverberation parameters, based on early reflections...|$|E
40|$|<b>Sound</b> <b>rendering</b> {{requires}} that many different aspects are considered, especially when rendering a real-time virtual environment. In <b>sound</b> <b>rendering,</b> {{much the same}} as for graphics, one of the major influencing factors is the number of reflective polygons in a scene and due to the increase in the ability of most common graphics cards; this number can now be very high, even when designers create the optimum scene using other optimizing tools such as Polygon Cruncher or Rational Reducer. In addition, the use of programs such as LightscapeTM [20], which is used to provide realistic lighting, by using per vertex shading, increases the number of polygons in a scene by several factors. Therefore a strong, pre-processing method is proposed that dramatically reduces the number of polygons in the scene to a suitable level for real-time <b>sound</b> <b>rendering.</b> The method can also be combined with other methods (e. g. scene partitioning) for even lower CPU usage. Categories and Subject Descriptor...|$|E
40|$|Abstract — Spatial <b>sound</b> <b>rendering</b> {{has many}} {{applications}} such as music production, movies, electronic gaming and teleconferencing. Each of the applications may have different quality and complexity requirements. This paper presents a new spatial <b>sound</b> <b>rendering</b> framework that aims at producing realistic multichannel audio while being flexible and scalable so that is can be extended and adopted by various applications. The proposed framework uses multi-channel measured room impulse response (MMRIR) {{as the basis for}} building a room acoustic model which is used to synthesize multi-channel audio. The proposed framework has been evaluated by informal listening tests...|$|E
50|$|Ulysses was {{originally}} rated 'X' in the UK after extensive cuts were demanded by BBFC censor John Trevelyan. However director Joseph Strick replaced the offending dialogue {{with a series}} of screeches and <b>sounds,</b> thus <b>rendering</b> the scenes unintelligible. Eventually the film was released uncut in 1970, and the rating was reduced to '15' for the video release in 1996.|$|R
5000|$|The word paika {{comes from}} the Sanskrit padatika meaning infantry. [...] "Akhada" [...] or akhara refers to a {{training}} hall, {{or in this case}} referring to a particular string of such schools. The former spelling is an alternate transcription of the proper Sanskrit akhara in which the Oriya letter ଡ଼ ṛ, a flapped [...] <b>sound,</b> is <b>rendered</b> as d.|$|R
6000|$|When {{the noise}} of the {{conflict}} announced that it was at the hottest, the Jester began to shout, with the utmost power of his lungs, [...] "Saint George and the dragon!--Bonny Saint George for merry England!--The castle is won!" [...] And these <b>sounds</b> he <b>rendered</b> yet more fearful, by banging against each other two or three pieces of rusty armour which lay scattered around the hall.|$|R
40|$|As an {{expansion}} of the NPSNET Research Group (NRG), the Auralization and Acoustics Laboratory (AA-Lab) at the Naval Postgraduate School studies the integration of aural cues into virtual environments. Currently, the AA-Lab focuses on spatialacoustic <b>sound</b> <b>rendering</b> via headphones (closed-field) and loudspeakers (open-field) ...|$|E
40|$|Extending the {{frontier}} of visual computing, <b>sound</b> <b>rendering</b> utilizes auditory display to communicate in-formation to a user and offers an alternative means of visualization. By harnessing the sense of hearing, <b>sound</b> <b>rendering</b> can further enhance a user’s expe-rience in a multimodal virtual world. In addition to immersive environments, auditory displays can provide a natural and intuitive human-computer interface for many desktop applications. In this paper, we give {{a brief overview of}} recent work at UNC Chapel Hill on fast algorithms for sound synthesis and sound propagation. These include physically-based sound synthesis for rigid bodies and liquid sound generation, as well as numeric and geometric algorithms for sound propagation. We highlight their performance on different benchmarks and briefly discuss some problems for future research. 1...|$|E
40|$|Interactive {{audiovisual}} applications {{also contain}} many recorded sounds. Recent advances in interactive 3 D <b>sound</b> <b>rendering</b> use frequency-domain approaches, effecting perceptually validated progressive processing {{at the level}} of Fourier Transform coefficients [Tsingos 2005]. For faster interactive rendering, perceptually based auditory masking and sound-source clustering can be used [Tsininria- 00607249...|$|E
5000|$|Aberdeen is {{pronounced}} [...] in Received Pronunciation, and [...] (with a short a sound) in Scottish Standard English. The local Doric pronunciation, [...] (with a long a <b>sound),</b> is frequently <b>rendered</b> [...] "Aiberdein".|$|R
500|$|Intermission/Meet the Soundtrack: The {{orchestra}} musicians depart and the Fantasia title card is revealed. After the intermission {{there is}} a brief jam session of jazz music led by a clarinettist as the orchestra members return. Then a humorously stylized demonstration of how <b>sound</b> is <b>rendered</b> on film is shown. An animated sound track [...] "character", initially a straight white line, changes into different shapes and colors based on the sounds played.|$|R
5000|$|Yeísmo: Most Cantabrian dialects do not {{distinguish}} between the /ʝ/ (written y) and /ʎ/ (written ll) fonemes, executing both with a single <b>sound</b> ʝ. Thus, <b>rendering</b> poyu and pullu (stone seat and chicken, respectively) homophones.|$|R
40|$|The aim of {{this paper}} is to show {{preliminary}} results of a study aimed at dealing with the specific problem of assuring real-time <b>sound</b> <b>rendering</b> in a Virtual Reality application concerning the virtual tour of the inside of the Baptistery of Pisa. It is well known that the impulse response of the Baptistery of Pisa is characterized by a 12 sec impulse response time, which raises serious computing difficulties for meeting real-time <b>sound</b> <b>rendering</b> constraints. In usual situations, like those of concert halls or historical theatres, the response time is from about 1 to 3 sec. The process of auralization of virtual reality environments, which is becoming an integral component in advanced applications of this kind, has, in recent times, attracted much research interest and is part of the wider domain of design of interior spaces with expected sound response behaviour. This article reports on the results of trial proofs performed with the Huron multi-DSP Sound Workstation, that is produced by the Australian LAKE Company, leader in the methodologies of the <b>sound</b> <b>rendering</b> at worldwide level. A preliminary investigation of the problem was developed in cooperation between the Norwegian University of Science and Technology of Trondheim and the National Research Council of Pisa, in Italy, in the framework of a stage at NTNU within the European Project "Mosart" (2003) ...|$|E
40|$|A {{comprehensive}} {{model of}} <b>sound</b> <b>rendering</b> is briefly summarized. Controversies surrounding each {{element of the}} model are described, all stemming from tradeoffs in quality vs. file size: sampling rates, compression methods, filtering and effects, dealing with artifacts of the process, and social/ethical issues arising from the tremendous flexibility of the digital medium...|$|E
40|$|Real-time <b>sound</b> <b>rendering</b> {{applications}} are memory-intensive and computation-intensive. To speed up computation {{and extend the}} simulated area, a real-time <b>sound</b> <b>rendering</b> system based on the hardware-oriented finite difference time domain algorithm (HO-FDTD) and time-sharing architecture is proposed and implemented by the {{field programmable gate array}} (FPGA) in this study. Compared with the traditional rendering system with parallel architecture, the proposed system extends by about 37 times in the simulated area because data are stored in the on-chip block memories instead of the D flip-flops. The hardware system becomes stable after 400 time steps in the impulse response. To render a three-minute Beethoven classical music clip, the hardware system carries it out in real-time while the software simulation takes about 63 min in a computer with 4 GB RAM and an AMD Phenom 9500 Quad-core processor running on 2. 2 GHz...|$|E
25|$|The {{weakness}} of the Confederate position was revealed at this time. Only four of the guns at Fort Bartow would bear on the Union gunboats. Forts Huger and Blanchard could not contribute at all. Fort Forrest, {{on the other side}} of the <b>sound,</b> was <b>rendered</b> completely useless when gunboat CSS Curlew, holed at the waterline, ran ashore directly in front in her effort to avoid sinking, and in so doing masked the guns of the fort.|$|R
5000|$|There is much {{speculation}} as to {{the nature}} of DiPT's aural distortion. At lower dosages, it has been reported to have an effect similar to a flanging, or a phase shift. At medium and higher dosages, the effect of DiPT is typically a radical shift downward in perceived pitch. This shift tends to be nonlinear, in that the shift downwards varies in relation to the initial pitch. This can produce bizarre <b>sounds</b> and <b>render</b> music disharmonious.|$|R
40|$|Abstract—The {{implementation}} of a prototype to establish a controllable sound field within a restricted area utilizing distributed loudspeakers is presented. Based on the near field beam-forming approach a demonstrator setup has been developed and implemented. The proposed solution should be a primary step towards providing a convincing alternative instead of wearing headphones which are inconvenient over long time periods for instance at a controller working position (CWP) in airport traffic control towers. Therefore this invention might improve the working process and therefore positively influence the safety conditions at all. The proposed setup consists of an off-the-shelf computer, a multi-channel audio converter, a flexible array of tiny loudspeakers and a tracking device. On the computer the <b>sound</b> field <b>rendering</b> process is realized with the programming language Pure Data (pd). Due to tracking data the <b>sound</b> field <b>rendering</b> process is able to trace {{the position of the}} listener in space and to calculate the required loudspeaker signals. Within this article the principle of function is briefly revisited and the required setup components as well as the implementation strategies are treaded. Index Terms — Acoustic beam focusing, sound field, focu...|$|R
