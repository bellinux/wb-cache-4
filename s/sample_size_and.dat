5501|10000|Public
25|$|As {{everything}} in inferential statistics {{it relies on}} <b>sample</b> <b>size,</b> <b>and</b> therefore under fat tails p-values may be seriously mis-computed.|$|E
25|$|Edgar Jones and Simon Wessely {{have argued}} that the small <b>sample</b> <b>size</b> <b>and</b> the single {{location}} studied limit the validity of the validation study.|$|E
25|$|Downconvert {{the audio}} to 2 channels, but keeping the {{original}} <b>sample</b> <b>size</b> <b>and</b> bit rate if the producer sets a flag on the DVD-A disc telling the player to do so.|$|E
40|$|Designing a {{tightened}} normal tightened {{sampling plan}} requires <b>sample</b> <b>sizes</b> <b>and</b> acceptance number with switching criterion. An evolutionary algorithm, the genetic algorithm, {{is designed to}} identify optimal <b>sample</b> <b>sizes</b> <b>and</b> acceptance number of a tightened normal tightened sampling plan for a specified consumer’s risk, producer’s risk, and switching criterion. Optimal <b>sample</b> <b>sizes</b> <b>and</b> acceptance number are obtained by implementing the genetic algorithm. Tables are reported for various choices of switching criterion, consumer’s quality level, and producer’s quality level...|$|R
40|$|Most current {{methods for}} product {{verification}} and reliability assurance {{are based upon}} statistical <b>sample</b> <b>sizes</b> <b>and</b> the underlying probability distributions. But that approach typically results in industrially unmanageable <b>sample</b> <b>sizes</b> <b>and</b> high test resource requirements. The proposed approach in this thesis uses a generalized "Inverse Most-Probable-Limit State" concept, which takes a Monte-Carlo based most likely noise factor scenario in all load dimensions into account for each failure mode...|$|R
40|$|This study {{investigated}} on {{power of the}} test for Alexander Govern method using adaptive trimmed mean as a central measurement (AH). The power of the test is controlled by three parametric specification namely significance level (α), <b>sample</b> <b>size</b> (n) <b>and</b> effect <b>size</b> (EF). Previous studies on Type I error rates found that this method is robust and perform very well even under extreme conditions. To check on the strength and weakness of the method with regards to power of test, variables such as {{the shape of the}} distribution of data, variance ratio, <b>sample</b> <b>sizes</b> <b>and</b> the pair between <b>sample</b> <b>sizes</b> <b>and</b> unequal variance were manipulated to create various conditions. Results from this study show that the power of the AH test can be considered high in all of the normal distribution. High power test is also consistent under non normal data for the case of unbalanced <b>sample</b> <b>sizes</b> <b>and</b> equal variance of positive pairing...|$|R
25|$|To compare {{competing}} {{statistics for}} small samples under realistic data conditions. Although type I error and power properties of statistics {{can be calculated}} for data drawn from classical theoretical distributions (e.g., normal curve, Cauchy distribution) for asymptotic conditions (i. e, infinite <b>sample</b> <b>size</b> <b>and</b> infinitesimally small treatment effect), real data often do not have such distributions.|$|E
25|$|In {{a screen}} with replicates, we can {{directly}} estimate variability for each compound; as a consequence, {{we should use}} SSMD or t-statistic that does not rely on the strong assumption that the z-score and z*-score rely on. One issue {{with the use of}} t-statistic and associated p-values is that they are affected by both <b>sample</b> <b>size</b> <b>and</b> effect size.|$|E
25|$|Another {{option is}} {{probability}} proportional to size ('PPS') sampling, {{in which the}} selection probability for each element is set to be proportional to its size measure, up {{to a maximum of}} 1. In a simple PPS design, these selection probabilities can then be used as the basis for Poisson sampling. However, this has the drawback of variable <b>sample</b> <b>size,</b> <b>and</b> different portions of the population may still be over- or under-represented due to chance variation in selections.|$|E
50|$|Another early paper {{provides}} graphs {{and tables}} for general values of ρ, for small <b>sample</b> <b>sizes,</b> <b>and</b> discusses computational approaches.|$|R
40|$|The {{purpose of}} this study is to {{identify}} and compare NIRT, PIRT and MIRT across different <b>sample</b> <b>sizes,</b> test length <b>and</b> correlation between dimensions in a two dimensional simple structures. Data sets in various conditions have been simulated. These conditions are <b>sample</b> <b>size</b> (100, 500, 1000 and 5000), test length (5, 15 and 25) and correlation between dimensions (0. 00, 0. 25 and 0. 50). From each experimental design, within the frame of Monte Carlo study, the findings have been obtained through 20 replications. For the item parameters and model data fit for the items, standard errors and significance values have been calculated. Having analyzed the findings of the research, with the increase of <b>sample</b> <b>sizes</b> <b>and</b> test length, it is also found out that the model data fit for the test has increased as well. It can be stated that tests consisting of less items fit better to MIRT models. In all simulation designs, model data fit for the items are calculated with quite low errors in NIRT. When the chi-square, infit and outfit values obtained for PIRT have been analyzed, it has been revealed that along with the increase of <b>sample</b> <b>sizes</b> <b>and</b> test length, all three coefficients exhibit better model fit. In NIRT, the standard errors belonging to Hi and p parameters tend to decrease with the increase of <b>sample</b> <b>sizes</b> <b>and</b> test length. In PIRT, a parameters tend to decrease when the <b>sample</b> <b>sizes</b> <b>and</b> test length increase...|$|R
30|$|As expected, from Table  1, it is {{observed}} that the performances of all frequentist coverage probabilities become better when the <b>sample</b> <b>size</b> increases <b>and</b> censored <b>sample</b> <b>size</b> decreases, <b>and</b> they are sensitive to the stress levels k.|$|R
25|$|Disagreement {{over the}} objectives, methodology, effect on stocks and overall {{success of the}} program {{continued}} in the scientific committee review of the feasibility study and full program. The full program introduced a change from previous use of the ICRW Article VIII research provision by not specifying an end date. The objectives were deemed unnecessary for stock management by some members and would not contribute significantly to previously identified research needs. The <b>sample</b> <b>size</b> <b>and</b> methods proposed were unlikely to satisfy program objectives and the ecosystem modeling {{was considered to be}} poorly developed.|$|E
25|$|Dolphins {{are often}} {{regarded}} as one of Earth's most intelligent animals, though it is hard to say just how intelligent. Comparing species' relative intelligence is complicated by differences in sensory apparatus, response modes, and nature of cognition. Furthermore, the difficulty and expense of experimental work with large aquatic animals has so far prevented some tests and limited <b>sample</b> <b>size</b> <b>and</b> rigor in others. Compared to many other species, however, dolphin behavior has been studied extensively, both in captivity and in the wild. See cetacean intelligence for more details.|$|E
25|$|All {{statistical}} hypothesis tests have a probability of making type I and type II errors. For example, all blood tests for a disease will falsely detect {{the disease in}} some proportion {{of people who do}}n't have it, and will fail to detect the disease in some proportion of people who do have it. A test's probability of making a type I error is denoted by α. A test's probability of making a type II error is denoted by β. These error rates are traded off against each other: for any given sample set, the effort to reduce one type of error generally results in increasing the other type of error. For a given test, the only way to reduce both error rates is to increase the <b>sample</b> <b>size,</b> <b>and</b> this may not be feasible.|$|E
3000|$|From Table 1, we {{see that}} the {{estimation}} procedure works very well. For different <b>sample</b> <b>sizes</b> <b>and</b> different parameters, the average mean squared errors of [...]...|$|R
50|$|Note: polls used {{different}} <b>sample</b> <b>sizes</b> <b>and</b> citizen groups. A candidate {{must have}} {{a majority of the}} vote (>50%) to avoid a runoff with their second place opponent.|$|R
40|$|Box-Cox {{transformation}} {{is one of}} the most commonly used methodologies when data do not follow normal distribution. However, its use is restricted since it usually requires the availability of covariates. In this paper, the use of a non-informative auxiliary variable is proposed for the implementation of Box-Cox transformation. Simulation studies are conducted to illustrate that the proposed approach is successful in attaining normality under different <b>sample</b> <b>sizes</b> <b>and</b> most of the distributions and in estimating transformation parameter for different <b>sample</b> <b>sizes</b> <b>and</b> mean-variance combinations. Methodology is illustrated on two real life data sets...|$|R
500|$|The {{cervical}} canal is {{a pathway}} through which sperm enter the uterus after sexual intercourse, and {{some forms of}} artificial insemination. Some sperm remains in cervical crypts, infoldings of the endocervix, which act as a reservoir, releasing sperm over several hours and maximising the chances of fertilisation. A theory states the cervical and uterine contractions during orgasm draw semen into the uterus. Although the [...] "upsuck theory" [...] has been generally accepted for some years, it has been disputed {{due to lack of}} evidence, small <b>sample</b> <b>size,</b> <b>and</b> methodological errors.|$|E
500|$|MacDougall's {{experiment}} {{has been}} the subject of considerable skepticism, and he has been accused of both flawed methods and outright fraud in obtaining his results. Noting that only one of the six patients measured supported the hypothesis, Karl Kruszelnicki has stated the experiment is a case of selective reporting, as MacDougall ignored the majority of the results. Kruszelnicki also criticized the small <b>sample</b> <b>size,</b> <b>and</b> questioned how MacDougall was able to determine the exact moment when a person had died considering the technology available in 1907. In 2008 physicist Robert L. Park wrote that MacDougall's experiments [...] "are not regarded today as having any scientific merit", and psychologist Bruce Hood wrote that [...] "because the weight loss was not reliable or replicable, his findings were unscientific". Professor Richard Wiseman said that within the scientific community, the experiment is confined to a [...] "large pile of scientific curiosities labelled 'almost certainly not true'".|$|E
2500|$|Specify {{arbitrary}} population, <b>sample</b> <b>size,</b> <b>and</b> sample statistic.|$|E
50|$|Cohen's d is {{frequently}} used in estimating <b>sample</b> <b>sizes</b> for statistical testing. A lower Cohen's d indicates {{the necessity of}} larger <b>sample</b> <b>sizes,</b> <b>and</b> vice versa, as can subsequently be determined together with the additional parameters of desired significance level and statistical power.|$|R
40|$|Abstract—In {{inspection}} and workpiece localization, sampling point data {{is an important}} issue. Since the devices for sampling only sample discrete points, not the completely surface, <b>sampling</b> <b>size</b> <b>and</b> location of the points will be taken into consideration. In this paper a method is presented for determining the <b>sampled</b> points <b>size</b> <b>and</b> location for achieving efficient sampling. Firstly, uncertainty analysis of the localization parameters is investigated. A localization uncertainty model is developed to predict {{the uncertainty of the}} localization process. Using this model the minimum <b>size</b> of the <b>sampled</b> points is predicted. Secondly, based on the algebra theory an eigenvalue-optimal optimization is proposed. Then a freeform surface is used in the simulation. The proposed optimization is implemented. The simulation result shows its effectivity. Keywords—eigenvalue-optimal optimization, freeform surface inspection, <b>sampling</b> <b>size</b> <b>and</b> location, <b>sampled</b> points. I...|$|R
3000|$|How {{can search}} {{algorithms}} be improved to incorporate {{different kinds of}} background knowledge, search over different classes of causal models, run faster, handle more variables <b>and</b> larger <b>sample</b> <b>sizes,</b> be more reliable at small <b>sample</b> <b>sizes,</b> <b>and</b> produced output that is as informative as possible? [...]...|$|R
2500|$|... where N is the <b>sample</b> <b>size</b> <b>and</b> K is {{the number}} of categories.|$|E
2500|$|... where N is {{the total}} <b>sample</b> <b>size</b> <b>and</b> n'i is the number in the ith category.|$|E
2500|$|... where n is the <b>sample</b> <b>size</b> <b>and</b> c(x,y) = 1 if x and y {{are alike}} and 0 otherwise.|$|E
50|$|Krippendorff's alpha is {{more general}} {{than any of}} these special purpose coefficients. It adjusts to varying <b>sample</b> <b>sizes</b> <b>and</b> affords {{comparisons}} across {{a wide variety of}} reliability data, mostly ignored by the familiar measures.|$|R
5000|$|<b>Sample</b> <b>size</b> {{calculations}} <b>and</b> {{data analysis}} are very straightforward.|$|R
5000|$|The Sound Manager {{moved to}} a {{predominantly}} component-based architecture in version 3.0: sound output devices were represented as components, and there were also component types for mixing multiple channels, converting between different <b>sample</b> rates <b>and</b> <b>sample</b> <b>sizes,</b> <b>and</b> encoding and decoding compressed formats.|$|R
2500|$|Smith-Gill {{developed}} a statistic based on Morisita’s index which {{is independent of}} both <b>sample</b> <b>size</b> <b>and</b> population density and bounded by −1 and +1. This statistic is calculated as follows ...|$|E
2500|$|The {{efficiency}} of the sample median, measured as {{the ratio of the}} variance of the mean to the variance of the median, depends on the <b>sample</b> <b>size</b> <b>and</b> on the underlying population distribution. For a sample of size [...] from the normal distribution, the efficiency for large N is ...|$|E
2500|$|First, a data set's {{average is}} determined. Next the {{absolute}} deviation between each data {{point and the}} average are determined. Thirdly, a rejection region is determined using the formula: where [...] is the critical value from the Student t distribution, n is the <b>sample</b> <b>size,</b> <b>and</b> s is the sample standard deviation.|$|E
50|$|Microchip based {{electrophoresis}} is {{a promising}} alternative to capillary electrophoresis {{since it has}} the potential to provide rapid protein analysis, straightforward integration with other microfluidic unit operations, whole channel detection, nitrocellulose films, smaller <b>sample</b> <b>sizes</b> <b>and</b> lower fabrication costs.|$|R
40|$|This article {{introduces}} the general concepts <b>and</b> methods of <b>sample</b> <b>size</b> estimation <b>and</b> testing power analysis. It focuses on parametric methods of <b>sample</b> <b>size</b> estimation, including <b>sample</b> <b>size</b> estimation of estimating the population mean {{and the population}} probability. It also provides estimation formulas and introduces how to realize <b>sample</b> <b>size</b> estimation manually <b>and</b> by SAS software...|$|R
25|$|Interventions {{to prevent}} events that occur only {{infrequently}} (e.g., {{sudden infant death}} syndrome) and uncommon adverse outcomes (e.g., a rare side effect of a drug) would require RCTs with extremely large <b>sample</b> <b>sizes</b> <b>and</b> may therefore best be assessed by observational studies.|$|R
