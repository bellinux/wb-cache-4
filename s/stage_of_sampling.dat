40|10000|Public
50|$|The project {{employed}} a stratified three-stage probability proportion to size (PPS) random sample design. The first <b>stage</b> <b>of</b> <b>sampling</b> uses counties; the second <b>stage</b> <b>of</b> <b>sampling</b> uses residential communities from the counties; the final <b>stage</b> <b>of</b> <b>sampling</b> uses extracted households from the neighborhoods and village committees. Each <b>stage</b> <b>of</b> <b>sampling</b> was implemented using the PPS sampling method, weighted by its population size. Interviews {{were carried out}} by highly trained students from Southwestern University of Finance and Economics (SWUFE). Each student was required to complete 56 hours of training and pass an exam before being allowed to participate. Households drawn in the sample had to refuse to be interviewed three times or be unavailable six times before being excluded from the survey. Local community representatives were extremely supportive of the project.|$|E
50|$|The main {{objective}} of OCBA is {{to maximize the}} probability of correct selection (PCS). PCS {{is subject to the}} sampling budget of a given <b>stage</b> <b>of</b> <b>sampling</b> τ.|$|E
40|$|When {{multilevel}} {{models are}} estimated from survey data derived using multistage sampling, unequal selection probabilities at any <b>stage</b> <b>of</b> <b>sampling</b> may induce bias in standard estimators, unless {{the sources of}} the unequal probabilities are fully controlled for in the covariates. This paper proposes alternative ways of weighting the estimation of a two-level model by using the reciprocals of the selection probabilities at each <b>stage</b> <b>of</b> <b>sampling.</b> Consistent estimators are obtained when both the sample number of level 2 units and the sample number of level 1 units within sampled level 2 units increase. Scaling of the weights is proposed to improve the properties of the estimators and to simplify computation. Variance estimators are also proposed. In a limited simulation study the scaled weighted estimators are found to perform well, although non-negligible bias starts to arise for informative designs when the sample number of level 1 units becomes small. The variance estimators perform extremely well. The procedures are illustrated using data from the survey of psychiatric morbidity...|$|E
50|$|First, a {{thin section}} <b>of</b> the <b>sample</b> is cut, often using a microtome. Various other <b>stages</b> <b>of</b> <b>sample</b> {{preparation}} may then take place.|$|R
40|$|Although not as {{efficient}} as simple random sampling, cluster sampling has been regarded as a valid sampling technique when the researcher is attempting to save cost. In order to do so, {{it is necessary that}} random selection occurs in all <b>stages</b> <b>of</b> <b>sampling.</b> This simulation study examines purposeful selection <b>of</b> cluster <b>sampling</b> in the second <b>stage</b> <b>of</b> a two <b>stage</b> cluster design...|$|R
40|$|Summary: Quality Control is a {{fundamental}} aspect of successful microarray data analysis. Simpleaffy is a BioConductor package that provides access {{to a variety of}} QC metrics for assessing the quality <b>of</b> RNA <b>samples</b> and <b>of</b> the intermediate <b>stages</b> <b>of</b> <b>sample</b> preparation and hybridization. Simpleaffy also offers fast implementations of popular algorithms for generating expression summaries and detection calls. Availability: Simpleaffy can be downloaded fro...|$|R
40|$|A {{multistage}} sampling technique, with probability {{proportional to}} size, for forest volume inventory using remote sensing data is developed and evaluated. The study area {{is located in}} the Southeastern Brazil. The LANDSAT 4 digital data of the study area are used in the first stage for automatic classification of reforested areas. Four classes of pine and eucalypt with different tree volumes are classified utilizing a maximum likelihood classification algorithm. Color infrared aerial photographs are utilized in the second <b>stage</b> <b>of</b> <b>sampling.</b> In the third state (ground level) the time volume of each class is determined. The total time volume of each class is expanded through a statistical procedure taking into account all the three stages of sampling. This procedure results in an accurate time volume estimate with a smaller number of aerial photographs and reduced time in field work...|$|E
40|$|In this paper, {{we address}} the problem of finding the {{simulated}} system with the best (maximum or minimum) expected performance when the number of alternatives is finite, but large enough that ranking-and-selection (R&S) procedures may require too much computation to be practical. Our approach is to use the data provided by the first <b>stage</b> <b>of</b> <b>sampling</b> in an R&S procedure to screen out alternatives that are not competitive, and thereby avoid the (typically much larger) second-stage sample for these systems. Our procedures represent a compromise between standard R&S procedures—which are easy to implement, but can be computationally inefficient—and fully sequential procedures—which can be statistically efficient, but are more difficult to implement and depend on more restrictive assumptions. We present a general theory for constructing combined screening and indifference-zone selection procedures, several specific procedures and a portion of an extensive empirical evaluation. 1...|$|E
40|$|There the {{purposes}} are to study by chromatography the thiol ethers, {{to develop the}} determination manner of thiols as the thiol ethers, to provide by metrology for the particular analysis procedures of natural and industrial objects including the <b>stage</b> <b>of</b> <b>sampling.</b> The series of thiol ethers have been studied, the indeces of confinement and the thermodynamical indicators of sorption have been obtained, the structural-sorption correlations have been ascertained, the mass spectra and the responses of detectors have been studied. The determination manner of thiols as the thiol ethers has been developed for the first time; the industrial standard samples to graduate the meter have been developed; the "Measurement Procedure of Content of Methane-, Ethane-, Propane-Thiols in Free Air" has been development and has been certifiedAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|E
40|$|Response surface {{functions}} {{are often used}} as simple and inexpensive replacements for computationally expensive computer models that simulate {{the behavior of a}} complex system over some parameter space. "Progressive" response surfaces are built up incrementally as global information is added from new sample points added to the previous points in the parameter space. As the response surfaces are globally upgraded, indications of the convergence of the response surface approximation to the exact (fitted) function can be inferred. Sampling points can be incrementally added in a structured or unstructured fashion. Whatever the approach, it is usually desirable to sample the entire parameter space uniformly (at least in early <b>stages</b> <b>of</b> <b>sampling).</b> At later <b>stages</b> <b>of</b> <b>sampling,</b> depending {{on the nature of the}} quantity being resolved it may be desirable to continue sampling uniformly (progressive response surfaces), or to switch to a focusing/economizing strategy <b>of</b> preferentially <b>sampling</b> certain regions <b>of</b> the parameter space based on information gained in previous <b>stages</b> <b>of</b> <b>sampling</b> ("adaptive" response surfaces). Here we consider progressive response surfaces where a balanced representation of global response over the parameter space is desired. We use Kriging and Moving-Least-Squares methods to fit Halton quasi-Monte-Carlo data samples and interpolate over the parameter space. On 2 -D test problems we use the response surfaces to compute various response measures and assess the accuracy/applicability of heuristic error estimates based on convergence behavior of the computed response quantities. Where applicable we apply Richardson Extrapolation for estimates of error and asymptotic convergence, and assess the accuracy of these estimates. We seek to develop a robust methodolog [...] ...|$|R
40|$|Response surface {{functions}} {{are often used}} as simple and inexpensive replacements for computationally expensive computer models that simulate {{the behavior of a}} complex system over some parameter space. Progressive response surfaces are ones that are built up progressively as global information is added from new sample points in the parameter space. As the response surfaces are globally upgraded based on new information, heuristic indications of the convergence of the response surface approximation to the exact (fitted) function can be inferred. Sampling points can be incrementally added in a structured fashion, or in an unstructured fashion. Whatever the approach, at least in early <b>stages</b> <b>of</b> <b>sampling</b> it is usually desirable to sample the entire parameter space uniformly. At later <b>stages</b> <b>of</b> <b>sampling,</b> depending {{on the nature of the}} quantity being resolved, it may be desirable to continue sampling uniformly over the entire parameter space (Progressive response surfaces), or to switch to a focusing/economizing strategy <b>of</b> preferentially <b>sampling</b> certain regions <b>of</b> the parameter space based on information gained in early <b>stages</b> <b>of</b> <b>sampling</b> (Adaptive response surfaces). Here we consider Progressive response surfaces where a balanced indication of global response over the parameter space is desired. We use a variant of Moving Least Squares to fit and interpolate structured and unstructured point sets over the parameter space. On a 2 -D test problem we compare response surface accuracy for three incremental sampling methods: Progressive Lattice Sampling; Simple-Random Monte Carlo; and Halton Quasi-Monte-Carlo sequences. We are ultimately after a system for constructing efficiently upgradable response surface approximations with reliable error estimates...|$|R
40|$|Aims: Despite {{its long}} history, the acid fast smear remains unstandardised. Technical {{variations}} {{in both the}} preparation of clinical material and subsequent staining mean that smear sensitivity relative to culture may vary from 50 % to over 80 %. This study assessed the sensitivity of acid fast microscopy at each <b>of</b> five <b>stages</b> <b>of</b> <b>sample</b> preparation and by both commonly used staining methods...|$|R
40|$|In {{this paper}} we {{address the problem of}} finding the {{simulated}} system with the best (maximum or minimum) expected performance when the number of alternatives is finite, but large enough that ranking-and-selection (R&S) procedures may require too much computation to be practical. Our approach is to use the data provided by the first <b>stage</b> <b>of</b> <b>sampling</b> in a R&S procedure to screen out alternatives that are not competitive and thereby avoid the (typically much larger) second-stage sample for these systems. Our procedures represent a compromise between standard R&S procedures [...] -that are easy to implement, but can be computationally inefficient [...] -and fully sequential procedures [...] -that can be statistically efficient, but are more difficult to implement and depend on more restrictive assumptions. We present a general theory for constructing combined screening and indifference-zone selection procedures, several specific procedures and a portion of an extensive empirical evaluation. 1 Introductio [...] ...|$|E
40|$|Statistical {{selection}} procedures {{can identify}} {{the best of a}} finite set of alternatives, where “best ” is defined in terms of the unknown expected value of each alternative’s simulation output. One effective Bayesian approach allocates samples sequentially to maximize an approximation to the expected value of information (EVI) from those samples. That existing approach uses both asymptotic and probabilistic approximations. This paper presents new EVI sampling allocations that avoid most of those approximations, but that entail sequential myopic sampling from a single alternative per <b>stage</b> <b>of</b> <b>sampling.</b> We compare the new and old approaches empirically. In some scenarios (a small, fixed total number of samples, few systems to be compared), the new greedy myopic procedures are better than the original asymptotic variants. In other scenarios (with adaptive stopping rules, medium or large number of systems, high required probability of correct selection), the original asymptotic allocations perform better. ...|$|E
30|$|This study {{employed}} a multiple stage sampling design which involved districts, pastoral/agropastoral associations and farm households. Five {{out of the}} seven semi-arid lowland administrative districts of the Borana Plateau were selected for the study in a first <b>stage</b> <b>of</b> <b>sampling</b> (Arero, Dire, Miyo, Moyale, Yabelo). Experts were consulted to ensure that these five districts gave adequate representation of the different agroclimates and farming systems in the region. In {{the second stage of}} selection, four associations (two pastoral and two agropastoral) were randomly selected from each of the five districts. At the third stage, 24 households from each of the twenty pastoral/agropastoral associations were then randomly selected to give a total of 480 sample households. The sample size selected was sufficient to allow at least a 95  % confidence level with 5  % precision or margin of error for the parameter estimate in order to draw conclusions from the data analysis (Cochran 1977).|$|E
40|$|Statisticians {{should be}} {{involved}} at all <b>stages</b> <b>of</b> <b>sample</b> surveys and courses on surveys need to reflect this by covering both theoretical and practical aspects. Teaching methods could include some hands-on experience, directed reading, and use of software designed for teaching or professional use, {{as well as more}} traditional lecturing. Suggestions are given for a course of about fifty hours...|$|R
30|$|To {{implement}} our empirical strategy, we {{have focused}} on some highly selective samples. Self-selection exists at various <b>stages</b> <b>of</b> <b>sample</b> construction. For example, some people chose to migrate to urban areas and others did not; some rural–urban migrants changed jobs and others did not. These issues are conveniently ignored. We hope that future work, with higher-quality data and better-designed empirical analyses, will help overcome these and other limitations of this study.|$|R
40|$|The paper {{studies the}} {{applicability}} of the acoustoelasticity method to estimation of the strain-stress state under cyclic loading. It has been found that the uniform distributions of acoustic anisotropy along the working part <b>of</b> the <b>sample</b> and <b>of</b> the velocities of longitudinal and transverse ultrasonic waves become substantially non-uniform with {{an increase in the number}} of stress cycles. Moreover, the largest absolute values of acoustic anisotropy fell on the points with the largest plastic deformations, in particular, in the area <b>of</b> <b>sample</b> dispersion. The effect was being recorded from the early <b>stages</b> <b>of</b> <b>sample</b> loading until the fracture <b>of</b> the <b>sample...</b>|$|R
40|$|This paper derives optimal {{rules for}} {{sequential}} mastery tests. In a sequential mastery test, {{the decision is}} to classify a subject as a master or a nonmaster or to continue sampling and administering another random item. The framework of minimax sequential decision theory (minimum information approach) is used; that is, optimal rules are obtained by minimizing the maximum expected losses associated with all possible decision rules at each <b>stage</b> <b>of</b> <b>sampling.</b> The main advantage {{of this approach is}} that costs of sampling can be explicitly taken into account. The binomial model is assumed for the probability of a correct response given the true level of functioning, and threshold loss is adopted for the loss function involved. Monotonicity conditions are derived, conditions sufficient for optimal rules to be in the form of sequential cutting scores. The paper concludes with the description of a simulation study in which the minimax sequential strategy is compared with other procedures that exist for similar classification decision problems in the literature...|$|E
40|$|Statistical {{selection}} {{procedures are}} used to select {{the best of a}} finite set of alternatives, where “best” is defined in terms of each alternative's unknown expected value, and the expected values are inferred through statistical sampling. One effective approach, which is based on a Bayesian probability model for the unknown mean performance of each alternative, allocates samples based on maximizing an approximation to the expected value of information (EVI) from those samples. The approximations include asymptotic and probabilistic approximations. This paper derives sampling allocations that avoid most of those approximations to the EVI but entails sequential myopic sampling from a single alternative per <b>stage</b> <b>of</b> <b>sampling.</b> We demonstrate empirically that the benefits of reducing the number of approximations in the previous algorithms are typically outweighed by the deleterious effects of a sequential one-step myopic allocation when more than a few dozen samples are allocated. Theory clarifies the derivation of selection procedures that are based on the EVI...|$|E
40|$|The {{purpose of}} this paper is to derive optimal rules for {{sequential}} mastery tests. In a sequential mastery test, the decision is to classify a subject as a master or a nonmaster, or to continue sampling and administering another random test item. The framework of minimax sequential decision theory (minimum information approach) is used; that is, optimal rules are obtained by minimizing the maximum expected losses associated with all possible decision rules at each <b>stage</b> <b>of</b> <b>sampling.</b> The binomial model is assumed for the probability of a correct response given the true level of functioning, whereas threshold loss is adopted for the loss function involved. Monotonicity conditions are derived, that is, conditions sufficient for optimal rules to be in the form of sequential cutting scores. The paper concludes with a simulation study in which the minimax sequential strategy is compared with other procedures that exist for similar classification decisions in the literature. (Contains 2 tables and 30 references.) (Author/SLD) Reproductions supplied by EDRS are the best that can be made from the original document...|$|E
40|$|A {{practical}} {{problem of}} estimating the total {{area under cultivation}} in In-dian districts is addressed by two-stage sampling with unequal selection-probabilities. To assess the accuracy in estimation bootstrap technique is employed in constructing confidence intervals and simulation-based perfor-mance criteria are evaluated from live-data as are shown for competitive pro-cedures. Rao-Hartley-Cochran’s (RHC, 1962) scheme is employed in both <b>stages</b> <b>of</b> <b>sampling.</b> Sitter’s mirrormatch bootstrap procedure is employed suitably modifying it to cover the two-stages. AMS (2000) subject classification. 62 D 05...|$|R
40|$|This study {{aimed to}} {{determine}} the concentration of HMF ((hydroxy methyl furfural) in Bone honey. Regency Bone with 145. 073 ha of forest, of area 4, 559 km² honey potential for livestock development. The quality of honey is determined  from the nectar source, geographic location, and the processing technology. HMF concentration {{is one of the}} indicators in honey {{to determine the}} quality marker and freshness of honey. This research was done in three stages, namely the <b>stage</b> <b>of</b> <b>sample</b> preparation, extraction phase, and phase identification. <b>Stages</b> <b>of</b> <b>sample</b> preparation were performed to determine sampling locations, the determination of the location of the extraction and identification. Extraction is done in analytical chemistry laboratorium Science Faculty, Hasanuddin University. Honey samples were extracted with methanol. The identification process carried out in an integrated laboratory Department of Chemical Science Faculty of Hasanuddin University to obtain the data of UV-Vis, HPLC performed while the data in an integrated laboratory public health, Hasanuddin University. The results obtained by HPLC showed the data HMF concentration of 0. 264 mg/kg, while the data obtained by UV-Vis HMF concentration of 0. 230 mg/kg. Based on these data concluded that honey Bone has a good quality in terms of content HMF...|$|R
25|$|In this approach, {{the sample}} is spiked with a species (internal standard) {{which is used to}} {{normalise}} the response of analyte, compensating for variables at any <b>stage</b> <b>of</b> the <b>sample</b> preparation and analysis, including ion suppression.|$|R
40|$|Indoor Air Quality (IAQ) {{is a part}} of Building Environment. Nowadays, the {{construction}} of new building took place over the world. Upon new building occupancy, a lot of indoors material was used without IAQ concern. This study has been conducted in a new constructed building of National Institute of Occupational Safety & Health (NIOSH) Malaysia. The goal of the study is to monitor on the level of IAQ parameters including chemical and physical parameters within three consequent stages which are before furniture install, after furniture install and during one month occupancy. This study was divided the sampling area into two main facilities which are training room and office room. The contaminants has been measured consist of nine parameters such as Carbon Dioxide (CO 2), Carbon Monoxide (CO), Total Volatile Organic Compounds (TVOC), Formaldehyde, Respirable Particulates (PM 10), Ozone, Relative Humidity (RH), Temperature and Air Movement. The result of the Temperature and Formaldehyde show increasing trends in the first and second stages but were reduced significantly the third <b>stage</b> <b>of</b> <b>sampling.</b> These finding indicates that furniture and fittings installed might be a potential sources of indoor air contaminants. Th...|$|E
40|$|ABSTRACT. Tree taper {{models are}} {{required}} for estimating the dimensions of products from trees (e. g., log under-bark diameters and volumes). Typically, taper models are parameterised and validated using underbark diameters taken along the stems {{of a large number}} of trees destructively sampled from the population of interest (application population). Such data collection is costly and time-consuming. Sometimes a limited data set from the application population may exist for estimating a preliminary taper model, or a taper model developed for a similar population may exist. However, the prediction accuracy of these models is unclear. This paper suggests a sequential accuracy testing approach to validate tree taper models under these specific situations. The prediction accuracy of the preliminary or existing taper model is tested using new stem diameter data from trees sampled progressively from the application population. At each <b>stage</b> <b>of</b> <b>sampling,</b> the prediction accuracy, measured by the observed cumulative squared errors, is evaluated against requirements specified by the users. Two parametric sequential accuracy tests are suggested for making decisions on the applicability of the tested models. Data collection stops once the decision to accept the tested model is reached. Otherwise, dat...|$|E
40|$|Solids removal {{often is}} the first {{parameter}} evaluated when testing the effectiveness of stormwater treatment devices. Therefore, {{it is imperative that}} the sources and magnitude of potential errors associated with each <b>stage</b> <b>of</b> <b>sampling</b> and measurement be understood and quantified. The following research examined the errors associated with the analysis of solids from collection via an automatic sampler through sample splitting with churn and cone splitters to, finally, solids concentration analysis by three common US analytical methodologies (Suspended Solids Concentration [SSC, ASTM D 3977 - 97 B] and Total Suspended Solids [TSS, by both US EPA Method 160. 2, identical to ISO 11923, and Standard Methods 2540 D]). Material gradient impacted the reliability of the autosampler. Sample splitting distributed representative and consistent subaliquots. SSC methodology best represented the known solids concentration in water for a representatively wide range of typical particle sizes. This research documented the magnitude of potential errors associated with each stage in solids measurement in stormwater. The question remains as to what effect these errors will have when combined into a single sampling and analysis activity. Large and variable errors may mask bias differences in the methods; also, error analysis and comprehension is {{a vital part of the}} stormwater control evaluation process...|$|E
30|$|In this {{descriptive}} study, a multistage {{sampling method}} {{was used in}} selecting 471 teachers from 52 private and 86 government secondary schools in the Ibadan North Local Government Area, Oyo State, Nigeria. These participants (n[*]=[*] 471) were selected through a multistage sampling technique from 138 secondary schools in the Ibadan North Local Government Area. In the final <b>stage</b> <b>of</b> <b>sample</b> selection, the number of teachers that were interviewed was determined by a proportional sampling method. The first teacher was randomly selected, and others were consecutively interviewed until the sample size in that school was met.|$|R
40|$|Traditional "crossed-grati,qg" moire, {{as well as}} newer "sampled-grating" (scanning) moire, {{have proven}} to be {{effective}} methods of shape measurement. There is speculation that the moire patterns <b>of</b> a <b>sampled</b> grating, which are duc to aliasing, can be modeled with crossed gratings. Wc compare the two and show that while crossed gratings can correctly predict the frequencies <b>of</b> a <b>sampled</b> grating, they cannot corrcctly predict the amplitudes. We go on to formulate a new model which accounts for multiple <b>stages</b> <b>of</b> <b>sampling</b> and transmission, and show how neglecting multiple stages can lead to mistakes in moire analysis. Wc demonstrate our model with an experiment using a digital imaging system...|$|R
40|$|The {{purpose of}} this study was to study the factors that {{influenced}} the behaviour of prenatal care. Its conducted to 180 <b>samples</b> <b>of</b> pregnan­ cies. There were 3 <b>stages</b> <b>of</b> <b>sampling</b> regent (kabupaten), district (kecamatan) /Health Centre and village. All pregnancies in selected villages were samples (cluster). The data collection was conducted using interview guide and analized using multiple linier regression. The results were: that variable with strongest influence to the prenatal care behaviour was level of pregnancy care knowledge. Other variables were accesibility to health fasilities and health education par­ticipation.   Keyword: Ante natal care<br /...|$|R
40|$|Packaging {{is gaining}} {{more and more}} {{importance}} for the marketing of products. Already in the early stage of development marketing-relevant considerations are taken into account, which decide over target groups and marketing strategies. For design analysis and the essential sampling of the packaging hitherto manufacturing possibilities were of insufficient quality (manual manufacturing, sample plotter) or verry cost-intensive (punching units). Both manufacturing options are no satisfying alternative from the user's point of view. At the <b>stage</b> <b>of</b> <b>sampling</b> and prototype production, {{as well as the}} individual production of small series, flexible systems are needed, which are capable of quick, cheap and most of all high quality manufacturing of even small series. In the context of this project concepts for manufacturing solutions with the water jet, which consider these requirements, are developed. Therefore, in a first step existing systems and techniques are evaluated. Based on the results, industry-specific hard and sofware concepts for the overall system are provided. This includes the concatenation of the system ant the technical integration of further necessary manufacturing processes (grooving, cutting and milling). With such a system new markets for small series production and sample production of cardboard packaging can be opened for the usually small and middle sized companies of equipment manufacturers. The users of the technology thus could increase their geometrical and logistic flexibility and improve the economy of small series especially...|$|E
40|$|Measurement, collection, and {{interpretation}} of network usage data commonly involves multiple <b>stage</b> <b>of</b> <b>sampling</b> and aggregation. Examples include sampling packets, aggregating them into flow statistics at a router, sampling and aggregation of usage records in a network data repository for reporting, query and archiving. Although unbiased estimates of packet, bytes and flows usage can be formed for each sampling operation, for many applications {{it is crucial to}} know the inherent estimation error. Previous work in this area has been limited mainly to analyzing the estimator variance for particular methods, e. g., independent packet sampling. However, the variance is of limited use for more general sampling methods, where the estimate may not be well approximated by a Gaussian distribution. This motivates our paper, in which we establish Chernoff bounds on the likelihood of estimation error in a general multistage combination of measurement sampling and aggregation. We derive the scale against which errors are measured, in terms of the constituent sampling and aggregation operations. In particular this enables us to obtain rigorous confidence intervals around any given estimate. We apply our method to a number of sampling schemes both in the literature and currently deployed, including sampling of packet sampled NetFlow records, Sample and Hold, and Flow Slicing. We obtain one particularly striking result in the first case: that for a range of parameterizations, packet sampling has no additional impact on the estimator confidence derived from our bound, beyond that already imposed by flow sampling...|$|E
40|$|Aim of {{this study}} is {{introduction}} one Approach for Monitoring a Two-Stage Process by Profile Quality Characteristic in the Second Stage. Nowadays, many processes are multistage and such processes often depend on each other. The implication is; the specification features for the product that are used to monitor the quality of that product and are usually assessed in one stage of the process, not only take form on the same stage but also take shape in the different phases of the process. This topic in statistical quality control is known as cascade property in multistage processes. In such a case, care must be taken that the lack of attention to this detail will cause an error in the analysis of control charts. Thus, in reviewing the literature, some methods are presented to reduce the error. In many situations, the quality of process or product is described by using the relationship between a response variable and an independent variable. Thus at each <b>stage</b> <b>of</b> <b>sampling,</b> a set of data is collected which can be shown by using a profile. Our goal in this study is to assess the cascade property for evaluating linear profiles that are in various stages of the processes. We have named this project as profile monitoring and evaluation of multistage processes. Hence, in this study, results have been studied by simulation of the average run length in Phase II...|$|E
50|$|The World Values Survey {{uses the}} sample survey as its mode of data collection, a {{systematic}} and standardized approach {{to collect information}} through interviewing representative national <b>samples</b> <b>of</b> individuals. The basic <b>stages</b> <b>of</b> a <b>sample</b> survey are Questionnaire design; Sampling; Data collection and Analysis.|$|R
40|$|We {{have devised}} {{a program that}} allows {{computation}} {{of the power of}} F-test, and hence determination <b>of</b> appropriate <b>sample</b> and subsample sizes, {{in the context of the}} one-way hierarchical analysis of variance with fixed effects. The power at a fixed alternative is an increasing function <b>of</b> the <b>sample</b> size and <b>of</b> the subsample size. The program makes it easy to obtain the power of F-test for a range <b>of</b> values <b>of</b> <b>sample</b> and subsample sizes, and therefore the appropriate sizes based on a desired power. The program can be used for the 'ordinary' case of the one-way analysis of variance, as well as for hierarchical analysis of variance with two <b>stages</b> <b>of</b> <b>sampling.</b> Examples are given of the practical use of the program...|$|R
40|$|Because {{of their}} high radioactivity, {{irradiated}} fuels are commonly examined in a hot cell. However, the Idaho National Laboratory (INL) has recently investigated irradiated U-Mo-Al metallic fuel from the Reduced Enrichment for Research and Test Reactors (RERTR) project using a conventional unshielded scanning electron microscope outside a hot cell. This examination was possible because of a two-step sample-preparation approach in which a small volume of fuel was isolated in a hot cell and shielding was introduced during later <b>stages</b> <b>of</b> <b>sample</b> preparation. The resulting sample contained numerous sample-preparation artifacts but allowed analysis of microstructures from selected areas...|$|R
