26|0|Public
5000|$|... 1998: Les affinités <b>suffisantes,</b> Musée Ianchelevici, La Louvière (19 September - 25 October).|$|E
50|$|French records {{state that}} <b>Suffisantes</b> actual captors were the 74-gun third-rate , the frigate , and the lugger Speedy. The French reports further state that Suffisante {{exchanged}} fire with the 20-gun lugger Speedy, but the arrival {{on the scene}} of Mars and Venus rendered further resistance futile.|$|E
50|$|Suffisante, , , and cutter Swift (2) {{shared in}} the {{recapture}} on 13 December 1800 of the Defiance. Defiance had been sailing from Penzance to London when she was captured. Another prize money notice gave the name of <b>Suffisantes</b> captain at the recapture as Jonas Rose. If not a reporting error, Rose may have been temporary while Wittman was on leave.|$|E
50|$|French {{records show}} that <b>Suffisantes</b> captain was {{lieutenant}} de vaisseau Nosten, and state that her actual captors were the 74-gun third-rate , the frigate , and the lugger Speedy. Apparently, Suffisante exchanged fire with the 20-gun lugger Speedy, but the arrival on the scene of Mars and Venus rendered further resistance futile. The Royal Navy took her into service as HMS Suffisante.|$|E
40|$|X_ 0 = x, μ_ 0 =μ. Nous établissons une {{relation}} entre le comportement asymptotique de μ_t et le comportement asymptotique d'un système dynamique déterministe (défini sur l'espace des probabilités). Nous étendons alors de précédents résultats à R^d. Nous donnons également des conditions <b>suffisantes</b> pour la convergence de μ_t. Enfin, nous illustrons, au chaptre 5, l'étude précédente de diffusions auto-interactives par quelques exemples en dimension deux...|$|E
40|$|International audienceWe provide {{essentially}} optimal, effective {{conditions to}} ensure that, when available, the Halberstam–Richert upper {{bound for the}} mean value of a non-negative multiplicative function actually furnishes the true order of magnitude. This is applied, in particular, to short sums of multiplicative functions over arithmetic progressions, to exponential sums with multiplicative coefficients, and to strong law of large numbers with multiplicative weights. Nous donnons des conditions <b>suffisantes</b> effectives quasi-optimales pour que la majoration de Halberstam–Richert fournisse effectivement l'ordre de grandeur de la valeur moyenne d'une fonction multiplicative positive ou nulle...|$|E
40|$|Résumé. Nous établissons des {{conditions}} <b>suffisantes</b> pour l’analyticité d’une application CR lisse entre deux variétés analytiques réelles. Analyticity of CR maps Abstract. We establish sufficient {{conditions for}} the analyticity of a smooth CR mapping between two real analytic manifolds. Abriged English Version In this note we establish two results giving sufficient {{conditions for the}} analyticity of a smooth CR map between two real analytic manifolds in complex affine spaces of different dimensions. Let X ⊂ IC n be a real analytic hypersurface in a neighborhood of a point p ∈ X, defined by a real analytic function ρ such that ∂ρ (p) ̸ = 0. The Cauchy- Riemann operators on X are define...|$|E
40|$|We give {{necessary}} and sufficient conditions, {{in terms of}} characteristics of the process, for finiteness of moments of passage times of general Lévy processes above horizontal, linear or certain curved boundaries. They apply in particular to processes which drift almost surely to infinity, and lead to estimates of {{the rate of growth}} of certain expectations, constituting generalised kinds of renewal theorems. Further results concern the inverse local time at the maximum and the ladder height process, {{the amount of time spent}} below a given level, and the overall minimum of the Lévy process. 2004 Elsevier SAS. All rights reserved. Résumé Des conditions nécessaires et <b>suffisantes</b> sont données pour la finitude des moments de certains temps de passage des processus de Lévy...|$|E
40|$|In {{this paper}} we study the {{necessary}} and sufficient condition under which an orbitally normalizable vector field of saddle or saddle-node type in C 2 is analytically conjugate to its formal normal form (i. e., normalizable). We first express this condition {{in terms of}} the relative exactness of a certain 1 -form derived from comparing the time form of the vector field with the time-form of the normal form. We then show that this condition is equivalent to a synchronicity condition: the vanishing of the integral of this 1 -form along certain asymptotic cycles defined by the vector field. This {{can be seen as a}} generalization of the classical theorem of Poincaré saying that a center is isochronous (i. e. synchronous to the linear center) if and only if it is linearizable. The results, in fact, allow us in many cases to compare any two vector fields which differ by a multiplicative factor. In these cases we obtain that the two vector fields which are multiples of each other are analytically conjugate if and only if their time forms are synchronous. Mathematical Subject Classification. subjclass 37 C 15, 34 C 20 Résumé Dans cet article nous étudions les conditions nécessaires et <b>suffisantes</b> pour qu’un champ de vecteurs orbitalement normalisable au voisinage d’un col ou col-nœud de C 2 soit analytiquement conjugue ́ a ̀ sa forme normale (c. -à-d. ...|$|E
40|$|We prove {{extension}} of CR functions from a hypersurface M of CN in {{presence of the}} so-called sector property. If M has finite type in the Bloom–Graham sense, then our result is already contained in [C. Rea, Prolongement holomorphe des fonctions CR, conditions <b>suffisantes,</b> C. R. Acad. Sci. Paris 297 (1983) 163 – 166] by Rea. We think however, that the argument of our proof carries an expressive geometric meaning and deserves interest on its own right. Also, our method applies in some case to hypersurfaces of infinite type; note that for these, the classical methods fail. CR extension is treated by many authors mainly in two frames: extension in directions of iterated of commutators of CR vector fields (cf., for instance, [A. Boggess, J. Pitts, CR extension near a point of higher type, Duke Math. J. 52 (1) (1985) 67 – 102; A. Boggess, J. C. Polking, Holomorphic {{extension of}} CR functions, Duke Math. J. 49 (1982) 757 – 784. [4]; M. S. Baouendi, L. Rothschild, Normal forms for generic manifolds and holomorphic extension of CR functions, J. Differential Geom. 25 (1987) 431 – 467. [1]]); extension through minimality towards unprecised directions [A. E. Tumanov, Extension of CR-functions into a wedge, Mat. Sb. 181 (7) (1990) 951 – 964. [6]; A. E. Tumanov, Analytic discs and the extendibility of CR functions, in: Integral Geometry, Radon Transforms and Complex Analysis, Venice, 1996, in: Lecture Notes in Math., vol. 1684, Springer, Berlin, 1998, pp. 123 – 14...|$|E
40|$|LA REGULATION DE LA CONDUITE SERVANT SA COHERENCES REQUIERT L'INTERDEPENDANCE DES PROCESSUS COGNITIFS ET CONATIFS. DEUX CONCEPTIONS DE L'INFLUENCE DE L'EXPERIENCE EMOTIONNELLE ET DU TRAIT CONATIF SUR LA COGNITION SE CONFRONTENT: SOIT, LA CONATION PRIVE LE SUJET DE RESSOURCES ATTENTIONNELLES <b>SUFFISANTES</b> DU FAIT QU'ELLE DETOURNE L'ATTENTION SUR SOI (CONSCIENCE DE SOI). SOIT, EN RAISON MEME DE L'ELEVATION DE LA CONSCIENCE DE SOI, LA CONATION REGULE L'ACTIVITE EN ORIENTANT LA PRISE D'INFORMATIONS ET LE DEGRE D'EFFORT. TROIS EXPERIMENTATIONS SONT MENEES DANS LESQUELLES LES EMOTIONS POSITIVE, NEGATIVE ET NEUTRE SONT PROVOQUEES CHEZ DES SUJETS ANXIEUX ET NON ANXIEUX. L'EFFICACITE DES INDUCTIONS EMOTIONNELLES EST EVALUEE PAR UNE ECHELLE D'HUMEUR ET UNE ECHELLE DE CONSCIENCE DE SOI TRANSITOIRE. LES TACHES RELEVENT DU DOMAINE DE LA CATEGORISATION. DES CATEGORIES FORMEES ET DES STRATEGIES MOBILISEES, NOUS INFERONS LE DEGRE D'EFFORT ALLOUE ET L'ORIENTATION DE LA PRISE D'INFORMATION. LES RESULTATS MONTRENT UNE SYMETRIE ENTRE LES MODES DE REGULATION AFFECTIVO-COGNITIVE : LA CONSCIENCE DE SOI ACCROIT L'EFFORT CHEZ LES SUJETS ANXIEUX MAIS LE DIMINUE CHEZ LES NON ANXIEUX. LES SUJETS NON ANXIEUX ALLOUENT DAVANTAGE D'EFFORT EN SITUATION AFFECTIVE NEUTRE. LA TONALITE DE L'EXPERIENCE EMOTIONNELLE ORIENTE LA PRISE D'INFORMATIONS : L'EMOTION POSITIVE FAVORISE UN TRAITEMENT DIRIGE PAR LES CONCEPTIONS, L'EMOTION NEGATIVE UN TRAITEMENT DIRIGE PAR LES DONNEES. CHEZ LES SUJETS ANXIEUX, L'EMOTION NEGATIVE ET NON L'EMOTION POSITIVE PARAIT FAVORISER LES CATEGORIES CONCEPTUELLES. CE SERAIT DANS LES CONDITIONS CONATIVES LES PLUS CONTRASTEES, QUE LES CONDUITES SERAIENT LES MIEUX REGULEES. PARIS 5 -BU Saints-Pères (751062109) / SudocPARIS-INETOP/CNAM (751052322) / SudocSudocFranceF...|$|E
40|$|HENRI POINCARE, FROM MATHEMATICS TO PHILOSOPHY. STUDY OF THE INTELLECTUAL, SOCIAL AND POLITICAL PROGRESSION OF A MATHEMATICIAN AT THE BEGINNING OF THE CENTURYAPRES LA MORT DE HENRI POINCARE EN 1912, SON OEUVRE SUSCITA UN GRAND NOMBRE D'ETUDES. GRACE A CES TRAVAUX ON CONNAIT SON OEUVRE SCIENTIFIQUE ET ON CONNAIT ASSEZ BIEN LE VERSANT TECHNIQUE DE SA PHILOSOPHIE; EN REVANCHE, ON SAIT TRES PEU DE CHOSES SUR LA DIMENSION PROPREMENT PHILOSOPHIQUE DE SON EPISTEMOLOGIE ET SUR LES RELATIONS QU'IL POUVAIT ENTRETENIR AVEC LES ACTEURS DE LA COMMUNAUTE PHILOSOPHIQUE ET INTELLECTUELLE DE SON TEMPS. DE FAIT, L'ETAT DES PUBLICATIONS SUR LA PHILOSOPHIE POINCAREIENNE ACCUSE UNE SORTE DE DESEQUILIBRE CHRONIQUE, CETTE THESE A POUR PRINCIPALE AMBITION DE CONTRIBUER A RETABLIR CET EQUILIBRE EN MONTRANT QUE LES CONSIDERATIONS MATHEMATIQUES ET PHYSIQUES CONSTITUENT DES EXPLICATIONS NECESSAIRES MAIS NON <b>SUFFISANTES</b> DE LA PENSEE PHILOSOPHIQUE POINCAREIENNE; ELLEENTEND MONTRER QU'UNE INTERPRETATION ADEQUATE DE CELLE-CI PASSE NON SEULEMENT PAR LA PRISE EN COMPTE DE SON ENRACINEMENT DANS LA PRATIQUE SCIENTIFIQUE ET DES DEBATS INTERNES A LA COMMUNAUTE SCIENTIFIQUE, MAIS EGALEMENT PAR LA MISE EN EVIDENCE DES LIENS PROFONDS QUI UNISSENT SON AUTEUR A LA COMMUNAUTE PHILOSOPHIQUE ET INTELLECTUELLE. A TRAVERS CE TRAVAIL, ON ABORDE CERTAINS THEMES QUI ONT PEU ATTIRE JUSQU'A PRESENT L'ATTENTION DES COMMENTATEURS DE POINCARE : L'ETUDE DES LIENS QUI UNISSENT LA PHILOSOPHIE POINCAREIENNE A CELLE DE BOUTROUX, LE RECIT DE L'ENTREE DU MATHEMATICIEN AU SEIN DU CHAMP PHILOSOPHIQUE, L'ANALYSE DE SES STRATEGIES DE PUBLICATION ET DE SON OEUVRE DE VULGARISATION OU L'ETUDE DE SES ENGAGEMENTS POLITIQUES CONSTITUENT QUELQUES-UNS DES MOMENTS ESSENTIELS D'UN CHEMINEMENT VISANT A CONSTRUIRE UNE BIOGRAPHIE INTELLECTUELLE DU MATHEMATICIEN ET PHILOSOPHE...|$|E
40|$|International audienceAt {{the outset}} (1829), Ch. F. Sturm's theorem of algebra {{provided}} an algorithm for counting {{the number of}} roots of a polynomial which lie within an interval of real numbers. Generalized by A. Tarski during the 1930 s, this theorem became a decision algorithm for the first-order theory in logic of the ordered field of the real numbers. In the following article, I consider these two events : the origins of Sturm's theorem, which lay in J. Fourier's similar theorem; and the transformation of Sturm's theorem by Tarski, which displayed its logical significance and, in this way, ushered in the revival to which this theorem still owes its actuality. From the viewpoint that I take in this study, I take up various general questions : about exchanging algebra and analysis : about definitions in terms of necessary and sufficient conditions; about the algebraic nature of methods; about the effectivity of procedures (meaning whatever enables procedures such as finitely recursive procedures to be carried out in a finite number of steps); and about the notion of algorithm. Au départ (1829), le théorème d'algèbre de Ch. F. Sturm fournit un algorithme pour compter le nombre de racines d'un polynôme sur un intervalle réel. Généralisé par A. Tarski (dans les années 1930), il devient un algorithme de décision pour la théorie logique du premier ordre du corps ordonné des nombres réels. L'article suivant considère ces deux moments : la naissance du théorème de Sturm à partir du théorème analogue de J. Fourier; sa transformation par Tarski qui en révèle la portée logique et inaugure ainsi le renouvellement auquel ce théorème doit encore son actualité. Diverses questions générales sont abordées par le biais de cette étude : sur les échanges entre Algèbre et Analyse, sur les définitions par conditions nécessaires et <b>suffisantes,</b> sur la nature algébrique des méthodes, sur l'effectivité des procédures, sur la notion d'algorithme...|$|E
40|$|The recent {{improvements}} in communication technologies and products {{in terms of}} services' diversification have a direct effect on home environment. Nowadays, homes host increasingly sophisticated devices providing {{a wide range of}} services such as HDTV, VoIP, multimedia storage, gaming, music and so on. This extension does not come for free. Indeed, Information and Communication Technology-enabled (ICT) devices consume about the quarter of the total power consumption within a typical home network, which puts forward the issue of energy e_ciency in the home network. In addition, electromagnetic radiation emissions are increasing within homes and may create fear likely to slow down future innovations for certain community of people. Nevertheless the increasing connectivity may help to solve these problems by disabling redundant devices or select more appropriate paths in the network. The problem {{can be viewed as a}} multi-criteria routing protocol generalization with the introduction of new metrics. A home network routing solution has therefore to support additional constraints in conjunction with QoS criteria. It has been a challenging goal to make path selection based on ecological criteria that are closely linked to the external environment (e. g. EM radiation); instead of the conventional routing metrics inherently dependent on network state (e. g. delay). In this thesis, we develop a new concept of radiation-aware routing algorithm for heterogeneous home networks in order to reduce radiated emissions level within a given area. The first contribution is the proposal of two models of electromagnetic radiated emissions stemming from Wi-Fi and PLC links based on a set of assumptions and mathematical approximation methods. We have then formulated a link-adaptive radiation-aware routing metric. The second contribution is the proposal of the Radiant Exposure (RE), which is a new routing metric for finding minimum radiated emissions paths in multi-hop heterogeneous network. The RE of a path is the expected radiated energy in units of W. s/m^ 2, within the radiation-sensitive area, while transmitting a packet along that path. The RE metric incorporates the effects of the distance between the radiating sources and the radiation-sensitive area as well as the asymmetry of this radiated energy regarding the two directions of each link. We describe the design and implementation of RE as routing metric that can _t any shortest path algorithm. For practical networks, using RE metric also maximizes the network throughput. We show by simulations that using RE metric reduces significantly radiated energy compared to the widely used minimum-hop count metric. The third contribution consists of proposing a mutli-criteria routing algorithm built upon the well-known normalized weighting function. Such method transforms a multi-objective problem into a mono-objective problem by multiplying each objective function by a weighting factor and summing up all the weighted criteria. We involve the user in the process of selecting weights in line with their preferences. The three criteria considered here are radiant exposure (RE), expected energy consumption and bandwidth. L'objectif majeur des travaux de cette thèse est de concevoir une solution de routage destinée aux réseaux domestiques et qui prend en considération les exigences écologiques émergentes des utilisateurs. Ces exigences concernent l'économie d'énergie et la minimisation des champs électromagnétiques rayonnés. La première contribution consiste à la proposition d'un nouveau concept de métrique de routage sensible aux rayonnements électromagnétiques qui peut être utilisée sans modifications avec n'importe quel algorithme de plus court chemin. Nous avons ensuite montré analytiquement que cette métrique remplisse les conditions <b>suffisantes</b> et nécessaires afin de garantir un routage cohérant, optimal et sans boucle. Cette métrique est désormais nommée Radiant Exposure (RE). Pour se faire nous avons proposé deux modèles analytiques des champs électromagnétiques provenant des équipements Wi-Fi et des lignes électriques quand celles-ci transportent des données en hautes fréquences. Dans la deuxième contribution, nous avons présenté les procédures de calcul et d'implémentation de notre métrique pour deux scenarios : un réseau sans fil et un réseau hétérogène. Ceci nous a conduit à étendre la définition initiale de la métrique RE pour couvrir le cas où les liens Wi-Fi et CPL coexistent dans le même réseau. Cela correspond à la proposition de deux algorithmes de routage pour les deux scenarios préalablement mentionnés. La dernière contribution consiste à la proposition d'un algorithme de routage multicritère basé sur la méthode à pondération normalisée. Les trois critères que nous avons considérés sont RE, la consommation d'énergie et la bande passante...|$|E
40|$|International audienceRésuméIntroduction Le Big Five Inventory (BFI) de John (1991) représente un des outils les plus consensuels de l'évaluation dimensionnelle de la personnalité. Les cinq grandes {{dimensions}} qu'il mesure sont~: l'Extraversion, l'Agréabilité, le Caractère consciencieux, le Névrosisme et l'Ouverture. Soto et John (2009) ont montré qu'il était possible de considérer deux facettes par dimension du BFI. L'objectif de cette étude est de savoir s'il est possible de retrouver ces dix facettes dans un échantillon français, en s'appuyant sur les corrélations avec les facettes équivalentes du NEO PI-R (NEO Personality Inventory Revised). Méthode Le premier échantillon constitué de 360 ~étudiants de l'Institut libre d'éducation physique supérieure (ILEPS) et de l'Université de Tours (en psychologie) a permis de tester la validité externe des facettes du BFI-Fr en lien avec celles du NEO PI-R. Le second échantillon constitué de 142 ~étudiants de psychologie de l'Université de Tours a permis de rechercher la validité temporelle (test et retest). Le troisième échantillon constitué de 252 ~étudiants en psychologie de l'Université de Paris-Nanterre a permis de tester la forme hétéro-évaluative du BFI-Fr. Résultats et discussion Le BFI-Fr permet d'évaluer neuf des dix facettes proposées par Soto et John~: Activité et Assertivité pour la dimension Extraversion~; Altruisme et Compliance pour la dimension Agréabilité~; Ordre et Autodiscipline pour la dimension Caractère consciencieux~; Anxiété pour la dimension Névrosisme~; Ouverture à l'esthétique et Ouverture aux idées pour la dimension Ouverture. L'étude a permis de définir une facette Instabilité émotionnelle en remplacement de la facette Dépression du Névrosisme qui ne présentait pas de qualités psychométriques <b>suffisantes.</b> AbstractIntroduction The Big Five Inventory (BFI) {{developed by}} John et al. (1991) {{is one of}} the most widely accepted tools for assessing dimensions of personality. It comprises 44 items that assess five broad dimensions of personality (the Big Five Factors) : Extraversion, Agreeableness, Conscientiousness, Neuroticism and Openness to experience. Based on correlations with the facets described in the NEO Personality Inventory Revised (NEO PI-R), another Big Five assessment tool with 240 items and 6 facets per dimension, Soto and John (2009) showed that the dimensions in the BFI could be divided into two facets each (ten facets altogether). These results are in line with those of DeYoung et al. (2007), who ran factorial analyses with all the NEO PI-R facets and the International Personality Item Pool (IPIP) and identified ten intermediate factors (between facets and dimensions) which they called ``aspects'' (two per dimension). The goal of the present study is to investigate the ten facets described by Soto and John in a French sample, using the French version of the BFI (BFI-Fr), which has good psychometric properties, and to check whether the pattern of correlations of these facets with the NEO PI-R match those of the American version. Method We created three groups. The first comprised 360 students from the Institut libre d'éducation physique supérieure (ILEPS) and Tours University (psychology undergraduates). Participants (mean age 21. 1 years ± 2. 30; 58 % women) completed the BFI-Fr and the NEO PI-R. The second comprised 142 psychology students from Tours University (mean age 20. 6 years ± 1. 78; 81 % women); they completed the BFI-Fr twice, two weeks apart (test and retest). The third comprised 252 psychology students from Paris-Nanterre University (mean age 23 years ± 4. 2; 89 % women) who described a total of 405 people they knew well (mean age 35. 2 ± 10. 8; 49 % women) using the peer-report format of the BFI-Fr. Results In the self-report format, eight of Soto and John's ten aspects had acceptable internal consistency (based on Guildford's (1954) internal consistency criteria, due to the small number of items), with Cronbach's α between 0. 60 and 0. 86 and test-retest correlations between 0. 71 and 0. 89, showing satisfactory temporal stability. We found a single facet for Extraversion (Assertiveness), two for Agreeableness (Altruism and Compliance), two for Conscientiousness (Self-Discipline and Order), one for Neuroticism (Anxiety), and two for Openness to Experience (Openness to aesthetics and Openness to ideas). Based on their convergence with the corresponding facets in the NEO PI-R, these eight facets showed satisfactory external validity. With regard to the peer-report format, the Activity facet of Extraversion, which did not have sufficient internal consistency in the self-report format, had acceptable properties (i. e. 9 out of 10 facets). Only the Depression facet of Neuroticism still had insufficient internal consistency. In this study, we proposed an improvement of two facets (Activity and Compliance) and added one facet specific to the French version (Emotional Instability) in place of the Depression facet. Discussion We showed that the BFI-Fr can be used to assess nine of the ten facets described by Soto and John. We also identified an Emotional Instability facet, replacing the Depression facet of Neuroticism. DeYoung et al. (2007) considered that anxiety and depression are indissociable and can be represented by a Neuroticism aspect they labeled Withdrawal. They suggested a second aspect of this dimension they called Volatility (with the N 2 Angry Hostility facet of the NEO PI-R as main marker and the N 5 Impulsiveness and N 3 Depression as secondary markers). The Emotional Instability facet we found corresponds closely to the N 2 Angry Hostility facet of the NEO PI-R and appears to be a satisfactory marker of DeYoung et al. 's (2007) Volatility aspect. Although this study has limitations, particularly related to the samples (students), the BFI-Fr facets (derived from those defined by Soto and John in the BFI or proposed as improvements on the original facets) match the corresponding NEO PI-R facets and can also be seen as main markers of the aspects defined by DeYoung et al...|$|E
40|$|Le lac de Comabhio (Lombardie, Italie du Nord) est un lac peu profond (z- = 4, 4 m) en {{comparaison}} à sa surface (3, 44 km 2). L'eutrophisation naturelle du lac a été accélérée par l'apport d'effluents domestiques pas traités. Pour améliorer la situation, les effluents conduits par un collecteur circumlacustre, ont été recueillis à l'extérieur du bassin versant. Les effets seront visibles seulement dans plusieurs années et ces interventions ne seront pas <b>suffisantes</b> pour ramener le lac à l'état de mésotrophie. Afin d'accélérer l'assainissement du lac, mais surtout pour éviter la forte mortalité du poisson périodiquement à la fin de l'été, on a pris la décision d'oxygéner artificiellement l'hypolimnion. Pour prévoir les effets de l'oxygénation on a fait une expérience dans le lac avec deux enceintes en PVC (diamètre : 40 m; profondeur : 6 m; volume : 7000 m 3). On a gardé une enceinte comme témoin; dans l'autre a été insufflé de l'oxygène pur. On a recueilli, à certains intervalles de temps, des échantillons d'eau et de plancton du lac et des deux enceintes. Les éléments nutritifs ont été analysés; la température, la transparence, la conductivité et le pH ont été mesurés sur place. L'effet le plus visible du traitement consistait dans une augmentation de la concentration de l'oxygène visant à produire des conditions normales pour la vie des poissons. Les effets de l'oxygénation sur les caractéristiques chimiques, physiques et biologiques du milieu sont discutés. Lake Comabbio (Lombardy, Northern Itaty) is {{a shallow}} Lake (zmax = 8. 0 m; z-= 4. 4 m; surface = 3. 44 km 2). Its naturally high trophic level has been increased {{by the huge}} nutrient loading from domestic effluents. Each year (except in winter, when commonly the lake is covered with ice) more phytoplankton blooms are observed and {{in the late summer}} oxygen depletion, with a consequent mass mortality of fish, occurs. At present, to reduce the nutrient charge, the effluents are collected in a channel and diverted from the lake watershed. Because of the significant nutrient release from the sediment ("internal eutrophication") the reduction of the external loading may slow the eutrophication rate, but it is not sufficient to restore the lake to an acceptable mesotrophic state within a reasonable time. To prevent the periodic mass mortality of fish and possibly to accelerate the restoration process of the ecosystem, the Lake Comabbio Protection Committee decided to oxygenate the hypolimnetic layer artificially. In order to avoid undesirable effects of aeration (e. g. nitrogen enrichment of lake water), dissolved oxygen injection was preferred. As there is no general agreement on the ecological effects of the oxygen (or air) addition it was decided, before applying oxygenation to the whole lake, to carry out experiments using the "enclosure" method. One experiment was carried out from 26 th September to 9 th December 1986 and this was replicated in 1987 from 22 nd April to 30 th November. This paper is concerned with the first experiment, as data treatment of the second is still in progress. Two cylindrical "enclosures" of PVC (diameter = 40. 0 m; height = 6. 0 m; volume = 7000 m 3) were settled in Lake Comabbio to isolate a water column with its sediments. One enclosure was treated with oxygen from 15 th November to 6 th December, the other was kept as control. The water aspirated from the enclosure was oxygenated (20. 3 mg O 2 /l) and injected in the same enclosure at 4 m depth at a rate of 4 m 3 /h. During the enclosure settlement the fish escaped from the enclosures. To simulate a complete ecosystem, the plan was to add to each enclosure 60 kg of the most abundant fish species (Scardinius erythrophtalmus). By mistake, fish were added to the enclosure that was to be oxygenated but not to the control. Water and zooplankton were sampled simultaneously from both the enclosures {{as well as from the}} Lake. Zooplankton was collected by vertical hauls from the bottom to the water surface. The following parameters were measured : temperature, transparency, pH, electrical conductivity, dissolved oxygen, alkalinity, phosphorus and nitrogen compounds, chlorophyll-a and pheopigments. The injection of water rich in oxygen did not resuspend the sediments and the absence of bubbles showed that the oxygen had completely dissolved. Soon after oxygenation the difference in oxygen concentration in the treated enclosure and in the control was progressively reduced until 26 th November, when both the enclosures had the same oxygen concentration. At the end of the experiment (9 th December) the oxygen concentration in the control was about 4 mg/l, whereas in the treated enclosure it attained a concentration of 6 mg/l, although in the latter the oxygen consumption was greater than in the control, owing to fish respiration and the increased activity of aerobic microorganisms. The higher concentrations of total phosphorus (TP), soluble reactive phosphorus (SRP) and ammonium (N-NH 4) measured in the lake were probably due to the external loading of nutrients. The nutrient concentrations in the treated enclosure were similar to those of the control. The greater concentration of nutrients was probably the cause of more abundant phytoplankton in the lake, which was also demonstrated by the lower values of water transparency, when compared with those measured in both the enclosures. The pH values of the control were similar to those of the treated enclosure. The pattern of abundance and species composition of the zooplankton in the treated enclosure did not differ from that of the control. The Entomostraca populations decreased in the lake as well as in both the enclosures from the beginning to the end of the experiment. From this experiment, carried out when oxygen depletion affected the whole water column, we may conclude that oxygenation can prevent fish mortality, without significantly influencing the structure and biomass of the planktonic community. The variations in the plankton and chemical and physical characteristics of the water (except oxygen concentration) were essentially due to the season and mot to the artificial oxygenation. In addition, oxygenation, by abolishing the periodical mortality of fish, may also prevent the release of a huge amount of organic substances from the dead fish into the water...|$|E
40|$|Parts of {{this thesis}} {{has been written}} during long term visits to ORIE - Cornell University and Statistics department, Columbia University. In this thesis, we study various {{contemporary}} issues in quantitative finance. The first chapter {{is dedicated to the}} stability of the semimartingale property under filtration expansion. We study first progressive filtration expansions with random times. We show how semimartingale decompositions in the expanded filtration can be obtained using a natural link between progressive and initial expansions. The link is, on an intuitive level, that the two coincide after the random time. We make this idea precise and use it to establish known and new results in the case of expansion with a single random time. The methods are then extended to the multiple time case, without any restrictions on the ordering of the individual times. We then look to the expanded filtrations {{from the point of view}} of filtration shrinkage. We turn then to studying progressive filtration expansions with processes. Using results from the weak convergence of sigma fields theory, we first establish a semimartingale convergence theorem, which we apply in a filtration expansion with a process setting and provide sufficient conditions for a semimartingale of the base filtration to remain a semimartingale in the expanded filtration. A first set of results is based on a Jacod's type criterion for the increments of the process we want to expand with. An application to the expansion of a Brownian filtration with a time reversed diffusion is given through a detailed study and some known examples in the litterature are recovered and generalized. Finally, we focus on filtration expansion with continuous processes and derive two new results. The first one is based on a Jacod's type criterion for the successive hitting times of some levels and the second one is based on honest times assumptions for these hitting times. We provide examples and see how those can be used as first steps toward harmful dynamic insider trading models. In the expanded filtration the finite variation term of the price process can become singular and arbitrage opportunities (in the sense of FLVR) can therefore arise in these models. In the second chapter, we reconcile structural models and reduced form models in credit risk from the perspective of the information induced credit contagion effect. That is, given multiple firms, we are interested on the behaviour of the default intensity of one firm at the default times of the other firms. We first study this effect within different specifications of structural models and different levels of information. Since almost all examples are non tractable and computationally very involved, we then work with the simplifying assumption that conditional densities of the default times exist. The classical reduced-form and filtration expansion framework is therefore extended to the case of multiple, non-ordered defaults times having conditional densities. Intensities and pricing formulas are derived, revealing how information-driven default contagion arises in these models. We then analyze the impact of ordering the default times before expanding the filtration. While not important for pricing, the effect is significant in the context of risk management, and becomes even more pronounced for highly correlated and asymmetrically distributed defaults. We provide a general scheme for constructing and simulating the default times, given that a model for the conditional densities has been chosen. Finally, we study particular conditional density models and the information induced credit contagion effect within them. In the third chapter, we provide a methodology for a real time detection of bubbles. After the 2007 credit crisis, financial bubbles have once again emerged as a topic of current concern. An open problem is to determine in real time whether or not a given asset's price process exhibits a bubble. Due to recent progress in the characterization of asset price bubbles using the arbitrage-free martingale pricing technology, we are able to propose a new methodology for answering this question based on the asset's price volatility. We limit ourselves to the special case of a risky asset's price being modeled by a Brownian driven stochastic differential equation. Such models are ubiquitous both in theory and in practice. Our methods use non parametric volatility estimation techniques combined with the extrapolation method of reproducing kernel Hilbert spaces. We illustrate these techniques using several stocks from the alleged internet dot-com episode of 1998 - 2001, where price bubbles were widely thought to have existed. Our results support these beliefs. During May 2011, there was speculation in the financial press concerning the existence of a price bubble in the aftermath of the recent IPO of LinkedIn. We analyzed stock price tick data from the short lifetime of this stock through May 24, 2011, and we found that LinkedIn has a price bubble. The last chapter is about discretely sampled variance swaps, which are volatility derivatives that trade actively in OTC markets. To price these swaps, the continuously sampled approximation is often used to simplify the computations. The purpose of this chapter is to study the conditions under which this approximation is valid. Our first set of theorems characterize the conditions under which the discretely sampled variance swap values are finite, given the values of the continuous approximations exist. Surprisingly, for some otherwise reasonable price processes, the discretely sampled variance swap prices do not exist, thereby invalidating the approximation. Examples are provided. Assuming further that both variance swap values exist, we study sufficient conditions under which the discretely sampled values converge to their continuous counterparts. Because of its popularity in the literature, we apply our theorems to the 3 / 2 stochastic volatility model. Although we can show finiteness of all swap values, we can prove convergence of the approximation only for some parameter values. Dans cette thèse, nous étudions différentes problématiques d'actualité en finance quantitative. Le premier chapitre est dédié à la stabilité de la propriété de semimartingale après grossissement de la filtration de base. Nous étudions d'abord le grossissement progressif d'une filtration avec des temps aléatoires et montrons comment la décomposition de la semimartingale dans la filtration grossie est obtenue en utilisant un lien naturel entre la filtration grossie initiallement et celle grossie progressivement. Intuitivement, ce lien se résume au fait que ces deux filtrations coincident après le temps aléatoire. Nous précisons cette idée et l'utilisons pour établir des résultats connus pour certains et nouveaux pour d'autres dans le cas d'un grossissement de filtrations avec un seul temps aléatoire. Les méthodes sont alors étendues au cas de plusieurs temps aléatoires, sans aucune restriction sur l'ordre de ces temps. Nous étudions ensuite ces filtrations grossies du point de vue des rétrécissements des filtrations. Nous nous intéressons enfin au grossissement progressif de filtrations avec des processus. En utilisant des résultats de la convergence faible de tribus, nous établissons d'abord un théorème de convergence de semimartingales, que l'on appliquera dans un contexte de grossissement de filtrations avec un processus pour obtenir des conditions <b>suffisantes</b> pour qu'une semimartingale de la filtration de base reste une semimartingale dans la filtration grossie. Nous obtenons des premiers résultats basés sur un critère de type Jacod pour les incréments du processus utilisé pour grossir la filtration. Nous nous proposons d'appliquer ces résultats au cas d'un grossissement d'une filtration Brownienne avec une diffusion retournée en temps et nous retrouvons et généralisons quelques examples disponibles dans la littérature. Enfin, nous concentrons nos efforts sur le grossissement de filtrations avec un processus continu et obtenons deux nouveaux résultats. Le premier est fondé sur un critère de Jacod pour les temps d'atteinte successifs de certains niveaux et le second est fondé sur l'hypothèse que ces temps sont honnêtes. Nous donnons des examples et montrons comment cela peut constituer un premier pas vers des modèles dynamiques de traders initiés donnant naissance à des opportunités d'arbitrage nocives. Dans la filtration grossie, le terme à variation finie du processus de prix peut devenir singulier et des opportunités d'arbitrage (au sens de FLVR) apparaissent clairement dans ces modèles. Dans le deuxième chapitre, nous réconcilions les modèles structuraux et les modèles à forme réduite en risque de crédit, du point de vue de la contagion de crédit induite par le niveau d'information disponible à l'investisseur. Autrement dit, étant données de multiples firmes, nous nous intéressons au comportement de l'intensité de défaut (par rapport à une filtration de base) d'une firme donnée aux temps de défaut des autres firmes. Nous étudions d'abord cet effet sous des spécifications différentes de modèles structuraux et sous différents niveaux d'information, et tirons, par l'exemple, des conclusions positives sur la présence d'une contagion de crédit. Néanmoins, comme plusieurs exemples pratiques ont un coup calculatoire élevé, nous travaillons ensuite avec l'hypothèse simplificatrice que les temps de défaut admettent une densité conditionnelle par rapport à la filtration de base. Nous étendons alors des résultats classiques de la théorie de grossissement de filtrations avec des temps aléatoires aux temps aléatoires non-ordonnés admettant une densité conditionnelle et pouvons ainsi étendre l'approche classique de la modélisation à forme réduite du risque de crédit à ce cas général. Les intensités de défaut sont calculées et les formules de pricing établies, dévoilant comment la contagion de crédit apparaît naturellement dans ces modèles. Nous analysons ensuite l'impact d'ordonner les temps de défaut avant de grossir la filtration de base. Si cela n'a aucune importance pour le calcul des prix, l'effet est significatif dans le contexte du management de risque et devient encore plus prononcé pour les défauts très corrélés et asymétriquement distribués. Nous proposons aussi un schéma général pour la construction et la simulation des temps de défaut, étant donné qu'un modèle pour les densités conditionnelles a été choisi. Finalement, nous étudions des modèles de densités conditionnelles particuliers et la contagion de crédit induite par le niveau d'information disponible au sein de ces modèles. Dans le troisième chapitre, nous proposons une méthodologie pour la détection en temps réel des bulles financières. Après la crise de crédit de 2007, les bulles financières ont à nouveau émergé comme un sujet d'intéret pour différents acteurs du marché et plus particulièrement pour les régulateurs. Un problème ouvert est celui de déterminer si un actif est en période de bulle. Grâce à des progrès récents dans la caractérisation des bulles d'actifs en utilisant la théorie de pricing sous probabilité risque-neutre qui caractérise les processus de prix d'actifs en bulles comme étant des martingales locales strictes, nous apportons une première réponse fondée sur la volatilité du processus de prix de l'actif. Nous nous limitons au cas particulier où l'actif risqué est modélisé par une équation différentielle stochastique gouvernée par un mouvement Brownien. Ces modèles sont omniprésents dans la littérature académique et en pratique. Nos méthodes utilisent des techniques d'estimation non paramétrique de la fonction de volatilité, combinées aux méthodes d'extrapolation issues de la théorie des reproducing kernel Hilbert spaces. Nous illustrons ces techniques en utilisant différents actifs de la bulle internet (dot-com bubble) de la période 1998 - 2001, où les bulles sont largement acceptées comme ayant eu lieu. Nos résultats confirment cette assertion. Durant le mois de Mai 2011, la presse financière a spéculé sur l'existence d'une bulle d'actif après l'OPA sur LinkedIn. Nous analysons les prix de cet actif en nous basant sur les données tick des prix et confirmons que LinkedIn a connu une bulle pendant cette période. Le dernier chapitre traite des variances swaps échantillonnés en temps discret. Ces produits financiers sont des produits dérivés de volatilité qui tradent activement dans les marchés OTC. Pour déterminer les prix de ces swaps, une approximation en temps continu est souvent utilisée pour simplifier les calculs. L'intérêt de ce chapitre est d'étudier les conditions garantissant que cette approximation soit valable. Les premiers théorèmes caractérisent les conditions sous lesquelles les valeurs des variances swaps échantillonnés en temps discret sont finies, étant donné que les valeurs de l'approximation en temps continu sont finies. De manière étonnante, les valeurs des variances swaps échantillonnés en temps discret peuvent etre infinies pour des modèles de prix raisonnables, ce qui rend la pratique de marché d'utiliser l'approximation en temps continu invalide. Des examples sont fournis. En supposant ensuite que le payoff en temps discret et son approximation en temps continu ont des prix finis, nous proposons des conditions <b>suffisantes</b> pour qu'il y ait convergence de la version discrète vers la version continue. Comme le modèle à volatilité stochastique 3 / 2 est de plus en plus populaire, nous lui appliquons nos résultats. Bien que nous pouvons démontrer que les deux valeurs des variances swaps sont finies, nous ne pouvons démontrer la convergence de l'approximation que pour certaines valeurs des paramètres du modèle...|$|E
40|$|China {{realized}} {{during the}} last 15 years spectacular economic growth success. However, its economic growth was also accompanied by serious environmental degradation problems. China has been ranked {{as one of the}} most polluted countries in the world by some international organizations, particularly on the aspect of air pollution. Although many scholars start to consider China as the future number one economic power given its current marvelous economic success, the verification of such hypothesis closely depends on the sustainability of China's future growth path—its actual economic growth speed can be sustainable only if today's economic achievement is not obtained by mortgaging that of tomorrow. This dissertation, focusing on the case of industrial SO 2 emission in China, aims to study the potential relationship between economic growth, trade liberalization and environment in China, in the aims of identifying the possibility, the sufficient and necessary conditions for China to realize its sustainable development. After a comprehensive literature review on the existing Environmental Kuznets Curve studies, I started my analysis by a reduced-form Environmental Kuznets Curve analysis (Chapter 2). The results showed that although the EKC analysis predicts a turning point at about 9000 yuan (1990 price) for the case of per capita industrial SO 2 emission, the evolution of total industrial SO 2 emission seems to continue its increasing tendency. To understand the underlying reasons for the increasing tendency in total SO 2 emission, I further carried out two structural analyses, in which the structural determinants of SO 2 emission are decomposed either parametrically (Chapter 3) or non-parametrically (Divisia index decomposition method based on the detailed data on production and SO 2 emission intensity of 13 industrial sectors in each province during 1991 - 2001, Chapter 4) into scale, composition and technique effects. The results showed that, the per capita income, acting as an omnibus variable representing all the three aspects of underlying structural determinants, only impart a “net-effect” of income growth on environment. The real reason for the ever-increasing trend of total industrial SO 2 emission in China is actually due to the domination of pollution-increasing impact of scale enlargement over the pollution-reducing contribution from technical progress, combined with a province-specific composition transformation which exerts modest pollution-increasing impact in most of the Chinese provinces, given their current industrialization process. The second part of this dissertation further amplified my decomposition efforts by giving particular attention to the emission determination role of international trade. Previously redeemed by some pessimistic economists as a channel for the richer developed countries to discharge their pollution burdens to their poorer trade-partners, international trade has been considered as a static explanation for the formation of the inverted-U-shaped growth-pollution relationship. Nevertheless, all three analyses carried out in this part, by investigating the different channels through which international trade can exerts impact on the three determinants of emission, only provide very limited supportive evidence for the “pollution haven” hypothesis in China. As China's factor-endowment-based comparative advantages are much attractive than its potential as a “pollution haven”, the conclusion of the ACT-style (Antweiler, Copeland and Taylor, 2001) model estimation in Chapter 5 shows that trade liberalization can actually reduce the pollution burden of China's industrialization process by deviating its industrial composition transformation towards less polluting labour-intensive sectors. By noticing that the actual role of trade is more complicated, in Chapter 6, I re-employed the decomposed results of Chapter 4 and further checked the indirect impact of trade (export and import separated this time) on industrial emission through its three structural determinants. This study confirmed the significantly positive impact of trade in both scale enlargement and technical progress. The analysis based on a simultaneous system in Chapter 7 permits me to combine these three aspects' indirect impact of international trade on emission into the same estimation. Its results reveal the total role of export in China is environment-friendly while that of import (measured by the accumulation of imported machinery and equipments) is pollution-enhancing. In the CGE model analysis in Chapter 8 (Part 3), I related emission results directly to energy input used in production activities and included the principal coefficients estimated in the previous into the modelling and simulation work. This model offered me an opportunity to parameterise the multiple aspects of trade-pollution and growth-pollution nexus and to finally obtain an explicit numerical comparison between the magnitude of environmental impact of trade and economic growth. This analysis reveals that, compared to the scale effect resulting from rapid economic expansion in China, the actual pollution-increasing impact of trade liberalization is very small. The most important pollution reduction contribution actually comes from efficiency improvement in energy uses and depends largely on the existence of a more stringent and efficient pollution control policies combined with a flexible energy substitution process. Facing the potential dangers for China's future environmental situation, I investigated in the chapter 9 (Part 3) the potential feedback effect from pollution to China's future growth sustainability. The analysis reveals a significant negative relationship between industrial SO 2 emission and public health after the industrial SO 2 emission density attains the critical threshold of 8 g/m 2. Fortunately, the estimated model inn this chapter seems also to reveal some possible dynamism through which the significant negative impact of industrial SO 2 emission on public health status can be gradually reduced with economic growth. But to realize this dynamism, China need to realise a more-than-proportion increase in de-sulfur technology investment with respect to its economic growth rate in the coming years. To sum up, the analyses carried out in this dissertation actually indicate both opportunity and challenge for China's pursuit for a sustainable development path. Given the current environmental deterioration tendency, whether China can preserve its future growth sustainability actually depends on the existence of the technological capacities to improve the pollution abatement efficiency (sufficient condition) and the correct function of a stricter pollution control policies (necessary condition). Both aspects further put forward the requirement for the availability of efficient institutional and market system in China. La Chine, pays le plus peuplé du monde, connaît depuis une quinzaine d'années des taux de croissance spectaculaires. Malheureusement, cette croissance a aussi été accompagnée d'une très forte dégradation de l'environnement et a positionné la Chine parmi les pays les plus pollués du monde, notamment au niveau de son atmosphère. Ainsi, le considérable succès que représente cette croissance économique, au point de vouloir présenter la Chine comme la prochaine première puissance économique mondiale, conduit à poser la question de sa soutenabilité. Nous postulons que la croissance actuelle de la Chine ne pourra être durable que dans la mesure où elle n'hypothèque pas celle de son futur. Cette thèse se base sur le cas de l'émission industrielle de SO 2 – la pollution aérienne la plus importante en Chine. En analysant son évolution au cours des années 1990 et en se focalisant sur ses relations avec la croissance économique, l'industrialisation et l'ouverture commerciale – les trois caractères les plus évidents du développement économique chinois –, cette thèse vise à identifier la possibilité et les conditions nécessaires et <b>suffisantes</b> pour la Chine de réaliser un développement soutenable. Après une revue de la littérature sur la Courbe de Kuznets Environnementale (CKE), nous commençons notre analyse par l'étude d'une CKE de forme réduite (Chap. 2), qui révèle une relation assez optimiste entre la croissance chinoise et l'émission industrielle de SO 2 par tête. Cependant, cette CKE « par tête » ne garantit pas un même retournement de trajectoire pour l'émission industrielle totale de SO 2. Bien que notre analyse prédise ce retournement aux environs de 9000 yuan par tête (prix constants 1990) pour le cas de l'émission industrielle de SO 2 par tête, l'évolution de l'émission industrielle de SO 2 totale semble continuer à augmenter. Pour comprendre les raisons de cette tendance à la hausse de l'émission industrielle de SO 2 totale, nous menons deux analyses structurelles, dans lesquelles les variations de l'émission sont décomposées de façon paramétrique (Chap. 3) et non-paramétrique (la méthode de décomposition de l'indice de Divisia basée sur des données détaillées de la production et de l'intensité d'émission de SO 2 de 13 secteurs industriels dans chaque province entre 1991 et 2002, Chap. 4) en contributions de ses trois déterminants structurels - effets d'échelle, de composition et de technique. Les résultats montrent que le revenu par tête fonctionne comme une variable omnibus qui capte les effets des trois déterminants structurels et révèle seulement un « effet net » de la croissance sur l'environnement. Le détail de nos analyses permet cependant de trouver les véritables raisons de cette tendance à la hausse de l'émission industrielle de SO 2. Cela serait principalement dû à une domination de l'effet d'échelle sur l'effet réducteur d'émission issu des progrès techniques, le tout combiné à une transformation de la composition industrielle exerçant un modeste impact à la hausse des émissions dans la plupart des provinces chinoises. La seconde partie de cette thèse accroît ses efforts de décomposition en donnant une attention particulière au rôle déterminant du commerce international sur l'émission. Perçu par certains économistes pessimistes comme un canal à travers lequel les pays riches déchargeaient leurs fardeaux de pollution sur leurs partenaires commerciaux relativement plus pauvres (hypothèse de « havre de pollution »), le commerce international a ainsi souvent été considéré comme une explication statique de la formation d'une relation en U inversé entre le revenu et la pollution. Cependant, les trois analyses menées dans cette partie, en recherchant les différents canaux de transmission à travers lesquels le commerce affecte les trois déterminants structurels de l'émission, nous offrent pour la Chine des preuves très limitées en faveur de l'hypothèse de « havre de pollution ». Etant donné que l'avantage comparatif de la Chine basé sur sa dotation extrêmement riche en travail est beaucoup plus attractif que son avantage de « havre de pollution », la conclusion d'une analyse de style Antweiler, Copeland et Taylor (ACT, 2001) au Chapitre 5 nous montre que la libéralisation commerciale peut réduire les dangers d'une augmentation de pollution liée à l'effet de « havre de pollution » en guidant la transformation de la composition industrielle chinoise vers les secteurs intensifs en travail, souvent considérés comme moins polluants. Le rôle du commerce sur l'environnement n'est cependant pas aussi simple. Dans le Chapitre 6, en re-employant les résultats de la décomposition de Divisia du Chapitre 4, nous vérifions les impacts indirects du commerce (exportations et importations introduites de façon séparées) sur l'émission à travers les trois déterminants structurels. Cette analyse confirme l'impact significativement positif du commerce dans l'élargissement de l'échelle de production et sur les progrès techniques. Les analyses basées sur un système d'équations simultanées au Chapitre 7 nous permettent de combiner ces trois aspects des impacts indirects du commerce sur l'émission et d'inclure leurs interactions potentielles dans une même estimation. Les résultats révèlent que le rôle total des exportations est positif pour l'environnement mais que celui des importations (mesurées par l'accumulation de machines et d'équipements importés) est négatif. Dans le modèle en Equilibre Général Calculable (EGC) du Chapitre 8 (Partie 3), nous relions directement les résultats de l'émission à la combustion des énergies dans les activités productives et incluons toutes les interactions entre les variables économiques et l'environnement dont nous avons pu discuter dans les chapitres précédents pour la spécification du modèle. Ce modèle nous offre l'opportunité de paramétrer et de simuler de multiples aspects des relations entre croissance, ouverture commerciale et émission. Les simulations basées sur ce modèle nous permettent d'obtenir des comparaisons numériques de l'ampleur des impacts environnementaux du commerce et de la croissance économique. La conclusion de ce chapitre montre que, sans une politique de contrôle de pollution plus efficace, la croissance économique chinoise devrait s'avérer très polluante et que l'accession à l'OMC devrait provoquer une hausse supplémentaire mais marginale de pollution. En considérant les dangers potentiels de cette situation sur l'environnement chinois, nous décidons de rechercher dans le dernier chapitre de cette thèse (Chap. 9) l'effet de « feedback » potentiel de la pollution sur la capacité de croissance de l'économie chinoise. Les résultats confirment un effet négatif de l'émission de SO 2 sur la prévalence des maladies chroniques au sein de la population. Une fois dépassé le seuil de 8 g/m 2, une augmentation de 1 g/m 2 de la densité de l'émission industrielle de SO 2 accroît la probabilité pour une personne représentative de souffrir de maladies chroniques de 0, 25 %. Cependant, si les progrès techniques réalisés dans les activités de contrôle de la pollution augmentent de façon continue dans le temps, nos résultats indiquent également une dynamique potentielle pouvant réduire de façon graduelle l'impact négatif de la pollution sur la santé avec la croissance économique. En résumé, les analyses menées dans cette thèse présentent un certain nombre de défis à relever et d'opportunités à saisir pour que la Chine puisse poursuivre un chemin de développement qui soit soutenable. Etant donnée la tendance actuelle à la détérioration de son environnement, la capacité de la Chine à préserver une croissance soutenable dans le futur dépendra étroitement de l'adoption de progrès techniques (conditions <b>suffisantes)</b> et d'un fonctionnement efficace et plus strict des politiques de contrôle de pollution (conditions nécessaires), ainsi que d'une meilleure efficacité des systèmes institutionnels et de marché...|$|E
40|$|The International Civil Aviation Organization (ICAO) {{has defined}} {{the concept of}} Global Navigation Satellite System (GNSS), which {{corresponds}} to the set of systems allowing to perform satellite-based navigation while fulfilling ICAO requirements. The US Global Positioning Sysem (GPS) is a satellite-based navigation system which constitutes one {{of the components of}} the GNSS. Currently, this system broadcasts a civil signal, called L 1 C/A, within an Aeronautical Radio Navigation Services (ARNS) band. The GPS is being modernized and will broadcast two new civil signals: L 2 C (not in an ARNS band) and L 5 in another ARNS band. Galileo is the European counterpart of GPS. It will broadcast three signals in an ARNS band: Galileo E 1 OS (Open Service) will be transmitted in the GPS L 1 frequency band and Galileo E 5 a and E 5 b will be broadcasted in the same 960 - 1215 MHz ARNS band than that of GPS L 5. GPS L 5 and Galileo E 1, E 5 a, E 5 b components are expected to provide operational benefits for civil aviation use. However, civil aviation requirements are very stringent and up to now, the bare systems alone cannot be used as a means of navigation. For instance, the GPS standalone does not implement sufficient integrity monitoring. Therefore, in order to ensure the levels of performance required by civil aviation in terms of accuracy, integrity, continuity of service and availability, ICAO standards define different systems/algorithms to augment the basic constellations. GPS, Galileo and the augmentation systems could be combined to comply with the ICAO requirements and complete the lack of GPS or Galileo standalone performance. In order to take benefits of new GNSS signals, and to provide the service level required by the ICAO, the architecture of future combined GNSS receivers must be standardized. The European Organization for Civil Aviation Equipment (EUROCAE) Working Group 62, which is in charge of Galileo standardization for civil aviation in Europe, proposes new combined receivers architectures, in coordination with the Radio Technical Commission for Aeronautics (RTCA). The main objective of this thesis is to contribute to the efforts made by the WG 62 by providing inputs necessary to build future receivers architecture to take benefits of GPS, Galileo and augmentation systems. In this report, we propose some key elements of the combined receivers' architecture to comply with approach phases of flight requirements. In case of perturbation preventing one of the needed GNSS components to meet a phase of flight required performance, it is necessary to be able to switch to another available component {{in order to try to}} maintain if possible the level of performance in terms of continuity, integrity, availability and accuracy. That is why future combined receivers must be capable of detecting the impact of perturbations that may lead to the loss of one GNSS component, in order to be able to initiate a switch. These perturbations are mainly atmospheric disturbances, interferences and multipath. In this thesis we focus on the particular cases of interferences and ionosphere perturbations. The interferences are among the most feared events in civil aviation use of GNSS. Detection, estimation and removal of the effect of interference on GNSS signals remain open issues and may affect pseudorange measurements accuracy, as well as integrity, continuity and availability of these measurements. In literature, many different interference detection algorithms have been proposed, at the receiver antenna level, at the front-end level. Detection within tracking loops is not widely studied to our knowledge. That is why, in this thesis, we address the problem of interference detection at the correlators outputs. The particular case of CW interferences detection on the GPS L 1 C/A and Galileo E 1 OS signals processing is proposed. Nominal dual frequency measurements provide a good estimation of ionospheric delay. In addition, the combination of GPS or GALILEO navigation signals processing at the receiver level is expected to provide important improvements for civil aviation. It could, potentially with augmentations, provide better accuracy and availability of ionospheric correction measurements. Indeed, GPS users will be able to combine GPS L 1 and L 5 frequencies, and future GALILEO E 1 and E 5 signals will bring their contribution. However, if affected by a Radio Frequency Interference, a receiver can lose one or more frequencies leading to the use of only one frequency to estimate the ionospheric code delay. Therefore, it is felt by the authors as an important task to investigate techniques aimed at sustaining multi-frequency performance when a multi constellation receiver installed in an aircraft is suddenly affected by radiofrequency interference, during critical phases of flight. This problem is identified for instance in [NATS, 2003]. Consequently, in this thesis, we investigate techniques to maintain dual frequency performances when a frequency is lost (L 1 C/A or E 1 OS for instance) after an interference occurrenceActuellement, on constate dans le domaine de la navigation, un besoin croissant de localisation par satellites. Apres une course a l'amelioration de la precision (maintenant proche de quelques centimetres grace a des techniques de lever d'ambiguite sur des mesures de phase), la releve du nouveau defi de l'amelioration de l'integrite du GNSS (GPS, Galileo) est a present engagee. L'integrite represente le degre de confiance que l'on peut placer dans l'exactitude des informations fournies par le systeme, ainsi que la capacite a avertir l'utilisateur d'un dysfonctionnement du GNSS dans un delai raisonnable. Le concept d'integrite du GNSS multi-constellation necessite une coordination au niveau de l'architecture des futurs recepteurs combines (GPS-Galileo). Le fonctionnement d'un tel recepteur dans le cas de passage du systeme multi-constellation en mode degrade est un probleme tres important pour l'integrite de navigation. Cette these se focalise sur les problemes lies a la navigation aeronautique multiconstellation et multi-systeme GNSS. En particulier, les conditions de fourniture de solution de navigation integre sont evaluees durant la phase d'approche APV I (avec guidage vertical). En disposant du GPS existant, du systeme Galileo et d'un systeme complementaire geostationnaire (SBAS), dont les satellites emettent sur des frequences aeronautiques en bande ARNS, la question fondamentale est comment tirer tous les benefices d'un tel systeme multi-constellation pour un recepteur embarque a bord d'un avion civil. En particulier, la question du maintien du niveau de performance durant cette phase de vol APV, en termes de precision, continuite, integrite et disponibilite, lorsque l'une des composantes du systeme est degradee ou perdu, doit etre resolue. L'objectif de ce travail de these est donc d'etudier la capacite d'un recepteur combine avionique d'effectuer la tache de reconfiguration de l'algorithme de traitement apres l'apparition de pannes ou d'interferences dans une partie du systeme GNSS multiconstellation et d'emettre un signal d'alarme dans le cas ou les performances de la partie du systeme non contaminee ne sont pas <b>suffisantes</b> pour continuer l'operation en cours en respectant les exigences de l'aviation civile. Egalement, l'objectif de ce travail est d'etudier les methodes associees a l'execution de cette reconfiguration pour garantir l'utilisation de la partie du systeme GNSS multi-constellation non contaminee dans les meilleures conditions. Cette etude a donc un interet pour les constructeurs des futurs recepteurs avioniques multiconstellation...|$|E
40|$|La prévision des inondations urbaines et de leur impact sur le milieu passe par la modélisation précise et lisible des flux inondants. Leur représentation est cependant rendue difficile par le caractère transitoire et multidirectionnel des écoulements, dans un milieu dont la géométrie est très irrégulière. Cet article traite plus spécifiquement du {{comportement}} du bâti africain vis à vis des écoulements, en situation inondante, et des lois de stockage et de vidange que l'on peut définir à différentes échelles représentatives de l'habitat : concession, bloc de concessions. Nous présentons trois propriétés du bâti nécessaires et <b>suffisantes</b> pour décrire le comportement hydraulique du milieu à ces échelles : sa pénétrabilité, sa stockabilité et sa transmissivité. L'étude du comportement hydraulique de l'objet bâti élémentaire, la concession, nous {{permet de}} relier ces propriétés à des caractéristiques géométriques de cet objet. Une approche agrégative conduit ensuite à définir un indicateur de la structure géométrique du bâti, l'HistoSeuil, équivalent à une densité d'ouvertures et caractéristique de la pénétrabilité du bâti. L'étude de sa pertinence géométrique, i. e. sa variabilité intra- et inter-quartiers a été réalisée dans le cas particulier de la ville de Ouagadougou (Burkina Faso); elle est basée sur le relevé systématique des ouvertures observables sur différentes façades de voiries de trois quartiers de types différents, (habitat individuel et spontané). Sa pertinence hydraulique, i. e. sa capacité à reproduire le comportement hydraulique moyen de l'objet urbain modélisé, est enfin abordée. Développée dans le contexte particulier de Ouagadougou, cette approche est généralisable à des configurations urbaines très diverses. Stormwater runoff generates {{one of the}} most critical natural risks in urban environments: impervious surfaces and high drainage network densities lead to frequent urban flooding events, with short process times and within small urban areas. In all parts of the world, urbanisation is growing, and urban flood hazards consequently occur more and more frequently. Examples of important flood damages suffered by urban populations are numerous, especially in tropical regions where the violence and rapidity of tropical storms often lead to an overloading of the drainage system and to the flooding of adjacent built-up areas. Prediction and evaluation of these damages require the determination of some important hydraulic characteristics of the flood, such as maximum water depth or flooding duration. Currently-used models are generally limited to checking the sewer system efficiency. Therefore new models are now expected to represent with accuracy and reliability the stormwater runoff, which can result from sewer system overloading. However, this kind of modelling is hard to carry out because of the geometric complexity of the urban media and because of the rapidity of urban storms and their associated flooding. Moreover, the modelling of the behaviour of the flooded built-up areas should sometimes be integrated into complete models of urban flooding, given their important influence on the hydrodynamics of the flood. However the geometric complexity of these built-up areas prevents us from a complete and accurate description of the different obstacles and water ways encompassed in such areas. Simplified descriptions at a larger scale are consequently to be found. This paper highlights the important physical characteristics that determine the hydraulic behaviour of every hydraulically-independent urban cell, and suggests a way to represent the exchange and storage laws of built-up areas at different scales: individual plots and blocks of plots. The study was performed in the particular case of Ouagadougou's areas. The hydraulic behaviour of every built-up area can be modelled with three important physical characteristics, the two first of which are related to the structure of the surrounding walls:- water perviousness : the ease with which the passing flood can enter or exit the plot. This is dependent upon the aperture density, which can vary according to the façade;- transmissivity: the ease with which water can pass through the plot. It depends on the perviousness of the different external or internal façades of the individual plot. If one façade is waterproof, the transmissivity becomes nil in the perpendicular direction;- storativity : determined with the storage capacity of an individual plot. It is a function of the internal surface area of the plot. These three characteristics are functions of height. Moreover, they are essential and sufficient to describe the behaviour of every basic or global urban object (plot, block of plots [...] .). At the "block of plots" level, the transmissivity and storativity concepts are comparable to the hydraulic roughness and urban porosity concepts that have already been proposed in scientific papers (e. g., Braschi et al. 1991). These two characteristics are sufficient for modelling the hydraulic behaviour of every open urban medium. Nevertheless, some urban media are non-transmissive because of a high connection level between the different obstacles. In these cases the perviousness property is very useful for modelling the different exchanges between the built-up areas and the adjacent flooded roads. The residential urban areas of Ouagadougou, used as an illustration for this study (Figure 1), correspond to this case of partitioned urban areas. The structure of the Ouagadougou's residential districts is standard and is organised around the individual plot, a parcel shielding one or several families: the individual plot is isolated from other plots and from the roads by a surrounding wall that constitutes one of the elementary hydraulic objects of the urban environment. The evolution of the flood water depth in a plot adjacent to a flooded road, determined by equation 1, depends on its floodable surface Sc and on its perviousness. Its perviousness is defined by the geometric characteristics of the apertures present in the wall: the type of aperture, weir or orifice; its height, hs, its length, Ls, and its opening if an orifice, a. Measures of exchanges between roads and plots made during some flood events in Ouagadougou (Hingray 1999) showed that the classical discharge laws for weirs or trough orifices can be used to model these exchanges (equations 2, 3 and 4). We suggest a way to simulate the exchange and storage laws of these built-up areas at a larger scale: the block of plots. An aggregation approach enables us to define a structure indicator: " l'HistoSeuil " (Figure 2). It is based on the description of the lengths of weirs and apertures found in road façades, and is equivalent to an aperture density function. The exchange discharge between the block of plots and the flooded adjacent road can be computed with a simply convolution (equation 6) between this HistoSeuil and the reference discharge laws for broad-crested weirs (equations 7 and 8). The geometric relevance of the indicator is next discussed: it seems to be a relatively stable geometric characteristic of an urban area (Figure 3). This result is given by a systematic survey of the apertures observed in 24 road façades belonging to 3 different districts of Ouagadougou. The two first are traditional residential districts, more and less developed. The "Patte d'Oie" district is fairly old and was established in the 1970 's (numerous well developed plots). The second one (Wemtenga 1) is a recent housing estate (1988) (numerous unfinished or empty plots). The final one (Wemtenga 2) is a very recent district of spontaneous development (disorganised built-up area structure). Furthermore, the hydraulic relevance of this indicator, its ability to reproduce the average hydraulic behaviour of a block of plots, is approached. Initial results seem to be positive. If both the hydraulic and geometric relevance of the structure indicator presented in this paper are validated by the additional work that we are carrying out at this present time, this approach may prove to be useful for the hydraulic modelling of built-up areas. Moreover the study of other types of built-up areas could lead to the determination of a hydraulic typology of urban areas. In particular, this study, performed in the case of Ouagadougou, a big city in a developing country, seems to be valid for every city where the built-up areas are highly partitioned...|$|E
40|$|Where an {{enterprise}} {{is able to}} anticipate its payments {{in such a manner}} as to precisely co-ordinate them with expected receipts, its treasury management is conducted in an optimal manner. In effect {{it will be possible to}} reduce costs to a minimum since the enterprise will neither need to hold near-cash assets in its treasury, with their low returns, nor retain an excessive cash balance in order to meet payments. However, the forecasts are of course, uncertain. By attaching to these treasury forecasts a probability, either relating to their timing or to their amount or both, the managers can estimate the risk of technical insolvency and from this deduce a margin of safety. This safety margin is made up of a combination of near-cash instruments and cash balances. The value of the cash balances and the level of receipts are interdependent. An enterprise that finances all of its working capital, after the deduction of operating costs, via long term capital will have less need of liquidity than one that uses short term borrowings, which are assumed to be intrinsically unstable. The choice of the means of financing working capital and the amount of near-cash assets retained are closely connected. According to classical theory the amount of long term funds used to finance working capital dictates the liquidity profile of the enterprise, its cash balances reflecting this. In this scenario, however, the greater the amount of long term funds the lower the profitability of the business, all other thing being equal. Similarly, to the extent that receipts are held in the cash balances the solvency of the enterprise is supported but the return on assets is reduced as a consequence. The objective of the manager is thus to define a compromise between liquidity and yield. Therefore, in order to enhance its safety margin {{an enterprise}} can only increase its net receipts or extend the maturity of its borrowings. These two strategies affect the profitability of the enterprise. This traditional rendition conducts the treasury manager to search for an optimal equilibrium that guarantees solvability. Financial equilibrium on the one hand, between inflows and outflows to the cash balances; monetary equilibrium on the other hand, between all the treasury's inflows and outflows, of which net receipts form the foundation. Actually, the issue in management of a treasury can be construed from a different perspective. Contrary to current opinion, an objective of maximising returns is not in conflict with that of maintaining a measure of liquidity. Optimisation consists of improving both the safety margin and the profitability, even within the context of a given treasury strategy. A treasury strategy reconciles the constraints of security and returns whilst minimising the allocation to cash assets. If an enterprise has net receipts that constantly fluctuate mildly around zero, all other things being equal, then the managers: are optimising liquidity movements, are efficiently allocating cash to profitable uses, understand exactly their financing needs, have optimised their banking relations. Without eradicating it completely, the uncertainty relating to receipts and expenditure can and must be considerably reduced and no longer justifies, in any circumstances, the retention of a “mattress” of liquidity. Awareness of the behaviour of cash flows that an enterprise is subject to, that is to say, fluctuations in the bank balances, is in the first instance, a problem of information. Every firm seeks this information. Emergency bank loans – advances and overdrafts – can be used for a relatively small amount and for a brief time to correct the forecasting errors relating to the synchronisation of the flow of funds. The rational use of such assistance requires specific attention to be paid to its adaptation to the need, taking account of the banking terms. The strategy of the treasury could in addition concern itself with the profitability of the various activities of the enterprise, which is the guarantee of long term liquidity. The activity of the enterprise raises the question of its solvability on a daily basis. Optimisation of the treasury function involves not only the coordination of the variations in the bank balances but also a surveillance of the events that generate the flow of funds: investment and its funding, the generation and application of liquidity, control of the fluctuations in the value of cash holdings. Economic literature accords a priority to the profitability criteria in financial strategy. Solvability appears only as a lower order problem that one attempts to resolve separately. Whereas in fact it is the quest to maintain the liquidity of the capital that is the primary objective and this subsumes that of profitability. This only becomes evident rarely, however, when credit is particularly expensive. Thus, all the financial concerns that derive from the operation of an enterprise are connected to importance of liquidity. The imperative of profitability is based upon the cost of holding capital. Every use implies a retention of funds, borrowed or not, for periods of different durations. However, the retention of funds involves costs: explicit costs every time the enterprise has to remunerate the providers of capital as well as opportunity costs corresponding to what the enterprise could have produced via an alternative usage. The operation of the enterprise is only justified if it extracts a sufficient amount from the employment of its resources in order to cover its costs. In other words, profitability is a prerequisite of the liquidity of the enterprise. Profitability and liquidity evolve in the same direction. Neither the retention of significant liquidity nor a collection of positive cash balances are a guarantee of security. Being solvable implies controlling every aspect of the evolution of the financial situation and involves a simultaneous maximisation of profitability and liquidity. Thus the optimal management of the treasury of an enterprise leads us to a consideration of all the financial problems faced by the firm. This comprehensive vision of the financial activity leads to a systematic approach of the treasury that consists of a dynamic response - to maintain the bank balances as close as possible to zero by optimising the funds flow, as well as a structural response – to control the replenishment of liquidity of the enterprise's capital by optimising liquidity flows. Lorsqu'une entreprise est capable de prévoir ses échéances de manière à les faire correspondre exactement à ses prévisions d'encaissements la gestion de la trésorerie est optimale. En effet, les coûts pourront être réduits au minimum puisqu'elle n'aura ni besoin de détenir des actifs liquides de faible rendement en trésorerie, ni de disposer d'un fonds de roulement pléthorique. Toutefois, les prévisions sont dans la réalité, incertaines. En associant aux prévisions de trésorerie une probabilité, soit en date, soit en montant, ou les deux, les responsables peuvent estimer le risque d'insolvabilité technique et en déduire une marge de sécurité. Cette marge de sécurité se compose d'un volant de liquidités et du fonds de roulement. La valeur du fonds de roulement et le niveau de l'encaisse sont interdépendants. Une entreprise qui finance en totalité ses actifs circulants nets des dettes d'exploitation par des capitaux à long terme aura moins besoin de liquidités que si elle les avait financés avec du crédit à court terme, supposé instable par nature. Le choix des moyens de financement des actifs circulants et la part des actifs liquides à maintenir sont étroitement liés. Selon la conception classique le volume des fonds permanents affectés au financement de l'actif circulant conditionne la liquidité de l'entreprise : le fonds de roulement est l'expression de cette liquidité. Or, plus l'immobilisation de fonds est importante et moins l'affaire est rentable, toutes choses égales par ailleurs. De même, plus l'encaisse détenue est forte plus la sécurité de l'entreprise est assurée, mais le rendement des actifs en est affaibli d'autant. L'objectif du responsable financier est donc de définir un équilibre entre liquidité et rentabilité. Ainsi, pour accroître sa marge de sécurité une entreprise ne peut qu'augmenter la proportion de son encaisse, ou allonger la durée de son endettement. Ces deux actions affectent la rentabilité de l'entreprise. Cette conception traditionnelle aboutit au niveau de la gestion de la trésorerie à rechercher un équilibre optimum gage de solvabilité. Equilibre financier, d'une part, entre les emplois et les ressources dont le fonds de roulement est le critère essentiel d'analyse. Equilibre monétaire, d'autre part, entre les flux d'entrée et de sortie de liquidités dont l'encaisse est la garantie. En fait, le problème de la gestion de la trésorerie se pose en d'autres termes. Contrairement à l'opinion courante l'objectif de rentabilité ne s'oppose pas au maintien de la liquidité. L'optimum réside dans l'amélioration conjointe de la sécurité et du profit, contenu même d'une politique de trésorerie. La politique de trésorerie concilie les contraintes de sécurité et de rentabilité en minimisant le volume de l'actif monétaire. Une entreprise dont l'encaisse fluctue en permanence légèrement autour de zéro indique, toutes choses égales par ailleurs, que les responsables : maîtrisent les flux de liquidités, réaffectent efficacement le cash-flow dans des emplois rentables, apprécient au plus juste leurs besoins de financement, négocient au mieux leurs conditions de banque. Sans disparaître totalement l'incertitude relative aux encaissements et aux décaissements peut et doit être sérieusement réduite et ne justifie plus, en tout état de cause, la détention d'un « matelas » de liquidités. La connaissance du comportement des flux monétaires qui transitent par l'entreprise, c'est-à-dire des variations du solde bancaire, est avant tout un problème d'information. La recherche de cette information est à la portée de toutes les firmes. Les crédits bancaires de « dépannage » - escompte et découverts - viennent éventuellement pour une faible part et un temps très court, corriger les erreurs de prévision relatives à la synchronisation des entrées et des sorties de fonds. L'utilisation rationnelle de ces concours nécessite une attention toute particulière de façon à les adapter aux besoins, compte tenu des conditions de banque. La politique de trésorerie doit en outre se préoccuper de la rentabilité des activités, gage de la liquidité à terme. L'activité de l'entreprise remet quotidiennement en question sa solvabilité. La maîtrise de la trésorerie passe non seulement par le contrôle des variations du solde bancaire mais aussi par la surveillance des faits générateurs des flux monétaires : l'investissement et son financement, la formation et l'affectation des flux de liquidités, le contrôle de la variation de la valeur de la monnaie. La littérature économique donne la place essentielle au critère de rentabilité dans la politique financière. La solvabilité n'apparaît que comme un « sous-problème » que l'on tente de résoudre séparément. Or, la recherche et le maintien de la liquidité du patrimoine est en réalité l'objectif prioritaire qui englobe celui de rentabilité. Cette évidence ne devient véritable contrainte que dans les périodes de rareté et de cherté de l'argent. Ainsi, toutes les préoccupations financières qui découlent elles-mêmes du fonctionnement de l'entreprise se rattachent à la nécessaire « liquidité ». L'impératif de rentabilité a pour origine le coût de détention des capitaux. Tout emploi implique une immobilisation de fonds, propres ou empruntés, pour une durée plus ou moins longue. Or toute immobilisation de fonds entraîne des coûts : coûts explicites chaque fois que l'entreprise doit assurer la rémunération des apporteurs de capitaux et coûts d'opportunité correspondants au produit que l'entreprise aurait pu tirer de l'emploi alternatif. L'activité d'entreprise ne se justifie que si elle dégage de ses emplois des ressources <b>suffisantes</b> pour couvrir ses coûts. Autrement dit, la rentabilité est la condition de la liquidité de l'entreprise. Rentabilité et liquidité varient dans le même sens. Ni la détention de liquidités importantes, ni un fonds de roulement positif ne sont une garantie de sécurité. Etre solvable signifie contrôler dans tous ses aspects l'évolution de la situation financière et se traduit par une maximisation simultanée de la rentabilité et de la liquidité. Ainsi la gestion optimale de la trésorerie des entreprises nous conduit à une synthèse de tous les problèmes financiers qui se posent à la firme. Cette vision globale de l'activité financière conduit à une approche système de la trésorerie qui se décline en une action conjoncturelle, dont l'objectif est de maintenir le solde bancaire le plus proche de zéro par la maîtrise des flux d'entrée et de sortie de fonds et une action structurelle, dont l'objectif est de contrôler le potentiel de reconstitution des liquidités du patrimoine de l'entreprise par la maîtrise de la formation et de l'affectation du flux de liquidité...|$|E
40|$|My {{research}} lies at {{the interface}} of Riemannian, contact, and symplectic geometry. It deals {{with the construction of}} Kähler and Sasaki-Einstein metrics, with the study of conformal Hamiltonian systems, the geometry of cosphere bundles, and proper Lie groupoids. The main theme of this thesis is the study of applications of Lie symmetries in differential geometry and dynamical systems. The first chapter of the thesis studies the singular reduction of cosphere fiber bundles. The copshere bundle of a differentiable manifold M (denoted by S^*(M)) is the quotient of its cotangent bundle without the zero section with respect to the action by multiplications of ^+ which covers the identity on M. It is a contact manifold which has the same privileged position in contact geometry that cotangent bundles have in symplectic geometry. Using a Riemannian metric on M, we can identify S^*(M) with its unitary tangent bundle and its Reeb vector field with the geodesic field on M. If M is endowed with the proper action of a Lie group G, the lift of this action on S^*(M) respects the contact structure and admits an equivariant momentum map J. We study the topological and geometrical properties of the reduced space of S^*(M) at zero momentum, i. e. (S^*(M)) _ 0 :=J^- 1 (0) /G. Thus, we generalize the results of dragulete [...] ornea [...] ratiu to the singular case. Applying the general theory of contact reduction developed by Lerman and Willett in lerman [...] willett and willett, one obtains contact stratified spaces that lose all information of the internal structure of the cosphere bundle. Even more, the cosphere bundle projection to the base manifold descends to a continuous surjective map from (S^*(M)) _ 0 to M/G, but it fails to be a morphism of stratified spaces if we endow (S^*(M)) _ 0 with its contact stratification and M/G with the customary orbit type stratification defined by the Lie group action. Based on the cotangent bundle reduction theorems, both in the regular and singular case, as well as regular cosphere bundle reduction, one expects additional bundle-like structure for the contact strata. To solve these problems, we introduce a new stratification of the contact quotient at zero, called the C-L stratification (standing for the coisotropic or Legendrian nature of its pieces). It is compatible with the contact stratification of (S^*(M)) _ 0 and the orbit type stratification of M/G. It is also finer than the contact stratification. Also, the natural projection of the C-L stratified quotient space (S^*(M)) _ 0 to its base space, stratified by orbit types, is a morphism of stratified spaces. Each C-L stratum is a bundle over an orbit type stratum of the base and it {{can be seen as a}} union of C-L pieces, one of them being open and dense in its corresponding contact stratum and contactomorphic to a cosphere bundle. Hence we have identified the maximal strata endowed with cosphere bundle structure. The other strata are coisotropic or Legendrian submanifolds in the contact components that contain them. Consequently, we can perform a complete geometric and topological analysis of the reduced space. We also study the behaviour of the projection on (S^*(M)) _ 0 of the Reeb flow (geodesic flow). The set of contact Hamiltonian vector fields (the analogous of Hamiltonian vector fields in symplectic geometry) form the "Lie" group of the algebra of contact transformations. In the first chapter we also present the reduction of contact systems (which locally are in bijective correspondence with the non-autonomus Hamilton-Jacobi equations) and time dependent Hamiltonian systems. In the second chapter of this thesis we study quotients of Kähler and Sasaki-Einstein manifolds. We construct a reduction procedure for symplectic and Kähler manifolds (endowed with symmetries generated by a Lie group) which uses the ray pre-images of the associated momentum map. More precisely, instead of considering as in the Marsden- Weinstein reduction (point reduction) the pre-image of a momentum value μ, we use the pre-image of ^+μ, its positive ray. We have three reasons to develop this construction. One is geometric: the construction of canonical reduced spaces of Kähler manifolds corresponding to a non zero momentum. By canonical we mean that the reduced Kähler structure is the projection of the initial Kähler structure. The point reduction (Marsden-Weinstein) given by M_μ:=J^- 1 (μ) /G_μ, where μ is a value of the momentum map J and G_μ the isotropy subgroup of μ with respect to the coadjoint action of G is not always well defined in the Kähler case (if G≠ G_μ). The problem is caused by the fact that the complex structure of M does not leave invariant the horizontal distribution of the Riemannian submersion which projects J^- 1 (μ) on M_μ. The solution proposed in the literature uses the reduced space at zero momentum of the symplectic difference of M with the coadjoint orbit of μ endowed with a unique Kähler-Einstein form (constructed, for insatnce, in besse, Chapter 8) and different from the Kostant-Kirillov-Souriau form. The uniqueness of the form on the coadjoint orbit ensures that the reduced space is well defined. On the other hand, not using the Kostant-Kirillov-Souriau form implies the fact that the reduced space is no longer canonical. The ray reduced space that we construct is canonical and can be defined for any momentum. It is the quotient of J^- 1 (^+μ) with respect to a certain normal subgroup of G_μ. The second reason is an application to the study of conformal Hamiltonian systems (see mclachlan [...] perlmutter). They are mechanical, non-autonomous systems with friction whose integral curves preserve, in the case of symmetries, the ray pre-images of the momentum map, but not the point (momentum) preimages of the Marsden-Weinstein quotient. We extend the notion of conformal Hamiltonian vector field by showing that one can thus include in this study new mechanical systems. Also, we present the reduction of conformal Hamiltonian systems. The third reason consists of finding the necessary and sufficient conditions for the ray reduced spaces of Kähler (Sasakian) -Einstein manifolds to be also Kähler (Sasakian) -Einstein. We deal with this problem in the second chapter of the thesis, in dragulete [...] ornea, and in dragulete [...] doi where we use techniques of A. Futaki. Thus, we can construct new Sasaki-Einstein structures. As examples of symplectic (Kähler) and contact (Sasakian) ray quotients we treat the case of cotangent and cosphere bundles and show that they are universal spaces for ray reductions. Examples of toric actions on spheres are also described. The third chapter of my thesis studies the space of orbits of a proper Lie groupoid. In weinstein [...] unu, weinstein [...] doi A. Weinstein has partially solved the problem of linearization of proper groupoids. In zung, N. T. Zung has completed it by showing a theorem of Bochner type for proper groupoids. Using ideas from foliation theory and the slice (linearization) theorem of Weinstein and Zung, we prove a stratification theorem for the orbit space of a proper groupoid. We show explicitely that the orbital foliation of a proper Lie groupoid is a Riemannian singular foliation in the sense of Molino. For all these we have two motivations. On one hand we want to prove that there is an equivalence between proper groupoids and orbispaces (the spaces which are locally quotients with respect to an action of a compact Lie group). On the other hand we would like to study the reduction of infinitesimal actions (actions of Lie algebras) which are not integrable to Lie group actions. These actions and their integrability have been studied, among others, by Palais (palais), Michor, Alekseevsky. Mes recherches se situent à l'interface de la géométrie Riemannienne et des géométries de contact et symplectique et portent sur la construction des métriques Kähler ou Sasakie-Einstein, sur l'étude des systèmes Hamiltonians conformes, la géométrie des fibrés cosphériques et les groupoïdes de Lie propres. Le thème principal de cette thèse est l'étude des applications des symétries Lie en géométrie différentielle et systèmes dynamiques. Le premier chapitre de cette thèse étudie la réduction singulière des symétries du fibré cosphérique, les propriétés conservatives des systèmes de contact et leurs réduction. Le fibré cosphérique d'une variété différentiable M (dénoté par S^*(M)) est le quotient de son fibré cotangent sans la section nulle par rapport à l'action par multiplication de ^+ qui couvre l'identité sur M. C'est une variété de contact qui détient en géométrie de contact la position analogue du fibré cotangent en géométrie symplectique. En utilisant une métrique Riemannienne sur M, on peut identifier S^*(M) avec son fibré tangent unitaire et son champ de Reeb avec le champ géodésique de M. Si M est munie de l'action propre d'un groupe de Lie G, le relèvement de cette action à S^*(M) respecte la structure de contact et admet une application moment équivariante J. Nous étudions les propriétés topologiques et géométriques de l'espace réduit à moment zéro de S^*(M), i. e. (S^*(M)) _ 0 :=J^- 1 (0) /G. Ainsi, nous généralisons les résultats de dragulete [...] ornea [...] ratiu au cas singulier. Appliquant la théorie générale de réduction de contact, théorie dévéloppée par Lerman et Willett dans lerman [...] willett et willett, on obtient des espaces qui perdent toute information sur la structure interne du fibré cosphérique. En plus, la projection du fibré cosphérique sur sa base descend à une surjection continue de (S^*(M)) _ 0 à M/G, mais qui n'est pas un morphisme d'espaces stratifiés si on munit l'espace réduit avec sa stratification de contact et l'espace de base avec la stratification standarde de type orbitale définie par l'action du groupe de Lie. Compte tenu des théorèmes de réduction du fibré cotangent (cas régulier et singulier) et du fibré cosphérique (cas régulier), on s'attend à ce que les strates de contact aient une structure fibrée additionnelle. Pour résoudre ces problèmes, nous introduisons une nouvelle stratification de (S^*(M)) _ 0, nommée la stratification C-L (les deux majuscules symbolisent la nature coisotrope ou Legendréenne de leurs strates). Elle est compatible avec la stratification de contact de (S^*(M)) _ 0 et la stratification de type orbital de M/G. Aussi, elle est plus fine que la stratification de contact et rend la projection de (S^*(M)) _ 0 sur M/G un morphism d'espaces stratifiés. Chaque strate C-L est un fibré sur une strate de type orbital de M/G et elle peut être vue comme une union de strates C-L, une d'entre elles étant ouverte et dense dans la strate de contact correspondante et difféomorphe à un fibré cosphérique. Ainsi, nous avons identifié les strates maximales munies de structure de fibrés cosférique. Les autres strates sont des sous-variétés coisotropes ou Legendre dans les composantes de contact qui les contiennent. Par conséquant nous faison une analyse géométrique et topologique complète de l'espace réduit. Nous analysons aussi le comportement de la projection sur (S^*(M)) _ 0 du flot de Reeb (flot géodésique). L'ensemble de champs de vecteurs de contact (les analogues des champs de vecteurs Hamiltonians en géométrie symplectique) forment le "groupe de Lie" de l'algèbre des transformations de contact. Dans le premier chapitre nous présentons aussi la réduction des systèmes de contact (qui, localement, sont en correspondence bijective avec les équations non-autonomes de Hamilton-Jacobi) et les systèmes Hamiltonians dépendants de temps. Dans le deuxième chapitre nous étudions les propriétés géométriques des quotients de variétés Sasaki et Kähler. Nous construisons une procédure de réduction pour les variétés symplectiques et Kähler (munies de symétries générées par un groupe de Lie) qui utilise les préimages rayon de l'application moment. Précisémmant, au lieu de considérer comme dans la réduction de Marsden-Weinstein (ponctuelle) la préimage d'une valeur moment μ, nous utilisons la préimage de ^+μ, le rayon positif de μ. Nous avons trois motivations pour développer cette construction. Une est géométrique: la construction des espaces réduits de variétés Kähler correspondant á un moment non nulle qui soient canoniques dans le sense que la structure Kähler réduite est la projection de la structure Kähler initiale. La réduction ponctuelle (Marsden-Weinstein) donnée par M_μ:=J^- 1 (μ) /G_μ où μ est une valeur de l'application moment J et G_μ est le sous-groupe d'isotropie de μ par rapport à l'action coadjointe de G n'est pas toujours bien définie dans le cas Kähler (si G≠ G_μ). Le problème est causé par le fait que la structure complexe de M ne préserve pas la distribution horizontale de la submersion Riemannienne qui projète J^- 1 (μ) sur M_μ. La solution proposée dans la litterature utilise l'espace réduit à moment zéro de la difference symplectique de M avec l'orbite coadjointe de μ munie d'une forme Kähler-Einstein unique (construite par exemple dans besse, Chapitre 8) et différente de la forme de Kostant-Kirillov-Souriau. L'unicité de la forme sur l'orbite coadjointe garantit un espace réduit bien défini. Par contre, ne plus utiliser la forme de Kostant-Kirillov-Souriau entraîne le fait que l'espace réduit n'est plus canonique. L'espace réduit rayon que nous construisons est canonique et peut être défini pour tout moment. Il est le quotient de J^- 1 (^+μ) par rapport à un certain sous-groupe normal de G_μ. La deuxième raison est une application à l'étude des systèmes Hamiltonians conformes (voir mclachlan [...] perlmutter). Ce sont des systèmes mécaniques non-autonomes, avec friction dont les courves intégrales préservent, dans le cas des symétries, les préimages rayons de l'application moment. Nous extendons la notion de champ Hamiltonian conforme, en montrant qu'on peut ainsi inclure dans cet étude de nouveaux systèmes mécaniques. également, nous présentons la réduction de systèmes Hamiltonians conformes. La troisième raison consiste à trouver des conditions necéssaires et <b>suffisantes</b> pour que les espaces réduits (rayons) des variétés Kähler (Sasakian) -Einstein soient aussi Kähler (Sasakian) -Einstein. Nous nous occupons de cela dans le deuxième chapitre de la thèse, dans dragulete [...] ornea et dans dragulete [...] doi où nous utilisons des techniques de A. Futaki. Ainsi, nous pouvons construire de nouvelles structures de Sasaki-Einstein. Comme exemples de réductions rayon symplectic (Kähler) et contact (Sasaki) nous traitons le cas des fibrés cotangent et cosphérique. Nous montrons qu'ils sont des espaces universels pour la réduction rayon. Des exemples d'actions toriques sur des sphères sont aussi décrits. Le troisième chapitre de cette thèse traite l'étude de l'espace des orbites d'un groupoïde propre. Dans weinstein [...] unu, weinstein [...] doi A. Weinstein a partiellement résolu le problème de la linéarisation des groupoïdes propres. En zung, N. T. Zung l'a achevé en démontrant un théorème de type Bochner pour les groupoïdes propres. Nous prouvons un théorème de stratification de l'espace d'orbites d'un groupoïde propre en utilisant des idées de la théorie des foliations et le théorème de "slice" (linéarisation) de Weinstein et Zung. Nous montrons explicitement que le feuilletage orbital d'un groupoïde propre est un feuilletage Riemannien singulier dans le sense de Molino. Pour cela nous avons deux motivations. D'un côté nous voulons montrer qu'il y ait une équivalence entre groupoïdes propres et "orbispaces" (des espaces qui sont localement des quotiens par rapport à l'action d'un groupe de Lie compact) et d'un autre nous voulons étudier la réduction des actions infinitésimales (actions d'algèbres de Lie) qui ne sont pas intégrables à l'action d'un groupe de Lie. Ces actions et leur intégrabilité ont été étudiées, entre autres, par Palais (palais), Michor, Alekseevsky...|$|E
40|$|Among {{the various}} natural hazards, mass {{movements}} (MM) {{are probably the}} most damaging to the natural and human environment in the Mediterranean countries, including Lebanon which represents a good case study of mountainous landscape. Although affecting vast areas in the country, the phenomenon was not studied at regional scale, and related maps are still lacking. Therefore, this research deals with the use of remote sensing and geographic information system (GIS) techniques in studying MM in Lebanon. In this context, the first part reviews existing knowledge on the topics of mass movements (MM) specifically in the Mediterranean region, and defines research gaps. It exposes the diverse types of MM, their magnitudes, the causative agents and their bad consequences. It clarifies confusions related to MM-terms (hazard, susceptibility, risk, etc.), and compares the efficiencies of the most used methods for MM susceptibility/hazard zonation. It includes also a statement on remote sensing and GIS benefits and constraints in mass movement studies, pointing out possible ways of research. The second part is dedicated to the detailed description of the study area "the Mediteranean slopes of central to north Lebanon" within Lebanon. Physical/morphodynamic and socioeconomic characteristics of the area are exposed, as well as the natural hazards, MM events, their socio-economic impacts and mitigation measures. All previous studies about MM hazard in Lebanon are reviewed. The studied area, extending from the Mediterranean coast to around 3000 m elevation, covers ~ 36 % of the total area of Lebanon. It represents the geoenvironmental diversity of this country in terms of geology, soil, hydrography, land cover and climate. It is characterized by problematic human activities (e. g., chaotic urban expansion, artificial recharge of groundwater, overgrazing, forest fire) enhancing environmental decline and inducing MM, with minimal government control. The third part compares the applicability of different satellite sensors (Landsat TM, IRS, SPOT 4) and preferred image processing techniques (False Color Composite "FCC", Pansharpen, Principal component analysis "PCA", Anaglyph) for the mapping of MM recognized as landslides, rock/debris falls and earth flows. Results from the imagery have been validated by field surveys and analysis of IKONOS imagery (1 m) acquired in some locations witnessing major MM during long periods. Then, levels of accuracies of detected MM from satellite imageries were plotted. This study has demonstrated that the anaglyph produced from the two panchromatic stereo-pairs SPOT 4 images remains the most effective tool setting the needed 3 -D properties for visual interpretation and showing maximum accuracy of 69 %. The PCA pan-sharpen Landsat TM-IRS image gave better results in detecting MM, among other processing techniques, with maximum accuracy level of 62 %. The errors in interpretation fluctuate not only according to the processing technique, but also due to the difference in MM type. They are minimal once 3 D anaglyph SPOT 4 is considered, varying between 31 % (landslides), 36 % (rock and debris falls) and reaching 46 % in the case of earth and debris flows. The fourth part explores relationships between MM occurrence and different factor terrain parameters. Parameters expressed by: 1 - preconditioning factors, like: elevation, slope gradient, slope aspect, slope curvature, lithology, proximity to fault line, karst type, distance to quarries, soil type, distance to drainage line, distance to water sources, land cover/use, and proximity to roads, and 2 - triggering MM factors, like: rainfall quantity, seismic events,floods and forest fires, were correlated with MM using GIS-approaches. This study indicates, depending on bivariate remote sensing and GIS statistical correlations (Kendall Tau-b correlation), that lithology is the most influencing on MM occurrence, having the highest correlation with other parameters (i. e. 7 times correlated at 1 % level of significance and 3 times at 5 %). It also shows that statistical correlations to mass movements exist best between parameters at the following decreasing order of importance: soil type/distance to water sources (acting similarly on MM occurrence), karst/distance to quarries/land cover-use, proximity to faults, slope gradient/proximity to roads/floods, seismic events, elevation/slope aspect/forest fires. These correlations were verified and checked through field observations and explained using univariate statistical correlations. Therefore, they could be extrapolated to other Mediterranean countries having similar geoenvironmental conditions. The fifth part proposes a mathematical decision making method - Valuing Analytical Bi- Univariate (VABU) that considers two-level weights for mapping MM susceptibility/hazard (1 : 50, 000 cartographic scale) within the study area. The reliability of this method is examined through field surveys and depending on a GIS comparison with other statistical methods - Valuing accumulation Area (VAA) (depending on one weight level) and Information Value (InfoVal) (requiring detailed measurements of MM areas). Three susceptibility maps were derived using preconditioning parameters, while hazard maps were produced from triggering ones. The coincidence values of overlapping susceptibility maps were found to be equal to 47. 5 % (VABU/VAA), 54 % (VABU/InfoVal) and 38 % (VAA/InfoVal). The agreement between hazard maps showed closer values than susceptibility ones, oscillating between 36. 5 % (VAA/InfoVal), 39 % (VABU/VAA), and 44 % (VABU/InfoVal). Field verification indicates that the total precision of the produced susceptibility maps ranges from 52. 5 % (VAA method), 67. 5 % (InfoVal method) and 77. 5 % (VABU method). This demonstrates the efficiency of our method, which consequently can be adopted for predictive mapping of MM susceptibility/hazard in other areas in Lebanon and may be easily extrapolated using the functional capacities of GIS. The sixth part predicts the geographic distribution and volume of block falls (m 3) across the study area using GIS decision-tree modelling. Such mapping was unavailable in Lebanon, but also in many other countries putting effort on landslide research rather than other types of MM. Several decision-tree models were developed using (1) all terrain parameters, (2) topographic parameters only, (3) geologic parameters only, and adopting various processing techniques (pruned and unpruned trees). The best regression tree model combined all parameters and explained 80 % of the variability in field blocks falls' measurements. The unpruned model built using four geological parameters (lithology, soil type, proximity to fault line, and karst type) seems also interesting, classifying 68 % of block falls and referring to a small amount of input data (4 parameters). The produced predictive quantitative block falls' map at 1 : 50, 000 appears extremely useful for decision-making, helping adoption of mitigation measures to reduce the occurrence of harmful block falls. The seventh part focuses on monitoring MM activity through integrating space borne radar data and Global Positioning System (GPS) techniques. ERS radar imageries were processed using InSAR and permanent scatters techniques. The analysis showed difficulties in detecting ground deformations due to MM. Nevertheless, the analysis is still in its preliminary stage and future planned work will take into consideration other manipulating procedures for detecting the displacements. On the other hand, a GPS installation in Hammana area; one of the Lebanese villages lying in a major landslide, was conducted. Two campaigns were raised, but results are still lacking since there is not enough data accumulation. More observations are still needed to build up a comprehensive picture on the direction and velocity of the movement. Parmi les aléas naturels, les mouvements de terrain (MT) sont probablement les plus nuisibles à l'environnement naturel et humain, notamment dans les pays méditerranéens, incluant le Liban qui représente un bon cas d'étude de région montagneuse. Ce phénomène n'a pas été étudié à l'échelle régionale bien qu'il affecte de vastes zones dans ce pays, et les cartes d'aléa manquent encore. La recherche présentée ici est consacrée à l'utilisation des techniques de télédétection et des systèmes d'informations géographiques (SIG), pour l'étude des MT au Liban. La première partie passe en revue les connaissances existantes sur le thème des mouvements de terrain (MT), plus spécifiquement dans la région méditerranéenne, et définit les lacunes de recherche. Elle expose les divers types existants de MT, leurs magnitudes, les agents causatifs, et leurs effets. Elle clarifie la terminologie utilisée pour les MT (aléa, susceptibilité, risque, etc.), et compare les méthodes les plus utilisées pour la cartographie de l'aléa/susceptibilité aux MT. Elle présente aussi un état des avantages et problèmes de la télédétection et du SIG dans les études de mouvements de terrain, en insistant sur les voies possibles de recherche. La deuxième partie est consacrée à la description détaillée de la région d'étude qui couvre les versants méditerranéens du nord du Liban central. Les caractéristiques physiques/morphodynamiques et socio-économiques de cette région sont exposées, ainsi que les aléas naturels, les événements de MT, les impacts socio-économiques et les mesures de conservation. Toutes les études sur l'aléa MT au Liban sont revisitées. La région d'étude, s'étendant de la côte méditerranéenne jusqu'à 3000 m d'altitude, couvre à peu près 36 % de la superficie totale du Liban. Elle est représentative de la diversité géo-environnementale de ce pays en termes de géologie, sol, hydrographie, occupation du sol et climat. Elle se caractérise par des activités humaines problématiques (par exemple une expansion urbaine chaotique, la recharge artificielle des eaux souterraines, un surpâturage, des incendies de forêt), accroissant la dégradation de l'environnement et induisant les MT, avec un contrôle gouvernemental minime. La troisième compare l'efficacité de différents capteurs satellitaires à résolutions variées (Landsat TM, IRS, SPOT 4) et diverses techniques de traitement d'image (composition colorée, fusion, analyse en composantes principales ACP, vision stéréoscopique) pour la détection visuelle des mouvements de terrain classés en glissements, éboulements de blocs rocheux et de débris, et coulées de boue. Les résultats ont été validés sur le terrain et en analysant des images IKONOS (1 m) acquises en certaines localités menacées par des MT sur de longues périodes. Ensuite, les niveaux de précision de la détection des MT à partir des images satellitaires ont été calculés. Cette étude a montré que l'anaglyphe produit à partir des images panchromatiques stéréo SPOT 4 reste l'outil le plus efficace grâce aux caractéristiques 3 D jouant un rôle essentiel dans l'interprétation visuelle et montrant un niveau de précision (pourcentage des MT détectés et vérifiés sur le terrain) maximal de 69 %. De plus, l'image de fusion Landsat TM-IRS, calculée par ACP, fournit des résultats de détection des MT meilleurs que les autres techniques, avec un niveau de précision de 62 %. Les erreurs d'interprétation fluctuent non seulement en fonction de la technique de traitement utilisée, mais aussi en fonction des types de MT. Elles sont minimes quand l'anaglyphe (3 D) SPOT 4 est pris en considération, variant de 31 % (glissements), 36 % (éboulements de blocs rocheux et de débris) à 46 % dans le cas des coulées de boue. La quatrième partie explore les relations entre l'occurrence de MT et les paramètres du terrain. Ces paramètres sont: 1 - les facteurs de prédisposition, comme l'altitude, la pente en gradient, l'aspect de pente, la courbure de pente, la lithologie, la proximité aux failles, le type de karst, la distance aux carrières, le type de sol, la distance aux réseaux de drainage, la distance aux sources, l'occupation/utilisation du sol et la proximité aux routes, et 2 - les facteurs déclenchants, comme la quantité de pluies, les événements sismiques, les inondations et les incendies de forêt, qui ont été corrélés avec les MT en utilisant les approches SIG. Cette étude montre, en se basant sur les corrélations statistiques bi-variées satellitaires et SIG (corrélation Kendal Tau-b), que la lithologie est ce qui influence le plus l'occurrence des MT, puisqu'elle a la corrélation la plus élevée avec les autres paramètres (7 fois corrélée à un niveau de signification de 1 %, et 3 fois à 5 %). Elle montre aussi que les corrélations statistiques entre ces paramètres et les mouvements de terrain existent suivant l'ordre d'importance décroissant suivant : type de sol/distance aux sources (agissant de manière similaire sur l'occurrence des MT), karst/distance aux carrières/occupation/utilisation du sol, proximité aux failles, gradient de pente/proximité aux routes/inondations, événements sismiques, altitude/aspect de pente/incendies de forêt. Ces corrélations sont vérifiées sur le terrain et expliquées en utilisant des corrélations statistiques uni-variées. Par conséquent, elles peuvent être extrapolées à d'autres pays méditerranéens caractérisés par des conditions géoenvironnementales similaires. La cinquième partie propose une méthode mathématique décisionnelle (méthode analytique bi-univariée d'évaluation ou "Valuing Analytical Bi-Univariate (VABU) ") qui considère deux niveaux de pondération pour la cartographie de l'aléa/susceptibilité des MT (échelle 1 / 50000) dans la région d'étude. La fiabilité de cette méthode est examinée sur le terrain et en la comparant avec d'autres méthodes statistiques - Valuing accumulation Area (VAA) (un seul niveau d'évaluation) and Information Value (InfoVal) (nécessitant des mesures détaillées des MT). Trois cartes de susceptibilité sont dérivées en utilisant les facteurs conditionnant l'occurrence des MT, tandis que les cartes d'aléa sont produites à partir des facteurs déclenchants. Les valeurs de coïncidence de superposition des cartes de susceptibilité sont de 47, 5 % (VABU/VAA), 54 % (VABU/InfoVal) et 38 % (VAA/InfoVal), respectivement. L'accord entre les cartes d'aléas montre des valeurs proches de celles des cartes de susceptibilité, variant entre 36, 5 % (VAA/InfoVal), 39 % (VABU/VAA), et 44 % (VABU/InfoVal). La validation sur le terrain indique que la précision totale des cartes de susceptibilité produites varie entre 52, 5 % (méthode VAA), 67, 5 % (méthode InfoVal) et 77, 5 % (méthode VABU). Cela démontre l'efficacité de notre méthode qui peut être adoptée pour une cartographie prédictive de l'aléa et de la susceptibilité des MT dans d'autres régions au Liban, et peut être aussi aisément extrapolée en utilisant les capacités fonctionnelles du SIG. La sixième partie prédit la distribution géographique et le volume des blocs rocheux (m 3) dans la région d'étude en utilisant la modélisation suivant un arbre décisionnel. Une telle cartographie est indisponible au Liban, mais aussi dans d'autres pays qui portent plutôt leur effort sur la recherche des glissements plutôt que les autres types de MT. Plusieurs modèles d'arbres décisionnels ont été développés en utilisant, (1) tous les paramètres de terrain, (2) les paramètres topographiques uniquement, (3) les paramètres géologiques, et en adoptant plusieurs techniques de traitement. Le meilleur arbre de régression combine tous les paramètres et explique 80 % de la variabilité dans les mesures des blocs rocheux sur le terrain. Le modèle construit en utilisant les quatre paramètres géologiques (lithologie, type de sol, proximité aux failles et type de karst) parait aussi intéressant car il classe 68 % des blocs rocheux tout en se référant à un petit nombre de données d'entrée (4 paramètres). La carte produite de 'prédiction quantitative des blocs rocheux' à l'échelle du 1 / 50 000 apparait extrêmement utile pour la décision, aidant à l'adoption des mesures de conservation afin de réduire l'occurrence de movements nuisibles de blocs rocheux. La septième partie s'intéresse à la surveillance de l'activité des MT à travers l'intégration des données spatiales radar et des techniques GPS (Système de positionnement global). Les données radar ERS sont traitées en utilisant les techniques InSAR et des réflecteurs permanents. Cette analyse montre des difficultés pour la détection des MT. Cependant, elle est jusqu'à présent préliminaire, et un plan de travail futur prendra en considération d'autres traitements pour la détection des déplacements. D'un autre côté, une installation GPS a été effectuée dans la région de Hammana, un village libanais menacé par un grand glissement. Deux campagnes ont été rassemblées, mais les résultats manquent encore puisqu'il n'y a pas des données accumulées <b>suffisantes.</b> Plus d'observations sont nécessaires afin de construire une représentation compréhensive de la direction et de la vitesse du mouvement...|$|E
40|$|The {{study of}} {{turbulent}} flows {{has always been}} a challenge for scientists. Turbulent flows are common in nature and have an important role in several geophysical processes related to a variety of phenomena such as river morphology, landscape modeling, atmospheric dynamics and ocean currents. At present, new measurement and observation techniques suitable for fieldwork can be combined with laboratory and theoretical work in order to advance in the understanding of river processes. In this Ph. D. dissertation, an Acoustic Doppler Velocity Profiler (ADVP) suspended from a deployable structure allowed the investigation of turbulent gravel-bed river flows. The ADVP, which was developed by the Laboratoire d'Hydraulique Environnementale (LHE), permits to obtain over the entire water depth, three-dimensional quasi-instantaneous information on the fluctuating velocity flow field in the production and inertial subranges of the spectral space. Improvements on the ADVP data quality were made with the implementation of a correction methodology for errors due to data aliasing. This reduced the range-velocity ambiguity in Doppler-based instruments. The results presented in this dissertation contribute to the understanding of transport and mixing processes in river flows. They are based on three sets of field measurements made in gravel-bed rivers with blockage ratios of h/D 84 ≈ 3. 0 and aspect ratios B/h between 23 and 32. The measurements were made during low-water periods. The fieldwork provided results on the mean and instantaneous velocity field. The flow was divided into three inviscid vertical layers with different mean field and Reynolds stress characteristics: the roughness layer, the blending or intermediate layer and the surface layer. In the lower layers of the flow three types of mean velocity profile were found: mono-logarithmic, s-shaped due to bed perturbations and double-logarithmic downstream bed perturbations. The determination of the shear stress distribution for each of these profile types is studied. In double-log profiles, the friction velocity and roughness length determined for the outer logarithmic layer are required for the velocity profile parameterization. The s-shaped profiles are described by a tangent-hyperbolic function in the lower layers compatible with an external log layer. Limitations of 2 D open-channel theories to parameterize the velocity distribution and to characterize the bottom drag are discussed. Bottom drag occurs in the predominant momentum direction. The direction changes {{as a function of the}} local bed forms. To estimate bottom drag one has to consider the actual momentum transport direction which varies with the flow depth. The wall effect of the riverbank is visible until y/h≈ 5. Bottom topography produces important secondary mean motion in the flow. A permanent structure of the flow was described in the upper layers, near the surface (z/h< 0. 80) : the Surface Layer Organized Movement – SLOM. It is composed by local jets (CH regions) and by lower velocity regions (CL regions) associated with a compensatory secondary motion with streamwise vorticity. Lateral momentum transfer exists between adjacent CL and CH regions. In one river, the bed form presented signatures of possible streamwise ridges. All the turbulent characteristics of the flow respond to the periodic riverbed morphology. The strips were produced during high water events affecting river processes on the long-term. The D-shaped profiles are investigated. They relate to the CL regions and are formed where the velocity is lower near the surface. The maximum velocity is situated at around zUmax/π~ 0. 80. The occurrence of D-shaped profiles shows a dependence on the local Froude number; the SLOM might be an inviscid response to the bed forms. Two distinct approaches in the study of the turbulent velocity field were made: an analysis of the mean turbulence characteristics and the analysis of particular instantaneous features in the turbulent flow. The normal Reynolds stress distributions are anisotropic because streamwise turbulent intensity (TI) dominates the TKE (50 to 80 % of the total energy with maximum at z/h≈ 0. 70). Spanwise and vertical contributions vary along the flow depth. The surface and the bottom layers exert a strong influence on the vertical TI profile inducing a parabolic distribution. Self-similarity of the flow is only found inside the blending layer. In the roughness layer all Reynolds shear stresses become equally important. Empirical formula established by previous authors to describe the Reynolds stress tensor components are analyzed. The diffusive terms in the TKE budget equation are negligible and consequently the pseudo-dissipation may be considered equal to the actual dissipation ε~ ≈ ε. Production mostly happens inside the boundary layer and is mainly due to the gradients of the streamwise component. Dissipation is more widely spread in the flow with a maximum value near the bottom. Production and dissipation follow the exponential laws e 5. 1 z/h and e 3. 9 z/h, respectively. The repartition of the flow energy through the eddy scales is studied and a characteristic curve is found having as parameters the scale corresponding to the peak energy (Λmax), and a dimensionless parameter representing the energy dispersion through the scales (σ+). Empirical power laws are presented for the turbulence scales, Taylor (λ), Kolmogorov (η), integral (L), energetic (Λ) and mixing length (lm). The relationship between the different scales is also studied. With conditional sampling techniques, particular features of the turbulent velocity and their role in flow dynamics were described: detection and analysis of bursting packets inside the boundary layer and the study of the passage of large-sale uniform momentum regions (UMR), which here were called Streamwise Velocity Pulsation (SVP). Innovative tools are applied in the study of the instantaneous velocity measurements: wavelet decomposition and multiresolution analysis, empirical mode decomposition (EMD) and phase averaging based on the Hilbert transform. SVP corresponds to a non-periodic passage of large-scale UMR with streamwise velocities alternately higher or lower than the mean. This phenomenon was identified and characterized. The SVP Strouhal number is within the range 0. 13 <SSVP< 0. 32 (defined with the local depth averaged velocity). A vertical linear phase shift of ≈- 3 / 4 π of the front exists. The front has a first quarter moon shape with an apex at z/h≈ 0. 35. It scales with h in the vertical and with 3. 1 – 7. 3 h in the streamwise direction. In the lower layer the fronts have an average angle of 15 °. In phase with the SVP, cycles of large-scale sweeps and ejections exist. Ejections dominate shear stress production with higher instantaneous amplitude and lower permanence. A periodic variation of the bottom drag is related nonlinearly with the SVP (direct effect). Due to the 3 D bursting process triggered by the passage of an UMR, the bottom drag is enhanced (indirect effect). The SVP may be generated by the combined shedding effect produced by boulders which are randomly and widely spaced in the riverbed. Several identical bursting packets composed of sequential ejections and sweeps ({x-z} shear events) linked to spanwise vortical cells were visualized and characterized. The feedback of these was evaluated in the velocity profile, Reynolds stresses and TKE budget. These packets are highly energetic. The sampled bursting packets are independent. The time interval between packets is stable and the most probable value corresponds to a Strouhal number of Si= 2. 5 (defined with the local friction velocity). The bursting Strouhal number is Sb= 12. 5. Fronts from large-scale high- and low-speed wedges are associated with the bursting packets. Near the bottom the fronts have a concave shape and an angle of 18 ° with the horizontal. L'étude des écoulements turbulents représente toujours un défi pour la communauté scientifique. De nombreuses études ont mis en évidence l'importance des écoulements turbulents pour les fluides géophysiques. Ainsi, la morphologie fluviale, la modélisation des paysages, la dynamique atmosphérique ou les courants océaniques peuvent tous être influencés par des écoulements turbulents. Actuellement, les techniques de mesures et observations in situ, couplées avec des travaux expérimental et théorique, permettent d'analyser certains processus de transport et de mélange dans les rivières. Dans cette thèse, l'étude des écoulements turbulents dans les rivières au lit de gravier a été effectuée grâce à un profileur acoustique à effet Doppler à haute résolution (Acoustic Doppler Velocity Profiler - ADVP). Cet appareil, développé au Laboratoire d'Hydraulique Environnementale (LHE), permet la mesure en 3 D d'un profil quasi-instantané de vitesses avec des fréquences de mesures largement <b>suffisantes</b> pour visualiser les régions d'inertie et de production du spectre d'énergie. Afin de réduire l'ambiguïté distance/vitesse inhérentes aux mesures Doppler une méthode de correction des erreurs de crénelage a été développée. Les résultats présentés dans cette thèse se divisent en deux groupes : champ des vitesses moyennes et champ des vitesses instantanées. Les relations de blocage des écoulements étudiés étaient h/D 84 ≈ 3. 0 et le rapport largeur/profondeur (B/h) se situait entre 23 et 32. Les mesures ont été faites pendant des périodes d'écoulement peu profond. L'écoulement peut être divisé en trois couches non-visqueuses caractérisées par des différences au niveau des distributions de vitesses et des tensions tangentielles de Reynolds: la couche rugueuse (z/h< 0. 20), la couche intermédiaire (0. 20 <z/h< 0. 80) et la couche de surface (z/h< 0. 80). On a identifié et caractérisé trois types de profils de vitesse : monologarithmique (monolog), en forme de "s" dû à l'inflexion du profil causé par une perturbation du fond et double-logarithmique (2 xlog) à l'aval d'une perturbation du fond. La distribution de la tension tangentielle de Reynolds pour chaque un de ces profils est analysée. Pour les profils 2 xlog on a besoin des paramètres de rugosité de la deuxième couche limite pour la description de la distribution de vitesses. Les profils "s" sont décrits par une loi tangente hyperbolique dans les couches inférieures de l'écoulement compatible avec une loi logarithmique dans la couche extérieure. Les limitations des théories bidimensionnelles pour la description du profil de vitesse et la détermination des effets de frottement au fond sont discutées. Le frottement au fond est exercé selon la direction prédominante du transport de quantité de mouvement qui varie avec la profondeur et la forme du fond. L'effet de paroi exercé par les berges est visible jusqu'à une distance normalisée de y/h≈ 5. La morphologie du fond génère un écoulement secondaire significatif. On a identifié et décrit une structure permanente de l'écoulement existant dans la couche supérieure de l'écoulement (z/h< 0. 80) : mouvement organisé dans la couche de surface (Surface Layer Organized Motion - SLOM). Cette structure est constituée de jets localisés (régions CH) et par des zones de vitesse inférieure à la moyenne (régions CL) associées à un écoulement secondaire de compensation avec vorticité longitudinale. Entre les régions CH et CL on vérifie un échange latéral de quantité de mouvement. Dans le fond d'une des rivières étudiées on a des indices de structures de fond longitudinales conditionnant toute la structure moyenne et turbulente de l'écoulement. Ces configurations de fond produites en périodes de crues, vont postérieurement conditionner à longue terme l'écoulement. Correspondant aux régions CL, les profils de vitesse ont la forme en "D". En cela, la vitesse maximale correspond à la profondeur z/h≈ 0. 80. L'occurrence des profils "D" est liée avec le régime d'écoulement local, ce qui justifie la réponse non-visqueuse du SLOM aux formes de fond. Dans cette thèse, on étudie le champ instantané de vitesses en regardant les caractéristiques moyennes de la turbulence et des processus individuels dans les signaux de vitesses turbulentes. La distribution des termes diagonaux du tenseur de Reynolds est anisotrope: les intensités turbulentes (IT) selon la direction de l'écoulement dominent la production d'énergie cinétique turbulente (ECT) – entre 50 et 80 % de l'énergie totale avec la valeur maximale à z/h≈ 0. 70. Les contributions des IT verticale et transversale pour l'ECT changent selon la verticale. La surface et le fond influencent la distribution verticale de l'IT induisant un profil parabolique. La distribution de la tension de Reynolds est similaire seulement dans la couche intermédiaire. Dans la couche rugueuse, tous les autres termes du tenseur de Reynolds deviennent également importants. On analyse des formules empiriques établies par d'autres chercheurs pour la distribution du tenseur des tensions de Reynolds. Les termes de diffusion dans l'équation de l'ECT sont négligeables et par conséquent la pseudo-dissipation peut-être considérée égale à la dissipation réelle ε~ ≈ ε. La production turbulente existe en majorité dans la couche limite et est due principalement aux gradients de vitesse longitudinale. La dissipation est épandue par l'écoulement, avec la valeur maximale près du fond. La production et la dissipation ont des distributions verticales proportionnelles à e 5. 1 z/h et e 3. 9 z/h. La répartition de ECT par des échelles énergétiques de l'écoulement est caractérisée par une courbe avec comme paramètres l'échelle la plus énergétique (Λmax) et un paramètre adimensionnel représentant la dispersion de l'énergie par les échelles (σ+). On présente des lois empiriques pour la description des échelles turbulentes suivantes: micro-échelle de Taylor (λ), Kolmogorov (η), intégrale (L), énergétique (Λ) et longueur de mélange (lm). On utilise plusieurs techniques d'échantillonnage conditionné d'un signal instantané de vitesse pour isoler des phénomènes et étudier son rôle dans la dynamique turbulente de l'écoulement, notamment: détection et analyse des groupements des événements de cisaillement {x-z} dans la couche limite et passage des régions de quantité de mouvement uniforme (RQMU) nommé pulsation de vitesse longitudinale (Streamwise Velocity Pulsation – SVP). Des outils mathématiques innovateurs en mécanique des fluides ont été appliqués: décomposition par ondelettes, décomposition empirique en modes (Empirical Mode Decomposition – EMD) et moyenne dans la phase en utilisant la transformé d'Hilbert. La SVP correspond au passage non-périodique de RQMU à grande échelle où les vitesses sont inférieures et supérieures à la moyenne. Le nombre de Strouhal de la SVP est dans l'intervalle 0. 13 <SSVP< 0. 32 (défini avec la vitesse moyenne dans le profil). Il y a une différence de la phase verticale d'environ ≈- 3 / 4 p. Les fronts de SVP ont la forme de « croissant de lune » avec le point d'inflexion situé a z/h≈ 0. 35. La RQMU a une dimension verticale h et une dimension longitudinale 3. 1 – 7. 3 h. Les fronts de RQMU font en moyenne un angle avec l'horizontale de 15 °. On observe en phase avec la SVP un cycle d'éjections et de balayages sur toute la hauteur de l'écoulement. On vérifie une réponse non-linéaire du frottement au fond associé a la SVP: effet direct. Dues au processus 3 D de déclenchement des événements induit par le passage d'une RQMU, le frottement au fond est influencé: effet indirect. L'éventuelle origine de la SVP est l'agglomération de tourbillons déclenchés par des divers obstacles au fond de la rivière. Plusieurs groupements identiques des événements de cisaillement dans le plan {x-z}, constitué par une séquence d'éjections et balayages connectées par des tourbillons d'axe transversal, ont été identifiés et caractérisés. On analyse les implications de ces groupes sur le profil instantané de vitesse, les tensions de Reynolds et dans le bilan de ECT. Ces groupements sont très énergétiques et indépendants. L'intervalle de temps le plus probable entre ces groupements correspond a un nombre de Strouhal de Si= 2. 5 (défini avec la vitesse de frottement). Le Strouhal correspondant a la durée d'un groupement est de Sb= 12. 5. Associé aux groupements d'interactions, il existe un front de séparation entre deux RQMU avec des vitesses inférieures et supérieures à la moyenne. Ces fronts ont une forme concave et un angle de 18 ° avec l'horizontal près du fond...|$|E

