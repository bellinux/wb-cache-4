17|258|Public
50|$|Software: The {{standard}} Visual 1050 shipped with CP/M Plus operating system, a CP/M <b>source</b> <b>disk,</b> {{a copy of}} WordStar {{word processor}} with MailMerge software, Microsoft Multiplan spreadsheet, Digital Research DR Graph charting software, Digital Research CBASIC computer language, and an RS-232C communications program.|$|E
5000|$|JkDefrag {{is a free}} open <b>source</b> <b>disk</b> defragmenting utility {{computer}} program for Windows. It was developed by Jeroen Kessels beginning in 2004 and is released under the GNU General Public License. The [...] "Jk" [...] part of the utility name is taken from the developer's name, Jeroen Kessels. In 2008, from version 4, much changed from previous versions; JkDefrag was renamed MyDefrag by its developer, and JkDefrag was discontinued, although still available. MyDefrag is closed source freeware.|$|E
50|$|After {{being charged}} with her murder, he admits that Simone is not a person, but a {{computer}} program. The chest containing the computer data is brought up empty. Viktor's daughter Lainey and ex-wife Elaine enter his studio to try to help. They find Viktor's forgotten virus <b>source</b> <b>disk</b> (Plague) and apply an anti-virus program to eradicate the computer virus. They revive Simone and have her appear on national television laughing while holding up a newspaper headline with her obituary. They pick up a confused Viktor who realizes that his connection with Simone is a life sentence. At the end, Simone and Viktor are remotely interviewed at home about their new (virtual) baby. Simone is concerned about her child's future and decides to enter politics. The film shows how the fake is produced using the chroma key technique.|$|E
5000|$|One <b>source</b> {{suggests}} that <b>disk</b> utility {{should be used}} when there are: ...|$|R
50|$|Shugart Associates was {{a common}} <b>source</b> of floppy <b>disk</b> drives, as was Tandon Corporation.|$|R
25|$|This {{software}} {{is compatible with}} Linux encrypted volumes (e.g. LUKS, cryptoloop, dm-crypt), allowing data encrypted under Linux to be read (and written) freely. It was the first open <b>source</b> transparent <b>disk</b> encryption system to support Windows Vista and PDAs.|$|R
50|$|A disk cloning program {{needs to}} be able to read even {{protected}} operating system files on the <b>source</b> <b>disk,</b> and must guarantee that the system is in a consistent state at the time of reading. It must also overwrite any operating system already present on the destination disk. To simplify these tasks, most disk cloning programs can run under an operating system different from the native operating system of the host computer, for example, MS-DOS or an equivalent such as PC DOS or DR-DOS, or Linux. The computer is booted from this operating system, the cloning program is loaded and copies the computer's file system. Many programs (e.g. Acronis True Image, MiniTool Partition Wizard, Paragon Partition Manager or AOMEI Backupper) can clone a disk, or make an image, from within the running system, with special provision for copying open files; but an image cannot be restored onto the Windows System Drive under Windows.|$|E
40|$|There is {{increasing}} interest in extended source effects in microlensing events, as probes of the unresolved sources. Previous work has either presumed a uniform source, or else required an approximate or numerical {{treatment of the}} amplification function averaged over the <b>source</b> <b>disk.</b> In this paper, I present analytic expressions for the angle-averaged amplification functions for the rotationally-symmetric intensity and polarization cases. These integrals {{will allow us to}} use the technology of inverse problems to study the source limb-darkening and limb-polarization functions. Comment: 5 pages, 4 figures, submitte...|$|E
40|$|The {{exponential}} space dependent {{heat source}} (ESHS) process is utilized {{to explore the}} thermal transport characteristics of Marangoni convective flow in a Cu-H 2 O nanoliquid due to an infinite disk. Flow is driven by linear temperature. Five distinct nanoparticle shapes such as sphere, tetrahedron, column, hexahedron and lamina are accounted. Impacts of Joule heating, radiation and viscous dissipation are also retained. Hamilton-Crosserâs expression is employed to deploy effective thermal conductivity of nanoliquid. Multi degree partial differential equations system is reduced by KÃ¡rmÃ¡n transformations and then solved via shooting method. It is figured out that the heat transfer rate is enhanced for stronger Marangoni convection and nanoparticle volume fraction. Also, shape of the nanoparticles significantly affects the flow fields. Keywords: Nanofluid, Magnetohydrodynamics, Marangoni convection, Joule heating, Exponential space dependent heat <b>source,</b> <b>Disk</b> flo...|$|E
40|$|The authors {{report the}} {{development}} of an automatic image analysis system that detects gamma-ray source regions in images obtained from a coded aperture, gamma-ray imager. The number of gamma sources in the image is not known prior to analysis. The system counts the number (K) of gamma sources detected in the image and estimates the lower bound for the probability that the number of sources in the image is K. The system consists of a two-stage pattern classification scheme in which the Probabilistic Neural Network is used in the supervised learning mode. The algorithms were developed and tested using real gamma-ray images from controlled experiments in which the number and location of depleted uranium <b>source</b> <b>disks</b> in the scene are known...|$|R
5000|$|The {{system could}} {{communicate}} image data externally over an 80M per second [...] "Yapbus" [...] or a 2M per second multibus to other hosts, data <b>sources</b> or <b>disks</b> {{and had a}} performance measured equivalent to 200 VUPS, or 200 times {{the speed of a}} VAX 11/780.|$|R
40|$|The total {{efficiency}} of cylindrical scintillation γ-ray detectors {{has been determined}} using a novel, primary interaction based Monte Carlo algorithm. With the use of biasing in these simulations, this approach has been made computationally efficient, yielding converged results with standard errors less than {{a fraction of a}} percent for about 104 histories which is about two orders of magnitude smaller than the conventional stochastic techniques. This methodology has been implemented in a MATLbased computer program, DSEMC. For thin <b>disk</b> <b>sources</b> of various radii having coaxial configurations, the predictions of the DSEMC code have been found in excellent agreement with the corresponding results obtained by using the direct analytical technique. Similar agreement has also been found in the DSEMC calculated values of {{total efficiency}} and the corresponding results obtained by the direct analytical technique for coaxial thin <b>disk</b> <b>sources</b> of various radii over a wide range of γ-energies. The DSEMC program has been used for the determination of total detection efficiency for off-axial configurations. As the <b>disk</b> <b>source</b> radius increases, results show a gradual decreasing trend in total efficiency. For these configurations, energy dependency of the total efficiency is found to follow the variation of the corresponding total attenuation coefficient which is consistent with the expected behavior. For coaxial thin <b>disk</b> <b>sources,</b> the total detection efficiency has been found to approach a corresponding maximum value as length is increased in the 1 - 20 cm range for various values of radii of <b>disk</b> <b>sources</b> and γ-ray energy, while keeping the diameter of the detector fixed at 7. 62 cm. A similar behavior is observed when the radius of the detector is increased from 1 - 20 cm, while keeping the length of the detector fixed at 7. 62 cm for various values of radii of <b>disk</b> <b>sources</b> and γ-ray energy...|$|R
40|$|Observations of microlensing transit {{events can}} be used to measure the limb darkening of the lensed star. We discuss the {{advantages}} and drawbacks of several microlensing light curve inversion methods. The method of choice in this work is inversion by means of decomposition of the stellar surface brightness profile. We construct an ideal basis by principal component analysis of brightness profiles obtained from model atmosphere calculations. Limb darkening approximations using such a basis are superior to those using standard power-law limb darkening laws. We perform a full analysis of simulated single-lens microlensing transit events including a detailed error analysis of the method. In realistic events with a low impact parameter the brightness profile of the source can be recovered with a relative accuracy of 2 % {{from the center of the}} <b>source</b> <b>disk</b> to 0. 9 of the disk radius. We show that in the particular case of the observed MACHO Alert 95 - 30 event the intrinsic complex variability of the lensed red giant hinders efforts to recover its surface features. Comment: 47 pages, 11 figures - accepted by The Astrophysical Journa...|$|E
40|$|A {{method is}} {{presented}} for {{the reconstruction of}} rotating monopole source distributions using acoustic pressures measured on a sideline parallel to the source axis. The method requires no a priori assumptions about the source other than that its strength at the frequency of interest varies sinusoidally in azimuth on the <b>source</b> <b>disk</b> so that the radiated acoustic field is composed of a single circumferential mode. When multiple azimuthal modes are present, the acoustic field can be decomposed into azimuthal modes and the method applied to each mode in sequence. The method proceeds in two stages, first finding an intermediate line source derived from the source distribution and then inverting this line source to find the radial variation in source strength. A far-field form of the radiation integrals is derived, showing that the far-field pressure is a band-limited Fourier transform of the line source, establishing a limit {{on the quality of}} source reconstruction, which can be achieved using far-field measurements. The method is applied to simulated data representing wind-tunnel testing of a ducted rotor system (tip Mach number of 0. 74) and to control of noise from an automotive cooling fan (tip Mach number of 0. 14), studies which have appeared in the literature of source identification...|$|E
40|$|Disk {{encryption}} {{has become}} an important security measure for a multitude of clients, including governments, corporations, activists, security-conscious professionals, and privacy-conscious individuals. Unfortunately, recent research has discovered an effective side channel attack against any disk mounted by a running machine [23]. This attack, known as the cold boot attack, is effective against any mounted volume using state-of-the-art disk encryption, is relatively simple to perform for an attacker with even rudimentary technical knowledge and training, and is applicable to exactly the scenario against which disk encryption is primarily supposed to defend: an adversary with physical access. To our knowledge, no effective software-based countermeasure to this attack supporting multiple encryption keys has yet been articulated in the literature. Moreover, since no proposed solution has been implemented in publicly available software, all general-purpose machines using disk encryption remain vulnerable. We present Loop-Amnesia, a kernel-based disk encryption mechanism implementing a novel technique to eliminate vulnerability to the cold boot attack. We offer theoretical justification of Loop-Amnesia’s invulnerability to the attack, verify that our implementation is not vulnerable in practice, and present measurements showing our impact on I/O accesses to the encrypted disk {{is limited to a}} slowdown of approximately 2 x. Loop-Amnesia is written for x 86 - 64, but our technique is applicable to other register-based architectures. We base our work on loop-AES, a state-of-the-art open <b>source</b> <b>disk</b> encryption package for Linux. ...|$|E
40|$|Conventional image {{correlation}} spectroscopy (ICS) analysis assumes point-like probe particles whose sizes are {{much smaller than}} the beam focus. This assumption yields erroneous results when the particle size is larger than a certain threshold. Here, a formalism is presented to study {{image correlation}} spectroscopy for particles of arbitrary geometries, sizes, and fluorophore distributions. We demonstrate the usefulness of this method by analyzing simulated image sequences of diffusing fluorescent point <b>sources,</b> <b>disks,</b> and randomly oriented rigid rods of various sizes. In addition, we also perform ICS analysis on confocal images of fluorescent microspheres of different diameters diffusing in a medium of known viscosity to experimentally validate the method. The new method, which we call template analysis, yields excellent agreement with theoretical predictions, thus extending the capability of ICS for studying dynamic processes in a probe-independent manner...|$|R
40|$|The Herschel Space Observatory {{was used}} to observe similar to 120 pre-main-sequence stars in Taurus {{as part of the}} GASPS Open Time Key project. Photodetector Array Camera and Spectrometer {{was used to}} measure the {{continuum}} as well as several gas tracers such as [O I] 63 mu m, [O I] 145 mu m, [C II] 158 mu m, OH, H 2 O, and CO. The strongest line seen is [O I] at 63 mu m. We find a clear correlation between the strength of the [O I] 63 mu m line and the 63 mu m continuum for <b>disk</b> <b>sources.</b> In outflow sources, the line emission can be up to 20 times stronger than in <b>disk</b> <b>sources,</b> suggesting that the line emission is dominated by the outflow. The tight correlation seen for <b>disk</b> <b>sources</b> suggests that the emission arises from the inner disk (...|$|R
50|$|They {{create a}} {{discrepancy}} between the original <b>source</b> code on <b>disk</b> and the observed behaviour {{that can be very}} confusing to anyone unaware of the patches' existence.|$|R
40|$|Abstract. We are {{presenting}} spectroscopic and imaging {{data on the}} Chamaeleon Infrared Nebula (Cha IRN). Imaging {{was done}} between 1 and 2. 5 µm and at 10 µm. Spectra of the object were obtained in the 2. 1 µm – 4. 6 µm range by ground-based measurements as well as using ISOPHOT-S in the 2. 5 µm – 11. 6 µm range. By combining these data with a K-band speckle image and IRAS-LRS data, we draw a complete picture of the source. The system’s geometry, consisting of central <b>source,</b> <b>disk,</b> and bipolar outflow cavities, is determined directly from the observational data. Thereby we confirm earlier assumptions on the basic geometry of the system as well as support newer speculations by Gledhill et al. (1996) {{about the presence of}} a binary system inside the Cha IRN. Additionally, we fit a radiative transfer model to the spectral energy distribution and derive the dust composition from the ISOPHOT spectra and give the resulting abundance ratios. The spectroscopic data point to an unusually silicate-poor environment of this young stellar object or a special geometric arrangement which leads to a suppression of the feature. They also indicate {{one of the most prominent}} H 2 O ice features known as well as the presence of CO, CO 2, and possibly NH 3 ice. Key words: stars: formation – ISM: jets and outflows – accretion, accretion disks – line: identification – radiative transfer 1...|$|E
40|$|Context. Not only is {{gravitational}} microlensing {{a successful}} tool for discovering distant exoplanets, {{but it also}} enables characterization of the lens and source stars involved in the lensing event. Aims. In high-magnification events, the lens caustic may cross over the <b>source</b> <b>disk,</b> which allows determination of the angular size of the source and measurement of its limb darkening. Methods. When such extended-source effects appear close to maximum magnification, the resulting light curve differs from the characteristic Paczy´nski point-source curve. The exact shape of the light curve close to the peak depends on the limb darkening of the source. Dense photometric coverage permits measurement of the respective limb-darkening coefficients. Results. In {{the case of the}} microlensing event OGLE 2008 -BLG- 290, the K giant source star reached a peak magnification at about 100. Thirteen different telescopes have covered this event in eight different photometric bands. Subsequent light-curve analysis yielded measurements of linear limb-darkening coefficients of the source in six photometric bands. The best-measured coefficients lead to an estimate of the source effective temperature of about 4700 + 100 − 200 K. However, the photometric estimate from colour-magnitude diagrams favours a cooler temperature of 4200 ± 100 K. Conclusions. Because the limb-darkening measurements, at least in the CTIO/SMARTS 2 Vs- and Is-bands, are among the most accurate obtained, the above disagreement needs to be understood. A solution is proposed, which may apply to previous events where such a discrepancy also appeared...|$|E
40|$|Disk {{encryption}} {{has become}} an important security measure for a multitude of clients, including governments, corporations, activists, security-conscious professionals, and privacy-conscious individuals. Unfortunately, recent research has discovered an effective side channel attack against any disk mounted by a running machineprincetonattack. This attack, known as the cold boot attack, is effective against any mounted volume using state-of-the-art disk encryption, is relatively simple to perform for an attacker with even rudimentary technical knowledge and training, and is applicable to exactly the scenario against which disk encryption is primarily supposed to defend: an adversary with physical access. To our knowledge, no effective software-based countermeasure to this attack supporting multiple encryption keys has yet been articulated in the literature. Moreover, since no proposed solution has been implemented in publicly available software, all general-purpose machines using disk encryption remain vulnerable. We present Loop-Amnesia, a kernel-based disk encryption mechanism implementing a novel technique to eliminate vulnerability to the cold boot attack. We offer theoretical justification of Loop-Amnesia's invulnerability to the attack, verify that our implementation is not vulnerable in practice, and present measurements showing our impact on I/O accesses to the encrypted disk {{is limited to a}} slowdown of approximately 2 x. Loop-Amnesia is written for x 86 - 64, but our technique is applicable to other register-based architectures. We base our work on loop-AES, a state-of-the-art open <b>source</b> <b>disk</b> encryption package for Linux. Comment: 13 pages, 4 figure...|$|E
5000|$|An {{attempt to}} copy the entire disk using [...] may omit the final block if it is of an {{unexpected}} length; whereas [...] may succeed. The <b>source</b> and destination <b>disks</b> {{should have the same}} size.|$|R
50|$|Randomness extractors {{are used}} widely in {{cryptographic}} applications, whereby a cryptographic hash function {{is applied to}} a high-entropy, but non-uniform <b>source,</b> such as <b>disk</b> drive timing information or keyboard delays, to yield a uniformly random result.|$|R
40|$|An {{infinite}} {{family of}} finite axisymmetric charged dust disks is presented. The disks are obtained by solving the Einstein-Maxwell equations for conformastatic spacetimes by assuming a functional dependency between the time-like {{component of the}} electromagnetic potential and the metric potential {{in terms of a}} solution of the Laplace equation. We give solutions to the Einstein-Maxwell equations with <b>disk</b> <b>sources</b> of finite extension in which the charge density is proportional to the energy surface density. We apply the well-know "inverse" approach to the gravitational potential representing finite thin disks given by Gonzalez and Reina to generate conformastatic charged dust thin discs. Exact examples of conformastatic metrics with <b>disk</b> <b>sources</b> are worked out in full. Comment: Physics and Mathematics of Gravitation: Proceedings of the Spanish Relativity Meeting 200...|$|R
40|$|Gravitational microlensing is {{not only}} a {{successful}} tool for discovering distant exoplanets, but it also enables characterization of the lens and source stars involved in the lensing event. In high magnification events, the lens caustic may cross over the <b>source</b> <b>disk,</b> which allows a determination of the angular size of the source and additionally a measurement of its limb darkening. When such extended-source effects appear close to maximum magnification, the resulting light curve differs from the characteristic Paczynski point-source curve. The exact shape of the light curve close to the peak depends on the limb darkening of the source. Dense photometric coverage permits measurement of the respective limb-darkening coefficients. In the case of microlensing event OGLE 2008 -BLG- 290, the K giant source star reached a peak magnification of about 100. Thirteen different telescopes have covered this event in eight different photometric bands. Subsequent light-curve analysis yielded measurements of linear limb-darkening coefficients of the source in six photometric bands. The best-measured coefficients lead to an estimate of the source effective temperature of about 4700 + 100 - 200 K. However, the photometric estimate from colour-magnitude diagrams favours a cooler temperature of 4200 +- 100 K. As the limb-darkening measurements, at least in the CTIO/SMARTS 2 V and I bands, are among the most accurate obtained, the above disagreement needs to be understood. A solution is proposed, which may apply to previous events where such a discrepancy also appeared. Comment: Astronomy & Astrophysics in pres...|$|E
40|$|In 1973, E. T. Newman {{considered}} the holomorphic extension Ẽ(x+iy) of the Coulomb field E(x) in R^ 3. By analyzing its multipole expansion, he {{showed that the}} real and imaginary parts of Ẽ(x+iy), viewed as functions of x for fixed y, are the electric and magnetic fields generated by a spinning ring of charge R. This represents the electromagnetic part of the Kerr-Newman solution to the Einstein-Maxwell equations. As already pointed out by Newman and Janis in 1965, this interpretation is somewhat problematic since the fields are double-valued. To make them single-valued, a branch cut must be introduced so that R {{is replaced by a}} charged disk D having R as its boundary. In the context of curved spacetime, D becomes a spinning disk of charge and mass representing the singularity of the Kerr-Newman solution. Here we confirm the above interpretation of the real and imaginary parts of Ẽ(x+iy) by computing the charge- and current densities directly as distributions in R^ 3 supported in the <b>source</b> <b>disk</b> D. This shows in particular that D spins rigidly at the critical rate, so that its rim R moves at the speed of light. It is a pleasure to thank Ted Newman, Andrzej Trautman and Iwo Bialinicki-Birula for many instructive discussions, particularly in Warsaw and during a visit to Pittsburgh. Comment: 15 pages, Invited paper, Workshop on Canonical and Quantum Gravity III, Polish Academy of Sciences, Warsa...|$|E
40|$|In the 1970 s and 1980 s, Planel et al. {{reported}} that the growth of paramecia was decreased by shield-ing them from background radiation. In the 1990 s, Takizawa et al. found that mouse cells displayed a decreased growth rate under shielded conditions. The {{purpose of the present}} study was to confirm that growth is impaired in organisms that have been shielded from background radiation. Radioprotection was produced with a shielding chamber surrounded by a 15 cm thick iron wall and a 10 cm thick paraffin wall that reduced the γ ray and neutron levels in the chamber to 2 % and 25 % of the background levels, respec-tively. Although the growth of Paramecium tetraurelia was not impaired by short-term radioprotection (around 10 days), which disagreed with the findings of Planel et al., decreased growth was observed after long-term (40 – 50 days) radiation shielding. When mouse lymphoma L 5178 Y cells were incubated inside or outside of the shielding chamber for 7 days, the number of cells present on the 6 th and 7 th days under the shielding conditions was significantly lower than that present under the non-shielding conditions. These inhibitory effects on cell growth were abrogated by the addition of a 137 Cs γ-ray <b>source</b> <b>disk</b> to the chamber. Furthermore, no growth retardation was observed in XRCC 4 -deficient mouse M 10 cells, which display impaired DNA double strand break repair...|$|E
2500|$|FreeOTFE was {{initially}} released by Sarah Dean in 2004, {{and was the}} first open <b>source</b> code <b>disk</b> encryption system that provided a modular architecture allowing 3rd parties to implement additional algorithms if needed. Older FreeOTFE licensing required that any modification to the program be placed in the public domain. This does not conform technically to section 3 of the Open Source definition. Newer program licensing omits this condition. The FreeOTFE license has not been approved by the [...] and is not certified to be labeled with the open-source certification mark.|$|R
40|$|Abstract. Halo clouds {{have been}} found about the three largest galaxies of the Local Group and in the halos of nearby spirals. This {{suggests}} they are a relatively generic feature of the galaxy evolution process {{and a source of}} fuel for galaxy disks. In this review, two main <b>sources</b> of <b>disk</b> star formation fuel, satellite material and clouds condensing from the hot halo medium, are discussed and their contribution to fueling the Galaxy quantified. The origin of the halo gas of M 31 and M 33 is also discussed. 1...|$|R
40|$|We present 850 μm and 450 μm {{data from}} the JCMT Gould Belt Survey {{obtained}} with SCUBA- 2 and characterise the dust attributes of Class I, Class II and Class III <b>disk</b> <b>sources</b> in L 1495. We detect 23 % of the sample at both wavelengths, with the detection rate decreasing through the Classes from I [...] III. The median disk mask is 1. 6 × 10 − 3 M⊙, and only 7 % of Class II <b>sources</b> have <b>disk</b> masses larger than 20 Jupiter masses. We detect {{a higher proportion of}} <b>disks</b> towards <b>sources</b> with stellar hosts of spectral type K than spectral type M. Class II disks with single stellar hosts of spectral type K have higher masses than those of spectral type M, supporting the hypothesis that higher mass stars have more massive disks. Variations in disk masses calculated at the two wavelengths suggests there may be differences in dust opacity and/or dust temperature between disks with hosts of spectral types K to those with spectral type M...|$|R
40|$|To {{predict the}} effects of a disk on the {{spectral}} energy distribution of a deeply embedded protostar, we construct disk models with power-law temperature distributions T is proportional to r(exp -q). We then use the spherically averaged disk emission as the central source for a spherical envelope, hence the term, 'spherical' disk. We then calculate the predicted spectral energy distribution of the disk and envelope, using a spherically symmetric radiative transport code. Applying this procedure to L 1551 IRS 5, we find that the predicted far-infrared flux is not very sensitive {{to the nature of the}} central source. The best source model is consistent with the far-infrared emission arising from the infalling region in an 'inside-out' collapse model, independent of the nature of the central <b>source.</b> <b>Disk</b> models are superior to the star-only model when we try to match millimeter interferometer data. While disks with various q can reproduce the observed 2. 7 mm interferometer flux, only an active disk (q = 0. 5) can produce enough emission in a region small enough to match the observed 2. 7 mm visibilities. However, if the disk is backwarmed by the envelope, even purely reprocessing disks can meet this constraint. All types of backwarmed disks are virtually in distinguishable in their millimeter properties. We find that all reasonable envelope models are sufficiently opaque in the mid-infrared to attenuate any disk model to a level well below the observations, unless the ratio of the mid-infrared to far-infrared dust opacities is similar to that of the dust opacities advocated by Mathis, Mezger, & Panagia (1983) ...|$|E
40|$|Thesis (M. A.) [...] Boston UniversityMilitary aerial {{photography}} is, at the present, {{beginning a}} stage of transition. The camera systems in use until this time usually have produced photography resolving approximately ten to twenty lines per millimeter on the print, in practice. For such photography to be useful in reasonably detailed photointertation, it has been necessary that it be at relatively large scales, normally {{in the range of}} 1 : 5, 000 to 1 : 15, 000. Since high flight altitude is desirable to provide a reasonable probability of successfully completing a mission in the presence of opposition, the scale requirement has necessitated the use of long focal length cameras. However, in the newer aircraft, the space and carrying capacity available is decreasing drastically. It appears certain that to provide further increases in aerodynamic performance in the future, this trend will continue. In addition, technical intelligence requirements are becoming much more severe, requiring that aerial photography resolve much smaller dimensions on the ground than has been the case previously. Fortunately, a new generation of aerial cameras is being developed to keep pace, the so called "high-acuity" cameras. Resolution with these may be expected to exceed considerably that obtained with previous cameras. These high-acuity systems offer the potential for keeping pace with the increasingly severe intelligence, aerodynamic and photographic interpretation requirements. The human eye may be considered to have a maximum resolving power of approximately ten lines per millimeter. Assuming that photography from high acuity systems will resolve at least fifty lines per millimeter, and possibly exceed one-hundred, a magnification of from five to more than ten will be required to fully benefit from this resolution. Such magnification may occur either in printing the negatives, in the viewing device used by the interpreter, or in some combination in the two stages. From several standpoints, however, it appears certain that an appreciable part will be in the printing process. In photography, it is axiomatic that resolution is lost in virtually every process. In projection printing, this loss is attributable to not only the projection: lens and printing material, but also to other characteristics of the projection printer. One of these of potential importance is the cone angle of the illumination furnished by the printer at a given point on the negative. Projection printers fall into two classes: those employing diffuse illumination, in which the illumination cone angle may approach 180 degrees, and those employing illumination directed by a condensing-lens system, in which type the cone angle may be in the vicinity of twenty degrees. Most authorities agree that, other conditions being equivalent, a print made with a diffuse-type printer will have less contrast than one made with a condenser-type printer. From theory, any loss of contrast should result in a loss of resolution. To fully benefit from the increased capability of the new high-acuity camera systems, it is important that losses of resolution which may arise in the projection printing process be minimized. Accordingly, this investigation was made to determine the dependence of projection print resolution on the cone angle of the illumination incident at a point on the negative. It can be shown that, so far as factors affecting print resolution are concerned, the difference in illumination cone angle represents the only practical difference between the two classes of projection printers. Therefore, the procedure used in the investigation consisted in varying the illumination cone angle in a projection printer in eight steps between the limits of seven and 180 degrees. Print resolution was evaluated, and the maximum at each step was plotted against the illumination cone angle of that step. The illumination source used was a four inch diameter disk of flashed opal glass, evenly illuminated from the reverse side by six 15 -watt daylight-type flourescent tubes. The various illumination cone angles at the negative were obtained by varying the distance between the negative and the <b>source</b> <b>disk.</b> Two negatives were used {{in the course of the}} experiment. Both were 1 : 22. 1 reductions of an original Buckbee-Meers U. S. Air Force resolution target, were made in a resolving-power test camera, and were high-contrast targets. One was on an Eastman Kodak 548 High Resolution Plate and had a limiting resolution of 446 lines per millimeter; the other was on Eastman Kodak special emulsion S. O. 1213 on film, and had a limiting resolution of 125 lines per millimeter. The latter was the one intended to approximate photography obtained with a high-acuity camera system. The projection lens was a Schneider Componon 80 mm focal length f/ 4 enlarging lens. The lens was used at f/ 5. 6 throughout the investigation; at this aperture and within the limited angular field used, it gave essentially diffraction-limited performance. The printing material employed was Kodabromide projection printing paper, series were made on both Contrast Two and Contrast Four papers. For each cone angle (with each negative and printing paper contrast grade), exposures were made at five lens-to-easel distances differing by a small increment; this insured a run through best focus. At each distance, five exposures were made differing in exposure by a one-half stop increment in terms of time; this insured a run through optimum exposure. All prints were developed for 90 seconds in Kodak Dektol 2 : 1 at 68 °F in a tray with continuous agitation, and fixed for seven minutes in one bath of Kodak Rapid Fix. Print resolution was evaluated by an experienced analyst; readings were made using a binocular microscope, reflected light, and either 13 X or 30 X magnification depending on the resolution level of the particular series. For a given level to be considered resolved, it was necessary that both the tangential and radial lines be resolved. Each print was evaluated twice, with the two readings being separated by approximately ten days. Analysis of the data resulting from the experiment revealed that with the H. R. 548 and S. O. 1213 emulsions used, there is no significant dependence of projection print resolution on the illumination cohe angle at the negative. Some differences were found, but these were both small and inconsistent. They are felt to have been the result of random inconsistencies, principally in the reading of resolution values at the justresolved level. The major factor which fubelieved to have been responsible for this lack of effect can be seen from a geometrical consideration of the passage of light through a negative. Consider light to be directed toward the negative at such an angle that, undeviated, it would reach the projection lens. For a clear area, the light will pass through largely undiminished and undeviated. Assuming no losses in the lens, this light will reach the image of the area. However, for an area having density, three effects will occur. A portion of the light will be absorbed in the negative and thus will be lost, photographically. Another portion will pass through undeviated. A third will be scattered in all directions; only so much of this as is within the angle subtended by the lens aperture will reach the image of the denser area. Presumably the latter two portions when combined in the image will render the dense area in the proper tonal relation. Now consider additional light to be made incident on the two areas of the negative, with the direction of this light being such that, undeviated, it would not reach the projection lens. Obviously a portion of this light will also be scattered by the dense area, and a portion of this will reach the image of that area. This, then, will alter the tonal relation of the print; it will, in fact, reduce the contrast of the two areas. This accounts for the frequently mentioned difference in contrast rendition between condenser-type and diffuse-type projection printers. The latter supply a large portion of their light at angles which are larger than that subtended by the projection lens from a given point on the negative. The reason that the foregoing did not affect the results of the present investigation lies in the fine grain nature of the negative materials used. While a coarse grained material of high density may scatter nearly all of the light transmitted, the same is not true of fine grained materials. The materials designed for use with high-acuity camera systems are fine-grained; and future materials will probably be even more so as emulsion technology advances. Two other factors probably contributed to the observed lack of dependence of resolution on cone angle. One is the fact that for a negative illuminated by a flat Lambertian source, the intensity reaching a point on the negative at a plane angle "a" to the nomal is proportional to cos^ 3 a. Further, the portion which is lost by reflection rapidly increases for larger values of angle a. The combined effect is that once a moderate angle is exceeded, further increases should have relatively little photographic effect. This, of course, applies regardless of the nature of the negative emulsion. As a result of this investigation, two major conclusions have been reached: a. With the newer films having very fine grain and a high resolution capability, any dependence of projection print resolution on negative-illumination cone angle is negligible. b. Projection printers intended for use with high-acuity camera systems need not be limited by a requirement for a particular illumination cone angle, but rather may be designed to best meet other pertinent considerations...|$|E
50|$|A little {{example of}} this {{modularity}} could be new ReAction gadget class available that is piechart.gadget. The main purpose of this gadget is displaying the data distribution among various <b>sources,</b> like shares, <b>disk</b> capacity and free space, etc. on a graphical pie chart. Optional interaction from the user is also possible.|$|R
5000|$|The north part of {{the disk}} also {{contains}} {{what appears to be}} a very intense region of star formation. [...] Unusually, the star formation within this region appears to be more intense than the star formation in the galaxy's nucleus, and it is the brightest infrared <b>source</b> within the <b>disk.</b>|$|R
40|$|We {{have carried}} out a {{quantitative}} trend analysis of the crystalline silicates observed in the ISO spectra of a sample of 14 stars with different evolutionary backgrounds. We have modeled the spectra using a simple dust radiative transfer model and have correlated the results with other known parameters. We confirm the abundance difference of the crystalline silicates in disk and in outflow sources, as found by Molster et al. (1999 a). We found some evidence that the enstatite over forsterite abundance ratio differs, it is slightly higher in the outflow sources {{with respect to the}} <b>disk</b> <b>sources.</b> It is clear that more data is required to fully test this hypothesis. We show that the 69. 0 micron feature, attributed to forsterite, may be a very suitable temperature indicator. We found that the enstatite is more abundant than forsterite in almost all sources. The temperature of the enstatite grains is about equal to that of the forsterite grains in the <b>disk</b> <b>sources</b> but slightly lower in the outflow sources. Crystalline silicates are on average colder than amorphous silicates. This {{may be due to the}} difference in Fe content of both materials. Finally we find an indication that the ratio of ortho to clino enstatite, which is about 1 : 1 in <b>disk</b> <b>sources,</b> shifts towards ortho enstatite in the high luminosity (outflow) sources...|$|R
5000|$|TCCBOOT, a hack where TCC {{loads and}} boots a Linux kernel from source in about 10 seconds. That is to say, it is a [...] "boot loader" [...] which reads Linux kernel <b>source</b> code from <b>disk,</b> writes {{executable}} instructions to memory, and begins running it. This did require {{changes to the}} Linux build process.|$|R
