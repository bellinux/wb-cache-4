10000|47|Public
5|$|Hammar, Lucia B. 1983. <b>Syntactic</b> {{and pragmatic}} options in Mongolian - {{a study of}} bol and n'. Ph.D. Thesis. Bloomington: Indiana University.|$|E
5|$|<b>Syntactic</b> Structures {{is a major}} work in {{linguistics}} by American linguist Noam Chomsky.|$|E
5|$|As cases {{cannot be}} stacked in Wagiman, these number suffixes cannot be called case suffixes, whereas the nominal suffixes {{discussed}} above (such as -binyju 'only'), {{show the same}} <b>syntactic</b> distribution - they occur {{in the same place}} - and therefore may be analysed as cases themselves.|$|E
5|$|Demonstratives are {{similarly}} considered nominals in Wagiman, {{and take the}} same case suffixes depending on their semantic and <b>syntactic</b> roles; their function within the sentence. That is, the demonstrative mahan 'this', or 'here' (root: mayh-), may take case {{just like any other}} nominal.|$|E
5|$|The accusative pronouns on {{the other}} hand, may be accusative or dative, {{depending}} on the <b>syntactic</b> requirements of the verb. In the traditional terminology, these pronouns can be either direct or indirect objects.|$|E
5|$|C# {{provides}} properties as <b>syntactic</b> sugar for {{a common}} pattern in which a pair of methods, accessor (getter) and mutator (setter) encapsulate operations on a single attribute of a class. No redundant method signatures for the getter/setter implementations need be written, and the property may be accessed using attribute syntax rather than more verbose method calls.|$|E
5|$|Many {{authors have}} speculated that {{at the core of}} this {{symbolic}} explosion, and in tandem, was the development of <b>syntactic</b> language that evolved through a highly specialized social learning system providing the means for semantically unbounded discourse. Syntax would have {{played a key role in}} this process and its full adoption could have been a crucial element of the symbolic behavioral package in the MSA.|$|E
5|$|Methods on {{objects are}} {{functions}} {{attached to the}} object's class; the syntax instance.method(argument) is, for normal methods and functions, <b>syntactic</b> sugar for Class.method(instance, argument). Python methods have an explicit self parameter to access instance data, {{in contrast to the}} implicit self (or this) in some other object-oriented programming languages (e.g., C++, Java, Objective-C, or Ruby).|$|E
5|$|Nearly 500 {{years of}} intense contact between {{speakers}} of Nahuatl and speakers of Spanish, {{combined with the}} minority status of Nahuatl and the higher prestige associated with Spanish has caused many changes in modern Nahuatl varieties, {{with large numbers of}} words borrowed from Spanish into Nahuatl, and the introduction of new <b>syntactic</b> constructions and grammatical categories.|$|E
5|$|Nouns {{can form}} noun phrases (NPs) {{where they are}} the <b>syntactic</b> head of the words that depend on them such as determiners, quantifiers, conjunctions or adjectives. Noun phrases can be short, such as the man, {{composed}} only of a determiner and a noun. They can also include modifiers such as adjectives (e.g. red, tall, all) and specifiers such as determiners (e.g. the, that). But they can also tie together several nouns into a single long NP, using conjunctions such as and, or prepositions such as with, e.g. the tall man with the long red trousers and his skinny wife with the spectacles (this NP uses conjunctions, prepositions, specifiers and modifiers). Regardless of length, an NP functions as a <b>syntactic</b> unit. For example, the possessive enclitic can, in cases which do not lead to ambiguity, follow the entire noun phrase, as in The President of India's wife, where the enclitic follows India and not President.|$|E
5|$|Python is {{intended}} to be a highly readable language. It is designed to have an uncluttered visual layout, often using English keywords where other languages use punctuation. Python does not use curly brackets to delimit blocks, and semicolons after statements are optional, in contrast to many other programming languages. Further, Python has fewer <b>syntactic</b> exceptions and special cases than C or Pascal.|$|E
5|$|Statives are a {{class of}} predicative words expressing a quality or state, whose <b>syntactic</b> {{properties}} fall in between those of verbs and adjectives in Indo-European languages. Like verbs, statives can sometimes be inflected for person but normally lack inflections for tense, aspect and other purely verbal categories. Statives can be adjectives, positionals or numerals.|$|E
5|$|<b>Syntactic</b> {{coordination}} and subordination {{is done by}} combining predicates in the superordinate moods (indicative, interrogative, imperative, optative) with predicates in the subordinate moods (conditional, causative, contemporative and participial). The contemporative has both coordinative and subordinative functions depending on context. The relative order of the main clause and its coordinate or subordinate clauses is relatively free, and mostly subject to pragmatic concerns.|$|E
5|$|The F571 {{extension}} is {{orthogonal to}} the presence of the boolean datatype in SQL (discussed later in this article) and, despite <b>syntactic</b> similarities, F571 does not introduce boolean or three-valued literals in the language. The F571 extension was actually present in SQL92, well before the boolean datatype was introduced to the standard in 1999. The F571 extension is implemented by few systems however; PostgreSQL is one of those implementing it.|$|E
5|$|Unlike ASL, BASL {{allows for}} the {{frequent}} use of <b>syntactic</b> repetition. In {{a study conducted by}} McCaskill, of 26 signers (13 Black and 13 White), Black signers had 57 instances of repetition compared to 19 from White signers, and of those 19 instances, 18 were made by a single signer. The use of repetition by BASL signers is considered to be pragmatic rather than as a way to clarify meaning.|$|E
25|$|Ergativity may be {{manifested}} through syntax, such {{as saying}} “Arrived I” for “I arrived”, {{in addition to}} morphology. <b>Syntactic</b> ergativity is quite rare, and while all languages that exhibit it also feature morphological ergativity, few morphologically ergative languages have ergative syntax. As with morphology, <b>syntactic</b> ergativity can be placed on a continuum, whereby certain <b>syntactic</b> operations may pattern accusatively and others ergatively. The degree of <b>syntactic</b> ergativity is then dependent {{on the number of}} <b>syntactic</b> operations that treat the subject like the object. <b>Syntactic</b> ergativity is also referred to as inter-clausal ergativity, as it typically appears in the relation of two clauses.|$|E
25|$|The {{dependency}} representations above (and further below) show <b>syntactic</b> dependencies. Indeed, most work in {{dependency grammar}} focuses on <b>syntactic</b> dependencies. <b>Syntactic</b> dependencies are, however, {{just one of}} three or four types of dependencies. Meaning–text theory, for instance, emphasizes the role of semantic and morphological dependencies in addition to <b>syntactic</b> dependencies. A fourth type, prosodic dependencies, can also be acknowledged. Distinguishing between these types of dependencies can be important, in part because if one fails to do so, the likelihood that semantic, morphological, and/or prosodic dependencies will be mistaken for <b>syntactic</b> dependencies is great. The following four subsections briefly sketch each of these dependency types. During the discussion, the existence of <b>syntactic</b> dependencies is taken for granted and used as an orientation point for establishing the nature of the other three dependency types.|$|E
25|$|<b>Syntactic</b> n-grams are {{intended}} to reflect <b>syntactic</b> structure more faithfully than linear n-grams, and have {{many of the same}} applications, especially as features in a Vector Space Model. <b>Syntactic</b> n-grams for certain tasks gives better results than the use of standard n-grams, for example, for authorship attribution.|$|E
25|$|However, {{in modern}} colloquial English almost any adverb {{may be found}} in this <b>syntactic</b> position, {{especially}} when the adverb and the verb form a close <b>syntactic</b> unit (really-pull, not-split).|$|E
25|$|Similarly, {{many of the}} <b>syntactic</b> {{results in}} proof theory can be proved in PRA, which implies that there are {{primitive}} recursive functions that carry out the corresponding <b>syntactic</b> transformations of proofs.|$|E
25|$|Brackets {{indicate}} <b>syntactic</b> structure.|$|E
25|$|The two {{arguments}} Sam and Sally in tree (a) {{are dependent}} on the predicate likes, whereby these arguments are also syntactically dependent on likes. What {{this means is that}} the semantic and <b>syntactic</b> dependencies overlap and point in the same direction (down the tree). Attributive adjectives, however, are predicates that take their head noun as their argument, hence big is a predicate in tree (b) that takes bones as its one argument; the semantic dependency points up the tree and therefore runs counter to the <b>syntactic</b> dependency. A similar situation obtains in (c), where the preposition predicate on takes the two arguments the picture and the wall; one of these semantic dependencies points up the <b>syntactic</b> hierarchy, whereas the other points down it. Finally, the predicate to help in (d) takes the one argument Jim but is not directly connected to Jim in the <b>syntactic</b> hierarchy, which means that that semantic dependency is entirely independent of the <b>syntactic</b> dependencies.|$|E
25|$|Each logical system {{comes with}} both a <b>syntactic</b> component, which among other things determines the notion of provability, and a {{semantic}} component, which determines the notion of logical validity. The logically valid formulas of a system are sometimes called the theorems of the system, especially {{in the context of}} first-order logic where Gödel's completeness theorem establishes the equivalence of semantic and <b>syntactic</b> consequence. In other settings, such as linear logic, the <b>syntactic</b> consequence (provability) relation may be used to define the theorems of a system.|$|E
25|$|In the lexical accounts, the causative {{alternation}} {{takes place}} {{at the level of the}} lexical conceptual structure (LCS), while in the <b>syntactic</b> accounts, the alternation happens at the level of the syntax, as a result of the interaction between the <b>syntactic</b> structure and the basic verbal element.|$|E
25|$|The {{masculine}} subject {{le chien}} in (a) demands the masculine {{form of the}} predicative adjective blanc, whereas the feminine subject la maison demands the feminine form of this adjective. A morphological dependency that is entirely independent of the <b>syntactic</b> dependencies therefore points again across the <b>syntactic</b> hierarchy.|$|E
25|$|As a {{primitive}} of the theory, {{the status of}} these functions is much different than in some constituency grammars. Traditionally, constituency grammars derive the <b>syntactic</b> functions from the constellation. For instance, the object is identified as the NP appearing inside finite VP, and the subject as the NP appearing outside of finite VP. Since DGs reject {{the existence of a}} finite VP constituent, they were never presented with the option to view the <b>syntactic</b> functions in this manner. The issue is a question of what comes first: traditionally, DGs take the <b>syntactic</b> functions to be primitive and they then derive the constellation from these functions, whereas constituency grammars traditionally take the constellation to be primitive and they then derive the <b>syntactic</b> functions from the constellation.|$|E
25|$|<b>Syntactic</b> {{dependencies}} are {{the focus}} of most work in dependency grammar, as stated above. How the presence and the direction of <b>syntactic</b> dependencies are determined is of course often open to debate. In this regard, it must be acknowledged that the validity of <b>syntactic</b> dependencies in the trees throughout this article is being taken for granted. However, these hierarchies are such that many dependency grammars can largely support them, although there will certainly be points of disagreement. The basic question about how <b>syntactic</b> dependencies are discerned has proven difficult to answer definitively. One should acknowledge in this area, however, that the basic task of identifying and discerning the presence and direction of the <b>syntactic</b> dependencies of dependency grammars is no easier or harder than determining the constituent groupings of constituency grammars. A variety of heuristics are employed to this end, basic constituency tests being useful tools; the <b>syntactic</b> dependencies assumed in the trees in this article are grouping words together in a manner that most closely matches the results of standard permutation, substitution, and ellipsis constituency tests. Etymological considerations also provide helpful clues about the direction of dependencies. A promising principle upon which to base the existence of <b>syntactic</b> dependencies is distribution. When one is striving to identify the root of a given phrase, the word that is most responsible for determining the distribution of that phrase as a whole is its root.|$|E
25|$|Generative {{linguists}} of the 1960s, including Noam Chomsky and Ernst von Glasersfeld, believed semantic {{relations between}} transitive verbs and intransitive verbs {{were tied to}} their independent <b>syntactic</b> organization. This meant that they saw a simple verb phrase as encompassing a more complex <b>syntactic</b> structure.|$|E
25|$|Morphological {{dependencies}} obtain between {{words or}} parts of words. When a given word or part of a word influences the form of another word, then the latter is morphologically dependent on the former. Agreement and concord are therefore manifestations of morphological dependencies. Like semantic dependencies, morphological dependencies can overlap with and point {{in the same direction}} as <b>syntactic</b> dependencies, overlap with and point in the opposite direction of <b>syntactic</b> dependencies, or be entirely independent of <b>syntactic</b> dependencies. The arrows are now used to indicate morphological dependencies.|$|E
25|$|Danny Fox {{discusses}} <b>syntactic</b> {{positions of}} QNPs {{as a way}} of introducing and illustrating the basic semantic and <b>syntactic</b> relations found in LF. By looking at the meaning of QNPs in relation to the property they are given, or their predicate, we can derive the meaning of the whole sentence.|$|E
25|$|In contrast, {{others have}} argued that {{although}} semantics does {{play a role in}} inalienable possession, it is not central to the <b>syntactic</b> class of case-derived possessives. For example, compare the difference between the book's contents and the book's jacket. While a book cannot be divorced from its contents, it can be removed from its jacket. Regardless, both phrases have the same <b>syntactic</b> structure. Another example is Mary's mother versus Mary's friend. The mother will always be Mary's mother, but an individual might not always be Mary's friend. Again, both have the same <b>syntactic</b> structure.|$|E
25|$|Poole, G. 2002. <b>Syntactic</b> theory. New York: Palgrave.|$|E
25|$|Borsley, R. 1991. <b>Syntactic</b> theory: A unified approach. London: Edward Arnold.|$|E
25|$|As just suggested, the {{phenomena}} that c-command {{is intended to}} address may be more plausibly examined in terms of linear order and a hierarchy of <b>syntactic</b> functions. Concerning the latter, some theories of syntax take a hierarchy of <b>syntactic</b> functions to be primitive. This is true of Head-Driven Phrase Structure Grammar (HPSG), Lexical Functional Grammar (LFG), and dependency grammars (DGs). The hierarchy of <b>syntactic</b> functions that these frameworks posit is usually something like the following: SUBJECT > FIRST OBJECT > SECOND OBJECT > OBLIQUE OBJECT. Numerous mechanisms of syntax are then addressed {{in terms of this}} hierarchy.|$|E
25|$|Many {{languages}} {{classified as}} ergative in fact show split ergativity, whereby <b>syntactic</b> and/or morphological ergative patterns are conditioned by the grammatical context, typically person or the tense/aspect of the verb. Basque {{is unusual in}} having an almost fully ergative system in case-marking and verbal agreement, though it shows thoroughly nominative–accusative <b>syntactic</b> alignment.|$|E
25|$|Ergativity can {{be found}} in both {{morphological}} and <b>syntactic</b> behavior.|$|E
