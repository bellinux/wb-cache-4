340|280|Public
30|$|Therefore, {{in light}} of Lemma 1 and theorem 1, it suffices {{to show that the}} TBD matrix A {{satisfies}} RIP. In fact, the TBD matrix can also be employed as the CS matrix when the <b>sparsifying</b> matrix is the adaptive <b>sparsifying</b> matrix in Section 3. The reason is that the coefficient vector β with respect to the adaptive <b>sparsifying</b> matrix in Eq. (12) also exhibits similar concentration characteristic to the DCT coefficients. However, it is inappropriate to employ the adaptive <b>sparsifying</b> matrix and the TBD matrix simultaneously in CS system. Firstly, the adaptive <b>sparsifying</b> matrix must be orthonormalized in this case, which undoubtedly increase the computational complexity of the CS system. Secondly, more parameters need to be adjusted. The last but not the least, the TBD matrix cannot considerably improve the reconstruction performance with respect to the adaptive <b>sparsifying</b> matrix for the extremely compressible coefficient vector β and limited approximation accuracy. Thus, we employ the DCT basis as the <b>sparsifying</b> matrix for speech signals in Section 4 and Section 5.|$|E
30|$|This {{work also}} {{constructs}} an adaptive <b>sparsifying</b> matrix for voiced speech {{based on the}} quasi-periodicity during voiced segments. And this adaptive <b>sparsifying</b> matrix {{is a kind of}} symmetric cyclic matrix which is generated {{on the basis of the}} long term prediction. Therefore, this adaptive <b>sparsifying</b> matrix is dependent on the previously reconstructed signal instead of the current signal.|$|E
40|$|In this paper, {{we present}} a new {{compressive}} image fusion method based on combined <b>sparsifying</b> transforms. First, the framework of compressive image fusion is introduced briefly. Then, combined <b>sparsifying</b> transforms are presented to enhance the sparsity of images. Finally, a reconstruction algorithm based on the nonlinear conjugate gradient is presented to get the fused image. The simulations demonstrate that by using the combined <b>sparsifying</b> transforms better results can be achieved {{in terms of both}} the subjective visual effect and the objective evaluation indexes than using only a single <b>sparsifying</b> transform for compressive image fusion...|$|E
30|$|Using the <b>sparsified</b> signals as reference, one {{can observe}} {{that there is}} an {{improvement}} using the <b>sparsified</b> sources in some cases, but the difference between the scores obtained when no sparsification procedure is employed and with the proposed sparsification method is small.|$|R
30|$|When {{the number}} of sources are estimated, both {{original}} and <b>sparsified</b> sources generated exactly the same results. For the estimation of the mixing matrices, {{there were no significant}} difference for 1 -s sources and the better performance achieved for the <b>sparsified</b> 5 -s sources was maintained but with smaller differences among the results.|$|R
30|$|For this speaker, the {{original}} form factor of the spectrogram computed with non-overlapping frames of 32 ms is 0.26 and the sparsification {{leads to a}} form factor of 0.22. Shifting the time-frequency analysis of the <b>sparsified</b> signal of 16 ms (half of a frame) increases the form factor of only 0.004. Choosing another frame length for the analysis (respectively, 16 and 64 ms) modifies the form factor of {{the original}} signal (respectively, 0.30 and 0.24) and the form factor of the <b>sparsified</b> signal (respectively, 0.24 and 0.21) in the same way, so that the <b>sparsified</b> signal is kept sparser than the original one.|$|R
30|$|Although this {{adaptive}} <b>sparsifying</b> matrix {{is not a}} canonical {{basis in}} a conventional sense, it has two advantages. On the one hand, as an adaptive <b>sparsifying</b> matrix which is constructed by the recovered signal, the decoder doesn’t need additional storage space and at the encoder {{it is not necessary}} to spend time attaining the training data to construct the codebook and to transmit it to the decoder such as the approach proposed in[5]. On the other hand, the approximate sparsity of speech signals with respect to this adaptive <b>sparsifying</b> matrix is superior to the DCT basis, which can be verified by the comparison of reconstruction performance between the adaptive <b>sparsifying</b> matrix and the DCT basis in the subsection 3.3.|$|E
40|$|The {{sparsity}} {{of signals}} {{in a certain}} transform domain or dictionary has been extended in different applications in signal processing, image processing, and medical imaging. Wavelets and DCT {{have been widely used}} for compression. Recently, new application of the data-driven learning of <b>sparsifying</b> dictionaries has discovered in denoising, inpainting, and compressed sensing. Here We study the <b>sparsifying</b> transform model related to its prior linear sparse models. Then, we formulate the problem for learning square <b>sparsifying</b> transforms from data. Here algorithm alternate between a sparse coding step and a transform update step. Compressed sensing (CS) utilizes the sparsity of MR images to enable reconstruction from undersampled k-space data. Recent CS methods have employed analytical <b>sparsifying</b> transforms such as wavelets, curvelets, and finite differences. This transform is sele cted on the basis of image type. In this thesis, we propose a framework for adaptively learning the <b>sparsifying</b> transform (dictionary), and reconstructing the image simultaneously from highly under sampled k-space data. This framework is enforced on overlapping image patches emphasizing local structure. Reconstruction algorithm learns the <b>sparsifying</b> dictionary, and uses it to remove aliasing and noise in one step, and restores. Then it fills in the k-space data in the other step...|$|E
40|$|A texture <b>sparsifying</b> {{transform}} {{for use in}} unsupervised clas-sification of sea-ice in polarimetric {{synthetic aperture}} radar (SAR) imagery is presented. The goal of the <b>sparsifying</b> transform is to compactly represent the underlying informa-tion of the SAR imagery to eliminate sources of unwanted noise and complexities (e. g., banding effect on RADARSAT- 2) commonly found in SAR imagery. The proposed algorithm {{is designed to be}} simple to implement and discriminative in sea-ice scenes. Performing unsupervised classification on the <b>sparsifying</b> transform space using scenes captured with C-band HV polarization yields experimental results that are much more accurate than common pixel-based methods, and performs comparably to a recent more complex method. Index Terms — sea-ice classification, <b>sparsifying</b> trans-form, texture model, synthetic aperture radar. 1...|$|E
30|$|For the <b>sparsified</b> signals, these metrics were {{computed}} taking as references the <b>sparsified</b> signals. Since {{the goal of}} the proposed scheme is to objectively distort the original sources while maintaining them perceptually unchanged, taking the original sources as references would lead to a meaningless distortion of objective metrics like SDR, SIR, and SAR, masking the performance of the source separation algorithm. This choice has the further advantage of assessing the performance of each processing step separately.|$|R
40|$|Abstract. Data {{distortion}} is {{a critical}} component to preserve privacy in security-related data mining applications, such as in data miningbased terrorist analysis systems. We propose a <b>sparsified</b> Singular Value Decomposition (SVD) method for data distortion. We also put forth a few metrics to measure {{the difference between the}} distorted dataset and the original dataset. Our experimental results using synthetic and real world datasets show that the <b>sparsified</b> SVD method works well in preserving privacy as well as maintaining utility of the datasets. ...|$|R
30|$|In this section, {{we define}} some notation, we review the CS fundamentals, and we briefly discuss {{the use of}} <b>sparsified</b> {{matrices}} in the CS literature.|$|R
30|$|As an {{important}} branch of signal processing, speech signal processing {{has achieved a}} considerable development in past decades. In addition, the application of CS theory {{to the field of}} speech signal processing is becoming a rising research focus. In[5, 6], the sparsity of the residual excitation is utilized to construct <b>sparsifying</b> matrices for voiced speech signals. However, in the aforementioned two literatures, the <b>sparsifying</b> matrix constructed using the impulse response for voiced speech is impractical for its dependence on the currently reconstructed signal itself. Therefore, a codebook of impulse response vectors generated from the training speech data is proposed as the <b>sparsifying</b> matrix in[5].|$|E
30|$|Moreover, if g[*]≠[*] 0, the {{adaptive}} <b>sparsifying</b> matrix Ψ defined in Eq. (11) is invertible.|$|E
40|$|This paper {{considers}} simultaneously optimizing the Sensing Matrix and <b>Sparsifying</b> Dictionary (SMSD) {{on a large}} training dataset. We propose {{an online}} algorithm that consists of a closed-form solution for optimizing the sensing matrix with a fixed <b>sparsifying</b> dictionary and a stochastic method for optimizing the <b>sparsifying</b> dictionary on a large training dataset when the sensing matrix is fixed. Benefiting from training on a large dataset, the obtained compressive sensing system via the proposed algorithm yields a much better performance in terms of signal recovery accuracy than the existing ones. The simulation results on natural images demonstrate the effectiveness and efficiency of the proposed online algorithm compared with the existing methods. Comment: 6 figures, 2 table...|$|E
40|$|The text {{retrieval}} method using Latent Semantic Indexing (LSI) {{with the}} truncated Singular Value Decomposition (SVD) has been intensively studied in recent years. The term-document matrices after SVD are full matrices, although the rank is reduced substantially. To reduce memory consumption, we examine some strategies to <b>sparsify</b> the truncated SVD matrices. After applying the sparsification strategies to three popular document databases, {{we find that}} some of our strategies not only <b>sparsify</b> the SVD matrices, but may also increase the accuracy of the text retrieval in some cases. ...|$|R
40|$|Abstract We have {{recently}} developed several ways of performing Canonical Correlation Analysis [1, 5, 7, 4] with probabilistic methods {{rather than the}} standard statistical tools. However, the computational demands of training such methods scales with {{the square of the}} number of samples, making these methods uncompetitive with e. g. artificial neural network methods [3, 2]. In this paper, we examine a recent development which <b>sparsifies</b> a probabilistic method of performing principal component analysis and then use this method to <b>sparsify</b> a new probabilistic method of performing canonical correlation analysis. Key-words: Probabilistic canonical correlation analysis, sparsification...|$|R
3000|$|... 1 -norm {{constraint}} optimization {{has been}} proposed to <b>sparsify</b> the tap weights of robust FSBB. Several design examples have been presented to illustrate {{the performance of the}} presented approaches.|$|R
3000|$|... is k-sparse. An {{example of}} the <b>sparsifying</b> {{transform}} matrix, ϕ, is the Karhunen Loeve transform of the messages.|$|E
3000|$|... [*]>[*] 0.5, the {{adaptive}} <b>sparsifying</b> matrix and the DCT basis have similar {{performance for the}} first type and third type of voiced speech. However, for {{the second type of}} voiced speech, the reconstruction performance of {{the adaptive}} <b>sparsifying</b> matrix is slightly worse than that of the DCT basis. The reason is that with the great attenuation of the amplitude, the quasi-periodicity of the second type of voiced speech is undesirable.|$|E
40|$|Abstract: Many {{algorithms}} {{have been}} proposed to achieve sparse representation over redundant dictionaries or transforms. A comprehensive understanding of these algorithms is needed when choosing and designing algorithms for particular applications. This research studies a representative algorithm for each category, matching pursuit (MP), basis pursuit (BP), and noise shaping (NS), {{in terms of their}} <b>sparsifying</b> capability and computational complexity. Experiments show that NS has the best performance in terms of <b>sparsifying</b> ca-pability with the least computational complexity. BP has good <b>sparsifying</b> capability, but is computationally expensive. MP has relatively poor <b>sparsifying</b> capability and the computations are heavily dependent on the problem scale and signal complexity. Their performance differences are also evaluated for three typical ap-plications of time-frequency analyses, signal denoising, and image coding. NS has good performance for time-frequency analyses and image coding with far fewer computations. However, NS does not perform well for signal denoising. This study provides guidelines for choosing an algorithm for a given problem and for designing or improving algorithms for sparse representation. Key words: sparse representation; redundant dictionary/transform; nonlinear approximation; matching pursuit; basis pursuit; noise shapin...|$|E
40|$|We examine {{discrete}} vortex {{dynamics in}} two-dimensional flow through a network-theoretic approach. The {{interaction of the}} vortices is represented with a graph, which allows the use of network-theoretic approaches to identify key vortex-to-vortex interactions. We employ sparsification techniques on these graph representations based on spectral theory for constructing <b>sparsified</b> models and evaluating the dynamics of vortices in the <b>sparsified</b> setup. Identification of vortex structures based on graph sparsification and sparse vortex dynamics are illustrated through an example of point-vortex clusters interacting amongst themselves. We also evaluate the performance of sparsification with increasing number of point vortices. The sparsified-dynamics model developed with spectral graph theory requires reduced number of vortex-to-vortex interactions but agrees well with the full nonlinear dynamics. Furthermore, the <b>sparsified</b> model derived from the sparse graphs conserves the invariants of discrete vortex dynamics. We highlight {{the similarities and differences}} between the present sparsified-dynamics model and the reduced-order models. Comment: 23 pages, 13 figure...|$|R
30|$|Relaxing this {{specification}} could however {{allow to}} process only the time-frequency bin where the separation fails and hence potentially improve the separation {{for the same}} quality of the <b>sparsified</b> sources.|$|R
40|$|Abstract — To {{reduce the}} model {{complexity}} for inductive interconnects, the vector potential equivalent circuit (VPEC) model was introduced recently and a localized VPEC model was developed based on geometry integration. In this paper, {{we show that}} the localized VPEC model is not accurate for interconnects with non-trivial sizes. We derive an accurate VPEC model by inverting inductance matrix under the partial element equivalent circuit (PEEC) model, and prove that the effective resistance matrix under the resulting full VPEC model is passive and strictly diagonal dominant. This diagonal dominance enables truncating small-valued off-diagonal elements to obtain a <b>sparsified</b> VPEC model named truncated VPEC (  VPEC) model with guaranteed passivity. To avoid inverting the entire inductance matrix, we further present another <b>sparsified</b> VPEC model with preserved passivity, the windowed VPEC (¡ VPEC) model based on inverting a number of inductance sub-matrices. Both full and <b>sparsified</b> VPEC models are SPICE compatible. Experiments show that the full VPEC model is as accurate as the full PEEC model but consumes less simulation time than the full PEEC model does. Moreover, the <b>sparsified</b> VPEC model is orders of magnitude (1000 X) faster and produces waveform with small errors (3 %) compared to the full PEEC model, and the ¡ VPEC uses less (up to 90 X) model building time yet is more accurate compared to the   VPEC model...|$|R
40|$|Many of the {{applications}} of compressed sensing {{have been based}} on variable density sampling, where certain sections of the sampling coefficients are sampled more densely. Furthermore, it has been observed that these sampling schemes are dependent not only on sparsity but also on the sparsity structure of the underlying signal. This paper extends the result of (Adcock, Hansen, Poon and Roman, arXiv: 1302. 0561, 2013) to the case where the <b>sparsifying</b> system forms a tight frame. By dividing the sampling coefficients into levels, our main result will describe how the amount of subsampling in each level is determined by the local coherences between the sampling and <b>sparsifying</b> operators and the localized level sparsities [...] the sparsity in each level under the <b>sparsifying</b> operator...|$|E
3000|$|... [*]≤[*] 0.5, the {{reconstruction}} {{performance of the}} adaptive <b>sparsifying</b> matrix is far better than that of DCT. But when u [...]...|$|E
40|$|In {{traditional}} compressed sensing MRI methods, single <b>sparsifying</b> transform {{limits the}} reconstruction quality because it cannot sparsely represent {{all types of}} image features. Based {{on the principle of}} basis pursuit, a method that combines <b>sparsifying</b> transforms to improve the sparsity of images is proposed. Simulation results demonstrate that the proposed method can well recover different types of image features and can be easily associated with total variation. This work was partially supported by NNSF of China under Grants (10774125 and 10605019) ...|$|E
40|$|We {{present an}} {{approximate}} algorithm for matrix multiplication based on matrix sketching techniques. First {{one of the}} matrix is chosen and <b>sparsified</b> using the online matrix sketching algorithm, and then the matrix product is calculated using the <b>sparsified</b> matrix. We prove when the sample number grows large compared to the sample dimensions the proposed algorithm achieves similar accuracy bound with a smaller computational cost compared to the state-of-the-art algorithms. Comment: Theorem 1 may be problematic, and more careful thought is required. The authors are discussing a solution on it. Currently {{it is better to}} withdraw the draf...|$|R
40|$|Abstract. We examine text {{retrieval}} strategies {{using the}} <b>sparsified</b> concept decomposition matrix. The centroid vector of a tightly structured text collection provides a general description of text documents in that collection. The {{union of the}} centroid vectors forms a concept matrix. The original text data matrix can be projected into the concept space spanned by the concept vectors. We propose a procedure to conduct text retrieval based on the <b>sparsified</b> concept decomposition (SCD) matrix. Our experimental results show that text retrieval based on SCD may enhance the retrieval accuracy and reduce the storage cost, compared with the popular text retrieval technique based on latent semantic indexing with singular value decomposition...|$|R
3000|$|Although it was {{experimentally}} shown that, for given parameters, {{this method}} <b>sparsifies</b> efficiently audio signals without audible distortion, the trade-off between sparsification and audio quality was not explored. In other terms, how sparse {{can we make}} the sources without audible distortion? [...]...|$|R
40|$|Compressed sensing (CS) {{utilizes}} the sparsity of MR {{images to}} enable ac-curate reconstruction from undersampled k-space data. Recent CS methods have employed analytical <b>sparsifying</b> transforms such as wavelets, curvelets, and finite differences. In this thesis, we propose a novel framework for adap-tively learning the <b>sparsifying</b> transform (dictionary), and reconstructing the image simultaneously from highly undersampled k-space data. The sparsity in this framework is enforced on overlapping image patches emphasizing lo-cal structure. Moreover, the dictionary {{is adapted to}} the particular image instance, thereby favoring better sparsities and consequently much higher un-dersampling rates. The proposed alternating reconstruction algorithm learns the <b>sparsifying</b> dictionary, and uses it to remove aliasing and noise in one step, and subsequently restores and fills in the k-space data in the other step. Numerical experiments are conducted on MR images and on real MR data of several anatomies {{with a variety of}} sampling schemes. The result...|$|E
40|$|The {{sparsity}} {{of natural}} signals in transform domains {{such as the}} DCT has been heavily exploited in various applications. Recently, we in-troduced the idea of learning <b>sparsifying</b> transforms from data, and demonstrated the usefulness of learnt transforms in image represen-tation, and denoising. However, the learning formulations therein were non-convex, and the algorithms lacked strong convergence properties. In this work, we propose a novel convex formulation for square <b>sparsifying</b> transform learning. We also enforce a doubly sparse structure on the transform, which makes its learning, stor-age, and implementation efficient. Our algorithm is guaranteed to converge to a global optimum, and moreover converges quickly. We also introduce a non-convex variant of the convex formulation, for which the algorithm is locally convergent. We show the su-perior promise of our learnt transforms as compared to analytical <b>sparsifying</b> transforms such as the DCT for image representation. Index Terms — Sparse representations, Convex learning 1...|$|E
40|$|Reconstruction in {{compressed}} sensing {{relies on}} {{knowledge of a}} <b>sparsifying</b> transform. In a setting where a sink reconstructs a field based on measurements from a wireless sensor network, this transform {{is tied to the}} locations of the individual sensors, which may not be available to the sink during reconstruction. In contrast to previous works, we do not assume that the sink knows the position of each sensor to build up the <b>sparsifying</b> basis. Instead, we propose the use of spatial interpolation based on a predetermined <b>sparsifying</b> transform, followed by random linear projections and ratio consensus using local communication between sensors. For this proposed architecture, we upper bound the reconstruction error induced by spatial interpolation, as well as the reconstruction error induced by distributed compression. These upper bounds are then utilized to analyze the communication cost tradeoff between communication to the sink and sensor-to-sensor communication...|$|E
50|$|In the IC industry, <b>sparsified</b> {{integral}} equation techniques are typically used tosolve capacitance and inductance extraction problems. The random-walk methods havebecome quite mature for capacitance extraction. For problems requiring the solution ofthe full Maxwell's equations (full-wave), both differential and {{integral equation}} approachesare common.|$|R
3000|$|In {{the time}} domain, the <b>sparsified</b> signal is synthetized {{according}} to the overlap-add method. The overlapping in reconstruction avoids the clicks that can be noticed using the time-domain implementation of [22]. On the other hand, it {{increases the risk of}} actual values of [...]...|$|R
40|$|Commonly used RNA folding {{programs}} {{compute the}} minimum free energy {{structure of a}} sequence under the pseudoknot exclusion constraint. They are based on Zuker’s algorithm which runs in time Oðn 3 Þ. Recently, it has been claimed that RNA folding can be achieved in average time Oðn 2 Þ using a sparsification technique. A proof of quadratic time complexity {{was based on the}} assumption that computational RNA folding obeys the -zeta property". Several variants of sparse RNA folding algorithms were later developed. Here, we present our own version, which is readily applicable to existing RNA folding programs, as it is extremely simple and does not require any new data structure. We applied it to the widely used Vienna RNAfold program, to create sibRNAfold, the first public <b>sparsified</b> version of a standard RNA folding program. To gain a better understanding of the time complexity of <b>sparsified</b> RNA folding in general, we carried out a thorough run time analysis with synthetic random sequences, both in the context of energy minimization and base pairing maximization. Contrary to previous claims, the asymptotic time complexity of a <b>sparsified</b> RNA folding algorithm using standard energy parameters remains Oðn 3 Þ under a wide variety of conditions. Consistent with our run-time analysis, we found that RNA folding does not obey the -zet...|$|R
