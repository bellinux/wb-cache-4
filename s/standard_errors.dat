5385|8535|Public
5|$|<b>Standard</b> <b>errors</b> of prediction, {{also known}} as {{accuracy}} or possible change value {{in the context of}} EBV and EPD predictions, are dependent on the quality of information used to predict an animal's EBV or EPD for a given trait. Errors in estimating genetic merit are being addressed in research programmes that aim to supplement phenotypic data extensively used in current BLUP predictions with genotypic data.|$|E
5|$|Inconsistent {{inheritance}} of myostatin mutations (for example, F94L in Limousins, nt821 in Angus, and Q204X in Charolais) by progeny {{is expected to}} result in possible BLUP prediction errors for EBVs and EPDs equalling or exceeding worst case <b>standard</b> <b>errors</b> of prediction. For example, average rib eye area for Limousins in US Meat Animal Research Center (USMARC) trials during the 1980s and early 1990s {{is reported to be}} 12.3in2, and the reported possible difference in rib eye area in progeny arising from {{inheritance of}} either two F94L mutations or two normal myostatin genes from heterozygous parents is estimated to be 1.8in2 (12.3in2 x 15%). This difference, which is unpredictable without DNA testing, is nearly four times the possible change value for a 0% BIF accuracy, reported to be 0.46in2 for the rib eye EPD.|$|E
25|$|The {{estimated}} percentage plus {{or minus}} its margin of error is a confidence interval for the percentage. In other words, {{the margin of error}} is half the width of the confidence interval. It can be calculated as a multiple of the standard error, with the factor depending of the level of confidence desired; a margin of one standard error gives a 68% confidence interval, while the estimate {{plus or minus}} 1.96 <b>standard</b> <b>errors</b> is a 95% confidence interval, and a 99% confidence interval runs 2.58 <b>standard</b> <b>errors</b> {{on either side of the}} estimate.|$|E
25|$|<b>Standard</b> <b>error</b> of {{regression}} is {{an estimate}} of σ, <b>standard</b> <b>error</b> of the error term.|$|R
50|$|The {{error of}} E(D) is the <b>standard</b> <b>error</b> of {{experimentally}} calculated E(D) {{values and the}} error of Var(D) is the <b>standard</b> <b>error</b> of experimentally calculated Var(D) values. These <b>standard</b> <b>error</b> values can be estimated using theoretical models or resampling methods (bootstrapping, jackknifing).|$|R
30|$|At-risk {{drinking}} and infection {{were not found}} to be independent predictors of circulating B lymphocytes, cytotoxic T lymphocytes, and CD 16 – monocytes after multiregression analysis. On the other hand, both at-risk drinking (β-coefficient[*]=[*]− 0.174, <b>standard</b> <b>error</b> of β-coefficient[*]=[*] 0.07, P[*]=[*] 0.01) and infection (β-coefficient[*]=[*]− 0.167, <b>standard</b> <b>error</b> of β-coefficient[*]=[*] 0.06, P[*]=[*] 0.01) were independently associated with noncytotoxic lymphocyte counts but not previous treatment with antibiotics (β-coefficient[*]=[*]− 0.161, <b>standard</b> <b>error</b> of β-coefficient[*]=[*] 0.11, P[*]=[*] 0.17), current smoking (β-coefficient[*]=[*]− 0.154, <b>standard</b> <b>error</b> of β-coefficient[*]=[*] 0.2, P[*]=[*] 0.24), and poor dental state (β-coefficient[*]=[*]− 0.174, <b>standard</b> <b>error</b> of β-coefficient[*]=[*] 0.12, P[*]=[*] 0.15).|$|R
25|$|A 2002 {{study found}} {{an average of}} 18.6% European genetic {{contribution}} and 2.7% Native American genetic contribution (with <b>standard</b> <b>errors</b> 1.5% and 1.4% respectively) {{in a sample of}} 232 African Americans. Meanwhile, in a sample of 187 European Americans from State College, Pennsylvania, there was an average of 0.7% West African genetic contribution and 3.2% Native American genetic contribution (with <b>standard</b> <b>errors</b> 0.9% and 1.6% respectively). Most of the non-European admixture was concentrated in 30% of the sample, with West African admixture ranging from 2 to 20%, with an average of 2.3%.|$|E
25|$|As {{explained}} below, long-term {{estimates of}} the length of the tropical year were used in connection with the reform of the Julian calendar, which resulted in the Gregorian calendar. Participants in that reform were only partially aware of the non-uniform rotation of the Earth, but now this can be taken into account to some degree. The amount that TT is ahead of UT1 is known as ΔT, or Delta T. The table below gives Morrison and Stephenson's (S & M) 2004 estimates and <b>standard</b> <b>errors</b> (σ) for dates significant {{in the process of developing}} the Gregorian calendar.|$|E
2500|$|Although serial {{correlation}} {{does not affect}} the consistency of [...] the estimated regression coefficients, it does affect our ability to conduct valid statistical tests. First, the F-statistic to test for overall significance of the regression may be inflated under positive {{serial correlation}} because the mean squared error (MSE) will tend to underestimate the population error variance. Second, positive serial correlation typically causes the ordinary least squares (OLS) <b>standard</b> <b>errors</b> for the regression coefficients to underestimate the true <b>standard</b> <b>errors.</b> As a consequence, if positive serial correlation is present in the regression, standard linear regression analysis will typically lead us to compute artificially small <b>standard</b> <b>errors</b> for the regression coefficient. These small <b>standard</b> <b>errors</b> will cause the estimated t-statistic to be inflated, suggesting significance where perhaps there is none. The inflated t-statistic, may in turn, lead us to incorrectly reject null hypotheses, about population values of the parameters of the regression model more often than we would if the <b>standard</b> <b>errors</b> were correctly estimated.|$|E
30|$|We opt for {{a linear}} {{specification}} of the retirement equations and adjust the <b>standard</b> <b>error</b> by estimating robust <b>standard</b> <b>error.</b>|$|R
30|$|The <b>Standard</b> <b>error</b> values varied for {{different}} rheological models and {{were found to}} depend on several parameters like graphene/cement ratio, shear rate, resting time and test geometry. For all rheological models, generally, lower <b>standard</b> <b>error</b> values were found for concentric cylinders when compared to parallel plates. For both geometries (cylinders and parallel plates), HB model with lowest <b>standard</b> <b>error</b> value {{was found in the}} best-fitted model, while, Casson model estimated the most scattered and higher average <b>standard</b> <b>error</b> values.|$|R
5000|$|... if the <b>standard</b> <b>error</b> {{of several}} {{individual}} quantities is known then the <b>standard</b> <b>error</b> of some {{function of the}} quantities can be easily calculated in many cases; ...|$|R
2500|$|The Std errors column shows <b>standard</b> <b>errors</b> of each {{coefficient}} estimate: ...|$|E
2500|$|... {{where the}} mean squared {{standard}} error of person estimate gives {{an estimate of}} the variance of the errors, , across persons. The <b>standard</b> <b>errors</b> are normally produced as a by-product of the estimation process. [...] The separation index is typically very close in value to Cronbach's alpha.|$|E
2500|$|The {{expected}} {{value of the}} median falls slightly as sample size increases while, as would be expected, the <b>standard</b> <b>errors</b> of both the median and the mean are proportionate to the inverse square root of the sample size. [...] The asymptotic approximation errs {{on the side of}} caution by overestimating the standard error.|$|E
50|$|Note: the <b>standard</b> <b>error</b> and the <b>standard</b> {{deviation}} of small samples tend to systematically underestimate the population <b>standard</b> <b>error</b> and <b>standard</b> deviation: the <b>standard</b> <b>error</b> {{of the mean}} is a biased estimator of the population <b>standard</b> <b>error.</b> With n = 2 the underestimate is about 25%, but for n = 6 the underestimate is only 5%. Gurland and Tripathi (1971) provide a correction and equation for this effect. Sokal and Rohlf (1981) give an equation of the correction factor for small samples of n < 20. See unbiased estimation of standard deviation for further discussion.|$|R
30|$|The {{average of}} the {{estimates}} from bootstrapping, used in both Methods 1 and 2 to estimate the <b>standard</b> <b>error</b> of the estimate of the total basal area, agreed closely with the actual <b>standard</b> <b>error</b> of the 50, 000 simulation estimates, that is, bootstrapping as applied here {{appeared to be an}} appropriate method to estimate the <b>standard</b> <b>error</b> for any one sample.|$|R
3000|$|... 15 Two issues {{arise in}} the {{specification}} of equation (9). First, we note that measurement error in the estimatio of ψ could bias the correction of the average wage in the industry that removes the non‐portable part. Notice, however, that for both France and the United States, our estimates of ψ are very precise. For the United States, the industry estimates of ψ (industry average firm effect) displayed in Additional file 1 : Table A 1 have an average <b>standard</b> <b>error</b> of 0.0004 (minimum <b>standard</b> <b>error</b> 0.00007, maximum <b>standard</b> <b>error</b> 0.0037). For France, the industry‐average firm effects displayed in Table A 2 have an average <b>standard</b> <b>error</b> of 0.0027 (minimum <b>standard</b> <b>error</b> 0.0004, maximum <b>standard</b> <b>error</b> of 0.0246). For both countries, then, the precision of estimation {{of the components of}} the raw industry wage differential is comparable to the precision of the estimates of the industry‐level national income account data used to compute the revenue and fixed assets estimates used in the equation.|$|R
2500|$|The American Statistical Association {{issued an}} April 8, 2014 {{statement}} criticizing {{the use of}} value-added models in educational assessment, without ruling out the usefulness of such models. [...] The ASA cited limitations of input data, the influence of factors {{not included in the}} models, and large <b>standard</b> <b>errors</b> resulting in unstable year-to-year rankings.|$|E
2500|$|Using {{these data}} it is {{possible}} to investigate the effect of sample size on the <b>standard</b> <b>errors</b> of the mean and median. [...] The observed mean is 3.16, the observed raw median is 3 and the observed interpolated median is 3.174. [...] The following table gives some comparison statistics. [...] The standard error of the median is given both from the above expression for [...] and from the asymptotic approximation given earlier.|$|E
2500|$|Various {{models have}} been created that allow for heteroscedasticity, i.e. the errors for {{different}} response variables may have different variances. [...] For example, weighted least squares is a method for estimating linear regression models when the response variables may have different error variances, possibly with correlated errors. (See also Weighted linear least squares, and Generalized least squares.) Heteroscedasticity-consistent <b>standard</b> <b>errors</b> is an improved method for use with uncorrelated but potentially heteroscedastic errors.|$|E
30|$|The {{accuracy}} is 0.785, the error rate is 0.215, and the <b>standard</b> <b>error</b> is 0.040. Applying the <b>standard</b> <b>error,</b> {{we get a}} 95 % confidence interval with the range [0.707, 0.863].|$|R
5000|$|The <b>standard</b> <b>error</b> of {{measurement}} is {{the variation in}} scores due to unreliability of the scale or measure used. Thus a change smaller than the <b>standard</b> <b>error</b> {{of measurement}} {{is likely to be}} the result of measurement error rather than a true observed change. Patients achieving a difference in outcome score of at least one <b>standard</b> <b>error</b> of measurement would have achieved a minimal clinically important difference.|$|R
50|$|The {{relative}} <b>standard</b> <b>error</b> of {{a sample}} mean is the <b>standard</b> <b>error</b> divided by the mean and expressed as a percentage. It can only be calculated if the mean is a non-zero value.|$|R
2500|$|In the Newsweek poll, Kerry's {{level of}} support p = 0.47 and n = 1,013. The {{standard}} error (.016 or 1.6%) helps to {{give a sense of}} the accuracy of Kerry's estimated percentage (47%). A Bayesian interpretation of the standard error is that although we do not know the [...] "true" [...] percentage, it is highly likely to be located within two <b>standard</b> <b>errors</b> of the estimated percentage (47%). [...] The standard error can be used to create a confidence interval within which the [...] "true" [...] percentage should be to a certain level of confidence.|$|E
2500|$|Several {{researchers}} have conducted statistical meta-analyses of the employment {{effects of the}} minimum wage. In 1995, Card and Krueger analyzed 14 earlier time-series studies on minimum wages and {{concluded that there was}} clear evidence of publication bias (in favor of studies that found a statistically significant negative employment effect). They point out that later studies, which had more data and lower <b>standard</b> <b>errors,</b> did not show the expected increase in t-statistic (almost all the studies had a t-statistic of about two, just above the level of statistical significance at the [...]05 level). Though a serious methodological indictment, opponents of the minimum wage largely ignored this issue; as Thomas Leonard noted, [...] "The silence is fairly deafening." ...|$|E
2500|$|Constant {{variance}} (a.k.a. homoscedasticity). [...] This {{means that}} different {{values of the}} response variable have the same variance in their errors, regardless {{of the values of}} the predictor variables. In practice this assumption is invalid (i.e. the errors are heteroscedastic) if the response variable can vary over a wide scale. In order to check for heterogeneous error variance, or when a pattern of residuals violates model assumptions of homoscedasticity (error is equally variable around the 'best-fitting line' for all points of x), it is prudent to look for a [...] "fanning effect" [...] between residual error and predicted values. This is to say there will be a systematic change in the absolute or squared residuals when plotted against the predictive variables. Errors will not be evenly distributed across the regression line. Heteroscedasticity will result in the averaging over of distinguishable variances around the points to get a single variance that is inaccurately representing all the variances of the line. In effect, residuals appear clustered and spread apart on their predicted plots for larger and smaller values for points along the linear regression line, and the mean squared error for the model will be wrong. Typically, for example, a response variable whose mean is large will have a greater variance than one whose mean is small. For example, a given person whose income is predicted to be $100,000 may easily have an actual income of $80,000 or $120,000 (a standard deviation of around $20,000), while another person with a predicted income of $10,000 is unlikely to have the same $20,000 standard deviation, which would imply their actual income would vary anywhere between -$10,000 and $30,000. (In fact, as this shows, in many cases—often the same cases where the assumption of normally distributed errors fails—the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) Simple linear regression estimation methods give less precise parameter estimates and misleading inferential quantities such as <b>standard</b> <b>errors</b> when substantial heteroscedasticity is present. However, various estimation techniques (e.g. weighted least squares and heteroscedasticity-consistent <b>standard</b> <b>errors)</b> can handle heteroscedasticity in a quite general way. Bayesian linear regression techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the response variable (e.g. fit the logarithm of the response variable using a linear regression model, which implies that the response variable has a log-normal distribution rather than a normal distribution).|$|E
5000|$|In statistics, the {{estimate}} of a proportion of a sample (denoted by p) has a <b>standard</b> <b>error</b> (<b>standard</b> deviation of <b>error)</b> given by: ...|$|R
3000|$|Notice {{that the}} <b>standard</b> <b>error</b> of the {{estimated}} treatment effect in column 2 is smaller than the corresponding <b>standard</b> <b>error</b> in column 1. Furthermore, the R-square value increases when these control variables X [...]...|$|R
50|$|In particular, the <b>standard</b> <b>error</b> of {{a sample}} {{statistic}} (such as sample mean) is the actual or estimated standard deviation of the error in {{the process by which}} it was generated. In other words, it is the actual or estimated standard deviation of the sampling distribution of the sample statistic. The notation for <b>standard</b> <b>error</b> can be any one of SE, SEM (for <b>standard</b> <b>error</b> of measurement or mean), or SE.|$|R
50|$|The {{topic of}} heteroscedasticity-consistent (HC) <b>standard</b> <b>errors</b> arises in {{statistics}} and econometrics {{in the context}} of linear regression as well as time series analysis. These are also known as Eicker-Huber-White <b>standard</b> <b>errors</b> (also Huber-White <b>standard</b> <b>errors</b> or White <b>standard</b> <b>errors),</b> to recognize the contributions of Friedhelm Eicker, Peter J. Huber, and Halbert White.|$|E
5000|$|Heteroscedasticity-{{consistent}} <b>standard</b> <b>errors</b> (HCSE), {{while still}} biased, improve upon OLS estimates. [...] HCSE is a consistent estimator of <b>standard</b> <b>errors</b> in regression models with heteroscedasticity. This method corrects for heteroscedasticity without altering {{the values of}} the coefficients. This method may be superior to regular OLS because if heteroscedasticity is present it corrects for it, however, if the data is homoscedastic, the <b>standard</b> <b>errors</b> are equivalent to conventional <b>standard</b> <b>errors</b> estimated by OLS. Several modifications of the White method of computing heteroscedasticity-consistent <b>standard</b> <b>errors</b> have been proposed as corrections with superior finite sample properties.|$|E
50|$|Although serial {{correlation}} {{does not affect}} the consistency of the estimated regression coefficients, it does affect our ability to conduct valid statistical tests. First, the F-statistic to test for overall significance of the regression may be inflated under positive {{serial correlation}} because the mean squared error (MSE) will tend to underestimate the population error variance. Second, positive serial correlation typically causes the ordinary least squares (OLS) <b>standard</b> <b>errors</b> for the regression coefficients to underestimate the true <b>standard</b> <b>errors.</b> As a consequence, if positive serial correlation is present in the regression, standard linear regression analysis will typically lead us to compute artificially small <b>standard</b> <b>errors</b> for the regression coefficient. These small <b>standard</b> <b>errors</b> will cause the estimated t-statistic to be inflated, suggesting significance where perhaps there is none. The inflated t-statistic, may in turn, lead us to incorrectly reject null hypotheses, about population values of the parameters of the regression model more often than we would if the <b>standard</b> <b>errors</b> were correctly estimated.|$|E
5000|$|... where [...] is the <b>standard</b> <b>error</b> of {{the maximum}} {{likelihood}} estimate (MLE). A reasonable estimate of the <b>standard</b> <b>error</b> for the MLE can be given by , where [...] is the Fisher information of the parameter.|$|R
2500|$|In particular, the <b>standard</b> <b>error</b> of each {{coefficient}} [...] {{is equal}} to square root of the j-th diagonal element of this matrix. The estimate of this <b>standard</b> <b>error</b> is obtained by replacing the unknown quantity σ2 with its estimate s2. Thus, ...|$|R
50|$|A {{variety of}} choices of {{measures}} of ‘study precision’ is available, including total sample size, <b>standard</b> <b>error</b> {{of the treatment}} effect, and inverse variance of the treatment effect (weight). Sterne and Egger have compared these with others, and conclude that the <b>standard</b> <b>error</b> is to be recommended.When the <b>standard</b> <b>error</b> is used, straight lines may be drawn to define a region within which 95% of points might lie {{in the absence of}} both heterogeneity and publication bias.|$|R
