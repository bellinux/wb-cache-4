2284|1519|Public
25|$|Approximately 60,000 {{households}} {{are eligible for}} the CPS. Sample {{households are}} selected by a multistage stratified statistical <b>sampling</b> <b>scheme.</b> A household is interviewed for 4 successive months, then not interviewed for 8 months, {{then returned to the}} sample for 4 months after that. An adult member of each household provides information for all members of the household.|$|E
25|$|For example, {{consider}} a street where the odd-numbered houses {{are all on}} the north (expensive) side of the road, and the even-numbered houses are all on the south (cheap) side. Under the <b>sampling</b> <b>scheme</b> given above, {{it is impossible to}} get a representative sample; either the houses sampled will all be from the odd-numbered, expensive side, or they will all be from the even-numbered, cheap side, unless the researcher has previous knowledge of this bias and avoids it by a using a skip which ensures jumping between the two sides (any odd-numbered skip).|$|E
25|$|Biodiversity {{inventory}} – An interspecific O–A relationship {{implies that}} those species {{that have a}} restricted distribution (and hence will be important for conservation reasons) will also have low abundance within their range. Thus, when {{it is especially important}} that a species be detected, that species may be difficult to detect. Gaston et al. note that because of this relationship, the intensiveness of a <b>sampling</b> <b>scheme</b> cannot be traded off for extensiveness. In effect, an intensive survey of a few sites will miss species with restricted distribution occurring at other sites, while an low-intensity extensive survey will miss species with low densities across most sites.|$|E
40|$|We first review {{existing}} sequential {{methods for}} estimating a binomial proportion. Afterward, we propose {{a new family}} of group sequential <b>sampling</b> <b>schemes</b> for estimating a binomial proportion with prescribed margin of error and confidence level. In particular, we establish the uniform controllability of coverage probability and the asymptotic optimality for such a family of <b>sampling</b> <b>schemes.</b> Our theoretical results establish {{the possibility that the}} parameters of this family of <b>sampling</b> <b>schemes</b> can be determined so that the prescribed level of confidence is guaranteed with little waste of samples. Analytic bounds for the cumulative distribution functions and expectations of sample numbers are derived. Moreover, we discuss the inherent connection of various <b>sampling</b> <b>schemes.</b> Numerical issues are addressed for improving the accuracy and efficiency of computation. Computational experiments are conducted for comparing <b>sampling</b> <b>schemes.</b> Illustrative examples are given for applications in clinical trials. Comment: 38 pages, 9 figure...|$|R
40|$|A {{sampling}} design is proposed {{for which a}} new product estimator is unbiased, and therefore {{it is possible to}} build associate <b>sampling</b> <b>schemes.</b> This result completes the theory of <b>sampling</b> <b>schemes</b> providing unbiased ratio estimators and regression estimators given respectively for the first time by Hájek (1949) and Singh and Srivastava (1980) ...|$|R
40|$|This paper {{introduces}} a general class of Inclusion Probability Proportional to Size (IPPS) <b>sampling</b> <b>schemes</b> for selecting two units from a finite population. All IPPS <b>sampling</b> <b>schemes,</b> identified as particular {{members of this}} class, possess some desirable properties {{with regard to the}} inclusion probabilities, and provide unbiased and non-negative variance estimators under Horvitz-Thomson (HT) model...|$|R
50|$|From U, the {{periodicity}} matrix, we {{can calculate}} the optimal sampling density {{for both the}} rectangular and hexagonal schemes. It is found {{that in order to}} completely recover the circularly band-limited signal, the hexagonal <b>sampling</b> <b>scheme</b> requires 13.4% fewer samples than the rectangular <b>sampling</b> <b>scheme.</b> The reduction may appear to be of little significance for a 2-dimensional signal. But as the dimensionality of the signal increases, the efficiency of the hexagonal <b>sampling</b> <b>scheme</b> will become far more evident. For instance, the reduction achieved for an 8-dimensional signal is 93.8%. To highlight the importance of the obtained result http://www.springerreference.com/docs/html/chapterdbid/318221.html, try and visualize an image as a collection of infinite number of samples. The primary entity responsible for vision, i.e. the photoreceptors (rods and cones) are present on the retina of all mammals. These cells are not arranged in rows and columns. By adapting a hexagonal <b>sampling</b> <b>scheme,</b> our eyes are able to process images much more efficiently. The importance of hexagonal sampling {{lies in the fact that}} the photoreceptors of the human vision system lie on a hexagonal sampling lattice and, thus, perform hexagonal sampling http://hyperphysics.phy-astr.gsu.edu/hbase/vision/rodcone.html. In fact, it can be shown that the hexagonal <b>sampling</b> <b>scheme</b> is the optimal <b>sampling</b> <b>scheme</b> for a circularly band-limited signal.|$|E
50|$|In 1952 Midzuno and Sen {{independently}} {{described a}} <b>sampling</b> <b>scheme</b> that provides an unbiased estimator of the ratio.|$|E
5000|$|When {{sampling}} {{a function}} of [...] variables, the range of each variable is divided into [...] equally probable intervals. [...] sample points are then placed to satisfy the Latin hypercube requirements; note that this forces the number of divisions, , to be equal for each variable. Also note that this <b>sampling</b> <b>scheme</b> does not require more samples for more dimensions (variables); this independence {{is one of the}} main advantages of this <b>sampling</b> <b>scheme.</b> Another advantage is that random samples can be taken one at a time, remembering which samples were taken so far.|$|E
40|$|Abstract. Estimation {{of states}} and events in {{randomly}} switching systems is studied under irregular and random <b>sampling</b> <b>schemes.</b> Probabilistic characterization of observability is presented under various <b>sampling</b> <b>schemes</b> and regime-switching processes. The characterization is derived {{on the basis of}} our recent results on sampling complexity for system observability. Observer design and algorithms are developed. 1. Introduction. Thi...|$|R
40|$|In this paper, we have {{established}} a new framework of multistage parametric estimation. Specially, we have developed <b>sampling</b> <b>schemes</b> for estimating parameters of common important distributions. Without any information of the unknown parameters, our <b>sampling</b> <b>schemes</b> rigorously guarantee prescribed levels of precision and confidence, while achieving unprecedented efficiency {{in the sense that}} the average sample numbers are virtually the sam...|$|R
40|$|An {{interactive}} {{sampling procedure}} is proposed to optimize environmental risk assessment. Subsequent sampling stages {{were used as}} quantitative pre-information. With this pre-information probability maps were made using indicator kriging to direct subsequent sampling. In this way, optimal use of the remaining sampling stages was guaranteed. Interactive sampling was applied to a lead-pollution in the Dutch city of Schoonhoven. Environmental risks were quantified by the probability of exceeding the intervention level. The data and <b>sampling</b> <b>schemes</b> were stored in a GIS. Using six conditional simulations of stochastic fields, interactive <b>sampling</b> <b>schemes</b> were compared to conventional <b>sampling</b> <b>schemes</b> by calculating type I and type II errors. The interactive schemes had much lower type I errors than the conventional schemes, and comparable type II errors. Moreover, the interactive <b>sampling</b> <b>schemes</b> left a smaller fraction of the not-sanitated area polluted than the conventional ones did. They predicted almost 70 % of the area correctly, as compared to 55 % by conventional schemes...|$|R
50|$|Social polling {{is a form}} of {{open access}} polling, which {{combines}} social media and opinion polling. In contrast to tradition polling the polls are formulated by the respondents themselves. Social polling is an example of nonprobability sampling that uses self-selection rather than a statistical <b>sampling</b> <b>scheme.</b>|$|E
5000|$|If one {{associates}} {{draws from}} the base measure [...] with every table, the resulting distribution over the sample space [...] is {{a random sample of}} a Dirichlet process.The Chinese restaurant process is related to the Pólya urn <b>sampling</b> <b>scheme</b> which yields samples from finite Dirichlet distributions.|$|E
50|$|The proportionator {{adjusts the}} <b>sampling</b> <b>scheme</b> to select samples {{that are likely}} to provide {{estimates}} that have a smaller difference. Thus the variance of the estimator is addressed without changing the workload. That results in a gain in efficiency due to the reduction in variance for a given cost.|$|E
40|$|In this paper, we {{consider}} M-estimators {{of the regression}} parameter in a spatial multiple linear regression model. We establish consistency and asymptotic normality of the M-estimators when the data-sites are generated by a class of deterministic {{as well as a}} class of stochastic spatial <b>sampling</b> <b>schemes.</b> Under the deterministic <b>sampling</b> <b>schemes,</b> the data-sites are located on a regular grid but may have an infill component. On the other hand, under the stochastic <b>sampling</b> <b>schemes,</b> locations of the data-sites are given by the realizations of a collection of independent random vectors and thus, are irregularly spaced. It is shown that scaling constants of different orders are needed for asymptotic normality under different spatial <b>sampling</b> <b>schemes</b> considered here. Further, in the stochastic case, the asymptotic covariance matrix is shown to depend on the spatial sampling density associated with the stochastic design. Results are established for M-estimators corresponding to certain non-smooth score functions including Huber's e-function and the sign functions (corresponding to the sample quantiles) ...|$|R
40|$|Several data {{acquisition}} schemes for diffusion MRI {{have been proposed}} and explored to date for {{the reconstruction of the}} 2 nd order tensor. Our main contributions in this paper are: (i) the definition of a new class of <b>sampling</b> <b>schemes</b> based on repeated measurements in every sampling point; (ii) two novel schemes belonging to this class; and (iii) a new reconstruction framework for the second scheme. We also present an evaluation, based on Monte Carlo computer simulations, of the performances of these schemes relative to known optimal <b>sampling</b> <b>schemes</b> for both 2 nd and 4 th order tensors. The results demonstrate that tensor estimation by the proposed <b>sampling</b> <b>schemes</b> and estimation framework is more accurate and robust...|$|R
40|$|Use of {{ranks in}} unequal {{probability}} sampling is examined for sample selection, stratification {{as well as}} determining the strata boundaries. A few <b>sampling</b> <b>schemes</b> are proposed and investigated, For samples of size two, two <b>sampling</b> <b>schemes</b> and their 1 PPS versions are discussed, An extension of these <b>schemes</b> to general <b>sample</b> sizes is outlined. Nonnegative unbiased variance estimators are proposed in each case, An empirical comparison is included...|$|R
50|$|All these samples {{must then}} be {{averaged}} {{to obtain the}} output color. Note this method of always sampling a random ray in the normal's hemisphere only works well for perfectly diffuse surfaces. For other materials, one generally has to use importance-sampling, i.e. probabilistically select a new ray according to the BRDF's distribution. For instance, a perfectly specular (mirror) material would not work with the method above, as the probability of the new ray being the correct reflected ray - {{which is the only}} ray through which any radiance will be reflected - is zero. In these situations, one must divide the reflectance by the probability density function of the <b>sampling</b> <b>scheme,</b> as per Monte-Carlo integration (in the naive case above, there is no particular <b>sampling</b> <b>scheme,</b> so the PDF turns out to be 1).|$|E
50|$|One of {{the reasons}} to adopt NDFT is that many signals have their energy {{distributed}} nonuniformly in the frequency domain. Therefore, a nonuniform <b>sampling</b> <b>scheme</b> could be more convenient and useful in many Digital Signal Processing (DSP) applications. For example, NDFT provides a variable spectral resolution controlled by the user.|$|E
5000|$|There {{are other}} {{considerations}} {{to take into}} account to ensure conservation of energy. In particular, in the naive case, the reflectance of a diffuse BRDF must not exceed [...] or the object will reflect more light than it receives (this however depends on the <b>sampling</b> <b>scheme</b> used, and can be difficult to get right).|$|E
40|$|Recently, new <b>sampling</b> <b>schemes</b> were {{presented}} for signals with finite rate of innovation (FRI) using sampling kernels reproducing polynomials or exponentials [1] [2]. In this paper, we extend those <b>sampling</b> <b>schemes</b> to a distributed acquisition architecture in which numerous and randomly located sensors are {{pointing to the}} same area of interest. We emphasize the importance played by moments and show how to acquire efficiently FRI signals {{with a set of}} sensors. More importantly, we also show that those <b>sampling</b> <b>schemes</b> can be used for accurate registration of affine transformed and low-resolution images. Based on this, a new super-resolution algorithm was developed and showed good preliminary results. Index Terms — Moment methods, image registration, image resolution, image sampling, image reconstruction, spline functions, distributed algorithms. 1...|$|R
3000|$|... -norm {{spectrum}} reconstruction scheme {{can be used}} {{to break}} through the bandwidth barrier of existing <b>sampling</b> <b>schemes</b> in CRNs.|$|R
40|$|Based on {{auxiliary}} information, certain new ratio-type {{strategies for}} estimating the population variance, {{aside from the}} existing ones that are biased, have been proposed {{with a view to}} achieving unbiasedness for them via appropriate <b>sampling</b> <b>schemes.</b> While mooting one of the estimators, consideration has been shown to the fact that knowledge of the population mean is available. Relative performance of these estimators has also been examined. Unbiased estimators of the population variance <b>Sampling</b> <b>schemes...</b>|$|R
50|$|Approximately 60,000 {{households}} {{are eligible for}} the CPS. Sample {{households are}} selected by a multistage stratified statistical <b>sampling</b> <b>scheme.</b> A household is interviewed for 4 successive months, then not interviewed for 8 months, {{then returned to the}} sample for 4 months after that. An adult member of each household provides information for all members of the household.|$|E
50|$|For example, {{consider}} a street where the odd-numbered houses {{are all on}} the north (expensive) side of the road, and the even-numbered houses are all on the south (cheap) side. Under the <b>sampling</b> <b>scheme</b> given above, {{it is impossible to}} get a representative sample; either the houses sampled will all be from the odd-numbered, expensive side, or they will all be from the even-numbered, cheap side, unless the researcher has previous knowledge of this bias and avoids it by a using a skip which ensures jumping between the two sides (any odd-numbered skip).|$|E
50|$|Every line {{apart from}} the CalCOFI <b>sampling</b> <b>scheme</b> and its {{corresponding}} stations has experienced some degree of difference and variation in spatial and temporal sampling frequency. Furthermore, technological advances have allowed increasing amounts of new chemical, physical, and biological properties to be measured within the water column. Line 90, which {{is a part of}} the core CalCOFI station domain positioned across the mid-Southern California Bight, is the best-sampled and most visited line in the time series. The data from Line 90 is used in many transect figures and analyses.|$|E
40|$|We {{explore the}} {{possibilities}} of obtaining compression in video through modified sampling strategies using multichannel imaging systems. The redundancies in video streams are exploited through compressive <b>sampling</b> <b>schemes</b> to achieve low power and low complexity video sensors. The sampling strategies {{as well as the}} associated reconstruction algorithms are discussed. These compressive <b>sampling</b> <b>schemes</b> could be implemented in the focal plane readout hardware resulting in drastic reduction in data bandwidth and computational complexity...|$|R
40|$|In {{this note}} we develop a prelimit {{analysis}} of performance measures for importance <b>sampling</b> <b>schemes</b> related to small noise diffusion processes. In importance sampling {{the performance of}} any change of measure is characterized by its second moment. For a given change of measure, we characterize the second moment of the corresponding estimator as the solution to a PDE, which we analyze via a full asymptotic expansion {{with respect to the}} size of the noise and obtain a precise statement on its accuracy. The main correction term to the decay rate of the second moment solves a transport equation that can be solved explicitly. The asymptotic expansion that we obtain identifies the source of possible poor performance of nevertheless asymptotically optimal importance <b>sampling</b> <b>schemes</b> and allows for more accurate comparison among competing importance <b>sampling</b> <b>schemes...</b>|$|R
40|$|This article {{compares the}} {{behaviour}} of sampling techniques for price indices using a scanner data set {{as a model}} population. Indices produced by two purposive deterministic cut-off designs and four probabilistic <b>sampling</b> <b>schemes</b> are compared {{with each other and}} with the ‘true’ population index from the whole data set. We found that the two deterministic cut-off <b>sampling</b> <b>schemes</b> show much different behaviour from the probabilistic <b>sampling</b> <b>schemes.</b> This is not unexpected, as the former schemes have a very restricted focus with respect to the variety of products. We also found that the probabilistic schemes are generally closer to each other and the ‘true’ value than the deterministic cut-off designs. The jackknife resampling technique is also explored as a means of estimating the SE of the index and compared with the actual results from repeated sampling...|$|R
50|$|H.261 was {{originally}} designed for transmission over ISDN lines on which data rates are multiples of 64 kbit/s. The coding algorithm {{was designed to}} be able to operate at video bit rates between 40 kbit/s and 2 Mbit/s. The standard supports two video frame sizes: CIF (352×288 luma with 176×144 chroma) and QCIF (176×144 with 88×72 chroma) using a 4:2:0 <b>sampling</b> <b>scheme.</b> It also has a backward-compatible trick for sending still images with 704×576 luma resolution and 352×288 chroma resolution (which was added in a later revision in 1993).|$|E
5000|$|A multistage, random cluster {{process was}} used to draw the sample {{surveyed}} {{in each of the}} provinces. Counties in the 9 provinces were stratified by income (low, middle, and high) and a weighted <b>sampling</b> <b>scheme</b> {{was used to}} randomly select four counties in each province. In addition, the provincial capital and a lower income city were selected when feasible, except that other large cities rather than provincial capitals had to be selected in two provinces. Villages and townships within the counties and urban and suburban neighborhoods within the cities were selected randomly.|$|E
50|$|The {{output of}} a {{universal}} quantum computer running, for example, Shor's factoring algorithm, can be efficiently verified classically, {{as is the case}} for all problems inthe non-deterministic polynomial-time (NP) complexity class. It is however not clear that a similar structureexists for the boson <b>sampling</b> <b>scheme.</b> Namely, as the latter is related to the problem of estimating matrix permanents (falling into #P-hard complexity class), it is not understood how to verify correct operation for large versions of the setup. Specifically, the naive verification of the output of a boson sampler by computing the corresponding measurement probabilities represents a problem intractable for a classical computer.|$|E
40|$|There {{are various}} {{importance}} <b>sampling</b> <b>schemes</b> to estimate rare event probabilities in Markovian {{systems such as}} Markovian reliability models and Jackson networks. In this work, we present a general state dependent importance sampling method which partitions the state space and applies the cross-entropy method to each partition. We investigate two versions of our algorithm and apply them to several examples of reliability and queueing models. In all these examples we compare our method with other importance <b>sampling</b> <b>schemes.</b> The performance of the importance <b>sampling</b> <b>schemes</b> {{is measured by the}} relative error of the estimator and by the effciency of the algorithm. The results from experiments show considerable improvements both in running time of the algorithm and the variance of the estimator. Cross-Entropy, Rare Events, Importance Sampling, Large-Scale Markov Chains...|$|R
40|$|Continuous {{sampling}} {{is one of}} {{the common}} approaches for assessing indoor pollutant level. It is believed that the longer the measurement time, the higher the accuracy and confidence level of the measurement can be achieved. In 2003, the Hong Kong Environmental Protection Department (HKEPD) launched an Indoor Air Quality (IAQ) certification scheme to promote an acceptable IAQ in workplaces. However, measurement efforts and uncertainties associated with the sampling method have not been addressed. Alternative <b>sampling</b> <b>schemes</b> taking shorter measurements in the sampling period were proposed in some circumstances. In this study, the average carbon dioxide (CO 2) concentration of a workplace is selected as an indicator of the indoor air quality to investigate the probable errors and measurement efforts in four <b>sampling</b> <b>schemes,</b> regarding the <b>sampling</b> period: in <b>Scheme</b> A, it is from a continuous sampling throughout the measurement; in Scheme B, it is from two sampling periods of two equal sessions of the measurement; in Scheme C, it is from two structural sampling periods of the two sessions; and in Scheme D, the average concentration is from four sampling periods in four equal sessions of the measurement. In particular, a year-round indoor CO 2 concentration at 17 locations in a typical office in Hong Kong was used to evaluate the probable errors using these four <b>sampling</b> <b>schemes.</b> At certain confidence levels, the required measurement times of the alternative <b>sampling</b> <b>schemes</b> (Schemes B, C, D) were evaluated and compared with that of an 8 -hour continuous one (Scheme A). It was found that Scheme C would offer a reduction of measurement effort up to 30 %. It is recommended to specify the uncertainties and efforts of measurement in future codes, and to consider these <b>sampling</b> <b>schemes</b> in determining practical strategies for IAQ measurement. Department of Building Services Engineerin...|$|R
40|$|This paper {{deals with}} the problem of perfect {{reconstruction}} of the spectrum of a weakly stationary stochastic process x(t) from a set of random samples {x(tn) }. A broad class of random <b>sampling</b> <b>schemes,</b> for which the sampling intervals are dependent, is constructed. It is shown that this class is “alias free≓ relative to various families of spectra. It is further shown that the alias free property of this class of <b>sampling</b> <b>schemes</b> is invariant under random deletion of samples...|$|R
