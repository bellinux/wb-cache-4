351|638|Public
25|$|Random {{error is}} the result of {{fluctuations}} around a true value because of <b>sampling</b> <b>variability.</b> Random error is just that: random. It can occur during data collection, coding, transfer, or analysis. Examples of random error include: poorly worded questions, a misunderstanding in interpreting an individual answer from a particular respondent, or a typographical error during coding. Random error affects measurement in a transient, inconsistent manner and it is impossible to correct for random error.|$|E
25|$|A {{systematic}} error or bias occurs {{when there is}} a difference between the true value (in the population) and the observed value (in the study) from any cause other than <b>sampling</b> <b>variability.</b> An example of {{systematic error}} is if, unknown to you, the pulse oximeter you are using is set incorrectly and adds two points to the true value each time a measurement is taken. The measuring device could be precise but not accurate. Because the error happens in every instance, it is systematic. Conclusions you draw based on that data will still be incorrect. But the error can be reproduced in the future (e.g., by using the same mis-set instrument).|$|E
50|$|Balanced {{repeated}} replication is {{a statistical}} technique for estimating the <b>sampling</b> <b>variability</b> of a statistic obtained by stratified sampling.|$|E
30|$|Principal {{component}} {{analysis of the}} 947 superimposed configurations {{in the space of}} the 78 form variables showed a substantial proportion of the total <b>sample</b> <b>variability</b> in the first four PCs (26 %, 10 %, 8 %, and 5 %, resp.). The variance on PCs beyond the third (all 5 % or less of the total) trail off gradually suggesting no strong patterns of intercorrelation amongst the variables. Nonetheless, the first two PCs together only represent 36 % of total <b>sample</b> <b>variability</b> and the first four only 49 %. In fact, it requires the first 27 PCs as a group to account for 90 % of total sample variation. This suggests that the bivariate approach used in constructing fit panels may be ignoring a substantial and important aspect of total <b>sample</b> <b>variability.</b>|$|R
5000|$|As {{with the}} [...] and R and {{individuals}} control charts, the [...] chart {{is only valid}} if the within-sample variability is constant. Thus, the s chart is examined before the [...] chart; if the s chart indicates the <b>sample</b> <b>variability</b> is in statistical control, then the [...] chart is examined {{to determine if the}} sample mean is also in statistical control. If on the other hand, the <b>sample</b> <b>variability</b> is not in statistical control, then the entire process is judged to be not in statistical control regardless of what the [...] chart indicates.|$|R
30|$|In {{order to}} make correct {{inference}} about the mismatch indicators reported in Section 5, {{it is necessary to}} take into proper account both the <b>sample</b> <b>variability</b> and the imputed nature of the skill measures (OECD 2013).|$|R
5000|$|... "Predicting Probabilities: Inherent and <b>Sampling</b> <b>Variability</b> in the Estimation of Discrete-Choice Models" [...] with D. Gordon, Z. Lin, and S. Phipps, Oxford Bulletin of Economics and Statistics, Vol. 56, No. 1, February 1994, pp. 13-31.|$|E
5000|$|The {{southern}} area of Oman, Yemen and Ethiopia. In this area, [...] recognize similar signs, but {{reject it}} as possible {{a result of}} [...] "either <b>sampling</b> <b>variability</b> and/or demographic complexity associated with multiple founders and multiple migrations." ...|$|E
50|$|In survey sampling, total survey error {{includes}} {{all forms of}} survey error including <b>sampling</b> <b>variability,</b> interviewer effects, frame errors, response bias, and non-response bias. Total survey error is discussed in detail in many sources including Salant and Dillman.|$|E
40|$|Abstract. A power {{analysis}} allows {{estimation of the}} probability of detecting upward or downward trends in abundance using linear regression, given number of samples and estimates of <b>sample</b> <b>variability</b> and rate of change. Alternatively, the minimum number or precision of samples required to detect trends with a given degree of confidence can be computed. The results are applicable to an experimental situation in which samples are taken at regular intervals in time or space. The effects of linear and exponential change and of having <b>sample</b> <b>variability</b> {{be a function of}} abundance are investigated. Results are summarized graphically and, as an example, applied to the monitoring of the California sea otter population with aerial surveys...|$|R
30|$|Twenty {{batches of}} bananas (Musa AAA Cavendish) cv. Brazil were {{purchased}} {{from a local}} wholesale market. To avoid the <b>sample</b> <b>variability</b> of the <b>samples,</b> 197 bananas with similar size, color, and weight (200 – 245 g) were selected for the following analysis.|$|R
40|$|Ever {{since the}} days of Francis Bacon it has been claimed that people {{perceive}} the world as less variable and more regular than it actually is. Such misperception, if shown to exist, could explain a host of perplexing behaviors. However, the only evidence supporting the claim is indirect, and there is no explanation of its cause. As a possible cause, we suggest the use of <b>sample</b> <b>variability</b> as an estimate of population variability. This is so since the sampling distribution of sample variance is downward attenuated, the attenuation being substantial for sample sizes that people are likely to consider. The results of five experiments show that people use <b>sample</b> <b>variability,</b> uncorrected for <b>sample</b> size, in tasks in which a correction is normatively called for, and indeed perceive variability as smaller than it actually is. ...|$|R
5000|$|Properties of aggregates. This {{includes}} {{the ideas of}} distributions, signal (a stable component of population/process such as averages) and noise (a variable component of population/process such as the deviations of individual value around an average) and types of 'noise' or variability (measurement variability, natural variability, <b>sampling</b> <b>variability).</b>|$|E
50|$|In New Zealand, a new {{curriculum}} for statistics {{has been developed}} by Chris Wild and colleagues at Auckland University. Rejecting the contrived, and now unnecessary due to computer power, approach of reasoning under the null and the restrictions of normal theory, they use comparative box plots and bootstrap to introduce concepts of <b>sampling</b> <b>variability</b> and inference. The developing curriculum also contains aspects of statistical literacy.|$|E
50|$|Random {{error is}} the result of {{fluctuations}} around a true value because of <b>sampling</b> <b>variability.</b> Random error is just that: random. It can occur during data collection, coding, transfer, or analysis. Examples of random error include: poorly worded questions, a misunderstanding in interpreting an individual answer from a particular respondent, or a typographical error during coding. Random error affects measurement in a transient, inconsistent manner and it is impossible to correct for random error.|$|E
40|$|Standard {{bacterial}} suspensions {{can be used}} {{to assess}} test method performance, via control charts, and inhibition of recovery when analyzing water <b>samples.</b> <b>Variability</b> in standard suspensions prepared from different strains and species and the use of frozen environmental samples for quality control for spore and bacteriophage analyses are also discussed...|$|R
40|$|Soil samples {{gathered}} by the Viking Lander {{from the surface of}} Mars were analyzed. The Martian fines were lower in aluminum, iron, sulfur, and chlorine than typical terrestrial continental soils or lunar mare fines. <b>Sample</b> <b>variabilities</b> were as great within a few meters as between lander locations (4500 km apart) implying the existence of a universal Martian regolith component of constant average composition...|$|R
3000|$|... 13 Finite <b>sample</b> <b>variability</b> {{should not}} be a major concern here given the large number of {{observations}} (on average above 1, 000) which are used to estimate the mode of each occupation. See, e.g., Dutta and Goswami (2010), who show that, for a Bernoulli distribution with sample size larger than 100, the mode of the empirical distribution matches the population mode with a probability close to 0.9.|$|R
50|$|To {{improve the}} quality of replications, larger sample sizes than those used in the {{original}} study are often needed. Larger sample sizes are needed because estimates of effect sizes in published work are often exaggerated due to publication bias and large <b>sampling</b> <b>variability</b> associated with small sample sizes in an original study. Further, using significance thresholds usually leads to inflated effects, because particularly with small sample sizes, only the largest effects will become significant.|$|E
50|$|A {{systematic}} error or bias occurs {{when there is}} a difference between the true value (in the population) and the observed value (in the study) from any cause other than <b>sampling</b> <b>variability.</b> An example of {{systematic error}} is if, unknown to you, the pulse oximeter you are using is set incorrectly and adds two points to the true value each time a measurement is taken. The measuring device could be precise but not accurate. Because the error happens in every instance, it is systematic. Conclusions you draw based on that data will still be incorrect. But the error can be reproduced in the future (e.g., by using the same mis-set instrument).|$|E
3000|$|... follow {{approximately}} a standard normal distribution. Deviations only result from <b>sampling</b> <b>variability</b> in [...]...|$|E
30|$|Laboratory spinal biomechanical tests using human cadaveric {{or animal}} spines have {{limitations}} {{in terms of}} disease transmission, high <b>sample</b> <b>variability,</b> decay and fatigue during extended testing protocols. Therefore, a synthetic biomimetic spine model may be an acceptable substitute. The goal of current study is to evaluate the properties of a synthetic biomimetic spine model; also to assess the mechanical performance of lateral plating following lateral interbody fusion.|$|R
40|$|AbstractFor high {{dimensional}} data sets the sample covariance matrix is usually unbiased but noisy if {{the sample is}} not large enough. Shrinking the sample covariance towards a constrained, low dimensional estimator {{can be used to}} mitigate the <b>sample</b> <b>variability.</b> By doing so, we introduce bias, but reduce variance. In this paper, we give details on feasible optimal shrinkage allowing for time series dependent observations...|$|R
30|$|Since {{this study}} data cover olive oils {{included}} in selected guide for 3  years, a panel data analysis was also conducted. The main {{advantages of the}} panel data approach are (i) observing a sample in a temporal and dynamic dimension, (ii) getting information about past attitudes of the analyzed units, and (iii) increasing {{the size of the}} sample allowing more degrees of freedom and more <b>sample</b> <b>variability</b> (Gujarati 2004; Hsiao 2007).|$|R
40|$|International audienceThe {{research}} {{reported here}} uses common items to assess statistical reasoning of teachers {{enrolled in a}} graduate-level education course to evaluate their reasoning about <b>sampling</b> <b>variability.</b> In particular, we discuss key aspects of a purposeful course design aimed at improving teachers’ learning and teaching of statistics, and the resulting different ways of reasoning about <b>sampling</b> <b>variability</b> that teachers exhibited {{before and after the}} course...|$|E
40|$|This paper {{considers}} four {{methods for}} obtaining bootstrap prediction intervals (BPIs) for the self-exciting threshold autoregressive (SETAR) model. Method 1 ignores the <b>sampling</b> <b>variability</b> of the threshold parameter estimator. Method 2 corrects the finite sample biases of the autoregressive coefficient estimators before constructing BPIs. Method 3 {{takes into account}} the <b>sampling</b> <b>variability</b> of both the autoregressive coefficient estimators and the threshold parameter estimator. Method 4 resamples the residuals in each regime separately. A Monte Carlo experiment shows that (1) accounting for the <b>sampling</b> <b>variability</b> of the threshold parameter estimator is necessary, despite its super-consistency; (2) correcting the small-sample biases of the autoregressive parameter estimators improves the small-sample properties of bootstrap prediction intervals under certain circumstances; and (3) the two-sample bootstrap can improve the long-term forecasts when the error terms are regime-dependent. Bootstrap Interval forecasting SETAR models Time series Simulation...|$|E
40|$|We {{studied the}} effects of inter-annual {{variability}} and serial correlation on the statistical power of monitoring schemes to detect trends in biomass of bream (Abramis brama) in Lake Veluwemeer (The Netherlands). In order to distinguish between `true¿ system variability and <b>sampling</b> <b>variability</b> we simulated {{the development of the}} bream population, using estimates for population structure and growth, and compared the resulting inter-annual variabilities and serial correlations with those from field data. In all cases the inter-annual variability in the field data was larger than in simulated data (e. g. for total biomass of all assessed bream ¿¿=¿ 0. 45 in field data, and ¿¿=¿ 0. 03 ¿ 0. 14 in simulated data) indicating that <b>sampling</b> <b>variability</b> decreased statistical power for detecting trends. Moreover, <b>sampling</b> <b>variability</b> obscured the inter-annual dependency (and thus the serial correlation) of biomass, which was expected because in this long-lived population biomass changes are buffered by the many year classes present. We did find the expected serial correlation in our simulation results and concluded that good survey data of long-lived fish populations should show low <b>sampling</b> <b>variability</b> and considerable inter-annual serial correlation. Since serial correlation decreases the power for detecting trends, this means that even when <b>sampling</b> <b>variability</b> would be greatly reduced, the number of sampling years to detect a change of 15 %·year¿ 1 in bream populations (corresponding to a halving or doubling in a six-year period) would in most cases be more than six. This would imply that the six-year reporting periods that are required by the Water Framework Directive of the European Union are too short for the existing fish monitoring schemes...|$|E
40|$|For high {{dimensional}} data sets the sample covariance matrix is usually unbiased but noisy if {{the sample is}} not large enough. Shrinking the sample covariance towards a constrained, low dimensional estimator {{can be used to}} mitigate the <b>sample</b> <b>variability.</b> By doing so, we introduce bias, but reduce variance. In this paper, we give details on feasible optimal shrinkage allowing for time series dependent observations. Sample covariance matrix Shrinkage Weak dependence...|$|R
40|$|Recent {{analyses}} {{have claimed}} the possible presence of non-stationarity, {{produced by the}} presence of either trend or long-term climatic fluctuations, in some historical hydrometeorological records observed in Europe as well as in other countries. Such non-stationarity might exert a remarkable effect on the estimation of the frequency distribution of the extreme events. However, {{it is well known that}} a reliable assessment of the presence of non-stationarity in hydrological records is not an easy task, because of the limited extension of the available data sets. This makes difficult distinguishing between non-stationarity, <b>sample</b> <b>variability</b> and long-term climatic fluctuations. This paper analyses two long rainfall records observed in the Emilia-Romagna region, in Northern Italy, in order to assess whether non-stationarity might be present. The results are compared with the outcomes of regional analyses aimed at assessing the presence of non-stationarity at regional scale. Moreover, some synthetic data are analysed, in order to assess how much the <b>sample</b> <b>variability</b> of a short stationary series might induce effects which could at first glance be ascribed to non-stationarity. ...|$|R
40|$|We {{present results}} from a {{monitoring}} program of 42 quasars from the Palomar-Green sample. The objects were observed for seven years at the Wise Observatory, {{as part of a}} long term effort to monitor AGN of various types. This is the most extensive program of its kind carried out to date on a well-defined optically-selected quasar sample. The typical sampling interval is present lightcurves for all of the sources and discuss the <b>sample</b> <b>variability</b> properties...|$|R
40|$|Project-SET is an NSF funded project {{aimed at}} {{developing}} teacher level materials to better facilitate student learning in statistics. The project {{is focused on}} two statistics topics; <b>sampling</b> <b>variability</b> and linear regression. After an extensive literature review, four instructional videos aimed at introducing <b>sampling</b> <b>variability</b> and regression were created, two videos per topic of interest. To test the teaching capabilities of these videos, a pre- and post-test was administered to two college level statistics courses at Loyola Marymount University before and after they viewed the videos. Participants provided definitions, solved problems, and rated their confidence in answering questions pertaining to each topic. Results indicate that the videos allowed students to gain {{a significant amount of}} confidence in answering questions regarding both topics, however their ability to correctly solve questions on these topics did not improve. Upon further analysis of our data, student misconceptions about <b>sampling</b> <b>variability</b> and linear regression were identified to help direct modifications in future iterations of the instructional videos...|$|E
40|$|In economics, {{the number}} of {{observations}} available for empirical work is often predetermined. Researchers assume some large sample distribution and carry through with measurement and testing applied to data sets of varying sizes. The consequences of <b>sampling</b> <b>variability</b> are generally ignored. It is shown in a re-sampling experiment, using data sets of different sizes and estimating log-linear male labour supply equations, that {{a wide range of}} what appears to be statistically supported estimates of the wage elasticity of labour supply are generated. Testing based on bootstrapped estimates shows that 4000 observations are required to reduce <b>sampling</b> <b>variability</b> to statistically acceptable levels. ...|$|E
30|$|In the frequentist domain, the {{standard}} error of a parameter captures the <b>sampling</b> <b>variability</b> of the parameter under hypothetical repeating sampling from a population; {{it does not}} capture uncertain knowledge about the parameter itself.|$|E
30|$|Lachenbruch et al. (1977) {{studied the}} {{performance}} of the QDF under non-normality. They generated random samples from non-normal distributions and the samples were transformed into components by using Johnson’s system of transformation. Among their findings, they found that, the overall sample standard deviation, the between <b>sample</b> <b>variability</b> of the individual error rates of the function (QDF) under normal or non-normal distributions was quite large. In the computation of the overall sample standard deviation, the between <b>sample</b> <b>variability</b> of the individual error rates in the QDF on normal or non-normal distributions was quite large and for that instability of QDF is pronounced. Also the actual error rates were considerably larger than the optimal rates in the case of zero mean difference (this is a very difficult problem in assignment). The QDF for non-normal samples generally did not do substantially worse than when the QDF was derived under normal samples which were obtained after transformation. Lachenbruch et al. (1977) compared the re-substitution method and the leave-one-out method. The re-substitution method had an unacceptably high bias. The leave-one-out method was far superior in respect of generally having a far lesser bias.|$|R
40|$|Abstract. This work {{presents}} a comparative study between geostatistical methods, the indicator sequential simulation and indicator kriging. Aluminium samples from Santa Catarina State, Brazil, {{are used to}} create regular grids in the Geostatistical Library tool (Deutsch and Journel, 1998) and in SPRING, a Geographical Information System environment. This study explores statistics metrics, like standart deviation from estimated values from both methods and from original samples. The results show that indicator simulation better represents the original <b>samples</b> <b>variability.</b> Pages: 999 - 100...|$|R
50|$|Produces biased {{estimates}} of grade and tonnage above an ore waste cut-off. Which {{is called the}} volume variance relationship i.e. the variability of the grade distribution depends on the volume of samples. Large volume <b>samples</b> mean small <b>variability</b> whereas small volume <b>samples</b> mean large <b>variability.</b>|$|R
