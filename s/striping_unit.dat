12|42|Public
40|$|Abstract. Improvements in disk speeds {{have not}} {{kept up with}} {{improvements}} in processor and memory speeds. One way to correct the resulting speed mismatch is to stripe data across many disks. In this paper, we address how to stripe data to get maximum performance from the disks. Specifically, we examine how to choose the <b>striping</b> <b>unit,</b> i. e. the amount of logically contiguous data on each disk. We synthesize rules for determining the best <b>striping</b> <b>unit</b> for a given range of workloads. We show how the choice of <b>striping</b> <b>unit</b> depends on only two parameters: 1) the number of outstanding requests in the disk system at any given time, and 2) the average positioning time × data transfer rate of the disks. We derive an equation for the optimal <b>striping</b> <b>unit</b> {{as a function of}} these two parameters; we also show how to choose the <b>striping</b> <b>unit</b> without prior knowledge about the workload...|$|E
40|$|Improvements in disk speeds {{have not}} {{kept up with}} {{improvements}} in processor and memory speeds. One way to correct the resulting speed mismatch is to stripe data across many disks. In this paper, we address how to stripe data to get maximum performance from the disks. Specifically, we examine how to choose the <b>striping</b> <b>unit,</b> i. e. the amount of logically contiguous data on each disk. We synthesize rules for determining the best <b>striping</b> <b>unit</b> for a given range of workloads. We show how the choice of <b>striping</b> <b>unit</b> depends on only two parameters: 1) the number of outstanding requests in the disk system at any given time, and 2) the average positioning time × data transfer rate of the disks. We derive an equation for the optimal <b>striping</b> <b>unit</b> {{as a function of}} these two parameters; we also show how to choose the <b>striping</b> <b>unit</b> without prior knowledge about the workload. 1. Introduction In recent years, computer technology has advanced at an astonishing rate: processor speed, memory speed, [...] ...|$|E
40|$|Abstract: Redundant disk arrays are an {{increasingly}} popular {{way to improve}} I/O system performance. Past research has studied how to stripe data in non-redundant (RAID Level 0) disk arrays, but none has yet been done on how to stripe data in redundant disk arrays such as RAID Level 5, or on how the choice of <b>striping</b> <b>unit</b> varies {{with the number of}} disks. Using synthetic workloads, we derive simple design rules for striping data in RAID Level 5 disk arrays given varying amounts of workload information. We then validate the synthetically derived design rules using real workload traces to show that the design rules apply well to real systems. We find no difference in the optimal striping units for RAID Level 0 and 5 for read-intensive workloads. For write-intensive workloads, in contrast, the overhead of maintaining parity causes full-stripe writes (writes that span the entire error-correction group) to be more efficient than read-modify writes or reconstruct writes. This additional factor causes the optimal <b>striping</b> <b>unit</b> for RAID Level 5 to be four times smaller for write-intensive workloads than for read-intensive workloads. We next investigate how the optimal <b>striping</b> <b>unit</b> varies with the number of disks in an array. We find that the optimal <b>striping</b> <b>unit</b> for reads in a RAID Level 5 varies inversely to the number of disks, but that the optimal <b>striping</b> <b>unit</b> for writes varies with the number of disks. Overall, we find that the optimal <b>striping</b> <b>unit</b> for workloads with an unspecified mix of reads and writes is independent of the number of disks. Together, these trends lead us to recommend (in the absence of specific workload information) that the <b>striping</b> <b>unit</b> over a wide range of RAID Level 5 disk array sizes be equal to 1 / 2 * average positioning time * disk transfer rate. ...|$|E
50|$|If an XFS {{file system}} is to be created on a striped RAID array, a <b>stripe</b> <b>unit</b> can be {{specified}} when the file system is created. This maximizes throughput by ensuring that data allocations, inode allocations and the internal log (the journal) are aligned with the <b>stripe</b> <b>unit.</b>|$|R
40|$|In this paper, we {{describe}} techniques {{for determining the}} <b>stripe</b> <b>unit</b> size {{and the degree of}} striping for efficient placement of continuous media data on disk arrays. To determine the <b>stripe</b> <b>unit</b> size, we present analytical models that use the server configuration and the workload characteristics to predict the load on the most heavily loaded disk in redundant and non-redundant arrays. We use these models to evaluate the effect of various system parameters on the optimal <b>stripe</b> <b>unit</b> size. We also present procedures for determining the optimal <b>stripe</b> <b>unit</b> size for various design scenarios. We demonstrate that striping a continuous media object across all disks in the array causes the number of clients supported to increase sub-linearly with {{increase in the number of}} disks. To maximize the number of clients supported in large arrays, we present a technique that partitions a disk array and stripes each media stream across a single partition. Since load imbalance can occur in such partitio [...] ...|$|R
40|$|The {{performance}} of striped disk arrays {{is governed by}} two parameters: the <b>stripe</b> <b>unit</b> size {{and the degree of}} striping. In this paper, we describe techniques for determining the <b>stripe</b> <b>unit</b> size and degree of striping for disk arrays storing variable bit rate continuous media data. We present an analytical model that uses the server configuration and the workload characteristics to predict the load on the most heavily loaded disk in redundant and non-redundant arrays. We then use the model to determine the optimal <b>stripe</b> <b>unit</b> size for different workloads. We also use the model to study the effect of various system parameters on the optimal <b>stripe</b> <b>unit</b> size. To determine the degree of striping, we first demonstrate that striping a continuous media stream across all disks in the array causes the number of clients supported to increase sub-linearly with {{increase in the number of}} disks. To maximize the number of clients supported in large arrays, we propose a technique that partitions a dis [...] ...|$|R
40|$|Redundant disk arrays are an {{increasingly}} popular {{way to improve}} I/O system performance. Past research has studied how to stripe data in non-redundant (RAID Level 0) disk arrays, but none has yet been done on how to stripe data in redundant disk arrays such as RAID Level 5. In this paper we discuss the tradeoffs involved in striping data in RAID Level 5 disk arrays, particularly for write-intensive workloads. We find that the overhead of maintaining parity causes fullstripe writes (writes that span the entire error-correction group) to be more efficient than readmodify writes or reconstruct writes. This additional factor causes the optimal <b>striping</b> <b>unit</b> for RAID Level 5 to be smaller for write-intensive workloads than for read-intensive workloads. We synthesize simple design rules for how to choose an optimal <b>striping</b> <b>unit</b> for RAID Level 5 when workload concurrency is known and also when no workload information is known. Last, we investigate how the optimal <b>striping</b> <b>unit</b> varies with [...] ...|$|E
40|$|There is {{a growing}} demand in high {{reliability}} beyond what current RAID can provide and there are various levels of user demand for data reliability. An efficient data placement scheme called RM 2 has been proposed in [11], which makes a disk array system tolerable against double disk failures. In this paper, we consider how to choose an optimal <b>striping</b> <b>unit</b> for RM 2 particularly when no workload information is available except read/write ratio. A disk array simulator for RM 2 has been developed for experimental works. It is shown that RM 2 has an optimal <b>striping</b> <b>unit</b> of two and half tracks {{in the case of}} disk read operations, and one third of a single track if any disk write operations are involved. Key words : data placement, disk array, performance, reliability, striping 1 Introduction In the past few years, the performance of processors has been growing steadily, doubling approximately every two years [6], and with the advance of parallel processing technology, the processi [...] ...|$|E
40|$|Digitization {{of audio}} yields a {{sequence}} of samples and that of video yields at sequence of frames. We refer to a continuously recorded sequence of audio samples or video frames as a media stream. Due to the immense sizes and data transfer rates of media streams, most multimedia servers are founded on disk arrays. To utilize a disk array effectively, multimedia servers interleave the storage of each media stream among disks in the array. The unit of interleaving, called a media block or a <b>striping</b> <b>unit,</b> denotes {{the maximum amount of}} logically contiguous data stored on a single disk. Each medi...|$|E
50|$|South Eastern Trains (2005-2006): {{white with}} yellow and black side <b>stripes.</b> <b>Unit</b> 508208 {{was one of}} the first to receive this, in May 2005.|$|R
50|$|Graco {{currently}} has airless sprayers, line <b>striping</b> <b>units,</b> fine finish equipment, texture sprayers, roof coating rigs, and pressure washers on the Contractor Equipment Division (CED).|$|R
50|$|The {{segments}} of sequential data written to or read from a disk before the operation continues {{on the next}} disk are usually called chunks, strides or <b>stripe</b> <b>units,</b> while their logical groups forming single striped operations are called strips or stripes. The amount of data in one chunk (<b>stripe</b> <b>unit),</b> often denominated in bytes, is variously {{referred to as the}} chunk size, stride size, stripe size, stripe depth or stripe length. The number of data disks in the array is sometimes called the stripe width, but it may also refer to the amount of data within a stripe.|$|R
40|$|This paper {{studies the}} impacts of work-file disk {{allocation}} and data striping {{on the performance of}} concurrent mergesorts in a multiprocessor database system. We exam-ine through detailed simulations an approach where workfile disks are logically partitioned into equal-sized groups and an arriving sort job selects one group to do the mergesort. The results show that (1) without data strip-ing, the best performance is achieved by using the entire workfile disks as a single partition if there are abundant workfile disks (or sys-tem workload is light); (2) however, if there are limited workfile disks (or system workload is heavy), the workfile disks should be parti-tioned into multiple groups and the optimal partition size is workload dependent; (3) data striping is beneficial only if the <b>striping</b> <b>unit</b> size is properly chosen. ...|$|E
40|$|With the {{increasing}} processing speeds, {{there is a}} shift away from the paradigm of centralized, sequential storage systems towards distributed and network based storage systems. Further, with the new imaging and real time multimedia applications, it is becoming more than ever important to design powerful, efficient and scaleable U 0 systems. In this paper, the requirements of storage subsystems in multimedia environment were presented. The storage system components relating to those requirements were analyzed. Current solutions were surveyed and classified. Then we proposed approaches to improve storage subsystem performance for multimedia. The first approach applies constrained layout currently used for single disk model to multi-disk system. The second calls for using a <b>striping</b> <b>unit</b> that meets both media and storage system optimization criteria. The third uses a pool of buffers instead of single buffer per stream. 1...|$|E
40|$|In realtime {{multimedia}} storage servers, it is {{very important}} to find the optimal values for the round length and the <b>striping</b> <b>unit</b> size since they have strong relation with the throughput. In getting the optimal values, we should not overlook any factor of multimedia servers since this may produce the wrong values far from the optimal. If we use wrong value, then we will suffer from 2 or 3 times performance degradation. We present a new scheme which considers all possible factors in multimedia servers such as disk properties, buffer space, the characteristics of multimedia objects, and users' trends of requesting. Since we do not use simple peak data rate but the maximum segment data rate which is much smaller than the peak data rate, we can use the resources very effectively and still guarantees no starvation and no overflow. 1 Introduction In recent years, significant advances in computing and networking technologies have made it possible and economically feasible to provide onlin [...] ...|$|E
40|$|Many storage {{systems have}} become so complex that that the system administrator’s salary {{represents}} {{almost half of the}} total cost of ownership. One approach to reducing this cost is to develop storage systems that can configure and manage themselves. Unfortunately, our ability to develop such software has been hindered by a limited understanding of how workloads and storage systems interact. In [10], we presented the design of the Distiller — our tool that automates the process of finding a workload’s key performance-affecting attributes. In this paper, we distill three production workloads and show that the values of the chosen attributes contain information that will help a self-configuring disk array choose a reasonable prefetch length and RAID <b>stripe</b> <b>unit</b> size. We also discuss how the chosen attributes may help direct the development of algorithms that compute near-optimal prefetch lengths and <b>stripe</b> <b>unit</b> sizes. 1...|$|R
40|$|Abstract The {{performance}} of striped disk arrays {{is governed by}} two parameters: the <b>stripe</b> <b>unit</b> size and the degree ofstriping. In this paper, we describe techniques for determining the <b>stripe</b> <b>unit</b> size and degree of striping for disk arrays storing variable bit rate continuous media data. We present an analytical model to determine the optimalstripe unit size in redundant and non-redundant disk arrays. We then use the model to study the effect of various system parameters on the optimal <b>stripe</b> <b>unit</b> size. To determine the degree of striping, we first demonstratethat striping a continuous media stream across all disks in the array causes the number of clients supported to increase sub-linearly with {{increase in the number}} of disks. To overcome this limitation, we propose a techniquethat partitions a disk array and stripes each media stream across a single partition. We then propose an analytical model to determine the optimal partition size and maximize the number of clients supported by the array. Keywords: Continuous media file servers, striping techniques, disk arrays 1 Introduction 1. 1 Motivation Advances in computing and communication technologies over the past few years have triggered the development ofa wide range of information services (e. g., electronic newspapers, distance learning and self-paced education, video mail, etc.). All of these services involve storing, accessing, and processing multiple types of information (e. g., text,audio, video, imagery, etc.,- which we collectively refer to as multimedia). Realizing such services will require the development of file servers that can efficiently handle multiple data types. To do so, such file servers will be requiredto employ efficient placement techniques...|$|R
40|$|In this paper, {{we propose}} an {{architectural}} approach, Supplementary Partial Parity (SPP), to addressing the availability issue of parity encoded RAID systems. SPP exploits free storage space and idle time to generate and update {{a set of}} partial parity units that cover a subset of disks (or data <b>stripe</b> <b>units)</b> during failure-free and idle/lightly-loaded periods, thus supplementing the existing full parity units for improved availability. By applying the exclusive OR operations appropriately among partial parity, full parity and data units, SPP can reconstruct the data on the failed disks with {{a fraction of the}} original overhead that is proportional to the partial parity coverage, thus significantly reducing the overhead of data regeneration...|$|R
40|$|Redundant disk arrays are an {{increasingly}} popular {{way to improve}} I/O system performance. Past research has studied how to stripe data in non-redundant (RAID Level 0) disk arrays, but none has yet been done on how to stripe data in redundant disk arrays such as RAID Level 5, or on how the choice of <b>striping</b> <b>unit</b> varies {{with the number of}} disks. Using synthetic workloads, we derive simple design rules for striping data in RAID Level 5 disk arrays given varying amounts of workload information. We then validate the synthetically derived design rules using real workload traces to show that the design rules apply well to real systems. We find no difference in the optimal striping units for RAID Level 0 and 5 for read-intensive workloads. For write-intensive workloads, in contrast, the overhead of maintaining parity causes full-stripe writes (writes that span the entire error-correction group) to be more efficient than read-modify writes or reconstruct writes. This additional factor causes [...] ...|$|E
40|$|Abstract—When {{files are}} striped in a {{parallel}} I/O system, requests to the files are decomposed {{into a number of}} subrequests that are distributed over multiple servers. If a request is not aligned with the striping pattern such decomposition can make the first and last sub-requests much smaller than the <b>striping</b> <b>unit.</b> Because hard-disk-based servers can be much less efficient in serving small requests than large ones, the system exhibits heterogeneity in serving sub-requests of different sizes, and the net throughput of the entire system can be severely degraded by the inefficiency of serving the smaller requests, or fragments. Because a request is not considered complete until its slowest sub-request is, the penalty is yet greater for synchronous requests. To make the situation even worse, the larger the request, or the more data servers the requested data is striped over, the larger the detrimental performance effect of serving fragments can be. This effect can become the Achilles’ heel of a parallel I/O system performance seeking scalability with large sequential accesses. In this paper we propose iBridge, a scheme that uses solidstate drives to serve request fragments and thereby bridge the performance gap between serving fragments and serving large sub-requests. We have implemented iBridge in the PVFS file system. Our experimental results with representative MPI-IO benchmarks show that iBridge can significantly improve the I/O throughput of storage systems, especially for large requests with fragments. Keywords-Solid state drive, parallel file systems, parallel I/O. I...|$|E
40|$|Analysis {{of large}} physics data sets {{is a major}} {{computing}} task at Fermilab. One step in such an analysis involves culling ``interesting`` events via the use of complex query criteria. What makes this unusual is the scale required: 100 `s of gigabytes of event data must be scanned at 10 `s of megabytes per second for the typical queries that are applied, and data must be extracted from 10 `s of terabytes based on {{the result of the}} query. The Physics Object Persistency Manager (POPM) system is a solution tailored to this scale of problem. A running POPM environment can support multiple queries in progress, each scanning at rates exceeding 10 megabytes per second, all of which are sharing access to a very large persistent address space distributed across multiple disks on multiple hosts. Specifically, POPM employs the following techniques to permit this scale of performance and access: Persistent objects: Experimental data to be scanned is ``populated`` as a data structure into the persistent address space supported by POPM. C++ classes with a few key overloaded operators provide nearly transparent semantics for access to the persistent storage. Distributed and parallel I/O: The persistent address space is automatically distributed across disks of multiple ``I/O nodes`` within the POPM system. A <b>striping</b> <b>unit</b> concept is implemented in POPM, permitting fast parallel I/O across the storage nodes, even for small single queries. Efficient Shared access: POPM implements an efficient mechanism for arbitration and multiplexing of I/O access among multiple queries on the same or separate compute nodes...|$|E
40|$|Large {{simulations}} which run {{for hundreds}} of hours on parallel computers often periodically generate snapshots of states, which are later post-processed to visualize the simulated physical phenomenon. For many applications, fast I/O during post-processing, which is dependent on an efficient organization of data on disk, {{is as important as}} minimizing computation-time I/O. In this paper we propose optimizations to support efficient parallel I/O for scientific simulations and subsequent visualizations. We present an ordering mechanism to linearize data on disk, a performance model to help to choose a proper <b>stripe</b> <b>unit</b> size, and a scheduling algorithm to minimize communication contention. Our experiments on an IBM SP show that the combination of these strategies provides a 20 - 25 % performance boost. 1 Introduction Progress in developing high-performance processors and fast-speed networks has made large-scale scientific simulations possible, such as the simulations of nuclear devices that [...] ...|$|R
40|$|Distributed {{redundant}} {{array of}} inexpensive disks (RAID) is often {{embedded in a}} cluster architecture. In a centralized RAID subsystem, the false sharing problem does not exist, because the disk array allows only mutually exclusive access by one user at a time. However, the problem does exist in a distributed RAID architecture, because multiple accesses may occur simultaneously in a distributed environment. This problem will seriously limit the effectiveness of collective I/O operations in network-based, cluster computing. Traditional accesses to disks in a RAID are done at block level. The block granularity is large, say 32 KB, often resulting in false sharing among fragments in the block. The false sharing problem becomes worse when the block size or the <b>stripe</b> <b>unit</b> becomes too large. To solve this problem, we propose an adaptive sector grouping approach to accessing a distributed RAID. Each sector has a fine grain of 512 B. Multiple sectors are grouped together to match with the d [...] ...|$|R
25|$|During {{the period}} up to 1915 the Romanian Gendarmerie wore a {{distinctive}} dress comprising a shako with white plume, dark blue tunic with red facings, white trefoil epaulettes and aiguillettes plus light blue trousers with red <b>stripes.</b> Mounted <b>units</b> of the Gendarmerie wore a silver helmet with spike and white plume, a similar tunic {{to the foot}} branch but with yellow epaulettes and aiguillettes, white breeches and high boots.|$|R
50|$|The first 100 {{were painted}} in Provincial sector's livery {{of blue and}} beige with light blue <b>stripe.</b> Twenty <b>units,</b> (156401-419/422) based at Tyseley depot, were later repainted into Regional Railways Express livery after the {{rebranding}} of Provincial. The last fourteen units were operated by Strathclyde PTE, and carried an orange and black livery. Following the delivery of the Class 158s in the early 1990s, the 156s began to be cascaded to less important services.|$|R
40|$|Data-intensive {{distributed}} file {{systems are}} {{emerging as a}} key component of large scale Internet services and cloud computing platforms. They are designed from the ground up and are tuned for specific application workloads. Leading examples, such as the Google File System, Hadoop distributed file system (HDFS) and Amazon S 3, are defining this new purpose-built paradigm. It is tempting to classify file systems for large clusters into two disjoint categories, those for Internet services and those for high performance computing. In this paper we compare and contrast parallel file systems, developed for high performance computing, and data-intensive distributed file systems, developed for Internet services. Using PVFS as a representative for parallel file systems and HDFS as a representative for Internet services file systems, we configure a parallel file system into a data-intensive Internet services stack, Hadoop, and test performance with microbenchmarks and macrobenchmarks running on a 4, 000 core Internet services cluster, Yahoo! 2 ̆ 7 s M 45. Once a number of configuration issues such as <b>stripe</b> <b>unit</b> sizes and application buffering sizes are dealt with, issues of replication, data layout and data-guided function shipping are found to be different, but supportable in parallel file systems. Performance of Hadoop applications storing data in an appropriately configured PVFS are comparable to those using a purpose built HDFS...|$|R
40|$|Abstract [...] This paper {{presents}} a new distributed disk-array architecture for achieving high I/O performance in scalable cluster computing. In a serverless cluster of computers, all distributed local disks {{can be integrated}} as a distributed-software redundant array of independent disks (ds-RAID) with a single I/O space. We report the new RAID-x design and its benchmark performance results. The advantage of RAID-x comes mainly from its orthogonal striping and mirroring (OSM) architecture. The bandwidth is enhanced with distributed striping across local and remote disks, while the reliability comes from orthogonal mirroring on local disks at the background. Our RAID-x design is experimentally compared with the RAID- 5, RAID- 10, and chained-declustering RAID through benchmarking on a research Linux cluster at USC. Andrew and Bonnie benchmark results are reported on all four disk-array architectures. Cooperative disk drivers and Linux extensions are developed to enable not only the single I/O space, but also the shared virtual memory and global file hierarchy. We reveal the effects of traffic rate and <b>stripe</b> <b>unit</b> size on I/O performance. Through scalability and overhead analysis, we find the strength of RAID-x in three areas: (1) improved aggregate I/O bandwidth especially for parallel writes, (2) orthogonal mirroring with low software overhead, and (3) enhanced scalability in cluster I/O processing. Architectural strengths and weakness of all four ds-RAID architectures are evaluated comparatively. The optimal choice among them depends on parallel read/write performance desired, the level of fault-tolerance required, and the cost-effectiveness in specific I/O processing applications...|$|R
40|$|In this paper, {{we propose}} an {{architectural}} approach, Supplementary Partial Parity (SPP), to addressing the availability issue of parity encoded RAID systems. SPP exploits free storage space and idle time to generate and update {{a set of}} partial parity units that cover a subset of disks (or data <b>stripe</b> <b>units)</b> during failure-free and idle/lightly-loaded periods, thus supplementing the existing full parity units for improved availability. By applying the exclusive OR operations appropriately among partial parity, full parity and data units, SPP can reconstruct the data on the failed disks with {{a fraction of the}} original overhead that is proportional to the partial parity coverage, thus significantly reducing the overhead of data regeneration, especially under heavy workload. By providing redundant parity coverage, SPP can potentially tolerate more than one disk failure with much better flexibility, thus significantly improving the system’s reliability and availability. Due to its supplementary nature, SPP provides a more efficient and flexible redundancy protection mechanism than the conventional full parity approach. SPP offers multiple optional levels depending on partial parity coverage and performance/cost targets. According to the actual workload and the available re-source, the SPP approach can be adaptively and dynamically activated, deactivated and adjusted while the original RAID system continues to serve user requests on-line. We conduct extensive trace-driven experiments to evaluate the performance of the SPP approach. The experiments results demonstrate that SPP significantly improves the reconstruction time and user response time simultaneously...|$|R
40|$|This paper {{presents}} a new distributed disk-array architecture for achieving high I/O performance in scalable cluster computing. In a serverless cluster of computers, all distributed local disks {{can be integrated}} as a distributed-software redundant array of independent disks (ds-RAID) with a single I/O space. We report the new RAID-x design and its benchmark performance results. The advantage of RAID-x comes mainly from its orthogonal striping and mirroring (OSM) architecture. The bandwidth is enhanced with distributed striping across local and remote disks, while the reliability comes from orthogonal mirroring on local disks at the background. Our RAID-x design is experimentally compared with the RAID- 5, RAID- 10, and chained-declustering RAID through benchmarking on a research Linux cluster at USC. Andrew and Bonnie benchmark results are reported on all four disk-array architectures. Cooperative disk drivers and Linux extensions are developed to enable not only the single I/O space, but also the shared virtual memory and global file hierarchy. We reveal the effects of traffic rate and <b>stripe</b> <b>unit</b> size on I/O performance. Through scalability and overhead analysis, we find the strength of RAID-x in three areas: 1) improved aggregate I/O bandwidth especially for parallel writes, 2) orthogonal mirroring with low software overhead, and 3) enhanced scalability in cluster I/O processing. Architectural strengths and weakness of all four ds-RAID architectures are evaluated comparatively. The optimal choice among them depends on parallel read/write performance desired, the level of fault tolerance required, and the cost-effectiveness in specific I/O processing applications. published_or_final_versio...|$|R
40|$|Synergistic {{interactions}} between the maternal regulatory factor dorsal (dl) and basic helix-loop-helix (bHLH) activators are essential for initiating differentiation of the mesoderm and neuroectoderm in the early Drosophila embryo. Here we present evidence that dl-bHLH interactions mediating gene expression in the neuroectoderm and mesoderm are fundamentally distinct. Close proximity of dl and bHLH binding sites {{is essential for the}} synergistic activation of gene expression in the lateral neuroectoderm, where there are diminishing levels of the dl regulatory gradient. In contrast, sharp on/off patterns of gene expression in the presumptive mesoderm do not require linkage of these sites. Analysis of minimal and synthetic promoter elements suggests that dl and bHLH activators, such as twist, might interact with different rate-limiting components of the transcription complex. These results are consistent with two distinct modes of dl-bHLH synergy: cooperative binding to DNA (requiring linkage of sites) and synergistic contact of basal transcription factors (not requiring linkage). Finally, the characterization of a 57 bp synthetic minimal <b>stripe</b> <b>unit</b> (MSU) provides evidence for a third tier of dl-bHLH synergy. Tandem copies of the MSU function as a bona fide enhancer and can mediate neuroectoderm expression in transgenic embryos even when placed 4. 5 kb downstream of a test promoter. Multiple copies of the MSU function synergistically only when linked, but not when separated. We propose that this linkage requirement provides the basis for the evolution of modular promoters composed of discrete, non-overlapping enhancers...|$|R
40|$|AbstractÐThis paper {{presents}} a new distributed disk-array architecture for achieving high I/O performance in scalable cluster computing. In a serverless cluster of computers,all distributed local disks {{can be integrated}} as a distributed-software redundant array of independent disks �ds-RAID) with a single I/O space. We report the new RAID-x design and its benchmark performance results. The advantage of RAID-x comes mainly from its orthogonal striping and mirroring �OSM) architecture. The bandwidth is enhanced with distributed striping across local and remote disks,while the reliability comes from orthogonal mirroring on local disks at the background. Our RAID-x design is experimentally compared with the RAID- 5,RAID- 10,and chained-declustering RAID through benchmarking on a research Linux cluster at USC. Andrew and Bonnie benchmark results are reported on all four disk-array architectures. Cooperative disk drivers and Linux extensions are developed to enable not only the single I/O space,but also the shared virtual memory and global file hierarchy. We reveal the effects of traffic rate and <b>stripe</b> <b>unit</b> size on I/O performance. Through scalability and overhead analysis,we find the strength of RAID-x in three areas: 1) improved aggregate I/O bandwidth especially for parallel writes, 2) orthogonal mirroring with low software overhead,and 3) enhanced scalability in cluster I/O processing. Architectural strengths and weakness of all four ds-RAID architectures are evaluated comparatively. The optimal choice among them depends on parallel read/write performance desired,the level of fault tolerance required,and the cost-effectiveness in specific I/O processing applications. Index TermsÐDistributed computing,parallel I/O,software RAID,single I/O space,Linux clusters,fault tolerance,Andrew and Bonnie benchmarks,network file servers,scalability and overhead analysis. ...|$|R
40|$|This study {{explores the}} {{cortical}} cell dynamics of unimodal and cross-modal working memory (WM). Neuronal activity was re-corded from parietal areas of monkeys performing delayed match-to-sample tasks with tactile or visual samples. Tactile memoranda (haptic samples) consisted of rods with differing surface features (texture or orientation of ridges) perceived by active touch. Visual memoranda (icons) consisted of striped patterns of differing orientation. In a haptic [...] haptic task, the animal had to retain {{through a period}} of delay the surface feature of the sample rod to select a rod that matched it. In a visual [...] haptic task, the animal had to retain the icon for the haptic choice of a rod with ridges of the same orientation as the icon’s <b>stripes.</b> <b>Units</b> in all areas responded with firing change to one or more task events. Also in all areas, cells responded differently to different sample memoranda. Differential sample coherent firing was present in most areas during the memory period (delay). It is concluded that neurons in somatosensory and association areas of parietal cortex participate in broad networks that represent various task events and stimuli (auditory, motor, proprioceptive, tactile, and visual). Neurons in the same networks take part in retaining in WM the memorandum for each trial, whether it is encoded haptically or visually. The VH association by parietal cells in WM is analogous to the auditory [...] visual association previously observed in prefrontal cortex. Both illustrate the capacity of cortical neurons to associate sensory information across time and across modalities in accord with the rules of a behavioral task...|$|R
40|$|Storage {{management}} cost is {{a significant}} {{fraction of the total}} cost of ownership of large, enterprise storage systems. Consequently, software automation of common storage management tasks so as to reduce the total cost of ownership is an active area of research. In this paper, we consider a policy-managed storage system [...] -a system that automates various management tasks [...] -and focus on the problem of the storage allocation techniques that should be employed in such a system. We study two fundamentally different storage allocation techniques for policy-managed systems: narrow and wide striping. Whereas wide striping techniques need very little workload information for making placement decisions, narrow striping techniques employ detailed information about the workload to optimize the placement and can achieve better performance. We systematically evaluate this trade-off between "information" and performance using a combination of simulations and experiments on a storage system testbed. Our results show that an idealized narrow striped system can outperform a comparable wide-striped system at low levels of utilization and for small requests. However, wide striping outperforms narrow striped systems when the system utilization is high or in the presence of workload skews that occur in real I/O workloads. Our experiments show that the additional workload information needed by narrow placement techniques may not necessarily translate to better performance. We identify the <b>stripe</b> <b>unit</b> size to be a critical parameter in the performance of wide striped systems, and based on our experimental results, recommend that (i) policy-managed systems use wide striping for object placement, and (ii) sufficient information be specified at storage allocation time to enable app [...] ...|$|R
40|$|Institute for Computing Systems ArchitectureRAID systems (Redundant Arrays of Inexpensive Disks) have {{dominated}} backend storage systems {{for more than}} two decades and have grown continuously in size and complexity. Currently they face unprecedented challenges from data intensive applications such as image processing, transaction processing and data warehousing. As the size of RAID systems increases, designers are faced with both performance and reliability challenges. These challenges include limited back-end network bandwidth, physical interconnect failures, correlated disk failures and long disk reconstruction time. This thesis studies the scalability of RAID systems in terms of both performance and reliability through simulation, using a discrete event driven simulator for RAID systems (SIMRAID) developed as part of this project. SIMRAID incorporates two benchmark workload generators, based on the SPC- 1 and Iometer benchmark specifications. Each component of SIMRAID is highly parameterised, enabling it to explore a large design space. To improve the simulation speed, SIMRAID develops a set of abstraction techniques to extract the behaviour of the interconnection protocol without losing accuracy. Finally, to meet the technology trend toward heterogeneous storage architectures, SIMRAID develops a framework that allows easy modelling of different types of device and interconnection technique. Simulation experiments were first carried out on performance aspects of scalability. They were designed to answer two questions: (1) given a number of disks, which factors affect back-end network bandwidth requirements; (2) given an interconnection network, how many disks can be connected to the system. The results show that the bandwidth requirement per disk is primarily determined by workload features and <b>stripe</b> <b>unit</b> size (a smaller <b>stripe</b> <b>unit</b> size has better scalability than a larger one), with cache size and RAID algorithm having very little effect on this value. The maximum number of disks is limited, as would be expected, by the back-end network bandwidth. Studies of reliability have led to three proposals to improve the reliability and scalability of RAID systems. Firstly, a novel data layout called PCDSDF is proposed. PCDSDF combines the advantages of orthogonal data layouts and parity declustering data layouts, so that it can not only survivemultiple disk failures caused by physical interconnect failures or correlated disk failures, but also has a good degraded and rebuild performance. The generating process of PCDSDF is deterministic and time-efficient. The number of stripes per rotation (namely the number of stripes to achieve rebuild workload balance) is small. Analysis shows that the PCDSDF data layout can significantly improve the system reliability. Simulations performed on SIMRAID confirm the good performance of PCDSDF, which is comparable to other parity declustering data layouts, such as RELPR. Secondly, a system architecture and rebuilding mechanism have been designed, aimed at fast disk reconstruction. This architecture is based on parity declustering data layouts and a disk-oriented reconstruction algorithm. It uses stripe groups instead of stripes as the basic distribution unit so that it can make use of the sequential nature of the rebuilding workload. The design space of system factors such as parity declustering ratio, chunk size, private buffer size of surviving disks and free buffer size are explored to provide guidelines for storage system design. Thirdly, an efficient distributed hot spare allocation and assignment algorithm for general parity declustering data layouts has been developed. This algorithm avoids conflict problems in the process of assigning distributed spare space for the units on the failed disk. Simulation results show that it effectively solves the write bottleneck problem and, at the same time, there is only a small increase in the average response time to user requests...|$|R
40|$|Excessive power {{consumption}} {{is becoming a}} major barrier to extracting the maximum performance from high-performance parallel systems. Therefore, techniques oriented towards reducing {{power consumption}} of such systems are expected to become increasingly important in the future. Since disk systems of high-performance architectures are known to constitute a large fraction of the overall power budget, they form an important optimization target. Previous work on disk power management focuses primarily on hardware based schemes. However, since disk access pattern, i. e., {{the order in which}} disks on a system are accessed, is mainly shaped by the program code access pattern and disk layout of data, software techniques can also {{play a critical role in}} disk power management. Motivated by this observation, this paper proposes and evaluates a profile-driven disk layout optimization scheme for reducing energy consumption. The proposed scheme analyzes the array access traces obtained through profiling and determines, for each disk-resident data structure, the start disk from which the data is striped, the number of disks over which the data is striped, and the <b>stripe</b> <b>unit.</b> This paper discusses implementation details of our approach and presents an experimental evaluation of it. Our experiments with the entire suite of Spec 95 floating-point benchmarks that are modified to operate on disk-resident data show that the proposed approach is very effective in reducing disk energy consumption. The results also show that the performance degradation caused by our approach is very small. This paper also compares our approach to a code restructuring based optimization mechanism and discusses how the two techniques can be combined for achieving the best results...|$|R
40|$|Here {{are some}} machine exhibitors at the CITME 06 Survey 3 : Knitting. BYT Group's regular double jersey machine BMC/PK(2 - 4) has 4 cam {{tracks in the}} {{cylinder}} and 2 cam tracks in the dial. Falmac Limited's PN 72 DU flexible double jersey knitting machine has a 34 -inch cylinder in E 24 with 72 feeders. Fujian Hongqi Co. Ltd. 's HQM/DM single jersey machine features 34 -inch, E 28 machine with 102 feeders. Mayer & Cie's Relanit 1. 6 R and D 4 - 2. 2 has a unique relative movement technology. Pai Lung Machinery Mill Co. Ltd. 's PL-KS 3 B-W with 4 cam track has a 30 -inch cylinder, 90 feeders in E 28 {{and is able to}} run up to 28 rpm. Pilotelli Textile Machinery Co. Ltd. 's JVCE- 3 was a 34 -inch, E 28 with 102 feeders for knitting plain fabric with elastomeric yarn. Ssangyong Machine Industry Co. Ltd. 's JD 4 C is a full jacquard double jersey machine with electronic needle selection. Terrot GmbH's UCC 548 is a double jersey circular machine with electronic needle selection and runs at 21 rpm. United texmac Pte. Ltd. 's single jersey machine is a USX- 1. 6 SK which is made for color stripe fabrics with 4 -finger <b>striping</b> <b>units.</b> Wellknit Machinery Co. Ltd. 's XP 986 has a cylinder diameter of 17 inches in E 11 with 12 feeders. Other notable companies include Wuxi Golden Dragon Manmade Furry Machinery Co. Ltd., Zhejiang Rifa Textile Machinery Co. Ltd., Beijing Yalong Machinery Co. Ltd. and in the straight bed knitting side include Fujian Hongqi Co. Ltd, Zhejiang Haisen Knitting Machine Technical Co. Ltd. and Changshu Ruide Computer Knitting Machinery Co. Ltd. Institute of Textiles and Clothin...|$|R
