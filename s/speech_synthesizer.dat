509|127|Public
5|$|A MOS Technology 6502 {{chip that}} {{operates}} at 894kHz generates the sound effects, and a <b>speech</b> <b>synthesizer</b> by Votrax generates Q*bert's incoherent expressions. The audio system uses 128B of random-access memory and 4KB of erasable programmable {{read only memory}} to store the sound data and code to implement it. Like other Gottlieb games, the sound system was thoroughly tested to ensure it would handle daily usage. In retrospect, audio engineer David Thiel commented that such testing minimized time available for creative designing.|$|E
5|$|In-game screen {{shows the}} cockpit view split between top half for plane's front view and bottom half for plane's instrumentation. The control panel shows planes crucial data, {{including}} speed, altitude, thrust, fuel, roll/pitch indicators and a compass. The panel {{also features a}} radar that provided an overview of important nearby objects. An onboard screen displays various flight information and warning messages, and features a <b>speech</b> <b>synthesizer</b> that occasionally announces dangers, such as approaching enemies or missile attacks. Finally, a small rear-view camera allows the player to observe enemies and missiles on their tail. The game also features a two player mode, where one player flies the plane and the other aims and fires the guns.|$|E
25|$|Build 4093 (build date of August 19, 2004) – It {{was one of}} {{the last}} builds {{compiled}} before the development reset. Considered highly unstable, it contained Sidebar, WinFS, and an Avalon-based Windows Movie Maker, a preliminary version of Windows Anytime Upgrade, and the Microsoft Anna <b>speech</b> <b>synthesizer.</b> There was an Avalon-based Display Properties control panel applet hidden in the \WINDOWS\SYSTEM32 folder, similar to the one in Build 3683.|$|E
50|$|It {{is still}} {{used in some}} text-to-speech voices, such as the diphone {{databases}} found in the Festival and Flite <b>speech</b> <b>synthesizers.</b>|$|R
50|$|Mac OS had Chinese <b>speech</b> <b>synthesizers</b> {{available}} up to version 9. This {{was removed}} in 10.0 and reinstated in 10.7 (Lion).|$|R
40|$|<b>Speech</b> <b>Synthesizers</b> have {{traditionally}} been built on carefully read speech that is recorded in studio environment. Such voices are suboptimal for use in noisy conditions, which is inevitable in a majority of deployed speech systems. In this work, we attempt to modify {{the output of the}} <b>speech</b> <b>synthesizers</b> to make it more appropriate for noisy environments. Comparison of spectral and prosodic features of speech in noise and results of some conversion techniques are presented. Index Terms: speech synthesis, speech in noise, companding, 1...|$|R
25|$|The {{activation}} pattern {{within the}} motor map determines the movement pattern of all model articulators (lips, tongue, velum, glottis) for a speech item. In {{order not to}} overload the model, no detailed modeling of the neuromuscular system is done. The Maeda articulatory <b>speech</b> <b>synthesizer</b> is used in order to generate articulator movements, which allows the generation of a time-varying vocal tract form and the generation of the acoustic speech signal for each particular speech item.|$|E
500|$|In Fabian Niciza's {{stories for}} Red Robin, Lonnie Machin's {{abilities}} as Moneyspider were revamped, {{with the character}} taking on the persona of an [...] "electronic ghost." [...] Comatose, Moneyspider was free to act through his mind via connections to the internet, and interacted with others via text messaging and a <b>speech</b> <b>synthesizer.</b> In this condition, he acts to [...] "create an international web that will [...] {{the ins and outs}} of criminal and corporate operations." [...] Within virtual reality, the character's augmented intelligence was described as a [...] "fused bicameral mind", able to maintain a presence online at all times, while another part of his mind separately interacted with others offline.|$|E
500|$|On August 15, 2008, DC Comics {{announced}} that Anarky would reappear in the December issue of Robin, issue No.181. With {{the publication of}} Robin No.181, [...] "Search For a Hero, Part 5: Pushing Buttons, Pulling Strings", on December 17, 2008, {{it was revealed that}} Lonnie Machin's role as Anarky had been supplanted by another Batman villain, Ulysses Armstrong. Fabian Nicieza, author of the issue and storyline in which Anarky appeared, depicted the character as being held hostage by Armstrong, [...] "paralyzed and catatonic", encased in an iron lung, and connected to computers through his brain. This final feature allowed the character to connect to the internet and communicate with others via a <b>speech</b> <b>synthesizer.</b> Nicieza's decision to give Machin's mantle as Anarky to another character was due to his desire to establish him as a nemesis for Tim Drake, while respecting the original characterization of Anarky, who Nicieza recognized as neither immature, nor a villain. Regardless, Nicieza did desire to use Machin and properly return the character to publication, and so favored presenting Ulysses H. Armstrong as Anarky, and Lonnie Machin as Moneyspider, a reference to a secondary name briefly used by Grant for Anarky in storyline published in 1990.|$|E
40|$|<b>Speech</b> <b>synthesizers</b> {{provide the}} {{computer}} {{with the possibility}} to speak. They are developed to produce natural-sounding output. However, the ability of <b>speech</b> <b>synthesizers</b> to mimic human speech is limited in many ways. For example, <b>speech</b> <b>synthesizers</b> don’t understand what they say, and as a result, sometimes they do not use the right style or do not emphasize useful information. This paper presents an insight on our attempt to provide a synthesizer with more understanding of its input in a shallow approach. The goals of our project can be summarized as follows: • to achieve a more natural like output of children stories by – identifying the speech quotes; – identifying the speaker; – assigning a certain mood to each clause; – implementing voice acting. The collection of training data consisted of a corpus of children’s stories by Hans Christian Andersen (available a...|$|R
50|$|Kurzweil {{predicted}} in 2005 {{that as the}} cost-performance ratio caused <b>speech</b> <b>synthesizers</b> to become cheaper and more accessible, more people would benefit {{from the use of}} text-to-speech programs.|$|R
5000|$|... "No Matter What You Do" [...] {{is a song}} by Italian DJ Benny Benassi {{released}} in his 2002 album Hypnotica. Like [...] "Satisfaction", it uses a female <b>speech</b> <b>synthesizers</b> in the vocals.|$|R
2500|$|At {{one point}} in the film, a small entity called [...] "Bit" [...] advises Flynn with only the words [...] "yes" [...] and [...] "no" [...] created by a Votrax <b>speech</b> <b>synthesizer.</b>|$|E
2500|$|Some of the {{earliest}} work {{in the study of}} how humans perceive speech sounds was conducted by Alvin Liberman and his colleagues at Haskins Laboratories. Using a <b>speech</b> <b>synthesizer,</b> they constructed speech sounds that varied in place of articulation along a continuum from [...] to [...] to [...] Listeners were asked to identify which sound they heard and to discriminate between two different sounds. The results of the experiment showed that listeners grouped sounds into discrete categories, even though the sounds they were hearing were varying continuously. Based on these results, they proposed the notion of categorical perception as a mechanism by which humans can identify speech sounds.|$|E
2500|$|On August 15, 2008, DC Comics {{announced}} that Anarky would reappear in the December issue of Robin, issue No.181. Several weeks later, Dan Didio {{announced that}} Anarky would be among several villains to be showcased in DC Comic's [...] "Faces of Evil" [...] event. With {{the publication of}} Robin No.181, {{it was revealed that}} Lonnie Machin's role as Anarky had been supplanted by another Batman villain, Ulysses Hadrian Armstrong. Further, Machin was depicted as being held hostage by Armstrong, [...] "paralyzed and catatonic", encased in an iron lung, and connected to computers through his brain. This final feature allowed the character to connect to the internet and communicate with others via a <b>speech</b> <b>synthesizer.</b>|$|E
5000|$|The Pronunciation Lexicon Specification (PLS) is used {{to define}} how words are pronounced. The {{generated}} pronunciation information {{is meant to be}} used by both speech recognizers and <b>speech</b> <b>synthesizers</b> in voice browsing applications.|$|R
2500|$|If a {{specific}} {{aspect of the}} acoustic waveform indicated one linguistic unit, {{a series of tests}} using <b>speech</b> <b>synthesizers</b> would be sufficient to determine such a cue or cues. However, there are two significant obstacles: ...|$|R
40|$|Previous {{research}} (e. g. Cawley [1, 2]) {{has demonstrated}} that arti cial neural networks can be trained to generate the speech sounds corresponding to a sequence of phonetic tokens, including the e ects of coarticulation required to produce natural sounding synthetic speech. The principal limiting factor {{in the performance of}} neural <b>speech</b> <b>synthesizers</b> has been found to lie in the amount of training data available. This paper presents the initial results of an investigation to determine the amount of training data required to reach optimal generalization in neural <b>speech</b> <b>synthesizers,</b> through an empirical exploration of the e ects of the number of training patterns on test set error. 1...|$|R
2500|$|Yamaha {{developed}} Vocaloid-flex, {{a singing}} software application {{based on the}} Vocaloid engine, which contains a <b>speech</b> <b>synthesizer.</b> According to the official announcement, users can edit its phonological system more delicately than those of other Vocaloid series {{to get closer to}} the actual speech language; for example, it enables final devoicing, unvoicing vowel sounds or weakening/strengthening consonant sounds. It was used in a video game [...] released on April 28, 2010. It is still a corporate product and a consumer version has not been announced. This software was also used for the robot model HRP-4C at CEATEC Japan 2009. Gachapoid has access to this engine and it is used through the software V-Talk.|$|E
2500|$|Medicine {{can benefit}} from robotic {{advances}} {{in the design of}} prostheses for the handicapped. Wiener mentions the Vocorder, a device from Bell Telephone Company that creates visual speech. He discusses the possibility of creating an automated prosthesis that inputs speech directly into the brain for processing, effectively giving deaf individuals the ability to [...] "hear" [...] speech again. Progress in these areas is ongoing and rapid, exemplified by such devices as the , a new device created to replace a damaged larynx; it uses a <b>speech</b> <b>synthesizer</b> to recreate words based on its ability to monitor tongue movements. [...] This device effectively rids people with damaged larynxes of the robotic tones associated with artificial speech synthesizers (like the one famously used by disabled physicist Stephen Hawking), enabling people to have more natural social interactions.|$|E
2500|$|The {{inspiration}} for the character's creation extends from Wolpaw's use of a text-to-speech program while writing lines for the video game Psychonauts. Other game developers working on Psychonauts found the lines funnier {{as a result of}} the synthesized voice. GLaDOS was originally intended to be present solely in the first area of Portal; she was well received by other designers and her role was expanded as a result. Play testers were motivated to complete tests in the game due to her guidance. While the game was initially designed with other characters, they were later removed, leaving GLaDOS as the only character players encounter. The physical appearance of GLaDOS went through several designs, one of which featured a large disk below her. McLain imitated dialog read aloud by a <b>speech</b> <b>synthesizer</b> with her own voice, which was then processed to sound more robotic, and performed songs in character during the closing credits of both entries in the series. [...] "Still Alive" [...] became hugely successful, notably appearing in the Rock Band game series, and has been a popular song for YouTube users to cover.|$|E
50|$|The {{original}} {{version of the}} reader was composed of a digital camera and a PDA, which contained specialised OCR software and <b>speech</b> <b>synthesizers</b> to read the scanned material aloud. It was released at a price of $3,495.|$|R
50|$|Ghoti {{is used to}} test <b>speech</b> <b>synthesizers.</b> The <b>Speech!</b> allophone-based {{speech synthesiser}} {{software}} for the BBC Micro was tweaked to pronounce ghoti as fish. Examination of the code reveals the string GHOTI used to identify the special case.|$|R
5000|$|LPC (linear {{predictive}} coding) was {{the speech}} synthesis technology used, which allowed applications to encode speech {{data in a}} compact form. The Echo II used the TMS 5220 LPC Speech Chip which was popular in other <b>speech</b> <b>synthesizers</b> ...|$|R
5000|$|... #Subtitle level 2: Quotations (with {{optional}} <b>speech</b> <b>synthesizer)</b> ...|$|E
5000|$|Heathkit HERO 1 (ET-18) Robot Votrax SC-01 <b>speech</b> <b>synthesizer.</b>|$|E
5000|$|Forrest Mozer invented and {{patented}} {{the first}} integrated circuit <b>speech</b> <b>synthesizer</b> in 1974. He first licensed this technology to TeleSensory Systems, which {{used it in}} the [...] "Speech+" [...] talking calculator for blind persons. Later National Semiconductor also licensed the technology, used for its [...] "DigiTalker" [...] <b>speech</b> <b>synthesizer,</b> the MM54104.|$|E
50|$|Forrest S. Mozer (February 13, 1929, Lincoln, Nebraska) is an American {{experimental}} physicist, inventor, and entrepreneur known {{best for}} his pioneering work on electric field measurements in space plasma and {{for development of}} solid state electronic <b>speech</b> <b>synthesizers</b> and <b>speech</b> recognizers.|$|R
50|$|The Java Speech API (JSAPI) is an {{application}} programming interface for cross-platform support of command and control recognizers, dictation systems, and <b>speech</b> <b>synthesizers.</b> Although JSAPI defines an interface only there are several implementations created by third parties, for example FreeTTS.|$|R
50|$|Speech output {{systems can}} be used to read screen text to {{computer}} users. Special software programs called screen readers attempt to identify and interpret what is being displayed on the screen and <b>speech</b> <b>synthesizers</b> convert data to vocalized sounds or text. Also it is used to produce music, speech or other sounds.|$|R
50|$|Christoph H. Müller - programming, bass, keyboards, <b>speech</b> <b>synthesizer</b> & fx.|$|E
5000|$|... #Caption: Texas Instruments Speak & Spell using a TMC0280 <b>speech</b> <b>synthesizer</b> ...|$|E
5000|$|SVA (first {{self-contained}} <b>speech</b> <b>synthesizer,</b> with a 6800 core {{running the}} NRL frontend) ...|$|E
40|$|The aim of {{the present}} thesis was to examine how people respond to {{synthetically}} produced lexical expressions of emotions. When speaking, both the content of spoken words and the prosodic cues, such as pitch and {{the speed of the}} speech, can mediate emotion-related information. To study how the pure content of spoken words affects human emotions, <b>speech</b> <b>synthesizers</b> offer good opportunities as they allow for good controllability over the prosodic cues. Synthetic speech can be generated using different techniques. Such speech can be purely machine generated or it can be based on different types (i. e. shorter or longer) of samples from human speech. On the basis of synthesis techniques, synthesizers can be classified according to the degree of human-likeness of the voice. Four different <b>speech</b> <b>synthesizers</b> were employed in this study, which all differed in their speech-production techniques. This also enabled an examination {{of the effects of the}} human-likeness of synthetic voices on human emotions. Three ke...|$|R
40|$|Duration {{model is}} a {{standard}} part of current <b>speech</b> <b>synthesizers.</b> Many types of models have been used recently, e. g. multiplicative models ([8]), sum-of-products models ([9]) or decision tree-based models ([4],[5]). This paper follows a decision tree approach. It describes several versions of the duration model for Czech speech synthesis. The model presented here will be implemented in the Czech TTS system Demosthenes ([1],[2]) ...|$|R
50|$|The core idea of A.I. systems {{integration}} is making individual software components, such as <b>speech</b> <b>synthesizers,</b> interoperable with other components, such as common sense knowledgebases, {{in order to}} create larger, broader and more capable A.I. systems. The main methods that have been proposed for integration are message routing, or communication protocols that the software components use to communicate with each other, often through a middleware blackboard system.|$|R
