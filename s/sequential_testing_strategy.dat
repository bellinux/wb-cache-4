9|7108|Public
40|$|Objectives. To {{estimate}} the possible economic {{benefit of a}} <b>sequential</b> <b>testing</b> <b>strategy</b> with NT-proBNP {{to reduce the number}} of echocardiographies. Methods. Retrospective study in a third-party payer perspective. The costs were calculated from three Swedish counties: Blekinge, Östergötland, and Uppland. Two cut-off levels of NT-proBNP were used: 400 and 300 pg/mL. The cost-effectiveness of the testing strategy was estimated through the short-term cost avoidance and reduction in demand for echocardiographies. Results. The estimated costs for NT-proBNP tests and echocardiographies per county were reduced by 33 %– 36 % with the 400 pg/mL cut-off and by 28 %– 29 % with the 300 pg/mL cut-off. This corresponded to a yearly cost reduction of approximately € 2 – 5 million per million inhabitants in these counties. Conclusion. The use of NT-proBNP as a screening test could substantially reduce the number of echocardiographies in the diagnostic work-up of patients with suspected cardiac failure, as well as the associated costs...|$|E
40|$|In this paper, {{we propose}} {{methods of the}} {{determination}} of the rank of matrix. We consider a rank test for an unobserved matrix for which an estimate exists having normal asymptotic distribution of order N 1 / 2 where N is the sample size. The test statistic is based on the smallest estimated singular values. Using Matrix Perturbation Theory, the smallest singular values of random matrix converge asymptotically to zero in the order O(N- 1) and the corresponding left and right singular vectors converge asymptotically in the order O(N- 1 / 2). Moreover, the asymptotic distribution of the test statistic is seen to be chi-squared. The test has advantages over standard tests in being easier to compute. Two approaches are be considered <b>sequential</b> <b>testing</b> <b>strategy</b> and information theoretic criterion. We establish a strongly consistent of {{the determination of the}} rank of matrix using both the two approaches. Some economic applications are discussed and simulation evidence is given for this test. Its performance is compared to that of the LDU rank tests of Gill and Lewbel (1992) and Cragg and Donald (1996) ...|$|E
40|$|This paper studies how {{schooling}} admission tests affect {{economic performance}} {{in an economy}} where individuals are endowed with both academic and non academic abilities and both abilities matter for labor productivity. We develop a simple model with selective government held schools, where individuals signal their abilities by taking an admission test that sorts them into different schools. When abilities are poorly correlated in the population, as documented in the literature, a standard test based only on academic abilities {{is expected to be}} less efficient than a more balanced test, that considers both ability types. Contrary to this expectation, we show that this is not generally true, but depends both on the distribution of abilities in the population and on the marginal contribution of each ability type to individual productivity. It is also not generally true that the outcome of a more balanced test can be replicated by a <b>sequential</b> <b>testing</b> <b>strategy,</b> with government held schools testing academic abilities and firms testing non academic abilities on the sub-sample of graduates of elite schools...|$|E
40|$|This article {{provides}} {{a set of}} general conditions to identify efficient <b>sequential</b> <b>testing</b> <b>strategies</b> when <b>test</b> information is uncertain. We first survey the Bayesian Value-of-Information (VOI) approach to test selection. Second, we extend the approach to study <b>sequential</b> <b>testing</b> systems as applied in toxicology, but also relevant in other domains. We show how the order of tests in the sequence and the stopping rule depend on prior beliefs, the diagnostic performance of tests, and testing costs. We illustrate our findings with an example from short-term genotoxicity testing and discuss implications for developing optimized <b>sequential</b> <b>testing</b> <b>strategies</b> for risk management of chemicals...|$|R
40|$|Abstract. Cost-sensitive {{decision}} tree and cost-sensitive naïve Bayes are both new cost-sensitive learning models proposed recently {{to minimize the}} total cost of test and misclassifications. Each of them has its advantages and disadvantages. In this paper, we propose a novel cost-sensitive learning model, a hybrid cost-sensitive {{decision tree}}, called DTNB, to reduce the minimum total cost, which integrates the advantages of cost-sensitive decision tree and of the cost-sensitive naïve Bayes together. We empirically evaluate it over various <b>test</b> <b>strategies,</b> and our experiments show that our DTNB outperforms cost-sensitive decision and the cost-sensitive naïve Bayes significantly in minimizing {{the total cost of}} tests and misclassification based on the same <b>sequential</b> <b>test</b> <b>strategies,</b> and single batch strategies. ...|$|R
5000|$|Rasch, D. & Kubinger, K.D. (2010). On Optimal Planning and Analysing Empirical Studies - <b>Sequential</b> <b>Testing</b> as a <b>Strategy</b> for Reducing Average Sample Size. Advances in Theory and Application, 3, 43-59.|$|R
40|$|In {{this paper}} we {{consider}} the issue of testing for a unit root when it is uncertain {{as to whether or}} not a linear deterministic trend is present in the data. The Dickey-Fuller-type tests of Elliott, Rothenberg and Stock (1996), based on (local) GLS detrended (demeaned) data, are near asymptotically efficient when a deterministic trend is (is not) present in the data generating process. We consider a variety of strategies which aim to select the demeaned variant when a trend is not present and the detrended variant otherwise. Asymptotic and finite sample evidence demonstrates that some sophisticated strategies which involve auxiliary methods of trend detection are generally outperformed by a simple decision rule of rejecting the unit root null whenever either the GLS demeaned or GLS detrended Dickey-Fuller-type tests reject. We show that this simple strategy is asymptotically identical to a <b>sequential</b> <b>testing</b> <b>strategy</b> proposed by Ayat and Burridge (2000). Moreover, our results make it clear that any other unit root testing strategy, however elaborate, can at best only offer a rather modest improvement over the simple one. Unit root test; trend uncertainty; initial condition; asymtotic power; union of rejections decision rule...|$|E
40|$|This paper {{examines}} {{whether there}} is spatial integration between and within paddy markets {{in the north and}} south of Vietnam. The empirical model developed uses estimates of transfer costs to generalise Ravallion&rsquo;s model of spatial market integration to allow for threshold effects. A <b>sequential</b> <b>testing</b> <b>strategy</b> is used to test for market segmentation, the number of thresholds, long-run integration, informational efficiency and the Law of One Price within an error correction framework. We find neither threshold effects nor weak evidence of paddy market integration between northern and southern Vietnam. There is, however, evidence of both threshold effects and stronger market integration within the Red and Mekong River deltas. Whenever price spreads exceed their thresholds, at least 60 % of price changes are transmitted between regional markets within a month. Nonetheless, the instantaneous version of the Law of One Price only holds for a few regimes and market pairs. These results suggest that national level policies cannot be relied upon to stabilise or support paddy prices in Vietnam. Instead, policies need to be designed with the specific production, consumption and marketing characteristics of northern and southern Vietnam in mind...|$|E
40|$|In this paper, {{we develop}} {{methods of the}} {{determination}} of the rank of random matrix. Using the matrix perturbation theory to construct or find a suitable bases of the kernel (null space) of the matrix and to determine the limiting distribution of the estimator of the smallest singular values. We propose a new rank test for an unobserved matrix for which a root-N-consistent estimator is available and construct a Wald- type test statistic (generalized Wald test). The test, based on matrix perturbation theory, enable to determine how many singular values of the estimated matrix are insignificantly different from zero and we fully characterise the asymptotic distribution of the generalized Wald statistic under the most general conditions. We show that it is chi- square distribution under the null. In particular case, when the asymptotic covariance matrix has a Kronecker product form, the test statistic is equivalent to likelihood ratio test statistic and to Multiplier Lagrange test statistic. Two approaches to be considered are <b>sequential</b> <b>testing</b> <b>strategy</b> and information theoretic criterion. We establish a strongly consistent of {{the determination of the}} rank of matrix using the two approaches. Rank Testing; Matrix Perturbation Theory; Rank Estimation; Subspace Methods; Singular Value Decomposition; Weighting Matrices; Sequential Testing Strategy; Information Theoretic Criterion. ...|$|E
40|$|We {{explore the}} {{theoretical}} foundations of a "twenty questions" approach to pattern recognition. The {{object of the}} analysis is the computational process itself rather than probability distributions (Bayesian inference) or decision boundaries (statistical learning). Our formulation is motivated by applications to scene interpretation {{in which there are}} a great many possible explanations for the data, one ("background") is statistically dominant, and it is imperative to restrict intensive computation to genuinely ambiguous regions. The focus here is then on pattern filtering: Given a large set Y of possible patterns or explanations, narrow down the true one Y to a small (random) subset (Y) over cap subset of Y of "detected" patterns to be subjected to further, more intense, processing. To this end, we consider a family of hypothesis tests for Y is an element of A versus the nonspecific alternatives Y is an element of A(c). Each test has null type I error and the candidate sets A subset of Y are arranged in a hierarchy of nested partitions. These tests are then characterized by scope (vertical bar A vertical bar), power (or type II error) and algorithmic cost. We consider <b>sequential</b> <b>testing</b> <b>strategies</b> in which decisions are made iteratively, based on past outcomes, about which test to perform next and when to stop testing. The set (Y) over cap is then taken to be the set of patterns that have not been ruled out by the tests performed. The total cost of a strategy is the sum of the "testing cost" and the "postprocessing cost" (proportional to vertical bar(Y) over cap vertical bar) and the corresponding optimization problem is analyzed. As might be expected, under mild assumptions good designs for <b>sequential</b> <b>testing</b> <b>strategies</b> exhibit a steady progression from broad scope coupled with low power to high power coupled with dedication to specific explanations. In the assumptions ensuring this property a key role is played by the ratio cost/power. These ideas are illustrated in the context of detecting rectangles amidst clutter...|$|R
40|$|Diabetes {{mellitus}} predisposes {{people to}} premature atherosclerotic {{coronary artery disease}} (CAD). The risk of a myocardial infarction in diabetics without overt evidence of obstructive CAD matches that of patients without diabetes {{who have had a}} previous myocardial infarction. The available data suggest that occult CAD is a common finding among asymptomatic diabetics, ranging from 20 % to > 50 %. The diagnostic accuracy of myocardial perfusion single-photon emission computed tomography (SPECT) in diabetics appears to be comparable to that observed in nondiabetic individuals. As shown in other patient groups, the ischemic burden assessed by stress SPECT in subjects with diabetes is also linked to their increased risk of adverse cardiovascular events. Among patients with normal stress SPECT, however, those with diabetes are at significantly greater risk than non-diabetics. Testing diabetics with an abnormal resting electrocardiogram or with evidence of peripheral or carotid occlusive arterial disease appears to result in an excellent yield of abnormal SPECT findings, as does testing in the setting of dyspnea. However, recent evidence suggests that achieving an adequate yield in asymptomatic diabetics without overt evidence of CAD is a greater challenge. Further investigation of <b>sequential</b> <b>testing</b> <b>strategies</b> is needed in order to identify an efficient means for screening asymptomatic patients with diabetes...|$|R
40|$|The two-test two-population model, {{originally}} {{formulated by}} Hui and Walter, for estimation of test accuracy and prevalence estimation assumes conditionally independent tests, constant accuracy across populations and binomial sampling. The binomial assumption is incorrect if all individuals {{in a population}} e. g. child-care centre, village in Africa, or a cattle herd are sampled or if the sample size is large relative to population size. In this paper, we develop statistical methods for evaluating diagnostic test accuracy and prevalence estimation based on finite sample data {{in the absence of}} a gold standard. Moreover, two tests are often applied simultaneously for the purpose of obtaining a 'joint' <b>testing</b> <b>strategy</b> that has either higher overall sensitivity or specificity than either of the two <b>tests</b> considered singly. <b>Sequential</b> versions of such strategies are often applied in order to reduce the cost of testing. We thus discuss joint (simultaneous and <b>sequential)</b> <b>testing</b> <b>strategies</b> and inference for them. Using the developed methods, we analyse two real and one simulated data sets, and we compare 'hypergeometric' and 'binomial-based' inferences. Our findings indicate that the posterior standard deviations for prevalence (but not sensitivity and specificity) based on finite population sampling tend to be smaller than their counterparts for infinite population sampling. Finally, we make recommendations about how small the sample size should be relative to the population size to warrant use of the binomial model for prevalence estimation. ID: 6524; Accession Number: 20043118503. Publication Type: Journal Article. Language: English. Number of References: 18 ref. Subject Subsets: Public Healt...|$|R
40|$|Yaws is a non-venereal treponemal {{infection}} {{caused by}} Treponema pallidum subspecies pertenue. The disease is targeted by WHO for eradication by 2020. Rapid diagnostic tests (RDTs) are envisaged for confirmation of clinical cases during treatment campaigns and for certification of the interruption of transmission. Yaws testing requires both treponemal (trep) and non-treponemal (non-trep) assays for diagnosis of current infection. We evaluate a <b>sequential</b> <b>testing</b> <b>strategy</b> (using a treponemal RDT before a trep/non-trep RDT) {{in terms of}} cost and cost-effectiveness, relative to a single-assay combined testing strategy (using the trep/non-trep RDT alone), for two use cases: individual diagnosis and community surveillance. We use cohort decision analysis to examine the diagnostic and cost outcomes. We estimate cost and cost-effectiveness of the alternative testing strategies {{at different levels of}} prevalence of past/current infection and current infection under each use case. We take the perspective of the global yaws eradication programme. We calculate the total number of correct diagnoses for each strategy over a range of plausible prevalences. We employ probabilistic sensitivity analysis (PSA) to account for uncertainty and report 95 % intervals. At current prices of the treponemal and trep/non-trep RDTs, the sequential strategy is cost-saving for individual diagnosis at prevalence of past/current infection less than 85 % (81 - 90); it is cost-saving for surveillance at less than 100 %. The threshold price of the trep/non-trep RDT (below which the sequential strategy would no longer be cost-saving) is US$ 1. 08 (1. 02 - 1. 14) for individual diagnosis at high prevalence of past/current infection (51 %) and US$ 0. 54 (0. 52 - 0. 56) for community surveillance at low prevalence (15 %). We find that the sequential strategy is cost-saving for both diagnosis and surveillance in most relevant settings. In the absence of evidence assessing relative performance (sensitivity and specificity), cost-effectiveness is uncertain. However, the conditions under which the combined test only strategy might be more cost-effective than the sequential strategy are limited. A cheaper trep/non-trep RDT is needed, costing no more than US$ 0. 50 - 1. 00, depending on the use case. Our results will help enhance the cost-effectiveness of yaws programmes in the 13 countries known to be currently endemic. It will also inform efforts in the much larger group of 71 countries with a history of yaws, many of which will have to undertake surveillance to confirm the interruption of transmission...|$|E
40|$|With its {{increasing}} {{integration into}} the world economy, agricultural exports and rural incomes in Vietnam have increased substantially in recent years. At the sub-national level, however, there are concerns that not all regions and categories of agricultural producers have and {{will benefit from the}} ongoing liberalization of agricultural markets. Vietnam's elongated geography and lack of spatial market integration pose special problems in this regard. Accordingly, this study aims to answer three interrelated questions: (a) whether there is spatial integration between paddy markets in the North and South of Vietnam; (b) whether there is spatial integration in paddy markets within the North and within the South; and, (c) if within-region integration is stronger and faster than between-region integration. The empirical model we develop to answer these questions, uses estimates of transfer costs to generalize the well known model of spatial market integration due to Ravallion to allow for the possibility of threshold effects. A <b>sequential</b> <b>testing</b> <b>strategy</b> is developed which progressively tests for market segmentation, the number of thresholds, long-run market integration, common dynamics/informational efficiency, and (a strict version of) the 'Law' of One Price within an error-correction framework. When the unrestricted version of this model is estimated using monthly paddy prices for eight markets between 1993 and 2006, we find weak evidence of market integration between paddy markets in the North and South of Vietnam with an absence of threshold effects. However, there is evidence of both threshold effects and stronger forms of spatial market integration for paddy markets within the North and within the South, with at least 60 % percent of price changes being transmitted between markets within one month whenever price spreads exceeds their upper or lower thresholds. The extent and speed of price transmission within regional paddy markets is generally faster in the South than the North of Vietnam. However, the instantaneous version of the 'Law' of One Price, which requires full price adjustment to occur within a month, only holds for a few regimes and market pairs. Three main policy implications flow from these results. First, since there is limited evidence of integration between paddy markets in the North and South of Vietnam, national level policies cannot be relied upon to stabilize or support paddy prices. Second, since there is evidence of spatial market integration within the Red River and Mekong River deltas, paddy markets within these regions can be relied upon to transmit price signals between deficit and surplus areas relatively well. Third, since the speed and extent of price transmission is relatively rapid within the North and within the South of Vietnam, the private sector trade can be relied upon to transfer rice and paddy between markets in an efficient manner. Problems might, however, emerge if large demand-supply imbalances were to emerge between the North and South, as transfer costs would prevent private sector trade taking place. In these circumstances, the public sector might need to intervene, in a consistent and market friendly way, to ensure adequate food supplies in the short-term. Market integration; Paddy market; Error-correction; Spatial integration; Vietnam...|$|E
40|$|This paper {{proposes a}} new <b>testing</b> <b>strategy</b> for {{unemployment}} hysteresis as the joint restriction of a unit-root in {{the unemployment rate}} and no feedback effect of unemployment in the Phillips wage equation. The associated test statistics are derived when this joint restriction is imposed and when a <b>sequential</b> two steps <b>testing</b> <b>strategy</b> is adopted. An empirical application leads to reject the null hypothesis of wage hysteresis {{for most of our}} OECD countries. Evidence against hysteresis is reinforced when accounting for wage adjustments in the bivariate approach. Copyright Springer-Verlag Berlin Heidelberg 2003 Key words: hysteresis, unemployment persistence, unit root, wage adjustments, Wald tests., JEL classification: C 12, C 22,...|$|R
40|$|This paper {{proposes a}} new <b>testing</b> <b>strategy</b> for {{unemployment}} hysteris as the joint restriction of a unit-root in {{the unemployment rate}} and no-effect {{of the level of}} unemployment in the Phillips wage equation. The relevant test statistics are derived when this joint restriction is imposed during estimation and when a <b>sequential</b> two steps <b>testing</b> <b>strategy</b> is adopted. The empirical application leads to rejection of the null hypothesis of wage hysteresis for most of our sample of OECD countries. We get an interesting contrast between the "core European countries" -rejecting hysteresis, but not unemployment non-stationarity- and the scandinavian ones where unemployment appears to be stabilized despite a lack of the wage correction. ...|$|R
40|$|Many {{continuous}} {{medical tests}} often {{rely on a}} threshold for diagnosis. There are two <b>sequential</b> <b>testing</b> <b>strategies</b> of interest: Believe the Positive (BP) and Believe the Negative (BN). BP classifies a patient positive if either the first test is greater than a threshold θ 1 or negative on the first test and greater than θ 2 on the second test. BN classifies a patient positive if the first test is greater than a threshold θ 3 and greater than θ 4 on the second test. Threshold pairs θ = (θ 1, θ 2) or (θ 3, θ 4), depending on strategy, are defined as optimal if they maximized GYI = Se + r(Sp – 1). Of interest is to determine if these optimal threshold, or optimal operating point (OOP), estimates are “good” when calculated from a sample. The methods proposed in this dissertation derive formulae to estimate θ assuming tests follow a binormal distribution, using the Newton-Raphson algorithm with ridging. A simulation study is performed assessing bias, root mean square error, percentage of over estimation of Se/Sp, and coverage of simultaneous confidence intervals and confidence regions for sets of population parameters and sample sizes. Additionally, OOPs are compared to the traditional empirical approach estimates. Bootstrapping is used to estimate the variance of each optimal threshold pair estimate. The study shows that parameters such as the area under the curve, ratio of standard deviations of disease classification groups within tests, correlation between tests within a disease classification, total sample size, and allocation of sample size to each disease classification group were all influential on OOP estimation. Additionally, the study shows that this method is an improvement over the empirical estimate. Equations for researchers to use in estimating total sample size and SCI width are also developed. Although the models did not produce high coefficients of determination, they are {{a good starting point}} for researchers when designing a study. A pancreatic cancer dataset is used to illustrate the OOP estimation methodology for <b>sequential</b> <b>tests...</b>|$|R
40|$|Uncomplicated Urinary Tract Infections (UTIs) {{are common}} in primary care {{resulting}} in substantial costs. Since antimicrobial resistance against antibiotics for UTIs is rising, accurate diagnosis is needed in settings with low rates of multidrug-resistant bacteria. To compare the cost-effectiveness of different strategies to diagnose UTIs in women who contacted their general practitioner (GP) with painful and/or frequent micturition between 2006 and 2008 in and around Amsterdam, The Netherlands. This is a model-based cost-effectiveness analysis using data from 196 women who underwent four tests: history, urine stick, sediment, dipslide, and the gold standard, a urine culture. Decision trees were constructed reflecting 15 diagnostic strategies comprising different parallel and sequential combinations of the four tests. Using the decision trees, for each strategy the costs {{and the proportion of}} women with a correct positive or negative diagnosis were estimated. Probabilistic sensitivity analysis was used to estimate uncertainty surrounding costs and effects. Uncertainty was presented using cost-effectiveness planes and acceptability curves. Most <b>sequential</b> <b>testing</b> <b>strategies</b> resulted in higher proportions of correctly classified women and lower costs than parallel <b>testing</b> <b>strategies.</b> For different willingness to pay thresholds, the most cost-effective strategies were: 1) performing a dipstick after a positive history for thresholds below € 10 per additional correctly classified patient, 2) performing both a history and dipstick for thresholds between € 10 and € 17 per additional correctly classified patient, 3) performing a dipstick if history was negative, followed by a sediment if the dipstick was negative for thresholds between € 17 and € 118 per additional correctly classified patient, 4) performing a dipstick if history was negative, followed by a dipslide if the dipstick was negative for thresholds above € 118 per additional correctly classified patient. Depending on decision makers' willingness to pay for one additional correctly classified woman, the strategy consisting of performing a history and dipstick simultaneously (ceiling ratios between € 10 and € 17) or performing a sediment if history and subsequent dipstick are negative (ceiling ratios between € 17 and € 118) are the most cost-effective strategies to diagnose a UT...|$|R
40|$|PURPOSE. To {{determine}} whether sensitivity estimates from an individual’s previous visual field tests {{can be incorporated}} into perimetric procedures to improve accuracy and reduce test– retest variability at subsequent visits. METHODS. Computer simulation was used to determine the error, distribution of errors and presentation count for a series of perimetric algorithms. Baseline procedures were Full Threshold and Zippy Estimation by <b>Sequential</b> <b>Testing</b> (ZEST). Retest <b>strategies</b> were (1) allowing ZEST to continue from the previous test without reinitializing the probability density func-tion [pdf]; (2) running ZEST with a Gaussian pdf centered about the previous result; (3) retest minimizing uncertainty (REMU), a new procedure combining suprathreshold and ZEST procedures incorporating prior test information. Empiric visual field data of 265 control and 163 patients with glaucoma wer...|$|R
40|$|The use of {{confirmatory factor}} {{analytic}} procedures {{to examine the}} dimensionality of writing skills as measured by a large-scale direct writing test was illustrated. Internal construct validity evidence {{about the nature of}} writing skills measured by the test was provided. Data uessd were scores assigned by about 100 trained professional raters on a state-wide competency-based test of writing skills. A total of 3, 430 students responded to all four prompts of the writing <b>tests.</b> A <b>sequential</b> hypothesis <b>testing</b> <b>strategy</b> was followed for a one-factor model, a two-factor mode-specific model, and a two-factor occasion-specific model. These were compared to a null model specifying no common factors. Results from these analyses provided evidence about the usefulness of the construct-based hypotheses for explaining the variability in data from this test. A one-factor model provided a more plausible and parsimonious representation of the data than did the two-factor hypotheses. Evidence in support of a unidimensional interpretation of the scores suggests there may be little practical distinction between narrative and explanatory scores. A table displays goodness-of-fit statistics. (SLD) Reproductions supplied by EDRS are the best that can be made from the original document...|$|R
40|$|A {{fundamental}} problem in managing product {{development is the}} optimal timing, frequency, and fidelity of <b>sequential</b> <b>testing</b> activities that are carried out to evaluate novel product concepts and designs. In this paper, we develop a mathematical model that treats testing as an activity that generates information about technical and customer-need related problems. An analysis of the model results in several important findings. First, optimal <b>testing</b> <b>strategies</b> need to balance the tension between several variables, including the increasing cost of redesign, {{the cost of a}} test as function of fidelity, and the correlation between <b>sequential</b> <b>tests.</b> Second, a simple form of our model results in an EOQ-like result: The optimal number of tests (called the Economic Testing Frequency or ETF) is the square root of the ratio of avoidable cost and the cost of a test. Third, the relationship between <b>sequential</b> <b>tests</b> can have an impact on optimal <b>testing</b> <b>strategies.</b> If <b>sequential</b> <b>tests</b> are increasing refinements of one another, managers should invest their budgets in a few high-fidelity tests, whereas if the tests identify problems independently of one another it may be more effective if developers carry out a higher number of lower-fidelity tests. Using examples, the implications for managerial practice are discussed and suggestions for further research undertakings are provided. Testing, Experimentation, Information, Problem Solving, Prototyping, Design, Product Development...|$|R
40|$|This paper {{suggests}} {{a new model}} for reliability demonstration of safety-critical systems, based on the TRW Software Reliability Theory. The paper describes the model; the test equipment required and <b>test</b> <b>strategies</b> based on the various constraints occurring during software development. The paper also compares a new testing method, Single Risk <b>Sequential</b> <b>Testing</b> (SRST), with the standard Probability Ratio <b>Sequential</b> <b>Testing</b> method (PRST), and concludes that: • SRST provides higher chances of success than PRST • SRST takes less time to complete than PRST • SRST satisfies the consumer risk criterion, whereas PRST provides a much smaller consumer risk than the requirement...|$|R
40|$|Discriminating {{subjects}} at clinical {{high risk}} (CHR) for psychosis who will develop psychosis {{from those who}} will not {{is a prerequisite for}} preventive treatments. However, it is not yet possible to make any personalized prediction of psychosis onset relying only on the initial clinical baseline assessment. Here, we first present a systematic review of prognostic accuracy parameters of predictive modeling studies using clinical, biological, neurocognitive, environmental, and combinations of predictors. In a second step, we performed statistical simulations to <b>test</b> different probabilistic <b>sequential</b> 3 -stage <b>testing</b> <b>strategies</b> aimed at improving prognostic accuracy on top of the clinical baseline assessment. The systematic review revealed that the best environmental predictive model yielded a modest positive predictive value (PPV) (63 %). Conversely, the best predictive models in other domains (clinical, biological, neurocognitive, and combined models) yielded PPVs of above 82 %. Using only data from validated models, 3 -stage simulations showed that the highest PPV was achieved by sequentially using a combined (clinical + electroencephalography), then structural magnetic resonance imaging and then a blood markers model. Specifically, PPV was estimated to be 98 % (number needed to treat, NNT = 2) for an individual with 3 positive <b>sequential</b> <b>tests,</b> 71 %- 82 % (NNT = 3) with 2 positive tests, 12 %- 21 % (NNT = 11 - 18) with 1 positive test, and 1 % (NNT = 219) for an individual with no positive tests. This work suggests that sequentially testing CHR subjects with predictive models across multiple domains may substantially improve psychosis prediction following the initial CHR assessment. Multistage <b>sequential</b> <b>testing</b> may allow individual risk stratification of CHR individuals and optimize the prediction of psychosis...|$|R
40|$|Abstract Background HPV DNA {{diagnostic}} tests for epidemiology monitoring (research purpose) or cervical cancer screening (clinical purpose) {{have often been}} considered separately. Women with positive Linear Array (LA) polymerase chain reaction (PCR) research test results typically are neither informed nor referred for colposcopy. Recently, a <b>sequential</b> <b>testing</b> by using Hybrid Capture 2 (HC 2) HPV clinical test as a triage before genotype by LA has been adopted for monitoring HPV infections. Also, HC 2 has been reported as a more feasible screening approach for cervical cancer in low-resource countries. Thus, knowing the performance of <b>testing</b> <b>strategies</b> incorporating HPV clinical test (i. e., HC 2 -only or using HC 2 as a triage before genotype by LA) compared with LA-only testing in measuring HPV prevalence will be informative for public health practice. Method We conducted a Monte Carlo simulation study. Data were generated using mathematical algorithms. We designated the reported HPV infection prevalence in the U. S. and Latin America as the “true” underlying type-specific HPV prevalence. Analytical sensitivity of HC 2 for detecting 14 high-risk (oncogenic) types {{was considered to be}} less than LA. Estimated-to-true prevalence ratios and percentage reductions were calculated. Results When the “true” HPV prevalence was designated as the reported prevalence in the U. S., with LA genotyping sensitivity and specificity of (0. 95, 0. 95), estimated-to-true prevalence ratios of 14 high-risk types were 2. 132, 1. 056, 0. 958 for LA-only, HC 2 -only, and <b>sequential</b> <b>testing,</b> respectively. Estimated-to-true prevalence ratios of two vaccine-associated high-risk types were 2. 359 and 1. 063 for LA-only and <b>sequential</b> <b>testing,</b> respectively. When designated type-specific prevalence of HPV 16 and 18 were reduced by 50  %, using either LA-only or <b>sequential</b> <b>testing,</b> prevalence estimates were reduced by 18  %. Conclusion Estimated-to-true HPV infection prevalence ratios using LA-only <b>testing</b> <b>strategy</b> are generally higher than using HC 2 -only or using HC 2 as a triage before genotype by LA. HPV clinical testing can be incorporated to monitor HPV prevalence or vaccine effectiveness. Caution is needed when comparing apparent prevalence from different <b>testing</b> <b>strategies...</b>|$|R
40|$|Abstract. Cost-sensitive {{decision}} tree learning {{is very important}} and popular in machine learning and data mining community. There are many literatures focusing on misclassification cost and test cost at present. In real world application, however, the issue of time-sensitive {{should be considered in}} costsensitive learning. In this paper, we regard the cost of time-sensitive in costsensitive learning as waiting cost (referred to WC), a novelty splitting criterion is proposed for constructing cost-time sensitive (denoted as CTS) {{decision tree}} for maximal decrease the intangible cost. And then, a hybrid <b>test</b> <b>strategy</b> that combines the <b>sequential</b> <b>test</b> with the batch <b>test</b> <b>strategies</b> is adopted in CTS learning. Finally, extensive experiments show that our algorithm outperforms the other ones with respect to decrease in misclassification cost. ...|$|R
40|$|This paper {{develops}} a simple sequential multiple horizon noncausation <b>test</b> <b>strategy</b> for trivariate VAR models (with one auxiliary variable). We apply the <b>test</b> <b>strategy</b> to a rolling window study of money supply and real income, {{with the price}} of oil, the unemployment rate and the spread between the Treasury bill and commercial paper rates as auxiliary processes. Ours is the first study to control simultaneously for common stochastic trends, sensitivity of causality tests to chosen sample period, null hypothesis over-rejection, <b>sequential</b> <b>test</b> size bounds, and the possibility of causal delays. Evidence suggests highly significant direct or indirect causality from M 1 to real income, in particular through the unemployment rate and M 2 once we control for cointegration. multiple horizon causality, Wald tests, parametric bootstrap, money-income causality, rolling windows, cointegration...|$|R
40|$|We compare <b>testing</b> <b>strategies</b> for Granger noncausality in vector autoregressions (VARs) {{that may}} or may not have unit roots and cointegration. <b>Sequential</b> <b>testing</b> methods are examined; these test for cointegration and use either a differenced VAR or a vector error {{correction}} model (VECM), in which to undertake the main noncausality test. Basically, the pretesting strategies attempt to verify the validity of appropriate standard limit theory. These methods are contrasted with an augmented lag approach that ensures the limiting Chi Square null distribution irrespective of the data’s nonstationarity characteristics. Our simulations involve bivariate and trivariate VARs in which we allow for the lag order to be selected by general to specific testing as well as by model selection criteria. We find that the current practice of pretesting for cointegration can result in severe over-rejections of the noncausal null while overfitting suffers less size distortion with often little loss in power. cointegration, error correction model, vector autoregressive model, lag length selection, model selection methods, <b>sequential</b> <b>testing,</b> information criteria...|$|R
40|$|Group <b>sequential</b> <b>tests</b> for Phase III {{clinical}} trials Distribution theory Computation Benefits of group <b>sequential</b> <b>testing</b> • Error spending tests • A survival data example • Group <b>sequential</b> <b>tests</b> with a delayed response • From group sequential to adaptive designs • Adapting the target population: Enrichment design...|$|R
40|$|Sequential {{approach}} to hypotheses testing {{is used in}} medical trials, statistical quality control, finance and other applications (see Mukhopadhyay et al. (2004)). <b>Sequential</b> <b>tests</b> are applied efficiently if observed data satisfy the hypothetical model. However, some part of observations does not follow the theoretical models in practice, so the models are distorted by outliers (see Huber (2004) and Hampel et al. (1984)). This results in considerable difference of the performance characteristics of <b>sequential</b> <b>tests</b> from the hypothetical values, and <b>sequential</b> <b>tests</b> lose their optimal properties. In the paper we develop the approach proposed in Kharin (2002) for robustness analysis of <b>sequential</b> <b>testing</b> of simple hypotheses on discrete distributions and the robustification method proposed in Kharin et al. (2002), for robustness analysis of <b>sequential</b> <b>tests</b> of simple hypotheses on the parameter of distributions. We also discuss some robustification techniques, and construct the robust <b>sequential</b> <b>test</b> under outliers. The approximate expressions for error probabilities of <b>sequential</b> <b>tests</b> of simple hypotheses under outliers are obtained, and the accuracy for these approximations is given. The asymptoti...|$|R
40|$|Buckley’s {{approach}} (Buckley (2004), (2005), (2006)) uses sets {{of confidence}} intervals by {{taking into consideration}} both of the uncertainty and impreciseness of concepts that produce triangular shaped fuzzy numbers for the estimator. This approach produces fuzzy test statistics and fuzzy critical values in hypothesis testing. In addition, the sample size is fixed for this test. When data comes sequentially, however, it is not suitable to study with a fixed sample size test. In such cases, sequential and group <b>sequential</b> <b>tests</b> are recommended. Unlike a <b>sequential</b> <b>test,</b> a group of <b>sequential</b> <b>test</b> provides substantial savings in sample and enables us to make decisions as early as possible. This intends paper to combine the benefits of group <b>sequential</b> <b>test</b> and Buckley's approach using α-cuts. It attempts to show that using α-cuts can be used within the group <b>sequential</b> <b>tests.</b> To illustrate the test more explicitly a numerical example is also given...|$|R
40|$|Rapid {{modeling}} and efficient testing methods {{are important in}} a number of aerospace applications. In this study efficient <b>testing</b> <b>strategies</b> were evaluated in a wind tunnel test environment and combined to suggest a promising approach for both ground-based and flight-based experiments. Benefits of using Design of Experiment techniques, well established in scientific, military, and manufacturing applications are evaluated in combination with newly developing methods for global nonlinear modeling. The nonlinear modeling methods, referred to as Learn-to-Fly methods, utilize fuzzy logic and multivariate orthogonal function techniques that have been successfully demonstrated in flight test. The blended approach presented has a focus on experiment design and identifies a <b>sequential</b> <b>testing</b> process with clearly defined completion metrics that produce increased testing efficiency...|$|R
40|$|A one-sided testing problem {{based on}} an i. i. d. sample of {{observations}} is considered. The usual one-sided <b>sequential</b> probability ratio <b>test</b> {{would be based on}} a random walk derived from these observations. Here we propose a <b>sequential</b> <b>test</b> where the random walk is replaced by Lindley’s random walk which starts anew at zero as soon as it becomes negative. We derive the asymptotics of the expected sample size and the error probabilities of this <b>sequential</b> <b>test.</b> We discuss the advantages of this test for certain nonsymmetric situations. Copyright Springer-Verlag 2004 <b>Sequential</b> <b>test,</b> Lindley’s random walk, expected sample size,...|$|R
40|$|In {{this study}} a group <b>sequential</b> <b>test</b> of {{non-parametric}} statistics is examined {{in order to}} compare two groups of survival data. A new general form for a group <b>sequential</b> <b>test</b> of non-parametric statistics is given. The distribution of test statistics, obtained {{at the end of}} each stage, have been derived for this general form. In addition, an example based on a simulated data set is used to illustrate the test process that covers the group <b>sequential</b> <b>test</b> of non-parametric statistics in the given general form...|$|R
5000|$|<b>Sequential</b> <b>Tests</b> of Statistical Hypotheses, Reading: Addison-Wesley 1970 ...|$|R
40|$|Hardware {{verification}} and <b>sequential</b> <b>test</b> generation are {{aspects of}} the [...] . In this paper, a common formal framework for hardware verification and <b>sequential</b> <b>test</b> pattern generation is presented, {{which is based on}} modeling the circuit behavior with temporal logic. In addition, a new approach to cope with non resetable ipops in <b>sequential</b> <b>test</b> generation is proposed, which is not restricted to stuck-at faults. Based on this verification view, it is possible to provide the designer with one tool for checking circuit correctness and generating test patterns. Its first implementation and application is also described...|$|R
40|$|This article {{presents}} a differential geometrical method for analyzing <b>sequential</b> <b>test</b> procedures. It {{is based on}} the primal result on the conformal geometry of statistical manifold developed in Kumon, Takemura and Takeuchi (2011). By introducing curvature-type random variables, the condition is first clarified for a statistical manifold to be an exponential family under an appropriate <b>sequential</b> <b>test</b> procedure. This result is further elaborated for investigating the efficient <b>sequential</b> <b>test</b> in a multidimensional curved exponential family. The theoretical results are numerically examined by using von Mises-Fisher and hyperboloid models...|$|R
