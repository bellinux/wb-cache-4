9789|110|Public
25|$|Stochastic {{optimization}} is {{an umbrella}} set of methods that includes <b>simulated</b> <b>annealing</b> {{and numerous other}} approaches.|$|E
25|$|Evolutionary methods, gene {{expression}} programming, <b>simulated</b> <b>annealing,</b> expectation-maximization, non-parametric methods and {{particle swarm optimization}} are other methods for training neural networks.|$|E
25|$|Stochastic {{tunneling}} {{attempts to}} overcome the increasing difficulty <b>simulated</b> <b>annealing</b> runs have in escaping from local minima as the temperature decreases, by 'tunneling' through barriers.|$|E
30|$|<b>Simulated</b> <b>annealling</b> {{was first}} {{described}} in Kirkpatrick et al. (1983). It {{is used to}} solve optimization problems from various areas. Tabu search is another widely used metaheuristic method originated from Glover and Laguna (1997).|$|R
40|$|We {{analyze the}} {{performance}} of <b>simulated</b> quantum <b>annealing</b> (SQA) on an optimization problem for which <b>simulated</b> classical <b>annealing</b> (SA) is provably inefficient because of a high energy barrier. We present evidence that SQA can pass through this barrier to find the global minimum efficiently. This demonstrates the potential for SQA to inherit some {{of the advantages of}} quantum annealing (QA), since this problem has been previously shown to be efficiently solvable by quantum adiabatic optimization...|$|R
40|$|The size of {{datasets}} {{is growing}} {{fast in the}} social sciences—e. g., administrative data with millions of observations • Our methods are becoming computationally intensive: MCMC, genetic optimization, <b>simulating</b> <b>annealing,</b> and MATCHING • These methods are often used for problems whose asymptotic order is exponential: O(cN), c> 1 • We try to turn exponential problems into polynomial time problems: • O(N 2 log(N)) for pair matching • O(N 3 log(N max(dist))) for full matchin...|$|R
25|$|Interacting Metropolis-Hasting {{algorithms}} (a.k.a. Sequential Monte Carlo) combined <b>simulated</b> <b>annealing</b> {{moves with}} an acceptance-rejection {{of the best}} fitted individuals equipped with an interacting recycling mechanism.|$|E
25|$|Dual-phase {{evolution}} {{is a family}} of algorithms and processes (to which <b>simulated</b> <b>annealing</b> belongs) that mediate between local and global search by exploiting phase changes in the search space.|$|E
25|$|Tabu search (TS) {{is similar}} to <b>simulated</b> <b>annealing</b> in that both {{traverse}} the solution space by testing mutations of an individual solution. While <b>simulated</b> <b>annealing</b> generates only one mutated solution, tabu search generates many mutated solutions {{and moves to the}} solution with the lowest fitness of those generated. To prevent cycling and encourage greater movement through the solution space, a tabu list is maintained of partial or complete solutions. It is forbidden to move to a solution that contains elements of the tabu list, which is updated as the solution traverses the solution space.|$|E
40|$|Abstract—Herein we {{describe}} the pseudo-codes for five heuristic approaches applied to a synchronous multicarrier multiuser detection (MUD) of multiple receive antennas {{code division multiple access}} (SIMO MC-CDMA), named 1 -opt local search (1 -LS), particle swarm optimization (PSO and woPSO), genetic algorithm (GA), simulation annealing (SA) and Tabu search (RTS and STTS) heuristic algorithms (Heur) in a single-objective optimization form. Index Terms—MC-CDMA, multiuser detection, local search, particle swarm optimization, genetic algorithm, <b>simulating</b> <b>annealing,</b> Tabu search, single-objective optimization...|$|R
40|$|Google Machine Reassignment Problem (GMRP) is a {{real world}} problem {{proposed}} at ROADEF/EURO challenge 2012 competition which must be solved within 5 min. GMRP consists in reassigning a set of services into a set of machines for which {{the aim is to}} improve the machine usage while satisfying numerous constraints. This paper proposes an evolutionary <b>simulating</b> <b>annealing</b> (ESA) algorithm for solving this problem. <b>Simulating</b> <b>annealing</b> (SA) is a single solution based heuristic, which has been successfully used in various optimisation problems. The proposed ESA uses a population of solutions instead of a single solution. Each solution has its own SA algorithm and all SAs work in parallel manner. Each SA starts with different initial solution which can lead to a different search path with distinct local optima. In addition, mutation operators are applied once the solution cannot be improved for a certain number of iterations. This will not only help the search avoid being trapped in a local optima, but also reduce computation time. Because new solutions are not generated from scratch but based on existing ones. This study shows that the proposed ESA method can outperform state of the art algorithms on GMRP...|$|R
40|$|Abstract. In {{this paper}} {{we present a}} {{methodology}} for blind source separation (BSS) based on a coherence function {{to solve the problem}} of linear instantaneous mixtures of signals. The proposed methodology consists of minimizing the coherence function using a heuristic algorithm based on the <b>simulating</b> <b>annealing</b> method. Also, we derived an analytical expression of the coherence for the BSS model, in which it is found that independent and identically distributed (iid) Gaussian components can be recovered. Our results show satisfactory performance in comparison with traditional methods...|$|R
25|$|TSP is a touchstone {{for many}} general {{heuristics}} devised for combinatorial optimization such as genetic algorithms, <b>simulated</b> <b>annealing,</b> Tabu search, ant colony optimization, river formation dynamics (see swarm intelligence) and the cross entropy method.|$|E
25|$|In {{artificial}} intelligence, stochastic programs work {{by using}} probabilistic methods to solve problems, as in <b>simulated</b> <b>annealing,</b> stochastic neural networks, stochastic optimization, genetic algorithms, and genetic programming. A problem itself may be stochastic as well, as in planning under uncertainty.|$|E
25|$|Before the Monte Carlo {{method was}} developed, {{simulations}} tested a previously understood deterministic problem and statistical sampling {{was used to}} estimate uncertainties in the simulations. Monte Carlo simulations invert this approach, solving deterministic problems using a probabilistic analog (see <b>Simulated</b> <b>annealing).</b>|$|E
40|$|This {{paper is}} {{concerned}} with the optimal controller design of networked control systems (NCSs). Aiming at a class of NCSs, in which network-induced delay and data packet dropout exist simultaneously from sensor to controller, this paper proposes a novel optimal controller based on a new adaptive genetic algorithm and <b>simulated</b> <b>anneal</b> algorithm (AGASA), guaranteed cost controller (GCC) and state feedback controller (SFC). The proposed controller is less conservative than GCC and SFC, and can be applied to both linear and nonlinear NCSs. Two examples are presented to illustrate the proposed controller. 1...|$|R
40|$|The strongest {{evidence}} for superiority of quantum annealing on spin glass problems {{has come from}} comparing <b>simulated</b> quantum <b>annealing</b> using quantum Monte Carlo (QMC) methods to <b>simulated</b> classical <b>annealing</b> [G. Santoro et al., Science 295, 2427 (2002) ]. Motivated by experiments on programmable quantum annealing devices we revisit the question of when quantum speedup may be expected for Ising spin glass problems. We find that even though a better scaling compared to <b>simulated</b> classical <b>annealing</b> can be achieved for QMC simulations, this advantage is due to time discretization and measurements which are not possible on a physical quantum annealing device. QMC simulations in the physically relevant continuous time limit, on the other hand, do not show superiority. Our results imply that care {{has to be taken}} when using QMC simulations to assess quantum speedup potential and are consistent with recent arguments that no quantum speedup should be expected for two-dimensional spin glass problems...|$|R
40|$|Abstract:-Aiming at the {{shortcoming}} {{of basic}} PSO algorithm, that is, easily trapping into local minimum, we propose an advanced PSO algorithm with SA and apply this new algorithm for solving TSP problem. The core of algorithm {{is based on}} the PSO algorithm. SA method is used to slow down the degeneration of the PSO swarm and increase the swarm’s diversity. The comparative experiments were made between PSO-SA, basic GA, basic SA and basic ACA on solving TSP problem. Results show PSO-SA is more superior to other methods. Key-Words:- particle swarm optimization; <b>simulating</b> <b>annealing</b> algorithm; TSP; GA...|$|R
25|$|In {{optimization}} problems, heuristic algorithms {{can be used}} to find {{a solution}} close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting {{closer and closer to the}} optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, <b>simulated</b> <b>annealing,</b> and genetic algorithms. Some of them, like <b>simulated</b> <b>annealing,</b> are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.|$|E
25|$|For {{any given}} finite problem, the {{probability}} that the <b>simulated</b> <b>annealing</b> algorithm terminates with a global optimal solution approaches 1 as the annealing schedule is extended. This theoretical result, however, is not particularly helpful, since the time required to ensure a significant probability of success will usually exceed the time required for a complete search of the solution space.|$|E
25|$|<b>Simulated</b> <b>annealing</b> (SA) is {{a related}} global {{optimization}} technique which traverses the search space by generating neighboring solutions {{of the current}} solution. A superior neighbor is always accepted. An inferior neighbor is accepted probabilistically based on the difference in quality and a temperature parameter. The temperature parameter is modified as the algorithm progresses to alter {{the nature of the}} search.|$|E
40|$|Deposition {{of silicon}} dioxide (SiO 2) is a {{critical}} step of integrated circuit manufacturing; hence it is monitored during the manufacturing process at a grid of points defined on the wafer area. Since collecting thickness measurements is expensive, it is a compelling issue to investigate how a sub grid can be identified. A strategy based on spatial prediction and <b>simulating</b> <b>annealing</b> is proposed to tackle the problem which proved to be effective when applied to a real process. A diagnostic device for monitoring the deposition process is also discussed which can be usefully adopted in the day-to-day activity by practitioners acting in process control of a microelectronics fab...|$|R
30|$|<b>Simulate</b> the <b>annealing</b> {{algorithm}} to locally {{optimize the}} position of each particle in the particle swarm optimization algorithm and repeatedly run the iteration process until the termination condition is satisfied. The improved algorithm is shown in Fig.  2.|$|R
40|$|Abstract—In recent years, Hidden Markov Models (HMM) {{have been}} {{increasingly}} applied in data mining applications. How-ever, most authors have used classical optimization Expectation-Maximization (EM) scheme. A new method of HMM learn-ing based on Particle Swarm Optimization (PSO) has been developed. Along with others global approaches as <b>Simulating</b> <b>Annealing</b> (SIM) and Genetic Algorithms (GA) the following local gradient {{methods have been}} also compared: classical Expectation-Maximization algorithm, Maximum A Posteriory approach (MAP) and Bayes Variational learning (VAR). The methods are evaluated on a synthetic data set using different evaluation criteria including classification problem. The most re-liable optimization approach in terms of performance, numerical stability and speed is VAR learning followed by PSO approach. I...|$|R
25|$|It {{has also}} been used to produce near-optimal {{solutions}} to the travelling salesman problem. They have an advantage over <b>simulated</b> <b>annealing</b> and genetic algorithm approaches of similar problems when the graph may change dynamically; the ant colony algorithm can be run continuously and adapt to changes in real time. This is of interest in network routing and urban transportation systems.|$|E
25|$|This {{notion of}} slow cooling {{implemented}} in the <b>Simulated</b> <b>Annealing</b> algorithm is interpreted as a slow decrease in the probability of accepting worse solutions as the solution space is explored. Accepting worse solutions is a fundamental property of metaheuristics because it allows for a more extensive search for the global optimal solution. In general, the <b>Simulated</b> <b>Annealing</b> algorithms work as follows. At each time step, the algorithm randomly selects a solution close to the current one, measures its quality, and then decides to move to it or {{to stay with the}} current solution based on either one of two probabilities between which it chooses {{on the basis of the}} fact that the new solution is better or worse than the current one. During the search, the temperature is progressively decreased from an initial positive value to zero and affects the two probabilities: at each step, the probability of moving to a better new solution is either kept to 1 or is changed towards a positive value; instead, the probability of moving to a worse new solution is progressively changed towards zero.|$|E
25|$|A very {{different}} kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other optimization algorithms are <b>simulated</b> <b>annealing,</b> beam search and random optimization.|$|E
40|$|Abstract. Recrystallization law was {{investigated}} at different annealing processes. The {{results showed that}} the annealing temperature was a main effect factor on the properties of Ti-IF steel. Nominal recrystallization temperature of the samples was evaluated as 620 ℃. <b>Simulated</b> batch <b>annealing</b> was performed by two-stage heating. Determined recrystallization temperature was 660 ℃, the recrystallization of samples heated at 660 ℃ was completed in 68 min. For 800 ℃, the equiaxial recrystallized grains were obtained. <b>Simulated</b> continuous <b>annealing</b> by rapidly heating to different temperatures, samples were held for 100 s and then cooled in air. Recrystallization nucleation was not observed until 660 ℃. The occurrence of secondary recrystallization was observed at 900 ℃...|$|R
40|$|The <b>simulated</b> <b>anneal</b> {{procedure}} {{based on}} the Metropolis Monte Carlo algorithm {{is presented as a}} new numerical method for the analysis of electron-spin-resonance spectra by the spin Hamiltonian formalism. It gives an alternative way for finding the global minimum of a function, which describes the difference for each of the resonance fields between the theoretically calculated energy difference and the experimentally measured microwave energy. Compared to the iterative procedures commonly used, it has the advantage of not getting stuck in local minima of the parameter space. This implies that the procedure can be started with arbitrary initial parameters. The usefulness of the method is illustrated by its application to the analysis of four defects in alkali halides doped with heavy metal ions. ...|$|R
40|$|The Traveling Salesman Problem (TSP) is {{an integer}} {{programming}} problem that {{falls into the}} category of NP-Hard problems. As the problem become larger, {{there is no guarantee that}} optimal tours will be found within reasonable computation time. Heuristics techniques, like genetic algorithm and <b>simulating</b> <b>annealing,</b> can solve TSP instances with different levels of accuracy. Choosing which algorithm to use in order to get a best solution is still considered as a hard choice. This paper suggests domain reduction as a tool to be combined with any meta-heuristic so that the obtained results will be almost the same. The hybrid approach of combining domain reduction with any meta-heuristic encountered the challenge of choosing an algorithm that matches the TSP instance in order to get the best results. ...|$|R
25|$|A {{solution}} to the puzzle is then found. Approaches for shuffling the numbers include <b>simulated</b> <b>annealing,</b> genetic algorithm and tabu search. Stochastic-based algorithms {{are known to be}} fast, though perhaps not as fast as deductive techniques. Unlike the latter however, optimisation algorithms do not necessarily require problems to be logic-solvable, giving them the potential to solve a wider range of problems. Algorithms designed for graph colouring are also known to perform well with Sudokus. It is also possible to express a Sudoku as an integer linear programming problem. Such approaches get close to a solution quickly, and can then use branching towards the end. The simplex algorithm is able to solve non-proper Sudokus, indicating if the Sudoku is not valid (no solution), or providing the set of answers when {{there is more than one}} solution.|$|E
500|$|The {{technique}} of <b>simulated</b> <b>annealing,</b> by which an existing MSA produced by another method is refined {{by a series}} of rearrangements designed to find better regions of alignment space than the one the input alignment already occupies. Like the genetic algorithm method, <b>simulated</b> <b>annealing</b> maximizes an objective function like the sum-of-pairs function. <b>Simulated</b> <b>annealing</b> uses a metaphorical [...] "temperature factor" [...] that determines the rate at which rearrangements proceed and the likelihood of each rearrangement; typical usage alternates periods of high rearrangement rates with relatively low likelihood (to explore more distant regions of alignment space) with periods of lower rates and higher likelihoods to more thoroughly explore local minima near the newly [...] "colonized" [...] regions. This approach has been implemented in the program MSASA (Multiple Sequence Alignment by <b>Simulated</b> <b>Annealing).</b>|$|E
2500|$|An {{open-source}} MATLAB {{program for}} general <b>simulated</b> <b>annealing</b> exercises.|$|E
40|$|Bee colony {{optimization}} (BCO) {{is one of}} {{the most}} recent algorithms in swarm intelligence that can be used in optimization problems this algorithm is based on the intelligent behavior of honey bees in foraging process. In this paper bee colony optimization is applied to solve the task scheduling problem which tasks have dependency with each other. Scheduling of tasks that represents by directed acyclic graph is a NP-complete problem. The main purpose of this problem is obtaining the minimum schedule length that is called make-span. To realize the performance of BCO in this problem, the obtained results are presented and compared with the most successful methods such as Ant colony system, Tabu search and <b>simulate</b> <b>annealing.</b> The comparison shows that BCO produces the solutions in a different way and it is still among the bests...|$|R
40|$|In this paper, a region-based spatio-temporal Markov random field (STMRF) {{model is}} {{proposed}} to segment moving objects semantically. The STMRF model combines segmentation results of four successive frames and integrates the temporal continuity in the uniform energy function. The segmentation procedure {{is composed of}} two stages: one is the short-term’s classification {{and the other is}} temporal integration. At the first stage, moving objects are extracted by a region-based MRF model between two frames in a frame group of four successive frames. At the second stage, the ultimate semantic object is labeled by minimization the energy function of the STMRF model. Such phased segmentation process is corresponding to a multi-level <b>simulated</b> <b>anneal</b> strategy. Experimental results show that the proposed algorithm can efficiently capture the motion semantic meaning of objects and accurately extract moving objects. 1...|$|R
40|$|In {{this paper}} {{optimization}} technique, <b>Simulate</b> <b>annealing</b> Algorithm(SA) is studied & applied to optimize Suspension system of automobile. Suspension system is collection of springs & dampers. Selection of values of stiffness {{of the spring}} & damping values of dampers is the important task, because ride comfort of the driver is depend on this. This selection is done here {{with the help of}} SA. Programming has been developed in MATLAB 7. 5, in which the steps of SA are followed to get the output as a seat acceleration, which is measure of ride, with input values of stiffness & damping constants. Principle behind optimization by SA is a naturally serial algorithm, but its behaviour can be controlled by the cooling schedule. Convergence of this method has been plotted with no. of generations...|$|R
