3|10000|Public
40|$|A {{detailed}} analysis of a carbon dioxide transcritical power cycle using an industrial low-grade <b>stream</b> <b>of</b> <b>process</b> gases as its heat source is presented. The methodology is divided in four steps: energy analysis, exergy analysis, finite size thermodynamics and calculation of the heat exchangers' surface. The results have been calculated for fixed temperature and mass flow rate of the heat source, fixed maximum and minimum temperatures in the cycle and a fixed sink temperature by varying the high pressure of the cycle and its net power output. The main results show {{the existence of an}} optimum high pressure {{for each of the four}} steps; in the first two steps, the optimum pressure maximises the thermal or exergetic efficiency while in the last two steps it minimises the product UA or the heat exchangers' surface. These high pressures are very similar for the energy and exergy analyses. The last two steps also have nearly identical optimizing high pressures that are significantly lower that the ones for the first two steps. In addition, the results show that the augmentation of the net power output produced from the limited energy source has no influence on the results of the energy analysis, decreases the exergetic efficiency and increases the heat exchangers' surface. Changing the net power output has no significant impact on the high pressures optimizing each of the four steps. Waste heat Energy analysis Exergy analysis Finite size thermodynamics Heat exchanger surface Optimization...|$|E
40|$|The Business Process Management field {{addresses}} design, improvement, management, support, {{and execution}} of business processes. In doing so, we argue that it focuses more on developing modeling notations and process design approaches than on the needs and preferences of the individual who is modeling (i. e., the user). New data-centric process modeling approaches are taken as a relevant and timely <b>stream</b> <b>of</b> <b>process</b> design approaches to test our argument. First, we provide {{a review of existing}} data-centric process approaches, culminating in a theoretical classification framework. Next, we empirically evaluate three specific approaches with regard to the claims they make. We had participants representative of actual users try out these approaches on realistic scenarios via a series of workshops. Participants assessed to what extent quality claims from the literature could be recognized within the workshop sessions. The results of this evaluation substantiate a number of claims behind the approaches, but also identify opportunities to further improve them. Most prominently, we found that the usability aspects of all considered approaches are a source of concern. This leads us to the insight that usability aspects of process design approaches are crucial and, in the perception of groups representative of actual users, leave much to be desired. In that sense, our research {{can be seen as a}} wake-up call for process modeling notation designers to consider the usability side—and as such, the interest of the human modeler—more than is currently the case...|$|E
40|$|Documentation is {{a process}} of slime; it lingers between the effect and affect of performance. It is a {{connected}} trace element that can be reconsidered by others – who may slide on the slippery <b>stream</b> <b>of</b> <b>process.</b> This may sound rather poetic, or perhaps messy, but using phenomenology and écriture féminine I propose a paper that explores how process, language and slime are connected to practice as research – and this is explored through the mollusc. The mollusc represents the safe-house containing the flesh of the body. There are distinct parts; the flesh is in excess of the harder exterior, yet they operate together. The flesh is able to protrude from the protective safe-house experiencing the balance between the weight of the protection and the lightness of the vulnerability that exposure and freedom brings. In order to move, or to progress, the body must take the chance of being vulnerable, to move out of its safe-house and explore new spaces. Meanwhile a processural trace is left to reveal the path followed, the snail-mollusc’s trail documenting its journey. When the original body is ready to leave the hard exterior, the hard exterior could always house another body, perhaps a different type of body, and not always a well-fitting body, but it would still be useful. Rather like a methodological approach or a theory applied to a body of work, {{it can be used to}} house all types of bodies but not all of them fit so well. The shell becomes a still space in which we can imagine an archive, a commercial gallery, a document of the process, but the process is no longer moving. The body on the other hand is transient; like all bodies it is sensual and touching, emotive and some are re-producing. The new body can start a new journey. If we become too reliant on the protective safe-house we would not be able to move or progress, we would perish without leaving a trace of any experience. We may have felt safe but we would have experienced very little...|$|E
50|$|The Marula tree is also favoured for its sap. Cicadas squirt a {{constant}} <b>stream</b> <b>of</b> <b>processed</b> sap from {{the tips of}} their abdomens, deftly directing the jet away from the tree.|$|R
5000|$|Foulant {{transport}} {{with the}} <b>stream</b> <b>of</b> the <b>process</b> fluid (most often by advection); ...|$|R
5000|$|When {{applying}} the [...] class {{one can use}} the instance properties , , and [...] of that class to access the standard <b>streams</b> <b>of</b> the <b>process.</b>|$|R
30|$|The open drying {{of plants}} or feed and the smoking {{of food in}} the exhaust <b>stream</b> <b>of</b> {{combustion}} <b>processes</b> are particularly prone to PCDD/F contamination.|$|R
30|$|In {{order to}} {{implement}} the tiered processing scheme of the servers, the software needs to be both clients and servers to facilitate the sending and receiving of video traffic and camera controls. Mid-level servers, for instance, may need to broadcast a <b>stream</b> <b>of</b> <b>processed</b> data to the high-level server for viewing by the user, {{while at the same}} time being able to download cropped object images from low-level servers.|$|R
50|$|The null {{device is}} {{typically}} used for disposing <b>of</b> unwanted output <b>streams</b> <b>of</b> a <b>process,</b> or as a convenient empty file for input streams. This is usually done by redirection.|$|R
50|$|The steam of {{evaporated}} oxygen vapour in {{the shell}} of the boiler vents back into the distillation column. It rises through the column packing material and encounters the descending <b>stream</b> <b>of</b> liquid <b>process</b> air.|$|R
50|$|In priority-based {{scheduling}} algorithms, a {{major problem}} is indefinite block, or starvation. A process that is ready to run but waiting for the CPU can be considered blocked. A priority scheduling algorithm can leave some low-priority processes waiting indefinitely. A steady <b>stream</b> <b>of</b> higher-priority <b>processes</b> can prevent a low-priority process from ever getting the CPU.|$|R
30|$|Therefore, {{the social}} {{perspective}} of learning shifts {{the focus of}} attention from information processing to the <b>processes</b> <b>of</b> interaction and participation that provide a real-life context for learning (Gherardi et al. 1998). In this perspective, learning can be considered as a continuous <b>stream</b> <b>of</b> social <b>processes,</b> which are the result of numerous connections and interactions between practitioners (Higgins and Mirza 2012).|$|R
50|$|Lime {{softening}} is now often {{combined with}} newer membrane processes to reduce waste streams. Lime softening {{can be applied}} to the concentrate (or reject <b>stream)</b> <b>of</b> membrane <b>processes,</b> thereby providing a <b>stream</b> <b>of</b> substantially reduced hardness (and thus TDS), that may be used in the finished stream. Also, in cases with very hard source water (often the case in Midwestern USA ethanol production plants), lime softening can be used to pre-treat the membrane feed water.|$|R
50|$|Hugo has {{exhibited}} at SHOWstudio, showing {{part of his}} body of work 'Reflecting The Bright Lights'. This formed part of the 'Practice to Deceive' exhibition in April 2011. For this body of work he built a room-sized camera and utilised an 18th-century lens to create a series of portraits shot directly onto glass plates. During the course of the exhibition, he re-built the camera on site in SHOWstudio for an online live <b>stream</b> <b>of</b> the <b>process</b> <b>of</b> creating the portraits.|$|R
40|$|Abstract—Due to the {{availability}} of virtualization technolo-gies and related cloud infrastructures, the amount and also the complexity of logging data of systems and services grow steadily. Automated correlation and aggregation techniques are required to support a contemporary processing and interpre-tation of relevant logging data. In the past, this was achieved using highly centralized logging systems. Based on this fact, the paper introduces a prototype for an automated semantical correlation, aggregation and condensation of logging information. The prototype relies on a NoSQL storage back-end {{that is used to}} persist consolidated messages of distributed logging sources in a highly performant manner. This step of consolidation includes strategies for minimizing long-term storage, and by using correlation techniques also offers possibilities to detect anomalies in the <b>stream</b> <b>of</b> <b>processed</b> messages. In this context, we will present the special requirements of handling scalable logging systems in highly dynamic infrastructures like enterprise cloud environments, which provide dynamic systems, services and applications...|$|R
3000|$|... b, Kilpatrick 2015; Kohonen 2001 b; Sandstede 2007; Troy 2008 a,b; Werning 2012 a). Thus, {{the neural}} {{information}} storage and retrieval {{in the long-term}} memory, for example, can be understand by means of computational adaptive resonance mechanisms in the dominant waveforms, or “modes” (Grossberg and Somers 1991), and by warming up and annealing of oscillation modes by <b>streams</b> <b>of</b> informational <b>processes</b> in the context of computational “energy functions,” like in “Harmony Theory” (Smolensky and Legendre 2006 a).|$|R
50|$|Naval {{stores are}} {{recovered}} from the tall oil byproduct <b>stream</b> <b>of</b> Kraft <b>process</b> pulping <b>of</b> pines in the United States. Tapping of living pines remains common {{in other parts of}} the world. Turpentine and pine oil may be recovered by steam distillation of oleoresin or by destructive distillation of pine wood; solvent extraction of shredded stumps and roots has become more common with the availability of inexpensive naphtha. Rosin remains in the still after turpentine and water have boiled off.|$|R
40|$|Gilles Kahn's elegant {{model of}} {{deterministic}} concurrent computation using sequential processes connected by Unix style pipes [Ka 74] {{has been used}} by Ashcroft, Wadge {{as the basis for the}} functional data flow programming language Lucid [W, A 85] Kahn's data flow model is "first order and so does not allow <b>streams</b> <b>of</b> <b>processes</b> for example. This has been reflected in Lucid by the restriction, until recently, to first order functions. In [Wa 91] Wadge suggests a treatment for implementing higher order function definitions in Lucid using intentsional multi-dimensional algebras. This has unfortunately placed the Lucid cart before the Kahn data flow horse. Is there a natural extension to Kahn's model which allows higher order functions, or can we only understand them in terms of a specific programming language such as Lucid? This report argues that higher order functions can be understood in terms of Kahn's data flow model by constructing a treatment therein for second order functions. This construction provides new insights into obtaining a generalisation of Kahn Data Flow which allows second order functions...|$|R
40|$|This paper draws {{attention}} to a nascent <b>stream</b> <b>of</b> strategy <b>process</b> research in which action is seen as primarily creative, rather than rational or normative. It shows how creative action theory, which emphasises the importance of embodied expression, emergent intention and social interaction, might furnish valuable new insights into strategic change. In particular, the paper {{highlights the importance of}} considering the strategist as fully embodied, intuitive and expressive. The paper draws on a novel empirical illustration to demonstrate both the potential and challenges of using creative action to reframe our understanding of strategic change...|$|R
5000|$|Anodizing {{is one of}} {{the more}} {{environmentally}} friendly metal finishing processes. With the exception of organic (aka integral color) anodizing, the by-products contain only small amounts of heavy metals, halogens, or volatile organic compounds. Integral color anodizing produces no VOCs, heavy metals, or halogens as all of the byproducts found in the effluent <b>streams</b> <b>of</b> other <b>processes</b> come from their dyes or plating materials. [...] The most common anodizing effluents, aluminium hydroxide and aluminium sulfate, are recycled for the manufacturing of alum, baking powder, cosmetics, newsprint and fertilizer or used by industrial wastewater treatment systems.|$|R
40|$|Over {{the past}} few years surveys have {{expanded}} to new populations, have incorporated measurement of new and more complex substantive issues and have adopted new data collection tools. At the same time there has been a growing reluctance among many household populations to participate in surveys. These factors have combined to present survey designers and survey researchers with increased uncertainty about the performance of any given survey design at any particular point in time. This uncertainty has, in turn, challenged the survey practitioner's ability to control the cost of data collection and quality of resulting statistics. The development of computer-assisted methods for data collection has provided survey researchers with tools to capture a variety <b>of</b> <b>process</b> data ('paradata') {{that can be used to}} inform cost-quality trade-off decisions in realtime. The ability to monitor continually the <b>streams</b> <b>of</b> <b>process</b> data and survey data creates the opportunity to alter the design during the course of data collection to improve survey cost efficiency and to achieve more precise, less biased estimates. We label such surveys as 'responsive designs'. The paper defines responsive design and uses examples to illustrate the responsive use of paradata to guide mid-survey decisions affecting the non-response, measurement and sampling variance properties of resulting statistics. Copyright 2006 Royal Statistical Society. ...|$|R
40|$|EP 219083 A UPAB: 19930922 Multiple electro-dialysis {{chambers}} {{are each}} formed between adjacent ion-selective membranes (30) laid on or affixed to frame surfaces stacked together. Each pair of adjacent cell frame members (3) are shaped as mirror-images w. r. t. an inter-mediate plane {{parallel to the}} membranes. On their end edges (1, 2) pane members have supply and connecting bones (8, 9 : 10, 11) perpendicular to the membranes, each with a hydraulic cross-section of about 100 - 500 sq. mm. and alternating with main inlet and outlet bores (33, 34) in end plates. Each pair of cell frame members has a fan-shaped pattern of distribution bores (4, 5) with hydraulic cross-section 0. 04 - 6. 5 sq. mm., {{the two halves of}} the bore are in upper and lower meeting surfaces which provide flat seals for the bores. Two different types of frame and end plates can provide stacks for up to four different <b>streams</b> <b>of</b> <b>process</b> liq. USE/ADVANTAGE - For desalination or water purificn. plant or for recovering valuable ions from aq. solns. Sealing problems are reduced, and pressure conditions optimised. 2 /...|$|R
40|$|This article {{discusses}} {{efficiency and}} effectiveness issues in caching {{the results of}} queries submitted to a Web search engine (WSE). We propose SDC (Static Dynamic Cache), a new caching strategy aimed to efficiently exploit the temporal and spatial locality present in the <b>stream</b> <b>of</b> <b>processed</b> queries. SDC extracts from historical usage data {{the results of the}} most frequently submitted queries and stores them in a static, read-only portion of the cache. The remaining entries of the cache are dynamically managed according to a given replacement policy and are used for those queries that cannot be satisfied by the static portion. Moreover, we improve the hit ratio of SDC by using an adaptive prefetching strategy, which anticipates future requests by introducing a limited overhead over the back-end WSE. We experimentally demonstrate the superiority of SDC over purely static and dynamic policies by measuring the hit ratio achieved on three large query logs by varying the cache parameters and the replacement policy used for managing the dynamic part of the cache. Finally, we deploy and measure the throughput achieved by a concurrent version of our caching system. Our tests show how the SDC cache can be efficiently exploited by many thread...|$|R
40|$|This paper discuses effciency and effectivenes {{issues in}} caching {{the results of}} queries {{submitted}} to a Web Search Engine (WSE). We propose SDC, a new caching strategy aimed to effciently exploit the temporal and spatial locality present in the <b>stream</b> <b>of</b> <b>processed</b> queries. SDC stores {{the results of the}} most frequently submitted queries in a static, read-only portion of the cache, while the queries that cannot be satisfied by the static portion compete for the remaining entries of the cache according to a given replacement policy. Moreover, we improved the hit-ratio of SDC by using a speculative prefetching strategy, which anticipates future requests by introducing a limited overhead over the backend WSE. We experimentally demonstrated the superiority of SDC over purely static and dynamic policies by measuring the hit-ratio achieved on three large query logs by varying the cache parameters and the replacement policy used for managing the dynamic part of the cache. Finally, we deployed and measured the throughput achieved by a concurrent version of our caching system. Our tests showed how the SDC cache can be efficiently exploited by several threads that concurrently serve the queries of different users...|$|R
40|$|In {{the recent}} years {{a plethora of}} studies {{attempted}} to investigate the apparent dissociation between two anatomically and behaviourally observed visual systems: the dorsal and the ventral <b>stream</b> <b>of</b> visual <b>processes.</b> An appropriate method that demonstrates behavioural dissociations in the healthy population are visual illusions of size. Under {{the effect of these}} illusions, besides perceptual misjudgments of distance, reaching and grasping is also affected, mostly under monocular vision. Investigating a ventral stream specific visual cue, namely familiar size, we used size illusions with familiar objects to explore the impact of recent training on reaching and grasping behaviour under binocular vision...|$|R
40|$|This paper {{addresses}} the synthesis and optimization <b>of</b> crystallization <b>processes</b> for p-xylene recovery for systems with feed <b>streams</b> <b>of</b> high concentration, {{a case that}} arises in hybrid designs where {{the first step is}} commonly performed by adsorption. A novel superstructure and its corresponding mixed-integer nonlinear programming (MINLP) model are proposed. The distinct feature of this superstructure is the capability to generate optimum or near optimum flowsheets {{for a wide range of}} specifications of p-xylene compositions in the feed <b>stream</b> <b>of</b> the <b>process.</b> In order to cope with the complexity of the MINLP model a two-level decomposition approach, consisting of the solution of an aggregated model and a detailed model, is proposed. The results obtained show good performance of the decomposition strategy, and the optimal flowsheets and p-xylene recoveries are in agreement with results reported in patents...|$|R
2500|$|The lighter product {{coming off}} the top of the tower as a vapour — called the {{overhead}} product — is then cooled so it will condense back into a liquid. [...] To achieve full separation, a <b>stream</b> <b>of</b> product is <b>processed</b> through a series of towers. “Spec” or high-grade product is taken {{off the top of}} a tower, and the bottom product becomes the feedstock for the next tower.|$|R
30|$|The human {{evolution}} is directly associated {{to the use}} of materials. During the last century, humanity had significant development powered by the use of fossil resources, which largely impacted the materials discovery and use. However, there are many concerns about the use of fossils, based mainly on economic and environmental issues. These concerns motivate the study, development, and use of renewable resources, including biobased materials. Brazil {{is one of the largest}} producers of agricultural commodities, which can be used to produce renewable materials. This review describes actual production of some renewable materials in Brazil and future possibilities to generate them using biomass residual <b>streams</b> <b>of</b> industrial <b>processes.</b>|$|R
40|$|Data <b>streams</b> <b>of</b> {{consecutive}} <b>processes</b> <b>of</b> polyergatic {{control systems}} have been analysed. The paper aims at the working out a type information model <b>of</b> controlled <b>processes</b> in ergatic control systems, concepts of its effective realization and tools for creation such models set-up for simulation. The new tabular-alphabetic information model <b>of</b> consecutive <b>processes</b> for ergatic control systems has been proposed; new concepts of storage such models in computer have been put forward, substantiated and tested in practice. Original tools for creation tabular-alphabetic models <b>of</b> consecutive <b>processes</b> set-up for simulation designed for use during the development of ergatic control {{systems have been}} worked out. The subject of reduction in practice is the program system. The paper result may find their field of application in data collection and data processing systems <b>of</b> consecutive <b>process</b> controlAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|The {{analysis}} of the material composition metallomagnetic admixtures of mineral premix. It is shown that the dressed metallomagnetic impurity includes low-magnetic particles with low magnetic susceptibility. Removing these particles from the product <b>stream</b> in <b>process</b> <b>of</b> magnetic separation using high-energy rare earth magnets is a challenging task...|$|R
40|$|Checking the {{compliance}} <b>of</b> a business <b>process</b> execution {{with respect to}} a set of regulations is an important issue in several settings. A common way of representing the expected behavior <b>of</b> a <b>process</b> is to describe it as a set of business con-straints. Runtime verification and monitoring facilities allow us to continuously determine the state of constraints on the current process execution, and to promptly detect violations at runtime. A plethora of studies has demonstrated that in several settings business constraints can be formalized in terms of temporal logic rules. However, in virtually all exist-ing works the process behavior is mainly modeled in terms of control-flow rules, neglecting the equally important data perspective. In this paper, we overcome this limitation by presenting a novel monitoring approach that tracks <b>streams</b> <b>of</b> <b>process</b> events (that possibly carry data) and verifies if the process execution is compliant with a set of data-aware business constraints, namely constraints not only referring to the temporal evolution of events, but also to the temporal evolution of data. The framework is based on the formal specification of business constraints in terms of first-order linear temporal logic rules. Operationally, these rules are translated into finite state automata for dynamically rea-soning on partial, evolving execution traces. We show the versatility of our approach by formalizing (the data-aware extension of) Declare, a declarative, constraint-based process modeling language, and by demonstrating its application on a concrete case dealing with web security...|$|R
40|$|The aim <b>of</b> <b>process</b> discovery, {{originating}} {{from the area}} <b>of</b> <b>process</b> mining, is to discover a process model based on business process execution data. A majority <b>of</b> <b>process</b> discovery techniques relies on an event log as an input. An event log is a static source of historical data capturing the execution <b>of</b> a business <b>process.</b> In this paper we focus on process discovery relying on online <b>streams</b> <b>of</b> business <b>process</b> execution events. Learning process models from event streams poses both challenges and opportunities, i. e. we need to handle unlimited amounts of data using finite memory and, preferably, constant time. We propose a generic architecture that allows for adopting several classes <b>of</b> existing <b>process</b> discovery techniques in context <b>of</b> event <b>streams.</b> Moreover, we provide several instantiations of the architecture, accompanied by implementations in the process mining tool-kit ProM ([URL] Using these instantiations, we evaluate several dimensions <b>of</b> stream-based <b>process</b> discovery. The evaluation shows that the proposed architecture allows us to lift process discovery to the streaming domain. Comment: Accepted for publication in "Knowledge and Information Systems; " (Springer: [URL]...|$|R
40|$|Process mining is an {{emerging}} research area {{that brings the}} well-established data mining solutions to the chal- lenging business process modeling problems. Mining <b>streams</b> <b>of</b> business <b>processes</b> in the real time as they are generated is a necessity to obtain an instant knowledge from big process data. In this paper, we introduce an efficient approach for exploring and counting process fragments from a <b>stream</b> <b>of</b> events to infer a process model using the Heuristics Miner algorithm. Our novel approach, called StrProM, builds prefix-trees to extract sequential patterns of events from the stream. StrProM uses a batch-based approach to continuously update and prune these prefix-trees. The models are generated from those trees after applying a decaying mechanism over their statistics. The extensive experimental evaluation demonstrates the superiority of our approach over a state-of-the-art technique in terms of execution time using a real dataset, while delivering models of a comparable quality. I...|$|R
5000|$|The {{relief is}} mountainous, with {{dissecting}} valleys {{of a larger}} number <b>of</b> smaller <b>streams.</b> The <b>process</b> <b>of</b> erosion is developed at several places. The Lužnica basin has an altitude 470-520 m. The area is surrounded by the Suva Mountain in the west, Ruj Mountain in the south, and Vlaška Mountain in the northeast.|$|R
40|$|A {{method for}} {{determining}} the efficiency and scope for improvement <b>of</b> a <b>process</b> plant's energy consumption, based on pinch technology, is reviewed. While this is a generic approach that applies in most cases, there are certain cases where the results are not clear cut. This paper identifies three cases which deviate from the norm. The first case considers a process with no process-process heat exchangers and explains its relevance in relation to "pinch parameters". The second case examines the <b>streams</b> <b>of</b> a <b>process,</b> the temperature range of {{which is not a}} continuum, and shows the consequences on pinch parameters. In the third case an algorithm is introduced which treats the potential energy savings from utilizing a utility "waste" stream, such as low pressure steam or condensate, the quantity of which is dependent on the quantity of source utility...|$|R
40|$|We {{study the}} {{sensitivity}} of seismic imaging to lossy data compression using {{a new generation of}} directional, nonseparable wavelets developed by Donoho et al [1, 3, 5, 6, 7], and compare these results to those obtained using a conventional biorthogonal wavelet compression technique. In our study we use two <b>streams</b> <b>of</b> minimally <b>processed</b> seismic data: the SEG-EAEG synthetic salt model and a field data set. Our objective is to show that features of the imaged data are much better preserved at compression ratios of 20 : 1 and higher with the newer technology. The advent of the Grid has emphasized the need for the improved compression and effective transmission of large data sets, such as seismic surveys, over the Internet. It is toward this purpose that this work is directed...|$|R
