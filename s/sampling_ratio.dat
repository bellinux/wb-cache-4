124|1603|Public
2500|$|In {{imbalanced}} datasets, {{where the}} <b>sampling</b> <b>ratio</b> {{does not follow}} the population statistics, one can resample the dataset in a conservative manner called minimax sampling. The minimax sampling has its origin in Anderson minimax ratio whose value is proved to be 0.5: in a binary classification, the class-sample sizes should be chosen equally. This ratio can be proved to be minimax ratio only under the assumption of LDA classifier with Gaussian distributions. The notion of minimax sampling is recently developed for a general class of classification rules, called class-wise smart classifiers. In this case, the <b>sampling</b> <b>ratio</b> of classes is selected so that the worst case classifier error over all the possible population statistics for class prior probabilities, would be the ...|$|E
2500|$|When data is convolved with a {{function}} with wide support, such as for downsampling {{by a large}} <b>sampling</b> <b>ratio,</b> because of the Convolution theorem and the FFT algorithm, it may be faster to transform it, multiply pointwise by the transform of the filter and then reverse transform it. [...] Alternatively, a good filter is obtained by simply truncating the transformed data and re-transforming the shortened data set.|$|E
5000|$|In {{imbalanced}} datasets, {{where the}} <b>sampling</b> <b>ratio</b> {{does not follow}} the population statistics, one can resample the dataset in a conservative manner called minimax sampling. The minimax sampling has its origin in Anderson minimax ratio whose value is proved to be 0.5: in a binary classification, the class-sample sizes should be chosen equally. This ratio can be proved to be minimax ratio only under the assumption of LDA classifier with Gaussian distributions. The notion of minimax sampling is recently developed for a general class of classification rules, called class-wise smart classifiers. In this case, the <b>sampling</b> <b>ratio</b> of classes is selected so that the worst case classifier error over all the possible population statistics for class prior probabilities, would be the ...|$|E
3000|$|... via {{two or more}} {{sampling}} in some {{order with}} given <b>sampling</b> <b>ratios.</b> The <b>sampling</b> <b>ratios</b> of the finally sampled point set [...]...|$|R
40|$|It is {{well known}} that {{samplers}} are linear time varying systems, so in general, the commutativity of samplers does not hold. There are some existing results on the commutativity of conventional decimators and expanders, block samplers with the same integer block lengths but different integer <b>sampling</b> <b>ratios,</b> and block samplers with different integer block lengths and integer <b>sampling</b> <b>ratios.</b> This paper extends the existing results to a necessary and sufficient condition for the commutativity of block decimators and expanders with arbitrary rational <b>sampling</b> <b>ratios</b> and block lengths. Â© 2012 Elsevier Inc. All rights reserved...|$|R
5000|$|... where ri' is {{the value}} of the <b>sample</b> <b>ratio</b> with the ith group omitted.|$|R
50|$|The {{oversampling}} ratio {{determines how}} many data elements to pull as samples. The {{goal is to}} get a good representation of the distribution of the data. If the data values are widely distributed, in that there are not many duplicate values, then a small <b>sampling</b> <b>ratio</b> is needed. In other cases where there are many duplicates in the distribution, a larger oversampling ratio will be necessary.|$|E
50|$|When data is convolved with a {{function}} with wide support, such as for downsampling {{by a large}} <b>sampling</b> <b>ratio,</b> because of the Convolution theorem and the FFT algorithm, it may be faster to transform it, multiply pointwise by the transform of the filter and then reverse transform it. Alternatively, a good filter is obtained by simply truncating the transformed data and re-transforming the shortened data set.|$|E
5000|$|Stratified {{sampling}} is {{not useful}} when the population cannot be exhaustively partitioned into disjoint subgroups.It {{would be a}} misapplication of the technique to make subgroups' sample sizes proportional {{to the amount of}} data available from the subgroups, rather than scaling sample sizes to subgroup sizes (or to their variances, if known to vary significantly -- e.g. by means of an F Test). Data representing each subgroup are taken to be of equal importance if suspected variation among them warrants stratified sampling. If subgroup variances differ significantly and the data needs to be stratified by variance, {{it is not possible to}} simultaneously make each subgroup sample size proportional to subgroup size within the total population. For an efficient way to partition sampling resources among groups that vary in their means, variance and costs, see [...] "optimum allocation".The problem of stratified sampling in the case of unknown class priors (ratio of subpopulations in the entire population) can have deleterious effect on the performance of any analysis on the dataset, e.g. classification. In that regard, minimax <b>sampling</b> <b>ratio</b> can be used to make the dataset robust with respect to uncertainty in the underlying data generating process.|$|E
40|$|Graduation date: 1984 The <b>sample</b> <b>ratio</b> of sublegal {{to legal}} male crabs {{retained}} in crab pots {{was used in}} calculations to predict future harvest of Dungeness crabs, Cancer magister, along the Oregon coast. Accurate predictions by this method require a <b>sample</b> <b>ratio</b> representative of the population <b>ratio.</b> I <b>sampled</b> the Oregon Dungeness crab population out of the ports of Astoria and Newport during the 1974 - 75 and 1975 - 76 crab seasons. Samples were taken with commercial crab pots modified to retain sublegal as well as legal crabs. My predictions of future harvest of Dungeness crab were inaccurate because the <b>sample</b> <b>ratio</b> was an inaccurate and biased estimator of the population ratio of sublegal to legal crabs. My {{results indicate that the}} samples obtained by my sampling gear and methods were not representative of the number of sublegal and legal crabs in the population. The <b>sample</b> <b>ratio</b> may be improved as an estimator of the population ratio by decreasing the time the modified crab pots are allowed to fish and by random sampling of the Dungeness crab population...|$|R
3000|$|The {{consecutive}} <b>sample</b> <b>ratio</b> β is {{a trade-off}} parameter. The optimal sparse recovery performance {{is to be}} expected for the case that A=N [...]...|$|R
30|$|As {{shown in}} Fig. 6, {{we can see}} that the PSNR of the reconstructed Lena at {{different}} <b>sampling</b> <b>ratios</b> was better than the other two low-memory techniques.|$|R
30|$|The {{noise figure}} {{of the system is}} further {{improved}} by using the Q enhancement technique. Although the receiver has already exhibited good performance with the simple ASK modulation and the <b>sampling</b> <b>ratio</b> of 20, more advanced modulation and the higher <b>sampling</b> <b>ratio</b> can be used to further improve the performance of the system.|$|E
40|$|Abstract. Although {{much work}} has been done on {{duplicate}} document detection (DDD) and its applications, we observe the absence of a systematic study of the performance and scalability of large-scale DDD. It is still unclear how various parameters of DDD, such as similarity threshold, precision/recall requirement, <b>sampling</b> <b>ratio,</b> document size, correlate mutually. In this paper, correlations among several most important parameters of DDD are studied and the impact of <b>sampling</b> <b>ratio</b> is of most interest since it heavily affects the accuracy and scalability of DDD algorithms. An empirical analysis is conducted on a million documents from the TREC. GOV collection. Experimental results show that even using the same <b>sampling</b> <b>ratio,</b> the precision of DDD varies greatly on documents with different size. Based on this observation, an adaptive sampling strategy for DDD is proposed, which minimizes the <b>sampling</b> <b>ratio</b> within the constraint of a given precision threshold. We believe the insights from our analysis are helpful for guiding the future large scale DDD work. ...|$|E
3000|$|... s, and the {{integration}} {{window of the}} charge sampling architecture can be depended on <b>sampling</b> <b>ratio</b> f/f [...]...|$|E
30|$|As {{evident in}} the above literatures, several {{researchers}} have done extensive work {{on the performance of}} various discriminant and classification functions under skewed or non normal distributions. However, not much attention has been focused on studying and evaluating the performance of these classifiers using three populations under skewed distribution considering different <b>sampling</b> <b>ratios,</b> under different centroid separators and under varying variable selections. This study therefore seeks to investigate the performance of a single classifier (i.e the QDF) under skewed distribution considering different variable selections, varying <b>sampling</b> <b>ratios</b> and varying centroid separators considering three groups/populations.|$|R
30|$|In addition, we used data {{augmentation}} {{to adjust}} {{the positive and negative}} <b>sample</b> <b>ratios</b> and {{to increase the number of}} training samples. The optimization function momentum[*]=[*] 0.9, weight decay 0.0001.|$|R
5000|$|The {{following}} four contingency tables contain observed cell counts, {{along with the}} corresponding <b>sample</b> odds <b>ratio</b> (OR) and <b>sample</b> log odds <b>ratio</b> (LOR): ...|$|R
40|$|Motivation:Measurements are {{commonly}} taken from two phenotypes {{to build a}} classifier, where the number of data points from each class is predetermined, not random. In this ‘separate sampling ’ scenario, the data cannot be used to estimate the class prior probabilities. Moreover, predetermined class sizes can severely degrade classifier performance, even for large samples. Results: We employ simulations using both synthetic and real data to show the detrimental effect of separate sampling {{on a variety of}} classification rules. We establish propositions related to the effect on the expected classifier error owing to a <b>sampling</b> <b>ratio</b> different from the population class ratio. From these we derive a sample-based mini-max <b>sampling</b> <b>ratio</b> and provide an algorithm for approximating it from the data. We also extend to arbitrary distributions the classical popu-lation-based Anderson linear discriminant analysis minimax <b>sampling</b> <b>ratio</b> derived from the discriminant form of the Bayes classifier. Availability: All the codes for synthetic data and real data examples are written in MATLAB. A function called mmratio, whose output is an approximation of the minimax <b>sampling</b> <b>ratio</b> of a given dataset, is also written in MATLAB. All the codes are available at...|$|E
30|$|As {{shown in}} Fig. 7, a ScanSAR image and Stripmap beam F 2 - 5, F 2 - 7, and post June 1, 2015 F 2 - 6 images have {{frequency}} overlap. Here, {{we can make}} a Stripmap-ScanSAR interferogram (Ortiz and Zebker 2007). In this case, a ScanSAR image can be used as same as a Stripmap image. One problem for performing a Stripmap-ScanSAR interferometry from L 1.1 images is that they are observed with different PRF, range/azimuth <b>sampling</b> <b>ratio</b> (ratio of the ground distance per pixel), and coverage area in order to minimize the range and the azimuth ambiguity. That is, the PRF of the PALSAR- 2 image depends on the off-nadir angle of the beam and the range <b>sampling</b> <b>ratio</b> depends on the frequency bandwidth. The azimuth <b>sampling</b> <b>ratio</b> is fixed for each mode if we use the L 1.1 standard product.|$|E
30|$|Of the 339 {{questionnaires}} {{distributed to}} active auditors in Taiwan, 326 valid responses were returned, representing {{a very high}} response rate of 96.17  % and a <b>sampling</b> <b>ratio</b> of 48.95  %.|$|E
30|$|This paper {{investigated}} the asymptotic performance of QDF on skewed training data for three populations (π _i,i= 1, 2, 3) with increasing group centroid (δ), with chosen variables and <b>sample</b> size <b>ratios.</b> Results {{from the study}} indicates that, the QDF performed quite poorly {{with an increase in}} error rates under <b>sample</b> <b>ratios</b> 1 : 2 : 2 and 1 : 2 : 3 for δ = 1 –δ = 3. Other results also indicates that, the QDF performs better under an equal <b>sample</b> size <b>ratio</b> (1 : 1 : 1) resulting in a reduced misclassification rate with minimized error rates. The group centroid separators increased with decreasing group error rates and sample sizes. In other words, the QDF performed better in classifying the observations into their respective groups when the group centroid separators were increased. Also with increasing number of variables, from 4 to 8, the average error rate for evaluating the performance of the QDF dropped under δ = 3, 4 for <b>sample</b> <b>ratios</b> 1 : 2 : 2 and 1 : 2 : 3.|$|R
30|$|Choose (randomly or on {{safe level}} basis) an equal <b>sample</b> <b>ratio</b> from each over_sampling {{instance}} sets per iteration. Combine {{it with the}} base set of instances forming a new data set for the next over_sampling process.|$|R
40|$|A. Objective [...] 3 B. Calculation {{procedure}} for Incidence Rate [IR]: [...] 4 1. IDB <b>Sample</b> <b>ratio</b> calculation [RAT] : Extrapolation factor [...] 6 2. Estimation {{of the number}} of cases at national level [EST] [...] ...|$|R
40|$|Periodic {{nonuniform}} sampling {{has been}} considered in literature as an effective approach to reduce the sampling rate far below the Nyquist rate for sparse spectrum multiband signals. In the presence of non-ideality the sampling parameters {{play an important role}} on the quality of reconstructed signal. Also the average <b>sampling</b> <b>ratio</b> is directly dependent on the sampling parameters that they should be chosen for a minimum rate and complexity. In this paper we consider the effect of sampling parameters on the reconstruction error and the <b>sampling</b> <b>ratio</b> and suggest feasible approaches for achieving an optimal sampling and reconstruction...|$|E
30|$|The {{fundamental}} idea of our {{approach is to}} generate the transmitted signal in a very high <b>sampling</b> <b>ratio</b> and store the decimated version before raw data generation. Raw data is generated by shifting the appropriate decimated data and finally demodulated to baseband.|$|E
40|$|The {{advent of}} {{computers}} {{and their impact on}} the graphic arts and printing industry has, and will continue to, change the methodology of working and workflow in prepress operations. The conversion of analog materials (prints, artwork, transparencies, studio work) into a digital format requires the use of scanners or digital cameras, coupled with the knowledge of output requirements as related to client expectations. The chosen input <b>sampling</b> <b>ratio</b> (sampling rate in relation to halftone screening) impacts output quality, as well as many aspects of prepress workflow efficiency. The ability to predict printed results begins with the correct conversion of originals into digital information and then an appropriate conversion into the output materials for the intended press condition. This conversion of originals into digital information can be broken down into four general components. First, the image must be scanned {{to the size of the}} final output. Second, the input <b>sampling</b> <b>ratio</b> must be determined, in relation to the screening requirements of the job. This ratio should be appropriate to the needs of the printing condition for the final press sheet. Third, the highlight, highlight to midtone and shadow placement points must be determined in order to achieve the correct tone reproduction. Fourth, decisions must be made as to the image correction system to be employed in order to obtain consistent digital files from the scanner and prepress workflow. Factors relating to image correction and enhancement include such details as gray balance, color cast correction, dot gain, ink trapping, hue error, unsharp masking, all areas that impact quality. These are generally applied from within software packages that work with the scanner, or from within image manipulation software after the digital conversion is complete. The question of what is the necessary input <b>sampling</b> <b>ratio</b> for traditional AM screening has traditionally been based on the Nyquist Sampling Theorem. The basis for determining input <b>sampling</b> <b>ratio</b> requirements for frequency modulated (FM) screening is less clear. The Nyquist Theorem (originally from electrical engineering and communications research) has been applied to the graphic arts, leading to the general acceptance of a standard 2 : 1 ratio for most prepress scanning work. The ratio means that the sampling rate should be twice the screen frequency. This thesis set out to determine if there are dif ferences in input <b>sampling</b> <b>ratio</b> scanning requirements, based on the screen frequency rx selection (lOOlpi AM, 1751 pi AM and 21 |lFM used in this study), when generating films and/or plates for printing, that might question this interpretation of the Nyquist Sam pling Theorem as it relates to the graphic arts. Five images were tonally balanced over three different screening frequencies and six different sampling ratios. A reference image was generated for each condition using the Nyquist <b>Sampling</b> <b>ratio</b> of 2 : 1. Observers were then asked to rate the images in terms of quality against the standard. Statistical analysis was then applied to the data in order to observe interactions, similarities and differences. A pilot study was first run in order to determine the amount of unsharp masking to use on the images that would be manipulated in the main study. Seven images were pre sented from which four were selected for the final study. Thirty observers were asked for their preference on the amount of sharpening to use. It was found that for this condition (7 images) observers preferred the same amount of sharpening for the 1751 pi AM and 21 u FM screens, but slightly more sharpening for the lOOlpi AM screen. This information was then applied to the main study images. An additional image previously published was added after the pilot study, as it contained elements not found in the other images The unsharp masking applied to this image was the same as at the time of publication. The main study focused on the interaction of image type, screen frequency and varia tions of input scanner sampling ratios as it relates to output. The results indicated that image type, <b>sampling</b> <b>ratio,</b> <b>sampling</b> <b>ratio</b> - frequency interaction were factors, but fre quency alone was not. However, viewing the interaction chart of frequency and <b>sampling</b> <b>ratio</b> for the 1751 pi AM and 21 u FM screens alone, an insignificant difference was indi cated (at a 95 % confidence level). The conclusion can therefore be drawn that at the higher screen frequencies tested in this study, viewer observations showed that the input sampling ratios should be the same for 1751 pi and 21) 1 FM screens. Continuous tone orginals should be scanned at a sam pling ratio of 1. 75 : 1. This answered the question of whether FM screening technology can withstand a reduced input <b>sampling</b> <b>ratio</b> and maintain quality, which this study finds cannot. At the lower screen ruling of lOOlpi the input scanner <b>sampling</b> <b>ratio</b> requirement, based on viewer preferences of the five images presented, can be reduced to a 1. 5 : 1...|$|E
40|$|AbstractA {{method is}} {{presented}} {{that uses the}} well-understood O 2 /Ar ion-molecule reaction system to determine the effective ion source residence time of a chemical ionization source. The process consists of: (1) defining the kinetic system in terms of reactions, reaction rates, and ionization cross sections; (2) solving the differential equations that describe the time evolution of the kinetic system, and (3) comparing the calculated results to experimentally measured relative ion intensities. These steps are repeated {{for a variety of}} O 2 /Ar <b>sample</b> <b>ratios</b> and inlet pressures. The method leads to a simple relationship between inlet pressure and effective ion source residence time, independent of the O 2 /Ar <b>sample</b> <b>ratio...</b>|$|R
30|$|This study {{investigates the}} {{asymptotic}} {{performance of the}} quadratic discriminant function (QDF) under skewed training samples. The main objective {{of this study is}} to evaluate the performance of the QDF under skewed distribution considering different <b>sample</b> size <b>ratios,</b> varying the group centroid separators and the number of variables. Three populations (π _i, i= 1, 2, 3) with increasing group centroid separator function were considered. A multivariate normal distributed data was simulated with MatLab R 2009 a. There was an increase in the average error rates of the <b>sample</b> size <b>ratios</b> 1 : 2 : 2 and 1 : 2 : 3 as the total sample size increased asymptotically in the skewed distribution when the centroid separator increased from 1 to 3. The QDF under the skewed distribution performed better for the <b>sample</b> size <b>ratio</b> 1 : 1 : 1 as compared to the other <b>sampling</b> <b>ratios</b> and under centroid separator (δ = 5).|$|R
40|$|In {{this project}} we propose {{a way of}} {{computing}} the needed sample size, for a binary endpoint and for a binary composite endpoint, needed to detect {{the effect of a}} certain treatment considering a type I error of alpha and a statistical power of 1 -beta in Randomized Clinical Trial. We propose the <b>Sample</b> <b>Ratio,</b> which is an efficiency measure to decide upon the use of a binary composite endpoint instead of a relevant endpoint. The impact of the relative overlaps on the required sample size, for the relevant endpoint and for the composite, is explored in dierent simulated scenarios. Similarly for the <b>Sample</b> <b>Ratio.</b> In addition, we compare our proposed method with the currently available Asymptotic Relative Eciency, which is specically used in the context of time-to-endpoint analyses...|$|R
40|$|It is {{well known}} that {{two-phase}} (or double) sampling is of significant use in practice when the population parameter(s) (say, population mean X-super-�) of the auxiliary variate x is not known. Keeping this in view, we have suggested a class of ratio-product estimators in two-phase sampling with its properties. The asymptotically optimum estimators (AOEs) in the class are identified in two different cases with their variances. Conditions for the proposed estimator to be more efficient than the two-phase <b>sampling</b> <b>ratio,</b> product and mean per unit estimator are investigated. Comparison with single phase sampling is also discussed. An empirical study is carried out to demonstrate the efficiency of the suggested estimator over conventional estimators. Auxiliary variate, double <b>sampling</b> <b>ratio</b> and product estimators, finite population mean, study variate,...|$|E
40|$|The {{performance}} of m-out-of-n bagging {{with and without}} replacement {{in terms of the}} <b>sampling</b> <b>ratio</b> (m/n) is analyzed. Standard bagging uses resampling with replacement to generate bootstrap samples of equal size as the original training set mwor = n. Without-replacement methods typically use half samples mwr = n/ 2. These choices of sampling sizes are arbitrary and need not be optimal in terms of the classification {{performance of}} the ensemble. We propose to use the out-of-bag estimates of the generalization accuracy to select a near-optimal value for the <b>sampling</b> <b>ratio.</b> Ensembles of classifiers trained on independent samples whose size is such that the out-of-bag error of the ensemble is as low as possible generally improve the performance of standard bagging and can be efficiently built...|$|E
40|$|Methods and {{apparatus}} {{are provided}} for automatic and/or manual adaptive bandwidth compression of telemetry. An adaptive sampler samples a video signal from a scanning sensor and generates {{a sequence of}} sampled fields. Each field and range rate information from the sensor are hence sequentially transmitted to and stored in a multiple and adaptive field storage means. The field storage means then, {{in response to an}} automatic or manual control signal, transfers the stored sampled field signals to a video monitor in a form for sequential or simultaneous display of a desired number of stored signal fields. The <b>sampling</b> <b>ratio</b> of the adaptive sample, the relative proportion of available communication bandwidth allocated respectively to transmitted data and video information, and the number of fields simultaneously displayed are manually or automatically selectively adjustable in functional relationship to each other and detected range rate. In one embodiment, when relatively little or no scene motion is detected, the control signal maximizes <b>sampling</b> <b>ratio</b> and causes simultaneous display of all stored fields, thus maximizing resolution and bandwidth available for data transmission. When increased scene motion is detected, the control signal is adjusted accordingly to cause display of fewer fields. If greater resolution is desired, the control signal is adjusted to increase the <b>sampling</b> <b>ratio...</b>|$|E
40|$|Abstract—Response surface {{methodology}} {{was used}} for quantitative investigation of water and solids transfer during osmotic dehydration of beetroot in aqueous solution of salt. Effects of temperature (25 – 45 o C), processing time (30 – 150 min), salt concentration (5 – 25 %, w/w) and solution to <b>sample</b> <b>ratio</b> (5 : 1 – 25 : 1) on osmotic dehydration of beetroot were estimated. Quadratic regression equations describing {{the effects of these}} factors on the water loss and solids gain were developed. It was found that effects of temperature and salt concentrations were more significant on the water loss than the effects of processing time and solution to <b>sample</b> <b>ratio.</b> As for solids gain processing time and salt concentration were the most significant factors. The osmotic dehydration process was optimized for water loss, solute gain, and weight reduction. The optimum conditions were found to be: temperature – 35 oC, processing time – 90 min, salt concentration – 14. 31 % and solution to <b>sample</b> <b>ratio</b> 8. 5 : 1. At these optimum values, water loss, solid gain and weight reduction were found to be 30. 86 (g/ 100 g initial sample), 9. 43 (g/ 100 g initial sample) and 21. 43 (g/ 100 g initial sample) respectively. Keywords—Optimization, Osmotic dehydration, Beetroot, salt solution, response surface methodology I...|$|R
40|$|Pigeons' {{short-term}} memory for fixed-ratio requirements {{was assessed using}} a delayed symbolic matching-to-sample procedure. Different choices were reinforced after fixed-ratio 10 and fixed-ratio 40 requirements, and delays of 0, 5, or 20 s were sometimes placed between <b>sample</b> <b>ratios</b> and choice. All birds made disproportionate numbers of responses to the small-ratio choice alternative when delays were interposed between ratios and choice, and this bias increased {{as a function of}} delay. Preference for the small fixed-ratio alternative was also observed on "no-sample" trials, during which the choice alternatives were presented without a prior <b>sample</b> <b>ratio.</b> This "choose-small" bias is analogous to results obtained by Spetch and Wilkie (1983) with event duration as the discriminative stimulus. The choose-small bias was attenuated when the houselight was turned on during delays, but overall accuracy was not influenced systematically by the houselight manipulation...|$|R
40|$|SUMMARY. This paper {{discusses}} sufficient {{conditions for}} standard Taylor-expansion based approximations of the bias and {{mean squared error}} of a <b>sample</b> <b>ratio</b> or <b>sample</b> regression coefficient in finite population random sampling. No superpopulation model is used. For a <b>sample</b> <b>ratio,</b> the sufficient conditions permit some population units to have denominator terms Xi equal to zero; and require the other population units to have Xi values above bounds that are decreasing in the sample size n. For the regression case, the conditions involve trade-offs between sample size and spacing {{of the population of}} Xi values. For both the ratio and regression cases, the proposed conditinos give relatively simple forms to the intuitive idea that standard moment approximations are satisfactory except in certain extreme cases, where the definition of “extreme case ” depends on the sample size. 1...|$|R
