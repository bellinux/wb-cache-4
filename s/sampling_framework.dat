317|390|Public
50|$|Sampling {{from the}} multivariate {{truncated}} normal distribution is considerably more difficult. Exact or perfect simulation is only feasible {{in the case}} of truncation of the normal distribution to a polytope region. In more general cases, Damien and Walker (2001) introduce a general methodology for sampling truncated densities within a Gibbs <b>sampling</b> <b>framework.</b> Their algorithm introduces one latent variable and, within a Gibbs <b>sampling</b> <b>framework,</b> it is more computationally efficient than the algorithm of Robert (1995).|$|E
50|$|The study’s {{use of the}} {{expanded}} Indian Sample Registration System for data harvest allows for a large <b>sampling</b> <b>framework,</b> but despite the 7 597 sampling units and reported 2.4 million households the framework still only covers a fraction of India’s more than 1 billion people and more than 9.5 million annual deaths.|$|E
50|$|The MDS used India’s {{existing}} Sample Registration System (SRS), {{a government}} program operated by the Registrar General of India since 1971, as a <b>sampling</b> <b>framework</b> for data harvest by trained surveyors. The collaborators of the MDS successfully arranged to expand the overall size of the SRS at {{the inception of the}} study in 1998.|$|E
50|$|A <b>sample</b> <b>framework</b> {{for general}} {{analysis}} is the SWOT analysis. Another is the Balanced Scorecard for performance measurement analysis.|$|R
30|$|African {{migration}} {{research is}} haunted {{by the lack of}} reliable official data and the absence of appropriate <b>sampling</b> <b>frameworks</b> in the form of census or survey data. Although these problems are far from resolved, recently, the availability of new migration databases has significantly expanded the scope to conduct analyses on migration from, to and within Africa 2.|$|R
50|$|DXUT (also {{called the}} <b>sample</b> <b>{{framework}})</b> is a layer built {{on top of}} the Direct3D API. The framework is designed to help the programmer spend less time with mundane tasks, such as creating a window, creating a device, processing Windows messages and handling device events. DXUT have been removed with the Windows SDK 8.0 and now distributed as source via CodePlex.|$|R
50|$|The {{main focus}} of the MDS is adult {{mortality}} and the <b>sampling</b> <b>framework</b> captures {{a limited number of}} maternal deaths. This indicates the study may not have sufficient statistical power to thoroughly analyze maternal mortality and its connection with newborn mortality. A wider selection of maternal and newborn health indicators on the surveyor questionnaires during data harvest would have allowed more detailed study of these sources of premature mortality.|$|E
40|$|In this paper, {{we study}} the {{generation}} of maximal Poisson-disk sets with varying radii on surfaces. Based on the concepts of power diagram and regular triangulation, we present a geometric analysis of gaps in such disk sets on surfaces, which is the key ingredient of the adaptive maximal Poisson-disk <b>sampling</b> <b>framework.</b> Moreover, we adapt the presented <b>sampling</b> <b>framework</b> for remeshing applications. Several novel and efficient operators are developed for improving the sampling/meshing quality over the state-of-theart. © 2012 ACM...|$|E
30|$|The <b>sampling</b> <b>framework</b> for {{the survey}} was the 2006 Census of Ireland, and the Enumeration Areas (EA) were {{randomly}} selected according to probability proportional to size sampling, where size {{is defined as the}} total number of non-Irish and non-British individuals.|$|E
40|$|Simulation {{methods for}} design flood {{analyses}} require estimates of extreme precipitation for simulating maximum discharges. This article evaluates the multi-exponential weather pattern (MEWP) model, a compound model based on weather pattern classification, seasonal splitting and exponential distributions, for its {{suitability for use}} in Norway. The MEWP model is the probabilistic rainfall model used in the SCHADEX method for extreme flood estimation. Regional scores of evaluation are used in a split <b>sample</b> <b>framework</b> to compare the MEWP distribution with more general heavy-tailed distributions, {{in this case the}} Multi Generalized Pareto Weather Pattern (MGPWP) distribution. The analysis shows the clear benefit obtained from seasonal and weather pattern-based subsampling for extreme value estimation. The MEWP distribution is found to have an overall better performance as compared with the MGPWP, which tends to overfit the data and lacks robustness. Finally, we take advantage of the split <b>sample</b> <b>framework</b> to present evidence for an increase in extreme rainfall in the southwestern part of Norway during the period 1979 – 2009, relative to 1948 – 1978...|$|R
40|$|Abstract- This paper {{introduces}} how {{to extend}} Google’s Android Platform {{as a social}} matchmaking tool by using the latest embedded software features, sensors and integration with social (Application Programming Interface) API’s such as Facebook and Twitter. It starts with {{the importance of social}} networks in the context of matchmaking and then proposes a <b>sample</b> <b>framework</b> by using Android’s (Software Development Kit) SDK and framework to build apps that intelligently match personalities and suggest potential matches...|$|R
40|$|This article {{outlines}} and {{summarizes the}} Diehl and Druckman evaluation framework {{that is used}} in the case studies that follow. An overview of the decision template is given and the three sets of goals (core, beyond traditional peacekeeping, and peacebuilding) are introduced. Two <b>sample</b> <b>framework</b> entries (violence abatement, and restoration, reconciliation and transformation respectively) are provided as illustrations. Application to peace operations in Bosnia is also used for illustrative purposes. 17 page(s...|$|R
40|$|We {{present a}} new {{estimator}} for counting {{the number of}} solutions of a Boolean satisfiability problem as a part of an importance <b>sampling</b> <b>framework.</b> The estimator uses the recently introduced SampleSearch scheme that is designed to overcome the rejection problem associated with distributions having a substantial amount of determinism. We show here that the sampling distribution of SampleSearch can be characterized as the backtrack-free distribution and propose several schemes for its computation. This allows integrating Sample-Search into the importance <b>sampling</b> <b>framework</b> for approximating the number of solutions and also allows using Sample-Search for computing a lower bound measure on the number of solutions. Our empirical evaluation demonstrates the superiority of our new approximate counting schemes against recent competing approaches...|$|E
40|$|We {{present a}} sampling-based Bayesian {{approach}} for modeling and forecasting a common factor time series {{model in which}} the common components are long-range dependent. The Gibbs <b>sampling</b> <b>framework</b> allows us to use a less computationally demanding ARMA model to approximate the common long-range dependent behavior in the sampling algorithm; we then adjust for the approximation using importance sampling. The <b>sampling</b> <b>framework</b> also allows for easy incorporation of other features, such as possible outlying observations and non-Gaussian disturbances. We illustrate the estimation procedure using simulated data and real inflation data for two neighboring European countries. We then discuss {{an extension of the}} method for estimation of a non-Gaussian multivariate stochastic volatility (MVSV) model with common long-range dependent components in the Bayesian framework. The common component MVSV model is applied to stock return volatility data for companies having similar annual sales. Keywords: Co [...] ...|$|E
40|$|Purpose: Validity is a contextual {{aspect of}} a scale which may differ across sample populations and study protocols. The {{objective}} of our study was to validate the Care-Related Quality of Life Instrument (CarerQol) across two different study design features, <b>sampling</b> <b>framework</b> (general population vs. different care settings) and survey mode (interview vs. written questionnaire) ...|$|E
40|$|Semi-supervised word {{alignment}} aims {{to improve}} the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query <b>sampling</b> <b>frameworks</b> like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner. ...|$|R
2500|$|In 2010, Google {{released}} {{a tool for}} validating authorized purchases for use within apps, but developers complained that this was insufficient and trivial to crack. Google responded that the tool, especially its initial release, {{was intended as a}} <b>sample</b> <b>framework</b> for developers to modify and build upon depending on their needs, not as a finished piracy solution. Android [...] "Jelly Bean" [...] introduced the ability for paid applications to be encrypted, so that they may work only on the device for which they were purchased.|$|R
40|$|The {{last ten}} years have {{witnessed}} the development of <b>sampling</b> <b>frameworks</b> that permit the construction of Markov chains which simultaneously traverse both parameter and model space. In this time substantial methodological progress has been made. In this article we present {{a survey of the}} current state of the art and evaluate some of the most recent advances in this field. We also discuss future research perspectives in the context of the drive to develop sampling mechanisms with high degrees of both efficiency and automation. ...|$|R
40|$|We {{study the}} problem of {{multi-stage}} stochastic optimization with recourse, and provide approximation algorithms using cost-sharing functions for such problems. Our algorithms use and extend the Boosted <b>Sampling</b> <b>framework</b> of [6]. We also show how the framework {{can be adapted to}} give approximation algorithms even when the inflation parameters are correlated with the scenario...|$|E
40|$|Abstract. Active {{learning}} {{has been successfully}} applied to many natural language processing tasks for obtaining annotated data in a cost-effective manner. We propose several extensions to an active learner that adopts the margin-based uncertainty <b>sampling</b> <b>framework.</b> Experimental results on a cause detection problem involving the classification of aviation safety reports demonstrate the effectiveness of our extensions. 1...|$|E
30|$|Different from {{standard}} sampling strategy in compressive sensing (CS), {{we present a}} compressive partial <b>sampling</b> <b>framework</b> called adaptive-random sampling and recovery (ASR) for image. It could faithfully recover images by hybridizing random samples with edge-extracted pixels with much lower sampling rate. The new framework preserves edge pixels containing essential information of images, and meanwhile employs the edge-preserving total variation (TV) regularizer. Assisted with the edges, three steps are adopted to recover the high-quality image. First, we extract the edges of a coarse image recovered with completely random measurements in our <b>sampling</b> <b>framework.</b> Then, the TV algorithm in the CS theory is employed for solving the Lagrangian regularization problem. Finally, we refine the coarse image to obtain a high-quality one with both the extracted edges and previous random measurements. Experimental {{results show that the}} novel ASR strategy achieves significant performance improvements over the current state-of-the-art schemes.|$|E
5000|$|In 2010, Google {{released}} {{a tool for}} validating authorized purchases for use within apps, but developers complained that this was insufficient and trivial to crack. Google responded that the tool, especially its initial release, {{was intended as a}} <b>sample</b> <b>framework</b> for developers to modify and build upon depending on their needs, not as a finished piracy solution. Android [...] "Jelly Bean" [...] introduced the ability for paid applications to be encrypted, so that they may work only on the device for which they were purchased.|$|R
40|$|Generalized <b>sampling</b> is new <b>framework</b> for <b>sampling</b> and {{reconstruction}} in infinite-dimensional Hilbert spaces. Given measurements (inner products) of an element {{with respect to}} one basis, it allows one to reconstruct in another, arbitrary basis, {{in a way that}} is both convergent and numerically stable. However, generalized sampling is thus far only valid for sampling {{and reconstruction}} in systems that comprise bases. Thus, in the first part of this paper we extend this framework from bases to frames, and provide fundamental sampling theorems for this more general case. The second part of the paper is concerned with extending the idea of generalized sampling to the solution of inverse and ill-posed problems. In particular, we introduce two generalized <b>sampling</b> <b>frameworks</b> for such problems, based on regularized and non-regularized approaches. We furnish evidence of the usefulness of the proposed theories by providing a number of numerical experiments. ...|$|R
40|$|In stated {{preference}} literature, {{the tendency}} to choose the alternative representing the status quo situation seems to exceed real life status quo effects. Accordingly, status quo bias can be a problem. In Choice Experiments, status quo bias {{is found to be}} strongly correlated with protest attitudes toward the cost attribute. If economic values are to be elicited, this problem is difficult to remedy. In a split <b>sample</b> <b>framework</b> we test a novel ex-ante entreaty aimed specifically at the cost attribute and find that it effectively reduces status quo bias and improves the internal validity of the hypothetical preferences. Choice Experiment, Status Quo Bias, Entreaty, Stated Preference,...|$|R
40|$|Decisions are {{accompanied}} by a degree of confidence that a selected option is correct. A sequential <b>sampling</b> <b>framework</b> explains the speed and accuracy of decisions and extends naturally to the confidence that the decision rendered {{is likely to be}} correct. However, discrepancies between confidence and accuracy suggest that confidence might be supported by mechanisms dissociated from the decision process. Here we show that this discrepancy can arise naturally because of simple processing delays. When participants were asked to report choice and confidence simultaneously, their confidence, reaction time and a perceptual decision about motion were explained by bounded evidence accumulation. However, we also observed revisions of the initial choice and/or confidence. These changes of mind were explained by a continuation of the mechanism that led to the initial choice. Our findings extend the sequential <b>sampling</b> <b>framework</b> to vacillation about confidence and invites caution in interpreting dissociations between confidence and accuracy...|$|E
40|$|The {{abundance}} of real-world data and limited labeling budget calls for active learning, {{which is an}} important learning paradigm for reducing human labeling efforts. Many recently developed active learning algorithms consider both uncertainty and represen-tativeness when making querying decisions. However, exploiting representativeness with uncertainty concurrently usually requires tackling sophisticated and challenging learning tasks, such as clustering. In this paper, we propose a new active learning framework, called hinted sampling, which takes both uncertainty and representative-ness into account in a simpler way. We design a novel active learning algorithm within the hinted <b>sampling</b> <b>framework</b> with an extended support vector machine. Experimental results validate that the novel active learning algorithm {{can result in a}} better and more stable performance than that achieved by state-of-the-art algorithms. We also show that the hinted <b>sampling</b> <b>framework</b> allows improving another active learning algorithm designed from the transductive support vector machine. ...|$|E
40|$|We {{consider}} {{the problem of}} selecting an optimal system from among a finite set of competing systems, based on a “stochastic ” objective function and subject to a single “stochastic ” constraint. By strategically dividing the competing systems, we derive a large deviations <b>sampling</b> <b>framework</b> that asymptotically minimizes the probability of false selection. We provide an illustrative example where a closed-form sampling law is obtained after relaxation. ...|$|E
40|$|We discuss an {{engineering}} case-study {{in which we}} address the issues of integration and secure consumption of a specialist software package for finite element meshing through a web service based interface. This work focuses on demonstrating the end-to-end requirements of a framework allowing the transparent integration of the web service into the design workflow of {{an engineering}} user. After detailing an example engineering problem, we discuss server-side issues and required modules such as local accounting, security and resource management which provide a <b>sample</b> <b>framework</b> for a chargeable web service. We then demonstrate how the client can easily interface to and consume the web service from within the engineering scripting language Matlab...|$|R
40|$|There {{are several}} {{innovation}} methodologies {{reported in the}} literature starting from simple concepts such as technology push and market pull all the way to Disruptive innovation. Almost all these methods do not provide for customizability and extensibility. The method described in this paper is called Quick and Dirty Innovation Method or QaDIM in short to represent the fact that the method can be used rather easily to identify incremental innovation opportunities. The paper will first describe the basic concept, and then proceed to give a <b>sample</b> <b>framework</b> before proceeding to describe the extensibility. The method allows firms and individuals to define new operators as well as select operators from an existing operator base thereby offering both extensibility and customizability...|$|R
40|$|In this paper, minimal {{conditions}} under which a semi-parametric binary response model is identified in a Bayesian framework are presented and compared to the conditions usually required in a <b>sampling</b> theory <b>framework.</b> Binary response models, Non parametric Bayesian Statistics, Dirichlet processes, Identiﬁcation...|$|R
40|$|The usual Gibbs <b>sampling</b> <b>framework</b> of the Bayesian mixture {{model is}} {{extended}} {{to account for}} binned data. This model involves {{the addition of a}} latent variable in the model which represents simulated values from the believed true distribution at each iteration of the algorithm. The technique results in better model fit and recognition of the more subtle aspects of the density of the data. ...|$|E
40|$|We {{introduce}} a reweighting scheme for the path ensembles {{in the transition}} interface <b>sampling</b> <b>framework.</b> The reweighting allows {{for the analysis of}} free energy landscapes and committor projections in any collective variable space. We illustrate the reweighting scheme on a two dimensional potential with a nonlinear reaction coordinate and on a more realistic simulation of the Trp-cage folding process. We suggest that the reweighted path ensemble can be used to optimize possible nonlinear reaction coordinates...|$|E
40|$|This dataset {{contains}} {{existing and}} potential areas of habitats associated with calcareous, coastal, upland and lowland heath landscapes. The dataset was initially created {{to provide a}} <b>sampling</b> <b>framework</b> for a field survey carried out in 1992 and 1993 by the Institute of Terrestrial Ecology (later part of the Centre for Ecology & Hydrology). It was derived {{from a range of}} geology, soils, altitude and land cover data (as described fully in the supporting information) ...|$|E
40|$|Abstract: There {{are several}} {{innovation}} methodologies {{reported in the}} literature starting from simple concepts such as technology push and market pull all the way to Disruptive innovation. Almost all these methods do not provide for customizability and extensibility. The method described in this paper is called Quick and Dirty Innovation Method or QaDIM in short to represent the fact that the method can be used rather easily to identify incremental innovation opportunities. The paper will first describe the basic concept, and then proceed to give a <b>sample</b> <b>framework</b> before proceeding to describe the extensibility. The method allows firms and individuals to define new operators as well as select operators from an existing operator base thereby offering both extensibility and customizability...|$|R
40|$|The rapid {{development}} of biological technology {{in recent years}} has generated an overwhelming quantity of highly complex genetical data. In retrospect it is perhaps now obvious that standard stochastic sampling algorithms were destined to fail when applied to the type of models that these data require. During this period, both the genetical and statistical research communities have developed a range of new algorithms that have endeavoured to avoid these failures. In this article we present a brief journey through the development of some of these algorithms. We hope to demonstrate that continued collaborative research and education in both genetics and statistics disciplines is crucial for the advancement of future <b>sampling</b> <b>frameworks,</b> and call on researchers in both fields to take up this challenge...|$|R
40|$|When {{reasoning}} {{in the presence}} of uncertainty there is a unique and self consistent set of rules for induction and model selection Bayesian inference Recent advances in neural networks have been fuelled by the adoption of this Bayesian framework either implicitly for example through the use of commit tees or explicitly through Bayesian evidence and <b>sampling</b> <b>frameworks</b> In this chapter we show how this second generation of neural network techniques can be applied to biomedical data and focus on the networks ability to provide assessments of the condence associated with its predictions This is an essen tial requirement for any automatic biomedical pattern recognition system It allows low condence decisions to be highlighted and deferred possibly to a human expert and falls naturally out of the Bayesian framewor...|$|R
