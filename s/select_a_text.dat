6|10000|Public
5000|$|A notable {{feature of}} the HyperTalk {{container}} model was its handling of text. Every collection of text, whether a literal string in a program or text typed into a text field, was itself considered a container with multiple collections of containers within it. This allowed scripts to parse text using the same navigational commands as any other container. For instance, while parsing a space-delimited data file, one might want to extract the third column, like this: put the third word of theFilesText into colThreeThis syntax allowed the script to [...] "walk" [...] down the text to find particular data, as in this example: put the first character of the third word of line 5 of card field [...] "sometext" [...] into theCharThis process of treating text as a container was known as [...] "chunking", and the functions as [...] "chunk expressions". These same sorts of expressions were used to handle file manipulation, along {{with a set of}} file management functions. The following code opens a known file, reads from it, extracts data, and then closes the file: on mouseDown answer file [...] "Please <b>select</b> <b>a</b> <b>text</b> file to open." [...] if it is empty then exit mouseDown put it into filePath if there is a file filePath then open file filePath read from file filePath until return put it into cd fld [...] "some field" [...] close file filePath set the textStyle of character 1 to 10 of card field [...] "some field" [...] to bold end if end mouseDownHyperTalk also included functions for chunking strings using a substring find operation using the [...] operator. The following code finds all examples of a given pattern using the [...] as part of the [...] loop, while [...] finds the location of that pattern within the string: function replaceStr pattern,newStr,inStr repeat while pattern is in inStr put offset(pattern,inStr) into pos put newStr into character pos to (pos +the length of pattern)-1 of inStr end repeat return inStr end replaceStr ...|$|E
40|$|This {{conceptual}} {{paper is}} an attempt to present and highlight the features required for the selected text as an instrument in translation studies which researchers should consider. This is a step towards making the translation studies more scientific. To fulfil the paper`s purpose, the author discussed some researches findings and based on the some findings, such as Conde`s (2009) ones, tried to present a standard framework to researchers to <b>select</b> <b>a</b> <b>text</b> as the instrument for translation quality assessment in quantitative translation studies. Finally the author brought some features up briefly named the length of the text and the readability information of the text resulted from reasons discussed in details in this paper...|$|E
40|$|Object: This paper {{presents}} a 3 D framework for Anatomy teaching. We are mainly {{concerned with the}} proper understanding of human anatomical 3 D structures. Materials and methods: The main idea of our approach is taking an electronic book such as Henry Gray’s Anatomy of the human body, {{and a set of}} 3 D models properly labeled, and constructing the correct linking that allows users to perform mutual searches between both media. Results: We implemented a system where learners can interactively explore textual descriptions and 3 D visualizations. Conclusion: Our approach allows easily performing two search tasks: first, the user may <b>select</b> <b>a</b> <b>text</b> region and get a view showing the objects that contain the selected structures, and second, using the interactive exploration of a 3 D model the user may automatically search for the textual description of the structures visible in the current view. Peer ReviewedPostprint (published version...|$|E
5000|$|Breaking into Print guides translators {{through the}} process of <b>selecting</b> <b>a</b> <b>text</b> and <b>an</b> {{appropriate}} publication venue and discusses obstacles particular to publishing literary translations.|$|R
30|$|Category is a {{supplementary}} axis {{that enables}} personal {{data to be}} <b>selected.</b> <b>A</b> <b>text</b> tag is one item of information in a category. It is also useful for filtering large amounts of data selected with the above viewpoint.|$|R
6000|$|Taking up the treasured {{book with}} great care, {{the young man}} before {{mentioned}} {{by the name of}} Totosy opened it and <b>selected</b> <b>a</b> <b>text.</b> [...] "Fear not, little flock, it is your Father's good pleasure to give you the kingdom." ...|$|R
40|$|Speech {{and other}} recognition-based {{technologies}} offer {{the promise of}} a simple, natural, and readilyavailable way of interacting with the devices and systems around us. But recognition is a difficult task and errors are bound to occur. My research is about how to design user interfaces that are efficient, enjoyable, and accessible despite reliance on imperfect recognition. Let me expand on these goals with respect to one particular application that I have worked on extensively, the entry of text using speech recognition. Efficient – People can speak at 200 words per minute (wpm), but corrected text entry rates using speech recognition are much slower (typically 5 - 25 wpm). Software like Dragon NaturallySpeaking has used the same basic correction process for years. Using Dragon, the user must first notice an error has occurred, then <b>select</b> <b>a</b> <b>text</b> region containing the error, and finally choose an alternate from a list of choices {{that may or may not}} contain the correct text. I believe this process can be significantly streamlined by cleverly leveraging the probabilistic information available from both the recognizer and the user. Enjoyable – Users of speech recognition software are often dissatisfied and frequently give up using speech recognition. One cause for this is that the correction process can become very frustrating if, while trying to correct a recognition error, another recognition error occurs that requires additional effort to correct. Interfaces need fallback methods that provide a clear and consistent route to correction success...|$|E
40|$|Mayer, Warner, Siedel and Lieberman's Business Law and the Legal Environment is an {{up-to-date}} textbook with {{comprehensive coverage}} {{of legal and}} regulatory issues — and organized to permit instructors to tailor the materials to their particular approach. The authors take special care to engage students by relating law to everyday events {{with which they are}} already familiar with their clear, concise and readable style. Business Law and the Legal Environment provides students with context and essential concepts across the entire range of legal issues with which managers and business executives must grapple. The texts provide the vocabulary and legal acumen necessary for business people to talk in an educated way to their customers, employees, suppliers, government officials — and to their own lawyers. Business Law and the Legal Environment was also written to solve a problem that many professors have with traditional business law textbooks. The text selection process for business law can be confusing because of the huge array of publications, and as you have most likely experienced, once you <b>select</b> <b>a</b> <b>text</b> you still might have to customize the text to meet your needs. For example, You might prefer to combine case summaries with a few excerpted cases so that students can experience reading original material, but many traditional books do not include both case summaries and excerpted cases. In the case of Business Law and the Legal Environment, the authors have created a text that not only has both case summaries and excerpted cases, but one that you can easily customize by deleting chapters, reordering the content, adding your own material, and even editing at the line level with Flat World's easy-to-use MIYO (Make It Your Own) Platform. Finally, the free online version of Business Law and the Legal Environment boasts embedded links to law-related videos at YouTube and other online sites for easy access by students and instructors...|$|E
40|$|This {{presentation}} {{reports on}} {{work in progress}} under {{the framework of a}} research project investigating word order in Second Language Acquisition (WOSLAC), based on two written learner corpora: WriCLE (L 1 Spanish - L 2 English) and CEDEL 2 (L 1 English - L 2 Spanish). In {{the first part of the}} presentation I will discuss (i) the motivation and objectives of the project, (ii) data collection, (iii) query software and (iv) data analysis. In the second part, I will briefly present the results of a preliminary study on the production of postverbal subjects by Spanish learners of English. The purpose of this three-year project is to determine the properties which constrain word order in the interlanguage of L 2 learners of English (with L 1 Spanish) and L 2 learners of Spanish (with L 1 English). We examine both lexicon-syntax and syntax-discourse properties. Word order in English and Spanish differs significantly: in English word order is often said to be â��fixedâ��, while Spanish allows for what is often referred to as â��free orderâ��. The two languages differ in the devices they employ to order constituents in the sentence In languages with free word order, information structure properties and discourse properties in general play a crucial role in the position occupied by constituents in sentences, while lexico-syntactic properties mostly determine the ordering of constituents in fixed word order languages. An in-depth investigation into word order in advanced learners of L 2 English and L 2 Spanish will thus offer answers to questions regarding the relative difficulty of acquiring lexical-syntactic and syntactic-discursive properties, as well as general issues related to L 1 transfer and the occurrence of constructions which cannot be attributed to the L 1 nor to the target language. Learner corpora are an invaluable tool to explore these issues. Our target is for WriCLE and CEDEL 2 to reach 1 million words by the end of the three year period. The corpora will be annotated using UAM CorpusTool, which has been adapted for this study. The tool allows an analyst to <b>select</b> <b>a</b> <b>text</b> from the corpus, and annotate it in various ways. The analyst can highlight a segment (e. g., an it-cleft) and then assign features to that segment. The tool produces an XML-encoded version of the text file, including the features assigned to the segments. Because hand-annotation is slow, the tool will allow the analyst to associate lexico-syntactic patterns with each feature, allowing the tool to automatically detect instances of the pattern. For instance, a pattern like: â��it be# NP thatâ�� would match sentences in the corpus like â��It was John that we sawâ��, and tentatively mark them with the feature it-cleft. The tool would then ask the user to eliminate false matches. This approach eliminates much of the corpus annotation effort. In the second part of the talk I will present briefly the results in Lozano & Mendikoetxea (in press) - a preliminary study whose purpose is to characterise the production of postverbal subjects in the Italian and Spanish subcorpora of ICLE (Granger et al. 2002). Our approach seeks to identify the conditions under which learners produce inverted subjects, regardless of problems to do with grammaticalition. Our findings reveal that Spanish and Italian learners of L 2 English produce postverbal subjects in the same contexts in which these are found in native English, though they show persistent grammaticalisation errors. That is, postverbal subjects are found when (H 1) the verb is unaccusative, (H 2) the subject is long or â��heavyâ��, and (H 3) the subject is new (or relatively new) information or â��focusâ��...|$|E
50|$|In {{an attempt}} to avoid opening the {{aforementioned}} keyboard when possible, Steel has a virtual keyboard which appears when <b>a</b> user <b>selects</b> <b>a</b> <b>text</b> box or the URL entry box in the toolbar. It is modeled after that of the iPhone, and as of version 0.0.4 causes the device to vibrate when a key is successfully pressed.|$|R
40|$|The {{provision}} of key text reading lists relies on students to select one they will utilise in undertaking a course. In {{anatomy and physiology}} <b>an</b> array of <b>texts</b> exist providing lecturers {{with the task of}} deciding the most suitable for inclusion within this list, the final choice for a student to undertake. Little evidence was found to identify the decision-making a student undertook in <b>selecting</b> <b>a</b> <b>text.</b> Based on disparate theoretical concepts an initial development of a conceptual process framework followed to provide a basis from which to identify influences which impacted on the student decision-making process. Using a mixed methods design a survey of students (N= 964) undertaking anatomy and physiology courses was conducted whose results, following analysis provided the focus for in depth interviews. These included students (n= 15), lecturers (n= 3) authors (n= 5) and publishers (n= 2). Thematic analysis of the transcripts identified four overarching themes these being the Perception of the Textbook, Choice of the Textbook, Mismatch of Perceived Needs and Place of the Textbook. The results suggested two main influences which impacted on the student when choosing <b>a</b> <b>text,</b> those of existing prior knowledge and recommendation. Without prior knowledge, comprehension and cognition of the text was difficult. Recommendation by a lecturer or reading list, a strong influence, saw students <b>selecting</b> <b>a</b> recommended <b>text</b> without considering their own needs leading to an inability to use this. Without knowledge and recommendation students utilised aesthetic preference and heuristics in <b>selecting</b> <b>a</b> <b>text,</b> with many <b>selecting</b> additional texts to assist in using recommended texts. The results {{led to the development of}} the conceptual process framework indicating choice was a complex process for the student. <b>Selecting</b> <b>a</b> <b>text</b> is complex and affected by numerous influences. The study highlights a process through which a student traverses as they undertake the selection of their text. The study conclusions have led to the development of the Process Framework for <b>Text</b> Selection providing <b>a</b> novel and coherent linking of established theoretical concepts...|$|R
5000|$|Many {{of these}} {{commands}} {{may be combined}} with [...] to <b>select</b> <b>a</b> region of <b>text.</b>|$|R
5000|$|To bypass this restriction, most pop-under ads trigger on a mouse click event {{listener}} attached {{directly to}} the document or the document's body. This enables catching all mouse click events that were not consumed by other click event handlers, and calling [...] without being blocked. For example, when the user <b>selects</b> <b>a</b> <b>text,</b> the mouse click triggers the mouse click handler attached to the document and a pop-under opens using the above code. Notice {{that there are more}} techniques to bypass the [...] call restriction by [...] "hijacking" [...] mouse clicks.|$|R
40|$|Some {{systems of}} divination {{are used to}} select {{particular}} sections of text, which are typically arcane and erudite, in which lies {{the answer to the}} particular, pressing problems of the client. Celebrated examples of such systems are the Chinese 1 Ching and the Yoruba Ifa. Werbner's work on Kalanga and Tswapong divination provides a case-study of the detailed praxis in such systems. Diviners have 3 multiple role when <b>a</b> divination technique <b>selects</b> <b>a</b> <b>text.</b> At each consultation they must satisfy themselves, their client, and their audience that they have followed the correct procedures to <b>select</b> the <b>text.</b> <b>A</b> second stage follows. The client has a particular question and the selected text was not composed as a specific answer to it. Interpretation is required to satisfy the client that the question has been answered. The diviner thus plays the role of indigenous critic, a role both similar to and different from that of literary critics in the Western tradition. The concept of 'dialogic' used by Barber in her analysis of Yoruba praise poetry is taken to illustrate similarities and differences between diviner and critic...|$|R
50|$|His book The New Encyclopedia of the Occult was <b>selected</b> as <b>a</b> {{reference}} <b>text</b> in 2005 by American Libraries {{and noted}} by Booklist and Publishers Weekly.|$|R
50|$|For {{any other}} {{software}} {{that is used}} on sites, {{one of the greatest}} concerns is due to issues arising regarding the balance between user experience and privacy. It is understood that the developers of Project Naptha are doing their best in attempting to allow the processing on the client side (i.e., within the browser). However, as text selected by users for extraction from the image are being processed in the cloud. This means that in order to achieve higher translation accuracy, there is still a need to rely on greater cloud processing and hence compromising on privacy. There is a default setting which helps to strike a delicate balance between having all the functionality made available and respecting user privacy. By default, when users begin <b>selecting</b> <b>a</b> <b>text,</b> <b>a</b> secure HTTPS request is sent. This is only contains the URL of the specific image and nothing else - no User Tokens, no Website Information, no Cookies or analytics and the requests are not logged. The server responds with a list of existing translations and OCR languages that have been done. This allows you to recognize <b>text</b> from <b>an</b> image with much more accuracy than otherwise possible.|$|R
50|$|Many of the Rectors {{were well}} and widely published. Below is <b>a</b> <b>select</b> list, <b>text</b> for many {{may be found}} via Google Books.|$|R
6000|$|As Lockhart says, [...] "Scott's diligent zeal had {{put him in}} {{possession}} of a variety of copies in various stages of preservation, and to the task of <b>selecting</b> <b>a</b> standard <b>text</b> among such <b>a</b> diversity of materials he brought a knowledge of old manners and phraseology, and a manly simplicity of taste, such as had never before been united in the person of a poetical antiquary." ...|$|R
5000|$|<b>Select</b> <b>a</b> {{portion of}} <b>text</b> by {{pressing}} the main mouse button while pointing the cursor {{at one end}} of the desired part of the text and dragging the cursor to the other end while holding the button pressed.|$|R
5000|$|... <b>selecting</b> <b>a</b> {{block of}} <b>text</b> to e.g. change size/font or copy to the clipboard, by holding shift and {{pressing}} the arrow cursor or other navigation keys, which commonly extends a coloured or inverse-video highlight over the selected area ...|$|R
40|$|Latent Semantic Analysis (LSA) {{has been}} shown to perform many {{linguistic}} tasks as well as humans do, and has been put forward as a model of human linguistic competence. But LSA pays no attention to word order, much less sentence structure. Researchers in Natural Language Processing have made significant progress in quickly and accurately deriving the syntactic structure of texts. But there is little agreement on how best to represent meaning, and the representations are brittle and difficult to build. This paper evaluates a model of language understanding that combines information from rule-based syntactic processing with a vector-based semantic representation which is learned from a corpus. The model is evaluated as a cognitive model, and as a potential technique for natural language understanding. Motivations Latent Semantic Analysis (LSA) was originally developed for the task of information retrieval, <b>selecting</b> <b>a</b> <b>text</b> which matches <b>a</b> query from a large database (Deerw [...] ...|$|R
5000|$|Oikonomia is {{the journal}} founded in 1999 at the Faculty of Social Sciences (FASS) of the Angelicum. It is a {{collaborative}} {{project of the}} lecturers and students of the faculty, and of scholars who work with the FASS. The issues that are covered {{are those of the}} social sciences, as we understand them in our tradition, covering five areas: philosophy, law, history, psico-sociological, economics. The subjects treated as the journal's editorial profile has developed have ranged from theoretical issues to reports on conferences, to reviews of important new books. Particular attention is given in every number to <b>selecting</b> <b>a</b> <b>text</b> from the recent or distant past, but which always has particular significance for the main theme of the number; this text, the [...] "classic page", is always directly connected with the editorial. The editorial committee ensures only that a correct methodology has been employed by the author of contributions. It does not vet the content of the articles, for which the sole responsibility lies with the authors.|$|R
30|$|The third method-machine learning-uses {{computer}} algorithms for {{the classification}} of textual sentiment. The main steps of machine learning are as follows: (i) <b>select</b> <b>a</b> specific <b>text</b> as the corpus training set and manually classify the words contained within, (ii) use a computer algorithm, such as the Naive Bayes algorithm, to train the text in the training set and establish the judgment rules for text classification, and (iii) apply the judgment rules to all text classifications.|$|R
50|$|In Opera <b>a</b> triple-click will <b>select</b> all <b>text</b> within <b>a</b> {{sentence}} while automatically {{popping up}} a list of commands to apply to the <b>selected</b> <b>text.</b> <b>A</b> quadruple-click will <b>select</b> all <b>text</b> within <b>a</b> single paragraph while keeping the aforementioned popup open.|$|R
50|$|In {{addition}} to teaching, Ramos published articles in educational journals including La Escuela Primaria (The Elementary School), El Pensamiento (Thinking), El Eco del Comercio (The Echo of Commerce), La Revista de Mérida (The Magazine of Merida), {{as well as}} textbooks. In 1875, his mathematics textbook received official recognition as <b>a</b> <b>text</b> for schools and was reprinted into four editions. In 1879, a textbook he created on line drawing was also <b>selected</b> as <b>an</b> official <b>text</b> for Yucatán schools.|$|R
30|$|Decision {{information}} {{can also be}} ‘stored’ in cultural forms, particularly the shared expectations and beliefs among {{members of a group}} that constitute high-status knowledge (Weick [1979]). Sixteen respondents reported that their colleagues held general expectations regarding teaching and learning, such that these norms represented a clearly identified body of information within their departments. In addition, six respondents reported that norms pertained to colleagues’ expectations regarding the canon of their discipline and the topics that should be taught to undergraduate students. These expectations were perceived as so strong as to be beyond debate, such that determining the curriculum for certain courses was as simple as <b>selecting</b> <b>a</b> canonical <b>text.</b>|$|R
40|$|The paper {{establishes}} {{and analyses}} {{a series of}} signification cores of some Hélène Cixous’s <b>texts,</b> giving <b>a</b> special relevance to the writing of time and, concretely, to {{the writing of the}} moment, of the «still». I intend to demonstrate that writing the «still» means in Hélène Cixous’s <b>selected</b> <b>texts</b> <b>an</b> exit door to the life, to the potential of happiness of the moment...|$|R
30|$|We often {{estimate}} {{the performance of}} a model by calculating its perplexity on <b>a</b> <b>selected</b> development <b>text.</b> Compared to estimation within speech recognition, this method is much faster, as we only need to estimate each sentence once. Furthermore, {{it has been shown that}} there is a correlation between a language model’s perplexity and its performance in speech recognition [20].|$|R
5000|$|A list builder, {{also known}} as a dual list, dual listbox, {{disjoint}} listbox, list shuttle, shuttle, swaplist and two sided multi <b>select</b> is <b>a</b> graphical control element in which <b>a</b> user can <b>select</b> <b>a</b> set of <b>text</b> values by moving values between two list boxes, one representing selected values and the other representing unselected ones. Moving values back and forth is usually accomplished through buttons reading [...] "Add" [...] and [...] "Remove", rather than by dragging and dropping them. The widget can sometimes also include the ability to rearrange the selected values.|$|R
50|$|Look Both Ways is a 2005 Australian {{independent}} film, {{written and}} directed by Sarah Watt, starring an ensemble cast, which was released on 18 August 2005. The film was supported by the Adelaide Film Festival Investment Fund and opened the 2005 Adelaide Film Festival. It won four AFI Awards, including Best Film and Best Direction. The film was <b>selected</b> as <b>a</b> film <b>text</b> by the Victorian Curriculum and Assessment Authority for the VCE English Course from 2007 to 2010.|$|R
40|$|The {{purpose of}} this study was to improve student {{literacy}} in the science classroom by providing weekly literacy practice. A total of 94 inner-city students from the Rochester City School District in the State of New York participated in weekly reading practice using selected reading passages that varied in difficulty. Students were instructed to complete 6 essential questions based on each reading passage and chart their weekly progress. These reading passages, accompanying questions, and diagnostic graphs were <b>selected</b> from <b>a</b> <b>text</b> entitled Reading in the Content Areas: Science (2005). It was hoped that students would develop an active searching attitude about what they read and therefore set the stage for higher comprehension. The weekly literacy practice, along with other literacy strategies used throughout the week, resulted in most student literacy scores either staying the same or increasing...|$|R
40|$|Textbooks provide novice {{teachers}} with guidance in course and activity design; it assures {{a measure of}} structure, consistency, and logical progression in a class; It meets a learner’s needs or expectations of having something concrete to work from and take home for further study; It may provide multiple resources: tapes, CDs, videos, self-study workbooks etc. While the quality of ESL reading textbooks has improved dramatically in recent years, the process of <b>selecting</b> <b>an</b> appropriate <b>text</b> has not become any easier for most teachers and administrators. Thus, the paper discusses for evaluating reading textbooks for use in ESL/EFL classrooms. Classroom teachers spend much time using textbooks in class, so choosing an appropriate one is important. And the paper describes {{the role of the}} textbook. Using this will make the textbook selection process more efficient and more reliable. </p...|$|R
40|$|To {{interpret}} <b>a</b> <b>text</b> means, {{under certain}} conditions, to simply iterate it with no personal contribution, or, {{in the terms}} of an already widely accepted expression, the best exegesis is the text itself. The interpretation can prejudice the chosen text by virtue of the cultural memory of the interpreter, tempted to place it at its intersection with other texts: thus, it will refer it to <b>a</b> <b>text</b> more or less different from the one concerned. This conclusion was also reached by those theoreticians of language who consider that ‘the only way of remaining absolutely true to <b>a</b> <b>text</b> is to repeat it’. It is from the perspective of this theory that, in the present article, I have approached interpretation as the reiteration of <b>a</b> <b>text</b> about our Romanian character, <b>selected</b> from <b>an</b> ampler <b>text</b> of Alexandru Paleologu. What allowed such an approach {{is the fact that the}} text reveals what any Romanian, at a certain moment, would say about himself, his personal or national history. About the condition of being Romanian...|$|R
40|$|This paper {{examines}} the editorial discussion of Wikipedia's policy of “Neutral Point of View” during the collaborative authorship of the entry for Che Guevara. Such data provide a rare source of unprompted lay metalanguage {{on the relationship}} between the lexicogrammatical form of <b>a</b> <b>text</b> and its evaluative orientation and make possible an inductive approach to exploring this longstanding issue in critical linguistics inquiry. The paper illustrates this inductive approach in <b>selecting</b> <b>a</b> stretch of <b>text</b> identified by the editors themselves as problematic and relating their metalinguistic commentaries to the lexicogrammatical features of both the original text and the variations proposed. However, recognizing that naturally occurring metalanguage is itself situated within its own social practice, the paper considers ways in which the editors' readings are a function of their role as Wikipedia “prosumers” and goes on to discuss the need to theorize specific reader types within critical linguistic approaches to discourse analysis...|$|R
40|$|Our {{starting}} point for this assignment was to investigate if {{it is possible to}} unite Muslim values in a European society? We choose to go into the two books, of respectively Tariq Ramadan and Safet Bektovic. They are both faithful Muslims, who are trying to present the problems which arise in the meeting of cultures. To begin with we made a summary of Ramadans point of view, and a following analysis of argument; then we <b>selected</b> <b>a</b> piece of <b>text,</b> which we made a close-up analysis of. Furthermore we have made a summary of which subjects Bektovic treats in his book, and in the end we compare the two books...|$|R
40|$|The article {{analyses}} <b>a</b> <b>selected</b> <b>text</b> of the travelogue of Czech nobleman and humanist Christopher Harant of Polžice and Bezdružice (1564 – 1621). Harant’s {{information on}} the ancient Roman custom of punishing debtors by mocking them in the theatre as instituted by the Lex Roscia or Iulia is confused. It is claimed that even though he gives exact bibliographic citations of several ancient authors he most probably did not misinterpret primary sources but a secondary source. Some general conclusions relevant for assessing Harant’s work with primary sources are drawn...|$|R
