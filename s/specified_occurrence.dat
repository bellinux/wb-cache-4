3|39|Public
5000|$|Protection {{policies}} - {{designed to}} provide a benefit, typically a lump sum payment, {{in the event of a}} <b>specified</b> <b>occurrence.</b> A common form - more common in years past - of a protection policy design is term insurance.|$|E
40|$|Multiple {{regression}} on catchment characteristics is used {{to obtain}} regional equations of the Log Pearson Type III descriptive parLiineters, mean, standard deviation and skew coefficient for the annual flood series. The estimates of the descriptive parameters are incorporated into a regional design flood estimation model which utilizes the Log Pearson Type III frequency factors listed in Table 9. 3 of Australian Hainfall and Runoff. The resulting equation provides estimates of peak discharge of <b>specified</b> <b>occurrence</b> frequency. Eighty-four variables including arithmetic and logarithmic transformations were investigated. From this list seven (three geomorphic, three climatic and one regional) are recommended for use in future research in the region utilizing multiple regression techniques. Tv/o sets of regional equations are recommended for mean and standard deviation. No significant relationship was obtained to estimate a skew coefficient...|$|E
40|$|Cet article a été réalisé dans le cadre d'un projet de trois ans visant à développer une méthodologie d'analyse, de prévision et de contrôle des risques d'inondation au Québec. Il présente une approche concrète pour calculer les impacts potentiels et le risque d'inondation et {{utiliser}} ces résultats afin d'évaluer la situation du risque local, de décider si les impacts doivent être minimisés et de choisir les moyens d'intervention appropriés. Le risque d'inondation est considéré comme étant le produit de la probabilité d'occurrence des crues et des conséquences occasionnées par ces événements. Les pertes de vies potentielles et les dommages directs sont évalués en simulant les niveaux d'eau de différents scénarios d'inondation à partir d'un modèle d'écoulement unidimensionnel non permanent, et en intégrant ces résultats à un logiciel géoréférencé de calcul des dommages d'inondation. L'analyse des impacts et du risque calculé {{permet de}} dresser un portrait du montant des dommages annuels potentiels sur les sites habités le long du cours d'eau et un portrait de l'évolution des impacts en fonction de l'amplitude des crues. Cette analyse mène à l'identification des sites où existe un risque jugé inacceptable selon des critères préétablis. Pour chaque site où des interventions sont justifiées par le niveau de risque, des scénarios de minimisation des impacts tenant compte des mécanismes d'inondation sont élaborés et ensuite simulés afin d'en mesurer l'efficacité. Un exemple d'application à un site de la rivière Châteauguay illustre la méthode et les gains pouvant découler de son utilisation. Each year, several rivers in Quebec {{are responsible for}} severe flooding and these events generate major socio-economic impacts. The frequency and magnitude of these episodes highlight {{the existence of a}} real flood risk. Using global information concerning level and extent of flood risk, authorities {{would be more likely to}} make appropriate decisions in the management of flood risk. This article results from a three year project aimed at developing a methodology for the analysis, forecasting and control of flood risk in Quebec. It suggests a concrete approach for the evaluation of the potential impact of floods in order to obtain a better knowledge of local risk in inhabited areas and exploits there results to evaluate the acceptability of the calculated risk and to plan appropriate risk minimisation interventions. Risk is defined as the product of the mathematical expectation of a <b>specified</b> <b>occurrence</b> with the expected consequences of the event. In floodplain studies, flood risk is the probability of the occurrence at a given flood multiplied by the expected consequences resulting from this event. Different types of consequences may be observed, clearly the easiest to evaluate being direct or material damages and potential loss of life. The risk calculated using the proposed definition is attributable in variable proportions to the frequency of the floods and the amount of damages. A given calculated global risk on a site could be the result of frequent floods, each causing moderate damage or of a single (or more) extreme event, with very low probability of occurrence, but causing severe damage. Risk associated with rare events could be considered as an acceptable risk, a risk we decide to live with, since the resources available to prevent flood damage are often limited and a decision is taken to optimise the allocation of these resources. The flood level corresponding to the limit between acceptable and unacceptable risk must be determined by the population concerned and be based on a good knowledge of the risk situation. The proposed methodology to evaluate and minimise flood risk for a site localised in a river flood plain involves six steps: 1. the realisation of a hydrologic frequency analysis to determine the amplitude of the floods associated with the flood frequency, 2. the hydraulic simulation of floods to predict water level and velocity in the stream for each scenario, 3. the assessment of direct damage and potential loss of life for each flood simulated, 4. the calculation of risk, 5. the risk analysis considering the limit of acceptable risk and 6. the choice and planning of appropriate intervention to eliminate unacceptable risk. This approach has been applied to the study of a site along the Châteauguay River, a tributary of the St Laurence River, a river that experiences flood events every two years or so. Seven flood scenarios (the 2, 3, 10, 20, 100, 1 000, and 10 000 year flood) are used to evaluate the risk for a site localised in the village of Huntingdon. Hydraulic characteristics, water level and velocity, associated with each flood scenario are determined using the DAMBRK model, a one-dimensional unsteady flow model. The results are incorporated in DOMINO, a geo-referenced software calculating flood impacts. This software allows the user to create a three-dimensional numerical model of the site based on topographic information. The superposition of hydraulic results provides the flow depth at any point within the site. Damage is evaluated by integrating the municipal roll number of Huntingdon, which provide the site location and value of each building, and gives an estimate of the population threatened by each flood event at the site. These results of direct damages are used to calculate the risk related to each flood event simulated on the Huntingdon site. For this application, the unacceptable risk has generally been agreed to be the risk resulting from the 20 year flood, or more frequent floods, for the material damages and to the number of potential losses of life associated with the 100 year flood or more frequent flood. The analysis indicates that an unacceptable risk of 23 993 $ per year for material damages and potential loss of life of 50 persons exist. Different site scale interventions to eliminate this risk have been simulated and proven to be efficient only if complemented with a few local modifications to the more exposed buildings. This approach may be extended to the study of any river because it takes into consideration local hydrologic and hydraulic conditions. It has the advantage of being based on existing information and to be automated, which limits the time and resources required to obtain the base data and perform the necessary simulations...|$|E
5000|$|Lastly, the {{commands}} {{can be further}} modified with V to turn on verify mode and with O to <b>specify</b> nth <b>occurrence</b> string mode: ...|$|R
40|$|International audienceWe find {{generating}} functions {{the number}} of strings (words) containing a <b>specified</b> number of <b>occurrences</b> of certain types of order-isomorphic classes of substrings called subword patterns. In particular, we find generating functions for {{the number of}} strings containing a <b>specified</b> number of <b>occurrences</b> of a given 3 -letter subword pattern...|$|R
40|$|CINTIL online {{concordancer}} (is a freely {{available online}} concordancing service {{to support the}} research usage of the CINTIL Corpus. It allows the use of generic patterns to <b>specify</b> the <b>occurrences</b> to be retrieved. This permits to uncover linguistic structures of high complexity and use this service as a powerful research tool...|$|R
40|$|We {{describe}} an algebraic technique for performing timing analysis on a restricted class of Petri nets with interval time delays specified on the places of the net. The timing analysis we perform determines the extreme separation in time between <b>specified</b> <b>occurrences</b> of pairs of transitions for all possible timed executions of the system. We present {{the details of}} the timing analysis algorithm and demonstrate polynomial running time on a non-trivial parameterized example. Petri nets with 3000 nodes and 10 16 reachable states have been analyzed using these techniques. 1 Introduction The majority of research involving the formal analysis of temporal issues in concurrent systems has focused on powerful models of concurrency and these techniques are therefore often prohibitively computationally expensive. This paper takes the approach of using a less expressive model of a concurrent system in favor of a more efficient analysis. Our model of a concurrent system is based on safe Petri ne [...] ...|$|R
30|$|We report {{results of}} multi-instrumented {{experimental}} efforts to better <b>specify</b> the <b>occurrence</b> of June solstice ESF in the American sector and {{to better understand}} the conditions prior to their development. As part of this effort, we used measurements made by a 14 -panel version of the Advanced Modular Incoherent Scatter Radar (AMISR- 14) system. Our results include the first UHF radar observations of June solstice ESF events over the Jicamarca Radio Observatory (11.95 ° S, 76.87 ° W, ∼[*] 1 ° dip latitude).|$|R
40|$|The paper {{introduces}} {{a theory of}} Lexical Event Structures {{as a means to}} represent the meaning of verbs. The theory is guided by the assumption that verbs refer to events that are internally structured {{in the sense that they}} consist of several subevents and states. The temporal properties and relations of these have to be <b>specified.</b> The <b>occurrence</b> of subevents is either implied or presupposed by the verb, and event participants are related to some, but not necessarily all subevents by semantic relations...|$|R
40|$|In this illustrative {{case study}} {{we examine the}} three {{forensic}} interviews of a girl who experienced repeated sexual abuse from ages 7 to 11. She disclosed the abuse after watching a serialized television show that contained a storyline similar to her own experience. This triggered an investigation that ended in successful prosecution of the offender. Because this case involved abuse that was repeated {{on a weekly basis}} for 4 years we thus investigated {{the degree to which the}} child 2 ̆ 7 s narrative reflected specific episodes or generic accounts, and both the interviewer 2 ̆ 7 s and child 2 ̆ 7 s attempts to elicit and provide, respectively, specific details across the 3 interviews collected in a 1 month period. Across the 3 interviews, the child 2 ̆ 7 s account was largely generic, yet on a number of occasions she provided details specific to individual incidents (episodic leads) that could have been probed further. As predicted: earlier interviews were characterized more by episodic than generic prompts and the reverse was true for the third interview; the child often responded using the same style of language (episodic or generic) as the interviewer; and open questions yielded narrative information. We discuss the importance of adopting children 2 ̆ 7 s words to <b>specify</b> <b>occurrences,</b> and the potential benefits of permitting generic recall in investigative interviews on children 2 ̆ 7 s ability to provide episodic leads. Despite the fact that the testimony was characterized by generic information about what usually happened, rather than specific episodic details about individual occurrences, this case resulted in successful prosecution...|$|R
40|$|Copyright © 2014 M. P. Silverman. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. In accor-dance of the Creative Commons Attribution License all Copyrights © 2014 are reserved for SCIRP {{and the owner of}} the intellectual property M. P. Silverman. All Copyright © 2014 are guarded by law and by SCIRP as a guardian. Run count statistics serve a central role in tests of non-randomness of stochastic processes of interest {{to a wide range of}} disciplines within the physical sciences, social sciences, business and finance, and other endeavors in-volving intrinsic uncertainty. To carry out such tests, it is often necessary to calculate two kinds of run count probabilities: 1) the probability that a certain number of trials results in a <b>specified</b> multiple <b>occurrence</b> of an event, or 2) the probability that a <b>specified</b> number of <b>occurrences</b> of an event take place within a fixed number of trials. The use of appropriate generating functions provides a systematic procedure for obtaining the distribu-tion functions of these probabilities. This paper examines relationships among the generating functions applica-ble to recurrent runs and discusses methods, employing symbolic mathematical software, for implementing nu-merical extraction of probabilities. In addition, the asymptotic form of the cumulative distribution function i...|$|R
40|$|As {{mentioned}} in the preceding additional information (hereafter called Part I), a series of strong earthquakes with magnitudes between 5. 2 and 5. 9 -units occurred during the two weeks period: 3 to 19 April, 2006 with epicenters lying at distances 80 to 100 km west of PAT station. Here, we show that the analysis in the natural time of the seismicity that occurred after the Seismic Electric Signals (SES) activity on February 13, 2006, <b>specifies</b> the <b>occurrence</b> time of the initiation of the aforementioned earthquake activity within a narrow range around two days. Furthermore, we provide the most recent information on some points {{mentioned in}} the main text. Comment: Seven pages, including 7 figures and 1 Table, Additional information for a paper accepted for publication in Phys. Rev. ...|$|R
2500|$|The Saros dial is {{the main}} lower spiral dial on {{the rear of the}} mechanism. The Saros cycle is 18 years and [...] days long (6585.333… days), which is very close to 223 synodic months (6585.3211 days). It is defined as the cycle of {{repetition}} of the positions required to cause solar and lunar eclipses, and therefore, it could be used to predict them— not only the month, but the day and time of day. Note that the cycle is approximately 8 hours longer than an integer number of days. Translated into global spin, that means an eclipse occurs not only eight hours later, but one-third of a rotation farther to the west. Glyphs in 51 of the 223 synodic month cells of the dial <b>specify</b> the <b>occurrence</b> of 38 lunar and 27 solar eclipses. Some of the abbreviations in the glyphs read: ...|$|R
40|$|The {{results of}} an {{investigation}} of boundary layers close to the stagnation point of an oscillating airfoil are reported. Two procedures for generating initial conditions, the characteristics box scheme and a quasi-static approach, were investigated, and the quasi-static approach was shown to be appropriate provided the initial region was far from any flow separation. With initial conditions generated in this way, the unsteady boundary layer equations were solved for the flow in the leading edge region of a NACA 0012 airfoil oscillating from 0 to 5 deg. Results were obtained for both laminar and turbulent flow, and, in the latter case, the effect of transition was assessed by <b>specifying</b> its <b>occurrence</b> at different locations. The results demonstrate {{the validity of the}} numerical scheme and suggest that the procedures should be applied to calculation of the entire flow around oscillating airfoils...|$|R
40|$|The {{paper is}} titled: Phraseology updates in the {{contemporary}} Czech language The aim {{of this paper is}} to collect and describe phraseological updates. The collection of materials (resources) took place from January 2013 to June 2015. Updates were sought out from various sources - the media, books, the theater, the internet and landline calls. With the help of professional literature we first limited the theoretical framework of the issue and based on these theoretical attainments we classified the material according to the sources and types of resources from which it originated. In the summary of updates we can see for every idiom update its original version, its type of update, its specific description and its source with a comment <b>specifying</b> its <b>occurrence</b> etc. With the performed analysis we determined that the most common kinds of updates are contamination and lexical updates...|$|R
40|$|The Simple Temporal Network {{formalism}} permits significant {{flexibility in}} <b>specifying</b> the <b>occurrence</b> time {{of events in}} temporal plans. However, to retain this flexibility during execution, {{there is a need}} to propagate the actual execution times of past events so that the occurrence windows of future events are adjusted appropriately. Unfortunately, this may run afoul of tight real-time control requirements that dictate extreme efficiency. The performance may be improved by restricting the propagation. However, a fast, locally propagating, execution controller may incorrectly execute a consistent plan. To resolve this dilemma, we identify a class of dispatchable networks that are guaranteed to execute correctly under local propagation. We show that every consistent temporal plan can be reformulated as an equivalent dispatchable network, and we present an algorithm that constructs such a network. Moreover, the constructed network is shown to have a minimum number of edges among all such n [...] ...|$|R
5000|$|The Saros dial is {{the main}} lower spiral dial on {{the rear of the}} mechanism. The Saros cycle is 18 years and [...] days long (6585.333… days), which is very close to 223 synodic months (6585.3211 days). It is defined as the cycle of {{repetition}} of the positions required to cause solar and lunar eclipses, and therefore, it could be used to predict them — not only the month, but the day and time of day. Note that the cycle is approximately 8 hours longer than an integer number of days. Translated into global spin, that means an eclipse occurs not only eight hours later, but one-third of a rotation farther to the west. Glyphs in 51 of the 223 synodic month cells of the dial <b>specify</b> the <b>occurrence</b> of 38 lunar and 27 solar eclipses. Some of the abbreviations in the glyphs read: ...|$|R
40|$|We {{suggest a}} new {{formalization}} of the existential termination problem of logic programs under the PROLOG leftmost selection rule and depth-first computation rule. First of all, we give a {{characterization of the}} problem in terms of occurrence sets, by proving that a hprogram; goali existentially terminates if and only if there exists a finite correct occurrence set. Then we show that in order to study existential termination, we do not need to <b>specify</b> the <b>occurrences</b> of the atoms, since existential termination turns out to be decidable, when instances of atoms are used more than once (up to renaming). We then reduce the verification of existential termination to the search of a suitable semi occurrence set for the pair hprogram; goali, by providing an algorithm for proving that the proposed semi occurrence set is a correct occurrence set. Finally we propose a simple method (based on abstract interpretation techniques) for generating such semi occurrence sets. Keywords: existe [...] ...|$|R
40|$|Electromechanical {{oscillations}} in any {{electrical power}} system is a typical characteristic {{of this kind of}} systems. Provided the damping associated with these oscillations lies above of a minimum <b>specified</b> value, the <b>occurrence</b> of these oscillation is not considered {{as a threat to the}} system operation. This paper focuses the attention on the application of modal analysis for assessing the dynamical behavior of a power system subjected to small disturbances for different operating conditions and transmission system topologies, as well. The reported results indicate, that modal analysis enables a straight identification of the causes that contribute negatively to the damping of the electromechanical modes...|$|R
40|$|When the {{partitions}} of [n] = { 1, 2, [...] .,n} {{are identified}} with the restricted growth functions on [n], under a known bijection, certain enumeration problems for classical word statistics are formulated for set partitions. In this paper we undertake the enumeration of partitions of [n] {{with respect to the}} number of occurrences of rises, levels and descents, of arbitrary integral length not exceeding n. This approach extends previously known cases. We obtain ordinary generating functions for the number of partitions with a <b>specified</b> number of <b>occurrences</b> of the three statistics. We also derive explicit formulas for the number of occurrences of each statistic among all partitions, besides other combinatorial results. ...|$|R
30|$|Equatorial spread F (ESF) {{irregularities}} {{are caused}} by plasma instability processes in the equatorial ionosphere and especially related to plasma bubble phenomena in the topside F region. Plasma bubbles result from nonlinear evolution of the Rayleigh-Taylor instability. The prereversal enhancement (PRE) of the postsunset vertical plasma drift increases {{the height of the}} equatorial F layer, creating conditions conducive to the growth of the Rayleigh-Taylor instability, and is an important factor that controls the generation of ESF. Three different relationships of the quantitative connection between the PRE and ESF occurrence have been derived. (1) A threshold PRE must be reached for the occurrence of ESF. ESF will be generated when the PRE is larger than the threshold and will not be generated when the PRE is smaller than the threshold. (2) The occurrence probability of ESF increases approximately linearly with the PRE. (3) The occurrence probability of ESF is characterized by a continuous probability distribution {{as a function of the}} PRE magnitude. The second and third relationships imply that the PRE can be used to <b>specify</b> the <b>occurrence</b> probability of ESF. This paper will review these relationships and discuss how these relationships are connected to each other. The effects of seeding perturbations on the generation and global distribution of ESF will be briefly discussed.|$|R
40|$|Temporal plans permit signicant {{flexibility}} in <b>specifying</b> the <b>occurrence</b> time of events. Plan execution can make {{good use of}} that flexibility. However, the advantage of execution flexibility is counterbalanced by the cost during execution of propagating the time of occurrence of events throughout the flexible plan. To minimize exe-cution latency, this propagation needs to be very ecient. Previous work showed that every tem-poral plan can be reformulated as a dispatchable plan, i. e., one for which propagation to immedi-ate neighbors is sucient. A simple algorithm was given that nds a dispatchable plan with a mini-mum number of edges in cubic time and quadratic space. In this paper, we focus on the eciency of the reformulation process, and improve on that result. A new algorithm is presented that uses linear space and has time complexity equivalent to Johnson’s algorithm for all-pairs shortest-path problems. Experimental evidence conrms the practical eectiveness of the new algorithm. For example, on a large commercial application, the performance is improved by at least two orders of magnitude. We further show that the dispatch-able plan, already minimal in the total number of edges, can also be made minimal in the max-imum number of edges incoming or outgoing at any node...|$|R
40|$|Load structure, {{the maximum}} {{permissible}} load is usually {{determined by the}} strength calculation (bearing data. Yield stress or strength of the material), or the parameters of fracture mechanics. The paper analyzes the calculations lamellar cracks formed in the lower crane girders steel belts. These cracks are formed in rolled sheets of non–metallic inclusions. The determination of {{the cracks in the}} existing designs are extremely difficult. It requires testing metallographic, ultrasonic or acoustic methods. Based on typical images of metallographic steel plates <b>specified</b> place of <b>occurrence</b> of cracks lamellar and sampling, and has subsequently been adopted for the calculation of numerical models of material structure and distribution of artificial joints. In order to analyze the propagation of cracks and formation of lamellar phase slots and destruction was carried out modeling studies and numerical methods cracking process...|$|R
40|$|This paper {{introduces}} {{a new way}} to predict contact slip using a resistive tactile sensor. The prototype sensor can be used to provide intrinsic information relating to geometrical features situated on the surface of grasped objects. Information along the gripper finger surface is obtained with a measurement resolution dependant on the number of discrete tactile elements. The tactile sensor predicts the partial slip of a tactile surface by sensing micro vibrations in tangential forces which are caused by an expansion of the slip regions within the contact area. The location of the local slip is not <b>specified</b> but its <b>occurrence</b> can be predicted immediately following micro vibration detection. Predictive models have been used to develop a set of rules which predict the slip based on fluctuations in tactile signal data...|$|R
40|$|As road {{networks}} and traffic volumes increase, road-effects on animal populations {{are becoming more}} prevalent. Our goal was to identify herpetofaunal crossing hotspots on roads and to use this information to prioritize deployment of mitigation efforts. Our focus was New York State where in a collaborative effort between ecologists and the New York State Department of Transportation we synthesized available literature to predict patterns of habitat use by 10 species of herpetofauna. A geographic information system was then used to develop habitat resistance-based models to predict hotspots of herpetofaunal abundance on roads. We developed three approaches for prioritizing model output for transportation planners: (1) Categorizing occurrence indices based on five quantiles; (2) overlaying the arterial classification code (a measure of traffic intensity) over model outputs, and (3) using the contiguous length of road remaining within <b>specified</b> high <b>occurrence</b> index values. Models were evaluated using field data derived from road surveys. Our models showed clear differences in the predicted occurrence of species of herpetofauna on roads depending on life-history strategies. Wide-ranging habitat generalists were predicted {{to have at least}} some probability of occurrence on most roads in the study area, for example the common snapping turtle, Chelydra serpentina, was predicted to occur on 98 % of roads. Conversely, species with limited movement ranges and specific aquatic an...|$|R
40|$|Handwritten {{document}} {{recognition is}} an area of pattern recognition that has been showing impressive performance in the machine printed text. Handwritten document recognition is an intricate task to various writing styles of individual person. The system first identifies the contour in a handwritten document for segmentation and features are extracted from the segmented character. This paper uses GLCM(Gray Level Co-occurrence Matrix) for character recognition. Features of a character has been computed based on calculating the pairs of pixel with specific values and <b>specified</b> spatial relationship <b>occurrence</b> in an image. First order and second order textures are used to measure the intensity of the original pixels. Data were collected from different persons, and the system is trained using SVM with various writing styles. The proposed system achieves a maximum recognition accuracy of 95. 2 % with training and testing data using GLCM as features and SVM with RBF kernel function. Keywords...|$|R
40|$|Abstract. A {{wide range}} of {{counting}} and occurrence constraints can be specified with just two global primitives: the Range constraint, which computes the range of values used by a sequence of variables, and the Roots constraint, which computes the variables mapping onto a set of values. We focus here on the Roots constraint. We show that propagating the Roots constraint completely is intractable. We therefore propose a decomposition {{which can be used}} to propagate the constraint in linear time. Interestingly, for all uses of the Roots constraint we have met, this decomposition does not destroy the global nature of the constraint as we still prune all possible values. In addition, even when the Roots constraint is intractable to propagate completely, we can enforce bound consistency in linear time simply by enforcing bound consistency on the decomposition. Finally, we show that <b>specifying</b> counting and <b>occurrence</b> constraints using Roots is effective and efficient in practice on two benchmark problems from CSPLib. ...|$|R
40|$|International audienceThis paper {{considers}} {{the design of}} a distribution network composed of a single supplier serving a set of retailers through a set of Distribution Centres (DCs) to locate. The number and location of DCs are decision variables and they are chosen from the set of retailer locations. Each retailer faces a random demand and the supplier lead-time is constant. Furthermore, some parameters are random and described by scenarios, each with a <b>specified</b> probability of <b>occurrence.</b> The problem is formulated as a two-stage non-linear discrete stochastic optimisation problem. At the first stage, we decide on the DCs location and at the second stage, we assign retailers to located DCs with respect to a given scenario. The goal is to minimise the expected total cost resulted from the DCs location, transportation, working inventory and safety stocks costs. A Lagrangian relaxation based approach is proposed to generate efficient solutions. Computational results are presented and analysed showing the effectiveness of the proposed approach...|$|R
40|$|Abstract niet beschikbaarIn {{this report}} a {{strategy}} for ecological standard setting is proposed for environmental variables {{in the field of}} acidification, eutrophication and desiccation, using a risk assessment. Risks at the species level and ecosystem level are specified. At the species level the risks are assessed on the base of the ecological amplitude of a species. The ecological amplitude describes the occurrence probability of a species as a function of environmental variables. Values of the environmental variable are assessed that correspond with a <b>specified</b> reduced <b>occurrence</b> probability. Reduced occurrence probability may be due to intoxication (too much) and limitation (too little). These values are quantified for each species as the uppervalue and lowervalue of the environmental variable that envisage 90 % of the observations of a species (5 % percentile and 95 % percentile values). At the ecosystem level risks are specified as the concentration that protects a specified percentages of all species. In the dutch environmental policy for pollutants this percentages is set at 95 %. The ecological standard is assessed by combining the risks at the species level and the ecosystem level. The number of species, for which the 5 % and 95 % percentiles are not exceeded, are plotted {{as a function of the}} environmental variable. From this relation the maximum environmental level can be assessed that protects 95 % of all species. This strategy for standards setting is illustrated with a case study in the Province of South-Holland. Ecological standards are calculated for nitrogen load and changes in groundwaterlevel using data on 285 plant species. This report will be issued in english...|$|R
40|$|This paper {{explores the}} notion that the {{theoretical}} basis for contemporary research concerning the structure and function of marine pelagic ecosystems is self-limiting. While some findings such as the microbial food web have extended our knowledge of the biological components of the upper water column and their relationships to fluxes of materials and energy, they have not advanced our understanding of why specific pelagic forms occur in time and space, and why only some attain dominant status and contribute the bulk of biogenic fluxes emanating from the mixed layer. It is argued here that a major impediment to improved conceptual models is the historic focus on resource-driven or 'bottom-up' factors as being the dominant variables structuring planktonic ecosystems. Evidence is presented that predation or 'top-down' trophic effects may be equally important in <b>specifying</b> the <b>occurrence</b> of particular taxa, the biomass within adjacent trophic levels, and the morphology of dominant herbivores and carnivores. It is suggested that key species, because of unique combinations of life history strategies, metabolic demands, and physiological performance, may exert a dominant role in the extent to which predatory interactions cascade through pelagic food webs. There is considerable evidence of evolution of predation avoidance strategies among phytoplankton and zooplankton. It is proposed that future research might profitably be directed toward the question of how the pelagic environment selects for life histories and morphologies of organisms under conditions when resource availability and predation are both significant structural buttresses. Methodological approaches should include detailed studies of dominant key taxa from different environments, with the goal of identifying the critical aspects of life history, behavior, or morphology which account for their success...|$|R
40|$|Abstract. Under the {{so-called}} culminative definition of stress, present-day linguists hold {{the view that}} within one word or larger domain only one syllable can bear the stress. This is {{in contrast with the}} classical (British-English) phonetic tradition which allows the occur-rence of two strong stresses within certain words, which are then called 'double-stressed'. Moreover, precisely the class of double-stressed words was said to be subject to rhythmic Variation (or 'stress clash'). The present paper purports to find acoustic and perceptual evi-dence that may allow us to choose between these competing proposals, comparing the be-haviour of Dutch adjectives with canonically rising, falling, and double-stress patterns, in spoken contexts that should bear out the predicted rhythmic changes in double-stressed words. Our results argue against a strictly culminative definition of stress. Introduction definition of stress [Trubetskoy, 1958; Hy-man, 1977]. Generative phonology explic-Culminative versus Equal Stress itly captures this principle in its rule mecha-Stress is an abstract, lexical property that nisms, which clearly exclude the <b>occurrence</b> <b>specifies</b> which syllable is the strengest in a of two equally strong, primary stresse...|$|R
60|$|I may here {{allude to}} a class of facts closely allied to, but {{somewhat}} different from, ordinary cases of inheritance. Sir H. Holland (12/34. 'Medical Notes and Reflections' 1839 pages 24, 34. See also Dr. P. Lucas 'L'Hered. Nat.' tome 2 page 33.) states that brothers and sisters of the same family are frequently affected, often {{at about the same}} age, by the same peculiar disease, not known to have previously occurred in the family. He <b>specifies</b> the <b>occurrence</b> of diabetes in three brothers under ten years old; he also remarks that children of the same family often exhibit in common infantile diseases, the same peculiar symptoms. My father mentioned to me the case of four brothers who died between the ages of sixty and seventy, in the same highly peculiar comatose state. An instance has already been given of supernumerary digits appearing in four children out of six in a previously unaffected family. Dr. Devay states (12/35. 'Du Danger des Mariages Consanguins' 2nd edition 1862 page 103.) that two brothers married two sisters, their first-cousins, none of the four nor any relation being an albino; but the seven children produced from this double marriage were all perfect albinoes. Some of these cases, as Mr. Sedgwick (12/36. 'British and Foreign Medico-Chirurg. Review' July 1863 pages 183, 189.) has shown, are probably the result of reversion to a remote ancestor, of whom no record had been preserved; and all these cases are so far directly connected with inheritance that no doubt the children inherited a similar constitution from their parents, and, from being exposed to nearly similar conditions of life, {{it is not surprising that}} they should be affected in the same manner and at the same period of life.|$|R
40|$|In the {{attempts}} for rational development of {{agriculture in the}} arid and emi-arid regions of the world, knowledge regarding suitable means for rapidly evaluating climate and moisture adequacies is important and crucial. Effective estimation of the climatic water balance over large areas depends {{to a large extent}} on the available supply and the ability to estimate potential demand. Mean monthly rainfall provides a general idea about the available supply. Hargreaves (1977) indicated that 75 % probability of rainfall occurrence is a much more reliable indication of moisture available for crop production than mean precipitation. With this idea, we are listing the dependable precipitation (DP) amounts for all the locations. Dependable precipitation is defined as the precipitation that has a <b>specified</b> probability of <b>occurrence</b> (in this case at least 3 to 4 years) based on an analysis of longtime precipitation records. Potential evapotranspiration (PE) gives an idea regarding the potential demand for water at a given location. It is defined as the amount of water transpired from an actively growing, short green plant cover (usually grass) with a full crop cover and a continuously adequate moisture supply. PE values were estimated for all the locations according to procedure described by Reddy and Virmani (1980) ...|$|R
40|$|The paper {{introduces}} a symbolic, discrete-event approach for online anomaly detection. The approach uses automata {{representations of the}} underlying physical process to make anomaly occurrence determination. Automata may represent a discrete-event formulation of {{the operation of the}} monitored system during both normal and abnormal conditions. Automata may also be constructed from generated symbol sequences associated with parametric variations of equipment. This collection of automata represents the symbolic behavior of the underlying physical process and {{can be used as a}} pattern for anomaly detection. Within the possible behavior, there is a special sub-behavior whose occurrence is required to detect. The special behavior may be <b>specified</b> by the <b>occurrence</b> of special events representing deviations of anomalous behaviors from the nominal behavior. These intermittent or non-persistent events or anomalies may occur repeatedly. An observation mask is then defined, characterizing the actual observation configuration available for collecting symbolic process data. The analysis task is to determine whether this observation configuration is capable of detecting the specified anomalies. The assessment is accomplished by evaluating several observability notions, such as detectability and diagnosability. To this end, polynomial-time, computationallyefficient verification algorithms have been developed. The synthesis of optimal observation masks can also be conducted to suggest an appropriate observation configuration guaranteeing the detection of anomalies and to construct associated monitoring agents for performing the specified on-line condition monitoring task. The proposed discrete-event approach and supporting techniques for anomaly detection via optimal symbolic observation of physical processes are briefly presented and illustrated with examples. ...|$|R
40|$|A {{theoretical}} framework and its practical implications for formulating and implementing model-based monitoring of discrete flow networks are discussed. Possible flows of items {{are described as}} discrete-event (DE) traces. Each trace defines the DE sequence(s) that are triggered when an entity follows a given flow-path, visiting tracking locations within the monitored system. To deal with alternative routing, creation of items, flow bifurcations and convergences are allowed. Given the set of possible discrete flows, a possible-behavior model-an interacting set of automata- is constructed, where each automaton models the item discrete flow at each tracking location. In this model, which assumes total observability, event labels or symbols contain all the information required to unambiguously distinguish each discrete movement. Within the possible behavior, {{there is a special}} sub-behavior whose occurrence is required to be detected. The special behavior may be <b>specified</b> by the <b>occurrence</b> of routing events, such as faults or route violations, for example. These intermittent or non-persistent events may occur repeatedly. An observation mask is then defined, characterizing the observation configuration available for collecting item tracking data. The verification task is to determine whether this observation configuration is capable of detecting the identified special behavior. The assessment is accomplished by evaluating several observability notions, such as detectability and diagnosibility. If the corresponding property is satisfied, associated formal observers are constructed to perform the monitoring task at hand. The synthesis of observation masks may also be conducted to suggest optimal observation configurations (specifying number, type, and tracking locations of observation devices) guaranteeing the detection of the special events and to construc...|$|R
40|$|One of {{the most}} {{pervasive}} needs within the Deep Space Network (DSN) Metric Prediction Generator (MPG) view period event generation is that of finding solutions to given occurrence conditions. While the general form of an equation expresses equivalence between its left-hand and right-hand expressions, the traditional treatment of the subject subtracts the two sides, leaving {{an expression of the}} form Integral of(x) = 0. Values of the independent variable x satisfying this condition are roots, or solutions. Generally speaking, there may be no solutions, a unique solution, multiple solutions, or a continuum of solutions to a given equation. In particular, all view period events are modeled as zero crossings of various metrics; for example, the time at which the elevation of a spacecraft reaches its maximum value, as viewed from a Deep Space Station (DSS), is found by locating that point at which the derivative of the elevation function becomes zero. Moreover, each event type may have several occurrences within a given time interval of interest. For example, a spacecraft in a low Moon orbit will experience several possible occultations per day, each of which must be located in time. The MPG is charged with finding all <b>specified</b> event <b>occurrences</b> that take place within a given time interval (or pass), without any special clues from operators as to when they may occur, for the entire spectrum of missions undertaken by the DSN. For each event type, the event metric function is a known form that can be computed for any instant within the interval. A method has been created for a mathematical root finder to be capable of finding all roots of an arbitrary continuous function, within a given interval, to be subject to very lenient, parameterized assumptions. One assumption is that adjacent roots are separated at least by a given amount, xGuard. Any point whose function value is less than ef in magnitude {{is considered to be a}} root, and the function values at distances xGuard away from a root are larger than ef, unless there is another root located in this vicinity. A root is considered found if, during iteration, two root candidates differ by less than a pre-specified ex, and the optimum cubic polynomial matching the function at the end and at two interval points (that is within a relative error fraction L at its midpoint) is reliable in indicating whether the function has extrema within the interval. The robustness of this method depends solely on choosing these four parameters that control the search. The roots of discontinuous functions were also found, but at degraded performance...|$|R
