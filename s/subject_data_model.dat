0|10000|Public
50|$|Once the {{clinical}} data has been harmonized into a unified <b>subject</b> centric <b>data</b> <b>model,</b> {{it can be}} transformed to CDISC compliant data sets. This means that {{the clinical}} operations teams at the Bio-Pharma companies can work with standardized data all through {{the life of the}} clinical trial than at the very end. Hence a reduced change of FDA rejection and faulty analysis.|$|R
40|$|The present book's <b>subject</b> is multidimensional <b>data</b> <b>models</b> and <b>data</b> <b>modeling</b> {{concepts}} {{as they are}} applied in real data warehouses. The book aims to present the most important concepts within this subject in a precise and understandable manner. The book's coverage of fundamental concepts includes data cubes and their elements, such as dimensions, facts, and measures and their representation in a relational setting; it includes architecture-related concepts; and it includes the querying of multidimensional databases. The book also covers advanced multidimensional concepts that are considered to...|$|R
40|$|In {{this paper}} {{we examine the}} role played by working memory demands in {{determining}} problem difficulty during the solution of Tower of Hanoi Problem isomorphs. We do so by describing a production system model that accounts for subjects' performance on these problems via a dynamic analysis of the memory load imposed by the problem and of changes in that load during the problem solving episode. We also present the results of detailed testing of the <b>model</b> against human <b>subject</b> <b>data.</b> The <b>model</b> uses a highly constrained working memory to account for a number of features of the problem solving behavior, including the dichotomous (exploratory and final path) nature of the problem solving, the relative difficulty of the problems, the particular moves made in each state of the problem space, and the temporal patterning of the final path moves...|$|R
40|$|This paper {{reports on}} {{the second part of}} an {{initiative}} of the authors on researching classification systems with the conceptual model defined by the Functional Requirements for <b>Subject</b> Authority <b>Data</b> (FRSAD) final report. In an earlier study, the authors explored whether the FRSAD conceptual model could be extended beyond <b>subject</b> authority <b>data</b> to <b>model</b> classification <b>data.</b> The focus of the current study is to determine if classification <b>data</b> <b>modeled</b> using FRSAD can be used to solve real-world discovery problems in multicultural and multilingual contexts. The paper discusses the relationships between entities (same type or different types) in the context of classification systems that involve multiple translations and /or multicultural implementations. Results of two case studies are presented in detail: (a) two instances of the DDC (DDC 22 in English, and the Swedish-English mixed translation of DDC 22), and (b) Chinese Library Classification. The use cases of conceptual models in practice are also discussed...|$|R
30|$|Preferred {{model can}} be {{affected}} by several factors such as in vivo kinetics of the VOI (underlying receptor density), subject status, input function (particularly parent fraction and metabolites), tracer free fraction, motion, and even the scan duration. It {{should be noted that}} a description of tracer kinetics and the identification of the preferred kinetic model using model selection criteria {{is only one of the}} many evaluations needed to identify the optimal model. For example, in order to determine the optimal kinetic model, several other datasets and studies are required such as test-retest data, data for both healthy controls and diseased <b>subjects,</b> <b>data</b> correlating <b>model</b> results with pathology, and/or outcome (disease duration, cognitive scores, or survival). Also, the intended application, e.g. differential diagnosis or response assessment, may or may not allow for the use of simplified models or methods. Moreover, formulating a hypothesis on the preferred kinetic model based on physiological properties of the tracer, e.g. derived from preclinical studies, is advised.|$|R
40|$|In this paper, a multi-driven {{approach}} to <b>data</b> <b>modeling</b> in <b>data</b> warehousing will be presented, which integrates three existing approaches normally used separately: goal-driven, user-driven and data-driven; and two approaches usually {{not used in}} data warehousing field: process-driven and technology-driven. Goal-driven approach produces subjects and KPI’s (Key Performance Indicators) of main business fields. User-driven approach produces analytical requirements represented by measures and dimensions of each subject. Process-driven approach propose improvements in business processes (by using and creating <b>subject</b> oriented enterprise <b>data</b> <b>model)</b> to satisfy the KPI’s, measures and dimensions identified in the previous approaches. Technology-driven approach is an enabler or an obstacle {{to be considered in}} a <b>data</b> warehouse <b>model.</b> Data-driven approach is a combination of the results of previous approaches and results in a <b>data</b> warehouse <b>model.</b> By using a multi-driven approach with five stages, a layered <b>data</b> warehouse <b>model</b> more aligned with business and individual needs can be obtained. This will be illustrated by using examples of a case study...|$|R
30|$|According to Edwards (2010) (Preface, page xiii) without models {{there are}} no data, meaning that {{whatever}} data – such as observations or climate projections – there are, they exist because of processing through “data models”. Direct climate observations are only available at certain points, i.e., measurement stations, and to derive continuous data {{in space and time}} these data are processed using models. Additionally, climate models are “data laden”, i.e., they are sensitive to calibration procedures. These depend {{on the quality of the}} <b>data</b> (measured, then <b>subjected</b> to a <b>data</b> <b>model)</b> to which the calibration is performed (Parker, 2011). The shifting composition of the input is one source of uncertainty.|$|R
40|$|Modeling is a {{powerful}} tool for managing complexity in problem solving. Problem solvers usually build a simplified model of the real world and then use it to generate a solution to their problem. To date, however, little is known about how people actually behave when building a model. This study concentrates on <b>data</b> <b>modeling,</b> which involves the representation of different types of data and their interrelationships. It reports on two laboratory studies, in which <b>subjects</b> engage in <b>data</b> <b>modeling</b> to solve a complex problem. Using a think-aloud process-tracing methodology, we examine the <b>data</b> <b>modeling</b> behavior as a set of activities that are managed by several heuristics. We found that some heuristics were effective in reducing the complexity of the problem. An important aspect we observed was how subjects moved across levels of abstraction in the problem representation. Overall, these observations help to explain how people deal with complexity in <b>data</b> <b>modeling.</b> They also suggest that it may be advantageous to design systems that support work at various levels of abstraction and support transitions among those levels. problem solving, <b>data</b> <b>modeling,</b> cognitive processes, process tracing, heuristics, levels of abstraction...|$|R
40|$|Cardiorespiratory Models. (Under the {{direction}} of Professor C. T. Kelley). The parameter identification problem attempts to find parameter values that cause the solution of a predictive <b>model</b> to match <b>data.</b> In this work, parameters in cardiovascular and respiratory models are identified. This work’s main contribution is in its application of gradient based optimization techniques and insight into methods to identify parameters that can be estimated given <b>subject</b> specific <b>data.</b> The <b>models</b> {{presented in this paper}} are lumped compartment models of the cardiovascular and respiratory systems. Lumped compartment models treat the cardiovascular and respiratory systems as collections of interconnected compartments transporting blood and exchanging oxygen and carbon dioxide. Using these compartments, a system of ordinary differential equations (ODE) is generated that incorporates several physiological parameters representing vascular resistances, compliances, and tissue metabolic rates. The solution to this ODE system is used to predict cerebral blood flow, systemic arterial blood pressure, and expired carbon dioxide partial pressures, which are then compared to <b>subject</b> <b>data...</b>|$|R
40|$|The {{ability to}} {{identify}} a target texture in a visual display is a basic capability of the human visual system. Traditionally, as with much psychophysical modeling in the cognitive sciences, models of texture discrimination have been fit to individual <b>subject</b> <b>data.</b> By estimating <b>model</b> parameters independently, this approach emphasizes individual differences, and does not model the similarities between subjects. We consider alternative assumptions about individual differences in texture discrimination, using a standard model and previously studied data. In particular, we show how a hierarchical Bayesian approach can capture both {{the similarities and differences}} over subjects in a theoretically satisfying way. We also show that the hierarchical Bayesian approach has a number of methodological advantages over existing analyses, including improving parameter estimation when data are either sparse or missing, and improving model predictions when generalizing to new or different stimuli...|$|R
40|$|International audienceNeurosciences are {{progressively}} {{moving into}} a mass computational intensive era with the fusion of numerous large heterogeneous data sets from cellular to system level. To process and share this mass of information in a consistent and computational amenable form, computerized techniques – among them, ontologies - are currently designed to store, analyze and access this information. Recently, we proposed a multi-layered and multi-components ontology to deal with MR images and regions-of-interest that can be represented onto the images. In the present paper, we extend our initial ontology by adding a core ontology of <b>subject</b> <b>data</b> acquisition instruments <b>modeling</b> neuroclinical, neuropsychological and neurobehavioral tests used for neurological, behavioral and cognitive skills assessment. This ontology deals with instruments per se as specific artefacts, their variables and measured scores, and actions performed using instruments. In the paper, we underline the major aspects of our approach and emphasize its potential interest as a semantic reference for various neuroscience applications...|$|R
40|$|This paper {{presents}} a method {{with a combination}} of diverse approaches for requirement elicitation of a Data Warehouse (DW) schema model. This method integrates five approaches: goal-driven, user-driven, process driven, technology-driven, and data driven. This method includes two approaches not usually used in the DW field: process-driven and technology driven. The role of each approach is: goal-driven produces subjects (DW processes) and KPI’s (Key Performance Indicators) of main business fields; user-driven produces analytical requirements represented by measures and dimensions of each subject; process-driven propose improvements in business processes (by using and creating <b>subject</b> oriented enterprise <b>data</b> <b>model)</b> to satisfy KPI’s, measures and dimensions identified in the previous approaches; technology driven is an enabler or an obstacle to be considered in a DW; and data-driven is a combination of the results of previous approaches and results in a DW schema. By using a method with five approaches, a layered DW schema aligned with business and individual needs can be obtained. This will be illustrated by using examples. This work was financed by FCT - Fundação para a Ciência e Tecnologia for the project: PEst-OE/EEI/UI 0319 / 201...|$|R
40|$|To {{evaluate}} the possible role of posttranscriptional mechanisms in the acute phase response, we determined the kinetics of transcription (by nuclear run-on assay) and mRNA accumulation of five human acute phase genes in Hep 3 B cells incubated with conditioned medium from LPS-stimulated monocytes. Increase in mRNA accumulation was comparable {{to increase in}} transcription rate for fibrinogen-alpha and alpha- 1 protease inhibitor, suggesting largely transcriptional regulation. In contrast, mRNA accumulation was about 10 - 20 -fold greater than transcriptional increase for serum amyloid A, C 3, and factor B, suggesting participation of posttranscriptional mechanisms. Since finding a disparity between the magnitudes of increase in mRNA and transcription does not definitively establish involvement of posttranscriptional mechanisms, we <b>subjected</b> our <b>data</b> to <b>modeling</b> studies and dynamic mathematical analysis to evaluate this possibility more rigorously. In modeling studies, accumulation curves resembling those observed for these three mRNAs could be generated from the nuclear run-on results only if posttranscriptional regulation was assumed. Dynamic mathematical analysis of relative transcription rates and relative mRNA abundance also strongly supported participation of posttranscriptional mechanisms. These observations suggest that posttranscriptional regulation plays a substantial role in induction of some, but not all acute phase proteins...|$|R
40|$|This paper {{extends the}} {{subjects}} dicussed in the Data Analysis and Dynamical Systems courses {{by looking at}} the <b>subject</b> of <b>modelling</b> <b>data.</b> This task is nontrivial as the underlying process could be non-linear. In the paper some common methods, including global and local polynomial fitting, are discussed in terms of their applicability, level of computation and accuracy. One example method, Measure based Reconstruction, has been investigated in greater detail and experimentation is carried out to evaluate the method. In this project we shall be looking at the different ways one can model chaotic time series. The reason we {{are going to look at}} a range of methods is that different methods are "good" for different applications. As the "goodness" of a model is subjective to the task one wishes to do, we will investigate a selected models and compare the prediction to see how one goes about testing a model. Comment: 56 pages, MSc. Dissertation at University of Yor...|$|R
5000|$|<b>Data</b> <b>modelling</b> is {{the process}} of {{creating}} a <b>data</b> <b>model</b> by applying formal <b>data</b> <b>model</b> descriptions using <b>data</b> <b>modelling</b> techniques. <b>Data</b> <b>modelling</b> is a technique for defining business requirements for a database. It is sometimes called database <b>modelling</b> because a <b>data</b> <b>model</b> is eventually implemented in a database.|$|R
5000|$|... it was {{determined}} that current <b>data</b> <b>modeling</b> methods were imparting data isolation into every data architecture in the form of islands of disparate data and information silos. This data isolation is an unintended artifact of the <b>data</b> <b>modeling</b> methodology that results in the development of disparate <b>data</b> <b>models.</b> Disparate <b>data</b> <b>models,</b> when instantiated as databases, form disparate databases. Enhanced <b>data</b> <b>model</b> methodologies have been developed to eliminate the data isolation artifact and to promote the development of integrated <b>data</b> <b>models.</b> One enhanced <b>data</b> <b>modeling</b> method recasts <b>data</b> <b>models</b> by augmenting them with structural metadata in the form of standardized data entities. As a result of recasting multiple <b>data</b> <b>models,</b> the set of recast <b>data</b> <b>models</b> will now share one or more commonality relationships that relate the structural metadata now common to these <b>data</b> <b>models.</b> Commonality relationships are a peer-to-peer type of entity relationships that relate the standardized data entities of multiple <b>data</b> <b>models.</b> Multiple <b>data</b> <b>models</b> that contain the same standard data entity may participate in the same commonality relationship. When integrated <b>data</b> <b>models</b> are instantiated as databases and are properly populated from a common set of master data, then these databases are integrated.|$|R
40|$|Humans {{learn how}} to behave {{directly}} through environmental experience and indirectly through rules and instructions. Behavior analytic {{research has shown that}} instructions can control behavior, even when such behavior leads to sub-optimal outcomes (Hayes, S (Ed) 1989. Rule-governed behavior: cognition, contingencies, and instructional control. Plenum Press.). Here we examine the control of behavior through instructions in a reinforcement learning task known to depend on striatal dopaminergic function. Participants selected between probabilistically reinforced stimuli, and were (incorrectly) told that a specific stimulus had the highest (or lowest) reinforcement probability Despite experience to the contrary, instructions drove choice behavior. We present neural network simulations that capture the interactions between instruction-driven and reinforcement-driven behavior via two potential neural circuits one in which the striatum is inaccurately trained by instruction representations coming from prefrontal cortex/hippocampus (PFC/HC), and another in which the striatum learns the environmentally based reinforcement contingencies, but is "overridden" at decision output. Both models capture the core behavioral phenomena but, because they differ fundamentally on what is learned, make distinct predictions for subsequent behavioral and neuroimaging experiments. Finally, we attempt to distinguish between the proposed computational mechanisms governing instructed behavior by fitting a series of abstract "Q-learning" and Bayesian <b>models</b> to <b>subject</b> <b>data.</b> The best-fitting <b>model</b> supports one of the neural models, suggesting the existence of a "confirmation bias" in which the PFC/HC system trains the reinforcement system by amplifying outcomes that are consistent with instructions while diminishing inconsistent outcomes...|$|R
5000|$|A <b>data</b> <b>model</b> {{explicitly}} {{determines the}} structure of data. Typical applications of <b>data</b> <b>models</b> include database models, design of information systems, and enabling exchange of <b>data.</b> Usually <b>data</b> <b>models</b> are specified in a <b>data</b> <b>modeling</b> language.3 ...|$|R
5000|$|A <b>data</b> <b>model</b> instance, i.e. {{applying}} a <b>data</b> <b>model</b> theory {{to create a}} practical <b>data</b> <b>model</b> instance for some particular application.|$|R
40|$|This paper {{introduces}} a Conceptual <b>Data</b> <b>Model</b> for <b>Data</b> Warehouse including multidimensional aggregation. It {{is based on}} Entity-Relationships <b>data</b> <b>model.</b> The conceptual <b>data</b> <b>model</b> gracefully extends standard Entity-Relationship <b>data</b> <b>model</b> with multidimensional aggregated entities. The model has a clear mathematical theoretic semantics grounded on standard ER semantics and the GMD logic-based multidimensional <b>data</b> <b>model.</b> The aim of this work is not to propose yet another conceptual <b>data</b> <b>model,</b> but {{to find the most}} general and precise formalism considering all the proposals for a conceptual <b>data</b> <b>model</b> in the <b>data</b> warehouse field, making therefore a possible formal comparison of the differences of the models in the literature, and to study the formal properties or extensions of such <b>data</b> <b>models.</b> ...|$|R
50|$|The data view {{starts with}} the data classes which can be {{decomposed}} into <b>data</b> <b>subjects</b> which can be further decomposed into data entities. The basic <b>data</b> <b>model</b> type which is most commonly used is called merda (master entity relationship diagrams assessment, see entity-relationship model). The Class, subject and entity forms a hierarchical view of data. Enterprises may have millions of instances of data entities.|$|R
40|$|This {{thesis is}} divided into two {{thematic}} parts. The first one targets geodatabes design process. The second section of thesis contains application of graph algorithms over the final <b>data</b> <b>model.</b> The goal of the first section is to design a <b>data</b> <b>model.</b> In the first step of <b>data</b> <b>modeling</b> process all the available data information is organized at the conceptual <b>data</b> <b>model.</b> On grounds of conceptual <b>data</b> <b>model</b> is built-up logical <b>data</b> <b>model.</b> Logical scheme is transformed to physical design of personal geodatabase. Final <b>data</b> <b>model</b> design follows all <b>data</b> <b>modeling</b> rules through the <b>data</b> <b>model</b> process. The goal of second section of this thesis is to realize graph algorithms over the <b>data</b> <b>model.</b> The application of graph algorithms over the network contributes to efficiency of organization and management at the sphare of logistic system. These algorithms will apply through ESRI extension Network Analyst. Functionality of <b>data</b> <b>model</b> will be verified {{on the basis of this}} part. Keywords: Geodatabase design, Network analyse Supervisor: Mgr. Přemysl Štych, Ph. D...|$|R
50|$|A <b>data</b> <b>model</b> {{explicitly}} {{determines the}} structure of <b>data.</b> <b>Data</b> <b>models</b> are specified in a <b>data</b> <b>modeling</b> notation, which is often graphical in form.|$|R
5000|$|There {{are three}} {{different}} types of <b>data</b> <b>models</b> produced while progressing from requirements to the actual database {{to be used for}} the information system. [...] The data requirements are initially recorded as a conceptual <b>data</b> <b>model</b> which is essentially a set of technology independent specifications about the data and is used to discuss initial requirements with the business stakeholders. The conceptual model is then translated into a logical <b>data</b> <b>model,</b> which documents structures of the data that can be implemented in databases. Implementation of one conceptual <b>data</b> <b>model</b> may require multiple logical <b>data</b> <b>models.</b> The last step in <b>data</b> <b>modeling</b> is transforming the logical <b>data</b> <b>model</b> to a physical <b>data</b> <b>model</b> that organizes the data into tables, and accounts for access, performance and storage details. <b>Data</b> <b>modeling</b> defines not just data elements, but also their structures and the relationships between them.|$|R
50|$|<b>Data</b> <b>models</b> {{formally}} define <b>data</b> {{objects and}} relationships among data objects for a domain of interest. Some typical applications of <b>data</b> <b>models</b> include supporting {{the development of}} databases and enabling the exchange of data for a particular area of interest. <b>Data</b> <b>models</b> are specified in a <b>data</b> <b>modeling</b> language. EXPRESS is a <b>data</b> <b>modeling</b> language defined in ISO 10303-11, the EXPRESS Language Reference Manual.|$|R
50|$|Note, see Logical <b>data</b> <b>model</b> for {{discussion}} {{of the relationship of}} these three DIV <b>data</b> <b>models,</b> with comparison of the Conceptual, Logical & Physical <b>Data</b> <b>Models.</b>|$|R
40|$|This paper {{describes}} the conceptual integration and computer-based support of two important groups of conceptual <b>data</b> <b>models,</b> Entity Relationship Models and Object Role Models (e. g. NIAM). We perform conceptual integration using the conceptual <b>data</b> <b>modelling</b> language CoCoA to specify separate <b>data</b> <b>models</b> of individual notations. We then merge these into an integrated conceptual <b>data</b> <b>model</b> for both notations. These <b>data</b> <b>models</b> {{form the basis}} of the repository for an I-CASE tool supporting modelling with both notations, with full consistency management between the two notation <b>data</b> <b>models...</b>|$|R
50|$|A georelational <b>data</b> <b>model</b> is a {{geographic}} <b>data</b> <b>model</b> that represents geographic features as an interrelated set of spatial and attribute <b>data.</b> The georelational <b>model</b> {{is the fundamental}} <b>data</b> <b>model</b> used in coverages.|$|R
40|$|Clustering is the {{division}} of data into groups of similar objects. In clustering, some details aredisregarded in exchange for data simplification. Clustering {{can be viewed as}} a <b>data</b> <b>modeling</b> technique thatprovides for concise summaries of the data. Clustering is therefore related to many disciplines and plays animportant role in a broad range of applications. The applications of clustering usually deal with largedatasets and data with many attributes. Exploration of such <b>data</b> is a <b>subject</b> of <b>data</b> mining. This surveyconcentrates on clustering algorithms from a data mining perspective with K means Clustering algo...|$|R
40|$|Summary. Clustering is the {{division}} of data into groups of similar objects. It disregards some details in exchange for data simplification. Informally, clustering {{can be viewed as}} <b>data</b> <b>modeling</b> concisely summarizing the data, and, therefore, it relates to many disciplines from statistics to numerical analysis. Clustering {{plays an important role in}} a broad range of applications, from information retrieval to CRM. Such applications usually deal with large datasets and many attributes. Exploration of such <b>data</b> is a <b>subject</b> of <b>data</b> mining. This survey concentrates on clustering algorithms from a data mining perspective. ...|$|R
40|$|Abstract. Just as in {{many areas}} of {{software}} engineering, patterns have been used in <b>data</b> <b>modeling</b> to create high quality <b>data</b> <b>models.</b> We provide a concept of <b>data</b> <b>model</b> pattern based on Fully Communication Oriented Information Modeling (FCO-IM), a fact oriented <b>data</b> <b>modeling</b> method. A <b>data</b> <b>model</b> pattern is defined as the relation between context, problem, and solution. This definition is adopted from the concept of pattern by Christopher Alexander. We define the concept of Information Grammar for Pattern (IGP) in the solution part of a pattern, which works as a template to create a <b>data</b> <b>model.</b> The IGP also shows how a pattern can relate to other patterns. The <b>data</b> <b>model</b> pattern concept is then used to describe 15 <b>data</b> <b>model</b> patterns, organized into 4 categories. A case study on geographical location is provided to show the use of the concept in a real case. Keywords: context; <b>data</b> modeling; <b>data</b> <b>model</b> pattern; FCO-IM; Information Grammar for Pattern; problem; solution...|$|R
40|$|AbstractBusiness process {{management}} (BPM) is becoming popular in business, {{and the business}} process modelling {{is a way of}} representing an organisation to enable its analysis and improvement. A business-friendly modelling is very helpful for business people, and also can act as a communication tool between them and technical IT people. This paper focuses on a new <b>data</b> <b>model,</b> called Source-Transaction-Agent (STA) <b>data</b> <b>model,</b> as a modelling technique for business process <b>modelling.</b> STA <b>data</b> <b>model</b> uses business metadata to assist business and IT person to communicate and participate effectively and efficiently in business <b>data</b> <b>modelling</b> of a system development process. The STA <b>data</b> <b>model</b> uses relational database concept and semantic <b>data</b> <b>modelling,</b> developed by combining Resource-Event-Agent (REA) <b>data</b> <b>model</b> and form-based approach. Entity Relationship Diagram (ERD) is used as the benchmark for the STA effectiveness evaluation. The results show that the STA <b>data</b> <b>model</b> is an effective <b>data</b> <b>model</b> technique for business process modelling...|$|R
30|$|For {{modeling}} IS, we generalize {{the concept}} of <b>data</b> <b>models.</b> <b>Data</b> <b>models</b> consist of collections (of data) so that each collection has gotten a name. The collections are set of data or multi-set (bag) of data of data types with well-defined properties and structure; the most typical representation of <b>data</b> <b>model</b> is either relational <b>data</b> <b>model</b> or object-relational <b>data</b> <b>model.</b> The instances of data types make up finite subsets of potential dataset.|$|R
50|$|This {{article is}} a {{comparison}} of <b>data</b> <b>modeling</b> tools which are notable, including standalone, conventional <b>data</b> <b>modeling</b> tools and <b>modeling</b> tools supporting <b>data</b> <b>modeling</b> {{as part of a}} larger modeling environment.|$|R
40|$|There {{are many}} <b>data</b> <b>modelling</b> {{languages}} used in today’s information systems engineering environment. Some of the <b>data</b> <b>modelling</b> languages used {{have a degree}} of hype surrounding their quality and applicability. We would like to understand exactly what makes some <b>data</b> <b>modelling</b> languages successful and in some way suggest how useful <b>data</b> <b>modelling</b> languages {{will be in the}} context of an organisation and why. We are also interested in a theory capable of unifying the disparate range of languages. To do these things we select a theory based on ontology using which <b>data</b> <b>modelling</b> languages can be investigated. In this context theory should allow us to understand, compare, evaluate, and strengthen <b>data</b> <b>modelling</b> languages. The theory may also be used to suggest how useful various <b>data</b> <b>modelling</b> languages may be in an organisational setting. In this paper we present Chisholm’s ontology which we use to investigate <b>data</b> <b>modelling</b> languages. We show how Chisholm’s ontology can be used as a unifying theory of <b>data</b> <b>models,</b> develop methods for comparing <b>data</b> <b>modelling</b> languages based on this theory and summarise our findings. In conclusion, we evaluate the methods and the theory and examine avenues for future research. In this paper we present a deeper understanding of method together with analysis of new <b>data</b> <b>modelling</b> languages...|$|R
40|$|In this paper, {{we present}} a multi-driven {{approach}} to <b>data</b> <b>modeling</b> in <b>data</b> warehousing which integrates three existing approaches normally used separately: goal-driven, user-driven and data-driven; and two approaches usually not used in data warehousing field: process-driven and technology-driven. Goal-driven approach produces subjects and KPIs (Key Performance Indicators) of main business fields. User-driven approach produces analytical requirements represented by measures and dimensions of each subject. Process-driven approach propose improvements in business processes (by using and creating <b>subject</b> oriented enterprise <b>data</b> schema) to satisfy the KPIs, measures and dimensions identified in the previous approaches. Technology-driven approach is an enabler or an obstacle to be considered when we <b>model</b> a <b>data</b> warehouse <b>data</b> <b>model.</b> Data-driven approach {{is a combination of}} the results of previous approaches and results in a <b>data</b> <b>model</b> of a <b>data</b> warehouse. By using a multi driven approach with five stages, we can get a layered <b>data</b> warehouse <b>model</b> more aligned with organizational and individual needs. This will be illustrated by using examples of our case study...|$|R
