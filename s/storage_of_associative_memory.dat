1|10000|Public
40|$|Strong {{evidence}} suggests that the macaque monkey perirhinal cortex is involved in both the initial formation as well as the long-term <b>storage</b> <b>of</b> <b>associative</b> <b>memory.</b> To examine the neurophysiological basis of associative memory formation in this area, we recorded neural activity in this region as monkeys learned new conditional-motor associations. We report that a population of perirhinal neurons signal newly learned associations by changing their firing rate correlated with the animal's behavioral learning curve. Individual perirhinal neurons signal learning of one or more associations concurrently and these neural changes could occur before, at the same time, or after behavioral learning was expressed. We also compared the associative learning signals in the perirhinal cortex to our previous findings in the hippocampus. We report global similarities in both the learning-related and task-related activity seen across these areas as well as clear differences in the within and across trial timing and relative proportion of different subtypes of learning-related signals. Taken together, these findings emphasize the important role of the perirhinal cortex in new associative learning and suggest that the perirhinal cortex together with the hippocampus contribute importantly to conditional-motor associative memory formation...|$|E
40|$|We {{study the}} {{probabilistic}} generative models parameterized by feedforward neural networks. An attractor dynamics for probabilistic inference in these models {{is derived from}} a mean field approximation for large, layered sigmoidal networks. Fixed points of the dynamics correspond to solutions of the mean field equations, which relate the statistics of each unit to those of its Markov blanket. We establish global convergence of the dynamics by providing a Lyapunov function and show that the dynamics generate the signals required for unsupervised learning. Our results for feedforward networks provide a counterpart to those of Cohen-Grossberg and Hopfield for symmetric networks. 1 Introduction Attractor neural networks lend a computational purpose to continuous dynamical systems. Celebrated uses of these networks include the <b>storage</b> <b>of</b> <b>associative</b> <b>memories</b> (Amit, 1989), the reconstruction of noisy images (Koch et al, 1986), {{and the search for}} shortest paths in the traveling salesman proble [...] ...|$|R
40|$|Quantum {{information}} processing in neural structures {{results in an}} exponential increase <b>of</b> patterns <b>storage</b> capacity and can explain the extensive memorization and inferencing capabilities of humans. An example {{can be found in}} neural <b>associative</b> <b>memories</b> if the synaptic weights are taken to be fuzzy variables. In that case, the weights' update is carried out {{with the use of a}} fuzzy learning algorithm which satisfies basic postulates of quantum mechanics. The resulting weight matrix can be decomposed into a superposition <b>of</b> <b>associative</b> <b>memories.</b> Thus, the fundamental memory patterns (attractors) can be mapped into different vector spaces which are related to each other via unitary rotations. Quantum learning increases the <b>storage</b> capacity <b>of</b> <b>associative</b> <b>memories</b> by a factor of 2 (N), where N is the number of neurons. (C) 2006 Elsevier B. V. All rights reserved...|$|R
40|$|Recent {{studies using}} {{transgenic}} mice lacking NMDA receptors in the hippocampus challenge the long-standing hypothesis that hippocampal long-term potentiation-like mechanisms underlie the encoding and <b>storage</b> <b>of</b> <b>associative</b> long-term spatial <b>memories.</b> However, {{it may not}} be the synaptic plasticity-dependent memory hypothesis that is wrong; instead, it may be the role of the hippocampus that needs to be re-examined. We present an account of hippocampal function that explains its role in both memory and anxiety...|$|R
40|$|The <b>storage</b> {{capacity}} <b>of</b> holographic <b>associative</b> <b>memories</b> is estimated. An argument {{based on}} the available degrees of freedom shows {{that the number of}} patterns that can be stored is limited by the space-bandwidth product of the hologram divided by the number of pixels in each pattern. A statistical calculation shows that if we attempt to store associations by multiply exposing the hologram, the cross talk among the stored items severely degrades the output fidelity. This confirms the storage capacity predicted by the degrees-of-freedom argument...|$|R
40|$|Synaptic {{changes at}} sensory inputs to the dorsal {{nucleus of the}} lateral {{amygdala}} (LAd) {{play a key role}} in the acquisition and <b>storage</b> <b>of</b> <b>associative</b> fear <b>memory.</b> However, neither the temporal nor spatial architecture of the LAd network response to sensory signals is understood. We developed a method for the elucidation of network behavior. Using this approach, temporally patterned polysynaptic recurrent network responses were found in LAd (intra-LA), both in vitro and in vivo, in response to activation of thalamic sensory afferents. Potentiation of thalamic afferents resulted in a depression of intra-LA synaptic activity, indicating a homeostatic response to changes in synaptic strength within the LAd network. Additionally, the latencies of thalamic afferent triggered recurrent network activity within the LAd overlap with known later occurring cortical afferent latencies. Thus, this recurrent network may facilitate temporal coincidence of sensory afferents within LAd during associative learning...|$|R
40|$|Neuroligin- 1 is {{a potent}} trigger for the de novo {{formation}} of synaptic connections, and it has recently been suggested that it {{is required for the}} maturation of functionally competent excitatory synapses. Despite evidence for the role of neuroligin- 1 in specifying excitatory synapses, the underlying molecular mechanisms and physiological consequences that neuroligin- 1 may have at mature synapses of normal adult animals remain unknown. By silencing endogenous neuroligin- 1 acutely in the amygdala of live behaving animals, we have found that neuroligin- 1 is required for the <b>storage</b> <b>of</b> <b>associative</b> fear <b>memory.</b> Subsequent cellular physiological studies showed that suppression of neuroligin- 1 reduces NMDA receptor-mediated currents and prevents the expression of long-term potentiation without affecting basal synaptic connectivity at the thalamo-amygdala pathway. These results indicate that persistent expression of neuroligin- 1 is required for the maintenance of NMDAR-mediated synaptic transmission, which enables normal development of synaptic plasticity and long-term memory in the amygdala of adult animals...|$|R
50|$|The Center for Neural Informatics, Neural Structures, and Neural Plasticity (CN3) pursues {{fundamental}} {{breakthroughs in}} neuroscience by fostering neuroinformatic and computational approaches to neuroplasticity and neuroanatomy. By bringing together faculty expertise in these multiple disciplines, the Center provides opportunities for cross-training in neuroscience, psychology, and engineering, {{both at the}} graduate and postdoctoral levels. CN3 researchers investigate the relationship between brain structure, activity, and function from the subcellular to the network level, with a specific focus on the biophysical and biochemical mechanisms of learning and memory. In the long term, we seek to create large-scale, biologically plausible network models of entire portions of the mammalian brain, such as the hippocampus, to understand the neural circuits and cellular events underlying the expression, <b>storage,</b> and retrieval <b>of</b> <b>associative</b> <b>memory.</b>|$|R
40|$|We are {{applying}} genetic algorithms to fully connected {{neural network model}} <b>of</b> <b>associative</b> <b>memory,</b> We reported elsewhere that random weight matrix evolves to store some number of patterns only {{by means of a}} Genetic Algorithm. And we also reported the Genetic Algorithm can enlarge <b>storage</b> capacity <b>of</b> Hebb-rule <b>associative</b> <b>memory.</b> In those two reports, however, we did not mention about the basin of attraction. In this paper, we report concerning the basin of attraction of the networks obtained in those two experiments above. I. Introduction <b>Associative</b> <b>memory</b> is a process in which we can store information by distributing it among neurons. One of the most appealing features is its capability to tolerate noisy and/or partial input in recalling the information, though it has a limited capacity. We are thinking of the possibility {{to use it as a}} memory organ of digital organisms of our Artificial Life research in the future. However, as a preliminary step, we are now investigating basic behav [...] ...|$|R
40|$|A complex <b>associative</b> <b>memory</b> {{neural network}} (CAMN 2) model is {{proposed}} for the recognition of handwritten characters. The input and the stored patterns here are derived from the complex valued representation of the boundary of the characters. The stored vector representation is formulated based on 1 -D representation of an optical pattern recogni-tion filter. Retrieval of stored patterns from a noisy and shifted input is accomplished by using the cor-relation in the inverse fourier domain. An adaptive thresholding scheme is then applied to obtain a 1 -step convergence. The number of convergence of patterns, usually measured as the <b>storage</b> capacity <b>of</b> the <b>associative</b> <b>memory</b> is found to increase sig-nificantly. But the major advantage obtained from the complex representation is that the recognition of patterns is invariant to translation, rotation and scaling of the input patterns. ...|$|R
40|$|The <b>storage</b> {{capacity}} <b>of</b> holographic <b>associative</b> <b>memories</b> is estimated. An argument {{based on}} the available degrees of freedom shows {{that the number of}} patterns that can be stored is limited by the space-bandwidth product of the hologram divided by the number of pixels in each pattern. A statistical calculation shows that if we attempt to store associations by multiply exposing the hologram, the cross talk among the stored items severely degrades the output fidelity. This confirms the storage capacity predicted by the degrees-of-freedom argument. An <b>associative</b> <b>memory</b> internally stores a set of distinct output signal vectors gm, i = 1, 2, [...] ., Min a oneto-one association with a second set of stimulus signals fmn {{in such a way as}} to make selective recall possible. That is, a signal g 9 is recalled by presenting its associated stimulus, f 4, as the input. Such a memory is sometimes described as being heteroassociative, of which autoassociative schemes in which the stimulus and stored signals are the same form a special case. The analogy between <b>associative</b> <b>memories</b> and holograph...|$|R
40|$|Short {{summary of}} the PhD thesis: The main focus {{of this study was}} to {{investigate}} the fundamental processes of learning and memory, with the aim of understanding the molecular mechanisms involved. It is believed that memories form in synapses, i. e. in the connections formed between neurons in the brain. So far, a large number of molecules that can modulate the strength of synaptic connections have been shown to be involved in learning and memory processes. In the current thesis, we aimed to investigate the role of two recently discovered, cAMP signalling avenues, A-kinase anchoring proteins (AKAPs) and exchange protein activated by cAMP (Epac), in memory processes. We could show that two members of the AKAP family, AKAP 150 and mAKAP as well exchange protein activated by cAMP variant 2 (Epac 2) are widely expressed in the mouse brain. Moreover, we could show that both AKAP 150 and Epac 2 play specific roles in the molecular machinery of learning and memory. Particularly, AKAP 150 appeared to be important in consolidation (or <b>storage)</b> <b>of</b> emotional <b>associative</b> <b>memory</b> and in novelty processing, whereas Epac activation improved emotional memory retrieval, or memory recollection. Moreover, our data show that both AKAPs and Epac might mediate a more fine-tuned level of organization for cAMP second messenger systems. Therefore, our results yield important insights in the role of AKAPs, compartmentalized cAMP signalling and Epac in the different stages of learning and memory. In addition to the fundamental significance of understanding cAMP signalling cascades, our research opens up new routes for the development of new therapeutic strategies for memory-associated disorders and pathologies. ...|$|R
40|$|Optical {{techniques}} for performing two computing tasks are investigated. First, acousto-optical systems that implement adaptive filtering structures are presented for operation in environments {{that are not}} well characterized a priori or are time-varying. Theoretical analyses along with experimental confirmations are given to identify the important system parameters that affect the performance. Extensions of the systems to the multidimensional domain of phased array signal processing are discussed as well as novel implementations that use photorefractive crystals as time-integrating elements. Also investigated are various <b>associative</b> <b>memory</b> models. An acousto-optic implementation of the so-called Hopfield model is presented. The system's storage capacity and attraction radius are characterized experimentally and are shown to agree with computer simulations. Secondly, an upper bound is derived for the <b>storage</b> capacity <b>of</b> holographic <b>associative</b> <b>memories</b> that use planar holograms. It is shown that if the space bandwidth product of the hologram is N 2, then the holographic memory can store at most N 2 /N 3 associations, where N 3 {{is the number of}} pixels in each output item. Finally, <b>associative</b> <b>memories</b> whose performance is invariant with respect to shifts in the input pattern position are considered. It is shown that nonlinear interconnections are required to achieve shift invariant operation, and optical implementations are discussed...|$|R
40|$|The {{theory of}} {{artificial}} neural networks {{has been successfully}} applied {{to a wide variety}} of pattern recognition problems. In this theory, the first step in computing the next state of a neuron or in performing the next layer neural network computation involves the linear operation of multiplying neural values by their synaptic strengths and adding the results. A nonlinear activation function usually follows the linear operation in order to provide for nonlinearity of the network and set the next state of the neuron. In this paper we introduce a novel class of artificial neural networks, called morphological neural networks, in which the operations of multiplication and addition are replaced by addition and maximum (or minimum), respectively. By taking the maximum (or minimum) of sums instead of the sum of products, morphological network computation is nonlinear before possible application of a nonlinear activation function. As a consequence, the properties of morphological neural networks are drastically different than those of traditional neural network models. The main emphasis of the research presented here is on morphological <b>associative</b> <b>memories.</b> We examine the computing and <b>storage</b> capabilities <b>of</b> morphological <b>associative</b> <b>memories,</b> and discuss differences between morphological models and traditional semilinear models such as the Hopfield net. 1...|$|R
40|$|Hippocampal CA 3 {{is crucial}} for the {{formation}} <b>of</b> long-term <b>associative</b> <b>memory.</b> It has a heavily recurrent connectivity, and memories {{are thought to be}} stored as memory engrams in the CA 3. However, despite its importance for memory storage and retrieval, spiking network models of the CA 3 to date are relatively small-scale, and exist as only proof-of-concept models. Specifically, how neurogenesis in the dentate gyrus affects memory encoding and retrieval in the CA 3 is not studied in such spiking models. Our work is the first to develop a biologically plausible spiking neural network model of hippocampal memory encoding and retrieval, with at least an order-of-magnitude more neurons than previous models. It is also the first to investigate the effect of neurogenesis on CA 3 memory encoding and retrieval. Using such a model, we first show that a recently developed plasticity model {{is crucial for}} good encoding and retrieval. Next, we show how neural properties related to neurogenesis and neuronal death enhance <b>storage</b> and retrieval <b>of</b> <b>associative</b> <b>memories</b> in the CA 3. In particular, we show that without neurogenesis, increasing number of CA 3 neurons are recruited by each new memory stimulus, resulting in a corresponding increase in inhibition and poor memory retrieval as more memories are encoded. Neurogenesis, on the other hand, maintains the number of CA 3 neurons recruited per stimulus, and enables the retrieval of recent memories, while forgetting the older ones. Our model suggests that structural plasticity (provided by neurogenesis and apoptosis) is required in the hippocampus for memory encoding and retrieval when the network is overloaded; synaptic plasticity alone does not suffice. The above results are obtained from an exhaustive study in the different plasticity models and network parameters...|$|R
40|$|Distributed {{representations}} {{were often}} criticized as inappropriate for encoding of data with a complex structure. However Plate's Holographic Reduced Representations and Kanerva's Binary Spatter Codes are recent schemes that allow on-the-fly encoding of nested compositional structures by real-valued or dense binary vectors of fixed dimensionality. In this paper we consider {{procedures of the}} Context-Dependent Thinning which were developed for representation of complex hierarchical items in the architecture of Associative-Projective Neural Networks. These procedures provide binding of items represented by sparse binary codevectors (with low probability of 1 s). Such an encoding is biologically plausible and allows a high <b>storage</b> capacity <b>of</b> distributed <b>associative</b> <b>memory</b> where the codevectors may be stored. In contrast to known binding procedures, Context-Dependent Thinning preserves the same low density (or sparseness) of the bound codevector for varied number of component codevectors. Besides, a bound codevector is not only similar to another one with similar component codevectors (as in other schemes), {{but it is also}} similar to the component codevectors themselves. This allows the similarity of structures to be estimated just by the overlap of their codevectors, without retrieval of the component codevectors. This also allows an easy retrieval of the component codevectors. Examples of algorithmic and neural-network implementations of the thinning procedures are considered. We also present representation examples for various types of nested structured data (propositions using role-filler and predicate-arguments representation schemes, trees, directed acyclic graphs) using sparse codevectors of fixed dimension. Such representations may provide a fruitful alternative to the symbolic representations of traditional AI, {{as well as to the}} localist and microfeature-based connectionist representations...|$|R
40|$|Neuron models <b>of</b> <b>associative</b> <b>memory</b> {{provide a}} new and {{prospective}} technology for reliable date storage and patterns recognition. However, even when the patterns are uncorrelated, the efficiency of most known models <b>of</b> <b>associative</b> <b>memory</b> is low. We developed a new version <b>of</b> <b>associative</b> <b>memory</b> with record characteristics <b>of</b> its <b>storage</b> capacity and noise immunity, which, in addition, is effective when recognizing correlated patterns. ...|$|R
40|$|We apply genetic {{algorithms}} to Hopfield's {{neural network model}} <b>of</b> <b>associative</b> <b>memory.</b> Previously, using ternary chromosomes, we successfully evolved both random weight matrix and over-loaded Hebbian weight matrix to function as an <b>associative</b> <b>memory.</b> In this paper, we present a real-encoded genetic algorithm to evolve random synaptic weights to store some number <b>of</b> patterns as <b>associative</b> <b>memory.</b> The goal is to study how can we use the Hopfield model as a test suite for evolutionary computations. 1 Introduction In the field of Evolutionary computations, various kinds of test functions has been proposed (see, e. g., [1, 2, 3]). In this paper, we discuss the Hopfield model <b>of</b> <b>associative</b> <b>memory</b> as a more challenging test suite for the evolutionary computations. Hopfield model <b>of</b> <b>associative</b> <b>memory</b> has almost infinite number of combinations of synaptic weights which give the network a function <b>of</b> <b>associative</b> <b>memory</b> [4]. At the same time, there is a tradeoff between its storage capacity an [...] ...|$|R
40|$|We apply {{evolutionary}} computations to Hopfield 's {{neural network}} model <b>of</b> <b>associative</b> <b>memory.</b> In the Hopfield model, almost infinite number of combinations of synaptic weights give a network a function <b>of</b> <b>associative</b> <b>memory.</b> Furthermore, there is a trade-off between the storage capacity and size of basin of attraction. Therefore, the model {{can be thought of}} as a test suite of multi-modal and/or multi-objective function optimization. As preliminary stages, we investigate the basic behaviors <b>of</b> <b>associative</b> <b>memory</b> under simple evolutionary processes. In this paper, we present some experiments using an evolution strategy. I. Introduction There have been a lot of researches which apply evolutionary techniques to layered neural networks (see e. g., [1], [2], and references quoted therein). However, their applications to fully-connected recurrent neural networks remain few so far. In this paper we present some basic behaviors of the Hopfield model <b>of</b> <b>associative</b> <b>memory</b> under an evolution stra [...] ...|$|R
40|$|Abstract:- From the {{viewpoint}} of practical applications, an important ability of neur l networks is to organize representations of the external world through learning. There is, however, a problem that the representations of the neural networks are too complicated to estimate the capability of the assistant system in practical use. A geometrical method to analyze the representation <b>of</b> an <b>associative</b> <b>memory</b> is introduced. As a result, a practical application <b>of</b> <b>associative</b> <b>memory</b> models- information categorization- is presented. In this application, the concept formation ability <b>of</b> <b>associative</b> <b>memory</b> is important. Key-Words:- information filtering, categorization, <b>associative</b> <b>memory,</b> concept formation, geometrical method...|$|R
30|$|Next we give {{a second}} {{numerical}} example on application <b>of</b> <b>associative</b> <b>memories</b> based on CVNNs.|$|R
50|$|Bidirectional <b>associative</b> <b>memory</b> (BAM) {{is a type}} of {{recurrent}} neural network. BAM was introduced by Bart Kosko in 1988. There are two types <b>of</b> <b>associative</b> <b>memory,</b> auto-associative and hetero-associative. BAM is hetero-associative, meaning given a pattern it can return another pattern which is potentially of a different size. It is similar to the Hopfield network in that they are both forms <b>of</b> <b>associative</b> <b>memory.</b> However, Hopfield nets return patterns of the same size.|$|R
50|$|Theoretical work on SDM by Kanerva has {{suggested}} that sparse coding increases the capacity <b>of</b> <b>associative</b> <b>memory</b> by reducing overlap between representations. Experimentally, sparse representations of sensory information have been observed in many systems, including vision, audition, touch, and olfaction. However, despite the accumulating evidence for widespread sparse coding and theoretical arguments for its importance, a demonstration that sparse coding improves the stimulus-specificity <b>of</b> <b>associative</b> <b>memory</b> has been lacking until recently.|$|R
40|$|Cellular neural {{networks}} (CNNs) have locally connected neurons. This characteristic makes CNNs adequate for hardware implementation and, consequently, for their employment {{on a variety}} of applications as real-time image processing and construction <b>of</b> efficient <b>associative</b> <b>memories.</b> Adjustments <b>of</b> CNN parameters is a complex problem involved in the configuration <b>of</b> CNN for <b>associative</b> <b>memories.</b> This paper reviews methods <b>of</b> <b>associative</b> <b>memory</b> design based on CNNs, and provides comparative performance analysis of these approaches...|$|R
5000|$|Yasushi Miyashita - The Discovery <b>of</b> <b>Associative</b> <b>Memory</b> Neurons in the Cerebral Cortex and Studies of the Cognitive Memory System ...|$|R
40|$|The neural {{correlates}} <b>of</b> <b>associative</b> <b>memory</b> {{in healthy}} older adults were investigated {{by examining the}} correlation <b>of</b> <b>associative</b> <b>memory</b> performance with spontaneous brain oscillations. Eighty healthy older adults underwent a resting-state functional MRI and took a paired-associative learning test (PALT). Correlations between the amplitude of low-frequency fluctuations (ALFF) as well as fractional ALFF (fALFF) in the whole brain and PALT scores were calculated. We found that spontaneous activity as indexed by both ALFF and fALFF in the parahippocampal gyrus (PHG) was significantly positively correlated with <b>associative</b> <b>memory</b> performance, suggesting that the PHG plays {{a critical role in}} <b>associative</b> <b>memory</b> in older people...|$|R
40|$|This paper {{describes}} a parallel associative pro-cessor, IXM 2, developed mainly for semantic network processing. IXM 2 consists of 64 as-sociative processors and 9 network processors, having {{a total of}} 256 K words <b>of</b> <b>associative</b> <b>memory.</b> The large <b>associative</b> <b>memory</b> en-ables 65, 536 semantic network nodes to be pro-cessed in parallel and reduces the order of al-gorithmic complexity to O(1) in basic semantic net operations. We claim that intensive use <b>of</b> <b>associative</b> <b>memory</b> provides far superior per-formance in carrying out the basic operations necessary for semantic network processing: in-tersection, marker-propagation, and arithmetic operations. ...|$|R
40|$|We apply {{evolutionary}} computations to Hopfield 's {{neural network}} model <b>of</b> <b>associative</b> <b>memory.</b> In the model, some of the appropriate configurations of the synaptic weights give a network a function <b>of</b> <b>associative</b> <b>memory.</b> One <b>of</b> our goals is to obtain the distribution of these configurations in the synaptic weight space. For the purpose, we explore a fitness landscape defined on the weight space. Although {{there have been a}} fair amount of studies regarding the ruggedness of the fitness landscape, all of them are for discrete genes. In this paper, we apply these methods to our continuous genes which represent the synaptic weights of the Hopfield network. I. Introduction <b>Associative</b> <b>memory</b> is a dynamical system which has a number of stable states with a domain of attraction around them [1]. If the system starts at any state in the domain, it will converge to the stable state. In 1982, Hopfield [2] proposed a fully connected {{neural network model}} <b>of</b> <b>associative</b> <b>memory</b> in which information is [...] ...|$|R
40|$|We apply {{evolutionary}} computations to the Hopfield's {{neural network}} model <b>of</b> <b>associative</b> <b>memory.</b> In the model, some of the appropriate configurations of synaptic weights give the network a function <b>of</b> <b>associative</b> <b>memory.</b> One <b>of</b> our goals is to obtain the distribution of these configurations in the synaptic weight space. In other words, our aim is to learn a geometry of a fitness landscape defined on the space. For the purpose, we use evolutionary walks to explore the fitness landscape in this paper. 1 INTRODUCTION <b>Associative</b> <b>memory</b> is a dynamical system which {{has a number of}} stable states with a domain of attraction around them (Koml'os and Paturi 1988). If the system starts at any state in the domain, it will converge to the stable state. Hopfield (1982) proposed a fully connected {{neural network model}} <b>of</b> <b>associative</b> <b>memory</b> in which information is stored by being distributed among neurons. The dynamical behavior of its neuron states strongly depends on synaptic strengths between ne [...] ...|$|R
5000|$|Herz, R.S. (1998). [...] "Are odors {{the best}} cues to memory? A cross-modal {{comparison}} <b>of</b> <b>associative</b> <b>memory</b> stimuli." [...] Annals of the New York Academy Sciences, 855, 670-674.|$|R
40|$|Presented {{below is}} an {{interesting}} type <b>of</b> <b>associative</b> <b>memory</b> called toggle memory based {{on the concept of}} T flip flops, as opposed to D flip flops. Toggle memory supports both reversible programming and charge recovery. Circuits designed using the principles delineated below permit matchlines to charge and discharge with near zero energy dissipation. The resulting lethargy is compensated by the massive parallelism <b>of</b> <b>associative</b> <b>memory.</b> Simulation indicates over 33 x reduction in energy dissipation using a sinusoidal power supply at 2 MHz, assuming realistic 50 nm MOSFET models...|$|R
40|$|Recent {{studies of}} the {{statistical}} mechanics of neural network models <b>of</b> <b>associative</b> <b>memory</b> are reviewed. The paper discusses models which have an energy function but depart from the simple Hebb rule. This includes networks with static synaptic noise, dilute networks and synapses that are nonlinear functions of the Hebb ru 1 e (e. g., clipped networks). The properties of networks that ernp loy the pro jection method are reviewed. A. The HODfield i' 1 odel!'- 1 odelsof neural networks which exhibit features <b>of</b> <b>associative</b> <b>memory</b> {{have been the subject}} of intense theoreticalactivity. l_B Followin...|$|R
40|$|Studies {{attempting}} to improve episodic memory performance with strategy instructions and training have had limited success in older adults: their training gains {{are limited in}} comparison to those of younger adults and do not generalize to untrained tasks and contexts. This limited success has been partly attributed to age-related impairments in <b>associative</b> binding <b>of</b> information into coherent episodes. We therefore investigated potential training and transfer effects <b>of</b> process-based <b>associative</b> <b>memory</b> training (i. e., repeated practice). Thirty-nine older adults (Mage = 68. 8) underwent 6 weeks <b>of</b> either adaptive <b>associative</b> <b>memory</b> training or item recognition training. Both groups improved performance in item memory, spatial memory (object-context binding) and reasoning. A disproportionate effect <b>of</b> <b>associative</b> <b>memory</b> training was only observed for item memory, whereas no training-related performance changes were observed for <b>associative</b> <b>memory.</b> Self-reported strategies {{showed no signs of}} spontaneous development <b>of</b> memory-enhancing <b>associative</b> <b>memory</b> strategies. Hence, the results do not support the hypothesis that process-based <b>associative</b> <b>memory</b> training leads to higher <b>associative</b> <b>memory</b> performance in older adults...|$|R
50|$|The neuroanatomical {{structures}} that govern <b>associative</b> <b>memory</b> {{are found in}} the medial temporal lobe. The main locations are the hippocampus and its surrounding structures of the entorhinal, perirhinal, and parahippocampal cortices. Humans with large medial temporal lobe lesions have shown to have impairments in recognition memory for different types of stimuli. The hippocampus has also shown to be the main location for memory consolidation, especially related to episodic memory. The inputs from these unrelated stimuli are collected in this location and the actual synaptic connections are made and strengthened. Additionally, involvement from the prefrontal cortex, frontal motor areas, and the striatum has been shown in the formation <b>of</b> <b>associative</b> <b>memories.</b> <b>Associative</b> <b>memory</b> is not considered to be localized to a single circuit, with different types <b>of</b> subsets <b>of</b> <b>associative</b> <b>memory</b> utilizing different circuitry.|$|R
50|$|<b>Associative</b> <b>memory</b> becomes poorer {{in humans}} as they age. Additionally, {{it has been}} shown to be non-correlational with single item (non-associative) memory function. Transcranial direct-current {{stimulation}} has improved performance on <b>associative</b> <b>memory</b> tasks. Patients with Alzheimer's disease have been shown to be poorer in multiple forms <b>of</b> <b>associative</b> <b>memory.</b>|$|R
40|$|We apply some {{variants}} of evolutionary computations to the Hopfield model <b>of</b> <b>associative</b> <b>memory.</b> In this paper, {{we use the}} Breeder Genetic Algorithm (BGA) to explore the optimal set of synaptic weights {{with respect to the}} storage capacity. We present the BGA has tremendous ability to search a solution in the massively multi-modal landscape of the synaptic weight space. The main goal {{of this study is to}} shed new light on the analysis of the Hopfield model <b>of</b> <b>associative</b> <b>memory.</b> We also expect the model to be used as a new test function of evolutionary computations. 1 INTRODUCTION <b>Associative</b> <b>memory</b> is a dynamical system which has a number of stable states with a domain of attraction around them [1]. If the system starts at any state in the domain, it will converge to the stable state. In 1982, Hopfield [2] proposed a fully connected neural network model <b>of</b> <b>associative</b> <b>memory</b> in which we can store information by distributing it among neurons, and we can recall it from its noisy and/or p [...] ...|$|R
