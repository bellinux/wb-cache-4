1203|3366|Public
25|$|Critics {{also argued}} that Rind et al.'s {{statistical}} approach for controlling for family environment {{as a cause of}} maladjustment was conceptually and methodologically invalid. Spiegel stated that inferring the source of maladjustment from analyzing the shared variance between CSA and family environment does not answer the question of which variable explains maladjustment better; the authors answered that this statement shows a misunderstanding of the <b>statistical</b> <b>procedure</b> used their meta-analysis. Dallam, however, addressed the topic of several prior studies having found statistically significant relations between CSA and maladjustment even after controlling for family environment.|$|E
2500|$|A {{standard}} <b>statistical</b> <b>procedure</b> {{involves the}} test {{of the relationship between}} two statistical data sets, or a data set and synthetic data drawn from idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a [...] "false positive") and Type II errors (null hypothesis fails to be rejected and an actual difference between populations is missed giving a [...] "false negative"). Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.|$|E
2500|$|In a [...] {{analysis}} of their data, scientists of the OPERA collaboration reported evidence that neutrinos they produced at CERN in Geneva and recorded at the OPERA detector at Gran Sasso, Italy, had traveled faster than light. The neutrinos were calculated to have arrived approximately 60.7nanoseconds (60.7billionths of a second) sooner than light would have if traversing the same distance in a vacuum. After six months of cross checking, on , the researchers announced that neutrinos had been observed traveling at faster-than-light speed. Similar results were obtained using higher-energy (28GeV) neutrinos, which were observed to check if neutrinos' velocity depended on their energy. The particles were measured arriving at the detector faster than light by approximately one part per 40,000, with a 0.2-in-a-million chance of the result being a false positive, assuming the error were entirely due to random effects (significance of six sigma). This measure included estimates for both errors in measuring and errors from the <b>statistical</b> <b>procedure</b> used. It was, however, a measure of precision, not accuracy, which could be influenced by elements such as incorrect computations or wrong readouts of instruments. For particle physics experiments involving collision data, the standard for a discovery announcement is a five-sigma error limit, looser than the observed six-sigma limit.|$|E
40|$|We {{introduce}} several coordinate invariant <b>statistical</b> <b>procedures</b> {{in order}} to test for local alignment of polarizations. A large scale alignment of optical polarizations from distant QSOs has recently been observed by and collaborators. The new <b>statistical</b> <b>procedures</b> are based on comparing polarizations at different angular coordinates by making a parallel transport. The results of these <b>statistical</b> <b>procedures</b> continue to support the existence of the large scale alignment effect in the QSO optical polarization data. The alignment is found to be much more pronounced in the data sample with low degrees of polarization p< 2...|$|R
5000|$|Use <b>statistical</b> <b>procedures</b> {{to create}} an overall {{interview}} score ...|$|R
5000|$|Using {{appropriate}} <b>statistical</b> <b>procedures</b> {{to establish}} norms for the test.|$|R
2500|$|With Le génocide français, Reynald Secher's thesis for the Doctorat d'État {{began with}} a {{generalization}} of the standard arguments to the whole region. Although La Chapelle-Basse-Mer served him repeatedly as a reference point, Secher illustrated his arguments with wide citations from national and regional archives to establish a broader frame of reference. Furthermore, he drew on graphic, nineteenth century accounts widely known to the historians of the Vendee: Carrier drownings and the [...] "infernal columns of Turreau". Most importantly, however, Secher broke with conventional assessments by asserting {{on the basis of}} minimal evidence, Tilly claims, that the pre-revolutionary Vendée was more prosperous than the rest of France (to better emphasize the devastation of the war and the repression). He used dubious statistical methods to establish population losses and fatalities, statistical processes that inflated {{the number of people in}} the region, the number and value of houses, and the financial losses of the region. Secher's <b>statistical</b> <b>procedure</b> relied on three unjustifiable assumptions. First, Secher assumes a constant birth rate of about 37 per thousand of population, when actually, Tilly maintains, the population was declining. Second, Secher assumes no net migration; Tilly maintains that thousands fled the region, or at least shifted where they lived within the region. Finally, Secher understated the population present at the end of the conflict by ending it 1802, not 1794.|$|E
50|$|Peirce's {{criterion}} is a <b>statistical</b> <b>procedure</b> for eliminating outliers.|$|E
5000|$|... 4. Sort the {{questions}} into groups which are called clusters or factors {{by using a}} <b>statistical</b> <b>procedure</b> ...|$|E
5000|$|... #Caption: <b>Statistical</b> <b>procedures</b> to {{characterize}} the infection/infestation of a sample of hosts.|$|R
50|$|Some {{packages}} are developed for specific purposes (e.g., time series analysis, factor analysis, calculators for probability distributions, etc.), {{while others are}} general packages, {{with a variety of}} <b>statistical</b> <b>procedures.</b> Others are meta-packages or statistical computing environments, which allow the user to code completely new <b>statistical</b> <b>procedures.</b> This article is a review of the general statistical packages.|$|R
40|$|TX. The Ethical Use of Statistical 2 A {{large body}} of {{literature}} in psychological research and methodology points out common misuses of <b>statistical</b> <b>procedures</b> in published works. <b>Statistical</b> <b>procedures,</b> when incorrectly applied, result in false and misleading results which, in turn, lessen the ethical justification for a research project. The present paper attempts to provide researchers {{in the field of}} psychology ethical guidelines for the proper use of <b>statistical</b> <b>procedures.</b> First, the problems with the misuse of statistical analyses in current literature are outlined. Next, the ethical principles which apply are discussed. Finally, previously proposed solutions and guidelines for researchers are presented. The Ethical Use of Statistical...|$|R
50|$|Robust {{principal}} component analysis (RPCA) is a modification of the widely used <b>statistical</b> <b>procedure</b> {{principal component}} analysis (PCA) which works well with respect to grossly corrupted observations.|$|E
50|$|Statistical {{power is}} an {{important}} consideration when choosing what <b>statistical</b> <b>procedure</b> to use, but it isn’t the only important one. All statistical procedures permit researchers to make statistical errors {{and they are not}} all equal in their ability to control the rate of occurrence of several important types of statistical error. As Table 1 shows, statisticians can’t agree on how error rate ought to be defined, but particular attention has been traditionally paid to what are called 'type 1 errors' and whether or not a <b>statistical</b> <b>procedure</b> is susceptible to type 1 error rate inflation.|$|E
50|$|A {{separation}} test is a <b>statistical</b> <b>procedure</b> for early-phase research, {{to decide whether}} to pursue further research. It is designed to avoid the prevalent situation in early-phase research, when a statistically underpowered test gives a negative result.|$|E
5000|$|Probability {{theory and}} statistics: time-series analysis, nonparametrics, {{asymptotic}} <b>statistical</b> <b>procedures,</b> and computer-intensive <b>statistical</b> methods ...|$|R
40|$|In {{this paper}} we develop a {{methodology}} for defining stopping rules {{in a general}} class of global random search algorithms {{that are based on}} the use of <b>statistical</b> <b>procedures.</b> To build these stopping rules we reach a compromise between the expected increase in precision of the <b>statistical</b> <b>procedures</b> and the expected waiting time for this increase in precision to occur...|$|R
40|$|When {{researchers}} {{introduce a}} new test they have to demonstrate that it is valid, using unbiased designs and suitable <b>statistical</b> <b>procedures.</b> In this article we use Monte Carlo analyses to highlight how incorrect <b>statistical</b> <b>procedures</b> (i. e., stepwise regression, extreme scores analyses) or ignoring regression assumptions (e. g., heteroscedasticity) contribute to wrong validity estimates. Beyond these demonstrations, and as an example, we re-examined the results reported by Warwick, Nettelbeck, and Ward (2010) concerning {{the validity of the}} Ability Emotional Intelligence Measure (AEIM). Warwick et al. used the wrong <b>statistical</b> <b>procedures</b> to conclude that the AEIM was incrementally valid beyond intelligence and personality traits in predicting various outcomes. In our re-analysis, we found that the reliability-corrected multiple correlation of their measures with personality and intelligence was up to. 69. Using robust <b>statistical</b> <b>procedures</b> and appropriate controls, we also found that the AEIM did not predict incremental variance in GPA, stress, loneliness, or well-being, demonstrating the importance for testing validity instead of looking for it...|$|R
50|$|A decision-theoretic {{justification}} {{of the use}} of Bayesian inference was given by Abraham Wald, who proved that every unique Bayesian procedure is admissible. Conversely, every admissible <b>statistical</b> <b>procedure</b> is either a Bayesian procedure or a limit of Bayesian procedures.|$|E
50|$|Like formal {{statistical}} inference, {{the purpose}} of informal inferential reasoning is to draw conclusions about a wider universe (population/process) from data (sample). However, {{it is to be}} contrasted with formal statistical inference that formal <b>statistical</b> <b>procedure</b> or methods are not necessarily used.|$|E
50|$|In {{statistics}} education, informal inferential reasoning (also called informal inference) {{refers to}} the process of making a generalization based on data (samples) about a wider universe (population/process) while taking into account uncertainty without using the formal <b>statistical</b> <b>procedure</b> or methods (e.g. P-values, t-test, hypothesis testing, significance test).|$|E
40|$|<b>Statistical</b> {{selection}} <b>procedures</b> {{are discussed}} in general terms. <b>Statistical</b> selection <b>procedures</b> have been designed specifically to answer questions like Which treatment or variety can {{be considered to be}} the best?. In a certain sense <b>statistical</b> selection <b>procedures</b> are more realistic in answering such a question than the usual testing and multiple comparisons <b>procedures.</b> The <b>statistical</b> <b>procedures</b> of Bechhofer and Gupta are considered. Some practical applications are given. Finally, attention is paid to the relatively new combined procedure of Hsu (1981, 1984) and the procedures of Somerville (1985) ...|$|R
5000|$|... #Caption: <b>Statistical</b> <b>procedures</b> {{to compare}} levels of infection/infestation across {{two or more}} samples of hosts.|$|R
5000|$|The {{following}} <b>statistical</b> <b>procedures</b> {{have been}} found to be useful in carrying out positioning analysis: ...|$|R
50|$|Meta-analysis is a <b>statistical</b> <b>procedure</b> {{to combine}} results across studies to {{integrate}} the findings. This phrase was coined in 1976 as a quantitative literature review. This type of evaluation is very powerful for determining the usability of a device because it combines multiple studies to provide very accurate quantitative support.|$|E
50|$|In statistics, M-estimators are a broad {{class of}} estimators, which are {{obtained}} as the minima of sums of {{functions of the}} data. Least-squares estimators are a special case of M-estimators. The definition of M-estimators was motivated by robust statistics, which contributed new types of M-estimators. The <b>statistical</b> <b>procedure</b> of evaluating an M-estimator on a data set is called M-estimation.|$|E
50|$|Soon after, Charles Spearman (1863-1945) {{developed}} the correlation-based <b>statistical</b> <b>procedure</b> of factor {{analysis in the}} process of building a case for his two-factor theory of intelligence, published in 1901. Spearman believed that people have an inborn level of general intelligence or g which can be crystallized into a specific skill in any of a number of narrow content area (s, or specific intelligence).|$|E
5000|$|ASTM E1488 - Standard Guide for <b>Statistical</b> <b>Procedures</b> to Use in Developing and Applying Test Methods ...|$|R
5000|$|... {{the power}} of the {{instruments}} and <b>statistical</b> <b>procedures</b> used to measure and detect the effects, and ...|$|R
30|$|The {{following}} <b>statistical</b> <b>procedures</b> {{would be}} employed to generate parameter estimates and diagnostic tests of the empirical models.|$|R
50|$|Rodger’s {{method is}} a <b>statistical</b> <b>procedure</b> for {{examining}} research data post hoc following an 'omnibus' analysis (e.g., after {{an analysis of}} variance - anova). The various components of this methodology were fully worked out by R. S. Rodger in the 1960s and 70s, and seven of his articles about it were published in the British Journal of Mathematical and Statistical Psychology between 1967 and 1978.|$|E
5000|$|Correction for {{attenuation}} is a <b>statistical</b> <b>procedure,</b> due to Spearman (1904), to [...] "rid {{a correlation}} coefficient from the weakening effect of measurement error" [...] (Jensen, 1998), a phenomenon {{also known as}} regression dilution. In measurement and statistics, it is also called disattenuation. The correlation between two sets of parameters or measurements is estimated {{in a manner that}} accounts for measurement error contained within the estimates of those parameters.|$|E
50|$|Although {{each year}} several {{different}} SSAT forms are utilized, the SSAT is administered and scored in a consistent (or standard) manner. The reported scores or scaled scores are comparable {{and can be}} used interchangeably, regardless of which test form students take. This score interchangeability is achieved through a <b>statistical</b> <b>procedure</b> referred to as score equating. Score equating is used to adjust for minor form difficulty differences, so that the resulting scores can be compared directly.|$|E
5000|$|Estimating the {{validity}} of the questions, by means of <b>statistical</b> <b>procedures</b> and/or judgement of experts in the field.|$|R
5000|$|Factor Analysis and Principal Component Analysis are multivariate <b>statistical</b> <b>procedures</b> used to {{identify}} relationships between hydrologic variables, [...]|$|R
40|$|<b>Statistical</b> <b>procedures</b> {{based on}} the general linear model (GLM) share {{much in common with}} one another, both conceptually and practically. The use of {{structural}} equation modeling path diagrams as tools for teaching the GLM as a body of connected <b>statistical</b> <b>procedures</b> is presented. A heuristic data set is used to demonstrate a variety of univariate and multivariate statistics as structural models. Implications for analytic strategies and education are discussed...|$|R
