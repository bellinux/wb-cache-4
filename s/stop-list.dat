7|5|Public
5000|$|Extraction {{indexing}} involves taking words {{directly from}} the document. It uses natural language and lends itself well to automated techniques where word frequencies are calculated and those with a frequency over a pre-determined threshold are used as index terms. A <b>stop-list</b> containing common words (such as [...] "the", [...] "and") would be referred to and such stop words would be excluded as index terms.|$|E
50|$|The {{organ of}} St. Philip and St. James {{was built by}} Forster and Andrews of Hull in 1876 {{at a cost of}} £450. It has two manuals and twenty stops. Its <b>stop-list</b> was almost identical, those of the organs in St. Paul's Glenageary and St. Patrick's Dalkey (also built by Forster and Andrews) prior to the latter two instruments' rebuilds.The organ was {{restored}} in the mid-20th Century during which time one new stop was added and an electric blower added.|$|E
50|$|Automated {{extraction}} indexing {{may lead}} to loss of meaning of terms by indexing single words as opposed to phrases. Although {{it is possible to}} extract commonly occurring phrases, it becomes more difficult if key concepts are inconsistently worded in phrases. Automated extraction indexing also has the problem that, even with use of a <b>stop-list</b> to remove common words, some frequent words may not be useful for allowing discrimination between documents. For example, the term glucose is likely to occur frequently in any document related to diabetes. Therefore use of this term would likely return most or all the documents in the database. Post-co-ordinated indexing where terms are combined at the time of searching would reduce this effect but the onus would be on the searcher to link appropriate terms as opposed to the information professional. In addition terms that occur infrequently may be highly significant for example a new drug may be mentioned infrequently but the novelty of the subject makes any reference significant. One method for allowing rarer terms to be included and common words to be excluded by automated techniques would be a relative frequency approach where frequency of a word in a document is compared to frequency in the database as a whole. Therefore a term that occurs more often in a document than might be expected based {{on the rest of the}} database could then be used as an index term, and terms that occur equally frequently throughout will be excluded.Another problem with automated extraction is that it does not recognise when a concept is discussed but is not identified in the text by an indexable keyword.|$|E
40|$|It has {{recently}} been argued that a Naive Bayesian classifier {{can be used to}} filter unsolicited bulk e-mail ("spam"). We conduct a thorough evaluation of this proposal on a corpus that we make publicly available, contributing towards standard benchmarks. At the same time we investigate the effect of attribute-set size, training-corpus size, lemmatization, and <b>stop-lists</b> on the filter's performance, issues that had not been previously explored. After introducing appropriate cost-sensitive evaluation measures, we reach the conclusion that additional safety nets are needed for the Naive Bayesian anti-spam filter to be viable in practice. Comment: 9 page...|$|R
40|$|Abstract: In a {{previous}} paper we presented a systematic computational {{study of the}} extraction of semantic representations from the word-word co-occurrence statistics of large text corpora. The conclusion was that semantic vectors of Pointwise Mutual Information (PMI) values from very small co-occurrence windows, together with a cosine distance measure, consistently resulted in the best representations {{across a range of}} psychologically relevant semantic tasks. This paper extends that study by investigating the use of three further factors, namely the application of <b>stop-lists,</b> word stemming, and dimensionality reduction using Singular Value Decomposition (SVD), that have been used to provide improved performance elsewhere. It also introduces an additional semantic task and explores the advantages of using a much larger corpus. This leads to the discovery and analysis of improved SVD based methods for generating semantic representations (that provide new state-of-the-art performance on a standard TOEFL task) and the identification and discussion of problems and misleading results that can arise without a full systematic study...|$|R
40|$|AbstractBulk data {{analysis}} eschews file extraction and analysis, common in forensic practice today, and instead processes data in “bulk,” recognizing and extracting salient details (“features”) of {{use in the}} typical digital forensics investigation. This article presents the requirements, design {{and implementation of the}} bulk_extractor, a high-performance carving and feature extraction tool that uses bulk {{data analysis}} to allow the triage and rapid exploitation of digital media. Bulk data analysis and the bulk_extractor are designed to complement traditional forensic approaches, not replace them. The approach and implementation offer several important advances over today's forensic tools, including optimistic decompression of compressed data, context-based <b>stop-lists,</b> and the use of a “forensic path” to document both the physical location and forensic transformations necessary to reconstruct extracted evidence. The bulk_extractor is a stream-based forensic tool, meaning that it scans the entire media from beginning to end without seeking the disk head, and is fully parallelized, allowing it to work at the maximum I/O capabilities of the underlying hardware (provided that the system has sufficient CPU resources). Although bulk_extractor was developed as a research prototype, it has proved useful in actual police investigations, two of which this article recounts...|$|R
40|$|Traditional program slicing {{requires}} two parameters: {{a program}} location and a variable, {{or perhaps a}} set of variables, of interest. <b>Stop-list</b> slicing adds a third parameter to the slicing criterion: those variables that are not of interest. This third parameter is called the stoplist. When a variable in the <b>stop-list</b> is encountered, the data-flow dependence analysis of slicing is terminated for that variable. <b>Stop-list</b> slicing further focuses on the computation of interest, while ignoring computations known or determined to be uninteresting. This {{has the potential to}} reduce slice size when compared to traditional forms of slicing. In order to assess the size of the reduction obtained via <b>stop-list</b> slicing, the paper reports the results of three empirical evaluations: a large scale empirical study into the maximum slice size reduction that can be achieved when all program variables are on the stop-list; a study on a real program, to determine the reductions that could be obtained in a typical application; and qualitative case-based studies to illustrate <b>stop-list</b> slicing in the small. The large-scale study concerned a suite of 42 programs of approximately 800 KLoc in total. Over 600 K slices were computed. Using the maximal stoplist reduced the size of the computed slices by about one third on average. The typical program showed a slice size reduction of about one-quarter. The casebased studies indicate that the comprehension effects are worth further consideration. ...|$|E
40|$|FarsiSum is {{an attempt}} to create an {{automatic}} text summarization system for Persian. The system is implemented as a HTTP client/server application written in Perl. It uses modules implemented in an existing summarizer geared towards the Germanic languages, a Persian <b>stop-list</b> in Unicode format and a small set of heuristic rules. ...|$|E
40|$|This paper {{describes}} an algorithm for document representation in a reduced vectorial space {{by a process}} of feature extraction. The algorithm is applied and evaluated {{in the context of}} the supervised classification of news articles from the collection of Le Monde newspaper issued in the years 2003 and 2004. We are generating a document representation (or profile), in a space of 800 dimensions, represented by semantic tags from a machine-readable dictionary. We are dealing with two issues: the synonymy handled by thematic conflation and polysemy for which we have developed a statistical method for word-sense disambiguation. We propose four variants for the profile generation (of a document) depending on whether a recursive system is used or not, and whether a corrective factor for polysemous words is taken into account or not. To determine the best classifier provided by our algorithm we have evaluated 32 variants, depending on the algorithm type (as previously) and on three other parameters that influence the document representation: grammatical category selection, 15 % reduction of the profile, and a <b>stop-list</b> of semantic tags. The evaluation is done on a set of documents from six categories by calculating the precision, the recall and the F-measure to determine the best algorithm related to the threshold detection. Some parameters (like profile reduction) have low influence on the classifier performance and others (corrective factor for the ambiguous words, <b>stop-list)</b> improve it noticeably. JRC. G. 2 -Support to external securit...|$|E
5000|$|Luhn {{proposed}} to assign more weight to sentences {{at the beginning}} of the document or a paragraph.Edmundson stressed the importance of title-words for summarization and was the first to employ <b>stop-lists</b> in order to filter uninformative words of low semantic content (e.g. most grammatical words such as [...] "of", [...] "the", [...] "a"). He also distinguished between bonus words and stigma words, i.e. words that probably occur together with important (e.g. the word form [...] "significant") or unimportant information.His idea of using key-words, i.e. words which occur significantly frequently in the document, is still one of the core heuristics of today's summarizers. With large linguistic corpora available today, the tf-idf value which originated in information retrieval, can be successfully applied to identify the key words of a text: If for example the word [...] "cat" [...] occurs significantly more often in the text to be summarized (TF = [...] "term frequency") than in the corpus (IDF means [...] "inverse document frequency"; here the corpus is meant by [...] "document"), then [...] "cat" [...] is likely to be an important word of the text; the text may in fact be a text about cats.|$|R
40|$|In this workshop, I {{will present}} an {{automatic}} procedure for extracting formulaic sequences from corpus data and guide participants through its practical implementation using example data and software tools. By {{the end of}} the workshop, participants {{will be able to use}} the N-Gram Processor (Buerki 2013) and the software SubString (Buerki 2011) to extract formulaic sequences from corpus data of their own. Participants will also be aware of some of the strengths and weaknesses of the procedure and its theoretical underpinnings. The workshop is divided into three parts. The first part addresses the question of how (or even whether) extraction procedures relate to theoretical understandings of formulaic sequences. While the procedure presented takes as its starting point a constructionist view of formulaic sequences, which identifies them as units of form and associated meaning that are conventional in a speech community, this understanding is briefly located within a broader context of thinking on the nature of formulaic sequences. Implications for identification procedures, including of views based on psycholinguistic processing, the traditional phraseological criterion triplet of polylexicality, idiomaticity and fixedness or the frequency-only approach that produces lexical bundles will also be discussed. In part two of the workshop, participants are invited to work through a hands-on example of how formulaic sequences are automatically extracted from corpus materials following the five-stage extraction procedure outlined in Buerki (2012) : •	Data preparation (normalisations, formatting) •	N-gram extraction using the N-Gram Processor (including the use of <b>stop-lists)</b> •	Consolidation of different length n-grams to derive a unified list using SubString •	Filtering (application of frequency thresholds and a lexico-structural filter) •	Assessment of accuracy and recall. This includes an introduction to the installation and use of the necessary open-source software tools. A corpus of Wikipedia texts will be provided as example data. In the final part of the workshop, strengths and limitations of the procedure will be discussed as well as potential alternatives. Strengths include the methodological transparency of the procedure and the ability to process large amounts of corpus data (subject to sufficiently powerful hardware); the limitations consist mainly of the flipside of this, namely that it is less accurate as an automatic procedure when applied to small amounts of data (< 1 million words). In a final discussion section, participants are invited to share their views on any aspect of the workshop topic including how remaining challenges might be overcome...|$|R
40|$|In {{the past}} few years the Naïve Bayesian (NB) {{classifier}} has been trained automatically to detect spam (unsolicited bulk e-mail). The paper introduces a simple feature selection algorithm to construct a feature vector on which the classifier will be built. We conduct an experiment on SpamAssassin public email corpus to measure the performance of the NB classifier built on the feature vector constructed by the introduced algorithm against the feature vector constructed by the Mutual Information algorithm which is widely used in the literature. The effect of the <b>stop-list</b> and the phrases-list on the classifier performance was also investigated. The results of the experiment show that the introduced algorithm outperforms the Mutual Information algorithm...|$|E

