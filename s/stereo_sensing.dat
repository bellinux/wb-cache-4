7|27|Public
40|$|In this paper, {{we propose}} a novel {{framework}} {{to evaluate the}} depth resolution of axially distributed <b>stereo</b> <b>sensing</b> (ADSS) under fixed resource constraints. The proposed framework can evaluate the performance of ADSS systems based on various sensing parameters such {{as the number of}} cameras, the number of total pixels, pixel size and so on. The Monte Carlo simulations for the proposed framework are performed and the evaluation results are presented...|$|E
30|$|As earlier {{works have}} shown the {{alternative}} of ground truth evaluation is limited to global statistics [8] over sparsely labelled ground truth (e.g. [11, 12]). This is inherently bias to the corrected of the (often highly textured) scene background. Here we attempt to qualitatively evaluate the ease of semantic disparity image interpretation with a bias towards the clarity and temporal stability of scene foreground objects (e.g. pedestrians/vehicles/street furniture). It is after all these which present key challenges for future driver assistance systems using <b>stereo</b> <b>sensing.</b> Qualitatively evaluating such foreground objects within such sequences is an area for future work.|$|E
40|$|We have {{developed}} an intelligent CMOS sensor named Profile Sensor with sampling rate at 3200 Hz of X and Y projection images. The Profile Sensor is a unique CMOS sensor aimed at high-speed position sensing by detecting projection images of X and Y-axes. We have constructed a <b>stereo</b> <b>sensing</b> module by using two profile sensors and confirmed {{the characteristics of the}} module for high-speed three-dimensional position detection of multi spots using LEDs and reflectors as a target. We would like to show that the sensor has high-speed and high-accurate position measurement capabilities and can be useful for industrial and amusement applications. ...|$|E
50|$|While no {{master tape}} {{was created in}} the {{traditional}} <b>sense,</b> <b>stereo</b> tapes were in fact created while cutting to enable future remastering.|$|R
40|$|Due to {{the beam}} related background, the inner chamber of BESIII MDC has aging effect after 5 years running. The gains {{of the inner}} chamber cells {{decrease}} obviously, and the max gain decrease is about 26 % for the first layer cells. A new inner drift chamber with eight <b>stereo</b> <b>sense</b> wire layers as a backup for MDC is under construction, which is almost {{the same as the}} current one but using stepped endplates to shorten the wire length beyond the effective solid angle. This new structure will be of benefit to reducing the counting rate of single cell. The manufacture of each component is going smoothly, and the new inner drift chamber will be finished by the end of April 2014. Comment: 4 pages, 4 figures, proceedings for the XXXIII international symposium on Physics in Collision (PIC 2013...|$|R
40|$|Autonomous vehicle {{operations}} in Antarctica challenge robotic perception. Flying ice and snow, changing illumination due to low sun angles {{and lack of}} contrast degrade <b>stereo</b> and laser <b>sensing.</b> Millimeter-wave radar offers remarkable advan- tages as a robotic perception modality {{because it is not}} as sensi- tive to the aforementioned conditions. Experiments with millimeter-wave radar in an Antarctic environment show mini...|$|R
40|$|Visual sensing, such as vision based localization, nav-igation, tracking, {{are crucial}} for {{intelligent}} robots, which have shown great advantage in many robotic applications. However, {{the market is}} still in lack of a powerful visual sensing platform to deal {{with most of the}} visual processing tasks. In this paper we introduce a powerful and efficient platform, Guidance, which is composed of one processor and multiple (up to five) <b>stereo</b> <b>sensing</b> units. Basic visual tasks including visual odometry, obstacle avoidance, depth generation, are given as built-in functions. Additionally, {{with the aid of a}} well documented SDK, Guidance is extremely flexible for users to develop other applications, such as autonomous navigation, SLAM, tracking. 1...|$|E
40|$|The {{past few}} years have seen a growing {{interest}} in the application &quot; of three-dimensional image processing. With the increasing demand for 3 -D spatial information for tasks of passive navigation[7, 12], automatic surveillance[9], aerial cartographył 0,l 3], and inspection in industrial automation, the importance of effective stereo analysis has been made quite clear. A particular challenge is to provide reliable and accurate depth data for input to object or terrain modelling systems (such as [5]. This paper describes an algorithm for such <b>stereo</b> <b>sensing</b> It uses an edge-based line-by-line stereo correlation scheme, {{and appears to be}} fast, robust, and parallel implementable. The processing consists of extracting edge descriptions for a stereo pair of images, linking these edges to their nearest neighbors to obtain the edge connectivity structure, correlating the edge descriptions on the basis of local edge properties, then cooperatively removmg those edge correspondences determined to be in error- those which violate th...|$|E
40|$|In {{this paper}} we {{describe}} the Hybrid Omnidirectional Pin-hole Sensor (HOPS) dual camera system. Since its joint camera calibration leads to a fully calibrated hybrid stereo pair from which 3 D information can be extracted, HOPS suits several kinds of applications. For example, {{it can be used}} for surveillance and robot self-localization or obstacle detection, offering the possibility to integrate <b>stereo</b> <b>sensing</b> with peripheral/foveal active vision strategies: once objects or regions of interest are localized on the wide-range sensor, the traditional camera can be used to enhance the resolution with which these areas can be analyzed. Tracking of multiple objects/people relying on high-resolution images for recognition and access control or estimating velocity, dimensions and trajectories are some examples of surveillance tasks for which HOPS is suitable. Accurate obstacle detection, landmark localization, robust ego-motion estimation or three-dimensional environment reconstruction are other examples of possible applications related to (autonomous/holonomous) robot navigation in semi-structured or completely unstructured environments. Some preliminary experiments performed to solve both surveillance and robot navigation with encouraging results are discussed through the paper...|$|E
40|$|Remote sensing {{by means}} of stereo images {{obtained}} from flown cameras and scanners provides the potential to monitor the dynamics of pollutant mixing over large areas. Moreover, stereo technology may permit monitoring of pollutant concentration and mixing with sufficient detail to ascertain {{the structure of a}} polluted air mass. Consequently, stereo remote systems can be employed to supply data to set forth adequate regional standards on air quality. A method of remote <b>sensing</b> using <b>stereo</b> images is described. Preliminary results concerning the planar extent of a plume based on comparison with ground measurements by an alternate method, e. g., remote hot-wire anemometer technique, are supporting the feasibility of using <b>stereo</b> remote <b>sensing</b> systems...|$|R
40|$|Abstract — The {{underwater}} environment presents many chal-lenges for robotic sensing including {{highly variable}} lighting, {{the presence of}} dynamic objects, and the six degree of freedom (6 DOF) 3 D environment. Yet {{in spite of these}} challenges the aquatic environment presents many real and practical applica-tions for robotic sensors. A common requirement of many of these tasks is the need to construct accurate 3 D representations of structures in the environment. In order to address this requirement we have developed a <b>stereo</b> vision-inertial <b>sensing</b> device that we have successfully deployed to reconstruct com-plex 3 D structures in both the aquatic and terrestrial domains. The sensor temporally combines 3 D information, obtained using stereo vision algorithms with a 3 DOF inertial sensor. The resulting point cloud model is then converted to a volumetric representation and a textured polygonal mesh is extracted for later processing. Recently obtained underwater reconstructions of wrecks and coral obtained with the sensor are presented. I...|$|R
40|$|Abstract—The {{underwater}} environment presents many chal-lenges for robotic sensing including {{highly variable}} lighting {{and the presence}} of dynamic objects such as fish and suspended particulate matter. The dynamic six-degree-of-freedom nature of the environment presents further challenges due to unpre-dictable external forces such as current and surge. Despite these challenges the aquatic environment presents many real and practical applications for robotic systems. A common require-ment of many of these tasks is the need to construct accurate 3 D representations of specific environmental structures. In order to address these needs we have developed a <b>stereo</b> vision-inertial <b>sensing</b> device that has been successfully deployed to reconstruct complex 3 D structures in both the aquatic and terrestrial domains. The sensor combines 3 D information, obtained using stereo vision algorithms, with 3 DOF inertial data to construct 3 D models of the environment. The resulting model representation is then converted to a textured polygonal mesh for later processing. Semi-automatic tools have been developed to aid in the processing of these representations. Reconstruction and segmentation of coral and other underwater structures obtained with the sensor are presented. I...|$|R
40|$|RGB-D {{cameras are}} novel sensing systems that capture RGB images along with per-pixel depth information. RGB-D cameras rely on either {{structured}} light patterns combined with <b>stereo</b> <b>sensing</b> [6, 10] or time-of-flight laser sensing [1] to generate depth estimates {{that can be}} associated with RGB pixels. Very soon, small, high-quality RGB-D cameras developed for computer gaming and home entertainment applications will become available at cost below $ 100. In this paper we investigate how such cameras can be used in the context of robotics, specifically for building dense 3 D maps of indoor environments. Such maps have applications in robot navigation, manipulation, semantic mapping, and telepresence. The robotics and computer vision communities have developed a variety of techniques for 3 D mapping based on laser range scans [8, 11], stereo cameras [7], monocular cameras [3], and unsorted collections of photos [4]. While RGB-D cameras provide the opportunity to build 3 D maps of unprecedented richness, they have drawbacks that make their application to 3 D mapping difficult: They provide depth only up to a limited distance (typically less than 5 m), depth values are much noisier than those provided by laser scanners, and their field of view (∼ 60 ◦) is far more constrained than that of specialized cameras or laser scanners typically used for 3 D mapping (∼ 180 ◦). In our work, we use a camera developed by PrimeSense [10]. The key insights of this investigation are: first, that existing frame matching techniques are not sufficient to provide robust visual odometry with these cameras; second, that a tight integration of depth and color information can yield robust frame matching and loop closure detection; third, that building on best practice techniques in SLAM and computer graphics makes it possible to build and visualize accurate and extremely rich 3 D maps with such cameras; and, fourth, that it will be feasible to build complete robot navigation and interaction systems solely based on cheap depth cameras...|$|E
40|$|Computer vision-based sensing {{of people}} enables {{a new class}} of public {{multi-user}} computer interfaces. Computing resources in public spaces, such as automated, information-dispensing kiosks, represent a computing paradigm that differs from the conventional desktop environment and, correspondingly, require a user-interface metaphor quite unlike the traditional WIMP interface. This paper describes a prototype public computer interface, which employs color and <b>stereo</b> tracking to <b>sense</b> the users' activity and an animated, speaking agent to attract attention and communicate through visual and audio modalities. URL: [URL] Keywords: Agents, Blob Tracking, Color Tracking, Facial Animation, HumanComputer Interaction, Smart Kiosk, Stereo c flDigital Equipment Corporation 1996. All rights reserved. CONTENTS i Contents 1 Introduction 1 2 Characteristics of Public User-Interfaces 2 3 The Smart Kiosk Interface 3 3. 1 Designing the I [...] ...|$|R
40|$|We {{propose a}} novel {{approach}} for fusing multiresolution stereo and motion (both rigid and non-rigid) analysis in order to complement each other's performance. A hierarchical frame-work is presented to couple motion correspondences and stereo correspondences in order to generate accurate disparity map and motion parameters. One scenario for such system is the analysis of time-varying multi-spectral observations of clouds from meteorological satellites. Our experiments involve such time-varying remote <b>sensing</b> <b>stereo</b> data sets, and the motion is typically nonrigid as the clouds undergo shape changes. Rigid motion matching may still be performed for initial fusion, and gradually raised to non-rigid motion matching as in a coarse-to-fine strategy. Both stereo disparities and motion correspondences are estimated using such multi-resolution coarse-to-fine strategy to a sub-pixel accuracy. Experimental results using timevarying data of visible channel from two satellites in geosynchronous orbit [...] ...|$|R
40|$|We {{propose to}} test {{experimentally}} a novel theory of cognition by building an autonomous intelligent robot. Our theory comprises three equally important principles. First, we submit that cognition, as manifest in humans, requires a well-integrated sensorimotor periphery. For {{the purposes of}} this experiment sensorimotor function includes binaural audio, <b>stereo</b> video, tactile <b>sense,</b> and proprioceptive control of motion and manipulation of objects. Our methods are designed to exploit the synergy intrinsic in the combined sensorimotor signals. This sensory fusion is essential {{for the development of a}} semantic representation of reality from which all other levels of linguistic structure derive. Second, for intelligent behavior, the widely accepted role of computation in the sense of Turing is of secondary significance to the primary mechanism of associative memory. We plan to experiment with several different architectures of such a memory. And, finally, the contents of the [...] ...|$|R
40|$|Autonomous vehicle {{operations}} in Antarctica challenge robotic perception. Flying ice and snow, changing illumination due to low sun angles {{and lack of}} contrast degrade <b>stereo</b> and laser <b>sensing.</b> Millimeter-wave radar offers remarkable advantages as a robotic perception modality {{because it is not}} as sensitive to the aforementioned conditions. Experiments with millimeter-wave radar in an Antarctic environment show minimal degradation of millimeter-wave sensing capabilities under blowing-snow conditions, as well as backscatter obtained from polar-terrain surfaces at grazing angles and detection of obstacles commonly found in polar areas. This paper presents issues relevant to short-range radar perception for a mobile robot in an Antarctic environment. The article describes the experiments and data-analysis procedures, and draws conclusions on the utility of millimeter-wave radar as a robotic sensor for obstacle avoidance and navigation in polar settings. Keywords [...] Millimeter-wave radar, m [...] ...|$|R
40|$|Autonomous {{planetary}} rovers {{operating in}} vast unknown environments must operate efficiently because of size, power and computing limitations. Recently, {{we have developed}} a rover capable of efficient obstacle avoidance and path planning. The rover uses binocular <b>stereo</b> vision to <b>sense</b> potentially cluttered outdoor environments. Navigation is performed {{by a combination of}} several modules that each ÒvoteÓ for the next best action for the robot to execute. The key distinction of our system is that it produces globally intelligent behavior with a small computational resourceÑ all processing and decision making is done on a single processor. These algorithms have been tested on our prototype rover, Bullwinkle, outdoors and have recently driven the rover 100 m at speeds of 15 cm/ sec. In this paper we report on the extensions on the systems that we have previously developed that were necessary to achieve autonomous navigation in this domain...|$|R
40|$|This paper {{presents}} a vision {{system for the}} task of actively acquiring and modeling the geometry of an unknown object. Using an active trinocular <b>stereo</b> head (VIRTUE), <b>sensed</b> 3 -D line segments are grouped into a polyhedral volumetric model through {{the aid of a}} constrained Delaunay triangulation. Partial models and a viewpointenumeration scheme are used to guide the image acquisition process and to determine `where to look next'. Results of the active vision recovery of a number of objects are provided with their associated volumetric and surface errors. Keywords: active modeling, trinocular stereopsis. 1. Introduction Current methods in contact-less sensing commonly involve the use of a laser, either to illuminate the object or to measure the time-of-flightbetween sensor and object (Arman and Aggarwal, 1993). Difficulties with laser approaches arise from specularities, in outdoor scenes (because of the need for active illumination) and because of speckles (patterns formed on the [...] ...|$|R
40|$|We {{present a}} new neural network algorithm, {{derived from the}} Kohonen self-organized mapping algorithm, for the {{solution}} of the problem of matching points in two pictures representing slightly displaced and distorted images of the same objects. We describe it hereafter {{in the context of a}} particular application, namely the matching of the images of marker-particles suspended in a moving fluid, seen in two pictures of them taken a small time interval apart. We illustrate the quality of the solutions it produces with representative results obtained for some test problems; in all cases it is outstandingly efficient. 1. Introduction We propose a new algorithm for the solution of a type of correspondence problem that is met in a large number of important applications, as for example, <b>stereo</b> vision, remote <b>sensing,</b> medical imaging, fluid motion visualization, [...] . We describe this algorithm hereafter in the particular context of one of these applications, namely fluid mechanics, but we believe [...] ...|$|R
40|$|Our mobile robots use {{trinocular}} <b>stereo</b> {{sensors to}} <b>sense</b> their environment, navigate around obstacles in a dynamic, unknown world and partner with humans in various tasks. Fundamental to these abilities is {{the capacity to}} build a map and locate themselves relative to it, while operating. Here we review a collection of methods that permit mobile robots to acquire spatial and visual maps of their environment incrementally, to localize themselves in these maps, and to share these maps when collaborating. The resulting dense spatial and appearance data {{can be used for}} visualization, modeling and remote interaction. Visually guided robots rely almost exclusively on passive light gathering sensors and so can be stealthy as well as small, since they need no emitters and need less power than active sensors. Progress in camera and processor miniaturization will permit embedded devices to see as they act and move about the world. 1 The Robots 1. 1 Mobile robots: Jos'e and Eric We use [...] ...|$|R
40|$|This report {{examines}} the general problem of high speed autonomous navigation from range image data {{as it applies}} to both <b>stereo</b> and lidar <b>sensing</b> systems. In order to intelligently guarantee its own safety, a high speed vehicle must be able to resolve the smallest obstacle that can present a hazard, process sensory data at a rate commensurate with its speed, respond fast enough to avoid obstacles, and maintain a sufficiently accurate model of the world to enable it to make correct decisions. These dimensions of the problem are analysed in a nondimensional manner and the implications of satisfying all requirements simultaneously are investigated. In this analysis, it is shown that to adopt a policy of guaranteed vehicle safety is to adopt a computational complexity of for range image processing where is the vehicle reaction time and is the velocity. This result implies that increased vehicle speed will require nonlinear growth in computational bandwidth. Further, it identif [...] ...|$|R
40|$|We {{introduce}} a new technique called shape from photomotion. It uses a series of 2 -D Lambertian images, generated by moving a light source around a scene, to recover the depth map. In each of the images, the object in the scene remains at a fixed position and the only variable is the light source direction. The movement of the light source causes {{a change in the}} intensity of any given point in the image. The change in intensity is what enables us to recover the unknown parameter, the depth map, since it remains constant in each of the input images. Our method differs from photometric <b>stereo</b> in the <b>sense</b> that the shape estimate is not only computed for each light source orientation, but also gradually refined by photomotion. 1 Introduction Shape from shading uses a single image to recover the shape information. It requires the least amount of input, however, this also introduces disadvantages. One disadvantage is that since it has less image information available, it is less accurate. A [...] ...|$|R
40|$|This {{research}} {{was sponsored by}} ARPA under contracts “Perception for Outdoor Navigation ” (contract num-ber DACA 76 - 89 -C- 0014, monitored by the US Army Topographic Engineering Center) and “Unmanned Ground Vehicle System ” (contract number DAAE 07 - 90 -C-RO 59, monitored by TACOM). The views and conclusions expressed in this document {{are those of the}} author and should not be interpreted as representing the official policies, either express or implied, of the US government. iAbstract This report examines the general problem of high speed autonomous navigation from range image data as it applies to both <b>stereo</b> and lidar <b>sensing</b> systems. In order to intelligently guarantee its own safety, a high speed vehicle must be able to resolve the smallest obstacle that can present a hazard, process sensory data at a rate commensurate with its speed, respond fast enough to avoid obsta-cles, and maintain a sufficiently accurate model of the world to enable it to make correct decisions. These dimensions of the problem are analysed in a nondimensional manner and the implications of satisfying all requirements simultaneously are investigated...|$|R
40|$|The {{views and}} {{conclusions}} {{expressed in this}} document {{are those of the}} author and should not be interpreted as representing the official policies, either express or implied, of the US government. This report examines the general problem of high speed autonomous navigation from range image data as it applies to both <b>stereo</b> and lidar <b>sensing</b> systems. In order to intelligently guarantee its own safety, a high speed vehicle must be able to resolve the smallest obstacle that can present a hazard, process sensory data at a rate commensurate with its speed, respond fast enough to avoid obstacles, and maintain a sufficiently accurate model of the world to enable it to make correct decisions. These dimensions of the problem are analysed in a nondimensional manner and the implications of satisfying all requirements simultaneously are investigated. In this analysis, it is shown that to adopt a policy of guaranteed vehicle safety is to adopt a computational complexity of O [TV] for range image processing where is the vehicle reaction time and is the velocity. ...|$|R
40|$|There is no {{such thing}} as a disembodied mind. We posit that {{cognitive}} development can only occur through interaction with the physical world. To this end, we are developing a robotic platform for the purpose of studying cognition. We suggest that the central component of cognition is a memory which is primarily associative, one where learning occurs as the correlation of events from diverse inputs. We also believe that human-like cognition requires a well-integrated sensorymotor system, to provide these diverse inputs. As implemented in our robot, this system includes binaural hearing, <b>stereo</b> vision, tactile <b>sense,</b> and basic proprioceptive control. On top of these abilities, we are implementing and studying various models of processing, learning and decision making. Our goal is to produce a robot that will learn to carry out simple tasks in response to natural language requests. The robot’s understanding of language will be learned concurrently with its other cognitive abilities. We have already developed a robust system and conducted a number of experiments on the way to this goal, some details of which appear in this paper. This is a progress report of what we believe will be a long term project with significant implications...|$|R
40|$|Photogrammetric terrain {{reconstruction}} from aerial {{and space}} stereopairs of images occupies {{a prominent place}} in cartography and remote <b>sensing.</b> <b>Stereo</b> vision systems determine depth from two or more images using automated techniques. The most important and time consuming task for a stereo vision system is the registration of both images, i. e. the matching of corresponding pixels. Area based stereo attempts to determine the correspondence for every pixel, which results in a dense depth map. Correlation is the basic method used to find corresponding pixels. However, correlation assumes that the depth is equal for all pixels of a correlation window, which is violated at depth discontinuities. The result is that object borders are blurred and small details or objects are removed, {{depending on the size}} of the correlation window. In this paper, we focus on the generation of reliable surface models using dense techniques. An overview is given of correlation-based techniques using adaptive windows. These adaptive techniques separate fore- from background information in a correlation window using structure specific information (e. g. gradient, segmentation) and are able to compensate for the blurring effect that occurs at object boundaries. The result is a dense surface model with an emphasis on reliability. A case study that compares the different techniques is presented. 1...|$|R
40|$|Coral reef habitat {{structural}} complexity influences key ecological processes, ecosystem biodiversity, and resilience. Measuring structural complexity underwater is not trivial {{and researchers}} have been searching for accurate and cost-effective methods {{that can be applied}} across spatial extents for over 50 years. This study integrated a set of existing multi-view, image-processing algorithms, to accurately compute metrics of structural complexity (e. g., ratio of surface to planar area) underwater solely from images. This framework resulted in accurate, high-speed 3 D habitat reconstructions at scales ranging from small corals to reef-scapes (10 s km 2). Structural complexity was accurately quantified from both contemporary and historical image datasets across three spatial scales: (i) branching coral colony (Acropora spp.); (ii) reef area (400 m 2); and (iii) reef transect (2 km). At small scales, our method delivered models with < 1 mm error over 90 % of the surface area, while the accuracy at transect scale was 85. 3 % ± 6 % (CI). Advantages are: no need for an a priori requirement for image size or resolution, no invasive techniques, cost-effectiveness, and utilization of existing imagery taken from off-the-shelf cameras (both monocular or <b>stereo).</b> This remote <b>sensing</b> method can be integrated to reef monitoring and improve our knowledge of key aspects of coral reef dynamics, from reef accretion to habitat provisioning and productivity, by measuring and up-scaling estimates of structural complexity...|$|R
40|$|Visual {{surveillance}} is {{an attempt}} to detect, recognize and track certain objects from image sequences, and more generally to understand and describe object behaviors. Future’s visual surveillance systems for outdoor (including security) applications will require mission platforms that are autonomous, asynchronous, adaptive and highly sensitive in complex, time-varying and possibly hostile environments. One fundamental problem in autonomous surveillance systems is how to detect multiple targets, sense and percept the world/environment in extreme outdoor conditions in which the presence of dust, fog, rain, changing illumination can dramatically degrade conventional <b>stereo</b> and laser <b>sensing.</b> Thus, radar-based imaging in autonomous surveillance systems has attracted extensive research attention in recent years since radar also allows for multiple targets detection within a single beam, whereas other range sensors are limited to one target return per emission. The main challenge arising from radar-based autonomous visual surveillance systems is always linked to radar image information processing and utilization, i. e., how to quickly and efficiently extract and analyse the information of interest for multiple targets from consecutive images acquired by radar imaging sensors. In this talk, a visual attention model-based algorithm is proposed to detect multiple targets from SAR (synthetic aperture radar) images. The algorithm extends the well-known Itti model according to the requirements of multiple target detection in SAR images. It locates salient regions in SAR images and reduces false alarms significantly by using an efficient top-down process. The performance of the proposed algorithm is demonstrated by using real SAR images with 20 vehicle targets. Acknowledgements: This research was supported by The Royal Society of Edinburgh (RSE) and The National Natural Science Foundation of China (NNSFC) under the RSE-NNSFC joint projects (2012 - 2015) [grant number 61211130309 and 61211130210] with Beihang University and Anhui University, China, respectively. It {{was supported in part by}} the “Sino-UK Higher Education Research Partnership for PhD Studies” joint-project (2013 - 2015) funded by the British Council China and The China Scholarship Council (CSC) ...|$|R
40|$|Current {{robotics}} {{research is}} largely {{driven by the}} vision of creatingan intelligent being that can perform dangerous, difficult orunpopular tasks. These can for example be exploring the surface of planet mars or the bottomof the ocean, maintaining a furnace or assembling a car.    They can also be more mundane such as cleaning an apartment or fetching groceries. This vision has been pursued since the 1960 s when the first robots were built. Some of the tasks mentioned above, especially those in industrial manufacturing, arealready frequently performed by robots. Others are still completelyout of reach. Especially, household robots are far away from beingdeployable as general purpose devices. Although advancements have beenmade in this research area, robots are not yet able to performhousehold chores robustly in unstructured and open-ended environments givenunexpected events and uncertainty in perception and execution. In this thesis, we are analyzing which perceptual andmotor capabilities are necessaryfor the robot to perform common tasks in a household scenario. In that context, an essential capability is tounderstand the scene that the robot has to interact with. This involvesseparating objects from the background but also from each other. Once this is achieved, many other tasks becomemuch easier. Configuration of objectscan be determined; they can be identified or categorized; their pose can be estimated; free and occupied space in the environment can be outlined. This kind of scene model can then inform grasp planning algorithms to finally pick up objects. However, scene understanding is not a trivial problem and evenstate-of-the-art methods may fail. Given an incomplete, noisy andpotentially erroneously segmented scene model, the questions remain howsuitable grasps can be planned {{and how they can}} be executed robustly. In this thesis, we propose to equip the robot with a set of predictionmechanisms that allow it to hypothesize about parts of the sceneit has not yet observed. Additionally, the robot can alsoquantify how uncertain it is about this prediction allowing it toplan actions for exploring the scene at specifically uncertainplaces. We consider multiple modalities includingmonocular and <b>stereo</b> vision, haptic <b>sensing</b> and information obtainedthrough a human-robot dialog system. We also study several scene representations of different complexity and their applicability to a grasping scenario. Given an improved scene model from this multi-modalexploration, grasps can be inferred for each objecthypothesis. Dependent on whether the objects are known, familiar orunknown, different methodologies for grasp inference apply. In thisthesis, we propose novel methods for each of these cases. Furthermore,we demonstrate the execution of these grasp both in a closed andopen-loop manner showing the effectiveness of the proposed methods inreal-world scenarios. QC 20111125 GRAS...|$|R
40|$|Visual {{surveillance}} is {{an attempt}} to detect, recognize and track certain objects from image sequences, and more generally to understand and describe object behaviors. Future’s visual surveillance systems for outdoor (including security) applications will require mission platforms that are autonomous, asynchronous, adaptive and highly sensitive in complex, time-varying and possibly hostile environments. One fundamental problem in autonomous surveillance systems is how to detect multiple objects, sense and percept the world/environment in extreme outdoor conditions in which the presence of dust, fog, rain, changing illumination can dramatically degrade conventional <b>stereo</b> and laser <b>sensing.</b> Thus, radar-based imaging in autonomous surveillance systems has attracted extensive research attention in recent years since radar also allows for multiple object detection within a single beam, whereas other range sensors are limited to one target return per emission. The main challenge arising from radar-based autonomous visual surveillance systems is always linked to radar image information processing and utilization, i. e., how to quickly and efficiently extract and analyse the information of interest for multiple object detection and extraction such as road networks, detecting vehicles from consecutive images acquired by radar imaging sensors. In this talk, we firstly present a novel neurobiologically-inspired approach to road extraction from radar images. The proposed approach is based on the neurobiological mechanism in which simple cells in primary visual cortex are believed to extract local contour information from a visual scene. The CORF (Combination of Receptive Fields) computational model is used in the proposed approach to road extraction from radar images. To demonstrate the effectiveness of the proposed approach, the numerical experimental results obtained from using several SAR(synthetic aperture radar) images are provided. Secondly, a visual attention model-based algorithm is proposed to detect vehicles from SAR (synthetic aperture radar) images. The performance of the proposed algorithm is demonstrated by using real SAR images with 20 vehicle targets. Finally, an algorithm based on kernel fisher discriminant analysis (KFDA) for vehicle detection and recognition in SAR images is presented. We obtain image samples with a dual-window approach and extract features of the inner and outer window samples using KFDA. An improved KFDA-IMED (Image Euclidean Distance) strategy is employed to obtain the projection in known sample space and combine with a support vector machine (SVM) to recognize vehicles effectively. The performance of this new method is validated with the MSTAR (moving and stationary target acquisition and recognition) database. Acknowledgements: This research is supported by The Royal Society of Edinburgh (RSE) and The National Natural Science Foundation of China (NNSFC) under the RSE-NNSFC joint projects (2012 - 2014) [grant number 61211130309 and 61211130210] with Beihang University and Anhui University, China, respectively. It is supported in part by the “Sino-UK Higher Education Research Partnership for PhD Studies” joint-project (2013 - 2015) funded by the British Council China and The China Scholarship Council (CSC). Both Amir Hussain and Erfu Yang were also funded, in part, by the UK Engineering and Physical Sciences Research Council (EPSRC) [grant number EP/I 009310 / 1]...|$|R
40|$|Visual object {{tracking}} {{is frequently}} employed in applications, such as intelligent video surveillance, human body tracking, {{and many other}} related problems. Therefore, it is a fundamental problem in video processing and computer vision. The general procedure of automatic object tracking consists of object detection, object representation, tracking strategy and model updating. Tracking strategy, in particular, {{is an important component}} because it performs prediction and inference of useful object information such as object location, object orientation and object size, from one frame to another. In this dissertation, a new visual object tracking algorithm using a novel Bayesian Kalman filter (BKF) with simplified Gaussian mixture (BKF-SGM) tracking strategy is proposed. The new BKF-SGM employs a GM representation of the state and noise densities and a novel direct density simplifying algorithm for avoiding the exponential complexity growth of conventional Kalman filters using GM. As the GM is simplified directly without resampling using particles, the proposed BKF-SGM considerably reduces the exponential arithmetic complexity and avoids performance degradation due to sampling degeneracy and impoverishment in conventional particle filtering (PF). When coupled with an improved mean shift (MS) algorithm, the original MS tracker is extended under the BKF-SGM framework above to a bank of parallel MS trackers, which offer a more robust tracking performance. The resultant algorithm, which is called the BKF-SGM with improved MS (BKF-SGM-IMS), is inherently parallel in nature and hence can be readily accelerated using Graphics Processing Unit (GPU) to meet the high computational requirement in real-time applications. The proposed BKF-SGM-IMS algorithm can successfully handle complex scenarios with good performance and low arithmetic complexity. Moreover, the performance of both non-training/training-based object recognition algorithms can be improved by using our tracking results as input. As depth information make machine vision one step closer to human vision by combining color and depth information, there is a recent interest in depth-aware video processing and computer vision both in the academic and industrial fields. However, high quality and high resolution depth map acquisition for real world scene is a challenging problem. Conventional depth acquisition algorithms which rely on stereo/multi-view vision (passive method) or depth sensing device (active method) alone are limited by complicated scenes or imperfections of the depth sensing devices. In this dissertation, a new system for indoor high resolution and high quality depth estimation using joint fusion of <b>stereo</b> and depth <b>sensing</b> data is proposed. By modeling the observations using Markov random field (MRF), the fusion problem is formulated as a maximum a posteriori probability (MAP) estimation problem. The reliability and the probability density functions for describing the observations from the two devices are also derived. The MAP problem is solved using a multi-scale belief propagation (BP) algorithm. To suppress possible estimation noise, the depth map estimated is further refined by color image guided depth matting and a 2 D polynomial regression (LPR) -based filtering. Experimental results and numerical comparisons show that our system can provide high quality and high resolution depth maps, thanks to the complementary strengths of both stereo vision and depth sensing device. published_or_final_versionElectrical and Electronic EngineeringDoctoralDoctor of Philosoph...|$|R

