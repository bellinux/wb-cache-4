14|0|Public
40|$|A 3 -D laser {{tracking}} scanner system analysis focusing on immunity to ambient sunlight and geometrical resolution and accuracy {{is presented in}} the context of a space application. The main goal of this development is to provide a robust sensor to assist in the assembly of the Space Station. This 3 -D laser scanner system can be used in imagery or in tracking modes, using either time-offlight (TOF) or triangulation methods for range acquisition. It uses two high-speed galvanometers and a collimated laser beam to address individual targets on an object. In the tracking mode of operation, we will compare the pose estimation and accuracy of the laser scanner using the different methods: triangulation, TOF (resolved targets), and photogrammetry (<b>spatial</b> <b>resection),</b> and show the advantages of combining these different modes of operation to increase the overall performances of the laser system. Keywords: Laser scanner, 3 D, ranging, tracking, light immunity, photogrammetry, <b>spatial</b> <b>resection,</b> tr [...] ...|$|E
40|$|In this paper, {{we compare}} the {{accuracy}} and resolution of a 3 D-laser scanner prototype that tracks {{in real-time and}} computes the relative pose of objects in a 3 D space. This 3 D-laser scanner prototype was specifically developed to study {{the use of such}} a sensor for space applications. The main objective of this project is to provide a robust sensor to assist in the assembly of the International Space Station where high tolerance to ambient illumination is paramount. The laser scanner uses triangulation based range data and photogrammetry methods (<b>spatial</b> <b>resection),</b> to calculate the relative pose of objects. Range information is used to increase the accuracy of the sensing system and to remove erroneous measurements. Two high-speed galvanometers and a collimated laser beam address individual targets mounted on an object to a resolution corresponding to an equivalent imager of 1000010000 pixels. Knowing the position coordinates of predefined targets on the objects, their relative poses can be computed using either the scanner calibrated 3 D coordinates or <b>spatial</b> <b>resection</b> methods...|$|E
40|$|Abstract: The {{problem of}} multiple-camera system calibration/spatial {{intersection}} is examined from a Bayesian statistical viewpoint. It is shown how {{a number of}} common situations such as basic calibration, machine vision self calibration, spatial intersection and <b>spatial</b> <b>resection</b> can be interpreted in the Bayesian framework. In each case, by choosing appropriate priors the standard method of solution {{can be interpreted as}} a MAP estimate. Two illustrative examples are given to show how the incorporation of realistic prior information can lead to improved accuracy...|$|E
40|$|We {{propose a}} point-based {{environment}} model (PEM) {{to represent the}} absolute coordinate frame in which camera motion is to be tracked. The PEM can easily be acquired by laser scanning both indoors and outdoors even over long distances. The approach avoids any expensive modeling step and instead uses the raw point data for scene representation. Also the approach requires no additional artificial markers or active components as orientation cues. Using intensity feature detection techniques key points are automatically extracted from the PEM and tracked across the image sequence. The orientation procedure of the imaging sensor is solely based on <b>spatial</b> <b>resection.</b> 1...|$|E
40|$|Abstract: The paper {{describes}} {{two different}} geometric projection models for a full-spherical camera and presents {{results of the}} calibration of full spherical camera systems based on them. Full spherical images can be generated by a rotating linear array camera equipped with a fisheye lens. Their projection (Fig. 1) differs considerably from the central perspective geometry. It can be modelled by introducing spherical coordinates or by applying a sensor related coordinate system for each recording position of the linear array sensor extended by fisheye lens projection equations. For a first test of the system, a calibration was performed by <b>spatial</b> <b>resection</b> from a single image of a calibration room suitable for full-spherical records. Full-spherical camera systems were calibrated with an accuracy better than 1 / 5 pixels...|$|E
40|$|Underwater {{applications}} of photogrammetric measurement techniques usually {{need to deal}} with multimedia photogrammetry aspects, which are characterized by the necessity of handling optical rays that are broken at interfaces between optical media with different refrative indices according to Snell’s Law. This so-called multimedia geometry has to be incorporated into geometric models in order to achieve correct measurement results. The paper shows a flexible yet strict geometric model for the handling of refraction effects on the optical path, which can be implemented as a module into photogrammetric standard tools such as <b>spatial</b> <b>resection,</b> spatial intersection, bundle adjustment or epipolar line computation. The module is especially well suited for applications, where an object in water is observed by cameras in air through one or more plane parallel glass interfaces, as it allows for some simplifications here...|$|E
40|$|The {{integration}} of digital aerial photogrammetry and Light Detetion And Ranging (LiDAR) is an inevitable trend in Surveying and Mapping field. We calculate the external orientation elements of images which identical with LiDAR coordinate to realize automatic high precision registration between aerial images and LiDAR data. There {{are two ways}} to calculate orientation elements. One is single image <b>spatial</b> <b>resection</b> using image matching 3 D points that registered to LiDAR. The other one is Position and Orientation System (POS) data supported aerotriangulation. The high precision registration points are selected as Ground Control Points (GCPs) instead of measuring GCPs manually during aerotriangulation. The registration experiments indicate that the method which registering aerial images and LiDAR points has a great advantage in higher automation and precision compare with manual registration...|$|E
40|$|Abstract. Robust local image {{features}} {{have been used}} successfully in robot localization and camera pose estimation; region tracking using affine warps is considered {{state of the art}} also for many years. Although such correspondences provide a warp of the local image region and are quite powerful, in direct pose estimation they are so far only considered as points and therefore three of them are required to construct a camera pose. In this contribution we show how it is possible to directly compute a pose based upon one such feature, given the plane in space where it lies. This differential correspondence concept exploits the texture warp and has recently gained attention in estimation of conjugate rotations. The approach can also be considered as the limiting case of the well-known <b>spatial</b> <b>resection</b> problem when the three 3 D points approach each other infinitesimally close. We show that the differential correspondence is more powerful than conic correspondences while its exploitation requires nothing more complicated than the roots of a third order polynomial. We give a detailed sensitivity analysis, a comparison against state-of-the-art pose estimators and demonstrate real-world applicability of the algorithm based on automatic region recognition. ...|$|E
40|$|The paper {{describes}} a geometric projection model for fisheye lenses and presents {{results of the}} calibration of fisheye lens camera systems based on it. For fisheye images the real projection model does not comply with the central perspective. In addition, the lenses are subject to considerable distortions. The fisheye lens projection follows an approximately linear relation between the incidence angle of an object point’s beam and {{the distance from the}} corresponding image point to the principle point. The developed projection model is extended by several distortion parameters. For the calibration of fisheye lens camera systems, a calibration room was created and equipped with number targets. They were arranged to have a good distribution all over the image. The reference object coordinates of the targets were determined with high precision. The calibration itself is done by <b>spatial</b> <b>resection</b> from a single image of the calibration room. The calibration parameters include interior orientation, exterior orientation and lens distortion. Besides the geometric model for fisheye lenses, the paper shows practical results from the calibration of a high quality fisheye lens on a 14 mega pixel camera as well as a consumer camera equipped with a fisheye lens. 1...|$|E
40|$|Commission V, WG V/ 5 The paper {{describes}} a geometric projection model for fisheye lenses and presents {{results of the}} calibration of fisheye lens camera systems based on it. For fisheye images the real projection model does not comply with the central perspective. In addition, the lenses are subject to considerable distortions. The fisheye lens projection follows an approximately linear relation between the incidence angle of an object point’s beam and {{the distance from the}} corresponding image point to the principle point. The developed projection model is extended by several distortion parameters. For the calibration of fisheye lens camera systems, a calibration room was created and equipped with number targets. They were arranged to have a good distribution all over the image. The reference object coordinates of the targets were determined with high precision. The calibration itself is done by <b>spatial</b> <b>resection</b> from a single image of the calibration room. The calibration parameters include interior orientation, exterior orientation and lens distortion. Besides the geometric model for fisheye lenses, the paper shows practical results from the calibration of a high quality fisheye lens on a 14 mega pixel camera as well as a consumer camera equipped with a fisheye lens. 1...|$|E
40|$|Automated close-range photogrammetric network {{orientation}} and camera calibration {{has traditionally been}} {{associated with the use of}} coded targets in the object space to allow for an initial relative orientation (RO) and subsequent <b>spatial</b> <b>resection</b> of the images. However, over the last decade, advances coming mainly from the computer vision (CV) community have allowed for fully automated orientation via feature-based matching techniques. There are a number of advantages in such methodologies for various types of applications, as well as for cases where the use of artificial targets might be not possible or preferable, for example when attempting calibration from low-level aerial imagery, as with UAVs, or when calibrating long-focal length lenses where small image scales call for inconveniently large coded targets. While there are now a number of CV-based algorithms for multi-image orientation within narrow-baseline networks, with accompanying open-source software, from a photogrammetric standpoint the results are typically disappointing as the metric integrity of the resulting models is generally poor, or even unknown. The objective addressed in this paper is target-free automatic multi-image orientation, maintaining metric integrity, within networks that incorporate wide-baseline imagery. The focus is on both the development of a methodology that overcomes the shortcomings that can be present in current CV algorithms, and on the photogrammetric priorities and requirements that exist in current processing pipelines. This paper also reports on the application of the proposed methodology to automated target-free camera self-calibration and discusses the process via practical examples...|$|E
40|$|The {{geometric}} referencing {{of digital}} image data and 3 D point clouds e. g. {{given by a}} terrestrial laser scanner is the prerequisite for different levels of integrated data interpretation such as point cloud or mesh model texture colourisation for visualisation purposes, interactive object modelling by monoplotting-like procedures or automatic point cloud interpretation. Therein, the characteristics of laser scanner data and camera data {{can be regarded as}} complementary, so that these data are suitable for a combined interpretation. A precondition for the geometric referencing between laser scanner data and digital images and consequently for an integrated use of both data sets is the extraction of corresponding features. With a set of corresponding features the orientation of the image to the point cloud can be obtained by <b>spatial</b> <b>resection.</b> With regard to a high automation level the paper presents an approach for finding correspondences between features extracted from laser scanner data and digital images automatically. The basic idea of the presented approach is to use the SIFT operator to detect corresponding points in the camera image and an intensity image of the laser scanner data. Determining correspondences consists of four steps: Detection of salient characteristics, description of the features, matching of the descriptions in both images and evaluation of correct matches. RANSAC is used to find sets of consistent matches. The approach is validated with a data set taken from a baroque building in Dresden. 1...|$|E
40|$|Automated close-range photogrammetric network {{orientation}} {{has traditionally}} {{been associated with the}} use of coded targets in the object space to allow for an initial relative orientation (RO) and subsequent <b>spatial</b> <b>resection</b> of the images. Over the past decade, automated orientation via feature-based matching (FBM) techniques has attracted renewed research attention in both the photogrammetry and computer vision (CV) communities. This is largely due to advances made towards the goal of automated relative orientation of multi-image networks covering untargetted (markerless) objects. There are now a number of CV-based algorithms, with accompanying open-source software, that can achieve multi-image orientation within narrow-baseline networks. From a photogrammetric standpoint, the results are typically disappointing as the metric integrity of the resulting models is generally poor, or even unknown, while the number of outliers within the image matching and triangulation is large, and generally too large to allow relative orientation (RO) via the commonly used coplanarity equations. On the other hand, there are few examples within the photogrammetric research field of automated markerless camera calibration to metric tolerances, and these too are restricted to narrow-baseline, low-convergence imaging geometry. The objective addressed in this paper is markerless automatic multi-image orientation, maintaining metric integrity, within networks that incorporate wide-baseline imagery. By wide-baseline we imply convergent multi-image configurations with convergence angles of up to around 90 °. An associated aim is provision of a fast, fully automated process, which can be performed without user intervention. For this purpose, various algorithms require optimisation to allow parallel processing utilising multiple PC cores and graphics processing units (GPUs) ...|$|E

