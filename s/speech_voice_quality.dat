3|3373|Public
40|$|Speech {{transmission}} quality measurement in cellular networks {{is a major}} {{indicator of}} performance for end-to-end quality of service standards. Many approaches have been proposed in the previous studies, but the results correlation with subjective experiments still need further optimization, especially for quality determination using languages with unique phonetic structures i. e. clicking sounds. Moreover, the evaluation test data is always {{a key element in}} order to obtain representative and consistent results. In this paper, we introduce TESPAR coding technique for the design of a new algorithm for <b>speech</b> (<b>voice)</b> <b>quality</b> measurement in cellular/wireless telecommunication networks. Our experiments show that the results from this algorithm correlates well with subjective experiments using a variety of speech samples. The proposed algorithm is also computationally efficient than the existing methods and is suitable for quality measurement of longer speech signals than the current 8 seconds speech test data mostly in use today. © 2009 IEEE...|$|E
40|$|BackgroundParkinson’s disease (PD) {{patients}} {{are affected by}} hypokinetic dysarthria, characterized by hypophonia and dysprosody, which worsens with disease progression. Levodopa’s (l-dopa) effect on quality of speech is inconclusive; no data are currently available for late-stage PD (LSPD). ObjectiveTo assess the modifications of speech and voice in LSPD following an acute l-dopa challenge. MethodLSPD patients [Schwab and England score 3 (MED ON) ] performed several vocal tasks before and after an acute l-dopa challenge. The following was assessed: respiratory support for <b>speech,</b> <b>voice</b> <b>quality,</b> stability and variability, speech rate, and motor performance (MDS-UPDRS-III). All voice samples were recorded and analyzed by a {{speech and language therapist}} blinded to patients’ therapeutic condition using Praat 5. 1 software. Results 24 / 27 (14 men) LSPD patients succeeded in performing voice tasks. Median age and disease duration of patients were 79 [IQR: 71. 5 – 81. 7] and 14. 5 [IQR: 11 – 15. 7] years, respectively. In MED OFF, respiratory breath support and pitch break time of LSPD patients were worse than the normative values of non-parkinsonian. A correlation was found between disease duration and voice quality (R[*]=[*] 0. 51; p[*]=[*] 0. 013) and speech rate (R[*]=[*]− 0. 55; p[*]=[*] 0. 008). l-Dopa significantly improved MDS-UPDRS-III score (20 %), with no effect on speech as assessed by clinical rating scales and automated analysis. ConclusionSpeech is severely affected in LSPD. Although l-dopa had some effect on motor performance, including axial signs, speech and voice did not improve. The applicability and efficacy of non-pharmacological treatment for speech impairment should be considered for speech disorder management in PD...|$|E
40|$|All of the {{experimental}} and theoretical work presented in this dissertation has been inspired by the general idea of applying event-related brain potential (ERP) measurement and assessment for practical purposes: cognitive diagnostics and Brain-Computer Interfaces (BCI) for paralyzed people. In Chapter 1, two new ERP paradigms are introduced, which were developed for the diagnostics of a particular cognitive function, the recognition of affective prosody. The affective state of a speaker can be identified from the prosody of her/his <b>speech.</b> <b>Voice</b> <b>quality</b> {{is the most important}} prosodic cue for emotion recognition from short verbal utterances and nonverbal exclamations, the latter conveying pure emotion, void of all semantic meaning. Two context violation ERP paradigms, passive oddball and priming, were adopted for the study the ERPs reflecting this recognition process. A new negative wave, the N 300, was found in the ERPs to contextually incongruous exclamations. This component was interpreted as analogical to the well known N 400 response to semantically inappropriate words. The N 300 appears to be a real-time psychophysiological measure of spontaneous emotion recognition from vocal cues, which could prove a useful tool for the examination of affective-prosody comprehension for the purposes of both fundamental psychological research and applied clinical diagnostics. Chapters 2 and 3 address the important issue of ERP component detection and quantification mostly from the perspective of individual data assessment, which is crucial for any reliable ERP diagnostics. The classical measures, area and peak, used in Chapter 1 are the most popular estimators of ERP components. Although they are in standard use in ERP research, they are very rough estimators, which heavily rely on visual inspection of the waveforms, and are thus prone to experimenter bias. Chapter 2 shows how the Continuous Wavelet Transform (CWT) can be used in ERP data analyses. A novel assessment method, the total-average-CWT, is introduced and demonstrated on the ERP data acquired in the emotional prosody experiments presented in Chapter 1. This method does not rely on visual inspection of the waveforms but allows for more automated detection of peaks. At the same time it provides more precise estimation of ERP components and yields larger statistical difference effects than classical methods do. The usage of the ERP technique in clinical applications for diagnostic purposes, requires special methods of EEG data assessment, based on single trial analysis. The total-average-CWT method introduced in Chapter 2 is an example for one such method. In Chapter 3, a new, largely improved version of the method, the t-CWT, is introduces. The t-CWT is based on the CWT and Student's t-statistics. The method was systematically tested in two prototypical ERP paradigms, oddball and sematnic priming, which belong to the basic tools of ERP-based cognitive diagnostics. The method was compared to other assessment procedures based on Principal Component Analysis (PCA) and the Discrete Wavelet Transform (DWT). Similarity to clinical settings was achieved by the individual assessment of each participants ERP data. Both whole waveforms and single ERP components were assessed by multivariate procedures including PCA data set reduction, Hotelling's test and a randomization test. The assessment of the whole ERP waveforms is particularly relevant to the paradigms of the context violation class introduced in Chapter 1. The results demonstrated the superiority of the t-CWT to the other assessment methods. In the study presented in Chapter 4, the detection and quantification method introduced in Chapter 3, the t-CWT, was applied in the classification of single ERP trials for the purposes of BCI. In this application, the t-CWT is used as a general feature extraction method, which provides the optimal variables describing the pattern that best discriminates between the ERPs reflecting different cognitive processes. The method has been validated in the International BCI Competition 2003, where it was a winner (provided best classification) on two ERP data sets acquired in two different BCI paradigms, P 300 speller and Slow Cortical Potential (SCP) feedback. In the P 300 speller paradigm the method provided results, which were as good as those obtained from simple and redundant features with a very powerful classifier based on machine learning, the Support Vector Machines (SVM). The t-CWT method has the advantage that it is very simple, intuitively plausible, readily visualizable and the extracted features have clear interpretation as ERP components. To summarize, two major results are presented in this dissertation: first, the newly found ERP component, the N 300 to inconsistent affective prosody expressed in emotional exclamations, and second, the newly developed t-CWT feature extraction method for fully automated detection and quantification of ERP components. Die Forschungsarbeit, die in der Dissertation dargestellt ist, wurde von der allgemeinen Idee inspiriert, die Technik der ereigniskorrelierten Hirnpotentiale (EKP) in klinischen Anwendungen mit gelähmten Patienten weiter zu entwickeln und zu verbessern. Im Kapitel 1 werden zwei neue EKP Paradigmen dargestellt, die speziell für die Untersuchung einer ganz bestimmten kognitiven Funktion entwickelt wurden: die Erkennung affektiver Prosodie, insbesondere der emotionalen Stimmqualität. Diese kognitive Funktion wurde bislang psychophysiologisch fast gar nicht erforscht, obwohl sie oft durch verschiedene neurologische und psychischen Erkrankungen beeinträchtigt wird. Das Ziel der Studie war, die EKP-Reaktion des Gehirns auf inkonsistente prosodische Reize zu untersuchen. Es wurden emotionale Ausrufe als Testreize benutzt. Die Vpn reagierten mit einer negativen EKP Welle, die N 300 auf inkonsistente Ausrufe, die mit dem Kontext der unmittelbar vorher präsentierten Emotionsnamen (Wörter) oder anderen emotionalen Ausrufen nicht vereinbar waren. Die gefundene EKP-Komponente ist der N 400 -Welle ähnlich, die von unpassenden Schlusswörtern in akustisch präsentierten Sätzen ausgelöst wird. Die Ergebnisse der Prosodiestudie zeigen, dass die emotionale Stimmqualität durch das Gehirn um ungefähr 100 Millisekunden schneller verarbeitet und erkannt wird als die semantische Information, die in gesprochenen Wörtern enthalten ist. Um die statistische Teststärke der im Kapitel 1 dargestellten Methode zu erhöhen und sie dadurch klinisch anwendbar zu machen, wurde eine neue EKP-Auswertungsmethode entwickelt, die im Kapitel 2 dargestellt wird. Die Methode, die eine bessere, automatische Detektion und Quantifizierung von EKP-Komponenten erlaubt, basiert auf der kontinuierlichen Wavelet-Transformation (CWT). Sie benutzt das Skalogramm der Mittelkurve, die aus allen Einzelsignalen, von allen Vpn und beiden Experimentalbedingungen berechnet wird. Die EKP-Komponenten werden als lokale Extrema in diesem „Totalmittelwertskalogramm“ identifiziert. Die entsprechende Wavelet-Maße der Wellenamplituden wurden mit den klassischen „area“- und „peak“-Maßen verglichen. Der im Kapitel 1 beschriebene N 300 -Effekt war statistisch viel signifikanter wenn er mit der CWT-Methode quantifiziert wurde, als wenn er mit den klassischen Methoden gemessen wurde. Das im Kapitel 2 eingeführte Konzept des Totalmittelwertes hat den Nachteil, dass sich positive und negative EKP-Wellen von verschiedenen Experimentalbedingungen in der Totalmittelkurve gelegentlich gegenseitig aufheben können. Um das auszuschließen, wurde eine neue CWT-basierte Methode entwickelt, die t-CWT, die im Kapitel 3 dargestellt wird. Das t-Wert-Skalogramm wird von dem Differenzskalogramm und dem statistischen t-Kennwert von Student berechnet. Es werden die CWTs der EEG-Einzelsignale berechnet und ein t-Wert wird für jeden einzelnen Punkt in der Zeit-Skala-Ebene ermittelt. Die Methode wurde an Daten von zwei prototypischen EKP-Paradigmen, Oddball und semantische Bahnung geprüft. Es wurden sowohl Gruppendaten als auch Individualdaten von einzelnen Vpn aus gewertet. Für den letzteren Fall ist die Nutzung von EEG-Einzelsignalen unumgänglich. Die Stärke der Methode wurde im Vergleich mit anderen Methoden demonstriert – Hauptkomponentenanalyse, diskrete Wavelet-Transforamtion (DWT), die o. g. „area“- und „peak“-Maße. Dabei wurden sowohl einzelne EKP-Komponenten automatisch detektiert und quantifiziert, als auch die ganzen EKP-Kurven multivariat statistisch verglichen. Der letztere Vergleich ist besonders relevant für die EKP-Paradigmen des Kontextverletzungstypus. Die im Kapitel 3 dargestellten Ergebnisse zeigen, dass die t-CWT-Methode größere statistische EKP-Effekte produziert als die anderen Auswertungsmethoden. Die im Kapitel 3 eingeführte t-CWT-Methode ist im wesentlichen eine Mustererkennungsmethode. Deswegen ist sie besonders gut geeignet für die Klassifikation von EEG-Einzelsignalen in Gehirn-Computer-Schnittstellen (BCI). Diese Anwendung wird im Kapitel 4 dargestellt. In BCIs werden EKPs als Kommunikationsmedium benutzt. Information wird von dem Gehirn zum Computer übertragen, indem der Benutzer willentlich seine EKPs steuert und die dabei entstehende EKP-Differenzen von dem Computer erkannt und als Bits eines Codes interpretiert werden. Die Erkennung der unterscheidenden Merkmale ist ein entscheidender Schritt jedes BCI-Algorithmus. Die t-CWT wurde als Mustererkennungsmethode in zwei unterschiedlichen BCI-Paradigmen eingesetzt – „P 300 -Speller“ und Selbstregulation von langsamen Hirnpotentialen (SCP). Die Ergebnisse wurden in dem internationalen BCI-Wettbewerb 2003 präsentiert und die t-CWT hat zweimal den ersten Platz gewonnen. Zusammengefasst werden in der Dissertation zwei Hauptergebnisse dargestellt – neue EKP-Paradigmen zur Untersuchung der Wahrnehmung affektiver Prosodie und neue EKP-Auswertungsmethoden zur automatischen Detektion und Quantifizierung von EKP-Komponenten...|$|E
40|$|Foundations of <b>Voice</b> and <b>Speech</b> <b>Quality</b> Perception {{starts out}} with the {{fundamental}} question of: "How do listeners perceive <b>voice</b> and <b>speech</b> <b>quality</b> and how can these processes be modeled?" Any quantitative answers require measurements. This is natural for physical quantities but harder to imagine for perceptual measurands. This book approaches the problem by actually identifying major perceptual dimensions of <b>voice</b> and <b>speech</b> <b>quality</b> perception, defining units wherever possible and offering paradigms to position these dimensions into a structural skeleton of perceptual <b>speech</b> and <b>voice</b> <b>quality.</b> The {{emphasis is placed on}} <b>voice</b> and <b>speech</b> <b>quality</b> assessment of systems in artificial scenarios. Many scientific fields are involved. This book bridges the gap between two quite diverse fields, engineering and humanities, and establishes the new research area of <b>Voice</b> and <b>Speech</b> <b>Quality</b> Perception...|$|R
40|$|Copyright © 2014 Carlos Monzo et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. This paper presents the perceptual experiments that were carried out in order to validate the methodology of transforming expressive <b>speech</b> styles using <b>voice</b> <b>quality</b> (VoQ) parameters modelling, along with the well-known prosody...|$|R
5000|$|In addition, speech {{synthesis}} {{is a valuable}} computational aid for the analysis and assessment of <b>speech</b> disorders. A <b>voice</b> <b>quality</b> synthesizer, developed by Jorge C. Lucero et al. at University of Brasilia, simulates the physics of phonation and includes models of vocal frequency jitter and tremor, airflow noise and laryngeal asymmetries. [...] The synthesizer {{has been used to}} mimic the timbre of dysphonic speakers with controlled levels of roughness, breathiness and strain.|$|R
40|$|International audienceIn {{this paper}} {{we present a}} {{flexible}} deterministic plus stochastic model (DSM) approach for parametric speech analysis and synthesis with high quality. The novelty of the proposed speech processing system lies in its extended means to estimate the un-voiced stochastic component and to robustly handle {{the transformation of the}} glottal excitation source. It is therefore well suited as speech system within the context of Voice Transformation and Voice Conversion. The system is evaluated {{in the context of a}} <b>voice</b> <b>quality</b> transformation on natural human <b>speech.</b> The <b>voice</b> <b>quality</b> of a <b>speech</b> phrase is altered by means of re-synthesizing the deterministic component with different pulse shapes of the glottal excitation source. A subjective listening test suggests that the speech processing system is able to successfully synthesize and arise to a listener the perceptual sensation of different <b>voice</b> <b>quality</b> characteristics. Additionally, improvements of the speech synthesis quality compared to a baseline method are demonstrated...|$|R
40|$|Cancer of {{the head}} and neck and its medical {{treatment}} and management, can have a negative impact on how a person sounds and talks. For the speech pathologist, rating a person’s <b>speech</b> intelligibility and <b>voice</b> <b>quality</b> is an important part of patient management. Rating someone’s <b>speech</b> or <b>voice,</b> however, can be difficult task to perform objectively as a listener’s ratings are often inconsistent. Computerized ratings, on the other hand, are consistent. This thesis has focused on developing automatic prediction models for <b>speech</b> intelligibility and <b>voice</b> <b>quality</b> assessment for two groups of speakers treated for head and neck cancer. The first group discussed in Part I of this thesis are people with advanced tumours in the head and neck. These people received a type of non-surgical cancer treatment, called concurrent chemoradiotherapy (CCRT). This type of treatment can affect a person’s <b>voice</b> and <b>speech.</b> The second group of people were treated for advanced tumors in the larynx. These people underwent a total laryngectomy (TL), in which the larynx (also known as the ’voice box’) is removed. After a TL, speaking is possible with the aid of a valve that redirects air past vibrating structures in the neck towards the mouth. This type of speech is called tracheoesophageal speech and it sounds very different to how a person sounded before the surgery...|$|R
40|$|International audienceSpeech {{enhancement}} like {{noise reduction}} is usually done as pre-processing before <b>speech</b> encoding. <b>Voice</b> <b>Quality</b> Enhancement algorithms have recently {{appeared as a}} solution to improve speech quality within networks. They involve decoding the bitstream, performing the enhancement on the decoded signal and re-encoding the processed speech. This method is computationally expensive, implies additive delay and reduces speech quality because of codec tandeming. As an alternative approach, modification of the bitstream itself would definitely avoid tandeming. In this context, a new approach to reduce distortions due to the environmental background noise is proposed in this article through a modification of the Linear Prediction filter in noisy condition. This solution is compared with classical solution based on decoding, applying Wiener filtering in the frequency domain and re-encoding the signal...|$|R
40|$|The {{present study}} proposes a new {{parameter}} for identifying breathy to tense <b>voice</b> <b>qualities</b> {{in a given}} speech segment using measurements from the wavelet transform. Techniques that can deliver robust information on the <b>voice</b> <b>quality</b> of a <b>speech</b> segment are desirable as they can help tune analysis strategies {{as well as provide}} automatic <b>voice</b> <b>quality</b> annotation in large corpora. The method described here involves wavelet-based decomposition of the speech signal into octave bands and then fitting a regression line to the maximum amplitudes at the different scales. The slope coefficient is then evaluated in terms of its ability to differentiate <b>voice</b> <b>qualities</b> compared to other parameters in the literature. The new parameter (named here Peak Slope) was shown to have robustness to babble noise added with signal to noise ratios as low as 10 dB. Furthermore, the proposed parameter was shown to provide better differentiation of breathy to tense <b>voice</b> <b>qualities</b> in both vowels and running <b>speech.</b> Index Terms: <b>Voice</b> <b>quality,</b> glottal source, wavelets 1...|$|R
40|$|International audienceIn {{this paper}} {{we present a}} {{flexible}} framework for paramet-ric speech analysis and synthesis with high quality. It constitutes an extended source-filter model. The novelty of the proposed speech processing system lies in its extended means to use a Deterministic plus Stochastic Model (DSM) for the estimation of the unvoiced stochastic component from a speech recording. Further contributions are the efficient and robust means to extract the Vocal Tract Filter (VTF) and the modelling of energy variations. The system is evaluated {{in the context of}} two <b>voice</b> <b>quality</b> transformations on natural human <b>speech.</b> The <b>voice</b> <b>quality</b> of a <b>speech</b> phrase is altered by means of re-synthesizing the deterministic component with different pulse shapes of the glottal excitation source. A Gaussian Mixture Model (GMM) is used in one test to predict energies for the re-synthesis of the deterministic and the stochastic component. The subjective listening tests suggests that the speech processing system is able to successfully synthesize and arise to a listener the perceptual sensation of different <b>voice</b> <b>quality</b> characteristics. Additionally, improvements of the speech synthesis quality compared to a baseline method are demonstrated...|$|R
40|$|Abstract: Background. Carcinoma of {{the larynx}} {{is the most common}} cancer {{affecting}} the head and neck region. In Northern Europe, early laryngeal cancer is almost universally treated by irradiation, but elsewhere it is treated by surgery. The main aim {{of this study was to}} determine whether there was any difference in survival between the two main therapeutic options. The secon-dary aim was to assess <b>speech</b> and <b>voice</b> <b>quality</b> in a small, randomized sample of patients from each treatment group. Methods. The subjects investigated were 488 patients with T 1 – 2, N 0 squamous cell carcinoma {{of the larynx}}. The patients form an unselected sequential group of our institution’s experience with treating this disease over three decades. Four hundred nineteen patients were treated by irradiation, and 69 were treated with surgery. Most surgical patients were treated earlier in the series, whereas radiotherapy later became th...|$|R
40|$|This paper {{presents}} the perceptual experiments that {{were carried out}} in order to validate the methodology of transforming expressive <b>speech</b> styles using <b>voice</b> <b>quality</b> (VoQ) parameters modelling, along with the well-known prosody (F 0, duration, and energy), from a neutral style {{into a number of}} expressive ones. The main goal was to validate the usefulness of VoQ in the enhancement of expressive synthetic speech in terms of speech quality and style identification. A harmonic plus noise model (HNM) was used to modify VoQ and prosodic parameters that were extracted from an expressive speech corpus. Perception test results indicated the improvement of obtained expressive speech styles using VoQ modelling along with prosodic characteristics...|$|R
40|$|Background and Aim: Total {{laryngectomy}} following laryngeal {{cancer has}} many sequelae, that loss of voice {{is the most}} important of them. Tracheoesophageal puncture (TEP) and prosthesis insertion has evolved into the most widely used and accepted technique for vocal rehabilitation. Materials and Methods: 10 patients that underwent TEP in Amir Alam and Imam Khomeini hospitals from Feb. 2002 through Nov. 2003; were included in this study. Prosthesis insertion in 4 patients is primary and in 6 patients is secondary; and all patients are men. Results: The age of patients was between 50 to 70. 90 % of patients had history of cigarette smoking and 10 % of them had history of drinking alcohol. Salivary leakage was seen in 30 % of patients that was improved with conservative management. Fluency of speech in 30 % of patients and intelligibility of <b>speech</b> & <b>voice</b> <b>quality</b> in 40 % of patients is good. Conclusion: We could conclude that TEP has less complication & better speech results of other vocal rehabilitation methods. Carefully selection of patients & size of prosthesis has important role in results of TEP...|$|R
40|$|HMM-based speech {{synthesis}} offers {{a way to}} generate <b>speech</b> with different <b>voice</b> <b>qualities.</b> However, sometimes databases contain certain inherent <b>voice</b> <b>qualities</b> {{that need to be}} parametrized properly. One example of this is vocal fry typically occurring at the end of utterances. A popular mixed excitation vocoder for HMM-based {{speech synthesis}} is STRAIGHT. The standard STRAIGHT is optimized for modal voices and may not produce high <b>quality</b> with other <b>voice</b> types. Fortunately, due to the flexibility of STRAIGHT, different F 0 and aperiodicity measures can be used in the synthesis without any inherent degradations in speech quality. We have replaced the STRAIGHT excitation with a representation based on a robust F 0 measure and a carefully determined two-band voicing. According to our analysis-synthesis experiments, the new parameterization can improve the speech quality. In HMM-based speech synthesis, the quality is significantly improved especially due to the better modeling of vocal fry. Index Terms: speech synthesis, hidden Markov models, vocal fry, mixed excitation, STRAIGH...|$|R
40|$|Voice {{imitation}} can {{be effective}} in different ways, both consciously and unconsciously, in situations such as language acquisition and for entertainment. The human voice is flexible, {{and it is possible}} to change the vocal tract in order to imitate other people’s speech behaviour. To succeed with the imitation, {{we have to figure out}} important and characteristic features of the target speaker. Such features may be the dialect, the intonation pattern, the <b>speech</b> style, <b>voice</b> <b>quality,</b> as well as the pronunciation of sound segments. A professional impersonator, who reproduces another speaker’s <b>voice</b> and <b>speech</b> behaviour, has to be aware of how to change the vocal tract and get close to the voice of the target speaker. For an impersonator the aim of voice imitation is to entertain or cheat. He probably has to exaggerate some of the features of the target speaker’s speech behaviour, like a caricature, for it to be entertaining. One hypothesis is that if he is close to the target speaker in some critical features, he may yet fail with other less important features in his imitation and the audience will still have the impression of a successful voice imitation (Zetterholm 1997) ...|$|R
40|$|Parametric speech {{synthesis}} has received increased attention {{in recent years}} follow-ing the development of statistical HMM-based {{speech synthesis}}. However, the speech produced using this method still does not sound as natural as human speech and there is limited parametric flexibility to replicate <b>voice</b> <b>quality</b> aspects, such as breathiness. The hypothesis of this thesis is that <b>speech</b> naturalness and <b>voice</b> <b>quality</b> can be more accurately replicated by a HMM-based speech synthesiser using an acoustic glot-tal source model, the Liljencrants-Fant (LF) model, to represent the source component of speech instead of the traditional impulse train. Two different analysis-synthesis methods were developed during this thesis, in or-der to integrate the LF-model into a baseline HMM-based speech synthesiser, {{which is based on}} the popular HTS system and uses the STRAIGHT vocoder. The first method, which is called Glottal Post-Filtering (GPF), consists of passing a chosen LF-model signal through a glottal post-filter to obtain the source signal and then generating speech, by passing this source signal through the spectral envelope filter. The sys...|$|R
40|$|In this paper, {{we present}} a new formant-type speech analysissynthesis system based on the ARX (Auto-Regressive with Exogenous Input) speech {{production}} model. The model consists of cascade formant-antiformant synthesizers driven by a voicing source and an unvoiced turbulent noise source. One of the key features of the proposed method {{is that we have}} an algorithm to automatically measure the voicing source, unvoiced source and formant-antiformant parameters of the synthesizer directly from natural speech waveforms. After having automatically obtained estimates of the parameters from natural speech, one can manipulate the estimates using a flexible editing tool that has been developed {{as a part of the}} system. By changing values of the fundamental frequency, glottal open quotient, spectral tilt parameter, turbulent noise level, formant-antiformant frequencies and bandwidths, we can synthesize natural sounding <b>speech</b> with various <b>voice</b> <b>qualities</b> including modal, breathy, tense, and whisper voice...|$|R
40|$|Abstract—This paper proposes an {{analysis}} method {{to separate the}} glottal source and vocal tract components of speech that is called Glottal Spectral Separation (GSS). This method can produce high-quality synthetic speech using an acoustic glottal source model. In the source-filter models commonly used in speech technology applications it is assumed the source is a spectrally flat excitation signal and the vocal tract filter can be represented by the spectral envelope of speech. Although this model can produce high-quality speech, it has limitations for voice transformation {{because it does not}} allow control over glottal parameters which are correlated with <b>voice</b> <b>quality.</b> The main problem with using a speech model that better represents the glottal source and the vocal tract filter is that current analysis methods for separating these components are not robust enough to produce the same speech quality as using a model based on the spectral envelope of speech. The proposed GSS method is an attempt to overcome this problem, and consists of the following three steps. Initially, the glottal source signal is estimated from the speech signal. Then, the speech spectrum is divided by the spectral envelope of the glottal source signal in order to remove the glottal source effects from the speech signal. Finally, the vocal tract transfer function is obtained by computing the spectral envelope of the resulting signal. In this work, the glottal source signal is represented using the Liljencrants-Fant model (LF-model). The experiments we present here show that the analysis-synthesis technique based on GSS can produce speech comparable to that of a high-quality vocoder that is based on the spectral envelope representation. However, it also permit control over <b>voice</b> <b>qualities,</b> namely to transform a modal voice into breathy and tense, by modifying the glottal parameters. Index Terms—Glottal Spectral Separation, LF-model, parametric <b>speech</b> synthesis, <b>voice</b> <b>quality</b> transformation...|$|R
5000|$|Qualcomm’s [...] (4G) is a {{suite of}} <b>voice</b> <b>speech</b> codecs used by CDMA {{networks}} that allows the network operators to dynamically prioritize <b>voice</b> <b>quality</b> to increase network capacity while maintaining <b>voice</b> <b>quality.</b> Currently, the 4GV suite offers EVRC-B and EVRC-WB.|$|R
40|$|Abstract: Nigeria with infant GSM mobile {{subscribers}} {{is mostly}} {{concerned about the}} quality of services they received from the GSM operators in the country. Nigeria is also concerned about the factors affecting <b>voice</b> <b>quality</b> such as degradation of <b>speech</b> (<b>voice),</b> messages and other anomalies encountered. The paper focuses on the evaluation of <b>voice</b> <b>quality</b> of some GSM operators in the country. It looks into seven items used to measure the dimension of <b>voice</b> <b>quality</b> such as echo of the sender’s voice, distortion in the receiver’s voice, volume of the receiver’s voice and overall <b>voice</b> <b>quality</b> perception. In this paper a survey design was used. The sample consists of 300 subscribers and these were selected by stratified random sampling technique. Questionnaire containing twenty six (26) items on factors affecting <b>voice</b> <b>quality</b> level in mobile communication was administered to the various categories of subscribers of mobile communication in Oyo State, Nigeria. Mean and standard deviation was used to answer six research questions. Results show that the highest overall mean opinion score is 3. 10 and the least value is 2. 93. It therefore implies that the <b>voice</b> <b>quality</b> levels are above average for the four GSM service providers under investigatio...|$|R
40|$|International audienceVerbal {{irony is}} {{a mode of}} {{expression}} in which what is stated differs from (or is even opposed to) what is meant. Irony exists {{in the majority of}} the languages and cultures of the world (Pexman, 2008). Some researchers have proposed that acoustic irony cues are only employed if the common ground is not sufficient to indicate the intended message (Cutler, 1974). More recent research has shown that ironic content can be identified even in absence of contextual cues thanks to global acoustic/prosodic cues (Bryant & Fox Tree 2002). However, we still do not know what is the actual role of prosody, in particular of intonational phonology features (Ladd, 1996 / 2008), in irony comprehension. Concerning actual acoustic cues, sarcasm appears to be encoded in speech through various global manipulations in acoustic parameters such as fundamental frequency (f 0), amplitude, <b>speech</b> rate, <b>voice</b> <b>quality</b> and vowel hyperarticulation (Attardo et al., 2003; Bryant & Fox Tree, 2005; Cheang & Pell, 2008; Rockwell, 2000; Sharrer & Christman, 2011; inter alia). In this study, we explore the expression of sarcasm in French, for which phonological data are still lacking. Specifically, here we test the acceptability of prototypical sarcastic tonal contours in presence of matching or conflicting contextual cues...|$|R
40|$|Impaired social {{communication}} and social reciprocity {{are the primary}} phenotypic distinctions between autism spectrum dis-orders (ASD) and other developmental disorders. We investi-gate quantitative conversational cues in child-psychologist in-teractions using acoustic-prosodic, turn-taking, and language features. Results indicate the conversational quality degraded for children with higher ASD severity, as the child exhibited difficulties conversing and the psychologist varied her speech and language strategies to engage the child. When interacting with children with increasing ASD severity, the psychologist exhibited higher prosodic variability, increased pausing, more <b>speech,</b> atypical <b>voice</b> <b>quality,</b> and less use of conventional con-versational cue such as assents and non-fluencies. Children with increasing ASD severity spoke less, spoke slower, responded later, had more variable prosody, and used personal pronouns, affect language, and fillers less often. We also investigated the predictive power of features from interaction subtasks with varying social demands placed on the child. We found that acoustic prosodic and turn-taking features were more predictive during higher social demand tasks, and that the most predictive features vary with context of interaction. We also observed that psychologist language features may be robust {{to the amount of}} speech in a subtask, showing significance even when the child is participating in minimal-speech, low social-demand tasks. Index Terms: autism spectrum disorders, atypical prosody, so-cial reciprocity, turn-taking, language cue...|$|R
40|$|Abstract. This work {{describes}} {{the development of}} an automatic estimator of perceptual femininity (PF) of an input utterance using speaker verification techniques. The estimator was designed for its clinical use and the target speakers are Gender Identity Disorder (GID) clients, especially MtF (Male to Female) transsexuals. The voice therapy for MtFs, which is conducted by the second author, comprises three kinds of training; 1) raising the baseline F 0 range, 2) changing the baseline <b>voice</b> <b>quality,</b> and 3) enhancing F 0 dynamics to produce an exaggerated intonation pattern. The first two focus on static acoustic properties of <b>speech</b> and the <b>voice</b> <b>quality</b> is mainly controlled by size and shape of the articulators, which can be acoustically characterized by the spectral envelope. Gaussian Mixture Models (GMM) of F 0 values and spectrums were built separately for biologically male speakers and female ones. Using the four models, PF was estimated automatically for each of 142 utterances of 111 MtFs. The estimated values were compared with the PF values obtained through listening tests with 3 female and 6 male novice raters. Results showed very high correlation (R= 0. 86) between the two, which is comparable to the intra- and inter-rater correlation. ...|$|R
40|$|This paper {{describes}} {{the development of}} an estimator of perceptual femininity (PF) of an input utterance using speaker recognition tech-niques. The estimator was designed for its clinical use and the target speakers are Gender Identity Disorder (GID) clients, especially MtF (Male to Female) transsexuals. The voice therapy for MtFs is com-posed of three kinds of training; 1) raising the baseline F 0 range, 2) changing the baseline <b>voice</b> <b>quality,</b> and 3) enhancing F 0 dynamics to produce an exaggerated intonation pattern. The first two focus on static acoustic properties of <b>speech</b> and the <b>voice</b> <b>quality</b> is mainly controlled by size and shape of the articulators, which can be acousti-cally characterized by the spectral envelope. GaussianMixture Mod-els (GMM) of F 0 values and spectrums were built separately for bi-ologically male speakers and female ones. Using the four models, PF was estimated automatically for each of 142 utterances of 111 MtFs. The estimated values were compared with the PF values ob-tained through listening tests. Results showed very high correlation (R= 0. 86), which is comparable to the intra-rater correlation. Index Terms — Femininity, GID, speaker recognition, GMM 1...|$|R
40|$|Bone-conducted (BC) speech can be {{used instead}} of air-conducted (AC) speech in an {{extremely}} noisy environment. However, its intelligibility is degraded when transmitted through bone-conduction. Therefore, <b>voice</b> <b>quality</b> and the intelligibility of BC speech need to be blindly improved in actual communication through speech {{and this is a}} challenging new topic in the field of speech signal processing. We proposed a linear prediction (LP) based model to restore BC <b>speech</b> to improve <b>voice</b> <b>quality</b> in a previous study. While other methods such as Long-term Fourier transform need to use numerous AC speech parameters to restore BC speech, the model we proposed demonstrated the expressed ability of blindly restoring BC speech by predicting AC-LP coefficients from BC-LP coefficients. We improved the previous model by (1) extending long-term processing to frame-basis processing, (2) using line spectral frequency (LSF) coefficients on an LP representation, and (3) using a recurrent neural network for predicting parameters. We evaluated the improved model in comparison with others to find out whether it could adequately improve <b>voice</b> <b>quality</b> and the intelligibility of BC speech, using objective measures (i. e., LSD, MCD, and LCD) and carrying out a subjective measure — a Japanese-word intelligibility test (JWIT). The experimental results proved significant improvements to our newly proposed models (LSF and LSF-SRN). The LSF model demonstrated it had significant capabilities for improving BC speech, i. e., both <b>voice</b> <b>quality</b> and intelligibility of speech. Our proposed model, LSF-SRN, demonstrated an expressed capability for improving the intelligibility of BC speech even when using blind restoration. リサーチレポート（北陸先端科学技術大学院大学情報科学研究科...|$|R
40|$|Abstract: Background. Endoscopic {{management}} of laryn-geal carcinoma has gained popularity among laryngologists {{based on the}} good oncologic and functional results. We eval-uated the <b>voice</b> <b>quality</b> after laser cordectomy for early glottic cancer {{in a variety of}} vocal situations and its relation with the extension of resection and the age. Methods. We conducted a cross-sectional study of <b>voice</b> <b>quality</b> in 42 consecutive male patients treated for T 1 glottic car-cinoma with laser cordectomy. Patients were compared with 21 controls. <b>Voice</b> <b>quality</b> was self-assessed by the patients. Per-ceptual analysis was done by a speech pathologist on a running speech sample [GRBAS (grade, roughness, breathiness, asthe-nicity, strain) ]. Acoustic analysis included fundamental fre-quency (F 0), jitter, shimmer, noise to harmonic ratio (N/H), and maximum phonation time (MPT) on the sustained vowels /a / and /i/, and on various running <b>speech</b> <b>voice</b> samples. Results. Distribution of the patients included in the study b...|$|R
50|$|Qualcomm’s fourth {{generation}} vocoder (4GV) is {{a suite of}} <b>voice</b> <b>speech</b> codecs expected {{to be used in}} future 4G networks as well CDMA networks, that allows the network operators to dynamically prioritize <b>voice</b> <b>quality</b> to increase network capacity while maintaining <b>voice</b> <b>quality.</b> Currently, the 4GV suite offers EVRC-B and EVRC-WB.|$|R
40|$|A great {{challenge}} for text-to-speech synthesis {{is to produce}} ex- pressive speech. The main {{problem is that it}} is difficult to syn- thesise high-quality speech using expressive corpora. With the increasing interest in audiobook corpora for speech synthesis, there is a demand to synthesise speech which is rich in prosody, emotions and voice styles. In this work, Self-Organising Fea- ture Maps (SOFM) are used for clustering the <b>speech</b> data using <b>voice</b> <b>quality</b> parameters of the glottal source, in order to map out the variety of voice styles in the corpus. Subjective evalu- ation showed that this clustering method successfully separated the speech data into groups of utterances associated with dif- ferent voice characteristics. This work can be applied in unit- selection synthesis by selecting appropriate data sets to synthe- sise utterances with specific voice styles. It can also be used in parametric speech synthesis to model different voice styles separately.   QC 20160426 </p...|$|R
40|$|In this paper, we {{describe}} a new database with audio recordings of non-native (L 2) speakers of English, and the perceptual evaluation experiment conducted with native English speakers {{for assessing the}} prosody of each recording. These annotations are then used to compute the gold standard using different methods, {{and a series of}} regression experiments is conducted to evaluate their impact on the performance of a regression model predicting the degree of Abstract naturalness of L 2 speech. Further, we compare the relevance of different feature groups modelling prosody in general (without speech tempo), speech rate and pauses modelling <b>speech</b> tempo (fluency), <b>voice</b> <b>quality,</b> and a variety of spectral features. We also discuss the impact of various fusion strategies on performance. Overall, our results demonstrate that the prosody of non-native speakers of English as L 2 can be reliably assessed using supra- segmental audio features; prosodic features seem {{to be the most important}} ones...|$|R
40|$|In this paper, {{automatic}} assessment {{models are}} developed for two perceptual variables: <b>speech</b> intelligibility and <b>voice</b> <b>quality.</b> The models are developed and tested on a corpus of Dutch tracheoesophageal (TE) speakers. In this corpus, each speaker read a text passage of approximately 300 syllables and two speech therapists provided consensus {{scores for the}} two perceptual variables. Model accuracy and stability are investigated {{as a function of}} the amount of speech that is made available for speaker assessment (clinical setting). Five sets of automatically generated acoustic-phonetic speaker features are employed as model inputs. In Part I, models taking complete feature sets as inputs are compared to models taking only the features which are expected to have sufficient support in the speech available for assessment. In Part II, the impact of phonetic content and stimulus length on the computer-generated scores is investigated. Our general finding is that a text encompassing circa 100 syllables is long enough to achieve close to asymptotic accuracy...|$|R
40|$|Surgical {{removal of}} the larynx results in radically reduced {{production}} of <b>voice</b> and <b>speech.</b> To improve <b>voice</b> <b>quality</b> a voice-producing element (VPE) is developed, based on the lip principle, called after the lips of a musician while playing a brass instrument. To optimize the VPE, a numerical model is developed. In this model, the finite element method is {{used to describe the}} mechanical behavior of the VPE. The flow is described by two-dimensional incompressible Navier–Stokes equations. The interaction between VPE and airflow is modeled by placing the grid of the VPE model in the grid of the aerodynamical model, and requiring continuity of forces and velocities. By applying and increasing pressure to the numerical model, pulses comparable to glottal volume velocity waveforms are obtained. By variation of geometric parameters their influence can be determined. To validate this numerical model, an in vitro test with a prototype of the VPE is performed. Experimental and numerical results show an acceptable agreement. ...|$|R
40|$|This {{contribution}} summarizes {{our recent}} investigations {{in the use}} of the glottal source for characterizing expressive voice. It is organized in three main parts. First, we study which methods are the most suited for estimating the glottal flow directly from the speech signal. This is a particularly difficult task which is a typical case of blind separation, since neither the vocal tract nor the glottal components are observable. Secondly, we focus on the parameterization of the resulting glottal flow estimates, highlighting which features are the most appropriate to characterize it. Finally, we report our results of glottal analysis of expressive speech, revealing interesting modifications in the glottal behavior when producing Lombard <b>speech,</b> various <b>voice</b> <b>qualities,</b> or hypo/hyperarticulated <b>speech.</b> As mentioned above, reliably and accurately estimating the glottal source from speech recordings is a complex issue. This usually requires to process speech frames synchronized on glottal closure instants and whose length is proportional to the pitch period. For this, three of the most efficient approaches are the following [1]. The Closed Phase Inverse Filtering (CPIF, [2]) method computes an estimation of the vocal tract response during the glottal closed phase, during which the effects of the subglottal cavities are minimized. The Iterative Adaptive Inverse Filtering (IAIF, [3]) technique is based on an iterative refinement of both the vocal tract and the glottal components in order to improve the qualit...|$|R
40|$|This study {{examined}} the quality of iPhone recordings for acoustic measurements of <b>speech</b> and <b>voice</b> <b>quality.</b> A selection of acoustic measures were extracted from voice samples recorded using the “voice memo” application in an iPhone and compared with those derived from signals directly digitized (DD) in a laptop via a 12 -bit A/D converter. Participants were 11 healthy adults, including six females and five males, aged between 27 to 67 years (Mean = 41. 8 years, SD = 16. 7). The participant was asked to read the first six sentences of the “rainbow passage”. In addition, two {{participants were asked to}} produce sustained vowels (/i/, /a/, and /u/) and a sentence (“We saw two cars”) ten times. The simultaneously recorded iPhone and DD signals were analysed to derive 10 acoustic measures, including spectral tilt for the whole sentence and fundamental frequency (F 0), percent jitter, percent shimmer, signal-to-noise ratio, amplitude of the first harmonic relative to that of the second harmonic, singing power ratio, and frequencies of the first and second formants (F 1 and F 2), and vowel space area for the vowel segment. A series of Pearson’s correlation procedures revealed that measures from iPhone and DD signals were highly correlated. Findings of the vowel effect on the experimental measures obtained from iPhone signals were consistent with those from DD signals. However, the mean normalized absolute differences between measures from iPhone and DD signals are optimal (i. e., lower than 20 %) only for F 0, F 1, and F 2. These findings suggest that iPhone recordings are as adequate as other types of high quality digital recordings for acoustic measurements of <b>voice</b> <b>quality</b> but most <b>voice</b> measures from different digital recording systems are not directly comparable...|$|R
40|$|Dysarthria, a {{neurological}} motor speech disorder caused by lesions {{to the central}} and peripheral nervous system, accounts for over 40 % of neurological disorders referred to pathologists in 2013 [1]. This affects the ability of speakers to control the movement of speech production muscles due to muscle weakness. Dysarthria is characterised by reduced loudness, high pitch variability, monotonous <b>speech,</b> poor <b>voice</b> <b>quality</b> and reduced intelligibility [2]. Current techniques for dysarthria assessment are based on perception, which do not give objective measurements for the severity of this speech disorder. There is therefore a need to explore objective techniques for dysarthria assessment and treatment. The goal {{of this research is}} to identify and extract the main acoustic features which can be used to describe the type and severity of this disorder. An acoustic feature extraction and classification technique is proposed in this work. The proposed method involves a pre-processing stage where audio samples are filtered to remove noise and resampled at 8 kHz. The next stage is a feature extraction stage where pitch, intensity, formants, zero-crossing rate, speech rate and cepstral coefficients are extracted from the speech samples. Classification of the extracted features is carried out using a single layer neural network. After the classification, a treatment tool is to be developed to assist patients, through tailored exercises, to improve their articulatory ability, intelligibility, intonation and <b>voice</b> <b>quality.</b> Consequently, this proposed technique will assist speech therapists in tracking the progress of patients over time. It will also provide an acoustic objective measurement for dysarthria severity assessment. Some of the potential applications of this technology include management of cognitive speech impairments, treatment of speech difficulties in children and other advanced speech and language applications...|$|R
40|$|Thesis (Ph. D.) [...] Wichita State University, Fairmount College of Liberal Arts and Sciences, Dept. of PsychologySmartphones have {{dominated}} the American mobile phone market since mid- 2012 (Nielsen, 2012). Forty-five percent of American adults own smartphones (Smith, 2012), and report frequently using their devices for the same tasks: text messaging, emailing, and social networking (comScore, 2013), {{all of which are}} dependent on text entry. Hardware and/or software text input options are available, but recently vendors have begun to abandon physical keyboards in their device portfolios. Empirical research does not indicate, comparatively, which smartphone text entry methods are the fastest, most accurate, and most preferred by consumers. Furthermore, potential relationships that users' anthropometry, <b>voice</b> <b>qualities</b> and age-related limitations may have with the accuracy and satisfaction of these input methods have not been addressed. Two experiments explored the impact that five frequently used smartphone input methods (physical and onscreen Qwerty keyboards, tracing, handwriting, and voice recognition) had on novice user performance, perceived workload, satisfaction, and preference. Relationships between anthropometry, <b>speech</b> and <b>voice</b> <b>qualities</b> with the input methods were also examined. Results from Study 1 demonstrate that younger adults were fastest with voice recognition, but committed fewer errors, reported lower workload, higher satisfaction, and preference with the physical keyboard. Results from Study 2 revealed that older adults were fastest, most accurate, reported lower workload, higher satisfaction and preference for voice recognition. A comparison between age groups suggested that older adults were generally slower and committed more errors. However, performance differences were not found between age groups for voice recognition entry rates and word error rates, as well as for physical Qwerty uncorrected error rates. Additionally, both groups had similar satisfaction and preference ratings for most methods...|$|R
