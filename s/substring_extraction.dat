6|1|Public
5000|$|New scalar {{operations}} such as string concatenation and <b>substring</b> <b>extraction,</b> {{date and}} time mathematics, and conditional statements.|$|E
40|$|Motivated by the {{imminent}} growth of massive, highly redundant genomic databases, we study {{the problem of}} compressing a string database while simultaneously supporting fast random access, <b>substring</b> <b>extraction</b> and pattern matching to the underlying string(s). Bille et al. (2011) recently showed how, given a straight-line program with r rules for a string s of length n, we can build an r-word data structure {{that allows us to}} extract any substring of length m in n + m time. They also showed how, given a pattern p of length m and an edit distance (k ≤ m), their data structure supports finding all approximate matches to p in s in r ((m k, k^ 4 + m) + n) + time. Rytter (2003) and Charikar et al. (2005) showed that r is always at least the number z of phrases in the LZ 77 parse of s, and gave algorithms for building straight-line programs with z n rules. In this paper we give a simple z n-word data structure that takes the same time for <b>substring</b> <b>extraction</b> but only z (m k, k^ 4 + m) + time for approximate pattern matching. Comment: Journal version of ISAAC ' 11 pape...|$|E
40|$|In this paper, {{we propose}} a {{stand-alone}} mobile visual search {{system based on}} binary features and the bag-of-visual words framework. The contribution {{of this study is}} three-fold: (1) We propose an adaptive <b>substring</b> <b>extraction</b> method that adaptively extracts informative bits from the original binary vector and stores them in the inverted index. These substrings are used to refine visual word-based matching. (2) A modified local NBNN scoring method is proposed in the context of image retrieval, which considers the density of binary features in scoring each feature matching. (3) In order to suppress false positives, we introduce a convexity check step that imposes a convexity constraint on the configuration of a transformed reference image. The proposed system improves retrieval accuracy by 11 % compared with a conventional method without increasing the database size. Furthermore, our system with the convexity check does not lead to false positive results...|$|E
40|$|In this paper, {{we propose}} Simplified Regular Expression (SRE) signature, which uses {{multiple}} sequence alignment techniques, drawn from bioinformatics, {{in a novel}} approach to generating more accurate exploit-based signatures. We also provide formal definitions of what is "a more specific" and what is "the most specific" signature for a polymorphic worm and show that the most specific exploit-based signature generation is NP-hard. The approach involves three steps: multiple sequence alignment to reward consecutive <b>substring</b> <b>extractions,</b> noise elimination to remove noise effects, and signature transformation to make the SRE signature compatible with current IDSs. Experiments {{on a range of}} polymorphic worms and real-world polymorphic shellcodes show that our bioinformatics approach is noise-tolerant and as that because it extracts more polymorphic worm characters, like one-byte invariants and distance restrictions between invariant bytes, the signatures it generates are more accurate and precise than those generated by some other exploit-based signature generation schemes. Department of Computin...|$|R
40|$|Many {{automatic}} testing, analysis, and verification {{techniques for}} {{programs can be}} effectively reduced to a constraint-generation phase followed by a constraint-solving phase. This separation of concerns often leads to more effective and maintainable software reliability tools. The increasing efficiency of off-the-shelf con-straint solvers makes this approach even more compelling. However, there are few effective and sufficiently expressive off-the-shelf solvers for string constraints generated by analysis of string-manipulating programs, so researchers end up implementing their own ad-hoc solvers. To fulfill this need, we designed and implemented HAMPI, a solver for string constraints over bounded string variables. Users of HAMPI specify constraints using regular expressions, context-free grammars, equality between string terms, and typical string operations such as concatenation and <b>substring</b> <b>extraction.</b> HAMPI then finds a string that satisfies all the constraints or reports that the constraints are unsatisfiable. We demonstrate HAMPI’s expressiveness and efficiency by applying it to program analysis and automated testing. We used HAMPI in static and dynamic analyses for finding SQL injection vulnerabilities in Web applications {{with hundreds of thousands}} of lines of code. We also used HAMPI in the context of automated bug finding in C programs using dynamic systematic testing (also known as concolic testing). We then compared HAMPI with another string solver, CFGAnalyzer, and show that HAMPI is several times faster. HAMPI’s source code, documentation, and experimental data are available a...|$|E
40|$|In {{order to}} extract rigid {{expressions}} {{with a high}} frequency of use, new algorithm that can efficiently extract both uninterrupted and interrupted collocations from very large corpora has been proposed. The statistical method recently proposed for calculating N-gram of m'bitrary N {{can be applied to}} the extraction of uninterrupted collocations. But this method posed problems that so large volumes of fractional and unnecessary expressions are extracted that it was impossible to extract interrupted collocations combining the results. To solve this problem, this paper proposed a new algorithm that restrains extraction of unnecessary substrings. This is followed by the proposal of a method that enable to extract interrupted collocations. The new methods are applied to Japanese newspaper articles involving 8. 92 million characters. In the case of uninterrupted collocations with string length of 2 or mere characters and frequency of appearance 2 or more times, there were 4. 4 millions types of expressions (total frequency of 31. 2 millions times) extracted by the N-gram method. In contrast, the new method has reduced this to 0. 97 million types (total frequency of 2. 6 million times) revealing a substantial reduction in fractional and unnecessary expressions. In the case of interrupted collocational <b>substring</b> <b>extraction,</b> combining the substring with frequency of 10 times or more extracted by the first method, 6. 5 thousand types of pairs of substrings with the total frequency of 21. 8 thousands were extracted. I...|$|E

