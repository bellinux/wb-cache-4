56|67|Public
50|$|The {{assessment}} of English language in primary school {{is divided into}} 3 sub-papers each {{with a focus on}} a specific skill of reading and writing, listening and speaking respectively. The {{assessment of}} English language in secondary school is divided into 4 sub-papers each focus on a specific skill of reading, writing, listening and speaking respectively. Candidates from every school will be chosen randomly to attend the <b>speaking</b> <b>assessment.</b>|$|E
30|$|The <b>speaking</b> <b>assessment</b> in LPATE was revised in June 2010.|$|E
30|$|The {{present study}} {{examined}} the validity of a university-based <b>speaking</b> <b>assessment</b> (Test of Oral Proficiency in English, TOPE for short) in mainland China. The <b>speaking</b> <b>assessment</b> {{was developed to meet}} the standards (Standard for Oral Proficiency in English, SOPE for short) set for teaching and learning of the oral English by the university.|$|E
40|$|This {{research}} {{is based on}} the analysis of English <b>speaking</b> skills <b>Assessment</b> strategies applied by the English teacher in the classroom to students of ninth grade of secondary education at Alfonso Cortés School. The research objectives are: To analyze the kind of English <b>speaking</b> skills <b>assessment</b> applied by the teacher in the classroom; to identify the English <b>speaking</b> skills <b>assessment</b> strategies applied by the teacher in the classroom; to evaluate the English students´reaction to the <b>speaking</b> skills <b>assessment</b> strategies applied by the teacher in the classroom and to explain the consequences about the lack of English <b>speaking</b> skills <b>assessment</b> strategies applied by the teacher in the classroom. The population for this research was English teacher and the students to ninth grade students of secondary education, at Alfonso Cortés School, Managua, from the 20 students 45...|$|R
30|$|Given {{that there}} is an {{increase}} in the use of computers in <b>speaking</b> <b>assessments,</b> it is necessary to better understand how the mode of computer delivery affects <b>speaking</b> <b>assessments.</b> Therefore, in the present study, the psychometric qualities of computer-delivered and face-to-face monologic tasks were examined; specifically, test scores and the underlying factor structures of these tasks were compared. Indeed, professional testing standards (AERA et al. 1999) have underscored that score equivalence is significant and should be established prior to the interpretation of computerized test scores. Furthermore, exploring how the underlying factor structures vary between the two modes will provide valuable insight into the interpretation of test scores on computer-delivered speaking tests.|$|R
40|$|This {{study was}} aimed {{to find out}} the {{practice}} of psychomotor domain in <b>speaking</b> performance <b>assessment</b> of English teaching at SMPN 24 Surabaya. Moreover, this study was also aimed {{to find out the}} follow-up result of psychomotor domain in <b>speaking</b> performance <b>assessment</b> of English teaching in that school which had been done by teacher. In this research, the researcher used descriptive qualitative as the research method. The researcher collected the data by using field note, classroom observation checklist, interview guideline, psychomotor domain in <b>speaking</b> performance <b>assessment</b> rubric, and document as instrument. The subject of this research was one of the English teacher at SMPN 24 Surabaya. In this research, the researcher started to observe the English teacher in 8 F class, who was practicing psychomotor domain in <b>speaking</b> performance <b>assessment</b> at that time. Firstly, the researcher collected the teacher’s lesson plan. Secondly, an interview with the English teacher was done to support the data of classroom observation and document. The data showed that English teacher had already practiced psychomotor domain in <b>speaking</b> performance <b>assessment.</b> However, the English teacher just practiced four of five aspect of psychomotor domain. The data also showed that English teacher practiced remediation and enrichment activity as the follow-up result of psychomotor domain in <b>speaking</b> performance <b>assessment.</b> Nevertheless, the researcher found that English teacher used remediation more dominantly than enrichment as the follow-up result. Based on this result, the researcher concluded that the English teacher did not totally assess all aspect of psychomotor domain in <b>speaking</b> performance <b>assessment.</b> The use of follow-up of result focused only on remediation activity. Therefore, the assessment of psychomotor domain in speaking performance needs improvement to gain its purpose...|$|R
40|$|This study {{sought to}} further {{understanding}} of EFL teachers’ knowledge of <b>speaking</b> <b>assessment</b> and how their knowledge informs their <b>speaking</b> <b>assessment</b> practices in classrooms. Based on a socio-cultural perspective, the present study aimed at investigating EFL novice teachers’ current knowledge and practice of <b>speaking</b> <b>assessment</b> in a Libyan secondary school context. The study {{is based on the}} interpretive paradigm and adopted social constructionism as a philosophical stance. Quantitative and qualitative methods of data collection were employed in two sequential phases. The findings of this study indicated that EFL novice teachers’ current knowledge of <b>speaking</b> <b>assessment</b> is complex and that was reflected in the different ways these teachers interpreted the concept of <b>speaking</b> <b>assessment</b> and in the ways they expressed their beliefs and values regarding how <b>speaking</b> <b>assessment</b> needs to be or is implemented in the classroom settings. Three main issues regarding teachers’ knowledge of <b>speaking</b> <b>assessment</b> emerged from the data. The first issue is that these teachers, although showing some variability in their knowledge and practice, seem to base their assessment practice on a view of spoken language being more about linguistic content than communicative effect. Secondly, that they mostly seem to afford more importance to summative assessment than to formative assessment. Thirdly, they have a view of assessment that focuses on the content to be assessed rather than on the process of assessment. The findings also indicated that while teachers refer to contextual factors that influence how they implement their knowledge into practice, their understanding of the notion of assessment seems to have more influence on their implementation of <b>speaking</b> <b>assessment</b> as process than that of the context. That is, their current understandings of the notion of spoken language seem to contribute to their current practice of <b>speaking</b> <b>assessment.</b> Also of significance are the participants’ views of the role of context. The results showed that although contexts are similar in some aspects, especially those related to institutional factors, teachers’ views show the uniqueness of the context, especially {{in the light of the}} unanticipated social, political and institutional changes. The implications of this study suggest that these EFL novice teachers’ current knowledge of language and of assessment goes beyond factual knowledge to their perceptions of language and their understandings of the purpose of assessment. They also suggest that context plays a role on their current knowledge and practice of <b>speaking</b> <b>assessment.</b> Thus, this study provides further understanding that what these teachers know and how they use their knowledge in practice arises from a complex interweaving of context and individual understandings. Libyan Higher Ministry of Educatio...|$|E
40|$|The {{topic of}} this study regards <b>speaking</b> <b>assessment</b> and the {{research}} {{is focused on the}} possibility of obtaining a set of specifications that helps my school 9 ̆ 2 s teachers design valid and reliable <b>speaking</b> <b>assessment</b> tools. I describe my school 9 ̆ 2 s context and analyse the reasons why there is the need to reinforce <b>speaking</b> <b>assessment</b> and, afterwards, I analyse and compare different speaking exams. From this analysis and comparison, a set of specifications is derived and applied in the design of a sample test. The topic of <b>speaking</b> <b>assessment</b> is relevant because it focuses on an area and a skill that have somewhat been neglected in the context of EFL learning, teaching and testing in Portugal. In addition, the introduction of a national school-leaving speaking exam, which has direct impact on the students 9 ̆ 2 final mark, raised awareness for the need for changes in our practice that imply testing speaking and listening thoroughly...|$|E
40|$|Recently {{there have}} been debates on {{assessing}} students’ performances on speaking since the cultural and subjective issues embedded in bringing awareness on how teachers construct their <b>speaking</b> <b>assessment.</b> The main focus {{of this paper is}} a way to design assessment for speaking suitable for the Indonesian context at a university level. This paper stresses the criteria of effective assessment proposed by Brown and Abeywicrama which consists of a specific criterion, an appropriate task, a maximum output and practical and a reliable scoring procedure. It is recommended that teachers develop their <b>speaking</b> <b>assessment</b> which is appropriate and contextual. </p...|$|E
30|$|In {{this article}} I propose a {{distinction}} between a ‘weak’ and ‘strong’ version of LSP performance assessment based on the empirical data collected over {{the last ten years}} of developing and embedding the Business Performance Language Assessment Scales (BUPLAS) into Asian call centres. A distinction that has been unclear to date in this area of research is whether a workplace LSP <b>spoken</b> <b>assessment</b> for work (meaning those assessment tasks that gauge employment entry levels as ‘predictive’ of work success) is the same as a workplace LSP <b>spoken</b> <b>assessments</b> at work (meaning those authentic assessments that gauge quality levels on-the-job as ‘observed’ success at work). For the Asian call centre industry this distinction is important because the purpose of recruitment (exclusion) {{is very different from the}} purpose of quality assurance (appraisal and coaching feedback). I argue that recruitment assessment with its attendant characteristics constitutes a ‘weak’ version of LSP performance assessment whilst its quality assurance counterpart constitutes a ‘strong’ version.|$|R
40|$|AbstractIt {{has been}} found from {{language}} assessment that the results were lower in terms of reliability compared to other aspects. Therefore, {{the development of a}} reliable model on English <b>speaking</b> skill <b>assessment</b> is very important and it will hopefully bring about English language teaching improvement. The objectives of this research were to study the steps and the components of a portfolio on English <b>speaking</b> skill <b>assessment</b> as well as to develop the English <b>speaking</b> skill <b>assessment</b> criteria for grade 6 students. The research methods used include a review of documents and interviews of nine experts on English language teaching and language assessment. The data was analysed using content analysis method. The results found that the component of the portfolio on English <b>speaking</b> skill <b>assessment</b> for grade 6 students comprises three parts: 1) Introduction 2) Contents and 3) Assessment criteria. There are 7 steps in using a portfolio in assessment: 1) planning 2) preparation for students 3) evidence collecting 4) progress monitoring 5) improvement of performance 6) reflection and 7) displaying the works. The tasks involved in English <b>speaking</b> skill <b>assessment</b> include interviewing, oral presentation, storytelling, making picture description. Analytic rating scale was applied as scoring criteria on vocabulary, syntax, cohesion, pronunciation ideational function and fluency...|$|R
40|$|This work {{introduces}} {{new methods}} for de-tecting non-scorable tests, i. e., tests that cannot be accurately scored automatically, in educational applications of <b>spoken</b> lan-guage proficiency <b>assessment.</b> Those in-clude cases of unreliable {{automatic speech recognition}} (ASR), often because of noisy, off-topic, foreign or unintelligible speech. We examine features that estimate signal-derived syllable information and compare it with ASR results in order to detect responses with problematic recognition. Further, we explore the usefulness of lan-guage model based features, both for lan-guage models that are highly constrained to the spoken task, and for task inde-pendent phoneme language models. We validate our methods on a challenging dataset of young English language learn-ers (ELLs) interacting with an automatic <b>spoken</b> <b>assessment</b> system. Our proposed methods achieve comparable performance compared to existing non-scorable detec-tion approaches, and lead to a 21 % rela-tive performance increase when combined with existing approaches. ...|$|R
30|$|Merits and {{limitations}} of using the CEFR for developing the <b>speaking</b> <b>assessment</b> (i.e., applicability of the CEFR for TOPE) were discussed. The study was concluded with its limitations and suggestions for future study.|$|E
40|$|As {{with any}} other area of {{language}} assessment, the fundamental issues {{to be considered in}} a <b>speaking</b> <b>assessment</b> are: (a) whether or not the test is used as intended, and (b) what its consequences may be (Bachman & Purpura, in press). To ensure that the uses and consequences of a speaking test are fair, the operational definition of speaking ability in the testing context should be examined, since the definition of speaking ability varies with respect to the targeted use and the decisions made. One way to elicit the construct of speaking ability for a certain context is through a scoring rubric which informs test users what a test aims to measure (Luoma, 2004). However, a scoring rubric can affect the <b>speaking</b> <b>assessment,</b> as there may be an interaction effect between the rating criteria and examinees ’ performance (Luoma, 2004; McNamara, 1996). Different interpretations of the construct may cause biased effects on test-takers ’ performance, leading to unfairness in scoring and test use. Thus, careful examination of how rating scales interact with speaking performance needs to be considered to determine the fairness of the <b>speaking</b> <b>assessment.</b> The first issue in examining rating scales is whether the scores given based on the rating scale truly reflect the quality of the test-taker’s speaking performance. Douglas (1994...|$|E
30|$|Learner {{corpus data}} {{can play a}} key role in <b>speaking</b> <b>assessment,</b> by {{providing}} rich data for level calibration and ironing out inconsistencies between raters. Yan (2014) also comments that rater alignment is particularly difficult at lower score levels and recommends training that focuses on rater disagreement.|$|E
40|$|Discussions of {{interaction}} in second language performance assessment {{have generally been}} loosely psychological in orientation, forming part of attempts to model the nature of communicative ability within the individual But in investigating the validity of performance assessments involving interactions between individuals (for example between candidate and interlocutor m <b>speaking</b> <b>assessments),</b> the intrinsically social nature of performance needs to be recognized What would be the consequences for language testing research if it adopted a social perspective on the nature {{of interaction}}? The paper explores the necessity for such a reonentation, and suggests areas that would feature in a consequent research agenda...|$|R
40|$|This chapter {{reports the}} {{research}} {{findings of a}} two-year study which investigated {{the effectiveness of a}} school-based Chinese character and reading curriculum designed for non-Chinese <b>speaking</b> preschoolers. <b>Assessment</b> scores, teachers and parents' feedbacks are reported, students' learning performance is described. discussed...|$|R
40|$|This {{article is}} based on the study {{conducted}} in July and August 2013 to identify the improvement of young learners 2 ̆ 7 speaking skill based on the implementation of Theme-Based Teaching. The study was conducted by using classroom action research design which involved 32 second graders of an Elementary School in Bandung. Furthermore, the data were gathered through <b>speaking</b> <b>assessments</b> and interview. The findings revealed that there was improvement in the students 2 ̆ 7 speaking skill which covered some aspects, including vocabulary, pronunciation and grammar. This article concludes by reviewing the result of the research and some activities {{that can be used in}} improving students 2 ̆ 7 speaking skill...|$|R
40|$|Selviani Vidya Lestari (1100061). An Analysis of Interview as a Technique to Assess the Students’ Speaking Ability. Under the {{supervision}} of Prof. H. Fuad Abdul Hamied, M. A., Ph. D. and Muhammad Handi Gunawan, M. Pd. The research analyses the implementation and implication of interview as a technique to assess the students’ speaking ability. Mix methods research design was used to obtain the data. The data were gathered from 10 eleventh grade students and taken from observation and tests. The tests administered in this research were TOEIC speaking test and interview test. The qualitative data were taken from observation. The observation was conducted during the interview test to see the implementatation of interview. The quantitative data {{were taken from the}} students’ TOEIC speaking test score and interview test score. The <b>speaking</b> <b>assessment</b> criteria used to score the students’ speaking ability through interview was the combination of IELTS and Adam and Frith <b>speaking</b> <b>assessment</b> criteria. The students’ TOEIC speaking test score and the interview score were computed by using Pearson Product Moment Correlation formula to see the implication of interview. The findings showed that the implementation of interview was appropriate based on some steps by Underhill (1987). Besides, the interview had covered the components defined by Harris (1969) {{in the form of the}} IELTS-Adam and Frith <b>speaking</b> <b>assessment</b> criteria. In addition, the correlation coefficient between the students’ TOEIC speaking test score and interview test score was 0, 7. It indicated thatthe correlation was strong and positive. This study brought to a close that interview is a good and appropriate technique to assess the students’ speaking ability. However, some consideration should be made in the relation of time, setting, and the topic of the interview. Keywords: Interview, <b>Speaking</b> <b>Assessment,</b> TOEIC Speaking Tes...|$|E
40|$|This {{collection}} contains four papers Lhat were presented {{as part of}} a short course delivered at the 1991 annual meeting of the Speech Communication Association. The papers discuss various aspects of the development and use of "The Competent Speaker, " a public <b>speaking</b> <b>assessment</b> instrument designed for the college sophomore level. The papers and their authors are as follows...|$|E
30|$|The {{present study}} {{investigated}} the validity of a university-based <b>speaking</b> <b>assessment</b> in mainland China. The degree of interaction among candidates in the group discussion was analyzed in terms of language functions included in the test syllabus, developed based on the CEFR. Quantitative analysis revealed {{that the majority of}} language functions intended by the test syllabus were observed at a range of levels in candidate performances.|$|E
30|$|The BUPLAS {{assessment}} tool was first developed by Company A, a business English consultancy {{firm based in}} Manila over a decade ago, and has gradually been refined, in close collaboration with the business, to meet different purposes within the call centre workplace (Lockwood 2010). These call centre assessment purposes are divided into two: first BUPLAS for recruitment purposes, and secondly, BUPLAS for quality assurance purposes. As suggested earlier, I will argue that BUPLAS recruitment (known by the industry users as BUPLAS Voice Assessment (VA)) {{is an example of}} a ‘weak’ LSP <b>spoken</b> <b>assessment</b> and BUPLAS quality assurance (known as BUPLAS Call Assessment (CA)) {{is an example of a}} ‘strong’ version. I will deconstruct the characteristics of each later in this article in support of these proposed definitions.|$|R
40|$|This study {{addresses}} the following question: Are different task characteristics and performance conditions (involving assumed {{different levels of}} cognitive demand) associated with different levels of fluency, complexity, or accuracy in test candidate responses? The materials for the were a series of narrative tasks involving a picture stimulus; the participants were 193 pre-university students taking English courses. We varied the conditions for tasks in each dimension and measured {{the impact of these}} factors on task performance with both familiar detailed discourse measures and specially constructed rating scales, analyzed using Rasch methods. We found that task performance conditions in each dimension failed to influence task difficulty and task performance as expected. We discuss implications for the design of <b>speaking</b> <b>assessments</b> and broader research...|$|R
30|$|The use of {{computers}} in <b>speaking</b> <b>assessments</b> is appealing; however, {{the absence of an}} interlocutor has resulted in concerns about the validity of using them as a replacement for interview tests. In test validation, language testers have long held an interest in specifying and minimizing the factors that confound score interpretation. For instance, mode effects, a facet of task conditions, have been discussed as a potential source of construct-irrelevant variance in computer-based tests. According to Chapelle and Douglas (2006), the most ubiquitous concern raised about assessing language via technology is that test takers’ performance on a computer test may not reflect their ability measured by other forms of assessments. Therefore, there has been a call for more research on the effects of computer-based testing (Alderson 2004); specifically, it appears that examinations comparing computer-based tests with conventional tests are needed (Chapelle 2003).|$|R
30|$|The Common European Framework of Reference for Languages (CEFR) (Council of Europe 2001, 2018) has {{commonly}} {{been adopted}} in language learning, teaching, and assessment throughout Europe and beyond. It has established common standards for formulating {{the objectives of}} language learning curricula and materials, as well as certifying learners’ proficiency in language skills. However, research on the application of descriptors in CEFR in English <b>speaking</b> <b>assessment</b> has been lacking.|$|E
40|$|Public {{speaking}} and oral assessments {{are common in}} higher education, {{and they can be}} a major cause of anxiety and stress for students. This study was designed to measure the student experience of public <b>speaking</b> <b>assessment</b> tasks in a mandatory first-year course at a regional Australian university. The research conducted was an instrumental case study, with a student-centred focus. Surveys were designed to elicit student perceptions of their emotions and experience before and after engaging in public speaking skill development exercises and a public assessment task. After undertaking public speaking desensitisation and assessment, students experienced increased satisfaction and decreased fear, indecision and confusion. However, students’ perceptions of their confidence to control nerves, maintain eye contact, use gestures and comfortably speak in front of 25 people reduced – an unexpected outcome of the research. The reasons for this remain unclear, which provides a window for further research. Public <b>speaking</b> <b>assessment</b> tasks should be aligned with learning activities, and opportunities to minimise the impact of barriers to students engaging in the learning activities or tasks should be incorporated into curriculum...|$|E
30|$|In {{tertiary}} education in Taiwan, rating training for language teachers is not commonly provided, {{although they are}} most often the primary raters. This study aims to determine the validity and reliability {{of the use of}} CEFR in <b>speaking</b> <b>assessment</b> in the higher education context. It compares the rating performance of two groups of teachers: one of which attended a rater standardization session beforehand, whereas the other without training rated according to the guidelines provided.|$|E
40|$|The {{first year}} {{research}} has aim to: (1) describe the learning assessment {{process of the}} speaking theory subject and (2) design the theorethic model of authentic <b>assessment</b> in <b>speaking</b> competence. The method used in the research is Research and Development method, i. e. the research method is used to produce certain product, and to test {{the effectiveness of the}} product. The results of the research are: (1) the assessment activity of the students’ speaking competence who takes the speaking theory subject have the characteristics of authentic assessment. It is based on the result of the curriculum review which has impact to the syllabus adjustment in directing to the teachers’ need at school. Therefore, the <b>speaking</b> competence <b>assessment</b> which focuses on the speaking ability, nowadays its academic competence focus is broading. In addition, the audiences’ affective respon is used as the assessment consideration; (2) the theoretic model of <b>speaking</b> competence <b>assessment</b> can be described that the <b>speaking</b> competence <b>assessment</b> is divided as two step categories, they are the process and the result assessment. The portofolio assessment is the most theorithic assessment tecnique which considered as it appropriate to gain the students’ speaking competence development. The result of the assessment is divided into two tests, i. e the middle semester test and the final semester test. The middle semester test is used to know the student’s competence which has closed relation to the speaking theories because the students have to mastery the speaking competence in order to practice the speaking competence, the students’ must also have to teach it. The final semester test is conducted to know the students’ competence in the speaking practice which can be conducted by the three techniques, they are group, peer, and individual technique. The tehcniques can be applied based on the needs and the characteristic of the tested competence...|$|R
40|$|Millions of {{customer}} services representatives are assessed {{each year by}} subject matter experts (e. g., recruiters, team leaders) in Asian contact centres to ensure good spoken communication skills when serving customers on the phones. In other workplace contexts, language experts are employed to do this work but in Asian contact centres, a successful transfer of language expert assessment knowledge and skills to subject matter experts is preferred for practical and cost reasons. To date however, no studies {{have been carried out}} to demonstrate that subject matter experts can do this language assessment in a reliable way. This study explores how a linguistically-informed <b>spoken</b> <b>assessment</b> rubric, called the Business Performance Assessment Scale (BUPLAS), is used by subject matter experts to assess the English communication skills {{of customer}} service representatives employed in an India-based contact centre. After a week-long training programme on how to use BUPLAS, the results revealed that the subject matter experts had high intra-rater and inter-rater reliability when they scored real calls. They also understood, interpreted, and ‘indigenously scored’ the linguistically-informed criteria. The implications of these results in deferring to subject matter experts to conduct language assessments in this workplace are discussed...|$|R
40|$|Despite {{the growing}} {{popularity}} of paired format <b>speaking</b> <b>assessments,</b> the effects of pre-task planning time on performance in these formats are not yet well understood. For example, some studies have revealed the benefits of planning but others have not. Using a multifaceted approach including analysis of the process of speaking performance, the aim {{of this paper is to}} investigate the effect of pre-task planning in a paired format. Data were collected from 32 students who carried out two decision-making tasks in pairs, under planned and unplanned conditions. The study used analyses of rating scores, discourse analytic measures, and conversation analysis (CA) of test-taker discourse to gain insight into co-constructing processes. A post-test questionnaire was also administered to understand the participants’ perceptions toward planned and unplanned interactions. The results from rating scores and discourse analytic measures revealed that planning had limited effect on performance, and analysis of the questionnaires did not indicate clear differences between the two conditions. CA, however, identified the possibility of a contrastive mode of discourse under the two planning conditions, raising concerns that planning might actually deprive test-takers of the chance to demonstrate their abilities to interact collaboratively...|$|R
40|$|The {{present study}} investigates the current {{practice}} in classroom <b>speaking</b> <b>assessment</b> {{in secondary schools}} in South Korea. Teacher-based speaking assessments conducted in the classroom are not only strongly recommended in Korean educational policies but are also the only tool used to evaluate students’ oral skills in the formal schooling system. However, {{there has been little}} systematic research investigating how teachers actually assess students’ oral skills in the classroom. Therefore, this study aims to investigate the current status of classroom <b>speaking</b> <b>assessment</b> in Korean middle schools and its effectiveness in light of an alternative assessment tool and pedagogical values. The data was collected from questionnaires and interviews where teachers were the only targeted respondents. 51 Korean English teachers recently working in middle schools participated in the questionnaire and six of them were interviewed. The results have revealed that classroom <b>speaking</b> <b>assessment</b> currently conducted in Korean middle schools has broadly employed performance-based tasks and that somewhat informative feedback has been offered to students in the form of criterion descriptions plus marking scores. However there was still a strong tendency here towards traditional formal testing to measure and report learning outcomes, one which resulted in teachers having an overall pessimistic attitude towards the positive effects of such testing on teaching and learning. It is evident from this study that there is need for improvements in order to facilitate better learning outcomes in the classroom. The study provides a range of suggestions for an improvement of current practices, starting with a process to change the perceptions of teachers, students, parents and policy makers towards classroom assessment followed by practical actions such as teacher training, cooperation with an English native teacher, and downsizing the number of students per class...|$|E
40|$|Abstract: This article {{describes}} the advantages of using analytic proceÂ­dure in <b>speaking</b> <b>assessment.</b> An analytic scoring guide, {{as compared to the}} impressionistic one, has a double function: as an instrument to meaÂ­sure the learner's speaking proficiency and as a diagnostic procedure for remedial teaching. Thus, it provides reliable sources of information in the form of scores of the speaking components and can be used as feed-back for the teacher and learner to identify which component needs imÂ­provement...|$|E
40|$|This paper {{presents}} a test development project for classroom <b>speaking</b> <b>assessment.</b> With {{the aim of}} enhancing and specifically easing the process of test preparation and administration and generating positive washback effects on learning, we developed a semi-direct speaking test called the Story Retelling Speaking Test (SRST). Although a story retelling technique has already been widely recognized as a teaching activity, its use for <b>speaking</b> <b>assessment</b> has not been fully studied. Thus, the paper discusses the potentiality of using this technique for the SRST and reports its pilot administration to 43 examinees. As a result, the high practicality of the test was confirmed at the test construction and implementation stages. In addition, the questionnaire distributed to the examinees yielded generally positive results regarding their perception toward the test usefulness and {{the appropriateness of the}} test procedures and task difficulty. With regard to the appropriateness of the texts, the examinees perceived that the retelling of stories was influenced most by text content and then by text length; however, these two factors appear to be interrelated. On the basis of these responses, we have suggested some revisions of the SRST and future validation and reliability studies...|$|E
40|$|Assessment {{is needed}} in the {{activity}} deals with teaching learning process {{as well as the}} learning of speaking ability in higher education. Basically, assessment for lecturers is not a novelty. Assessment is important since it is done in every learning process. It is necessary to be done because the result of assessment can be used as one of the standards of learners’ competency in understanding the material. Number and value can be used as the standard of learning success since they are the characteristics of the assessment. In doing the assessment of learning outcomes, the lecturers should notice the assessment principles as the qualified assessments guidelines. The assessment principles which are needed to be actualized in the assessment of learning outcomes in speaking ability are validation principle, competence education, objective, fair, open, continuity, comprehensive and economical. Based on the reality, some <b>speaking</b> ability’s <b>assessment</b> principles in higher education have not fully actualized yet.   The effort to realize the <b>speaking</b> ability’s <b>assessment</b> principle in higher education is by doing assessment based on information technology. The use of information technology in assessing speaking ability can actualize the assessment principle. The actualized principles are economical, open, accountable, continuity, and integrated principle. Keywords: <b>assessment</b> principle, <b>speaking</b> ability, higher education...|$|R
30|$|Generally <b>speaking,</b> {{the three-year}} <b>assessment</b> from 2014 to 2016, as {{illustrated}} in Fig. 9, indicates that most Iranian provinces were located in the 3 th and 4 th quadrants meaning {{that they are in}} general unsuccessful in road safety performance in terms of technology advancement; meanwhile, eleven provinces progressed in road safety performance in terms of efficiency improvement.|$|R
30|$|Typically in this industry, <b>spoken</b> <b>assessment</b> {{tasks and}} rating scales have been {{developed}} internally by the business and/or have been sourced off the internet; but without a good applied linguistic understanding about what they should contain and how they should be administered, they have, in general, proven to be unreliable and invalid (Lockwood 2010, Lockwood 2012). For example, many CSR applicants are often rejected at recruitment because the business managers worry about the narrow concerns of grammatical mistakes and accents, citing that they ‘sound too Filipino’, ‘they don’t have the right accent’ or that ‘they make too many grammar mistakes’. This limited view of language has had a negative washback into internal assessment tools developed for this industry. ‘Accent’ and ‘grammar accuracy’ unfortunately persist as the key indicators of functional language ability {{in the minds of}} many call centre managers today. Quality assurance appraisal on-the -floor is measured against a tool called a ‘scorecard’ that similarly privileges accent and grammar accuracy (see Lockwood et al. 2009 for a full discussion about scorecard use in call centres). These quality scorecards ideally aim to assess business product and process knowledge and communication skills during live or recorded authentic calls for judgmental (appraisal) and developmental (coaching) purposes. Developing a ‘strong’ LSP performance measure, as informed by applied linguistic theory and practice, and involving ethnographic studies, analyses of the authentic exchanges and SME input into a revised scorecard, is argued to add validity to this tool.|$|R
