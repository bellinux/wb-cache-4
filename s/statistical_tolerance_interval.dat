5|396|Public
40|$|<b>Statistical</b> <b>tolerance</b> <b>{{interval}}</b> {{is another}} type of interval estimator used for making statistical inference on an unknown population. Simply stated, it is an interval estimator, based on a sample from preliminary experiment, which can be asserted with confidence level 1 −α (for example 0. 95), to contain at least a speci- fied proportion, say 1 −γ (for example 0. 99), {{of the items in}} the population under consideration. The limits of a <b>statistical</b> <b>tolerance</b> <b>interval</b> are called statistical tolerance limits. The confidence level 1 − α is the probability that a <b>statistical</b> <b>tolerance</b> <b>interval</b> constructed in the prescribed manner (i. e. based on result of an experiment conducted under identical conditions) will contain at least a pro- portion 1 − γ of possibly infinite sequence of items coming from the considered (unknown) population (i. e. realizations of independent random variables from the given distribution). In contrast with other statistical intervals commonly used for statistical inference, like e. g. the confidence intervals for the parameters and/or the prediction intervals for future observation(s), the tolerance intervals are used relatively rarely. One reason is that the theoretical concept and compu- tational complexity is significantly more difficult, if compared with the commonly used confidence and prediction intervals. In this paper we briefly describe the theoretical background and computational approaches for computing the toler- ance factors and limits for statistical tolerance intervals based on samples from univariate normal (Gaussian) populations...|$|E
40|$|One of {{the first}} {{questions}} {{that has to be}} answered in the survey design process is "How many subjects should be interviewed?" The answer can have significant implications for the cost of project preparation, since in Latin America and the Caribbean costs per interview can range from US$ 20 to US$ 100. Traditionally, the sample size question has been answered in an unsatisfactory way by either dividing an exogenously fixed survey budget by the cost per interview or by employing some variant of a standard <b>statistical</b> <b>tolerance</b> <b>interval</b> formula. The answer is not {{to be found in the}} environmental economics literature. But, it can be developed by adapting a Bayesian decision analysis approach from business statistics. The paper explains and illustrates, with a worked example, the rationale for and mechanics of a sequential Bayesian optimization technique, which is only applicable when there is some monetary payoff to alternative courses of action that can be linked to the sample data. Economics, Environmental Policy, The Optimal Sample Size for Contingent Valuation Surveys: Applications to Project Analysis...|$|E
40|$|Saffaj et al., {{recently}} {{proposed an}} uncertainty profile {{for evaluating the}} validity of analytical methods using the statistical methodology of γ-confidence β-content tolerance intervals. This profile assesses {{the validity of the}} method by comparing the method measurement uncertainty to a pre defined acceptance limit stating the maximum uncertainty suitable for the method under study. Several years earlier as stated by these authors a SFSTP (Société Française des Sciences et Techniques Pharmaceutique) commission has developed a similar profile called accuracy profile used to assess the validity of analytical methods. This accuracy profile also uses the methodology of statistical tolerance intervals, but β-expectation tolerance intervals. The uncertainty profile of Saffaj et al. and the accuracy profile of the SFSTP commission are both fulfilling the same final purpose. The core question is finally what <b>statistical</b> <b>tolerance</b> <b>interval</b> to use ? The aim of this letter to the editor is to discuss this question and provide arguments that β-expectation tolerance intervals should be prefered to assess the validity of the method as this type of interval give the guarantee that each future results has high probability to fall within pre-specified acceptance limits. Peer reviewe...|$|E
40|$|The role of <b>statistical</b> <b>tolerance</b> <b>intervals</b> for {{developing}} ratio edit tolerances in a parametric setup is investigated. The {{performance of the}} methodology is assessed for the normal and Weibull distributions. The numerical results show {{that in terms of}} Type I and Type II errors, <b>statistical</b> <b>tolerance</b> <b>intervals</b> exhibit better performance compared to other ratio edit procedures available in the literature. The methodology is illustrated using 2010 and 2011 data from the Annual Survey of Manufacturers...|$|R
40|$|<b>Statistical</b> <b>tolerance</b> <b>intervals</b> {{are another}} tool for making {{statistical}} inference on an unknown population. The <b>tolerance</b> <b>interval</b> is an interval estimator {{based on the}} results of a calibration experiment, which can be asserted with stated confidence level 1 −, for example 0. 95, to contain at least a specified proportion 1 −, for example 0. 99, of the items in the population under consideration. Typically, the limits of the <b>tolerance</b> <b>intervals</b> functionally depend on the tolerance factors. In contrast to other statistical intervals commonly used for <b>statistical</b> inference, the <b>tolerance</b> <b>intervals</b> are used relatively rarely. One reason is that the theoretical concept and computational complexity of the <b>tolerance</b> <b>intervals</b> is significantly more difficult than that of the standard confidence and prediction intervals. In this paper we present a brief overview of the theoretical background and approaches for computing the tolerance factors based on samples from one or several univariate normal (Gaussian) populations, as well as the tolerance factors for the non-simultaneous and simultaneous two-sided <b>tolerance</b> <b>intervals</b> for univariate linear regression. Such <b>tolerance</b> <b>intervals</b> are well motivated by their applicability in the multiple-use calibration problem and in construction of the calibration confidence intervals. For illustration, we present examples of computing selected tolerance factors by the implemented algorithm in MATLAB. </p...|$|R
5000|$|His more {{conventional}} work {{led him to}} formulate the <b>statistical</b> idea of <b>tolerance</b> <b>intervals</b> and to propose his data presentation rules, which are listed below: ...|$|R
40|$|Abstract: Industrial {{quality control}} is {{necessary}} {{at least for}} two reasons: • Monitoring product or process quality in order to perform interventions for assuring a desired quality level. • Monitoring product or process quality {{in order to meet}} certain requirements (imposed by law or customers) for documentation. Especially in the second case legal requirements or consumers demand that “professional” methods are used. Therefore, industry relies on the methods offered by “professional ” bodies like the International Organization for Standardization (ISO) or its national counterparts for example the German DIN Deutsches Institut für Normung, because they are widely acknowl-edged as professional. One of the standards which are in use in German industries is DIN 55303 (Teil 5) : “Statistische Auswertung von Daten- Bestimmung eines statistischen Anteilsbereichs ” of February 1987 which corresponds to ISO 3207 - 1975 “Statistical interpretation of data: determination of a <b>statistical</b> <b>tolerance</b> <b>interval.</b> ” This paper introduces in detail statistical tolerance intervals and subsequently examines the standard critical. This first part is devoted to the case that the normal approximation is used and results in the recommendation not to use the methods offered in the standard. A second part will investigate the case that the normal approximation is not made. ...|$|E
40|$|Fundamentally, {{this paper}} is {{about the value of}} information. Whenever a {{cost-benefit}} analysis has to be undertaken using benefits that are estimated from household survey data the size of the survey sample must be specified. The most obvious case in the valuation of environmental amenity improvements through contingent valuation (CV) surveys of willingness to pay. One of the first questions that has to be answered in the survey design process is "How many subjects should be interviewed?" The answer can have significant implications for the cost of project preparation. Traditionally, the sample size question has been answered in an ad hoc way either be dividing an exogenously fixed survey budget by the cost per interview or employing some variant of a standard <b>statistical</b> <b>tolerance</b> <b>interval</b> formula. Neither of these approaches can balance the gains to additional sampling effort against the extra interviewing costs. A better answer if not {{to be found in the}} environmental economics literature, though it can be developed by adapting a Bayesian decision analysis approach from business statistics. The paper explains and illustrates, with a worked example, the rationale for and mechanics of a sequential Bayesian optimization technique, which is applicable when there is some monetary payoff to alternative courses of action that can be linked to the sample data. In this sense, unlike pure valuation studies that are unconnected to a policy decision, investigators who use contingent valuation results directly in cost-benefit analysis have a hidden advantage that can be exploited to optimize the sample size. The advantage lies in the link between willingness to pay and the decision variable, the net present value of the prospective investment. The core objective of the paper is practical. Readers without a statistical background can easily implement the method. An Appendix shows how, with just six key pieces of information, anyone can solve the optimal sample size problem in a spreadsheet. An automated spreadsheet algorithm is available from the authors on request. To run the program all the users has to do is enter the key data and then activate a macro that automatically computes the optimum number of additional observations needed to augment any initial "small" survey sample. ...|$|E
40|$|A {{review on}} <b>statistical</b> <b>tolerance</b> <b>intervals</b> {{shows that the}} {{derivation}} of two-sided <b>tolerance</b> <b>intervals</b> is far more challenging than that of their one-sided counterparts. Much of the existing construction of two-sided <b>tolerance</b> <b>intervals</b> are through a numerical approach. This study addresses the problems of constructing two-sided <b>tolerance</b> <b>intervals</b> in balanced one-way random effects models and for a general family of distributions. The Bayesian <b>tolerance</b> <b>interval</b> developed by Ong and Mukerjee (2011) using probability matching priors (PMP) is compared via Monte Carlo simulation with the modified large sample (MLS) <b>tolerance</b> <b>interval</b> of Krishnamoorthy and Mathew (2009) for normal and non-normal experimental errors with respect to coverage probabilities and expected widths. Data generated from normal and nonnormal experimental errors were studied to see the effects on the <b>tolerance</b> <b>intervals</b> since real data may not necessarily follow the normal distribution. Results show that the PMP <b>tolerance</b> <b>interval</b> appears to be less conservative for data with moderate and large number of classes while the MLS <b>tolerance</b> <b>interval</b> is preferable for smaller sample sizes. For {{the second part of}} the study, the PMP as well as frequentist two-sided <b>tolerance</b> <b>intervals</b> are constructed for a general family of parametric models. Simulation studies show that the asymptotic results are well-reflected in finite sample sizes. The findings are then applied to real data. The results obtained in this research are a contribution to the area of <b>statistical</b> <b>tolerance</b> regions...|$|R
40|$|Validation of {{analytical}} methods is required {{prior to their}} routine use. In addition, the current implementation of the Quality by Design (QbD) framework in the pharmaceutical industries aims at {{improving the quality of}} the end products starting from its early design stage. However, no regulatory guideline or none of the published methodologies to assess method validation propose decision methodologies that effectively take into account the final purpose of developed {{analytical methods}}. In this work a solution is proposed for the specific case of validating analytical methods involved in the assessment of the Content Uniformity or Uniformity of Dosage Units of a batch of pharmaceutical drug products as proposed in the European or US pharmacopoeias. This methodology uses <b>statistical</b> <b>tolerance</b> <b>intervals</b> as decision tools. Moreover it adequately defines the Analytical Target Profile of analytical methods in order to obtain analytical methods that allow to make correct decisions about Content Uniformity or Uniformity of Dosage Units with high probability. The applicability of the proposed methodology is further illustrated using an HPLC-UV assay as well as a Near Infra-Red Spectrophotometric method. Peer reviewe...|$|R
40|$|This report {{discusses}} {{the treatment of}} uncertainties stemming from relatively few samples of random quantities. The importance of this topic extends beyond experimental data uncertainty to situations involving uncertainty in model calibration, validation, and prediction. With very sparse data samples it is not practical to have a goal of accurately estimating the underlying probability density function (PDF). Rather, a pragmatic goal is that the uncertainty representation should be conservative so as to bound a specified percentile range of the actual PDF, say the range between 0. 025 and. 975 percentiles, with reasonable reliability. A second, opposing objective is that the representation not be overly conservative; that it minimally over-estimate the desired percentile range of the actual PDF. The presence of the two opposing objectives makes the sparse-data uncertainty representation problem interesting and difficult. In this report, five uncertainty representation techniques are characterized for their performance on twenty-one test problems (over thousands of trials for each problem) according to these two opposing objectives and other performance measures. Two of the methods, <b>statistical</b> <b>Tolerance</b> <b>Intervals</b> and a kernel density approach specifically developed for handling sparse data, exhibit significantly better overall performance than the others...|$|R
40|$|The {{understanding}} of the method {{is a major concern}} when developing a stability-indicating method and even more so when dealing with impurity assays from complex matrices. In the presented case study, a Quality-by-Design approach was applied in order to optimize a routinely used method. An analytical issue occurring at the last stage of a long-term stability study involving unexpected impurities perturbing the monitoring of characterized impurities needed to be resolved. A compliant Quality-by-Design (QbD) methodology based on a Design of Experiments (DoE) approach was evaluated within the framework of a Liquid Chromatography (LC) method. This approach allows the investigation of Critical Process Parameters (CPPs), which have an impact on Critical Quality Attributes (CQAs) and, consequently, on LC selectivity. Using polynomial regression response modeling as well as Monte Carlo simulations for error propagation, Design Space (DS) was computed in order to determine robust working conditions for the developed stability-indicating method. This QbD compliant development was conducted in two phases allowing the use of the Design Space knowledge acquired during the first phase to define the experimental domain of the second phase, which constitutes a learning process. The selected working condition was then fully validated using accuracy profiles based on <b>statistical</b> <b>tolerance</b> <b>intervals</b> in order to evaluate the reliability of the results generated by this LC/ESI-MS stability-indicating method. A comparison was made between the traditional Quality-by-Testing (QbT) approach and the QbD strategy, highlighting the benefit of this QbD strategy {{in the case of an}} unexpected impurities issue. On this basis, the advantages of a systematic use of the QbD methodology were discussed. Peer reviewe...|$|R
40|$|Taking {{into account}} its non-invasive, {{non-destructive}} character and fast data acquisition, near infrared spectroscopy {{is more and}} more integrated in production processes to acquire analytical results. Implementation of a NIR quantitative method is performed using an iterative heuristic approach that will ultimately build a model allowing the prediction of the concentration of the analyte of interest. In this context, the aim {{of the present study was}} to develop an innovative approach based on <b>statistical</b> <b>tolerance</b> <b>intervals</b> and the desirability index FMI (Fitting Model Index) to select the most appropriate prediction model from a list of candidate models instead of using conventional criteria such as R², RMSEC, RMSECV and RMSEP [1 - 2] without objective decision rules. This new approach is illustrated on different steps of a real pharmaceutical manufacturing process: water and Active Pharmaceutical Ingredient (API) determinations in pharmaceutical pellets. Variability sources such as production campaigns, batches, days and operators were introduced in the calibration and validation sets. Partial Least Square (PLS) regression on the calibration sets was performed to build prediction models of which the ability to quantify accurately was tested with the validation sets. Regarding the product specifications, the acceptance limits were set at 20 % and 5 %, for the moisture and API determination, respectively. As can be seen from Figure 1 and 2, this innovative approach based on the desirability index FMI of the accuracy profile enabled to build and select the most appropriate prediction model in full accordance with its very final goal, to quantify as accurately as possible the analytes of interest. [1] Hubert Ph. et al., J. Pharm. Biomed. Anal., 36, 2007, 579 - 586. [2] Rozet E. et al., Ana. Chim. Acta, 591, 2007, 239 - 247. Peer reviewe...|$|R
40|$|For a given sample set, {{there are}} already {{different}} methods for building possibility distributions encoding the family of probability distributions that may have generated the sample set. Almost all the existing methods are based on parametric and distribution free confidence bands. In this work, we introduce some new possibility distributions which encode different kinds of uncertainties not treated before. Our possibility distributions encode <b>statistical</b> <b>tolerance</b> and prediction <b>intervals</b> (regions). We also propose a possibility distribution encoding the confidence band of the normal distribution which improves the existing one for all sample sizes. In this work we keep the idea of building possibility distributions based on intervals which are among the smallest intervals for small sample sizes. We also discuss {{the properties of the}} mentioned possibility distributions...|$|R
30|$|The vital {{governing}} factor {{influencing the}} machining excellence is the geometric and dimensional tolerance embedded into the product {{as well as}} into the process. The two main facets of tolerancing include the arithmetic and <b>statistical</b> <b>tolerancing.</b> In arithmetic tolerancing {{it is assumed that}} the detail part dimension can have any value but within the tolerance range; whereas, in the <b>statistical</b> <b>tolerancing</b> scheme, it is assumed that detail part dimensions vary randomly according to a normal distribution, centered at the mid-point of the tolerance range and with its ± 3 σ spread covering the <b>tolerance</b> <b>interval.</b>|$|R
40|$|Abstract: <b>Tolerance</b> <b>intervals</b> {{are widely}} used in {{industrial}} applications. So far attention has been mainly focused {{on the construction of}} <b>tolerance</b> <b>intervals</b> for continuous distributions. In this paper we introduce a unified analytical approach to the construction of <b>tolerance</b> <b>intervals</b> for discrete distributions in exponential families with quadratic variance functions. These <b>tolerance</b> <b>intervals</b> are shown to have desirable probability matching properties and outperform existing <b>tolerance</b> <b>intervals</b> in the literature...|$|R
40|$|ABSTRACT In {{this article}} we provide an {{asymptotic}} upper b-expectation and b-content g-level <b>tolerance</b> <b>intervals</b> for a new family of distributions, namely the Exponentiated Scale family of distributions. Expected coverage of a proposed b-expectation <b>Tolerance</b> <b>Interval</b> is obtained. Bootstrap-based tolerance limits are obtained for data arising from an exponentiated exponential distribution. KEY WORDS: b-expectation <b>tolerance</b> <b>interval,</b> b-content g-level <b>tolerance</b> <b>interval,</b> expected coverage, exponentiated scale family, exponentiated exponential distribution...|$|R
50|$|The <b>tolerance</b> <b>interval</b> {{is related}} to a {{prediction}} interval in that both put bounds on variation in future samples. The prediction interval only bounds a single future sample, however, whereas a <b>tolerance</b> <b>interval</b> bounds the entire population (equivalently, an arbitrary sequence of future samples). In other words, a prediction interval covers a specified proportion of a population on average, whereas a <b>tolerance</b> <b>interval</b> covers it with a certain confidence level, making the <b>tolerance</b> <b>interval</b> more appropriate if a single interval is intended to bound multiple future samples.|$|R
5000|$|One-sided normal <b>tolerance</b> <b>intervals</b> have {{an exact}} {{solution}} {{in terms of}} the sample mean and sample variance based on the noncentral t-distribution. Two-sided normal <b>tolerance</b> <b>intervals</b> can be obtained based on the noncentral chi-squared distribution.|$|R
40|$|The {{construction}} of <b>tolerance</b> <b>intervals</b> (TIs) for discrete variables, such as binomial and Poisson variables, {{has been critical}} in industrial applications in various sectors, including manufacturing and pharmaceuticals. Inaccurate estimation of coverage probabilities leads to improper {{construction of}} <b>tolerance</b> <b>intervals</b> and may lead to serious financial losses for the manufacturers. This article proposes procedures to compute the exact minimum and average coverage probabilities of the <b>tolerance</b> <b>intervals</b> for Poisson and binomial variables. These procedures are illustrated with examples and real data applications. Based on these procedures, improved <b>tolerance</b> <b>intervals</b> are proposed that can ensure that the true minimum or average coverage probabilities {{are very close to}} the nominal levels...|$|R
50|$|The <b>tolerance</b> <b>interval</b> is less {{widely known}} than the {{confidence}} interval and prediction interval, a situation some educators have lamented, {{as it can}} lead to misuse of the other <b>intervals</b> where a <b>tolerance</b> <b>interval</b> is more appropriate.|$|R
40|$|Saffaj et al., {{recently}} {{proposed an}} uncertainty profile {{for evaluating the}} validity of analytical methods using the statistical methodology of γ-confidence β-content <b>tolerance</b> <b>intervals.</b> This profile assesses {{the validity of the}} method by comparing the method measurement uncertainty to a pre defined acceptance limit stating the maximum uncertainty suitable for the method under study. In this letter we comment on the response (T. Saffaj, B. Ihssane, Talanta 94 (2012) 361 - 362) these authors have made to our previous letter (E. Rozet, E. Ziemons, R. D. Marini, B. Boulanger, Ph. Hubert, Talanta 88 (2012) 769 – 771). In particular, we demonstrate that β-expectation <b>tolerance</b> <b>intervals</b> are prediction intervals, we show that β-expectation <b>tolerance</b> <b>intervals</b> are highly usefull for assessing analytical methods validation and for estimating measurement uncertainty and finally we show what are the differences and implications for these two topics (validation and uncertainty) when using either the methodology of β-expectation <b>tolerance</b> <b>intervals</b> or the γ-confidence β-content <b>tolerance</b> <b>tolerance</b> <b>interval</b> one. Peer reviewe...|$|R
40|$|ABSTRACT. Issues {{regarding}} <b>tolerance</b> {{and confidence}} <b>intervals</b> are discussed {{within the context}} of educational measurement and conceptual distinctions are drawn be-tween these two types of intervals. Points are raised about the advantages of <b>tolerance</b> <b>intervals</b> when the focus is on a particular observed score rather than a particular examinee. Because <b>tolerance</b> <b>intervals</b> depend on strong true score models, a practical implication of the study is that true score <b>tolerance</b> <b>intervals</b> are fairly insensitive to differences in assumptions among the five models studied. Perhaps the most important practical use that can be made of the concept of measurement error in educational testing is in discouraging too literal inter-pretations of observed scores. Confidence intervals for true scores are often used for this purpose. In this paper, I analyze an alternative to confidence intervals for encouraging cautious interpretation of observed scores. The al-ternative, referred to here as a <b>tolerance</b> <b>interval,</b> covers a chosen proportion of the conditional distribution of true scores for a given observed score. In general, <b>tolerance</b> <b>intervals</b> are designed to cover a proportion of...|$|R
40|$|Age-matched {{reference}} values are generally presented with 5 th and 95 th percentiles as 'normal' reference range. However, they are mostly determined in relatively small groups, which renders this presentation inaccurate. We determined {{reference values}} for B-lymphocyte subpopulations in healthy {{children with the}} <b>statistical</b> method of <b>tolerance</b> <b>intervals</b> that deals far better with the relatively small numbers tested, and compared these to the cut-off values used in the currently used EUROclass classification for common variable immunodeficiency disorders (CVID) in children. CVID is a heterogeneous group of primary immunodeficiency diseases characterized by low serum immunoglobulin levels and inadequate response to vaccination. Disease-modifying heterozygous amino acid substitutions in TACI are found in around +/- 10 % of CVID patients. Interestingly, we found that age is the primary determinant of TACI-expression on B-lymphocytes, independent of switched memory B-lymphocyte numbers. Immunophenotyping of B-lymphocyte subpopulations is increasingly used to classify patients with CVID into subgroups with different clinical prognosis according to the composition of their B-lymphocyte compartment. These classifications were mainly developed with data obtained in adults. Because of the maturing paediatric immune system, {{they may not be}} equally applicable in children: our and other age-matched reference values show great changes in the composition of the B-lymphocyte compartment during development. Although the greatest changes in B-lymphocyte subpopulations occur below the age of 2 years, when the diagnosis of CVID cannot yet be made, it is likely that a classification developed in adults cannot be used to classify the prognosis of children...|$|R
5000|$|<b>Tolerance</b> <b>interval</b> ("confidence {{intervals}} for the pth quantile") ...|$|R
40|$|In this paper, we {{consider}} the inference for the component and system lifetime distribution of a k-unit parallel system with independent components based on system data. The components are assumed to have identical Weibull distribution. We obtain the maximum likelihood estimates of the unknown parameters based on system data. The Fisher information matrix has been derived. We propose -expectation <b>tolerance</b> <b>interval</b> and -content -level <b>tolerance</b> <b>interval</b> for the life distribution of the system. Performance of the estimators and <b>tolerance</b> <b>intervals</b> is investigated via simulation study. A simulated dataset is analyzed for illustration...|$|R
40|$|The {{problem of}} {{determining}} sample size for two-sided-[beta]-content <b>tolerance</b> <b>intervals</b> which control both tails {{of the normal}} distribution is investigated. The tolerance limits are defined {{to assure that the}} tail proportions do not exceed specified values p 1 and p 2. We determine the minimum sample size so that, at a given confidence level, the tail proportions will not be too small, i. e., so that the <b>tolerance</b> <b>interval</b> will not be overly conservative. Tables of the sample size n and the corresponding factors are provided. [beta]-content <b>tolerance</b> <b>intervals</b> sample size determination...|$|R
40|$|The use of {{regression}} models for prediction purposes {{is one of}} the most common applications of chemometrics. In most cases, only reporting the point predictions is unsatisfactory and intervals are needed to quantify the uncertainty involved. It is common practice to use the 95 % prediction interval for this goal and to interpret such an interval as if 95 % of the future responses will be contained within this interval. However, this is inappropriate, and it can result in a gross underestimation of the real uncertainty of the predicted response, especially in cases where the degrees of freedom is small. To correctly quantify the uncertainty of a prediction, so-called <b>tolerance</b> <b>intervals</b> should be used. Although the theoretical background on <b>tolerance</b> <b>intervals</b> is well documented in the statistical literature, practical guidelines to calculate and use <b>tolerance</b> <b>intervals</b> in real-world applications are lacking. Another less known concept is that of simultaneous versus non-simultaneous intervals. In this tutorial, we explain the origin, interpretation and practical calculation of confidence, prediction, and <b>tolerance</b> <b>intervals,</b> both in their simultaneous and non-simultaneous form. Depending on the case, equations for exact or approximate <b>tolerance</b> <b>intervals</b> are provided. The accuracy of the approximations is discussed. A MATLAB program is available to estimate the exact width of <b>tolerance</b> <b>intervals</b> in all cases involving ordinary least squares regression. (c) 2007 Elsevier B. V. All rights reserved. status: publishe...|$|R
50|$|The <b>tolerance</b> <b>interval</b> {{differs from}} a {{confidence}} interval {{in that the}} confidence interval bounds a single-valued population parameter (the mean or the variance, for example) with some confidence, while the <b>tolerance</b> <b>interval</b> bounds the range of data values that includes a specific proportion of the population. Whereas a confidence interval's size is entirely due to sampling error, and will approach a zero-width interval at the true population parameter as sample size increases, a <b>tolerance</b> <b>interval's</b> size is due partly to sampling error and partly to actual variance in the population, and will approach the population's probability interval as sample size increases.|$|R
5000|$|The <b>statistical</b> {{confidence}} <b>interval</b> or <b>tolerance</b> <b>interval</b> of {{the initial}} measurement.|$|R
40|$|A {{tolerance}} {{region for}} {{a population is}} a region computed using a random sample, so that the region will include a specified proportion {{or more of the}} population, with a given confidence level. The theory of <b>statistical</b> <b>tolerance</b> regions has undergone vigorous development during the last several years. In particular, the computation of satisfactory tolerance regions for multivariate normal populations and multivariate regression models have been investigated, and approximations have been developed for computing the required tolerance factor. The available literature in the case of multivariate normal populations deal with the derivation of ellipsoidal tolerance regions only. The present research is motivated by several observations: for ellipsoidal tolerance regions, the available numerical methods to compute the tolerance factor are unsatisfactory, and the available approximations are not always accurate. Furthermore, the literature does not address the computation of simultaneous <b>tolerance</b> <b>intervals.</b> For computing the tolerance factor required to obtain an ellipsoidal tolerance region, an approximation coupled with Monte Carlo simulation is investigated. The methodology is developed when the unknown population covariance matrix does not have any specific structure, and when it has the intra-class covariance structure. Numerical results show that this approach results in an accurate tolerance factor. The methodology is extended to the case of a multivariate linear regression model for obtaining an ellipsoidal tolerance region under a fixed set of values of the covariates. The second part of the research deals with the derivation of simultaneous <b>tolerance</b> <b>intervals</b> and simultaneous prediction intervals. This is motivated by the fact that an ellipsoidal tolerance region cannot provide information on the distribution of the individual components of the response vector. The case of a general covariance matrix, and that of the intra-class covariance matrix are separately considered, and the derivation of both one-sided and two-sided <b>tolerance</b> <b>intervals</b> are addressed. In the case of the intra-class covariance structure, the derivation of a tolerance rectangle is also investigated. Numerical results are given to assess the accuracy of the proposed solutions, and illustrative examples and applications are provided...|$|R
40|$|Exact two-sided guaranteed-coverage <b>tolerance</b> <b>intervals</b> for the {{exponential}} distribution which satisfy the traditional "equal-tailedness" condition are derived in the failure-censoring case. The available empirical information {{is provided by}} the first r ordered observations in a sample of size n. A Bayesian approach for the construction of equal-tailed <b>tolerance</b> <b>intervals</b> is also proposed. The degree of accuracy of a given <b>tolerance</b> <b>interval</b> is quantified. Moreover, the number of failures needed to achieve the desired accuracy level is predetermined. The Bayesian perspective is shown to be superior to the frequentist viewpoint in terms of accuracy. Extensions to other statistical models are presented, including the Weibull distribution with unknown scale parameter. An alternative <b>tolerance</b> <b>interval</b> which coincides with an outer confidence interval for an equal-tailed quantile interval is also examined. Several important computational issues are discussed. Three censored data sets are considered to illustrate the results developed. ...|$|R
40|$|In many {{research}} areas (such {{as public}} health, environmental contamination, and others) one {{deals with the}} necessity of using data to infer whether some proportion (%) of a population of interest is (or one wants it to be) below and/or over some threshold, through the computation of <b>tolerance</b> <b>interval.</b> The idea is, once a threshold is given, one computes the <b>tolerance</b> <b>interval</b> or limit (which might be one or two - sided bounded) and then to check if it satisﬁes the given threshold. Since in this work {{we deal with the}} computation of one - sided <b>tolerance</b> <b>interval,</b> for the two-sided case we recomend, for instance, Krishnamoorthy and Mathew [5]. Krishnamoorthy and Mathew [4] performed the computation of upper tolerance limit in balanced and unbalanced one-way random eﬀects models, whereas Fonseca et al [3] performed it based in a similar ideas but in a tow-way nested mixed or random eﬀects model. In case of random eﬀects model, Fonseca et al [3] performed the computation of such interval only for the balanced data, whereas in the mixed eﬀects case they dit it only for the unbalanced data. For the computation of twosided <b>tolerance</b> <b>interval</b> in models with mixed and/or random eﬀects we recomend, for instance, Sharma and Mathew [7]. The {{purpose of this paper is}} the computation of upper and lower <b>tolerance</b> <b>interval</b> in a two-way nested mixed eﬀects models in balanced data. For the case of unbalanced data, as mentioned above, Fonseca et al [3] have already computed upper <b>tolerance</b> <b>interval.</b> Hence, using the notions persented in Fonseca et al [3] and Krishnamoorthy and Mathew [4], we present some results on the construction of one-sided <b>tolerance</b> <b>interval</b> for the balanced case. Thus, in order to do so at ﬁrst instance we perform the construction for the upper case, and then the construction for the lower case...|$|R
5000|$|... #Caption: <b>Statistical</b> <b>tolerance</b> versus {{confidence}} {{rating for}} water-vapour isoline retrieval.|$|R
30|$|Armillotta (2015) {{proposes a}} method for {{tolerance}} analysis on planar structures and mechanisms. The fundamental deficit of the actual tolerancing and specification systems is illustrated by Weckenmann and Hartmann (2015). For tolerancing of a component, Yu and Lub (2016) use quality-oriented <b>statistical</b> <b>tolerancing</b> (ST) technique, which helped to overcome the challenges of modern manufacturing. Jean-Marc (2016) show that <b>statistical</b> <b>tolerancing</b> becomes risky when idealized centering assumptions are not perfectly achieved. Alain Van Hoecke (2016) introduced risk tool in <b>statistical</b> <b>tolerancing</b> and its verification management to optimize customers’ and suppliers’ risks. He shows that tolerancing–verification coupling increases benefits by enlarging tolerances through risk control.|$|R
5000|$|A <b>tolerance</b> <b>{{interval}}</b> is a <b>statistical</b> interval within which, {{with some}} confidence level, a specified proportion of a sampled population falls. [...] "More speciﬁcally, a 100×p%/100×(1−α) <b>tolerance</b> <b>interval</b> provides limits within {{which at least}} a certain proportion (p) of the population falls with a given level of conﬁdence (1−α)." [...] "A (p, 1−α) <b>tolerance</b> <b>interval</b> (TI) based on a sample is constructed {{so that it would}} include at least a proportion p of the sampled population with conﬁdence 1−α; such a TI is usually referred to as p-content − (1−α) coverage TI." [...] "A (p, 1−α) upper tolerance limit (TL) is simply an 1−α upper confidence limit for the 100 p percentile of the population." ...|$|R
