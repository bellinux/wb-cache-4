1248|895|Public
5|$|There are {{a number}} of basic {{operations}} that can be applied to modify matrices, called matrix addition, scalar multiplication, transposition, matrix multiplication, row operations, and <b>submatrix.</b>|$|E
5|$|Although {{there exist}} {{polynomial}} time algorithms {{to find a}} matrix having given row and column sums, the solution may be far from unique: any <b>submatrix</b> {{in the form of}} a 22 identity matrix can be complemented without affecting the correctness of the solution. Therefore, researchers have searched for constraints on the shape to be reconstructed that can be used to restrict the space of solutions. For instance, one might assume that the shape is connected; however, testing whether there exists a connected solution is NP-complete. An even more constrained version that is easier to solve is that the shape is orthogonally convex: having a single contiguous block of squares in each row and column.|$|E
5|$|When two proteins' {{distance}} matrices {{share the}} same or similar features in approximately the same positions, they {{can be said to}} have similar folds with similar-length loops connecting their secondary structure elements. DALI's actual alignment process requires a similarity search after the two proteins' distance matrices are built; this is normally conducted via a series of overlapping submatrices of size 6x6. <b>Submatrix</b> matches are then reassembled into a final alignment via a standard score-maximization algorithm— the original version of DALI used a Monte Carlo simulation to maximize a structural similarity score that {{is a function of the}} distances between putative corresponding atoms. In particular, more distant atoms within corresponding features are exponentially downweighted to reduce the effects of noise introduced by loop mobility, helix torsions, and other minor structural variations. Because DALI relies on an all-to-all distance matrix, it can account for the possibility that structurally aligned features might appear in different orders within the two sequences being compared.|$|E
40|$|We {{investigate}} the maximal size of distinguished <b>submatrices</b> of a Gaussian random matrix. Of interest are <b>submatrices</b> whose entries have average {{greater than or}} equal to a positive constant, and <b>submatrices</b> whose entries are well-fit by a two-way ANOVA model. We identify size thresholds and associated (asymptotic) probability bounds for both large-average and ANOVA-fit <b>submatrices.</b> Results are obtained when the matrix and <b>submatrices</b> of interest are square, and in rectangular cases when the matrix <b>submatrices</b> of interest have fixed aspect ratios. In addition, we obtain a strong, interval concentration result for the size of large average <b>submatrices</b> in the square case. A simulation study shows good agreement between the observed and predicted sizes of large average <b>submatrices</b> in matrices of moderate size. Comment: 25 pages, 3 figure...|$|R
3000|$|... (K×L) are <b>submatrices</b> of F. Because F^HF = I, {{specific}} relations hold {{between these}} <b>submatrices,</b> e.g. F_B^HF_A=-F_D^HF_C and F_A^HF_A+F_C^HF_C=I.|$|R
3000|$|The matrix in (18) is a nonVandermonde matrix. The invertibility of this matrix and its <b>submatrices</b> {{is proven}} by using brute force simulations. This {{is done by}} finding all the <b>submatrices</b> of (18) and calculating the {{determinants}} of these <b>submatrices.</b> For this matrix, we found inverses for one [...]...|$|R
25|$|A basic minor of {{a matrix}} is the {{determinant}} of a square <b>submatrix</b> {{that is of}} maximal size with nonzero determinant.|$|E
500|$|A {{principal}} <b>submatrix</b> is {{a square}} <b>submatrix</b> obtained by removing certain rows and columns. [...] The definition varies from author to author. According to some authors, a principal <b>submatrix</b> is a <b>submatrix</b> {{in which the}} set of row indices that remain {{is the same as}} the set of column indices that remain. Other authors define a principal <b>submatrix</b> to be one in which the first k rows and columns, for some number k, are the ones that remain; this type of <b>submatrix</b> has also been called a leading principal <b>submatrix.</b>|$|E
500|$|A <b>submatrix</b> of {{a matrix}} is {{obtained}} by deleting any collection of rows and/or columns. For example, {{from the following}} 3-by-4 matrix, we can construct a 2-by-3 <b>submatrix</b> by removing row 3 and column 2: ...|$|E
40|$|In a {{previous}} paper, we derived necessary and sufficient {{conditions for the}} invertibility of square <b>submatrices</b> of the Pascal upper triangular matrix. To do so, we established a connection with the two-point Birkhoff interpolation problem. In this paper, we extend this result by deriving a formula for the rank of <b>submatrices</b> of the Pascal matrix. Our formula works for both square and non-square <b>submatrices.</b> We also provide bases for the row and column spaces of these <b>submatrices.</b> Further, we apply our result to one-point lacunary polynomial approximation...|$|R
40|$|The eigenvectors and {{eigenvalues}} of block circulant matrices {{had been}} found for real symmetric matrices with symmetric <b>submatrices,</b> and for block circulant matrices with circulant <b>submatrices.</b> The eigenvectors are now found for general block circulant matrices, including the Jordan Canonical Form for defective eigenvectors. That analysis is applied to Stephen J. Watson’s alternating circulant matrices, which reduce to block circulant matrices with square <b>submatrices</b> of order 2...|$|R
50|$|In {{order to}} {{approximate}} the entire matrix , it is split {{into a family}} of submatrices.Large <b>submatrices</b> are stored in factorized representation, while small <b>submatrices</b> are stored in standard representationin order to improve the efficiency.|$|R
2500|$|A Jacobi {{rotation}} has {{the same}} form as a Givens rotation, but is used to zero both off-diagonal entries of a [...] symmetric <b>submatrix.</b>|$|E
2500|$|Divide {{through by}} x3 to get Cramer's rule for the {{solution}} {{of a set of}} two linear equations in two unknowns. [...] Notice that this yields a point in the z = 1 plane only when the 2 × 2 <b>submatrix</b> associated with x3 has a non-zero determinant.|$|E
2500|$|... computable in {{polynomial}} {{time as the}} determinant of a maximal principal <b>submatrix</b> of the Laplacian matrix of G, an early result in algebraic graph theory known as Kirchhoff’s Matrix–Tree theorem. Likewise, the dimension of the bicycle space at [...] can be computed in {{polynomial time}} by Gaussian elimination.|$|E
40|$|The {{inversion}} {{of a large}} (n × n) positive matrix is considered. We assume that the matrix has a semi-separable structure, which implies that all <b>submatrices</b> away from the main diagonal have rank less than q (the matrix itself may be full). In practice, a specified matrix will not exactly have low-rank <b>submatrices.</b> Given a threshold and a positive matrix T, the <b>submatrices</b> of T are rank truncated to this threshold (balanced model reduction) and the inverse of a Cholesky factor of T is computed using time-varying state-space techniques. The proposed algorithm requires O(n 2 q) operations, where q is the average rank of the <b>submatrices</b> as detected by the algorithm...|$|R
5000|$|The {{resultant}} of [...] {{may differ}} from the corresponding modified resultant in sign and by a constant factor. Determinants of <b>submatrices</b> of [...] are called subresultants and likewise determinants of <b>submatrices</b> of [...] are called modified subresultants.|$|R
40|$|AbstractThe {{results of}} our study are twofold. From the random matrix theory point of view we obtain results on the rank {{distribution}} of column <b>submatrices.</b> We give the moments and the covariances between the ranks (q−rank) of such <b>submatrices.</b> We conjecture the counterparts of these results for arbitrary <b>submatrices.</b> The case of higher correlations gets drastically complicated even {{in the case of}} three <b>submatrices.</b> We give a formula for the correlation of ranks of three <b>submatrices</b> and a conjecture for its closed form. From the code theoretical point of view our study yields the covariances of the coefficients of the weight enumerator of a random code. Particularly interesting is that the coefficients of the weight enumerator of a code with random parity check matrix are uncorrelated. We give a conjecture for the triple correlations between the coefficients of the weight enumerator of a random code...|$|R
2500|$|If A is {{a square}} matrix, then the minor of the {{entry in the}} i-th row and j-th column (also called the (i,j) minor, or a first minor) is the {{determinant}} of the <b>submatrix</b> formed by deleting the i-th row and j-th column. This number is often denoted Mi,j. The (i,j) cofactor is obtained by multiplying the minor by [...]|$|E
2500|$|... where [...] {{denote the}} ordered {{sequences}} of indices (the indices are in natural order of magnitude, as above) complementary to , {{so that every}} index [...] appears exactly one time in either [...] or , but not in both (similarly for the [...] and [...] ) and [...] denotes the determinant of the <b>submatrix</b> of A formed by choosing the rows of the index set [...] and columns of index set [...] Also, [...] A simple proof can be given using wedge product. Indeed, ...|$|E
2500|$|Let A be an m × n matrix and k {{an integer}} with 0 < k ≤ m, and k ≤ n. A k × k minor of A, also called minor {{determinant}} of order k of A or, if , (n-k):th minor determinant of A, {{with the word}} [...] "determinant" [...] often omitted and the word [...] "order" [...] sometimes replaced by [...] "degree", is the determinant of a k × k matrix obtained from A by deleting m − k rows and n − k columns. Sometimes the term is {{used to refer to}} the k × k matrix obtained from A as above (by deleting m − k rows and n − k columns), but this matrix should be referred to as a (square) <b>submatrix</b> of A, leaving the term [...] "minor" [...] to refer to the determinant of this matrix. For a matrix A as above, there are a total of [...] minors of size k × k. Minor of order zero is often defined to be 1. For a square matrix, zeroth minor is just the determinant of the matrix.|$|E
40|$|Cataloged from PDF {{version of}} article. The {{results of our}} study are twofold. From the random matrix theory point of view we obtain results on the rank {{distribution}} of column <b>submatrices.</b> We give the moments and the covariances between the ranks (q- rank) of such <b>submatrices.</b> We conjecture the counterparts of these results for arbitrary <b>submatrices.</b> The case of higher correlations gets drastically complicated even {{in the case of}} three <b>submatrices.</b> We give a formula for the correlation of ranks of three <b>submatrices</b> and a conjecture for its closed form. From the code theoretical point of view our study yields the covariances of the coefficients of the weight enumerator of a random code. Particularly interesting is that the coefficients of the weight enumerator of a code with random parity check matrix are uncorrelated. We give a conjecture for the triple correlations between the coefficients of the weight enumerator of a random code. © 2009 Elsevier Inc. All rights reserved...|$|R
40|$|The {{results of}} our study are twofold. From the random matrix theory point of view we obtain results on the rank {{distribution}} of column <b>submatrices.</b> We give the moments and the covariances between the ranks (q- rank) of such <b>submatrices.</b> We conjecture the counterparts of these results for arbitrary <b>submatrices.</b> The case of higher correlations gets drastically complicated even {{in the case of}} three <b>submatrices.</b> We give a formula for the correlation of ranks of three <b>submatrices</b> and a conjecture for its closed form. From the code theoretical point of view our study yields the covariances of the coefficients of the weight enumerator of a random code. Particularly interesting is that the coefficients of the weight enumerator of a code with random parity check matrix are uncorrelated. We give a conjecture for the triple correlations between the coefficients of the weight enumerator of a random code. © 2009 Elsevier Inc. All rights reserved...|$|R
40|$|International audienceOrder-preserving <b>submatrices</b> are an {{important}} tool {{for the analysis of}} gene expression data. As finding large order-preserving <b>submatrices</b> is a computationally hard problem, previous work has investigated both exact but exponential-time as well as polynomial-time but inexact algorithms for finding large order-preserving <b>submatrices.</b> In this paper, we propose a novel exact algorithm to find maximum order preserving <b>submatrices</b> which is fixed parameter tractable with respect to the number of columns of the provided gene expression data. In particular, our algorithm is based on solving a sequence of mixed integer linear programs and it exhibits better guarantees as well as better runtime performance as compared to the state-of-the-art exact algorithms. Our empirical study in benchmark datasets shows large improvement in terms of computational speed...|$|R
50|$|A {{principal}} <b>submatrix</b> is {{a square}} <b>submatrix</b> obtained by removing certain rows and columns. The definition varies from author to author. According to some authors, a principal <b>submatrix</b> is a <b>submatrix</b> {{in which the}} set of row indices that remain {{is the same as}} the set of column indices that remain. Other authors define a principal <b>submatrix</b> to be one in which the first k rows and columns, for some number k, are the ones that remain; this type of <b>submatrix</b> has also been called a leading principal <b>submatrix.</b>|$|E
5000|$|... where R is the 3×3 <b>submatrix</b> {{describing}} {{rotation and}} T is the 3×1 <b>submatrix</b> describing translation.|$|E
50|$|More generally, if a <b>submatrix</b> {{is formed}} from the rows with indices {i1, i2, …, im} and the columns with indices {j1, j2, …, jn}, then the {{complementary}} <b>submatrix</b> is formed from the rows with indices {1, 2, …, N} \ {j1, j2, …, jn} and the columns with indices {1, 2, …, N} \ {i1, i2, …, im}, where N {{is the size}} of the whole matrix. The nullity theorem states that the nullity of any <b>submatrix</b> equals the nullity of the complementary <b>submatrix</b> of the inverse.|$|E
3000|$|... is {{such that}} all its <b>submatrices</b> are invertible. We show some of the matrix designs {{for a number of}} {{different}} sizes. We prove that the new designed matrices and their <b>submatrices</b> are invertible by finding the inverses using simulations. Also, the maximum degree determinant is calculated for [...]...|$|R
40|$|We {{provide a}} set of maximal rank-deficient <b>sub{{matrices}}</b> of a Kronecker product of two matrices A ⊗ B, {{and in particular the}} Kronecker product of Fourier matrices F = Fn 1 ⊗ [...] . ⊗ Fnk. We show how in the latter case, maximal rank-deficient <b>submatrices</b> can be constructedas tilings of rank-one blocks. Such tilings exist for any subgroup of a suitable Abelian group associated to the matrix F. These maximal rank-deficient <b>submatrices</b> are also related to an uncertainty principle for Fourier transforms over finite Abelian groups, for which we can then obtain stronger versions...|$|R
40|$|AbstractWell-known {{sufficiency}} {{conditions for}} total unimodularity are relaxed {{to include more}} general classes of matrices, whose determinants are related to Fibonacci sequences. It is then shown {{that in order to}} study determinants of <b>submatrices</b> of the two-commodity transportation problem, one should study precisely these generalized unimodular matrices. (Note that all <b>submatrices</b> of an ordinary transportation problem are unimodular.) This result enables us to establish determinantal values for <b>submatrices</b> of two-commodity transportation problems (in {{terms of the number of}} disjoint capacitated routes) and to identify a totally unimodular class of two-commodity transportation problems...|$|R
5000|$|A <b>submatrix</b> of {{a matrix}} is {{obtained}} by deleting any collection of rows and/or columns. For example, {{from the following}} 3-by-4 matrix, we can construct a 2-by-3 <b>submatrix</b> by removing row 3 and column 2: ...|$|E
5000|$|Every {{principal}} <b>submatrix</b> of {{a positive}} definite matrix is positive definite.|$|E
5000|$|The Hadamard {{product is}} a {{principal}} <b>submatrix</b> of the Kronecker product.|$|E
40|$|Ankara : The Department of Mathematics and the Institute of Engineering and Sciences of Bilkent University, 2009. Thesis (Ph. D.) [...] Bilkent University, 2009. Includes bibliographical {{references}} leaves 52 - 54. Results of {{our study}} are two fold. From the code theoretical point of view our study yields the expectations and the covariances of the coefficients of the weight enumerator of a random code. Particularly interesting is that, the coefficients of the weight enumerator of a code with random parity check matrix are uncorrelated. We give conjectures for the triple correlations of the coefficients of weight enumerator of random codes. From the random matrix theory point of view we obtain results in the rank distribution of column <b>submatrices.</b> We give the expectations and the covariances between the ranks (q −rank) of such <b>submatrices</b> over Fq. We conjecture the counterparts of these results for arbitrary <b>submatrices.</b> The case of higher correlations gets drastically complicated even {{in the case of}} three <b>submatrices.</b> We give a formula for the correlation of ranks of three <b>submatrices</b> and a conjecture for its closed form. Özen, İbrahimPh. D...|$|R
30|$|In the {{sections}} on G^ 0 -, and G^ 1 -continuity, the <b>submatrices</b> of the Gram matrix G_m,n are defined. Each section {{uses the same}} notation for these <b>submatrices,</b> but with different dimensions. It should be clear that the form used within a section is the one defined within that section.|$|R
50|$|Every 5 &times; 5 matrix has exactly 251 square <b>submatrices.</b>|$|R
