18|505|Public
40|$|The paper {{deals with}} vector {{constrained}} extremum problems. A separation scheme is recalled; starting from it, a vector Lagrangian duality theory is developed. The linear duality due to Isermann can be embedded in this separation approach. Some classical applications are {{extended to the}} multiobjective framework in the linear case, exploiting the duality theory of Isermann. Vector Optimization, <b>Separation,</b> <b>Image</b> Space Analysis, Lagrangian Duality, Set-Valued Function. ...|$|E
40|$|We {{study the}} {{sparsity}} of spectro-temporal representation of speech in reverberant acoustic conditions. This study motivates {{the use of}} structured sparsity models for efficient speech recovery. We formulate the underdetermined convolutive speech separation in spectro-temporal domain as the sparse signal recovery where we leverage model-based recovery algorithms. To tackle the ambiguity of the real acoustics, we exploit the Image Model of the enclosures to estimate the room impulse response function through a structured sparsity constraint optimization. The experiments conducted on real data recordings demonstrate {{the effectiveness of the}} proposed approach for multi-party speech applications. Index Terms: speech sparsity, structured sparsity models, underdetermined convolutive speech <b>separation,</b> <b>Image</b> Mode...|$|E
40|$|Abstract—In this paper, we {{deal with}} a problem of {{separating}} the effect of reflection from images captured behind glass. The input consists of multiple polarized images captured from the same view point but with different polarizer angles. The output is the high quality separation of the reflection layer and the background layer from the images. We formulate this problem as a constrained optimization problem and propose a framework {{that allows us to}} fully exploit the mutually exclusive image information in our input data. We test our approach on various images and demonstrate that our approach can generate good reflection separation results. Index Terms—reflection <b>separation,</b> <b>image</b> enhancement (a) (b...|$|E
50|$|PostScript Level 2 was {{introduced}} in 1991, and included several improvements: improved speed and reliability, support for in-RIP <b>separations,</b> <b>image</b> decompression (for example, JPEG images could be rendered by a PostScript program), support for composite fonts, and the form mechanism for caching reusable content.|$|R
30|$|As a result, if the {{convolution}} <b>separation</b> and <b>image</b> re-sampling techniques work cooperatively, {{the theoretical}} detection speed will be six-folded by 3 [*]×[*] 3 filter.|$|R
40|$|Previously {{published}} {{online at}} DOI: 10. 1373 /clinchem. 2006. 076075 Background: We modeled {{the expression of}} proteins in baseline bronchoalveolar lavage (BAL) samples from asymptomatic 60 -year-old lifelong current smokers or healthy never-smokers, who were reevaluated after 6 to 7 years to record clinical outcome. Methods: Applying a technology toolbox consisting of replicate 2 -dimensional gel <b>separations,</b> <b>image</b> annotation, and mass spectrometry identification, we catalogued a global set of proteins that were differentially expressed in individuals by presence, absence, and intensity scores. Results: By use of multivariate analysis, we selected a subset of proteins that accurately separated smokers from never-smokers based on composite scoring. Follow-u...|$|R
40|$|The {{independent}} {{component analysis}} (ICA) algorithm and ICA basic model were studied {{in detail in}} this paper. Here mainly discussed the mathematical theory of ICA and FastICA algorithm {{of the most widely}} used at home and abroad. Then, carried out simulation in blind sources process of the mixing images. Through simulation of three color images (640 * 480) mixed by a 3 * 3 random matrix, and using the Fast ICA separation algorithm, realized the mixed blind source separation. The simulation experiment results show that the FastICA algorithm can gain a good approach effect, and the <b>separation</b> <b>image</b> is basically consistent with the original image...|$|E
40|$|We {{cast the}} under-determined convolutive speech {{separation}} as sparse approximation of the spatial spectra of the mixing sources. In this framework we {{compare and contrast}} the major practical al-gorithms for structured sparse recovery of speech signal. Specific {{attention is paid to}} characterization of the measurement matrix. We first propose how it can be identified using the Image model of multi-path effect where the acoustic parameters are estimated by localizing a speaker and its images in a free space model. We further study the circumstances in which the coherence of the projections induced by microphone array design tend to affect the recovery performance. Index Terms — Structured sparse signal recovery, convolutive source <b>separation,</b> <b>Image</b> model, sparse microphone array 1...|$|E
40|$|Weighted median, in {{the form}} of either solver or filter, has been {{employed}} {{in a wide range of}} computer vision solu-tions for its beneficial properties in sparsity representation. But it is hard to be accelerated due to the spatially varying weight and the median property. We propose a few efficient schemes to reduce computation complexity from O(r 2) to O(r) where r is the kernel size. Our contribution is on a new joint-histogram representation, median tracking, and a new data structure that enables fast data access. The effec-tiveness of these schemes is demonstrated on optical flow estimation, stereo matching, structure-texture <b>separation,</b> <b>image</b> filtering, to name a few. The running time is largely shortened from several minutes to less than 1 second. The source code is provided in the project website. 1...|$|E
40|$|We {{present a}} halo model {{prediction}} of the <b>image</b> <b>separation</b> distribution of strong lenses. Our model takes {{account of the}} subhalo population, which has been ignored in previous studies, {{as well as the}} conventional halo population. Halos and subhalos are linked to central and satellite galaxies by adopting an universal scaling relation between masses of (sub-) halos and luminosities of galaxies. Our model predicts that 10 %- 20 % of lenses should be caused by the subhalo population. The fraction of lensing by satellite galaxies (subhalos) peaks at ~ 1 '' and decreases rapidly with increasing <b>image</b> <b>separations.</b> We compute fractions of lenses which lie in groups and clusters, and find them to be ~ 14 % and ~ 4 %, respectively: Nearly half of such lenses are expected to be produced by satellite galaxies, rather than central parts of halos. We also study mass distributions of lensing halos and find that even at <b>image</b> <b>separations</b> of ~ 3 '' the deviation of lens mass distributions from isothermal profiles is large: At or beyond ~ 3 '' <b>image</b> <b>separations</b> are enhanced significantly by surrounding halos. Our model prediction agrees reasonably well with observed <b>image</b> <b>separation</b> distributions from galaxy to cluster scales. Comment: 11 pages, 11 figures, accepted for publication in MNRA...|$|R
40|$|The authors {{report the}} {{discovery}} of a cluster-scale lensed quasar, SDSS J 1029 + 2623, selected from the Sloan Digital Sky Survey. The lens system exhibits two lensed images of a quasar at z{sub s} = 2. 197. The <b>image</b> <b>separation</b> of 22. 5 makes it the largest separation lensed quasar discovered to date. The similarity of the optical spectra and the radio loudnesses of the two components support the lensing hypothesis. Images of the field show a cluster of galaxies at z{sub l} {approx} 0. 55 that is responsible for the large <b>image</b> <b>separation.</b> The lensed <b>images</b> and the cluster light center are not collinear, which implies that the lensing cluster has a complex structure...|$|R
40|$|Background: Excessive lipid {{accumulation}} {{within the}} myocardial tissue predisposes to ventricular arrhythmias and {{increased risk of}} sudden cardiac death. This study aimed to detect intramyocardial fat infiltration in patients with chronic myocardial infarction (MI) using advanced cardiac magnetic resonance (CMR) imaging techniques – fat-water separation and feature tracking. Methods: Twenty patients with chronic MI underwent advanced CMR imaging. The study protocol included conventional cine, late gadolinium enhancement (LGE) and fat-water separation (2 -point mDixon) imaging. Left ventricular (LV) circumferential (EccLV) and radial (ErrLV) strain were calculated using dedicated software (CMR 42, Calgary, Canada). The extent of global scar tissue was calculated in LGE and fat-water <b>separation</b> <b>images</b> to compare advanced and conventional imaging techniques. Results: The infarct size as derived from conventional LGE and fat-water <b>separation</b> <b>images</b> was similar. However, detection of intramyocardial fat was only possible with fat-water separation imaging. Subjects with intramyocardial fat deposition demonstrated significantly smaller percentage of fibrosis than those without fat deposition (10. 68 ± 5. 07 % vs. 13. 83 ± 6. 30 %; p = 0. 005). There {{was no significant difference}} in LV circumferential (EccLV) and radial (ErrLV) strain between myocardial segments containing fibrosis only and fibrosis with fat deposition (- 11. 94 ± 5. 92 % vs. - 12. 63 ± 7. 14 %; p = 0. 668 for EccLV and 20. 85 ± 16. 48 % vs. 17. 89 ± 12. 43 %; p = 0. 607 for ErrLV). Conclusions: Advanced CMR imaging enables more detailed tissue characterization in patients with previous MI without relevant increase in examination or post-processing time. LGE and myocardial feature tracking techniques are unable to detect intramyocardial fat deposition...|$|R
40|$|An {{unsupervised}} classification algorithm is derived by modeling observed data as {{a mixture of}} several mutually exclusive classes that are each described by linear combinations of independent, non-Gaussian densities. The algorithm estimates the density of each class {{and is able to}} model class distributions with non-Gaussian structure. The new algorithm can improve classification accuracy compared with standard Gaussian mixture models. When applied to blind source separation in nonstationary environments, the method can switch automatically between classes, which correspond to contexts with different mixing properties. The algorithm can learn efficient codes for images containing both natural scenes and text. This method shows promise for modeling non-Gaussian structure in high-dimensional data and has many potential applications. Index TermsUnsupervised classification, Gaussian mixture model, independent component analysis, blind source <b>separation,</b> <b>image</b> coding, automatic contex [...] ...|$|E
40|$|An {{unsupervised}} classification algorithm {{is derived from}} an ICA mixture model assuming that the observed data can be categorized into several mutually exclusive data classes whose components are generated by linear mixtures of independent non-Gaussian sources. The algorithm finds the independent sources, the mixing matrix for each class and also computes the class membership probability for each data point. The new algorithm can improve classification accuracy compared with standard Gaussian mixture models. When applied to blind source separation in nonstationary environments, the method can switch automatically between learned mixing matrices. The algorithm can learn efficient codes to represent images containing both natural scenes and text. This method shows promise for modeling structure in high-dimensional data and has many potential applications. Index Terms: Unsupervised classification, Gaussian mixture model, independent component analysis, blind source <b>separation,</b> <b>image</b> coding [...] ...|$|E
30|$|An obvious asset of imaging in full-field mode (and the {{near-field}} diffraction regime) is {{the simple}} and robust experimental setup, flexible sample environments {{but most importantly}} a uniquely fast acquisition of a large 3 D dataset [16]. The timing of such experiments is critical and is up to now only weakly correlated with the dynamics in the sample. One reason {{is that there is}} practically no feedback available at these acquisition rates. Our concept of real-time data monitoring at the imaging beamline at MAX IV will enable an informed decision taking process in terms of acquisition control and raw data recording. This step is critical for ensuring that only relevant raw data are saved and enter the offline analysis pipeline. One way to achieve this is through fast tomographic reconstruction and feature <b>separation</b> (<b>image</b> binarization) with consequent basic topological and statistical measurement on selected tomographic slices.|$|E
40|$|Using an {{analytical}} model, we compute {{the distribution of}} <b>image</b> <b>separations</b> resulting from gravitational lensing of distant sources, for 7 COBE-normalized CDM models with various combinations of Omega_ 0 and lambda_ 0. Our model assumes that multiple imaging results from strong lensing by individual galaxies. We model galaxies as nonsingular isothermal spheres, and {{take into account the}} finite angular size of the sources. Our model neglects the contribution of the background matter distribution, and assumes that lensing is entirely caused by galaxies. To test the validity of this assumption, we performed a series of ray-tracing experiments to study the effect of the background matter on the distribution of <b>image</b> <b>separations.</b> The analytical model predicts that the distributions of <b>image</b> <b>separations</b> are virtually indistinguishable for flat, cosmological constant models with different values of Omega_ 0. For models with no cosmological constant, the distributions of <b>image</b> <b>separations</b> do depend upon Omega_ 0, but this dependence is weak. We conclude that while the number of multiple-imaged sources can put strong constraints on the cosmological parameters, the distribution of <b>image</b> <b>separations</b> does not constrain the cosmological models in any significant way, and mostly provides constraints on the structure of the galaxies responsible for lensing. Comment: One Plain TeX file, with 12 postscript figures. Accepted for publication in The Astrophysical Journa...|$|R
40|$|Using an {{analytical}} model, we compute {{the distribution of}} <b>image</b> <b>separations</b> resulting from gravitational lensing of distant sources, for 7 COBE-normalized CDM models with various combinations of Ω 0 and λ 0. Our model assumes that multiple imaging results from strong lensing by individual galaxies. We model galaxies as nonsingular isothermal spheres whose parameters are functions of the luminosity and morphological type, and {{take into account the}} finite angular size of the sources. Our model neglects the contribution of the background matter distribution to lensing, and assumes that lensing is entirely caused by galaxies. To test the validity of this assumption, we performed a series of ray-tracing experiments to study the effect of the background matter on the distribution of <b>image</b> <b>separations.</b> Our results are the following: (1) The presence of the background matter tends to increase the <b>image</b> <b>separations</b> produced by lensing galaxies, making the distributions of <b>image</b> <b>separations</b> wider. However, this effect is rather small, and independent of the cosmological model. (2) Simulations with galaxies and background matter often produce a secondary peak in the distribution of <b>image</b> <b>separations</b> at large separations. This peak does not appear when the background matter is excluded from the simulations. (3) The effect of the background matter on the magnification distribution is negligible in low density universes (Ω 0 = 0. 2) with small density contrast (σ 8 = 0. 4), bu...|$|R
3000|$|This <b>image</b> <b>separation</b> {{problem can}} be solved {{efficiently}} using an iterative shrinkage algorithm proposed in [17] (Figure  5).|$|R
40|$|Abstract. This article {{presents}} {{the participation of}} the MIILab (Medi-cal Image Information Laboratory) group in ImageCLEFmed 2013. There are three types of tasks for ImageCLEFmed 2013 : modality classification, image retrieval and compound–image <b>separation.</b> <b>Image</b> modality classi-fication and medical image retrieval are targeted according to MIILab’s research interest. The main goal is to perform a feasibility test on ap-plying existing techniques on new applications, such as applying image denoising techniques on image retrieval and classification. Both global features and local features were employed. Fast filtering tech-niques were used to obtain global features on color, shape and texture. These global features serves to perform a pre–classification on images. Both low–level and high–level local features were extracted. Bags of fea-tures model was used to build final feature vector. Both kNN and SVM classifiers were tried out in modality classification task. Reciprocal kNN was used to perform result fusion in image retrieval task...|$|E
40|$|Abstract—We {{propose a}} physically-based {{approach}} to separate reflection using multiple polarized images {{with a background}} scene captured behind glass. The input consists of three polarized images, each captured from the same view point but with a different polarizer angle separated by 45 degrees. The output is the high-quality separation of the reflection and background layers {{from each of the}} input images. A main technical challenge for this problem is that the mixing coefficient for the reflection and background layers depends on the angle of incidence and the orientation of the plane of incidence, which are spatially-varying over the pixels of an image. Exploiting physical properties of polarization for a double-surfaced glass medium, we propose a multiscale scheme which automatically finds the optimal separation of the reflection and background layers. Thorough experiments, we demonstrate that our approach can generate superior results to those of previous methods. Index Terms—reflection <b>separation,</b> <b>image</b> enhancement, polarized light, computational photography. ...|$|E
40|$|This work {{addresses}} the recovery and demixing problem of signals that are sparse in some general dictionary. Involved applications include source <b>separation,</b> <b>image</b> inpainting, super-resolution, and restoration of signals corrupted by clipping, saturation, impulsive noise, or narrowband interference. We employ the ℓ_q-norm (0 < q < 1) for sparsity inducing and propose a constrained ℓ_q-minimization formulation for the recovery and demixing problem. This nonconvex formulation is approximately solved by two efficient first-order algorithms based on proximal coordinate descent and alternative direction method of multipliers (ADMM), respectively. The new algorithms are convergent in the nonconvex case under some mild conditions and scale well for high-dimensional problems. A convergence {{condition of the}} new ADMM algorithm has been derived. Furthermore, extension of the two algorithms for multi-channels joint recovery has been presented, which can further exploit the joint sparsity pattern among multi-channel signals. Various numerical experiments showed that the new algorithms can achieve considerable performance gain over the ℓ_ 1 -regularized algorithms. Comment: 13 pages, 9 figure...|$|E
40|$|Abstract — <b>Image</b> <b>separation</b> {{is defined}} as decomposing a real world image mixture into {{individual}} images objects. Independent component analysis is an active area of research and is being utilized for its capability in statistically independent <b>separation</b> <b>images.</b> Neural network algorithm ICA {{has been used to}} extract interference and mixed images and a very rapid developed statistical method during last few years, but because of very less literature (content) is available on the performance and analysis, i. e., how does it behave in communication science and environment? So, in this paper fast fixed point algorithms for ICA-based blind source separation has been presented. In blind source separation primary goal is to recover all original images using the observed mixtures only. Independent Component Analysis (ICA) is based on higher order statistics aiming at searching for the components in the mixed signals that are statistically as independent from each other as possible. In this paper a neural network algorithm based blind source separation using fast fixed point Independent component analysis simulations is presented to demonstrate the results of our analysis...|$|R
40|$|The cold {{dark matter}} {{scenario}} predicts {{that a large number}} of dark subhalos should be located within the halo of each Milky-way sized galaxy. One tell-tale signature of such dark subhalos could be additional milliarcsecond-scale image splitting of quasars previously known to be multiply-imaged on arcsecond scales. Here, we estimate the <b>image</b> <b>separations</b> for the subhalo density profiles favoured by recent N-body simulations, and compare these to the angular resolution of both existing and upcoming observational facilities. We find, that the <b>image</b> <b>separations</b> produced are very sensitive to the exact subhalo density profile assumed, but in all cases considerably smaller than previous estimates based on the premise that subhalos can be approximated by singular isothermal spheres. Only the most optimistic subhalo models produce <b>image</b> <b>separations</b> that would be detectable with current technology, and many models produce <b>image</b> <b>separations</b> that will remain unresolved with all telescopes expected to become available in the foreseeable future. Detections of dark subhalos through image-splitting effects will therefore be far more challenging than currently believed, albeit not necessarily impossible. Comment: 7 pages, 3 figures, accepted for publication in Ap...|$|R
40|$|We {{report the}} {{discovery}} of a cluster-scale lensed quasar, SDSS J 1029 + 2623, selected from the Sloan Digital Sky Survey. The lens system exhibits two lensed images of a quasar at z_s= 2. 197. The <b>image</b> <b>separation</b> of 22. 5 " makes it the largest separation lensed quasar discovered to date. The similarity of the optical spectra and the radio loudnesses of the two components support the lensing hypothesis. Images of the field show a cluster of galaxies at z_l~ 0. 55 that is responsible for the large <b>image</b> <b>separation.</b> The lensed <b>images</b> and the cluster light center are not collinear, which implies that the lensing cluster has a complex structure. Comment: 5 pages, 4 figures. Accepted for publication in ApJ Letter...|$|R
40|$|Sparse {{decomposition}} {{has been}} widely used for different applications, such as source <b>separation,</b> <b>image</b> classification and image denoising. This paper presents a new algorithm for segmentation of an image into background and foreground text and graphics using sparse decomposition. First, the background is represented using a suitable smooth model, which is a linear combination of a few smoothly varying basis functions, and the foreground text and graphics are modeled as a sparse component overlaid on the smooth background. Then the background and foreground are separated using a sparse decomposition framework and imposing some prior information, which promote the smoothness of background, and the sparsity and connectivity of foreground pixels. This algorithm has been tested on a dataset of images extracted from HEVC standard test sequences for screen content coding, and is shown to outperform prior methods, including least absolute deviation fitting, k-means clustering based segmentation in DjVu, and shape primitive extraction and coding algorithm. Comment: arXiv admin note: substantial text overlap with arXiv: 1602. 02434. appears in IEEE Signal Processing in Medicine and Biology Symposium, 201...|$|E
40|$|Sparse {{decomposition}} {{has been}} widely used for different applications, such as source <b>separation,</b> <b>image</b> classification, image denoising and more. This paper presents a new algorithm for segmentation of an image into background and foreground text and graphics using sparse decomposition and total variation minimization. The proposed method is designed {{based on the assumption}} that the background part of the image is smoothly varying and can be represented by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics can be modeled with a sparse component overlaid on the smooth background. The background and foreground are separated using a sparse decomposition framework regularized with a few suitable regularization terms which promotes the sparsity and connectivity of foreground pixels. This algorithm has been tested on a dataset of images extracted from HEVC standard test sequences for screen content coding, and is shown to have superior performance over some prior methods, including least absolute deviation fitting, k-means clustering based segmentation in DjVu and shape primitive extraction and coding (SPEC) algorithm. Comment: 5 pages in IEEE, International Conference on Image Processing, 201...|$|E
40|$|Virtual {{reality has}} much {{potential}} and many challenges. We investigate geometric image distortion issues arising from human factors concerns in systems using stereoscopic head-tracked displays. These displays are stationary {{and attached to}} a desk, tabletop or wall. The user perceives a true 3 D image and can examine the virtual scene from different viewpoints by physically moving around the display. Stereoscopic displays raise concerns beyond those found in simpler monoscopic display systems. To maximize viewing comfort and user interaction, viewing parameters must be automatically and dynamically adjusted. This thesis contributes the following: a framework for understanding, classifying, and comparing software techniques that help the viewer fuse the two stereoscopic 2 D images into a single 3 D perception analytic descriptions of the distortions induced by the following fusion control techniques: false eye <b>separation</b> <b>image</b> shifting image scaling {{a comparison of the}} geometric properties of the above three techniques {{with each other and with}} the other fusion control techniques a fusion control technique with fewer dynamic, geometric distortion components than prior methods a technique that balances multiple stereo viewing issues when traveling through an extensive, global terrain virtual environment while maintaining an exo-centric, or orbital view geometric guidelines for matching an application's geometric requirements to a set of appropriate fusion control technique...|$|E
40|$|QC 351 A 7 no. 52 Multiband {{photography}} {{has been}} employed {{for several years}} {{as a tool for}} aerial reconnaissance. Recent experiments indicate a potential value in earth resources studies, particularly those utilizing an electro-optical image sensor. This report lists the imaging properties of an electro-optical multiband camera that uses one combined image tube /camera tube sensor to achieve high- quality imagery with maximum registration between <b>separation</b> <b>images.</b> A study was made to determine which camera tube would be the best choice for the earth resources program. The results of the study indicate the best choice is a standard vidicon with fiber optic faceplate coupled to an image intensifier with an extended red photocathode. This decision was based on lifetime, image quality, ruggedization, and cost criteria...|$|R
40|$|Abstract. We {{investigate}} {{the problem of}} source <b>separation</b> in <b>images</b> in the Bayesian framework using the color channel dependencies. As {{a case in point}} we consider the source <b>separation</b> of color <b>images</b> which have dependence between its components. A Markov Random Field (MRF) is used for modeling of the inter and intra-source local correlations. We resort to Gibbs sampling algorithm for obtaining the MAP estimate of the sources since non-Gaussian priors are adopted. We test the performance of the proposed method both on synthetic color texture mixtures and a realistic color scene captured with a spurious reflection. ...|$|R
50|$|The {{preparation}} for Digital Embossing is not easy. An additional Black <b>Separation</b> with <b>image</b> {{in all the}} areas {{you would like to}} have Digital Embossing is needed. Because of the exceptional accuracy of the process (each sheet is registered optically before printing) the most complex shapes can be embossed.|$|R
40|$|Purpose In {{the recent}} past, the {{potential}} suitability of fixed samples to 2 D-DIGE studies {{has been demonstrated}} on model tissues, but not on “real-world” archival tissues. Therefore, this study was aimed to assess {{the quality of the}} results delivered by 2 D-DIGE on samples retrieved from hospital tissue repositories. Experimental design Diseased and normal tissue samples (namely, human gastric adenocarcinoma and normal gastric tissue, human lung neuroendocrine tumors, canine mammary tubulo-papillary carcinoma and normal mammary tissue, sheep liver with cloudy swelling degeneration and normal liver tissue) were retrieved from human and veterinary biorepositories and subjected to full-length protein extraction, cyanine labeling, 2 D-DIGE <b>separation,</b> <b>image</b> analysis, MS analysis, and protein identification. Results Archival samples could be successfully subjected to 2 D-DIGE, providing maps of satisfactory resolution, although with varying pattern complexity (possibly influenced by preanalytical variables). Moreover, differentially expressed protein identities were consistent with the disease biology. Conclusions and clinical relevance 2 D-DIGE can support biomarker discovery and validation studies on large sample cohorts. In fact, although some information complexity is lost when compared to fresh-frozen tissues, their vast availability and the associated patient information can considerably boost studies suffering limited sample availability or involving long-distance exchange of samples. </br...|$|E
40|$|The Machine Learning for Signal Processing Technical Committee (MLSP TC) {{is at the}} {{interface}} between theory and application, developing novel theoretically-inspired methodologies targeting both longstanding and emergent signal processing applications. Central to MLSP is on-line/adaptive nonlinear signal processing and data-driven learning methodologies. Since application domains provide unique problem constraints/assumptions and thus motivate and drive signal processing advances, it is only natural that MLSP research has a broad application base. MLSP thus encompasses new theoretical frameworks for statistical signal processing (e. g. machine learning-based and information-theoretic signal processing), new and emerging paradigms in statistical signal processing (e. g. independent component analysis (ICA), kernel-based methods, cognitive signal processing) and novel developments in these areas specialized to the processing {{of a variety of}} signals, including audio, speech, image, multispectral, industrial, biomedical, and genomic signals. The MLSP TC is focused on fostering research in these areas, the application of these techniques, and in educating the technical community about research developments in these areas. The MLSP TC organizes related technical sessions at ICASSP and has an annual international workshop, now in its nineteenth year, with the above-described technical scope. In addition, MLSP has technical presence on the IEEE Transactions on Signal Processing and (currently) the IEEE Transactions on Image Processing Editorial Boards and has initiated a new set of EDICS for IEEE Transactions on Signal Processing, including a recently approved area, Cognitive Information Processing, which is emerging and expected to attract strong interest in coming years. Fields of Interest Algorithm and Architectures: Artificial neural networks, kernel methods, committee models, Gaussian processes, independent component analysis, advanced (adaptive, nonlinear) signal processing, (hidden) Markov models, Bayesian modeling, parameter estimation, generalization, optimization, design algorithms. Applications: Biomedical engineering, bioinformatics, speech processing, blind source <b>separation,</b> <b>image</b> processing (computer vision, OCR), multimodal interactions, multi-channel processing, intelligent multimedia and web processing, robotics, sonar and radar, financial analysis, time series prediction, data fusion, data mining, adaptive filtering, communications, sensors, system identification, and other signal processing and pattern recognition applications. Implementations: Parallel and distributed implementation, hardware design, and other general implementation technologie...|$|E
25|$|There {{are several}} other methods for finding Green's functions, {{including}} {{the method of}} <b>images,</b> <b>separation</b> of variables, and Laplace transforms (Cole 2011).|$|R
5000|$|<b>Separation,</b> or specifying <b>images</b> or text {{to be put}} on plates {{applying}} individual printing media (inks, varnishes, etc.) to {{a common}} print.|$|R
30|$|We herein {{propose a}} new RI {{measurement}} method that utilizes the image defocusing technique {{in conjunction with}} a three-pinhole aperture, but in a different way from the aforementioned micro-refractometer [11]. While the previous study used the direct comparison of the <b>image</b> <b>separation,</b> we suggest a method that utilizes the change rate of the <b>image</b> <b>separation.</b> This method might have a wider range of applications due to its insensitivity to uncertainties in the environment, such as the working distance, sample thickness, and substrate material.|$|R
40|$|Condition: Poor; <b>separation</b> of <b>image</b> from glass plate.; Title from {{reference}} sources, see file 203 / 09 / 00017 - 02.; Part of collection: The Geoffrey Ingram {{collection of}} ballet {{photographs from the}} Ballets Russes Australian tour, 1936 - 1940. Dancer: Lubov Zlatina also known as Genevieve Moulin, see file 203 / 09 / 00017 - 02...|$|R
