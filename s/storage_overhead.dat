644|320|Public
2500|$|Another {{disadvantage}} of linked lists is the extra storage needed for references, which often makes them impractical for lists of small data {{items such as}} characters or boolean values, because the <b>storage</b> <b>overhead</b> for the links may exceed {{by a factor of}} two or more the size of the data. In contrast, a dynamic array requires only the space for the data itself (and a very small amount of control data)., where [...] is a per-array constant, [...] is a per-dimension constant, and [...] is the number of dimensions. [...] and [...] are typically on the order of 10 bytes. [...] It can also be slow, and with a naïve allocator, wasteful, to allocate memory separately for each new element, a problem generally solved using memory pools.|$|E
50|$|Compress {{the visible}} set data {{in order to}} {{minimize}} <b>storage</b> <b>overhead.</b>|$|E
5000|$|Limited pointer: In this {{approach}} directory information of {{limited number of}} blocks is kept at the directory to reduce <b>storage</b> <b>overhead.</b>|$|E
30|$|The {{discussion}} of <b>storage</b> <b>overheads</b> {{in the experimental}} section was from the financial perspective. We now consider workload-based optimizations to reduce the space requirement of the SPLIT scheme.|$|R
40|$|QOSPF [6 - 8] is a {{well-known}} link state based unicast QoS routing protocol. It is a precomputation-based approach with the strength of low routing latency but suffers from high control and <b>storage</b> <b>overheads.</b> Its overhead will be exacerbated not only for large-scale network {{but also for the}} extension to support multicast routing. Thus, few researches strive to directly extend the work of QOSPF to multicast capability. In this paper, we introduce a simple multicast extension version of QOSPF protocol named MQOSPF (Multicast extension to QOSPF), which uses a scope-limited advertising scheme to limit both control and <b>storage</b> <b>overheads.</b> Through extensive simulations, its excellent performances on low routing latency, high success rate to find a feasible attachment path to existing multicast tree and to construct low cost multicast delivery tree are demonstrated. KEY WORDS multicast, dynamic routing, QoS routing, precomputation, scope-limited advertising...|$|R
50|$|Associative arrays {{provide a}} {{mechanism}} for array-like functionality without huge <b>storage</b> <b>overheads</b> when the index values are sparse. For example, an array that contains values only at indexes 1 and 2 billion may benefit from using such a structure. Specialized associative arrays with integer keys include Patricia tries, Judy arrays, and van Emde Boas trees.|$|R
5000|$|... 1.4, {{released}} July 10, 2013, added counters, secondary indexing improvements, reduced object <b>storage</b> <b>overhead,</b> handoff progress reporting, and enhancements to MDC replication.|$|E
5000|$|Full bit-vector: A {{bit field}} for each {{processor}} at the directory node are maintained. The <b>storage</b> <b>overhead</b> scales {{with the number}} of processors.|$|E
50|$|<b>Storage</b> <b>overhead</b> for TSL is next {{to nothing}} since only one lock is required. Uncontended latency is also low since only one atomic {{instruction}} and branch are needed.|$|E
30|$|Here, we {{analyze the}} memory <b>storage,</b> {{communication}} <b>overhead,</b> and resiliency {{for the proposed}} scheme.|$|R
50|$|Riak 1.4 {{features}} PN-Counters, {{which is}} the database's first distributed data type, which are eventually consistent and can be incremented and decremented on any node across the cluster. It has a compact binary data format that reduces <b>storage</b> <b>overheads</b> connected with small objects or large bucket names. Riak CS 1.4 provides compatibility with OpenStack, OpenStack Object Storage (Swift) and CEPH object stores with increased scalability.|$|R
50|$|In {{computational}} {{fluid dynamics}} (CFD), {{it is impossible to}} numerically simulate turbulence without discretizing the flow-field as far as the Kolmogorov microscales, which is called direct numerical simulation (DNS). Because DNS simulations are exorbitantly expensive due to memory, computational and <b>storage</b> <b>overheads,</b> turbulence models are used to simulate the effects of turbulence. A variety of models are used, but generally TKE is a fundamental flow property which must be calculated in order for fluid turbulence to be modelled.|$|R
50|$|There are {{different}} kind of parallelization techniques which are used {{on the basis of}} data <b>storage</b> <b>overhead,</b> degree of parallelization and data dependencies. Some of the known techniques are: DOALL, DOACROSS and DOPIPE.|$|E
5000|$|Image Derivations: derived {{images are}} {{generated}} during run-time based on descriptions such as rotation, grid and overlay. These images depend on other images {{stored in the}} HEIF file. The <b>storage</b> <b>overhead</b> of derived images is small.|$|E
5000|$|Low <b>storage</b> <b>overhead</b> as a {{percentage}} of the drive size: Even with relatively small sector sizes, the storage space required for the bitmap is small. A 2 TiB drive could be fully represented with a mere 64 MiB bitmap.|$|E
40|$|On-chip wire delays are {{becoming}} increasingly problematic in modern microprocessors. To alleviate the negative effect of wire delays, architects have considered splitting up large L 2 /L 3 caches into several banks, with each bank having a different access latency depending on its physical proximity to the core. In particular, several recent papers have considered dynamic non-uniform cache architectures (D-NUCA) for chip multi-processors. These caches are dynamic {{in the sense that}} cache lines may migrate towards the cores that access them most frequently. In order to realize the benefits of data migration, however, a “smart search ” mechanism for finding the location of a given cache line is necessary. These papers assume an oracle and leave the smart search for future work. Existing search mechanisms either entail high performance <b>overheads</b> or inordinate <b>storage</b> <b>overheads.</b> In this paper, we propose a smart search mechanism, based on Bloom filters. Our approach is complexity-effective: {{it has the potential to}} reduce the performance and <b>storage</b> <b>overheads</b> of D-NUCA implementations. Also, Bloom filters are simple structures that incur little design complexity. We present the results of our initial explorations, showing the promise of our novel search mechanism. ...|$|R
40|$|Maintenance {{costs and}} <b>storage</b> <b>overheads</b> {{incurred}} by indexes often {{limit the number}} of indexes created per table in an RDBMS. For sparse data, where a table may have hundreds of attributes, indexing only a few attributes means that a vanishingly small percentage of attributes will have indexes, which unfortunately means that a table scan is the only evaluation plan for almost all selection queries on that table. This paper demonstrates that sparsity of the data actually enables index support for most, if not all, attributes in the data. Our approach leverages "sparse indexes", which are partial indexes that store only non-null values. Sparse indexes incur low maintenance costs and <b>storage</b> <b>overheads</b> because most values in a sparse table are null. Properties of the data lead us to two other contributions toward index support for sparse data; we show that sparse indexes benefit greatly from building all indexes in one-pass of the data; and we identify that multi-column sparse indexes are preferable as covering indexes when attributes in the data are correlated. We qualitatively evaluate our approaches with synthetic and real-world data to show that our suggestions significantly out-perform traditional indexing approaches designed for dense data...|$|R
40|$|As {{outsourcing}} data {{to remote}} storage servers gets popular, protecting user’s pattern in accessing these data {{has become a}} big concern. ORAM constructions are promising solutions to this issue, but their application in practice has been impeded by the high communication and <b>storage</b> <b>overheads</b> incurred. Towards addressing this challenge, this paper proposes a segmentation-based ORAM (S-ORAM). It adopts two segment-based techniques, namely, piece-wise shuffling and segment-based query, to improve the performance of shuffling and query by factoring block size into design. Extensive security analysis shows that S-ORAM is a provably highly secure solution with a negligible failure probability of O(NlogN). In terms of communication and <b>storage</b> <b>overheads,</b> S-ORAM out-performs the Balanced ORAM (B-ORAM) and the Path ORAM (P-ORAM), which are the state-of-the-art hash and index based ORAMs respectively, in both practical and theoretical evaluations. Particularly under practical settings, the communication overhead of S-ORAM is 12 to 23 times less than B-ORAM when {{they have the same}} constant-size user-side storage, and S-ORAM consumes 80 % less server-side storage and around 60 % to 72 % less bandwidth than P-ORAM when they have the similar logarithmic-size user-side storage...|$|R
50|$|Each {{directory}} entry must have 1 bit stored per processor per cache line, along with bits for tracking {{the state of}} the directory. This leads to the total size required being (number of processors)×number of cache lines, having a <b>storage</b> <b>overhead</b> ratio of (number of processors)/(cache block size×8).|$|E
50|$|Computer {{systems have}} {{increasingly}} cheap and plentiful memory, and <b>storage</b> <b>overhead</b> is not generally an overriding issue outside specialized embedded systems. Where {{it is still}} desirable to reduce the overhead of a linked list, unrolling provides a more practical approach (as well as other advantages, such as increasing cache performance and speeding random access).|$|E
50|$|In {{this case}} the {{directory}} entry uses 1 bit {{for a group of}} processors for each cache line. For the same example as Full Bit Vector format if we consider 1 bit for 8 processors as a group, then the <b>storage</b> <b>overhead</b> will be 128/(32×8)=50%. This is a significant improvement over the Full Bit Vector format.|$|E
30|$|We {{present a}} {{detailed}} evaluation of SPLIT on benchmark databases and demonstrate that its execution times are always within twice the corresponding plaintext times, thus providing an attractive security performance trade-off against an extremely strong adversary. Further, while SPLIT does incur large <b>storage</b> <b>overheads,</b> the extremely low resource costs on the Cloud {{allow it to}} retain viability. Finally, SPLIT is attractive from a deployment perspective also since it can be implemented as a security layer over existing database engines, without necessitating internal changes.|$|R
50|$|Each {{aggregate}} also incurs a <b>storage</b> capacity <b>overhead</b> {{of approximately}} 7-11%, {{depending on the}} disk type. On systems with many aggregates this can result in lost storage capacity.|$|R
30|$|When a fault occurs, the Monitoring & Fault {{handling}} unit waits for 't' time {{to examine whether}} it is a transient fault. If the services are not restored within 't' time, the Monitoring & Fault {{handling unit}} automatically hands over the report taken at the check point to the next ranked service provider to complete the task and the service execution is continued without any further delay. To avoid additional <b>storage</b> <b>overheads</b> at Monitoring & Fault handling unit, the report taken at checkpoint is overwritten during the next checkpoint.|$|R
50|$|It can be {{observed}} that directory overhead scales linearly {{with the number of}} processors. While this may be fine for a small number of processors, when implemented in large systems the size requirements for the directory becomes excessive. For example, with a block size of 32 bytes and 1024 processors, the <b>storage</b> <b>overhead</b> ratio becomes 1024/(32×8) = 400%.|$|E
5000|$|Standard BCD {{requires}} four bits per digit, roughly 20 {{percent more}} space than a binary encoding (the ratio of 4 bits to log210 bits is 1.204). When packed so that three digits are encoded in ten bits, the <b>storage</b> <b>overhead</b> is greatly reduced, {{at the expense}} of an encoding that is unaligned with the 8-bit byte boundaries common on existing hardware, resulting in slower implementations on these systems.|$|E
50|$|Mura, {{in terms}} of business/process improvement, is avoided through Just-In-Time systems {{which are based on}} keeping little or no inventory. These systems supply the {{production}} process with the right part, at the right time, In the right amount, using first-in, first-out (FIFO) component flow. Just-In-Time systems create a “pull system” in which each sub-process withdraws its needs from the preceding sub-processes, and ultimately from an outside supplier. When a preceding process does not receive a request or withdrawal it does not make more parts. This type of system is designed to maximize productivity by minimizing <b>storage</b> <b>overhead.</b>|$|E
40|$|Memory {{encryption}} {{has become}} a common approach to providing a secure processing environment, but current schemes suffer from extra performance and <b>storage</b> <b>overheads.</b> This paper presents predecryption {{as a method of}} providing this security with less overhead by using well-known prefetching techniques to retrieve data from memory and perform decryption before it is needed by the processor. Our results, tested mostly on SPEC 2000 benchmarks, show that using our predecryption scheme can actually result in no increase in execution time despite an extra 128 cycle decryption latency per memory block access. 1...|$|R
40|$|Abstract: Traditional dual copy, CAID, and RAID DASD {{subsystems}} {{can offer}} improved data reliability {{in cases of}} actuator and/or media failures. However, these schemes impose a write penalty for the extra I/Os required to maintain image copies or parity information for data contained in the subsystem. In this paper, we will employ {{a review of the}} 3990 - 3 / 6 read and write data flows as a basis for discussing write penalties and <b>storage</b> <b>overheads</b> associated with these extended count key data (ECKD) CAID and RAID DASD subsystems. ...|$|R
40|$|First-generation {{peer-to-peer}} storage systems unnecessarily couple {{the unit}} of client data access to {{the unit of}} infrastructure data management. Designs that require all peers to operate on data at a fixed granularity lead to inefficiencies such as high query load and high per-block <b>storage</b> <b>overheads.</b> To provide variable granularity access and support more efficient peer-to-peer storage systems, we introduce two-level naming of self-verifying data. We describe how to implement two-level naming and advocate an extension to the traditional API used by peer-topeer storage systems to support two-level naming. ...|$|R
5000|$|Another {{disadvantage}} of linked lists is the extra storage needed for references, which often makes them impractical for lists of small data {{items such as}} characters or boolean values, because the <b>storage</b> <b>overhead</b> for the links may exceed {{by a factor of}} two or more the size of the data. In contrast, a dynamic array requires only the space for the data itself (and a very small amount of control data). [...] It can also be slow, and with a naïve allocator, wasteful, to allocate memory separately for each new element, a problem generally solved using memory pools.|$|E
5000|$|Sparse columns {{that happen}} to contain data have a <b>storage</b> <b>overhead</b> of 4 bytes per column in {{addition}} to storage for the data type itself (e.g., 4 bytes for datetime columns). This impacts the amount of sparse-column data that you can associate with a given row. This size restriction is relaxed for the varchar data type, which means that, if one hits row-size limits in a production system, one has to work around it by designating sparse columns as varchar {{even though they may}} have a different intrinsic data type. Unfortunately, this approach now subverts server-side data-type checking.|$|E
50|$|Also, all of full disk {{encryption}} schemes don't {{protect from}} tampering (or silent data corruption, i.e. bitrot). That means they only provides privacy, not integrity. Block cipher-based encryption modes used for full disk encryption are not authenticated encryption themselves {{because of concerns}} of the <b>storage</b> <b>overhead</b> needed for authentication tags. Thus, if tampering would be done to data on the disk, the data would be decrypted to garbled random data when read and hopefully errors may be indicated depending on which data is tampered (for the case of file system metadata by the OS; for the case of file data by the corresponding program to process the file). To protect from these concerns, file systems with full data integrity via checksums (like Btrfs or ZFS) must be used on top of full disk encryption.|$|E
40|$|Current OSes {{include many}} logical sharing {{techniques}} (shared library, symbolic link, etc.) on memory and storage. Unfortunately they cause security and management problems which {{come from the}} dynamic management of logical sharing; e. g., search path replacement attack, GOT (Global Offset Table) overwrite attack, Dependency Hell, etc. This paper proposes that self-contained binaries eliminate the problems caused by logical sharing. The memory and <b>storage</b> <b>overheads</b> caused by self-contained binaries are mitigated by physical sharing (memory and disk deduplication). The effect of deduplication was investigated on the KVM virtual machine with KS...|$|R
40|$|The {{two sources}} for packet losses in multi-hop {{wireless}} ad hoc networks are link error and malicious packet dropping. This paper demonstrates that determining whether the losses {{are caused by}} link errors only, or by the combined effect of link errors and malicious drop. It is achieved through the implementation of homomorphic linear authenticator (HLA) based public auditing architecture that allows the detector to verify the truthfulness of the packet loss information reported by nodes. This architecture is privacy preserving, collusion proof, and incurs low communication and <b>storage</b> <b>overheads...</b>|$|R
30|$|When the above-described {{template}} matching {{is used for}} lattice rescoring in LVCSR, the computation and <b>storage</b> <b>overheads</b> are still high. However, certain redundancies in the training templates {{can be reduced to}} improve computation and storage efficiency. We propose three methods of template selection and compression to address this problem. In template selection, the goal is to choose a small subset of templates as the representatives for the full set of training templates. In template compression, new GMMs are generated for labeling the frames of the selected template representatives so as to better capture the information in the training Templates.|$|R
