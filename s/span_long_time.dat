8|10000|Public
50|$|Because IRC {{connections}} are usually unencrypted and typically <b>span</b> <b>long</b> <b>time</b> periods, {{they are an}} attractive target for DoS/DDoS attackers and hackers. Because of this, careful security policy is necessary to ensure that an IRC network is not susceptible to an attack such as a takeover war. IRC networks may also K-line or G-line users or servers that have a harming effect.|$|E
40|$|A single, {{stationary}} topic model such as latent Dirichlet allocation {{is inappropriate}} for modeling corpora that <b>span</b> <b>long</b> <b>time</b> periods, as {{the popularity of}} topics is likely to change over time. A number of models that incorporate time have been proposed, but in general they either exhibit limited forms of temporal variation, or require computationally expensive inference methods. In this paper we propose non-parametric Topics over Time (npTOT), a model for time-varying topics that allows an unbounded number of topics and exible distribution over the temporal variations in those topics' popularity. We develop a collapsed Gibbs sampler for the proposed model and compare against existing models on synthetic and real document sets. Comment: 9 page...|$|E
40|$|Neural {{networks}} have proven poor at learning the structure in complex and extended temporal sequences in which contingencies among elements can <b>span</b> <b>long</b> <b>time</b> lags. The principle of history compression [18] {{provides a means}} of transforming long sequences with redundant information into equivalent shorter sequences; the shorter sequences are more easily manipulated and learned by neural networks. The principle states that expected sequence elements can {{be removed from the}} sequence to form an equivalent, more compact sequence without loss of information. The principle was embodied in a neural net predictive architecture that attempted to anticipate the next element of a sequence given the previous elements. If the prediction was accurate, the next element was discarded; otherwise, it was passed on to a second network that processed the sequence in some fashion (e. g., recognition, classification, autoencoding, etc.). As originally proposed, a binary judgement was made as to the predictabi [...] ...|$|E
30|$|In {{this section}} {{the three main}} field models <b>spanning</b> <b>long</b> <b>time</b> intervals, namely gufm 1 (1590 – 1990), IGRF- 11 (the entire 20 th century) and CM 4 (1960 – 2002), are used to {{describe}} the secular variation.|$|R
50|$|There are 400 amphorae in the museum. The {{exhibited}} amphorae <b>span</b> a <b>long</b> <b>time</b> from 3200 BC to 1800 AD.|$|R
5000|$|Furthermore, HGT poses {{challenges}} for the ambitious reconstruction of the earliest events in evolution. Because the early branches of {{the tree of life}} <b>spanned</b> <b>long</b> <b>time</b> intervals and involved large numbers of organisms, many low-probability, as more accurately put highly-improbability. These HGT events are certain to have occurred even though these events were highly improbable, as stated [...] "low probability" [...] to have happened.|$|R
40|$|International audienceVisual {{representations}} of time-series {{are useful for}} tasks such as identifying trends, patterns and anomalies in the data. Many techniques have been devised to make these visual representations more scalable, enabling the simultaneous display of multiple variables, {{as well as the}} multi-scale display of time-series of very high resolution or that <b>span</b> <b>long</b> <b>time</b> periods. There has been comparatively little research on how to support the more elaborate tasks associated with the exploratory visual analysis of time- series, e. g., visualizing derived values, identifying correlations, or discovering anomalies beyond obvious outliers. Such tasks typically require deriving new time-series from the original data, trying different functions and parameters in an iterative manner. We introduce a novel visualization technique called ChronoLenses, aimed at supporting users in such exploratory tasks. ChronoLenses perform on-the-fly transformation of the data points in their focus area, tightly integrating visual analysis with user actions, and enabling the progressive construction of advanced visual analysis pipelines...|$|E
40|$|Abstract – New {{methods are}} {{presented}} to model, visualize and automatically recognize wide-area activities, which essentially are activities that span large areas (such as a facility or urban neighborhood) and that usually <b>span</b> <b>long</b> <b>time</b> intervals (such as hours and weeks). We introduce the no-go topology method and the chokepoint-observation interaction method, and then show how new algorithms {{can be built}} on them to recognize a category of wide-area activity, called process-type activities. Experimental results are presented for recognizing a manufacturing process observed using persistent GMTI sensor data. Then we present experimental results illustrating how an interesting activity can be detected as a deviation from a learned widearea normalcy model, and how new wide-area activity patterns can be discovered using simple visualizations of the results. One objective {{of this paper is}} to demonstrate that it is theoretically possible to recognize wide-area and processtype activities in built-up environments using GMTI data. The results presented here use somewhat ideal sensor data (small positional error ellipses, continuous GMTI observations, repetitive activities) and our approach is to move toward realistic parameters in operational situations (larger error ellipses, fewer observations, figuring out how to exploit additional kinds of activities) ...|$|E
40|$|Traditionally, {{scheduling}} in high-end parallel systems {{focuses on}} how to minimize the average job waiting time and {{on how to}} maximize the overall system utilization. Despite the development of scheduling strategies that aim at maximizing system utilization, parallel supercomputing traces that <b>span</b> <b>long</b> <b>time</b> periods indicate that such systems are mostly underutilized. Much of the time there is simply not enough load to keep the system fully utilized, although time periods do exist where system utilization levels peak at nearly 95 %. In this paper, we propose a new family of scheduling policies that aims at minimizing power consumption and cooling costs by selectively choosing to power down (or put in “sleep ” mode) parts of the system during periods of low load. Our goal {{is the development of}} a scheduling mechanism that adaptively adjusts the number of processors to the offered load while meeting predefined service-level agreements (SLAs). This scheduling mechanism uses online simulation, i. e., lightweight simulation modules that can execute while the system and its scheduler are in operation, and can guide resource provisioning in parallel systems. Detailed experimentation using traces from the Parallel Workloads Archive indicates that the proposed online mechanism is a viable alternative to conserve energy while meeting performance-based SLAs...|$|E
50|$|The novel <b>spans</b> a <b>long</b> <b>time</b> period, from 1918 {{when the}} {{settlement}} {{was established to}} 1957 when Aboriginal workers went on a strike, {{but most of the}} action takes place after 1930.|$|R
40|$|Abstract: The role of {{radiotherapy}} in {{the treatment}} of thymoma and thymic carcinoma has been evaluated by many investigators over the past two decades. The low incidence of these neoplasms has limited most published studies to small series <b>spanning</b> <b>long</b> <b>time</b> intervals or population-based studies. The exact indications and protocols for the use of radiotherapy {{as a part of the}} multidisciplinary approach to thymoma and thymic carcinoma are still unclear. However, a review of recent literature shows potential benefits for certain patients based on stage and grade of disease as well as the extent of surgical resection...|$|R
40|$|Data <b>spanning</b> <b>long</b> <b>time</b> periods, such as {{that over}} 1860 – 2012 for the UK, seem likely to have {{substantial}} errors of measurement that may even be integrated of order one, but which are probably cointegrated for cognate variables. We analyze and simulate the impacts of such measurement errors on parameter estimates and tests in a bivariate cointegrated system with trends and location shifts which reflect the many major turbulent events that have occurred historically. When trends or shifts therein are large, cointegration analysis is not much affected by such measurement errors, leading to conventional stationary attenuation biases dependent on the measurement-error variance, unlike the outcome {{when there are no}} offsetting shifts or trends...|$|R
40|$|Closed access. This {{article was}} {{published}} in the journal, Earth Surface Processes and Landforms [© John Wiley & Sons, Ltd. ] and the definitive version is available on open access on the publisher's website at: [URL] importance of glacigenic dust in the Earth's system during glacial periods is widely acknowledged. Under contemporary conditions, the world's largest dust sources are in low-lying, hot, arid regions and this is where most aeolian research is focused. However the processes of dust production and emissions are still operating in cold climate regions, particularly in proglacial areas. This paper assesses current understanding of the relationship between glacierised landscapes and dust emissions and inputs to the global dust cycle. It focuses on how elements in the glacial and aeolian geomorphic sub-systems interact to determine the magnitude, frequency and timing of aeolian dust emissions, and on feedback mechanisms between the systems. Where they have been measured, dust emission intensity and deposition rates in glacierised catchments are very high, in some cases far exceeding those in lower latitudes, however, few studies <b>span</b> <b>long</b> <b>time</b> scales. The impact of future glacier retreat on the balance between sediment supply, availability and aeolian transport capacity and implications for glacigenic dust emissions is also considered. This balance depends on relative spatial and temporal changes in meltwater suspended sediment concentration and wind strengths, which promote dust emissions, and patterns and rates of soil development and vegetation succession on recently-deglaciated terrain which protect sediments from deflation. Retreat of the Antarctic ice sheet could mean that in future glacigenic contributions to the dust cycle exceed those of non-glacigenic sources in the southern hemisphere...|$|E
30|$|Macro-clusters usually <b>span</b> across <b>long</b> <b>time</b> {{periods and}} exhibit various bursts of emails {{reflecting}} different campaigns, which use various topics {{and can even}} be operated in different countries. An example of a macrocampaign is illustrated in Figure 10, where it consists of six different scam campaigns of various sizes that include UK and Nigerian phone numbers.|$|R
40|$|This paper evaluates {{household}} transport consumption inequalities in France and the UK, investigates their temporal {{dynamics and}} estimates the redistributive effects of taxes on various commodity categories. A decomposition by expenditure {{component of the}} Gini index is applied, using household-level data from repeated cross-sections of expenditure surveys <b>spanning</b> <b>long</b> <b>time</b> periods. The results highlight the effect of car social diffusion. The relative contribution of vehicle use items to total expenditure inequality decreases over time, thus reflecting the more and more widespread use of the car. Moreover, fuel taxes become regressive (i. e. they affect the poor more than the rich), while the progressive character of taxes on the remaining car use commodities weakens with time. Therefore equity issues should not be ignored when designing policies to attenuate {{the environmental impact of}} cars. Increasing car use costs, notably fuel prices, through an increase of uniform taxes would be particularly inequitable...|$|R
40|$|This paper {{describes}} a methodology {{to model the}} propagation conditions for Earth observation data downlink operating at Ka band. It relies {{on the use of}} numerical weather forecast models to perform local high resolution reanalysis of the meteorological conditions on which the propagation effects can be computed. From the meteorological simulations <b>spanning</b> <b>long</b> durations, <b>time</b> series representative of attenuation between an orbiting satellite and a ground station are extracted, knowing orbital and RF characteristics of the system...|$|R
5000|$|Awarded with Rajasthans [...] "Maharana Pratap Award" [...] in 1987 for {{outstanding}} {{performance in the}} field of Basketball sport at the National and International levels for a <b>long</b> <b>span</b> of <b>time.</b>|$|R
50|$|The {{ability to}} print music arises {{from a series}} of {{technological}} developments in print and art histories over a <b>long</b> <b>span</b> of <b>time</b> (from the 11th to the 18th century) of which two will be highlighted.|$|R
40|$|GRB 080319 B {{is one of}} the {{brightest}} and most extensively sampled bursts. It has good coverage at many wavelengths. Here we present the optical observations of the Palomar 60 inch telescope, which <b>spans</b> a <b>long</b> <b>time</b> interval after the burst. We augment the optical dataset with freely available Swift BAT and XRT observations reduced by us. We also compare our conclusions with the published parameters from the rich literature about this burst. Comment: 2009 Fermi Symposium - to appear in eConf Proceedings C 09112...|$|R
5000|$|Al-Haafidh Ibn Kathir {{said about}} him:"He was the Shaikh ul Islaam, an Imaam, a Scholar, outstandingly proficient. There {{was not found}} in his time or before it by a <b>long</b> <b>span</b> of <b>time,</b> anyone {{possessing}} more Fiqh than him." ...|$|R
30|$|In addition, {{our data}} <b>span</b> a <b>long</b> <b>time</b> series of active trade policy. A before–after {{comparison}} {{of a single}} episode of trade liberalization would be missing out the important fact that the comparison is between an initial pattern of trade protection and another one with lower protection, but not between autarky and free trade (Goldberg and Pavcnik 2007). As in Galiani and Porto (2010), our time series of cross-sections overcomes this limitation by allowing for the comparison of trends in trade reforms {{and those in the}} outcome variable of interest, labor informality.|$|R
25|$|This {{dramatic}} shift in climate coincided with an apparent increase, rather than decrease, in {{the abundance of}} Tenontosaurus. This shows Tenontosaurus {{to have been a}} remarkably adaptable animal, which persisted for a <b>long</b> <b>span</b> of <b>time</b> in one area despite changes to its environment.|$|R
40|$|This paper {{reviews the}} extant {{empirical}} studies of financial innovation. Adopting broad criteria and <b>spanning</b> a <b>long</b> <b>time</b> horizon, we found surprisingly few studies (39), with most (23) having been conducted since 1998. Especially striking {{is that only}} two studies test hypotheses advanced in many descriptive articles as to the economic/environmental conditions that encourage financial innovation. We offer conjectures as to why empirical studies of financial innovation are comparatively rare, including as a culprit the absence of accessible data. We urge financial regulators to undertake more surveys of financial innovation {{and to make the}} resulting data available to researchers. ...|$|R
40|$|Large Engineering and Construction Projects (LECPs) form an {{important}} area of economic activity, covering {{a range of}} different artefacts. These projects have in common that they are massive undertakings, <b>spanning</b> <b>long</b> <b>time</b> periods and they involve large capital investments. Uncertainty and risk are the ruling paradigms. In the oil, gas and petrochemical industry, Engineering Contractors (ECs) {{play a key role}} in the development and implementation of the LECPs for processing facilities. The contract between owner and EC formalises their relationship, specifying the obligations and liabilities of the parties as well as the allocation of risk. The contracting process (covering the entire project life-cycle) comprises {{an important}} governance mechanism on LECPs. An experience-based theoretical framework of contracting for LECPs is developed by analysing and modelling contracting strategies and tactics and validation through normalised data. The research considers: 1. Contracting and market conditions; 2. Competitive lump sum bidding; 3. Reciprocal dependency between owner and EC; 4. Relational risk in cooperative contracting; 5. Earned value and cost phasing; and 6. Contract types and performance. The findings provide guidelines for effective contracting (efficiently with the ability to adapt to changing circumstances), facilitating the development and execution of LECPS faster and at lower cost than with traditional approaches. Mechanical Maritime and Materials Engineerin...|$|R
5000|$|This view of {{consolidation}} {{has been}} disputed, {{as it seems}} to suggest consolidation occurs over <b>long</b> <b>spans</b> of <b>time,</b> not just minutes or days, and [...] "requires physiological changes lasting years or decades." [...] Such long-term consolidation processes would seem to require multiple stages of consolidation, which remain hypothetical.|$|R
40|$|Abstract. Virtual agents {{can provide}} a sense of {{continuity}} in applications that <b>span</b> <b>long</b> periods of <b>time</b> and incorporate diverse activities, media, and modali-ties. We describe the design of a virtual lactation educator- agent that promotes breastfeeding in three settings, across different time spans, using a range of me-dia and counseling techniques. The agent provides “interpersonal continuity of care ” that is important in many areas of medicine. The results of a pilot study and an ongoing clinical trial are presented...|$|R
40|$|Quantum {{mechanics}} {{is usually}} presented starting {{from a series}} of postulates about the mathematical framework. In this work we show that those same postulates can be derived by assuming that measurements are discrete interactions: that is, that we measure at specific moments in time (as opposed to a continuous measurement that <b>spans</b> a <b>long</b> <b>time</b> interval) and that the system is in general affected by our measurement. We believe that this way of presenting quantum mechanics would make it easier to understand by laying out a more cohesive view of the theory and making it resonate more with our physics intuition. Comment: 8 pages, 4 figures, submitted to the American Journal of Physic...|$|R
25|$|Windows 9x/ME {{set aside}} two blocks of 64 KB memory regions for GDI and heap resources. By running {{multiple}} applications, applications with numerous GDI elements or by running applications over a <b>long</b> <b>span</b> of <b>time,</b> it could exhaust these memory areas. If free system resources dropped below 10%, Windows would become unstable and likely crash.|$|R
50|$|The Platte River Campground Site, {{designated}} 20BZ16, is {{an archaeological}} site located along the Platte River, within the Sleeping Bear Dunes National Lakeshore near Empire, Michigan. It is significant as a largely intact record of prehistoric life over a <b>long</b> <b>span</b> of <b>time.</b> It was {{listed on the}} National Register of Historic Places in 1990.|$|R
50|$|Windows 9x/ME {{set aside}} two blocks of 64 KB memory regions for GDI and heap resources. By running {{multiple}} applications, applications with numerous GDI elements or by running applications over a <b>long</b> <b>span</b> of <b>time,</b> it could exhaust these memory areas. If free system resources dropped below 10%, Windows would become unstable and likely crash.|$|R
50|$|Scottish {{lawyer and}} {{geologist}} Charles Lyell published his famous and influential work Principles of Geology in 1830-1833 which interpreted geologic {{change as the}} steady accumulation of minute changes over enormously <b>long</b> <b>spans</b> of <b>time</b> and that natural processes, uniformly applied over the length of that existence (uniformitarianism), could account for what men saw and studied in creation.|$|R
40|$|A {{variety of}} {{anthropogenic}} factors, including {{global climate change}} and eutrophication, are causing increases in cyanobacterial abundance. Increased prevalence of cyanobacteria can be detrimental, as some genera of cyanobacteria {{have the ability to}} produce toxic secondary metabolites known as cyanotoxins. The cyanotoxin microcystin- a hepatotoxin, is the most ubiquitous and toxic cyanotoxin in freshwater ecosystems, and has been quantified in a number of aquatic organisms. Microcystin was quantified in 8 fish species from St. Mary’s River in June, Saginaw Bay in September, and from Stony Creek Lake in October in 2014 by the Michigan Department of Environmental Quality and the Michigan Department of Natural Resources and findings were compared to current microcystin consumption advisories in order to determine what quantities of fish could be safely consumed. Total microcystin was determined in each sample with enzyme linked immuno sorbent assays and the total quantity of the microcystin variant microcystin-LR (the most toxic variant of microcystin) was determined with Liquid Chromatography coupled with tandem Mass Spectrometry. Results indicated that fish from these water bodies were well within established consumption limits. However, although results indicate that consumption need not be limited, findings from the present study should not be extrapolated beyond the specific time frames and locations studied, due to variability in environmental microcystin concentrations. Therefore, further research <b>spanning</b> <b>long</b> <b>time</b> frames and locations {{needs to be done in}} order to confidently determine safe fish consumption limits in regards to microcystin...|$|R
40|$|The paper evaluates {{transport}} consumption inequalities among {{households in}} France, Denmark and Cyprus, investigates their temporal dynamics and estimates the redistributive effects of taxes on different commodity categories. The redistributive {{effect of a}} tax is measured by its impact on inequalities. A decomposition by expenditure component of the Gini index is applied, using household-level data from repeated cross-sections of expenditure surveys <b>spanning</b> <b>long</b> <b>time</b> periods. The results highlight the effect of car social diffusion. The relative contribution of vehicle use items to total expenditure inequality decreases over time, reflecting the more and more widespread use of the car. Moreover, fuel taxes become regressive (i. e. they affect the poor more than the rich), while the progressive character of taxes on the remaining car use commodities weakens with time. Taxes on transport goods and services as a whole are progressive (i. e. they affect the rich more than the poor). However, this is principally due to the progressivity of taxes on automobile purchases. The progressivity of taxes on car purchases is by far much stronger in Denmark. In this country, these taxes are so high that car purchase costs can be afforded only by high incomes. These findings highlight that equity issues should not be overlooked when designing policies to attenuate {{the environmental impact of}} cars. Increasing car use costs, notably fuel prices, through an increase of uniform taxes would be particularly inequitable. Area-specific measures may be more appropriate...|$|R
50|$|Studies {{have shown}} that crop {{rotations}} greatly increase soil organic carbon (SOC) content, the main constituent of soil organic matter. Carbon, along with hydrogen and oxygen, is a macronutrient for plants. Highly diverse rotations <b>spanning</b> <b>long</b> periods of <b>time</b> have shown {{to be even more}} effective in increasing SOC, while soil disturbances (e.g. from tillage) are responsible for exponential decline in SOC levels. In Brazil, conversion to no-till methods combined with intensive crop rotations has been shown an SOC sequestration rate of 0.41 tonnes per hectare per year.|$|R
40|$|Abstract. Our {{understanding}} of the dynamical processes which control the structure and evolution of the interaction region between an accretion disk and the central star is reviewed. If the central star is unmagnetized, this interaction {{is in the form}} of a classical boundary layer. However, if the central star is strongly magnetized, it is expected that the inner disk is truncated by the stellar field, and the accretion flow follows the stellar field lines to the magnetic poles. Several outstanding questions remain regarding this picture. It is likely that numerical MHD simulations will prove essential to answering these questions. However, in order to model the MHD turbulence, angular momentum transport, and possible dynamo action associated with the dynamics of star-disk interaction, fully threedimensional simulations <b>spanning</b> <b>long</b> dynamical <b>times</b> are required. ...|$|R
30|$|Trajectories {{which did}} not <b>span</b> {{sufficient}} <b>long</b> <b>time</b> periods were rejected. All other trajectories were rendered with a Gaussian low pass filter. Drift correction was performed with affine matrices (see Additional file 1 : Figure S 1 b and Additional file 2 : Figure S 2). The registration of different spectral channels was performed by identifying fiducial markers that were present in both spectral data sets. A non-linear translation matrix was calculated from all fiducial markers in both channels. All coordinate processing routines were written in Python, Scipy and Numpy (Peterson 2009) (Additional file 1 : Figure S 1 b). Super-resolution images were generated with rapid STORM and overlaid in Fiji (Schindelin et al. 2012); 3 D data was visualized using PyMol (Delano 2004).|$|R
