588|630|Public
25|$|The {{process that}} allows infants to use {{prosodic}} cues in <b>speech</b> <b>input</b> {{to learn about}} language structure has been termed “prosodic bootstrapping”.|$|E
2500|$|Such <b>speech</b> <b>input</b> output {{imitation}} often occurs {{independently of}} speech comprehension {{such as in}} speech shadowing when a person automatically says words heard in earphones, and the pathological condition of echolalia in which people reflexively repeat overheard words. This links to speech repetition of words being separate in the brain to speech perception. Speech repetition occurs in the [...] dorsal speech processing stream while speech perception occurs in the ventral speech processing stream. Repetitions are often incorporated unawares by this route into spontaneous novel sentences immediately or after delay following [...] storage in phonological memory.|$|E
50|$|Speechpad.pw uses Google's speech {{recognition}} engine and Chrome native messaging API to provide direct <b>speech</b> <b>input</b> in Linux.|$|E
30|$|Melto, A., et al., Evaluation of {{predictive}} text and <b>speech</b> <b>inputs</b> in a multimodal mobile route guidance application, in Proceedings of the 10 th {{international conference on}} Human computer interaction with mobile devices and services. 2008, ACM: Amsterdam, The Netherlands.|$|R
3000|$|... in the MDF structure. Utilizing these results, we {{show how}} the SP tap-selection can be {{incorporated}} into the MDF structure for fast convergence and low delay. The computational complexity for the proposed algorithm is discussed in Section 3.3. In Section 4, we present the simulation results and discussions using both colored Gaussian noise (CGN) and <b>speech</b> <b>inputs</b> for NEC. Finally, conclusions are drawn in Section 5.|$|R
40|$|This paper {{discusses}} {{the significance of}} the multimodal interaction in virtual environments (VE) and the criticalities involved in integration and coordination between modes during interaction. Also, we present an architecture and design of the integration mechanism with respect to information access in second language learning. In this connection, we have conducted an experiential study on <b>speech</b> <b>inputs</b> to understand how far users’ experience of information can be considered to be supportive to this architecture. ...|$|R
50|$|The French company Kapsys {{offers a}} {{navigation}} system without a display, {{that works with}} <b>speech</b> <b>input</b> and output, called Kapten.|$|E
50|$|TSF {{enables a}} text service to store {{metadata}} with a document, {{a piece of}} text, or an object within the document. For example, a <b>speech</b> <b>input</b> text service can store sound information associated with a block of text.|$|E
50|$|A {{simulation}} of speech perception involves presenting the TRACE computer program with mock <b>speech</b> <b>input,</b> running the program, and generating a result. A successful simulation {{indicates that the}} result {{is found to be}} meaningfully similar to how people process speech.|$|E
40|$|This paper {{proposes a}} new {{matching}} algorithm to retrieve speech {{information from a}} speech database by speech query that allows continuous input. The algorithm is called Shift Continuous DP (CDP). Shift CDP extracts similar sections between two speech data sets. Two speech data sets are considered as reference patterns that are regarded as a <b>speech</b> database and <b>input</b> <b>speech</b> respectively. Shift CDP applies CDP to a constant length of unit reference patterns and provides a fast match between arbitrary sections in the reference pattern and the <b>input</b> <b>speech.</b> The algorithm allows endless input and real-time responses for the <b>input</b> <b>speech</b> query. Experiments were conducted for conversational speech and the results showed Shift CDP was successful in detecting similar sections between arbitrary sections of the reference speech and arbitrary sections of the <b>input</b> <b>speech.</b> This method {{can be applied to}} all kinds of time sequence data such as moving images...|$|R
3000|$|We have {{proposed}} SPMMax-MDF for network echo cancellation in VoIP. This algorithm achieves {{a faster rate}} of convergence, low complexity, and low delay by novelly exploiting both the MMax and SP tap-selection in the frequency domain using MDF implementation. We discussed two approaches of incorporating MMax tap-selection into MDF and showed their tradeoff between rate of convergence and complexity. Simulation results using both colored Gaussian noise and <b>speech</b> <b>inputs</b> show that the proposed SPMMax-MDF achieves up to [...]...|$|R
40|$|As {{the number}} of viable {{applications}} for unmanned aerial vehicle (UAV) systems increases at an exponential rate, interfaces that reduce the reliance on highly skilled engineers and pilots must be developed. Recent work aims {{to make use of}} common human communication modalities such as speech and gesture. This paper explores a multimodal natural language interface that uses a combination of <b>speech</b> and gesture <b>input</b> modalities to build complex UAV flight paths by defining trajectory segment primitives. Gesture inputs are used to define the general shape of a segment while <b>speech</b> <b>inputs</b> provide additional geometric information needed to fully characterize a trajectory segment. A user study is conducted in order to evaluate the efficacy of the multimodal interface...|$|R
50|$|Audio interfaces. The runtime {{includes}} {{objects for}} performing <b>speech</b> <b>input</b> from the microphone or speech output to speakers (or any sound device); {{as well as}} to and from wave files. It is also possible to write a custom audio object to stream audio to or from a non-standard location.|$|E
50|$|As a {{multimodal}} {{control language}} in the MultiModal Interaction framework, combining VoiceXML 3.0 dialogs with dialogs in other modalities including keyboard and mouse, ink, vision, haptics, etc. It may also control combined modalities such as lipreading (combined speech recognition and vision) <b>speech</b> <b>input</b> with keyboard as fallback, and multiple keyboards for multi-user editing.|$|E
5000|$|EVRC {{compresses}} each 20 milliseconds of 8000 Hz, 16-bit sampled <b>speech</b> <b>input</b> into output {{frames of}} one of three different sizes: full rate - 171 bits (8.55 kbit/s), half rate - 80 bits (4.0 kbit/s), eighth rate - 16 bits (0.8 kbit/s). A quarter rate {{was not included in}} the original EVRC specification and eventually became part of EVRC-B.|$|E
40|$|Face-to-face {{communication}} involves both {{hearing and}} seeing speech. Heard and seen <b>speech</b> <b>inputs</b> interact during audiovisual speech perception. Specifically, seeing the speaker's mouth and lip movements improves identification of acoustic speech stimuli, especially in noisy conditions. In addition, visual speech may even change the auditory percept. This occurs when mismatching auditory speech is dubbed onto visual articulation. Research {{on the brain}} mechanisms of audiovisual perception aims at revealing where, when and how inputs from different modalities interact. In this thesis, {{functional magnetic resonance imaging}} (fMRI), magnetoencephalography (MEG) and behavioral methods were used to study the neurocognitive mechanisms of audiovisual speech perception. The results suggest that interactions during audiovisual and visual speech perception have an effect on auditory speech processing at early levels of processing hierarchy. The results also suggest that auditory and visual <b>speech</b> <b>inputs</b> interact in the motor cortical areas involved in speech production. Some of these regions are part of the "mirror neuron system" (MNS). The MNS performs a specialized primate cerebral function of coupling two fundamental processes – motor action execution and perception – together. It is suggested that this action-perception coupling mechanism might be involved in audiovisual integration of speech. reviewe...|$|R
40|$|In {{this paper}} we present some {{experiments}} using a deep learn-ing model for speech denoising. We propose a very lightweight procedure that can predict clean speech spectra {{when presented with}} noisy <b>speech</b> <b>inputs,</b> and we show how various parameter choices impact {{the quality of the}} denoised signal. Through our experiments we conclude that such a structure can perform bet-ter than some comparable single-channel approaches and that it is able to generalize well across various speakers, noise types and signal-to-noise ratios...|$|R
30|$|After {{smoothing}} stage, this algorithm resynthesizes {{the segregated}} <b>speech</b> from <b>input</b> mixture {{and the final}} mask C with an inverse filter of the gammatone filterbank[12].|$|R
50|$|Enhanced Variable Rate Codec B (EVRC-B) is {{a speech}} codec used by CDMA networks. EVRC-B is an {{enhancement}} to EVRC and compresses each 20 milliseconds of 8000 Hz, 16-bit sampled <b>speech</b> <b>input</b> into output frames {{of one of}} the four different sizes: Rate 1 - 171 bits, Rate 1/2 - 80 bits, Rate 1/4 - 40 bits, Rate 1/8 - 16 bits.|$|E
5000|$|PSQM (Perceptual Speech Quality Measure) is a {{computational}} {{and modeling}} algorithm defined in ITU Recommendation ITU-T P.861 that objectively evaluates and quantifies voice quality of voice-band (300 - 3400 Hz) [...]It {{may be used}} to rank the performance of these [...] with differing <b>speech</b> <b>input</b> levels, talkers, bit rates and transcodings. The ITU-T has Withdrawn P.861 and replaced it with P.862 (PESQ) which contains an improved speech assessment algorithm.|$|E
50|$|The {{argument}} for prosodic bootstrapping {{was first introduced}} by Gleitman and Wanner (1982), who observed that infants might use prosodic cues (particularly acoustic cues) to discover underlying grammatical information about their native language. These cues (e.g intonation contour in a question phrase, lengthening a final segment) could aid infants in dividing the <b>speech</b> <b>input</b> into different lexical units, and furthermore aid in placing these units into syntactic phrases appropriate to the language.|$|E
40|$|We {{present a}} Wizard of Oz {{evaluation}} of dual–purpose speech, a technique {{designed to provide}} support during a face–to–face conversation by leveraging a user’s conversational <b>speech</b> for <b>input.</b> With a dual–purpose speech interaction, the user’s speech is meaningful {{in the context of}} a human–to–human conversation while providing useful input to a computer. For our experiment, we evaluate the ability to schedule appointments with our calendaring application, the Calendar Navigator Agent. We examine the relative difference between using <b>speech</b> for <b>input</b> compared to traditional pen input on a PDA. We found that speech is more direct and our participants can use their conversational <b>speech</b> for computer <b>input.</b> In doing so, we reduce the manual input needed to operate a PDA while engaged in a calendaring conversation...|$|R
40|$|Speech dialog systems need to {{deal with}} various kinds of {{ill-formed}} <b>speech</b> <b>inputs</b> that appear in natural human-human dialog. Self-correction (or speech-repair) is a particularly problematic phenomenon. Although many ways of dealing with selfcorrection have been proposed, these have limitations in both detecting and correcting for this phenomenon. In this paper, we propose a method to overcome these problems in Japanese speech dialog. We evaluate the proposed method using our speech dialog corpus and discuss its limitations and the work that remains to be done. ...|$|R
40|$|International audienceNamed Entity Recognition (NER) is a {{well-known}} Natural Language Processing (NLP) task, used as a preliminary processing to provide a semantic level to more complex tasks. Recently {{a new set of}} named entities has been defined, this set has a multilevel tree structure, where base entities are combined to define more complex ones. In this paper I describe, an effective and original NER system robust to noisy <b>speech</b> <b>inputs</b> that ranked first at the 2012 ETAP NER evaluation campaign with results far better than those of the other participating systems...|$|R
50|$|Beginning 2014 a JavaScript HTML5 {{version of}} ToonTalk called ToonTalk Reborn for the Web has been available. It runs on any modern web browser and {{differs from the}} desktop version of ToonTalk in a few ways. ToonTalk {{programs}} can run on any DOM element and various browser capabilities (audio, video, style sheets, <b>speech</b> <b>input</b> and output, and browser events) are available to ToonTalk programs. Web services such as Google Drive are integrated. ToonTalk Reborn is free and open source.|$|E
50|$|DragonDictate {{was first}} {{released}} for DOS, and utilized hidden Markov models, a probabilistic method for temporal pattern recognition. At the time, the hardware was not {{powerful enough to}} address the problem of word segmentation, and DragonDictate was unable to determine the boundaries of words during continuous <b>speech</b> <b>input.</b> Users were forced to enunciate one word at a time, each clearly separated by a small pause. DragonDictate was based on a trigram model, and is known as a discrete utterance speech recognition engine.|$|E
50|$|Inputlog {{is one of}} {{the most}} used keyloggers. It enables {{researchers}} to observe the online writing process unobtrusively. It logs every input action that is used to produce a text, viz. keystrokes (incl. navigation keys), mouse movements and clicks and <b>speech</b> <b>input</b> via Dragon Naturally Speaking (Nuance). The program also provides a time stamp (in ms) and detailed information about the Windows environment that is activated (e.g. URL of a web page). Researchers can download the program from the Inputlog website for free (after registration).|$|E
40|$|Most {{present day}} Speaker Identification (SID) systems {{focus on the}} speech {{features}} used for modeling the speakers without any concern for the <b>speech</b> being <b>input</b> to the system. Knowing how reliable the <b>input</b> <b>speech</b> information is can be very important and useful. The idea of SID-usable speech is to identify and extract those portions of corrupted <b>input</b> <b>speech,</b> which are more reliable for SID systems, thereby enhancing the speaker identification process. In this paper, usability in speech, with reference to speaker identification is presented which is called SID-usable speech. Here the SID system itself is used to determine those speech frames that are usable for accurate speaker identification. Two novel approaches to identify SID-usable speech frames are presented which resulted in 78 % and 72 % correct detection of SID-usable speech. It is also shown that SID performance can be quantified by comparing the amount of speech data required for correct identification. The amount of SIDusable <b>speech</b> as <b>input</b> was approximately 30 % less than entire input data with a considerable enhancement in SID performance. 1. ...|$|R
40|$|This paper {{describes}} a technique for synthesizing synchronized lip movements from auditory <b>input</b> <b>speech</b> signal. The technique {{is based on}} an algorithm for parameter generation from HMM with dynamic features, which has been successfully applied to text-to-speech synthesis. Audio-visual speech unit HMMs, namely, syllable HMMs are trained with parameter vector sequences that represent both auditory and visual <b>speech</b> features. <b>Input</b> <b>speech</b> is recognized using the syllable HMMs and converted into a transcription and a state sequence. A sentence HMM is constructed by concatenating the syllable HMMs corresponding to the transcription for the <b>input</b> <b>speech.</b> Then an optimum visual speech parameter sequence is generated from the sentence HMM in ML sense. Since the generated parameter sequence reflects statistical information of both static and dynamic features of several phonemes before and after the current phonemes, synthetic lip motion becomes smooth and realistic. We show experimental results which demonstrate the effectiveness of the proposed technique. 1...|$|R
40|$|Augmented Reality (AR) has the {{capability}} {{to interact with the}} virtual objects and physical objects simultaneously since it combines the real world with virtual world seamlessly. However, most AR interface applies conventional Virtual Reality (VR) interaction techniques without modification. In this paper we explore the multimodal fusion for AR with speech and hand gesture input. Multimodal fusion enables users to interact with computers through various <b>input</b> modalities like <b>speech,</b> gesture, and eye gaze. At the first stage to propose the multimodal interaction, the input modalities are decided to be selected before be integrated in an interface. The paper presents several related works about to recap the multimodal approaches until it recently {{has been one of the}} research trends in AR. It presents the assorted existing works in multimodal for VR and AR. In AR, multimodal considers as the solution to improve the interaction between the virtual and physical entities. It is an ideal interaction technique for AR applications since AR supports interactions in real and virtual worlds in the real-time. This paper describes the recent studies in AR developments that appeal gesture and <b>speech</b> <b>inputs.</b> It looks into multimodal fusion and its developments, followed by the conclusion. This paper will give a guideline on multimodal fusion on how to integrate the gesture and <b>speech</b> <b>inputs</b> in AR environment...|$|R
50|$|At 7.5 months English-learning infants {{have been}} shown to be able to segment words from speech that show a strong-weak (i.e., trochaic) stress pattern, which is the most common stress pattern in the English language, but they were not able to segment out words that follow a weak-strong pattern. In the {{sequence}} ‘guitar is’ these infants thus heard ‘taris’ as the word-unit because it follows a strong-weak pattern.The process that allows infants to use prosodic cues in <b>speech</b> <b>input</b> to learn about language structure has been termed “prosodic bootstrapping”.|$|E
50|$|Such <b>speech</b> <b>input</b> output {{imitation}} often occurs {{independently of}} speech comprehension {{such as in}} speech shadowing when a person automatically says words heard in earphones, and the pathological condition of echolalia in which people reflexively repeat overheard words. This links to speech repetition of words being separate in the brain to speech perception. Speech repetition occurs in the dorsal speech processing stream while speech perception occurs in the ventral speech processing stream. Repetitions are often incorporated unawares by this route into spontaneous novel sentences immediately or after delay following storage in phonological memory.|$|E
5000|$|In 1939, {{the model}} 12 <b>Speech</b> <b>Input</b> Console, in {{addition}} to the 26C limiter amplifier, was licensed to Canadian Marconi Co. for both sales in Canada and Her Majesties Service for the war effort. Collins success in constructing broadcast transmitters continued to grow, selling well over a thousand up to the start of World War II. During World War II, Collins expertise grew in higher power transmitters producing designs which ran well over 15 kilowatts of RF power on a continuous basis. After the war a limited number of AM transmitters were produced called the 300G and remain the finest in low power AM transmitters (300W) ever produced.|$|E
40|$|Named Entity Recognition is a {{well-known}} Natural Language Processing (NLP) task, used as a preliminary processing to provide a semantic level to more complex tasks. Recently {{a new set of}} named entities has been defined; this set has a multilevel tree structure, where base entities are combined to define more complex ones. In this paper I describe an effective and original NER system robust to noisy <b>speech</b> <b>inputs</b> that ranked first at the 2012 ETAPE NER evaluation campaign with results far better than those of the other participating systems. Index Terms — named entities recognition, Conditional Random Fields, discretization of numeric feature...|$|R
30|$|Our VC {{approach}} is exemplar-based, which {{is different from}} the conventional GMM-based VC. Exemplar-based VC using NMF has been proposed in [5]. In this framework, parallel training data is stored as source and target dictionaries. <b>Input</b> <b>speech</b> is decomposed into a linear combination of source exemplars from the source dictionary. Selected source exemplars are replaced with target exemplars, and the <b>input</b> <b>speech</b> is converted.|$|R
30|$|The {{sensitivity}} test is performed {{in view of}} security analysis to evaluate {{the protection of the}} proposed cryptosystem against various attacks [24]. To ensure the sensitivity of encrypted <b>speech,</b> the <b>input</b> <b>speech</b> signal is permuted for multiple levels using multiple chaotic maps to change the position of sampled values as the chaotic generators generate a voluminous random numbers. For testing the sensitivity of the proposed cryptosystem, the encrypted signal is decrypted with the reverse process of encryption method using the corresponding chaotic maps at the same four levels and resulting good quality speech recovered.|$|R
