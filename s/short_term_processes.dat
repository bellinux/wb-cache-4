11|10000|Public
2500|$|Alan Baddeley's {{theory of}} working memory has yet another aspect to which memory {{can be stored}} short term. [...] The visuo-spatial sketchpad is this store that holds visual {{information}} for manipulation. [...] The visuo-spatial sketchpad {{is thought to be}} its own storage of working memory in that it does not interfere with the <b>short</b> <b>term</b> <b>processes</b> of the phonological loop. [...] In research, it has been found that the visuo-spatial sketchpad can work simultaneously with the phonological loop to process both auditory and visual stimuli without either of the processes affecting the efficacy of the other. [...] Baddeley re-defined the theory of short-term memory as a working memory to explain this phenomenon. [...] In the original theory of short-term memory, it is understood that a person only has one store of immediate information processing which could only hold a total of 7 items plus or minus two items to be stored in {{a very short period of}} time, sometimes a matter of seconds. [...] The digit-span test is a perfect example of a measurement for classically defined short-term memory. Essentially, if one is not able to encode the 7 plus or minus two items within a few minutes by finding an existing association for the information to be transferred into long-term memory, then the information is lost and never encoded.|$|E
40|$|An {{adequate}} {{tool for}} preparing long term plans for water management is the probabilistic water quality prediction. Because {{of the long}} simulation period, and numerous repeated, stochastic varied calculations normally a static calculation with large time steps is used. Withal the erosion process at flood events, which is also important for long term water quality development in the catchment area, proceed on a short time scale. Therefore erosion cannot be considered adequately by rough time-discretised long term models. For considering <b>short</b> <b>term</b> <b>processes</b> with long term effects relevant <b>short</b> <b>term</b> <b>processes</b> have {{to be included in}} the modelling. This thesis deals with the consequent integration of <b>short</b> <b>term</b> <b>processes</b> in a Monte-Carlo-based long term water quality prediction. The new developed model LGpro is presented, which uses for long term calculation a time step of one month and simulates the relevant hydraulic, transport, transformation and sediment processes. In this simulation short term calculations of flood events are nested, which solve in an unsteady way all processes with time steps of few minutes. For construction the hydraulic, water quality and sediment modules from literature suitable approaches are chosen. Afterwards the severals moduls are validated. With one synthetic and one practical example the usage and the benefit of the new model LGpro is demonstrated. The time nested approach makes a better consideration of unsteady erosion processes in rivers possible and affords a better long term water quality prediction in catchment areas...|$|E
40|$|Senior thesis {{written for}} Oceanography 444 In this study, {{dissolved}} oxygen {{was measured in}} Barkley Sound and Effingham Inlet {{with the use of}} a CTD and Winkler titrations. Oxygen minimum zones or "dead zones" in the world's oceans can be strongly influenced by circulation patterns. Oxygen is a useful tracer of circulation and long and <b>short</b> <b>term</b> <b>processes</b> that can have serious consequences for macrofauna. Areas of old and newer water in Barkley Sound were identified. University of Washington, School of Oceanograph...|$|E
5000|$|A rhythmite {{consists}} of layers of sediment or sedimentary rock which are laid {{down with an}} obvious periodicity and regularity. They may be created by annual processes such as seasonally varying deposits reflecting variations in the runoff cycle, by <b>shorter</b> <b>term</b> <b>processes</b> such as tides, or by longer <b>term</b> <b>processes</b> such as periodic floods.|$|R
5000|$|Reception Center (RC): {{provides}} <b>short</b> <b>term</b> {{housing to}} <b>process,</b> classify and evaluate incoming inmate ...|$|R
2500|$|Reception Center (RC): [...] "Provides <b>short</b> <b>term</b> {{housing to}} <b>process,</b> {{classify}} and evaluate incoming inmates." ...|$|R
40|$|International audienceWe {{present a}} {{treatment}} of the cation exchange capacity (CEC) of clays in contact with an aqueous solution, during precipitation, using an ideal solid solution approach. It combines in a single calculation <b>short</b> <b>term</b> <b>processes</b> such as cation exchange and long term reactions like precipitation. This approach has been implemented and optimized in {{a new version of}} the NANOKIN code, which is thus now able to provide the time dependence of all particle sizes, composition and precipitated amount, as well as aqueous solution activities, pH and saturation states. It has been applied to the description of montmorillonite and illite precipitation from aqueous solutions at different temperatures and initial conditions: (1) an initial supersaturation with an aqueous solution composition which mimics a fresh water at T = 25 degrees C, a sea water at the same temperature, and an hydrothermal fluid at T = 150 degrees C; (2) a progressive supersaturation resulting from the kinetic dissolution of granitic rock-forming minerals, under weathering or hydrothermal conditions. The competition between illites and montmorillonites known in these various environments is well reproduced, as well as their dominant cation composition and typical particle sizes. (C) 2011 Elsevier Ltd. All rights reserved...|$|E
40|$|Observations of glacial {{isostatic}} adjustment (GIA) {{offers a}} powerful {{window into the}} properties of the Earth's interior. Combined with dynamical modeling of the GIA process we can use the observations to infer properties such as the elastic structure of the lithosphere, the rheology of the mantle and changes in the stress conditions in the Earth. This information aids our understanding of the long term evolution of the Earth, e. g. mantle convection, but also illuminates <b>short</b> <b>term</b> <b>processes</b> such as magma generation, earthquakes and shoreline migration. As present day warming trends causes glacier retreat world wide, GIA offers the opportunity to gain local insight into the Earth. In this thesis I develop an implementation of the pre-stress advection term in finite element modeling. I apply this to current GIA in Iceland, and conclude that local variations in the elastic thickness of the lithosphere can potentially be detected close to the largest ice cap. I study the magnitude of dehydration stiffening in the uppermost Icelandic mantle. The results indicate that the increase in viscosity over the dry solidus is of small magnitude, implying a non-linear rheology in the uppermost mantle beneath Iceland. The present deglaciation in Iceland causes additional melting of the mantle. I find an increased melt production rate of 100 - 140 % at present, although the melt supply rate {{at the base of the}} lithosphere is found to be delayed, with estimated present day perturbations ranging from neglible up to 120 %. In the last section of the thesis I focus on the role of ice sheet reconstructions in GIA modeling. I compare three reconstruction of the Weichselian ice sheet and discuss similarities and difference as well as the fit to present day uplift rates in Fennoscandia. The results provide input to improvements in the ice sheet models...|$|E
40|$|For {{conservation}} biology and sustainable management, the natural character of tropical forest {{is a crucial}} issue. Its assessment is usually based on ecological proxies such as forest composition and structure. However the estimation made on this basis only considers <b>short</b> <b>term</b> <b>processes</b> at a local scale. In contrast the long term processes are appraised by palaeoecological proxies (such as pollen) at a regional scale. So as to assess the degree of naturalness of tropical ecosystems in a conservation perspective {{it is important to}} combine a long temporal scale as well as a fine resolution spatial scale. Such approaches using palaeoecological proxies have been recently tested in temperate Europe but little in tropical ecosystems. Nonetheless the long term preservation of the palaeoecological material and its broad presence in the environment are necessary conditions to fulfill. In this perspective soil charcoal appears to meet these requirements. In this paper we aimed at assessing the naturalness of tropical forest using soil charcoal from southeastern Cameroon. Fieldwork involving as well archaeology as botany was undertaken at 53 sites. We quantified charcoal in soil samples by layers of 10 cm taken from pits located in the center of plots of botanical inventory. Spatial projections were performed using statistics together with multivariate analyses. Radiocarbon dating allowed interpreting the temporal framework. Results showed the ubiquitous presence of charcoal at each site. Main charcoal peaks were interpreted as fields (slash-and-burn agriculture) in the vicinity of ancient villages. These practices shaped the forest over time which cannot be considered as natural anymore. This underlines the potential input of the use of palaeoecological material in {{conservation biology}} and sustainable management issues. Charcoal fragments are under taxonomical identification and may provide new insights on the long term history of forest composition. Peer reviewe...|$|E
5000|$|Reception Center (RC) - {{provides}} <b>short</b> <b>term</b> {{housing to}} <b>process,</b> classify and evaluate incoming inmates ...|$|R
5000|$|Reception Center (RC): [...] "Provides <b>short</b> <b>term</b> {{housing to}} <b>process,</b> {{classify}} and evaluate incoming inmates." ...|$|R
50|$|CitComS is one {{component}} of a larger collection of software encompassed by the former GeoFramework project, a collaboration between the Center for Advanced Computing Research (CACR) and the Seismological Laboratory, both at Caltech, and the Victorian Partnership for Advanced Computing in Australia. The GeoFramework project developed a suite of tools to model multi-scale deformation for Earth science problems. This effort was motivated {{by the need to}} understand interactions between the long-term evolution of plate tectonics and <b>shorter</b> <b>term</b> <b>processes</b> such as the evolution of faults during and between earthquakes. During 2005 and 2006 much of the remaining software developed by GeoFramework was released under a GPL license and made available from Computational Infrastructure for Geodynamics (CIG).|$|R
40|$|Monitoring of {{air quality}} (AQ) and its {{transport}} at the continental scale, {{as well as}} the development of efficient forecast systems for air quality is one of the issues included in the GMES (Global Monitoring for Environment and Security) European Programme {{for the establishment of a}} European capacity for Earth Observation. The availability of satellite instruments which have the ability to monitor tropospheric ozone in the lowermost troposphere would be a step forward for this system. To monitor small scale and <b>short</b> <b>term</b> <b>processes</b> as involved in pollution event development, a geostationary Earth orbit (GEO) observing system is particularly well adapted. Future GEO missions dedicated to air quality monitoring using thermal infrared (TIR) instruments are planned to be operating over the USA, Japan and Korea, while existing and planned missions over Europe are not well adapted for this task. One of the objectives of the GeoQAIR project is to evaluate different satellite instrument concepts for their ability to monitor AQ and in particular quantify the possible impact for AQ forecasting. Four instruments have been considered for this study: the existing instrument IASI on MetOp-A (Low Earth Orbit - LEO - mission), the planned IASI-NG on the EPS-SG platform (LEO mission) and IRS on Sentinel 4 /MTG platform (GEO mission mainly dedicated to meteorology) and a new GEO mission concept, MAGEAQ, dedicated to AQ monitoring and proposed at the last Earth Explorer 8 call of ESA. Pseudo-observations for the four instruments have been generated to simulate one month of ozone observations over Europe. About 45 millions of individual measurements have been simulated using the EGI facilities. A first analysis of the performances of the different instruments to measure ozone in the lowermost troposphere demonstrates that the short time and space scale processes implied in air pollution development will not be correctly apprehended with the current existing and planned missions. Dedicated instrument with sufficient spectral resolution and signal to noise ratio, as proposed within the MAGEAQ mission concept, are necessary to correctly represent these processes...|$|E
40|$|While {{long term}} erosion rates of bedrock {{material}} may be estimated by dating methods, current day erosion rates are – {{if at all}} available – based on rough estimates or on point measurements. Precise quantification of short term erosion rates are required to improve our understanding of <b>short</b> <b>term</b> <b>processes,</b> for input in landscape evolution models, {{as well as for}} studying the mechanics and efficiency of different erosion processes in varying geomorphological settings. Typical current day erosion rates in the European Alps range from sub-millimetre to several millimetres per year depending on the dominant erosion processes. The level of surveying accuracy required for recurring sub-millimetre to millimetre measurements in the field is demanding. A novel surveying setup for in-situ measurement of bedrock erosion was tested recently in three different locations in Switzerland. Natural bedrock was investigated in the Gornera gorge close to Zermatt. Further on, bedrock samples were installed in exposed locations in the Erlenbach research watershed close to Einsiedeln, and in the Illgraben debris flow channel, located in the Canton Schwyz and Valais, respectively. A twofold measurement approach was chosen for all locations. For the first setup control points providing an absolute reference frame for recurrent measurements were embedded close to the area of interest. Close range photogrammetry was applied to measure surface changes on the bedrock samples. The precision for surface measurements in the field was 0. 1 mm (1 σ) and thus suitable for the application. The equipment needed for the surveys can easily be carried to the field. At one field site a structured light scanner was used along with the photogrammetric setup. Although the current generation of structured light scanners appeared less suitable for field application, data acquisition was much faster and checking the data for completeness in the field was straight forward. The latest generation of compact structured light scanners will probably be most suitable for similar applications...|$|E
40|$|We {{present an}} {{analysis}} of J, H, and Ks time series photometry obtained with the southern 2 MASS telescope over a 0. 84 ◦× 6 ◦ region centered near the Trapezium region of the Orion Nebula Cluster. These data are used to establish the near-infrared variability properties of pre-main-sequence stars in Orion on time scales of 1 - 36 days, 2 months, and 2 years. A total of 1235 near-infrared variable stars are identified, 93 % of which are likely associated with the Orion A molecular cloud. The variable stars exhibit a diversity of photometric behavior with time, including cyclic fluctuations with periods up to 15 days, aperiodic day-to-day fluctuations, eclipses, slow drifts in brightness over one month or longer, colorless variability (within the noise limits of the data), stars that become redder as they fade, and stars that become bluer as they fade. The mean peak-to-peak amplitudes of the photometric fluctuations are 0. 2 m in each band and 77 % of the variable stars have color variations less than 0. 05 m. The more extreme stars in our sample have amplitudes as large as 2 m and change in color {{by as much as}} 1 m. The typical time scale of the photometric fluctuations is less than a few days, indicating that near-infrared variability results primarily from <b>short</b> <b>term</b> <b>processes.</b> We examine rotational modulation of cool and hot star spots, variable obscuration from an inner circumstellar disk, and changes in the mass accretion rate and other physical properties in a circumstellar disk as possible physical origins of the near-infrared variability. Cool spots alone can explain the observed variability characteristics in ~ 56 - 77 % of the stars, while the properties of the photometric fluctuations are more consistent with hot spots or extinction changes in at least 23 % of the stars, and with variations in the disk mass accretion rate or inner disk radius in 1 % of our sample. However, differences between the details of the observations and the details of variability predicted by hot spot, extinction, and accretion disk models suggest either that another variability mechanism not considered here may be operative, or that the observed variability represents the net results of several of these phenomena. Analysis of the star count data indicates that the Orion Nebula Cluster {{is part of a larger}} area of enhanced stellar surface density which extends over a 0. 4 ◦ × 2. 4 ◦ (3. 4 pc× 20 pc) region containing ~ 2700 stars brighter than Ks = 14 m...|$|E
50|$|In animals, homeostatic and homeorhetic {{processes}} {{are involved in}} the abnormally high growth rates. Homeostatic processes usually affect compensatory growth in the <b>short</b> <b>term,</b> whereas homeorhetic <b>processes</b> usually have a long-term effect.|$|R
40|$|International audienceThree {{hours of}} high {{frequency}} vertical windspeed {{and carbon dioxide}} concentration data recorded over tropical forest in Brazil are presented and discussed in relation to various detrending techniques used in eddy correlation analysis. Running means with time constants 100, 1000 and 1875 s and a 30 minute linear detrend, as commonly used to determine fluxes, have been calculated for each case study and are presented. It is shown that, for different trends in the background concentration of carbon dioxide, the different methods {{can lead to the}} calculation of radically different fluxes over an hourly period. The examples emphasise the need for caution when interpreting eddy correlation derived fluxes especially for <b>short</b> <b>term</b> <b>process</b> studies. Keywords: Eddy covariance; detrending; running mean; carbon dioxide; tropical fores...|$|R
40|$|Abstract: Idea {{generation}} is a cognitive process that plays {{a central role}} in inquiry learning tasks. This paper presents results from a controlled experiment in which we investigate the affect on productivity and learning from doing idea generation tasks individually versus in pairs, with versus without automatic support from a virtual brainstorming agent called VIBRANT. Our finding is that individuals brainstorming with VIBRANT produced more ideas than individuals who brainstormed with a human peer. However, an additional finding is that while brainstorming in pairs lead to <b>short</b> <b>term</b> <b>process</b> losses in <b>terms</b> of idea generation, with a corresponding reduction in learning in terms of pre to post test gains, it produced a productivity gain for a subsequent distinct individual inquiry task. Furthermore, automatically generated feedback from VIBRANT improved learning during idea generation but did not mitigate the process losses that were associated with reduced learning in the pairs conditions...|$|R
40|$|The Ems estuary has {{constantly}} {{changed over}} the past centuries both from man-made and natural influences. On the time scale of thousands of years, sea level rise has created the estuary and dynamically changed its boundaries. More recently, storm surges created the Dollard sub-basin in the 14 th - 15 th centuries. Beginning in the 16 th century, diking and reclamation of land has greatly altered the surface area of the Ems estuary, particularly in the Dollard. These natural and anthropogenic changes to the surface area of the Ems altered the flow patterns of water, the tidal characteristics, and the patterns of sediment deposition and erosion. Since 1945, reclamation of land has halted and {{the borders of the}} Ems estuary have changed little. Sea level rise has continued, and over the past 40 years the rate of increase in mean high water (MHW) along the German coast has accelerated to 40 cm/ century. Climate has varied on a decadal time scale due to long-term variations in the North Atlantic Oscillation (NAO), which controls precipitation, temperature, and the direction and magnitude of winds. Between 1960 and 1990 the most intense variation in the NAO index on record was observed. As a result the magnitude and frequency of storm surges increased, and mean wave heights increased at 1 - 2 cm/year. Currently the NAO index—and therefore storminess—is trending downwards. Over the longer term, global warming models predict an average temperature rise of 2 degrees Celsius over the next century. A doubling of CO 2 is expected to increase sea level by 30 cm, while the significant wind speed and wave heights in the North Sea are predicted to increase by 50 cm/s and 50 cm, respectively. Beginning in the late 1950 ’s, dredging activity and construction measures in harbours and shipping channels greatly altered the physical processes in the Ems. Deepening and streamlining the Ems River and shipping channel between the 1960 s and 1990 s decreased the hydraulic roughness and increased the tidal range in the river above Emden by as much as 1. 5 m. At the turbidity maximum between Emden and Papenburg, concentrations of sediment are currently between 1 - 2 orders of magnitude larger than in the 1950 ’s, and fluid mud layers of several meters thickness occur. Other man-made changes, such as gas pipelines and the expansion of harbours, have often caused significant, but more localized, changes to the estuary. Between the mid 19 th century and the 1970 ’s, dumping of organic waste—agricultural, industrial, and human—severely stressed the ecology of the Dollard sub-basin in particular. Since then the input of organic waste has been greatly reduced and anoxic conditions eliminated. However, the increase in turbidity at the turbidity maximum has caused depleted oxygen concentrations and periodic anoxia between Pogum and Papenburg during the summer months (personal communication, H. Juergens; Talke et al, 2005). The Ems is a relatively well studied estuary. Significant research projects have included the BOEDE project in the 1970 ’s [...] 1980 ’s and the BOA and INTRAMUD projects in the 1990 ’s. These projects and other efforts have amassed a deep literature in the knowledge of tidal flats, fluid mud and flocculation, and mixing and dispersion processes. Projects currently underway are focusing on tidal dynamics and the affects of dredging in the high turbidity zone between Emden and Herbrum. Optimal management of the estuary is the goal of the HARBASINS project. Many analytical and numerical models have been applied to the Ems estuary to estimate tidal range, storm surges, wave fields, sediment transport, and mixing and dispersion processes. Analytical models to estimate mixing of scalars and sediment fluxes (Sediment Trend Analysis) have been extensively used. Numerical models such as WAQUA, unTRIM, MIKE 3, Telemac 2 D, SWAN, Delft 3 D –Sed, and others have been applied to the Ems. While reasonable results are found for <b>short</b> <b>term</b> <b>processes</b> (order of days), long-term morphological change cannot yet be predicted. For the Ems catchement basin, the numerical models REGFLUD and FLUMAGIS are used to estimate nutrient inputs from diffuse sources and to visualize and evaluate the effects of land-use change...|$|E
40|$|Electrical energy {{consumption}} forms 99 % {{of the environmental}} impact of machining operations. Whilst replacing existing machineries for more energy efficient ones does not deem possible in <b>short</b> <b>term,</b> <b>process</b> planning for machining with {{energy consumption}} in mind is a more accessible solution. The effect of cutting parameters on power consumption in CNC milling of 6082 T 6 aluminum alloy was investigated in this paper. Mathematical models were developed to estimate the energy and power consumption in CNC milling machines. The analysis indicated that the two less studied parameters of axial and radial depth of cut have significant impact on the total energy consumption of machining processes. Increased axial and radial depth of cut not only increase material removal rate but also increase the portion of machine tool’s power consumption dedicated to material cutting. This study indicated that 82 % reduction in energy consumption can be achieved through precise selection of cutting parameters...|$|R
25|$|Structural {{unemployment}} {{covers a}} variety of possible causes of unemployment including a mismatch between workers' skills and the skills required for open jobs. Large amounts of structural unemployment can occur when an economy is transitioning industries and workers find their previous set of skills {{are no longer in}} demand. Structural unemployment is similar to frictional unemployment since both reflect the problem of matching workers with job vacancies, but structural unemployment covers the time needed to acquire new skills not just the <b>short</b> <b>term</b> search <b>process.</b>|$|R
40|$|We use ordinal {{patterns}} and symbolic analysis to construct global climate networks and uncover long and <b>short</b> <b>term</b> memory <b>processes.</b> The data analyzed is the monthly averaged surface air temperature (SAT field) {{and the results}} suggest that the time variability of the SAT field is determined by patterns of oscillatory behavior that repeat from time to time, with a periodicity related to intraseasonal oscillations and to El Niño on seasonal-to-interannual time scales. Comment: 10 pages, 13 figures Enlarged version, new sections and figures. Accepted in Chao...|$|R
40|$|The {{application}} of artificial intelligence (AI) techniques {{to the design}} of personal learning environments is an enterprise of both theoretical and practical interest. In the <b>short</b> <b>term,</b> the <b>process</b> of developing and testing intelligent tutoring programs serves as a new experimental vehicle for exploring alternative cognitive and pedagogical theories. In the long term, such programs should supplement the educational supervision and guidance provided by human teachers. This paper illustrates our long term perspective by a scenario with a hypothetical tutoring system for elementary graphics programming...|$|R
50|$|Structural {{unemployment}} {{covers a}} variety of possible causes of unemployment including a mismatch between workers' skills and the skills required for open jobs. Large amounts of structural unemployment can occur when an economy is transitioning industries and workers find their previous set of skills {{are no longer in}} demand. Structural unemployment is similar to frictional unemployment since both reflect the problem of matching workers with job vacancies, but structural unemployment covers the time needed to acquire new skills not just the <b>short</b> <b>term</b> search <b>process.</b>|$|R
5000|$|CIW has 120 acre. Its {{facilities}} include Level I ("Open dormitories {{without a}} secure perimeter") housing, Level II ("Open dormitories with secure perimeter fences and armed coverage") housing, and Level III ("Individual cells, fenced perimeters and armed coverage") housing. In addition, a Reception Center [...] "provides <b>short</b> <b>term</b> housing to <b>process,</b> classify and evaluate incoming inmates." ...|$|R
5000|$|Structural {{unemployment}} {{covers a}} variety of possible causes of unemployment including a mismatch between workers' skills and the skills required for open jobs. [...] Large amounts of structural unemployment can occur when an economy is transitioning industries and workers find their previous set of skills {{are no longer in}} demand. Structural unemployment is similar to frictional unemployment as both reflect the problem of matching workers with job vacancies, but structural unemployment also covers the time needed to acquire new skills in addition to the <b>short</b> <b>term</b> search <b>process.</b>|$|R
40|$|This paper {{describes}} the long <b>term</b> and <b>short</b> <b>term</b> planning <b>processes</b> {{of the nine}} major military airframe builders in the United States. The {{first part of the}} paper characterizes the nine firms and explores the risks and rewards inherent in their dependence on their major customer, the United States Government. The remainder of the paper provides a description of the internal planning processes that is based on personal interviews of the planning executives of the nine firms. The environmental forecast, which precedes the formulation of the long <b>term</b> and <b>short</b> <b>term</b> plans, and the corporate review process, in which actual performance is measured against the <b>short</b> <b>term</b> plan, are also discussed. supported by the Foundation Research Program of the Naval Postgraduate School with funds provided by the Chief of Naval Research. [URL]...|$|R
40|$|While the {{recommendations}} for remediation of systemic quality issues {{made by the}} Confidential Inquiry into Premature Deaths of People with Learning Disabilities have great value and should be acted upon in the <b>short</b> <b>term,</b> the <b>process</b> of inquiry into the deaths of people with learning disabilities in the U. K. {{should not be considered}} complete. Implementation of these recommendations is only the first step for establishing a more robust system of surveillance and timely intervention that can guide initiatives for the prevention of avoidable deaths and improvement in quality of life for people with learning disabilities...|$|R
40|$|The {{main goal}} of this work is to {{demonstrate}} the feasibility and potential of recovering motion from a sequence of range images as an alternate solution to the complex motion problem. The work presented in this thesis {{can be divided into}} two separate parts. The first part describes the long <b>term</b> <b>process,</b> and the second part discusses the <b>short</b> <b>term</b> <b>process.</b> The major problem for the long <b>term</b> <b>process</b> is to reliably find matching features in two or more successive images. An approach is proposed to establish the best match of point features between successive frames using a Hopfield neural network. A model is developed to convert the correspondence problem to the problem of minimizing an energy function, which occurs at the stable state of a Hopfield neural network. After establishing the feature matching, a δ-bound matching concept is introduced to detect the reliable matching features, therefore increase the accuracy of the estimated motion parameters by removing the effect of mismatching features. In this way, the algorithm is tolerant to noise due to feature detection or occlusion. For the <b>short</b> <b>term</b> <b>process,</b> the case of a single rigid moving object is first studied. A simple, yet powerful, algorithm is proposed to estimate motion of a single rigid object. The motion problem is modeled as solving a set of linear equations. A weighted least squares technique has been found to provide the best performance among several other versions of least squares techniques. Theoretical analysis on the necessary and sufficients conditions for the unique interpretation of the motion parameters and on the sensitivity of the estimated motion parameters to noise provides further insight into the behavior of the algorithm. For more complicated motion such as nonrigid motion, the complete process can be viewed in two separate levels: low and high. In this thesis, {{attention has been paid to}} the low level processing. A 3 D velocity field has been chosen to be the output of the low level stage. We first develop an algorithm which uniquely estimates 3 D velocities of points on smooth surfaces by its first and second order partial derivatives, except at parabolic points. The algorithm is very fast and easy to implement in hardware or software. However, it does not provide reliable estimates of velocities near edge points. Hence we propose another algorithm, which is based on the correlation of the local structure of principal curvatures. The advantage of this correlation approach is that it can estimate velocities of both corner points as well as points on smooth curved surfaces, and vernier velocities of line edge points. The disadvantage is that it is computationally intensive compared with the approach for smooth surfaces. Therefore, we suggest that two algorithms should be combined together to give the best performance. Many experimental results on both synthetic and real images are presented in this thesis...|$|R
40|$|Abstract: In this paper, {{we provide}} pricing {{formulae}} for both European and American yield {{options in the}} generalized Cox-Ingersoll-Ross (1985) single-factor term structure model with time-dependent parameters. Our results are established by forward-neutral pricing. The law of the generalized CIR <b>short</b> <b>term</b> interest rate <b>process</b> under the forward-neutral probability is obtained by using results on {{the relation between the}} generalized square-root process and squared Bessel processes with time-varying dimension...|$|R
40|$|The cheap {{nickel oxide}} (NiO) is a {{potential}} catalyst candidate to replace the expensive available platinum group metals (PGM). However, the current methods to adhere the NiO powder on the metallic substrates are complicated. Therefore, this work explored the development of nickel oxide using nickel (Ni) on FeCrAl substrate through the combination of nickel electroplating and oxidation process for catalytic converter application. The approach was started with assessment of various nickel electroplating process based on the weight gain during oxidation. Then, the next experiment used the best {{process in which the}} pre-treatment using the solution of SiC and/or Al 2 O 3 in methanol. The specimens then were carried out to <b>short</b> <b>term</b> oxidation <b>process</b> using thermo gravimetric analysis (TGA) at 1000 oC. Meanwhile, the long <b>term</b> oxidation <b>process</b> was conducted using an automatic furnace at 900, 1000 and 1100 oC. The atomic force microscopy (AFM) was used for surface analysis in nanometer range scale. Meanwhile, roughness test was used for roughness measurement analysis in micrometer range scale. The scanning electron microscope (SEM) attached with energy dispersive X-ray (EDX) were used for surface and cross section morphology analysis. The specimen of FeCrAl treated using ultrasonic prior to nickel electroplating showed the lowest weight gain during oxidation. The surface area of specimens increased after ultrasonic treatment. The electroplating process improved the high temperature oxidation resistance. In <b>short</b> <b>term</b> oxidation <b>process</b> indicated that the ultrasonic with SiC provided the lower parabolic rate constant (kp) and the Al 2 O 3 and NiO layers were also occurred. The Ni layer was totally disappeared and converted to NiO layer on FeCrAl surface after long <b>term</b> oxidation <b>process.</b> From this work, the ultrasonic treatment prior to nickel electroplating was the best method to adhere NiO on FeCrAl substrate...|$|R
40|$|Estuarine shorelines {{are often}} {{classified}} as low energy coasts {{and thus are}} expected to undergo little variation. Port Stephens (SE Australia) is a ria-like microtidal estuary (Roy et al., 2001) located 230 km north from Sydney, on a wave dominated coast (Figure 1). It has an E-W orientation and connects to the ocean through a 1. 24 km wide entrance that allows the dominant SE waves to propagate into the estuary. The outer port is tide dominated and has a large and shallow flood-tide delta which is affected by waves and provides some protection to the estuarine shorelines. The northern shoreline of outer Port Stephens is a continuous stretch of sand (over 4 km) that comprises a barrier (Yacaaba) undergoing accretion, a narrow beach (Jimmy’s beach) undergoing erosion (greater than 1 m/yr), and a spit (Winda Woppa) which has narrowed in width and extended its length more than 500 m in the last 50 years. Previous studies in the area indicate contradictory longshore transport with geomorphologic studies stating westwards transport while numerical models show eastwards transport. The present study establishes longshore current directions and intensities under different conditions, through field based process studies, and links <b>short</b> <b>term</b> <b>process</b> to the decadal evolution obtained with GIS analyses...|$|R
2500|$|As of October 30, 2013 {{the prison}} had a design {{capacity}} of 3,082 but a total institution population of 4,223, for an occupancy rate of 137 percent. [...] It has Level I ("Open dormitories without a secure perimeter") housing; Level II ("Open dormitories with secure perimeter fences and armed coverage") housing; a Reception Center (RC) which [...] "provides <b>short</b> <b>term</b> housing to <b>process,</b> classify and evaluate incoming inmates"; and a Condemned unit.|$|R
40|$|Here it is {{reported}} that the free recall search process increases the error rate for <b>short</b> <b>term</b> memory (about 1 % per second in data from Murdock & Okada (1970)) {{but not for long}} term memory (in data from McDermott (1996)). If the <b>short</b> <b>term</b> memory search <b>process</b> introduces random excitations, which would account for the search errors, the subjects should be unaware of making such errors. This is in agreement with DRM findings (Gallo, 2010) and the new finding that the error terminated distributions in Murdock (1962) are the same as those terminated by studied items...|$|R
30|$|As {{stated in}} {{principle}} 1, {{the most important}} principle of mobile software development is delivering valuable software to users continuously in the <b>short</b> <b>term.</b> Flexible <b>processes</b> and willing minds are needed to accept dynamically changing requirements from various users. Mistakes and misunderstandings have to be reduced through frequent communication and specifications of important changes and feedback. During these steps, developers should pay attention to technical excellence and preserve usability {{in order to make}} the product valuable to users. Moreover, important change-centered documentation is pursued instead of heavy documentation. The ultimate purpose of these principles of mobile software development is to produce a human-centered application valuable to real users through flexible processes and active communications.|$|R
40|$|ABSTRACT,The {{application}} of artificial intelligence (AI) techniques {{to the design}} of personal learning environments is an enterprise of both theoretical and practical interest. In the <b>short</b> <b>term,</b> the <b>process</b> of developing and testing intelligent tutoring programs serves is a new experimental vehicle for exp. oring alternative cognitive and pedagogical theories. In the long term, such programs should supplement the edo-ational supervision and guidance provided by human teachers. Th-s paper illustrates the long term perspective by a scenario with Sherlock, a hypothetical LOGO tutoring system for elementary graphics programming which was in a preliminary design stage at the time this paper was written. Twenty-three references are listed. (Author/LLS) * Reproductions supplied by EDRS are the best that can be made...|$|R
