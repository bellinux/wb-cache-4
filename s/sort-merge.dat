108|0|Public
50|$|Three {{fundamental}} algorithms {{for performing}} a join operation exist: nested loop join, <b>sort-merge</b> join and hash join.|$|E
50|$|The <b>sort-merge</b> join (also {{known as}} merge join) is a join {{algorithm}} {{and is used}} in the implementation of a relational database management system.|$|E
5000|$|Also in 1997, she {{received}} the IEEE Computer Pioneer Award from the IEEE Computer Society {{for developing the}} <b>sort-merge</b> generator which, according to IEEE, [...] "inspired the first ideas about compilation." ...|$|E
50|$|The {{basic problem}} of a join {{algorithm}} is to find, for each distinct value of the join attribute, the set of tuples in each relation which display that value. The key idea of the <b>sort-merge</b> algorithm is to first sort the relations by the join attribute, so that interleaved linear scans will encounter these sets at the same time.|$|E
5000|$|A parameter-driven <b>sort-merge</b> program, {{capable of}} {{handling}} very large volumes of data. Sort parameters could either be read in from the paper-tape reader, for one-off sorts, or [...] "compiled" [...] in (really just stored in the program). There were extensive user [...] "hooks" [...] where user-supplied code could be put in at various stages of the sort/merge process.|$|E
5000|$|Let's {{say that}} we have two {{relations}} [...] and [...] and [...] [...] fits in [...] pages memory and [...] fits in [...] pages memory. So, in the worst case <b>sort-merge</b> join will run in [...] I/Os. In the case that [...] and [...] are not ordered the worst case time cost will contain additional terms of sorting time: , which equals [...] (as linearithmic terms outweigh the linear terms, see Big O notation - Orders of common functions).|$|E
50|$|The {{result of}} the join {{can be defined as}} the outcome of first taking the Cartesian product (or Cross join) of all rows in the tables (combining every row in table A with every row in table B) and then {{returning}} all rows which satisfy the join predicate. Actual SQL implementations normally use other approaches, such as hash joins or <b>sort-merge</b> joins, since computing the Cartesian product is slower and would often require a prohibitively large amount of memory to store.|$|E
50|$|In practice, {{the most}} {{expensive}} part of performing a <b>sort-merge</b> join is arranging for both inputs to the algorithm to be presented in sorted order. This can be achieved via an explicit sort operation (often an external sort), or {{by taking advantage of}} a pre-existing ordering in {{one or both of the}} join relations. The latter condition can occur because an input to the join might be produced by an index scan of a tree-based index, another merge join, or some other plan operator that happens to produce output sorted on an appropriate key.|$|E
50|$|External sorting is a {{class of}} sorting {{algorithms}} that can handle massive amounts of data. External sorting is required when the data being sorted do not fit into the main memory of a computing device (usually RAM) and instead they must reside in the slower external memory, usually a hard disk drive. External sorting typically uses a hybrid <b>sort-merge</b> strategy. In the sorting phase, chunks of data {{small enough to fit}} in main memory are read, sorted, and written out to a temporary file. In the merge phase, the sorted subfiles are combined into a single larger file.|$|E
50|$|Since {{a logical}} {{operator}} is only {{responsible for the}} semantics of an operation but does not consist of any algorithms, the logical query plan must be transformed into an executable counterpart. This is called a physical query plan. The distinction between a logical and a physical operator plan allows more than one implementation for the same logical operator. The join, for example, is logically the same, although it can be implemented by different algorithms like a Nested loop join or a <b>Sort-merge</b> join. Notice, these algorithms also strongly depend on the used stream and processing model.Finally, the query is available as a physical query plan.|$|E
40|$|Skew Handling Techniques in <b>Sort-Merge</b> Join Joins {{are among}} the most {{frequently}} executed operations. Sev-eral fast join algorithms have been developed and extensively studied; these can be categorized as <b>sort-merge,</b> hash-based, and index-based algorithms. While all three types of algo-rithms exhibit excellent performance over most data, amelio-rating the performance degradation in the presence of skew has been investigated only for hash-based algorithms. How-ever, for <b>sort-merge</b> join, even a small amount of skew present in realistic data can result in a significant performance hit on a commercial DBMS. This paper examines the negative ramifi-cations of skew in <b>sort-merge</b> join and proposes several refine-ments that deal effectively with data skew. Experiments show that some of these algorithms also impose virtually no penalty in the absence of data skew and are thus suitable for replacing existing <b>sort-merge</b> implementations. We also show how <b>sort-merge</b> band join performance is significantly enhanced with these refinements. 1...|$|E
40|$|In {{this paper}} we {{experimentally}} study {{the performance of}} main-memory, parallel, multi-core join algorithms, focusing on <b>sort-merge</b> and (radix-) hash join. The relative performance of these two join approaches have been a topic of discussion for a long time. With the advent of modern multicore architectures, {{it has been argued}} that <b>sort-merge</b> join is now a better choice than radix-hash join. This claim is justified based on the width of SIMD instructions (<b>sort-merge</b> outperforms radix-hash join once SIMD is sufficiently wide), and NUMA awareness (<b>sort-merge</b> is superior to hash join in NUMA architectures). We conduct extensive experiments on the original and optimized versions of these algorithms. The experiments show that, contrary to these claims, radixhash join is still clearly superior, and <b>sort-merge</b> approaches to performance of radix only when very large amounts of data are involved. The paper also provides the fastest implementations of these algorithms, and covers many aspects of modern hardware architectures relevant not only for joins but for any parallel data processing operator. 1...|$|E
40|$|Joins {{are among}} the most {{frequently}} executed operations. Several fast join algorithms have been developed and extensively studied; these can be categorized as <b>sort-merge,</b> hash-based, and index-based algorithms. While all three types of algorithms exhibit excellent performance over most data, ameliorating the performance degradation in the presence of skew has been investigated only for hash-based algorithms. However, for <b>sort-merge</b> join, even a small amount of skew present in realistic data can result in a significant performance hit on a commercial DBMS. This paper examines the negative ramifications of skew in <b>sort-merge</b> join and proposes several refinements that deal effectively with data skew. Experiments show that some of these algorithms also impose virtually no penalty in the absence of data skew and are thus suitable for replacing existing <b>sort-merge</b> implementations. We also show how sortmerge band join performance is significantly enhanced with these refinements...|$|E
40|$|This thesis {{studies the}} utility of parallelizing <b>sort-merge</b> join in a shared-memory {{processing}} environment. The investigated algorithm partitions {{the work of the}} sortmerge equi-join by dividing the domain of the join attribute into a set of nonoverlapping ranges. Two distinct ways of sorting the relations in parallel for the <b>sort-merge</b> join are investigated: one, based on a concurrent heap used for replacement selection, and a second, using in-memory sorting. The investigation includes both theoretical analysis and implementation results, with direct comparability to previous work on parallel hash- and nested loops joins. Both processor- and I/O-bound environments are considered. It is concluded that parallel <b>sort-merge</b> join is inferior to hash-based join algorithms unless the joining relations are already sorted. Parallel run forming in a shared-memory environment, and the parallel sorting of arrays with large elements are also discussed. ii Acknowledgements Thanks are in order to t [...] ...|$|E
40|$|In {{databases}} {{with time}} interval attributes, query processing techniques {{that are based}} on <b>sort-merge</b> or sort-aggregate deteriorate. This happens because for intervals no total order exists and either the start or end point is used for the sorting. Doing so leads to inefficient solutions with lots of unproductive comparisons that do not produce an output tuple. Even if just one tuple with a long interval is present in the data, the number of unproductive comparisons of <b>sort-merge</b> and sort-aggregate gets quadratic. In this paper we propose disjoint interval partitioning (DIP), a technique to efficiently perform sort-based operators on interval data. DIP divides an input relation into the minimum number of partitions, such that all tuples in a partition are non-overlapping. The absence of overlapping tuples guarantees efficient <b>sort-merge</b> computations without backtracking. With DIP the number of unproductive comparisons is linear in the number of partitions. In contrast to current solutions with inefficient random accesses to the active tuples, DIP fetches the tuples in a partition sequentially. We illustrate the generality and efficiency of DIP by describing and evaluating three basic database operators over interval data: join, anti-join and aggregation...|$|E
40|$|Join is an {{important}} database operation. As computer architectures evolve, the best join algorithm may change hand. This paper re-examines two popular join algorithms – hash join and <b>sort-merge</b> join – {{to determine if the}} latest computer architecture trends shift the tide that has favored hash join for many years. For a fair com-parison, we implemented the most optimized parallel version of both algorithms on the latest Intel Core i 7 platform. Both imple-mentations scale well with the number of cores in the system and take advantages of latest processor features for performance. Our hash-based implementation achieves more than 100 M tuples per second which is 17 X faster than the best reported performance on CPUs and 8 X faster than that reported for GPUs. Moreover, the performance of our hash join implementation is consistent over a wide range of input data sizes from 64 K to 128 M tuples and is not affected by data skew. We compare this implementation to our highly optimized sort-based implementation that achieves 47 M to 80 M tuples per second. We developed analytical models to study how both algorithms would scale with upcoming processor architecture trends. Our analysis projects that current architectural trends of wider SIMD, more cores, and smaller memory bandwidth per core imply better scalability potential for <b>sort-merge</b> join. Con-sequently, <b>sort-merge</b> join is likely to outperform hash join on up-coming chip multiprocessors. In summary, we offer multicore im-plementations of hash join and <b>sort-merge</b> join which consistently outperform all previously reported results. We further conclude that the tide that favors the hash join algorithm has not changed yet, but the change is just around the corner. 1...|$|E
40|$|ABSTRACT- In {{this paper}} we analyze and compare four {{parallel}} join algorithms. Grace and Hybrid hash represent {{the class of}} hash-based join methods, Simple hash represents a loop ing algorithm with hashing, and our last algorithm is the more traditional <b>sort-merge.</b> The performance {{of each of the}} algo-rithms with different tuple distribution policies, the addition of bit vector filters, varying amounts of main-memory for joining, and non-uniformly distributed join attribute values is studied. The Hybrid hash-join algorithm is found to be superior except when the join attribute values of the inner relation are non-uniformly distributed and memory is limited. In this case, a more conserva-tive algorithm such as the <b>sort-merge</b> algorithm should be used. The Gamma database machine serves as the host for the perfor-mance comparison. 1...|$|E
40|$|Support for {{exploratory}} {{interaction with}} databases in ap-plications such as data mining {{requires that the}} first few results of an operation be available as quickly as possible. We study the algorithmic side of what can and what can-not be achieved for processing join operations. We develop strategies that modify the strict two-phase processing of the <b>sort-merge</b> paradigm, intermingling join steps with selected merge phases of the sort. We propose an algorithm that pro-duces early join results for a broad class of join problems, in-cluding many not addressed well by hash-based algorithms. Our algorithm has no {{significant increase in the}} number of I/O operations needed to complete the join compared to standard <b>sort-merge</b> algorithms. Categories and Subject Descriptors H. 3. 3 [Information Storage and Retrieval]: Informa...|$|E
40|$|This {{thesis is}} {{concerned}} with the development of a unique parallel <b>sort-merge</b> system suitable for implementation in VLSI. Two new sorting subsystems, a high performance VLSI sorter and a four-way merger, were also realized during the development process. In addition, the analysis of several existing parallel sorting architectures and algorithms was carried out. Algorithmic time complexity, VLSI processor performance, and chip area requirements for the existing sorting systems were evaluated. The rebound sorting algorithm was determined to be the most efficient among those considered. The rebound sorter algorithm was implemented in hardware as a systolic array with external expansion capability. The second phase of the research involved analyzing several parallel merge algorithms and their buffer management schemes. The dominant considerations for this phase of the research were the achievement of minimum VLSI chip area, design complexity, and logic delay. It was determined that the proposed merger architecture could be implemented in several ways. Selecting the appropriate microarchitecture for the merger, given the constraints of chip area and performance, was the major problem. The tradeoffs associated with this process are outlined. Finally, a pipelined <b>sort-merge</b> system was implemented in VLSI by combining a rebound sorter and a four-way merger on a single chip. The final chip size was 416 mils by 432 mils. Two micron CMOS technology was utilized in this chip realization. An overall throughput rate of 10 M bytes/sec was achieved. The prototype system developed is capable of sorting thirty two 2 -byte keys during each merge phase. If extended, this system is capable of economically sorting files of 100 M bytes or more in size. In order to sort larger files, this design should be incorporated in a disk-based <b>sort-merge</b> system. A simplified disk I/O access model for such a system was studied. In this study the <b>sort-merge</b> system was assumed {{to be part of a}} disk controller subsystem...|$|E
40|$|Two {{emerging}} hardware trends {{will dominate}} the database system {{technology in the}} near future: increasing main memory capacities of several TB per server and massively parallel multi-core processing. Many algorithmic and control techniques in current database technology were devised for disk-based systems where I/O dominated the performance. In this work we take {{a new look at}} the well-known <b>sort-merge</b> join which, so far, has not been in the focus of research in scalable massively parallel multi-core data processing as it was deemed inferior to hash joins. We devise a suite of new massively parallel <b>sort-merge</b> (MPSM) join algorithms that are based on partial partition-based sorting. Contrary to classical <b>sort-merge</b> joins, our MPSM algorithms do not rely on a hard to parallelize final merge step to create one complete sort order. Rather they work on the independently created runs in parallel. This way our MPSM algorithms are NUMA-affine as all the sorting is carried out on local memory partitions. An extensive experimental evaluation on a modern 32 -core machine with one TB of main memory proves the competitive performance of MPSM on large main memory databases with billions of objects. It scales (almost) linearly in the number of employed cores and clearly outperforms competing hash join proposals - in particular it outperforms the "cutting-edge" Vectorwise parallel query engine by a factor of four. Comment: VLDB 201...|$|E
40|$|Applying {{existing}} {{feature selection}} algorithms to video classification is impractical. A novel algorithm called Basic <b>Sort-Merge</b> Tree (BSMT) is proposed {{to choose a}} very small subset of features for video classification in linear time {{in the number of}} features. We reduce the cardinality of the input data by sorting the individual features by their effectiveness in categorization, and then merging pairwise these features into feature sets of cardinality two. Repeating this <b>Sort-Merge</b> process several times results in the learning of a small-cardinality, efficient, but highly accurate feature set. As the wrapper model, this paper exploits a novel combination of Fastmap for dimensionality reduction and Mahalanobis distance for likelihood determination. The time complexity of this induction part is linear in the number of training data. We provide theoretical proof of time cost and empirical validation of the accuracy. 1...|$|E
40|$|Graduation date: 1989 This {{thesis is}} {{concerned}} with the development of a unique parallel <b>sort-merge</b> system suitable for implementation in VLSI. Two new sorting subsystems, a high performance VLSI sorter and a four-way merger, were also realized during the development process. In addition, the analysis of several existing parallel sorting architectures and algorithms was carried out. Algorithmic time complexity, VLSI processor performance, and chip area requirements for the existing sorting systems were evaluated. The rebound sorting algorithm was determined to be the most efficient among those considered. The rebound sorter algorithm was implemented in hardware as a systolic array with external expansion capability. The second phase of the research involved analyzing several parallel merge algorithms and their buffer management schemes. The dominant considerations for this phase of the research were the achievement of minimum VLSI chip area, design complexity, and logic delay. It was determined that the proposed merger architecture could be implemented in several ways. Selecting the appropriate microarchitecture for the merger, given the constraints of chip area and performance, was the major problem. The tradeoffs associated with this process are outlined. Finally, a pipelined <b>sort-merge</b> system was implemented in VLSI by combining a rebound sorter and a four-way merger on a single chip. The final chip size was 416 mils by 432 mils. Two micron CMOS technology was utilized in this chip realization. An overall throughput rate of 10 M bytes/sec was achieved. The prototype system developed is capable of sorting thirty two 2 -byte keys during each merge phase. If extended, this system is capable of economically sorting files of 100 M bytes or more in size. In order to sort larger files, this design should be incorporated in a disk-based <b>sort-merge</b> system. A simplified disk I/O access model for such a system was studied. In this study the <b>sort-merge</b> system was assumed {{to be part of a}} disk controller subsystem...|$|E
30|$|The work {{introduced}} in [28] proposes a new data analysis platform to support incremental one-pass analytics. It replaces the <b>sort-merge</b> implementation in MapReduce with a purely hash-based framework that enables fast in-memory processing of the reduce function. In addition, it employs a new frequent key based technique to extend in-memory processing to workloads {{that require a}} large key-state space.|$|E
40|$|In this thesis, we {{consider}} the complexity of computing the optimal join order sequences for star queries and general queries. We consider the following join methods in our thesis - indexed nested loop joins, <b>sort-merge</b> joins, hash joins and block nested loop joins. The use of cartesian products is avoided and only linear trees are considered for query execution...|$|E
40|$|High time {{complexity}} is a bottle-neck {{in video}} segmentation, classification, analysis, and retrieval. In this paper {{we use a}} heuristic method called Fastconverging <b>Sort-Merge</b> Tree (FSMT) to construct automatically a hierarchy of small subsets of features that are progressively more useful for video data exploration. The method combines the virtues of a wrapper model approach for high accuracy, with those of a filter method approach for deriving the appropriate features quickly. FSMT speeds up a more fundamental method, the Basic <b>Sort-Merge</b> Tree (BSMT) approach, while retaining its performance. We demonstrate FSMT's high accuracy: it has a 0. 001 error rate in a frame classification task on 75 minutes of instructional video, and a 0. 98 precision and 0. 89 recall in a segment retrieval task on 30 minutes of sports video. Additionally, FSMT is more than 80 % faster than its predecessor, BSMT. 1...|$|E
40|$|The join {{operator}} {{has been}} a cornerstone of relational database systems since their inception. As such, much time and effort has gone into making joins efficient. With the obvious trend towards multiprocessors, attention has focused on efficiently parallelizing the join operation. In this paper we analyze and compare four parallel join algorithms. Grace and Hybrid hash represent the class of hash-based join methods, Simple hash represents a looping algorithm with hashing, and our last algorithm is the more traditional <b>sort-merge.</b> The Gamma database machine serves as the host for the performance comparison. Gamma's shared-nothing architecture with commercially available components is becoming increasingly common, both in research and in industry. 1. Introduction During the last 10 years, {{a significant amount of}} effort has been focused on developing efficient join algorithms. Initially, nested loops and <b>sort-merge</b> were the algorithms of choice. However, work by [KITS 83, BRAT 84, DEWI 84 [...] ...|$|E
40|$|Sorting {{is one of}} {{the most}} {{important}} operations in database systems and its efficiency can influences drastically the overall system performance. To speed up the performance of database system, parallelism is applied to the execution of the data administration operations. Conventional parallel hardware architectures, which employ a highly parallel software architecture are used in current parallel database systems. Thereby a common technique is to decluster the data sets among a number of parallel and independent disk drives. In this paper we revisit parallel sorting algorithms in the context of declustered data of parallel database systems. We adapt the well known and well studied parallel <b>sort-merge</b> and bitonic-sort algorithms for declustered data and compare analytically their performance. We show that the adapted bitonic sort outperforms the adapted <b>sort-merge</b> algorithm for declustered data in resemblance to the results of their conventional counterparts. Keyword Parallel Database [...] ...|$|E
40|$|An {{external}} duplicate deletion {{algorithm is}} here developed, which makes an {{extensive use of}} hashing. With the current large main memories, a two-phase version of the algorithm is sufficient in most practical situations. The first phase deletes part of the duplicates at once, and divides {{the rest of the}} elements into mutually disjoint subfiles. These are then processed separately in the second phase. Experiments show that the new algorithm performs about the same number of disk I/O's as the traditional <b>sort-merge</b> technique, but the number of comparisons is considerably smaller. The CPU time is only about half of the <b>sort-merge</b> time in most cases. 1 Introduction Deletion of duplicates (here DD for short) from a multiset is one of the fundamental operations in database applications. The relations of a relational database are mathematical sets, by definition, and thus free of duplicates (FOD for short). Although many relational DBMSs are not too strict about this issue, an early DD may be [...] ...|$|E
40|$|Discovery of {{association}} rules {{is an important}} problem in Data Mining. The classical approach is to generate all itemsets that have support (i. e., the fraction of transactions containing the itemset) above a user given threshold. Most existing algorithms aim at {{reducing the number of}} scans over the transaction database, i. e., the I/O overhead. We consider {{the problem of how to}} calculate efficiently the support, i. e., we try to optimize both I/O and CPU time. A straightforward way is to maintain, for each itemset, the relevant transaction identifiers directly into a list and use a <b>sort-merge</b> algorithm to do the intersection of two itemsets. Instead, we propose bitmap based algorithms. The basic idea is that every couple is represented by a bit in an index bitmap, and the logical operation AND is used in place of the <b>sort-merge</b> algorithm. We propose two variations of the bitmap based algorithm : the naïve bitmap algorithm (N-BM) and the hierarchical bitmap algorith [...] ...|$|E
40|$|This study {{introduces}} Overlap-Join {{which is}} non-equi self join that joins a table to {{itself with a}} non-equal condition for joining. Overlap-Join arises in real word queries that deal with time. Time scheduling and time tabling applications are clear examples for time overlapping, this {{in addition to its}} usage in temporal databases. JOIN is the most expensive operation in relational databases. For this reason an efficient algorithm is needed. Overlap-Join and two parameters for Overlapping; Overlap Coefficient (OC) and Span Coefficient (SC) have been defined. Three properties for overlapping has been developed and discussed. Two algorithms have been proposed. These algorithms are modified versions of two known join algorithms; the block nested-loop join and the <b>Sort-merge</b> join. Models for joining costs have been presented and analyzed. The modifications take advantage of the fact that overlap-Join is self-join and the sc concept. The study shows that performance of <b>sort-merge</b> join is not better than the performance of block nested loop join for Overlap-Join when the SC is high...|$|E
40|$|We {{explore the}} problem of rapid {{automatic}} semantic tagging of video frames of unstructured (unedited) videos. We apply the <b>Sort-Merge</b> algorithm for feature selection on a large (> 1000) heterogeneous feature set for videos showing lectures, to quickly locate low-level image features most predictive for concepts such as “key frame with text ” or “key frame with computer source code”. For evaluation, we introduce a “keeper ” heuristic for feature retention, which provides a baseline comparison. We then compare early fusion and late fusion of diverse feature types; based on experiments on 12, 395 frames, we find that in general late fusion offers higher Average Precision accuracy at lower computation cost, compared to early fusion. However, mergers of redundant feature types do not necessarily improve performance over single feature types; exploration of both merged and unmerged performance is necessary. Index Terms — unstructured video analysis, feature selection, semantic tags, SVM, <b>Sort-Merge</b> 1...|$|E
40|$|Evaluating the {{relational}} join {{is one of}} the central algorithmic and most well-studied problems in database systems. A staggering number of variants have been considered including Block-Nested loop join, Hash-Join, Grace, <b>Sort-merge</b> (see Grafe [20] for a survey, and [5, 8, 27] for discussions of more modern issues). Commercial database engines use finely tuned join heuristics that take into account a wide variety of factor...|$|E
40|$|In this thesis, we {{consider}} the problem of computing optimal join order sequence for star queries, a subclass of tree queries. A join is allowed to be computed using both nested-loop and <b>sort-merge</b> join methods {{and the use of}} Cartesian products is avoided. This thesis extends the scope of the problem studied by Ibaraki and Kameda [IK 84] and by Krishnamurthy, Boral and Zaniolo [KBZ 86]...|$|E
40|$|Inequality joins, which join {{relational}} {{tables on}} inequality conditions, {{are used in}} various applications. While {{there have been a}} wide range of optimization methods for joins in database systems, from algorithms such as <b>sort-merge</b> join and band join, to various indices such as B+-tree, R*-tree and Bitmap, inequality joins have received little attention and queries containing such joins are usually very slow. In this paper, we introduce fast inequality join algorithms. We put columns to be joined in sorted arrays and we use permutation arrays to encode positions of tuples in one sorted array w. r. t. the other sorted array. In contrast to <b>sort-merge</b> join, we use space efficient bit-arrays that enable optimizations, such as Bloom filter indices, for fast computation of the join results. We have implemented a centralized version of these algorithms on top of PostgreSQL, and a distributed version on top of Spark SQL. We have compared against well known optimization techniques for inequality joins and show that our solution is more scalable and several orders of magnitude faster...|$|E
40|$|Abstract. One of the {{differences}} between relational and object-oriented databases (OODB) is that attributes in OODB can of a collection type (e. g. sets, lists, arrays, bags) as well as a simple type (e. g. integer, string). Consequently, explicit join queries in OODB may be based on collection attributes. One form of collection join queries in OODB is collection-intersect join queries, where the joins are based on collection attributes and the queries check for whether there is an intersection between the two join collection attributes We propose two algorithms for parallel pro-cessing of collection-intersect join queries. The first one is based on <b>sort-merge,</b> and the second is based on hash. We also present two data par-titioning methods (i. e. simple replication and ”divide and partial broad-cast”) used in conjunction with the parallel collection-intersect join al-gorithms. The parallel <b>sort-merge</b> algorithm can only make use of the divide and partial broadcast data partitioning, whereas the parallel hash algorithm may have a choice which of the two data partitioning to use. ...|$|E
40|$|The {{research}} community has considered hash-based parallel join algorithms the algorithms {{of choice for}} almost a decade. However, almost none of the commercial parallel database systems use hashing-based join algorithms, using instead nested-loops with index or <b>sort-merge.</b> While the research literature abounds with comparisons between the various hash-based and <b>sort-merge</b> join algorithms, to our knowledge there is no published comparison between the parallel hash-based algorithms and a parallel nested loops algorithm with index. In this paper we present a comparison of four variants of parallel index nested loops algorithms with the parallel hybrid hash algorithm. The conclusions of our experiments both with an analytic model and with an implementation in the Gamma parallel database system are that (1) overall, parallel hybrid hash is the method of choice, but (2) there are cases where nested-loops with index wins big enough that systems could pro t from implementing both algorithms. Furthermore, our experiments show that among the nested loop algorithms, one of them, subset nested loops with sorting, clearly dominates...|$|E
