14|9000|Public
40|$|Concepts {{basic to}} the spatial {{analysis}} of forest inventory data are introduced, with {{special attention to}} autocorrelation and the estimation of population totals and means. In most cases, the estimates based on spatial analysis procedures {{are not expected to}} equal those resulting from traditional <b>statistical</b> <b>estimation</b> <b>procedures.</b> Initial findings of a comparative study are briefly discussed...|$|E
40|$|Various schemes of {{prevention}} measures {{in public health}} are developed and ana- lyzed {{on the basis of}} a general mathematical model. Features related to cost issues, including primary and secondary prevention interventions, differential survival ex- periences and communicable diseases are in turn used to show the potentialities of the theoretical framework. <b>Statistical</b> <b>estimation</b> <b>procedures</b> are briefly discussed and a numerical application is presented with reference to Italian cancer data...|$|E
40|$|We study {{statistical}} risk minimization problems under a privacy {{model in}} which the data is kept confidential even from the learner. In this local privacy framework, we establish sharp {{upper and lower bounds}} on the convergence rates of <b>statistical</b> <b>estimation</b> <b>procedures.</b> As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, as measured by convergence rate, of any statistical estimator or learning procedure. Comment: 60 page...|$|E
40|$|This paper {{addresses}} {{the problem of}} modeling the relationships between observed samples of data as a distribution. An L-step dependent model is constructed and an online algorithm is designed for the model {{in order to minimize}} the Kullback measure. The algorithm is analyzed to show that it converges weakly to global optimum of Kullback measure for that model. Simulation studies indicate that the algorithm has better tracking properties for time varying distributions, when compared with <b>statistical</b> <b>estimation</b> <b>procedure...</b>|$|R
40|$|In <b>statistical</b> <b>estimation</b> <b>procedure</b> prior {{information}} regarding the unknown value of parameter is utilizing and it {{may result in a}} decrease of sampling variability of the estimator or it may save sample size which is desirable in many <b>estimation</b> <b>procedures.</b> The commonly used approaches in statistical inference which utilize {{prior information}} are Bayesian approach, preliminary test <b>procedure</b> and shrinkage <b>estimation.</b> The paper proposes preliminary test estimator and shrinkage preliminary test estimator for the variance in normal distribution and studies its property under Linex loss function. The paper also proposes and suggests shrinkage preliminary test estimator for the variance in negative exponential distribution and studies its property under Linex loss function. Key word...|$|R
40|$|By {{considering}} the "kinetic-energy" {{term of the}} minimum principle for the Schrödinger equation {{as a measure of}} information, that minimum principle is viewed as a <b>statistical</b> <b>estimation</b> <b>procedure,</b> analogous to the manner in which Jaynes (Phys. Rev., 106, 620, 1957) interpreted statistical mechanics. It is shown that the entropy formula of Boltzmann and Jaynes obey a property in common with the quantum-mechanical kinetic energy, in which both quantities are interpreted as measures of correlation. It is shown that this property is shared by the key terms in the minimum principles of relativistic quantum mechanics and General Relativity. It is shown how this principle may be extended to non-Riemannian nonEuclidean spaces, which leads to novel field equations for the torsion...|$|R
40|$|The {{purpose of}} this paper is to unite two current {{approaches}} to modeling dispersed spatial-interaction behavior: the entropy-smoothing approach, and the cost-efficiency approach. The main result of the paper is to show that those interaction flows determined by entropy-smoothing techniques correspond (for large flows) to the most probable flow patterns consistent with cost-efficient spatial-interaction behavior. In addition, it is shown that under very general conditions, these flow patterns are indeed overwhelmingly most probable. Thus, these results establish a clear behavioral foundation for entropy-smoothing techniques in terms of the cost-efficiency theory. Finally, a number of <b>statistical</b> <b>estimation</b> <b>procedures</b> are developed for operationalizing this theory. ...|$|E
40|$|We {{consider}} {{a family of}} Markov chains whose transition dynamics are affected by model parameters. Understanding the parametric dependence of (complex) performance measures of such Markov chains is often of significant interest. The derivatives of the performance measures w. r. t. the parameters play important roles, for example, in numerical optimization of the performance measures, and quantification of the uncertainties in the performance measures when there are uncertainties in the parameters from the <b>statistical</b> <b>estimation</b> <b>procedures.</b> In this paper, we establish conditions that guarantee the differentiability of various types of intractable performance measures [...] -such as the stationary and random horizon discounted performance measures [...] -of general state space Markov chains and provide probabilistic representations for the derivatives...|$|E
40|$|It is {{well-known}} that a Gaussian dependence structure implies asymptotic indepen-dence {{in the sense}} that, two correlated risk factors which are jointly normally distributed are extremely unlikely to take very high (or low) values together (unless they are com-pletely dependent). This is obviously true for many other dependence structures be-yond the Gaussian. We concentrate on multivariate regularly varying distributions (our paradigm for multivariate heavy-tailed distributions) which often exhibit a similar prop-erty of asymptotic independence. Under asymptotic independence, though a joint ex-treme event is quite unlikely, yet it is still possible. We provide a formulation of hidden regular variation to find the part that is missed under the cruder normalization of multi-variate regular variation in such cases and discuss some <b>statistical</b> <b>estimation</b> <b>procedures</b> for assessing probabilities of such joint risk events under this definition of hidden regular variation. Keywords...|$|E
40|$|The {{purpose of}} the study is to develop and verify a {{methodology}} capable of predicting the vibration levels and estimating the aerodynamic loads and rotor impedance of a rotorcraft blade. Simulated flight test data is generated, blade airloads and elastic hub motions are estimated from the simulated data through the use of the Kalman filter/smoother, simulation upgrading and parameter identification are performed, and the ability to identify rotor impedance from a simulation by isolating the rotor model and providing a prescribed motion for the hub as rotor excitation is demonstrated. It is pointed out that the <b>statistical</b> <b>estimation</b> <b>procedure</b> utilized in the proposed methodology minimizes the impact of sensor noise, truncation error, and instrumentation bias on the results...|$|R
40|$|The {{purpose of}} this article is to propose and {{highlight}} a <b>statistical</b> <b>estimation</b> <b>procedure</b> for joint identification of irregularities in population distribution in urban areas having a directional dimension, and the estimation of the parameters of the model's separate regimes. The method we propose here is an application of the switching regimes regression technique developed by Quandt (1958, 1960) for identifying the most likely allocation of n observations into two separate regimes, each associated with a different mechanism that generates its own set of observations. A subsequent purpose is to apply empirically the switching regimes method to the city of Tel-Aviv - Yafo, and discern possible regimes where population densities are generated by different processes. Switching regimes, CBD, density gradients, spatial models...|$|R
40|$|The {{determination}} of acceptability prices of contingent claims requires {{the choice of}} a stochastic model for the underlying asset price dynamics. Given this model, optimal bid and ask prices can be found by stochastic optimization. However, the model for the underlying asset price process is typically based on data and found by a nonparametric <b>statistical</b> <b>estimation</b> <b>procedure.</b> We define a confidence set of possible estimated models by a nonparametric neighborhood of a baseline model. This neighborhood serves as ambiguity set for a stochastic optimization problem under model uncertainty. We obtain distributionally robust solutions of the acceptability pricing problem and derive the dual problem formulation. Moreover, we relate the bid and ask prices under model ambiguity {{to the quality of the}} observed data. Comment: 23 pages, 4 figure...|$|R
40|$|Current public {{telephone}} networks compromise voice qual-ity by bandlimiting {{the speech}} signal. Telephone speech {{is characterized by}} a bandpass response from 300 to 3400 Hz. The voice quality is perceived as being much worse than for wideband speech (50 – 7000 Hz). We present a novel approach which combines equalization and estima-tion to create a wideband signal, with reconstructed com-ponents in the 3400 Hz to 7000 Hz range. Equalization is used in the 3400 - 4000 Hz range. Its performance is better than <b>statistical</b> <b>estimation</b> <b>procedures,</b> because the mutual dependencies between the narrowband and highband pa-rameters are not sufficiently large. Subjective evaluation using an Improvement Category Rating shows that the re-constructed wideband speech using both equalization and estimation substantially enhances the quality of telephone speech. We have also evaluated the performance on the nar-rowband output of several standard codecs. Overall, the use of equalization for part of the highband regeneration makes the system more robust to phonetic variability and speaker gender. ...|$|E
40|$|Astandard {{combinatorial}} {{problem is}} to estimate the number (T) ofcoupons, drawn at random, needed to complete a collection of all possible m types. Generalizations of this problem have found many engineering applications. The usefulness of the model is hampered by the difficulties in obtaining numerical results for moments or distributions. We show two computational paradigms that are well suited {{for this type of}} problems: one, following Flajolet et al. [21], is the calculus of generating functions over regular languages. We use it to provide relatively efficient answers to several questions about the sampling process – we show itispossible to compute arbitrarily accurate approximations for quantities such as E[T], inatime which is linear in m, for any type distribution, while an exponential time is required for exact calculation. It also leads to a proof of a long-standing folk-theorem, concerning the extremality of uniform reference probabilities. The second method is a generalization of the Poisson transform [25], which we use to discuss <b>statistical</b> <b>estimation</b> <b>procedures.</b> Ke y words: coupon collector, engineering applications, combinatorics of languages, numerical methods, Poisson transform, prediction function. 2 Boneh, Hofri: Coupon Collecting Revisited [...] . 1...|$|E
40|$|In {{this paper}} {{the theory and}} {{estimation}} procedures for several consumer preference models are discussed. Predictive accuracy {{in the form of}} internal consistency of these models is compared in an empirical application. Consumer decision situations are classified into two classes: decisions under certainty and decisions under uncertainty. For each of the two classes of decision situations two modeling strategies have been used: statistical estimation and algebraic solution. An additive conjoint, an additive and a multiplicative measurable value, and an additive and a multiplicative utility model are considered. Our main finding is that the <b>statistical</b> <b>estimation</b> <b>procedures</b> outperform their algebraic counterparts on the criterion of predictive accuracy. The utility model provides better predictions for decisions under uncertainty than the widely used conjoint models. The relationship between models for decisions under certainty and decisions under uncertainty is discussed. It is shown how a conjoint or a measurable value function model can be transformed into a utility model with minimum additional information from the subjects. A concept of relative risk attitude is proposed to segment consumers by the degree of their risk aversion or risk seeking propensities. marketing: consumer preference models, empirical decision theory...|$|E
40|$|The {{mathematical}} {{description and}} implementation of the <b>statistical</b> <b>estimation</b> <b>procedure</b> known as the Houston integrated spatial/spectral estimator (HISSE) is discussed. HISSE is based on a normal mixture model and is designed to take advantage of spectral and spatial information of LANDSAT data pixels, utilizing the initial classification and clustering information provided by the AMOEBA algorithm. The HISSE calculates parametric estimates of class proportions which reduce the error inherent in estimates derived from typical classify and count procedures common to nonparametric clustering algorithms. It also singles out spatial groupings of pixels which are most suitable for labeling classes. These calculations are designed to aid the analyst/interpreter in labeling patches with a crop class label. Finally, HISSE's initial performance on an actual LANDSAT agricultural ground truth data set is reported...|$|R
40|$|Many {{empirical}} studies nd that {{the distribution of}} the estimated innovations of a multivariate GARCH volatility model for nancial returns is heavy tailed. In such cases, it may be desirable to employ a <b>statistical</b> <b>estimation</b> <b>procedure</b> that is more robust to deviations from normality than the gaussian quasi-maximum likelihood estimator. This paper introduces the class of elliptical M-estimators for multivari-ate conditionally heteroscedastic time series models. Furthermore, we generalize the robust M-estimators proposed by Muler and Yohai (2006) for the univariate GARCH model to the multivariate case. In a Monte Carlo experiment, classical M-estimators like the gaussian and student t maximum likelihood estimators are shown to be adversely aected by outliers in the data and the usefulness of robust M-estimators is illustrated. In the empirical analysis, M-estimators are used to nd the minimum conditional variance portfolio allocation between the French an...|$|R
40|$|Learning is a {{flexible}} and {{effective means of}} extracting the stochastic structure of the environment. It provides an effective method for blind separation and deconvolution in signal processing. Two different types of learning are used, namely batch learning and on-line learning. The batch learning procedure uses all the training examples repeatedly so that its performance is compared to the <b>statistical</b> <b>estimation</b> <b>procedure.</b> On-line learning is more dynamical, updating the current estimate by observing a new datum one by one. On-line learning is slow in general but works well in the changing environment. The present paper gives a unified framework of statistical analysis for batch and on-line learning. The topics include the asymptotic learning curve, generalization error and training error, over-fitting and over-training, efficiency of learning, and an adaptive method of determining learning rate. keywords learning from examples, batch learning, on-line learning, over-training, adapt [...] ...|$|R
40|$|The {{traditional}} approach to design flood estimation (for example, to derive the 100 -year flood) {{is to apply}} a statistical model to time series of peak river flow measured by gauging stations. Such records are typically not very long, for example in the UK only about 10 % of the stations have records that are more than 50 years in length. Along-explored way to augment the data available from a gauging station is to derive information about historical flood events and paleo-floods, which {{can be obtained from}} careful exploration of archives, old newspapers, flood marks or other signs of past flooding that are still discernible in the catchment, and the history of settlements. The inclusion of historical data in flood frequency estimation has been shown to substantially reduce the uncertainty around the estimated design events and is likely to provide insight into the rarest events which might have pre-dated the relatively short systematic records. Among other things, the FEH Local project funded by the Environment Agency aims to develop methods to easily incorporate historical information into the standard method of statistical flood frequency estimation in the UK. Different <b>statistical</b> <b>estimation</b> <b>procedures</b> are explored, namely maximum likelihood and partial probability weighted moments, and {{the strengths and weaknesses of}} each method are investigated. The project assesses the usefulness of historical data and aims to provide practitioners with useful guidelines to indicate in what circumstances the inclusion of historical data is likely to be beneficial in terms of reducing both the bias and the variability of the estimated flood frequency curves. The guidelines are based on the results of a large Monte Carlo simulation study, in which different estimation procedures and different data availability scenarios are studied. The study provides some indication of the situations under which different estimation procedures might give a better performance...|$|E
40|$|The {{average length}} of life from birth until death in a human {{population}} is a single statistic that {{is often used to}} characterise the prevailing health status of the population. It is one of many statistics calculated from an analysis that, for each age, combines the number of deaths with the size of the population in which these deaths occur. This analysis is generally known as life table analysis. Life tables have only occasionally been produced specifically for South Australia, although the necessary data has been routinely collected since 1842. In this thesis, the mortality pattern of South Australia over the period of 150 years of European settlement is quantified by using life table analyses and estimates of {{average length of}} life. In Chapter 1, a mathematical derivation is given for the lifetime statistical distribution function that is the basis of life table analysis, and from which the average length of life or current expected life is calculated. This derivation uses mathematical notation that clearly shows the deficiency of current expected life as a measure of the life expectancy of an existing population. Four <b>statistical</b> <b>estimation</b> <b>procedures</b> are defined, and the computationally intensive method of bootstrapping is discussed as an estimation procedure for the standard error of each of the estimates of expected life. A generalisation of this method is given to examine the robustness of the estimate of current expected life. In Chapter 2, gender and age-specific mortality and population data are presented for twenty five three-year periods; each period encompassing one of the colonial (1841 - 1901) or post-Federation (1911 - 96) censuses that have been taken in South Australia. For both genders within a census period, four types of estimate of current expected life, each with a bootstrap standard error, are calculated and compared, and a robustness assessment is made. In Chapter 3, an alternate measure of life expectancy known as generation expected life is considered. Generation expected life is derived by extracting, from official records arranged in temporal order, the mortality pattern of a notional group of individuals who were born in the same calendar year. Several estimates of generation expected life are calculated using South Australian data, and each estimate is compared to the corresponding estimate of current expected life. Additional estimates of generation expected life calculated using data obtained from the Roll of Honour at the Australian War Memorial quantify the reduction in male generation expected life for 1881 - 1900 as a consequence of military service during World War I, 1914 - 18, and the Influenza Pandemic, 1919. Thesis (M. Sc.) [...] University of Adelaide, School of Applied Mathematics, 2003...|$|E
40|$|The {{response}} of large volume Airborne Gamma Spectrometry (AGS) detectors has been modelled using Monte Carlo methods {{as part of}} an assessment of ways to enhance monitoring of beaches and other coastal areas for radioactive particles. Airborne survey methods are capable of rapid surveys of large areas, and of covering diverse environments effectively, including areas where ground based work is difficult to implement safely. They have potential for increasing the effectiveness of ground based surveys, working in a complementary manner to identify areas where detailed work including recovery operations would be targeted. The study presented here is a continuation of an earlier study defining point source detection limits for airborne gamma spectrometry in natural radioactivity backgrounds representative of the West Cumbria beaches. Here anthropogenic backgrounds in the area are also reviewed together with their impact on detection limits. Similarly the signals that would be expected from particles already recovered from the beaches of West Cumbria are assessed in comparison with observed background for 137 Cs to appraise the extent to which those particles recovered may have contributed to past airborne surveys of the beaches. Theoretical sensitivities have been assessed using airborne systems operated under two survey designs: a “rapid” design at 75 m (200 ft) ground clearance and a 70 knot (30 m s- 1) survey speed, capable of surveying > 60 km of shore in a 2 h survey with multiple lines; and also a “slow and low” design based on 15 m (50 ft) ground clearance and 15 knot (5 ms- 1) forward velocity, suitable to surveys of 5 km of beach in a 2 h flight. Additional consideration is given to surveys at further reduced height and speed, to meet a detection criterion of 105 Bq sources at 10 cm burial depth. Monte-Carlo methods were used to simulate the full spectral responses for a series of 137 Cs and 241 Am point sources placed in simulated natural background environments with varying 137 Cs background distributions. The MCII Monte Carlo code developed at SUERC, which has been extensively validated in the past and incorporates <b>statistical</b> <b>estimation</b> <b>procedures</b> to simulate spectra at airborne source-detector separations, was used. A review of the distribution of dispersed 137 Cs on the beaches showed areas of beach with negligible 137 Cs activity, areas with approximately uniform 137 Cs backgrounds, and areas with patches of dimensions of 5 - 10 m with elevated 137 Cs activity. Samples collected from the higher activity areas of the beaches had activity concentrations of 50 Bq kg- 1. Spectra were simulated for 137 Cs background distributions for single and multiple patches of activity with 10 m dimensions and 50 Bq kg- 1 concentrations, and uniform distributions of different concentrations, at 75 and 15 m heights. The analyses for point sources with the medium natural background conducted in the earlier study were repeated with these different 137 Cs backgrounds. The data for the location, depth and activity for the recovered particles were used to simulate the spectra that an airborne system would observe from these particles, and compared with the spectra recorded during earlier surveys of the beaches of West Cumbria. Each particle recovered is below the detection limit of a 75 m survey, and even cumulatively the recovered finds would not result in any observed signal in the past surveys. The past airborne survey data show that theronsiderable 137 Cs inventory on the beaches between St Bees and Duddon), of which the recovered s repre 001...|$|E
30|$|An {{alternative}} {{is that it}} is possible to guess these parameters using the radar’s polarisation and azimuth angle as a guide to possible Pareto clutter parameters. Such a crude practice can be justified for the Pareto scale parameter based upon the results in [15], where it is demonstrated that various detectors for the clutter model of interest do not seem to be overly sensitive to this parameter. Hence, bounds can be used to approximate it. For example, it has been found that based upon the Defence Science and Technology Organisation (DSTO) X-band high-resolution radar clutter sets [14], very spiky horizontally polarised returns, in the upwind direction, will generally have very small shape parameters (3 <α< 5), with scale parameter β< 0.1. For vertically polarised returns, also in the upwind direction, the shape parameter tends to be large (α> 12), while the scale parameter 0.1 <β< 1. Based upon these bounds, we can approximate β quite easily. However, a <b>statistical</b> <b>estimation</b> <b>procedure</b> is still required for approximation of the texture shape parameter.|$|R
40|$|Information {{about the}} {{strength}} of gas sources in buildings {{has a number of}} applications in the area of building automation and control, including temperature and ventilation control, fire detection and security systems. Here, we consider the problem of estimating {{the strength of}} a gas source in an enclosure when some of the parameters of the gas transport process are unknown. Traditionally, these problems are either solved by the Maximum-Likelihood (ML) method which is accurate but computationally intense, or by Recursive Least Squares (RLS, also Kalman) filtering which is simpler but less accurate. In this paper, we suggest a different <b>statistical</b> <b>estimation</b> <b>procedure</b> based on the concept of Method of Moments. We outline techniques that make this procedure computationally efficient and amenable for recursive implementation. We provide a comparative analysis of our proposed method based on experimental results as well as Monte-Carlo simulations. When used with the building control systems, these algorithms can estimate the gaseous strength in a room both quickly and accurately, and can potentially provide improved indoor air quality in an efficient manner. Comment: 10 pages, 9 figure...|$|R
40|$|Abstract—Information {{about the}} {{strength}} of gas sources in buildings {{has a number of}} applications in the area of building automation and control, including temperature and ventilation control, fire detection and security systems. Here, we consider the problem of estimating {{the strength of}} a gas source in an enclosure when some of the parameters of the gas transport process are unknown. Traditionally, these problems are either solved by the Maximum-Likelihood (ML) method which is accurate but com-putationally intense, or by Recursive Least Squares (RLS, also Kalman) filtering which is simpler but less accurate. In this paper, we suggest a different <b>statistical</b> <b>estimation</b> <b>procedure</b> based on the concept of Method of Moments. We outline techniques that make this procedure computationally efficient and amenable for recursive implementation. We provide a comparative analysis of our proposed method based on experimental results as well as Monte-Carlo simulations. When used with the building control systems, these algorithms can estimate the gaseous strength in a room both quickly and accurately, and can potentially provide improved indoor air quality in an efficient manner...|$|R
40|$|The {{response}} of large volume Airborne Gamma Spectrometry (AGS) detectors has been modelled using Monte Carlo methods {{as part of}} an assessment of ways to enhance monitoring of beaches and other coastal areas for radioactive particles. Airborne survey methods are capable of rapid surveys of large areas, and of covering diverse environments effectively, including areas where ground based work is difficult to implement safely. They have potential for increasing the effectiveness of ground based surveys, working in a complementary manner to identify areas where detailed work including recovery operations would be targeted. This study examined the theoretical sensitivities that might be obtained using airborne systems operated at 75 m (200 ft) ground clearance and a 70 knot (30 m s- 1) survey speed, and also a “slow and low” design based on 15 m (50 ft) ground clearance and 15 knot (5 ms- 1) forward velocity, or slower. Monte-Carlo methods were used to simulate the full spectral responses for a series of 137 Cs and 241 Am point sources placed in simulated natural background environments. The MCII Monte Carlo code, which has been extensively validated in the past and incorporates <b>statistical</b> <b>estimation</b> <b>procedures</b> to simulate spectra at airborne source-detector separations, was used. This code was set up for automatic sampling of the complex gamma ray spectra associated with the natural uranium and thorium decay series, with energy- intensity files collated from the JEFF 3. 1. 1 nuclear data for all lines from the natural decay series and 40 K, without truncating low intensities. Thus a comprehensive description of the photon intensities of natural sources was achieved. The code was re-validated in this study using simulated and measured data for concrete calibration pads at SUERC and shown to be in excellent agreement with measured spectra in close coupled geometries. A brief assessment of the GEANT Monte Carlo package indicated that it was not ideally suited to the large source-detector geometries of airborne measurements. The geology and geochemistry of the Cumbrian coastline was reviewed, and data used to define mass absorption coefficients for substrates with a composition similar to St Bees Sandstone. For energies above 200 keV, the mass absorption coefficients are insensitive to the range of major element compositions of different materials in the Sellafield area. These coefficients were incorporated in the Monte Carlo code, and used to simulate reference spectra for unit concentrations of 40 K, Uranium and Thorium series sources at the two airborne survey ground clearances defined for the study. Uniform vertical and lateral activity distributions were used for these natural background reference spectra, representing a reasonable starting point for the work. These were combined and scaled to the natural radioelement concentrations determined previously from an area of beach south of St. Bees Head, and north of Sellafield, which had been surveyed in 2000 {{as part of a larger}} study commissioned by DEFRA. The agreement with the measured spectra from the earlier survey in the energy range associated with natural gamma rays was excellent, thus validating the simulation at airborne heights. Data from past surveys and other ground based measurements were also reviewed to assess the levels and variations of natural background sources in the coastal areas of West Cumbria. These were used to define three working levels (“Low”, “Medium” and “High”) of natural background concentrations, whose AGS contributions were simulated at the two airborne survey ground clearances, to form the natural background levels against which detection limits for point sources could be considered in the study. Additional simulations of the system responses to 137 Cs sources in superficial locations, and at a burial depth of 10 cm, and of superficial 241 Am sources were conducted for the two reference heights, and a series of source-detector offset distances. These were used to assess detection limits and signal significance levels for different survey scenarios, based on full energy peak count rates for the source spectra and the simulated backgrounds. The signal profiles corresponding to flight lines directly over and adjacent to the sources were also simulated for the first time. The results have shown that AGS surveys at 75 m (200 ft) ground clearance and 70 knots (30 m s- 1), capable of surveying > 60 km of shore in a 2 h survey with multiple lines, should detect 137 Cs sources of 5 - 10 MBq or greater in the presence of uniform natural radiation fields representing typical compositions for the area. More detailed surveys at 15 m (50 ft) ground clearance and 5 knots (2 m s- 1) should detect 137 Cs sources of 105 Bq at a depth of 10 cm. Surveys at these reduced heights and speeds should also detect superficial 241 Am sources of 106 Bq or greater. A high density survey, in which the source is registered in more than one measurement, would increase the significance of the summed signal by about 0. 5 σ. Reducing ground clearance would increase full energy peak count rates, by an order of magnitude at 5 m compared to 15 m. It would be expected that surveys at these ground clearances would significantly increase the significance of full-energy peak counts from sources, although the natural series spectra have not been simulated to quantify this. A data processing methodology that utilises the scattered part of the spectra (> 80...|$|E
40|$|Introduction and objectives: For {{some time}} now {{regression}} models, often calibrated using the ordinary least-squares (OLS) estimation procedure, have become common tools for forecasting the demand for air transportation. However, in recent years more and more decision makers have begun to use these models not only to forecast traffic, but also for analyzing alternative policies and strategies. Despite this increase in scope {{for the use of}} these models for policy analysis, few analysts have investigated in depth the validity and precision of these models with respect to their expanded use. In order to use these models properly and effectively it is essential not only to understand the underlying assumptions and their implications which lead to the estimation procedure, but also to subject these assumptions to rigorous scrutiny. For example, one of the assumptions that is built into the ordinary least-squares estimation technique is that the explanatory variables should not be correlated among themselves. If the variables are fairly collinear, then the sample variance of the coefficient estimators increases significantly, which results in inaccurate estimation of the coefficients and uncertain specification of the model with respect to inclusion of those explanatory variables. As a corrective procedure, it is a common practice among demand analysts to drop those explanatory variables out of the model for which the t-statistic is insignificant. This is not a valid procedure since if collinearity is present the increase in variance of the coefficients will result in lower values of the t-statistic and rejection from the demand model of those explanatory variables which in theory do explain the variation in the dependent variable. Thus, if {{one or more of the}} assumptions underlying the OLS estimation procedure are violated, the analyst must either use appropriate correction procedures or use alternative estimation techniques. The purpose of the study herein is three-fold: (1) develop a "good" simple regression model to forecast as well as analyze the demand for air transportation; (2) using this model, demonstrate the application of various statistical tests to evaluate the validity of each of the major assumptions underlying the OLS estimation procedure with respect to its expanded use of policy analysis; and, (3) demonstrate the application of some advanced and relatively new <b>statistical</b> <b>estimation</b> <b>procedures</b> which are not only appropriate but essential in eliminating the common problems encountered in regression models when some of the underlying assumptions in the OLS procedure are violated. The incentive for the first objective, to develop a relatively simple single equation regression model to forecast as well as analyze the demand for air transportation (as measured by revenue passenger miles in U. S. Domestic trunk operations), stemmed from a recently published study by the U. S. Civil Aeronautics Board [CAB, 1976]. In the CAB study a five explanatory variable regression equation was formulated which had two undesirable features. The first was the inclusion of time as an explanatory variable. The use of time is undesirable since, from a policy analysis point of view, the analyst has no "control" over this variable, and it is usually only included to act as a proxy for other, perhaps significant, variables inadvertently omitted from the equation. The second undesirable feature of the CAB model is the "delta log" form of the equation (the first difference in the logs of the variables),which allowed a forecasting interval of only one year into the future. This form was the result of the application of a standard correction procedure for collinearity among some of the explanatory variables. In view of these two undesirable features, it was decided to attempt to improve on the CAB model. In addition to the explanatory variables considered in the CAB study a number of other variables were analyzed to determine their appropriateness in the model. Sections II and III of this report describe the total set of variables investigated as well as a method for searching for the "best" subset. Then, Section IV outlines the decisions involved in selecting the appropriate form of the equation. The second objective of this study is to describe a battery of statistical tests, some common and some not so common, which evaluate the validity of each of the major assumptions underlying the OLS estimation procedure with respect to single equation regression models. The major assumptions assessed in Section V of this report are homoscedasticity, normality, autocorrelation, and multicollinearity. The intent here is not to present all of the statistical tests that are available, for to do so would be the purpose of regression textbooks, but to scrutinize these four major assumptions enough to remind the analyst that it is essential to investigate in depth the validity and precision of the model with respect to its expanded use of policy analysis. It is hopeful that the procedure outlined in this report sets an example to demand modeling analysts of the essential elements used in the development of reliable forecasting tools. The third and ultimate objective of this work is to demonstrate the use of some advanced corrective procedures in the event that any of the four above mentioned assumptions have been violated. For example, the problem of autocorrelation can be resolved by the use of generalized least-squares(GLS), which is demonstrated in Section VI of this report; and the problem of multicollinearity, usually corrected by employing the cumbersome and restrictive delta log form of equation, has been eliminated by using Ridge regression (detailed in Section VII). Finally, in Section VIII an attempt is made to determine the "robustness" of a model by first performing an examination of the residuals using such techniques as the "hat matrix", and second by the application of the recently developed estimation procedures of Robust regression. Although the techniques of Ridge and Robust regression are still in the experimental stages, sufficient research has been performed to warrant their application to significantly improve the currently operational regression models. August 1977 Includes bibliographical references (p. 76 - 78...|$|E
40|$|This paper {{presents}} a new simple retrieval algorithm for estimating area-time averaged rain rates over tropical oceans by using single channel microwave measurements from satellites. The algorithm was tested {{by using the}} Nimbus- 5 Electrically Scanning Microwave Radiometer and a simple microwave radiative transfer model to retrieve seasonal 5 -deg x 5 -deg area averaged rainrate over the tropical Atlantic and Pacific from December 1973 to November 1974. The brightness temperatures were collected and analyzed into histograms for each season and in each grid box from December 1973 to November 1974. The histograms suggest a normal distribution of background noise plus a skewed rain distribution at the higher brightness temperatures. By using a <b>statistical</b> <b>estimation</b> <b>procedure</b> based upon normally distributed background noise, the rain distribution was separated from the raw histogram. The radiative transfer model {{was applied to the}} rain-only distribution to retrieve area-time averaged rainrates throughout the tropics. Despite limitations of single channel information, the retrieved seasonal rain rates agree well in the open ocean with expectations based upon previous estimates of tropical rainfall over the oceans...|$|R
40|$|Using mark-recaptured {{methodology}} and network sampling procedures, a statistical model {{was developed to}} estimate the number of African-American physicians in the United States. A sample (stratified by geographic region, medical specialty and an age surrogate) was selected from the National Medical Association's Masterfile of Black Physicians (NMAMBP). Respondents were asked to list the names of five black physicians who resided or practiced in their immediate geographic area. Data also were collected about citizenry {{as well as other}} demographic and professional information. The NMAMBP was used mathematically as a "marked" group that could then be "recaptured," allowing mark-recapture methodology to be used as the nucleus of the <b>statistical</b> <b>estimation</b> <b>procedure.</b> The results revealed that in 1991, the total number of US African-American physicians (black US citizens) was estimated to be 16, 282 with a conservative standard error of 764 and an approximate 95 % confidence interval, yielding a range of 14, 754 to 17, 810 physicians. This estimate is from 17 % to about 32 % lower than the 21, 538 black doctors reported by the 1990 Bureau of the Census and has important implications for attempts to reform the health-care system and policies designed to produce more African-American physicians...|$|R
40|$|Stochastic {{processes}} offer {{a flexible}} mathematical formalism to model and reason about systems. Most analysis tools, however, {{start from the}} premises that models are fully specified, so that any parameters controlling the system's dynamics must be known exactly. As this is seldom the case, many methods have been devised {{over the last decade}} to infer (learn) such parameters from observations {{of the state of the}} system. In this paper, we depart from this approach by assuming that our observations are {it qualitative} properties encoded as satisfaction of linear temporal logic formulae, as opposed to quantitative observations of the state of the system. An important feature of this approach is that it unifies naturally the system identification and the system design problems, where the properties, instead of observations, represent requirements to be satisfied. We develop a principled <b>statistical</b> <b>estimation</b> <b>procedure</b> based on maximising the likelihood of the system's parameters, using recent ideas from statistical machine learning. We demonstrate the efficacy and broad applicability of our method on a range of simple but non-trivial examples, including rumour spreading in social networks and hybrid models of gene regulation...|$|R
40|$|We {{propose a}} periodic±normal mixture (PNM) model to ®t {{transcription}} pro®les of periodically expressed (PE) genes in cell cycle microarray experiments. The model {{leads to a}} principled <b>statistical</b> <b>estimation</b> <b>procedure</b> that produces more accurate estimates of the mean cell cycle length and the gene expression periodicity than existing heuristic approaches. A central component of the proposed procedure is the resynchronization of the observed transcription pro®le of each PE gene according to the PNM with estimated periodicity parameters. By using a two-component mixture-Beta model to approximate the PNM ®tting residuals, we employ an empirical Bayes method to detect PE genes. We estimate that {{about one-third of the}} genes in the genome of Saccharomyces cerevisiae are likely to be transcribed periodically, and identify 822 genes whose posterior probabilities of being PE are greater than 0. 95. Among these 822 genes, 540 are also in the list of 800 genes detected by Spellman. Gene ontology annotation analysis shows that many of the 822 genes were involved in important cell cycle-related processes, functions and components. When matching the 822 resynchronized expression pro®les of three independent experiments, little phase shifts were observed, indicating that the three synchronization methods might have brought cells to the same phase at the time of release...|$|R
40|$|We {{introduce}} {{a class of}} Markov processes, called m-polynomial, for which the calculation of (mixed) moments up to order m only requires the computation of matrix exponentials. This class contains affine processes, processes with quadratic diffusion coefficients, as well as Lévy-driven SDEs with affine vector fields. Thus, many popular models such as exponential Lévy models or affine models are covered by this setting. The applications range from <b>statistical</b> GMM <b>estimation</b> <b>procedures</b> to new techniques for option pricing and hedging. For instance, the efficient and easy computation of moments {{can be used for}} variance reduction techniques in Monte Carlo methods. Comment: revised and extended version, accepted for publication in Finance and Stochastic...|$|R
40|$|We are {{concerned}} with the quantum inverse scattering problem. The corresponding Marchenko integral equation is solved by using the collocation method together with piece-wise polynomials, namely, Hermite splines. The scarcity of experimental data and the lack of phase information necessitate the generation of the input reflection coefficient by choosing a specific profile and then applying our method to reconstruct it. Various aspects of the single and coupled channels inverse problem and details about the numerical techniques employed are discussed. We proceed to apply our approach to synthetic seismic reflection data. The transformation of the classical one-dimensional wave equation for elastic displacement into a Schr¨odinger-like equation is presented. As an application of our method, we consider the synthetic reflection travel-time data for a layered substrate from which we recover the seismic impedance of the medium. We also apply our approach to experimental seismic reflection data collected from a deep water location in the North sea. The reflectivity sequence and the relevant seismic wavelet are extracted from the seismic reflection data by applying the <b>statistical</b> <b>estimation</b> <b>procedure</b> known as Markov Chain Monte Carlo method to the problem of blind deconvolution. In order to implement the Marchenko inversion method, the pure spike trains have been replaced by amplitudes having a narrow bell-shaped form to facilitate the numerical solution of the Marchenko integral equation from which the underlying seismic impedance profile of the medium is obtained. PhysicsD. Phil. (Physics...|$|R
40|$|Statistical {{information}} about the flow sizes in the traffic passing through a network link helps a network operator to characterize network resource usage, infer traffic demands, detect traffic anomalies, and improve network performance through traffic engineering. Previous work on estimating the flow size distribution for the complete population of flows has produced techniques that either make inferences from sampled network traffic, or use data streaming approaches. In this work, we identify and solve a more challenging problem of estimating the size distribution and other statistical {{information about}} arbitrary subpopulations of flows. Inferring subpopulation flow statistics is more challenging than the complete population counterpart, since subpopulations of interest are often specified a posteriori (i. e., after the data collection is done), {{making it impossible for}} the data collection module to “plan in advance”. Our solution consists of a novel mechanism that combines data streaming with traditional packet sampling to provide highly accurate estimates of subpopulation flow statistics. The algorithm employs two data collection modules operating in parallel — a NetFlow-like packet sampler and a streaming data structure made up of an array of counters. Combining the data collected by these two modules, our estimation algorithm uses a <b>statistical</b> <b>estimation</b> <b>procedure</b> that correlates and decodes the outputs (observations) from both data collection modules to obtain flow statistics for any arbitrary subpopulation. Evaluations of this algorithm on real-world Internet traffic traces demonstrate its high measurement accuracy...|$|R
40|$|This paper {{presents}} some <b>statistical</b> {{models and}} <b>estimation</b> <b>procedures</b> when the explanatory variables are functions. We put {{the stress on}} the fact that regularization techniques are needed in order to get stable and reliable estimations. Then, an application in remote sensing in presented. It shows the potential of these kind of models to handle real life problems. Copyright # 2005 John Wiley & Sons, Ltd. KEY WORDS: functional principal components analysis; ill-posed problems; Karhunen–Loeve expansion; longitudinal data; multilogit model; penalty; regularization; smoothing; SPOT 4 /V!eg!etation sensor 1...|$|R
40|$|Abstract—Pairwise {{statistical}} significance (PSS) {{has been found}} to be able to accurately identify related sequences (homology detection), which is a fundamental step in numerous applications relating to sequence analysis. Although more accurate than database {{statistical significance}}, it is both computationally intensive and data intensive to construct the empirical score distribution during the estimation of PSS, which poses a big challenge in terms of performance and scalability. Multicore computers and clusters have become increasingly ubiquitous and more powerful than before. In this paper, we evaluate the use of OpenMP, MPI and hybrid paradigms to accelerate the estimation of PSS of local sequence alignment. Through distributing the compute-intensive kernels of the pairwise <b>statistical</b> significance <b>estimation</b> <b>procedure</b> across multiple computational units, we achieve a speedup of up to 113. 10 × using 128 cores. Keywords-Pairwise statistical significance; Multicore, OpenMP; MPI; Hybrid; I...|$|R
40|$|Comprehensive {{coverage}} of national accounts estimates is important; however, {{it is often}} thwarted by gaps in the recording of economic activity – the so-called “unrecorded economy”. This paper sets out pragmatic statistical approaches for incorporating the unrecorded economy in the national accounts. It describes sources and methods to capture the unrecorded economy and discusses specific issues that arise {{from the use of}} indirect sources and techniques. Furthermore, the paper elaborates approaches for collecting data on the unrecorded economy, particularly on economic activities of the household sector. Gross domestic product;National accounts;surveys, statistics, survey, data sources, data collection, statistical system, statisticians, household expenditure, sampling, sensitivity analysis, counting, household consumption, input-output, statistical office, statistical offices, sample design, household surveys, income distribution, time series, financial system, sampling errors, equation, household income, government agencies, <b>estimation</b> method, <b>statistical</b> tool, <b>estimation</b> <b>procedure,</b> income data, input prices...|$|R
