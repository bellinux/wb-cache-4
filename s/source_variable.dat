46|1654|Public
2500|$|Inspecting , we {{note that}} term a11 is the {{transfer}} function between the input and the output [...] setting the control parameter, P, to zero; term a12 is the transfer function between the output and the controlled variable xj [...] setting the input source, xS, to zero; term a21 represents the transfer function between the <b>source</b> <b>variable</b> and the inner variable, xi when the controlled variable [...] xj is set to zero (i.e., when the control parameter, P is set to zero); term a22 gives {{the relation between the}} independent and the controlled inner variables setting control parameter, P and input variable, xS, to zero." ...|$|E
5000|$|There {{are five}} school unit files, {{organized}} around the year of data collection. A school unit record consists of information about a school {{as the result of}} source data captured from family interviews, a classroom teacher, or the school principal. The structure of this data file is different from others in that rather than being merged on a common key, the records are actually stacked one upon the other in groups. The first part of the file consists of family data, the middle portion consists of teacher data, and the final portion consists of principal data. A key variable to the construction of this dataset is the REC_SRC (record <b>source)</b> <b>variable.</b> It informs the user as to the source of the data in the record. The abbreviations are [...] "fi," [...] "ta," [...] and [...] "qp" [...] for family interview, teacher questionnaire (Part A), and questionnaire for principals, respectively. The data viewed as the centerpiece of these datasets are the school climate survey variables and their associated factor scores.|$|E
50|$|In {{the above}} {{equation}} Aij(f) {{is an element}} of A(f) - a Fourier transform of MVAR model coefficients A(t), where aj(f) is j-th column of A(f) and the asterisk denotes the transpose and complex conjugate operation. Although it is a function operating in the frequency domain, the dependence of A(f) on the frequency has not a direct correspondence to the power spectrum. From normalization condition it follows that PDC takes values from the interval 0,1. PDC shows only direct flows between channels. Unlike DTF, PDC is normalized to show a ratio between the outflow from channel j to channel i to all the outflows from the source channel j, so it emphasizes rather the sinks, not the sources. The normalization of PDC affects the detected intensities of flow as was pointed out in. Namely, adding further variables that are influenced by a <b>source</b> <b>variable</b> decreases PDC, although the relationship between source and target processes remains unchanged. In another words: the flow emitted in one direction will be enhanced in comparison to the flows of the same intensity emitted from a given source in several directions.|$|E
40|$|As compilers {{increasingly}} rely on optimizations {{to achieve}} high performance, {{the effectiveness of}} source level debuggers for optimized code continues to falter. Even if values of <b>source</b> <b>variables</b> are computed in {{the execution of the}} optimized code, source level debuggers of optimized code are unable to always report the expected values of <b>source</b> <b>variables</b> at breakpoints. In this paper, we present FULLDOC, a debugger that can report all of the expected values of <b>source</b> <b>variables</b> that are computed in the optimized code. FULLDOC uses statically computed information to guide the gathering of dynamic information that enables full reporting. FULLDOC can report expected values at breakpoints when reportability is affected because values have been overwritten early, due to code hoisting or register reuse, or written late, due to code sinking. Our debugger can also report values that are path sensitive in that a value may be computed only along one path or the location of the va [...] ...|$|R
50|$|Neutron <b>source</b> <b>variables</b> {{include the}} energy of the {{neutrons}} emitted by the source, the rate of neutrons emitted by the source, the size of the source, the cost of owning and maintaining the source, and government regulations related to the source.|$|R
40|$|Abstract. As compilers {{increasingly}} rely on optimizations {{to achieve}} high performance, {{the effectiveness of}} source level debuggers for optimized code continues to falter. Even if values of <b>source</b> <b>variables</b> are computed in {{the execution of the}} optimized code, source level debuggers of optimized code are unable to always report the expected values of <b>source</b> <b>variables</b> at breakpoints. In this paper, we present FULLDOC, a debugger that can report all of the expected values of <b>source</b> <b>variables</b> that are computed in the optimized code. FULLDOC uses statically computed information to guide the gathering of dynamic information that enables full reporting. FULL-DOC can report expected values at breakpoints when reportability is affected because values have been overwritten early, due to code hoisting or register reuse, or written late, due to code sinking. Our debugger can also report values that are path sensitive in that a value may be computed only along one path or the location of the value may be different along different paths. We implemented FULLDOC for C programs, and experimentally evaluated the effectiveness of reporting expected values. Our experimental results indicate that FULLDOC can report 31 % more values than are reportable using only statically computed information. We also show improvements of at least 26 % over existing schemes that use limited dynamic information. ...|$|R
3000|$|Let us now {{adjust the}} second {{internal}} parameter of the GLDPC-Staircase codes, N 1, {{which represents the}} degree of the <b>source</b> <b>variable</b> nodes of the base code matrix H [...]...|$|E
40|$|We {{describe}} a novel non parametric statistical hypothesis test of relative depen-dence between a <b>source</b> <b>variable</b> and two candidate target variables. Such a test enables one to answer whether one <b>source</b> <b>variable</b> is significantly more depen-dent {{on the first}} target variable or the second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC), resulting {{in a pair of}} empirical dependence measures (source-target 1, source-target 2). Modeling the covariance between these HSIC statistics leads to a provably more powerful test than the con-struction of independent HSIC statistics by sub sampling. The resulting test is consistent and unbiased, and (being based on U-statistics) has favorable conver-gence properties. The test can be computed in quadratic time, matching the com-putational complexity of standard empirical HSIC estimators. We demonstrate the effectiveness of the test on a real-world linguistics problem of identifying lan-guage groups from a multilingual corpus. ...|$|E
30|$|In particular, the {{demographic}} {{structure of the}} species within the native vegetation indicates the characteristics of an expanding population process with a decreasing average age away from propagule sources and with mature reproductive individuals within the native vegetation. This explains that the distance to propagule <b>source</b> <b>variable</b> was a strong predictor of density and size of individuals.|$|E
5000|$|Some {{researchers}} {{have been criticized for}} misinterpreting the ELM. One such instance is Kruglanski and Thompson thinking that the processing of central or peripheral routes is determined by the type of information that affects message persuasion. For example, message variables are only influential when the central route is used and information like <b>source</b> <b>variables</b> is only influential when the peripheral route is used.In fact, the ELM does not make statements about types of information being related to routes. Rather, the key to the ELM is how any type of information will be used depending on central or peripheral routes, regardless of what that information is. For example, the central route may permit <b>source</b> <b>variables</b> to influence preference for certain language usage in the message (e.g. [...] "beautiful") or validate a related product (e.g. cosmetics), while the peripheral route may only lead individuals to associate the [...] "goodness" [...] of <b>source</b> <b>variables</b> with the message. Theoretically, all of these could occur simultaneously. Thus, the distinction between central and peripheral routes is not the type of information being processed as those types can be applied to both routes, but rather how that information is processed and ultimately whether processing information in {{one way or the other}} will result in different attitudes.|$|R
40|$|Abstract—We {{consider}} a new {{variant of the}} exact repair dis-tributed storage problem, the multi-source exact repair problem, wherein the reconstruction decoders are each only required to provide {{a subset of the}} <b>source</b> <b>variables.</b> To best illustrate the idea, we generalize the (n, k, d) = (3, 2, 2) exact repair distributed storage problem to the multisource case. When every decoder demands all <b>source</b> <b>variables,</b> the rate region of the (3, 2, 2) exact repair problem is known to be same as that of the (3, 2, 2) functional repair problem, while the rate region for (3, 2, 2) case with multiple sources is unknown. We find achievable rate regions for vector binary and scalar binary codes via an automated approach. Index Terms—Distributed storage, exact repair, multi-source network coding I...|$|R
5000|$|Runtime {{inspection}} of stack and <b>source</b> code <b>variables</b> defined ...|$|R
3000|$|... of the <b>source</b> <b>variable</b> varies {{during the}} {{quantization}} process. It is practically infeasible to design separate codebooks optimized for every different source distribution and distortion function, or the encoder and the decoder {{may not have}} the ability to store a large number of codebooks. In these situations, it is convenient to use a quantizer whose codebook is constructed by a transformation of a fixed codebook based on the current statistical distribution of the <b>source</b> <b>variable.</b> These types of quantizers are generally called transformed quantizers [22, 23], and have been used in the conventional source coding area with a linear orthogonal transformation followed by a product quantizer. We provide in this subsection an analysis of the generalized vector quantizer, which is described in Section 2, when a transformed codebook is used. Detailed applications to finite-rate feedback MISO systems with a transformed codebook over spatially correlated fading channels are provided in Section 5.|$|E
40|$|International audienceWe {{describe}} a novel non-parametric statistical hypothesis test of relative dependence between a <b>source</b> <b>variable</b> and two candidate target variables. Such a test {{enables us to}} determine whether one <b>source</b> <b>variable</b> is significantly more dependent on a first target variable or a second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC), resulting {{in a pair of}} empirical dependence measures (source-target 1, source-target 2). We test whether the first dependence measure is significantly larger than the second. Modeling the covariance between these HSIC statistics leads to a provably more powerful test than the construction of independent HSIC statistics by sub-sampling. The resulting test is consistent and unbiased, and (being based on U-statistics) has favorable convergence properties. The test can be computed in quadratic time, matching the computational complexity of standard empirical HSIC estimators. The effectiveness of the test is demonstrated on several real-world problems: we identify language groups from a multilingual corpus, and we prove that tumor location is more dependent on gene expression than chromosomal imbalances. Source code is available for download at [URL]...|$|E
40|$|Symbolic debuggers are system {{development}} tools that can accelerate the validation speed of behavioral specifications by allowing a user {{to interact with}} an executing code at the source level. In response to a user query, the debugger retrieves {{the value of a}} <b>source</b> <b>variable</b> {{in a manner consistent with}} respect to the source statement where execution has halted. However, when a behavioral specification has been optimized using transformations, values of variables may be inaccessible in the run-time state...|$|E
30|$|For {{details and}} the <b>sources</b> of <b>variables</b> see Sect.  2.|$|R
40|$|AbstractWe {{consider}} a one-dimensional two-phase Stefan problem, modeling {{a layer of}} solid material floating on liquid. The model includes internal heat <b>sources,</b> <b>variable</b> total mass (resulting e. g. from sedimentation or erosion), and a pressure-dependent melting point. The problem is reduced {{to a set of}} nonlinear integral equations, which provides the basis for an existence and uniqueness proof and a new numerical method. Numerical results are presented...|$|R
40|$|The Pi of the Sky project observes optical {{flashes of}} astronomical origin and other light <b>sources</b> <b>variable</b> on short timescales, down {{to tens of}} seconds. We search mainly for optical {{emissions}} of Gamma Ray Bursts, but also for variable stars, blazars, etc. Precise photometry with a very large field of view (20 ?× 20 ?) requires a careful study and modelling of a point spread function (PSF), as presented in this paper...|$|R
40|$|We {{describe}} a novel non-parametric statistical hypothesis test of relative dependence between a <b>source</b> <b>variable</b> and two candidate target variables. Such a test {{enables us to}} determine whether one <b>source</b> <b>variable</b> is significantly more dependent on a first target variable or a second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC), resulting {{in a pair of}} empirical dependence measures (source-target 1, source-target 2). We test whether the first dependence measure is significantly larger than the second. Modeling the covariance between these HSIC statistics leads to a provably more powerful test than the construction of independent HSIC statistics by sub-sampling. The resulting test is consistent and unbiased, and (being based on U-statistics) has favorable convergence properties. The test can be computed in quadratic time, matching the computational complexity of standard empirical HSIC estimators. The effectiveness of the test is demonstrated on several real-world problems: we identify language groups from a multilingual corpus, and we prove that tumor location is more dependent on gene expression than chromosomal imbalances. Source code is available for download at [URL] International Conference on Machine Learning, Jul 2015, Lille, Franc...|$|E
40|$|International audienceTests of {{dependence}} {{are an important}} tool in statistical analysis, and are widely applied in many data analysis contexts. For many problems in data analysis, however, {{the question of whether}} dependence exists is secondary: there may be multiple dependencies, and the question becomes which dependence is the strongest. We present a novel non parametric statistical method which describes hypothesis test of relative dependence between a <b>source</b> <b>variable</b> and two candidate target variables. Such a test enables one to answer whether one <b>source</b> <b>variable</b> is significantly more dependent on the first target variable or the second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC), resulting in a pair of empirical dependence measures (source-target 1, source-target 2). Modeling the covariance between these HSIC statistics leads to a provable more powerful test than the construction of independent HSIC statistics by sub-sampling. The resulting test is consistent and unbiased, and (being based on U-statistics) has favorable convergence properties. The test can be computed in quadratic time, matching the computational complexity of standard empirical HSIC estimators. The effectiveness of the test is demonstrated on several real-world problems: we identify language groups from a multilingual corpus, and we prove that the tumor location is more dependent to gene expression than chromosomal imbalances...|$|E
40|$|A {{new class}} of exact {{solutions}} of Einstein’s field equations with a perfect fluid <b>source,</b> <b>variable</b> gravitational coupling G and cosmological term Λ for FRW spacetime is obtained by considering variable deceleration parameter models for the universe. The nature of the variables G(t), Λ(t) and the energy density ρ(t) have been examined for three cases: (i) exponential (ii) polynomial and (iii) sinusoidal form. The special types of models for dust, Zel’dovich and radiating universe are also mentioned in all these cases. The behaviour of these models of the universe are also discussed {{in the light of}} recent supernovae Ia observations...|$|E
50|$|A cell {{complex and}} its dual {{constitute}} a valid framework {{to describe the}} association of physical variables with the oriented space elements. To be precise, the configuration variables, which are associated with space elements with inner orientation, {{may be associated with}} the elements of the primal complex, while the <b>source</b> <b>variables,</b> which are associated with the space elements with an outer orientation, can be associated with the elements of the dual complex.|$|R
50|$|Although {{solar power}} is mainly covered, the {{principle}} applies generally to <b>sources</b> with <b>variable</b> power: for example, optical power transmission and thermophotovoltaics.|$|R
40|$|Quantifying synergy among {{stochastic}} variables is {{an important}} open problem in information theory. Information synergy occurs when multiple sources together predict an outcome variable better {{than the sum of}} single-source predictions. It is an essential phenomenon in biology such as in neuronal networks and cellular regulatory processes, where different information flows integrate to produce a single response, but also in social cooperation processes as well as in statistical inference tasks in machine learning. Here we propose a metric of synergistic entropy and synergistic information from first principles. The proposed measure relies on so-called synergistic random variables (SRVs) which are constructed to have zero mutual information about individual <b>source</b> <b>variables</b> but non-zero mutual information about the complete set of <b>source</b> <b>variables.</b> We prove several basic and desired properties of our measure, including bounds and additivity properties. In addition, we prove several important consequences of our measure, including the fact that different types of synergistic information may co-exist between the same sets of variables. A numerical implementation is provided, which we use to demonstrate that synergy is associated with resilience to noise. Our measure may be a marked step forward in the study of multivariate information theory and its numerous applications...|$|R
40|$|We {{propose that}} {{country of origin}} has a dual impact on product evaluations, acting as informational cue, but also as <b>source</b> <b>variable,</b> moderating the impact of ads on product evaluations. In support, we find a direct effect of country of origin on product evaluations, and a {{three-way}} interaction between country of origin, claim favorability and ad involvement. Further analyses show that country of origin influences {{the way in which}} consumers respond to moderate and extreme claims under conditions of low and high ad involvement. The dual impact of country of origin on consumer behavior emphasizes its relevance to (international) marketin...|$|E
40|$|Low-density parity-check (LDPC) codes {{with the}} parity-based {{approach}} for distributed joint source channel coding (DJSCC) with decoder side information {{is described in}} this paper. The parity-based approach is theoretical limit achievable. Different edge degree distributions are used for <b>source</b> <b>variable</b> nodes and parity variable nodes. Particularly, the codeword-averaged density evolution (CADE) is presented for asymmetrically correlated nonuniform sources over the asymmetric memoryless transmission channel. Extensive simulations show that the splitting of variable nodes can improve the coding efficiency of suboptimal codes and lower the error floor. Comment: 9 pages, 4 figure, extended version of the published journal pape...|$|E
40|$|Bounliphone W., Gretton A., Tenenhaus A., Blaschko M., ''Kernel {{non-parametric}} {{tests of}} relative dependency'', 8 th international {{conference of the}} ERCIM WG on computational and methodological statistics - CMStatistics 2015, December 12 - 14, 2015, London, UK (invited talk). Tests of dependence are an important tool in statistical analysis, and are widely applied in many data analysis contexts. For many problems in data analysis, however, {{the question of whether}} dependence exists is secondary: there may be multiple dependencies, and the question becomes which dependence is the strongest. We present a novel non parametric statistical which describe hypothesis test of relative dependence between a <b>source</b> <b>variable</b> and two candidate target variables. Such a test enables one to answer whether one <b>source</b> <b>variable</b> is significantly more dependent on the first target variable or the second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC), resulting in a pair of empirical dependence measures (source-target 1, source-target 2). Modeling the covariance between these HSIC statistics leads to a provably more powerful test than the construction of independent HSIC statistics by sub-sampling. The resulting test is consistent and unbiased, and (being based on U-statistics) has favorable convergence properties. The test can be computed in quadratic time, matching the computational complexity of standard empirical HSIC estimators. The effectiveness of the test is demonstrated on several real-world problems: we identify language groups from a multilingual corpus, and we prove that the tumor location is more dependent to gene expression than chromosomal imbalances. status: publishe...|$|E
3000|$|On {{the other}} hand, for the uniform decay of {{solutions}} of the viscoelastic equation with a nonlinear <b>source</b> or <b>variable</b> diffusion coefficient [...]...|$|R
40|$|Monte Carlo is an {{extremely}} powerful method of simulating complex, three dimensional environments without excessive problem simplification. However, it is often time consuming to simulate models in which the source can be highly varied. Similarly difficult are optimization studies involving sources in which many input parameters are variable, such as particle energy, angle, and spatial distribution. Such studies are often approached using brute force methods or intelligent guesswork. One field in which these problems are often encountered is accelerator-driven Boron Neutron Capture Therapy (BNCT) {{for the treatment of}} cancers. Solving the reverse problem of determining the best neutron source for optimal BNCT treatment can be accomplished by separating the time-consuming particle-tracking process of a full Monte Carlo simulation from the calculation of the source weighting factors which is typically performed {{at the beginning of a}} Monte Carlo simulation. By post-processing these weighting factors on a recorded file of individual particle tally information, the effect of changing <b>source</b> <b>variables</b> can be realized in a matter of seconds, instead of requiring hours or days for additional complete simulations. By intelligent source biasing, any number of different source distributions can be calculated quickly from a single Monte Carlo simulation. The source description can be treated as variable and the effect of changing multiple interdependent <b>source</b> <b>variables</b> on the problem's solution can be determined. Though the focus of this study is on BNCT applications, this procedure may be applicable to any problem that involves a <b>variable</b> <b>source...</b>|$|R
40|$|The {{temperature}} variation of a thermister enclosed {{in a small}} cell was calculated. This {{is a problem of}} the heat conduction with a <b>variable</b> <b>source</b> and <b>variable</b> thermal conductivity. According to the result of calculation, the heat transfer coefficient from the thermister to the surrounding medium is almost constant and the time response and sensitivity of this apparatus depend on the voltage applied to the thermister...|$|R
40|$|A {{new class}} of exact {{solutions}} of Einstein's field equations with a perfect fluid <b>source,</b> <b>variable</b> gravitational coupling G and cosmological term Λ for FRW spacetime is obtained by considering variable deceleration parameter models for the universe. The nature of the variables G(t), Λ(t) and the energy density ρ(t) have been examined for three cases : (i) exponential (ii) polynomial and (iii) sinusoidal form. The special types of models for dust, Zel'dovich and radiating universe are also mentioned in all these cases. The behaviour of these models of the universe are also discussed {{in the light of}} recent supernovae Ia observations. Comment: 15 pages, 8 figure...|$|E
40|$|A {{new class}} of exact {{solutions}} of Einstein’s field equations with a perfect fluid <b>source,</b> <b>variable</b> gravitational coupling G and cosmological term / for FRW space-time in higher dimensions is obtained. The nature of the variables G(t), /(t), and the energy density U(t) have been examined for three cases. It is shown that in these models particle horizon exist and the cosmological term is decaying with time. Further, {{it is observed that}} {{new class of}} solutions include some previous cases in four-dimensional space-time. The results of the present studies are well within the range of observational limits. Key words: cosmology, higher dimensions, variable gravitational and cosmological terms. PACS number: 98. 80. -k, 98. 80. Es 1...|$|E
3000|$|It is {{convenient}} {{to assume a}} three-dimensional configuration with observation variable x = (x,y,z), <b>source</b> <b>variable</b> ξ = (ξ,η,ζ [...]), and time [...] t > 0 [...]. Suppose a linear differential operator O describes the dynamic property of a physical system and appropriate interface and boundary conditions relate the field quantities of specified problems. Obviously, for a concrete elastodynamic problem, the linear differential operator O is given by the well-known Navier–Stoke’s field equations. The Green’s function is then defined as the fundamental solution of the system. In other words, for the problem discussed here, the Green’s function corresponds to {{the solution of the}} governing equations as the point source {{takes the form of a}} Dirac delta function in both spatial and temporal domains.|$|E
40|$|Jet flap {{interaction}} acoustic {{data obtained}} statically from a model-scale study of STOL-OTW configurations with a conical nozzle mounted above the wing and using various external deflectors to provide jet-flow attachment are correlated. The acoustic data are correlated {{in terms that}} consider the jet/flap interaction noise contributions associated primarily with fluctuating lift, trailing edge, and configuration wake noise <b>sources.</b> <b>Variables</b> considered include deflector geometry, flap setting and wing size. Finally, the configuration overall noise levels are related to static lift and thrust measurements {{in order to provide}} insight into possible acoustic/aerodynamic performance trade-off benefits...|$|R
40|$|Linear Genetic Programming (LGP) is a Genetic Programming variant {{that uses}} linear {{chromosomes}} for solution encoding. Each LGP chromosome is {{a sequence of}} C language instructions. Each instruction has a destination <b>variable</b> and several <b>source</b> <b>variables.</b> One of the variables is usually chosen to provide {{the output of the}} program. In this paper, we enrich the LGP technique by allowing it to encode multiple solutions for a problem in the same chromosome. Numerical experiments show that the proposed Multi-Solution LGP significantly outperforms the standard Single-Solution LGP on the considered test problems...|$|R
30|$|In {{order to}} assess how much {{variance}} in each dependent measure was accounted for by nationality and how much was actually shared with other <b>sources,</b> eighteen <b>variables</b> were tested as confounding variables.|$|R
