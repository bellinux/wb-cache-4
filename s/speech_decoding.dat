66|140|Public
50|$|RX DTX handler {{performs}} <b>speech</b> <b>decoding</b> {{and comfort}} noise computation.|$|E
50|$|Reception {{done by a}} Software Defined Radio and <b>speech</b> <b>decoding</b> done by {{the program}} DSD 1.7 (Digital Speech Decoder).|$|E
50|$|JRTk {{utilizes}} {{the concept}} of Hidden Markov Models (HMMs) for acoustic modeling and offers many state-of-the-art techniques for acoustic pre-processing, acoustic model training, and <b>speech</b> <b>decoding.</b> Through its flexible, object oriented architecture it allows to configure all components in a very flexible way (e.g., pre-processing steps to execute, HMM topology, training sequence, algorithm parameters, adaptation sequences, etc.), without the need to modify source code or recompile.|$|E
50|$|Tc'a {{are large}} methane-breathing five-eyed yellow snakelike beings, and the chi are yellow arthropod-like creatures. The two species are related {{in a way}} none of the oxygen {{breathers}} understand, but are (presumably) symbiotic. They are very technologically advanced and powerful, although understanding them is tricky at best, since their brains are multi-part and their <b>speech</b> <b>decodes</b> as complex matrices of intertwined meanings. They run the methane side of most space stations.|$|R
5000|$|An {{automatic}} <b>Speech</b> recognizer (ASR) <b>decodes</b> <b>speech</b> into text. Domain-specific recognizers can be configured {{for language}} {{designed for a}} given application. A [...] "cloud" [...] recognizer will be suitable for domains that do not depend on very specific vocabularies.|$|R
40|$|We {{present an}} {{attention}} shift decoding (ASD) method inspired by human speech recognition. In {{contrast to the}} traditional auto-matic speech recognition (ASR) systems, ASD <b>decodes</b> <b>speech</b> inconsecutively using reliability criteria; the gaps (unreliable <b>speech</b> regions) are <b>decoded</b> with the evidence of islands (reli-able speech regions). On the BU Radio News Corpus, ASD pro-vides significant improvement (2. 9 % absolute) over the baseline ASR results when it is used with oracle island-gap informa-tion. At {{the core of the}} ASD method is the automatic island-gap detection. Here, we propose a new feature set for automatic island-gap detection which achieves 83. 7 % accuracy. To cope with the imperfect nature of the island-gap classification, we also propose a new ASD algorithm using soft decision. The ASD with soft decision provides 0. 4 % absolute (2. 2 % relative) improvement over the baseline ASR results when it is used with automatically detected islands and gaps. Index Terms: <b>speech</b> recognition, <b>decoding,</b> attention, island. 1...|$|R
50|$|In late 2009, Li Deng invited Geoff Hinton to {{work with}} him and colleagues at Microsoft to apply deep {{learning}} to speech recognition. They co-organized the 2009 NIPS Workshop on Deep Learning for Speech Recognition. The workshop was motivated by the limitations of deep generative models of speech, and the possibility that the big-compute, big-data era warranted a serious try of deep neural nets (DNN). It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, Deng et al. at Microsoft soon after discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more advanced generative model-based speech recognition systems. This finding was verified by other research groups subsequently. Further, the nature of recognition errors produced by the two types of systems was found to be characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time <b>speech</b> <b>decoding</b> system deployed by all major speech recognition players.|$|E
40|$|Abstract—A new {{parameter}} for {{estimating the}} difficulty of a continuous speech recognition task, called <b>speech</b> <b>decoding</b> difficulty, is presented in this work. It is obtained from the language model defined for the recognition task and the phonetic similarity between the transcriptions of the words {{that make up the}} vocabulary used. Two variants of the proposed task difficulty measure are introduced: ideal <b>speech</b> <b>decoding</b> difficulty (ISDD), which is uninfluenced by practical considerations on the recognition system implemented, and a second, more realistic variant, called practical <b>speech</b> <b>decoding</b> difficulty (PSDD) to study the performance of a specific recognition system when confronting a given task. Index Terms — Perplexity, recognition task, speech recognition, task difficulty. I...|$|E
40|$|The {{problem of}} <b>speech</b> <b>decoding</b> is {{considered}} {{here in a}} Decision Theoretic framework and a modified <b>speech</b> <b>decoding</b> procedure to minimize the expected risk under a general loss function is formulated. A specific word error rate loss function is considered and an implementation in an N-best list rescoring procedure is presented. Methods for estimation of {{the parameters of the}} resulting decision rules are provided for both supervised and unsupervised training. Preliminary experiments on an LVCSR task show small but statistically significant error rate improvements. 1...|$|E
40|$|Recent {{advances}} in <b>speech</b> fragment <b>decoding</b> techniques This paper addresses {{the problem of}} recognising speech {{in the presence of}} a competing speaker. We employ a <b>speech</b> fragment <b>decoding</b> technique that treats segregation and recognition as coupled problems. Data-driven techniques are used to segment a spectro-temporal representation into a set of spectro-temporal fragments, such that each fragment is dominated by one or other of the speech sources. A speech fragment decoder is used which employs missing data techniques and clean speech models to simultaneously search for the set of fragments and the word sequence that best matches the target speaker model. The paper reports recent {{advances in}} this technique, and presents an evaluation based on artificially mixed speech utterances. The fragment decoder produces significantly lower error rates than a conventional recogniser, and mimics the pattern of human performance whereby performance increases as the target-masker ratio is reduced below- 3 dB. 1...|$|R
40|$|Vector adaptive/predictive {{technique}} for digital encoding of <b>speech</b> signals yields <b>decoded</b> <b>speech</b> {{of very good}} quality after transmission at coding rate of 9. 6 kb/s and of reasonably good quality at 4. 8 kb/s. Requires 3 to 4 million multiplications and additions per second. Combines advantages of adaptive/predictive coding, and code-excited linear prediction, yielding speech of high quality but requires 600 million multiplications and additions per second at encoding rate of 4. 8 kb/s. Vector adaptive/predictive coding technique bridges gaps in performance and complexity between adaptive/predictive coding and code-excited linear prediction...|$|R
40|$|JUS++ {{which has}} been {{developed}} in our laboratory for over 20 years and recently fully reimplemented from scratch. SPOJUS++ employs a context-dependent Hidden Markov Model (HMM) as an acoustic model and an N-gram model as a language model to <b>decode</b> <b>speech.</b> SPOJUS++ has many novel features including a dynamic expansion of linear dictionary, a use of likelihood index for efficient handling of the inter-word dependency and one pass decoding. SPOJUS++ can <b>decode</b> <b>speech</b> in real time on current standard PC without painful degradation of the performance even though it uses a context-dependent HMM and a high order N-gram language model (N> = 3). Also, SPOJUS++ can construct a confusion network which leads to word error rate minimization recog-nition. Constructed confusion networks {{can be used in}} many kinds of post-processing applications which require automatic speech recognition results. We evaluated SPOJUS++ in terms of word accuracy, real time factor and search error. Experimental results showed that SPOJUS++ is comparable to state-of-the-arts. Key–Words: Automatic <b>speech</b> recognition, LVCSR, <b>decode...</b>|$|R
30|$|Both (non-native) <b>speech</b> <b>decoding</b> and {{utterance}} verification {{are the key}} {{aspects of}} this research. We will now relate our research on both problems to other recent work.|$|E
40|$|Children's {{abilities}} {{to process the}} phonological structure of words are important predictors of their literacy development. In the current study, we examined the interrelatedness between implicit (i. e., <b>speech</b> <b>decoding)</b> and explicit (i. e., phonological awareness) phonological abilities, and especially the role therein of lexical specificity (i. e., the ability to learn to recognize spoken words based on only minimal acoustic-phonetic differences). We tested 75 Dutch monolingual and 64 Turkish-Dutch bilingual kindergartners. SEM analyses showed that <b>speech</b> <b>decoding</b> predicted lexical specificity, which in turn predicted rhyme awareness in the first language learners but phoneme awareness in the second language learners. Moreover, in the latter group there was an impact of the second language: Dutch <b>speech</b> <b>decoding</b> and lexical specificity predicted Turkish phonological awareness, which in turn predicted Dutch phonological awareness. We conclude that language-specific phonological characteristics underlie different patterns of transfer from implicit to explicit phonological abilities in first and second language learners...|$|E
40|$|Abstract. Context-sensitive speech recognizers use {{environment}} or discourse information to influence language model probabilities used in <b>speech</b> <b>decoding.</b> This is usually done by switching language models between utterances. This paper explores {{the use of}} a continuously context-sensitive language model that uses incremental interpretation to update context at every time step in decoding. Because it only explores the world model incrementally, this semantic model {{does not need to be}} pre-computed, raising the possibility of representing continuouslyvariable concepts as semantic referents (such as time points and measurements, or real numbers themselves), and supporting dynamic reasoning about consequences of actions during <b>speech</b> <b>decoding.</b> ...|$|E
40|$|This paper {{presents}} a robust speech recognition technique called audio-visual <b>speech</b> fragment <b>decoding</b> (AV-SFD), {{in which the}} visual signal is exploited both as a cue for source separation and as a carrier of phonetic information. The model builds on the existing audio-only SFD technique which, based on the auditory scene analysis account of perceptual organisation, works by combining a bottom-up layer which identifies sound fragments, and a model-driven layer which searches for fragment groupings that {{can be interpreted as}} recognisable speech utterances. In AV-SFD, the visual signal is used in the model-driven stage improving the ability of the decoder to distinguish between foreground and background fragments. The system has been evaluated using an audio-visual version of Pascal Speech Separation Challenge. At low SNRs, recognition error rates are reduced by around 20 % relative to the performance of a conventional multistream AV-ASR system. Index Terms: audio visual speech recognition, <b>speech</b> fragment <b>decoding</b> 1...|$|R
5000|$|Her research, {{summarised}} in {{the book}} Native Listening, centres on human listeners’ recognition of spoken language, and in particular on how the brain’s processes of <b>decoding</b> <b>speech</b> are shaped by language-specific listening experience.|$|R
40|$|CONCLUSION ILAH, LSR and LILAH from [4] {{exceed the}} {{performance}} of the optimized baseline on unencoded speech, and at clipping levels between- 3 and- 14 dBFS on <b>decoded</b> <b>speech,</b> an important range in mobile telephony applications. Performance is maintained at SNRs down to 5 dB. The averaging nature of histogram approach in ILAH, and the spectral approach of LSR both provide robustness to noise. The baseline method with ε = 0. 71 (optimized) in <b>decoded</b> <b>speech</b> generates many FPs, and loses accuracy at high ODF because it does not adapt to each speaker, whilst the F 1 score improves at high ODF because it crudely labels all samples of significant amplitude as clipped...|$|R
40|$|Abstra t | In digital mobile {{communication}} usu-ally <b>speech</b> <b>decoding</b> algorithms are closely combined with error concealment techniques {{to reduce the}} sub-jective effects of residual bit errors. In general these error concealment techniques require reliability in-formation which {{is provided by the}} channel decoder. The amount of additional information may vary from 1 bit per received frame, e. g. in GSM a BFI (bad frame indicator) for a speech frame of 260 bits (Full Rate (FR) codec) or for a frame of 244 bits (Enhanced Full Rate (EFR) codec), up to several bits per re-ceived bit for Softbit <b>Speech</b> <b>Decoding.</b> In applica-tions such as the GSM infrastructure where the chan-nel decoder and the speech decoder are placed at spa-tially distant locations, the additional data rate re-quired to transmit the reliability information might not be available. Therefore, we break up the close connection between error concealment and <b>speech</b> <b>decoding</b> and move the error concealment unit to the base transceiver station where the channel decoder is lo-cated. As the error concealment algorithm pro-duces estimated speech parameters with high ampli-tude resolution a re-quantization of these estimated speech parameters is required which implies a loss in estimation quality. In this paper it is shown that this loss in quality is negligible in most practical cases. In addition, methods for a reduction of the re-quantization loss are discussed. I...|$|E
30|$|As {{discussed}} in Section 2.1, {{it has been}} observed in several studies that by adapting or retraining native acoustic models (AM) with non-native <b>speech,</b> <b>decoding</b> performance can be increased. To investigate whether this is also the case in a constrained task as described in this paper, we retrained the baseline acoustic models with non-native speech.|$|E
30|$|We {{evaluated}} the <b>speech</b> <b>decoding</b> setups using the utterance error rate (UER), {{which is the}} percentage of utterances where the 1 -Best decoding result deviates from the transcription. Filled pauses are not taken into account during evaluation. That is, decoding results and reference transcriptions were compared after deletion of filled pauses. For each UER the 95 % confidence interval was calculated to evaluate whether UERs between conditions were significantly different.|$|E
40|$|In {{this paper}} we {{describe}} our design, implementation, and first {{results of a}} prototype connected-phoneme-based speech recognition system on the Cell Broadband Engine{trademark} (Cell/B. E.). Automatic <b>speech</b> recognition <b>decodes</b> <b>speech</b> samples into plain text (other representations are possible) and must process samples at real-time rates. Fortunately, the computational tasks involved in this pipeline are highly data-parallel and can receive significant hardware acceleration from vector-streaming architectures such as the Cell/B. E. Identifying and exploiting these parallelism opportunities is challenging, but also critical to improving system performance. We observed, from our initial performance timings, that a single Cell/B. E. processor can recognize speech from thousands of simultaneous voice channels in real time [...] a channel density that is orders-of-magnitude greater than the capacity of existing software speech recognizers based on CPUs (central processing units). This result emphasizes the potential for Cell/B. E. -based speech recognition and will likely lead to the future development of production speech systems using Cell/B. E. clusters...|$|R
40|$|A {{method for}} {{optimising}} LPC filters in linear prediction based speech coders is described. The optimisation process compensates for errors incurred through coding the excitation signal, providing {{an improvement in}} the quality of the <b>decoded</b> <b>speech,</b> with no increase in bit rate. Peer ReviewedPostprint (published version...|$|R
5000|$|Mobile {{termination}} (MT) - offers common functions such as: {{radio transmission}} and [...] handover, [...] <b>speech</b> encoding and <b>decoding,</b> error detection and correction, [...] signalling {{and access to}} the SIM. The IMEI code is attached to the MT. It is equivalent to the network termination of an ISDN access.|$|R
40|$|I n digital mobile radio systems {{the speech}} quality can be {{degraded}} severly if the channel decoder produces residual bit errors, e. g. due to heavy burst errors on the channel. A new combined speech extrapolation nnd error detection algorithm is presented which moslly improves the speech signi/cantly {{in case of}} residual bit errors. This algorithm, {{which is part of}} [lie <b>speech</b> <b>decoding</b> process, uses a pos-teriori-probabilities of speech pnrameters. 1...|$|E
40|$|In {{this paper}} we present {{the design and}} the {{implement}} of the language understanding component of a Chinese spoken language dialogue system EasyNav. In pursuing the coherence {{with the goal of}} understanding, we design the structure of system with <b>speech</b> <b>decoding</b> and language understanding integrated closely. Thus the language understanding component need to be restrictive besides portable. Actually we implement a general syntactic parser and domain-specific semantic parser for the purpose. The grammar rules written for understanding are suitable for spoken language. The feature of spoken language also exists throughout the system. 1...|$|E
40|$|International audienceEarly {{linguistic}} {{experience has}} an impact on the way we decode audiovisual speech in face-to-face communication. The present study examined whether differences in visual <b>speech</b> <b>decoding</b> could be linked to a broader difference in face processing. To identify a phoneme we have to do an analysis of the speaker's face to focus on the relevant cues for <b>speech</b> <b>decoding</b> (e. g., locating the mouth with respect to the eyes). Face recognition processes were investigated through two classic effects in face recognition studies: the Other-Race Effect (ORE) and the Inversion Effect. Bilingual and monolingual participants did a face recognition task with Caucasian faces (own race), Chinese faces (other race), and cars that were presented in an Upright or Inverted position. The results revealed that monolinguals exhibited the classic ORE. Bilinguals did not. Overall, bilinguals were slower than monolinguals. These results suggest that bilinguals' face processing abilities differ from monolinguals'. Early exposure to more than one language may lead to a perceptual organization that goes beyond language processing and could extend to face analysis. We hypothesize that these differences could be {{due to the fact that}} bilinguals focus on different parts of the face than monolinguals, making them more efficient in other race face processing but slower. However, more studies using eye-tracking techniques are necessary to confirm this explanation...|$|E
25|$|APD is a {{difficult}} disorder to detect and diagnose. The subjective symptoms that lead to an evaluation for APD include an intermittent inability to process verbal information, leading the person to guess {{to fill in the}} processing gaps. There may also be disproportionate problems with <b>decoding</b> <b>speech</b> in noisy environments.|$|R
40|$|The {{implementation}} of a programmable microprocessor for real time speech processing is described. The design of a fast special-purpose computer operating at 150 nsec per instruction time reduces the problem of <b>speech</b> encoding and <b>decoding</b> to a software problem. Each instruction is a twelve-bit word, and the information conveyed by a word is explained. The microprocessor is controlled by two clocks - the A/D clock, or input sampling clock, which loads the digital input into the input register and presets the counter to a value set by the external switches; and the instruction clock, or system clock, which operates at 6 MHz. At this system clock rate and a sampling clock rate of 30 KHz, 200 instructions can be executed between samples, and for many <b>speech</b> encoding and <b>decoding</b> algorithms, 200 instructions are more than enough. The microcomputer {{is being used to}} test various delta modulator encoding algorithms...|$|R
40|$|In this paper, an {{efficient}} method for language model look-ahead probability generation is presented. Traditional methods generate language model look-ahead (LMLA) probabilities for each node in the LMLA tree recursively in a bottom to up manner. The new method {{presented in this}} paper makes use of the sparseness of the n-gram model and starts the process of generating an n-gram LMLA tree from a backoff LMLA tree. Only a small number of nodes are updated with explicitly estimated LM probabilities. This speeds up the bigram and trigram LMLA tree generation by a factor of 3 and 12 respectively. Index: language model, <b>Speech</b> Recognition, <b>decoding</b> 1...|$|R
40|$|This paper {{presents}} {{some recent}} improvements in automatic transcription of Italian broadcast news obtained at ITCirst. A first preliminary activity {{was carried out}} in order to develop a suitable speech corpus for the Italian language. The resulting corpus, formed by recordings covering 30 hours of radio news, was exploited for developing a baseline system for transcription of broadcast news. The system performs in different stages: acoustic segmentation and classification, speaker clustering, acoustic model adaptation and <b>speech</b> <b>decoding.</b> Major recent advances allowing performance improvement concern with speech segmentation and clustering, acoustic modeling, acoustic model adaptation and the language model...|$|E
40|$|How do {{children}} use phonological {{knowledge about}} spoken language in acquiring literacy? Phonological precursors of literacy include phonological awareness, <b>speech</b> <b>decoding</b> skill, and lexical specificity (i. e., {{the richness of}} phonological representations in the mental lexicon). An intervention study investigated whether early literacy skills can be enhanced by training lexical specificity. Forty-two prereading 4 -year-olds {{were randomly assigned to}} either an experimental group that was taught pairs of new words that differed minimally or a control group that received numeracy training. The experimental group gained on a rhyme awareness task, suggesting that learning phonologically specific new words fosters phonological awareness...|$|E
40|$|Gaussian Mixture Model (GMM) {{computation}} {{is known}} {{to be one of the}} most computation-intensive components in <b>speech</b> <b>decoding.</b> In our previous work, context-independent model based GMM selection (CIGMMS) was found to be an effective way to reduce the cost of GMM computation without significant loss in recognition accuracy. In this work, we propose three methods to further improve the performance of CIGMMS. Each method brings an additional 5 - 10 % relative speed improvement, with a cumulative improvement up to 37 % on some tasks. Detailed analysis and experimental results on three corpora are presented. 1...|$|E
50|$|All H.323 {{terminals}} providing {{video communications}} shall {{be capable of}} encoding and decoding video according to H.261 QCIF. All H.323 terminals shall have an audio codec and shall be capable of encoding and <b>decoding</b> <b>speech</b> according to ITU-T Rec. G.711. All terminals shall be capable of transmitting and receiving A-law and μ-law. Support for other audio and video codecs is optional.|$|R
40|$|Anovel {{variable}} rate {{linear predictive coding}} (LPC) parameter quantization scheme is proposed in which linear prediction is done by using either the current (forward LPC) or previously <b>decoded</b> (backward LPC) <b>speech</b> blocks. The proposed LPC quantization scheme was integrated into the FS 1016 Federal Standard CELP coder. Signi cant LPC bit rate reduction is achieved without compromising the <b>decoded</b> <b>speech</b> quality...|$|R
40|$|How {{the human}} {{auditory}} system extracts perceptually relevant acoustic features of speech is unknown. To address this question, we used intracranial recordings from nonprimary auditory cortex {{in the human}} superior temporal gyrus to determine what acoustic information in speech sounds can be reconstructed from population neural activity. We found that slow and intermediate temporal fluctuations, such as those corresponding to syllable rate, were accurately reconstructed using a linear model based on the auditory spectrogram. However, reconstruction of fast temporal fluctuations, such as syllable onsets and offsets, required a nonlinear sound representation based on temporal modulation energy. Reconstruction accuracy was highest {{within the range of}} spectro-temporal fluctuations that {{have been found to be}} critical for <b>speech</b> intelligibility. The <b>decoded</b> <b>speech</b> representations allowed readout and identification of individual words directly from brain activity during single trial sound presentations. These findings reveal neural encoding mechanisms of speech acoustic parameters in higher order human auditory cortex. © 2012 Pasley et al...|$|R
