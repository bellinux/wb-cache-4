31|245|Public
25|$|Barcode {{scanners}} {{are relatively}} low cost and extremely accurate compared to key-entry, with only about 1 <b>substitution</b> <b>error</b> in 15,000 to 36 trillion characters entered. The exact error rate {{depends on the}} type of barcode.|$|E
50|$|One {{problem with}} using a generic formula {{such as the}} one above, however, is that no account is taken of the effect that {{different}} types of error may have on the likelihood of successful outcome, e.g. some errors may be more disruptive than others and some may be corrected more easily than others. These factors are likely to be specific to the syntax being tested. A further problem is that, even with the best alignment, the formula cannot distinguish a <b>substitution</b> <b>error</b> from a combined deletion plus insertion error.|$|E
50|$|Specifically, when {{creating}} a candidate set for overload resolution, some (or all) candidates of that set {{may be the}} result of instantiated templates with (potentially deduced) template arguments substituted for the corresponding template parameters. If an error occurs during the substitution of a set of arguments for any given template, the compiler removes the potential overload from the candidate set instead of stopping with a compilation error, provided the <b>substitution</b> <b>error</b> is one the C++ standard grants such treatment. If one or more candidates remain and overload resolution succeeds, the invocation is well-formed.|$|E
2500|$|High {{proportion}} of double <b>substitution</b> <b>errors</b> (two separate single <b>substitution</b> <b>errors</b> {{in the same}} string, for example 7234587 for 1234567) ...|$|R
40|$|We {{extend the}} n-gram-based {{data-driven}} prediction approach (Elghafari, Meurers and Wunsch, 2010) to identify function word errors in non-native academic texts {{as part of}} the Helping Our Own (HOO) Shared Task. We focus on <b>substitution</b> <b>errors</b> for four categories: prepositions, determiners, conjunctions, and quantifiers. These error types make up 12 % of the errors annotated in the HOO training data. In our best submission in terms of the error detection score, we detected 67 % of preposition and determiner <b>substitution</b> <b>errors,</b> 40 % of conjunction <b>substitution</b> <b>errors,</b> and 33 % of quantifier <b>substitution</b> <b>errors.</b> For approximately half of the errors detected, we were also able to provide an appropriate correction. ...|$|R
40|$|Error {{correction}} of noisy reads obtained from high-throughput DNA sequencers {{is an important}} problem since read quality significantly affects downstream analyses such as detection of genetic variation and the complexity and success of sequence assembly. Most of the current error correction algorithms are only capable of recovering <b>substitution</b> <b>errors.</b> In this work, Pindel, an algorithm that simultaneously corrects insertion, deletion and <b>substitution</b> <b>errors</b> in reads from next generation DNA sequencing platforms is presented. Pindel corrects insertion, deletion and <b>substitution</b> <b>errors</b> by modelling the sequencer output as emissions of an appropriately defined Hidden Markov Model (HMM). Reads are corrected to the corresponding maximum likelihood paths using an appropriately modified Viterbi algorithm. When compared with Karect and Fiona, the top two current algorithms capable of correcting insertion, deletion and <b>substitution</b> <b>errors,</b> Pindel exhibits superior accuracy {{across a range of}} datasets...|$|R
50|$|Assemblathon 1 was {{conducted}} in 2011 and featured 59 assemblies from 17 different groups and the organizers. The goal of this Assembalthon was to most accurately and completely assemble a genome that consisted of two haplotypes (each with three chromosomes of 76.3, 18.5, and 17.7 Mb, respectively) that was generated using Evolver. Numerous metrics {{were used to assess}} the assemblies, including: NG50 (point at which you reach 50% of the total genome size when scaffold lengths are summer from the longest to the shortest), LG50 (number of scaffolds that are greater than, or equal to, the N50 length), genome coverage, and <b>substitution</b> <b>error</b> rate.|$|E
5000|$|Mayor Menino {{was known}} for his {{distinctive}} voice, thick Boston accent and speech errors, some of which are malapropisms. The examples here also include substitution, deletion, and addition or epenthesis, which means whole words are used in place of the intended word, portions of a word are eliminated, or portions of a word are added into the intended word, respectively. As a result of these various errors, some commentators (such as conservative radio show host and author Howie Carr) refer to the mayor as [...] "Mumbles Menino" [...] or [...] "Mayor Mumbles." [...] A typical example of one of his speech errors involves a reference to Boston's parking shortage as [...] "an Alcatraz around my neck" [...] (meaning, instead, an albatross around his neck), which is a <b>substitution</b> <b>error.</b>|$|E
40|$|Abstract—As {{different}} text {{input devices}} lead to different typing error patterns, considering the device characteristics when designing an error correction mechanism {{can lead to}} significantly improved results. In this paper, we propose and evaluate a spelling algorithm specifically designed for a five-key chording keyboard. It {{is based on the}} maximum a posteriori probability criterion, taking into account a dictionary model and the probabilities that one character is typed for another. These probabilities are determined experimentally. In our experiment, the proposed method reduced the <b>substitution</b> <b>error</b> rate from 7. 60 % to 1. 59 %. As comparison, MsWord and iSpell reduced the <b>substitution</b> <b>error</b> rates to 3. 12 % and 3. 94 %, respectively. Keywords—error correction; confusion matrix; maximum a posteriori probability; chording keyboard I...|$|E
5000|$|<b>Substitution</b> <b>errors</b> {{involve a}} clear phonological substitution, such as [...] "ragon" [...] for wagon.|$|R
5000|$|<b>Substitution</b> <b>errors,</b> for instance, reveal {{parts of}} the {{organization}} {{and structure of the}} mental lexicon.|$|R
2500|$|All single <b>substitution</b> <b>errors</b> (the <b>substitution</b> of {{a single}} {{character}} for another, for example 4234 for 1234) ...|$|R
30|$|The beat {{detection}} {{errors are}} divided into three classes: substitution, insertion and deletion errors. <b>Substitution</b> <b>error</b> means that a beat is poorly estimated {{in terms of the}} tempo or bar-position. Insertion errors and deletion errors are false-positive and false-negative estimations. We assume that a player does not know the other's score, thus one estimates score position by number of beats {{from the beginning of the}} performance. Beat insertions or deletions undermine the musical ensemble because the cumulative number of beats should be correct or the performers will lose synchronization. Algorithm 1 shows how to detect inserted and deleted beats. Suppose that a beat-tracker correctly detects two beats with a certain false estimation between them. When the method just incorrectly estimates a beat there, we regard it as a <b>substitution</b> <b>error.</b> In the case of no beat or two beats there, they are counted as a deleted or inserted beats, respectively.|$|E
40|$|Abstract — We survey some of {{our recent}} results on {{communication}} over channels with varying sampling rate. Varying sampling rate is caused by inadequate timing recovery and {{can result in a}} synchronization error, exhibited as a repetition or a deletion of a symbol. We review number-theoretic constructions and the upper bounds of sets of strings capable of overcoming certain number of synchronization errors. We also review results on how to use some practical <b>substitution</b> <b>error</b> correcting codes when the varying sampling rate causes a synchronization error. In particular we present the results for the Reed-Muller(1,m) code and the array-based LDPC codes. I...|$|E
40|$|The paper {{describes}} a neural network and expert system model for conflict resolution of unconstrained handwritten characters and it completely resolves the confusion between the conflicting characters. The basic recognizer is the neural network. The neural network classifier {{is a combination}} of a modified self-organizing map (MSOM) and learning vector quantization (LVQ). It solves most cases, but fails in certain confusing cases. The expert system, the second recognizer, resolves the confusions generated by the neural network. The results obtained from this two-tier architecture are compared with the comments collected from an experiment conducted with a group of human experts specializing in unconstrained handwritten character recognition. The <b>substitution</b> <b>error</b> is eliminate...|$|E
40|$|We {{report on}} an Italian brain-damaged patient with {{impaired}} written spelling. The patient’s errors, in different fonts and scripts, consist mainly of letter substitutions (e. g., filo [thread] fi TILO). The results of various tests indicate that letter <b>substitution</b> <b>errors</b> arise {{because of a}} deficit in accessing the letter-form representations supporting written spelling. Letter substi-tutions occurred predominantly between letters with common strokes (e. g., C and G; b and p). Similarities in terms of global letter shape or letter sound were not valid predictors of letter <b>substitution</b> <b>errors.</b> Letter frequency, consonant–vowel status, and letter gemination were factors affecting letter <b>substitution</b> <b>errors.</b> The results of our investigation suggest that informa-tion about letter strokes are stored {{at the level of}} letter-form representations, and that access to these representations is sensitive to letter frequency. The results further indicate that letter-form representations do not specify whether a letter is a consonant or a vowel, or is a geminate. ª 2002 Elsevier Science (USA) A few studies have recently documented brain-damaged patients with spelling im-pairments consisting primarily of letter <b>substitution</b> <b>errors</b> (Black, Behrmann, Bass, &...|$|R
5000|$|Some <b>substitution</b> <b>errors</b> {{which are}} based on phonological {{similarities}} supply evidence that the mental lexicon is also organized in terms of sound.|$|R
40|$|Shape unifying {{is a very}} {{efficient}} preprocessing technique used in lossy SPM-JBIG 2 systems. It permits isolated errors between the current bitmap and its reference to improve refinement coding efficiency. Compared to lossless coding, it can improve compression by about 32 % while causing very little visual information loss. When bigger error clusters are permitted in shape unifying, further compression gain can be achieved but {{at the price of}} more noticeable visual information loss and even character <b>substitution</b> <b>errors.</b> In this paper we propose a feature monitored shape unifying procedure that can significantly lower the risk of <b>substitution</b> <b>errors</b> when permitting bigger errors. Experiments show that, compared to the unmonitored shape unifying, the feature monitored version can suppress more than 2 / 3 of all <b>substitution</b> <b>errors</b> while achieving additional compression improvements of 30 - 40 %. 1...|$|R
40|$|For spoken term detection, it {{is crucial}} to {{consider}} out-of-vocabulary (OOV) and the mis-recognition of spoken words. Therefore, various sub-word unit based recognition and re-trieval methods have been proposed. We also proposed a dis-tant n-gram indexing/retrieval method for spoken queries, which is based on a syllable n-gram and incorporates a dis-tance metric in a syllable lattice. The distance represents confidence score of the syllable n-gram assumed the recog-nition error such as <b>substitution</b> <b>error,</b> insertion error and deletion error. To address spoken queries, we propose a com-bination of candidates obtained through some ASR systems which are based on syllable or word units. We run some ex-periments on the NTCIR- 11 SpokenQuery&Doc Task and report the evaluation results...|$|E
40|$|The paper {{describes}} a three-dimensional (3 -D) neural network recognition system for conflict resolution {{in recognition of}} unconstrained handwritten numerals. This neural network classifier {{is a combination of}} modified self-organizing map (MSOM) and learning vector quantization (LVQ). The 3 -D neural network recognition system has many layers of such neural network classifiers and the number of layers forms the third dimension. The Experiments are conducted employing SOM, MSOM, SOM and LVQ, and MSOM and LVQ networks. These experiments on a database of unconstrained handwritten samples show that the combination of MSOM and LVQ performs better than other networks in terms of classification, recognition and training time. The 3 -D neural network eliminates the <b>substitution</b> <b>error...</b>|$|E
40|$|Nucleotide <b>substitution</b> <b>error</b> {{frequencies}} {{were determined}} for several specific guanine base {{positions in the}} genomes of cloned vesicular stomatitis virus populations. Predetermined sites were examined in coding regions for the N, M, and L proteins and at a site in the genome 5 '-end regulatory region. Misincorporation frequencies were estimated {{to be on the}} order of 10 (- 3) to 10 (- 4) at all positions analyzed. Isolates taken from virus populations after disruption of equilibrium conditions displayed replicase fidelity similar to that of cloned wild-type vesicular stomatitis virus. These mutation frequencies apply to all virus genomes present, including viruses rendered nonviable by lethal mutations. At one selected site in the N gene, two of three G [...] N base substitutions generated lethal nonsense mutations, yet their frequency was also very high. Biological implications for rapid virus evolution are discussed...|$|E
3000|$|It {{is worth}} {{mentioning}} that the <b>errors</b> are just <b>substitution</b> <b>errors</b> in both methods because the polyphony number is known. In this case, the output vector ([...] [...]...|$|R
40|$|A new {{block code}} is {{introduced}} which {{is capable of}} correcting multiple insertion, deletion and <b>substitution</b> <b>errors</b> present in a single block. An inner code resilient to synchronisation errors provides soft inputs to an outer code capable of correcting <b>substitution</b> <b>errors.</b> The decoder does not require knowledge of the block boundaries. Many coding methods have been proposed to cope with synchronisation errors. Most {{fall into one of}} two categories, either correcting limited synchronisation errors [1, 2] or imposing run-length limiting constraints [3]. In this paper we present a block code capable of correcting multiple synchronisation and <b>substitution</b> <b>errors</b> using a probabilistic decoder. We apply the code to a model binary channel with an input queue. At each use, one of three events occurs. With probability P i a random bit is inserted into the received stream. With probability Pd the next queued bit is deleted. With probability P t = (1 Pd P i) the next queued bit is transmitted [...] ...|$|R
40|$|We {{explore the}} {{features}} of a corpus of naturally occurring word <b>substitution</b> speech <b>errors.</b> Words are replaced by more imageable competitors in semantic <b>substitution</b> <b>errors</b> but not in phonological <b>substitution</b> <b>errors.</b> Frequency effects in these errors are complex and the details prove difficult for any model of speech production. We argue that word frequency mainly affects phonological errors. Both semantic and phonological substitutions are constrained by phonological and syntactic similarity between the target and intrusion. We distinguish between associative and shared-feature semantic <b>substitutions,</b> Associative <b>errors</b> originate from outside the lexicon, while shared-feature errors arise within the lexicon and occur when particular properties of the targets make them less accessible than the intrusion. Semantic errors arise early while accessing lemmas from a semantic-conceptual input, while phonological errors arise late when accessing phonological forms from lemmas. Semantic errors are primarily sensitive to {{the properties of the}} semantic field involved, whereas phonological errors are sensitive to phonological properties of the targets and intrusions...|$|R
40|$|The Deletion-Insertion Correcting Code {{construction}} {{proposed by}} Davey and MacKay {{consists of an}} inner code that recovers synchronization and an outer code that provides <b>substitution</b> <b>error</b> protection. The inner code uses low-weight codewords which are added (modulo two) to a pilot sequence. The receiver is able to synchronise on the pilot sequence {{in spite of the}} changes introduced by the added codeword. The original bit-level formulation of the inner decoder assumes that all bits in the sparse codebook are identically and independently distributed. Not only is this assumption inaccurate, but it also prevents the use of soft a- priori input to the decoder. We propose an alternative symbol-level inner decoding algorithm that takes the actual codebook into account. Simulation results show that the proposed algorithm has an improved performance with only a small penalty in complexity, and it allows other improvements using inner codes with larger minimum distance...|$|E
40|$|The {{aim of this}} {{research}} {{study was to identify}} the characteristics of precision, comprehension and reading speed in children with Reading Learning Disability. Fourteen children, between the ages of 8 and 11 years old, participating in the study, were diagnosed according to the DSM IV–Tr criteria. They were assessed through the reading test taken from the Evaluación Neuropsicológica Infantil (ENI) (Neuropsychological Assessment for Children). Results showed specific characteristics of the disability according to gender, age and school level. Performance was low on text reading precision and reading speed. The most frequent errors were literal and derivational <b>substitution,</b> <b>error</b> in functional word and failure in reading rhythm. The study concluded that the Reading Learning Disability has diverse manifestations and that diagnostic criteria may not include processes underlying the disability. It is recommended that assessment processes emphasize tasks where major difficulties can be found and that rehabilitation strategies respond to particular characteristics of the disability...|$|E
40|$|The study {{examined}} the articulation error patterns of 120 Spanish-speaking children aged three to five years, seven months. The common misarticulations produced by the children are described. Interest in normative data for languages other than English has in-creased {{over the past several}} years. The lack of information regarding the acquisition of speech sounds in Spanish was noted by Anderson (1981). More recently, Cole (1985) also noted the need for developmental data on minority language populations. Studies investigating the age of acquisition of Spanish phonemes have been conducted in recent years (Mason, Smith & Hinshaw, 1976; Linares, 1981; Jimenez, 1986). Infor-mation regarding the age of acquisition of speech sounds is essential in order {{to determine whether or not}} a child has an articulation disorder. A sound <b>substitution</b> <b>error</b> is viewed differently by the speech clinician depending upon the age of the child producing the error. For example, a three-year-old English-speaking child saying &dquo;wabbit&dquo; for &dquo;rabbit&dquo; woul...|$|E
40|$|The work {{presented}} here investigated how word forms are stored and accessed for language production. While {{the study of}} single word reading has made significant use {{of the concept of}} lexical neighborhoods (the number of similar words there are in the language), the study of word production has not. Data from natural and experimental investigations of both tip-of-the-tongue (TOT) states and word <b>substitution</b> <b>errors</b> were used to evaluate the organizational system for word forms. This type of detailed investigation should aid in discovering the relevant form parameters for determining similarity for production. It was shown that although the parameters of form that are relevant for TOT states and word <b>substitution</b> <b>errors</b> are similar, <b>substitution</b> <b>errors</b> provide a better source of data regarding lexical retrieval. This is because TOT data reflect a variety of problem solving mechanisms which likely do not play a role in lexical retrieval under normal circumstances. The evaluation of specific form parameters overlapping between <b>substitution</b> <b>errors</b> and targets suggested significantly greater degree of similarity than had been reported previously. Furthermore, experimental investigation of word form overlap indicated that the method employed here and elsewhere does not tap the same processes as the natural data. Experimental investigations of TOT states for both real and novel targets suggested that new techniques employed in this dissertation may be useful in evaluating lexical parameters involved in TOT states and in the process of lexicalization in adults...|$|R
30|$|Insertions (e.g. ‘thaat’ {{instead of}} ‘that’) are not {{considered}} errors if the inserted character(s) are repeats of the previous correct character. In other cases (e.g. ‘thast’) the inserted characters are considered as additional <b>substitution</b> <b>errors.</b>|$|R
40|$|Codes {{have been}} {{considered}} to combat different noise effects (e. g., <b>substitution</b> <b>errors,</b> synchronization errors, erasures). A unified theory treating arbitrary patterns of errors of any nature is sketched here by giving suitably general definitions of “error-correcting”, “decodable with bounded delay”, and “error-limiting” (or synchronizable) codes; and by establishing the usual implications. As a by-product the essence of those notions is brought out with greater clarity. The {{second part of the}} paper presents two applications of the general theory. One is a generalization of a previous result, giving sufficient conditions for a code to be decodable with bounded delay (and hence also error-correcting) with respect to certain patterns of up to e <b>substitution</b> or synchronization <b>errors.</b> The second is an extension of the basic Hamming Theorem: a block code (of word length n) has Levenshtein distance ≧ 2 e + 1 between any two distinct words (with 2 e < n) if and only if it can correct up to e <b>substitution</b> <b>errors</b> in every word or up to e <b>substitution</b> and synchronization <b>errors</b> in the whole transmitted sequence...|$|R
40|$|Despite {{considerable}} {{research on}} language production errors involving speech, little research {{exists in the}} complementary domain of writing. Two experiments investigated the production of homophone substitution errors, which occur when a contextually appropriate word (e. g., beech) is replaced with its homophone, e. g., beach tree. Participants wrote down auditorily-presented sentences containing dominant or subordinate homophones. Homophones were preceded by a lexical prime that overlapped in phonology and orthography (e. g., teacher) or only orthography (e. g., headmaster) with the target homophone. Results showed more substitution errors when the context elicited a subordinate homophone relative to a dominant homophone. Furthermore, both types of primes equivalently increased production of homophone errors relative to control word (e. g., lawyer), suggesting that only orthographic overlap between the prime and target was necessary to influence errors. These results are explained within dual route models of spelling, which postulate an interaction between lexical and sublexical routes when spelling. Homophone Substitution Errors 3 Why Did I Right That? Factors that Influence the Production of Homophone <b>Substitution</b> <b>Error...</b>|$|E
40|$|We {{have asked}} whether exonucleolytic {{proofreading}} occurs during simian virus 40 origin-dependent, bidirectional DNA replication in extracts of human HeLa cells. In addition, we have compared the fidelity of leading and lagging strand DNA synthesis. In a fidelity assay that scores single-base substitution errors that revert a TGA codon in the lacZ alpha gene in an M 13 mp vector, providing {{an excess of}} a single dNTP substrate over the other three dNTP substrates in a replication reaction generates defined, strand-specific errors. Fidelity measurements with two vectors having the origin of replication {{on opposite sides of}} the opal codon demonstrate that error rates for two different A. dCTP and T. dGTP mispairs increase when deoxyguanosine monophosphate is added to replication reaction mixtures or when the concentration of deoxynucleoside triphosphates is increased. The data suggest that exonucleolytic proofreading occurs on both strands during bidirectional replication. Measurements using the two simian virus 40 origin-containing vectors suggest that base <b>substitution</b> <b>error</b> rates are similar for replication of the leading and lagging strands...|$|E
40|$|We analyse {{the demand}} for money since the “break up” of the Czech-Slovak Republics at the {{beginning}} of 1993 and for the aggregates M 0, Ml, and M 2 using monthly data. Due to the widespread use of foreign currency in formally centrally planned economies, we also investigate the issue of currency substitution. Because of our relatively small sample period the Johansen cointegration approach is not used and instead we use the general to specific methodology in a single equation framework. Previous empirical evidence on money demand in Eastern Europe, and specifically Czech Republic, has been mixed. Both graphical and empirical results suggest that any currency substitution was a one-off event due to increased uncertainty at the end of 1992 {{at the time of the}} monetary dissolution. Certainly, currency substitution in the Czech Republic is not as strong as has been found in other former centrally planned economies. However, our results do indicate that Czech National Bank may have to take account of foreign interest rates when interpreting movements in the monetary aggregates. Demand for Money, Currency <b>Substitution,</b> <b>Error</b> Correction Model,...|$|E
40|$|Motivation: Assemblies of {{next-generation}} sequencing (NGS) data, although accurate, still contain {{a substantial number}} of errors that need to be corrected after the assembly process. We develop SEQuel, a tool that corrects errors (i. e. insertions, deletions and <b>substitution</b> <b>errors)</b> in the assembled contigs. Fundamental to the algorithm behind SEQuel is the positional de Bruijn graph, a graph structure that models k-mers within reads while incorporating the approximate positions of reads into the model. Results: SEQuel reduced the number of small insertions and deletions in the assemblies of standard multi-cell Escherichia coli data by almost half, and corrected between 30 % and 94 % of the <b>substitution</b> <b>errors.</b> Further, we show SEQuel is imperative to improving single-cell assembly, which is inherently more challenging due to higher error rates and non-uniform coverage; over half of the small indels, and <b>substitution</b> <b>errors</b> in the single-cell assemblies were corrected. We apply SEQuel to the recently assembled Deltaproteobacterium SAR 324 genome, which is the first bacterial genome with a comprehensive single-cell genome assembly, and make over 800 changes (insertions, deletions and substitutions) to refine this assembly. Availability: SEQuel {{can be used as a}} post-processing step in combination with any NGS assembler and is freely available a...|$|R
3000|$|An {{alternative}} metric {{based on}} the speaker diarization error score from NISTa was proposed by Poliner and Ellis [34] to evaluate multiple f 0 estimation methods. The NIST metric consists of a single error score which takes into account <b>substitution</b> <b>errors</b> (mislabeling an active voice, E [...]...|$|R
40|$|Semantic <b>substitution</b> <b>errors</b> (e. g., saying "arm" when "leg" is intended) {{are among}} the most common types of errors {{occurring}} during spontaneous speech. It has been shown that grammatical gender of German target nouns is preserved in the errors (E. Marx, 1999). In 3 experiments, the authors explored different accounts of the grammatical gender preservation effect in German. In all experiments, semantic <b>substitution</b> <b>errors</b> were induced using a continuous naming paradigm. In Experiment 1, it was found that gender preservation disappeared when speakers produced bare nouns. Gender preservation was found when speakers produced phrases with determiners marked for gender (Experiment 2) but not when the produced determiners were not marked for gender (Experiment 3). These results are discussed in the context of models of lexical retrieval during production...|$|R
