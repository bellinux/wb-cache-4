25|19|Public
50|$|Since the {{directional}} response of small loop antennas includes a <b>sharp</b> <b>null</b> {{in the direction}} normal to {{the plane of the}} loop, they are used in radio direction finding at longer wavelengths.|$|E
50|$|Because all {{stations}} {{are being forced}} from channels above 51, the station had a construction permit for channel 28, and an application to modify their permit for a much stronger upgrade on channel 50. This application was approved as a construction permit, and would move the station antenna a few blocks south into downtown Atlanta {{to become the first}} broadcaster on the SunTrust Plaza (One Peachtree Center) building. It would also have given it an omnidirectional broadcast range, as opposed to its previous cardioid directional antenna with a <b>sharp</b> <b>null</b> in its radiation pattern to the northwest. However, this permit, granted in 2005, expired in 2008.|$|E
40|$|The {{problem of}} testing a <b>sharp</b> <b>null</b> {{hypothesis}} by a Bayesian procedure shows some controversial issues. Here {{this problem is}} faced {{in the framework of}} vague distributions through the concept of pseudodensity. It is shown that the paradoxical aspects can be easily overcome by posing the problem in a proper setting which avoids resorting to probability densities...|$|E
50|$|Thus {{mounting}} {{the loop}} in a horizontal plane will produce an omnidirectional antenna which is horizontally polarized; mounting the loop vertically yields a weakly directional antenna with vertical polarization and <b>sharp</b> <b>nulls</b> along {{the axis of}} the loop.|$|R
50|$|Also, some {{directional}} antennas {{have very}} narrow, <b>sharp</b> <b>nulls.</b> These antennas when used for direction finding are aimed {{away from the}} signal of interest until the antenna's null is pointed at the signal, and the signal cannot be detected.|$|R
40|$|We {{discuss a}} {{filtering}} technique {{for reducing the}} frequency fluctuations due to the troposphere, ionosphere, and mechanical vibrations of the ground antenna in spacecraft Doppler tracking searches for gravitational radiation. This method {{takes advantage of the}} sinusoidal behavior of the transfer function to the Doppler observable of these noise sources, which displays <b>sharp</b> <b>nulls</b> at selected Fourier components...|$|R
40|$|A {{framework}} for causal inference from two-level factorial designs is proposed. The framework utilizes {{the concept of}} potential outcomes {{that lies at the}} center stage of causal inference and extends Neyman's repeated sampling approach for estimation of causal effects and randomization tests based on Fisher's <b>sharp</b> <b>null</b> hypothesis to the case of 2 -level factorial experiments. The framework allows for statistical inference from a finite population, permits definition and estimation of estimands other than "average factorial effects" and leads to more flexible inference procedures than those based on ordinary least squares estimation from a linear model. Comment: Preliminary version; comments welcome. Added figures and a tabl...|$|E
40|$|The Kalman filter is sued {{to derive}} {{updating}} equations for the Bayesian data density in discrete time linear regression models with stochastic regressors. The implied "Bayes model" has time varying parameters and conditionally heterogeneous error variances. A sigma-finite "Bayes model" measure is given {{and used to}} produce a new model selection criterion (PIC) and objective posterior odds tests for <b>sharp</b> <b>null</b> hypotheses like {{the presence of a}} unit root. Simulation results and an empirical application are reported. The simulations show that the new model selection criterion "PIC" works very well and is generally superior to the Schwarz criterion BIC even in stationary systems. Kalman filter, Bayesian data density, stochastic regressors...|$|E
40|$|WORKING PAPER M 10 / 13, SOUTHAMPTON STATISTICAL SCIENCES RESEARCH INSTITUTE, UNIVERSITY OF SOUTHAMPTON, UK. We {{develop a}} new class of prior {{distributions}} for Bayesian comparison of nested models, which we call intrinsic moment priors. We aim at testing the equality of two proportions, based on independent samples, and thus focus on discrete data models. We illustrate our methodology in a running Bernoulli example, where we test a <b>sharp</b> <b>null</b> hypothesis, then we implement our procedure to test the equality of two proportions. A detailed analysis of the properties of our method is presented together with an application to a collection of real-world 2 * 2 tables involving a sensitivity analysis and a crossvalidation study...|$|E
5000|$|Denver faces unique {{multipath}} interference problems {{largely due}} to its mountainous location; its antennas on Lookout Mountain will need to increase in height to overcome obstacles to digital reception, but attempts to get local zoning approval have met with strong opposition. Federal legislation was ultimately used to require that Denver stations be allowed to construct their post-transition digital facilities but <b>sharp</b> <b>nulls</b> and gaps in coverage remain.|$|R
50|$|Instead of triangulation, {{a second}} dipole or {{vertical}} antenna can be electrically {{combined with a}} loop or a loopstick antenna. Called a sense antenna, connecting the second antenna changes the combined radiation pattern to a cardioid, with a null in only one, less precise direction. The {{general direction of the}} transmitter can be determined using the sense antenna, and then disconnecting the sense antenna returns the <b>sharp</b> <b>nulls</b> in the loop antenna pattern, allowing a precise bearing to be determined.|$|R
40|$|Abstract—Adaptive-array {{beamforming}} {{achieves high}} resolution and sidelobe suppression by producing <b>sharp</b> <b>nulls</b> in the adaptive beampattern. Large-aperture sonar arrays with many elements have small resolution cells; interferers may move through many resolution {{cells in the}} time required for accumulating a full-rank sample covariance matrix. This leads to “snapshot-deficient” processing. In this paper, the null-broadening technique originally developed for an ideal stationary problem is extended to the snapshot-deficient problem combined with white-noise constraint (WNC) adaptive processing. Null broadening allows the strong interferers to move through resolution cells and increases the number of degrees of freedom, thereby improving the detection of weak stationary signals. Index Terms—Covariance matrix taper (CMT), null broadening, robust adaptive beamforming, snapshot-deficient processing, white-noise constraint (WNC). I...|$|R
40|$|This {{paper is}} {{a comment on}} P. C. B. Phillips, "To criticize the critics: an {{objective}} Bayesian analysis of stochastic trends" [Phillips, (1991) ]. Departing from the likelihood of an univariate autoregressive model different routes that lead to a posterior odds analysis of the unit root hypothesis are explored, where the differences in routes are due to the different choices of the prior. Improper priors like the uniform and the Jeffreys prior are less suited for Bayesian inference on a <b>sharp</b> <b>null</b> hypothesis as the unit root. A proper normal prior on {{the mean of the}} process is analyzed and empirical results using extended Nelson-Plosser data are presented. Copyright 1991 by John Wiley & Sons, Ltd. ...|$|E
40|$|R. A. Fisher {{questioned}} the sampling-based approach to statistical inference {{on the grounds}} that it often cannot really answer the scientific question of interest. Fisher’s fiducial argument and the Dempster-Shafer (DS) theory are inferential methods that strive towards answering these situation-specific questions. For some important problems, such as testing of a <b>sharp</b> <b>null</b> hypothesis, these alternative theories suffer from the same drawbacks as their samplingbased counterparts. The Weak Belief (WB) extension of DS is applied in such cases to achieve the best of both worlds: the desirable personal probability-based inference of DS with the additional flexibility of WB. We formulate a general framework for situation specific inference, which we call the WB-DS method. Applications of the WB-DS method are illustrated in two important statistical problems, namely large-scale simultaneous hypothesis testing and nonparametrics. We show in simulations that the WB-DS procedures, suitably calibrated...|$|E
40|$|Abstract. The {{problem of}} {{combining}} experimental results to test <b>sharp</b> <b>null</b> hypotheses is considered from a Bayesian viewpoint. Relying on results of Berger and Sellke [S], lower bounds on the posterior probability of the null are obtained based on classes of priors. It {{is suggested that}} plots of these lower bounds, as functions of the prior probability, provide a useful summary of results for ap-praising evidence. An example involving the combination of ex-periments concerning the value of aspirin usage for heart attack patients is presented. The discussion includes comparison with clas-sical p-values associated with meta-analysis. The general problem of combining information {{from a variety of}} sources is both challenging and important. Types of information available are typically categorized as "Data, " by which statisticians usually mean observational results of experiments, and "Judgment, " which refers to expert scientific opinion concerning the phenomena under study as well as beliefs about th...|$|E
40|$|We {{discuss a}} jiltering {{technique}} {{for reducing the}} two-way Doppler frequency fluctua-tions of noise sources localized in space (like the tmposphem, or the master clock) that aflect the sensitivity of spacecmft Doppler tmcking seamhes for gmvitational mdiation. This method {{takes advantage of the}} sinusoidal behavior of the tmnsfer function to the Doppler observable of these noise sources, which displays <b>sharp</b> <b>nulls</b> at selected Fourier components. The non-zero gravitational wave signal remaining at these frequencies makes this Doppler tmcking technique the equivalent of a series of narrow-band detectors of gmvitational mdiation[ll, distrt”buted across the low-frequency band. Estimates for the sensititn”ties achievable with the future Cassini Doppler tmcking experiments am presented in the contezt of broad-band gravitational wave bursts, monochromatic signals, and a stochas-tic background of gravitational mdiation[21...|$|R
50|$|Transmission line {{loudspeakers}} employ this tube-like resonant cavity, {{with the}} length set between 1/6 and 1/2 the wavelength {{of the fundamental}} resonant frequency of the loudspeaker driver being used. The cross-sectional area of the tube is typically comparable to the cross-sectional area of the driver's radiating surface area. This cross section is typically tapered down to approximately 1/4 of the starting area at the terminus or open end of the line. While not all lines use a taper, the standard classical transmission line employs a taper from 1/3 to 1/4 area (ratio of terminus area to starting area directly behind driver). This taper serves to dampen the buildup of standing waves within the line, which can create <b>sharp</b> <b>nulls</b> in response at the terminus output at even multiples of the driver's Fs.|$|R
30|$|As a {{follow-on}} to the 2002 {{digital television}} (DTV) broadcast demonstration from a solar-powered stratospheric flying wing, a prototype stratospheric airship {{was used for}} a more realistic DTV broadcast demonstration in 2004, albeit at a lower altitude. The DTV signal was occasionally lost at the receiver directly below the airship during the demonstration. Adverse antenna-vehicle integration effects were investigated using a commercially available antenna simulation software, because the radiation pattern of the antenna on the airship could not be measured directly. The ground handling bars on the airship gondola were found to introduce deep and <b>sharp</b> <b>nulls</b> into the radiation pattern of the broadcast antenna. Some mitigation techniques that would have fitted within {{the constraints of the}} time are discussed. Changing to nonconductive ground handling bars and a multiturn helical antenna would have avoided the problem, according to the simulation results.|$|R
40|$|Fisherian {{randomization}} inference {{is often}} dismissed as testing an uninteresting and implausible hypothesis: the <b>sharp</b> <b>null</b> of no effects whatsoever. We {{show that this}} view is overly narrow. Many randomization tests are also valid under a more general "bounded" null hypothesis under which all effects are weakly negative (or positive), thus accommodating heterogenous effects. By inverting such tests we can form one-sided confidence intervals for the maximum (or minimum) effect. These properties hold for all effect-increasing test statistics, which include both common statistics such as the mean difference and uncommon ones such as Stephenson rank statistics. The latter's sensitivity to extreme effects permits detection of positive effects even when the average effect is negative. We argue that bounded nulls are often of substantive or theoretical interest, and illustrate with two applications: testing monotonicity in an IV analysis and inferring effect sizes in a small randomized experiment...|$|E
40|$|Simulation-based {{inference}} plays a {{major role}} in modern statistics, and often employs either reallocating (as in a randomization test) or resampling (as in bootstrapping). Reallocating mimics random allocation to treatment groups, while resampling mimics random sampling from a larger population; does it matter whether the simulation method matches the data collection method? Moreover, do the results differ for testing versus estimation? Here we answer these questions in a simple setting by exploring the distribution of a sample difference in means under a basic two group design and four different scenarios: true random allocation, true random sampling, reallocating, and resampling. For testing a <b>sharp</b> <b>null</b> hypothesis, reallocating is superior in small samples, but reallocating and resampling are asymptotically equivalent. For estimation, resampling is generally superior, unless the effect is truly additive. Moreover, these results hold regardless of whether the data were collected by random sampling or random allocation...|$|E
40|$|Model-based {{clustering}} methods using Dirichlet process (DP) mixture {{models have}} been pro-posed to exploit clustering for increased sensitivity in multiple hypothesis testing. Rather than yielding a probability of a hypothesis for each object, existing methods can only provide a rank-ing of the objects by their evidence for a particular hypothesis. In this work, we adapt the frame-work on [1] to accommodate point (i. e., <b>sharp)</b> <b>null</b> hypotheses. For that, we use a spike and slab distribution which {{is a mixture of}} both a point-mass distribution and a continuous distribu-tion as the centering distribution for the Dirichlet process prior. The method yields probabilities that genes follow the hypotheses of interest, whether those hypotheses be sharp or not. These probabilities not only rank the genes, but their interpretation is very natural and have a variety of uses, for example, in discovery of genes with a specified expected number of false discoveries. We apply our method in gene expression context and show how to simultaneously infer gen...|$|E
40|$|ITC/USA 2007 Conference Proceedings / The Forty-Third Annual International Telemetering Conference and Technical Exhibition / October 22 - 25, 2007 / Riviera Hotel & Convention Center, Las Vegas, NevadaRobust {{adaptive}} beamforming using worst-case performance optimization {{is developed}} in recent years. It had good performance against array response errors, but it cannot reject strong interferences. In this paper, we propose a scheme for robust adaptive beamforming with broad nulls to reject strong interferences. We add a quadratic constraint {{to suppress the}} power of the array response over a spatial region of the interferences. The optimal weighting vector is then obtained by minimizing {{the power of the}} array output subject to quadratic constrains on the desired signal and interferences, respectively. We derive the formulations for the optimization problem and solve it efficiently using Newton recursive algorithm. Numerical examples are presented to compare the performances of the robust adaptive beamforming with no <b>null</b> constrains, <b>sharp</b> <b>nulls</b> and broad nulls. The results show its powerful ability to reject strong interferences...|$|R
40|$|The {{return of}} the ATS 6 {{satellite}} to a western longitude during the fall of 1976 presented {{a unique opportunity to}} perform low angle of elevation measurements at 30 GHz. For this purpose a receiver using a 1. 5 m antenna was set up at Port Aransas, Texas, resulting in a propagation path entirely over water. The 30 GHz beacon was monitored daily for at least one hour from 8 September 1976 to 21 September 1976. During the time the elevation angle changed from 1. 5 deg to 17. 3 deg, the mean attenuation decreased from 20 dB to 2 dB and the standard deviation from over 6 dB to less than. 2 dB. The deep fades at angles below 4 deg show significantly <b>sharper</b> <b>nulls</b> than peaks on a log scale. Spectra of the log amplitude fluctuations vary as the (- 8 / 3) power of the spectral frequency in the limit. A flattening is noticeable at the low frequencies. A precipitation event at 8. 5 deg elevation produced a 16 dB fade and significantly increased the variance...|$|R
50|$|Receiving antennas do {{not have}} to be as {{efficient}} as transmitting antennas since in this band the signal to noise ratio is determined by atmospheric noise. The noise floor in the receiver is far below the noise in the signal, so antennas small in comparison to the wavelength, which are inefficient and produce low signal strength, can be used. The most common receiving antenna is the ferrite loopstick antenna (also known as a ferrite rod aerial), made from a ferrite rod with a coil of fine wire wound around it. This antenna is small enough that it is usually enclosed inside the radio case. In addition to their use in AM radios, ferrite antennas are also used in portable radio direction finder (RDF) receivers. The ferrite rod antenna has a dipole reception pattern with <b>sharp</b> <b>nulls</b> along the axis of the rod, so that reception is at its best when the rod is at right angles to the transmitter, but fades to nothing when the rod points exactly at the transmitter. Other types of loop antennas and random wire antennas are also used.|$|R
40|$|Many {{outcomes}} {{of interest in}} the social and health sciences, as well as in modern applications in computational social science and experimentation on social media platforms, are ordinal and do not have a meaningful scale. Causal analyses that leverage this type of data, termed ordinal non-numeric, require careful treatment, as much of the classical potential outcomes literature is concerned with estimation and hypothesis testing for outcomes whose relative magnitudes are well defined. Here, we propose a class of finite population causal estimands that depend on conditional distributions of the potential outcomes, and provide an interpretable summary of causal effects when no scale is available. We formulate a relaxation of the Fisherian <b>sharp</b> <b>null</b> hypothesis of constant effect that accommodates the scale-free nature of ordinal non-numeric data. We develop a Bayesian procedure to estimate the proposed causal estimands that leverages the rank likelihood. We illustrate these methods with an application to educational outcomes in the General Social Survey...|$|E
40|$|Testing a point (<b>sharp)</b> <b>null</b> {{hypothesis}} {{is arguably the}} most widely used statistical inferential procedure in many fields of scientific research, nevertheless, the most controversial, and misapprehended. Since 1935 when Buchanan-Wollaston raised the first criticism against hypothesis testing, this foundational field of statistics has drawn increasingly active and stronger opposition, including draconian suggestions that statistical significance testing should be abandoned or even banned. Statisticians should stop ignoring these accumulated and significant anomalies within the current point-null hypotheses paradigm and rebuild healthy foundations of statistical science. The foundation for a paradigm shift in testing statistical hypotheses is suggested, which is testing interval null hypotheses based on implications of the Zero probability paradox. It states that in a real-world research point-null hypothesis of a normal mean has zero probability. This implies that formulated point-null hypothesis of a mean {{in the context of the}} simple normal model is almost surely false. Thus, Zero probability paradox points to the root cause of so-called large n problem in significance testing. It discloses that there is no point in searching for a cure under the current point-null paradigm...|$|E
30|$|The coupled {{cavities}} {{behave in}} a similar fashion to electromagnetically induced transparency (EIT), where two nearly degenerate resonances can interfere creating <b>sharp</b> <b>null</b> in the transmission spectrum. V_phase acts as a variable coupling between the two resonant cavities and can be used to shift in wavelength the interference null in the transmission spectrum (cf. Refs. [77, 78]). However, for disturbance rejection we require that the transmission is suppressed at all wavelengths. This can be achieved in the CCD if the linewidth of the controller cavity and the phase shift induced in the feedback signal u can be controlled independently, while the other parameters are fixed [75]. Disturbance rejection means that, with suitable parameter values, the output field z is in the vacuum state regardless of the amplitude and phase of the input field w. Physically, this results from all the input power being routed to the output port z' due to the interference between the cavities. We are unable to tune the cavity linewidths in situ with this generation of the CCD, however, we will evaluate whether disturbance rejection can be achieved with the parameter values determined at fabrication and the in situ tuning capabilities we do have.|$|E
40|$|The {{standard}} concordance {{model of}} the Universe {{is based on the}} cosmological constant as the driver of accelerating expansion. This concordance model is being subjected to a growing range of inter-locking observations. In addition to using generic observational tests, one can also design tests that target the specific properties of the cosmological constant. These null tests do not rely on parametrisations of observables, but focus on quantities that are constant only if dark energy is a cosmological constant. We use supernova data in null tests that are based on the luminosity distance. In order to extract derivatives of the distance in a model-independent way, we use Gaussian Processes. We find that the concordance model is compatible with the Union 2. 1 data, but the error bars are fairly large. Simulated datasets are generated for the DES supernova survey and we show that this survey will allow for a <b>sharper</b> <b>null</b> test of the cosmological constant if we assume the Universe is flat. Allowing for spatial curvature degrades the power of the null test. Comment: 7 pages, 5 figures. Revised version with corrections, improvements and new appendix. Accepted by Phys Rev...|$|R
40|$|This thesis {{describes}} a mathematical and {{experimental study of}} circular arrays of directional antennas. For directional pattern synthesis in circular arrays of omnidirectional elements, mode-analysis or Fourier harmonic analysis technique has previously been used. This thesis describes the effect of using any directional element on the mode properties - in the horizontal plane, and on the vertical directional pattern of such arrays. The effect of random amplitude and phase errors in the excitation of circular arrays and of mutual coupling with reference to directional pattern synthesis is also studied. A procedure is outlined for designing circular arrays of directional elements and a design of an experimental 16 - element circular array is then presented. This is an 8 foot diameter array of 16 wideband elements each having a pattern of the form (l+cosφ) for operation at 200 - 400 MHz frequency band. Various mode patterns are computed for the array to determine their purity. The theoretical wideband performance of the array is also examined. New techniques are described for the synthesis of single and 2 n <b>sharp</b> <b>nulls</b> in otherwise omnidirectional patterns of circular arrays and the mechanism for independent steering of these nulls is also described. Patterns are computed to demonstrate and theoretically study the techniques. Finally, experimental results are presented to complement the theoretical study of circular arrays and null patterns...|$|R
40|$|Beamforming (BF) {{algorithm}} {{is one of}} the major smart antenna function that forming beams towards the direction of the desired user while simultaneously suppressing signals origination from other directions. Minimum Variance Distortionless Response (MVDR) is basically a unity gain adaptive beamformer which is suffering from performance degradation due to the presence of interference and noise. Also, MVDR is sensitive to the direction of arrival mismatch, and unsatisfactory null-forming level. This thesis presents two BF techniques to enhancing the MVDR null-forming level. First, the zero-null constraint adds to the MVDR beamformer based on uniform linear antenna arrays. The proposed MVDRZN is based on reconstructing the excitation weight vector coefficients to enforcing the undesired signal energy equal to zero (or near zero) and the desired signal energy equal to one (unity gain). Metaheuristic optimization algorithms are used widely to solve many engineering problems. Second, hybrid Particle Swarm Optimization/Gravitational Search Algorithm (PSOGSA) is used to obtain a desired radiation pattern by enhancing the MVDR nulling level. The proposed MVDRPSOGSA method combines the search methods of PSO and GSA, thus achieving the improved exploration ability needed to obtain high accuracy with deep null-forming in the directions of the interference sources. Whereas in the BF applications, Signal-to-Interference plus Noise Ratio (SINR) is a valid fitness function because it measures how well the array’s radiation pattern focuses energy on a Signal Of Interest (SOI) and steers nulls towards interference. In addition, to provide high accuracy beampattern and to enhance the null-forming level to suppress the interference and noise deeply. The performance of the proposed approaches is judged by the beampattern accuracy for azimuth and elevation scanning angles, SINR improvement through a deep null-forming level. The null width in the azimuth and elevation scanning angle also have been assessed. The result shows that the proposed MVDRZN method clearly introduce more than 300 dB negative power to serve the interference source with average SINR improvements approximately 250 dB and accurate azimuth and elevation angles. It is observed that the MVDRZN can provide a perfect radiation pattern with relatively few snapshots records. The obtained results confirm the complete agreement between MVDR technique and hybrid intelligent swarm PSOGSA algorithm. The proposed MVDRPSOGSA approach can successfully place very <b>sharp</b> <b>nulls</b> (- 200 dB deep, on average) at the undesired angles. It is providing additional support to the smart antenna array system to combat the co-channel interference and array noise reduction. These approaches achieve significant SINR improvement by reducing the effects of multiple access interference in the wireless communication systems...|$|R
40|$|One of the {{well-known}} problems with testing for <b>sharp</b> <b>null</b> hypotheses against two-sided alternatives is that, when sample sizes diverge, every consistent test rejects the null with a probability converging to one, {{even when it}} is true. This kind of problem emerges in practically all applications of traditional two-sided tests. The main purpose of the present paper is to overcome this very intriguing impasse by considering a general {{solution to the problem of}} testing for an equivalence null interval against a two one-sided alternative. Our goal is to go beyond the limitations of likelihood-based methods by working in a nonparametric permutation framework. This solution requires the nonparameteric Combination of dependent permutation tests, which is the methodological tool that achieves Roy’s Union–intersection principle. To obtain practical solutions, the related algorithm is presented. To appreciate its effectiveness for practical purposes, a simple example and some simulation results are also presented. In addition, for every pair of consistent partial test statistics it is proved that, if sample sizes diverge, when the effect lies in the open equivalence interval, the Rejectioprobability (RP) converges to zero. Analogously, if the effect lies outside that interval, the RP converges to one...|$|E
40|$|The Kalman filter is used {{to derive}} {{updating}} equations for the Bayesian data density in discrete time linear regression models with stochastic regressors. The implied “Bayes model” has time varying parameters and conditionally heterogeneous error variances. A σ-finite Bayes model measure is given and used to produce a new-model-selection criterion (PIC) and objective posterior odds tests for <b>sharp</b> <b>null</b> hypotheses like {{the presence of a}} unit root. This extends earlier work by Phillips and Ploberger [18]. Autoregressive-moving average (ARMA) models are considered, and a general test of trend-stationarity versus difference stationarity is developed in ARMA models that allow for automatic order selection of the stochastic regressors and the degree of the deterministic trend. The tests are completely consistent in that both type I and type II errors tend to zero as the sample size tends to infinity. Simulation results and an empirical application are reported. The simulations show that the PIC works very well and is generally superior to the Schwarz BIC criterion, even in stationary systems. Empirical application of our methods to the Nelson-Plosser [11] series show that three series (unemployment, industrial production, and the money stock) are level- or trend-stationary. The other eleven series are found to be stochastically nonstationary. ...|$|E
30|$|When within-network {{experimental}} {{variation is}} used to identify social effects, careful attention must {{be paid to the}} important issue of inference. At one extreme, a researcher might have many (often thousands) of nodes embedded in the same network, and construct a valid comparison group using similar nodes embedded in {{a different part of the}} network to the treated node. The complication in computing standard errors comes from the fact that all nodes are embedded within the same network, and may face correlated unobserved shocks. Though these shocks may not affect identification, they will generate correlations in the outcomes of units, and must be accounted for when conducting inference. The availability of only a single network makes it very difficult to derive large sample approximations of distributions and thereby to calculate valid standard errors. Athey et al. (2015) extend the method of randomisation inference, which calculates exact p values, to this setting. Under randomisation inference, the distribution of the test statistic is generated by considering all possible realisations of the treatment assignment, keeping the potential outcomes and characteristics of units fixed. A drawback of this procedure is that it allows for testing of <b>sharp</b> <b>null</b> hypotheses—e.g. the treatment has no effect whatsoever—only. However, we often want to test non-sharp hypotheses. Athey et al. (2015) develop methods for the computation of p values for three specific null hypotheses.|$|E
40|$|In {{this paper}} we prove global well-posedness and {{modified}} scattering for the massive Maxwell-Klein-Gordon equation in the Coulomb gauge on R^ 1 +d (d ≥ 4) for data with small critical Sobolev norm. This {{extends to the}} general case m^ 2 > 0 the results of Krieger-Sterbenz-Tataru (d= 4, 5) and Rodnianski-Tao (d ≥ 6), who considered the case m= 0. We proceed by generalizing the global parametrix construction for the covariant wave operator and the functional framework from the massless case to the Klein-Gordon setting. The equation exhibits a trilinear cancelation structure identified by Machedon-Sterbenz. To treat it one needs <b>sharp</b> L^ 2 <b>null</b> form bounds, which we prove by estimating renormalized solutions in null frames spaces {{similar to the ones}} considered by Bejenaru-Herr. To overcome logarithmic divergences we rely on an embedding property of ^- 1 in conjunction with endpoint Strichartz estimates in Lorentz spaces. Comment: 75 page...|$|R
40|$|The aim of {{this paper}} is twofold. Firstly, to {{investigate}} the integration process within the European Union retail banking sector by analysing deposit and lending rates to the household sector during the period 2003 - 2011. Secondly, {{to assess the impact of}} the 2008 global financial crisis on the banking integration process, an area that is yet unexplored. An important contribution of the paper is the application of the recently developed Phillips and Sul (2007 a) panel convergence methodology which has not hitherto been employed in this area. This method analyses the degree as well as the speed of convergence, identifies the presence of club formation, and measures the behaviour of each country's transition path relative to the panel average. The empirical results point to the presence of convergence in all deposit and lending rates to the household sector up to 2007. In <b>sharp</b> contrast, the <b>null</b> of convergence is rejected in all deposit and credit markets after the onset of the 2008 financial crisis. These results show that the global crisis has had a detrimental effect on the banking integration process. We find some convergence in a few sub-clusters of countries but the rate of convergence is typically slow and several countries are identified as diverging altogether. In addition, we find that the credit market, in general, is far more heterogeneous than the savings market. ?? 2013 Elsevier B. V...|$|R
40|$|A <b>sharp</b> <b>null</b> {{hypothesis}} may {{be strongly}} rejected by a standard sampling-theory test of significance and yet be awarded high odds by a Bayesian analysis {{based on a}} small prior probability for the null hypothesis and a diffuse distribution of one’s remaining probability over the alternative hypothesis. This disagreement between sampling-theory and Bayesian methods was first studied by Harold Jeffreys (1939), and it was first called a paradox by Dennis Lindley (1957). The paradox can be exhibited in the simple case where we are testing θ = 0 using a single observation Y from a normal distribution with variance one and mean θ. If we observe a large value y for Y (y = 3, for example), then standard sampling theory allows us to confidently reject the null hypothesis. But the Bayesian approach advocated by Jeffreys can give quite a different result. Jeffreys advised that we assign a non-zero prior probability π 0 to the null hypothesis and distribute {{the rest of our}} probability over the real line according to a fairly flat probability density π 1 (θ). If the range of possible values for θ is very wide, then the set of values within a few units of y will be very unlikely under π 1 (θ), and consequently the overall likelihood of the alternative hypothesis...|$|E
