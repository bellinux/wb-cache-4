35|6|Public
5000|$|The paper [...] "Decision Field Theory" [...] was {{published}} by Jerome R. Busemeyer and James T. Townsend in 1993. [...] The DFT {{has been shown to}} account for many puzzling findings regarding human choice behavior including violations of stochastic dominance, violations of strong <b>stochastic</b> <b>transitivity,</b> violations of independence between alternatives, serial position effects on preference, speed accuracy tradeoff effects, inverse relation between probability and decision time, changes in decisions under time pressure, as well as preference reversals between choices and prices. The DFT also offers a bridge to neuroscience. [...] Recently, the authors of decision field theory also have begun exploring a new theoretical direction called Quantum Cognition.|$|E
40|$|The concurrent-chains {{procedure}} {{has been}} used to measure how choice depends on various aspects of reinforcement, such as its delay and its magnitude. Navarick and Fantino (1972, 1974, 1975) have found that choice in this procedure can violate the condition of <b>stochastic</b> <b>transitivity</b> that is required if a unidimensional scale for reinforcements is to be possible. It is shown in this paper that two simple unidimensional models of choice on concurrent chains can produce violations of <b>stochastic</b> <b>transitivity.</b> It is argued that such violations may result from the complex contingencies of the concurrent-chains procedure...|$|E
40|$|We {{introduce}} the regularity and <b>stochastic</b> <b>transitivity</b> conditions, as necessary and well-behaved conditions respectively, for {{the consistency of}} discrete choice preferences with the Random Utility Model (RUM). For the specific case of a three-alternative nested logit (NL) model, we synthesise these conditions {{in the form of}} a simple two-part test for rationality, and reconcile this test with the conventional 0 1 bounds on the structural parameter. We show that, whilst regularity and weak <b>stochastic</b> <b>transitivity</b> support the lower bound of zero, moderate and strong <b>stochastic</b> <b>transitivity</b> may, for some preference orderings, give rise to a lower bound greater than zero. On the other hand, we show that neither regularity nor the <b>stochastic</b> <b>transitivity</b> conditions constrain the upper bound at one. If therefore the conventional bounds 0 1 are imposed on model estimation, preferences which violate regularity and/or transitivity may either go undetected and/or be unwittingly admitted, and preferences which comply with regularity and transitivity may be unwittingly excluded. Against this background, we show that the imposition of 0 1 on model estimation may compromise model fit, inferences of willingness-to-pay, and forecasts of choice behaviour. Finally, we show that in data characteristed by a negative θ, and thus a violation of regularity, the use of positive starting values for θ in estimation may prevent an analyst from revealing the regularity failures...|$|E
40|$|Rubinstein's (1988) {{procedure}} for choosing between risky prospects, based, in part, upon similarities between prizes and probabilities across lotteries, is modified and extended {{to apply to}} a more general class of binary choices. This modified procedure is shown to imply behaviors following from Loomes and Sugden's (1982) Regret Theory, although under more general conditions, and provides an alternative explanation {{for much of the}} data whieh led to the speeification of Prospect Theory's value and decision weighing functions. The procedure also explains observed violations of <b>stochastic</b> dominance, <b>transitivity,</b> and invariance not accounted for in available alternatives to expeeted utility...|$|R
40|$|A simple {{model in}} which an animal makes a choice between two {{simultaneously}} available foraging options is used {{to show that we}} cannot expect natural selection to assign an absolute value (based on fitness) to each option. The example shows that the value of an option depends on its context; in particular, it depends on the option with which it is paired. This dependence results in a pattern of choice that violates a <b>stochastic</b> form of <b>transitivity...</b>|$|R
40|$|For a {{reciprocal}} fuzzy relation Q: x 2 + [O, 11 on a finite set of alternatives X, we introduce {{the concept of}} U-transitivity which {{is based upon the}} ordering of the matrix elements qij, qjk and qki of Q for all triples of indices i, j and k. We show that U-transitivity generalizes both <b>stochastic</b> and fuzzy <b>transitivity.</b> The U-transitivity of the secalled utility model is investigated. Furthermore, we describe a new dice game model of preferences given by rational numbers in the unit interval and establish the corresponding U-transitivity...|$|R
40|$|Recently, we {{introduced}} {{the framework of}} cycletransitivity as a general means of representing transitivity conditions on reciprocal relations. In this framework, for reciprocal relations, the concepts of T-transitivity and <b>stochastic</b> <b>transitivity</b> can be cast. The upper bound functions encountered in this framework for product-transitivity and dice-transitivity, conceptually related to the probabilistic sum, lead to natural questions concerning the meaning of similar upper bound functions...|$|E
40|$|This paper {{develops}} {{the first model}} of probabilistic choice under subjective uncertainty (when probabilities of events are not objectively known). The model is characterized by seven standard axioms (probabilistic completeness, weak <b>stochastic</b> <b>transitivity,</b> nontriviality, event-wise dominance, probabilistic continuity, existence of an essential event, and probabilistic independence) {{as well as one}} new axiom. The model has an intuitive econometric interpretation as a Fechner model of (relative) random errors. The baseline model is extended from binary choice to decisions among m> 2 alternatives using a new method, which is also applicable to other models of binary choice...|$|E
40|$|Violations {{of strong}} <b>stochastic</b> <b>transitivity</b> in concurrent-chains choice were first {{reported}} by Navarick and Fantino. In {{a series of}} articles, Navarick and Fantino concluded that neither a unidimensional model capable of predicting exact choice probabilities nor a fixed-variable equivalence rule was possible for the concurrent-chains procedure. I show that when choice is modeled contextually (i. e., when preference for a schedule is affected by factors other than the schedule itself, e. g., aspects of the alternative schedule), a unidimensional, exact-choice probability model is possible that both predicts the intransitivities reported by Navarick and Fantino and provides a fixed-variable equivalence rule for the concurrent-chains procedure. The contextual model {{is an extension of}} the generalized matching law and violates a key assumption underlying traditional choice models—simple scalability—because of (a) schedule interdependence and (b) bias from procedural contingencies. Therefore, strong <b>stochastic</b> <b>transitivity</b> cannot be expected to hold. Contextual scalability is analyzed to reveal a hierarchy of context effects in choice. Navarick and Fantino's intransitivities can be satisfactorily explained by bias. If attribute sensitivity is context dependent, however, and if there are similarity structures among choice alternatives, the contextual model is shown to be able to predict violations of ordinal preference. Therefore, {{it may be possible to}} formulate a deterministic, general psychophysical model of choice as a behavioral alternative to probabilistic, multidimensional choice theories...|$|E
40|$|We {{present a}} {{preference}} foundation for Chance Theory (CT), {{a model of}} decision making under uncertainty where the evaluation of an act depends distinctively on its lowest outcome. This outcome is evaluated with the riskless value function � and the potential increments over it are evaluated by subjective expected utility with a risky utility function �. In contrast to earlier approaches with models that aimed at separating riskless and risky utility, CT does not violate basic rationality principles like first-order <b>stochastic</b> dominance or <b>transitivity.</b> Decision makers with CT-preferences always prefer the expected value of a lottery to the latter, so they are weakly risk averse. Besides explaining behavioral irregularities like the expected utility paradoxes of Allais and Rabin, CT also separates risk attitude in the strong sense from attitude towards wealth. Risk attitude is completely determined by the curvature of � and is independent of the value function �. Conversely, attitude towards wealth is reflected solely through the curvature of � without imposing constraints on �...|$|R
40|$|In {{this paper}} we propose a model for {{decision}} making under risk that is capable of predicting empirically observed preference patterns that {{have been found to be}} incompatible with the expected utility model. The model departs from the classical expected utility model by allowing utilities to depend on the lottery. The dependence of utilities on the lottery being evaluated is achieved by restricting the utility measure to a convenient parametric family of functions. The idea then is to use each lottery to determine a specific parameter value thus characterizing the utility function for each particular lottery. The expected value of this lottery dependent utility function provides the overall measure of preference. The model retains the properties of <b>transitivity,</b> <b>stochastic</b> dominance, and continuity. It also permits types of analyses, such as exploitation of basic attitudes toward risk through risk aversion properties, that have been found useful in decision theory. The primary use of our model is in descriptive or predictive research and applications. For some decision makers who wish to retain the preference patterns that are incompatible with the substitution principle, even after the implications of their choices are made transparent, our model could be of prescriptive use as well. utility theory, preference theory...|$|R
40|$|For a long time, most {{financial}} {{economists have}} largely ignored experimental evidence on decision making under risk, mainly because introducing behavioral elements into asset pricing models while preserving investor rationality {{is a very}} challenging task. This thesis focuses on a relatively novel set of preferences that exhibit attitudes toward risk termed disappointment aversion preferences. These preferences can capture well documented patterns for risky choices, such as asymmetric marginal utility over gains and losses, without violating first-order <b>stochastic</b> dominance, <b>transitivity</b> of preferences or aggregation of investors. In my dissertation, I employ disappointment aversion preferences {{in an attempt to}} resolve two of the most prominent puzzles in asset pricing: the equity premium puzzle in the cross-section of expected stock returns, and the credit spread puzzle in corporate bond markets. The first chapter of my dissertation explains the cross-section of expected stock returns for the U. S. economy using an empirically tractable solution for the disappointment aversion discount factor. The consumption-based asset pricing framework introduced in the first chapter does not rely on additional risk processes, backwards-looking state variables, or extremely persistent macroeconomic shocks to generate large equity risk premia. In contrast, estimation results highlight the importance of disappointment events, defined as periods during which consumption growth drops below its forward-looking certainty equivalent. Finally, the disappointment aversion model can generate smaller in- and out-of-sample pricing errors than popular factor-based models using aggregate consumption growth as the only independent variable. Structural models of default are unable to generate measurable Baa-Aaa credit spreads, when these models are calibrated to realistic values for default rates and losses given default. Motivated by recent results in behavioral economics, the second chapter proposes a consumption-based asset pricing model with disappointment aversion preferences in an attempt to resolve the credit spread puzzle. Simulation results suggest that as long as losses given default and default boundaries are countercyclical, then the disappointment model can resolve the Baa-Aaa credit spread puzzle using preference parameters that are consistent with experimental findings. Further, the disappointment aversion discount factor can almost perfectly match key moments for stock market returns, the price-dividend ratio, and the risk-free rate...|$|R
40|$|Most economists define {{rationality}} {{in terms}} Of consistency principles. These principles place "bounds" on rationality-bounds {{that range from}} perfect consistency to weak <b>stochastic</b> <b>transitivity.</b> Several decades of research on preferential choice has demonstrated how and when people violate these bounds. Many of these violations are interconnected and reflect systematic behavioral principles. We discuss the robustness of the violations and review the theories {{that are able to}} predict them. We further discuss the adaptive functions of the violations. From this perspective, choices do more than reveal preferences; they also reflect subtle, yet often quite reasonable, dependencies on the environment...|$|E
40|$|AbstractGiven a {{collection}} of random variables, we build a probabilistic relation that, {{in the case of}} continuous random variables, expresses for each couple of random variables the probability that the first one takes a greater value than the second one. In order to compute this probability, the random variables are artificially coupled by means of a fixed commutative copula. The main result of this paper pertains to the transitivity of this probabilistic relation. Provided the commutative copula satisfies some additional condition, this transitivity can be described elegantly within the cycle-transitivity framework. It ranges between two known types of transitivity: TL-transitivity and partial <b>stochastic</b> <b>transitivity...</b>|$|E
40|$|This paper {{presents}} {{a new model}} of probabilistic binary choice under risk. In this model, a decision maker always satisfies first-order stochastic dominance. If neither lottery stochastically dominates the other alternative, a decision maker chooses in a probabilistic manner. The proposed model is derived from four standard axioms (completeness, weak <b>stochastic</b> <b>transitivity,</b> continuity, and common consequence independence) and two relatively new axioms. The proposed model provides a better fit to experimental data than do existing models. The baseline model can be extended to other domains such as modeling variable consumer demand. This paper was accepted by Peter Wakker, decision analysis. probabilistic choice, first-order stochastic dominance, random utility, strong utility...|$|E
40|$|We {{consider}} $(\epsilon,\delta) $-PAC maximum-selection and ranking {{for general}} probabilistic models whose comparisons probabilities satisfy strong <b>stochastic</b> <b>transitivity</b> and stochastic triangle inequality. Modifying the popular knockout tournament, we propose a maximum-selection algorithm that uses $\mathcal{O}\left(\frac{n}{\epsilon^ 2 }\log \frac{ 1 }{\delta}\right) $ comparisons, a number tight up to a constant factor. We then derive a general framework that improves {{the performance of}} many ranking algorithms, and combine it with merge sort and binary search to obtain a ranking algorithm that uses $\mathcal{O}\left(\frac{n\log n (\log \log n) ^ 3 }{\epsilon^ 2 }\right) $ comparisons for any $\delta\ge\frac 1 n$, a number optimal up to a $(\log \log n) ^ 3 $ factor...|$|E
40|$|We {{develop an}} axiomatic theory of random choice that builds on Luce’s (1959) model to {{incorporate}} {{a role for}} perception. We identify agents ’ “perception priorities ” from their violations of Luce’s axiom of independence from irrelevant alternatives. Using such perception priorities, we adjust choice probabilities {{to account for the}} effects of perception. Our axiomatization requires that the agents ’ adjusted random choice conforms to Luce’s model. The theory can explain the attraction, compromise, and similarity effects, which are very well-documented behavioral phenomena in individual choice. It can also explain violations of <b>stochastic</b> <b>transitivity,</b> and the effects of having to choose on what choice is made. ...|$|E
40|$|This article {{summarizes}} the cumulative progress of a cognitive-dynamical approach to {{decision making and}} preferential choice called decision field theory. This review includes applications to (a) binary decisions among risky and uncertain actions, (b) multi-attribute preferential choice, (c) multi-alternative preferential choice, and (d) certainty equivalents such as prices. The theory provides natural explanations for violations of choice principles including strong <b>stochastic</b> <b>transitivity,</b> independence of irrelevant alternatives, and regularity. The theory also accounts for the relation between choice and decision time, preference reversals between choice and certainty equivalents, and preference reversals under time pressure. Comparisons with other dynamic models of decision-making and other random utility models of preference are discussed...|$|E
40|$|The paper {{examines}} the random preference model, which can explain inherent variability of preferences in managerial and individual {{decision making and}} provides axiomatizations for the utility components of two such models differentiated by the structure of core preferences: expected utility (EU) and betweenness-like preferences. We then examine the possibility of violations of weak <b>stochastic</b> <b>transitivity</b> for these models and for a model with core dual EU preferences. Such violations correspond {{to the existence of}} Condorcet cycles, and therefore, the analysis has implications for managerial decision making and for majority rule voting. The paper also investigates implications of its findings for two popular experimental settings. ...|$|E
40|$|A body of {{data and}} theory has been {{developing}} within psychology which should {{be of interest to}} economists. Taken at face value the data are simply inconsistent with preference theory and have broad implications about research priorities within economics. The inconsistency is deeper than the mere lack of transitivity or even <b>stochastic</b> <b>transitivity.</b> It suggests that no optimization principles of any sort lie behind even the simplest of human choices and that the uniformities in human choice behavior which lie behind market behavior may result from principles which are of a completely different sort from those generally accepted. This paper reports the results of a series of experiments designed to discredit the psychologists' works as applied to economics...|$|E
40|$|A general {{framework}} {{for studying the}} transitivity of reciprocal relations is presented. The key feature is the cyclic evaluation of transitivity: triangles (i. e. any three points) are visited in a cyclic manner. An upper bound function acting upon the ordered weights encountered provides an upper bound for the ‘sum minus 1 ’ of these weights. Commutative quasi-copulas allow to translate a general definition of fuzzy transitivity (when applied to reciprocal relations) elegantly into the framework of cycletransitivity. Similarly, a general notion of <b>stochastic</b> <b>transitivity</b> corresponds to a particular class of upper bound functions. Special attention is given to selfdual upper bound functions. Keywords: Cycle-transitivity, reciprocal relations, self-dual upper bound function, t-conorm. ...|$|E
40|$|We model {{behavioral}} allocation on concurrent chains {{in which}} the initial links are independent variable-interval schedules. We also quantify the relationship between behavior during the initial links and the probability of entering a terminal link. The behavior that maximizes overall reinforcement rate is then considered and compared with published experimental data. Although all the trends in the data are predicted by rate maximization, there are considerable deviations from the predictions of rate maximization when reward magnitudes are unequal. We argue from our results that optimal allocation on concurrent chains, and prey choice as used {{in the theory of}} optimal diets, are distinct concepts. We show that the maximization of overall rate can lead to apparent violations of <b>stochastic</b> <b>transitivity...</b>|$|E
40|$|Transitivity and {{dominance}} are {{key concepts}} built {{deep into the}} fundaments of most economic models of decision-making. One of the arguments in favour of using the two concepts {{is that they are}} normative, i. e., symptomatic of perfect, rational decision-making. This paper describes several specific axioms stemming from these concepts and appearing in axiomatic models of decision-making, gives possible arguments speaking for or against the normativeness of a given concept and adds examples of empirically observed violations of the concept by human decision-making. In the conclusion, it offers an assessment of whether the use of transitivity and dominance in economic models of decision-making is justified or not. transitivity, <b>stochastic</b> <b>transitivity,</b> dominance, stochastic dominance, axioms, lexicographic semiorder, Condorcet paradox...|$|E
40|$|Humans {{deviate from}} {{rational}} choice theory if their {{estimates of the}} attribute values for one alternative change {{as a function of}} the attribute values of competing alternatives (1). In our paper (2), we report such “context-dependent” (CD) deviations from rationality in the form of a frequent winner (FW) effect and the corresponding weak <b>stochastic</b> <b>transitivity</b> (WST) violations. Davis-Stober et al. (3) claim that this observation is an artifact of aggregating over different trial types, and suggest that a context-independent (CI) decision model might explain our data. We thank Davis-Stober et al. (3) for drawing attention to a potentially important issue. However, we find their analysis to be misleading. Below, we comprehensively rebut their claims and present evidence that corroborates our original findings...|$|E
40|$|This paper {{presents}} an axiomatic model of probabilistic choice under risk. In this model, {{when it comes}} to choosing one lottery over another, each alternative has a chance of being selected, unless one lottery stochastically dominates the other. An individual behaves as if he compares lotteries to a reference lottery—a least upper bound or a greatest lower bound in terms of weak dominance. The proposed model is compatible with several well-known violations of expected utility theory such as the common ratio effect and the violations of the betweenness. Necessary and sufficient conditions for the proposed model are completeness, weak <b>stochastic</b> <b>transitivity,</b> continuity, common consequence independence, outcome monotonicity, and odds ratio independence. Probabilistic choice, first-order stochastic dominance, expected utility theory, random utility model, risk...|$|E
40|$|As {{datasets}} capturing human choices grow in {{richness and}} scale [...] -particularly in online domains [...] -there {{is an increasing}} need for choice models that escape traditional choice-theoretic axioms such as regularity, <b>stochastic</b> <b>transitivity,</b> and Luce's choice axiom. In this work we introduce the Pairwise Choice Markov Chain (PCMC) model of discrete choice, an inferentially tractable model that does not assume {{any of the above}} axioms while still satisfying the foundational axiom of uniform expansion, a considerably weaker assumption than Luce's choice axiom. We show that the PCMC model significantly outperforms the Multinomial Logit (MNL) model in prediction tasks on both synthetic and empirical datasets known to exhibit violations of Luce's axiom. Our analysis also synthesizes several recent observations connecting the Multinomial Logit model and Markov chains; the PCMC model retains the Multinomial Logit model as a special case. Comment: Advances in Neural Information Processing Systems (NIPS) 29, 201...|$|E
40|$|Motivated by {{applications}} in recommender systems, web search, social choice and crowdsourcing, {{we consider the}} problem of identifying the set of top K items from noisy pairwise comparisons. In our setting, we are non-actively given r pairwise comparisons between each pair of n items, where each comparison has noise constrained by a very general noise model called the strong <b>stochastic</b> <b>transitivity</b> (SST) model. We analyze the competitive ratio of algorithms for the top-K problem. In particular, we present a linear time algorithm for the top-K problem which has a competitive ratio of Õ(√(n)); i. e. to solve any instance of top-K, our algorithm needs at most Õ(√(n)) times as many samples needed as the best possible algorithm for that instance (in contrast, all previous known algorithms for the top-K problem have competitive ratios of Ω̃(n) or worse). We further show that this is tight: any algorithm for the top-K problem has competitive ratio at least Ω̃(√(n)) ...|$|E
40|$|As Duncan Luce {{and other}} {{prominent}} scholars {{have pointed out}} on several occasions, testing algebraic models against empirical data raises difficult conceptual, mathematical, and statistical challenges. Empirical data often result from statistical sampling processes, whereas algebraic theories are nonprobabilistic. Many probabilistic specifications lead to statistical boundary problems and are subject to nontrivial order constrained statistical inference. The present paper discusses Luce&# 8217;s challenge for a particularly prominent axiom: Transitivity. The axiom of transitivity is a central component in many algebraic theories of preference and choice. We offer the currently most complete solution to the challenge {{in the case of}} transitivity of binary preference on the theory side and two-alternative forced choice on the empirical side, explicitly for up to five, and implicitly for up to seven, choice alternatives. We also discuss the relationship between our proposed solution and weak <b>stochastic</b> <b>transitivity.</b> We recommend to abandon the latter as a model of transitive individual preferences...|$|E
40|$|Pairwise {{comparison}} data {{arises in}} many domains, including tournament rankings, web search, and preference elicitation. Given noisy comparisons of a fixed subset of pairs of items, we study {{the problem of}} estimating the underlying comparison probabilities under the assumption of strong <b>stochastic</b> <b>transitivity</b> (SST). We also consider the noisy sorting subclass of the SST model. We show that when the assignment of items to the topology is arbitrary, these permutation-based models, unlike their parametric counterparts, do not admit consistent estimation for most comparison topologies used in practice. We then demonstrate that consistent estimation is possible when the assignment of items to the topology is randomized, thus establishing a dichotomy between worst-case and average-case designs. We propose two estimators in the average-case setting and analyze their risk, showing that {{it depends on the}} comparison topology only through the degree sequence of the topology. The rates achieved by these estimators are shown to be optimal for a large class of graphs. Our results are corroborated by simulations on multiple comparison topologies...|$|E
40|$|Decision {{field theory}} {{provides}} for a mathematical foundation leading to a dynamic, stochastic theory of decision behavior in an uncertain environment. This theory is used to explain (a) viola-tions of stochastic dominance, (b) violations of strong <b>stochastic</b> <b>transitivity,</b> (c) violations of inde-pendence between alternatives, (d) serial position effects on preference, (e) speed-accuracy trade-off effects in decision making, (f) the inverse relation between choice probability and decision time, (g) changes {{in the direction of}} preference under time pressure, (h) slower decision times for avoidance as compared with approach conflicts, and (i) preference reversals between choice and selling price measures of preference. The proposed theory is compared with 4 other theories of decision making under uncertainty. Beginning with von Neumann and Morgenstern's (1947) classic expected utility theory, steady {{progress has been made in}} the development of formal theories of decision making under risk and uncertainty. For rational theorists, the goal has been to formulate a logical foundation for representing the pref-erences of an ideal decision maker (e. g., Machina, 1982; Savage...|$|E
40|$|Rationality {{principles}} are {{the bedrock of}} normative theories of decision-making in biology and microeconomics, but whereas in microeconomics, consistent choice underlies the notion of utility; in biology, the assumption of consistent selective pressures justifies modelling decision mechanisms {{as if they were}} designed to maximize fitness. In either case, violations of consistency contradict expectations and attract theoretical interest. Reported violations of rationality in non-humans include intransitivity (i. e. circular preferences) and lack of independence of irrelevant alternatives (changes in relative preference between options when embedded in different choice sets), but {{the extent to which these}} observations truly represent breaches of rationality is debatable. We tested both principles with starlings (Sturnus vulgaris), training subjects either with five options differing in food delay (exp. 1) or with six options differing in reward probability (exp. 2), before letting them choose repeatedly one option out of several binary and trinary sets of options. The starlings conformed to economic rationality on both tests, showing strong <b>stochastic</b> <b>transitivity</b> and no violation of the independence principle. These results endorse the rational choice and optimality approaches used in behavioural ecology, and highlight the need for functional and mechanistic enquiring when apparent violations of such {{principles are}} observed...|$|E
40|$|In {{different}} fields like decision making, psychology, game theory and biology, {{it has been}} observed that paired-comparison data like preference relations defined by humans and animals can be intransitive. Intransitive relations cannot be modeled with existing machine learning methods like ranking models, because these models exhibit strong transitivity properties. More specifically, in a stochastic context, where often the reciprocity property characterizes probabilistic relations such as choice probabilities, it has been formally shown that ranking models always satisfy the well-known strong <b>stochastic</b> <b>transitivity</b> property. Given this limitation of ranking models, we present a new kernel function that together with the regularized least-squares algorithm is capable of inferring intransitive reciprocal relations in problems where transitivity violations cannot be considered as noise. In this approach it is the kernel function that defines the transition from learning transitive to learning intransitive relations, and the Kronecker-product is introduced for representing the latter type of relations. In addition, we empirically demonstrate on two benchmark problems, one in game theory and one in theoretical biology, that our algorithm outperforms methods not capable of learning intransitive reciprocal relations. Transitivity Reciprocal relations Utility functions Kernel methods Preference learning Decision theory Game theory...|$|E
40|$|There {{are various}} {{parametric}} models for analyzing pairwise comparison data, including the Bradley-Terry-Luce (BTL) and Thurstone models, but their reliance on strong parametric assumptions is limiting. In this work, we study a flexible model for pairwise comparisons, {{under which the}} probabilities of outcomes are required only to satisfy a natural form of <b>stochastic</b> <b>transitivity.</b> This class includes parametric models including the BTL and Thurstone models as special cases, but is considerably more general. We provide various examples of models in this broader stochastically transitive class for which classical parametric models provide poor fits. Despite this greater flexibility, we show that the matrix of probabilities can be estimated {{at the same rate}} as in standard parametric models. On the other hand, unlike in the BTL and Thurstone models, computing the minimax-optimal estimator in the stochastically transitive model is non-trivial, and we explore various computationally tractable alternatives. We show that a simple singular value thresholding algorithm is statistically consistent but does not achieve the minimax rate. We then propose and study algorithms that achieve the minimax rate over interesting sub-classes of the full stochastically transitive class. We complement our theoretical results with thorough numerical simulations...|$|E
40|$|Normative {{models of}} choice usually predict {{preferences}} between alternatives by computing their value {{according to some}} criterion, then identifying the alternative with greatest value. An important consequence of this procedure is captured in the economic concept of rationality, defined {{through a number of}} principles that are necessary for the existence of an ordinal scale of value upon which organisms base their choices. Violations of these principles, such as some recently reported breaches of transitivity and regularity in birds and honeybees, have strong implications for the understanding of decision mechanisms in humans and nonhumans alike. We investigated rationality in risky choice using European starlings, Sturnus vulgaris. Birds had to choose between two or three food sources, each associated with a different variance in delay to reward. In three experiments, starlings were strongly risk prone, showing regular and consistent preferences in binary and trinary choices. Preferences also satisfied weak and strong <b>stochastic</b> <b>transitivity.</b> Our results extend the generality of previous research in risk-sensitive foraging to situations where more than two alternatives are present and suggest that violations of rationality in risk-sensitive choices may be expressed only under restricted sets of conditions. © 2002 The Association for the Study of Animal Behaviour. Published by Elsevier Science Ltd. All rights reserved...|$|E
40|$|AbstractIn domains like {{decision}} theory and social choice theory {{it is known}} for a long time that <b>stochastic</b> <b>transitivity</b> properties yield necessary and sufficient conditions for the ranking or utility representability of reciprocal preference relations. In this article we extend these results for reciprocal preference relations originating from the pairwise comparison of random vectors in a machine learning context. More specifically, the expected ranking accuracy (ERA) is such a reciprocal relation that occurs in multi-class classification problems, when ranking or utility functions are fitted to the data in a pairwise manner. We establish necessary and sufficient conditions for which these pairwise bipartite ranking functions can be simplified to a single ranking function such that the pairwise expected ranking accuracies of both models coincide. Similarly as for more common reciprocal preference relations, cycle transitivity plays a crucial role in this new setting. We first consider the finite sample case, for which expected ranking accuracy can be estimated by means of the area under the ROC curve (AUC), and subsequently, we further generalize these results to the underlying distributions. It turns out that the ranking representability of pairwisely compared random vectors can be expressed elegantly in a distribution-independent way by means of a specific type of cycle transitivity, defined by a conjunctor that is closely related to the algebraic product...|$|E
40|$|According to {{normative}} theories, reward-maximizing agents {{should have}} consistent preferences. Thus, {{when faced with}} alternatives A, B, and C, an individual preferring A to B and B to C should prefer A to C. However, it has been widely argued that humans can incur losses by violating this axiom of transitivity, despite strong evolutionary pres- sure for reward-maximizing choices. Here, adopting a biologically plausible computational framework, we show that intransitive (and thus economically irrational) choices paradoxically improve accuracy (and subsequent economic rewards) when decision formation is cor- rupted by internal neural noise. Over three experiments, we show that humans accumulate evidence over time using a “selective inte- gration” policy that discards information about alternatives with mo- mentarily lower value. This policy predicts violations of the axiom of transitivity when three equally valued alternatives differ circularly in their number of winning samples. We confirm this prediction in a fourth experiment reporting significant violations of weak <b>stochastic</b> <b>transitivity</b> in human observers. Crucially, we show that relying on selective integration protects choices against “late” noise that other- wise corrupts decision formation beyond the sensory stage. Indeed, we report that individuals with higher late noise relied more strongly on selective integration. These findings suggest that violations of ra- tional choice theory reflect adaptive computations that have evolved in response to irreducible noise during neural information processing...|$|E
