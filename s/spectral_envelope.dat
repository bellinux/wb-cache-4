604|175|Public
2500|$|The engine smooths the timbre {{around the}} {{junction}} of the samples. The timbre of a sustained vowel is generated by interpolating spectral envelopes of the surrounding samples. For example, when concatenating a sequence of diphones [...] "s-e, e, e-t" [...] of the English word [...] "set", the <b>spectral</b> <b>envelope</b> of a sustained ē at each frame is generated by interpolating ē {{in the end of}} [...] "s-e" [...] and ē in the beginning of [...] "e-t".|$|E
5000|$|... #Caption: Concatenation with <b>spectral</b> <b>envelope</b> {{interpolation}} (on Vocaloid) ...|$|E
5000|$|The changes both of <b>spectral</b> <b>envelope</b> (formant-glide) and {{fundamental}} frequency (micro-intonation) ...|$|E
40|$|It is {{well known}} that <b>spectral</b> <b>envelopes</b> of brasstones vary with {{performed}} dynamic [D. Luce and M. Clark, J. Acoust. Soc. Am. 42, 1232 – 1243 (1967) ]. In this study average <b>spectral</b> <b>envelopes</b> were computed as functions of normalized instantaneous rms amplitude using time‐variant spectral data from a group of 15 trumpet swell tones with fundamental frequencies extending over a 2 ‐octave range. It was found that 10 <b>spectral</b> <b>envelopes,</b> defined by amplitudes in each of 24 standard critical bands spanning the frequency range 0 to 10 [*] 000 Hz, were sufficient to characterize the tone set. Time‐variant spectra were synthesized using the original rms amplitude‐versus‐time envelopes as driving functions to dynamically select and interpolate between the appropriate <b>spectral</b> <b>envelopes</b> for each moment of time. The time‐variant spectra were then converted into sound signals using conventional additive synthesis. Since the synthetic tones sound almost identical to the originals, it appears that dynamic <b>spectral</b> <b>envelopes</b> are an important defining characteristic of trumpettone behavior...|$|R
40|$|Abstract—This paper {{presents}} a Kalman-filter approach for computing <b>spectral</b> <b>envelopes</b> of current waveforms for nonin-trusive load monitoring on the electric utility. <b>Spectral</b> <b>envelopes</b> represent time-varying frequency content and {{phase of the}} current relative to the voltage. Thus, the techniques {{presented in this paper}} may be applicable to a variety of lock-in measurement and signal processing techniques. The lock-in and computational per-formance of the proposed method favorably compares to previous efforts. The performance is demonstrated with data from the field. Index Terms—Load monitoring, power quality, synchronous detection. I...|$|R
40|$|The aim of {{the three}} psychoacoustic {{experiments}} described here was to clarify whether there are speaker individualities in the <b>spectral</b> <b>envelopes,</b> in which frequency bands such individualities exist, and how frequency bands having speaker individualities can be manipulated. The LMA analysis-synthesis system was used to prepare stimuli varied specific frequency bands, and the frequency bands having speaker individualities were estimated expermentally. The results indicate that (1) speaker individualities exist in <b>spectral</b> <b>envelopes,</b> (2) these individualities are mainly at frequencies higher than 22 ERB rate (2212 Hz) and vowel characteristics exist from 12 ERB rate (603 Hz) to 22 ERB rate, and (3) the voice quality can be controlled by replacing the higher frequency band of one talker with that of other talkers. The replace point is the adjacent spectral local minimum below the spectral local maximum around 23 ERB rate in the <b>spectral</b> <b>envelopes...</b>|$|R
50|$|A <b>spectral</b> <b>{{envelope}}</b> is {{the envelope}} {{curve of the}} amplitude spectrum. It describes {{one point in time}} (one window, to be precise).|$|E
50|$|In voiced segments, the {{residual}} signal {{is represented by}} two parameters: the pitch period and the <b>spectral</b> <b>envelope.</b> The pitch period is estimated from the peak values of the autocorrelation of {{the residual}} signal. In this process, the residual signal is compared against shifted copies of itself, and the shift which yields the greatest similarity by measure of linear dependence is identified as the pitch period. The <b>spectral</b> <b>envelope</b> is represented {{by a set of}} amplitude values, one per harmonic. To extract these values, the LPC residual signal is transformed into the DFT-domain. The DFT-spectrum is segmented into bands, one band per harmonic. The frequency band for the m-th harmonic consists of the DFT-coefficients from (m-1/2)ω0 to (m+1/2)ω0, ω0 being the pitch frequency. The amplitude value for the m-th harmonic is chosen to optimally represent these DFT-coefficients. Phase information is discarded in this process. The <b>spectral</b> <b>envelope</b> is then coded using variable-dimension weighted vector quantization. This process is also referred to as Harmonic VQ.|$|E
50|$|Harmonic and Individual Lines and Noise (HILN) is a {{parametric}} codec for audio. The {{basic premise}} of the encoder is that most audio, and particularly speech, can be synthesized from only sinusoids and noise. The encoder describes individual sinusoids with amplitude and frequency, harmonic tones by fundamental frequency, amplitude and the <b>spectral</b> <b>envelope</b> of the partials, and the noise by amplitude and <b>spectral</b> <b>envelope.</b> This type of encoder is capable of encoding audio to between 6 and 16 kilobits per second for a typical audio bandwidth of 8 kHz. The framelength of this encoder is 32 ms.|$|E
40|$|The {{subject of}} this paper is the {{conversion}} of a given speaker’s voice (the source speaker) into another identified voice (the target one). We assume we have at our disposal a large amount of speech samples from source and target voice with at least a part of them being parallel. The proposed system is built on a mapping function between source and target <b>spectral</b> <b>envelopes</b> followed by a frame selection algorithm to produce final <b>spectral</b> <b>envelopes.</b> Converted speech is produced by a basic LP analysis of the source and LP synthesis using the converted <b>spectral</b> <b>envelopes.</b> We compared three types of conversion: without mapping, with mapping and using the excitation of the source speaker and finally with mapping using the excitation of the target. Results show that the combination of mapping and frame selection provide the best results, and underline the interest to work on methods to convert the LP excitation. Index Terms — Voice conversion, frame selection, voice mapping 1...|$|R
30|$|<b>Spectral</b> <b>envelopes</b> {{extracted}} by STRAIGHT analysis [26] {{are used}} in the source and target features. The other features extracted by STRAIGHT analysis, such as F 0 and the aperiodic components, are used to synthesize the converted signal without any conversion.|$|R
40|$|INTERSPEECH 2007 : 8 th Annual Conference of the International Speech Communication Association, August 27 - 31, 2007, Antwerp, Belgium. We {{proposed}} a speaking aid system using statistical voice conversion for laryngectomees, whose vocal folds have been removed. This paper investigates {{the influence of}} various small sound sources on the voice conversion accuracy. <b>Spectral</b> <b>envelopes</b> and power of sound sources are controlled independently. In total 8 different kinds of sound source signals, e. g. pulse train, sierra wave and so on, are examined. Results of objective and subjective evaluations demonstrate that for voice conversion, sound sources with various <b>spectral</b> <b>envelopes</b> and power in a large degree are acceptable unless the power of them is {{comparable to that of}} silence parts...|$|R
50|$|In {{remote sensing}} using a spectrometer, the <b>spectral</b> <b>envelope</b> of a {{feature is the}} {{boundary}} of its spectral properties, {{as defined by the}} range of brightness levels in each of the spectral bands of interest.|$|E
50|$|It can be {{combined}} with any audio compression codec: the codec itself transmits the lower and midfrequencies of the spectrum, while SBR replicates higher frequency content by transposing up harmonics from the lower and midfrequencies at the decoder. Some guidance information for reconstruction of the high-frequency <b>spectral</b> <b>envelope</b> is transmitted as side information.|$|E
5000|$|The {{most common}} speech coding scheme is Code Excited Linear Prediction (CELP) coding, {{which is used}} for example in the GSM standard. In CELP, the {{modelling}} is divided in two stages, a linear predictive stage that models the <b>spectral</b> <b>envelope</b> and code-book based model of the residual of the linear predictive model.|$|E
40|$|This work {{introduces}} a maximum-likelihood based model order (MO) selection technique for <b>spectral</b> <b>envelopes</b> to apply speaker dependent adaptation in the feature-space similar to vocal tract length normalization. Speech recognition systems based on <b>spectral</b> <b>envelopes</b> {{are using a}} fixed MO for the underlying linear parametric model. Using a fixed MO over different speakers or channels might not be optimal. To address this problem we investigated the use of warped and scaled minimum variance distortionless response spectral estimation techniques with speaker dependent MOs based on a maximum-likelihood criteria. Comparing experimental results on the Translanguage English Database we can show an improvement by 1, 9 % relative compared to the word error rate by the fixed MO and 3, 5 % relative to the traditional Mel-frequency cepstral coefficients. 1...|$|R
40|$|This paper {{presents}} an articulatory-acoustic mapping where detailed <b>spectral</b> <b>envelopes</b> are estimated. During the estimation, the harmonics {{of a range}} of F 0 values are derived from the spectra of multiple voiced speech signals vocalized with similar articulator settings. The envelope formed by these harmonics is represented by a cepstrum, which is computed by fitting the peaks of all the harmonics based on the weighted least square method in the frequency domain. The experimental result shows that the <b>spectral</b> <b>envelopes</b> are estimated with the highest accuracy when the cepstral order is 48 - 64 for a female speaker, which suggests that representing the real response of the vocal tract requires high-quefrency elements that conventional speech synthesis methods are forced to discard in order to eliminate the pitch component of speech. 1...|$|R
40|$|Abstract—The paper {{presents}} {{a framework for}} representing sinusoidal and harmonic components by means of linear prediction. In order to improve estimation accuracy parametric representation of the signal is used instead of its waveform. The main target application of the proposed technique is accurate estimation of <b>spectral</b> <b>envelopes</b> in hybrid (stochastic/deterministic) speech processing. I...|$|R
50|$|Alternatively, the {{reconstruction}} {{process can be}} greatly simplified if the information desired is encoded in the <b>spectral</b> <b>envelope</b> of the input signal instead of the temporal envelope. In such a scenario, the true output can be reconstructed simply by directly de-warping the measured output given the designed warp kernel. This has been achieved experimentally for optical image compression.|$|E
50|$|LPC is {{frequently}} used for transmitting <b>spectral</b> <b>envelope</b> information, {{and as such}} {{it has to be}} tolerant of transmission errors. Transmission of the filter coefficients directly (see linear prediction for definition of coefficients) is undesirable, since they are very sensitive to errors. In other words, a very small error can distort the whole spectrum, or worse, a small error might make the prediction filter unstable.|$|E
50|$|Since {{the late}} 1970s, most non-musical vocoders have been {{implemented}} using linear prediction, whereby the target signal's <b>spectral</b> <b>envelope</b> (formant) is estimated by an all-pole IIR filter. In linear prediction coding, the all-pole filter replaces the bandpass filter bank of its predecessor and is used at the encoder to whiten the signal (i.e., flatten the spectrum) and again at the decoder to re-apply the spectral shape of the target speech signal.|$|E
40|$|The X-ray {{absorption}} {{near edge}} structure of Ti in 16 oxides and silicates has been collected and analyzed by computer fitting Gaussian {{lines to the}} <b>spectral</b> <b>envelopes.</b> Comparison of feature intensities and energies with geometric site parameters reveals that the near edge changes systematically with site symmetry variation and mean Ti-O bond length...|$|R
40|$|Abstract—In many speech-coding-related problems, {{there is}} {{available}} information and lost information {{that must be}} recovered. When there is significant correlation between the available and the lost information source, coding with side information (CSI) {{can be used to}} benefit from the mutual information between the two sources. In this paper, we consider CSI as a special VQ problem which will be referred to as conditional vector quantization (CVQ). A fast two-step divide-and-conquer solution is proposed. CVQ is then used in two applications: the recovery of highband (4 – 8 kHz) <b>spectral</b> <b>envelopes</b> for speech spectrum expansion and the recovery of lost narrowband <b>spectral</b> <b>envelopes</b> for voice over IP. Comparisons with alternative approaches like estimation and simple VQ-based schemes show that CVQ provides significant distortion reductions at very low bit rates. Subjective evaluations indicate that CVQ provides noticeable perceptual improvements over the alternative approaches. I...|$|R
40|$|Abstract-A {{new method}} is {{introduced}} for parametric modeling of <b>spectral</b> <b>envelopes</b> when only a discrete set of spectral points is given. This method, {{which we call}} discrete all-pole (DAP) modeling, uses a discrete version of the Itakura-Saito distortion measure as its error criterion. One result is a new autocorrelation matching condition that overcomes the limitations of linear prediction and produces better fitting <b>spectral</b> <b>envelopes</b> for spectra that are representable by a relatively small discrete set of values, such as in voiced speech. We present an iterative algorithm for DAP modeling that is shown to converge to a unique global minimum. We also present results of applying DAP modeling to real and synthetic speech. DAP modeling is extended to allow frequency-dependent weighting of the error measure, so that spectral accuracy can be enhanced in certain frequency regions relative to others. 1...|$|R
50|$|Linear {{predictive}} coding (LPC) {{is a tool}} used mostly in audio signal processing and speech processing for representing the <b>spectral</b> <b>envelope</b> of a digital signal of speech in compressed form, using the information of a linear predictive model. It {{is one of the}} most powerful speech analysis techniques, and one of the most useful methods for encoding good quality speech at a low bit rate and provides extremely accurate estimates of speech parameters.|$|E
5000|$|The engine smooths the timbre {{around the}} {{junction}} of the samples. The timbre of a sustained vowel is generated by interpolating spectral envelopes of the surrounding samples. For example, when concatenating a sequence of diphones [...] "s-e, e, e-t" [...] of the English word [...] "set", the <b>spectral</b> <b>envelope</b> of a sustained ē at each frame is generated by interpolating ē {{in the end of}} [...] "s-e" [...] and ē in the beginning of [...] "e-t".|$|E
50|$|Today’s “digital” {{radio that}} carries coded data over various phase modulations (such as GMSK, QPSK etc.) {{and also the}} {{increasing}} demand for spectrum have forced {{a dramatic change in}} the way radio is used, e.g. the cellular radio concept. Today’s cellular radio and digital broadcast standards are extremely demanding in terms of the <b>spectral</b> <b>envelope</b> and out of band emissions that are acceptable (in the case of GSM for example, &minus;70 dB or better just a few hundred kilohertz from center frequency). Digital transmitters must therefore operate in the linear modes, with much attention given to achieving low distortion.|$|E
30|$|The method {{assumes that}} the <b>spectral</b> <b>envelopes</b> of the {{analysed}} sounds tend to vary smoothly {{as a function of}} frequency. The spectral smoothness principle has successfully been used in different ways in the literature [7, 26 – 29]. A novel smoothness measure based on the convolution of the hypothetical harmonic pattern with a Gaussian window is proposed.|$|R
40|$|We {{have found}} {{analytical}} expressions for the envelopes of maxima and minima of interference reflection and transmission spectra for both transparent and absorbing two- and three-layer structures under {{the conditions of}} arbitrary light incidence and polarization state. General regularities for the shape of those <b>spectral</b> <b>envelopes</b> are established, depending on the optical thicknesses of constituent layers...|$|R
40|$|In this paper, we {{investigate}} the dependency between the <b>spectral</b> <b>envelopes</b> of speech in disjoint frequency bands, one covering the telephone bandwidth from 0. 3 kHz to 3. 4 kHz and one covering the frequencies from 3. 7 kHz to 8 kHz. The <b>spectral</b> <b>envelopes</b> are jointly modeled with a Gaussian mixture model based on mel-frequency cepstral coefficients and the log-energy-ratio of the disjoint frequency bands. Using this model, we quantify the dependency between bands through their mutual {{information and the}} perceived entropy of the high frequency band. Our {{results indicate that the}} mutual information is {{only a small fraction of}} the perceived entropy of the high band. This suggests that speech bandwidth extension should not rely only on mutual information between narrow- and high-band spectra. Rather, such methods need to make use of perceptual properties to ensure that the extended signal sounds pleasant...|$|R
50|$|Each Shepard tone {{consists}} {{of a set of}} octave-related sinusoids, whose amplitudes are scaled by a fixed bell-shaped <b>spectral</b> <b>envelope</b> based on a log frequency scale. For example, one tone might consist of a sinusoid at 440 Hz, accompanied by sinusoid at the higher octaves (880 Hz, 1760 Hz, etc.) and lower octaves (220 Hz, 110 Hz, etc.). The other tone might consist of a 311 Hz sinusoid, again accompanied by higher and lower octaves (622 Hz, 155.5 Hz, etc.). The amplitudes of the sinusoids of both complexes are determined by the same fixed-amplitude envelope - for example, the envelope might be centered at 370 Hz and span a six-octave range.|$|E
5000|$|According to Robert Erickson (1975, p. 95) What?? {{contains}} [...] "no obvious discrete changes" [...] and [...] "no sharply defined sections". The piece uses several drone pitches, {{though they}} are presented at a time scale where pitch change is [...] "hardly noticed." [...] Interference beats are highlighted (as the only apparent rhythm) and, as [...] "an elegant touch," [...] the microtonal pitches that produce those beats [...] "are worked into a larger pattern of pitch relations". [...] "All possible timbral dimensions are manipulated: <b>spectral</b> <b>envelope,</b> including harmonic and inharmonic partials; time envelope phenomena, such as beats and tremolo; micropitch changes, both fast and slow. Transformations between pitch (with timbre) → chord, chord → 'a sound', 'a sound' → pitch (with timbre) abound." ...|$|E
40|$|Abstract—Although {{considerable}} {{effort has}} been devoted to both fundamental frequency and <b>spectral</b> <b>envelope</b> estimation in the field of speech processing, the problem of determining and spectral envelopes has largely been tackled independently. If were known in advance, then the <b>spectral</b> <b>envelope</b> could be esti-mated very reliably. On the other hand, if the <b>spectral</b> <b>envelope</b> were known in advance, then we could obtain a reliable esti-mate. and the <b>spectral</b> <b>envelope,</b> each of which is a prerequisite of the other, should thus be estimated jointly rather than indepen-dently in succession. On this basis, we develop a parametric speech spectrum model that allows us to estimate the and spectral enve-lope simultaneously. We confirmed experimentally the significant advantage of this joint estimation approach for both estimation and <b>spectral</b> <b>envelope</b> estimation. Index Terms—Expectation–maximization (EM) algorithm, estimation, <b>spectral</b> <b>envelope</b> estimation, speech analysis...|$|E
40|$|We {{present a}} method for {{morphing}} between smooth <b>spectral</b> magnitude <b>envelopes</b> of speech. An important element of our method {{is the notion of}} audio flow, which is inspired by similar notions of optical flow computed between images in computer vision applications. Audio flow defines the correspondence between two smooth <b>spectral</b> magnitude <b>envelopes,</b> and encodes the formant shifting that occurs from one sound to another. We present several algorithms for the automatic computation of audio flow from a small 20 second corpus of speech. In addition, we present an algorithm for morphing smoothly between any two <b>spectral</b> magnitude <b>envelopes,</b> given the computed audio flow between them. 1...|$|R
30|$|The {{proposed}} GB conversion, as {{most other}} methods, simply replaces the <b>spectral</b> <b>envelopes</b> {{extracted from the}} source signal with the converted outcome. As a result, the synthesized output has the same speaking rate as the source speaker. Further improvement {{can be obtained by}} modifying the duration of each converted utterance to match, on average, its corresponding value for the target speaker.|$|R
40|$|We {{present a}} new {{additive}} synthesis method based on <b>spectral</b> <b>envelopes</b> and inverse Fast Fourier Transform (FFT - 1). User control is {{facilitated by the}} use of <b>spectral</b> <b>envelopes</b> to describe the characteristics of the short term spectrum of the sound in terms of sinusoidal and noise components. Such characteristics can be given by users or obtained automatically from natural sounds. Use of the inverse FFT reduces the computation cost by a factor on the order of 15 compared to oscillators. We propose a low cost real-time synthesizer design allowing processing of recorded and live sounds, synthesis of instruments and synthesis of speech and the singing voice. Introduction Many musical sound signals may be described as a combination of a pseudoperiodic waveform and of colored noise [1]. The pseudo-periodic part of the signal {{can be viewed as a}} sum of sinusoidal components, named partials, with time-varying frequency and amplitude [2]. Such sinusoidal components are easily observed on [...] ...|$|R
