32|198|Public
5000|$|In an X9.95 trusted {{timestamp}} scheme, {{there are}} five entities: the time <b>source</b> <b>entity,</b> the Time Stamp Authority, the requestor, the verifier, and a relying party.|$|E
50|$|At {{the end of}} the Death of the New Gods mini-series, {{with all}} the gods now dead, the <b>Source</b> <b>entity</b> merges Apokolips and New Genesis into a single planet and {{indicates}} that it will possibly be repopulated.|$|E
5000|$|Before a timestamp-service commences operations, the Time Stamp Authority {{calibrates}} its clock(s) with an upstream time <b>source</b> <b>entity,</b> such as {{a legally}} defined master clock for the jurisdiction the TSA is time-stamping evidence for. When trusted time has been acquired, the TSA can issue timestamps for unsigned and digitally signed data based {{on all of the}} jurisdictions it maintains timing solutions for.|$|E
40|$|This paper {{proposes a}} {{framework}} for generating sizing function in meshing assemblies. Size control is crucial in obtaining a high-quality mesh with a reduced number of elements, which decreases computational time and memory use during mesh generation and analysis. This proposed framework is capable of generating a sizing function based on geometric and non-geometric factors that influence mesh size. The framework consists of a background octree grid for storing the sizing function, a set of <b>source</b> <b>entities</b> for providing sizing information based on geometric and non-geometric factors, and an interpolation module for calculating the sizing on the background octree grid using the <b>source</b> <b>entities.</b> <b>Source</b> <b>entities</b> are generated by performing a detailed systematic study to identify all the geometric factors of anassembly. Disconnected skeletons are extracted and used as tools to measure 3 D-proximity and 2 D-proximity, which {{are two of the}} geometric factors. Non-geometric factors such as user-defined size and pre-meshed entities that influence size are also addressed. The framework is effective in generating a variety of meshes of industry models with less computational cost...|$|R
40|$|MEntoR is {{a source}} code {{analysis}} tool that is integrated into the development environment (IDE) to suggest implementation improvements. MEntoR is a front-end for Prospector. Prospector extracts properties of <b>source</b> code <b>entities</b> of an application, and calculates association rules which indicate which properties tend to occur together. The purpose of MEntoR is to identify missing properties in the <b>source</b> code <b>entity</b> that is being browsed in the IDE based on other properties that the entity has. For instance, showing whenever a <b>source</b> code <b>entity</b> violates an idiom...|$|R
40|$|This paper {{describes}} {{the creation of}} a new type of size function – the mesh size function that honors the existing mesh on premeshed geometry entities and radiates the mesh sizes from the premeshed <b>source</b> <b>entities</b> to the attached entities – from the technology of using background overlay grids. The creation of faceted meshes from premeshed <b>source</b> <b>entities</b> (i. e. edges or faces) is presented in a more general way, which allows the use of existing procedure of size function implementations. The introduction of the mesh size function has greatly enhanced the capabilities of the three types of size functions that were already available (including a fixed size function, a curvature size function and a proximity size function) and provided nice solutions to the situations where the old size functions did not work desirably. Meshing results of the new size function with controlled mesh sizes are given...|$|R
5000|$|Time <b>source</b> <b>entity</b> - Most {{countries}} have an official source {{of time and}} this has been codified over the last hundred years through any number of Mutual Recognition Agreement's and Legal Metrological Agreements (see http://www.oiml.org for more information on Legal Metrology). Why this is important is now that the Internet has made it possible to reach directly into the laboratory that operates the official source of time for that jurisdiction, the many layers of [...] "middlemen” who stood between the end-user and the source of time are now gone. As such, time that can be shown as traceable to the specific national measurement institute or master clock of that jurisdiction is the only source that provides the approved [...] "Time Calibration Source" [...] for X9.95. Examples include NIST in the US and Bureau International des Poids et Mesures (BIPM). Other regulatory frameworks also require that time that is moved through the Network Time Protocol ntp is properly certified and authenticated meaning unauthenticated use of time from any provider will fail X9.95 requirements for obtaining time in a provable manner.|$|E
5000|$|Taking {{place in}} both the yearlong series Countdown to Final Crisis (2007-2008) and its spin-off, Death of the New Gods, written by Jim Starlin, was a story-arc {{involving}} the mysterious deaths of the New Gods across the universe {{in preparation for the}} coming storylines in Grant Morrison's Final Crisis, published later in 2008. As elaborated in Death of the New Gods, the mysterious Godkiller {{turned out to be an}} agent of the sentient Source itself, which sought to destroy the imperfect Fourth World — compromised by the disruption in its creation by the Old Gods — in favor of a more perfect [...] "Fifth World" [...] by reuniting the Source with the Anti-Life Equation. The Source's initial attempts to recreate the Fifth World had been hampered by the Crisis on Infinite Earths which unified the Multiverse and forged an impenetrable Source Wall around the Anti-Life Equation. By subtly manipulating characters such as Alexander Luthor, Jr., and Booster Gold to recreate the Multiverse made the Source Wall less impenetrable. The Source's agent is revealed to be the New God Infinity-Man. Darkseid acquires the powers of the Anti-Life Equation and capitalizes on the deaths of the New Gods by using the human Jimmy Olsen as a [...] "soul-catcher" [...] for the Gods, from which he can claim all their powers and recreate the universe in his own image, but he is killed when the Source is able to send Darkseid's resurrected son, Orion, to rip out his heart. Orion leaves the scene of the fray to die of his own wounds; and, seemingly with success, the <b>Source</b> <b>entity</b> manages to reunite with the Anti-Life entity and merge Apokolips with New Genesis to create the Fifth World, with the New Gods of the Fourth World all deceased.|$|E
40|$|We {{report on}} {{experiments}} for the Related Entity Finding task {{in which we}} focus on only using Wikipedia as a target corpus in which to identify (related) entities. Our approach is based on co-occurrences between the <b>source</b> <b>entity</b> and potential target entities. We observe improvements in performance when a context-independent co-occurrence model is combined with context-dependent co-occurrence models in which we {{stress the importance of}} the expected relation between source and target entity. Applying type filtering yields further improvements results...|$|E
40|$|Goal {{and source}} {{thematic}} roles {{have been shown}} to influence pronoun resolution, an effect that has been linked to the reader’s tendency to focus on the consequences of the event (Stevenson, Crawley, & Kleinman, 1994). Using a story continuation ex-periment, I show that speakers also tend to use pronouns more often for goal <b>entities</b> than <b>source</b> <b>entities.</b> Furthermore, the experiment and a corpus analysis reveal that speakers tend to refer more frequently to goal <b>entities</b> than <b>source</b> <b>entities</b> overall. I use the parallel findings about pronoun use and frequency of reference continuation to argue that referent accessibility is influenced by the comprehender’s estimate of the likelihood that a referent will be continued in the discourse. Pronoun comprehension has been argued to be influenced by the accessibility of potential referents in the discourse representation, which is driven by a number of factors (see Arnold, 1998, for a review). One such factor that has received atten-tion is the thematic roles of discourse referents (e. g., Garnham, Traxler, Oakhill...|$|R
40|$|Abstract. Due to the {{increasing}} of software requirements and software features, modern software systems continue to grow in size and complexity. Locating <b>source</b> code <b>entities</b> that required to implement a feature in millions lines of code is labor and cost intensive for developers. To this end, several studies have pro-posed the use of Information Retrieval (IR) to rank <b>source</b> code <b>entities</b> based on their textual similarity to an issue report. The ranked <b>source</b> code <b>entities</b> could be at a class or function granularity level. <b>Source</b> code <b>entities</b> at the class-level are usually large in size and might contain a lot of functions that are not implemented for the feature. Hence, we conjecture that the class-level feature location tech-nique requires more effort than function-level feature location technique. In this paper, we investigate the impact of granularity levels on a feature location tech-nique. We also presented a new evaluation method using effort-based evaluation. The results indicated that function-level feature location technique outperforms class-level feature location technique. Moreover, function-level feature location technique also required 7 times less effort than class-level feature location tech-nique to localize the first relevant <b>source</b> code <b>entity.</b> Therefore, we conclude that feature location technique at the function-level of program elements is effective in practice...|$|R
30|$|For {{classical}} Chinese <b>sources</b> organizational <b>entities</b> in {{the sense}} of the NER framework are of little relevance and will not be discussed here.|$|R
40|$|Related entity {{finding is}} the task of {{returning}} a ranked list of homepages of relevant entities of a specified type that need {{to engage in a}} given relationship with a given <b>source</b> <b>entity.</b> We propose a framework for addressing this task and perform a detailed analysis of four core components; co-occurrence models, type filtering, context modeling and homepage finding. Our initial focus is on recall. We analyze the performance of a model that only uses cooccurrence statistics. While it identifies a set of related entities, it fails to rank them effectively. Two types of error emerge: (1) entities of the wrong type pollute the ranking and (2) while somehow associated to the <b>source</b> <b>entity,</b> some retrieved entities do not engage in the right relation with it. To address (1), we add type filtering based on category information available in Wikipedia. To correct for (2), we add contextual information, represented as language models derived from documents in which source and target entities co-occur. To complete the pipeline, we find homepages of top ranked entities by combining a language modeling approach with heuristics based on Wikipedia’s external links. Our method achieves very high recall scores on the end-to-end task, providing a solid starting point for expanding our focus to improve precision; additional heuristics lead to state-of-the-art performance...|$|E
40|$|International audienceConsidering {{the high}} {{heterogeneity}} of the ontologies pub-lished on the web, ontology matching {{is a crucial}} issue whose aim is to establish links between an entity of a source ontology and one or several entities from a target ontology. Perfectible similarity measures, consid-ered as sources of information, are combined to establish these links. The theory of belief functions is a powerful mathematical tool for combining such uncertain information. In this paper, we introduce a decision pro-cess based on a distance measure to identify the best possible matching entities for a given <b>source</b> <b>entity...</b>|$|E
40|$|Abstract—MBMS is a {{unidirectional}} point-to-multipoint {{bearer service}} in which data are transmitted {{from a single}} <b>source</b> <b>entity</b> to multiple recipients. For a mobile to support the MBMS, MBMS client functions as well as MBMS radio protocols should be designed and implemented. In this paper, we analyze the MBMS client functions and describe the implementation of them in our mobile test-bed. User operations and signaling flows between protocol entities to control the MBMS functions are designed in detail. Service announcement utilizing the file download MBMS service and four MBMS user services are demonstrated in the test-bed to verify the MBMS client functions...|$|E
5000|$|The Office of Open Source Information {{would be}} {{established}} under the Deputy Director of National Intelligence for Estimates and Analysis. [...] All current open <b>source</b> <b>entities</b> would be consolidated under this Act, and {{be led by}} a DNI appointed director. [...] The office would manage a single open source program and budget, coordinate collection, analysis, and dissemination of information with potential intelligence value. [...] Under McCurdy’s proposal, the Office of Open Source Information would be the sole entity for all open source activity.|$|R
40|$|Abstract. Well-established {{instruments}} such as authority {{files and}} a growing set of data structures such as CIDOC CRM, FRBRoo, and MODS provide the foundation for emerging, new digital services. While solid, these instruments alone neither capture the essential data on which traditional scholarship depends nor enable the services which we can already identify as fundamental to any eResearch, cyberinfrastructure or virtual research environment for intellectual discourse. This paper describes a general model for primary <b>sources,</b> <b>entities</b> and thematic topics, the gap between this model and emerging infrastructure, and the tasks necessary to bridge it. ...|$|R
40|$|In {{the context}} of ontology-based {{information}} extraction, identity resolution {{is the process of}} deciding whether an instance extracted from text refers to a known entity in the target domain (e. g. the ontology). We present an ontology-based framework for identity resolution which can be customised to different application domains and extraction tasks. Rules for identify resolution, which compute similarities between target and <b>source</b> <b>entities</b> based on class information and instance properties and values, can be defined for each class in the ontology. We present a case study of the application of the framework to the problem of multi-source job vacancy extraction. 1...|$|R
40|$|Abstract – This paper {{introduces}} {{the concept of}} asserted resolution as a technique for entity resolution. In asserted resolution trusted information sources are used to force the equivalence (or non-equivalence) of entity references and identity structures regardless of matching conditions. The paper proposes five specific forms of assertion to support entity identity information management, the process of storing and maintaining identity information in an information system so that references to an entity can be recognized and labeled with the same identifier over time. The paper also gives {{a description of the}} way in which asserted resolution is being implemented in the OYSTER open <b>source</b> <b>entity</b> resolution system...|$|E
40|$|Object-oriented model, {{conceptual}} warehouse design, temporal information. We {{develop a}} decision support system, {{based on the}} data warehousing approach. We define a conceptual object-oriented data warehouse model, which describes the data warehouse as a central repository of relevant, complex and temporal data. Our model integrates three concepts: warehouse object, environment and warehouse class. Each warehouse object representing a <b>source</b> <b>entity,</b> is composed of one current state, several past states (modelling its detailed evolutions) and several archive states (modelling its evolutions within a summarised form). The environment concept defines temporal parts in a data warehouse schema with significant granularities (attribute, class, sub graph). We also provide seven functions {{that are used to}} define warehouse classes. ...|$|E
40|$|This paper {{addresses}} {{the problem of}} related entity finding, which was proposed in trec 2009. The overall aim of related entity finding (REF) is to perform entity-related search on Web data, which address common information needs that are not that well modeled as ad hoc document search. In this paper, a novel framework was proposed based on a probabilistic model for related entity finding in a Web collection. This model consists of two parts. One is the probability indicating {{the relation between the}} <b>source</b> <b>entity</b> and the candidate entities. The other is the probability indicating the relevance between the candidate entities and the topic. Using ClueWeb 09 dataset, the experimental evaluations show the effectiveness of our REF framework. 1...|$|E
40|$|We study a {{class of}} graph {{analytics}} SQL queries, which we call relationship queries. Relationship queries are a wide superset of fixed-length graph reachability queries and of tree pattern queries. Intuitively, it discovers target entities that are reachable from <b>source</b> <b>entities</b> specified by the query. It usually also finds aggregated scores, which correspond to the target entities and are calculated by applying aggregation functions on measure attributes, which are found on the target <b>entities,</b> the <b>source</b> <b>entities</b> and the paths from the sources to the targets. We present real-world OLAP scenarios, where efficient relationship queries are needed. However, row stores, column stores and graph databases are unacceptably slow in such OLAP scenarios. We briefly comment on the straightforward extension of relationship queries that allows accessing arbitrary schemas. The GQ-Fast in-memory analytics engine utilizes a bottom-up fully pipelined query execution model running on a novel data organization that combines salient features of column-based organization, indexing and compression. Furthermore, GQ-Fast compiles its query plans into executable C++ source codes. Besides achieving runtime efficiency, GQ-Fast also reduces main memory requirements because, unlike column databases, GQ-Fast selectively allows more dense forms of compression including heavy-weighted compressions, which do not support random access. We used GQ-Fast to accelerate queries for two OLAP dashboards in the biomedical field. It outperforms Postgres by 2 - 4 orders of magnitude and outperforms MonetDB and Neo 4 j by 1 - 3 orders of magnitude when {{all of them are}} running on RAM. In addition, it generally saves space due to the appropriate use of compression methods. Comment: 22 page...|$|R
2500|$|The first {{recorded}} {{name for the}} Duchy was [...] "Land of the Croats" [...] (...) [...] Croatia was not yet a kingdom {{at the time and}} the term regnum is used in terms of a country in general. In Byzantine <b>sources</b> the <b>entity</b> was usually called just [...] "Croatia" [...] (...) [...]|$|R
40|$|The Department of Energy is {{preparing}} {{spent nuclear fuel}} (SNF) for interim storage at the major SNF sites. At the same time, work is proceeding to analyze the requirements for disposal of the SNF in a geologic repository, currently proposed to be located at Yucca Mountain in Nevada. To assist with the placement of SNF in either interim storage or the repository, certain technologies must be developed and implemented {{to assure that the}} storage can be safely and efficiently achieved. Technology development funding is diffused through a variety of resources within the DOE complex. A tool is required to show the integration of technology development activities with each of the funding <b>sources,</b> show the <b>entities</b> performing the development work, and demonstrate how the technology development assists with the interim storage and final disposition of SNF. A series of requirements for this tool were defined and a tool developed to assist with showing the required information. The tool has taken the form of Technology Development Maps that link development information, funding <b>sources,</b> <b>entities</b> performing development activities, and the material disposition path for each SNF type. These maps will be maintained as living documents to assist with integrating development activities for the SNF program...|$|R
40|$|Abstract. One of {{the major}} {{prerequisites}} for Long Term Evolution (LTE) networks is the mass provision of multimedia services to mobile users. To this end, Evolved- Multimedia Broadcast/Multicast Service (E-MBMS) is envisaged to play an instrumental role during LTE standardization process and ensure LTE’s proliferation in mobile market. E-MBMS targets at the economic delivery, in terms of power and spectral efficiency, of multimedia data from a single <b>source</b> <b>entity</b> to multiple destinations. This paper proposes a novel mechanism for efficient radio bearer selection during E-MBMS transmissions in LTE networks. The proposed mechanism {{is based on the}} concept of transport channels combination in any cell of the network. Most significantly, the mechanism manages to efficiently deliver multiple E-MBMS sessions. The performance of the proposed mechanism is evaluated and compared with several radio bearer selection mechanisms in order to highlight the enhancements that it provides. ...|$|E
40|$|There are {{a number}} of models for access control in {{pervasive}} environments that are based on trust propagation. Iterative multiplication of the trust values on a path from a <b>source</b> <b>entity</b> to a target entity is one of the common strategies for trust propagation. In this paper, we evaluate the effectiveness of iterative multiplication for trust propagation. The data set used for this evaluation is the real web of trust of Advogato. org that comprises of over 11, 000 vertices (users) and over 50, 000 directed weighted edges (trust relationships between users). We find that a significantly strong positive linear correlation exists between trust values based on direct experience and the corresponding propagated trust values derived through the iterative multiplication approach. This finding provides empirical support for the access control models for pervasive environments that employ the iterative multiplication strategy for trust propagation...|$|E
40|$|Multimedia Broadcast/Multicast Services (MBMS), {{introduced}} in Universal Mobile Telecommunication System (UMTS), have the aim to allow transmissions {{from a single}} <b>source</b> <b>entity</b> to multiple destinations. From the radio perspective, MBMS foresees both pointto- point (PtP) and point-to-multipoint (PtM) transmission mode, supported by Dedicated, Common, and Shared channels. The High Speed Downlink Packet Access (HSDPA), analyzed in this paper, can guarantee a higher data rate through the introduction of High Speed Downlink Shared Channel (HS-DSCH), thus improving the performance of MBMS transmissions. The aim {{of this paper is}} to investigate the impact of the User Equipment (UE) speed on the maximum number of users that the HS-DSCH can support for MBMS applications. In particular, two different mobility profiles are taken into account (Pedestrian and Vehicular) and the obtained results are validated by considering different transmission power levels, cell coverage sizes and bit rates. Postprint (published version...|$|E
40|$|Today, {{refactoring}} reconstruction {{techniques are}} snapshot-based: they compare two revisions {{from a source}} code man-agement system and calculate the shortest path of edit op-erations to go from the one to the other. An inherent risk with snapshot-based approaches is that a refactoring may be concealed by later edit operations acting on the same <b>source</b> code <b>entity,</b> a phenomenon we call refactoring mask-ing. In this paper, we performed an experiment to find out at which point refactoring masking occurs and confirmed that a snapshot-based technique misses refactorings when several edit operations are performed on the same <b>source</b> code <b>entity.</b> We present a way of reconstructing refactorings using fine grained changes that are recorded live from an integrated development environment and demonstrate on two cases —PMD and Cruisecontrol — that our approach is more accurate in {{a significant number of}} situations than the state-of-the-art snapshot-based technique RefFinder...|$|R
40|$|Entity-relationship-structured data is {{becoming}} more important on the Web. For example, large knowledge bases have been automatically constructed by information extraction from Wikipedia and other Web <b>sources.</b> <b>Entities</b> and relationships can be represented by subject-property-object triples in the RDF model, and can then be precisely searched by structured query languages like SPARQL. Because of their Boolean-match semantics, such queries often return too few or even no results. To improve recall, it is thus desirable to support users by automatically relaxing or reformulating queries {{in such a way}} that the intention of the original user query is preserved while returning a sufficient number of ranked results. In this paper we describe comprehensive methods to relax SPARQL-like triple-pattern queries, possibly augmented with keywords, in a fully automated manner. Our framework produces a set of relaxations by means of statistical language models for structured RDF data and queries. The quer...|$|R
40|$|The Web is {{typically}} our first {{source of information}} about new software vulnerabilities, exploits and cyber-attacks. Information is found in semi-structured vulnerability databases as well as in text from security bulletins, news reports, cybersecurity blogs and Internet chat rooms. It can be useful to cybersecurity systems if {{there is a way to}} recognize and extract relevant information and represent it as easily shared and integrated semantic data. We describe such an automatic framework that generates and publishes a RDF linked data representation of cybersecurity concepts and vulnerability descriptions extracted from the National Vulnerability Database and other text <b>sources.</b> <b>Entities,</b> relations and concepts are represented using custom ontologies for the cybersecurity domain and also mapped to objects in the DBpedia knowledge base, producing a rich resource of machine-understandable linked data. The resulting cybersecurity linked data collection can be used for many purposes, including automating early vulnerability identification, mitigation and prevention efforts...|$|R
40|$|Abstract—Long Term Evolution (LTE) {{promises}} {{the delivery}} of rich multimedia services in a more power and spectral efficient way than its predecessor Universal Mobile Telecommunication System (UMTS). To this direction, the newly introduced Enhanced- Multimedia Broadcast/Multicast Service (E-MBMS) framework is envisaged to play a fundamental role during the LTE standardization. E-MBMS constitutes the successor of MBMS which {{was introduced in the}} Release 6 of UMTS in order to deliver multimedia data from a single <b>source</b> <b>entity</b> to multiple destinations. This paper proposes a novel mechanism for efficient radio bearer selection during E-MBMS transmissions in LTE networks. The proposed mechanism is based on the concept of transport channels combination (pointto-point and/or point-to-multipoint radio bearers) in any cell/sector of the network in which multicast users are residing. The mechanism is evaluated through several realistic scenarios and is compared with several radio bearer selection mechanisms in order to highlight the enhancements that it provides...|$|E
40|$|International audienceThere are {{a number}} of models for access control in {{pervasive}} environments that are based on trust propagation. Iterative multiplication of the trust values on a path from a <b>source</b> <b>entity</b> to a target entity is one of the common strategies for trust propagation. In this paper, we evaluate the effectiveness of iterative multiplication for trust propagation. The data set used for this evaluation is the real web of trust of Advogato. org that comprises of over $ 11, 000 $ vertices (users) and over 50, 000 directed weighted edges (trust relationships between users). We find that a significantly strong positive linear correlation exists between trust values based on direct experience and the corresponding propagated trust values derived through the iterative multiplication approach. This finding provides empirical support for the access control models for pervasive environments that employ the iterative multiplication strategy for trust propagation...|$|E
40|$|International audienceSecurity {{assessment}} {{tasks and}} intrusion detection systems do rely on automated fingerprinting of devices and services. Most current fingerprinting approaches use a signature matching scheme, where {{a set of}} signatures are compared with traffic issued by an unknown entity. The entity is identified by finding the closest match with the stored signatures. These fingerprinting signatures are found mostly manually, requiring a laborious activity and needing advanced domain specific expertise. In this paper we describe a novel approach to automate this process and build flexible and efficient fingerprinting systems {{able to identify the}} <b>source</b> <b>entity</b> of messages in the network. We follow a passive approach without need to interact with the tested device. Application level traffic is captured passively and inherent structural features are used for the classification process. We describe and assess a new technique for the automated extraction of protocol fingerprints based on arborescent features extracted from the underlying grammar. We have successfully applied our technique to the Session Initiation Protocol (SIP) used in Voice over IP signalling...|$|E
30|$|Even though SVC and the OPTIMIX {{architecture}} support {{application layer}} adaptation of SVC bitstreams in different locations (i.e., the server or a media aware network element), in this article, {{we assume that}} the application layer adaptation takes place in the server (i.e., the <b>source).</b> The <b>entity</b> controlling the adaptation is the application controller, presented later in this section with more details.|$|R
40|$|Abstract. The current Squeak Smalltalk IDE {{provides}} a structural perspective on a software system {{in terms of}} packages, classes and methods. However, from this perspective {{it is difficult to}} gain an understanding of how <b>source</b> <b>entities</b> participate at system’s run-time. Hermion enriches the traditional IDE with a view on the dynamics of the system, (i) by offering a complementary feature-centric perspective of a software system to allow developers to reason about how specific run-time features of their software are implemented, (ii) by integrating dynamic information into the static perspective on a system, i. e., source code, and (iii) by providing mechanisms to query run-time information. 1 Enhancing the IDE with Dynamic Information The problems of understanding object-oriented software are poorly addressed by current Squeak development tools, as these tools purely focus on a structural perspective of a software system by displaying static source artifacts such as class categories, classes or methods. The running of the system, however, i. e., th...|$|R
40|$|Abstract: Information {{integration}} {{is the focus}} of different research domains for several years. With the emergence of the Internet as a very large database, this topic became of extreme relevance for a larger audience, seeking for mechanisms to align information from different data sources according to semantic needs and constraints. The classical integration approaches do not satisfy new operational requirements, thus new strategies should be proposed and developed. We suggest the adoption of a light alignment mechanism without merging data source. The alignment process occurs at ontological level, setting the components and transformation functions necessary to translate data from source to the target. Establishing the alignment process at ontological level allows the system to reason about the semantic embodied in the information and thus enhance the alignment results, according to both the source and target specifications. The resulting specifications are then applied in the transformation procedure, executed independently of data <b>source</b> <b>entities...</b>|$|R
