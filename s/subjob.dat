9|8|Public
40|$|Accurate, {{continuous}} resource {{monitoring and}} profiling {{are critical for}} enabling performance tuning and scheduling optimization. In desktop grid systems that employ sandboxing, these issues are challenging because (1) subjobs inside sandboxes are executed in a virtual computing environment and (2) {{the state of the}} virtual computing environment within the sandboxes is reset to empty after each <b>subjob</b> completes...|$|E
40|$|In {{parallel}} computation we often need an algorithm for dividing one computationally expensive job into a fixed number, say N, of subjobs, {{which can be}} processed in parallel (with reasonable overhead due to additional communication). In practice it is often easier to repeatedly bisect jobs, i. e., split one job into exactly two subjobs, than to generate N subjobs at once. In order to balance the load among the N machines, we want to minimize {{the size of the}} largest <b>subjob</b> (according to some measure, like cpu-time or memory usage) ...|$|E
40|$|Summary form only given. Scheduling {{policies}} are proposed for parallelizing data intensive particle physics analysis applications on computer clusters. Particle physics analysis jobs require {{the analysis of}} tens of thousands of particle collision events, each event requiring typically 200 ms processing time and 600 KB of data. Many jobs are launched concurrently by a large number of physicists. At a first view, particle physics jobs seem to be easy to parallelize, since particle collision events can be processed independently one from another. However, since large amounts of data need to be accessed, the real challenge resides in making an efficient use of the underlying computing resources. We propose several job parallelization and scheduling policies aiming at reducing job processing times and at increasing the sustainable load of a cluster server. Since particle collision events are usually reused by several jobs, cache based job splitting strategies considerably increase cluster utilization and reduce job processing times. Compared with straightforward job scheduling on a processing form, cache based first in first out job splitting speeds up average response times by an order of magnitude and reduces job waiting times in the system's queues from hours to minutes. By scheduling the jobs out of order, according to the availability of their collision events in the node disk caches, response times are further reduced, especially at high loads. In the delayed scheduling policy, job requests are accumulated during a time period, divided into <b>subjob</b> requests according to a parameterizable <b>subjob</b> size, and scheduled at the beginning of the next time period according to the availability of their data segments within the disk node caches. Delayed scheduling sustains a load close to the maximal theoretically sustainable load of a cluster, but at the cost of longer average response times. Finally we propose an adaptive delay scheduling approach, where the scheduling delay is adapted to the current load. This last scheduling approach sustains very high loads and offers low response times at normal loads...|$|E
40|$|The {{purpose of}} this study was to {{investigate}} the control implications of simultaneously processing several different programs on an electronic computer. This paper reports on the advantages of prescribing computer programs such that they have no arbitrary sequence constraints and can be segmented into independent <b>subjobs</b> which can be processed by one component of a computer, for example, an input device. Combining several programs writtern in this fashion would create a macro-program composed of a hierarchy of <b>subjobs.</b> Such a prescription method would allow a properly constituted automatic dispatching component to recursively subdivide this macro-program into <b>subjobs</b> that are performable by an independent component of the computer. The purpose of creating programs so they can automatically be combined and segmented is to provide a queue of work for each available operating component of a computer. This study attempts to demonstrate the advantage of this approach as a method of obtaining more capacity with developed equipments. The results give some evidence for a change in programming methods and a need to devote more resources to improving control procedures to allow simultaneous processing of many jobs. ...|$|R
40|$|Under current analysis, soft {{real-time}} tardiness bounds {{applicable to}} global earliest-deadline-first scheduling and related policies depend on per-task worst-case execution times. By splitting job budgets to create <b>subjobs</b> with shorter periods and worst-case execution times, such bounds {{can be reduced}} to near zero for implicit-deadline sporadic task systems. However, doing so could potentially cause more preemptions and create problems for synchronization protocols. This paper analyzes this tradeoff between theory and practice by presenting an overhead-aware schedulability study pertaining to job splitting. In this study, real overhead data from a scheduler implementation in LITMUSRT was factored into schedulability analysis. This study shows that despite practical issues affecting job splitting, it can still yield substantial reductions in tardiness bounds for soft real-time systems. ...|$|R
40|$|Abstract iii The {{purpose of}} this thesis is to present a {{specification}} language and its execution mechanism used to define tasks in a networked, distributed computing environment. Because of {{technological advancements in both}} computer hardware and software, computers are increasingly being connected into networks and systems. Machines in a computer network could be used more efficiently if a complex activity comprised of many sub-jobs could be divided and executed on multiple machines. A new type of command file is needed where the underlying interpreting mechanism takes advantage of resources network-wide, detects possibilities for parallel executions of <b>subjobs,</b> and executes them on separate machines. A specification language is presented here that allows a user to create such command files. This file is to be called a &quot;task file, &quot; and it contains information about which sub-jobs should be executed. It is similar to a batch command file; the difference lies i...|$|R
40|$|Mobile agent based {{distributed}} job workflow execution is {{a promising}} paradigm for data intensive collaborative scientific computations over the grid. In this paper, a mobile code collaboration framework (MCCF) for distributed job workflow execution over the grid is described and an algorithm to identify partners for agent communication in MCCF is presented. Previous work of mobile agent communication focuses mainly on agent location and communication. Little {{work has been}} done on communication partner identification. In MCCF, a novel <b>subjob</b> grouping algorithm for preprocessing the job workflow's static specification is developed. The obtained information is then used during runtime to identify partners for agent communication. The mobile agent dynamic location and communication based on this approach limits the number of agents required for communication during the dynamic job workflow execution. The algorithm is evaluated through a comparison study using simulated job workflows executed on a prototype implementation of the MCCF. The results show that the algorithm is scalable and efficient. Department of ComputingRefereed conference pape...|$|E
40|$|This {{paper is}} {{concerned}} with the design, implementation, and evaluation of algorithms for communication partner identification in mobile agent-based distributed job workflow execution. We first describe a framework for distributed job workflow execution over the Grid: the Mobile Code Collaboration Framework (MCCF). Based on the study of agent communications during a job workflow execution on MCCF, we identify the unnecessary agent communications that degrade the system performance. Then, we design a novel <b>subjob</b> grouping algorithm for preprocessing the job workflow's static specification in MCCF. The obtained information is used in both static and dynamic algorithms to identify partners for agent communication. The mobile agent dynamic location and communication based on this approach is expected to reduce the agent communication overhead by removing unnecessary communication partners during the dynamic job workflow execution. The proof of the dynamic algorithm's correctness and effectiveness are elaborated. Finally, the algorithms are evaluated through a comparison study using simulated job workflows executed on a prototype implementation of the MCCF on a LAN environment and an emulated WAN setup. The results show the scalability and efficiency of the algorithms as well as the advantages of the dynamic algorithm over the static one. Department of Computin...|$|E
40|$|Service Level Agreements (SLAs) are {{currently}} {{one of the}} major research topics in Grid Computing, as they serve as a foundation for reliable and predictable Grids. SLAs define an explicit statement of expectations and obligations in a business relationship between provider and customer. Thus, SLAs should guarantee the desired and a-priori negotiated Quality of Service (QoS), which is a mandatory prerequisite for the Next Generation Grids. This development is proved by a manifold research work about SLAs and architectures for implementing SLAs in Grid environments. However, this work is mostly related to SLAs for standard, monolithic Grid jobs and neglects the dependencies between different steps of operation. The complexity of an SLA-specification for workflows grows significantly, as characteristics of correlated sub-jobs, the data transfer phases, the deadline constraints and possible failures have to be considered. Thus, an architecture for an SLA-aware workflow implementation needs sophisticated mechanisms for specification and management, <b>subjob</b> mapping, data transfer optimization and fault reaction. Therefore, this paper presents an architecture for SLA-aware Grid workflows. The main contributions are an improved specification language for SLA-aware workflows, a mapping and optimization algorithm for sub-job assignment to Grid resources and a prototype implementation using standard middleware. Experimental measurements prove the quality of the development...|$|E
40|$|As a programmer, one is {{aspired to}} solve ever larger, more memory {{intensive}} problems, or simply solve problems with greater speed than possible on a sequential computer. A programmer {{can turn to}} parallel programming and parallel computers to satisfy these needs. Parallel programming methods on parallel computers gives access to greater memory and Central Processing Unit (CPU) resources which is not available on sequential computers. This paper discusses the benefits of developing 2 D and 3 D convex hull on mixed mode MPI, OpenMP applications on both single and clustered SMPs. In this experimentation for purpose of optimization of 3 D convex hull we merged both MPI and OpenMP library which gives another mixed mode programming method to get optimized results. The job is divided into sub-jobs and are submitted to cluster of SMP nodes using MPI and these <b>subjobs</b> are computed in parallel using OpenMP threads in SMP nodes. Experiments on sequential, MPI, OpenMP and Hybrid programming models show that Hybrid programming model substantially outperforms others...|$|R
40|$|Accurate, {{continuous}} resource {{monitoring and}} profiling {{are critical for}} enabling performance tuning and scheduling op-timization. In desktop grid systems that employ sandbox-ing, these issues are challenging because (1) <b>subjobs</b> in-side sandboxes are executed in a virtual computing envi-ronment and (2) {{the state of the}} virtual computing environ-ment within the sandboxes is reset to empty after each sub-job completes. DGMonitor is a monitoring tool which builds a global, ac-curate, and continuous view of real resource utilization for desktop grids with sandboxing. Our monitoring tool mea-sures performance unobtrusively and reliably, uses a simple performance data model, and is easy to use. Our measure-ments demonstrate that DGMonitor can scale to large desk-top grids (up to 12000 workers) with low monitoring over-head in terms of resource consumption (less than 0. 1 %) on desktop PCs. Though we developed DGMonitor with the Entropia DC-Grid platform, our tool is easily integrated into other desk-top grid systems. In all of these systems, DGMonitor data can support existing and novel information services, partic-ularly for performance tuning and scheduling...|$|R
40|$|Ganga {{has been}} widely used for several years in ATLAS, LHCb {{and a handful of}} other communities. Ganga {{provides}} a simple yet powerful interface for submitting and managing jobs to a variety of computing backends. The tool helps users configuring applications and keeping track of their work. With the major release of version 5 in summer 2008, Ganga's main user-friendly features have been strengthened. Examples include a new configuration interface, enhanced support for job collections, bulk operations and easier access to <b>subjobs.</b> In addition to the traditional batch and Grid backends such as Condor, LSF, PBS, gLite/EDG a point-to-point job execution via ssh on remote machines is now supported. Ganga is used as an interactive job submission interface for end-users, and also as a job submission component for higher-level tools. For example GangaRobot is used to perform automated, end-to-end testing of distributed data analysis. Ganga comes with an extensive test suite covering more than 350 test cases. The development model involves all active developers in the release management shifts which is an important and novel approach for the distributed software collaborations. Ganga 5 is a mature, stable and widely-used tool with long-term support from the HEP community...|$|R
40|$|The Map-Reduce {{computing}} framework rose {{to prominence}} with datasets of such size that dozens of machines {{on a single}} cluster were needed for individual jobs. As datasets approach the exabyte scale, a single job may need distributed processing not only on multiple machines, but on multiple clusters. We consider a scheduling problem to minimize weighted average completion time of N jobs on M distributed clusters of parallel machines. In keeping with {{the scale of the}} problems motivating this work, we assume that (1) each job is divided into M "subjobs" and (2) distinct subjobs of a given job may be processed concurrently. When each cluster is a single machine, this is the NP-Hard concurrent open shop problem. A clear limitation of such a model is that a serial processing assumption sidesteps the issue of how different tasks of a given <b>subjob</b> might be processed in parallel. Our algorithms explicitly model clusters as pools of resources and effectively overcome this issue. Under a variety of parameter settings, we develop two constant factor approximation algorithms for this problem. The first algorithm uses an LP relaxation tailored to this problem from prior work. This LP-based algorithm provides strong performance guarantees. Our second algorithm exploits a surprisingly simple mapping to the special case of one machine per cluster. This mapping-based algorithm is combinatorial and extremely fast. These are the first constant factor approximations for this problem. Comment: A shorter version of this paper (one that omitted several proofs) appeared in the proceedings of the 2016 European Symposium on Algorithm...|$|E
40|$|The {{prevalence}} of chip multiprocessor opens opportunities of running data-parallel applications originally in clusters {{on a single}} machine with many cores. MapReduce, a simple and elegant programming model to program large scale clusters, has recently {{been shown to be}} a promising alternative to harness the multicore platform. The differences such as memory hierarchy and communication patterns between clusters and multicore platforms raise new challenges to design and implement an efficient MapReduce system on multicore. This paper argues that it is more efficient for Map-Reduce to iteratively process small chunks of data in turn than processing a large chunk of data at one time on shared memory multicore platforms. Based on the argument, we extend the general MapReduce programming model with “tiling strategy”, called Tiled-MapReduce (TMR). TMR partitions a large MapReduce job into a number of small sub-jobs and iteratively processes one <b>subjob</b> at a time with efficient use of resources; TMR finally merges the results of all sub-jobs for output. Based on Tiled-MapReduce, we design and implement several optimizing techniques targeting multicore, including the reuse of input and intermediate data structure among sub-jobs, a NUCA/NUMA-aware scheduler, and pipelining a sub-job’s reduce phase with the successive sub-job’s map phase, to optimize the memory, cache and CPU resources accordingly. We have implemented a prototype of Tiled-MapReduce based on Phoenix, an already highly optimized MapReduce runtime for shared memory multiprocessors. The prototype, namely Ostrich, runs on an Intel machine with 16 cores. Experiments on four different types of benchmarks show that Ostrich saves up to 85 % memory, causes less cache misses and makes more efficient uses of CPU cores, resulting in a speedup ranging from 1. 2 X to 3. 3 X...|$|E
40|$|The {{emergence}} of Grid computing infrastructures enables researchers to share resources and collaborate in more efficient ways than before, despite belonging to different organizations and being distanced geographically. While the Grid computing paradigm offers new opportunities, {{it also gives}} rise to new difficulties. One such problem is the selection of resources for user applications. Given the large and disparate set of Grid resources, manual resource selection becomes impractical, even for experienced users. This thesis investigates methods, algorithms and software for a Grid resource broker, i. e., a scheduling agent that automates the resource selection process for the user. The development of such a component is a non-trivial task as Grid resources are heterogeneous in hardware, software, availability, ownership and usage policies. A wide range of algorithmically difficult issues must also be solved, including characterization of jobs, prediction of resource performance, data placement considerations, and, how to provide Quality of Service guarantees. One contribution of this thesis {{is the development of}} resource brokering algorithms that enable resource selection based on Grid job performance predictions and use advance reservations to provide Quality of Service guarantees. The thesis also includes an algorithm for coallocation of sets of jobs. This algorithm guarantees a simultaneous start of each <b>subjob,</b> as required e. g., when running larger-than-supercomputer simulations that involve multiple resources. We today have the somewhat paradoxal situation where Grids, originally aimed to overcome interoperability problems between different computing platforms, themselves struggle with interoperability problems caused by the wide range of interfaces, protocols and data formats that are used in different environments. The reasons for this situation are obvious, expected and almost impossible to avoid, as the task of defining appropriate standards, models and best-practices must be preceded by basic research, proof-of-concept implementations and real-world testing. We address the interoperability problem with a generic Grid resource brokering architecture and job submission service. By using (proposed) standard formats and protocols, the service acts as an interoperability-bridge that translates job requests between clients and resources running different Grid middlewares. This concept is demonstrated by the integration of the service with three different Grid middlewares. The service also enables users to both fine-tune the existing resource selection algorithms and plug in custom brokering algorithms tailored to their requirements...|$|E
40|$|We {{developed}} mixed {{integer programming}} (MIP) models and hybrid genetic-local search algorithms for the scheduling problem of unrelated parallel machines with job sequence and machine-dependent setup times and with job splitting property. The first contribution {{of this paper}} is to introduce novel algorithms which make splitting and scheduling simultaneously with variable number of <b>subjobs.</b> We proposed simple chromosome structure which is constituted by random key numbers in hybrid genetic-local search algorithm (GAspLA). Random key numbers are used frequently in genetic algorithms, but it creates additional difficulty when hybrid factors in local search are implemented. We developed algorithms that satisfy the adaptation of results of local search into the genetic algorithms with minimum relocation operation of genes’ random key numbers. This is the second contribution of the paper. The third contribution {{of this paper is}} three developed new MIP models which are making splitting and scheduling simultaneously. The fourth contribution of this paper is implementation of the GAspLAMIP. This implementation let us verify the optimality of GAspLA for the studied combinations. The proposed methods are tested on a set of problems taken from the literature and the results validate the effectiveness of the proposed algorithms...|$|R
40|$|Fixed Priority Scheduling (FPS) is the {{de facto}} {{standard}} in industry {{and it is the}} scheduling algorithm used in OSEK/AUTOSAR. Applications in such systems are compositions of so-called runnables, the functional entities of the system. Runnables are mapped to operating system tasks during system synthesis. In order to improve system performance it is proposed to execute runnables non-preemptively while varying the tasks threshold between runnables. This allows simpler resource access, which can reduce the stack usage of the system and improve the schedulability of the task sets. FPDS*, as a special case of fixed-priority scheduling with deferred preemptions, executes <b>subjobs</b> non-preemptively and preemption points have preemption thresholds, providing exactly the proposed behavior. However OSEK/AUTOSAR-conform systems cannot execute such schedules. In this paper we present an approach allowing the execution of FPDS* schedules. In our approach we exploit pseudo resources in order to implement FPDS*. It is further shown that our optimal algorithm produces a minimum number of resource accesses. In addition, a simulation-based evaluation is presented in which the number of resource accesses as well as the number of required pseudo-resources by the proposed algorithms are investigated. Finally, we report the overhead of resource access primitives using our measurements performed on an AUTOSAR-compliant operating system...|$|R

