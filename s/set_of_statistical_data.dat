24|10000|Public
5000|$|In statistics, the {{mid-range}} or mid-extreme of a <b>set</b> <b>of</b> <b>statistical</b> <b>data</b> {{values is}} the arithmetic {{mean of the}} maximum and minimum values in a data set, defined as: ...|$|E
40|$|Colloque avec actes et comité de lecture. internationale. International audienceMedical studies often {{generate}} a large <b>set</b> <b>of</b> <b>statistical</b> <b>data.</b> Finding relationships and patterns in those data is a painful and error-prone task. Graphical representations {{has been recognized}} as {{a powerful tool for}} data analysis. In this paper, we will introduce two visualization techniques, TreeMaps and Hyperbolic Trees, {{that could be used for}} statistical medical data analysis...|$|E
40|$|We {{develop a}} {{practical}} quantum tomography protocol and implement measurements of pure states of ququarts realized with polarization states of photon pairs (biphotons). The method {{is based on}} an optimal choice of the measuring scheme's parameters that provides better quality of reconstruction for the fixed <b>set</b> <b>of</b> <b>statistical</b> <b>data.</b> A high accuracy of the state reconstruction (above 0. 99) indicates that developed methodology is adequate. Comment: 8 pages, 4 figure...|$|E
40|$|We have {{witnessed}} {{a significant increase}} of the user requirements {{in the field of}} multimedia services. Therefore, service providers are forced to assure specific level of quality to earn the trust of theirs customers. QoE together with video quality have become essential terms in regard of designing any video-providing services. However, it was proved that current methods in the user-centered service design are inaccurate in considering resulting user experience and perceived quality of video streams. In this paper we are outlining actual state of art and future challenges in this particular problematic. All of which results in introducing of our new designed concept of QoE prediction model based on deep analysis and understanding of network behaving represented by <b>sets</b> <b>of</b> <b>statistical</b> <b>data...</b>|$|R
40|$|The Voronoi diagram {{approach}} {{was applied to}} quantify the level of microstructural homogeneity of ceramic ZTA samples. From SEM pictures of polished cross-sections of ZTA samples a point pattern representing {{the distribution of the}} zirconia phase in the composite was generated. This point pattern was converted into a Voronoi diagram. The level of microstructural homogeneity was quantified by <b>statistical</b> analysis <b>of</b> the relevant properties (area, perimeter and number of faces) of the Voronoi polygons. A dimensionless parameter defining the level of microstructural homogeneity was calculated from the different <b>sets</b> <b>of</b> <b>statistical</b> <b>data.</b> The calculated parameters indicated significant differences in homogeneity between the ZTA samples. These differences were in qualitative agreement with previously published wear rates of the same ZTA composites. This illustrates the relevance of microstructural homogeneity for wear performance...|$|R
40|$|Dataverse is a {{real-time}} 3 D visualisation framework. The framework {{was developed}} to generate real‐time 3 D audio visual environments from arbitrary <b>sets</b> <b>of</b> <b>statistical</b> <b>data</b> {{for the purpose of}} data visualisation, composition and live audio visual performance. This particular version of Dataverse is optimised for stock market visualisation in real-time. In this project, we investigated the stock performance of the 30 largest and most widely held public companies, which make up the Dow Jones Average, relative to the performance of the U. S stock market. The data contains the largest stocks that cover major sectors of the U. S economy: Consumer Discretionary, Consumer Staples, Energy, Financials, Industrials, Information Technology, Health Care, Materials, Telecommunications and Utilities. The data was collected daily spanning the period from 28 th September 2008 to 24 th September 2009 (approximately 8000 observations) ...|$|R
40|$|We {{introduce}} {{a set of}} statistical geometric tools designed to identify the objects being manipulated through speech and gesture in a multimodal augmented reality system. SenseShapes are volumetric regions of interest that can be attached to parts of the user’s body to provide valuable information about the user’s interaction with objects. To assist in object selection, we generate a rich <b>set</b> <b>of</b> <b>statistical</b> <b>data</b> and dynamically choose which data to consider based on the current situation. 1...|$|E
40|$|The current article {{presents}} {{the current situation}} of ERP(Enterprise Resource Planning) systems and the way these systems have been changing the global economic environment. The analysis {{is based on a}} <b>set</b> <b>of</b> <b>statistical</b> <b>data</b> originating from the corporate reports and specialized literature. A special attention is paid the competitive advantages induced by using ERP at a corporate level. As a result the article identifies the main benefits of ERP systems and indicates the advantages of using such systems at a corporate leve...|$|E
40|$|TPM Vol. 21, No. 4, December 2014, 435 - 447 – Special Issue © 2014 Cises. Cluster {{analysis}} or classification usually {{concerns a}} set of exploratory multivariate data analysis methods and techniques for grouping either a <b>set</b> <b>of</b> <b>statistical</b> <b>data</b> units or the associated set of descriptive variables, into clusters of similar and, hopefully, well separated elements. In this work we refer to an extension of this paradigm to generalized three-way data representations and particularly to classification of interval variables. Such approach appears to be especially useful in large data bases, mostly in a data mining context. A health sciences case study is partially discussed...|$|E
40|$|This paper investigates {{whether the}} ‘Kuznets hypothesis’, that {{economic}} growth from {{low levels of}} GDP per capita is initially associated {{with an increase in}} income inequality and later followed by a decline in inequality, is supported by evidence for a less-developed country, Indonesia. The paper outlines the relevant features of the process of rapid growth and structural change, in particular industrialisation since the 1960 s. It notes the possible consequences of this process for changes in income distribution, and draws on disparate <b>sets</b> <b>of</b> <b>statistical</b> <b>data</b> to trace trends in income inequality in Indonesia. The paper concludes that the evidence for Indonesia suggests an increase in inequality during the 1970 s and a subsequent decrease of inequality until 1997. A comparison of the evidence with historical data for the UK and Japan suggests that income inequality in Indonesia was relatively low. ...|$|R
40|$|In this research, {{credit risks}} are {{analyzed}} for financial organizations using data mining techniques applied to actual data. The two <b>sets</b> <b>of</b> actual <b>statistical</b> <b>data</b> characterizing the borrowers are employed for constructing mathematical {{models in the}} form of the nonlinear logit regression, decision trees, and Bayesian networks. The constructed models are analyzed with a <b>set</b> <b>of</b> appropriate <b>statistical</b> criteria, providing a basis for selecting the best alternative model. A series of computational experiments have been carried out using the two <b>sets</b> <b>of</b> actual <b>statistical</b> <b>data</b> from a Ukrainian bank. As a result of the performed computations, it was established that the best models in this application turned out to be nonlinear logit equations and Bayesian networks. In the future studies, we suppose to expand the number of model constructing techniques and to apply the idea of combining the estimates generated by the alternative models. Also, a specialized decision support system is to be constructed for the purpose of carrying research in the area of financial risks estimation and prediction. ???????????????? ????????? ????? ?????????? ??????????? ? ??????? ??????? ????????????????? ??????? ??????. ??????????? ?????????????? ??????, ??????? ????????????? ????????? ????????, ???????????? ??? ?????????? ?????????????? ??????? ? ????? ????????? ???? ?????, ???????? ??????? ? ??????????? ?????. ???????? ??????????? ??????? ???????????????? ? ??????? ????????? ??????????????? ?????????????? ?????????, ??????? ???? ????????? ??? ?????? ?????? ?????????????? ??????. ? ?????????????? ???? ??????? ?????????? ?????? ???????? ??? ?????????????? ????????????? ? ???????????, ??? ??????? ????????? ?????? ???? ????? ? ??????????? ????. ????????????????? ?????????? ????????? ??????? ?????????? ?????????????? ??????? ? ?????????? ???? ?????????????? ??????, ??????????????? ??????????????? ????????. ?????????? ???????????????? ?????????? ? ?????????? ?????????????????? ??????? ????????? ???????? ??????? ??? ?????????? ???????????? ? ????? ?????????? ? ??????????????? ?????????? ??????...|$|R
40|$|Transnational {{movements}} of academics shape {{the production and}} dissemination of knowledge and thus the geographies of contemporary knowledge economies. In this paper, I investigate the complex relationship between knowledge production and spatial movement by examining three key aspects of academic mobility to Germany in the period 1981 to 2000 : first, global patterns of interaction, second, motivations to work in Germany for a limited period of time and, third, resulting publications and collaborations. The study is based on two <b>sets</b> <b>of</b> <b>statistical</b> <b>data</b> and a postal survey involving about 1200 respondents from 90 countries. I argue that the motivations for and outcomes of transnational academic mobility are not only shaped by {{a great variety of}} influences that constitute society, academia and the individual but also by varying spatial relations of different research practices, which help to explain typical cultures of academic mobility and collaboration. Drawing upon an actor-network based understanding of both the natural and technical sciences and the arts and humanities, a three-dimensional matrix is developed that conceptualises varying spatial relations of scientific practice and interaction in different fields and at different stages of knowledge production...|$|R
40|$|We {{evaluate}} {{the profitability of}} investments in residential property in Germany after unification {{with a focus on}} the comparison of East and West Germany. Calculations are carried out for (1) the after-tax return an investor might have expected {{at the beginning of the}} 1990 s, and (2) the after-tax return that has been realized ten years after. We compare a <b>set</b> <b>of</b> <b>statistical</b> <b>data</b> for investments in fifty major cities by using complete financial budgeting. The results show that tax subsidies could not always protect investors from losing money, but they have boosted realized returns after tax considerably. Therefore, it was indeed the taxpayers, not the investors, who have borne the cost of reconstructing East Germany...|$|E
40|$|The {{relationship}} between psychological maltreatment in childhood and adult well-being {{has previously been}} established via the statistical modelling of psychometric data. This study examines a <b>set</b> <b>of</b> <b>statistical</b> <b>data</b> regarding abuse alongside personal accounts. One hundred and sixty five participants completed several psychometric scales, including a measure of childhood emotional abuse. Several participants were then invited to interviews exploring issues in more depth. Statistical analysis supported previous findings, but the interviews exposed themes pertinent to the examination of long-term effects of emotional abuse, such as acknowledgement of abuse. The findings {{support the use of}} a mixed methodology, as statistical measurements alone did not reveal some detail. The implications of this finding for psychological research and practice are discussed...|$|E
40|$|In {{this work}} we {{presented}} {{a model for}} parallel processing of statistical data. The model presented implement and comparatively test four versions obtained from three algorithms, which process a <b>set</b> <b>of</b> <b>statistical</b> <b>data.</b> The versions differ through communication mode; two are of serial type and two are of parallel type. At last algorithm that order data, was implemented also by three versions, which differ by ordering mechanism of data (list and tree). The programs were tested at a large set of data (more than 15. 000 input fields, more than 19. 000 output fields). A version of these programs (those labeled "csf") can run in parallel at the same files (one write, other read) even at non-based parallel systems. A comparative study for execution of these programs was give...|$|E
40|$|This {{article was}} {{published}} in the journal Social Geography and is also available at: www. soc-geogr. net/ 2 / 97 / 2007 /Transnational movements of academics shape the production and dissemination of knowledge and thus the geographies of contemporary knowledge economies. In this paper, I investigate the complex relationship between knowledge production and spatial movement by examining three key aspects of academic mobility to Germany in the period 1981 to 2000 : first, global patterns of interaction, second, motivations to work in Germany for a limited period of time and, third, resulting publications and collaborations. The study is based on two <b>sets</b> <b>of</b> <b>statistical</b> <b>data</b> and a postal survey involving about 1200 respondents from 90 countries. I argue that the motivations for and outcomes of transnational academic mobility are not only shaped by a great variety of influences that constitute society, academia and the individual but also by varying spatial relations of different research practices, which help to explain typical cultures of academic mobility and collaboration. Drawing upon an actor-network based understanding of both the natural and technical sciences and the arts and humanities, a three-dimensional matrix is developed that conceptualises varying spatial relations of scientific practice and interaction in different fields and at different stages of knowledge production...|$|R
40|$|A {{computer}} program was written {{that would enable}} the user to generate measurement control charts or make statistical decisions based on the means or variations <b>of</b> two <b>sets</b> <b>of</b> <b>data.</b> <b>Statistical</b> t-test and F-test results were obtained using an experimental example, indicate how this program {{could be used to}} aid in engineering decision making. Cleanroom parameters such as temperature, humidity and particle counts were obtained and plotted...|$|R
40|$|Abstract: <b>Statistical</b> {{agencies}} collect <b>data</b> from {{surveys and}} create data warehouses by combining {{data from a}} variety of sources. To be suitable for analytic purposes, the files must be relatively free of error. Record linkage (Fellegi and Sunter, JASA 1969) is used for identifying duplicates within a file or across a <b>set</b> <b>of</b> files. <b>Statistical</b> <b>data</b> editing and imputation (Fellegi and Holt, JASA 1976) are used for locating erroneous values of variables and filling-in for missing data. Although these powerful methods were introduced in the statistical literature, the primary means of implementing the methods have been via computer science and operations research (Winkler, Information Systems 2004 a). This paper provides an overview of the recent developments. ...|$|R
40|$|From the {{experimental}} {{point of view}} probability enters quantum theory just like classical statistical physics, i. e. as an expected relative frequency. However {{it is well known}} that the statistical formalism of quantum theory is quite different from the usual Kolmogorovian one involving, for example, complex numbers, amplitudes, Hilbet spaces [...] . The quantum statistical formalism has been described, developped, applied, generalized with the contributions of many authors; however its theoretical status remained, until recently, quite obscure, as shown by the widely contrasting statements that one can find in the vast literature concerning the following questions. Is it possible to justify the choice of the classical or the quantum statistical formalism, for the description of a given <b>set</b> <b>of</b> <b>statistical</b> <b>data,</b> on rigorous mathematical criteria rather than on empirical ones...|$|E
40|$|The {{amount of}} {{available}} Linked Data on the Web is increasing, and data providers start to publish statistical datasets that comprise numerical data. Such statistical datasets {{differ significantly from}} the currently predominant networkstyle data published on the Web. We explore the possibility of integrating statistical data from multiple Linked Data sources. We provide a mapping from statistical Linked Data into the Multidimensional Model used in data warehouses. We use an extract-transform-load (ETL) pipeline to convert statistical Linked Data into a format suitable for loading into an open-source OLAP system, and thus demonstrate how standard OLAP infrastructure {{can be used for}} elaborate querying and visualisation of integrated statistical Linked Data. We discuss lessons learned from three experiments and identify areas which require future work to ultimately arrive at a well-interlinked <b>set</b> <b>of</b> <b>statistical</b> <b>data</b> from multiple sources which is processable with standard OLAP systems...|$|E
40|$|P. R. E. S. S. is an R package {{developed}} to allow researchers {{to get access}} to and manipulate on a large <b>set</b> <b>of</b> <b>statistical</b> <b>data</b> on protein residue-level structural properties such as residue-level virtual bond lengths, virtual bond angles, and virtual torsion angles. A large set of high-resolution protein structures are downloaded and surveyed. Their residue-level structural properties are calculated and documented. The statistical distributions and correlations of these properties can be queried and displayed. Tools are also provided for modeling and analyzing a given structure in terms of its residue-level structural properties. In particular, new tools for computing residue-level statistical potentials and displaying residue-level Ramachandran-like plots are developed for structural analysis and refinement. P. R. E. S. S. will be released in R as an open source software package, with a user-friendly GUI, accessible and executable by a public user in any R environment...|$|E
30|$|This study {{compiled}} a global database {{that contains a}} large <b>set</b> <b>of</b> observations and <b>statistical</b> <b>data</b> gathered from various sources for assessing potential storm surge impacts on populations and economies. The information diffusion method was used to calculate relative sea-level rise caused by storm surges (hazard intensity). Global expected annual maximum relative water level rise was obtained by interpolating the tidal station estimations through inverse distance weighted interpolation method. GIS tools were used to create the potential affected population and GDP maps.|$|R
40|$|Available {{high energy}} data for both pp and \=pp total cross {{sections}} ($ 5 \ GeV \ < \ \sqrt s \ < \ 1. 8 \ TeV$) are described {{by means of}} two well-known distinct parametrizations, characteristic of theoretical (``Regge-like" expression) and experimental (``Froissart-Martin-like" expression) practices, respectively. Both are compared from the <b>statistical</b> point <b>of</b> view. For the whole <b>set</b> <b>of</b> present <b>data,</b> <b>statistical</b> analysis ($\chi^ 2 /d. o. f. $) seems to favour a ``Froissart-like" ((ln $s$) $^{\gamma \approx 2 }$) rise of the total cross section rather than a ``Regge-like" ($s^{\epsilon}$) one...|$|R
40|$|As {{a measure}} for the {{centrality}} of a point in a <b>set</b> <b>of</b> multivariate <b>data,</b> <b>statistical</b> depth functions play important roles in multivariate analysis, because one may conveniently construct descriptive as well as inferential procedures relying on them. Many depth notions have been proposed in the literature to fit to different applications. However, {{most of them are}} mainly developed for the location setting. In this paper, we discuss the possibility of extending some of them into the regression setting. A general concept of regression depth function is also provided...|$|R
30|$|Maximum {{likelihood}} estimation (MLE) {{is considered}} one of the most robust parameter estimation techniques. It constructs a likelihood function for a <b>set</b> <b>of</b> <b>statistical</b> <b>data,</b> which is optimized to find its extremum with respect to the distribution parameters. The MLE method can handle survival and interval data better than rank regression approaches, particularly when dealing with heavily censored data sets that contain few points of highly accurate observed data. Teimouri et al. (2013) compares the MLE method with other four methods [the Method of Logarithm Moment (MLM), the Percentile Method (PM), the L-Moments Method (LM), and the Method of Moments (MM)] to determine Weibull parameters. One of the main findings of this work is that estimation of parameters is better performed using MLE and LM estimators. However, MLE leads to likelihood equations that need to be solved numerically. Therefore, low convergence rates and efficient iterative methods must be properly addressed, which can be particularly difficult with censored data (Balakrishnan and Kateri 2008).|$|E
40|$|It {{is often}} found even today in Europe that for certain {{statistical}} investigations the conclusion is drawn that {{the extent of}} the avail-able statistical data is not sufficient. Going to the root of this pretention, however, we notice that there is a want of clear con-ception about the extent that is in fact necessary in order that a valid conclusion may with greater probability be arrived at. This, for instance, is the case when obvious tariff reductions are shirked from by erRrenching oneself behind the law of large num-bers, which by its very nature can in actual practice be never accomplished in its inherent sense. Apart from a proper understanding of the limits within which a <b>set</b> <b>of</b> <b>statistical</b> <b>data</b> may subject to certain assumptions be as-cribed full measure of credence, there is further a lack of the neces-sary tools that would permit, on the basis of ascertainable values alone, far-reaching conclusions to be drawn or a maximum of usefu...|$|E
40|$|Numerous {{studies of}} how people reason with {{statistical}} data suggest that human judgment often fails to approximate rational probabilistic (Bayesian) inference. We argue that {{a major source of}} error in these experiments may be misunderstanding causal structure. Most laboratory studies demonstrating probabilistic reasoning deficits fail to explain the causal relationships behind the statistics presented, or they suggest causal mechanisms that are not compatible with people’s prior theories. We propose that human reasoning under uncertainty naturally operates over causal mental models, rather than pure statistical representations, and that statistical data typically support correct Bayesian inference only when they can be incorporated into a causal model consistent with people’s theory of the relevant domain. We show that presenting people with questions that clearly explain an intuitively natural causal structure responsible for a <b>set</b> <b>of</b> <b>statistical</b> <b>data</b> significantly improves their performance. In particular, we describe two modifications to the standard medical diagnosis scenario that each eliminates the phenomenon of base-rate neglect, merely by clarifying the causal structure behind false-positive test results...|$|E
40|$|In {{the frame}} of the {{research}} Project a group of the Latvian scientists work on developing of the engineering economical indicators’ system for multifunctional nanocoatings technology. For the audience of the Conference the researchers propose preliminary results within an analytical evaluation of commercialization aspects from the carried practical surveys. Among the main findings the authors point to the current official data limitations in identification of the homogeneous branch at a consistent approach to generate a <b>set</b> <b>of</b> the <b>statistical</b> <b>data</b> for the nanotechnology industries in Latvia. Therefore, the authors applied holistic analysis method. Currently development of the nanotechnology in Latvia is gradual, and existing scientific fundamentals indicate the investors’ interest. Recently a share of the developments and services produced for the aircraft and spacecraft segment in Latvia had not exceeded 15 per cents of the entire turnover from the targeted business and scientific institutions, however, particular business units had reported very strong capabilities and high ROA ratio in comparison to overall official sectorial dynamics...|$|R
40|$|The paper {{presents}} a <b>set</b> <b>of</b> composite indicators {{of economic activity}} for Poland based on qualitative data from business and consumer surveys. They refer {{to the concept of}} economic sentiment indicator (ESI) used in EU countries, but some alternative concepts proposed by the author are tested as well. Time series of the indicators have been calculated for the period 1994 – 2001, using four alternative formulas and two different <b>sets</b> <b>of</b> survey <b>data.</b> <b>Statistical</b> properties <b>of</b> the indicators are analysed and business tendencies revealed by their evolution are compared with the actual economic developments, as reflected by GDP and industrial production index. The ultimate purpose is to assess the performance of such indicators in business cycle analysis. 1...|$|R
40|$|This corpus-based {{study is}} {{conducted}} {{to gain an}} insight into the lexis of Telecommunication English, {{with the aim of}} characterizing the lexical profile of this specialized language. The applied methodology integrates quantitative techniques and qualitative interpretations to perform an analysis from two different perspectives, and according to two parameters: restriction and keyness. The first approach is focused on the lexical behaviour and the extent that a word is restricted to the constituent areas of a domain, whereas the second approach is directed towards the extent that a word is significant in the domain, regardless of lexical category. The <b>set</b> <b>of</b> empirical and <b>statistical</b> <b>data</b> obtained contribute to map the lexical profile and will serve as a baseline for future studies. ...|$|R
40|$|Purpose – The {{purpose of}} this paper is to present a {{comparative}} analysis and usage overview of the most common business excellence models: the European Foundation for Quality Management Model, the Malcolm Baldrige National Quality Award Model, the Deming Prize Model and the Iberoamerican Model for Excellence in Management. Design/methodology/approach – In order to achieve such goals, the authors have performed a set of statistical analysis over public data sets, related to each one of the analyzed models, as well as making a comparative analysis of the model contents. Findings – The different business excellence models do share a similar set of principles and criteria. However, different adoption patterns have been found across regions of the globe, regarding the use of such business excellence models over the last decades. Originality/value – As far as the authors were able to find out, based on the literature review carried out, this is the first time that a <b>set</b> <b>of</b> <b>statistical</b> <b>data</b> results, related to the worldwide implementation of business excellence models, is being presented for publication...|$|E
40|$|This article {{discusses}} {{the potential of}} ‘historical bibliometric’ methodologies for understanding past cultures and offers a vision for how historical bibliometric research might be conducted on a comparative and global scale. Drawing on conceptual work being undertaken at the Western Sydney University {{in order to further}} develop and extend the widely respected ‘French Book Trade in Enlightenment Europe’ (FBTEE) database project, it explores how historians might proceed to correlate, map, and analyse multiple spatially referenced data sets pertaining to the creation, publication, dissemination, ownership, consumption, reception, policing, and geographic setting of texts. While the authors recognise the many dangers and limitations inherent in reducing the cultural history of text to a <b>set</b> <b>of</b> <b>statistical</b> <b>data,</b> they observe that historians frequently use the production and circulation of texts as a useful proxy for understanding the circulation of ideas. Hence historical bibliometrics can provide measurable indicators of cultural resonance. The challenge, then, is to meaningfully integrate algorithmic abstractions with qualitative-based humanities research. This paper and the suite of projects it discusses seek to provide a way forward...|$|E
40|$|There {{are many}} methodologies for {{assessing}} structural changes in {{domestic and foreign}} practice. But there are no criteria for method selection according to research purposes, which complicates the choice of instruments. The paper presents the original results of calculations of structural shift indices. This indices can explain the differences between regional economic systems of Southern Russia. The author used the indicators of gross value added, employment and investment by sector of economy since 2005 for 2012 years for calculating indices of structural shifts. The research is based on Ryabtsev index estimation. For analysis of the structural differences of regions of Southern Russia the author chose Ryabtsev index for several reasons: {{the possibility of using}} any <b>set</b> <b>of</b> <b>statistical</b> <b>data</b> and the availability of an evaluation scale of structural differences. The results show that the smallest structural shifts we can observe in employment and the largest structural shifts in investment. In accordance with the degree of structural differences in the analyzed indicators, the author allocated groups of regions of Southern Russia, using a comparison of their economic structures. The data obtained in this research can be used to develop a strategy of regional development in view of their structural features...|$|E
40|$|Inductive {{representation}} of conditional knowledge means to complete knowledge appropriately {{and can be}} looked upon as an instance of quite a general representation problem. The crucial problem of discovering relevant conditional relationships in <b>statistical</b> <b>data</b> can also be addressed in this formal framework. The main point in {{this paper is to}} consider knowledge discovery as an operation which is inverse to inductive knowledge representation, giving rise to phrasing the inverse representation problem. This allows us to embed knowledge discovery in a theoretical framework where the vague notion of relevance can be given a precise meaning: relevance here means relevance with respect to an inductive representation method. In order to exemplify our ideas, we present an approach to compute <b>sets</b> <b>of</b> conditionals from <b>statistical</b> <b>data,</b> which are optimal with respect to the information-theoretical principle of maximum entropy...|$|R
40|$|State-controlled {{banks are}} {{currently}} {{at the core}} of financial intermediation in Russia. This paper aims to assess the magnitude of government banking, and to reveal some of its special features and arrangements. We distinguish between directly and indirectly state-controlled banks and construct a <b>set</b> <b>of</b> bank-level <b>statistical</b> <b>data</b> covering the period between 2000 and 2011. By January 2011 the market share of state-controlled banks reached almost 54 percent of all bank assets, putting Russia in the same league with China and India and widening the gap from typical European emerging markets. We show that direct state ownership is gradually substituted by indirect ownership and control. It tends to be organized in corporate pyramids that dilute public property, take control away from government bodies, and underpin managerial opportunism. Statecontrolled banks blur the borderline between commercial banking and development banking. Dominance of public banks has a bearing on empirical studies whose results might suggest state-owned banks’ greater (or lesser) efficiency or competitiveness compared to other forms of ownership. We tend to interpret such results as influenced by the choice of indicator, period of observations, sample selection, etc., {{in the absence of an}} equal playing field for all groups of players. We suggest that the government’s planned retreat from the banking sector will involve non-core assets mainly, whereas control over core institutions will just become more subtle. Russia, banks, government, state-owned banks, public sector...|$|R
40|$|Texture-based {{recognition}} for image segmentation and classification {{is very important}} in many domains and different numerical features coming from a variety of approaches have been proposed. Texture segmentation using six features based on the fractal dimension has been used elsewhere. This paper studies properties of these features {{from the point of view}} of dimensionality reduction, mutual relation, differential relevance, discrete quantization, and classification ability. In an experimental framework, a <b>set</b> <b>of</b> <b>statistical,</b> soft computing, <b>data</b> mining and machine learning methods were used on a <b>set</b> <b>of</b> different textures (Pearson's correlation, rough sets, principal components, and inductive classification). It was found that fractal features effectively have texture recognition ability. Some of these are very relevant (the fractal dimension of smoothed versions of the original image and the multi-fractal dimension). Not so many quantization levels of fractal dimension variables are required in order to achieve high recognition performance. This novel methodology can be used in another type of databases to know its more relevant attributes and be able to simplify models. Postprint (published version...|$|R
