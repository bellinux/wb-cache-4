47|25|Public
5000|$|The SANY <b>syntactic</b> <b>analyzer,</b> which parses {{and checks}} the spec for syntax errors.|$|E
50|$|TLA+ has {{a set of}} {{standard}} modules containing common operators. They are distributed with the <b>syntactic</b> <b>analyzer.</b> The TLC model checker uses Java implementations for improved performance.|$|E
50|$|Haibt was {{in charge}} of section four of the FORTRAN project. She {{analyzed}} the flow of programs produced by other sections of the compiler. Her estimates of flow in high-traffic areas of the computer were obtained by calculating how often basic blocks of the program would execute. Haibt employed Monte Carlo methods (statistical analysis) for these calculations. Through this process, she also created the first <b>syntactic</b> <b>analyzer</b> of arithmetic expressions. Haibt planned and programmed the entire section.|$|E
40|$|The goal of {{this thesis}} is to {{establish}} a system for the automatic syntactic analysis of real-world text. Syntactic analysis in this thesis denotes computation of in-depth syntactic structures that are grounded in syntactic theories like Head-Driven Phrase Structure Grammar (HPSG). Since syntactic structures provide essential components for computing meanings of natural language sentences, the establishment of <b>syntactic</b> <b>analyzers</b> is a starting point for intelligent natural language processing. <b>Syntactic</b> <b>analyzers</b> are strongly demanded in natural language processing applications, including question answering, dialog systems, and text mining. To date, however, few <b>syntactic</b> <b>analyzers</b> can process naturally occurring sentences such as newswire texts. This task involves two significant obstacles. One is the scalability of a grammar to analyze realworld texts. Grammar theories that successfully worked in a toy system could not be applied to the analysis of real-world sentences. Despite intensive research on syntactic analysis, development of wide-coverage grammars is almost impractical. This is due to the inherent difficulty in scaling up a grammar; as a grammar becomes larger, the maintenance of the consistency of the grammar is more difficult. Modern syntactic theories, which are called lexicalized grammars, explain diverse syntactic structures with various combinations of lexical entries to express word-specific constraint...|$|R
50|$|In {{computer}} science, SYNTAX is {{a system}} used to generate lexical and <b>syntactic</b> <b>analyzers</b> (parsers) (both deterministic and non-deterministic) {{for all kinds of}} context-free grammars (CFGs) as well as some classes of contextual grammars. It has been developed at INRIA (France) for several decades, mostly by Pierre Boullier, but has become free software since 2007 only. SYNTAX is distributed under the CeCILL license.|$|R
40|$|Design and {{development}} of Spell Checker for Tamil language and details of the implementation have been discussed in this paper. Lexicons with morphological and syntactic information are needed {{for the development of}} spell checker that can be integrated in word processors, {{as well as for the}} development of morphological and <b>syntactic</b> <b>analyzers</b> that can be exploited by more complex natural language processing applications. 1...|$|R
40|$|Abstract: This paper {{describes}} one of {{the processing}} systems for the quantum programming language NDQJava. Its main feature lies that the classical parts of NDQJava programs are processed by the Java processing system, so this effort emphasizes on the processing of program’s quantum parts. This processing system follows the compilation-interpretation approach, and it includes lexical analyzer, <b>syntactic</b> <b>analyzer,</b> code transformer, quantum assembler and quantum interpreter. By {{the end of this}} paper, some examples are given. The system was implemented by simulation in June 2006 on the classical computer. Key words: quantum programming language; compilation; interpretation; simulation; lexical analyzer; <b>syntactic</b> <b>analyzer</b> and code transformer; quantum assembler and quantum interprete...|$|E
40|$|In {{this paper}} {{a system which}} understands and conceptualizes scenes {{descriptions}} in natural language is presented. Specifically, the following components of the system are described: the <b>syntactic</b> <b>analyzer,</b> based on a Procedural Systemic Grammar, the semantic analyzer relying on the Conceptual Dependency Theory, and the dictionary...|$|E
40|$|Some basic {{points from}} the {{automated}} {{creation of a}} Bulgarian WordNet – an analogue of the Princeton WordNet, are treated. The used computer tools, the received results and their estimation are discussed. A side effect from the proposed approach is the receiving of patterns for the Bulgarian <b>syntactic</b> <b>analyzer...</b>|$|E
40|$|We {{present an}} {{integrated}} probabilistic model for Japanese syntactic and case structure analysis. Syntactic and case structure are simultaneously analyzed based on wide-coverage case frames that are constructed from a huge raw corpus in an unsupervised manner. This model selects the syntactic and case structure {{that has the}} highest generative probability. We evaluate both syntactic structure and case structure. In particular, the experimental results for syntactic analysis on web sentences show that the proposed model significantly outperforms known <b>syntactic</b> <b>analyzers.</b> ...|$|R
40|$|Ambiguity is a {{major reason}} why {{computers}} do not yet understand natural language. We have made great deal strides towards developing tools for morphological and <b>syntactic</b> <b>analyzers</b> for Arabic in recent years. The absence of diacritics, which represent most vowels, in the written text creates ambiguity which hinders the development of Arabic natural language processing applications. Thus, ambiguity increases the range of possible interpretations of natural language. In this paper, we give a road map of solutions to common ambiguity problems inherent in parsing of Arabic sentence. 1...|$|R
40|$|A {{lot of work}} {{remains to}} be done in the domain of a better {{integration}} of speech recognition and language processing systems. This paper gives an overview of several strategies for integrating linguistic models into speech understanding systems and investigates several ways of producing sets of hypotheses that include more “semantic” variability than usual language models. The main goal is to present and demonstrate by actual experiments that sequential coupling may be efficiently achieved by word-lattice <b>syntactic</b> <b>analyzers,</b> efficiently parsing the huge number of hypothesis (i. e. possible sentences) contained in the lattice produced by the speech recognizer...|$|R
30|$|For the Portuguese language, another {{evaluation}} {{resource is}} Floresta Sintática 23 [43], {{which is a}} publicly available treebank for Portuguese. It can be used within NLP research {{as a tool to}} test/train and evaluate a <b>syntactic</b> <b>analyzer</b> [8], which can benefit systems that need morphosyntactic annotations, such RE systems [11, 38, 42, 96].|$|E
40|$|URL] audienceWe {{present the}} system we used for the TempEval competition. This system relies on a deep <b>syntactic</b> <b>analyzer</b> that has been {{extended}} {{for the treatment of}} temporal ex-pressions. So, together with the temporal treatment needed for TempEval purposes, further syntactico-semantic information is also calculated, making thus temporal processing a complement for a better gen-eral purpose text understanding system...|$|E
30|$|Texts on Floresta Sintática {{come from}} two {{distinct}} corpora: CETEM Público, 24 composed by news content written in European Portuguese and CETENFolha, 25 composed by news content written in Brazilian Portuguese, both automatically annotated by the <b>syntactic</b> <b>analyzer</b> Palavras [6]. The resources {{produced by the}} Floresta Sintática project also comprehend Floresta Virgem, its unrevised syntactic tree set, and Bosque, which corresponds to the revised part of Floresta Sintática, composed by 9.368 sentences and 190.513 lexical items.|$|E
40|$|The present paper {{describes}} Treex (formerly TectoMT), a multi-purpose open-source {{framework for}} developing Natural Language Processing applications. It facilitates the development by exploiting {{a wide range}} of software modules already integrated in Treex, such as tools for sentence segmentation, tokenization, morphological analysis, part-of-speech tagging, shallow and deep syntax parsing, named entity recognition, anaphora resolution, sentence synthesis, word-level alignment of parallel corpora, and other tasks. The most elaborate application of Treex is an English-Czech machine translation system with transfer on deep syntactic (tectogrammatical) layer. Besides research, Treex is used for teaching purposes and helps students to implement morphological and <b>syntactic</b> <b>analyzers</b> of foreign languages in a very short time...|$|R
40|$|Predicting {{possible}} code-switching points {{can help}} develop more accurate methods for automatically processing mixed-language text, such as multilingual language models for speech recognition systems and <b>syntactic</b> <b>analyzers.</b> We present {{in this paper}} exploratory results on learning to predict potential codeswitching points in Spanish-English. We trained different learning algorithms using a transcription of code-switched discourse. To evaluate {{the performance of the}} classifiers, we used two different criteria: 1) measuring precision, recall, and F-measure of the predictions against the reference in the transcription, and 2) rating the naturalness of artificially generated code-switched sentences. Average scores for the code-switched sentences generated by our machine learning approach were close to the scores of those generated by humans. ...|$|R
40|$|A {{lot of work}} {{remains to}} be done in the domain of a better {{integration}} of speech recognition and language processing systems. This paper gives an overview of several strategies for integrating linguistic models into speech understanding systems and investigates several ways of producing sets of hypotheses that include more "semantic" variability than usual language models. The main goal is to present and demonstrate by actual experiments that sequential coupling may be efficiently achieved by word-lattice <b>syntactic</b> <b>analyzers,</b> efficiently parsing the huge number of hypothesis (i. e. possible sentences) contained in the lattice produced by the speech recognizer. 1. Motivations The past decade has seen significant progress in speech recognition technology: word (recognition) error rates continue to drop by a factor of 2 every two years (Rabiner et al., 1996) and high performance systems are now becoming available. Several factors have contributed to this rapid progress: ffl Generalisati [...] ...|$|R
40|$|We propose, in this paper, a constituent-based {{automated}} <b>syntactic</b> <b>analyzer</b> fr om natural language; it has capabilities for grammatical rules modification. The parser uses {{bottom-up method}} for syntactic tree generation and right-left, left-right verifying for level tree building. The parser itself was {{built as a}} part of an application for automatic obtaining of UML primitives from natural language; due to this fact, weneed syntactic rules edition for incorporing new phrases and for easing data processing to other analysis modules...|$|E
40|$|A natural {{language}} understanding system is described which extracts contextual information from Japanese texts. It integrates syntactic, semantic and contextual processing serially. The <b>syntactic</b> <b>analyzer</b> obtains rough syntactic structures from the text. The semantic analyzer treats modifying relations inside noun phrases and case relations among verbs and noun phrases. Then, the contextual analyzer obtains contextual information from the semantic structure extracted by the semantic analyzer. Our system understands the context using precoded contextual knowledge on terrorism and plugs the event information in input sentences into-the contextual structure...|$|E
30|$|Another {{important}} {{resource for}} RE systems is {{the availability of}} syntactic treebanks. A treebank is a corpus containing sentences with annotations regarding their syntactic structure. For that reason, treebanks {{can be used to}} train and evaluate syntactic analyzers. The use of a <b>syntactic</b> <b>analyzer</b> of good quality impacts on the performance of RE task. That {{is due to the fact}} that many RE systems depend on natural language processing tools as a pre-processing step, which are error prone and can hinder the performance of the system [2].|$|E
40|$|This paper {{deals with}} the formal {{derivation}} of an efficient tabulation algorithm for tabledriven bottom-up tree acceptors. Bottom-up tree acceptors {{are based on a}} notion of match sets. First we derive a naive acceptance algorithm using dynamic computation of match sets. Tabulation of match sets leads to an efficient acceptance algorithm, but tables may be so large that they can not be generated due to lack of space. Introduction of a convenient equivalence relation on match sets reduces this effect and improves the tabulation algorithm. 1 Introduction Nowadays, many parts of a compiler can be generated automatically. For instance, the automatic generation of lexical and <b>syntactic</b> <b>analyzers</b> using notations based on regular expressions and context-free grammars is commonly used (see e. g. [1]). However, much research is still going on in the field of universal code-generator generators, which take a description of a machine as input and deliver a (good) code generator for that machine. C [...] ...|$|R
40|$|Syntactic {{complexity}} as {{an indicator}} {{in the study of}} English learners’ language proficiency has been frequently employed in language development assessment. Using the <b>Syntactic</b> Complexity <b>Analyzer,</b> developed by Lu (2010), this article collected data representing the syntactic complexity indexes from the writing of Chinese non-English major students and from the writing of proficient users of English on a similar task. The results indicate that there is {{a significant difference in the}} use of complex nominals, the mean length of sentences, and the mean length of clauses between the writings of EFL Chinese students and more proficient users. This study provides suggestions for EFL writing teaching, particularly writing at the sentence level...|$|R
40|$|Neste trabalho, apresentaremos uma aplicação computacional da teoria X-barra (cf. HAEGEMAN 1994, MIOTO et al. 2004), através do programa Grammar Play, um parser sintático em Prolog. O Grammar Play analisa sentenças declarativas simples do português brasileiro, identificando sua estrutura de constituintes. Sua gramática é implementada em Prolog, com o recurso das DCGs, e é baseada nos moldes propostos pela teoria X-barra. O parser é uma primeira tentativa de expandir a cobertura de analisadores semelhantes, como o esboçado em Pagani (2004) e Othero (2004). Os objetivos que guiam a presente versão do Grammar Play são o de implementar computacionalmente modelos lingüísticos coerentes aplicados à descrição do português e o de criar uma ferramenta computacional que possa ser usada didaticamente em aulas de introdução à sintaxe e lingüística, por exemplo. In this article, {{we present}} an {{application}} of X-bar syntax in a computational enviroment. We present the parser Grammar Play, a syntactic parser in Prolog. The parser analyses simple declarative sentences of Brazilian Portuguese, identifying their constituent structure. The grammar is implemented in Prolog, making use of DCGs, and {{it is based on}} the X-bar theory (HAEGEMAN 1994, MIOTO et al. 2004). The parser is an attempt to broaden the coverage of similar <b>syntactic</b> <b>analyzers,</b> as the ones presented in Pagani (2004) and Othero (2006). The main goals of the present version of the Grammar Play are not related to broad coverage, but to the computational implemenation of coherent linguistic models applied to the description of Portuguese, and to the developement of a computational linguistics tool that can be used didactically in introductory classes of Syntax or Linguistics...|$|R
40|$|This paper {{presents}} {{a system which}} analyzes an in~ lout text syntactically and morphologically and converts the text from the graphcmic to the phonetic :representation (or vice versa). We describe the gram- mar formalism used and report a parsing experiment which compared eight parsing strategies within the :kamework of chart parsing. Although the morpho- logical and <b>syntactic</b> <b>analyzer</b> has been developed for a text-to-speech system for German, it is language independent and genera] enough to bc used for dialog systems, NL-interfaces or speech recognition sys- 'terns...|$|E
40|$|Abstract. In this communication, {{we propose}} {{a method for}} the {{automatic}} extraction of numerical fields in handwritten documents. The approach exploits the known syntactic structure of the numerical field to extract, combined {{with a set of}} contextual morphological features to find the best label of each connected component. Applying an HMM based <b>syntactic</b> <b>analyzer</b> on the overall document allows to localize/extract fields of interest. Reported results on the extraction of zip codes, phone numbers and customer codes from handwritten incoming mail documents demonstrate the interest of the proposed approach. ...|$|E
40|$|In {{this paper}} we present EULIA, a tool {{which has been}} {{designed}} {{for dealing with the}} linguistic annotated corpora generated by a set of different linguistic processing tools. The objective of EULIA is to provide a flexible and extensible environment for creating, consulting, visualizing, and modifying documents generated by existing linguistic tools. The documents used as input and output of the different tools contain TEI-conformant feature structures (FS) coded in XML. The tools integrated until now are a lexical database, a tokenizer, a wide-coverage morphosyntactic analyzer, a general purpose tagger/lemmatizer, and a shallow <b>syntactic</b> <b>analyzer...</b>|$|E
40|$|This thesis {{deals with}} {{automatic}} syntactic analysis of natural languagetext, {{also known as}} parsing. The parsing approach is data-driven, whichmeans that parsers are constructed by means of machine learning, lookingat training data {{in the form of}} annotated natural language sentences. The syntactic framework used in the thesis is dependency-based. Robustness is one of the characteristics of the data-driven approaches investigated here. The overall aim of this thesis is to maintain robustness while increasing accuracy. The content of the thesis falls naturally into two tracks, a transformation track and a combination track. The  rst type of transformation investigatedis called pseudo-projective, because it enables strictly projective dependency parsers to recover non-projective dependency relations. Informally,a non-projective dependency tree contains crossing binary directed relations, when drawn above the sentence. Experimental results show that pseudo-projective transformations can improve accuracy significantly for a range of languages. The second type of transformation aims to facilitate the processing of specific linguistic constructions such as coordination and verb groups. Experimental results again show a positive effect on parsing accuracy for several languages, often greater than for the pseudo-projective transformations. However, the improvement of the transformations dependson the internal structure of the base parser, which is not the case for thepseudo-projective transformations. The combination track compares various approaches for combining data driven dependency parsers, again as a means of improving accuracy. As different parsers have different strengths and weaknesses, making parsers collaborate in order to  nd one single syntactic analysis may result in higher accuracy than any of the <b>syntactic</b> <b>analyzers</b> can produce by itself. The experimental results show that accuracy improves across languages, giventhat appropriate parsers are combined. The thesis ends with an attempt to combine the two tracks, showing that combining parsers with different tree transformations also increases accuracy. Moreover, this experiment indicates that high diversity among a small set of parsers is much more important than a large number of parsers with low diversity...|$|R
40|$|This article {{describes}} {{the construction of a}} morphological, <b>syntactic</b> and semantic <b>analyzer</b> to operate a high-grade search engine for Hebrew texts. A good search engine must be complete and accurate. In Hebrew or Arabic script most of the vowels are not written, many particles are attached to the word without space, a double consonant is written with one letter, and some letters signify both vowels and consonants. Thus, almost every string of characters may designate many words (the average in Hebrew is almost three words). As a consequence, deciphering...|$|R
40|$|Abstract. The present article {{involves}} an empirical psycholinguistic study aimed at examining syntactic complexity in English as a Foreign Language (EFL) by early balanced Bosnian/Swedish bilingual EFL learners. 15 early balanced bilingual Bosnian/Swedish EFL learners were recruited {{for the study}} and matched with their respective control groups of intermediate EFL learners (15 speakers of Bosnian as their first language (L 1) and 15 speakers of Swedish as their L 1). The experimental task involved an unprepared writing assignment in English about the most significant invention of the 20 th century. The corpus of the participants’ written assignments was analysed in L 2 <b>Syntactic</b> Complexity <b>Analyzer</b> and SPSS software programs respectively. Data analysis involved measures of syntactical complexity. It {{has been found that}} the participants’ written assignments are characterised by statistically significant number of T-units scores in comparison with the Swedish L 1 monolingual controls. These findings are further presented and discussed in the article...|$|R
40|$|In this communication, {{we propose}} {{a method for}} the {{automatic}} extraction of numerical fields in handwritten documents. The approach exploits the known syntactic structure of the numerical field to extract, combined {{with a set of}} contextual morphological features to find the best label to each connected component. Applying an HMM based <b>syntactic</b> <b>analyzer</b> on the overall document allows to localize/extract fields of interest. Reported results on the extraction of zip codes, phone numbers and customer codes from handwritten incoming mail documents demonstrate the interest of the proposed approach. 1...|$|E
40|$|PM, Propositional Model, is {{a highly}} {{interactive}} model of language comprehension which can be contrasted with language processing models which assume an autonomous syntactic component. Models assuming an autonomous syntax {{can be divided into}} two basic types: (a) Strong Autonomy Models which assume that only the part of speech of lexical items is available to the <b>syntactic</b> <b>analyzer,</b> and (b) Weak Autonomy Models which assume that information other than the part of speech, but of a purely syntactic nature, is available to <b>syntactic</b> <b>analyzer.</b> This paper presents the results of two experiments using the cumulative self-paced reading task which are intended to distinguish PM from both Strong and Weak Autonomy Models. The first experiment considers the influence of verb argument preferences on the initial determination of structure. The existence of such influences would argue against the Strong Autonomy Model and in favor of Weak Autonomy or Interactive Models. The second experiment considers the influence of object schematicity on the initial determination of structure. An influence of object schematicity argues against the Weak Autonomy Models and in favor of Interactive Models like PM. The results of the two experiments do indeed show effects of verb argument and object schematicity preferences. If the cumulative self-paced reading task is a legitimate measure of immediate, on-line processing, then Interactive Models are supported by these experiments and Autonomous models are not...|$|E
40|$|We {{present the}} system we used for the TempEval competition. This system relies on a deep <b>syntactic</b> <b>analyzer</b> that has been {{extended}} {{for the treatment of}} temporal expressions, thus making temporal processing a complement to a better general purpose text understanding system. 1 General presentation and system overview Although interest in temporal and aspectual phenomena is not new in NLP and AI, temporal processing of real texts is a topic that has been of growing interest in the last years (Mani et al. 2005). The work we have done concerning temporal processing of texts is part of a more general proces...|$|E
40|$|Under the {{direction}} of Dr. Michael A. Covington) This thesis describes a rule-based computer program, the Shallow <b>Syntactic</b> Complexity <b>Analyzer</b> (ShaC), for determining the syntactic complexity of English-language text. Syntactic complexity is determined by comparing strings of text to templates. The templates were constructed following the modified D-Level scale (Covington et al., 2004), which ranks syntactic complexity based on {{the age at which}} young children first acquire various syntactic structures. The later a structure is acquired, the higher ranking it gets. ShaC is unique in that it attempts to give a good estimate of syntactic complexity without doing a deep syntactic analysis. Such a detailed analysis would be time consuming; ShaC uses heuristics and generalizations to greatly simplify the task at hand. The result is a quick and efficient method for estimating syntactic complexity. ShaC score correlates highly with D-Level. Sentences with a verb taking a finite complement, verbs taking an-ing complement, verbs taking a non-finite complement, and comparatives drive the correlation. Index words...|$|R
40|$|Noun phrase (NP) {{centered}} {{structures are}} distinctive syntactic devices in academic discourse. The commonly employed subordination-based complexity measures cannot adequately capture {{the development of}} syntactic complexity of noun phrases expected of advanced student academic writing (Biber, Gray, & Poonpon, 2011). Following the call for more {{research in this area}} (e. g. Lu, 2011, p. 57), the current study compared noun phrase complexity in two corpora: one is a corpus of MA dissertations written by Chinese EFL students and the other comprises published research articles in applied linguistics journals. The study examined overall noun phrase complexity using an automatic <b>syntactic</b> complexity <b>analyzer</b> and specifically identified features of one aspect of NP complexity: NP postmodification. The quantitative results were further contextualized in a textual analysis of excerpts from the two corpora for demonstrating the significance of NP complexity to the establishment of discourse coherence. Results of the analyses showed significant underdevelopment of NP postmodification complexity in student writing relative to published texts, meanwhile explicating the circumstances under which the difference is meaningful. Implications of the findings for the teaching of EFL academic writing were also discussed. Department of Englis...|$|R
40|$|A {{prominent}} {{stumbling block}} in {{the spread of the}} C++ programming language has been a lack of programming and analysis tools to aid development and maintenance of C++ systems. One way to make the job of tool developers easier and to increase the quality of the tools they create is to factor out the common components of tools and provide the components as easily (re) used building blocks. Those building blocks include lexical, <b>syntactic,</b> and semantic <b>analyzers,</b> tailored database derivers, code annotators and instrumentors, and code generators. From these building blocks, tools such as structure browsers, data-flow analyzers, program/specification verifiers, metrics collectors, compilers, interpreters, and the like can be built more easily and cheaply. We believe that for C++ programming and analysis tools the most primitive building blocks are centered around a common representation of semantically analyzed C++ code. In this paper we describe such a representation, called Repri [...] ...|$|R
