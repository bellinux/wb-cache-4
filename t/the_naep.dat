128|93|Public
25|$|The city's {{public school}} system, Charlotte-Mecklenburg Schools, is the 2nd largest in North Carolina and 17th {{largest in the}} nation. In 2009, it won <b>the</b> <b>NAEP</b> Awards, the Nation's Report Card for urban school systems with top honors among 18 city systems for 4th grade math, 2nd place among 8thgraders. An {{estimated}} 144,000 students are taught in 164 separate elementary, middle, and high schools. Elementary-Middle schools also include the independent Charlotte Preparatory School, with learning based on Montessori education principles.|$|E
25|$|Certain {{educational}} organizations {{viewed the}} SAT re-centering initiative {{as an attempt}} to stave off international embarrassment in regards to continuously declining test scores, even among top students. As evidence, it was presented that the number of pupils who scored above 600 on the verbal portion of the test had fallen from a peak of 112,530 in 1972 to 73,080 in 1993, a 36% backslide, {{despite the fact that the}} total number of test-takers had risen over 500,000. Other authors have argued that the evidence for a decline in student quality is mixed, citing that top scorers on the ACT have shown little change in the same period, and that the proportion of 17-year-olds scoring at the highest performance level on <b>the</b> <b>NAEP</b> long-term trend assessment has been roughly stable for decades.|$|E
50|$|Carol is {{a member}} of <b>the</b> <b>NAEP</b> 2009 Reading Framework {{planning}} committee and <b>the</b> <b>NAEP</b> 2011 Writing Framework planning committee.|$|E
50|$|In {{addition}} to <b>the</b> assessments, <b>NAEP</b> coordinates {{a number of}} related special studies that often involve special data collection processes, secondary analyses of NAEP results, and evaluations of technical procedures.|$|R
40|$|AbstractThis data article {{contains}} information {{based on the}} 2011 National Assessment of Educational Progress in Writing Restricted-Use Data, available from the National Center for Education Statistics (NCES Pub. No. 2014476). [URL] The data include the statistical relationships between survey reports of teachers and students regarding prior use of computers and other technology and writing achievement levels on <b>the</b> 2011 computer-based <b>NAEP</b> writing assessment. This data article accompanies “The Effects of Prior Computer Use on Computer-Based Writing: <b>The</b> 2011 <b>NAEP</b> Writing Assessment” [1]...|$|R
40|$|The U. S. Department of Education {{has not yet}} {{published}} an official guidance document for using NAEP achievement level scores to confirm state testing results. A review of the literature, however, identified four principles that inform the valid use of NAEP scores in a confirming analysis. These principles address <b>the</b> appropriate <b>NAEP</b> statistic to use to confirm state testing results, <b>the</b> difference between <b>NAEP</b> and state definitions of - proficiency,- a rationale for avoiding point-by-point comparisons, and a rationale for using trend analysis...|$|R
5000|$|NSCs provide many {{important}} services for <b>the</b> <b>NAEP</b> program and are responsible for: ...|$|E
5000|$|<b>The</b> <b>NAEP</b> {{has since}} {{collected}} and analyzed data through 2008. Overall, the White-Hispanic and the White-Black gap for NAEP scores have significantly decreased since the 1970s.The Black-White Gap demonstrates: ...|$|E
5000|$|Over the years, NCES has {{conducted}} {{a number of}} other studies related to different aspects of <b>the</b> <b>NAEP</b> program. A few studies from the recent past are listed below: ...|$|E
40|$|This data article {{contains}} information {{based on the}} 2011 National Assessment of Educational Progress in Writing Restricted-Use Data, available from the National Center for Education Statistics (NCES Pub. No. 2014476). [URL] The data include the statistical relationships between survey reports of teachers and students regarding prior use of computers and other technology and writing achievement levels on <b>the</b> 2011 computer-based <b>NAEP</b> writing assessment. This data article accompanies “The Effects of Prior Computer Use on Computer-Based Writing: <b>The</b> 2011 <b>NAEP</b> Writing Assessment” [1]...|$|R
40|$|Two major trends {{have emerged}} {{in the field of}} {{education}} in the last two decades: a dramatic {{increase in the number of}} English learning (EL) students, and a shortage of teachers prepared to work with this student population (National Clearinghouse for English Language Acquisition, 2010). The number of EL students has increased by 57 percent over the past ten years (Ballantyne, Sanderman 2 ̆ 6 Levy, 2008). Unfortunately, EL students are not faring well on standardized tests in comparison to their English proficient (EP) peers. In fact, while 76 percent of eighth-grade EP students passed the reading National Assessment of Educational Progress (NAEP) and 74 percent passed <b>the</b> mathematics <b>NAEP,</b> only 30 percent of EL students passed <b>the</b> reading <b>NAEP</b> and 31 percent passed <b>the</b> mathematics <b>NAEP</b> (Ballantyne, Sanderman 2 ̆ 6 Levy, 2008). One contributing factor to this achievement gap may be the lack of teachers prepared to educate EL students. Dr. Katie Brooks 2 ̆ 7 contribution to: Flessner, Ryan, Miller, Grant, and Horwitz, Julie, eds. Agency through Teacher Education : Reflection, Community, and Learning. Blue Ridge Summit, US: R 2 ̆ 6 L Education, 2012...|$|R
25|$|Some NAEP {{critics argue}} that those results are {{unreliable}} because of differences in how states handle students with disabilities. In <b>the</b> latest <b>NAEP</b> testing, Kentucky did exclude higher proportions of learning disabled students in reading and writing than was typical across the nation.|$|R
50|$|There are two NAEP websites: the NCES NAEP {{website and}} The Nation’s Report Card website. The first site details <b>the</b> <b>NAEP</b> program holistically, {{while the second}} focuses {{primarily}} on the individual releases of data.|$|E
50|$|In 2009, {{only some}} 27% of {{students}} in Alaska were proficient in Reading on <b>the</b> <b>NAEP</b> test. For some reason, though Anchorage's population exceeded 250,000 NAEP did not include Anchorage in its review of Urban Districts.|$|E
50|$|NAEP is {{conducted}} {{in partnership with}} states. <b>The</b> <b>NAEP</b> program provides funding for a full-time NSC in each state. He or she serves as the liaison between NAEP, the state’s education agency, and the schools selected to participate.|$|E
40|$|The study {{compared}} a {{group of}} 35 states which scored {{at or above the}} national mean on the 2003 fourth grade NAEP reading assessment with {{a group of}} 13 states which scored below <b>the</b> national <b>NAEP</b> mean. There were substantial {{differences between the two groups}} on most of the eight predictor variables. The two groups of states were also compared on <b>the</b> 2015 <b>NAEP</b> fourth grade reading assessment. The differences which existed between the two groups in 2003 also were found in 2015. For the most part the rank order of the total group of 48 states in 2015 was quite similar to the rank order in 2003. The correlation between state reading scores in 2003 and state reading scores in 2015 was 0. 82...|$|R
30|$|Six test {{forms were}} {{administered}} in a two-stage {{design for the}} MCBS. The items were taken from an existing item pool to form five 25 -minute blocks. The existing item pool was composed of 8 th grade mathematics items from <b>the</b> 2011 <b>NAEP</b> P&P operational assessment.|$|R
50|$|This report {{takes a look}} at the {{obsession}} with standardized testing, at <b>the</b> newly available <b>NAEP</b> data, and at international comparisons.|$|R
50|$|This special {{study in}} {{multi-stage}} testing, implemented in 2011, investigated {{the use of}} adaptive testing principles in <b>the</b> <b>NAEP</b> context. A sample of students were given an online mathematics assessment which adapts to their ability level. All {{of the items in}} the study are existing NAEP items.|$|E
50|$|This report again {{examines}} {{the implementation of}} the federal No Child Left Behind Act, specifically related to atrocities committed by states with regards to the testing requirement and about the issue of funding. The report also looks at trend in <b>the</b> <b>NAEP</b> and conservatives’ reaction to charter school performance among other topics.|$|E
50|$|The HSTS {{explores the}} {{relationship}} between grade 12 NAEP achievement and high school academic careers by surveying the curricula being followed in our nation’s high schools and the course-taking patterns {{of high school students}} through a collection of transcripts. Recent studies have placed an emphasis on STEM education and how it correlates to student achievement on <b>the</b> <b>NAEP</b> mathematics and science assessments.|$|E
5000|$|America's Charter Schools was a {{pilot study}} {{conducted}} {{as a part of}} <b>the</b> 2003 <b>NAEP</b> assessments in mathematics and reading at the fourth-grade level. While charter schools are similar to other public schools in many respects, they differ in several important ways, including the makeup of the student population and their location.|$|R
50|$|NAGB sets <b>the</b> {{calendar}} for <b>NAEP</b> assessments. Please {{refer to}} the entire assessment schedule for all NAEP assessments since 1968 and those planned through 2017.|$|R
40|$|To help teachers, policy analysts, {{and others}} {{understand}} the 1994 National Assessment of Educational Progress (NAEP) Assessment in Reading, this journal issue presents the objectives {{and examples of}} how students in grades 4, 8, and 12 will be assessed. <b>The</b> issue discusses <b>NAEP's</b> view of reading; key features of the assessment; reading purposes; reading stances; and question types and allocation of testing time. A table listing aspects of reading literacy in <b>the</b> 1994 <b>NAEP</b> assessment, and a table presenting the distribution of items and percentage of assessment time by question type and grade are included. (RS) * Reproductions supplied by EDRS are the best {{that can be made}} * * from the original document. ...|$|R
5000|$|For a study {{commissioned}} by the US Department of Education, McLaughlin et al (2000) analyzed the performance of students in over 2500 schools on state exams adjusted for difficulty via their scores on <b>the</b> <b>NAEP</b> (national) exams. The analysis showed that after controlling for student background, the only factor positively correlated with student test scores was class size. In this study, student achievement was even more strongly linked to smaller classes in the upper than the lower grades.|$|E
50|$|The city's {{public school}} system, Charlotte-Mecklenburg Schools, is the 2nd largest in North Carolina and 17th {{largest in the}} nation. In 2009, it won <b>the</b> <b>NAEP</b> Awards, the Nation's Report Card for urban school systems with top honors among 18 city systems for 4th grade math, 2nd place among 8th graders. An {{estimated}} 144,000 students are taught in 164 separate elementary, middle, and high schools. Elementary-Middle schools also include the independent Charlotte Preparatory School, with learning based on Montessori education principles.|$|E
50|$|The National Assessment of Educational Progress {{reports the}} {{national}} Black-White gap and the Hispanic-White Gap {{in math and}} reading assessments, measured at the 4th and 8th grade level. The trends show both gaps widen in mathematics as students grow older, but tend to stay the same in reading. Furthermore, <b>the</b> <b>NAEP</b> measures the widening and narrowing of achievement gaps on a state level. From 2007 to 2009, the achievement gaps {{for the majority of}} states stayed the same, although more fluctuations were seen at the 8th grade level than the 4th grade level.|$|E
5000|$|Charles H. Eccleston is an American author, {{consultant}} and lecturer in US National Environmental Policy Act (NEPA). Dr. Eccleston has received many awards including the National Association of Environmental Professional’s (NAEP) Outstanding Achievement Award (2001), <b>the</b> <b>NAEP's</b> Outstanding Environmental Leadership Award (2010). and his college's Outstanding Alumni Award. He {{is listed in}} Marquis’ Who’s Who in the World and Who’s Who in America {{as one of the}} leading authorities on preparing Environmental Impact Statements (EIS) and the NEPA. He has written 10 books and authored over 70 environmental and scientific publications. His books are used by professionals and in university curriculum. His recent works include: The EIS Book, NEPA and Environmental Planning, Global Environmental Policy, Inside Energy, and Preparing NEPA Environmental Assessments. He developed a suite of peer-reviewed tools and techniques for improving and streamlining NEPA. Many of these tools are used by government agencies and in legal cases: ...|$|R
30|$|In {{order to}} answer such questions, NAEP {{needed to move}} beyond its {{original}} design and methodologies, which yielded interpretations of the data that were fixed to the individual items used in the assessments. The framework put forth by Messick and his colleagues for <b>the</b> new <b>NAEP</b> design addressed <b>the</b> desire to move beyond such limited interpretations and, in doing so, {{changed the face of}} large-scale assessments.|$|R
50|$|Overall, {{differences}} in student performance {{that arise from}} gender tend to be smaller than that of other demographic differences, such as race or socioeconomic class. The results of <b>the</b> 1992 <b>NAEP</b> 12th grade science tests, on a 500-point scale, show that the differences of scores between white and African American students were around 48 points, while differences {{between male and female}} students were around 11 points.|$|R
5000|$|NAEP's {{heavy use}} of {{statistical}} hypothesis testing has drawn some criticism related to interpretation of results. For example, the Nation's Report Card reported [...] "Males Outperform Females at all Three Grades in 2005" [...] {{as a result}} of science test scores of 100,000 students in each grade. Hyde and Linn criticized this claim, because the mean difference was only 4 out of 300 points, implying a small effect size and heavily overlapped distributions. They argue that [...] "small differences in performance in <b>the</b> <b>NAEP</b> and other studies receive extensive publicity, reinforcing subtle, persistent, biases." ...|$|E
5000|$|Diane Ravitch, Research Professor of Education at New York University and a nonresident {{senior fellow}} at the Brookings Institution, {{similarly}} criticizes the film's lack of accuracy. The most substantial distortion in the film, according to Ravitch, is the film's claim that [...] "70 percent of eighth-grade students cannot read at grade level," [...] a misrepresentation of data from the National Assessment of Educational Progress. Ravitch served as a board member with <b>the</b> <b>NAEP</b> and says that [...] "the NAEP doesn't measure performance in terms of grade-level achievement," [...] as claimed in the film, but only as [...] "advanced," [...] "proficient," [...] and [...] "basic." [...] The film assumes that any student below proficient is [...] "below grade level," [...] but this claim is not supported by <b>the</b> <b>NAEP</b> data. Ravitch says that a study by Stanford University economist Margaret Raymond of 5000 charter schools found that only 17% are superior in math test performance to a matched public school, and many perform badly, casting doubt on the film's claim that privately managed charter schools are the solution to bad public schools. (The film says, however, that it is focusing on the one in five superior charter schools, or close to 17%, that do outperform public schools.) One {{of the reasons for the}} high test scores, writes Ravitch, is that many charter schools expel low-performing students to bring up their average scores. Ravitch also writes that many charter schools are involved in [...] "unsavory real estate deals" ...|$|E
5000|$|Based on STAR data, Krueger and Whitmore (2002) {{estimated}} {{that if all}} students were assigned to a small class in grades K-3 for one to four years, the black-white test-score gap would drop by 38 percent in grades K-3 and by 15 percent thereafter. They also {{estimated that}} national trends in pupil-teacher ratios for {{black and white students}} between 1971 and 1999 accounted for nearly all of the narrowing of the black-white test score gap over that period, as measured by <b>the</b> <b>NAEP</b> exam. In addition, they concluded that smaller classes in grades K-3 would lead to a narrowing of the black-white gap in taking college entrance exams by 60%, and would shrink the gap in scores on these exams.|$|E
40|$|Open {{the sports}} or {{business}} section {{of your daily}} newspaper, and you are immediately bombarded {{with an array of}} graphs, tables, diagrams, and statistical reports that require interpretation. Across all walks of life, the need to understand statistics is fundamental. Given that our youngsters’ future world will be increasingly data laden, scaffolding their statistical understanding and reasoning is imperative, from the early grades on. The National Council of Teachers of Mathematics (NCTM) continues to emphasize the importance of early statistical learning; data analysis and probability was the Council’s professional development “Focus of the Year” for 2007 – 2008. We need such a focus, especially given the results of the statistics items from <b>the</b> 2003 <b>NAEP.</b> As Shaughnessy (2007) noted, students’ performance was weak on more complex items involving interpretation or application of items of information in graphs and tables. Furthermore, little or no gains were made between <b>the</b> 2000 <b>NAEP</b> and <b>the</b> 2003 <b>NAEP</b> studies. One approach I have taken to promote young children’s statistical reasoning is through data modeling. Having implemented in grades 3 – 9 a number of model-eliciting activities involving working with data (e. g., English 2010), I observed how competently children could create their own mathematical ideas and representations—before being instructed how to do so. I thus wished to introduce data-modeling activities to younger children, confi dent that they would likewise generate their own mathematics. I recently implemented data-modeling activities in a cohort of three first-grade classrooms of six year- olds. I report on some of the children’s responses and discuss the components of data modeling the children engaged in...|$|R
40|$|Issues {{involved}} in linking the National Assessment of Educational Progress (<b>NAEP)</b> to <b>the</b> proposed Voluntary National Tests (VNTs) are discussed. Linkage {{is used to}} refer to procedures intended to permit scores from two different tests that are designed to measure the same variable to be expressed on the same scale. There are substantial differences between <b>NAEP</b> and <b>the</b> VNT that present serious challenges to linking <b>the</b> VNT to <b>NAEP.</b> <b>The</b> single greatest consideration in evaluating the potential for score scales to be linked is that of construct equivalence. Allied with the notion of construct equivalence are questions of the purposes of the assessments, administration conditions, and the stakes, visibility, and motivation. The greatest impact on overall construct equivalence {{is the extent to which}} content covered on the proposed VNTs can be viewed as consistent with that covered by <b>the</b> respective <b>NAEP</b> tests. Achievement levels, reporting methods, interpretations, and audiences must also be considered. The technical problems are serious enough, and the weight of policy considerations and uncertainty about how a VNT will affect NAEP are also worth contemplating. There are policy issues that should be addressed before considering linking methods. (Contains 1 table and 11 references.) (SLD) Reproductions supplied by EDRS are the best that can be made from the original document...|$|R
5000|$|Since 1983, ETS has {{conducted}} the National Assessment of Educational Progress (<b>NAEP),</b> known as <b>the</b> [...] "Nation's Report Card," [...] {{under contract to}} the US National Center for Education Statistics. <b>NAEP</b> is <b>the</b> only nationally representative and continuing assessment of what US students know and can do. ETS is responsible for coordination among <b>the</b> nine <b>NAEP</b> Alliance contractors, for item development, and for design, data analysis, and reporting.|$|R
