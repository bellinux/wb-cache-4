10000|10000|Public
25|$|This {{implies that}} in a {{weighted}} sum of variables, the variable with the largest weight will have a disproportionally large weight in <b>the</b> <b>variance</b> <b>of</b> the total. For example, if X and Y are uncorrelated {{and the weight of}} X is two times the weight of Y, then the weight of <b>the</b> <b>variance</b> <b>of</b> X will be four times the weight of <b>the</b> <b>variance</b> <b>of</b> Y.|$|E
25|$|That is, <b>the</b> <b>variance</b> <b>of</b> {{the mean}} {{decreases}} when n increases. This formula for <b>the</b> <b>variance</b> <b>of</b> the mean {{is used in}} the definition of the standard error of the sample mean, which {{is used in the}} central limit theorem.|$|E
25|$|That <b>the</b> <b>variance</b> <b>of</b> {{the random}} {{variable}} describing the next event grows smaller and smaller.|$|E
2500|$|<b>The</b> <b>variances</b> <b>of</b> <b>the</b> log inverse {{variables}} are identical to <b>the</b> <b>variances</b> <b>of</b> <b>the</b> log variables: ...|$|R
3000|$|From {{equations}} (21) and (23) it {{is apparent}} that <b>the</b> <b>variances</b> <b>of</b> <b>the</b> two-stage estimators Y_ 2 [...] and Ỹ_ 2 [...] depend on <b>the</b> <b>variances</b> <b>of</b> <b>the</b> y [...]...|$|R
30|$|Other {{more complex}} cases {{can be viewed}} as the {{combination}} of these two cases. Here, notice that <b>the</b> <b>variances</b> <b>of</b> channel gains are constant, but <b>the</b> <b>variances</b> <b>of</b> <b>the</b> estimation errors depend on the position of the code block.|$|R
25|$|Fréchet {{distribution}}: <b>The</b> <b>variance</b> <b>of</b> this {{distribution is}} defined only for α > 2.|$|E
25|$|For non-normal samples, <b>the</b> <b>{{variance}}</b> <b>of</b> {{the sample}} variance {{depends on the}} kurtosis; for details, please see variance.|$|E
25|$|The {{population}} variance matches <b>the</b> <b>variance</b> <b>of</b> the generating probability distribution. In this sense, {{the concept of}} population can be extended to continuous random variables with infinite populations.|$|E
3000|$|... [...]. <b>The</b> <b>variances</b> <b>of</b> each {{block in}} the frame is then summed up to get <b>the</b> motion <b>variance</b> measure <b>of</b> each frame.|$|R
40|$| {{significance}} level when <b>the</b> <b>variances</b> <b>of</b> treatment groups are unequal, and,|$|R
5000|$|... where [...] is as above, [...] <b>the</b> average <b>variance</b> <b>of</b> each {{component}} (item), and [...] {{the average of}} all covariances between the components across the current sample of persons (that is, without including <b>the</b> <b>variances</b> <b>of</b> {{each component}}).|$|R
25|$|When {{the mean}} is not known, the minimum {{mean squared error}} {{estimate}} of <b>the</b> <b>variance</b> <b>of</b> a sample from Gaussian distribution is achieved by dividing by nnbsp&+nbsp&1, rather than nnbsp&−nbsp&1 or nnbsp&+nbsp&2.|$|E
25|$|Coal {{homogenization}} {{refers to}} the process of mixing coal to reduce <b>the</b> <b>variance</b> <b>of</b> the product supplied. This homogenization process is performed during the coal stockpiling operation. Although the terms blending and homogenization are often used interchangeably, there are differences as the definitions show. The most notable difference is that blending refers to stacking coal from different sources together on one stockpile. The reclaimed heap would then typically have a weighted average output quality of the input sources. In contrast, homogenization focuses on reducing <b>the</b> <b>variance</b> <b>of</b> only one source. A blending operation will cause some homogenization.|$|E
25|$|In other words, the {{standard}} deviation σ (sigma) is the square root of <b>the</b> <b>variance</b> <b>of</b> X; i.e., it is the square root of the average value of (X−μ)2.|$|E
2500|$|It also {{follows that}} <b>the</b> <b>variances</b> <b>of</b> <b>the</b> logit {{transformed}} variables are: ...|$|R
50|$|The {{key reason}} for studentizing is that, in {{regression}} {{analysis of a}} multivariate distribution, <b>the</b> <b>variances</b> <b>of</b> <b>the</b> residuals at different input variable values may differ, even if <b>the</b> <b>variances</b> <b>of</b> <b>the</b> errors at these different input variable values are equal. The issue {{is the difference between}} errors and residuals in statistics, particularly the behavior of residuals in regressions.|$|R
3000|$|... (n) 2 are {{estimated}} from <b>the</b> <b>variances</b> <b>of</b> satellite MF model coefficients in 2005, averaged over all orders. Given Equation (1), the relevant time-scales are by definition τ _c(n)=√(3)σ _g(n) / σ _ġ(n), with σ _ġ(n) <b>the</b> secular variation <b>variances,</b> estimated from <b>the</b> <b>variances</b> <b>of</b> satellite SV model coefficients in 2005, averaged over all orders (see Gillet et al. 2013).|$|R
25|$|These values can be {{used for}} a {{statistical}} criterion as to the goodness of fit. When unit weights are used, the numbers should be divided by <b>the</b> <b>variance</b> <b>of</b> an observation.|$|E
25|$|However, if one changes coordinates, the {{way that}} {{coefficients}} change depends on <b>the</b> <b>variance</b> <b>of</b> the object, and one cannot ignore the distinction; see covariance and contravariance of vectors.|$|E
25|$|The weights should, ideally, {{be equal}} to the {{reciprocal}} of <b>the</b> <b>variance</b> <b>of</b> the measurement. applies. In this case the weight matrix should ideally {{be equal to}} the inverse of the variance-covariance matrix of the observations.|$|E
5000|$|Variance-time plot: {{based on}} <b>the</b> {{analysis}} <b>of</b> <b>the</b> <b>variances</b> <b>of</b> <b>the</b> aggregate processes ...|$|R
30|$|Nevertheless, after Period 7, <b>the</b> <b>variance</b> {{decomposition}} <b>of</b> {{stock price}} demonstrates that interest rate shocks account for approximately 9 % <b>of</b> <b>the</b> forecast error <b>variance</b> <b>of</b> <b>the</b> stock price. Similarly, <b>the</b> <b>variance</b> decomposition <b>of</b> interest rates {{shows that the}} stock price shocks accounted for approximately 13 % <b>of</b> <b>the</b> forecast error <b>variance</b> <b>of</b> <b>the</b> interest rates, and the remaining proportion is accounted for by its own innovation.|$|R
3000|$|Compared with <b>the</b> <b>variances</b> <b>of</b> channel {{estimation}} {{over one}} OFDMA symbol as in (22)–(24), <b>the</b> estimation <b>variances</b> (29)–(31) <b>of</b> <b>the</b> weighted average estimator (15)–(18) are significantly reduced {{owing to the}} fact that [...]...|$|R
25|$|These same {{formulae}} {{can be used}} {{to obtain}} confidence intervals on <b>the</b> <b>variance</b> <b>of</b> residuals from a least squares fit under standard normal theory, where k is now the number of degrees of freedom for error.|$|E
25|$|The gamma {{distribution}} {{is widely used}} as a conjugate prior in Bayesian statistics. It is the conjugate prior for the precision (i.e. inverse of <b>the</b> <b>variance)</b> <b>of</b> a normal distribution. It is also the conjugate prior for the exponential distribution.|$|E
25|$|This {{will work}} in cases when a single color is to be {{compared}} to a single color and the need is to simply know whether a distance is greater. If these squared color distances are summed, such a metric effectively becomes <b>the</b> <b>variance</b> <b>of</b> the color distances.|$|E
3000|$|... from <b>the</b> <b>variances</b> <b>of</b> <b>the</b> {{coefficient}} series h_n^m(t) and v_n^m(t) in dipole coordinates, first obtained {{over the}} period 1960 to 2010 for which the model is weakly sensitive to the a priori matrices (we would underestimate <b>the</b> <b>variances</b> <b>of</b> <b>the</b> model parameters by considering the era before 1960, where fewer, less accurate data are available). Coefficients are then rotated back into geographic coordinates.|$|R
2500|$|From {{equation}} (7), {{it follows}} that <b>the</b> <b>variances</b> <b>of</b> indicated receiver position and time are ...|$|R
5000|$|... (where [...] and [...] are <b>the</b> <b>variances</b> <b>of</b> <b>the</b> {{distributions}} of q and p, respectively) ...|$|R
25|$|The Cramér–Rao bound {{can also}} be used to bound <b>the</b> <b>variance</b> <b>of</b> biased estimators of given bias. In some cases, a biased {{approach}} can result in both a variance and a mean squared error that are below the unbiased Cramér–Rao lower bound; see estimator bias.|$|E
25|$|No {{good way}} of calculating the {{variance}} is known. The best known method is to approximate the multivariate Wallenius distribution by a multivariate Fisher's noncentral hypergeometric distribution with the same mean, and insert the mean as calculated above in the approximate formula for <b>the</b> <b>variance</b> <b>of</b> the latter distribution.|$|E
25|$|Two {{cross-country}} comparisons {{have found}} great {{variation in the}} gender differences regarding the degree of variance in mathematical ability. In most nations males have greater variance. In a few females have greater variance. Hyde and Mertz argue that boys and girls differ in <b>the</b> <b>variance</b> <b>of</b> their ability due to sociocultural factors.|$|E
5000|$|From {{equation}} (7), {{it follows}} that <b>the</b> <b>variances</b> <b>of</b> indicated receiver position and time are ...|$|R
5000|$|Following are <b>the</b> <b>variances</b> <b>of</b> <b>the</b> {{posterior}} distribution obtained with these three prior probability distributions: ...|$|R
30|$|Calculate <b>the</b> initial <b>variances</b> <b>of</b> <b>the</b> {{measurement}} noise {σ _vk^ 2 (0)}_k= 1 ^K {{which represent}} <b>the</b> <b>variances</b> <b>of</b> <b>the</b> estimation error of the covariance-fitting-based algorithm at t= 0. These variances {{can also be}} obtained from the diagonal entries of (54) for a large number of samples.|$|R
