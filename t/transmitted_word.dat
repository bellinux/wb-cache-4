20|150|Public
2500|$|The above proof {{suggests}} a simple algorithm for burst error detection/correction in cyclic codes: given a <b>transmitted</b> <b>word</b> (i.e. a polynomial of degree [...] ), compute {{the remainder of}} this word when divided by [...] If the remainder is zero (i.e. if the word is divisible by [...] ), then it is a valid codeword. Otherwise, report an error. To correct this error, subtract this remainder from the <b>transmitted</b> <b>word.</b> The subtraction result is going to be divisible by [...] (i.e. {{it is going to be}} a valid codeword).|$|E
2500|$|Proof. First {{we observe}} that a code can detect all bursts of length [...] if {{and only if}} no two codewords differ by a burst of length [...] Suppose that we have two code words [...] and [...] that differ by a burst [...] of length [...] Upon {{receiving}} , we can not tell whether the <b>transmitted</b> <b>word</b> is indeed [...] with no transmission errors, or whether it is [...] with a burst error [...] that occurred during transmission. Now, suppose that every two codewords differ by more than a burst of length [...] Even if the transmitted codeword [...] is hit by a burst [...] of length , {{it is not going to}} change into another valid codeword. Upon receiving it, we can tell that this is [...] with a burst [...] By the above observation, we know that no two codewords can share the first [...] symbols. The reason is that even if they differ in all the other [...] symbols, they are still going to be different by a burst of length [...] Therefore, the number of codewords [...] satisfies [...] Applying [...] to both sides and rearranging, we can see that [...]|$|E
2500|$|Proof. First {{we observe}} that a code can correct all bursts of length [...] if {{and only if}} no two codewords differ by the sum of two bursts of length [...] Suppose that two codewords [...] and [...] differ by bursts [...] and [...] of length [...] each. Upon {{receiving}} [...] hit by a burst , we could interpret that as if it was [...] hit by a burst [...] We can not tell whether the <b>transmitted</b> <b>word</b> is [...] or [...] Now, suppose that every two codewords differ by more than two bursts of length [...] Even if the transmitted codeword [...] is hit by a burst of length , {{it is not going to}} look like another codeword that has been hit by another burst. For each codeword [...] let [...] denote the set of all words that differ from [...] by a burst of length [...] Notice that [...] includes [...] itself. By the above observation, we know that for two different codewords [...] and [...] and [...] are disjoint. We have [...] codewords. Therefore, we can say that [...] Moreover, we have [...] By plugging the latter inequality into the former, then taking the base [...] logarithm and rearranging, we get the above theorem.|$|E
5000|$|To <b>transmit</b> <b>words</b> {{of length}} n, the ideal channel {{is to be}} applied n times, so we {{consider}} the tensor power ...|$|R
5000|$|... is {{therefore}} {{a measure of the}} ability of the channel to <b>transmit</b> <b>words</b> of length n faithfully by being invoked m times.|$|R
50|$|Charles Bourseul (1829-1912), scientist, {{inventor}} of {{a method of}} <b>transmitting</b> <b>words</b> using electricity. He lived at No. 62 Rue d'Arcueil (since named Rue Paul Vaillant-Couturier).|$|R
2500|$|Proof: Suppose {{that we have}} an [...] {{code that}} can correct all bursts of length [...] Interleaving can provide us with a [...] code that can correct all bursts of length [...] for any given [...] If we want to encode a message of an {{arbitrary}} length using interleaving, first we divide it into blocks of length [...] We write the [...] entries of each block into a [...] matrix using row-major order. Then, we encode each row using the [...] code. What we will get is a [...] matrix. Now, this matrix is read out and transmitted in column-major order. The trick is that if there occurs a burst of length [...] in the <b>transmitted</b> <b>word,</b> then each row will contain approximately [...] consecutive errors (More specifically, each row will contain a burst of length at least [...] and at most [...] ). If [...] then [...] and the [...] code can correct each row. Therefore, the interleaved [...] code can correct the burst of length [...] Conversely, if [...] then at least one row will contain more than [...] consecutive errors, and the [...] code might fail to correct them. Therefore, the error correcting ability of the interleaved [...] code is exactly [...] The BEC efficiency of the interleaved code remains the same as the original [...] code. This is true because: ...|$|E
5000|$|The above proof {{suggests}} a simple algorithm for burst error detection/correction in cyclic codes: given a <b>transmitted</b> <b>word</b> (i.e. a polynomial of degree [...] ), compute {{the remainder of}} this word when divided by [...] If the remainder is zero (i.e. if the word is divisible by [...] ), then it is a valid codeword. Otherwise, report an error. To correct this error, subtract this remainder from the <b>transmitted</b> <b>word.</b> The subtraction result is going to be divisible by [...] (i.e. {{it is going to be}} a valid codeword).|$|E
5000|$|Many of the ASCII control {{characters}} {{were designed for}} devices {{of the time that}} are not often seen today. For example, code 22, [...] "synchronous idle" [...] (SYN), was originally sent by synchronous modems (which have to send data constantly) when there was no actual data to send. (Modern systems typically use a start bit to announce the beginning of a <b>transmitted</b> <b>word</b> - [...] this is a feature of asynchronous communication. Synchronous communication links were more often seen with mainframes, where they were typically run over corporate leased lines to connect a mainframe to another mainframe or perhaps a minicomputer.) ...|$|E
50|$|The {{disparity}} of a {{bit pattern}} {{is the difference}} in the number of one bits vs the number of zero bits. The running disparity is the running total of the disparity of all previously <b>transmitted</b> <b>words.</b>|$|R
50|$|Api assists Hobson with {{procuring}} {{equipment for}} the scientist's studies. A radio transceiver is set up, and the duo <b>transmit</b> <b>words</b> and Morse code across the world. They receive no response. Hobson's investigations reveal no reason for their exemption from the Effect. The men face the prospect that they are alone.|$|R
5000|$|Since {{the source}} is only 4 bits {{then there are}} only 16 {{possible}} <b>transmitted</b> <b>words.</b> Included is the eight-bit value if an extra parity bit is used (see Hamming(7,4) code with an additional parity bit). (The data bits are shown in blue; the parity bits are shown in red; and the extra parity bit shown in green.) ...|$|R
50|$|The bonded {{serial bus}} {{architecture}} was chosen over the traditional parallel bus due to inherent {{limitations of the}} latter, including half-duplex operation, excess signal count, and inherently lower bandwidth due to timing skew. Timing skew results from separate electrical signals within a parallel interface traveling through conductors of different lengths, on potentially different printed circuit board (PCB) layers, and at possibly different signal velocities. Despite being transmitted simultaneously as a single word, signals on a parallel interface have different travel duration and arrive at their destinations at different times. When the interface clock period is shorter than the largest time difference between signal arrivals, recovery of the <b>transmitted</b> <b>word</b> is no longer possible. Since timing skew over a parallel bus can amount to a few nanoseconds, the resulting bandwidth limitation is {{in the range of}} hundreds of megahertz.|$|E
5000|$|Proof. First {{we observe}} that a code can detect all bursts of length [...] if {{and only if}} no two codewords differ by a burst of length [...] Suppose that we have two code words [...] and [...] that differ by a burst [...] of length [...] Upon {{receiving}} , we can not tell whether the <b>transmitted</b> <b>word</b> is indeed [...] with no transmission errors, or whether it is [...] with a burst error [...] that occurred during transmission. Now, suppose that every two codewords differ by more than a burst of length [...] Even if the transmitted codeword [...] is hit by a burst [...] of length , {{it is not going to}} change into another valid codeword. Upon receiving it, we can tell that this is [...] with a burst [...] By the above observation, we know that no two codewords can share the first [...] symbols. The reason is that even if they differ in all the other [...] symbols, they are still going to be different by a burst of length [...] Therefore, the number of codewords [...] satisfies [...] Applying [...] to both sides and rearranging, we can see that [...]|$|E
5000|$|Proof. First {{we observe}} that a code can correct all bursts of length [...] if {{and only if}} no two codewords differ by the sum of two bursts of length [...] Suppose that two codewords [...] and [...] differ by bursts [...] and [...] of length [...] each. Upon {{receiving}} [...] hit by a burst , we could interpret that as if it was [...] hit by a burst [...] We can not tell whether the <b>transmitted</b> <b>word</b> is [...] or [...] Now, suppose that every two codewords differ by more than two bursts of length [...] Even if the transmitted codeword [...] is hit by a burst of length , {{it is not going to}} look like another codeword that has been hit by another burst. For each codeword [...] let [...] denote the set of all words that differ from [...] by a burst of length [...] Notice that [...] includes [...] itself. By the above observation, we know that for two different codewords [...] and [...] and [...] are disjoint. We have [...] codewords. Therefore, we can say that [...] Moreover, we have [...] By plugging the latter inequality into the former, then taking the base [...] logarithm and rearranging, we get the above theorem.|$|E
2500|$|The first AM {{transmission}} {{was made}} by Canadian researcher Reginald Fessenden on 23 December 1900 using a spark gap transmitter with a specially designed high frequency 10kHz interrupter, over a distance of 1 mile (1.6km) at Cobb Island, Maryland, USA. [...] His first <b>transmitted</b> <b>words</b> were, [...] "Hello. One, two, three, four. [...] Is it snowing where you are, Mr. Thiessen?". [...] The words were barely intelligible above the background buzz of the spark.|$|R
50|$|The {{authors of}} most Subhashita are unknown. This form of Indian epigrammatic poetry had a wide following, were created, memorized and <b>transmitted</b> by <b>word</b> of mouth.|$|R
5000|$|Alexander Marr, The Production and Distribution of Mutio Oddis Dello squadro, in S. Kusukawa and I. Maclean (eds.), <b>Transmitting</b> Knowledge: <b>Words,</b> Images and Instruments Early Modern Europe (Oxford-Warburg, 2006), pp. 165-192.|$|R
5000|$|Proof: Suppose {{that we have}} an [...] {{code that}} can correct all bursts of length [...] Interleaving can provide us with a [...] code that can correct all bursts of length [...] for any given [...] If we want to encode a message of an {{arbitrary}} length using interleaving, first we divide it into blocks of length [...] We write the [...] entries of each block into a [...] matrix using row-major order. Then, we encode each row using the [...] code. What we will get is a [...] matrix. Now, this matrix is read out and transmitted in column-major order. The trick is that if there occurs a burst of length [...] in the <b>transmitted</b> <b>word,</b> then each row will contain approximately [...] consecutive errors (More specifically, each row will contain a burst of length at least [...] and at most [...] ). If [...] then [...] and the [...] code can correct each row. Therefore, the interleaved [...] code can correct the burst of length [...] Conversely, if [...] then at least one row will contain more than [...] consecutive errors, and the [...] code might fail to correct them. Therefore, the error correcting ability of the interleaved [...] code is exactly [...] The BEC efficiency of the interleaved code remains the same as the original [...] code. This is true because: ...|$|E
40|$|We define new error {{correcting}} codes {{based on}} extractors. We show that for certain choices of parameters these codes have better list decoding properties than {{are known for}} other codes, and are provably better than Reed-Solomon codes. We further show that codes with strong list decoding properties are equivalent to slice extractors, a variant of extractors. We give an application of extractor codes to extracting many hardcore bits from a one-way function, using few auxiliary random bits. Finally, we show that explicit slice extractors for certain other parameters would yield optimal bipartite Ramsey graphs. 1 Introduction An error-correcting code is used to transmit information over noisy channels. If the received word differs from the <b>transmitted</b> <b>word</b> in not too many places, the closest codeword to the received word will equal the <b>transmitted</b> <b>word.</b> If such unique decoding is required, then error correcting codes cannot correct a number of errors {{which is more than}} half the minimum di [...] ...|$|E
40|$|Abstract—The sequence-reconstruction {{problem was}} first {{proposed}} by Levenshtein in 2001. This problem studies the model where {{the same word}} is transmitted over multiple channels. If the <b>transmitted</b> <b>word</b> belongs to some code of minimum distance d and there are at most r errors in every channel, then the minimum number of channels that guarantees a successful decoder (under the assumption that all channel outputs are distinct) has to be greater than the largest intersection of two balls of radius r and with distance at least d between their centers. This paper studies the combinatorial problem of computing the largest intersection of two balls for two cases. In the first part we solve this problem in the Grassmann graph for all values of d and r. In the second part we derive similar results for permutations under Kendall’s τ-metric for some special cases of d and r. I...|$|E
5000|$|RT to Controller Transfer. The Bus Controller sends one <b>transmit</b> command <b>word</b> to a Remote Terminal. The Remote Terminal then sends {{a single}} Status word, {{immediately}} followed by 1 to 32 words.|$|R
40|$|Abstract—In {{this paper}} a physical-layer network coded twoway relay system {{applying}} Low-Density Parity-Check (LDPC) codes for error correction is considered, where two sources A and B desire to exchange information {{with each other}} by the help of arelayR. The critical process in such a system is the calculation of the network-coded <b>transmit</b> <b>word</b> at the relay on basis of the superimposed channel-coded words of the two sources. For this joint channel-decoding and network-encoding task a generalized Sum-Product Algorithm (SPA) is developed. This novel iterative decoding approach outperforms other recently proposed schemes as demonstrated by simulation results. I...|$|R
40|$|Abstract—A physical-layer network coded two-way relay sys-tem {{applying}} Low-Density Parity-Check (LDPC) {{codes for}} error correction is considered in this paper, where two sources A and B desire to exchange information {{with each other}} by {{the help of a}} relay R. The critical process in such a system is the calculation of the network-coded <b>transmit</b> <b>word</b> at the relay on basis of the superimposed channel-coded QPSK words of the two sources. For this joint channel-decoding and network-encoding task a generalized Sum-Product Algorithm (SPA) over F 16 is developed. This novel iterative decoding approach outperforms other recently proposed schemes as demonstrated by simulations. I...|$|R
40|$|The naming game (NG) {{describes}} the agreement dynamics {{of a population}} of N agents interacting locally in pairs leading {{to the emergence of}} a shared vocabulary. This model has its relevance in the novel fields of semiotic dynamics and specifically to opinion formation and language evolution. The application of this model ranges from wireless sensor networks as spreading algorithms, leader election algorithms to user-based social tagging systems. In this paper, we introduce the concept of overhearing (i. e., at every time step of the game, a random set of N-delta individuals are chosen from the population who overhear the <b>transmitted</b> <b>word</b> from the speaker and accordingly reshape their inventories). When delta = 0 one recovers the behavior of the original NG. As one increases delta, the population of agents reaches a faster agreement with a significantly low-memory requirement. The convergence time to reach global consensus scales as log N as delta approaches 1. Copyright (C) EPLA, 201...|$|E
30|$|Shannon {{established}} {{communication theory}} in 1949 and defined the basic concept of secure communication from the information-theoretic perspective [1]. Using Shannon’s approaches, a sender, Alice, securely transmits an information message M to a legitimate receiver, Bob, across a public channel. To be “perfectly secure", {{the requirement of}} the mutual information I(M;X)= 0 must be satisfied between Alice’s information message M and the <b>transmitted</b> <b>word</b> X. From this definition, Shannon proved that Alice and Bob must share a key string to achieve perfect security. This theory was {{the introduction of the}} key distribution problem and is the basis of symmetric key cryptography defense systems for the upper layer implemented today. Present systems based on cryptography prevent the extraction of information without a secure key string when information is exposed to the eavesdropper Eve. This public key algorithm depends on the computational limit of the eavesdropper to ensure computational security. In spite of the improvements in public key algorithms, there remains a problem for security based on the assumption of Eve’s limited computational resources considering the advancement of available computing power.|$|E
40|$|We present here an {{encoding}} procedure for ordered numbers {{in order to}} minimize the mean magnitude error of a signal, caused by transmission through a binary channel, where only t n fixed positions of n-words may be disturbed. It is shown that our code is optimal for the case when the probability of error is small enough. 1 Introduction Suppose we have to send send each of 2 n numbers k 1; :::; k 2 n through a binary channel. For example, we may assume that these numbers were taken from the output of an analog to code digital converter, and so we have to assign numbers k i to each vector of the n-cube. It is assumed that only single errors are likely in a <b>transmitted</b> <b>word,</b> and that n Γ t fixed positions of a word (0 t n) are error-free and the other t positions may be disturbed with probability p. If the vector assigned to k i was transmitted and the vector assigned to k j was received, then let Δ d ij = jk i Γ k j j denotes the absolute value of the error. Our [...] ...|$|E
5000|$|... "It {{was also}} rumoured that English {{technicians}} to whom Manzetti illustrated his method for <b>transmitting</b> spoken <b>words</b> on the telegraph wire intended {{to apply the}} invention on several private telegraph lines in England." ...|$|R
5000|$|Every fast link pulse burst <b>transmits</b> a <b>word</b> of 16 bits {{known as}} a link code word. The first such word is {{known as a}} base link code word, and its bits are used as follows: ...|$|R
5000|$|Cyranoids are [...] "people who do {{not speak}} {{thoughts}} originating in their own central nervous system: Rather, the words they speak originate {{in the mind of}} another person who <b>transmits</b> these <b>words</b> to the cyranoid by radio transmission".|$|R
30|$|A {{security}} {{system based on}} the physical layer was introduced by Wyner in 1975 [2] and information-theoretically secure communication was studied in [3, 4]. According to the wiretap channel model defined by Wyner, the main channel was defined between the sender, Alice, and the legitimate receiver, Bob; the wiretap channel {{was defined as a}} degraded version of the main channel. The main and wiretap channels were assumed to be discrete memoryless channels. Suppose that Alice sends Bob an s-bit message M across the main channel. Alice encodes M into an n-bit <b>transmitted</b> <b>word</b> X. Bob and Eve receive message X across the main and wiretap channel, respectively. Bob and Eve’s channel observations are denoted by Y and Z, respectively. Alice encodes the information for two objectives [2] as follows: (i) the error probability between the message M and Bob’s decoded message M̂_B of the received message Y must converge to zero (with negligibly small probability of error) [reliability]. ii) no information is shared between information message M and Eve’s received message Z. For a precise expression, the formulation is articulated as the rate of mutual information 1 /nI(M;Z)→ 0 when n→∞ [security]. Wyner defined that physical layer security is achieved without key distribution using forward error correction (FEC) when it corresponds to the considerations of reliability and security. Moreover, the secrecy rate is defined by the rate s/n, where s and n are the number of secret message bits and the number of bits transmitted over the channel, respectively. A detailed explanation of Wyner code could be found in [5].|$|E
40|$|Abstract—We (people) are memory machines. Our {{decision}} processes, {{emotions and}} {{interactions with the}} world around us are based on and driven by associations to our memories. This natural association paradigm will become critical in future memory systems, namely, the key question will not be “How do I store more information? ” but rather, “Do I have the relevant information? How do I retrieve it?” The focus {{of this paper is to}} make a first step in this direction. We define and solve a very basic problem in associative retrieval. Given a word W, the words in the memory that are t-associated with W are the words in the ball of radius t around W. In general, given a set of words, say W, X and Y, the words that are t-associated with {W, X, Y} are those in the memory that are within distance t from all the three words. Our main goal is to study the maximum size of the t-associated set as a function of the number of input words and the minimum distance of the words in memory- we call this value the uncertainty of an associative memory. We derive the uncertainty of the associative memory that consists of all the binary vectors with an arbitrary number of input words. In addition, we study the retrieval problem, namely, how do we get the t-associated set given the inputs? We note that this paradigm is a generalization of the sequences reconstruction problem that was proposed by Levenshtein (2001). In this model, a word is transmitted over multiple channels. A decoder receives all the channel outputs and decodes the <b>transmitted</b> <b>word.</b> Levenshtein computed the minimum number of channels that guarantee a successful decoder- this value happens to be the uncertainty of an associative memory with two input words. I...|$|E
40|$|We {{consider}} coding {{schemes for}} computationally bounded channels, which can introduce an arbitrary set of errors {{as long as}} (a) the fraction of errors is bounded with high probability by a parameter p and (b) the process which adds the errors can be described by a sufficiently “simple” circuit. Codes for such channel models are attractive since, like codes for standard adversarial errors, they can handle channels whose true behavior is unknown or varying over time. For two classes of channels, we provide explicit, efficiently encodable/decodable codes of optimal rate where only inefficiently decodable codes were previously known. In each case, we provide one encoder/decoder that works for every channel in the class. The encoders are randomized, and probabilities are taken over the (local, unknown to the decoder) coins of the encoder {{and those of the}} channel. Unique decoding for additive errors: We give the first construction of a polynomial-time encodable/decodable code for additive (a. k. a. oblivious) channels that achieve the Shannon capacity 1 −H(p). These are channels which add an arbitrary error vector e ∈ 0, 1 N of weight at most pN to the transmitted word; the vector e can depend on the code but not on the particular <b>transmitted</b> <b>word.</b> Such channels capture binary symmetric errors and burst errors as special cases. List-decoding for polynomial-time channels: For every constant c 3 ̆e 0, we give a Monte Carlo construction of an code with optimal rate (arbitrarily close to 1 −H(p)) that efficiently recovers a short list containing the correct message with high probability for channels describable by circuits of size at most Nc. We are not aware of any channel models considered in the information theory literature, other than purely adversarial channels, which require more than linear-size circuits to implement. We justify the relaxation to list-decoding with an impossibility result showing that, in a large range of parameters (p 3 ̆e 1 / 4), codes that are uniquely decodable for a modest class of channels (online, memoryless, nonuniform channels) cannot have positive rate...|$|E
60|$|The servant did so; but the {{mistress}} of the house merely <b>transmitted</b> her former <b>words.</b>|$|R
25|$|Music <b>transmitted</b> by <b>word</b> {{of mouth}} through a community, in time, {{develops}} many variants, because {{this kind of}} transmission cannot produce word-for-word and note-for-note accuracy. Indeed, many traditional singers are quite creative and deliberately modify the material they learn.|$|R
5000|$|It was {{eventually}} discovered that before Magus left Earth, he placed an offspring underwater. This offspring {{was approached by}} members of the Purifiers, enemies of the X-Men, who [...] "awoke" [...] it when the submarine used by The Purifiers <b>transmitted</b> the <b>word</b> [...] "Warlock." ...|$|R
