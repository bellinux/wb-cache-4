2|3|Public
40|$|In this paper, {{we present}} {{approaches}} to musical rhythm pattern extraction, rhythm-based music retrieval, and rhythm-synchronized music mixing. A probabilistic model {{is used to}} jointly estimate tempo and time signature {{as a basis for}} beat tracking and measure detection. A representative rhythm pattern is then extracted through clustering to characterize the rhythm of a song. Based on this, a probabilistic approach is used for retrieving songs with simi-lar rhythmic patterns. These are then mixed rhythm-synchronously with transitions maintaining continuity and regularity of beats. We apply the presented methods into workout-mix generation, which aims at automatically selecting rhythmically similar music given a seed song and a user-defined <b>tempo</b> <b>profile.</b> Our probabilistic ap-proaches achieve accuracies similar to best published results, but avoid manually tuned parameters and “fudge factors”. Index Terms — rhythmic pattern, rhythm-synchronized mixing, rhythm-based retrieval, tempo induction, workout-mix 1...|$|E
40|$|Rhythm {{together}} with melody {{is one of}} the basic elements in music. According to Longuet-Higgins ([LH 76]) human listeners are much more sensitive to the perception of rhythm than to the perception of melody or pitch related features. Usually it is easier for trained and untrained listeners as well as for musicians to transcribe a rhythm, than details about heard intervals, harmonic changes or absolute pitch. “The term rhythm refers to the general sense of movement in time that characterizes our experience of music (Apel, 1972). Rhythm often refers to the organization of events in time, such that they combine perceptually into groups or induce a sense of meter (Cooper & Meyer, 1960; Lerdahl & Jackendoff, 1983). In this sense, rhythm is not an objective property of music, it is an experience that has both objective and subjective components. The experience of rhythm arises from the interaction of the various materials of music – pitch, intensity, timbre, and so forth – within the individual listener. ” [LK 94] In general the task of transcribing the rhythm of a performance is different from the previously described beat tracking or tempo detection issue. For estimating a <b>tempo</b> <b>profile</b> it is sufficient to infer score time positions for a set of dedicated anchor notes (i. e., beats respectively clicks), for the rhythm transcriptio...|$|E
5000|$|The Group's retail {{operations}} are conducted through the wholly owned Willys, Hemköp and PrisXtra chains, comprising 225 stores in all. In addition, Axfood collaborates {{with a large}} number of proprietor-run stores that are tied to Axfood through agreements. These include stores within the Hemköp and Willys chains as well as stores run under the Handlar'n and <b>Tempo</b> <b>profiles.</b> In all, Axfood collaborates with some 840 proprietor-run stores.|$|R
40|$|This paper {{deals with}} the effect of tempo on phonological structure. In two {{production}} experiments, German speakers were asked to read texts at three selfselected rates, "normal", "fast", and "slow". Different speaker strategies were inspected in terms of pausing, phrasing, pitch accent structure and segmental reductions. The first aim {{of the study is}} to describe the reorganisation of the phonological structure {{as a function of the}} three speech rate categories. The second goal is to discuss the strategies used in speaking faster and slower than normal, considering in particular the homogeneity among speakers and the symmetry within speakers. The differences found between and within speakers provide a basis for modelling individual <b>tempo</b> <b>profiles</b> at the phonological level, which could be exploited e. g. for the synthesis of individual voices and speaking styles. Dieser Artikel befaßt sich mit der Auswirkung von Sprechtempo auf die phonologische Struktur. In zwei Sprachproduktionsexperimenten wurden Muttersprachle...|$|R
40|$|Measurements of tempo and {{dynamics}} from audio files or MIDI data are frequently, {{used to get}} insight into a performer's contribution to music. The measured variations in tempo {{and dynamics}} are often represented in different formats by different authors. Few systematic comparisons have been made between these representations. Moreover, it is unknown what data representation comes closest to subjective perception. The reported study tests the perceptual validity of existing data representations by comparing their ability to explain the subjective similarity between pairs of performances. In two experiments, 40 participants rated the similarity between performances of a Chopin prelude and a Mozart sonata. Models based on different representations of the tempo and dynamics of the performances were fitted to these similarity ratings. The results favor other data representations of performances than generally used, and imply that comparisons between performances are made perceptually {{in a different way}} than often assumed. For example, the best fit was obtained with models based on absolute tempo and absolute tempo times loudness, while conventional model's based on normalized variations, or on correlations between <b>tempo</b> <b>profiles</b> and loudness profiles, did not explain the similarity ratings well...|$|R

