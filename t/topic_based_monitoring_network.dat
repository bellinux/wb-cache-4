0|10000|Public
30|$|Detail running data of each {{transformer}} unit, {{including all}} single battery voltage, single battery temperature, and detailed operation information etc, have been transmitted from supplier local monitoring system to master station layer through IEC 60875 - 5 - 104 (IEC- 104) protocol <b>based</b> <b>monitoring</b> <b>network.</b>|$|R
40|$|The {{suitability}} of {{a mathematical model}} to simulate photochemical phenomena in European cities is currently being investigated. Model predictions are compared against measurements obtained {{in the city of}} Turin during a day when photochemical activity was observed. Helicopter sampling was conducted to establish the pollutant and temperature structure over and across the city. A ground <b>based</b> <b>monitoring</b> <b>network</b> provided data on ground level concentrations and the surface wind field. The performance of the model for this day is discussed...|$|R
40|$|In {{this paper}} the {{approach}} for an acoustical model <b>based</b> <b>monitoring</b> <b>network</b> is demonstrated. This network {{is capable of}} reconstructing a noise map, based on the combination of measured sound levels and an acoustic model of the area. By pre-calculating the sound attenuation within the network the noise map can be shown in real-time. The <b>monitoring</b> <b>network</b> offers an analysis tool for instance for industry or legal authorities. Alternatively, {{it can be an}} extension to the EU legislated noise maps. In principle the <b>monitoring</b> <b>network</b> works as follows. Battery powered microphones are deployed in an outdoor area. SPL's are measured in octave bands. Wireless communication and multi-hopping is used to collect the data at a central station. The SPL values are used to localize non-directional point sources, while accounting for obstacles and meteorological effects in the sound attenuation. The final result is a list of reconstructed sources with their locations and strengths and a noise contour map. The <b>monitoring</b> <b>network</b> is scalable and seems feasible and useful for large areas...|$|R
5000|$|Statistical anomaly-based detection: An IDS {{which is}} anomaly <b>based</b> will <b>monitor</b> <b>network</b> traffic {{and compare it}} against an {{established}} baseline. The baseline will identify what is [...] "normal" [...] for that network - what sort of bandwidth is generally used and what protocols are used. It may however, raise a False Positive alarm for legitimate use of bandwidth if the baselines are not intelligently configured.|$|R
40|$|Stanford 2 ̆ 7 s Space Systems Development Laboratory (SSDL) has {{initiated}} a new space system technology initiative {{in order to}} develop, demonstrate, and validate a beacon monitoring system for spacecraft. This system consists of automated fault detection on board a spacecraft, a state of health beacon signal broadcast by the spacecraft, a ground <b>based</b> <b>monitoring</b> <b>network,</b> and a mission control center capable of efficiently integrating this health assessment strategy into its operating architecture. SSDL is investigating this technique by identifying fundamental design drivers, developing a system responsive to these drivers, and deploying the resulting system on micro spacecraft and within SSDL 2 ̆ 7 s developing, global, automated space operations network. This paper reviews the beacon monitoring concept, describes the design criteria for such an operations strategy, and presents the current development of the SSDL beacon system...|$|R
40|$|The {{thesis is}} focused on {{automated}} development of network attack detectors. It describes a design of patterns developed for normal and offensive behaviors <b>based</b> on <b>monitoring</b> <b>network</b> traffic of selected services. Patterns are represented by statistics {{with a focus on}} suitable metrics. Using machine learning algorithms attack detectors are created from behavioral patterns. Finally, a module was implemented for Nemea system in C/C++ programming language based on the proposal...|$|R
40|$|Noise control starts by {{understanding}} the actual noise situation. Especially for {{situations where the}} distance between industrial and traffic noise sources and a local community is {{in the order of}} one kilometer or more, it may not be clear what sources are the main contributors to annoyance. Then a combination of monitoring and modeling is desired to get the required insight. This is demonstrated for a local community where strongly timevarying noise was perceived from both industry and traffic. With the use of data from a nearby meteorological mast the time-varying sound propagation was calculated and the actual traffic flow was used on an hourly basis. The calculated results matched with the occurrence of complaints. Furthermore, it was shown that for some periods of time the more nearby industry contributes in the same amount as the more distant traffic. A standard model predicts much lower traffic noise levels, using average traffic flows and a standard meteorology. For a second project a large scale model <b>based</b> <b>monitoring</b> <b>network</b> for traffic noise is briefly highlighted. It was aimed to localize and quantify traffic noise sources, {{but it can also be}} used in combination with industrial noise sources...|$|R
40|$|Carbon dioxide (CO 2) storage into {{geological}} formations {{is regarded}} as an important mitigation strategy for anthropogenic CO 2 emissions to the atmosphere. This study first simulates the leakage of CO 2 and brine from a storage reservoir through the caprock. Then, we estimate the resulting pressure changes at the zone overlying the caprock also known as Above Zone Monitoring Interval (AZMI). A data-driven approach of arbitrary Polynomial Chaos (aPC) Expansion is then used to quantify the uncertainty in the above zone pressure prediction based on the uncertainties in different geologic parameters. Finally, a global sensitivity analysis is performed with Sobol indices based on the aPC technique to determine {{the relative importance of}} different parameters on pressure prediction. The results indicate that there can be uncertainty in pressure prediction locally around the leakage zones. The degree of such uncertainty in prediction depends on the quality of site specific information available for analysis. The scientific results from this study provide substantial insight {{that there is a need}} for site-specific data for efficient predictions of risks associated with storage activities. The presented approach can provide a basis of optimized pressure <b>based</b> <b>monitoring</b> <b>network</b> design at carbon storage sites...|$|R
40|$|Abstract:- With {{advances}} in sensor technology {{and availability of}} low cost integrated circuits, a wireless <b>monitoring</b> sensor <b>network</b> has been {{considered to be the}} new generation technology for structural health monitoring. Wireless Intelligent Sensor and Actuator Network (WISAN) has hence been developed as a vibration <b>based</b> structural <b>monitoring</b> <b>network</b> that allows extraction of mode shapes from output-only vibration data from a structure. The mode shape information can further be used in modal methods of damage detection. This network has been tested on a pre-stressed concrete bridge in Kuala Lumpur, Malaysia. The results have been compared with a similar sized steel girder bridge. Key-Words:- wirelesss, intelligent sensor, actuator <b>network,</b> bridge <b>monitoring,</b> damage detection...|$|R
40|$|This {{bachelor}} thesis {{deals with}} an IP flow <b>based</b> data <b>network</b> <b>monitoring</b> system. It presents {{the architecture of}} the NetFlow <b>based</b> <b>monitoring,</b> explains the basic terms, the NetFlow protocol and its alternatives. Further, weak spots of the monitoring systems are determined and a conceptual solution is proposed. This solution is implemented and described in detail. Finally, testing methods and results are discussed and the possibilities of further development and optimization are proposed...|$|R
40|$|The {{purpose of}} INL’s {{research}} on this project is to demonstrate the feasibility of a host event <b>based</b> <b>network</b> <b>monitoring</b> tool and the effects on host performance. Current host <b>based</b> <b>network</b> <b>monitoring</b> tools work on polling which can miss activity if it occurs between polls. Instead of polling, a tool could be developed that makes use of event APIs in the operating system to receive asynchronous notifications of network activity. Analysis and logging of these events will allow the tool to construct the complete real-time and historical network configuration of the host while the tool is running. This research focused on three major operating systems commonly used by SCADA systems: Linux, WindowsXP, and Windows 7. Windows 7 offers two paths that have minimal impact on the system and should be seriously considered. First is the new Windows Event Logging API, and, second, Windows 7 offers the ALE API within WFP. Any future work should focus on these methods...|$|R
40|$|Web {{services}} {{is one of}} the emerging approaches in network management. This paper describes the design and implementation of four Web services <b>based</b> <b>network</b> <b>monitoring</b> prototypes. Each prototype follows a speciﬁc approach to retrieve management data, ranging from retrieving a single management data object, to retrieving an entire table of such objects at once. We have focused on the interfaces table (ifTable), as described in the IF-MIB. ...|$|R
40|$|The {{complexity}} of the Internet has rapidly increased, making it more important and challenging to design scalable <b>network</b> <b>monitoring</b> tools. <b>Network</b> <b>monitoring</b> typically requires rolling data analysis, i. e., continuously and incrementally updating (rolling-over) various reports and statistics over highvolume data streams. In this paper, we describe DBStream, which is an SQL-based system that explicitly supports incremental queries for rolling data analysis. We also present {{a performance comparison of}} DBStream with a parallel data processing engine (Spark), showing that, in some scenarios, a single DBStream node can outperform a cluster of ten Spark nodes on rolling <b>network</b> <b>monitoring</b> workloads. Although our performance evaluation is <b>based</b> on <b>network</b> <b>monitoring</b> data, our results can be generalized to other Big Data problems with high volume and velocit...|$|R
40|$|Objectives: The {{evolution}} of the Western Mediterranean Sea and the Gulf of Cadiz is inherently governed by (i) plate convergence between Nubia (Africa) / Eurasia and (ii) subduction related slab-roll back. Both processes {{are responsible for the}} surface features / topography of the Alboran Sea / Rif / Betic domain and deep-seated features related to the consumption of African lithosphere. The cruise is part of the ESF-EUROCORES programme TOPO- EUROPE (Project TOPO-MED) and is aiming to study the interrelation between convergence and major tectonic fault zones in the Alboran Sea (Trans-Alboran-Shear-Zone – the Alboran Ridge) and in the Gulf of Cadiz and Miocene subduction, causing deep-seated seismicity (40 - 150 km depth) under the western Alboran basin. Furthermore, active tectonic features and fault zones will mimic the plate boundary configuration between Europe/Iberia and Nubia/Morocco. <b>Monitoring</b> <b>networks</b> with ocean bottom seismometers (OBS) were installed in the Alboran Sea and in the Gulf of Cadiz, recording local and regional earthquakes. Two deployment periods of approx. 6 month (in total one year) were conducted. In August of 2009 30 OBS were deployed during the RV Poseidon cruise P 389. Instruments were recovered now during the Poseidon cruise P 393 and have been re-deployed in the Gulf of Cadiz (Fig. 1). The recovery of OBS deployed in the Gulf of Cadiz stations will be the main aim of the RV Maria S. Merian cruise MSMS 15 / 5, scheduled for July of 2010. During the deployment in the Alboran Sea the ocean bottom seismic instruments recorded a wealth of local earthquakes (see Fig. 1). The distribution of seismicity is going to outline tectonically active features and faults. In addition, data will be used for tomographic inversion, providing seismic constraints on the structure of crust and mantle in the Gibraltar arc / Gulf of Cadiz area and the Alboran domain. Furthermore, land <b>based</b> <b>monitoring</b> <b>networks</b> operated {{during the time of the}} marine deployments will provide a regional coverage of the entire area between Morocco and Spain, including the northern Moroccan continent and southern Spain...|$|R
40|$|As company intranets {{continue}} to grow it is increasingly important that network administrators are aware of and have {{a handle on the}} different types of traffic that is traversing their <b>networks.</b> Traffic <b>monitoring</b> and analysis is essential in order to more effectively troubleshoot and resolve issues when they occur, so as to not bring network services to a stand still for extended periods of time. Numerous tools are available to help administrators with the monitoring and analysis of network traffic. This paper discusses router <b>based</b> <b>monitoring</b> techniques and non-router <b>based</b> <b>monitoring</b> techniques (passive versus active). It gives an overview of the three most widely used router <b>based</b> <b>network</b> <b>monitoring</b> tools available (SNMP, RMON, and Cisco Netflow), and provides information about two newer monitoring methods that use a combination of passive and active monitoring technique...|$|R
5000|$|Originally {{written by}} Vern Paxson, [...] Bro {{is an open}} source Unix <b>based</b> <b>network</b> <b>monitoring</b> framework. Often {{compared}} to a network intrusion detection system (NIDS), Bro {{can be used to}} build a NIDS but is much more. Bro can also be used for collecting network measurements, conducting forensic investigations, traffic baselining and more. Bro has been compared to tcpdump, Snort, netflow, and Perl (or any other scripting language) all in one. It is released under the BSD license.|$|R
40|$|Abstract—Through the {{introduction}} of Self-Organizing Networks (SON) into mobile networks, a potentially large number of SON functions are available. These SON functions automatically perfom management actions. There will be SON functions for many aspects of fault, configuration, accounting, performance, and security (FCAPS) management. The functions are executed <b>based</b> on <b>monitored</b> <b>network</b> behavior, which may lead to several functions being active concurrently in the same network area. Simultaneous execution of different functions with contradicting goals may lead to oscillating function execution and service degradation in the worst case. Therefore, SON function coordination is indispensable for SON-enabled networks in order to align the executed SON funtions and thus assure that they take full effect (improved performance and fault handling). Coordination mechanisms {{need to be developed}} and verified before they are deployed into the network. For this purpose, an experimental system realizes SON function coordination based on flexible policybased decisions. Coverage and Capacity Optimization (CCO) is presented as use case to demonstrate succesful coordination of multiple independent SON functions. I...|$|R
40|$|Abstract. While {{conventional}} wisdom holds that residential users experience {{a high degree}} of compromise and infection, this presumption has seen little validation in the way of an in-depth study. In this paper we present a first step towards an assessment <b>based</b> on <b>monitoring</b> <b>network</b> activity (anonymized for user privacy) of 20, 000 residential DSL customers in a European urban area, roughly 1, 000 users of a community network in rural India, and several thousand dormitory users at a large US university. Our study focuses on security issues that overtly manifest in such data sets, such as scanning, spamming, payload signatures, and contact to botnet rendezvous points. We analyze the relationship between overt manifestations of such activity versus the “security hygiene ” of the user populations (anti-virus and OS software updates) and potential risky behavior (accessing blacklisted URLs). We find that hygiene has little correlation with observed behavior, but risky behavior—which is quite prevalent—more than doubles the likelihood that a system will manifest security issues. ...|$|R
40|$|Abstract. The paper {{presents}} {{our approach}} {{of using a}} formal verification method, the model checking, to verify whether a particular component of hardware design matches its specification. We have applied this approach in the Liberouter project, which is aimed to develop an FPGA <b>based</b> high-speed <b>network</b> <b>monitoring</b> and routing hardware. In the paper, we focus on a FIFO component – the process of its verification, detected errors, and the way of their correction. ...|$|R
40|$|VoIP {{applications}} are emerging today {{as an important}} component in business and communication industry. In this paper, we address the intrusion detection and prevention in VoIP networks and describe how a conceptual solution based on the Bayes inference approach {{can be used to}} reinforce the existent security mechanisms. Our approach is <b>based</b> on <b>network</b> <b>monitoring</b> and analyzing of the VoIP-specific traffic. We give a detailed example on attack detection using the SIP signaling protocol. 1...|$|R
40|$|A key to {{predicting}} {{climate change}} is to observe an understand the global distribution of clouds and their physical properties such as optical thickness and droplet size. Since clouds change rapidly over short time and space intervals, they are difficult to simulate in computer models. But {{it is essential that}} global climate models predict realistic spatial and temporal distribution of cloud optical depth. The best way to verify these distributions is to infer optical depth from global coverage satellite data. However, satellite methods have many sources of uncertainty; thus, independent and reliable ground-based estimates are essential for validation. For aerosol, there is the AERONET - a ground <b>based</b> <b>monitoring</b> <b>network</b> that consists of identical multi-channel radiometers for assessing aerosol optical properties and validating their satellite retrievals. In addition to AEROSOL, we want the <b>network</b> <b>monitoring</b> CLOUD optical properties. It will use AERONET "time" (inappropriate for aerosol studies) to make basic new measurements related to cloud physics. In the presentation we will report on a new technique that retrieves cloud optical thickness for even broken clouds above green vegetation from surface measurements of zenith radiance in the visible (VIS) and near-IR (NIR) spectral regions. The idea of the method is simple: since green vegetation reflects 40 - 50 % of incoming radiation in the NIR and only 5 - 10 % in the VIS region, ground measurements under thin clouds have little spectral contrast between VIS and NIR, while thick clouds reflect much more of the surface-reflected radiation in the NIR than in VIS. Based on this idea, we use a combination of measurements (spectral indices) in VIS and NIR to estimate cloud optical thickness. By analogy with NDVI, the simplest index that can be defined is the Normalized Difference Cloud Index (NDCI) which is a ratio between the difference and the sum of two radiances measured for two narrow spectral bands in VIS and NIR...|$|R
40|$|This {{dissertation}} develops techniques, <b>based</b> on <b>monitoring</b> <b>network</b> traffic, that automate signature generation for wide-spreading malicious payloads such as Internet worms. Fast signature detection {{is required}} to achieve effective content-based filtering. The main thesis is that content prevalence analysis in network payloads across distributed networks is a good basis for automated signature generation for wide-spreading malicious payloads, and can be performed without compromising the privacy of participating networks. Content-prevalence analysis extracts unique payload patterns that are identical and invariant over all the flows that convey a wide-spreading malicious payload. Distributed monitoring enables us to rapidly capture many sample payloads, thus expediting the signature generation. Extra care for privacy encourages more networks {{to participate in the}} distributed monitoring and makes the approach practical. The first part of this dissertation presents a system, Autograph, that generates network payload signatures for Internet worms by utilizing the content invariance and wide-spreading communication patterns of Internet worm traffic. Signature generation speed is improved further by extending Autograph to share port scanner lists with distributed Autograph monitors...|$|R
40|$|The {{demand of}} highly {{flexible}} {{and easy to}} deploy <b>network</b> <b>monitoring</b> systems has pushed companies toward software <b>based</b> <b>network</b> <b>monitoring</b> probes implemented with commodity hardware rather than with expensive and highly specialized network devices. Deploying software probes under virtual machines executed on the same physical box is attractive for reducing deployment costs and for simplifying the management of advanced <b>network</b> <b>monitoring</b> architectures built on top of heterogeneous monitoring tools (i. e. Intrusion Detection Systems and Performance Monitoring Systems). Unfortunately, software probes are usually not {{able to meet the}} performance requirements when deployed in virtualized environments as virtualization introduces severe performance bottlenecks when performing packet capture, which is the core activity of passive <b>network</b> <b>monitoring</b> systems. This paper covers the design and implementation of vPF_RING, a novel framework for efficiently capturing packets on virtual machines running on commodity hardware. This solution allows network administrators to exploit the benefits of virtualization such as reduced costs and centralized administration, while preserving the ability to capture packets at wire speed even when deploying applications in virtual machines. The validation process has demonstrated that this solution can be profitably used for multi-gigabit <b>network</b> <b>monitoring,</b> paving the way to low-cost virtualized monitoring systems...|$|R
40|$|The aim of {{this network}} is to study the {{functioning}} of organic suckler cattle farms, their technical and economic results. They are compared to conventional farms to underline there specificities and the keys of the system. As the context on organic farming is evolving a lot (market, techniques, structures of farms…), they are long term studies. The study is <b>based</b> on a <b>monitoring</b> <b>network</b> of 7 organic farms and 79 conventional each year. Economic and technical data are recorded on the farms twice a year and analyzed according to the methodology of our unit. This methodology {{is based on the}} relationship between functioning, technical data and economic results. The observation over 3 years (2000 - 2003) showed that the technical results from the herd management (herd productivity criteria) are identical to those of conventional systems. The ban on chemical fertilisers entails a drop in pasture yield of 20...|$|R
40|$|In {{cognitive}} radio networks, {{the accuracy of}} the spectrum sensing is vital to protect the primary network and is often a function of channel sensing duration. The choice of the sensing duration, on the other hand, directly affects the achievable throughput of the secondary system. In this paper, we propose a spectrum sensing method <b>based</b> on cognitive <b>monitoring</b> <b>network</b> (CMoN) which is a network of sensors deployed in the network coverage area which carries out collaborative spectrum sensing. Consequently, the achievable throughput can be maximized irrespective of the sensing duration. In this technique, the secondary users {{do not need to be}} equipped with spectrum sensors and the availability of the spectrum is assessed from the CMoN through a signaling protocol. Here we further propose a two-tier decision fusion mechanism at the base station and show that through the proposed method a significant improvement in the network throughput is achieved. Numerical results also confirm that the proposed method outperforms the conventional spectrum sensing in terms of achievable throughput...|$|R
40|$|Monitoring of the {{atmospheric}} {{composition of the}} Earth is essential for studying the processes occurring in different layers of the atmosphere and, consequently, for air quality control and the climate change prediction. The most important conclusions {{are based on the}} data from the long-term global observations. Nowadays this data is coming from both space- or airborne instruments and ground <b>based</b> <b>monitoring</b> <b>networks.</b> The ozone (O 3) and the methane (CH 4) are among the most important trace gases. They play crucial role in the physical and chemical processes in the atmosphere, like formation of the protective stratospheric ozone layer or contribution to the greenhouse effect by absorption of the solar radiation (by both ozone and methane) or radiation re-emitted by the Earthâ s surface. Another important factor is that both ozone and methane concentration and distribution changes serve as the important markers of the anthropogenic influence on the environment. The atmospheric composition is derived after processing the spectra acquired by the satellite-borne instruments. Most methods utilize the fitting of the observed and synthetic spectra. Modelled spectra are produced based on the preliminary assumptions of approximate concentrations, temperatures and altitude distribution of the trace gases. These methods require a precise knowledge of the electromagnetic radiation absorption characteristics of the different atmospheric gases in a wide spectral range, as well as the temperature and pressure dependences of these parameters. The uncertainties in these tabulated spectral data lead to the uncertainties in the resulting retrieved concentrations and distribution profiles of atmospheric gases. Most of the modern instruments operate in the ultraviolet, visible and infrared parts of the spectrum between 250 nm and 1000 nm for ozone observations. For methane detection, infrared regions of the spectrum around 1, 6 or 2, 4 micron are mostly used. Despite the fact that different research groups have been analysing the absorption spectra of both ozone and methane with a lot of scrutiny for decades, there still is a room for improvement {{of the quality of the}} data. Consequently, the new detailed spectroscopic data would allow to further increase the quality of {{the atmospheric}} observations. Demand for an updated and improved (in terms of uncertainties and parameterization capabilities) spectroscopic data for ozone and methane from the remote sensing community was a major motivating factor for this study. This work is dedicated to the acquisition of the new high-quality broadband absorption spectra of ozone and methane, corresponding data analysis and the methods of achieving these goals...|$|R
40|$|Abstract — This paper {{compares the}} {{performance}} of Web services <b>based</b> <b>network</b> <b>monitoring</b> to traditional, SNMP <b>based,</b> <b>monitoring.</b> The study focuses on the ifTable, and investigates performance as function {{of the number of}} retrieved objects. The following aspects are examined: bandwidth usage, CPU time, memory consumption and round trip delay. For our study several prototypes of Web services based agents were implemented; these prototypes can retrieve singleifTable elements,ifTable rows, ifTable columns or the entire ifTable. This paper presents a generic formula to calculate SNMP’s bandwidth requirements; the bandwidth consumption of our prototypes was compared to that formula. The CPU time, memory consumption and round trip delay of our prototypes was compared to Net-SNMP, as well as several other SNMP agents. Our measurements show that SNMP is more efficient in cases where only a single object is retrieved; for larger number of objects Web services may be more efficient. Our study also shows that, if performance is the issue, the choice between BER (SNMP) or XML (Web services) encoding is generally not the determining factor; other choices can have stronger impact on performance...|$|R
30|$|Each {{component}} of <b>monitoring</b> <b>network</b> {{needs to be}} improved for the overall improvement of <b>monitoring</b> <b>network</b> in the study area. Depending upon groundwater and monitoring status, the improvement objectives could account for enhancement in understanding of <b>monitoring</b> <b>network,</b> legal requirements, or socio-economic aspect of monitoring (Thakur et al. 2011 a). The <b>monitoring</b> strategy is <b>based</b> on groundwater <b>monitoring</b> effort, reduction of uncertainties, socioeconomic needs, legal requirements, etc. With the established outcomes from this study, the <b>monitoring</b> <b>network</b> can be substantially optimized and the existing strategies for monitoring can be modified {{in the view of}} multiple purpose of overcoming the present difficulty in maintaining immense number of monitoring wells. Similarly, since the research also portrays the monitoring needs for the unmonitored sites, the outcome can be used to develop improved <b>monitoring</b> <b>network</b> strategies with the incorporation of such sites for effective strategies.|$|R
40|$|Abstract — With the {{emergence}} of gigabit per second and higher bandwidth networks, software based packet capturing faced severe performance challenges in two areas: lossless packet acquisition at high arrival rate and high precision timestamping. Many research projects proposed hardware <b>based</b> <b>network</b> <b>monitoring</b> solutions in order to eliminate these performance bottlenecks. In contrast, the common microsecond resolution software based packet processing has not been enhanced to meet the measurement requirements of high performance networks. In a previous paper, we already proposed an alternative packet capturing solution {{that is based on}} the libpcap library and supports 10 - 9 second resolution timestamping. We are now evaluating the performance of the proposed solution in practice. Experimental evidence shows that our approach represent...|$|R
40|$|We present here a {{framework}} {{together with a}} set of paradigms for mobile agent <b>based</b> active <b>monitoring</b> of <b>network</b> systems. In our framework mobile agents are used to perform remote information filtering and control functions. Such agents can detect basic events or correlate existing events that are stored in a database to enforce system policies. A system administrator can securely modify the monitoring policies and information filtering functions of its agents, or install new agents at a node. The framework presented here includes monitor, subscriber, auditor and inspector agents. The policies and itineraries of these agents can be modified dynamically. In response to certain trigger events agents may change their itineraries to correlate event data. We present here a set of experiments that we have conducted using the Ajanta mobile agent system to evaluate and demonstrate the capabilities of our mobile agent framework...|$|R
40|$|In this paper, we {{describe}} a simple, yet effective method to detect bot-infected machines {{within a given}} network that relies on detection of the communication channel between bot and Command & Control server (C&C server). The presented techniques are mainly <b>based</b> on passively <b>monitoring</b> <b>network</b> traffic for unusual or suspicious IRC nicknames, IRC servers, and uncommon server ports. By using n-gram analysis and a scoring system, {{we are able to}} detect bots that use uncommon communication channels, which are commonly not detected by classical intrusion detection systems. Upon detection, it is possible to determine the IP address of the C&C server, as well as, the channels a bot joined and the additional parameters which were set. The software Rishi implements the mentioned features and is able to automatically generate warning emails to report infected machines to an administrator. Within the 10 GBit network of RWTH Aachen university, we detected 82 botinfected machines within two weeks, some of them using communication channels not picked up by other intrusion detection systems. ...|$|R
30|$|The {{encryption}} {{of network}} traffic complicates legitimate <b>network</b> <b>monitoring,</b> traffic analysis, and network forensics. In this paper, we present real-time lightweight identification of HTTPS clients <b>based</b> on <b>network</b> <b>monitoring</b> and SSL/TLS fingerprinting. Our experiment {{shows that it}} is possible to estimate the User-Agent of a client in HTTPS communication via the analysis of the SSL/TLS handshake. The fingerprints of SSL/TLS handshakes, including a list of supported cipher suites, differ among clients and correlate to User-Agent values from a HTTP header. We built up a dictionary of SSL/TLS cipher suite lists and HTTP User-Agents and assigned the User-Agents to the observed SSL/TLS connections to identify communicating clients. The dictionary was used to classify live HTTPS network traffic. We were able to retrieve client types from 95.4 % of HTTPS network traffic. Further, we discussed host-based and network-based methods of dictionary retrieval and estimated the quality of the data.|$|R
40|$|This thesis {{deals with}} design and {{implementation}} of a flow <b>based</b> <b>monitoring</b> probe. The monitoring task performed by the probe is divided into hardware layer, which is capable of measurement at high packet rates, and software layer, which provides large memory for flow storage. Analysis done in the work shows that this concept offers many advantages when compared to software <b>based</b> flow <b>monitoring</b> applications. The probe {{is designed to be}} used with a hardware accelerator card and offers high flexibility and performance by a way of user defined monitoring process. The designed system has been implemented and thoroughly tested and is ready for deployment for tasks such as  operational <b>monitoring,</b> <b>network</b> traffic classification, anomalies and attacks detection and many others...|$|R
40|$|Abstract—This paper {{compares the}} {{performance}} of Web services <b>based</b> <b>network</b> <b>monitoring</b> to traditional, SNMP <b>based,</b> <b>monitoring.</b> The study focuses on the ifTable, and investigates performance as function {{of the number of}} retrieved objects. The following aspects are examined: bandwidth usage, CPU time, memory consumption and round trip delay. For our study several prototypes of Web services based agents were implemented; these prototypes can retrieve single ifTable elements, ifTable rows, ifTable columns or the entire ifTable. This paper presents a generic formula to calculate SNMP’s bandwidth requirements; the bandwidth consumption of our prototypes was compared to that formula. The CPU time, memory consumption and round trip delay of our prototypes was compared to Net-SNMP, as well as several other SNMP agents. Our measurements show that SNMP is more efficient in cases where only a single object is retrieved; for larger number of objects Web services may be more efficient. Our study also shows that, if performance is the issue, the choice between BER (SNMP) or XML (Web services) encoding is generally not the determining factor; other choices can have stronger impact on performance. Index Terms—SNMP, Web services, performance, bandwidth usage, CPU time, memory consumption, round trip delay, BER, XML, compression, ifTable. I...|$|R
40|$|Wireless {{networks}} {{based on}} IEEE 802. 11 are becoming integral parts of any enterprise network. The inherent openness of these networks makes them {{a target for}} attackers. The coverage of wireless networks cannot be confined by walls or obstacles. The task of an enterprise network administrator is thus compounded {{by the introduction of}} wireless technology. Most of the attacks on wireless networks are due to vulnerabilities in the Medium Access Control (MAC) Layer. This fact drives the need for a MAC layer <b>network</b> <b>monitoring</b> system. As part of the Konark project we have developed a mobile-agent <b>based</b> <b>network</b> <b>monitoring</b> system for the wired network. This system facilitates centralized viewing of network alerts through cooperating agents. The main contribution of this project is the development and deployment of an analysis and attack detection tool for 802. 11 wireless networks. Events generated by this tool are correlated using Konark monitoring agents and the administrator is alerted. We focus on detection of MAC address spoofing, Denial of Service attacks and network misconfigurations. We also provide services to users and applications. This report describes the different modes in which <b>network</b> <b>monitoring</b> could be done in an enterprise network using such a tool. The trade-offs involved with each mode of operation is described too. ...|$|R
40|$|Time cheats {{represent}} some of {{the most}} crucial issues in online gaming. Since they act on timing properties of generated game events, these malicious schemes are particularly difficult to thwart when distributed games are deployed over peer-to-peer architectures. Indeed, the absence of a global clock shared among peers enables cheaters to see into the future by waiting for events generated by other peers before generating its own ones (lookahead cheat). This may give an unfair advantage to the cheater. We consider a version of lookahead cheat generalized in the context of real-time (i. e., not round-based) games. To face this time cheat, we present AC/DC, an Algorithm for Cheating Detection by Cheating. This algorithm enables to detect cheaters <b>based</b> on <b>monitoring</b> of <b>network</b> latencies. The basic idea is that of conducting against each suspected peer a sort of cheating counterattack, by delaying events before notifying them to the (hypothetic) cheater. This permits to detect whether that peer waits for these events before generating its own ones. Our claim is that an approach <b>based</b> on the <b>monitoring</b> of communication patterns among peers allows cheat detection without affecting the performances of the game...|$|R
