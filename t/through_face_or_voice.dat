0|10000|Public
40|$|In {{this short}} paper I {{illustrate}} {{by a few}} selected examples several compelling similarities in the functional organization of face and voice cerebral processing: (1) Presence of cortical areas selective to <b>face</b> <b>or</b> <b>voice</b> stimuli, also observed in non-human primates, and causally related to perception; (2) Coding of <b>face</b> <b>or</b> <b>voice</b> identity using a “norm-based” scheme; (3) Personality inferences from faces and voices in a same Trustworthiness–Dominance “social space”...|$|R
50|$|Research {{has been}} {{conducted}} to see if <b>faces</b> <b>or</b> <b>voices</b> {{make it easier to}} identify individuals and recall semantic memory and episodic memory. These experiments look at all three stages of face processing. The experiment method was to show two groups celebrity and familiar <b>faces</b> <b>or</b> <b>voices</b> with a between-group design and ask the participants to recall information about them. The participants are first asked if the stimulus is familiar. If they answer yes then they are asked to information (semantic memory) and memories they have of the person (episodic memory) that fits the <b>face</b> <b>or</b> <b>voice</b> presented. These experiments all demonstrate the strong phenomenon of the face advantage and how it persists through different follow-up studies with different experimental controls and variables.|$|R
50|$|Rocky and Bullwinkle {{never see}} through Boris's thin disguises, though Rocky often remarks that his <b>face</b> <b>or</b> <b>voice</b> seems somehow familiar.|$|R
6000|$|Emerick. Under the {{imminent}} {{risk of death}} she lies, [...] Or irrecoverable loss of reason, [...] 275 [...] If known friend's <b>face</b> <b>or</b> <b>voice</b> renew the frenzy.|$|R
6000|$|... 'And wherefore, damsel? {{tell me all}} ye know. [...] You cannot scare me; nor rough <b>face,</b> <b>or</b> <b>voice,</b> [...] Brute bulk of limb, or {{boundless}} savagery [...] Appal me {{from the}} quest.' ...|$|R
50|$|The short, {{with the}} {{exception}} of the narrator and some background extras, featured no <b>face</b> <b>or</b> <b>voice</b> actors, only suit actors, and only used recycled soundbytes for both the voices of Ultraman and Kamen Rider 1.|$|R
60|$|The loving {{mother can}} {{scarcely}} {{be described as}} resuming her insipid and affected air when she made this exclamation; for she had never cast it off; nor was it likely that she ever would or could, in any other place than in the grave. But hurriedly dismissing whatever shadow of earnestness, or faint confession of a purpose, laudable or wicked, that her <b>face,</b> <b>or</b> <b>voice,</b> <b>or</b> manner: had, for the moment, betrayed, she lounged upon the couch, her most insipid and most languid self again, as Edith entered the room.|$|R
2500|$|The former Conservative Prime Minister Harold Macmillan once remarked of Tebbit: [...] "Heard a chap on {{the radio}} this morning talking with a cockney accent. They tell me {{he is one of}} Her Majesty's ministers". Peter Dorey of Cardiff University wrote that [...] "it was Norman Tebbit... who was perhaps the public <b>face</b> <b>or</b> <b>voice</b> of Essex Man, and {{articulated}} his views and prejudices".|$|R
5000|$|The Conjurer (voiced by Kenan Thompson) - Impresario's evil counterpart, who {{can also}} conjure up {{anything}} he imagines, but his conjurings are red and don't have his mom's <b>face</b> <b>or</b> <b>voice,</b> unlike the real Impresario. He wears a purple helmet with a purple visor, purple gloves, purple boots, a purple singlet, and a purple cape. Like Impresario, he wears the feather and gem on his helmet.|$|R
40|$|Exposure, <b>or</b> adaptation, to <b>faces</b> <b>or</b> <b>voices</b> biases {{perceptions}} of subsequent stimuli, for example, causing faces to appear more normal {{than they would}} be otherwise if they are similar to the previously presented stimuli. Studies also suggest that there may be cross-modal adaptation between sound and vision, although the evidence is inconsistent. We examined adaptation effects within and across voices and faces and also tested whether adaptation crosses between male and female stimuli. We exposed participants to sex-typical or sex-atypical stimuli and measured the perceived normality of subsequent stimuli. Exposure to female <b>faces</b> <b>or</b> <b>voices</b> altered {{perceptions of}} subsequent female stimuli, and these adaptation effects crossed modality; exposure to voices influenced judgments of faces, and vice versa. We also found that exposure to female stimuli did not influence perception of subsequent male stimuli. Our data demonstrate that recent experience of faces and voices changes subsequent perception and that mental representations of faces and voices may not be modality dependent. Both unimodal and cross-modal adaptation effects appear to be relatively sex-specific...|$|R
60|$|At this, apparently, {{recollection}} of Adam's <b>face</b> <b>or</b> <b>voice</b> {{returned to the}} showman. He remained silent while with palsied fingers Adam unlatched my bolts and bars. Bent almost double and half-stifled, I sat there in sight, my clothes spread brightly out about me. The cool air swirled in, {{and for a while}} my eyes dazzled at the bubbling blaze of a naphtha lamp suspended from the pole of the tent above the criss-cross green-bladed grass at my feet. I lifted my head.|$|R
6000|$|... "I {{suppose it}} would," [...] said Bell, {{answering}} him without {{a sign of}} feeling in her <b>face</b> <b>or</b> <b>voice.</b> But she took in every word that he spoke, and disputed their truth inwardly with all the strength of her heart and mind, and with the very vehemence of her soul. [...] "As if a woman cannot bear more than a man!" [...] she said to herself, as she walked {{the length of the}} room alone, when she had got herself free from the doctor's arm.|$|R
2500|$|The Face Advantage allows {{information}} and memories to be recalled easier through {{the presentation of}} a person's face rather than a person's voice. Faces and voices are very similar stimuli that reveal similar {{information and}} result in similar processes of memory recall. During face perception, there are three stages of memory recall that include recognition, followed by the remembering of semantic memory and episodic memory, and finally name recall. The Face Advantage is shown through an experiment where participants are presented with faces and voices of unfamiliar faces and recognizable celebrity faces. [...] The stimuli are presented with a between-group design. The participants are asked to say if the <b>face</b> <b>or</b> <b>voice</b> is familiar. If the answer is yes, {{they are asked to}} recall semantic and episodic memories and finally the name of the <b>face</b> <b>or</b> <b>voice.</b> It was much easier for those presented with a celebrity's face to recall information than for those presented with a voice. The results show that in the second stage of face perception when memories are recalled, information is recalled faster and more accurate after a face is perceived, and slower, less accurate and with less detail after a voice is perceived. [...] A possible explanation is that the connections between face representations and semantic and episodic memory are stronger than that of voices.|$|R
40|$|Both {{faces and}} voices {{are rich in}} socially-relevant information, which humans are remarkably adept at extracting, {{including}} a person's identity, age, gender, affective state, personality, etc. Here, we review accumulating evidence from behavioral, neuropsychological, electrophysiological, and neuroimaging studies which suggest that the cognitive and neural processing mechanisms engaged by perceiving <b>faces</b> <b>or</b> <b>voices</b> are highly similar, despite the very different nature of their sensory input. The similarity between the two mechanisms likely facilitates the multi-modal integration of facial and vocal information during everyday social interactions. These findings emphasize a parsimonious principle of cerebral organization, where similar computational problems in different modalities are solved using similar solutions...|$|R
40|$|Abstract- Biometrics {{is one of}} {{the recent}} trends in security, which is mainly used for {{verification}} and recognition systems. By using biometrics we confirm a particular person’s claimed identity based on particular person’s physiological or behavioral characteristics such as fingerprint, <b>face</b> <b>or</b> <b>voice</b> etc. Most biometric systems deployed in real-world applications are unimodal, such as they use a single source of information for authentication (e. g., single fingerprint, face, voice etc.). Some of the limitations imposed by unimodal biometric systems can be overcome by including multiple sources of information for establishing identity. In this paper, it is shown that fingerprint and face recognition can form a good combination for a multimodal biometric system...|$|R
40|$|We {{present a}} system to {{retrieve}} all clips from a meeting archive that show a particular individual speaking, using a single <b>face</b> <b>or</b> <b>voice</b> sample as the query. The system incorporates three novel ideas. One, rather than match the query to each individual sample in the archive, samples within a meeting are grouped first, generating a cluster of samples per individual. The query is then matched to the cluster, taking advantage of multiple samples to yield a robust decision. Two, automatic audio-visual association is performed which allows a bi-modal retrieval of clips, even when the query is uni-modal. Three, the biometric recognition uses individual-specific score distributions learnt from the clusters, in a likelihood ratio based decision framework that obviates the need for explicit normalization or modalit...|$|R
40|$|Recognizing {{familiar}} {{individuals is}} achieved by the brain by combining cues from several sensory modalities, including {{the face of a}} person and her voice. Here we used functional magnetic resonance (fMRI) and a whole-brain, searchlight multi-voxel pattern analysis (MVPA) to search for areas in which local fMRI patterns could result in identity classification as a function of sensory modality. We found several areas supporting <b>face</b> <b>or</b> <b>voice</b> stimulus classification based on fMRI responses, consistent with previous reports; the classification maps overlapped across modalities in a single area of right posterior superior temporal sulcus (pSTS). Remarkably, we also found several cortical areas, mostly located along the middle temporal gyrus, in which local fMRI patterns resulted in identity “cross-classification”: vocal identity could be classified based on fMRI responses to the <b>faces,</b> <b>or</b> the reverse, or both. These findings are suggestive of a series of cortical identity representations increasingly abstracted from the input modality...|$|R
40|$|Previous {{studies have}} shown that the {{perception}} of facial and vocal affective expressions interacts with each other. Facial expressions usually dominate vocal expressions when we perceive the emotions of face-voice stimuli. In most of these studies, participants were instructed to pay attention to the <b>face</b> <b>or</b> <b>voice.</b> Few studies compared the perceived emotions with and without specific instructions regarding the modality to which attention should be directed. Also, these studies used combinations of the face and voice which expresses two opposing emotions, which limits the generalizability of the findings. The purpose of this study is to examine whether the emotion perception is modulated by instructions to pay attention to the <b>face</b> <b>or</b> <b>voice</b> using the six basic emotions. Also we examine the modality dominance between the face and voice for each emotion category. Before the experiment, we recorded faces and voices which expresses the six basic emotions and orthogonally combined these faces and voices. Consequently, the emotional valence of visual and auditory information was either congruent or incongruent. In the experiment, there were unisensory and multisensory sessions. The multisensory session was divided into three blocks according to whether an instruction was given to pay attention to a given modality (face attention, voice attention, and no instruction). Participants judged whether the speaker expressed happiness, sadness, anger, fear, disgust or surprise. Our results revealed that instructions to pay attention to one modality and congruency of the emotions between modalities modulated the modality dominance, and the modality dominance is differed for each emotion category. In particular, the modality dominance for anger changed according to each instruction. Analyses also revealed that the modality dominance suggested by the congruency effect can be explained in terms of the facilitation effect and the interference effect...|$|R
40|$|Biometrics {{is one of}} {{the recent}} trends in security, which is mainly used for {{verification}} and recognition systems. By using biometrics we confirm a particular person’s claimed identity based on particular person’s physiological or behavioral characteristics such as fingerprint, <b>face</b> <b>or</b> <b>voice</b> etc. This is the extension work of my previous two papers, which tries to overcome the difficulties of single modality. These limitations are addressed by multi-modal biometric verification system as explained in my previous papers. I have chosen existing methodologies like Facial and Finger Print verification modals, ANN to be combined for verification. In this we use Linear Discriminant analysis (LDA) for face recognition and Directional filter bank (DFB) for fingerprint matching. Based on experimental results, the proposed system can reduce FAR down to 0. 0000121...|$|R
60|$|The {{pretty little}} one so cried, and raved, and tore herself that I {{could not have}} held her, but for her swooning on my arm {{as if she had}} been shot. Master came up - in manner, <b>face,</b> <b>or</b> <b>voice,</b> no more the master that I knew, than I was he. He took me (I laid the little one upon her bed in the hotel, and left her with the chamber-women), in a carriage, furiously through the darkness, across the desolate Campagna. When it was day, and we stopped at a miserable post-house, all the horses had been hired twelve hours ago, and sent away in {{different}} directions. Mark me! by the Signor Dellombra, who had passed there in a carriage, with a frightened English lady crouching in one corner.|$|R
60|$|He hardly {{recognized}} her <b>face</b> <b>or</b> her <b>voice,</b> {{but what she}} said proclaimed her to be Allie. She enveloped him. Her arms, strong, convulsive, clasped him. Up came her face, white, gleaming, joyous, strange to Neale, but he knew somehow that it was held up to be kissed. Dazedly he kissed her--felt cool sweet lips touch his lips again and then again.|$|R
40|$|Despite of {{processing}} elements which {{are thousands of}} {{times faster than the}} neurons in the brain, modern computers still cannot match quite a few processing capabilities of the brain, many of which we even consider trivial (such as recognizing <b>faces</b> <b>or</b> <b>voices,</b> <b>or</b> following a conversation). A common principle for those capabilities lies in the use of correlations between patterns in order to identify patterns which are similar. Looking at the brain as an information processing mechanism with [...] maybe among others [...] associative processing capabilities together with the converse view of associative memories as certain types of artificial neural networks initiated a number of interesting results, ranging from theoretical considerations to insights in the functioning of neurons, as well as parallel hardware implementations of neural associative memories. This paper discusses three main aspects of neural associative memories: ffl theoretical investigations, e. g. on the information storage [...] ...|$|R
40|$|Information {{derived from}} facial and vocal nonverbal {{expressions}} {{plays an important}} role in social communication in the real and virtual worlds. In the present study, we investigated cultural differences between Japanese and Dutch participants in the multisensory perception of emotion. We used a face and voice that expressed incongruent emotions as stimuli and conducted two experiments. We presented either the <b>face</b> <b>or</b> <b>voice</b> in Experiment 1, and both the face and voice in Experiment 2. We found that both visual and auditory information were important for Japanese participants judging in-group stimuli, while visual information was more important for other combinations of participants and stimuli. Additionally, we showed that the in-group advantage provided by auditory information was higher in Japanese than Dutch participants. Our findings indicate that audio-visual integration of affective information is modulated by the perceiver's cultural backgrou...|$|R
40|$|Iannarelli's studies {{demonstrated}} that ear shape represents a biometric identifier able to authenticate {{people in the}} same way as more established biometrics, like <b>face</b> <b>or</b> <b>voice</b> for instance. However, not many researches can be found in literature about ear recognition. In most cases existing algorithms are borrowed from other biometric contexts. An example is PCA (Principal Component Analysis). Eigen-ears only provide high recognition rate in closely controlled conditions, while performances decay even for small changes in environmental conditions. We propose a fractal based technique, namely HERO (Human Ear Recognition against Occlusions) to classify human ears. The feature extraction process has been made local, so that the system gets robust with respect to small changes in pose/illumination and partial occlusions. Experimental results confirm the superiority of this approach over several linear and non linear techniques. © 2010 IEEE...|$|R
5|$|Typical {{attachment}} development {{begins with}} unlearned infant reactions to social signals from caregivers. The ability to {{send and receive}} social communications through facial expressions, gestures and voice develops with social experience by seven to nine months. This {{makes it possible for}} an infant to interpret messages of calm <b>or</b> alarm from <b>face</b> <b>or</b> <b>voice.</b> At about eight months, infants typically begin to respond with fear to unfamiliar or startling situations, and to look to the faces of familiar caregivers for information that either justifies or soothes their fear. This developmental combination of social skills and the emergence of fear reactions results in attachment behavior such as proximity-seeking, if a familiar, sensitive, responsive, and cooperative adult is available. Further developments in attachment, such as negotiation of separation in the toddler and preschool period, depend on factors such as the caregiver's interaction style and ability to understand the child's emotional communications.|$|R
40|$|Understanding older adults' social {{functioning}} difficulties requires insight into their recognition of emotion processing in voices and bodies, not just faces, {{the focus of}} most prior research. We examined 60 young and 61 older adults' recognition of basic emotions in facial, vocal, and bodily expressions, and when matching faces and bodies to voices, using 120 emotion items. Older adults were worse than young adults in 17 of 30 comparisons, with consistent difficulties in recognizing both positive (happy) and negative (angry and sad) vocal and bodily expressions. Nearly three quarters of older adults functioned at a level similar to the lowest one fourth of young adults, suggesting that age-related changes are common. In addition, we found that older adults' difficulty in matching emotions was not explained by difficulty on the component sources (i. e., <b>faces</b> <b>or</b> <b>voices</b> on their own), suggesting an additional problem of integration. Copyright 2009, Oxford University Press. ...|$|R
50|$|Typical {{attachment}} development {{begins with}} unlearned infant reactions to social signals from caregivers. The ability to {{send and receive}} social communications through facial expressions, gestures and voice develops with social experience by seven to nine months. This {{makes it possible for}} an infant to interpret messages of calm <b>or</b> alarm from <b>face</b> <b>or</b> <b>voice.</b> At about eight months, infants typically begin to respond with fear to unfamiliar or startling situations, and to look to the faces of familiar caregivers for information that either justifies or soothes their fear. This developmental combination of social skills and the emergence of fear reactions results in attachment behavior such as proximity-seeking, if a familiar, sensitive, responsive, and cooperative adult is available. Further developments in attachment, such as negotiation of separation in the toddler and preschool period, depend on factors such as the caregiver's interaction style and ability to understand the child's emotional communications.|$|R
40|$|Abstract—Biometrics is {{combination}} of two Greek words Bios (life) and metrics (measure). It is recognized that some human body characteristics such as <b>face,</b> Finger <b>or</b> <b>voice</b> {{can be used to}} distinguish individual from a group of people. In a biometrics system a person is recognized on the basis of physical and behavioral traits. This paper gives a comparison on the various techniques used for finger print recognition, face recognition and speech recognition...|$|R
40|$|Successful social {{interaction}} hinges on accurate perception of emotional signals. These signals are typically conveyed multi-modally by {{the face and}} voice. Previous research has demonstrated uni-modal contrastive aftereffects for emotionally expressive <b>faces</b> <b>or</b> <b>voices.</b> Here {{we were interested in}} whether these aftereffects transfer across modality as theoretical models predict. We show that adaptation to facial expressions elicits significant auditory aftereffects. Adaptation to angry facial expressions caused ambiguous vocal stimuli drawn from an anger-fear morphed continuum to be perceived as less angry and more fearful relative to adaptation to fearful faces. In a second experiment, we demonstrate that these aftereffects are not dependent on learned face-voice congruence, i. e. adaptation to one facial identity transferred to an unmatched voice identity. Taken together, our findings provide support for a supra-modal representation of emotion and suggest further that identity and emotion may be processed independently from one another, at least at the supra-modal level of the processing hierarchy...|$|R
5000|$|Previous formats {{included}} country, Saturday night Rock and Roll, and Sunday morning Big Band. During the weekdays {{a segment}} of the afternoon shift was dedicated to local callers to hawk their goods with free advertising through the [...] "Swap Shop" [...] segment. Broadcasts started around 5:30AM local time with the broadcast day ending at midnight. KPRK also broadcast local high school sports, rodeos and fairs. Several locals were also familiar <b>faces</b> <b>or</b> <b>voices</b> on the station for many years. KPRK also broadcast local news three times a day with the local court report during that time. KPRK staff received several awards for their news contributions to the Montana AP for news reports gathered during 1999. The former country music slogan was [...] "Cool Country 1340 KPRK". The station also featured an uninterrupted [...] "Cool Country Triple Play" [...] where two newer songs were played followed by a [...] "Hit from Yesterday", otherwise known as a country classic.|$|R
60|$|So, having roped the camels {{into a long}} line, we went on alone, truly {{thankful to}} be rid of them, and praying, every one of us, that never in this world or the next might we see the <b>face</b> <b>or</b> hear the <b>voice</b> of another Abati.|$|R
40|$|There {{are many}} daily pattern {{recognition}} tasks that humans routinely carry out without thinking twice. For example, we can recognize those {{that we know}} by looking at their <b>face</b> <b>or</b> hearing their <b>voice.</b> You can recognize the letters and words you are reading now because you have trained yourself to recogniz...|$|R
40|$|Purpose: Interacting {{with others}} by reading their {{emotional}} expressions {{is an essential}} social skill in humans. How this ability develops during infancy and what brain processes underpin infants' perception of emotion in different modalities are the questions dealt with in this paper. Methods: Literature review. Results: The first part provides a systematic review of behavioral findings on infants' developing emotion-reading abilities. The second part presents a set of new electrophysiological studies that provide insights into the brain processes underlying infants' developing abilities. Throughout, evidence from unimodal (<b>face</b> <b>or</b> <b>voice)</b> and multimodal (<b>face</b> and voice) processing of emotion is considered. The implications of the reviewed findings for our understanding of developmental models of emotion processing are discussed. Conclusions: The reviewed infant data suggest that (a) early in development, emotion enhances the sensory processing of faces and voices, (b) infants' ability to allocate increased attentional resources to negative emotional information develops earlier in the vocal domain than in the facial domain, and (c) at least {{by the age of}} 7 months, infants reliably match and recognize emotional information across face and voice...|$|R
6000|$|Of course Barret {{feared that}} she would {{recognise}} him, and had been greatly exercised as to his precise duty in the circumstances; but when he found {{that she did not}} recognise either his <b>face</b> <b>or</b> his <b>voice,</b> he felt uncertain whether it would not be, perhaps, better to say nothing at all about the matter in the meantime. Indeed, the grateful old lady gave him no time to make a [...] "clean breast of it," [...] as he had at first intended to do.|$|R
60|$|The eye-witnesses {{of these}} memorable events have {{declared}} that, {{at a given}} moment, he looked up suddenly with a curious, eager expression in his eyes, and then raised himself in the cart {{and seemed to be}} trying to penetrate the gloom round him, as if in search of a <b>face,</b> <b>or</b> perhaps a <b>voice.</b>|$|R
60|$|That Quilp lied most heartily in this speech, {{it needed}} no very great {{penetration}} to discover, although {{for anything that}} he suffered to appear in his <b>face,</b> <b>voice,</b> <b>or</b> manner, {{he might have been}} clinging to the truth with the quiet constancy of a martyr.|$|R
