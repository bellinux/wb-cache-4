2|10000|Public
40|$|The {{present study}} {{is part of a}} large {{research}} project on growth, development, nutrition, and health of Caboclo populations from the Brazilian Amazon. The aim {{of this paper is to}} analyze the age of menarche in adolescents (N = 164) and adult women (219) in the studied populations. Caboclo are admixed rural, peasant groups that live along the Amazon river and its tributaries, and there are few previous studies about them. Probit analysis of the status quo data yielded a median age at menarche of 12. 29 ± 1. 76 years. The retrospective method was applied <b>to</b> <b>recall</b> <b>data</b> of the 77 post-menarcheal adolescents, yielding an average of 13. 06 ± 1. 27 years. Number of children in the family did not show any statistical influence on age at menarche in any age group. In adult women, age at menarche decreased from 14. 50 in those born in 1930 to 12. 88 for those born in 1980 (F = 4. 371, P = 0. 001). The downward trend found was, on average, 0. 237 year per decade in the study period. The median age at menarche in the adolescents (12. 29 years) is one of the lowest values found for Central and South American populations. In the ecological context, a low age at menarche could be an adaptive advantage because it provides a greater chance for reproduction at a young age in an environment where, until recently, life expectancy was low. As has been reported for other developing countries, the change found in age at menarche in the women born from the 1930 s to the 1980 s is likely to be related to changes in health and nutritional factors that occurred in Brazil because this country experienced significant improvement in living standards related to education, vaccination, and health conditions, which, although not equally, reached all regions after the 1960 s. Am. J. Hum. Biol. 18 : 83 - 92, 2006. © 2005 Wiley-Liss, Inc. [URL]...|$|E
40|$|A recent {{development}} in Computational Fluid Dynamics (CFD) {{has been the}} meshless method calledWeakly Compressible Smoothed Particle Hydrodynamics (WCSPH), which is a Lagrangian method that tracks physical quantities of a fluid as it moves in time and space. One disadvantage of WCSPH is the small time steps required due {{to the use of}} the weakly compressible Tait equation of state, so large scale simulations using WCSPH have so far been rare and only performed on very expensive CPU-based supercomputers. As CFD simulations grow larger and more detailed, the need to use high performance computing also grows. There is therefore great interest in any computer technology that can provide the equivalent computational power of the CPU-based supercomputer for a fraction of the cost. Hence the excitement aroused in the SPH community by the Graphics Processing Unit (GPU). The GPU offers great potential for providing significant increases in computational performance due to its much smaller size and power consumption relative to the more established and traditional high performance computers comprising hundreds or thousands of CPUs. However, there are some disadvantages in programming GPUs. The memory structure of the GPU is more complex and more variable in speed, and there are other factors that can seriously affect performance, such as the thread grid dimensions which drives the occupancy of the GPU. The aim of this thesis is to describe how WCSPH can be efficiently implemented on multiple GPUs. First, some CFD methods and their success or otherwise in simulating free surfaces are discussed, and examples of previous attempts at implementing CFD algorithms on GPUs are given. The mathematical theory of WCSPH is then presented, followed by a detailed examination of the architecture of a GPU and how to program a GPU. Two different implementations of the same WCSPH algorithm are then described to simulate a well known experiment of a collapse of a column of water to highlight two possible uses of the GPU memory. The first method uses the fast shared memory of the GPU, which is recommended by the GPU manufacturer, while the second method uses the texture i memory of the GPU, which acts as a cache. It is shown that due to the theory of WCSPH, which allows particles to only interact with other particles a short distance apart, that despite the speed of the shared memory and the power of coalescing data into the shared memory, the texture memory method is currently the most efficient, but that this method of implementing WCSPH on a single GPU requires a much higher degree of complexity of programming than the shared memory method. It is also shown that the size of the thread block can have a significant effect on performance. Riemann solvers add more computational effort but can provide more accuracy. The use of Riemann solvers in WCSPH and their success or otherwise is then examined, and the results and performance of one particular WCSPH algorithm that uses an approximate Riemann solver when executed on a GPU are reported. The treatment of boundaries has been and continues to be a problem in WCSPH, and there are a number of creative proposals for boundary treatments. Some of these are described in detail before a new boundary treatment is proposed that builds upon a boundary treatment that was recently proposed, and improves its performance in execution time on a GPU by using the registers and not the slower memories of the GPU. This new boundary treatment builds a unique private grid of boundary particles for each fluid particle close to the boundary. All computation is performed in the registers, the properties of the boundary particles depend on the fluid particle only, and there is no requirement <b>to</b> <b>recall</b> <b>data</b> from the slower global or texture memories of the GPU. The new boundary treatment is also shown to propagate a solitary wave further, preserves the wave height more and takes less execution time to compute than the original boundary treatment this new treatment builds on. A unique and simple implementation of WCSPH on multiple GPUs is then described, and the results of a simulation of a collapse of a column of water in 3 D are reported and compared against the results from a simulation of the same problem with the same WCSPH algorithm executed on a large cluster of multi core CPUs. The conclusion is that simulations on a small cluster of GPUs can achieve greater performance than from a cluster of multi core CPUs, but to achieve this the slow GPU memories, including the texture ii memory, must be avoided by using the registers as much as possible, and the architecture of the network linking the GPUs together must be exploited. The former was achieved by using the new boundary treatment proposed in this thesis and discussed above, and the latter was achieved by the use of the MPI Group functionality. The GPUs used for this thesis were already connected together in boxes of 4 by the manufacturer. The cluster used for this thesis consisted of 8 of these boxes, giving a total of 32 GPUs. These boxes of 4 GPUs were connected together through a common host, but the communication speed over the connection between the box and the host is much slower than that between the GPUs inside the box. The total communication time was minimized by grouping the GPUs inside a box together with their private unique MPI communicator, and a communication procedure was created to minimize communication over the relatively slow connection between the boxes of GPUs and the host. Finally, some conclusions are drawn and suggestions for further work are made. ...|$|E
40|$|Eye/brain/task (EBT) testbed records electroencephalograms, {{movements}} of eyes, and structures of tasks to provide comprehensive data on neurophysiological experiments. Intended to serve continuing {{effort to develop}} means for interactions between human brain waves and computers. Software library associated with testbed provides capabilities <b>to</b> <b>recall</b> collected <b>data,</b> <b>to</b> process data on {{movements of}} eyes, to correlate eye-movement data with electroencephalographic data, and to present data graphically. Cognitive processes investigated in ways not previously possible...|$|R
40|$|Abstract Background Studies on {{the health}} {{benefits}} from breastfeeding often rely on maternal recall of breastfeeding. Although short-term maternal recall {{has been found to}} be quite accurate, less is known about long-term accuracy. The objective of this study was to assess the accuracy of long-term maternal recall of breastfeeding duration. Methods In a prospective study of pregnancy and birth outcome, detailed information on breastfeeding during the child’s first year of life was collected from a cohort of Norwegian women who gave birth in 1986 – 88. Among 374 of the participants, data on breastfeeding initiation and duration were compared <b>to</b> <b>recalled</b> <b>data</b> obtained from mailed questionnaires some 20 years later. Intraclass correlation coefficient (ICC), Bland-Altman plot, and Kappa statistics were used to assess the agreement between the two sources of data. Logistic regression was used to assess predictors of misreporting breastfeeding duration by more than one month. Results Recorded and recalled breastfeeding duration were strongly correlated (ICC= 0. 82, p Conclusion Breastfeeding duration was recalled quite accurately 20 years after mothers gave birth in a population where breastfeeding is common and its duration long. </p...|$|R
5000|$|Private.me uses {{a process}} to encrypt and {{distribute}} data to the Data Neutrality Administration, a network of privacy nonprofits that have been established with the mission of stewarding user data. This process is called the Dispersed Storage System (DSS). It is accessible via an API. and will be offered as a privacy tool. The API requires explicit permission in order <b>to</b> <b>recall</b> a user’s <b>data</b> ...|$|R
40|$|Background Studies on {{the health}} {{benefits}} from breastfeeding often rely on maternal recall of breastfeeding. Although short-term maternal recall {{has been found to}} be quite accurate, less is known about long-term accuracy. The objective of this study was to assess the accuracy of long-term maternal recall of breastfeeding duration. Methods In a prospective study of pregnancy and birth outcome, detailed information on breastfeeding during the child’s first year of life was collected from a cohort of Norwegian women who gave birth in 1986 – 88. Among 374 of the participants, data on breastfeeding initiation and duration were compared <b>to</b> <b>recalled</b> <b>data</b> obtained from mailed questionnaires some 20 years later. Intraclass correlation coefficient (ICC), Bland-Altman plot, and Kappa statistics were used to assess the agreement between the two sources of data. Logistic regression was used to assess predictors of misreporting breastfeeding duration by more than one month. Results Recorded and recalled breastfeeding duration were strongly correlated (ICC= 0. 82, p < 0. 001). Nearly two thirds of women <b>recalled</b> their breastfeeding <b>to</b> within one month. <b>Recall</b> <b>data</b> showed a modest median overestimation of about 2 weeks. There were no apparent systematic discrepancies between the two sources of information, but recall error was predicted by the age when infants were introduced to another kind of milk. Across categories of breastfeeding, the overall weighted Kappa statistic showed an almost perfect agreement (κ = 0. 85, 95 % confidence interval [CI] 0. 82 – 0. 88). Conclusion Breastfeeding duration was recalled quite accurately 20 years after mothers gave birth in a population where breastfeeding is common and its duration long...|$|R
40|$|This study {{investigated}} the generation of emotional inferences during the reading and recall of narrative texts. Experiment 1 compared the fit of two simulations of text comprehension <b>to</b> the <b>recall</b> <b>data.</b> One simulation examined causal and referential inferences, while the other examined causal, referential and emotional inferences. We found that the simulation that involved emotional inferences provided a better fit to the human data than the other simulation. Experiment 2 tested whether emotional inferences are generated online by recording lexical decision times at pre-inference and inference locations. Lexical decision times were faster at the inference than the pre-inference locations. These findings suggest that emotional inferences {{play a role in}} the understanding of natural texts, and that they require the reader to establish connections between text segments...|$|R
5000|$|All Air Route Traffic Control Centers (ARTCC) {{have the}} {{capability}} <b>to</b> <b>recall</b> recorded radar <b>data.</b> The National Track Analysis Program (NTAP) can identify and track targets which are at a sufficient altitude to be tracked by radar {{whether or not they}} are being [...] "controlled" [...] by the ARTCC. NTAPs requested by the AFRCC have proven to be very helpful during an aircraft search by providing the route of flight and last radar position of an aircraft being searched for.|$|R
5000|$|... (This book {{takes the}} form of a small {{textbook}} allowing everyone to understand the Hindu religion in a simple, clear and fast way and to understand the main historical and structural elements of this religion, from its origins to its development. (...) Strong points: A clear, pedagogical and structured discourse that makes it possible to approach and understand Hinduism in its various aspects; summaries at the end of the chapter make it possible <b>to</b> <b>recall</b> the essential <b>data.</b> And the mention of works of recognized and frequently referenced Indianists. The use of synthetic tables which make it easier to read certain data ...|$|R
40|$|In this paper, we {{investigate}} two questions. First, we explore which entity (the NHTSA or the manufacturer) {{is more likely}} to initiate a given auto safety recall campaign. Second, we analyze the determinants of owner response rates <b>to</b> safety <b>recalls.</b> Our <b>data</b> spans nineteen years (1980 - 1998) for the six largest auto manufacturers. We find evidence that the government initiates larger, less hazardous recalls involving older models and ¯nancially weak firms. Inexpensive recalls {{are more likely to be}} manufacturer initiated. The largest owner repair responses are associated with newsworthy hazardous defects of new domestic vehicles in their inaugural model year...|$|R
40|$|Weather can be {{unpredictable}} {{as there}} are a lot of uncertainties in predicting thunderstorms. Most of our navigation systems, including those on air, land and water, as well as broadcasting systems, are directly affected by weather on a daily basis. The inconsistent and unreliable nature of storms brings out the importance of research in atmospheric electric field data logging systems. This paper presents a study to develop a virtual instrument with the capability to analyse and store the magnitude (data) of atmospheric electric fields. The study was carried out using a LabVIEW virtual instrument and tested using data acquisition (DAQ) and a function generator. The developed virtual instrument consists of waveform chart, tabulated data, and histogram for real time observation. Moreover, it has feature <b>to</b> save and <b>recall</b> <b>data</b> for further analysis...|$|R
40|$|Previous {{research}} {{demonstrates that}} older adults are poor at dual tasking, {{but there is}} less agreement on whether their decrement is worse than that predicted from single‐task performance. This study investigated whether task domain moderates dual‐task costs in old age. In two experiments, young and older adults retrieved either previously learned associates (episodic retrieval) or overlearned category members (semantic retrieval) under single or working‐memory load conditions, using cued recall (Experiment 1) and recognition (Experiment 2) procedures. In both experiments the proportional costs of dual tasking were age invariant for semantic retrieval but were particularly marked for episodic retrieval, although {{the size of the}} age effect was reduced in recognition compared <b>to</b> cued <b>recall.</b> The <b>data</b> suggest that age effects in dual tasking may be domain specific...|$|R
30|$|Our {{results should}} be {{interpreted}} {{in the context of}} some limitations. The retrospective assessment of AAO might have been subject <b>to</b> <b>recall</b> bias. However, <b>data</b> were gathered through direct interview of the patients as well as with systematic review of medical charts decreasing the probability of a systematic bias in the assessment of AAO. An additional limitation is the lack of a systematic approach in collecting family history data, which might have influenced the assessment of familial load in our sample. Moreover, external corroboration for AAO was obtained, whenever possible, by directly interviewing a first-degree family member or other significant individuals. Further, our samples of BD 1 and BD 2 patients might not have had an adequate statistical power to detect association signals of small to moderate magnitude. Finally, our study did not consider birth cohort effect in our analysis.|$|R
40|$|Data {{visualisation}} {{is centered}} on new ways of processing and displaying large data sets to support pattern recognition by humans rather than by machines. The motivation for approaches based on data visualisation is to encourage data exploration and curiosity by analysts. They should help formulating the right question more than addressing specific predefined issues or expectations. Translated into IAEA’s terms, they should help verify the completeness of information declared to the IAEA more than their correctness. Data visualisation contrasts with traditional information retrieval where one needs first to formulate a query {{in order to get}} to a narrow slice of data. Using traditional information retrieval, it remains unknown what is missed out. The system may fail <b>to</b> <b>recall</b> relevant <b>data</b> due <b>to</b> the way the query was formulated, or the query itself may not be the most relevant one to be asked in the first place. Examples of data visualisations relevant to safeguards are illustrated, including new approaches for the review of surveillance images and for trade analysis. Common to these examples is the attempt to enlarge the view of the analyst on a universe of data, where context or detailed data is presented on-demand and by levels of abstraction. JRC. E. 8 -Nuclear securit...|$|R
40|$|This study {{attempts}} {{to understand the}} wishful thinking effect through a group ap-proach. We build a group decision making model that tries to explain how a group dynamic can lead the group <b>to</b> interpret and <b>recall</b> <b>data</b> so that they correspond to more favorable beliefs about the group future prospects. Group members have to take one single common decision determining the group effort level in a joint production. We find that (a) An agent's incentive to enter into denial when others are in denial is higher than an agent's incentive to enter into denial when he is alone; (b) An agent's incentive to enter into denial when others are realist is lower than an agent's incentive to enter into denial when he is alone; (c) An agent's incentive to enter into denial increases with the riskiness of the project; (d) {{the presence of a}} leader can either increase or decrease the incentive to enter into denial depending on assumptions made on costs. We build an experimental protocol allowing to try to test the previous model and validate or not its predictions...|$|R
40|$|The {{purpose of}} this study is the {{analysis}} of the possibilities and difficulties of e-recruitment practices for SMEs in Malaysia, elaborating their effectiveness, and explaining some practical and managerial implications concerning these actions. Statistical analyses and empirical findings expressed here show that the attitudes and opinions of SME executives towards e-recruitment are considered a sub-function of an integrated e-HRM and can be seen as a valued technological improvement in the main critical activities of human resource management. However, findings show that the ability to communicate and deal with a multilingual organizational and working environment is the most important operational benefit of a web-based e-HRM and e-recruiting system. This technology is seen as a pathway to improving external and internal interactions and communication between job seekers, firms, employees, and other stakeholders, as well as to build data bases <b>to</b> store and <b>recall</b> <b>data.</b> These are the main strategic advantages of a recruitment system for SMEs in Malaysia. The findings of this study also show that e-recruitment in Malaysia is still in-progress work and even the big recruiting firms have not fully implemented this technology...|$|R
40|$|AbstractAimTo {{identify}} lifestyle {{factors affecting}} risk of relapse. MethodsA comparison of 131 relapsed melanoma patients with 147 non-relapsers. ResultsRelapsers {{were more likely}} to report financial hardship using a number of different measures including access to holidays and feeling financially insecure (odds ratio (OR) 5. 7, 95 % confidence interval (CI) (1. 5, 21. 4)). Relapsers worked longer hours (mean 37 h per week compared with 31, p= 0. 02). There was no reported difference in stress associated with recent life events. There was no effect of housing quality, employment factors or body mass index (BMI) on risk of relapse. There was a protective effect of antibiotics in the peri-operative period. ConclusionThe study provides preliminary evidence for adverse effects of chronic financial hardship, but not recent stressful events on cancer relapse. As these data were collected in a retrospective case–control study subject <b>to</b> <b>recall</b> bias, the <b>data</b> must now be explored in a prospective study...|$|R
40|$|Several {{researchers}} (e. g., Howard Kahana, 2002) {{have proposed}} that recalling an event is bound up with recall of that event's surrounding context, and that retrieved context information {{can be used to}} cue memory for other items from that context. In this study, we sought evidence for this contextual reinstatement process using fMRI. Specifically, we wanted to know whether the task being performed when forming a memory would be recalled along with that memory, and how this would influence subsequent recalls. Subjects studied lists of 24 words, performing either a size, animacy or pleasantness judgment task on each word. After a series of arithmetic distractors, subjects were asked <b>to</b> <b>recall</b> out loud and in any order the words from the most recent list. Since subjects were being scanned during both study and recall phases, we trained a classifier on the study period to distinguish which of the three tasks were being performed. We then tested this classifier during <b>recall</b> <b>to</b> estimate the degree to which each task representation was active in the subject's mind, moment by moment (Polyn et al, 2005). <b>To</b> analyze the <b>recall</b> <b>data,</b> we labelled each recall with its judgment task from the study period. These were predicted better than chance by the classifier's estimates of task activity at recall. We broke the data down further, looking at the transitions from one <b>recall</b> <b>to</b> the next. We found that high classifier activity for one kind of task judgment indicated that the next recall would be another item from that task, and that the inter-response latency would be small. In other words, a highly active task representation would facilitate recalls of other items from the same task. These results support the contextual reinstatement theory, suggesting that reinstating the context surrounding an event improves recall of other items that were studied in that context...|$|R
30|$|The {{study used}} primary data {{obtained}} from a well-structured farm survey that was conducted between August and December of 2008. The survey used a quantitative and qualitative questionnaire to interview randomly selected cotton farmers, asking them <b>to</b> <b>recall</b> input–output <b>data</b> related <b>to</b> the cotton-growing season of 2007 – 2008. The study pre-tested the questionnaire by interviewing 20 randomly selected farmers in the Khonka district of the Khorezm province. This practice presented {{a clearer picture of}} the prospective problems that might arise during the interview for both the enumerators and the respondents. Enumerators with good knowledge of the study areas and the agricultural activities administered the farm survey. Individuals with effective communication skills were chosen and trained for a week to clarify the structure of the questionnaire and on what they should focus on during the interview. The study selected survey respondents using the multistage sampling technique. The first step comprised a purposeful selection of districts to capture an adequate representation of the province. For the analysis, eight out of ten districts were selected. The second step involved randomly selecting two to three villages (depending on the number of farmers) within the chosen districts. The study conducted the survey in 23 villages. In the third step, cotton farmers were randomly selected from each chosen village. The final analysis, after adjusting for missing cases and outliers, covered 298 cotton farms (for detailed explanations see Karimov [2012]).|$|R
40|$|A {{letter report}} {{issued by the}} Government Accountability Office with an {{abstract}} that begins "Two large food recalls completed in 2003 were associated with 8 deaths and nearly 100 serious illnesses in at least 16 states. Manufacturers voluntarily recall potentially unsafe food by notifying their customers to return or destroy it. The U. S. Department of Agriculture (USDA), for meat, poultry, and egg products, and the Food and Drug Administration (FDA), for other food, have programs to monitor voluntary food recalls, verify that companies contact their customers, and maintain <b>recall</b> <b>data.</b> GAO (1) examined the recall programs and procedures USDA and FDA use to protect consumers from unsafe foods and (2) compared their food recall authority with the authority of agencies <b>to</b> <b>recall</b> other consumer products. ...|$|R
40|$|In {{an attempt}} to appease {{domestic}} auto manufacturers in 1981, the National Highway Traffic Safety Administration (NHTSA) stopped issuing a press release for each automotive safety recall. This policy change has potential harmful affects on public access <b>to</b> <b>recall</b> in-formation. Only Ford Motor Company continues to issue public announcements for every recall. We find that since 1981 Ford has been subject to more media scrutiny due to its policy of announcing every recall. Key factors that influence the likelihood of newspaper coverage include the number of vehicles recalled, defect severity, and cost of repair. ∗Special thanks to Brad Barber and Masako Darrough for making theirWall Street Journal <b>recall</b> <b>data</b> available. We thank Jeffrey DeSimone and Richard Rupp for their valuable comments...|$|R
40|$|Questions {{have been}} raised about the {{validity}} of data obtained from retrospective survey designs, since these are heavily dependent on the accuracy of respondent recall. One of the most serious problems is over-reporting, typically attributed to "forward telescoping", where respondents report events that occurred outside of the time period under consideration, thereby inflating the results. This study tested two procedures for reducing forward telescoping: the provision of a landmark event to clearly mark the beginning of the recall period; and a double question bounded recall procedure whereby respondents were first asked <b>to</b> <b>recall</b> events for a longer time period (previous eight weeks), then for the time period of interest (previous four weeks). This paper presents the results of this research and discusses the implications for studies dependent upon <b>recall</b> <b>data...</b>|$|R
50|$|In {{the study}} of human memory, the {{contiguity}} effect {{has been found in}} studies of free recall. Analyses of free <b>recall</b> <b>data</b> indicates that there tends to be the greatest number of +/- 1 transitions between words, suggesting that a person is more likely <b>to</b> <b>recall</b> words together that are closer together in a list. This is shown in a graph of conditional response probability as a function of lag as originated by Dr. Michael Kahana. The probability of recall (y-axis) is plotted against the lag, or separation between subsequently recalled words. For example, if two items A and B are learned together, when cued with B, A is retrieved and vice versa due to their temporal contiguity, although there will be a stronger forward association (when cued with A, B is recalled).|$|R
40|$|Studies {{show that}} roughly one-third of {{searches}} that are {{performed on the}} web require the user to initiate subsequent searches. Bates [1] theorized that with every search the user will encounter new information, which in turn leads to new ideas and directions. This process causes a change, not simply in the query terms being used {{but also to the}} nature of the information retrieval task itself; Bates called this the Evolving Search. She also noted that Evolving Searches utilize many different information sources, generate substantial quantities of data and require easy methods <b>to</b> save and <b>recall</b> <b>data.</b> Although current search tools are exceptionally efficient at locating highly ranked pages, the tools do not encourage or support the user in an evolving search. In this paper we present techniques that aid users to find, view and manage data produced from their evolving searches. In particular, we introduce the EvoBerry environment, which we have developed for use with evolving searches. EvoBerry includes methods to visualize additional search result information (such as length of page or file type), manage the user’s session and browsing history, compare result sets, and store and bookmark items for future reference. Keywords—Evolving searches, Search result visualization, Information Visualization, Berry-picking model. ...|$|R
40|$|This {{study is}} the first multi-year {{examination}} of the relative influence of the four main variables said <b>to</b> influence sponsorship <b>recall.</b> Sponsor <b>recall</b> <b>data</b> were collected from season ticket holders (STHs) of 10 professional sports teams, over periods ranging from 3 to 5 years per team. Across those teams and over that time, 309 sponsor–team relationships were examined, and sponsor <b>recall</b> <b>data</b> from over 117, 000 individual STHs were collected. Sponsorship length and level were shown to have the strongest impact on recall, followed by relatedness and prominence. These variables affected both the recall of current sponsors and the decay rates of residual recall {{following the end of}} a sponsorship. The average rates of sponsor recall growth and decline have been derived from these data, giving managers a tool by which to benchmark sport sponsorship recall performance...|$|R
40|$|Government statisticians are {{intrigued}} {{by the possibility of}} accessing administrative data as a way to enhance survey and census data. Survey data are rich in attributes, but they exhibit sampling errors. More consequentially, they demonstrate non-sampling errors, often because of nonresponse, but also because of responses that are incomplete or inaccurate, frequently due to the respondent’s inability <b>to</b> <b>recall.</b> Additionally, panel <b>data</b> are subject to attrition. Administrative data can help compensate for these problems importantly because administrative data, such as social security files, include almost everybody. Further, supplementing statistical data can be cost-effective, because an enormous amount of administrative data are already collected to support the functional operations of government agencies, and so are potentially available. In many instances, respondent burden in surveys can be lessened because certain attribute values may be available from administrative records. Some U. S. federal agencies, e. g., the Social Security Administration (McNabb et al., 2009) have linked administrative data with survey data to broaden its demographic and socioeconomic measures and also {{to improve the quality of}} the survey data. See National Research Council (2005), pp. 45 – 4...|$|R
40|$|American Sociological Association meetings. The authors {{acknowledge}} the thoughtful comments of two anonymous reviewers which helped us {{to refine the}} arguments made here. 2 Has the image of Che Guevara lost its power to evoke radical politics {{in the face of}} pervasive commodification? The commercialization of this Sixties political icon has called into question the power of the market to shape collective memories. Meanwhile, antisystemic movements of the Left continue to erect his image at protest events. In light of this contest over how Che Guevara is remembered, we investigate who is most likely <b>to</b> <b>recall</b> him using <b>data</b> from a survey of Spanish citizens. We find qualified support for the theory of generational imprinting⎯Che is more often recalled by those generations who saw him rise to prominence during their formative years, although prominent as a collective symbol rather than as a living person. Our results also corroborate the claim that historical figures or events are more salient for, and therefore more likely to be remembered by, some sub-generational units than others. Thus, although the younger generations are in general more likely than their elders <b>to</b> <b>recall</b> Che, he is most frequently remembered by the highly educated leftists who espouse post-materialist and post-traditionalist values and identify more with their local regions than with the nation of Spain. These patterns suggest that, in contrast to the dire predictions of mass culture theorists, the memory of Che Guevara has become increasingly tied from markers of social, ethnic-regional, and political identity...|$|R
40|$|Abstract Although much {{is known}} about the {{dynamics}} of memory search in the free recall task, relatively little {{is known about}} the factors related <b>to</b> <b>recall</b> termination. Reanalyzing individual trial data from 14 prior studies (1, 079 participants in 28, 015 trials) and defining termination as occurring when a final response is followed by a long nonresponse interval, we observed that termination probability increased throughout the recall period and that retrieval was more likely to terminate following an error than following a correct response. Among errors, termination probability was higher following prior-list intrusions and repetitions than following extralist intrusions. To verify that this pattern of results can be seen in a single study, we report a new experiment in which 80 participants contributed <b>recall</b> <b>data</b> from a total of 9, 122 trials. This experiment replicated the pattern observed in the aggregate analysis of the prior studies...|$|R
40|$|While {{researchers}} have extensively documented the equity response <b>to</b> product <b>recalls</b> and subsequent shareholder losses, less {{attention in the}} literature {{has been given to}} examining the damaging recall attributes. Using 1973 – 1998 automotive safety <b>recall</b> <b>data,</b> this study identifies the kinds of recalls that cause significant shareholder losses. After constructing 10 an equally-weighted automotive market index to control for industry effects and adjusting the abnormal returns to account for the degree of surprise in the recall announcement, the study estimates both percentage and real dollar abnormal returns. We find that the indirect costs of automotive recalls are likely larger than the direct costs...|$|R
40|$|Abstract. BASS 2000 is the French solar {{database}} for ground-based instru-ments. We describe hereafter our organization, our tasks and the products we can deliver {{to the international}} community. Our prospects cover data mining into the THéMIS archive, a participation to the EST endeavour and the creation and curation of the ESPaDOnS/NARVAL stellar spectra database. 1. Organization, tasks and products BASS 20001 is the archive and database of several ground-based solar instru-ments such as the NRH (Nançay Radio-Heliograph), the Observatoire de Paris-Meudon spectroheliograph, the Pic du Midi coronograph (HACO, now CLIMSO) and the French-Italian telescope THéMIS installed in Tenerife. BASS 2000 is located at two different sites. Its main centre is hosted by the Observatoire Midi-Pyrénées, and it is situated {{on the campus of}} Tarbes about 150 km south-west of the main town of Toulouse. It is the main database and archive of THéMIS. The later archive represents now about 11 To of mostly raw data collected since 1999. Concerning THéMIS data, it is quite important <b>to</b> <b>recall</b> its <b>data</b> policy here. Indeed, after a PI-ship of one year, all data become public. On-line query forms are available to users in order to select data of interest for them. Afterwards we take care of supplying to any user the requested data over the network or using standard media, depending on the requested volume, as quickly as possible. The BASS 2000 centre at Observatoire de Paris-Meudon is mainly in charge of systematic data from other observatories than THéMIS. Daily images of the Meudon spectroheliograph, of the Nançay decametric array and radio-heliograph, and of the Pic du Midi coronograph are indeed available here. This centre also provides additional services and tools such as solar ephemeris and reference spectra on-line...|$|R
40|$|This {{study is}} {{concerned}} with the practical application of repetition effects in a classroom setting. In Expt 1 subjects listened to a short passage. At various points in the passage they were required to answer a self-assessment question (SAQ) which referred to an item of information in the immediately preceding portion of the text. Each SAQ was then repeated either immediately, or after one minute, two minutes, or four minutes. Three hours later subjects were given an unexpected recall test in which they attempted <b>to</b> <b>recall</b> the answers they had given earlier. The <b>recall</b> <b>data</b> showed that accuracy increased as a function of lag between presentation and repetition of SAQs. Experiment 2 examined whether the effect of lag in Expt 1 was due to the increased effort that might be involved in answering an SAQ repeated after a longer lag. The data showed that increasing the lag between an SAQ and its target information did not affect subsequent delayed recall. Practical aspects of the data are considered...|$|R
40|$|Despite the {{importance}} of agriculture to economic development, and a vast accompanying literature on the subject, {{little research has been}} done {{on the quality of the}} underlying data. Due to survey logistics, agricultural data are usually collected by asking respondents <b>to</b> <b>recall</b> the details of events occurring during past agricultural seasons that took place a number of months prior to the interview. This gap can lead <b>to</b> <b>recall</b> bias in reported data on agricultural activities. The problem is further complicated when interviews are conducted over the course of several months, thus leading <b>to</b> <b>recall</b> of variable length. To test for such recall bias, the length of time between harvest and interview is examined for three African countries with respect to several common agricultural input and harvest measures. The analysis shows little evidence of <b>recall</b> bias impacting <b>data</b> quality. There is some indication that more salient events are less subject <b>to</b> <b>recall</b> decay. Overall, the results allay some concerns about the quality of some types of agricultural <b>data</b> collected through <b>recall</b> over lengthy periods. Crops&Crop Management Systems,Educational Sciences,Rural Development Knowledge&Information Systems,Regional Economic Development,Rural Poverty Reduction...|$|R
40|$|Zero tillage (ZT) for wheat {{is one of}} {{the most}} widely adopted resource-conserving {{technologies}} in the rice-wheat systems in northern India. In areas of Haryana with rice-wheat systems, 36. 5 percent of all farmers practice ZT on 35 percent of their wheat area. Yet the literature measuring the impact of ZT on farmers’ fields is scarce. This study fills this gap by using the data collected from a random sample of 717 farmers from 50 villages in 10 districts of Haryana. It applies the difference-in-differences method <b>to</b> five-year <b>recall</b> <b>data</b> on wheat yields in ZT and conventionally tilled plots of land to quantify the crop loss due to unseasonal rains right before wheat harvests in March 2015. The results reveal significantly lower wheat yield losses in the ZT plots than in the conventionally tilled plots. On average, farmers suffered yield losses ranging between 3. 73 and 4. 53 quintals per hectare in 2015 due to unseasonal rains. The loss was lower by 1. 05 – 1. 10 quintals per hectare in ZT plots. The analysis clearly shows that adoption of ZT helped in reducing crop loss in wheat by 24 – 28 percent, valued at 1, 523 – 1, 595 Indian rupees (Rs.) per hectare (approximately US$ 22. 50 per hectare). The loss avoided due to ZT is nearly equal to the prevailing rental rate of the ZT machine (Rs. 1, 500 per hectare) in Haryana. Climate models suggest that the incidence of short-duration acute hydro meteorological events is likely to increase in years to come. Such events are hard to predict and prepare for, and dealing with them hinges mainly on disaster relief. However, our results show that adoption of ZT is one possible way to reduce potential loss from some of these weather events and that ZT is therefore well characterized as a climate-smart technology. Discussion paperNon-PRIFPRI 1; CRP 7; A Ensuring Sustainable food production; D Transforming Agriculture; E Building Resilience; F Strengthening institutions and governanceSAOCGIAR Research Program on Climate Change, Agriculture and Food Security (CCAFS...|$|R
40|$|To {{test the}} {{hypothesis}} that white potatoes (WP), oven-baked fries (OBF), and french fries (FF) contribute important nutrients within energy needs to children 2 ̆ 7 s and adolescents 2 ̆ 7 diets, secondary analysis of 24 -hour dietary <b>recall</b> <b>data</b> from the National Health and Nutrition Examination Survey 2003 - 2006 was conducted. Potato content of survey foods was determined using US Department of Agriculture recipe databases (Standard Reference (SR) -Link files). Nutrient content of potatoes was determined by linking SR codes to US Department of Agriculture food composition data. Daily nutrient intakes from potatoes were determined by applying the composition database to respondent 2 ̆ 7 s <b>recall</b> <b>data.</b> Sample-weighted data were analyzed; t tests assessed differences between age and sex groups. Results indicated that approximately 35...|$|R
40|$|Surveys in many {{academic}} fields ask respondents <b>to</b> <b>recall</b> {{the number}} of events that occurred over a specific period {{of time with the}} goal of learning about the mean frequency of these events among the population. Research has shown that the choice of the recall period, particularly the length, affects the results by influencing the cognitive recall process. We combine experimental <b>recall</b> <b>data</b> with use data to learn about this relationship in the context of consumer payments, specifically for the mean frequency of use of the four most popular payment instruments (cash, credit card, debit card, check). Overall, our analysis suggests that day-based recall is inefficient, with mean-squared errors of population estimates minimized for longer recall periods, although the optimal recall period differs among payment instruments. In addition, for cash, we develop a model relating <b>recalled</b> values <b>to</b> individual frequency of use in order to study the relationship between demographic variables and accuracy at different recall lengths. We find little link between demographic characteristics and accuracy of different recall periods for an individual...|$|R
40|$|Using {{recall and}} diary food {{expenditure}} data from Canada, we compare {{estimates of the}} household size elasticity of per capita food expenditure. In contrast to Gibson (2002), we find negative elasticities in both <b>recall</b> and diary <b>data.</b> This in turn means we find evidence of the “Deaton–Paxson puzzle” in both diary and <b>recall</b> <b>data.</b> <b>Recall</b> error cannot be the sole explanation of the puzzle...|$|R
