10|59|Public
50|$|Recent events (increase in {{processor}} speed, {{lower cost}} <b>terabyte</b> <b>disk</b> storage, RAID drive functionality, etc.) {{have made it}} easier to store the ISO files directly to hard drive, in effect creating large-scale DVD media servers. Playback of these ISO files can be done directly on a network connected computer running a virtual DVD ROM emulator (like Daemon Tools Lite), or even through open source media systems (like XBMC).|$|E
50|$|With {{the recent}} cost {{decreases}} in hard disk drives, oral archivists are considering moving {{many of their}} popular holdings to permanent storage in a server farm. For example, a single <b>terabyte</b> <b>disk</b> drive costing under $100 USD can hold 1,900 hours of uncompressed audio. A CD-R by contrast can only hold 76 minutes of uncompressed audio. Disk drive array cards such as the 3ware 9650SE can field 8TB of redundantly protected data in a standard PC case. One of the big advantages of doing this is that as the servers age and are retired, the files can simply be copied to newer, larger replacement servers making hardware obsolescense {{a thing of the}} past.|$|E
40|$|Digital Store ” {{described}} {{our efforts}} to encode, store, and allow easy access to all of a person’s information for personal and professional use [1]. The goals included understanding the effort to digitize a lifetime of legacy content {{and the elimination of}} paper as a permanent storage medium. We used Gordon Bell’s document archive as well as his current activities as a vehicle for this research. It was presumed that an emerging <b>terabyte</b> <b>disk</b> would hold a lifetime of accumulated information of a moderately active professional person. This article describes the project’s progress, insights, and surprises over the last five years. From the original plan of simply storing files of scanned papers, we evolved a concept of what the PC of the future should look like as we developed the SQLbased MyLifeBits platform...|$|E
5000|$|Computer hardware: Hitachi {{introduced}} the world's first one <b>terabyte</b> hard <b>disk</b> drive in 2007.|$|R
40|$|Abstract. The Informedia Digital Video Library {{contains}} over {{a thousand}} hours of video, consuming over a <b>terabyte</b> of <b>disk</b> space. This paper summarizes the multimedia abstractions used to represent this video in prior systems and introduces the visualization techniques employed to browse and navigate multiple video documents at once...|$|R
40|$|In this paper, {{we report}} on our recent {{progress}} {{on the development of}} super-resolution optical disk for ultra-high density optical data storage. A novel optical two-beam technique compatible with a standard optical drive is applied in the recording and reading process to enable the 10 <b>Terabytes</b> optical <b>disk</b> development...|$|R
40|$|Abstract—As disk volume grows rapidly with <b>terabyte</b> <b>disk</b> {{becoming}} a norm, RAID reconstruction process {{in case of}} a failure takes prohibitively long time. This paper presents a new RAID architecture, S 2 -RAID, allowing the disk array to reconstruct very quickly {{in case of a}} disk failure. The idea is to form skewed sub-arrays in the RAID structure so that reconstruction can be done in parallel dramatically speeding up data reconstruction process and hence minimizing the chance of data loss. We analyse the data recovery ability of this architecture and show its good scalability. A prototype S 2 -RAID system has been built and implemented in the Linux operating system for the purpose of evaluating its performance potential. Real world I/O traces including SPC, Microsoft, and a collection of a production environment have been used to measure the performance of S 2 -RAID as compared to existing baseline software RAID 5, Parity Declustering, and RAID 50. Experimental results show that our new S 2 -RAID speeds up data reconstruction time by a factor 2 to 4 compared to the traditional RAID. Meanwhile, S 2 -RAID keeps comparable production performance to that of the baseline RAID layouts while online RAID reconstruction is in progress...|$|E
40|$|Abstract [...] As disk volume grows rapidly with <b>terabyte</b> <b>disk</b> {{becoming}} a norm, RAID reconstruction time {{in case of}} a failure takes prohibitively long time. This paper presents a new RAID architecture, S 2 -RAID, allowing the disk array to reconstruct very quickly {{in case of a}} disk failure. The idea is to form skewed sub RAIDs (S 2 -RAID) in the RAID structure so that reconstruction can be done in parallel dramatically speeding up data reconstruction time and hence minimizing the chance of data loss. To make such parallel reconstruction conflict-free, each sub-RAID is formed by selecting one logic partition from each disk group with size being a prime number. We have implemented a prototype S 2 -RAID system in Linux operating system for the purpose of evaluating its performance potential. SPC IO traces and standard benchmarks have been used to measure the performance of S 2 -RAID as compared to existing baseline software RAID, MD. Experimental results show that our new S 2 -RAID speeds up data reconstruction time by a factor of 3 to 6 compared to the traditional RAID. At the same time, S 2 -RAID shows similar or better production performance than baseline RAID while online RAID reconstruction is in progress. ...|$|E
40|$|The NASA/NOAA/AMS Earth Science Electronic Theater {{presents}} Earth science {{observations and}} visualizations {{in a historical}} perspective. Fly in from outer space to Florida and the KSC Visitor's Center. Go {{back to the early}} weather satellite images from the 1960 s see them contrasted with the latest International global satellite weather movies including killer hurricanes & tornadic thunderstorms. See the latest spectacular images from NASA and NOAA remote sensing missions like GOES, NOAA, TRMM, SeaWiFS, Landsat 7, & new Terra which will be visualized with state-of-the art tools. Shown in High Definition TV resolution (2048 x 768 pixels) are visualizations of hurricanes Lenny, Floyd, Georges, Mitch, Fran and Linda. See visualizations featured on covers of magazines like Newsweek, TIME, National Geographic, Popular Science and on National & International Network TV. New Digital Earth visualization tools allow us to roam & zoom through massive global images including a Landsat tour of the US, with drill-downs into major cities using 1 m resolution spy-satellite technology from the Space Imaging IKONOS satellite, Spectacular new visualizations of the global atmosphere & oceans are shown. See massive dust storms sweeping across Africa. See ocean vortexes and currents that bring up the nutrients to feed tiny plankton and draw the fish, giant whales and fisherman. See the how the ocean blooms in response to these currents and El Nino/La Nina climate changes. The demonstration is interactively driven by a SGI Octane Graphics Supercomputer with dual CPUs, 5 Gigabytes of RAM and <b>Terabyte</b> <b>disk</b> using two projectors across the super sized Universe Theater panoramic screen...|$|E
40|$|The Informedia Digital Video Library {{contains}} over {{a thousand}} hours of video, consuming over a <b>terabyte</b> of <b>disk</b> space. This paper summarizes the multimedia abstractions used to represent this video in prior systems and introduces the visualization techniques employed to browse and navigate multiple video documents at once...|$|R
5000|$|The Internet 1996 World Exposition is a web site, {{distributed}} on 8 servers {{around the}} world in a [...] "public park for the global village," [...] which has received 5 million visitors from 130 countries. In-kind contributions from sponsors included the first DS3 over the Pacific Ocean and two <b>terabytes</b> of <b>disk</b> drives.|$|R
2500|$|However, {{this does}} not mean that a MS Jet (Red) {{database}} cannot match MS SQL Server in storage capacity. [...] A 5 billion record MS Jet (Red) database with compression and encryption turned on requires about 1 <b>terabyte</b> of <b>disk</b> storage space, comprising hundreds of (*.mdb) files, each acting as partial table, and not as a database in itself.|$|R
40|$|The NASA/NOAA/AMS Earth Science Electronic Theater {{presents}} Earth science {{observations and}} visualizations {{in a historical}} perspective. Fly in from outer space to Florida and the KSC Visitor's Center. Go {{back to the early}} weather satellite images from the 1960 s see them contrasted with the latest International global satellite weather movies including killer hurricanes and tornadic thunderstorms. See the latest spectacular images from NASA and the National Oceanic and Atmospheric Administration (NOAA) remote sensing missions like the Geostationary Operational Environmental Satellites (GOES), NOAA, Tropical Rainfall Measuring Mission (TRMM), SeaWiFS, Landsat 7, and new Terra which will be visualized with state-of-the art tools. Shown in High Definition TV resolution (2048 x 768 pixels) are visualizations of hurricanes Lenny, Floyd, Georges, Mitch, Fran, and Linda. See visualizations featured on covers of magazines like Newsweek, TIME, National Geographic, Popular Science, and on National and International Network TV. New Digital Earth visualization tools allow us to roam and zoom through massive global images including a Landsat tour of the US, with drill-downs into major cities using one meter resolution spy-satellite technology from the Space Imaging IKONOS satellite. Spectacular new visualizations of the global atmosphere and oceans are shown. See massive dust storms sweeping across Africa. See ocean vortexes and currents that bring up the nutrients to feed tiny plankton and draw the fish, giant whales and fisherman. See the how the ocean blooms in response to these currents and El Nino/La Nina climate changes. The demonstration is interactively driven by a SGI Octane Graphics Supercomputer with dual CPUs, 5 Gigabytes of RAM and <b>Terabyte</b> <b>disk</b> using two projectors across the super sized Universe Theater panoramic screen...|$|E
40|$|The Etheater {{presents}} visualizations which {{span the}} period from the original Suomi/Hasler animations of the first ATS- 1 GEO weather satellite images in 1966, to the latest 1999 NASA Earth Science Vision for the next 25 years. Hot off the SGI-Onyx Graphics-Supercomputer are NASA''s visualizations of Hurricanes Mitch, Georges, Fran and Linda. These storms have been recently featured {{on the covers of}} National Geographic, Time, Newsweek and Popular Science. Highlights will be shown from the NASA hurricane visualization resource video tape that has been used repeatedly this season on National and International network TV. Results will be presented from a new paper on automatic wind measurements in Hurricane Luis from 1 -min GOES images that appeared in the November BAMS. The visualizations are produced by the NASA Goddard Visualization & Analysis Laboratory, and Scientific Visualization Studio, as well as other Goddard and NASA groups using NASA, NOAA, ESA, and NASDA Earth science datasets. Visualizations will be shown from the Earth Science ETheater 1999 recently presented in Tokyo, Paris, Munich, Sydney, Melbourne, Honolulu, Washington, New York, and Dallas. The presentation Jan 11 - 14 at the AMS meeting in Dallas used a 4 -CPU SGI/CRAY Onyx Infinite Reality Super Graphics Workstation with 8 GB RAM and a <b>Terabyte</b> <b>Disk</b> at 3840 X 1024 resolution with triple synchronized BarcoReality 9200 projectors on a 60 ft wide screen. Visualizations will also be featured from the new Earth Today Exhibit which was opened by Vice President Gore on July 2, 1998 at the Smithsonian Air & Space Museum in Washington, as well as those presented for possible use at the American Museum of Natural History (NYC), Disney EPCOT, and other venues. New methods are demonstrated for visualizing, interpreting, comparing, organizing and analyzing immense HyperImage remote sensing datasets and three dimensional numerical model results. We call the data from many new Earth sensing satellites, HyperImage datasets, because they have such high resolution in the spectral, temporal, spatial, and dynamic range domains. The traditional numerical spreadsheet paradigm has been extended to develop a scientific visualization approach for processing HyperImage datasets and 3 D model results interactively. The advantages of extending the powerful spreadsheet style of computation to multiple sets of images and organizing image processing were demonstrated using the Distributed Image SpreadSheet (DISS). The DISS is being used as a high performance testbed Next Generation Internet (NGI) VisAnalysis of: 1) El Nino SSTs and NDVI response 2) Latest GOES 10 5 -min rapid Scans of 26 day 5000 frame movie of March & April 198 weather and tornadic storms 3) TRMM rainfall and lightning 4) GOES 9 satellite images/winds and NOAA aircraft radar of hurricane Luis, 5) lightning detector data merged with GOES image sequences, 6) Japanese GMS, TRMM, & ADEOS data 7) Chinese FY 2 data 8) Meteosat & ERS/ATSR data 9) synchronized manipulation of multiple 3 D numerical model views; etc. will be illustrated. The Image SpreadSheet has been highly successful in producing Earth science visualizations for public outreach...|$|E
40|$|The Electronic Theater (E-theater) {{presents}} visualizations which {{span the}} period from the original Suomi/Hasler animations of the first ATS- 1 GEO weather satellite images in 1966 to the latest 1999 NASA Earth Science Vision for the next 25 years. Hot off the SGI-Onyx Graphics-Supercomputer are NASA's visualizations of Hurricanes Mitch, Georges, Fran and Linda. These storms have been recently featured {{on the covers of}} National Geographic, Time, Newsweek and Popular Science. Highlights will be shown from the NASA hurricane visualization resource video tape that has been used repeatedly this season on National and International network TV. Results will be presented from a new paper on automatic wind measurements in Hurricane Luis from 1 -min GOES images that appeared in the November BAMS. The visualizations are produced by the NASA Goddard Visualization and Analysis Laboratory (VAL/ 912), and Scientific Visualization Studio (SVS/ 930), as well as other Goddard and NASA groups using NASA, NOAA, ESA, and NASDA Earth science datasets. Visualizations will be shown from the Earth Science E-Theater 1999 recently presented in Tokyo, Paris, Munich, Sydney, Melbourne, Honolulu, Washington, New York, and Dallas. The presentation Jan 11 - 14 at the AMS meeting in Dallas used a 4 -CPU SGI/CRAY Onyx Infinite Reality Super Graphics Workstation with 8 GB RAM and a <b>Terabyte</b> <b>Disk</b> at 3840 X 1024 resolution with triple synchronized BarcoReality 9200 projectors on a 60 ft wide screen. Visualizations will also be featured from the new Earth Today Exhibit which was opened by Vice President Gore on July 2, 1998 at the Smithsonian Air & Space museum in Washington, as well as those presented for possible use at the American Museum of Natural History (NYC), Disney EPCOT, and other venues. New methods are demonstrated for visualizing, interpreting, comparing, organizing and analyzing immense HyperImage remote sensing datasets and three dimensional numerical model results. We call the data from many new Earth sensing satellites, HyperImage datasets, because they have such high resolution in the spectral, temporal, spatial, and dynamic range domains. The traditional numerical spreadsheet paradigm has been extended to develop a scientific visualization approach for processing HyperImage datasets and 3 D model results interactively. The advantages of extending the powerful spreadsheet style of computation to multiple sets of images and organizing image processing were demonstrated using the Distributed image SpreadSheet (DISS). The DISS is being used as a high performance testbed Next Generation Internet (NGI) VisAnalysis of: 1) El Nino SSTs and NDVI response 2) Latest GOES 10 5 -min rapid Scans of 26 day 5000 frame movie of March & April ' 98 weather and tornadic storms 3) TRMM rainfall and lightning 4) GOES 9 satellite images/winds and NOAA aircraft radar of hurricane Luis, 5) lightning detector data merged with GOES image sequences, 6) Japanese GMS, TRMM, & ADEOS data 7) Chinese FY 2 data 8) Meteosat & ERS/ATSR data 9) synchronized manipulation of multiple 3 D numerical model views; and others will be illustrated. The Image SpreadSheet has been highly successful in producing Earth science visualizations for public outreach. Many of these visualizations have been widely disseminated through the world wide web pages of the HPCC/LTP/RSD program which can be found at [URL] The one min interval animations of Hurricane Luis on ABC Nightline and the color perspective rendering of Hurricane Fran published by TIME, LIFE, Newsweek, Popular Science, National Geographic, Scientific American, and the "Weekly Reader" are some of the examples which will be shown...|$|E
5000|$|As of 2007, Seagate {{believed}} it could produce 300 terabit (37.5 <b>terabyte)</b> Hard <b>disk</b> drives using HAMR technology. Some news sites erroneously reported that Seagate would launch a 300 TB HDD by 2010. Seagate {{responded to this}} news stating that 50 terabit per-square-inch density is well past the 2010 timeframe and that this may also involve a combination of Bit Patterned Media.|$|R
5000|$|However, {{this does}} not mean that a MS Jet (Red) {{database}} cannot rival MS SQL Server in storage capacity. A 5 billion record, ODBC-enabled, MS Jet (Red) database with compression and encryption turned on requires about 1 <b>terabyte</b> of <b>disk</b> storage space, comprising hundreds of (*.mdb) files, each acting as a partial table, and not as a database in itself.|$|R
50|$|Bluefire {{uses the}} POWER6 microprocessor, each having a clock speed of 4.7 gigahertz. The system {{consists}} of 4,064 such processors, 12 terabytes of memory, and 150 <b>terabytes</b> of DS4800 <b>disk</b> storage.|$|R
40|$|Details of {{the science}} stories and {{scientific}} results behind the Etheater Earth Science Visualizations from the major remote sensing institutions around {{the country will be}} explained. The NASA Electronic Theater presents Earth science observations and visualizations in a historical perspective. Fly in from outer space to Temple Square and the University of Utah Campus. Go back to the early weather satellite images from the 1960 s see them contrasted with the latest US/Europe/Japan global weather data. See the latest images and image sequences from NASA & NOAA missions like Terra, GOES, NOAA, TRMM, SeaWiFS, Landsat 7 visualized with state-of-the art tools. A similar retrospective of numerical weather models from the 1960 s will be compared with the latest "year 2002 " high-resolution models. See the inner workings of a powerful hurricane as it is sliced and dissected using the University of Wisconsin Vis- 5 D interactive visualization system. The largest super computers are now capable of realistic modeling of the global oceans. See ocean vortexes and currents that bring up the nutrients to feed phitoplankton and zooplankton as well as draw the crill fish, whales and fisherman. See the how the ocean blooms in response to these currents and El Nino/La Nina climate regimes. The Internet and networks have appeared while computers and visualizations have vastly improved over the last 40 years. These advances make it possible to present the broad scope and detailed structure of the huge new observed and simulated datasets in a compelling and instructive manner. New visualization tools allow us to interactively roam & zoom through massive global images larger than 40, 000 x 20, 000 pixels. Powerful movie players allow us to interactively roam, zoom & loop through 4000 x 4000 pixel bigger than HDTV movies of up to 5000 frames. New 3 D tools allow highly interactive manipulation of detailed perspective views of many changing model quantities. See the 1 m resolution before and after shots of lower Manhattan and the Pentagon after the September 11 disaster as well as shots of Afghanistan from the Space Imaging IKONOS as well as debris plume images from Terra MODIS and SPOT Image. Shown by the SGI-Octane Graphics-Supercomputer are visualizations of hurricanes Michelle 2001, Floyd, Mitch, Fran and Linda. Our visualizations of these storms have been featured on the covers of the National Geographic, Time, Newsweek and Popular Science. Highlights will be shown from the NASA's large collection of High Definition TV (HDTV) visualizations clips New visualizations of a Los Alamos global ocean model, and high-resolution results of a NASA/JPL Atlantic ocean basin model showing currents, and salinity features will be shown. El Nino/La Nina effects on sea surface temperature and sea surface height of the Pacific Ocean will also be shown. The SST simulations will be compared with GOES Gulf Stream animations and ocean productivity observations. Tours will be given of the entire Earth's land surface at 500 m resolution from recently composited Terra MODIS data, Visualizations will be shown from the Earth Science Etheater 2001 recently presented over the last years in New Zealand, Johannesburg, Tokyo, Paris, Munich, Sydney, Melbourne, Honolulu, Washington, New York City, Pasadena, UCAR/Boulder, and Penn State University. The presentation will use a 2 -CPU SGI/CRAY Octane Super Graphics workstation with 4 GB RAM and <b>terabyte</b> <b>disk</b> array at 2048 x 768 resolution plus multimedia laptop with three high resolution projectors. Visualizations will also be featured from museum exhibits and presentations including: the Smithsonian Air & Space Museum in Washington, IMAX theater at the Maryland Science Center in Baltimore, the James Lovell Discovery World Science museum in Milwaukee, the American Museum of Natural History (NYC) Hayden Planetarium IMAX theater, etc. The Etheater is sponsored by NASA, NOAA and the American Meteorological Society. This presentation {{is brought to you by}} the University of Utah College of Mines and Earth Sciences and, the Utah Museum of Natural History...|$|E
25|$|In 2003, {{the fastest}} {{computer}} in Canada, {{which is used}} by the Department of Astronomy and Astrophysics at the University of Toronto, was named after Bob and Doug. The $900,000 computer is being used to simulate supermassive black holes and collisions of galaxies. The machine, nicknamed McKenzie, has 268 gigabytes of memory and 40 <b>terabytes</b> of <b>disk</b> space, and consists of two master nodes (Bob and Doug), 256 compute nodes, and eight development nodes.|$|R
40|$|A new {{parallel}} search algorithm {{running on a}} large computer cluster solves a popular board game by efficiently computing the best moves from all reachable positions. As such, the algorithm uses the main memories for frequently and randomly accessed data and stores terbytes of less frequently acessed intermediate results on disks. All processors repeatably inform each other about positions' intermediate values, generating more than a petabit of interprocessor communication as well as <b>terabytes</b> of <b>disk</b> I/O...|$|R
50|$|Watson {{had access}} to 200 million pages of {{structured}} and unstructured content consuming four <b>terabytes</b> of <b>disk</b> storage including {{the full text of}} Wikipedia, but was not connected to the Internet during the game. For each clue, Watson's three most probable responses were displayed on the television screen. Watson consistently outperformed its human opponents on the game's signaling device, but had trouble in a few categories, notably those having short clues containing only a few words.|$|R
40|$|Abstract — In today’s Internet, BGP is {{extremely}} chatty — the most minor connectivity change produces hundreds of updates {{and a significant}} peering loss can generate millions. While gigahertz processors and <b>terabyte</b> <b>disks</b> {{have made it possible}} to capture and record BGP events via passive peering, making sense of the deluge of data remains difficult. We have developed statistical algorithms to extract the large-scale structure of BGP event streams and visualization techniques to display that structure in operationally meaningful ways, i. e., to quickly answer questions like “what happened?”, “where did it happen? ” and “how does it affect me?”. These tools {{can also be used to}} provide real-time views of an ISP’s or an enterprise’s interdomain topology that help rapidly diagnose problems like misconfigured community tags, policy filters with unintended consequences, unexpected or unwanted leaked paths, route oscillations, peering traffic imbalance, etc. The analysis is fast enough to run in real time on a modern processor even when dealing with, for example, the entire backbone mesh of a typical Tier- 1 ISP. We will describe the algorithms and show case studies from variety of data taken on both a large ISP backbone and a large institutional network. I...|$|R
50|$|The Coates cluster {{consists}} of 982 64-bit, 8-core HP Proliant DL165 G5p and 11 64-bit, 16-core HP Proliant DL585 G5 systems using AMD 2380 and AMD 8380 processors with various combinations of 16-128 gigabytes of RAM, 500 GB to 2 <b>terabytes</b> of <b>disk</b> and 10 Gigabit Ethernet (10GigE) local to each node. Coates uses Cisco and Chelsio network equipment. The cluster's nodes are arrayed in five logical sub-clusters each with different memory and storage configurations {{designed to meet}} the varying needs of the researchers using Coates.|$|R
50|$|It was a {{computer}} cluster based on IBM's commercial RS/6000 SP computer. 512 of these machines were connected together for ASCI White, with 16 processors per node and 8,192 375 MHz processors in total with 6 terabytes {{of memory and}} 160 <b>terabytes</b> of <b>disk</b> storage. It was almost exclusively used for large-scale computations requiring dozens, {{hundreds or thousands of}} processors. The computer weighed 106 tons and consumed 3 MW of electricity with a further 3 MW needed for cooling. It had a theoretical processing speed of 12.3 teraflops. The system ran IBM's AIX operating system.|$|R
50|$|Standard {{interior}} {{features include}} a 20-speaker Toyota Premium Sound system, a 1 <b>Terabyte</b> Hard <b>disk</b> drive navigation system, XM NavTraffic, a real-time traffic monitoring system with dynamic rerouting; Keyless SmartAccess with push-button start, Optitron instrument panel, auxiliary MP3 and MP4 player inputs, and a 5-position tire pressure display. A 24-hour concierge/emergency aid service, with the analogous G-Link navigation system is offered in Japan. Japanese market Crown sedans also feature MiniDisc compatibility, television reception, on board security surveillance cameras, and remote cellphone access. The seats are fully electric, {{and have a}} massage feature.|$|R
50|$|Many of the backup-to-disk {{technologies}} advertise up to 15 to 1 compression ratios. This {{also allows}} {{the information technology}} department to store more data on less disk space. With de-duplication a disk appliance with 5 <b>terabytes</b> of raw <b>disk</b> space can store as much as 30 terabytes of compressed and de-duplicated data.|$|R
40|$|The Alpha 21364 {{processor}} {{provides a}} high-performance, highly scalable, and highly reliable network urchitecture. The router runs at I. 2 GHz and routes packets at a peak bandwidth of 22. 4 GB/s. The network architecture scales up to a 128 -processor configuration, which can support {{up to four}} terabytes of distributed Rambus memory and hundreds of <b>terabytes</b> of <b>disk</b> storage. The distributed Rambus memory is kept coherent viu a scalable, directory-based, cache coherence scheme. The network also provides a variety of reliability features, such as per-flit ECC. These features make the 21364 network architecture well-suited to support communication-intensive server applications. 1...|$|R
50|$|The Paragon {{architecture}} soon {{led to the}} Intel ASCI Red supercomputer in the United States, {{which held}} the top supercomputing spot {{to the end of}} the 20th century as part of the Advanced Simulation and Computing Initiative. This was also a mesh-based MIMD massively-parallel system with over 9,000 compute nodes and well over 12 <b>terabytes</b> of <b>disk</b> storage, but used off-the-shelf Pentium Pro processors that could be found in everyday personal computers. ASCI Red was the first system ever to break through the 1 teraflop barrier on the MP-Linpack benchmark in 1996; eventually reaching 2 teraflops.|$|R
50|$|The first {{commercial}} digital disk storage device was the IBM 350 which shipped in 1956 {{as a part}} of the IBM 305 RAMAC computing system. The random-access, low-density storage of disks was developed to complement the already used sequential-access, high-density storage provided by tape drives using magnetic tape. Vigorous innovation in disk storage technology, coupled with less vigorous innovation in tape storage, has reduced the difference in acquisition cost per <b>terabyte</b> between <b>disk</b> storage and tape storage; however, the total cost of ownership of data on disk including power and management remains larger than that of tape.|$|R
50|$|Steele {{consisted}} of 893 64-bit, 8-core Dell PowerEdge 1950 and nine 64-bit, 8-core Dell PowerEdge 2950 systems with various combinations of 16-32 gigabytes RAM, 160 GB to 2 <b>terabytes</b> of <b>disk,</b> and Gigabit Ethernet and SDR InfiniBand to each node. The cluster had a theoretical peak performance {{of more than}} 60 teraflops. Steele and its 7,216 cores replaced the Purdue Lear cluster supercomputer which had 1,024 cores but was substantially slower. Steele is primarily networked utilizing a Foundry Networks BigIron RX-16 switch with a Tyco MRJ-21 wiring system delivering over 900 Gigabit Ethernet connections and eight 10 Gigabit Ethernet uplinks.|$|R
50|$|Verne a 5-node {{cluster of}} Dell R505 quad-socket/quad-core Opteron servers {{dedicated}} to data analysis and high-end visualization was retired in September 2010. Each node contained 16 processor cores, 128 gigabytes of memory, and 4 <b>terabytes</b> of local <b>disk</b> space. The {{primary purpose of}} Verne was to enable data analysis and visualization of simulation data generated on Kraken.|$|R
30|$|In July of 2016, it arose an {{opportunity}} to use a supercomputer of COPPE/UFRJ. Lobo Carneiro (LOBOC), {{in honor of the}} late professor Fernando Luiz Lobo Barboza Carneiro (1913 – 2001), is a supercomputer with the capacity of 226 teraflops, 16 Tbytes of RAM and 720 <b>terabytes</b> of <b>disk,</b> capable of running 226 trillion of mathematical operations per second. With these features available, the single thread limitation no longer exists (memory limitation over one single thread of NodeJs), and the fault handling returned to the previous state without having to split the Optimizer tasks among multiple clients (simpler and faster). It allowed us to conduct experiments with larger libraries (in lines of code) in a much shorter period.|$|R
40|$|For {{the first}} time in {{computational}} group theory, parallel disk-based search algorithms are used. These algorithms emphasize streaming access while avoiding the high latency of disk. Search and enumeration is a recurring theme throughout computational group theory. Examples include orbit computation, permutation group membership (permutation group order, random element of a permutation group,etc.), condensation of matrix representations, generation of permutation representations from matrix representations, group intersection, centralizer, and normalizer. Without the use of disk, larger computations of this type would be infeasible. An analysis of these algorithms is presented. Formulas with application-specific parameters are derived that successfully predict each of their run times. Armed with these algorithms, we compute a permutation representation of the Baby Monster sporadic simple group using 8 <b>terabytes</b> of distributed <b>disk.</b> In addition, we compute a matrix condensation of Fischer’s group F i 23 using 2 <b>terabytes</b> of distributed <b>disk.</b> Finally, we present and implement a library with a well-developed API that allows computational group theory programmers to easily use the search algorithms presented in this paper...|$|R
40|$|This {{research}} {{addresses a}} major gap in our conceptual understanding of synaptic and brain-like network dynamics. Over {{the course of}} several years we have designed and implemented increasingly complex and powerful brain-like simulators which apply recent advances in computer and networking technology towards the goal of understanding brain function in terms of pulse-coded information networks. These simulations have been run on increasingly powerful clusters of computers. Currently we have a cluster of 128 processors with a total of 256 GB of RAM and more than a <b>Terabyte</b> of <b>disk</b> storage, interconnected with a Myrinet 2000 highspeed/low-latency interconnection network. On this cluster we are able to run simulations on the order of 3 million synapses per processor, with the capability of receiving stimulus input from remote devices. ...|$|R
50|$|Built by NEC, ES {{was based}} on their SX-6 architecture. It {{consisted}} of 640 nodes with eight vector processors and 16 gigabytes of computer memory at each node, {{for a total of}} 5120 processors and 10 terabytes of memory. Two nodes were installed per 1 metre × 1.4 metre × 2 metre cabinet. Each cabinet consumed 20 kW of power. The system had 700 <b>terabytes</b> of <b>disk</b> storage (450 for the system and 250 for the users) and 1.6 petabytes of mass storage in tape drives. It was able to run holistic simulations of global climate in both the atmosphere and the oceans down to a resolution of 10 km. Its performance on the LINPACK benchmark was 35.86 TFLOPS, which was almost five times faster than the previous fastest supercomputer, ASCI White.|$|R
