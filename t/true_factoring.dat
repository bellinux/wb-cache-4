1|319|Public
5000|$|The {{starting}} point of (<b>true)</b> <b>factoring</b> is a long-term contractual arrangement between the municipality and a private person, {{on the basis of}} which the private person provides a public service. Due to the assured flow of payments (payment according to time) this model could in principle be considered for all PSPPs. The factoring consists in the further stage in which the private operator / owner sells a part of the future income from the contract with the state at cash value (minus a commission) to a factoring institute and uses the price to finance the project, so that only a low level of owner’s equity is needed. (see Kirchhoff 1995). This is especially useful for medium-sized enterprises, in order to compete with the strongly capitalized large corporations.|$|E
40|$|This paper {{examines}} {{the use of}} proxies (or reference variables) for the <b>true</b> <b>factors</b> in the Arbitrage Pricing Theory (APT). It generalises the work of Reisman(1992) and Shanken(1992) and shows that, when there are more reference variables than the <b>true</b> <b>factors,</b> the APT still holds. The possibility of fewer reference variables than the <b>true</b> <b>factors</b> is also considered, but the APT is not shown to hold, in the same sense, for this case. This work builds on an earlier paper by Ingersoll(1984), and our propositions {{can be thought of}} as specialisations of his Theorems 1 and 6. Our work does not use the mathematics of Hilbert and Banach spaces (as used by Reisman(1992)) and, thus, is open to a much wider audience. Deriving the APT when the Number of Factors is Unknown...|$|R
5000|$|Underfactoring {{occurs when}} too few factors are {{included}} in a model. This {{is considered to be}} a greater error than overfactoring. If not enough factors {{are included in}} a model, there is likely to be substantial error. Measured variables that load onto a factor not included in the model can falsely load on factors that are included, altering <b>true</b> <b>factor</b> loadings [...] This can result in rotated solutions in which two factors are combined into a single <b>factor,</b> obscuring the <b>true</b> <b>factor</b> structure.|$|R
40|$|This paper {{examines}} {{the use of}} proxies (or reference variables) for the <b>true</b> <b>factors</b> in the arbitrage pricing theory (APT). It generalizes other authors' existing work and shows that, when there are more reference variables than the <b>true</b> <b>factors,</b> the APT still holds. The possibility of fewer reference variables than the <b>true</b> <b>factors</b> is also considered, but the APT is not shown to hold, in the same sense, for this case. This work builds on an earlier paper by Ingersoll (Ingersoll J 1984 J. �Finance 39 1021 - 39), and our propositions {{can be thought of}} as specializations of his theorems. Similar to Nawalkha (Nawalkha S 1997 J. Financial Economics 46 357 - 81), our work does not use the mathematics of Hilbert and Banach spaces and, thus, is open to a much wider audience. The practical implication of our results is that model builders should be generous with the number of factors they use, as excessively parsimonious models suffer from inaccuracy. ...|$|R
40|$|AbstractWe {{construct}} the large sample distributions of the OLS and GLS R 2 ’s {{of the second}} pass regression of the Fama and MacBeth (1973) two pass procedure when the observed proxy factors are minorly correlated with the <b>true</b> unobserved <b>factors.</b> This implies an unexplained factor structure in the first pass residuals and, consequently, a large estimation error in the estimated beta’s which is spanned by the beta’s of the unexplained <b>true</b> <b>factors.</b> The average portfolio returns and the estimation error of the estimated beta’s are then both linear in the beta’s of the unobserved <b>true</b> <b>factors</b> which leads to possibly large values of the OLS R 2 of the second pass regression. These large values of the OLS R 2 are not indicative {{of the strength of}} the relationship. Our results question many empirical findings that concern the relationship between expected portfolio returns and (macro-) economic factors...|$|R
40|$|Under the errors-in-variables parameterization, the {{limiting}} {{behavior of the}} estimators of {{the parameters of the}} factor analysis model is investigated. An explicit expression is given for the covariance matrix of {{the limiting}} distribution of the estimators. It is demonstrated that the limiting distribution of the vector containing the estimated error variances and the estimated coefficients holds {{for a wide range of}} assumptions about the <b>true</b> <b>factors.</b> factor analysis covariance matrix of limiting distribution maximum likelihood estimator...|$|R
40|$|Ankara : The Department of Economics, İhsan Doğramacı Bilkent University, 2013. Thesis (Master's) [...] Bilkent University, 2013. Includes bibliographical refences. In this thesis, we adopted Elastic Net estimators for {{selecting}} <b>true</b> number of <b>factors</b> in factor models with stationary and nonstationary factors. Elastic Net {{is a member}} of shrinkage estimators family. As a member of shrinkage estimators family, elastic net estimators are stable to changes in data and in general they do not over parametrize the models. These two properties of elastic net estimators makes elastic net more favourable than information based criterion penalty methods for estimating <b>true</b> <b>factor</b> number. Since Principal Components Analysis (PCA) based algorithms always tends to give only single factor for nonstationary data sets, we use Sparse Principal Components Analysis (SPCA) algorithm which is a regression-type optimization formulation of PCA. Simulations show the performance of Elastic Net estimator for estimation of <b>true</b> <b>factor</b> number with stationary and nonstationary factors cases. Konak, DenizM. S...|$|R
50|$|A more {{rigorous}} approach to slope stability analysis is limit analysis. Unlike limit equilibrium analysis which makes ad-hoc though often reasonable assumptions, limit analysis {{is based on}} rigorous plasticity theory. This enables, among other things, the computation of {{upper and lower bounds}} on the <b>true</b> <b>factor</b> of safety.|$|R
30|$|Since {{principal}} component analysis is the default method of extraction in many popular statistical software packages, including SPSS and SAS, which likely contributes to its popularity. Principal component analysis can produce similar results to <b>true</b> <b>factor</b> analysis when measurement reliability is high and/or the number of factored variables/items increases (Thompson 2004).|$|R
40|$|We {{construct}} the large sample distributions of the OLS and GLS R 2 ’s {{of the second}} pass regression of the Fama-MacBeth (1973) two pass procedure when the observed proxy factors are minorly correlated with the <b>true</b> unobserved <b>factors.</b> The small correlation implies a sizeable unexplained factor structure in the first pass residuals and, consequently, a sizeable estimation error in the estimated beta’s which is spanned by the beta’s of the unexplained <b>true</b> <b>factors.</b> The average portfolio returns and the estimation error of the estimated beta’s are then both linear in the beta’s of the unobserved <b>true</b> <b>factors</b> which leads to possibly large values of the OLS R 2 of the second pass regression. These large values of the OLS R 2 are not indicative {{of the strength of}} the relationship between the expected portfolio returns and the (macro-) economic factors. We propose an easy manner for diagnosing it using a statistic that reflects the unexplained factor structure in the first pass residuals. Similar arguments apply to the second pass t-statistic which are resolved using the identification robust factor statistics of Kleibergen (2009). Our results put into question many of the empirical findings that concern the relationship between expected portfolio returns and (macro-) economic factors. We discuss some prominent ones in passing...|$|R
40|$|AbstractUnder the errors-in-variables parameterization, the {{limiting}} {{behavior of the}} estimators of {{the parameters of the}} factor analysis model is investigated. An explicit expression is given for the covariance matrix of {{the limiting}} distribution of the estimators. It is demonstrated that the limiting distribution of the vector containing the estimated error variances and the estimated coefficients holds {{for a wide range of}} assumptions about the <b>true</b> <b>factors...</b>|$|R
40|$|Bilinear data {{matrices}} may {{be resolved}} into abstract factors by factor analysis. The underlying chemical processes that generated the data may be deduced from the abstract factors by hard (model fitting) or soft (model-free) analyses. We propose {{a novel approach}} that combines the advantages of both hard and soft methods, in {{that only a few}} parameters have to be fitted, but the assumptions made about the system are very general and common to a range of possible models: The <b>true</b> chemical <b>factors</b> span the same space as the abstract factors and may be mapped onto the abstract factors by a transformation matrix T, since they are a linear combination of the abstract factors. The difference between the <b>true</b> <b>factors</b> and any other linear combination of the abstract factors is that the <b>true</b> <b>factors</b> conform to known chemical constraints (for instance, nonnegativity of absorbance spectra or monomodality of chromatographic peaks). Thus, by excluding linear combinations of the abstract factors that are not physically possible (assuming a unique solution), we can find the <b>true</b> chemical <b>factors.</b> This is achieved by passing the elements of a transformation matrix to a nonlinear optimization routine, to find the best estimate of T that fits the criteria. The optimization routine usually converges to the correct minimum with random starting parameters, but more difficult problems require starting parameters based on some other method, for instance EFA. We call the new method resolving factor analysis (RFA). The use of RFA is demonstrated with computer-generated kinetic and chromatographic data and with real chromatographic (HPLC) data. RFA produces correct solutions with data sets that are refractory to other methods, for instance, data with an embedded nonconstant baseline...|$|R
40|$|Education {{and caring}} work {{are based on}} {{interpersonal}} relationships with high emotional involvement. Besides their scientific background of knowledge and competence, educators need to acquire self knowledge and affective maturity, which allow them to become <b>true</b> <b>factors</b> of personal growth. The phenomenological approach in professionals' education and training, permits to increase the "human" dimensions of education and to realize the care for onself as a necessary condition of the care for others...|$|R
50|$|Treatment {{algorithms}} rely {{on breast}} cancer classification to define specific subgroups that are each treated {{according to the}} best evidence available. Classification aspects must be carefully tested and validated, such that confounding effects are minimized, making them either <b>true</b> prognostic <b>factors,</b> which estimate disease outcomes such as disease-free or overall survival {{in the absence of}} therapy, or <b>true</b> predictive <b>factors,</b> which estimate the likelihood of response or lack of response to a specific treatment.|$|R
2500|$|The result when multiplied {{with the}} {{displacement}} power factor (DPF) is the overall, <b>true</b> power <b>factor</b> or just power factor (PF): ...|$|R
40|$|In {{order to}} {{discover}} whether Management by Objectives {{could be used}} to create optimal experience in the workplace, two tests were constructed. These were a measure of Management by Objectives (42 items) and a measure of Optimal Experience (24 items). First and second-order factor analyses were performed on both inventories to identify the <b>true</b> <b>factors.</b> Item analyses were performed to verify the reliability of both instruments. Pearson Product moment correlations were computed to assess the relationship between the constructs. The implications are discussed...|$|R
40|$|With {{their many}} benefits, Variable Speed Drives (VSDs) have grown rapidly in their usage in recent years. However, a unfortunate {{side effect of}} their usage is the {{introduction}} of harmonic distortion in the power system and reduction of <b>true</b> power <b>factor</b> (TPF). Low <b>true</b> power <b>factor</b> means poor electrical efficiency. The lower the <b>true</b> power <b>factor,</b> the higher the apparent power drawn from the distribution network. A filter connected at the input side of a VSD converter reduces energy losses in the power supply system by reducing harmonics and improving the <b>true</b> power <b>factor.</b> In this thesis, a typical three-phase rectifier is used as an example to present a methodology of designing such filters in technical and economic terms. The relationship between the cost of input filters and the energy cost reduction they provide on the power supply side of VSDs is discussed. By extending the same method to different loads, filters are designed and power savings are calculated. The pricing of relevant harmonic filters is also discussed. The basics of harmonics and the power factor are presented in the introduction. The summary of the results and suggestions for further research are contained in the last sections of the thesis...|$|R
5000|$|Chynn EW, Talamo JH, Seligman MS. Acanthamoeba keratitis: {{is water}} {{exposure}} a <b>true</b> risk <b>factor?</b> CLAO. 23(1): 55-56, 1997. [...] First article published on CLAO’s inaugural home page.|$|R
40|$|This paper {{states that}} the induced charge should not be {{neglected}} in the electric Aharonov-Bohm effect. If the induced charge is taken into account, the interference pattern of the moving charge will not change with the potential {{difference between the two}} metal tubes. It means that the scale potential itself can not affect the phase of the moving charge, and the <b>true</b> <b>factor</b> affecting the phase of the moving charge is the energy of the system including the moving charge and the induced charge. Comment: 12 pages, 1 figur...|$|R
40|$|We {{consider}} {{the problem of}} sparse estimation via a lasso-type penalized likelihood procedure in a factor analysis model. Typically, the model estimation is done {{under the assumption that}} the common factors are orthogonal (uncorrelated). However, the lasso-type penalization method based on the orthogonal model can often estimate a completely different model from that with the <b>true</b> <b>factor</b> structure when the common factors are correlated. In order to overcome this problem, we propose to incorporate a factor correlation into the model, and estimate the factor correlation along with parameters included in the orthogonal model by maximum penalized likelihood procedure. An entire solution path is computed by the EM algorithm with coordinate descent, which permits the application {{to a wide variety of}} convex and nonconvex penalties. The proposed method can provide sufficiently sparse solutions, and be applied to the data where the number of variables is larger than the number of observations. Monte Carlo simulations are conducted to investigate the effectiveness of our modeling strategies. The results show that the lasso-type penalization based on the orthogonal model cannot often approximate the <b>true</b> <b>factor</b> structure, whereas our approach performs well in various situations. The usefulness of the proposed procedure is also illustrated through the analysis of real data. Comment: 19 pages. arXiv admin note: substantial text overlap with arXiv: 1205. 586...|$|R
40|$|Abstract: The {{aim of this}} {{research}} is to deepen the understanding of the concept of company growth. With sales growth used as an operational measure, and in contrast with growth models offered by e. g. Greiner (1972 / 98) and Churchill & Lewis (1983), this study reveals a number of motives, enablers and consequences of growth. The empirical part investigates these aspects of growth within wholesale-SMEs between 1991 and 1996. With a response rate of 41 %, information is gathered on management, finance, and marketing issues related to the <b>true</b> <b>factors</b> that drive, enable, and follow from sales growth...|$|R
40|$|Finding {{the factors}} of an integer can be {{achieved}} by various experimental techniques, based on an algorithm developed by Schleich et al., which uses specific properties of Gaußsums. Experimental limitations usually require truncation of these series, but if the truncation parameter is too small, it is no longer possible to distinguish between factors and so-called "ghost" factors. Here, we discuss two techniques for distinguishing between <b>true</b> <b>factors</b> and ghost factors while keeping the number of terms in the sum constant or only slowly increasing. We experimentally test these modified algorithms in a nuclear spin system, using NMR. Comment: 4 pages, 5 figure...|$|R
40|$|The {{aim of this}} {{research}} is to deepen the understanding of the concept of company growth. With ‘sales growth’ used as an operational measure, and in contrast with growth models offered by e. g. Greiner (1972 / 98) and Churchill & Lewis (1983), this study reveals a number of motives, enablers and consequences of growth. The empirical part investigates these aspects of growth within wholesale-SMEs between 1991 and 1996. With a response rate of 41 %, information is gathered on management, finance, and marketing issues related to the <b>true</b> <b>factors</b> that drive, enable, and follow from sales growth. ...|$|R
40|$|Despite their popularities {{in recent}} years, factor models {{have long been}} criticized {{for the lack of}} identification. Even when a large number of {{variables}} are available, the factors can only be consistently estimated up to a rotation. In this paper, we try to identify the underlying factors by associating them to a set of observed variables, and thus give interpretations to the orthogonal factors estimated by the method of Principal Components. We first propose a estimation procedure to select a set of observed variables, and then test the hypothesis that <b>true</b> <b>factors</b> are exact linear combinations of the selected variables. Our estimation method is shown to able to correctly identity the <b>true</b> observed <b>factor</b> even in the presence of mild measurement errors, and our test statistics are shown to be more general than those of Bai and Ng (2006). The applicability of our methods in finite samples and the advantages of our tests are confirmed by simulations. Our methods are also applied to the returns of portfolios to identify the underlying risk factors. ...|$|R
50|$|Although peer {{tutoring}} {{has been proven}} {{to be an effective}} way of learning that engages and promotes academic achievement in students, does it {{have an effect on the}} achievement gap? It is an obvious fact that there is a large academic performance disparity between White, Black, and Latino students, and it continues to be an issue that has to be targeted. In an article it was mentioned that no one has been able to identify the <b>true</b> <b>factors</b> that cause this discrepancy. However it was mentioned that by developing effective {{peer tutoring}} programs in schools could be a factor that can potentially decrease the achievement gap in the United States.|$|R
40|$|The main {{difficulty}} of financial APT analysis concerns identifying unambiguously the hidden statistical factors. Lack of effective techniques {{to retrieve the}} <b>true</b> <b>factors</b> often leads to inappropriate interpretation of the underlying factor structure. In literature, PCA and MLFA, assuming multivariate Gaussian distributions, and ICA, assuming non-Gaussian distributions, are used to extract factors and determine the corresponding factor loadings. Recently, a new technique called TFA is proposed in [1, 2] which seeks {{to solve the problem}} of rotation indeterminacy encountered in conventional factor analysis. In this paper we will focus on statistical tests and inference on the APT temporal factor loadings recovered by TFA. 1...|$|R
40|$|Sufficient {{conditions}} for mean square convergence of factor predictors in common factor analysis are given by Guttman, by Williams, and by Schneeweiss and Mathes. These conditions {{do not hold}} for confirmatory factor analysis or when an error variance equals zero (Heywood cases). Two sufficient conditions are given for the three basic factor predictors and a predictor from rotated principal components analysis to converge to the factors of the model for confirmatory factor analysis, including Heywood cases. For certain model specifications the conditions are necessary. The conditions are sufficient {{for the existence of}} a unique <b>true</b> <b>factor.</b> A geometric interpretation is given for factor indeterminacy and mean square convergence of best linear factor prediction...|$|R
6000|$|... "And so," [...] he concluded, [...] "the <b>true</b> psychic <b>factor</b> {{of music}} took nearly three {{thousand}} years to impress {{itself on the}} Western mind. Debussy more nearly attains the idea-engendering and suggestive serenity--say {{of the time of}} Pythagoras--than any of his fore-runners--" ...|$|R
40|$|We {{consider}} large factor models where factors’explanatory power {{does not}} strongly dominate the explanatory {{power of the}} idiosyncratic terms in …nite samples, which is the situation often observed in the empirical applications. To study the principal components (PC) estimator of such a weak factors, we introduce a Pitman-drift-like asymptotic device, which we call weak factors asymptotics. We …nd the probability limits of the PC estimator under weak factors asymptotics when the idiosyncratic terms can be both cross-sectionally and temporally correlated. We show that the probability limits may be drastically di¤erent from the <b>true</b> <b>factors</b> and factor loadings even for factors with substantial explanatory power. For a special case of no cross-sectional and temporal correlation of the idiosyncratic terms, we establish the second order weak factors asymptotics of the PC estimator. The estimator is asymptotically normal with the covariance matrix depending {{on the strength of}} the factors and on the ratio of the cross-sectional and the temporal dimensions of the data. JEL code: C 13, C 33. Key words: approximate factor models, principal components, weak factors, inconsistency, bias, asymptotic distribution, Marµcenko-Pastur law...|$|R
40|$|Females ofCallosobruchus maculatus (F.) avoid ovipositing on host seeds already bearing conspecific eggs, {{and thus}} {{distribute}} eggs evenly among seeds. This behavior was {{presumed to be}} mediated by an ether-soluble “oviposition marker” that is deposited with the egg and can be extracted from egg-laden artificial hosts (glass beads). Ablation experiments revealed that the <b>true</b> <b>factors</b> promoting an even dispersion of eggs were perceived by the maxillary and labial palpi. In contrast, receptors on the antennae were largely responsible for avoidance of seeds treated with “oviposition marker. ” Taken together, {{these results suggest that}} a careful distinction should be drawn between factors that promote spacing of eggs under natural conditions and general oviposition deterrents that may be isolated from both sexes...|$|R
40|$|This article {{presents}} a new method for discovering hidden patterns in high-dimensional dataset resulting from image registration. It {{is based on}} <b>true</b> <b>factor</b> analysis, a statistical model that aims to find clusters of correlated variables. Applied to medical imaging, factor analysis can potentially identify regions that have anatomic significance and lend insight to knowledge discovery and morphometric investigations related to pathologies. Existent factor analytic methods require the computation of the sample covariance matrix and are thus limited to low-dimensional variable spaces. The proposed algorithm is able to compute the coefficients of the model without the need of the covariance matrix, expanding its spectrum of applications. The method’s efficiency and effectiveness is demonstrated {{in a study of}} volumetric variability related to the Alzheimer’s disease. 1...|$|R
40|$|This paper {{analyzes}} {{gender and}} {{ethnic differences in}} vulnerability and {{resilience to external shocks}} and stresses in Mexico. Vulnerability and resilience are measured by a combination of the level of household incomes per capita and the degree of diversification of these incomes. Thus, households which have poorly diversified incomes falling below the national poverty line are classified as highly vulnerable, whereas households which have highly diversified incomes above the poverty line are classified as highly resilient. The analysis shows that both gender and ethnicity are almost irrelevant as explanatory factors of vulnerability whereas education levels, dependency ratios and the age of the head of household are very important. Determining the <b>true</b> <b>factors</b> that affect vulnerability is important in order to devise effective policies to reduce vulnerability...|$|R
40|$|The popular Alternating Least Squares (ALS) {{algorithm}} for tensor decomposition is {{efficient and}} easy to implement, but often converges to poor local optima [...] -particularly when the weights of the factors are non-uniform. We propose a modification of the ALS approach that is as efficient as standard ALS, but provably recovers the <b>true</b> <b>factors</b> with random initialization under standard incoherence assumptions on the factors of the tensor. We demonstrate the significant practical superiority of our approach over traditional ALS {{for a variety of}} tasks on synthetic data [...] -including tensor factorization on exact, noisy and over-complete tensors, as well as tensor completion [...] -and for computing word embeddings from a third-order word tri-occurrence tensor. Comment: Minor updates to presentation. Appears in ICML' 1...|$|R
5000|$|It must {{be noted}} that the power factor {{mentioned}} above is the displacement power factor. There is another power factor that depends on THD. <b>True</b> power <b>factor</b> can be taken to mean the ratio between average real power and the magnitude of RMS voltage and current, [...]|$|R
5000|$|Substituting this {{in for the}} {{equation}} for <b>true</b> power <b>factor,</b> {{it becomes clear that}} the quantity can be taken to have two components, one of which is the traditional power factor (neglecting the influence of harmonics) and one of which is the harmonics’ contribution to power factor: ...|$|R
30|$|Tc and 177 Lu, respectively. The phantom was imaged {{using the}} same {{acquisition}} and reconstruction protocols previously described for the phantom inserts. The <b>true</b> calibration <b>factor</b> was calculated (equation 1) from the uniform phantom data using the reconstructed total number of counts in the SPECT field of view.|$|R
