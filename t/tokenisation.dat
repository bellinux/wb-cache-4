67|1|Public
50|$|Apple Pay {{uses the}} EMV Payment <b>Tokenisation</b> Specification.|$|E
40|$|Challenging the {{assumption}} that traditional whitespace/punctuation-based <b>tokenisation</b> is the best solution for any NLP application, I propose an alternative approach to segmenting text into processable units. The proposed approach is nearly knowledge-free, in {{that it does not}} rely on language-dependent, man-made resources. The text segmentation approach is applied to the task of automated error reduction in texts with high noise. The results are compared to conventional <b>tokenisation...</b>|$|E
40|$|Identifier {{names are}} the main vehicle for {{semantic}} information during program comprehension. For tool-supported program comprehension tasks, including concept location and requirements traceability, identifier names need to be tokenised into their semantic constituents. In this paper we present an approach to the automated <b>tokenisation</b> of identifier names that improves on existing techniques in two ways. First, it improves the <b>tokenisation</b> accuracy for single-case identifier names and for identifier names containing digits, which existing techniques largely ignore. Second, performance gains over existing techniques are achieved using smaller oracles, making the approach easier to deploy. Accuracy was evaluated by comparing our algorithm to manual tokenizations of 28, 000 identifier names drawn from 60 well-known open source Java projects totalling 16. 5 MSLOC. Moreover, the projects were used to perform a study of identifier <b>tokenisation</b> features (single case, camel case, use of digits, etc.) per object-oriented construct (class names, method names, local variable names, etc.), thus providing an insight into naming conventions in industrial-scale object-oriented code. Our <b>tokenisation</b> tool and datasets are publicly available...|$|E
40|$|Tools for the {{analysis}} of historical data, especially from non-Indo-European languages, have to solve specific challenges pertaining to, e. g., the synchronised representation of original script and transliterations, deep search over non-Latin script, data models to allow for customised <b>tokenisations,</b> etc. While the implementation of new software solutions for a specific research question and specific data in this context is a plausible solution, it is perfectly unsustainable. We present ANNIS, a browser-based, re-usable search and analysis tool for multi-layer linguistic corpora. ANNIS can be, and has been, used to searches and analyses over a number of historical corpora as well as corpora with non-Latin script. It is driven by a graph-based data model that is able to take up potentially unlimited types of annotation, and can therefore be used to represent data coming from various different sources and formats. The possibility of conversion from several different formats via the compatible conversion framework Pepper makes ANNIS highly re-usable {{in a wide variety of}} research contexts. It also features different, pluggable, visualisation options so that the different corpus strata can be presented in optimal form. We exemplarily present a use case for search in the Coptic SCRIPTORIUM, a multi-layer corpus of Coptic...|$|R
40|$|Identifier {{names are}} the main vehicle for {{semantic}} information during program comprehension. For tool-supported program comprehension tasks, including concept location and requirements traceability, identifier names need to be tokenised into their semantic constituents. We present INTT, a Java library that implements an approach to the automated <b>tokenisation</b> of identifier names which improves on existing techniques in two ways. First, it improves the <b>tokenisation</b> accuracy for single-case identifier names and for identifier names containing digits, which existing techniques largely ignore. Second, performance gains over existing techniques are achieved using smaller oracles, making the approach easier to deploy. Our <b>tokenisation</b> library and the datasets used for its evaluation are made available in this package. Also included is a database of unique identifier names extracted from the 60 Java projects, {{as a resource for}} further research on program comprehension...|$|E
40|$|In {{this paper}} we discuss {{technical}} issues arising from the interdependence between <b>tokenisation</b> and XML-based annotation tools, in particular those which use standoff annotation {{in the form of}} pointers to word tokens. It is common practice for an XML-based annotation tool to use word tokens as the target units for annotating such things as named entities because it provides appropriate units for stand-off annotation. Furthermore, these units can be easily selected, swept out or snapped to by the annotators and certain classes of annotation mistakes can be prevented by building a tool that does not permit selection of a substring which does not entirely span one or more XML elements. There is a downside to this method of annotation, however, in that it assumes that for any given data set, in whatever domain, the optimal <b>tokenisation</b> is known before any annotation is performed. If mistakes are made in the initial <b>tokenisation</b> and the word boundaries conflict with the annotators’ desired actions, then either the annotation is inaccurate or expensive retokenisation and reannotation will be required. Here we describe the methods we have developed to address this problem. We also describe experiments which explore the effects of different granularities of <b>tokenisation</b> on NER tagger performance. ...|$|E
40|$|This paper {{describes}} a non-deterministic tokeniser implemented {{and used for}} the development of a French finite-state grammar. The tokeniser includes a finite-state automaton for simple tokens and a lexical transducer that encodes a wide variety of multiword expressions, associated with multiple lexical descriptions when required. 1 Introduction Usually <b>tokenisation</b> has been seen as an independent process [5, 9] in natural language processing. In many parsing systems the <b>tokenisation</b> has had little attention and, especially when parsing English, tokens are often supposed to be sequences of letters between two blanks. In our approach the <b>tokenisation</b> is a firm part of the morphological analysis. Our tokens are defined for the needs of a syntactic parser (i. e. they are the basic components of the parsing). This leads us to a large collection of different tokens, like: ffl a simple word, ffl several words forming one token as in a priori, ffl a same string ambiguously producing one or [...] ...|$|E
40|$|We {{investigate}} {{the impact of}} pre-extracting and tokenising bigram collocations on topic models. Using extensive experiments on four different corpora, we show that incorporating bigram collocations in the document representation creates more parsimonious models and improves topic coherence. We point out some problems in interpreting test likelihood and test perplexity to compare model fit, and suggest an alternate measure that penalises model complexity. We show how the Akaike information criterion is a more appropriate measure, which suggests that using a modest number (up to 1000) of top-ranked bigrams is the optimal topic modelling configuration. Using these 1000 bigrams also results in improved topic quality over unigram <b>tokenisation.</b> Further increases in topic quality {{can be achieved by}} using up to 10, 000 bigrams, but this is at the cost of a more complex model. We also show that multiword (bigram and longer) named entities give consistent results, indicating that they should be represented as single tokens. This is the first work to explicitly study the effect of n-gram <b>tokenisation</b> on LDA topic models, and the first work to make empirical recommendations to topic modelling practitioners, challenging the standard practice of unigram-based <b>tokenisation...</b>|$|E
40|$|The Nepali National Corpus (NNC) was, in {{the process}} of its creation, {{annotated}} with part-of-speech (POS) tags. This paper describes the extension of automated text and corpus annotation in Nepali from POS tags to lemmatisation, enabling a more complex set of corpus-based searches and analyses. This work also addresses certain practical compromises embodied in the initial tagging of the NNC. First, some particular aspects of Nepali morphology – in particular the complexity of the agglutinative verbal inflection system – necessitated improvements to the underlying <b>tokenisation</b> of the text before lemmatisation could be satisfactorily implemented. In practical terms, both the <b>tokenisation</b> and lemmatisation procedures require linguistic knowledge resources to operate successfully: a set of rules describing the default case, and a lexicon containing a list of individual exceptions: words whose form suggests a particular rule should apply to them, but where that rule in fact does not apply. These resources, particularly the lexicons of irregularities, were created by a strongly data-driven process working from analyses of the NNC itself. This approach to <b>tokenisation</b> and lemmatisation, and associated linguistic knowledge resources, may be illustrative and of use to researchers looking at other languages of the Himalayan region, most especially those that have similar morphological behaviour to Nepali...|$|E
40|$|The paper {{describes}} {{the preparation of}} a Buddhist corpus in the Middle Indo-Aryan language Pāli, which is available only in a flat TEI format, for content-based analysis. This task includes transforming the file into a hierarchical TEI P 5 representation, followed by <b>tokenisation</b> (including sandhi resolution), lemmatisation, and POS tagging...|$|E
40|$|Chemistry {{text mining}} tools should be {{interoperable}} and adaptable regardless of system-level implementation, installation or even programming issues. We aim to abstract the functionality of these tools from the underlying implementation via reconfigurable workflows for automatically identifying chemical names. To achieve this, we refactored an established named entity recogniser (in the chemistry domain), OSCAR {{and studied the}} impact of each component on the net performance. We developed two reconfigurable workflows from OSCAR using an interoperable text mining framework, U-Compare. These workflows can be altered using the drag-&-drop mechanism of the graphical user interface of U-Compare. These workflows also provide a platform to study the relationship between text mining components such as <b>tokenisation</b> and named entity recognition (using maximum entropy Markov model (MEMM) and pattern recognition based classifiers). Results indicate that, for chemistry in particular, eliminating noise generated by <b>tokenisation</b> techniques lead to a slightly better performance than others, in terms of named entity recognition (NER) accuracy. Poor <b>tokenisation</b> translates into poorer input to the classifier components which in turn leads {{to an increase in}} Type I or Type II errors, thus, lowering the overall performance. On the Sciborg corpus, the workflow based system, which uses a new tokeniser whilst retaining the same MEMM component, increases the F-score from 82. 35 % to 84. 44 %. On the PubMed corpus, it recorded an F-score o...|$|E
40|$|This paper {{describes}} LINGUA - {{an architecture}} for text processing in Bulgarian. First, the pre-processing modules for <b>tokenisation,</b> sentence splitting, paragraph segmentation, partof -speech tagging, clause chunking and noun phrase extraction are outlined. Next, the paper proceeds to describe {{in more detail}} the anaphora resolution module. Evaluation results are reported for each processing task...|$|E
40|$|Abstract––This paper {{presents}} the approach towards converting {{text to speech}} using new methodology. The text to speech conversion system enables user to enter text in Punjabi and as output it gets sound. The paper {{presents the}} steps followed for converting text to speech for Punjabi (gurmukhi) language and the algorithm used for it. The focus {{of this paper is}} based on the <b>tokenisation</b> process and the orthographic representation of the text that shows the mapping of letter to sound using the description of language’s phonetics. Here the main focus is on the text to IPA transcription concept. It is in fact, a system that translates text to IPA transcription which is the primary stage for text to speech conversion. The whole procedure for converting text to speech involves {{a great deal of time}} as it’s not an easy task and requires efforts. Keywords––IPA,Orthographic representation, phonetic, <b>tokenisation,</b> transcription. I...|$|E
40|$|We {{describe}} LT TTT, {{a recently}} developed software system which provides tools to perform text <b>tokenisation</b> and mark-up. The system includes ready-made components to segment text into paragraphs, sentences, words {{and other kinds}} of token but, crucially, it also allows users to tailor rule-sets to produce mark-up appropriate for particular applications. We present three case studies of our use of LT TTT: named-entity recognition (MUC- 7), citation recognition and mark-up and the preparation of a corpus in the medical domain. We conclude with a discussion of the use of browsers to visualise marked-up text. 1. Introduction The LTG's Text <b>Tokenisation</b> Toolkit (LT TTT, Grover et al., 1999) was developed within an XML processing paradigm whereby tools are combined together in a pipeline allowing each to add, modify or remove some piece of mark-up. The tools are compatible with the LT XML toolset (Thompson et al., 1997) and use the LT XML API to manipulate attribute values and character data [...] ...|$|E
40|$|This paper reconsiders {{the task}} of MRD-based word sense disambiguation, in extend-ing the basic Lesk {{algorithm}} to investigate the impact onWSD performance of different <b>tokenisation</b> schemes, scoring mechanisms, methods of gloss extension and filtering methods. In experimentation over the Lex-eed Sensebank and the Japanese Senseval- 2 dictionary task, we demonstrate that char-acter bigrams with sense-sensitive gloss ex-tension over hyponyms and hypernyms en-hances WSD performance. ...|$|E
40|$|Thesis (M. A. (Applied Language and Literary Studies)) [...] North-West University, Potchefstroom Campus, 2006. An {{important}} core {{technology in}} the development of human language technology applications is an automatic morphological analyser. Such a morphological analyser consists of various modules, one of which is a tokeniser. At present no tokeniser exists for Afrikaans and it has therefore been impossible to develop a morphological analyser for Afrikaans. Thus, in this research project such a tokeniser is being developed, and the project therefore has two objectives: i) to postulate a tag set for integrated <b>tokenisation,</b> and ii) to develop an algorithm for integrated <b>tokenisation.</b> In order to achieve the first object, a tag set for the tagging of sentences, named-entities, words, abbreviations and punctuation is proposed specifically for the annotation of Afrikaans texts. It consists of 51 tags, which can be expanded in future in order to establish a larger, more specific tag set. The postulated tag set can also be simplified according to the level of specificity required by the user. It is subsequently shown that an effective tokeniser cannot be developed using only linguistic, or only statistical methods. This is due to the complexity of the task: rule-based modules should be used for certain processes (for example sentence recognition), while other processes (for example named-entity recognition) can only be executed successfully by means of a machine-learning module. It is argued that a hybrid system (a system where rule-based and statistical components are integrated) would achieve the best results on Afrikaans <b>tokenisation.</b> Various rule-based and statistical techniques, including a TiMBL-based classifier, are then employed to develop such a hybrid tokeniser for Afrikaans. The final tokeniser achieves an ∫-score of 97. 25 % when the complete set of tags is used. For sentence recognition an ∫-score of 100 % is achieved. The tokeniser also recognises 81. 39 % of named entities. When a simplified tag set (consisting of only 12 tags) is used to annotate named entities, the ∫-score rises to 94. 74 %. The conclusion of the study is that a hybrid approach is indeed suitable for Afrikaans sentencisation, named-entity recognition and <b>tokenisation.</b> The tokeniser will improve if it is trained with more data, while the expansion of gazetteers as well as the tag set will also lead to a more accurate systemMaster...|$|E
40|$|Morphological {{processing}} of Polish is seriously {{hampered by the}} poor availability of general-purpose tools. This article presents an attempt to create such a set of tools following the de facto standard of the IPIC corpus. Currently, the package contains pieces of software able to perform the following tasks: text <b>tokenisation,</b> morphological analysis with heuristics for unknown words, division into sentences and morphosyntactic disambiguation. The described tools will be made available under the GNU general public licence...|$|E
40|$|Abstract. We present three {{different}} approaches to tokenising a gesture trajectory into a label sequence for gestural language analysis. Firstly, we review a scale-space method to segment and project trajectories into tokens in gesture space. Secondly, we adapt an entropy-over-scale saliency algorithm to the gesture segmentation task. Thirdly, we show how the SAX algorithm can directly tokenize a gesture trajectory into a sequence of symbols. Lastly, we propose to combine aspects of these methods for visual gesture <b>tokenisation.</b> ...|$|E
40|$|This paper {{describes}} how Tralics converts a sequence characters into {{a sequence of}} tokens, into a math list, and finally into a MathML formula. <b>Tokenisation</b> rules {{are the same as}} in TeX, the meaning of these tokens is the same as in LaTeX, and can be given in packages. Math formulas are handled in the same spirit as TeX, but construction of the MathML result is not obvious, due to particularities of both TeX and MathML...|$|E
40|$|Many {{text mining}} {{operations}} can be recast as tag insertion problems—we illustrate several. The {{size of the}} search space of the tag insertion problem is explored, using a number of proofs and heuristics: Viterbi Search, One-Tag-at-a-Time and Automatic <b>Tokenisation.</b> These greatly {{reduce the size of}} the search space from approximately to approximately nodes. Properties of the SGML standard are also used to reduce the complexity of the search. We work through several examples taken from bibliographies, showing search space size and possible errors. ...|$|E
40|$|We {{describe}} the annotation of chemical named entities in scientific text. A set of annotation guidelines defines 5 types of named entities, and provides {{instructions for the}} resolution of special cases. A corpus of fulltext chemistry papers was annotated, with an inter-annotator agreement score of 93 %. An investigation of named entity recognition using LingPipe suggests that scores of 63 % are possible without customisation, and scores of 74 % are possible {{with the addition of}} custom <b>tokenisation</b> and the use of dictionaries. ...|$|E
40|$|ILSP FBT Tagger is an {{adaptation}} of the Brill tagger trained on Greek text. It uses a PAROLE compatible tagset of 584 different tags which capture the morphosyntactic particularities of the Greek language. Working on the output of a sentence detection and <b>tokenisation</b> tool, the tagger assigns initial tags, looking up in a lexicon created from a manually annotated corpus during training. A suffix lexicon is used for initially tagging unknown words. 799 contextual rules are then applied to improve the initial phase output...|$|E
40|$|The Repetition Blindness effect (RB) is {{a robust}} {{characteristic}} of visual perception that involves {{a failure to}} report the repeated occurrence of a target stimulus displayed {{in a series of}} rapidly presented visual stimuli. Huber and colleagues attribute the RB to cognitive aftereffects, primarily habituation, which inhibit encoding of the repeated target into memory. In contrast, Kanwisher and many in the lexical domain propose that the emergence of the RB represents a failure in binding the generalised mental representation or type of a target to its second presentation or token. Three experiments investigated the RB, contrasting theories and methodologies from the episodic and lexical fields to better understand the foundation of the effect. Prime duration, word frequency, lag, and target colour were manipulated within rapid serial visual presentation streams and masked-target lexical decision tasks. Approaching the RB from the <b>Tokenisation</b> perspective, it was hypothesized that non-coloured targets would produce pronounced blindness effects with no influence of word frequency and in a masked target lexical-decision task, repetition priming would be seen. In contrast to the <b>Tokenisation</b> predictions, coloured targets and high-frequency words exhibited greater repetition blindness, along with repetition deficits in the lexical decision task, results that are broadly consistent with the Habituation account of the RB...|$|E
40|$|Setswana, a Bantu {{language}} in the Sotho group, {{is one of the}} eleven official languages of South Africa. The language is characterised by a disjunctive orthography, mainly affecting the important word category of verbs. In particular, verbal prefixal morphemes are usually written disjunctively, while suffixal morphemes follow a conjunctive writing style. Therefore, Setswana <b>tokenisation</b> cannot be based solely on whitespace, {{as is the case in}} many alphabetic, segmented languages, including the conjunctively written Nguni group of South African Bantu languages. This paper shows how a combination of two tokeniser transducers and a finite-state (rule-based) morphological analyser may be combined to effectively solve the Setswana <b>tokenisation</b> problem. The approach has the important advantage of bringing the processing of Setswana beyond the morphological analysis level in line with what is appropriate for the Nguni languages. This means that the challenge of the disjunctive orthography is met at the tokenisation/morphological analysis level and does not in principle propagate to subsequent levels of analysis such as POS tagging and shallow parsing, etc. Indeed, the approach ensures that an aspect such as orthography does not obfuscate sound linguistics and, ultimately, proper semantic analysis, which remains the ultimate aim of linguistic analysis and therefore also computational linguistic analysis. ...|$|E
40|$|Language {{identification}} {{is the task}} of identifying the language a given document is written in. This paper describes a detailed examination of what models perform best under different conditions, based on experiments across three separate datasets {{and a range of}} <b>tokenisation</b> strategies. We demonstrate that the task becomes increasingly difficult as we increase the number of languages, reduce the amount of training data and reduce the length of documents. We also show {{that it is possible to}} perform language identification without having to perform explicit character encoding detection. ...|$|E
40|$|This project {{sets out}} to {{discover}} and develop techniques for the lemmatisation of a historical corpus of the Cornish language in order that a lemmatised dictionary macrostructure can be generated from the corpus. The system should be capable of uniquely identifying every lexical item that is attested in the corpus. A survey of published and unpublished Cornish dictionaries, glossaries and lexicographical notes was carried out. A corpus was compiled incorporating specially prepared new critical editions. An investigation {{into the history of}} Cornish lemmatisation was undertaken. A systemic description of Cornish inflection was written. Three methods of corpus lemmatisation were trialed. Findings were as follows. Lexicographical history shapes current Cornish lexicographical practice. Lexicon based <b>tokenisation</b> has advantages over character based <b>tokenisation.</b> System networks provide the means to generate base forms from attested word types. Grammatical difference is the most reliable way of disambiguating homographs. A lemma that contains three fields, the canonical form, the part-of-speech and a semantic field label, provides of a unique code for every lexeme attested in the corpus. Programs which involve human interaction during the lemmatisation process allow bootstrapping of the lemmatisation database. Computerised morphological processing may be used at least to partially create the lemmatisation database. Disambiguation of {{at least some of the}} most common homographs may be automated by the use of computer programs...|$|E
40|$|A {{peculiarity}} of Welsh and {{the other}} Celtic languages is their system of Initial Mutations. These are regular alternations of word-initial phonemes triggered {{by a variety of}} lexical and syntactic triggering contexts. This feature of the Celtic languages poses a number of challenges to grammatical description, not least because it requires direct reference to adjacency relations in the linear string. We describe here an approach which covers the full range of mutation processes and their distribution in Welsh using the XLE grammar development environment and the associated finite state and <b>tokenisation</b> tools (Crouch et al., 2006). ...|$|E
40|$|Contents 1 Tokeniser 2 2 Gazetteer 3 3 Grammar 4 3. 1 Use of Context............................... 7 3. 2 Use of Priority............................... 8 4 Walkthrough example 10 4. 1 Step 1 - <b>Tokenisation............................</b> 10 4. 2 Step 2 - List Lookup............................ 11 4. 3 Step 3 - Grammar Rules.......................... 11 5 Formal Description of the JAPE Grammar 11 6 Relation to CPSL 14 7 Algorithms for JAPE Rule Application 15 7. 1 The rst algorithm............................. 15 7. 2 Algorithm 2................................. 19 8 Label Binding Scheme 22 9 Classes 23 10 Implementation 23 10. 1 A Walk-Through.............................. 23 10. 2 Example RHS code............................. 24 11...|$|E
40|$|The WISPR project (“Welsh and Irish Speech Processing Resources”) {{has been}} {{building}} text-to-speech synthesis systems for Welsh and for Irish, as well as building links between the developers and potential users of the software. The Welsh half of the project has encountered various challenges, {{in the areas of}} the <b>tokenisation</b> of input text, the formatting of letter-to-sound rules, and the implementation of the “greedy algorithm ” for text selection. The solutions to these challenges have resulted in various tools which may be of use to other developers using Festival for TTS for other languages. These resources are made freely available. 1...|$|E
40|$|Abstract- In this paper, we have {{described}} the steps for converting Punjabi text to IPA. The conversion of text to IPA is the initial step for converting text to sound. In a text to speech synthesis system, series of steps are followed to generate speech from text [8]. We have developed a system that converts Punjabi text written in gurmukhi script into IPA symbols that is an alphabetic system of phonetic notation based primarily on the alphabet. Here, we {{have described}} the <b>tokenisation</b> of Punjabi text and the generation of IPA corresponding to Punjabi text {{that will help us}} to generate speech in our future work. I...|$|E
40|$|Abstract Background Record linkage {{refers to}} the process of joining records that relate to the same entity or event in one or more data collections. In the absence of a shared, unique key, record linkage {{involves}} the comparison of ensembles of partially-identifying, non-unique data items between pairs of records. Data items with variable formats, such as names and addresses, need to be transformed and normalised in order to validly carry out these comparisons. Traditionally, deterministic rule-based data processing systems have been used to carry out this pre-processing, which is commonly referred to as "standardisation". This paper describes an alternative approach to standardisation, using a combination of lexicon-based <b>tokenisation</b> and probabilistic hidden Markov models (HMMs). Methods HMMs were trained to standardise typical Australian name and address data drawn from a range of health data collections. The accuracy of the results was compared to that produced by rule-based systems. Results Training of HMMs was found to be quick and did not require any specialised skills. For addresses, HMMs produced equal or better standardisation accuracy than a widely-used rule-based system. However, acccuracy was worse when used with simpler name data. Possible reasons for this poorer performance are discussed. Conclusion Lexicon-based <b>tokenisation</b> and HMMs provide a viable and effort-effective alternative to rule-based systems for pre-processing more complex variably formatted data such as addresses. Further work is required to improve the performance of this approach with simpler data such as names. Software which implements the methods described in this paper is freely available under an open source license for other researchers to use and improve. </p...|$|E
40|$|Existing {{techniques}} for <b>tokenisation</b> and sentence boundary identification are extremely accurate when {{the data is}} perfectly clean (Mikheev, 2002), and have been applied successfully to corpora of news feeds and other post-edited corpora. Informal written texts are readily available, and {{with the growth of}} other informal text modalities (IRC, ICQ, SMS etc.) are becoming an interesting alternative, perhaps better suited as a source for lexical resources and language models for studies of dialogue and spontaneous speech. However, the high degree of spelling errors, irregularities and idiosyncrasies in the use of punctuation, white space and capitalisation require specialised tools. In this paper we study the design and implementation of a tool for pre-processing and normalisation of noisy corpora. We argue that rather than having separate tools for <b>tokenisation,</b> segmentation and spelling correction organised in a pipeline, a unified tool is appropriate because of certain specific sorts of errors. We describe how a noisy channel model can be used at the character level to perform this. We describe how the sequence of tokens needs to be divided into various types depending on their characteristics, and also how the modelling of white-space needs to be conditioned on the type of the preceding and following tokens. We use trainable stochastic transducers to model typographical errors, and other orthographic changes and a variety of sequence models for white space and the different sorts of tokens. We discuss the training of the models and various efficiency issues related to the decoding algorithm, and illustrate this with examples from a 100 million word corpus of Usenet news...|$|E
40|$|Corpus-based Machine Learning of {{linguistic}} annotations {{has been a}} key topic for all areas of Natural Language Processing. This paper presents a survey, along three dimensions of classification. First we outline different linguistic level of analysis: <b>Tokenisation,</b> Part-of-Speech tagging, Parsing, Semantic analysis and Discourse annotation. Secondly, we introduce alternative approaches to Machine Learning applicable to linguistic annotation of corpora: N-gram and Markov models, Neural Networks, Transformation-Based Learning, Decision Tree learning, and Vector-based classification. Thirdly, we examine a range of Machine Learning systems for the most challenging level {{of linguistic}} annotation, discourse analysis; these illustrate the various Machine Learning approaches. Our overall aim is to provide an ontology or framework for further development of our research...|$|E
40|$|The ALesKo learner corpus is a {{small-scale}} comparable corpus {{consisting of}} two subcorpora: annotated essays by advanced Chinese learners of German and comparable essays by German native speakers. The motivation for its compilation was the investigation of discourse-related phenomena such as local coherence in second-language acquisition of German. After introducing how the texts were compiled and annotated, the article focuses on quantitative studies at the token level. We discuss problems of <b>tokenisation</b> and part-of-speech tagging and compare the inventory of the two subcorpora in terms of frequently used N-grams and lexical richness, among other aspects. We conclude the article by describing possible applications of the study in foreign language acquisition research and language teaching...|$|E
40|$|In some languages, {{spaces and}} {{punctuation}} marks {{are used to}} delimit word boundaries. This {{is the case with}} Cornish. However there is considerable inconsistency of segmentation to be found within the Corpus of Cornish. The individual texts that make up this corpus are not even internally consistent. The first stage in lemmatising the Corpus of Cornish, therefore, involves the resegmentation of the corpus into tokens. The whole notion of what {{is considered to be a}} word has to be examined. A method for the logical representation of segmentation into tokens is proposed in this paper. The existing segmentation of the Corpus of Cornish, as indicated by spaces in the text, is abandoned and an algorithm for dictionary based critical <b>tokenisation</b> of the corpus is proposed...|$|E
40|$|With {{more and}} more text being {{available}} in electronic form, it is becoming relatively easy to obtain digital texts together with their translations. The paper presents the processing steps necessary to compile such texts into parallel corpora, an extremely useful language resource. Parallel corpora {{can be used as}} a translation aid for second-language learners, for translators and lexicographers, or as a datasource for various language technology tools. We present our work in this direction, which is characterised by the use of open standards for text annotation, the use of publicly available third-party tools and wide availability of the produced resources. Explained is the corpus annotation chain involving normalisation, <b>tokenisation,</b> segmentation, alignment, word-class syntactic tagging, and lemmatisation. Two exploitation results over our annotated corpora are also presented, namely a Web concordancer and the extraction of bi-lingual lexica...|$|E
