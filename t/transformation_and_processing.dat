8|10000|Public
50|$|The {{nomenclature}} term graph {{is associated}} with the field of term graph rewriting, which involves the <b>transformation</b> <b>and</b> <b>processing</b> of expressions by the specification of rewriting rules, whereas abstract semantic graph is used when discussing linguistics, programming languages, type systems and compilation.|$|E
50|$|Using Data Lake Analytics, {{users can}} develop and run {{parallel}} data <b>transformation</b> <b>and</b> <b>processing</b> programs in U-SQL, a query language that combines SQL with C#. U-SQL {{was designed as}} an evolution of the declarative SQL language with native extensibility through the user code written in C#. U-SQL uses C# data types and the C# expression language. The U-SQL script bellow shows the three major steps of processing data with U-SQL.|$|E
40|$|Abstract. Collaborative {{systems that}} {{automate}} {{the sharing of}} programmer-defined user interfaces offer limited coupling flexibility, typically forcing all users of an application to share {{all aspects of the}} user interfaces. Those that automatically support high coupling flexibility are tied to a narrow set of predefined user-interfaces. We have developed a framework that provides high-level and flexible coupling support for arbitrary, programmer-defined user interfaces. The framework refines an abstract layered model of collaboration with structured application layers and automatic acquisition, <b>transformation,</b> <b>and</b> <b>processing</b> of updates. It has been used to easily provide flexible coupling in complex, existing single-user software and shown to support all known ways to share user-interfaces. Coupling flexibility comes at the cost of a small amount of additional programming. We have carefully crafted the framework to ensure that this overhead is proportional to the degree of coupling flexibility desired. 1...|$|E
5000|$|Macro {{recording}} {{feature to}} facilitate complex text <b>transformations</b> <b>and</b> data <b>processing.</b>|$|R
30|$|The {{design of}} the system {{includes}} two parts: information <b>transformation</b> <b>and</b> security <b>processing</b> <b>and</b> testing.|$|R
40|$|Inspired by the {{pioneering}} work of Joan Vickers, Quiet Eye (QE) research has {{gained increased attention}} from researchers in disciplines ranging from Sports Science to Neuroscience. A recent target article by Vickers (2016) {{provides an overview of}} QE research relating to expert performance, oculomotor control, attention, anxiety, and child development. In this commentary, we provide a neuroscientific perspective on QE and optimal oculomotor control and discuss their possible underlying brain mechanisms. We focus primarily {{on the role of the}} parietal-frontal network and question its involvement in visuomotor <b>transformations</b> <b>and</b> <b>processing</b> of an efference copy. To address these issues, we discuss the potential benefits of adapting transcranial magnetic stimulation techniques to QE research. In addition, a brief perspective on QE research in patients with neurodegenerative disorders and aging is provided. status: accepte...|$|R
40|$|The {{amount of}} {{mathematical}} knowledge {{available in the}} electronic format is enormously increasing. This creates a need for many new technologies which must better support the interaction between humans and this vast information pool. Especially promising direction is content-based ways of representing mathematics, as this establishes grounds to real automation, interoperability, <b>transformation</b> <b>and</b> <b>processing</b> of mathematics. However, the main problem of semantic-based techniques is the actual authoring cost of semantically enriched documents [AA 04]. As an example how can management of such documents be greatly facilitated, we present nOMDoc mode, an extension of emacs editor which exploits specific traits of mathematical documents written in an XML-based format that embraces both Content MathML and OpenMath: OMDoc format. In particular, the mode features a direct semantic math web search from the editor and further explores possible interactions between a search engine and an ccc editor environment...|$|E
40|$|This {{dissertation}} addresses {{two important}} problems in reusing intellectual properties (IPs) {{in the form}} of reusable design or verification components. The first problem is associated with fast and effective integration of reusable design components into a System-on-chip (SoC), so faster design turn-around time can be achieved, leading to faster time-to-market. The second problem has the same goals of faster product design cycle, but emphasizes on verification model reuse, rather than design component reuse. It specifically addresses reuse of reusable verification IPs to enable a “write once, use many times ” verification strategy. This dissertation is accordingly divided into part I and part II which are related but describe the two problems and our solutions to them. These two related but distinctive problems faced by system design companies have been tackled through a unique approach which hither-to-fore only have been used in the software engineering domain. This approach is called metamodeling, which allows creating customized meta-language to describe the syntax and semantics for a modeling domain. It provides a way to create, transform and analyze domain specific languages, which are themselves described by metamodels, and the <b>transformation</b> <b>and</b> <b>processing</b> of models in such languages are als...|$|E
40|$|The {{development}} of analytical instruments and computer technologies {{in recent decades}} has facilitated {{significant changes in the}} methodologies used in scientific studies of agricultural air quality. A variety of instruments and sensors have been used for long-term and continuous measurements at commercial animal facilities and laboratories for determining baseline pollutant emissions and testing mitigation technologies. New measurement strategies were developed for real-time measurement and multi-location sampling. Optimization of this technology change necessitates an up-to-date system to acquire high-frequency data, control instruments and sampling locations, and monitor system operation. While various air quality research projects involve similar objectives and instrumentation to meet those objectives, they are usually conducted with monitoring plans that differ among sites and among projects. Special data acquisition and control (DAC) hardware and software have to be adapted for each monitoring plan. This paper summarizes various measurement and control devices used for comprehensive air quality studies of livestock and poultry environments. The paper further presents methods for real-time data <b>transformation</b> <b>and</b> <b>processing.</b> It introduces an air quality DAC system, which provided novel, flexible, and user-friendly features. The methodology and technology used in the new DAC system reduces system development and operational cost, increase reliability and work efficiency, and enhances data quality...|$|E
40|$|Abstract. Computational {{linguistics}} is {{an application}} of computer science which presents interesting challenges from the programming methodology point of view. Developing a realistic platform {{for the treatment of}} a natural language in its phonological, morphological, syntactic, and ultimately semantic aspects demands a principled modular architecture with complex cooperation between the various layers. Representing large lexical data bases, treating sophisticated phonological <b>and</b> morphological <b>transformations,</b> <b>and</b> <b>processing</b> in real time large corpuses demands fast finite-state methods toolkits. Analysing the syntactic structure, computing anaphoric relations, and dealing with the representation of information flow in dialogue understanding, demands the processing of complex constraints on graph structures, with sophisticated sharing of large nondeterministic search spaces. The talk reports on experiments in using declarative programming for the processing of the sanskrit language, in its phonological and morphologica...|$|R
5000|$|The {{functionality}} {{included in}} Waffles is very broad, including algorithms for dimensionality reduction, collaborative filtering, visualization, clustering, supervised learning, optimization, linear algebra, data <b>transformation,</b> image <b>and</b> signal <b>processing,</b> policy learning, <b>and</b> sparse matrix operations.|$|R
40|$|ABSTRACT: Image {{recognition}} and segmentation techniques are playing {{key role in}} the field of image processing. Present researchers are working on the design concepts of accurate image processing. This paper explains the method for designing of accurate image processing {{with the help of the}} principle called automatic construction of tree structural image <b>transformation</b> <b>and</b> graphics <b>processing</b> unit as a hardware unit. Genetic algorithms are also used to obtain fast image processing on graphic processors...|$|R
30|$|However, {{during the}} last few years, the export of live animals from Somaliland has been slowing, leading to the decline in {{national}} earning (SLCCIA 2015; SLCCIA 2016), and reducing pastoralists’ income. Adding value to the livestock and livestock products provides an avenue for reversing the declining incomes and sustaining the livelihoods of pastoralists. This can be achieved by slaughtering, processing and exporting meat (chilled carcasses or packaged meat) and by investing in the <b>transformation</b> <b>and</b> <b>processing</b> of livestock by-products like hides and skins, and bones, tallow and horns. Previous studies have shown that slaughtering and exporting processed meat involve higher costs of refrigeration (high costs of electricity in Somaliland) and transport to the importing countries (Negassa et al. 2012). On the other hand, improving the quality of hides and skins produced and developing the value chain beyond the wet blue tanning offer a more viable investment than exports of raw wet or dried hides and skins (Wanyoike et al. 2018). The same applies to harnessing and processing of tallow, bones and horns into an array of products that include soap, cooking oil and candles (from tallow) as well as an assortment of bone trinkets (jewellery, buttons, carvings etc.) from bones and horns. Of interest, processing of bones {{has been shown to be}} an important source of employment and income for women and youths (Kinyanjui and Noor 2013).|$|E
40|$|Consistent, {{accurate}} and timely data {{are essential to}} the functioning of a modern organization. Managing the integrity of an organization’s data assets in a systematic manner is a challenging task in the face of continuous update, <b>transformation</b> <b>and</b> <b>processing</b> to support business operations. Classic approaches to constraint-based integrity focus on logical consistency within a database and reject any transaction that violates consistency, but leave unresolved how to fix or manage violations. More ad hoc approaches focus on the accuracy of the data and attempt to clean data assets after the fact, using queries to flag records with potential violations and using manual efforts to repair. Neither approach satisfactorily addresses the problem from an organizational point of view. In this thesis, we provide a conceptual model of constraint-based integrity management (CBIM) that flexibly combines both approaches in a systematic manner to provide improved integrity management. We perform a gap analysis that examines the criteria that are desirable for efficient management of data integrity. Our approach involves creating a Data Integrity Zone and an On Deck Zone in the database for separating the clean data from data that violates integrity constraints. We provide tool support for specifying constraints in a tabular form and generating triggers that flag violations of dependencies. We validate this by performing case studies on two systems used to manage healthcare data: PAL-IS and iMED-Learn. Our case studies show that using views to implement the zones does not cause any significant increase in the running time of a process...|$|E
40|$|According to a {{generally}} accepted concept, the stationary {{structure of the}} receptive field of a visually sensitive central neuron predetermines <b>transformation</b> <b>and</b> central <b>processing</b> of the incoming information, including that related to moving visual stimuli. We found, however, that {{a small group of}} visually sensitive neurons of the cat extrastriate cortical area 21 a does not fit this statement and exhibits no responses to stationary visual stimuli while responding vigorously to moving images. The results of our experiments showed that response patterns of these neurons to moving stimuli display high degrees of diversification <b>and</b> <b>processing</b> of incoming visual information. We suppose that these neurons may be strictly specialized in the detection <b>and</b> central <b>processing</b> of visual information necessary for perception of moving images...|$|R
40|$|Sensor {{data fusion}} imposes {{a number of}} novel {{requirements}} on query languages <b>and</b> query <b>processing</b> techniques. A spatial/temporal query language called SIGMAQL has been proposed to support the retrieval and fusion of multimedia information from multiple sources and databases. In this paper we investigate fusion techniques, multimedia data <b>transformations</b> <b>and</b> SIGMAQL query <b>processing</b> techniques for sensor data fusion. Fusion techniques including fusion by the merge operation, the detection of moving objects, and the incorporation of belief values, have been developed. An experimental prototype has been implemented and tested to demonstrate the feasibility of these techniques...|$|R
40|$|The {{purpose of}} the work: the {{development}} of the elements of the applied theory of the homogeous structures of <b>transformation</b> <b>and</b> digital <b>processing</b> of the signals in interconnection with {{the development of the}} conception of the schemotechnical modeling of the systems. The methodology of the schemotechnical modeling, the new elements of the theory of the objects of investigation, the new principles of building the parallel processors of signals and the equipment of the transformation of signals with the scientific properties have been offered. The developed cross-system and the design of the parallel processor of signals have been introduced in the scientific research process of the number of organisations. Available from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|Abstract. Architectures for {{focal plane}} image {{processing}} are discussed. On-chip image preprocessing for solid-state imagers using analog CCD circuits is described for low, medium, and high density detector arrays. A spatially parallel architecture for low density, high throughput applications is described. For sparse illumination or event detection, a content· addressable architecture is proposed. A new pipelined vector pixel pro· cessor architecture for medium density infrared staring focal plane arrays is described. Neighborhood reconstruction during serial readout of high density TV-quality imagers for a pixel processor is considered using delay and analog frame memory techniques. The potential of on-chip read/write analog frame memory for image <b>transformation</b> <b>and</b> frame·toframe <b>processing</b> is discussed. Subject terms: charge-coupled devices; focal plane image processing; solid-state imager sensors; space surveillance; focal plane array; machine vision...|$|R
40|$|In this paper, {{we discuss}} a vision based system for {{autonomous}} guidance of vehicles. An autonomous intelligent vehicle has {{to perform a}} number of functionalities. Segmentation of the road, determining the boundaries to drive in and recognizing the vehicles and obstacles around are the main tasks for vision guided vehicle navigation. In this article we propose a set of algorithms which lead to the solution of road and vehicle segmentation using data from a color camera. The algorithms described here combine gray value difference and texture analysis techniques to segment {{the road from the}} image, several geometric <b>transformations</b> <b>and</b> contour <b>processing</b> algorithms are used to segment lanes, and moving cars are extracted with the help of background modeling and estimation. The techniques developed have been tested in real road images and the results are presented...|$|R
40|$|Abstract—The {{compressed}} sensing (CS) {{theory is}} a novel way {{to break through the}} existent difficulty in ultra-wideband jamming method development. In this paper, the application of the CS theory in linear frequency modulated signal processing is introduced, a new retransmitted jamming system based on CS is designed with its composition and workflow. Then, two generation modes of jamming signal are illustrated, i. e. the sensing matrix <b>transformation</b> <b>and</b> recovery atomic <b>processing.</b> At last, due to the comparative analysis through a simulation in these two modes, the feasibility and efficiency of this method are verified...|$|R
40|$|Information {{becomes a}} {{more and more}} {{valuable}} asset in today’s organizations. Therefore the need of creating an integrated view over all available data sources arises. Several technical problems must be overcome {{in the design and}} implementation of a system for integrating different data sources. To the main obstacles count autonomy, data heterogeneity and different query capabilities of the repositories. This thesis presents the data integration system AMOS II, which is based on the wrapper-mediator approach. The main focus of this work lies on data model <b>transformation</b> <b>and</b> query <b>processing.</b> The following extensions to the AMOS II system are described in this thesis: • A framework for transforming various data models into the objectoriented model of AMOS II is presented. • The roles and tasks of wrappers are described. In particular their participation in query <b>processing</b> <b>and</b> query optimization is discussed. • A way for describing and utilizing the query capabilities of the differen...|$|R
40|$|The spatial {{variation}} of the backscattering cross section is {{the primary source of}} contrast in present applications of optical coherence tomography (OCT). We introduce and analyze a technique for obtaining OCT images of the local concentration of an absorbing compound in biological tissues and other highly scattering media. A pair of light-emitting diodes, one emitting in a vibrational absorption band of the chemical compound of interest and the other emitting just outside this band, are used as sources at the input of the interferometer. The differential absorption of the probe beam is determined by Fourier <b>transformation</b> <b>and</b> ratiometric <b>processing</b> of the measured interference signals. The ability of the technique to distinguish lipid and water inclusions in a scattering material is demonstrated with an OCT system that uses a pair of light-emitting-diode sources with center wavelengths of 1. 3 mu m and 1. 46 mu m. (C) 1998 Optical Society of America...|$|R
40|$|We {{present the}} first public release (v 0. 1) of the {{open-source}} GADGET Dataframe Library: gadfly. The aim of this package is to leverage {{the capabilities of the}} broader python scientific computing ecosystem by providing tools for analyzing simulation data from the astrophysical simulation codes GADGET and GIZMO using pandas, a thoroughly documented, open-source library providing high-performance, easy-to-use data structures that is quickly becoming the standard for data analysis in python. Gadfly is a framework for analyzing particle-based simulation data stored in the HDF 5 format using pandas DataFrames. The package enables efficient memory management, includes utilities for unit handling, coordinate <b>transformations,</b> <b>and</b> parallel batch <b>processing,</b> <b>and</b> provides highly optimized routines for visualizing smoothed-particle hydrodynamics (SPH) datasets. Comment: 5 pages, 4 figures. Submitted for publication in PAS...|$|R
40|$|In {{order to}} solve the problem of {{industrial}} sensor signal denoising, an integrated denoising method for sensor mixed noises based on wavelet packet transform and energy-correlation analysis is proposed. The architecture of proposed method is designed and the key technologies, such as wavelet packet <b>transformation,</b> energy-correlation analysis, <b>and</b> <b>processing</b> method of wavelet packet coefficients based on energy-correlation analysis, are presented. Finally, a simulation example for a specific signal and an application of shearer cutting current signal, which mainly contain white Gaussian noise and impact noise, are carried out, and the simulation and application results show that the proposed method is effective and is outperforming others...|$|R
40|$|Besides {{the common}} fiber {{technologies}} like splicing, polishing and coating {{there is a}} demand for specific methods for advanced packaging solutions {{in the field of}} telecom and medical applications. We present here three of such methods for lensing glass fibers {{that can be used to}} increase optical performance and reliability as well as to reduce the packaging efforts. These technologies are very useful for micro optical assembly, i. e. fiber connectors for high power applications, collimators and telecom transceivers and endoscope imaging or sensor systems. At the Fraunhofer Institute for Reliability and Microintegration Berlin special photonic packaging solutions are developed. Optical interconnects have been obtained a great importance for optical data transfer. Optical fibers are necessary for disturbance free communication of high data rates via long distances. Optical system components perform generation, distribution, <b>transformation,</b> amplifying <b>and</b> <b>processing</b> of optical signals. That's why optical systems are a assembly of different kinds of functional basic elements such as optical fibers, splitters, switches, modulators, transmitters and detectors. Furthermore, industrial applications need a reliable and cost effective coupling of optical fibers to those systems...|$|R
40|$|To {{support the}} {{retrieval}} and fusion of multimedia information from multiple sources and databases, a spatial/temporal query language called SQL is proposed. SQL {{is based upon}} the s-operator sequence and in practice expressible in SQL-like syntax. SQL allows a user to specify powerful spatial/temporal queries for both multimedia data sources and multimedia databases, eliminating the need to write different queries for each. A SQL query can be processed in the most effective manner by first selecting the suitable transformations of multimedia data to derive the multimedia static schema, <b>and</b> then <b>processing</b> the query with respect to this multimedia static schema. In this paper we illustrate this approach by data fusion examples, investigate multimedia data <b>transformations</b> <b>and</b> provide query <b>processing</b> algorithms. 1. INTRODUCTION With the rapid expansion of the wired and wireless networks, a large number of soft real-time, hard real-time and non-real-time sources of information need to be [...] ...|$|R
40|$|Cryptographic hash {{functions}} are built up from individual components, namely pre-processing, step <b>transformation,</b> <b>and</b> final <b>processing.</b> Some of the hash functions, such as SHA- 256 and STITCH- 256, employ non-linear message expan sion in their pre-processing stage. However, STITCH- 256 was claimed to produce high diffusion in its message expan sion. In a cryptographic algorithm, high diffusion is desirable as it helps prevent an attacker finding collision-producing differences, {{which would allow}} one to find collisions of the whole function without resorting to a brute force search. In this paper, we analyze d the diffusion property of message expansion of STITCH- 256 by observing {{the effect of a}} single bit difference over the output bits, and compare the result with that of SHA- 256. We repeated the same procedure in 3 experiments of different round. The results from the experiments showed that the minimal weight in the message ex pansion of STITCH- 256 is very much lower than that in the message expansion of SHA- 256, i. e. message expansion of STITCH- 256 produce high diffusion. Significantly, we showed that the probability to construct differential characteris tic in the message expansion of STITCH- 256 is reduced. </p...|$|R
40|$|Using of {{mathematical}} <b>transformations</b> <b>and</b> digital signal <b>processing</b> (DSP) methods of recorded voltage and currents in power systems {{in order to}} extract important informations about physical processes is already known. Higher resolution of measurements and massive introduction of PMU devices in the power systems allows appliance of DSP - based fault location methods in complex power networks. High availability of power systems is required in the modern societies. Fast fault location algorithms and consequent fast repairing of faults is necessary condition for high availability of power networks. Here is represented a numerical procedure for fault location and testing of method already introduced in scientific literature on complex configuration of power network consisting of XLPE underground submarine and land cables between land and offshore substation connecting large wind park with main power system...|$|R
30|$|In this paper, {{we propose}} a novel {{polarization}} filtering-based approach for RSNs, the suggested scheme suppresses the interferences from the radar members {{by using the}} oblique projection polarization filtering (OPPF) [16 – 20], and the OPPF is the extension to the CPF and the NPSPF. The proposed OPPF can separate the signals effectively if the polarization information of them is different, and the polarized states are not needed to be orthogonal due to {{the merits of the}} oblique projections [20]. The detailed implementation of the OPPF is to construct the polarization subspaces of the target and the interference, respectively, then the filtering operator is established according to the oblique projection operator. After passing through the OPPF, the interferences (other radar members' returned signals) are effectively cancelled while keeping the desired return with the same amplitude/phase and the polarization before the operation by the OPPF, even if their waveforms and polarized states are not orthogonal. The proposed scheme can effectively separate the desired returned signal and the interferences without additional <b>transformation</b> <b>and</b> compensation <b>processing.</b> With the desired return not suffering distortions after separation, the scheme is still valid when the desired return and the interferences hold the same polarized angle but with different phase difference in polarized angle.|$|R
40|$|Abstract—In this paper, {{we propose}} a data {{transformation}} pattern to transform sequential data into {{a set of}} binary/categorical features and numerical features to enable data analysis. These features capture both structural and temporal information inherent in sequential data. I. CATEGORIZATION Assuming data analysis consists of four phases: data cleansing, data <b>transformation,</b> data <b>processing,</b> <b>and</b> result validation, the proposed pattern is a data transformation pattern. II. INTENT Extract features from sequential data composed of events {{for the purpose of}} data analysis. The features should capture event ordering information inherent in the data. If the events are structured, the features should capture structural information inherent in events...|$|R
5000|$|... #Subtitle level 3: Segregation <b>and</b> <b>processing</b> <b>and</b> <b>transformation</b> {{of solid}} wastes ...|$|R
40|$|Copyright © 2013 Norziana Jamil et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Cryptographic hash functions are built up from individual components, namely pre-processing, step <b>transformation,</b> <b>and</b> final <b>processing.</b> Some of the hash functions, such as SHA- 256 and STITCH- 256, employ non-linear message expan-sion in their pre-processing stage. However, STITCH- 256 was claimed to produce high diffusion in its message expan-sion. In a cryptographic algorithm, high diffusion is desirable as it helps prevent an attacker finding collision-producing differences, which would allow one to find collisions of the whole function without resorting to a brute force search. In this paper, we analyzed the diffusion property of message expansion of STITCH- 256 by observing {{the effect of a}} single bit difference over the output bits, and compare the result with that of SHA- 256. We repeated the same procedure in 3 experiments of different round. The results from the experiments showed that the minimal weight in the message ex-pansion of STITCH- 256 is very much lower than that in the message expansion of SHA- 256, i. e. message expansion of STITCH- 256 produce high diffusion. Significantly, we showed that the probability to construct differential characteris-tic in the message expansion of STITCH- 256 is reduced...|$|R
40|$|A {{new class}} of images called salient stills is {{demonstrated}} and a software development platform for their creation is discussed. These images do not represent one discrete moment of time, as do a photograph or single video frame. Rather, one image reflects the aggregate of the temporal changes that occur in a moving image sequence with the salient features preserved. By the application of an affine <b>transformation</b> <b>and</b> non-linear temporal <b>processing,</b> multiple frames of an image sequence, which may include variations in focal-length or field-of-view, are combined to create a single still image. The still image may have multi-resolution patches, a larger field-of-view, or higher overall resolution than any individual frame in the original image sequence. It may also contain selected salient objects from {{any one of the}} sequence of video frames. The still can be created automatically or with user intervention. A by-product of the salient still process is a structured representation of moving image data. [Keywords: image processing, optical flow, structured video, image reproduction...|$|R
30|$|Aiming at {{resisting}} both routine unmalicious degradations and malicious attacks, various {{approaches have}} been proposed in literatures for constructing image hashes, {{although there is no}} universallyoptimal hashing approach that is robust against all types of attacks. For example, Radon Soft Hash algorithm (RASH) [12] shows robustness against geometric <b>transformation</b> <b>and</b> some image <b>processing</b> attacks using Radon transform and principle component analysis (PCA). Swaminathan's hashing scheme [8] incorporates pseudorandomization into Fourier-Mellin transform to achieve better robustness to geometric operations. However, it suffers from some classical signal processing operations such as noising. It was also proposed in [9] to generate the hash by detecting invariant feature points, though the expensive searching and removal of feature points by malicious attacks such as cropping and blurring limit its performance in practice. Other content-preserving features based on statistics [1] and spectrum information [2, 13] have also contributed to the development of image hashing and enlightened some novel directions.|$|R
30|$|A {{supply chain}} refers to “a network of materials, information, <b>and</b> {{services}} <b>processing</b> {{links with the}} characteristics of supply, <b>transformation,</b> <b>and</b> demand” (Chen and Paulraj 2004, p. 119).|$|R
40|$|Cost-efficient use of {{big data}} by {{enterprises}} is challenging. Data from multiple heterogeneous sources are typically combined. This data {{needs to be}} handled by various interacting components in different systems for automated <b>transformation,</b> filtering, <b>processing</b> <b>and</b> analysis, {{as well as for}} representation according to adopted industry standards and interpretation according to prevailing industry models. The derived information may be shared with many other systems, and applied for many purposes. Hence, big data interoperability and integration is a major concern that must be addressed at different levels and along the (extended) value chain. The Big Data Interoperability for Enterprises Workshop comprises eight paper presentations covering data interoperability problems and solutions at different points in the value chain and for different industrial sectors and application areas...|$|R
