10000|10000|Public
5|$|Analysis of {{mitochondrial}} DNA, from {{a limited}} number of specimens, suggested that there was gene flow within the continental Asian populations until the 20th century reductions in range, and that Australia was colonized only in the Late Pleistocene, some 35,000 years ago. This has been corroborated by nDNA microsatellite analyses with four times <b>the</b> <b>sample</b> <b>size.</b> This study further suggests that the Australian population is quite inbred. As there exists the possibility of (limited) hybridization with the genetically distinct brolga, the Australian sarus crane can be expected to be an incipient species.|$|E
5|$|The maximum spacing {{estimator}} is {{a consistent}} estimator {{in that it}} converges in probability to the true value of the parameter, θ0, as <b>the</b> <b>sample</b> <b>size</b> increases to infinity. The consistency of maximum spacing estimation holds under much more general conditions than for maximum likelihood estimators. In particular, {{in cases where the}} underlying distribution is J-shaped, maximum likelihood will fail where MSE succeeds. An example of a J-shaped density is the Weibull distribution, specifically a shifted Weibull, with a shape parameter less than 1. The density will tend to infinity as x approaches the location parameter rendering estimates of the other parameters inconsistent.|$|E
25|$|The {{results of}} the INAA study were later defended in March 2006 in two {{articles}} in Latin American Antiquity, in particular contrasting <b>the</b> <b>sample</b> <b>size</b> of the INAA study (roughly 1000) with <b>the</b> <b>sample</b> <b>size</b> of the petrography analysis (20).|$|E
3000|$|... grow rapidly when <b>the</b> <b>sample</b> <b>sizes</b> increase. Therefore, we can, so far, only target small <b>sample</b> <b>sizes</b> in our work.|$|R
40|$|AbstractKiefer {{considered}} the asymptotics of q-sample Cramer-Von Mises statistics for a fixed q and <b>sample</b> <b>sizes</b> tending to infinity. For univariate observations, McDonald proved the asymptotic normality of these statistics when q goes to infinity while <b>the</b> <b>sample</b> <b>sizes</b> stay fixed. Here we define {{a class of}} multivariate randomness statistics that generalizes the class considered by McDonald. We also prove the asymptotic normality of such statistics when <b>the</b> <b>sample</b> <b>sizes</b> stay fixed while q tends to infinity...|$|R
40|$|The aim of {{this paper}} is to extend one-way random effects ANOVA to {{situations}} in which we can not previously know <b>the</b> <b>sample</b> <b>sizes.</b> In this case it is more appropriate to consider <b>the</b> <b>sample</b> <b>sizes</b> as realizations of random variables. We will obtain the distribution of the F-tests, which has random degrees of freedom for the errors. Moreover we will show the equivalence between two expressions for the F-tests...|$|R
25|$|In {{both cases}} the {{data suggest that the}} null {{hypothesis}} is false (that is, the coin is not fair somehow), but changing <b>the</b> <b>sample</b> <b>size</b> changes the p-value. In the first case, <b>the</b> <b>sample</b> <b>size</b> is not large enough to allow the null hypothesis to be rejected at the 0.05 level (in fact, the p-value can never be below 0.05 for the coin example).|$|E
25|$|This {{index is}} {{relatively}} {{independent of the}} population density but is affected by <b>the</b> <b>sample</b> <b>size.</b>|$|E
25|$|This {{demonstrates}} that in interpreting p-values, one must also know <b>the</b> <b>sample</b> <b>size,</b> which complicates the analysis.|$|E
30|$|In addition, {{although}} <b>the</b> overall <b>sample</b> <b>size</b> in {{this study}} was sufficient for the estimation of important psychometric properties, <b>the</b> <b>sample</b> <b>sizes</b> for some of the subgroup analyses were small enough to adversely affect the power of the hypothesis tests. Future studies using the GRCD will undoubtedly use larger samples.|$|R
30|$|In most {{countries}} <b>the</b> <b>sample</b> <b>sizes</b> {{for some of}} the categories are small and the corresponding estimates are unstable.|$|R
5000|$|There is {{a formula}} {{to compute the}} rank-biserial from the Mann-Whitney U and <b>the</b> <b>sample</b> <b>sizes</b> of each group: ...|$|R
25|$|This is {{the square}} of the {{coefficient}} of variation divided by N - 1 where N is <b>the</b> <b>sample</b> <b>size.</b>|$|E
25|$|<b>The</b> <b>sample</b> <b>size</b> was 1,978 {{adults in}} September and 1,878 in November. The results were {{weighted}} and are said by YouGov to {{be representative of}} all GB adults (aged 18+).|$|E
25|$|Efficiency, i.e., it {{achieves}} the Cramér–Rao {{lower bound}} when <b>the</b> <b>sample</b> <b>size</b> tends to infinity. This means that no consistent estimator has lower asymptotic {{mean squared error}} than the MLE (or other estimators attaining this bound).|$|E
40|$|A {{substantial}} {{investment has}} been made in the generation of large public resources designed to enable the identification of tag SNP sets, but data establishing the adequacy of <b>the</b> <b>sample</b> <b>sizes</b> used are limited. Using large-scale empirical and simulated data sets, we found that <b>the</b> <b>sample</b> <b>sizes</b> used in <b>the</b> HapMap project are sufficient to capture common variation, but that performance declines substantially for variants with minor allele frequencies of < 5 %...|$|R
40|$|Orban and Wolfe (1982) and Kim (1999) {{provided}} the limiting distribution for linear placement statistics under null hypotheses only {{when one of}} <b>the</b> <b>sample</b> <b>sizes</b> goes to infinity. In this paper we prove the asymptotic normality and the weak convergence of the linear placement statistics of Orban and Wolfe (1982) and Kim (1999) when <b>the</b> <b>sample</b> <b>sizes</b> of each group go to infinity simultaneously. Central limit theorem Weak convergence Distribution-free procedure Linear placement statistic...|$|R
3000|$|... are <b>the</b> <b>sample</b> <b>sizes</b> for {{treatments}} i and j. This {{process is}} repeated for every pairwise-comparison and the maximum is reported.|$|R
25|$|The pre-IPO {{studies are}} {{sometimes}} criticized because <b>the</b> <b>sample</b> <b>size</b> is relatively small, the pre-IPO transactions {{may not be}} arm's length, and the financial structure and product lines of the studied companies may have changed during the three year pre-IPO window.|$|E
25|$|Suppose {{one has a}} {{sequence}} of observations {X1, X2, …} from a normal N(μ, σ2) distribution. To estimate μ based on the first n observations, one can use the sample mean: Tn=(X1 + … + Xn)/n. This defines {{a sequence}} of estimators, indexed by <b>the</b> <b>sample</b> <b>size</b> n.|$|E
25|$|The CPS {{began in}} 1940, and {{responsibility}} for conducting the CPS {{was given to the}} Census Bureau in 1942. In 1994 the CPS was redesigned. CPS is a survey that is: employment-focused, enumerator-conducted, continuous, and cross-sectional. The BLS increased <b>the</b> <b>sample</b> <b>size</b> by 10,000 as of July 2001. The sample represents the civilian noninstitutional population.|$|E
30|$|<b>The</b> mean patient <b>sample</b> <b>size</b> of presentations with a {{subsequent}} full text publication {{was significantly higher}} than the presentations without a full text publication (p[*]=[*] 0.039). <b>The</b> mean <b>sample</b> <b>size</b> of presentations with a corresponding publication was 154 (standard error [SE] = 27) while for presentations without corresponding publications <b>the</b> mean <b>sample</b> <b>size</b> was 93 (SE[*]=[*] 13).|$|R
2500|$|If using Student's {{original}} {{definition of}} the t-test, the two populations being compared {{should have the same}} variance (testable using F-test, Levene's test, Bartlett's test, or the Brown–Forsythe test; or assessable graphically using a Q–Q plot). [...] If <b>the</b> <b>sample</b> <b>sizes</b> in <b>the</b> two groups being compared are equal, Student's original t-test is highly robust to the presence of unequal variances. Welch's t-test is insensitive to equality of the variances regardless of whether <b>the</b> <b>sample</b> <b>sizes</b> are similar.|$|R
30|$|<b>The</b> <b>sample</b> <b>sizes</b> of lab {{experiments}} are criticized {{as being too}} small, although this is refuted such that <b>sample</b> <b>sizes</b> are stated to adequately correspond to this method and thus yield valid assertions.|$|R
25|$|There are {{two basic}} ways to reduce random error in an {{epidemiological}} study. The first is to increase <b>the</b> <b>sample</b> <b>size</b> of the study. In other words, add more subjects to your study. The second {{is to reduce the}} variability in measurement in the study. This might be accomplished by using a more precise measuring device or by increasing the number of measurements.|$|E
25|$|An {{independent}} survey {{taken in}} early November showed 48% of {{people wanted to}} wait and see what evidence the police had before they made a judgment on the raids, while 36% said they were already satisfied with the way the police reacted and 13% thought the police over-reacted. The statistics were much different among Māori, with 40% of Māori saying the police had over-reacted. <b>The</b> <b>sample</b> <b>size</b> was 750 people.|$|E
25|$|Important {{examples}} include the sample variance and sample standard deviation. Without Bessel's correction (using <b>the</b> <b>sample</b> <b>size</b> n instead of the degrees of freedom n−1), these are both negatively biased but consistent estimators. With the correction, the corrected sample variance is unbiased, while the corrected sample standard deviation is still biased, but less so, and both are still consistent: the correction factor converges to 1 as sample size grows.|$|E
40|$|Step-up {{procedures}} {{have been shown}} to be powerful testing methods in clinical trials for comparisons of several treatments with a control. In this paper, a determination of <b>the</b> optimal <b>sample</b> <b>size</b> for a step-up procedure that allows a pre-specified power level to be attained is discussed. Various definitions of power, such as all-pairs power, any-pair power, per-pair power and average power, in one- and two-sided tests are considered. An extensive numerical study confirms that square root allocation of <b>sample</b> <b>size</b> among treatments provides a better approximation of <b>the</b> optimal <b>sample</b> <b>size</b> relative to equal allocation. Based on square root allocation, tables are constructed, and users can conveniently obtain <b>the</b> approximate required <b>sample</b> <b>size</b> for <b>the</b> selected configurations of parameters and power. For clinical studies with difficulties in recruiting patients or when additional subjects lead to a significant increase in cost, a more precise computation of <b>the</b> required <b>sample</b> <b>size</b> is recommended. In such circumstances, our proposed procedure may be adopted to obtain <b>the</b> optimal <b>sample</b> <b>size.</b> It is also found that, contrary to conventional belief, the optimal allocation may considerably reduce <b>the</b> total <b>sample</b> <b>size</b> requirement in certain cases. The determination of <b>the</b> required <b>sample</b> <b>sizes</b> using both allocation rules are illustrated with two examples in clinical studies...|$|R
30|$|<b>The</b> test <b>sample</b> {{is placed}} {{between the two}} environments. <b>The</b> <b>Samples</b> <b>sizes</b> are (27  ×  27  × e) cm 3, the {{thickness}} e varies from 2 to 6  cm.|$|R
5000|$|The {{maximum value}} of U {{is the product}} of <b>the</b> <b>sample</b> <b>sizes</b> for <b>the</b> two <b>samples.</b> In such a case, the [...] "other" [...] U would be 0.|$|R
25|$|To {{adjust for}} a large {{sampling}} fraction, the fpc factored into the calculation of the margin of error, which {{has the effect of}} narrowing the margin of error. It holds that the fpc approaches zero as <b>the</b> <b>sample</b> <b>size</b> (n) approaches the population size (N), which has the effect of eliminating the margin of error entirely. This makes intuitive sense because when N = n, the sample becomes a census and sampling error becomes moot.|$|E
25|$|The sample extrema can be {{used for}} a simple {{normality}} test, specifically of kurtosis: one computes the t-statistic of the sample maximum and minimum (subtracts sample mean and divides by the sample standard deviation), and if they are unusually large for <b>the</b> <b>sample</b> <b>size</b> (as per the three sigma rule and table therein, or more precisely a Student's t-distribution), then the kurtosis of the sample distribution deviates significantly from that of the normal distribution.|$|E
25|$|Using several {{data sets}} (including {{breeding}} bird surveys from New York and Pennsylvania and moth collections from Maine, Alberta and Saskatchewan) Frank W. Preston (1948) argued that species abundances (when binned logarithmically in a Preston plot) follow a Normal (Gaussian) distribution, {{partly as a}} result of the Central Limit Theorem (Figure 4). This means that the abundance distribution is Lognormal. According to his argument, the right-skew observed in species abundance frequency histograms (including those described by Fisher et al. (1943)) was, in fact, a sampling artifact. Given that species toward the left side of the x-axis are increasingly rare, they may be missed in a random species sample. As <b>the</b> <b>sample</b> <b>size</b> increases however, the likelihood of collecting rare species in a way that accurately represents their abundance also increases, and more of the normal distribution becomes visible. The point at which rare species cease to be sampled has been termed Preston's veil line. As <b>the</b> <b>sample</b> <b>size</b> increases Preston's veil is pushed farther to the left and more of the normal curve becomes visible(Figure 6). Interestingly, Williams' moth data, originally used by Fisher to develop the logseries distribution, became increasingly lognormal as more years of sampling were completed.|$|E
30|$|The {{study which}} has begun from this {{work can be}} widened out {{comparing}} the data obtained with a control group and increasing <b>the</b> <b>samples</b> <b>size.</b>|$|R
3000|$|... 2 test [15] {{to higher}} {{dimensions}} {{suffers from the}} curse of dimensionality [40] caused by the space sparsity unless <b>the</b> <b>sample</b> <b>sizes</b> are large enough.|$|R
40|$|This paper investigates <b>the</b> {{utility of}} <b>sampling</b> as an evaluation-relaxation {{technique}} in genetic algorithms (GAs). In many real-world applications, sampling {{can be used}} to generate a less accurate, but computationally inexpensive fitness evaluator to speed GAs up. This pa-per focuses on the problem of polynomial regression as an example of problems with positive dependency among genes. Via statistical analysis of the noise introduced by sampling, this paper develops facet-wise models for <b>the</b> optimal <b>sampling</b> <b>size,</b> and these models are empir-ically verified. The results show that when the population is <b>sized</b> properly, small <b>sampling</b> <b>sizes</b> are preferred for most applications. When a fixed population size is adopted, which is usually the case in real-world applications, an optimal <b>sampling</b> <b>size</b> exists. If <b>the</b> <b>sampling</b> <b>size</b> is too small, <b>the</b> <b>sampling</b> noise increases, and GAs would perform poorly because of an insufficiently large population. If <b>the</b> <b>sampling</b> <b>size</b> is too large, the GA would spend too much time in fitness calculation and cannot perform well either within limited run duration. ...|$|R
