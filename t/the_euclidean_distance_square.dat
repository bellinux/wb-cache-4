0|10000|Public
40|$|Abstract. Within image {{analysis}} the distance transform has many applications. The distance transform measures {{the distance of}} each object point from the nearest boundary. For ease of computation, a commonly used approximate algorithm is the chamfer distance transform. This paper presents an efficient linear-time algorithm for calculating <b>the</b> true <b>Euclidean</b> distance-squared of each point from the nearest boundary. It works by performing a 1 D distance transform on each row of the image, and then combines the results in each column. It is shown that <b>the</b> <b>Euclidean</b> <b>distance</b> <b>squared</b> transform requires fewer computations than the commonly used 5 x 5 chamfer transform. ...|$|R
50|$|A {{review of}} cluster {{analysis}} in health psychology research {{found that the}} most common distance measure in published studies in that research area is <b>the</b> <b>Euclidean</b> <b>distance</b> or <b>the</b> <b>squared</b> <b>Euclidean</b> <b>distance.</b>|$|R
50|$|A {{polynomial}} {{of degree}} 0 is always homogeneous; {{it is simply}} {{an element of the}} field or ring of the coefficients, usually called a constant or a scalar. A form of degree 1 is a linear form. A form of degree 2 is a quadratic form. In geometry, <b>the</b> <b>Euclidean</b> <b>distance</b> is <b>the</b> <b>square</b> root of a quadratic form.|$|R
40|$|Abstract. Short proofs {{are given}} to various {{characterizations}} of <b>the</b> (circum-) <b>Euclidean</b> <b>squared</b> <b>distance</b> matrices. Linear preserver problems related to these matrices are discussed. Key words. Distance geometry, multidimensional scaling, eigenvalues, interlacing inequalities, linear preservers. AMS subject classifications. 51 K 05, 15 A 57, 15 A 48 1. Introduction. Distanc...|$|R
40|$|Abstract. Corrections {{are given}} {{to some of the}} proofs of the paper above In this note, we point out and correct some errors in the proofs of the main results in the paper [1]. The main results {{themselves}} are correct as stated, but the proofs need modification. To begin, the proof of Lemma 3. 3 in [1] is incorrect. Thus, h in [1,(3. 12) ] is the linearization of a mapping ∂C → ∂C, and so has n degrees of freedom, not (n+ 1) as indicated there. Moreover, the information given on solving the PDE in (3. 12) is insufficient. In addition, the proof of Lemma 3. 2 also requires the Nash-Moser implicit function theorem; the linearization L of H is not surjective since one has no gain of regularity in the τ direction. In Lemma 1 below, we state a slightly more general version of Lemmas 3. 2 and 3. 3 of [1] and then proceed with the proof. We recall that the main point of these results is to construct a foliation of prescribed mean curvature with harmonic coordinates along the leaves, such that the lapse and shift are prescribed at the boundary. The rest of the work in [1, § 3] then proceeds as before. We begin by describing the initial set-up of the issue. Let C 0 be the unit ball B n (1) in Cartesian coordinates x i, 1 ≤ i ≤ n and let D 0 = C 0 × [0, 1] be the vertical cylinder over C 0 in coordinates x α = (τ, x i). Let ρ 2 = ∑ (x i) 2 − 1 be <b>the</b> <b>Euclidean</b> <b>distance</b> (<b>squared)</b> to ∂C 0 and view the grap...|$|R
40|$|Optimal {{ordering}} procedure, {{which is}} computationally {{complex in the}} conventional vertical Bell-labs layered space-time (V-BLAST) algorithm, can be simplified. Among those low-complexity alternatives to the optimal ordering, non-uniform power allocation is intuitively attractive. The unified analysis of power optimized zero forcing (ZF) VBLAST proposed by prior work ([32]) is extended from BPSK to high-level modulations (also known as high-order modulations) by introducing a generic SER approximation [...] -average union bound (AUB) [...] -for multi-path fading channels. This AUB model represents SER by using two parameters related to the constellation [...] -Ds, <b>the</b> normalized minimum <b>Euclidean</b> <b>distance</b> <b>squared</b> and Nc, <b>the</b> average number of the nearest constellation neighbors. First of all, the validity of AUB is demonstrated. Consequently, general average block error rate (BLER) of ZF-VBLAST can be approximated by a closed-form expression of D s and Nc for high-level modulation, the optimum power allocation solution are obtained at high and low SNR. The SNR gain and the robustness of this optimization are also rigorously defined and investigated in a generic manner. Additionally, the above performance variations related to the constellation size are discussed. At last, an efficient Monte Carlo simulation methodology, parallel MatlabRTM, is adopted for the simulations in this thesis. The observations of parallelism efficiency are summarized...|$|R
40|$|In data mining, it {{is usually}} to {{describe}} a set of individuals using some summaries (means, standard deviations, histograms, confidence intervals) that generalize individual descriptions into a typology description. In this case, data can be described by several values. In this paper, we propose an approach for computing basic statics for such data, and, in particular, for data described by numerical multi-valued variables (interval, histograms, discrete multi-valued descriptions). We propose to treat all numerical multi-valued variables as distributional data, i. e. as individuals described by distributions. To obtain new basic statistics for measuring the variability and the association between such variables, we extend the classic measure of inertia, calculated with <b>the</b> <b>Euclidean</b> <b>distance,</b> using <b>the</b> <b>squared</b> Wasserstein <b>distance</b> defined between probability measures. The distance is a generalization of the Wasserstein distance, that is a distance between quantile functions of two distributions. Some properties of such a distance are shown. Among them, we prove the Huygens theorem of decomposition of the inertia. We show {{the use of the}} Wasserstein distance and of the basic statistics presenting a k-means like clustering algorithm, for the clustering of a set of data described by modal numerical variables (distributional variables), on a real data set. Keywords: Wasserstein distance, inertia, dependence, distributional data, modal variables. Comment: 19 pages, 3 figure...|$|R
40|$|We apply {{upper and}} lower {{compensated}} convex transforms, which are `tight' one-sided approximations of a given function, to the extraction of fine geometric singularities from semiconvex/semiconcave functions and DC-functions in R^n (difference of convex functions). Well-known examples of (locally) semiconcave functions include <b>the</b> <b>Euclidean</b> <b>distance</b> and <b>squared</b> <b>distance</b> functions. For a locally semiconvex function f with general modulus, we show that `locally' a point is a singular (non-differentiable) point {{if and only if}} it is a scale 1 -valley point, and if x is a singular point, then locally the limit of the scaled valley transform exists at every point x and _λ→∞λ V_λ (f) (x) =r_x^ 2 / 4, where r_x is the radius of the minimal bounding sphere of the (Fréchet) subdifferential ∂_- f(x) and V_λ (f) (x) is the valley transform at x. Thus the limit function V_∞(f) (x) :=_λ→+∞λ V_λ (f) (x) =r_x^ 2 / 4 gives a `scale 1 -valley landscape function' of the singular set for a locally semiconvex function f, and also provides an asymptotic expansion of the upper transform C^u_λ(f) (x) when λ→∞. For a locally semiconvex function f with linear modulus we show that the limit of the gradient of the upper compensated convex transform _λ→+∞∇ C^u_λ(f) (x) exists and equals the centre of the minimal bounding sphere of ∂_- f(x, and that for a DC-function f=g-h, the scale 1 -edge transform satisfies _λ→+∞λ E_λ (f) (x) ≥ (r_g,x-r_h,x) ^ 2 / 4, where r_g,x and r_h,x are the radii of the minimal bounding spheres of the subdifferentials ∂_- g and ∂_- h of the convex functions g and h at x respectively. Comment: A Chinese version of the material in this manuscript has been published in Zhang, Kewei, Crooks, Elaine and Orlando, Antonio, Compensated convex transforms and geometric singularity extraction from semiconvex functions (in Chinese), Sci. Sin. Math., 46 (2016) 747 - 768, doi: 10. 1360 /N 012015 - 0033...|$|R
40|$|Maximum-a-posteriori (MAP) {{estimation}} is {{the main}} Bayesian estimation methodology {{in many areas of}} data science such as mathematical imaging and machine learning, where high dimensionality is addressed by using models that are log-concave and whose posterior mode can be computed efficiently by using convex optimisation algorithms. However, despite its success and rapid adoption, MAP estimation is not theoretically well understood yet, and the prevalent view is that it is generally not proper Bayesian estimation in a decision-theoretic sense. This paper presents a new decision-theoretic derivation of MAP estimation in Bayesian models that are log-concave. Our analysis is based on differential geometry and proceeds as follows. First, we exploit the log-concavity of the model to induce a Riemannian geometry on the parameter space. We then use differential geometry to identify the natural or canonical loss function to perform Bayesian point estimation in that Riemannian manifold. For log-concave models this canonical loss is the Bregman divergence of the negative log posterior density, a similarity measure rooted in convex analysis that in addition to the relative position of points also takes into account the geometry of the space, and which generalises <b>the</b> <b>Euclidean</b> <b>squared</b> <b>distance</b> to non-Euclidean settings. We then show that the MAP estimator is the Bayesian estimator that minimises the expected canonical loss, and that the posterior mean or MMSE estimator minimises the expected dual canonical loss. Finally, we establish universal performance and stability guarantees for MAP and MMSE estimation in high dimensional log-concave models. These results provide a new understanding of MAP and MMSE estimation under log-concavity, and reveal new insights about their good empirical performance and about the roles that log-concavity plays in high dimensional inference problems...|$|R
5000|$|<b>The</b> {{standard}} <b>Euclidean</b> <b>distance</b> can be <b>squared</b> {{in order}} to place progressively greater weight on objects that are farther apart. In this case, the equation becomes ...|$|R
40|$|The generic {{number of}} {{critical}} points of <b>the</b> <b>Euclidean</b> <b>distance</b> function from a data {{point to a}} variety is called <b>the</b> <b>Euclidean</b> <b>distance</b> degree. <b>The</b> two special loci of the data points where the number of critical points is smaller then the ED degree are called <b>the</b> <b>Euclidean</b> <b>Distance</b> Data Singular Locus and <b>the</b> <b>Euclidean</b> <b>Distance</b> Data Isotropic Locus. In this article we present connections between these two special loci of an affine cone and its dual cone. Comment: 11 pages, 3 figure...|$|R
30|$|Most {{research}} on content-based music recommendation has adopted <b>the</b> <b>Euclidean</b> <b>distance</b> function [31] (<b>Euclidean</b> <b>distance</b> {{is a special}} case of the Minkowski <b>distance).</b> When using <b>the</b> <b>Euclidean</b> <b>distance</b> to measure music similarity, each dimension is given the same importance. However, each music feature has a different impact on the similarity measure. For example, rhythm is usually more important in similarity calculations for rock music {{than it is for}} classical music [32]. Taking the weight of each dimension into consideration in <b>the</b> <b>Euclidean</b> <b>distance</b> calculation gives <b>the</b> weighted <b>Euclidean</b> <b>distance.</b> To limit <b>the</b> computation, the square root is not calculated. Instead, <b>the</b> weighted <b>squared</b> <b>Euclidean</b> <b>distance</b> is used as the music distance. As every user has a different rating, the weight is separately computed for each.|$|R
40|$|We {{study the}} {{minimum number of}} {{different}} distances defined by {{a finite number of}} points in the following cases: a) we consider metrics different from <b>the</b> <b>euclidean</b> <b>distance</b> in <b>the</b> plane, b) we consider <b>the</b> <b>euclidean</b> <b>distance</b> but restricted to subsets of the plane of special interest, c) we consider other topological surfaces: the cylinder and the flat torus. All these results extend those obtained by Erdös and other mathematicians for <b>the</b> <b>euclidean</b> <b>distance</b> in <b>the</b> plane...|$|R
3000|$|... {{denotes the}} size of the patch set. We used <b>the</b> <b>Euclidean</b> <b>distance</b> in <b>the</b> {{clustering}} (and in (3)) and choose the number of clusters depending on the desired vocabulary size. The choice of <b>the</b> <b>Euclidean</b> <b>distance</b> to compare SIFT features is common [5].|$|R
40|$|In {{a recent}} paper, Drusvyatskiy, Lee, Ottaviani, and Thomas {{establish}} a "transfer principle" {{by means of}} which <b>the</b> <b>Euclidean</b> <b>distance</b> degree of an orthogonally-stable matrix variety can be computed from <b>the</b> <b>Euclidean</b> <b>distance</b> degree of its intersection with a linear subspace. We generalise this principle...|$|R
3000|$|... : <b>the</b> <b>Euclidean</b> <b>distance,</b> <b>the</b> CIE 94 color difference, and the Mahalanobis distance. In {{order to}} {{guarantee}} that the distance is perceptually uniform, the CIE 94 color difference equation is used instead of <b>the</b> <b>Euclidean</b> <b>distance</b> in CIELab color space [29, 30]. While <b>the</b> <b>Euclidean</b> <b>distance</b> and <b>the</b> CIE 94 simply measure the geometric distance between two feature vectors in <b>the</b> <b>Euclidean</b> coordinates without considering the distribution of color features, the Mahalanobis distance explicitly considers the distribution of color features after clustering process [31]. Three distances are defined as follows.|$|R
50|$|A subset S of Rn is bounded {{with respect}} to <b>the</b> <b>Euclidean</b> <b>distance</b> {{if and only if}} it bounded as subset of Rn with the product order. However, S may be bounded as subset of Rn with the {{lexicographical}} order, but not {{with respect to}} <b>the</b> <b>Euclidean</b> <b>distance.</b>|$|R
50|$|Here, {{the norm}} is <b>the</b> <b>Euclidean</b> <b>distance</b> in Cn.|$|R
30|$|In {{this section}} the {{definition}} of <b>the</b> <b>Euclidean</b> <b>distance</b> matrix is given, {{and the relationship between}} points and distances is summarized. A characterization theorem for <b>the</b> <b>Euclidean</b> <b>distance</b> matrix is proved in a concise way that brings out the underlying structure and is readily applicable to the algorithms that follow.|$|R
40|$|We obtain several {{formulas}} for <b>the</b> <b>Euclidean</b> <b>distance</b> degree (ED degree) of {{an arbitrary}} nonsingular variety in projective space: {{in terms of}} Chern and Segre classes, Milnor classes, Chern-Schwartz-MacPherson classes, and an extremely simple formula equating <b>the</b> <b>Euclidean</b> <b>distance</b> degree of X with the Euler characteristic of an open subset of X...|$|R
40|$|This study {{investigated}} {{the effect of the}} curved path followed by the hand on the estimation of <b>the</b> <b>euclidean</b> <b>distance</b> (straight-line) between <b>the</b> start- and end-points of the movement. This question has initially been raised in haptics by Lederman, Klatzky and Barber (1), who reported that <b>the</b> <b>euclidean</b> <b>distance</b> (ED) is increasingl...|$|R
40|$|Time-series, or time-sequence, {{data show}} {{the value of}} a {{parameter}} over time. A common query with time-series data is to find all sequences which are similar to a given sequence. The most common technique for evaluating similarity between two sequences involves calculating <b>the</b> <b>Euclidean</b> <b>distance</b> between them. However, many examples can be given where two similar sequences are separated by a large <b>Euclidean</b> <b>distance.</b> In this paper, instead of calculating <b>the</b> <b>Euclidean</b> <b>distance</b> directly between two sequences, the sequences are transformed into a feature vector and <b>the</b> <b>Euclidean</b> <b>distance</b> between <b>the</b> feature vectors is then calculated. Results show that this approach is superior for finding similar sequences. 2...|$|R
3000|$|Also, {{it is of}} {{importance}} {{to note that the}} right-hand side of (14) based on <b>the</b> <b>Euclidean</b> <b>distance</b> is strictly Schur-concave whereas, had it been based on the city-block distance, the corresponding expression would have been Schur-concave, but not strictly Schur-concave. This follows from the fact that (a) <b>the</b> <b>Euclidean</b> <b>distance</b> between P [...]...|$|R
5000|$|... <b>the</b> <b>square</b> of <b>the</b> <b>Euclidean</b> <b>distance</b> between <b>the</b> vectors [...] and [...]|$|R
5000|$|... where [...] denotes <b>the</b> <b>Euclidean</b> <b>distance,</b> [...] is <b>the</b> general {{solution}} satisfied ...|$|R
5000|$|... so <b>the</b> <b>Euclidean</b> <b>distance</b> in <b>the</b> {{diffusion}} coordinates approximates {{the diffusion}} distance.|$|R
5000|$|The Paris metric {{makes the}} plane {{into a real}} tree. It is defined as follows: one fixes an origin , and if two points {{are on the same}} ray from , their {{distance}} is defined as <b>the</b> <b>Euclidean</b> <b>distance.</b> Otherwise, their distance is defined to be the sum of <b>the</b> <b>Euclidean</b> <b>distances</b> of these two points to the origin [...]|$|R
50|$|This generalizes <b>the</b> <b>Euclidean</b> space example, since <b>Euclidean</b> {{space with}} <b>the</b> <b>Euclidean</b> <b>distance</b> is a metric space.|$|R
40|$|Recently, many {{scholars}} investigated interval, triangular, and trapezoidal approximations of fuzzy numbers. These researches can be grouped into two classes: <b>the</b> <b>Euclidean</b> <b>distance</b> class and <b>the</b> non-Euclidean distance class. Most approximations in <b>the</b> <b>Euclidean</b> <b>distance</b> class {{can be calculated}} by formulas, but calculating approximations in the other class is more complicated. In this paper, we study interval, triangular, and trapezoidal approximations under a weighted <b>Euclidean</b> <b>distance</b> which generalize all approximations in <b>the</b> <b>Euclidean</b> <b>distance</b> class. First, we embed fuzzy numbers into a Hilbert space, and then introduce these weighted approximations by means of best approximations from closed convex subsets of the Hilbert space. Finally, we apply the reduction principle to simplify calculations of these approximations...|$|R
2500|$|For each sample, {{calculate}} <b>the</b> <b>Euclidean</b> <b>distance</b> [...] between <b>the</b> pair of co-ordinates.|$|R
5000|$|<b>The</b> <b>Euclidean</b> <b>{{distance}}</b> between p and q is just <b>the</b> <b>Euclidean</b> {{length of}} this distance (or displacement) vector: ...|$|R
50|$|Note: In {{software}} that implements Ward's method, {{it is important}} to check whether the function arguments should specify <b>Euclidean</b> <b>distances</b> or <b>squared</b> <b>Euclidean</b> <b>distances.</b>|$|R
25|$|In <b>the</b> <b>Euclidean</b> TSP (see below) the {{distance}} between two cities is <b>the</b> <b>Euclidean</b> <b>distance</b> between <b>the</b> corresponding points.|$|R
40|$|The {{purpose of}} this paper is to discuss pros and cons of fitting general curves and {{surfaces}} to 2 D and 3 D edge and range data using <b>the</b> <b>Euclidean</b> <b>distance.</b> In <b>the</b> past researchers have used approximate distance functions rather than <b>the</b> <b>Euclidean</b> <b>distance.</b> But <b>the</b> main disadvantage of <b>the</b> <b>Euclidean</b> fitting, computational cost, has become less important due to rising computing speed. Experiments with <b>the</b> real <b>Euclidean</b> <b>distance</b> show <b>the</b> limitations of suggested approximations like the Algebraic distance or Taubin's approximation. We compare the performance of various fitting algorithms in terms of efficiency, correctness, robustness and pose invariance...|$|R
30|$|By “dissimilarity” in {{our study}} we are {{referring}} to <b>the</b> <b>Euclidean</b> <b>distance</b> between user vectors.|$|R
2500|$|... {{and this}} {{quantity}} is <b>the</b> <b>square</b> of <b>the</b> <b>Euclidean</b> <b>distance</b> between [...] and the origin.|$|R
