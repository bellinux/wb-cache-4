3|7|Public
25|$|TS (transfer to storage): Store {{register}} A at {{the specified}} memory address. TS also detects, and corrects for, overflows {{in such a}} way as to propagate a carry for multi-precision add/subtract. If the result has no overflow (leftmost 2 bits of A the same), nothing special happens; if there is overflow (those 2 bits differ), the leftmost one goes the memory as the sign bit, register A is changed to +1 or −1 accordingly, and control skips to the second instruction following the TS. Whenever overflow is a possible but abnormal event, the TS was followed by a TC to the no-overflow logic; when it is a normal possibility (as in multi-precision add/subtract), the TS is followed by CAF ZERO (CAF = XCH to fixed memory) to complete the formation of the carry (+1, 0, or −1) into the next higher-precision word. Angles were kept in single precision, distances and velocities in double precision, and elapsed time in <b>triple</b> <b>precision.</b>|$|E
50|$|B32 Business Basic was {{a highly}} {{compatible}} interpreter which ran on the Eclipse MV line. It lifted many of the Data General Business Basic constraints, and ran significantly faster by using the full power of the 32-bit processor. B32 stored all variables internally as 64-bit, and emulated double and <b>triple</b> <b>precision</b> as required. It also provided new language features. B32 was ported to Unix and later to DOS, allowing Data General's customers to readily move to other hardware vendors. B32 also had substantial compatibility with Bluebird Business Basic.|$|E
5000|$|... (transfer to storage): Store {{register}} A at {{the specified}} memory address. [...] also detects, and corrects for, overflows {{in such a}} way as to propagate a carry for multi-precision add/subtract. If the result has no overflow (leftmost 2 bits of A the same), nothing special happens; if there is overflow (those 2 bits differ), the leftmost one goes the memory as the sign bit, register A is changed to +1 or −1 accordingly, and control skips to the second instruction following the [...] Whenever overflow is a possible but abnormal event, the [...] was followed by a [...] to the no-overflow logic; when it is a normal possibility (as in multi-precision add/subtract), the [...] is followed by [...] ZERO ( [...] = [...] to fixed memory) to complete the formation of the carry (+1, 0, or −1) into the next higher-precision word. Angles were kept in single precision, distances and velocities in double precision, and elapsed time in <b>triple</b> <b>precision.</b>|$|E
5000|$|The UE In-Ear Reference Monitors with <b>Triple</b> Balanced, <b>precision</b> armatures, with {{multiple}} Integrated Passive Crossover ...|$|R
40|$|Modeling hypernymy, such as poodle is-a dog, is an {{important}} generalization aid to many NLP tasks, such as entailment, relation extraction, and question answering. Supervised learning from labeled hypernym sources, such as WordNet, limit the coverage of these models, which can be addressed by learning hypernyms from unlabeled text. Existing unsupervised methods either do not scale to large vocabularies or yield unacceptably poor accuracy. This paper introduces distributional inclusion vector embedding (DIVE), a simple-to-implement unsupervised method of hypernym discovery via per-word non-negative vector embeddings learned by modeling diversity of word context with specialized negative sampling. In an experimental evaluation more comprehensive than any previous literature {{of which we are}} aware - evaluating on 11 datasets using multiple existing as well as newly proposed scoring metrics - we find that our method can provide up to double or <b>triple</b> the <b>precision</b> of previous unsupervised methods, and also sometimes outperforms previous semi-supervised methods, yielding many new state-of-the-art results...|$|R
40|$|An automatic, sol idstate direct-readout {{system was}} built for the Buchier-Cotlove chioridometer. The device, {{described}} in detail, {{makes it possible to}} decrease sample size and titration time. The speed of chloride determinations with use of this device is <b>tripled,</b> while <b>precision</b> is significantly im-proved. Additional Keyphrases computer input #{ 149 } speeded assays with increased precision We have designed and constructed an attach-ment to the Buchier-Cotlove chioridometer that permits fully automatic titration of chloride in liquid samples and digital display of the results in the numerical units of choice. The attachment essentially consists of a variable-rate pulse genera-tor, a binary counter, and a decimal readout (Figure 1). {{it is well known that}} the amount of chloride, when titrated at a constant voltage, is linearly proportional to titration time ranging up to several hundred seconds (1). It was, therefore, possible to allow for calibration of the timing device of the instrument by simple potentiometric rate control of the pulse generator producing the desired number of counts per time unit. In addition, blank titration values were automatically corrected by incorporating a counter-reset mechanism, the logic of which allows subtraction of any desired number from 0. 1 through 9. 9 at the onset of titra-tion. ‘When the titration is completed, results are held in binary code in the binary counter, as well as displayed in decimal numbers by the digital readout. This availability of the binary signal makes the device easily adaptable to direct com-puter input...|$|R
40|$|We {{examined}} {{precision of}} size, age, growth, and mortality parameters for four reef fishes at sample sizes ranging from 25 to 1000 using bootstrapped population samples. The results are illustrative rather than prescriptive {{in that we}} do not determine "optimum" sample sizes, but rather describe improvements in precision with increasing sample size. Furthermore, we do not address the related issue of accuracy. In general, a sample size needed to be <b>tripled</b> to halve <b>precision</b> at that sample size. Mean lengths and ages were most precise, reaching 10 % by a sample size of 75 for all species. von Bertalanffy growth parameters were up to an order of magnitude more precise when constraints were placed upon the fitting process. Asymptotic lengths, L₀₀, were up to eight times as precise as Brody growth coefficients, K. Catch curves were generally less precise than two other mortality estimators, but we cannot advocate any estimator until accuracy is addressed. We propose a general rule of collecting an average of 710 fish per age-class to estimate a variety of parameters. However, we more strongly suggest applying similar analyses for focal species and, where possible, with consideration of the application of parameters (e. g., sensitivity analyses) ...|$|R
40|$|The WW {{production}} {{is the primary}} channel to directly probe the triple gauge couplings. We first analyze the e^+ e^- → W^+ W^- process at the future lepton collider, China's proposed Circular Electron-Positron Collider (CEPC). We use the five kinematical angles in this process to constrain the anomalous triple gauge couplings and relevant dimension six operators at the CEPC up {{to the order of}} magnitude of 10 ^- 4. The most sensible information is obtained from the distributions of the production scattering angle and the decay azimuthal angles. We also estimate constraints at the 14 TeV LHC, with both 300 fb^- 1 and 3000 fb^- 1 integrated luminosity from the leading lepton p_T and azimuthal angle difference Δϕ_ll distributions in the di-lepton channel. The constrain is somewhat weaker, up to the order of magnitude of 10 ^- 3. The limits on the triple gauge couplings are complementary to those on the electroweak precision observables and Higgs couplings. Our results show that the gap between sensitivities of the electroweak and <b>triple</b> gauge boson <b>precision</b> can be significantly decreased to less than one order of magnitude at the 14 TeV LHC, and that both the two sensitivities can be further improved at the CEPC. Comment: 36 pages, 5 figures, 8 tables, version to appear in JHE...|$|R
40|$|Characterizing fish {{population}}s: {{effects of}} {{sample size and}} population structure on the precision of demographic parameter estimates J. P. Kritzer, C. R. Davies, and B. D. Mapstone Abstract: We examined precision of size, age, growth, and mortality parameters for four reef fishes at sample sizes ranging from 25 to 1000 using bootstrapped population samples. The results are illustrative rather than prescriptive in {{that we do not}} determine “optimum ” sample sizes, but rather describe improvements in precision with increasing sam-ple size. Furthermore, we do not address the related issue of accuracy. In general, a sample size needed to be <b>tripled</b> to halve <b>precision</b> at that sample size. Mean lengths and ages were most precise, reaching 10 % by a sample size of 75 for all species. von Bertalanffy growth parameters were up to an order of magnitude more precise when constraints were placed upon the fitting process. Asymptotic lengths, L, were up to eight times as precise as Brody growth coeffi-cients, K. Catch curves were generally less precise than two other mortality estimators, but we cannot advocate any es-timator until accuracy is addressed. We propose a general rule of collecting an average of 7 – 10 fish per age-class to estimate a variety of parameters. However, we more strongly suggest applying similar analyses for focal species and, where possible, with consideration of the application of parameters (e. g., sensitivity analyses) ...|$|R

