37|158|Public
50|$|If the <b>threshold</b> <b>element</b> is a neon lamp, {{the circuit}} also {{provides}} {{a flash of light}} with each discharge of the capacitor. This lamp example is depicted below in the typical circuit used to describe the Pearson - Anson effect. The discharging duration can be extended by connecting an additional resistor in series to the <b>threshold</b> <b>element.</b> The two resistors form a voltage divider; so, the additional resistor has to have low enough resistance to reach the low threshold.|$|E
40|$|Abstract- This paper {{deals with}} a CMOS based {{artificial}} neuron implemented by threshold elements. We consider the artificial neuron as a <b>threshold</b> <b>element</b> with controlled inputs having weights formed during a learning process. A so-called β-driven <b>threshold</b> <b>element</b> is used for {{in the scheme of}} the neuron. Functioning of this element is described in a specific ratio form. The β-driven implementation is based on using summarized conductivities of n-and p-chains of a CMOS gate as the ratio of weighted sums. The <b>threshold</b> <b>element</b> has a wider functional capability in comparison with the traditional functional basis. Moreover, its functional capability can be enriched. We propose a method for increasing the functional capability of the <b>threshold</b> <b>element</b> by introducing socalled functional inputs. Each functional input corresponds to a Boolean sum (or product) of a particular subset of input variables. This sum (or product) serves as a single input of the <b>threshold</b> <b>element.</b> It is shown that introducing functional inputs enables expansion of the functional capability of β-driven elements up to the capability to implement an arbitrary monotonic function. The CMOS based implementation of the β-driven <b>threshold</b> <b>element</b> with newly proposed functional inputs is presented. Methods of the current stabilization of functional inputs are proposed. In the proposed implementation of the artificial neuron, each input weight is determined by the current value via a suitable current stabilizer. This value can be effectively controlled by the value of the voltage at the gate of one of the current stabilizer’s transistors. The paper presents examples of the SPICE simulation of behavior of the proposed artificial neuron in the modes of learning and maintaining the input weights values. Key-Words: Artificial neuron, <b>threshold</b> <b>element,</b> learnable circuit, CMOS circuits, SPICE-simulation. ...|$|E
40|$|In {{the paper}} we propose a new coding/decoding on the {{divergence}} principle. A new divergent multithreshold decoder (MTD) for convolutional self-orthogonal codes contains two threshold elements. The second <b>threshold</b> <b>element</b> decodes the code with the code distance one greater than for the first <b>threshold</b> <b>element.</b> Errorcorrecting possibility of the new MTD modification have been higher than traditional MTD. Simulation results show that the performance of the divergent schemes allow to approach area of its effective work to channel capacity approximately on 0, 5 dB. Note that we include the enough effective Viterbi decoder instead of the first <b>threshold</b> <b>element,</b> the divergence principle can reach more. Index Terms — error-correcting coding, convolutional code, decoder, multithreshold decoder, Viterbi algorithm...|$|E
40|$|During {{the last}} ten years the growing {{interest}} can be noted in using <b>threshold</b> <b>elements</b> as a functional basis of artificial neural networks [1 - 4]. This interest to <b>threshold</b> <b>elements</b> and <b>threshold</b> logics, {{can be explained by the}} fact that functional capabilities of <b>threshold</b> <b>elements</b> are significantly wider in comparison with the traditionally based ones’. However, the effectiveness of using the threshold basis basically depends on the complexity of implementation of th...|$|R
30|$|The sum {{drives a}} <b>thresholding</b> <b>element</b> to {{generate}} the estimated class label ŷ.|$|R
40|$|Abstract:- We {{show in a}} {{constructive}} way the universality of a two layer perceptron. We show how to build any minterm of any logical function with one perceptron and how to build the OR of all the minterms with another perceptron. This way {{it is possible to}} implement a logical function with N minterms with a two layer perceptron with N+ 1 perceptrons. Next we addres the question of minimizing the number of perceptrons or <b>threshold</b> <b>elements</b> to implement a given logical function. Here we have to answer to two open questions that have not yet answered in the literature: 1) Is the logical function linearly separable? 2) If the logical function is linearly non-separable how to minimize the number of perceptrons or <b>threshold</b> <b>elements</b> to implement the logical function? If we know that the logical function is linearly separable it can be implemented by one perceptron trained with the Rosenblatt’s Perceptron Learning Rule [1]-[2] that guarantees convergence {{in the case of the}} existence of a solution. We propose as a test of linearly non-separable logical function the Perceptron Learning Rule did not converge after a very great number of epochs or iterations. As a method of minimization of the number of perceptrons or <b>threshold</b> <b>elements</b> to implement a linearly non-separable function we propose a trial and error procedure that generates all manners to implement the logical function with the mnimum number of perceptrons or <b>threshold</b> <b>elements,</b> and if for all of them the learning rule did not converge for all associations of minterms then we must increment the number of perceptrons and generate and test again all manners of implementation o...|$|R
40|$|In {{this paper}} we {{investigate}} small depth linear <b>threshold</b> <b>element</b> networks for multi-operand addition. In particular, we consider depth- 2 linear <b>threshold</b> <b>element</b> networks and block save addition. We improve {{the overall cost}} in terms of gates and wires of the block save addition with {{the inclusion of the}} telescopic sums proposed by Minnick. We show that previously proposed schemes require about twice the number of linear threshold gates for common operand lengths. Furthermore, we show that the number of wires required by an implementation for previously proposed schemes is also about two times higher than the number of wires required for the scheme we describe for commonly architected operand sizes...|$|E
40|$|Learning and {{convergence}} {{properties of}} linear threshold elements or percept,rons are well understood {{for the case}} where the input vectors (or the training sets) to the perceptron are linearly separable. However, {{little is known about}} the behavior of a linear <b>threshold</b> <b>element</b> when the training sets are linearly non-separable. In this paper we present the first known results on the structure of linearly non-separable training sets and on the behavior of perceptrons when the set of input vectors is linearly non-separable. More precisely, we show that using the well known perceptron learning algorithm a linear <b>threshold</b> <b>element</b> can learn the input vectors that are provably learnable, and identify those vectors that cannot be learned without committing errors. We also show how a linear <b>threshold</b> <b>element</b> can be used to learn large linearly separable subsets of any given non-separable training set. In order to develop our results, we first establish formal characterizations of linearly non-separable training sets and define learnable structures for such patterns. We also prove computational complexity results for the related learning problems. Next, based on such characterizations, we show that a perceptron do,es the best one can expect for linearly non-separable sets of input vectors and learns as much as is theoretically possible...|$|E
40|$|Only a {{small part}} of all Boolean {{functions}} of n-variables can be realized by one <b>threshold</b> <b>element</b> (T. E.). For all other functions the net must be built with at least two The problem of constructing a fault-tolerant two-level network from T. E. is investigated. The notion of limiting function is introduced. It...|$|E
40|$|Many {{threshold}} devices {{placed on}} single substrate. Integrated circuits containing optoelectronic <b>threshold</b> <b>elements</b> developed {{for use as}} planar arrays of artificial neurons in research on neural-network computers. Mounted with volume holograms recorded in photorefractive crystals serving as dense arrays of variable interconnections between neurons...|$|R
40|$|We {{investigate}} conventional {{counters and}} their employment to produce multi-operand addition using threshold logic. The primary {{concern of the}} study is the area reduction of the implementations. We use direct reduction schemes that minimize the number of <b>threshold</b> <b>elements</b> and hierarchical reduction to diminish threshold gate requirements at the expenses of area...|$|R
40|$|A liquid-crystal TV spatial light {{modulator}} (LCTV SLM) {{input device}} and LCTV nonlinear <b>thresholding</b> <b>element</b> are presently used to accomplish an all-optical implementation of an inner-product neural associative memory. This architecture represents {{an alternative to the}} vector-matrix multiplication method of the Hopfield model, which is most often employed by neural network associative memory models. LCTV SLM experimental results are presented and discussed...|$|R
40|$|It is {{well known}} that a neurone with all-or-none {{property}} acts as a <b>threshold</b> <b>element</b> in the nervous system and that the neurone produces nerve impulses {{as a result of its}} activity. Up to the present time we have had many excellent works by various authors since Hodgkin, Huxley and Katz et al. about the mechanism of impuls...|$|E
40|$|Herein we obtain several {{theoretical}} results concerning {{properties of}} the transition function given by a finite net of threshold elements. In particular, {{we are able to}} give an exact characterization of these transition functions solely in terms of the Boolean Ring. Specializing this result we find necessary and sufficient conditions that a Boolean function is realizable by a single <b>threshold</b> <b>element.</b> Also we furnish axioms for the transition functions of McCulloch-Pitts Nets, etc...|$|E
40|$|An {{adaptive}} {{network with}} visual inputs has {{been trained to}} balance an inverted pendulum. Simulation {{results show that the}} network is capable of extracting the necessary state information from time sequences of crude visual images. A single linear adaptive <b>threshold</b> <b>element</b> (ADALINE) was adequate for this task. When tested by simulation, the performace achieved was sufficient to keep the pendulum from falling. The adaptive network’s ability to generalize made this possible since the training set encompassed only a fraction of all possible states. ...|$|E
40|$|Lower {{and upper}} bounds for the {{capacity}} of multilevel <b>threshold</b> <b>elements</b> are estimated, using two essentially different enumeration techniques. It is demonstrated that {{the exact number of}} multilevel threshold functions depends strongly on the relative topology of the input set. The results correct a previously published estimate and indicate that adding threshold levels enhances the capacity more than adding variables...|$|R
40|$|An exact {{analytical}} {{expression for}} the information transmitted through an array of <b>threshold</b> <b>elements</b> subject to a uniformly distributed signal and internal noise is presented. Additionally, approximations that accurately predict the optimal noise intensity and information gains achieved by the recently reported effect of suprathreshold stochastic resonance (N. G. Stocks, Phys. Rev. Lett. 84 (2000) 2310) are obtained. (C) 2001 Elsevier Science B. V. All rights reserved...|$|R
40|$|AbstractWe {{investigate}} how geometric properties translate into functional properties in sparse networks of computing elements. Specifically, we {{determine how the}} eigenvalues of the interconnection graph (which in turn reflect connectivity properties) relate to the quantities, number of items stored, amount of error-correction, radius of attraction, and rate of convergence, in an associative memory model consisting of a sparse network of <b>threshold</b> <b>elements</b> or neurons...|$|R
40|$|It is {{generally}} {{believed that a}} neuron is a <b>threshold</b> <b>element</b> that fires when some variable u reaches a threshold. Here we pursue {{the question of whether}} this picture can be justified and study the four-dimensional neuron model of Hodgkin and Huxley as a concrete example. The model is approximated by a response kernel expansion in terms of a single vari-able, the membrane voltage. The first-order term is linear in the input and its kernel has the typical form of an elementary postsynaptic potential. Higher-order kernels take care of nonlinear interactions between input spikes. In contrast to the standard Volterra expansion, the kernels depend on the firing time of the most recent output spike. In particular, a zero-order kernel that describes the shape of the spike and the typical after-potential is included. Our model neuron fires if the membrane voltage, given by the truncated response kernel expansion, crosses a threshold. The threshold model is tested on a spike train generated by the Hodgkin-Huxley model with a stochastic input current. We find that the threshold model predicts 90 percent of the spikes correctly. Our results show that, to good approximation, the description of a neuron as a <b>threshold</b> <b>element</b> can indeed be justified. ...|$|E
40|$|This paper {{analyses}} {{the concept}} of ‘work-relatedness’ in Australian workers’ compensation and {{occupational health and safety}} (OHS) systems. The concept of work-relatedness is important because it is a crucial element circumscribing the limits of the protection afforded to workers under the preventative OHS statutes, and is a <b>threshold</b> <b>element</b> which has to be satisfied before an injured or ill worker can recover statutory compensation. While the preventive and compensatory regimes do draw on some similar concepts of work-relatedness, as this paper will illustrate, there are significant differences both between, and within, these regimes. ...|$|E
40|$|Autonomous <b>threshold</b> <b>element</b> circuit {{networks}} {{are used to}} inves-tigate the structure of neural networks. With these circuits, as the transition functions are threshold functions, {{it is necessary to}} consider the existence of sequences of state configurations that cannot be tran-sitioned. In this study, we focus on all logical functions of four or fewer variables, and we discuss the periodic sequences and transient series that transition from all sequences of state configurations. Fur-thermore, by using the sequences of state configurations in the Garden of Eden, we show that it is easy to obtain functions that determine the operation of circuit networks. ...|$|E
40|$|Abstract. This paper {{discusses}} {{some aspects}} {{regarding the use}} of uni-versal linear <b>threshold</b> <b>elements</b> implemented in a standard double-poly CMOS technology, which might be used for neural networks as well as plain, or mixed-signal, analog and digital circuits. The 2 -transistor ele-ments can have their threshold adjusted in real time, and thus the basic Boolean function, by changing the voltage on {{one or more of the}} in-puts. The proposed elements allow for significant reduction in transistor count and number of interconnections. This in combination with a power supply voltage in the range of less than 100 mV up to typically 1. 0 V allow for Power-Delay-product improvements typically in the range of hundreds to thousands of times compared to standard implementations in a 0. 6 micron CMOS technology. This makes the circuits more sim-ilar to biological neurons than most existing CMOS implementations. Circuit examples are explored by theory, SPICE simulations and chip measurements. A way of exploiting inherit fault tolerance is briefly men-tioned. Potential improvements on operational speed and chip area of linear <b>threshold</b> <b>elements</b> used for perceptual tasks are shown. ...|$|R
40|$|Attention {{is called}} to {{previous}} research on realization of an arbitrary switching function by a network of threshold gates (or <b>threshold</b> <b>elements)</b> and modulo- 2 gates (or parity elements), establishment of greatest lower bounds {{on the number of}} gates needed, artifices that lead to further network reduction in special cases, and systematic minimization of the number of module- 2 gates required. Although not widely published, a report describing the research in detail is readily available...|$|R
40|$|Linear <b>threshold</b> <b>elements</b> (LTEs) are {{the basic}} {{processing}} elements in artificial neural networks. An LTE computes a function that {{is a sign of}} a weighted sum of the input variables. The weights are arbitrary integers; actually they can be very big integers-exponential in the number of input variables. However, in practice, {{it is very difficult to}} implement big weights. So the natural question that one can ask is whether there is an efficient way to simulate a network of LTEs with big weights by a network of LTEs with small weights. We prove the following results: 1) every LTE with big weights can be simulated by a depth- 3, polynomial size network of LTEs with small weights, 2) every depth-d polynomial size network of LTEs with big weights can be simulated by a depth-(2 d+ 1), polynomial size network of LTEs with small weights. To prove these results, we use tools from harmonic analysis of Boolean functions. Our technique is quite general, it provides insights to some other problems. For example, we were able to improve the best known results on the depth of a network of <b>threshold</b> <b>elements</b> that computes the COMPARISON, ADDITION and PRODUCT of two n-bits numbers, and the MAXIMUM and the SORTING of n n-bit numbers...|$|R
40|$|A neuron {{is modeled}} as a linear {{threshold}} gate, {{and the network}} architecture considered is the layered feedforward network. It is shown how common arithmetic functions such as multiplication and sorting can be efficiently computed in a shallow neural network. Some known results are improved by showing that the product of two n-bit numbers and sorting of n n-bit numbers can be computed by a polynomial-size neural network using only four and five unit delays, respectively. Moreover, the weights of each <b>threshold</b> <b>element</b> in the neural networks require O(log n) -bit (instead of n -bit) accuracy. These results can be extended to more complicated functions such as multiple products, division, rational functions, and approximation of analytic functions...|$|E
40|$|AbstractThe {{focus of}} {{the paper is the}} {{estimation}} of the maximum number of states that can be made stable in higher-order extensions of neural network models. Each higher-order neuron in a network of n elements is modeled as a polynomial <b>threshold</b> <b>element</b> of degree d. It is shown that regardless of the manner of operation, or the algorithm used, the storage capacity of the higher-order network is of the order of one bit per interaction weight. In particular, the maximal (algorithm independent) storage capacity realizable in a recurrent network of n higher-order neurons of degree d is of the order of ndd!. A generalization of a spectral algorithm for information storage is introduced and arguments adducing near optimal capacity for the algorithm are presented...|$|E
40|$|An area of {{application}} of neural networks is considered. A neuron is modeled as a linear threshold gate, and the network architecture considered is the layered feedforward network. It is shown how common arithmetic functions such as multiplication and sorting can be efficiently computed in a shallow neural network. Some known results are improved by showing that the product of two n-bit numbers and sorting of n n-bit numbers can be computed by a polynomial-size neural network using only four and five unit delays, respectively. Moreover, the weights of each <b>threshold</b> <b>element</b> in the neural networks require O(log n) -bit (instead of n-bit) accuracy. These results can be extended to more complicated functions such as multiple products, division, rational functions, and approximation of analytic functions...|$|E
40|$|Usually, simple linear <b>threshold</b> <b>elements</b> {{are adopted}} as neuron {{models for the}} arti-ficial neurocomputing. Although such simple neuron models have been useful to derive some {{fundamental}} principles in the artificial neurocomputing, there exists criticism {{from the viewpoint of}} physiology that real neurons are far more complicated than such over [...] simplified models. In fact, the existence of deterministic chaos was confirmed experimen-tally with squid giant axons and other biological neurons. Moreover, it was also verified that nonlinear dynamics producing the nerve chaos can be understood in the framework of nerve ODE like the Hodgkin-Huxley equations and the FitzHugh-Nagumo equations. These results clearly showed that characteristics of real neurons are not · so simple and static as linear <b>threshold</b> <b>elements,</b> but more complicated and actually chaotic. Even if the deterministic chaos in the real neurons can be well described by the nerve ODE, these equations are too complicated to be adopted as constituent elements for large-scale artificial neural networks. It is a lesson we have learnt from deterministic chaos that complicated phenomena can b. e not rarely explained by simple deterministic dynamics. From this motivation, a simple neuron model with chaotic dynamics, which can repro-duce the nerve chaos qualitatively, was proposed on the basis of physiological propertie...|$|R
30|$|Brains are neural systems, {{which allow}} quick {{adaption}} to changing situations during lifetime of an organism. Neural networks are complex systems (Mainzer 2008 a) of <b>threshold</b> <b>elements</b> with firing and nonfiring states, according to learning strategies (e.g., Hebbian learning). Beside deterministic homogeneous Hopfield networks, there are so-called Boltzmann machines with stochastic network architecture of nondeterministic processor elements and a distributed knowledge representation, which is described mathematically by an energy function. While Hopfield systems use a Hebbian learning strategy, Boltzmann machines favor a backpropagation strategy (Widrow-Hoff rule) with hidden neurons in a many-layered network.|$|R
40|$|The work {{covers the}} {{properties}} of the automatic control systems with neuro-network regulators. The designing procedure of the neuro-network regulators, algorithms of learning neuro-network regulators on the control object model, algorithms for synthesis of the dimensions of a quantity and parameters of the neuro-network regulator with <b>threshold</b> <b>elements</b> have been developed. The neuro-network regulators provide the high quality in control of the objects with fast process passing. The program complex for adjustment and investigation of the neuro-network regulators has been introducedAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|Neural {{networks}} {{models have}} attracted {{a lot of}} interest in recent years mainly because there were perceived as a new idea for computing. These models can be described as a network in which every node computes a linear threshold function. One of the main difficulties in analyzing the properties of these networks is the fact that they consist of nonlinear elements. I will present a novel approach, based on harmonic analysis of Boolean functions, to analyze neural networks. In particular I will show how this technique can be applied to answer the following two fundamental questions (i) what is the computational power of a polynomial <b>threshold</b> <b>element</b> with respect to linear threshold elements? (ii) Is it possible to get exponentially many spurious memories when we use the outer-product method for programming the Hopfield model?...|$|E
40|$|Circuits of {{threshold}} elements (Boolean input, Boolean output neurons) {{have been}} shown to be surprisingly powerful. Useful functions such as XOR, ADD and MULTIPLY can be implemented by such circuits more efficiently than by traditional AND/OR circuits. In view of that, we have designed and built a programmable <b>threshold</b> <b>element.</b> The weights are stored on polysilicon floating gates, providing long-term retention without refresh. The weight value is increased using tunneling and decreased via hot electron injection. A weight is stored on a single transistor allowing the development of dense arrays of threshold elements. A 16 -input programmable neuron was fabricated in the standard 2 pm double- poly, analog process available from MOSIS. A long term goal of this research is to incorporate programmable threshold elements, as building blocks in Field Programmable Gate Arrays. ...|$|E
40|$|The authors {{introduce}} a restricted {{model of a}} neuron which is more practical {{as a model of}} computation then the classical model of a neuron. The authors define a model of neural networks as a feedforward network of such neurons. Whereas any logic circuit of polynomial size (in n) that computes the product of two n-bit numbers requires unbounded delay, such computations can be done in a neural network with constant delay. The authors improve some known results by showing that the product of two n-bit numbers and sorting of n n-bit numbers can both be computed by a polynomial size neural network using only four unit delays, independent of n. Moreover, the weights of each <b>threshold</b> <b>element</b> in the neural networks require only O(log n) -bit (instead of n-bit) accuracy...|$|E
30|$|This {{numerical}} simulation adopts the effective plastic strain rate failure criterion as the rock failure criterion. When the effective plastic strain reaches a given <b>threshold,</b> the <b>element</b> is damaged.|$|R
40|$|A {{monolithic}} 10 x 10 {{two-dimensional array}} of 'optical neuron' optoelectronic <b>threshold</b> <b>elements</b> for neural network applications has been designed, fabricated, and tested. Overall array dimensions are 5 x 5 mm, while the individual neurons, composed of an LED that {{is driven by}} a double-heterojunction bipolar transistor, are 250 x 250 microns. The overall integrated structure exhibited semiconductor-controlled rectifier characteristics, with a breakover voltage of 75 V and a reverse-breakdown voltage of 60 V; this is attributable to the parasitic p-n-p transistor which exists {{as a result of}} the sharing of the same n-AlGaAs collector between the transistors and the LED...|$|R
40|$|We {{investigate}} {{the effect of}} topological disorder on a system of forced <b>threshold</b> <b>elements,</b> where each element is arranged on top of complex heterogeneous networks. Numerical {{results indicate that the}} response of the system to a weak signal can be amplified at an intermediate level of topological disorder, thus indicating the occurrence of topological-disorder-induced resonance. Using mean field method, we obtain an analytical understanding of the resonant phenomenon by deriving the effective potential of the system. Our findings might provide further insight into the role of network topology in signal amplification in biological networks. Comment: 12 pages, 4 figure...|$|R
