10|91|Public
50|$|Baron and Rude {{discussed}} {{plans to}} either revive the series or release a movie, possibly in animated form. (A brief animated <b>test</b> <b>clip</b> was shown at comics conventions). From July 2007 through July 2009 they published the miniseries Space Opera, which {{culminated in a}} double-size issue #101/102.|$|E
50|$|Beach Chair {{was a short}} {{computer}} animation <b>test</b> <b>clip</b> created by animator Eben Ostby for Pixar in 1986. It depicts a chair walking across the sand, dipping its leg into the water, and then moving along. Ostby made the project, with the feedback of John Lasseter, to work out details of rendering software. It was exhibited at SIGGRAPH in Dallas in 1986, along with Lasseter's landmark computer-animated short Luxo Jr. and another test project, Flag and Waves by Bill Reeves.|$|E
40|$|This {{invention}} is a Dual In-line Package (DIP) <b>test</b> <b>clip</b> for {{use when}} troubleshooting circuits containing DIP integrated circuits. This <b>test</b> <b>clip</b> {{is a significant}} improvement over existing DIP test clips in that it has retractable pins which will permit troubleshooting without risk of accidentally shorting adjacent pins together when moving probes to different pins on energized circuits or when the probe is accidentally bumped while taking measurements...|$|E
50|$|On January 25, 2013, NGCodec {{announced}} {{the availability of}} free H.265/HEVC compliance <b>test</b> <b>clips.</b>|$|R
50|$|On September 11, 2013, NGCodec Inc. {{announced}} {{availability of}} free 4K HEVC/H.265 <b>test</b> <b>clips.</b>|$|R
50|$|Test points can be {{labelled}} and {{may include}} pins for attachment of alligator clips or may have complete connectors for <b>test</b> <b>clips.</b>|$|R
40|$|This {{extended}} abstract details our {{submission to}} the Music Information Retrieval Evaluation eXchange (MIREX) 2011 for the audio training task. First of all, we extract a fixed-length feature vector (composed of some timbral {{as well as}} modulation spectrum features) from each training clip. Then, by representing a fixed-length feature vector (extracted from a <b>test</b> <b>clip)</b> as a linear combination of all training feature vectors, we classify this <b>test</b> <b>clip</b> as a class with the minimal re-construction residual. This is so-called a sparse representation based classifier (SRC). 1...|$|E
40|$|Rapid {{development}} of the mult imedia and the associated technologies urge the processing of a huge database of video clips. The processing efficiency depends on the search methodologies utilized in the video processing system. Use of inappropriate search methodologies may make the processing system ineffective. Hence, an effective video retrieval system is an essential pre-requisite for searching relevant videos from a huge collection of videos. In this paper, an effective content based video retrieval system based on some dominant features such as motion, color and edge is proposed. The system is evaluated using the video clips of format MPEG- 2 and then precision-recall is determined for the <b>test</b> <b>clip.</b> Index Terms: Content based video retrieval (CBVR) system; shot segmentation; motion feature; quantized color feature; edge density; Latent Semantic Indexing (LSI). © 2011 Published by MECS Publisher. Selection and/or peer review under responsibility of the Researc...|$|E
40|$|TRODUCTION. 2. Hewlett-Packard {{miniature}} oscilloscope probes ignificantly {{reduce the}} problem of probing densely populated IC components or the characteristically minute conductors on IC circuit boards (see figure 1). These small, light-weight probes allow measurements hat were previously very difficult, while reducing he hazard of shorting. The probe body fits in the hand as comfortably as a pencil, and the needle point tip easily penetrates protective coatings for positive contact. Two accessories that further simplify and improve connection to dual in-line packages are he IC grabber (MP 7, supplied) and the Ie <b>test</b> <b>clip</b> HP Model 10024 A, available accessory). 3. Length of the probe body is 45 mm (1. 78 in.) ith an outside diameter of 2. 5 mm (0. 10 in.). Even ith insulating sleeve MP 6 installed, the probe bod i only 75 mm (2. 95 in.) long with an outside diame er of 3. 3 mm (0. 13 in.). Probe specifications are listed in table 1...|$|E
30|$|The {{sampling}} {{frequencies of}} an audio file {{have an effect}} on the amplitude of the scaling factor. With the same soundcard, the scaling distortion is also relative to the sampling rate of <b>test</b> <b>clips.</b>|$|R
500|$|On the [...] "Killer Cable Snaps" [...] {{episode of}} the popular science {{television}} series MythBusters, which aired on October 11, 2006, the possibility that audio could be transcribed onto pottery was <b>tested.</b> <b>Clips</b> from [...] "Hollywood A.D." [...] were shown during the segment.|$|R
5000|$|... {{electronics}} parts: resistors, capacitors, chokes, diodes, transistors, regulators, LEDs, photodetectors, loudspeakers and microphones, thermistors, op-amps, microcontrollers, resonators, {{buttons and}} switches, magnets, headers, jacks and plugs, ribbon cable and connectors, <b>test</b> <b>clips,</b> heat shrink tubing, solder/desolder, battery connections, batteries, motors, transducers, bell wire, magnet wire, blank PCB substrate.|$|R
30|$|In our experiments, the {{synchronization}} code is a PN {{sequence of}} 31 bits, and the watermark is {{the length of}} 32 bits. Six stages of DWT with db 2 wavelet base are applied. The length of each DWT coefficient section (denoted by L as shown in Figure 8) is 8. With Equation 20, the data embedding capacity is 28.71 bits for audio signal of 1 s at 44.1 kHz. For hiding both a synchronization code and a watermark sequence, a portion of length 2.2 s is needed. For a <b>test</b> <b>clip</b> of length 56 s, we can hide the information of 800 bits (25 synchronization codes and 25 watermarks). We test a set of audio signals including light, pop, piano, rock, drum, and electronic organ (mono, 16 bits/sample, 44.1 kHz and WAVE format). Here, we select four clips titled by march.wav, drum.wav, flute.wav, and speech.wav to report experimental results. The file speech.wav is about a daily dialog while others three are music generated by the respective music instruments, such as drum, flute.|$|E
40|$|Estimates if {{the visual}} speed of human {{movements}} such as hand gestures, facial expressions and locomotion are important during social interactions {{because they can}} be used to infer mood and intention. However {{it is not clear how}} observers use retinal signals to estimate real-world movement speed. We conducted a series of experiments to investigate adaptation-induced changes in apparent human locomotion speed, to test whether the changes show repulsion of similar speeds or global re-normalisation of all apparent speeds. Participants adapted to videos of walking or running figures at various playback speeds, and then judged the apparent movement speed of subsequently presented test clips. Their task was to report whether each <b>test</b> <b>clip</b> appeared to be faster or slower than a ‘natural’ speed. After adaptation to a slow-motion or fast-forward video, psychometric functions showed that the apparent speed of all test clips changed, becoming faster or slower respectively, consistent with global re-normalisation rather than with repulsion of test speeds close to the adapting speed. The adaptation effect depended on the retinal speed of the adapting stimulus but did not require recognizably human movements...|$|E
40|$|Rapid {{development}} of the multimedia and the associated technologies urge the processing of a hugedatabase of video clips. The processing efficiency lies on the search methodologies utilized in the videoprocessing system. Usage of inappropriate search methodologies may make the processing systemineffective. Hence, an effective video retrieval system is an essential pre-requisite for searching a relevantvideo from a huge collection of videos. In this paper, an effective content based video retrieval systembased on some dominant features such as motion, color and edge is proposed. The system is comprised oftwo stages, namely, feature extraction and retrieval of similar video clips for the given query clip. Priorto perform the feature extraction, the database video clips are segmented into different shots. In thefeature extraction, firstly, the motion feature is extracted using Squared Euclidean distance. Secondly,color feature is extracted based on color quantization. Thirdly, edge density feature is extracted for theobjects present in the database video clips. When a video clip is queried in the system, the second stage ofthe system retrieves a given number of video clips from the database {{that are similar to}} the query clip. The retrieval is performed based on the Latent Semantic Indexing, which measures the similarity betweenthe database video clips and the query clip. The system is evaluated using the video clips of formatMPEG- 2 and then precision-recall is determined for the <b>test</b> <b>clip...</b>|$|E
40|$|Multiple-instance {{learning}} algorithms train classifiers from lightly supervised data, i. e. labeled {{collections of}} items, rather than labeled items. We compare the multiple-instance learners mi-SVM and MILES {{on the task}} of classifying 10 - second song clips. These classifiers are trained on tags at the track, album, and artist levels, or granularities, that have been derived from tags at the clip granularity, allowing us to test the effectiveness of the learners at recovering the clip labeling in the training set and predicting the clip labeling for a held-out test set. We find that mi-SVM is better than a control at the recovery task on training clips, with an average classification accuracy as high as 87 % over 43 tags; on <b>test</b> <b>clips,</b> it is comparable to the control with an average classification accuracy of up to 68 %. MILES performed adequately on the recovery task, but poorly on the <b>test</b> <b>clips...</b>|$|R
50|$|The Guard is {{responsible}} for the running of trains. Their duties include {{but are not limited to}} carrying out terminal/intermediate brake <b>tests,</b> <b>clipping</b> passenger tickets, and managing crowds during photo-runs. They are also able to engage in conversation with passengers, or simply, enjoy the views from the guards van or AT viewing cars. Put simply, they are responsible for the safe running of each train service.|$|R
40|$|This {{extended}} abstract details our {{submission to}} the Music Information Retrieval Evaluation eXchange (MIREX) 2011 for the audio tag classification task. First of all, we extract a fixed-length feature vector (composed of some timbral {{as well as}} modulation spectrum features) from each song clip. Then, by using l 1 -reconstruction to represent each <b>test</b> song <b>clip</b> as a linear combination of all training songs (also known as sparse coding), we use the label matrix of training song clips to transform the sparse reconstruction coefficients of each <b>test</b> song <b>clip</b> to the label vector space. Finally, the labels with the largest values are used as the final tags for each <b>test</b> song <b>clip.</b> 1...|$|R
30|$|The {{fabrication}} {{of the second}} type of SbSI nanosensor can be briefly described as follows. In the first step, SbSI xerogel was dispersed in toluene (e.g., in ratio: 0.05  mg SbSI gel/ 1  ml toluene) using ultrasonic reactor (InterSonic IS-UZP- 2). A droplet of dispersed solution was placed onto Si/SiO 2 substrates or onto glass chips (model IAME-co-IME 2 - 1 AU made by Abtech Scientific Inc.) using insulin syringe equipped with 31 G needle. These substrates were equipped with gold microelectrodes separated by a gap of 1  μm. The direct current electric field-assisted technique [14] was used to align the nanowires perpendicularly to the electrodes. During the deposition of SbSI sol, electric field of 5 [*]×[*] 105  V/m was applied to electrodes on Si/SiO 2 substrate. The control of SbSI sol concentration is allowed to obtain an array of a few nanowires. The samples were dried in a glove box 830 -ABC/EXP (Plas-Labs Products). In the next step, ultrasonic bonding technique was used to connect SbSI nanowires with Au microelectrodes (Fig.  1 c). The detailed description of a setup for ultrasonic processing, applied procedure, and parameters of the process were presented in [7, 9]. The Si/SiO 2 substrate with array of a few SbSI nanowires was stuck to standardized metal semiconductor package TO- 5 (Fig.  1 d). The Au microelectrodes were connected with the TO- 5 pins using HB 05 wire bonder (TPT Wire Bonder). The TO- 5 packages were easily mounted in a socket of measurement system. Glass chips IAME-co-IME 2 - 1 AU were connected with measurement system using STC 7 <b>Test</b> <b>Clip</b> (Abtech Scientific Inc.).|$|E
30|$|We adopt {{a set of}} 16 -bit signed mono audio {{files in}} the WAVE format as <b>test</b> <b>clips.</b> These files are sampled at 8, 11.025, 16, 22.05, 32, 44.1, 48, 96, and 128 kHz to {{investigate}} the effect of sampling frequency. All audio files are played back with the software Window Media Player 9.0. The DA/AD distorted audio signals are recorded using the audio editing tool Cool Edit V 2.1.|$|R
50|$|Casting {{was done}} through auditions, and screen tests were {{performed}} individually and in pairs. Selected audition and screen <b>test</b> video <b>clips</b> were later aired in a spin-off program called Qpids Sunday Loventures.|$|R
30|$|The UCSD dataset {{consists}} of the Ped 1 and Ped 2 subsets, which are taken from the UCSD campus by stationary monocular cameras. The density of the crowd varies from sparse to very crowded. The only normality in the scene is pedestrians walking on the walkway. The abnormalities include bikers, skaters, and vehicles crossing the walkway. We adopt the Ped 1 subset for experiments since it provides complete ground truth for evaluating performance. The Ped 1 dataset contains 34 training and 36 <b>testing</b> <b>clips,</b> in which each clip contains 200 frames with a resolution of 158 [*]×[*] 238 pixels. We resize the resolution to 160 [*]×[*] 240. The training set contains 34 clips of normal event, and the testing set contains 36 <b>testing</b> <b>clips.</b> The sequence is first divided into a set of volumes with a size of 16 [*]×[*] 16 [*]×[*] 5, in which each volume is considered as an event. Then, the volume is further divided into a set of cuboids with a size of 4 [*]×[*] 4 [*]×[*] 5. We extract the slow features proposed in our previous works [34] from each cuboid as the regional feature, which is robust and discriminative. We construct a 2 D HOCG descriptor for each event, i.e., the spatial direction range is quantized into 8 directions with each direction being 45 °. The dimensionality of the HOCG descriptor is 8. In contrast to the 20 -dimensional regional features, the dimensionality is reduced significantly. The number of atoms for the dictionary is set to 20, and λ[*]=[*] 0.5.|$|R
40|$|In {{conventional}} motion compensated 3 -D subband/wavelet coding, {{where the}} motion compensation is unidirectional, incorrect classification of connected and unconnected pixels caused by incorrect motion vectors (MVs) {{has resulted in}} some coding inefficiency and visual artifacts in the embedded low frame-rate video. In this paper, we introduce bidirectional motion compensated temporal filtering (MCTF) with unconnected pixel detection and I blocks. We also incorporate a recently suggested lifting implementation of the subband/wavelet filter for improved MV accuracy in an MC-EZBC coder. Simulation results compare PSNR performance of this new version of MC-EZBC versus H. 26 L under the constraint of equal GOP size, and show a general parity with this state-of-the-art nonscalable coder on several <b>test</b> <b>clips.</b> I...|$|R
40|$|This paper {{describes}} our submitted {{method for}} MIREX 2009 audio tag classification task. We extract features to capture different musical information aspects, including dynamics, rhythm, timbre, pitch, tonal information. An ensemble classifier for each tag is exploited to predict on the <b>testing</b> music <b>clips.</b> 1...|$|R
40|$|We {{have used}} {{molecular}} modeling to design substitutions in an invariant chain-derived peptide (CLIP), {{so as to}} alter the stability of its complex with class II major histocompatibility complex (MHC) proteins. We sought first to <b>test</b> whether <b>CLIP</b> binds {{in the same way}} to different class II MHC proteins...|$|R
50|$|Several <b>tests</b> {{and feature}} <b>clips</b> for the {{motoring}} program Fifth Gear have been filmed here. They frequently feature racing driver Tiff Needell.|$|R
3000|$|Given {{a set of}} N {{video clips}} in the dataset{([...] x^(n),e^(n),y^(n))}_n = 1 ^N, we aim at {{learning}} a model wherein emotion labels e are used to assign a behavior label y to an unseen <b>test</b> video <b>clip</b> x. In training phase, each example is represented as a tuple (f, e, y) where f ∊ F [...]...|$|R
50|$|During the <b>test,</b> {{soft nose}} <b>clips</b> {{may be used}} to prevent air {{escaping}} through the nose. Filter mouthpieces {{may be used to}} prevent the spread of microorganisms.|$|R
25|$|In 2004 <b>test</b> {{images and}} <b>clips</b> {{continued}} {{to appear on}} the Televirtual website and finally on 17 August 2004, the full 13 minute pilot was posted on the Internet.|$|R
40|$|Abstract: Th e {{analgesic}} eff ect of the ethanolic whole {{plant extract}} of Matricaria aurea L. (Asteraceae) was studied in rats using {{the cold water}} tail fl ick assay and in mice using the tail immersion, tail clip, acetic-acid-induced writhing, and formalin pain tests. Th e results showed dose-dependent and signifi cant (P < 0. 001) increases in pain threshold at 60 min post-treatment with doses of 100, 200, and 300 mg/kg of the extract in tail fl ick, tail immersion, and tail <b>clip</b> <b>tests.</b> Th e eff ects of the extract were signifi cantly (P < 0. 001) lower than those produced by morphine (10 mg/kg) in the same tests. Th e extract (100, 200, and 300 mg/kg) exhibited a dose-dependent inhibition of writhing and also showed a signifi cant (P < 0. 001) inhibition in both phases of the formalin pain test, but with a less intense eff ect in the fi rst phase. Th e {{results indicate that the}} analgesic eff ect of M. aurea ethanolic extract is both centrally and peripherally mediated. Key words: Matricaria aurea, analgesic activity, formalin pain <b>test,</b> tail <b>clip</b> <b>test,</b> mouse writhing assa...|$|R
40|$|This paper {{looks into}} a new {{direction}} in movie clips analysis – model based ranking of highlight level. A movie clip, containing a short story, is composed of several continuous shots, which is much simpler than the whole movie. As a result, clip based analysis provides a feasible way for movie analysis and interpretation. In this paper, clip-based ranking of highlight level is proposed, where the challenging problem in detecting and recognizing events within clips is not required. Due {{to the lack of}} publicly available datasets, we firstly construct a database of movie clips, where each clip is associated with manually derived highlight level as ground truth. From each clip a number of effective visual cues are then extracted. To bridge the gap between low-level features and highlight level semantics, a holistic method of highlight ranking model is introduced. According to the distance between <b>testing</b> <b>clips</b> and selected templates, appropriate kernel function of support vector machine (SVM) is adaptively selected. Promising results are reported in automatic ranking of movie highlight levels...|$|R
40|$|Multiple-instance {{learning}} (MIL) algorithms train classifiers from lightly supervised data – {{collections of}} instances, called bags, are labeled {{rather than the}} instances themselves – algorithms can classify bags or instances, we focus on instances •Motivation for applying MIL to MIR: – propagate metadata between granularities: artist, album, track, 10 -second clip – e. g. train clip classifiers using metadata from Last. fm, Pandora, the All Music Guide, etc. •This work compares 2 MIL algorithms, mi-SVM and MILES, on tag classification –Data: tags at track, album, and artist granularities, derived from MajorMiner clip tags –Two tasks: recover tags for training clips, predict tags for held-out <b>test</b> <b>clips.</b> –Results: mi-SVM better than control at recovery, comparable at prediction MILES performs adequately on recovery task, but poorly on prediction task 2. Metadata granularity • Instances: the atomic elements being classified, in this case 10 -second clips of songs •Bags: labeled collections of instances, in this case tracks, albums, and artists –A bag is labeled negative if no instance in it is positiv...|$|R
40|$|A {{technique}} {{has been developed}} to selectively induce metastable pitting while preventing the transition to stable pit growth. The current-limited imposed-potential (CLIP) technique limits available cathodic current to an initiated site using a resistor in series with the working electrode to form a voltage divider. Potentiodynamic <b>CLIP</b> <b>testing</b> yields a distribution of breakdown potentials from a single experiment. Potentiostatic <b>CLIP</b> <b>testing</b> yields induction time data, {{which can be used}} as input to a calculation of germination rate. Initial data indicate that a one-to-one correlation exists between electrochemical transients and observed pitting sites. The CLIP technique provides a consistent means of gathering quantitative potential and current transients associated with localized oxide breakdown...|$|R
40|$|The paper {{discusses}} {{methods for}} robust audio-video broadcast over the digital video broadcasting-handheld (DVB-H) system. DVB-H includes a link-layer {{forward error correction}} (FEC) scheme known as multiprotocol encapsulation (MPE) FEC, which provides equal error protection (EEP) to the transmitted media streams. Several approaches for unequal error protection (UEP) have been proposed in the literature, and the applicability of {{some of them to}} DVB-H is analyzed in the paper. A link-layer UEP method based on priority segmentation of the media streams is chosen for more detailed analysis. According to the method, audio and the most important coded video pictures are protected by MPE-FEC more robustly compared to the remaining coded pictures. In order to compare EEP and UEP in a DVB-H environment, an error-prone DVB-H channel was simulated, audio-visual clips were sent through it, and a comprehensive subjective quality evaluation was conducted in a controlled laboratory environment. The results of the subjective evaluation revealed that the use of UEP improves the subjective quality of some <b>test</b> <b>clips</b> noticeably when the channel conditions were severe, while in other tested channel conditions and clips, UEP and EEP performed equally well. </p...|$|R
40|$|We {{present results}} of {{subjective}} viewer assessment of video quality of MPEG- 2 compressed video containing wide-band Gaussian noise. The video test sequences consisted of seven <b>test</b> <b>clips</b> (both classical and new materials) to which noise with a peak-signal-to-noise-ratio (PSNR) of from 28 dB to 47 dB was added. We used software encoding and decoding at five bit-rates ranging from 1. 8 Mb/s to 13. 9 Mb/s. Our panel of 32 viewers rated {{the difference between}} the noisy input and the compressionprocessed output. For low noise levels, the subjective data suggests that compression at higher bit-rates can actually improve the quality of the output, effectively acting like a low-pass filter. We define an objective and a subjective measure of scene criticality (the difficulty of compressing a clip) and find the two measures correlate for our data. For difficult-to-encode material (high criticality), the data suggest that the effects of compression may be less noticeable at mid-level noise, while for easy-to-encode video (low criticality), the addition of a moderate amount of noise to the input led to lower quality scores. This suggests that either the compression process may have reduced noise impairments or a form of masking may occur in scenes that have high levels of spatial detail. 1...|$|R
40|$|We {{present a}} general {{approach}} to video understanding, inspired by semantic transfer techniques {{that have been}} successfully used for 2 D image analysis. Our method considers a video to be a 1 D sequence of clips, each one associated with its own semantics. The nature of these semantics [...] natural language captions or other labels [...] depends on the task at hand. A test video is processed by forming correspondences between its clips and the clips of reference videos with known semantics, following which, reference semantics can {{be transferred to the}} test video. We describe two matching methods, both designed to ensure that (a) reference clips appear similar to <b>test</b> <b>clips</b> and (b), taken together, the semantics of the selected reference clips is consistent and maintains temporal coherence. We use our method for video captioning on the LSMDC' 16 benchmark, video summarization on the SumMe and TVSum benchmarks, Temporal Action Detection on the Thumos 2014 benchmark, and sound prediction on the Greatest Hits benchmark. Our method not only surpasses the state of the art, in four out of five benchmarks, but importantly, it is the only single method we know of that was successfully applied to such a diverse range of tasks...|$|R
