2|3025|Public
40|$|Photograph of {{two pages}} in a mission music tutor at Mission Santa Barbara, ca. 1900. The fingers of two hands hold the book open {{vertically}} {{at the top}} corners in each two-page spread. The book stands on a table or shelf against a leather case (?). Compares to CHS- 4479 and CHS- 4480, which show pages from the same book. Third spread: Handwritten <b>textual</b> <b>description</b> <b>about</b> the performance of music...|$|E
40|$|In {{this paper}} we propose a {{framework}} for predicting kernelized classifiers in the visual domain for categories with no training images where the knowledge comes from <b>textual</b> <b>description</b> <b>about</b> these categories. Through our optimization framework, the proposed approach is capable of embedding the class-level knowledge from the text domain as kernel classifiers in the visual domain. We also proposed a distributional semantic kernel between text descriptions which is shown to be effective in our setting. The proposed framework is not restricted to textual descriptions, and can also be applied to other forms knowledge representations. Our approach was applied for the challenging task of zero-shot learning of fine-grained categories from text descriptions of these categories...|$|E
40|$|This study {{describes}} {{experiments on}} automatic detection of semantic concepts, which are <b>textual</b> <b>descriptions</b> <b>about</b> the digital video content. The concepts {{can be further}} used in content-based categorization and access of digital video repositories. Temporal Gradient Correlograms, Temporal Color Correlograms and Motion Activity low-level features are extracted from the dynamic visual content of a video shot. Semantic concepts are detected with an expeditious method {{that is based on}} the selection of small positive example sets and computational low-level feature similarities between video shots. Detectors using several feature and fusion operator configurations are tested in 60 -hour news video database from TRECVID 2003 benchmark. Results show that the feature fusion based on ranked lists gives better detection performance than fusion of normalized low-level feature spaces distances. Best performance was obtained by pre-validating the configurations of features and rank fusion operators. Results also show that minimum rank fusion of temporal color and structure provides comparable performance. 1...|$|R
40|$|Abstract—Bitstream {{structure}} description {{languages are}} able to auto-matically create <b>textual</b> <b>descriptions</b> containing information <b>about</b> the high-level structure of binary multimedia resources. Such a description can eas-ily be transformed into an adapted description. The adapted description is then used to create an adapted version of the original resource. BFlavor, a new bitstream structure description language, {{is a combination of}} two existing bitstream structure description languages: MPEG- 21 BSDL and XFlavor. Both languages {{are able to}} translate the structure of a multimedia resource into an XML document. The intention of BFlavor is to combine the advantages of both languages, and to eliminate their disadvantages. In this paper, we will discuss the motivation behind the creation of BFlavor and its working. A comparison between BFlavor, XFlavor, and MPEG- 21 BSDL is presented as well. Keywords—BFlavor, XFlavor, MPEG- 21 BSDL I...|$|R
30|$|The {{approaches}} {{based on}} hearing comprise (i) <b>textual</b> <b>description,</b> {{in which a}} <b>textual</b> <b>description</b> of the graphical content was provided to learners prior to the lecture—the <b>textual</b> <b>description</b> was synthesized by screen readers; (ii) verbal description, in which the graphical representation content was read aloud by the educator during the lecture; and (iii) assistant support, in which the graphical representation content was read aloud by an assistant during the lecture.|$|R
40|$|The <b>textual</b> <b>description</b> {{of video}} {{sequences}} exploits conceptual {{knowledge about the}} behavior of depicted agents. An explicit representation of such behavioral knowledge facilitates not only the <b>textual</b> <b>description</b> of video evaluation results, but {{can also be used}} for the inverse task of generating synthetic image sequences from <b>textual</b> <b>descriptions</b> of dynamic scenes. Moreover, it is shown here that the behavioral knowledge representation within a cognitive vision system can be exploited even for prediction of movements of visible agents, thereby improving the overall performance of a cognitive vision system...|$|R
40|$|Documenting {{business}} processes using process models is {{common practice in}} many organizations. However, not all process information is best captured in process models. Hence, many organizations complement these models with <b>textual</b> <b>descriptions</b> that specify additional details. The problem with this supplementary use of <b>textual</b> <b>descriptions</b> is that existing techniques for automatically searching process repositories are limited to process models. They are not capable of taking the information from <b>textual</b> <b>descriptions</b> into account and, therefore, provide incomplete search results. In this paper, we address this problem and propose a technique {{that is capable of}} searching textual as well as model-based process descriptions. It automatically extracts process information from both descriptions types and stores it in a unified data format. An evaluation with a large Austrian bank demonstrates that the additional consideration of <b>textual</b> <b>descriptions</b> allows us to identify more relevant processes from a repository...|$|R
40|$|The {{algorithmic}} {{generation of}} <b>textual</b> <b>descriptions</b> of real world image sequences requires conceptual knowledge. The algorithmic generation of synthetic image sequences from <b>textual</b> <b>descriptions</b> requires conceptual knowledge, too. An explicit representation formalism for behavioral knowledge based on formal logic is presented {{which can be}} utilized in both tasks [...] Understanding and Creation of video sequences...|$|R
5000|$|DESCRIPTION: A <b>textual</b> <b>description</b> of the {{functioning}} of the command or function.|$|R
40|$|This thesis {{investigates the}} task of {{learning}} visual object category recognition from <b>textual</b> <b>descriptions.</b> The work contributes primarily to the recognition of fine-grained object categories, such as animal and plant species, where {{it may be difficult}} to collect many images for training. but where <b>textual</b> <b>descriptions</b> are readily available, for example from online nature guides. The idea of using <b>textual</b> <b>descriptions</b> for fine-grained object category recognition is explored in three separate but related tasks. The first is {{the task of}} learning recognition of object categories solely from textual descriptions; no category-specific training images are used. Our proposed framework comprises three components: (i) natural language processing to build object category models from textual descriptions; (ii) visual processing to extract visual attributes from test images; (Hi) generative model connecting textual terms and visual attributes from images. As an 'upper-bound' we also evaluate how well humans perform in a similar task. The proposed method was evaluated on a butterfly dataset as an example, performing substantially better than chance, and interestingly comparable to the performance of non-native English speakers. The second task is an extension to the first. Here we focus on the problem of learning models for attribute terms (e. g. "orange bands"), from a set of training classes disjoint from the test classes. Attribute models are learnt independently for each attribute term in a weakly supervised fashion from <b>textual</b> <b>descriptions,</b> and are used in conjunction with <b>textual</b> <b>descriptions</b> of the test classes to build probabilistic models for object category recognition. A modest accuracy was achieved with our method when evaluated on a butterfly dataset, although performance was substantially improved with some human supervision to combine similar attribute terms. The third task explores how <b>textual</b> <b>descriptions</b> can be used to automatically harvest training images for each object category. Starting with just the category name, a <b>textual</b> <b>description</b> and no example images, web pages are gathered from search engines, and images filtered based on how similar their surrounding texts are to the given <b>textual</b> <b>description.</b> The idea is that images in close proximity to texts that are similar to the <b>textual</b> <b>description</b> are more likely to be example images of the desired category. The proposed method is demonstrated for a set of butterfly categories. where images were successfully re-ranked based on their corresponding text blocks alone, with many categories achieving higher precision than their baselines at early stages of recall. The proposed approaches of exploiting <b>textual</b> <b>descriptions,</b> although still in their infancy, shows potential for visual object recognition tasks, effectively reducing the amount of human supervision required for annotating images. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
5000|$|Description : Zero or more. ML_STRING. A non-formatted <b>textual</b> <b>description</b> of the event.|$|R
5000|$|Exhibitor {{profiles}} (normally a <b>textual</b> <b>description</b> of each exhibitor plus their contact details) ...|$|R
5000|$|Area {{of concern}} view: <b>Textual</b> <b>description</b> of a {{phenomenon}} {{represented in the}} role model.|$|R
40|$|<b>Textual</b> <b>descriptions</b> {{of design}} {{patterns}} are ambiguous and {{may lead to}} conflicting interpretations. Since patterns are meant for communication and education, a correct and complete understanding is a prerequisite to their successful usage. Formal specification of design patterns is meant to complement existing <b>textual</b> <b>descriptions.</b> Formal specification allows a rigorous reasoning of design patterns and facilitates tool support for their usage...|$|R
50|$|Julia's job was {{to explore}} a virtual world {{consisting}} of pages of <b>textual</b> <b>descriptions,</b> with links between them, and to construct an internal map of that world and answer questions about it (including path information such as the shortest route from one room to another, and matching information, such as which rooms contained {{a certain kind of}} object or <b>textual</b> <b>description).</b>|$|R
40|$|Abstract—Concern {{localization}} {{refers to}} the process of locating code units that match a particular <b>textual</b> <b>description.</b> It takes as input textual documents such as bug reports and feature requests and outputs a list of candidate code units that need to be changed to address the bug reports or feature requests. Many information retrieval (IR) based concern localization techniques have been proposed in the literature. These techniques typically represent code units and <b>textual</b> <b>descriptions</b> as a bag of tokens at one level of abstraction, e. g., each token is a word, or each token is a topic. In this work, we propose multi-abstraction concern localization. A code unit and a <b>textual</b> <b>description</b> is represented at multiple abstraction levels. Similarity of a <b>textual</b> <b>description</b> and a code unit, is now made by considering all these abstraction levels. We have evaluated our solution on AspectJ bug reports and feature requests from the iBugs benchmark dataset. Th...|$|R
5000|$|High level {{graphical}} and <b>textual</b> <b>description</b> of {{operational concept}} (high level organizations, missions, geographic configuration, connectivity, etc.).|$|R
5000|$|Validating UML models: quality {{engineers}} {{restore a}} <b>textual</b> <b>description</b> of a domain, original and restored descriptions are compared.|$|R
40|$|Concern {{localization}} {{refers to}} the process of locating code units that match a particular <b>textual</b> <b>description.</b> It takes as input textual documents such as bug reports and feature requests and outputs a list of candidate code units that need to be changed to address the bug reports or feature requests. Many information retrieval (IR) based concern localization techniques have been proposed in the literature. These techniques typically represent code units and <b>textual</b> <b>descriptions</b> as a bag of tokens at one level of abstraction, e. g., each token is a word, or each token is a topic. In this work, we propose multi-abstraction concern localization. A code unit and a <b>textual</b> <b>description</b> is represented at multiple abstraction levels. Similarity of a <b>textual</b> <b>description</b> and a code unit, is now made by considering all these abstraction levels. We have evaluated our solution on AspectJ bug reports and feature requests from the iBugs benchmark dataset. The experiment shows that our proposed approach outperforms a baseline approach, in terms of Mean Average Precision, by up to 19. 36 %...|$|R
40|$|Abstract—Developers often receive many feature requests. To {{implement}} these features, developers can leverage various methods {{from third}} party libraries. In this work, we propose an automated approach that takes as input a <b>textual</b> <b>description</b> of a feature request. It then recommends methods in library APIs that developers {{can use to}} implement the feature. Our recommendation approach learns from records of other changes made to software systems, and compares the <b>textual</b> <b>description</b> of the requested feature with the <b>textual</b> <b>descriptions</b> of various API methods. We have evaluated our approach on more than 500 feature requests of Axis 2 /Java, CXF, Hadoop Common, HBase, and Struts 2. Our experiments show that our approach is able to recommend the right methods from 10 libraries with an averag...|$|R
50|$|The {{interface}} {{is available}} in many languages, and the <b>textual</b> <b>description</b> of each item may have multiple versions in different languages.|$|R
30|$|Items of {{this type}} again confront examinees with a brief <b>textual</b> <b>description</b> of a {{business}} transaction, identical to the items-type described above.|$|R
50|$|The National Center for Biomedical Ontology (www.bioontology.org) {{develops}} {{tools for}} automated annotation of database records {{based on the}} <b>textual</b> <b>descriptions</b> of those records.|$|R
5000|$|High-Level Operational Concept Graphic (OV-1) : High level {{graphical}} and <b>textual</b> <b>description</b> of {{operational concept}} (high level organizations, missions, geographic configuration, connectivity, etc.).|$|R
5000|$|ROLAP {{tools are}} better at {{handling}} non-aggregatable facts (e.g., <b>textual</b> <b>descriptions).</b> MOLAP tools tend to suffer from slow performance when querying these elements.|$|R
30|$|The {{acronyms}} {{were derived}} from the original Spanish names. Therefore, the <b>textual</b> <b>descriptions</b> {{do not reflect the}} acronyms. We also provide the English acronyms in parentheses.|$|R
40|$|Concern {{localization}} {{refers to}} the process of locating code units that match a particular <b>textual</b> <b>description.</b> It takes as input textual documents such as bug reports and feature requests and outputs a list of candidate code units that are relevant to the bug reports or feature requests. Many information retrieval (IR) based concern localization techniques have been proposed in the literature. These techniques typically represent code units and <b>textual</b> <b>descriptions</b> as a bag of tokens at one level of abstraction, e. g., each token is a word, or each token is a topic. In this work, we propose a multi-abstraction concern localization technique named MULAB. MULAB represents a code unit and a <b>textual</b> <b>description</b> at multiple abstraction levels. Similarity of a <b>textual</b> <b>description</b> and a code unit is now made by considering all these abstraction levels. We combine a vector space model and multiple topic models to compute the similarity and apply a genetic algorithm to infer semi-optimal topic model configurations. We have evaluated our solution on 136 concerns from 8 open source Java software systems. The experimental results show that MULAB outperforms the state-of-art baseline PR, which is proposed by Scanniello et al. in terms of effectiveness and rank...|$|R
30|$|Descriptive metadata. This {{includes}} the project {{name and a}} <b>textual</b> <b>description</b> of the project, {{which can be used}} to enable projects to be retrieved by free text searches.|$|R
40|$|The {{algorithmic}} {{generation of}} <b>textual</b> <b>descriptions</b> of image sequences requires conceptual knowledge. In our case, a stationary camera recorded image sequences of road tra#c scenes. The necessary conceptual knowledge {{has been provided}} {{in the form of}} a so-called Situation Graph Tree (SGT). Other endeavors such as the generation of a synthetic image sequence from a <b>textual</b> <b>description</b> or the transformation of machine vision results for use in a driver assistance system could profit from the exploitation of the same conceptual knowledge, but more in a planning (pre-scriptive) rather than a de-scriptive context...|$|R
3000|$|Projects - 10, 104 tuples with 52 {{attributes}} describing project {{information about}} program references, activities, <b>textual</b> <b>description</b> of project scope and objectives, details about partners and so on; [...]...|$|R
5000|$|Validating a bug fix: {{given an}} {{original}} and modified source code, quality engineers restore a <b>textual</b> <b>description</b> of the bug that was fixed, original and restored descriptions are compared.|$|R
40|$|We {{present an}} {{in-depth}} {{analysis of the}} Xilinx bitstream format. The information gathered in this paper allows bitstream compilation and decompilation. While not actually compromising current bitstream security, the easiness of the decompilation process should raise awareness about bitstream security issues. Available documentation from Xilinx and some custom assumptions about the bitstream format are presented and analyzed, so as to first gather a database mapping bitstream data to its related netlist elements, thanks to a suitable algorithm applied to a well-chosen bitstream. This database is then used as input to an efficient program which can compile a bitstream from a low-level <b>textual</b> <b>description</b> or conversely decompile a bitstream to the same <b>textual</b> <b>description</b> for any subsequent input. The whole process of database gathering and the decompilation of the bitstream format for a particular chip runs at about the speed of bitgen compilation. The sole process of compiling/decompiling a bitstream from/to its associated <b>textual</b> <b>description</b> runs two orders of magnitude faster...|$|R
40|$|International audienceDevelopers often receive many feature requests. To {{implement}} these features, developers can leverage various methods {{from third}} party libraries. In this work, we propose an automated approach that takes as input a <b>textual</b> <b>description</b> of a feature request. It then recommends methods in library APIs that developers {{can use to}} implement the feature. Our recommendation approach learns from records of other changes made to software systems, and compares the <b>textual</b> <b>description</b> of the requested feature with the <b>textual</b> <b>descriptions</b> of various API methods. We have evaluated our approach on more than 500 feature requests of Axis 2 /Java, CXF, Hadoop Common, HBase, and Struts 2. Our experiments show that our approach is able to recommend the right methods from 10 libraries with an average recall-rate@ 5 of 0. 690 and recall-rate@ 10 of 0. 779 respectively. We also show that the state-of-the-art approach by Chan et al., that recommends API methods based on precise text phrases, is unable to handle feature requests...|$|R
40|$|In this paper, {{we present}} a {{metamodel}} for <b>textual</b> use case <b>descriptions,</b> structurally conforming to the UML, to specify the behavior of use cases in a flow-oriented manner. While being primarily targeted at supporting requirements engineers in creating consistent use case models, the metamodel defines a textual representation of use case behavior that is easily understandable for readers, who are unaware of the underlying metamodel. Hence, the known benefits of natural language use case descriptions are preserved. Being formalized, consistency between UML-based use case representations and their <b>textual</b> <b>descriptions</b> can be automatically ensured. With NaUTiluS {{we present a}}n extensible, Eclipse-based toolkit, which offers integrated UML use case modeling support, as well as editing capabilities for their <b>textual</b> <b>descriptions...</b>|$|R
50|$|Video {{papers are}} a recent {{addition}} to practice of scientific publications. They most often combine an online video demonstration {{of a new}} technique or protocol combined with a rigorous <b>textual</b> <b>description.</b>|$|R
5000|$|Validating {{model changes}} {{for a new}} requirement: given an {{original}} and changed versions of a model, quality engineers restore the <b>textual</b> <b>description</b> of the requirement, original and restored descriptions are compared.|$|R
