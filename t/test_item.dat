733|3698|Public
25|$|A pretest {{is usually}} {{performed}} using a simulator {{to confirm the}} specified overall sound pressure level and spectrum can be achieved. The pre-test {{is also used to}} verify any special control features such as; abort tolerances, response limits, field shaping and emergency shut-down procedures. The microphone responses should then be examined to evaluate the resulting field for uniformity, coherence and if available, structural response. Then the simulator is replaced with the actual <b>test</b> <b>item</b> in the speaker circle and the test process is repeated.|$|E
25|$|Many psychometricians {{are also}} {{concerned}} with finding and eliminating test bias from their psychological tests. Test bias {{is a form}} of systematic (i.e., non-random) error which leads to examinees from one demographic group having an unwarranted advantage over examinees from another demographic group. According to leading experts, test bias may cause differences in average scores across demographic groups, but differences in group scores are not sufficient evidence that test bias is actually present because the test could be measuring real differences among groups. Psychometricians use sophisticated scientific methods to search for test bias and eliminate it. Research shows that it is usually impossible for people reading a <b>test</b> <b>item</b> to accurately determine whether it is biased or not.|$|E
500|$|There are {{a variety}} of {{individually}} administered IQ tests in use in the English-speaking world. Not all report test results as [...] "IQ", but most now report a standard score with a median score level of 100. When a test-taker scores higher or lower than the median score, the score is indicated as 15 standard score points higher or lower for each standard deviation difference higher or lower in the test-taker's performance on the <b>test</b> <b>item</b> content.|$|E
40|$|This study {{sought to}} {{determine}} the cognitive levels of questioning of end-of-chapter <b>test</b> <b>items</b> accompanying selected fifth-grade science textbooks. The extent to which <b>test</b> <b>items</b> emphasized high level questioning and significant differences among texts {{in the distribution of}} knowledge and above-knowledge-level <b>test</b> <b>items</b> were also investigated. Data were derived from 500 <b>test</b> <b>items</b> randomly selected from five most widely used fifth-grade scienc...|$|R
5000|$|It is {{important}} to choose critical <b>test</b> <b>items,</b> that means <b>test</b> <b>items</b> which are difficult to encode {{and are likely to}} produce artifacts. At the same time, the <b>test</b> <b>items</b> should be ecological valid. Meaning they should be representative of broadcast material and not some synthetic signals especially designed to be difficult to encode. A method to choose critical material is presented by Ekkeroot et al. who propose a ranking by elimination procedure. While this {{is a good way to}} choose the most critical <b>test</b> <b>items,</b> it does not ensure to include a variety of <b>test</b> <b>items</b> prone to different artifacts.|$|R
40|$|To start {{a teacher}} {{professional}} development programme {{on the relationship}} between classroom summative assessment and learning, the current practices and dispositions of geography teachers towards internal school-based examinations in pre-vocational education in the Netherlands were investigated. A questionnaire provided data on how teachers construct these examinations and how they perceive {{the extent to which they}} use <b>test</b> <b>items</b> in these examinations that appeal to distinct cognitive processes. The data were statistically analysed to explore teachers' practices regarding the construction of the examinations and the correlation with their perceptions on <b>test</b> <b>items</b> appealing to distinct cognitive processes. The results showed that teachers rarely construct <b>test</b> <b>items</b> themselves; instead, they rely to a considerable degree on <b>test</b> <b>items</b> created by outside sources. In particular, older teachers and teachers with greater teaching experience tend to use more <b>test</b> <b>items</b> from outside sources. According to the respondents, about two-thirds of the <b>test</b> <b>items</b> appeal to higher cognitive processes. When teachers do construct <b>test</b> <b>items</b> themselves, however, they perceive to use more <b>test</b> <b>items</b> that appeal to higher cognitive processes. Furthermore, teachers' dispositions regarding the purpose of the internal school-based examinations seem to be highly influenced by high-stakes tests, such as the national exam...|$|R
2500|$|The process {{requires}} the transport to and assembly of a speaker {{circle around the}} test article. The size of the circle {{is dependent on the}} size of the test article. Generally, a circle [...] in diameter larger and [...] taller than the test article is required. The arrangement should avoid symmetry to reduce the potential for adverse coupling of plane waves. The test article can be mounted on a platform or suspended. Multiple microphones, eight to sixteen, should be used for control with either the SISO or MIMO methods. The microphones should be placed randomly around the test article. The distance from the surface of the drivers to the surface of the control microphones should be [...] The distance from the control microphones to the surface of the test article should be [...] The height of the control microphones should be centered at mid-height of the <b>test</b> <b>item</b> and randomly varied up and down by about one-eighth of the <b>test</b> <b>item</b> height. The orientation of the free-field microphones in a DFAT test arrangement is not critical. However, reflections from the test article can be minimized with the microphone oriented toward the sound source with a 0 degree incidence. Most modern day, quality measurement, free-field microphones are factory adjusted to compensate for incident angle. This phenomenon is most pronounced at high frequencies, above 10kHz for a 1/4" [...] microphone, and is inversely proportional to microphone diaphragm diameter.|$|E
2500|$|In {{order to}} reduce the burden of field testing, the Texas State Board of Education has not {{released}} to the public those questions used to determine student scores on the Spring 2005 or Spring 2007 TAKS tests. [...] Regrettably, this prevents public review of the questions and answers (for appropriateness and correctness) and denies opportunities for students, teachers, and others to learn from the [...] tests. [...] However, university-level experts in each of the fields review each high school-level test for accuracy. [...] Grade-level teachers also review test items for appropriateness prior to field testing and review the field test results in order to select the best questions for inclusion in the <b>test</b> <b>item</b> bank.|$|E
2500|$|In psychometrics, item {{response}} theory (IRT), {{also known}} as latent trait theory, strong true score theory, or modern mental test theory, is a paradigm for the design, analysis, and scoring of tests, questionnaires, and similar instruments measuring abilities, attitudes, or other variables. It is a theory of testing based {{on the relationship between}} individuals’ performances on a <b>test</b> <b>item</b> and the test takers’ levels of performance on an overall measure of the ability that item was designed to measure. Several different statistical models are used to represent both item and test taker characteristics. Unlike simpler alternatives for creating scales and evaluating questionnaire responses, it does not assume that each item is equally difficult. This distinguishes IRT from, for instance, the assumption in Likert scaling that [...] "All items are assumed to be replications of each other or in other words items are considered to be parallel instruments" [...] (p.197). By contrast, item response theory treats the difficulty of each item (the ICCs) as information to be incorporated in scaling items. ICC stands for item characteristic curve.|$|E
40|$|The {{paper is}} devoted to the <b>test</b> <b>items</b> marking {{technology}} and distribution of different types items by the levels of complication. At distribution of <b>test</b> <b>items</b> on complication levels it is offered to use the integrated functional model which allows to apply different ways of the <b>test</b> <b>items</b> distribution included in the test...|$|R
40|$|Abstract: This paper {{focuses on}} the {{concepts}} of reliability, construct and content validation of mathematics achievement <b>test</b> <b>items.</b> Methods for constructing domain specifications for a pool of mathematics items are discussed. Mathematics achievement <b>test</b> <b>items</b> are presented {{for use in a}} review of content validity. The features of a computer program for automatically generating representative <b>test</b> <b>items</b> from domain specifications are discussed. 1...|$|R
40|$|Producing a {{high quality}} of <b>test</b> <b>items</b> needs a {{substantial}} time and efforts in developing and examining each testitem. Literature suggested this limitation can be remedied with a large collection of <b>test</b> <b>items</b> along with their measurement characteristics which termed as item bank. However, a well-developed item bank is formed {{by a set of}} high quality of <b>test</b> <b>items</b> using a rigorous development and validation procedures. In achieving this, this study attempts to describe the development of Malaysian secondary school Form two Mathematics item bank; and to examine the psychometric properties of the <b>test</b> <b>items</b> which developed based on Malaysian Mathematics Curriculum Specifications...|$|R
5000|$|... where pi is the {{proportion}} of correct responses to <b>test</b> <b>item</b> i, qi is {{the proportion}} of incorrect responses to <b>test</b> <b>item</b> i (so that pi + ...|$|E
50|$|Performances {{are then}} {{compared}} to a chart and the grades from A to F for each <b>test</b> <b>item.</b> An A grade constitutes excellent performance and an E grade constitutes barely passing. On the other hand, an F grade indicates that the participant has failed that <b>test</b> <b>item.</b>|$|E
50|$|Stress testing: Providing {{variable}} weights in strength-of-materials stress-testing systems. Shot pours from {{a hopper}} into a basket, which {{is connected to}} the <b>test</b> <b>item.</b> When the <b>test</b> <b>item</b> fractures, the chute closes and the mass of the lead shot in the basket is used to calculate the fracture stress of the item.|$|E
40|$|The {{purpose of}} this study is to develop a {{classification}} framework for the test elements of the National Registered Nurse's License Examination and to divide the <b>test</b> <b>items</b> into standard and basic core on the basis of the RN's job descriptions. And the adequa to proportion of the basic core <b>test</b> <b>items</b> is going to be identified. Method and results: In order to develop the classification framework of the National Registered Nurse's License Examination, RN's job descriptions, nursing standards, and the specific learning objectives of nursing courses were reviewed. And a survey was used to identify which entity would be appropriate for a reference to the basic core <b>test</b> <b>items.</b> 146 of professors from schools of nursing and members of each division of Korean Academic Society of Nursing(KASN) were participated in the survey. The study showed the 98 % of respondents agreed to use RN's job descriptions in selecting the basic core <b>test</b> <b>items</b> and 30 % for the basic core test would be appropriate. And the contents, the selection criteria, and the proportion of the basic core <b>test</b> <b>items</b> were developed by the members of this research, the members of the National RN's License Examination subcommittee, and the presidents of each division of KASN. The total of 1990 standard <b>test</b> <b>items</b> were selected among 3524 items, that 3 out of 7 members in the research team agreed to choose. Duplicated items in the standard items were deleted. 205 items out of the 1990 standard items were selected as the basic core <b>test</b> <b>items.</b> And 14 items were added in Medical Laws and Ethics which leads the total of 219 basic core <b>test</b> <b>items.</b> ln conclusion, the 99 items, 30 % of total current examination items were chosen as the final basic core <b>test</b> <b>items</b> using the delphimethod. Further studies are needed to validate the current National License Examination for RN on the basis of the 99 basic core <b>test</b> <b>items...</b>|$|R
40|$|One of {{objectives}} of evaluation {{in teaching and}} learning activities is to know how far the achievement is a test. In order to gain a good result of measurement, a good test is needed so that there will not be any wrong interpretation. This research is proposed to analyse qualitatively and quantitatively the uneven semester <b>test</b> <b>items</b> of mathematics for the first grade of SMAN 1 Campurdarat in the period 2010 / 2011 based on the <b>test</b> <b>items</b> and students answer sheet. The analysis of the <b>test</b> <b>items</b> quantitatively concerned with reliability, difficulty index, discrimination power, and the effectiveness of the distracted, while the analysis of the <b>test</b> <b>items</b> qualitatively concerned with content validity. The result of the research shows that the even semester <b>test</b> <b>items</b> of the mathematics is advanced reliability, that is 0, 77; the proportion of difficulty index has not normally spread yet, there are 28...|$|R
40|$|Even for {{electrical}} appliances, {{testing for}} illegal behaviors becomes difficult since the software system in an electrical appliance has already become large in size. Actually, the conventional method cannot generate sufficient <b>test</b> <b>items</b> for illegal behaviors. But testing illegal behaviors {{becomes more and}} more important, since the failure of electrical appliances would cause fatal effects on our daily life. We therefore propose a new method for generating appropriate <b>test</b> <b>items</b> to check illegal behaviors, which consists of the following steps: (1) Describe software behavior using use case notation, (2) Analyze illegal behavior by the deviation analysis technique, (3) Construct a software fault tree using the above information, and (4) Generate <b>test</b> <b>items</b> from the software fault tree. This paper also reports the experimental applications to actual development of an electrical appliance. The evaluation results identified that all necessary <b>test</b> <b>items</b> for illegal behaviors are included in the resultant <b>test</b> <b>items.</b> ...|$|R
5000|$|Determination of {{standard}} {{based on the}} contents of each <b>test</b> <b>item.</b>|$|E
5000|$|... #Caption: An {{example of}} one kind of IQ <b>test</b> <b>item,</b> modeled after items in the Raven's Progressive Matricestest ...|$|E
50|$|The Heinkel HeS 1 (HeS - Heinkel Strahltriebwerke) was Germany's first jet engine, {{which was}} a {{stationary}} <b>test</b> <b>item</b> that ran on hydrogen.|$|E
3000|$|Apart from few {{matching}} tasks, the <b>test</b> <b>items</b> {{are designed}} primarily using open-ended formats {{in terms of}} performance tasks and analytic writing tasks (cf. the Collegiate Learning Assessment approach by Shavelson 2008). Subsequently, two examples of <b>test</b> <b>items</b> are illustrated: [...]...|$|R
40|$|Teachers {{assess and}} try to judge their {{students}} as how much s/he learns, s/he passes or fails, and as well judgment of how much the students who learn in class. Based on exam <b>test</b> <b>items</b> teachers receive equality in education in a class. The <b>test</b> <b>items</b> which are made by individual teachers as to the learning of learners are assessed. The exam <b>test</b> <b>items</b> are different from each other. So the exams are not standardized, mathematic {{is seen as an}} abstract subject in particular. The experience of almost all the educated layer is that the context of the textbook and the context of teaching in schools, the learning level is the low level which is memorization and recalling (the formulas) are the core to motivate the students instead of to have understanding and problem solving approach. I want to know how much teachers are aware of the learning levels, in particular in math, which levels of learning they assess and <b>test</b> <b>items</b> are measuring which learning levels. Additionally the teachers are not well aware of the goals of the subject in curriculum, in other words the national curriculum is not a document which can guide teachers in the assessment. Teachers make their <b>test</b> <b>items</b> based on textbooks they teach in schools. The main aims of the study is to reveal some aspects of the grade subject exam by analyzing the <b>test</b> <b>items</b> from teachers and will study the weaknesses and challenges of the grade exam <b>test</b> <b>items.</b> The study collects <b>test</b> <b>items</b> of math subject from 100 teachers in grade 7 urban schools to compare and analyze the level of learning hierarchy. Bloom’s taxonomy will be the frame of analysis. Questionnaires helped me to collect the respondents’ background data. The analysis the <b>test</b> <b>items</b> were more of text analysis when the <b>test</b> <b>items</b> were classified into the categories used by Bloom’s taxonomy this is the reason why qualitative approach was used. Almost 87 % of teachers (male and female) said that, they construct questions to assess students understanding level, which is the second learning/knowledge level in Bloom´s taxonomy. Overall result of analysis. While 79 % of teachers always using recalling formulas. Concepts: Learning levels, Bloom Taxonomy, assessment, Summative and formative assessments, curriculum, exam test...|$|R
40|$|In {{teaching}} learning process, teachers {{convey the}} materials from early {{until the end}} of teaching learning period. In the end of teaching learning period, teachers need a test. Test is an activity used to measure the ability of the students in teaching learning process. The test which is given to the students must have the qualities or characteristics; they are practicality, reliability, and validity. The most important validity from the test is content validity. The objectives of this study are 1) To describe the content validity of reading <b>test</b> <b>items</b> based on syllabus of School Based-Curriculum that the students are expected to understand the short functional text and short simple essay, 2) To describe the content validity of writing <b>test</b> <b>items</b> based on syllabus of School Based-Curriculum that the students are expected to express the short functional text and short simple essay. The primary data are Regional English <b>test</b> <b>items</b> for the first and second semester test of SMP in Karanganyar region. The data were collected by finding and learning the <b>test</b> <b>items,</b> reading the <b>test</b> <b>items</b> to get more understanding, observing the <b>test</b> <b>items,</b> selecting and categorizing data, arranging and developing the selecting data. Then the data were analyzed by using indicators of School Based-Curriculum. The result of this study shows that 1) the content of Regional English <b>test</b> <b>items</b> for reading skill is valid based on School Based-Curriculum. It is based on the following evidences: a) Reading skill is very good (100...|$|R
5000|$|... #Caption: An IQ <b>test</b> <b>item</b> in {{the style}} of a Raven's Progressive Matrices test. Given eight patterns, the subject must {{identify}} the missing ninth pattern ...|$|E
50|$|Belt, Individual Equipment - Adopted; {{but with}} the {{standard}} buckle. The <b>test</b> <b>item</b> had been equipped with the Davis two-piece aluminum buckle which provided a quick-release capability.|$|E
50|$|In most EQA schemes, {{laboratories}} receive {{scores for}} their results. The most popular score is the Z-score, also called standard deviation index (SDI). The score is given per analyte and per <b>test</b> <b>item.</b>|$|E
40|$|Jamiatul Ulya. 2015. The Difference of Students’ Ability in Answering Matching and Multiple Choice Test Items on Dialogues at Grade X of MAN 3 Banjarmasin. Thesis. English Education Department. Faculty of Tarbiyah and Teachers Training. Advisor: Dra. Hj. NidaMufidah, M. Pd. Keywords: Matching, Multiple Choices, Dialogues This study {{describes}} the differences of students’ ability in answering matching and multiple choices <b>test</b> <b>items</b> on dialogues at grade X of MAN 3 Banjarmasin. The problem statements {{of this research}} are: how students’ ability in answering matching <b>test</b> <b>items</b> on dialogues is, how students’ ability in answering multiple choice <b>test</b> <b>items</b> on dialogues is, and how the difference of students’ ability in answering matching and multiple choice <b>test</b> <b>items</b> on dialogues is. The population {{of this research is}} 242 students at grade X of MAN 3 Banjarmasin academic year 2014 / 2015. The writer uses purposive random sampling technique by taking 15 students for each class. Thus, the total sample of this study is 90 students. Object of this study is students’ ability in answering matching on dialogues, and students’ ability in answering multiple choice <b>test</b> <b>items</b> on dialogues and the difference of students’ ability in answering matching and multiple choice <b>test</b> <b>items</b> on dialogues. To collect the data, the writer uses test, observation and documentary. Data processing in this study is divided into four steps, they are editing, coding and classification, tabulating, and interpreting. Then, all the data are analyzed descriptive quantitatively. The result of this study states that the students’ ability in answering matching <b>test</b> <b>items</b> on dialogues at grade X of MAN 3 Banjarmasin is in poor category, {{it is based on the}} mean score of students 57, 33, whereas in multiple choice, the students’ ability is in very good category, it is based on the mean score of students 82, 22. It means multiple choice <b>test</b> <b>items</b> are easier than matching <b>test</b> <b>items.</b> The result of calculation by using Zcountis - 11, 33 while the value of 5...|$|R
40|$|This {{research}} aims {{to produce}} a valid and reliable mathematics assessment instrument {{in the form of}} HOTS <b>test</b> <b>items,</b> and describe the quality of HOTS <b>test</b> <b>items</b> to measure the high order thinking skill of grade VIII SMP students. This study was a research and development study adapting Borg & Gall’s development model, including the following steps: research and information collection, planning, the early product development, limited try out, revising the early product, field try out, and revising the final product. The research’s result shows that the HOTS assessment instrument in the form of HOTS <b>test</b> <b>items</b> consists of 24 multiple – choice <b>test</b> <b>items</b> and 19 essay <b>test</b> <b>items,</b> based on the judgement of the materials, construction, and language is valid and appropriate to be used. The reliability coefficients of the instrument are 0. 713 for the multiple choice items, and 0. 920 for essays. The multiple choice items has the average of item difficulty 0. 406 (average), the average of item discrimination 0. 330 (good), and the distractors function well. The essay <b>test</b> <b>items</b> has the average of item difficulty 0. 373 (average) and the average of item discrimination 0. 508 (good). Keywords: development, assessment instrument, Higher Order Thinking Skills (HOTS), mathematics in the junior high schoo...|$|R
40|$|Imagine two {{identical}} people receive {{exactly the}} same training on how to classify certain objects. Perhaps surprisingly, we show that one can then manipulate them into classifying some <b>test</b> <b>items</b> in opposite ways, simply depending on what other <b>test</b> <b>items</b> {{they are asked to}} classify (without label feedback). We call this the Test-Item Effect, which can be induced by the order or the distribution of <b>test</b> <b>items.</b> We formulate the Test-Item Effect as online semi-supervised learning, and extend three standard human category learning models to explain it. 1...|$|R
50|$|Lightweight. The {{lightweight}} test {{is a test}} {{performed on}} the lightweight shock machine. Weight of the <b>test</b> <b>item</b> including fixture to attach it to the test machine shall be less than 550 pounds.|$|E
50|$|Students sitting this <b>test</b> <b>item</b> format have {{a greater}} chance of {{answering}} incorrectly if they cannot synthesise and apply their knowledge as shown through the work of Susan Case and David Swanson (1989).|$|E
50|$|Medium weight. The medium {{weight test}} {{is a test}} {{performed}} on the medium weight shock machine. Weight of the <b>test</b> <b>item</b> including fixture to attach it to the test machine shall be less than 7,400 pounds.|$|E
50|$|When writing {{achievement}} <b>test</b> <b>items,</b> writers usually {{begin with}} a list of content standards (either written by content specialists or based on state-created content standards) which specify exactly what students are expected to learn in a given school year. The goal of item writers is to create <b>test</b> <b>items</b> that measure the most important skills and knowledge attained in a given grade-level. The number and type of <b>test</b> <b>items</b> written is determined by the grade-level content standards. Content validity is determined by the representativeness of the items included on the final test.|$|R
40|$|Abstract — The virtual probe (VP) technique, {{based on}} recent {{breakthroughs}} in compressed sensing, has demonstrated its ability for accurate prediction of spatial variations {{from a small}} set of measurement data. In this paper, we explore its application to cost reduction of production testing. For a number of <b>test</b> <b>items,</b> the measurement data from a small subset of chips {{can be used to}} accurately predict the performance of other chips on the same wafer without explicit measurement. Depending on their statistical characteristics, <b>test</b> <b>items</b> can be classified into three categories: highly predictable, predictable, and un-predictable. A case study of an industrial RF radio transceiver with more than 50 production <b>test</b> <b>items</b> shows that a good fraction of these <b>test</b> <b>items</b> (39 out of 51 items) are predictable or highly predictable. In this example, the 3 σ error of VP prediction is less than 12 % for predictable or highly predictable <b>test</b> <b>items.</b> Applying the VP technique can on average replace 59 % of test measurement by prediction and, consequently, reduce the overall test time b...|$|R
40|$|Evaluation is an {{activity}} to get, to analyze, and {{to interpret the}} data from the measurement process. The evaluation has close relation to the learning activities, because evaluation is a continuous process which should have a goal and instrument to measure the progress of the student’s achievement. To measure them, it needs an instrument called a test. For this reason, a test must be valid and reliable in order to get accurate data to fulfill the judgment. In this thesis, the writer only focused on the content validity of the English <b>test</b> <b>items</b> in national examination (UN) 2005 / 2006. Dealing with the research method, the writer used descriptive research design to describe and to analyze the content validity of English <b>tests</b> <b>items</b> in national examination (UN) 2005 / 2006. The English <b>test</b> <b>items</b> in UN 2005 / 2006 and Competence Based Curriculum or Curriculum 2004 were taken as the data. The collected were analyzed to know whether the English <b>test</b> <b>items</b> in UN 2005 / 2006 are valid or not. The result of the data analysis showed that the English <b>test</b> <b>items</b> in national examination (UN) 2005 / 2006 were valid in term of their content. In other words, the English <b>test</b> <b>items</b> in UN 2005 / 2006 were considered high validity because it covered 98...|$|R
