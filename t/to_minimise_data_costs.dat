0|10000|Public
30|$|We {{present a}} hybrid system {{spanning}} a fixed-function microarchitecture and a general-purpose microprocessor, designed to amplify the throughput and decrease the power dissipation of collision detection relative to {{what can be}} achieved using CPUs or GPUs alone. The primary component {{is one of the}} two novel microarchitectures designed to perform the principal elements of broad-phase collision detection. Both microarchitectures consist of pipelines comprising a plurality of memories, which rearrange the input into a format that maximises parallelism and bandwidth. The two microarchitectures are combined with the remainder of the system through an original method for sharing data between a ray tracer and the collision-detection microarchitectures <b>to</b> <b>minimise</b> <b>data</b> structure construction <b>costs.</b> We effectively demonstrate our system using several benchmarks of varying object counts. These benchmarks reveal that, for over one million objects, our design achieves an acceleration of 812 × relative to a CPU and an acceleration of 161 × relative to a GPU. We also achieve energy efficiencies that enable the mitigation of silicon power-density challenges, while making the design amenable to both mobile and wearable computing devices.|$|R
40|$|Although {{altmetrics}} {{and other}} web-based alternative indicators are now commonplace in publishers' websites, {{they can be}} difficult for research evaluators to use {{because of the time}} or expense of the data, the need to benchmark in order to assess their values, the high proportion of zeros in some alternative indicators, and the time taken to calculate multiple complex indicators. These problems are addressed here by (a) a field normalisation formula, the Mean Normalised Log-transformed Citation Score (MNLCS) that allows simple confidence limits to be calculated and is similar to a proposal of Lundberg, (b) field normalisation formulae for the proportion of cited articles in a set, the Equalised Mean-based Normalised Proportion Cited (EMNPC) and the Mean-based Normalised Proportion Cited (MNPC), to deal with mostly uncited data sets, (c) a sampling strategy <b>to</b> <b>minimise</b> <b>data</b> collection <b>costs,</b> and (d) free unified software to gather the raw data, implement the sampling strategy, and calculate the indicator formulae and confidence limits. The approach is demonstrated (but not fully tested) by comparing the Scopus citations, Mendeley readers and Wikipedia mentions of research funded by Wellcome, NIH, and MRC in three large fields for 2013 - 2016. Within the results, statistically significant differences in both citation counts and Mendeley reader counts were found even for sets of articles that were less than six months old. Mendeley reader counts were more precise than Scopus citations for the most recent articles and all three funders could be demonstrated to have an impact in Wikipedia that was significantly above the world average. Comment: Thelwall, M. (in press). Three practical field normalised alternative indicator formulae for research evaluation. Journal of Informetrics. doi: 10. 1016 /j. joi. 2016. 12. 002 Changes from the previous version are highlighted in yello...|$|R
40|$|Traditionally the {{interest}} in voice gender conversion was of a more theoretical nature rather than founded in real [...] life applications. However, {{with the increase in}} mobile communication and the resulting limitation in transmission bandwidth new approaches <b>to</b> <b>minimising</b> <b>data</b> rates have <b>to</b> be developed. Here voice gender normalisation (VGN) presents an efficient method of achieving higher compression rates by using the VGN algorithm to remove gender specific components of a speech signal and thus enhancing the information content to be transmitted...|$|R
40|$|We explore two {{different}} threading approaches on a {{graphics processing unit}} (GPU) exploiting {{two different}} characteristics of the current GPU architecture. The fat thread approach tries <b>to</b> <b>minimise</b> <b>data</b> access time by relying on shared memory and registers potentially sacrificing parallelism. The thin thread approach maximises parallelism and tries to hide access latencies. We apply these two approaches to the parallel stochastic simulation of chemical reaction systems using the stochastic simulation algorithm (SSA) by Gillespie (J. Phys. Chem, Vol. 81, p. 2340 - 2361, 1977). In these cases, the proposed thin thread approach shows comparable performance while eliminating the limitation of the reaction system’s size...|$|R
40|$|There {{is limited}} {{research}} {{carried out to}} date in the academic literature addressing {{the issue of the}} ideal in-sample size when forecasting volatility. This paper therefore considers how much data is required in order to produce accurate forecasts. Broadly speaking, two views exist between practitioners/investors who typically prefer a small in-sample <b>to</b> <b>minimise</b> <b>data</b> holding requirements and researchers/academics who typically chose large in-sample periods. Using a process of expanding window regressions where the in-sample start period expands (backward recursion) we conduct forecasts over twenty-three international markets, including both developed and emerging. Our findings, which demonstrate a degree of homogeneity, show that {{for the majority of the}} markets large in-sample periods are not necessary in order to produce the most accurate forecasts supporting the practitioners’/investors’ view...|$|R
40|$|This study {{presents}} a minimum jitter-based adaptive decision feedback equaliser (DFE) for giga-bit-per-second (Gbps) serial links. The adaptation {{in search for}} the optimal tap coefficients of DFE is carried out with the objective <b>to</b> <b>minimise</b> <b>data</b> jitter {{at the edge of}} data eyes. Jitter minimisation is achieved by adjusting the slope of the DFE that counteracts that of the channel. The effectiveness of the proposed adaptive DFE is evaluated by embedding the DFE in a 2 Gbps serial link. The data link is analysed using Spectre from Cadence Design Systems with BSIM 4 device models. Simulation results demonstrate that the proposed adaptive DFE is capable of opening closed data eyes with 83 % vertical opening, 68 % horizontal opening and 16 % data jitter over 1 m FR 4 channel while consuming 15. 45 mW...|$|R
40|$|While service-oriented cloud {{workflow}} shows potentials of inherent scalability, {{loose coupling}} and expenditure reduction, {{such issues as}} data transfer and efficiency have popped up as major concerns. In this paper, a dataflow opt imisation mechanism is proposed, including data transfer strategy and data placement strategy. The data transfer strategy uses integrated data structures to enable {{the elimination of the}} SOAP serialisation and de-serialisation within the web service invocation by the exchange of their references. In additi on, the data placement strategy groups the existing datasets and dynamically cluste rs newly generated datase ts <b>to</b> <b>minimise</b> <b>data</b> transfers while keeping a polynomial time complexity. By integrating the optimisation mechanism into service-oriented cloud workflow system, we can expect efficiency improvements in data transfer and workflow execution. Experiments and analysis supported our efforts. Peng Zhang, Yanbo Han, Muhammad Ali Baba...|$|R
30|$|Distance: Human {{mobility}} and distance {{play an important}} role in the formation of links, both online and offline, and have been shown to be highly indicative of social ties and informative for link prediction [28]. We calculate the distance between the geographic coordinates of two users’ most frequent check-in locations as the Haversine distance, the most common measure of great-circle spherical distance: dist_ij = haversine (lat_i,lon_i, lat_j,lon_j), where the coordinate pairs for i,j are those of the places where users with more than two check-ins have checked in most frequently, equivalent to the mode in the multiset of the venues where they have checked in. This allows us <b>to</b> <b>minimise</b> <b>data</b> loss motivated by the typical long-tail distribution of activities shown in empirical studies of Foursquare [24], while increasing the probability that a most frequent location will emerge, similar to previous related work in the field [29 – 31].|$|R
50|$|Legislation will be {{required}} to set in place access to the VII data and communications between applicable agencies. In the USA, for example, an Interstate is a Federal roadway that is often maintained by the State, but the local county or municipal authorities may be involved too. The legislation would need to set the levels of authority of each agency. In Pennsylvania, for example, municipalities tend to have greater authority than counties and sometimes even the State whereas neighboring Maryland has more authority at the county level than at municipal level; and State roads are almost exclusively controlled by the State. It would also have to be determined which other agencies can use the data (i.e. law enforcement, Census, etc.) and to what degree it is permissible to use the information. Law enforcement would be needed <b>to</b> <b>minimise</b> <b>data</b> misuse. The various levels of authority could also increase incompatibility.|$|R
2500|$|Legislation will be {{required}} to set in place access to the VII data and communications between applicable agencies. [...] In the USA, for example, an Interstate is a Federal roadway that is often maintained by the State, but the local county or municipal authorities may be involved too. [...] The legislation would need to set the levels of authority of each agency. [...] In Pennsylvania, for example, municipalities tend to have greater authority than counties and sometimes even the State whereas neighboring Maryland has more authority at the county level than at municipal level; and State roads are almost exclusively controlled by the State. [...] It would also have to be determined which other agencies can use the data (i.e. law enforcement, Census, etc.) and to what degree it is permissible to use the information. Law enforcement would be needed <b>to</b> <b>minimise</b> <b>data</b> misuse. [...] The various levels of authority could also increase incompatibility.|$|R
40|$|Traditionally the {{interest}} in voice gender conversion was of a more theoretical nature rather than founded in real-life applications. However, {{with the increase in}} mobile communication and the resulting limitation in transmission bandwidth new approaches <b>to</b> <b>minimising</b> <b>data</b> rates have <b>to</b> be developed. Here voice gender normalisation (VGN) presents a novel method of achieving higher compression rates by using the VGN algorithm to remove all gender specific components of a speech signal and thus leaving only the information content to be transmitted. A second application for VGN is in the field of speech controlled systems, where current speech recognition algorithms {{have to deal with the}} voice characteristics of a speaker as well as the information content. Here again the use of VGN can remove the speakers voice characteristics leaving only the pure information. Therefore, such a system would be capable of achieving much higher recognition rates while being independent of the speaker. This paper presents the theory of a gender removal system based on VGN and furthermore, outlines an efficient real-time hardware implementation for use in portable communications equipment...|$|R
40|$|Abstract—Wearable sensor nodes {{monitoring}} {{the human body}} must operate autonomously for very long periods of time. Online and low power data compression embedded within the sensor node is therefore essential <b>to</b> <b>minimise</b> <b>data</b> storage/transmission overheads. This paper presents a low power MSP 430 compressive sensing implementation for providing such compression, focusing particularly {{on the impact of}} the sensor node architecture on the compression performance. Compression power performance is compared for four different sensor nodes incorporating different strategies for wireless transmission/on-sensor-node local storage of data. The results demonstrate that the compressive sensing used must be designed differently depending on the underlying node topology, and that the compression strategy should not be guided only by signal processing considerations. We also provide a practical overview of state-of-the-art sensor node topologies. Wireless transmission of data is often preferred as it offers increased flexibility during use, but in general at the cost of increased power consumption. We demonstrate that wireless sensor nodes can highly benefit from the use of compressive sensing and now can achieve power consumptions comparable to, or better than, the use of local memory. Index Terms—Compressive sensing, MSP 430, low power con-sumption, wearable medical sensors, body area networks, EEG. I...|$|R
40|$|The {{following}} work {{focuses on}} the requirements and development of a component to provide information for Collaborative Augmented Reality Systems by using Web technologies. In collaborative mobile environments a mechanism is needed to distribute information by event handling on different devices for multiple users. Therefore a component is required <b>to</b> <b>minimise</b> the <b>data</b> traffic which occur during information exchange between all collaborative users. An analysis of useful scenarios and derivation of possible resulting requirements to {{form the basis for}} this component will be shown in this paper. Finally the component was successfully integrated in a collaborative AR environment in the industrial field...|$|R
40|$|Abstract Background High-throughput custom {{designed}} genotyping arrays are {{a valuable}} resource for biologically focused research studies and increasingly for validation of variation predicted by next-generation sequencing (NGS) technologies. We investigate the Illumina GoldenGate chemistry using custom designed VeraCode and sentrix array matrix (SAM) assays {{for each of these}} applications, respectively. We highlight applications for interpretation of Illumina generated genotype cluster plots to maximise data inclusion and reduce genotyping errors. Findings We illustrate the dramatic effect of outliers in genotype calling and data interpretation, as well as suggest simple means to avoid genotyping errors. Furthermore we present this platform as a successful method for two-cluster rare or non-autosomal variant calling. The success of high-throughput technologies to accurately call rare variants will become an essential feature for future association studies. Finally, we highlight additional advantages of the Illumina GoldenGate chemistry in generating unusually segregated cluster plots that identify potential NGS generated sequencing error resulting from minimal coverage. Conclusions We demonstrate the importance of visually inspecting genotype cluster plots generated by the Illumina software and issue warnings regarding commonly accepted quality control parameters. In addition to suggesting applications <b>to</b> <b>minimise</b> <b>data</b> exclusion, we propose that the Illumina cluster plots may be helpful in identifying potential in-put sequence errors, particularly important for studies to validate NGS generated variation. </p...|$|R
40|$|The last temptation is the {{greatest}} treason: To do the right deed for the wrong reason. (T. S. Eliot. Murder in the Cathedral) The last temptation is {{the greatest}} treason: To do the right deed for the wrong reason. (T. S. Eliot. Murder in the Cathedral) EC 5. 2 An ACM member, whenever dealing with data concerning individuals, shall always consider {{the principle of the}} individual’s privacy and seek the following: <b>To</b> <b>minimise</b> the <b>data</b> collected. <b>To</b> limit authorized access to the data. To provide proper security for the data. To determine the required retention period of the data. To ensure the proper disposal of the data...|$|R
40|$|We {{present the}} {{knowledge}} organisation of a database, {{which aims to}} collect information for the project "Plants in European Masterpieces". This project is financed by the European Union within the Culture 2000 program. The project is aimed to link art with plants within the frame of a common European history {{and to make it}} accessible to a wide public. The informatics aspect of this project is focused on the creation of a data base which collects detailed information on selected plants and masterpieces in which these plants are presented. The main efforts in the design phase have been devoted to knowledge organisation in order to retain the taxonomic structure of the botanical knowledge, and <b>to</b> <b>minimise</b> <b>data</b> structure redundancies by linking some features which {{can be applied to the}} plants at different levels of their taxonomy to the correct level without repeating it. At present the database is installed in the Pc of all the experts that in the 5 involved countries collaborate to collect data for this project. This data is weekly merged in a central database, using a specific automatic procedure that avoids possible duplications of data inserted and checks for possible inconsistencies among data inserted in different temporal databases. The multi-language and easy to use features of this data base make it an interesting prototype for the spreading of scientific information to common publi...|$|R
40|$|Measurement of {{household}} food insecurity {{is needed to}} identify the magnitude of food insecurity and {{assess the impact of}} development interventions. However, there is no commonly agreed measure {{of household}} food insecurity. While researchers continually experiment with new measures, the resultant measures are often complex and include numerous variables that still do not distinguish clearly between the food secure and the food insecure. This study set out to prepare a quick and convenient tool to measure household food security, using common household demographic and socio-economic variables commonly collected through a variety of household surveys. This has <b>minimised</b> <b>data</b> collection <b>costs</b> and assisted national food security units to continually measure and monitor household food insecurity. Food insecurity levels were estimated using data from a baseline survey conducted in a community in KwaZulu-Natal, South Africa. Food security was estimated using a number of measures, including food quantity (adequacy), dietary diversity, dietary quality, coping strategies employed and the Coping Strategies Index...|$|R
40|$|Our {{ultimate}} aim was {{to achieve}} commodity telepresence systems capable of communicating both what someone looks like and what, within the technology joined space, they are looking at. Towards this we have implemented a previously distributed approach to reconstructing form from multiple video streams, so that it runs on a single computer. Importantly, {{the way in which}} the problem is parallelised has been optimised to reflect the various stages of the process rather than the need <b>to</b> <b>minimise</b> <b>data</b> communication across a network. The Exact Polyhedral Visual Hull (EPVH) algorithm had previously been distributed to achieve real time frame rates. EPVH has five sequential steps of which four were previously parallelised as two pairs. The metric for parallelisation of each pair was thus the best fit across both sequential steps within it and the outcome of the first stage of a pair could not determine the parallelisation of the second. We parallelised all five stages according to both distinct metrics and data from the previous stage. In this way we provided a better fit of parallelisation to both process and data. The study proposes a method of parallelisation theoretically more tailored to execution on a single machine, providing a detailed description of the implementation along with a number of optimisations to further improve performance and provides indicative results, for example multicore CPU and GPU platforms that might be of interest to researchers and practitioners wishing to implement a real-time 3 D reconstruction system...|$|R
30|$|The {{evaluation}} of the CRC error correction {{is based on a}} large set of corrupted packets that is collected in an office environment. In this case, the receiver was fixed on a desk, whilst the wearable transmitter was positioned in different locations within the office and the surrounding areas. <b>To</b> <b>minimise</b> interference, the <b>data</b> collection sessions were performed at off-peak hours when the office was empty. In a period of over 10 h, we collected a total of approximately 400, 000 packets, 6000 of which were corrupted.|$|R
40|$|Location-based {{services}} and mobile applications that use cartographic data {{are becoming increasingly}} popular. When serving mobile clients with map data across a network it is important <b>to</b> <b>minimise</b> the <b>data</b> <b>to</b> be transferred. A vector-based data representation together with employing cartographic map generalisation to produce multiple maps of the same area at decreasing level of detail serves this goal. This paper presents core concepts in map-based applications and map generalisation. The theory of map generalisation {{and the state of}} the art algorithms in that field are explained, with a focus on preserving the topological relationships between map objects. The SmoothShp map generalisation algorithm is conveyed and discussed. A method for measuring the effect of map generalisation in a complete system for location-based services with web and mobile phone clients is presented, measurements are carried out and the results discussed. </p...|$|R
40|$|Slow and {{suspicious}} activities on modern computer networks are increasingly hard to detect. An attacker may take days, {{weeks or months}} to complete an attack life cycle. A particular challenge is to monitor for stealthy attempts deliberately designed to stay beneath detection thresholds. This doctoral research presents a theoretical framework for effective monitoring of such activities. The main contribution of this work is a scalable monitoring scheme proposed in a Bayesian framework, which allows for detection of multiple attackers by setting a threshold using the Grubbs’ test. Second contribution is a tracing algorithm for such attacks. Network paths from a victim to its immediate visible hops are mapped and profiled in a Bayesian framework and the highest scored path is prioritised for monitoring. Third contribution explores an approach <b>to</b> <b>minimise</b> <b>data</b> collection by employing traffic sampling. The traffic is sampled using the stratification sampling technique with optimum allocation method. Using a 10 % sampling rate was sufficient to detect simulated attackers, and some network parameters affected on sampling error. Final contribution is a target-centric monitoring scheme to detect nodes under attack. Target-centric approach is quicker to detect stealthy attacks and has potential to detect collusion as it completely independent from source information. Experiments are carried out in a simulated environment using the network simulator NS 3. Anomalous traffic is generated along with normal traffic within and between networks using a Poisson arrival model. Our work addresses a key problem of network security monitoring: a scalable monitoring scheme for slow {{and suspicious}} activities. State size, {{in terms of a}} node score, is a small number of nodes in the network and hence storage is feasible for very large networks...|$|R
40|$|Online social {{networks}} {{make it easier}} for people to find and communicate with other people based on shared interests, values, membership in particular groups, etc. Common {{social networks}} such as Facebook and Twitter have hundreds of millions or even billions of users scattered all around the world sharing interconnected data. Users demand low latency access to not only their own data but also theirfriends’ data, often very large, e. g. videos, pictures etc. However, social network service providers have a limited monetary capital to store every piece of <b>data</b> everywhere <b>to</b> <b>minimise</b> users’ <b>data</b> access latency. Geo-distributed cloud services with virtually unlimited capabilities are suitable for large scale social networks data storage in different geographical locations. Key problems including how to optimally store and replicate these huge datasets and how to distribute the requests to different datacenters are addressed in this paper. A novel genetic algorithm-based approach is used to find a near-optimal number of replicas for every user’s data and a near-optimal placement of replicas <b>to</b> <b>minimise</b> monetary cost while satisfying latency requirements for all users. Experiments on a large Facebook dataset demonstrate our technique’s effectiveness in outperforming other representative placement and replication strategies...|$|R
40|$|Since {{computers}} and computer information systems (IS) have appeared, {{the necessity for}} the existing data processing methodology modification according to possibilities of computer data processing has been forced. The very IS based on computer data processing have its evolution, starting from partial, integrated and up to enterprise resource planning (ERP) systems. Most often problems in this area are: (1) the problem of getting the adequate analyses for the management and implementation of business policy because the obtained information is too old or incomplete; (2) implementation of information systems lasts inappropriately long and demands huge costs; (3) {{there is no significant}} rationalization of administrative work and papers. Changing these methodologies, one comes to standardisation of the database model as well as <b>to</b> <b>minimising</b> the <b>data</b> input flows for the database loading and updating. In this way, the quality of information for of management has been improved and the business rationalized. Therefore, we developed an integral IS that can improve business of the company and the sustainable development of the economy as wel...|$|R
40|$|A {{spatial data}} {{clearinghouse}} is an electronic facility for searching, viewing, transferring, ordering, advertising, and disseminating spatial data from numerous sources via the Internet. Governments {{and other institutions}} have been implementing spatial <b>data</b> clearinghouses <b>to</b> <b>minimise</b> <b>data</b> duplication and thus reduce the <b>cost</b> of spatial <b>data</b> acquisition. Underlying these clearinghouses are geoportals and databases of geospatial metadata. A geoportal is an access point of a spatial data clearinghouse and metadata is data that describes data. The success of a clearinghouse's spatial data discovery system is dependent {{on its ability to}} communicate the contents of geospatial metadata by providing both visual and analytical assistancet o a user. The model currently adopted by the geographic information community was inherited from generic information systems and thus to an extent ignores spatial characteristics of geographic data. Consequently, research in Geographic Information Retrieval (GIR) has focussed on spatial aspects of webbased data discovery and acquisition. This thesis considers how the process of GIR from geoportals can be enhanced through multidimensional visualisation served by web-based geographic data sources. An approach is proposed for the presentation of search results in ontology assisted GIR. Also proposed is an approach for the visualisation of multidimensional geographic data from web-based data sources. These approaches are implemented in two prototypes, the Geospatial Database Online Visualisation Environment (GeoDOVE) and the Spatio-Temporal Ontological Relevance Model (STORM). A discussion of their design, implementation and evaluation is presented. The results suggest that ontology-assisted visualisation can improve a user's ability to identify the most relevant multidimensional geographic datasets from a set of search results. Additional results suggest {{that it is possible to}} offer the proposed visualisation approaches on existing geoportal frameworks. The implication of the results is that multidimensional visualisation should be considered by the wider geographic information community as an alternative to historic approaches for presenting search results on geoportals, such as the textual ranked list and two-dimensional maps. EThOS - Electronic Theses Online ServiceUniversity of Newcastle upon TyneGBUnited Kingdo...|$|R
40|$|This thesis investigates visual, {{spatial and}} {{temporal}} qualities of video based reconstruction with respect to telepresence. State of the art was improved and validated through a new parallelisation of an established algorithm; a tool that allows visio-spatial impact of algorithm and camera arrangement to be visualised; {{and a set of}} experiments to derive requirements and investigate outcomes. The motivation is to support the exchange of appearance and attention between moving humans through video based reconstruction. A previous research project showed moving humans could faithfully convey attention in virtual environments and appearance through video-conferencing, suggesting that {{it may be possible to}} combine the two. Video based 3 D reconstruction of humans appeared to be able to achieve both, but it was uncertain whether this could be achieved at sufficient quality. Research began by justifying the approach and setting the requirements. A literature survey and initial experiments indicated that the visual hull provided a form suitable for modeling humans. However, evidence of visual and temporal qualities necessary to support gaze was not found. A state of the art visual hull reconstruction algorithm was parallelised to run on a modern multi-core processor, enabling human reconstruction on a single computer, thus providing stable visual and temporal qualities. A parallelisation scheme theoretically better suited for execution on a single multi-core processor than distributed over a network is proposed. Importantly, the way in which the problem is parallelised has been optimised to reflect the various stages of the process rather than the need <b>to</b> <b>minimise</b> <b>data</b> communication across a network. A utility application has been developed providing a framework for rapidly testing algorithms, validating requirements, and as a platform for conducting experiments. This underpinned a collaborative experiment that showed for the first time that eye gaze could be conveyed to accuracies sufficient for human social interaction. To facilitate the analysis, the utility allowed the impact of camera placement on spatial and visual quality to be investigated...|$|R
40|$|Addressing {{housing needs}} in post {{conflict}} housing reconstruction leads to successful housing reconstruction. As {{part of a}} study of investigating how the housing needs can be effectively addressed in post conflict housing reconstruction, this paper identifies the gaps in managing housing needs in post conflict housing reconstruction {{within the context of}} Sri Lanka and presents the recommendations <b>to</b> <b>minimise</b> such gaps. <b>Data</b> was collected through un-structured interviews conducted with 37 participants, comprising policy makers, practitioners, academics and beneficiaries who engaged in post conflict housing reconstruction in Sri Lanka. Gaps were mainly found in conflict sensitivity, measures related to physical housing, performance of implementing agencies, policy and practice issues. On the job training, application of ‘do no harm’ principles, enhanced beneficiary participation, enhanced accountability, effective monitoring, enhanced knowledge sharing, adequate drinking water facilities, irrigation development and initiatives for material manufacturing were suggested as recommendations <b>to</b> <b>minimise</b> these gaps. Identification of gaps in managing housing needs in post conflict housing reconstruction and recommendations <b>to</b> <b>minimise</b> them inform policy makers to address the housing needs effectively through incorporating these aspects into the related policies. This in turn enhances the sustainability in housing development after conflicts...|$|R
40|$|In {{many cases}} future {{offshore}} projects will {{be not only}} larger than any existing projects, but also located further from the nearest harbour facilities and at greater water depths. As a result the technical and com-mercial risks related to reliability will be dramatically increased relative to the projects that form the ex-perience base of today. The paper presents the ARM (Availability, Reliability, Maintainability) model developed by Siemens Wind Power. The ARM model is used to quantify the risks and to highlight areas of design modifications required <b>to</b> <b>minimise</b> risks. Background <b>data</b> from existing offshore wind farms are presented. The methodology and conclusions of the ARM model are shown, and using the Siemens 3. 6 MW wind turbine as example {{the implementation of the}} conclusions in real life is demonstrated...|$|R
40|$|This {{research}} {{attempts to}} describe and recommend a way to share a precise information environment between Business to Business (B 2 B) users in the fashion retail business. The contribution {{of this research is}} to propose a framework to improve components of Information Quality (IQ) with Automated Information System (AIS) for the fashion retail business in a practical way via a case study of a logistics distribution firm in Thailand. The core set of information quality and Benefits/Costs (BC) quantitative and qualitative analysis of IQ have been presented through a case study which can be obtained by using the Analytic Network Process (ANP) model. Several designed procedures in AIS have been upgraded <b>to</b> <b>minimise</b> mismatched <b>data.</b> The major result indicates that, based upon the BC analysis, a fully-integrated IQ system should be invested. Moreover, with the enhancement of the IQ project, the logistic distribution firm can prevent loss of current customer dramatically. In terms of quantitative analysis alone, the BC analysis helps to reveal a general theme that loss of current customer risks plays a more important role to the firm than loss of potential new customer. The other results, limitations and recommendations are also presented. Keywords...|$|R
40|$|Key-Value Stores (KVSs) {{have become}} a {{standard}} component for many web services and applications due to their inherent scalability, availability, and reliability. Many enterprises are now adopting them for use on servers leased from Infrastructure-as-a-Service (IaaS) providers. The defining characteristic of IaaS is resource elasticity. KVSs benefit from elasticity, when they incorporate new resources on-demand as KVS nodes to deal with increasing workload, and decommission excess resources to save on operational costs. Elasticity of a KVS poses challenges in allowing efficient, dynamic node arrivals and departures. On one hand, the workload needs to be quickly balanced among the KVS nodes. However, current data partitioning and migration schemes provide low priority to populate new nodes, thereby reducing {{the effect of adding}} resources on increasing workload. On the other hand, dynamic node changes downgrade data durability at multiple node failures caused by hardware failure in IaaS Cloud, which is built from commodity components that fail as the norm at large scales; but current replica placement strategies tend to rely on static mapping of data to nodes for high durability. This thesis proposes a set of data management schemes to address these issues. Firstly, it presents a decentralised automated partitioning algorithm and a lightweight migration strategy, to improve the efficiency of node changes. Secondly, it presents the design of ElasCass, an elastic KVS that incorporates these schemes, implemented atop Apache Cassandra. Finally, it presents a replica placement algorithm with a proof that shows its correctness, to fill the gap of allowing dynamic node changes while maintaining high data durability at multiple node failures. Contributions of this thesis lie in this set of novel schemes for data partitioning, placement, and migration, which provide efficient elasticity for decentralised, shared-nothing KVSs. The evaluations of ElasCass, conducted on Amazon EC 2, revealed that, the proposed schemes reduce node incorporation time and improve load- balancing, thereby increasing scalability and query performance. The other evaluation simulated thousands of KVS nodes and demonstrated that the proposed placement algorithm maintains a close <b>to</b> <b>minimised</b> <b>data</b> loss probability under different failure scenarios, and exhibits better scalability and elasticity than state-of-the-art placement schemes...|$|R
40|$|Missing data {{reduces the}} {{statistical}} power of clinical trials {{and can lead}} to elaborate statistical gymnastics to offset the shortcomings of incomplete study databases. 1 This article provides a case-in-point and offers suggestions <b>to</b> <b>minimise</b> missing <b>data</b> in future studies. The United Kingdom Prospective Diabetes Study (UKPDS) was a randomised therapeutic clinical trial in 5, 102 subjects with newly diagnosed Type 2 diabetes. 2 Subjects attended the clinics every 3 - 4 months, with comprehensive clinical reviews at entry and thereafter every 3 years. At these triennial clinical reviews, visual acuity (VA) was measured and retinal photographs were taken. A coordinating centre in Oxford, England managed 23 research centres in England, Scotland and Wales. The first subject was randomised in 1977. Median follow-up period was 10 years. Results were first published in the Lancet in 1998. 3 We are now using UKPDS study records in a retrospective study of the relationship of lesions of diabetic retinopathy to VA. Our analysis requires at least two consecutive sets of retinal photographs and concurrent VA assessments. With this data, we can measure the affect of changes in diabetic retinopathy lesions on VA. Retinal photography was introduced into UKPDS in 1982, with funding from the U. S...|$|R
30|$|This study {{provides}} preliminary {{evidence that a}} trial protocol with more complete details on the PRO endpoint may {{reduce the risk of}} avoidable missing PRO data. Poor compliance led to non-reporting of PROs for 2 RCTs, meaning that efforts invested into PRO data collection for these RCTs was wasted as the PRO data cannot possibly impact patient care. It also provides evidence that the reporting of PROs requires improvement, particularly reporting of the rates, reasons and impact of missing PRO data. Given that rates of avoidable and informative missing PRO data were quite high in this sample, clear reporting is crucial and should include a transparent discussion of generalisability concerns in light of avoidable and informative missing data. Investigators should refer to the forthcoming SPIRIT-PRO Extension [23] to develop PRO aspects of trial protocols with clear strategies <b>to</b> <b>minimise</b> the missing <b>data,</b> as well as the CONSORT-PRO guidance for reporting [20]. Such efforts will ensure high-quality PRO findings are accurately interpreted and can meaningfully impact patient care.|$|R
40|$|For this {{assignment}} we are aiming {{to use data}} mining techniques {{in the analysis of}} data recorded about road traffic accidents in the West Midlands Area in the year 2000. This data will then hopefully provide drivers with guidelines relating to what measures can be taken to help reduce the chances of them being injured in a road traffic accident. This analysis is therefore important in order to identify potential risks and circumstances which contribute to such accidents, and attempt to highlight measures which can be taken <b>to</b> <b>minimise</b> them. The <b>data</b> is currently in an unmanageable format which hinders the investigation of finding specific links between attributes. This means that no useful conclusions can be accurately drawn at present. We therefore intend to complete the analysis by using Envisioner software and classification techniques to determine attributes of high relevance within the data. Conclusions will then be drawn, helping to identify factors such as speed, weather and road conditions which contribute to an accident occurrence...|$|R
40|$|Recently we have {{introduced}} a novel characterisation {{of the distribution}} of twin primes that consists of three essential elements. These are: that the twins are most naturally viewed as a subsequence of the primes themselves, that the likelihood of a particular prime in sequence being the first element of a twin is akin to a fixed-probability random event, and that this probability varies with π 1, the count of primes up to this number, in a simple way. Our initial studies made use of two unproven assumptions: that it was consistent to model this fundamentally discrete system with a continuous probability density, and that the fact that an upper-bound cut-off for prime separations exists could be consistently ignored in the continuous analysis. The success of the model served as a posteriori justification for these assumptions. Here we perform the analysis using a discrete formalism – not passing to integrals – and explicitly include a self-consistently defined cut-off. In addition, we reformulate the model so as <b>to</b> <b>minimise</b> the input <b>data</b> needed...|$|R
40|$|Purpose: Patient {{reported}} {{outcome measures}} (PROMs) {{are designed to}} assess patients' perceived health states or health-related quality of life. However, PROMs are susceptible to missing data, which can affect the validity of conclusions from randomised controlled trials (RCTs). This review aims to assess current practice in the handling, analysis and reporting of missing PROMs outcome data in RCTs compared to contemporary methodology and guidance. Methods: This structured {{review of the literature}} includes RCTs with a minimum of 50 participants per arm. Studies using the EQ- 5 D- 3 L, EORTC QLQ-C 30, SF- 12, and SF- 36 were included if published in 2013, those using the less commonly implemented HUI, OHS, OKS, and PDQ were included if published between 2009 and 2013. Results: The review included 237 records (4 - 76 per relevant PROM). Complete case analysis and single imputation were commonly used in 33 % and 15 % of publications, respectively. Multiple imputation was reported for 9 % of the PROMs reviewed. The majority of publications (93 %) failed to describe the assumed missing data mechanism, while low numbers of papers reported methods <b>to</b> <b>minimise</b> missing <b>data</b> (23 %), performed sensitivity analyses (22 %) or discussed the potential influence of missing data on results (16 %). Conclusions: Considerable discrepancy exists between approved methodology and current practice in handling, analysis and reporting of missing PROMs outcome data in RCTs. Greater awareness is needed of the potential biases introduced by inappropriate handling of missing data; as well as the importance of sensitivity analysis and clear reporting to enable appropriate assessments of treatment effects and conclusions from RCTs...|$|R
30|$|<b>Minimising</b> the <b>data</b> centre’s energy consumption, on {{one hand}} {{acknowledges}} the potential of ICT for saving energy across many segments of the economy, {{on the other hand}} helps ICT sector to show the way {{for the rest of the}} economy by reducing its own carbon footprint. In this paper, we show that it is possible to save energy by studying the case of a single-site private cloud data centres. We believe that through the federation of several cloud data centres (both private and public), it is possible <b>to</b> <b>minimise</b> both the energy consumption as well as CO 2 emissions.|$|R
40|$|This thesis {{describes}} the interfacing of a minicomputer with linear and rotary electrohydraulic control systems, {{to form a}} machine tool configuration for the generation of complex surfaces such as marine screw propeller or gas turbine blade profiles. The study is primarily concerned with {{the ability of the}} system to generate a given profile, specified in numerical form by the computer. The effect of cutting forces {{is beyond the scope of}} this present work. <b>To</b> <b>minimise</b> the <b>data</b> storage area within the minicomputer memory, component shapes have been described in terms of their curvature. An off-line method of developing a linear curvature profile from a known mathematical curve is presented and applied to a Joukowski aerofoil section. The results show that for a loss in accuracy of around 0. 1 %, a saving in memory storage area of 947 can be achieved when the profile Cartesian position co-ordinates are replaced by a representative set of curvature/arc length linear elements. A knowledge of the system model was necessary so that off-line digital computer simulation techniques could be employed to calculate controller parameters. Employing analytical methods, a system model was postulated and experimental testing in the frequency and time domains confirmed the dominant dynamic characteristics. Three different types of control strategy were implemented, the first being direct computer control where the axes of the machine tool were driven from the analogue output of an on-line cutter co-ordinate calculation program. The second type included the minicomputer within the control loop and employed a software PID control algorithm. Finally, the control parameters for an optimal controller designed <b>to</b> <b>minimise</b> a quadratic performance criterion were evaluated and employed on-line. After proving the capability of the system to manufacture an aerofoil blade section, a series of comparative accuracy tests were performed with the purpose of determining which control method would permit the machine tool to operate at the highest possible production rate, compatible with the required geometrical accuracy and surface finish. The tests indicate that there is no practical advantage in placing an additional control loop around the system to include the minicomputer programmed in PID mode. It was further demonstrated that when manufacturing under an optimal control policy, feed-rates can be more than doubled without deterioration in dimensional accuracy or surface finish...|$|R
