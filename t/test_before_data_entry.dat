0|10000|Public
40|$|The {{market model}} of Sharpe {{when applied to}} European, U. S. A. and Japan stock markets usually results with {{heteroscedastic}} error structure. Since heteroscedasticity in error terms cause inefficient parameter estimation, it should be <b>tested</b> <b>before</b> <b>data</b> analysis. The objective {{of this paper is}} to present five widely used likelihood based constructive heteroscedasticity tests which are  the ordinary likelihood ratio test, the conditional likelihood ratio test, the corrected modified likelihood ratio test, the modified likelihood ratio test, the profile likelihood ratio test and the residual likelihood ratio test. Also simulation study is performed to compare these tests...|$|R
5000|$|Altering in an {{unauthorized}} way. This requires little technical expertise and is {{common form of}} theft by employees altering the <b>data</b> <b>before</b> <b>entry</b> or entering false data, or by entering unauthorized instructions or using unauthorized processes; ...|$|R
40|$|Lipase-catalyzed {{production}} of triethanolamine-based esterquat by esterification of oleic acid (OA) with triethanolamine (TEA) in n-hexane {{was performed in}} 2 [*]L stirred-tank reactor. A set of experiments was designed by central composite design to process modeling and statistically evaluate the findings. Five independent process variables, including enzyme amount, reaction time, reaction temperature, substrates molar ratio of OA to TEA, and agitation speed, were studied under the given conditions designed by Design Expert software. Experimental data were examined for normality <b>test</b> <b>before</b> <b>data</b> processing stage and skewness and kurtosis indices were determined. The mathematical model developed {{was found to be}} adequate and statistically accurate to predict the optimum conversion of product. Response surface methodology with central composite design gave the best performance in this study, and the methodology as a whole has been proven to be adequate for the design and optimization of the enzymatic process...|$|R
30|$|Data was {{collected}} using Personal Digital Assistants (PDAs) Hewlett Packard IPAQ model no. 110. Once {{the questionnaire was}} adapted it was <b>tested</b> <b>before</b> the actual <b>data</b> collection process, finally the questionnaire was uploaded into PDAs {{to be used by}} the data collectors as e-questionnaire.|$|R
40|$|This project {{discusses}} the design, reconstruction, {{and evaluation of}} a dynamometer. A new test stand for the Quarter Scale team would improve the testing of the engines and the Continuously Variable Transmission (CVT). All the readings will be done electronically {{in order to provide}} the most accurate data collection. The dynamometer will be a closed hydraulic system for ease of use. This system will be able to test all of Quarter Scale engines. The data collected will be used to determine which engines are in the best condition, and which may need to be fixed. With the new design, the Quarter Scale team will also be able to experiment and determine the best springs to use in conjunction with the CVT. Testing demonstrated that the dynamometer needs more long term <b>testing</b> <b>before</b> <b>data</b> can be used in the confines of a Quarter Scale report. Initial testing shows that the dynamometer does work on the bases of providing a good test stand for future of Quarter Scale...|$|R
40|$|Copyright © 2013 Hamid Reza Fard Masoumi et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Lipase-catalyzed production of triethanolamine-based esterquat by esterification of oleic acid (OA) with triethanolamine (TEA) in n-hexane was performed in 2 L stirred-tank reactor. A set of experiments was designed by central composite design to process modeling and statistically evaluate the findings. Five independent process variables, including enzyme amount, reaction time, reaction temperature, substrates molar ratio of OA to TEA, and agitation speed, were studied under the given conditions designed by Design Expert software. Experimental data were examined for normality <b>test</b> <b>before</b> <b>data</b> processing stage and skewness and kurtosis indices were determined. Themathematical model developed {{was found to be}} adequate and statistically accurate to predict the optimum conversion of product. Response surface methodology with central composite design gave the best performance in this study, and themethodology as awhole has been proven to be adequate for the design and optimization of the enzymatic process. 1...|$|R
40|$|The UltraSTAR {{structured}} <b>data</b> <b>entry</b> {{system is}} now in routine use for reporting ultrasound studies at Brigham and Women's Hospital, having been used for 3722 reports in its first ten months of service. Reports entered through GUI-based forms are uploaded via HL 7 to a radiology information system and distributed through a hospital network. UltraSTAR introduces collaborative reporting, in which nonmedical and medical staff collaborate to produce a single report for each patient visit. Performance of UltraSTAR was measured as user satisfaction, <b>data</b> <b>entry</b> time, report completeness, free text annotation rate, and referring-physician satisfaction with reports. Results show high satisfaction with UltraSTAR among radiologists and acceptance of the system among ultrasound technicians. <b>Data</b> <b>entry</b> times averaged 5. 3 minutes per report. UltraSTAR reports were slightly more complete than comparable narrative reports. Free text annotations were needed in only 25. 2 % of all UltraSTAR reports. Referring physicians were neutral to slightly positive toward UltraSTAR's outline-format reports. UltraSTAR is successful at structured <b>data</b> <b>entry</b> despite somewhat long reporting times. Its success {{can be attributed to}} efficiencies from collaborative reporting and from integration with existing information systems. UltraSTAR shows that the advantages of structured <b>data</b> <b>entry</b> can outweigh its difficulties even <b>before</b> problems of <b>data</b> <b>entry</b> time and concept representation are solved...|$|R
40|$|Computerized {{nutrient}} databases are {{an important}} tool for dietitians and researchers alike. They are used to assess nutrient intake, plan specialized diets, analyse recipes and menus, and develop research aids. In December 2001, the prototype of a new fatty acid database (1) (FADB) was released at the NSA conference in Canberra. As with any food composition table or nutrient database there are limitations and potential sources of error that could contribute to inaccuracies in the final data (2). This paper reports on the limitations and potential sources of error the researcher or clinician {{should be aware of}} before using this new database. In a previous randomised cross-over study examining the effect of soy product consumption compared with dairy consumption, beneficial effects on cardiovascular outcomes was seen. The diet histories of these subjects provided an excellent starting point for examining the intake of individual fatty acids. The diet histories of 23 subjects previously analysed on FoodworksTM (Xyris Software, Brisbane, version 2. 10), using the Australian Nutrient Database NUTTAB and AUSNUT, were re-examined using the Australia fatty acid database attached to FoodworksTM (Xyris Software, Brisbane, version 3. 0), with regards to the polyunsaturated fatty acid (PUFA) content. Conversion of volume amounts to gram amounts of each food item was completed <b>before</b> <b>data</b> <b>entry</b> was started. Substitution of some food items was required and was based on fat content, and ingredient similarity. Meat items were matched as closely as possible with regards to the area of cut and fat content. Comparison between the original FoodworksTM data and the FADB showed significant differences for total PUFA intake i...|$|R
3000|$|... 3  =  0  MPa) has {{a shorter}} elastic {{deformation}} stage but a longer plastic deformation stage before the peak point, and the predicted curve {{from the model}} is relatively in poor agreement with the <b>test</b> <b>data</b> <b>before</b> the peak point. Figure  7 b indicates that the tested axial stress–strain curve of sample # 2 (σ [...]...|$|R
40|$|The ATLAS Detector at the Large Hadron Collider at CERN is {{equipped}} with a superconducting magnet system that consists of a Barrel Toroid, two End-Cap Toroids and a Central Solenoid. The four magnets generate the magnetic field for the muon- and inner tracking detectors, respectively. After 10 years of construction in industry, integration and on-surface tests at CERN, the magnets are now in the underground cavern where they undergo the ultimate <b>test</b> <b>before</b> <b>data</b> taking in the detector can start during the course of next year. The system with outer dimensions of 25 m length and 22 m diameter is based on using conduction cooled aluminum stabilized NbTi conductors operating at 4. 6 K and 20. 5 kA maximum coil current with peak magnetic fields in the windings of 4. 1 T and a system stored magnetic energy of 1. 6 GJ. The Barrel Toroid and Central Solenoid were already successfully charged after installation to full current in autumn 2006. This year the system is completed with two End Cap Toroids. The ultimate test of all toroids by which maximum mechanical and thermal loads are exerted is being prepared and will be performed late 2007 where after normal operation will commence. The status of the project in its nearly completed state as well as the first experience with testing and operation in the underground cavern are reported...|$|R
40|$|Numerical {{modelling}} and simulations {{are needed}} to develop and test specific analysis methods by providing <b>test</b> <b>data</b> <b>before</b> BIRDY would be launched. This document describes the "satellite data simulator" which is a multi-sensor, multi-spectral satellite simulator produced especially for the BIRDY mission {{which could be used}} as well to analyse data from other satellite missions providing energetic particles data in the Solar system...|$|R
40|$|Because {{cognitive}} linguists {{assert that}} primary and complex conceptual metaphors are theoretical constructs with a plausible yet uncertain psychological reality, we investigated if and how EEG coherence would differ {{between these two}} categories during comprehension. We formalized an algorithm of conceptual metaphor processing {{for the purpose of}} hypothesis <b>testing,</b> <b>before</b> collecting EEG <b>data</b> from 50 normal adults, looking for condition-specific EEG coherence patterns. Results confirm the psychological reality of these two metaphor categories. However, they also support alternative conceptions regarding the algorithm and nature of complex metaphors, developed and discussed in this article...|$|R
40|$|Abstract- This paper {{explains}} {{the importance of}} Smoke review on test data. It helps the reviewer to health check the <b>test</b> <b>data</b> <b>before</b> the tester to use test data in application testing. Smoke review of test data determines the stability of test data and it provides criteria for accepting the test data for detailed testing. So reviewer will give the go or not go to testing on the bases of Smoke review of Test data...|$|R
40|$|Backgrounds : The {{prevalence}} and incidence of {{congenital heart disease}} (CHD) are quite high and tend both of them keep increasing over years yet public knowledge about CHD is still quite low. One {{of the factors that}} influence formation of people’s knowledge is health services’ information. Objective: This study aims to evaluate the difference of knowledge about CHD between parents of patients in education hospital and non-education hospital. Methods: This was an observational study with cross sectional approach. Subjects for this study were the parents of pediatric patients who came to pediatric polyclinic RSUP Dr. Kariadi and RS Panti Wilasa Dr Cipto from March until June 2012. Data were collected by filling a questionnaire which had been <b>tested</b> <b>before.</b> The <b>data</b> were analyzed by using Chi square test. Results: This study recruited 94 respondents; 47 from Dr. Kariadi Hospital and 47 from Panti Wilasa Dr Cipto Hospital. Knowledge’s level of patient parents in both RSUP Dr. Kariadi dan RS Panti Wilasa are low; 68, 1...|$|R
40|$|This study aims {{to analyze}} the affect of firm size, the quality of {{accounting}} public firm, profitibility, leverage, and audit opinion to the audit delay on manufacturing companies listed in Indonesia Stock Exchange year 2012 - 2014. The population in this study inlude manufacturing companies listed in Indonesia Stock Exchange year 2012 - 2014. The sampling technique {{used in this study}} is purposive sampling and 183 samples obtained. This study used quantitative method using secondary data obtained from direct access to www. idx. co. id. The requirement analysis test which cover normality test, multicollinearity test, heteroscedasticity test and autocorrelation <b>test</b> conducted <b>before</b> <b>data</b> analysis. Method of data analysis used in this research is multiple linear regression. The result of this study concludes that there are four of five factor s that may affect audit delay, which are firm size, profitibility, leverage, and audit opinion, whereas the quality of accounting public firm do not affect the audit delay...|$|R
40|$|Hyperspectral {{infrared}} instruments {{have many}} potential applications in remote-sensing CubeSat constellations. This paper describes {{the design of}} a 6 U-compatible infrared Fourier Transform spectrometer (FTS) instrument that is designed for meteorological and remote sensing applications, and describes the results of an environmental test campaign designed to increase the technology readiness level (TRL) of the instrument to TRL 6. It includes a description of the major subsystems of the instrument, as well as performance <b>test</b> <b>data</b> <b>before</b> and after exposure. Potential applications for this instrument are discussed...|$|R
3000|$|... 3  =  5  MPa) {{contains}} closure {{stage for}} voids and cracks, while the mechanical properties of rock {{in this stage}} are not considered in the model, so the predicted curve from the model is also relatively in poor agreement with the <b>test</b> <b>data</b> <b>before</b> the peak point. However, the comparison of predicted curves from the model and test data illustrates that, from the overall perspective, the new model proposed in this paper can preferably simulate the strain softening property of lime mudstone and the axial stress–strain response in rock failure process.|$|R
40|$|Signal {{smoothing}} {{is applied}} to the clusters of flash ADC’s of the beam <b>test</b> <b>data</b> <b>before</b> the cuts on the signal pulse determination, hit point making, and track finding were applied to study the performance of the prototype of the Central Drift Chamber(CDC) for JLC (Joint Linear Collider). This study investigates the effects of signal smoothing to the basic chamber performance of the chamber including the wire efficiency, spatial resolution, and two-track separation capability. The computer analysis of the data was done using the CDC analysis software which uses the JSF (JLC Study Framework). I...|$|R
40|$|An {{effective}} {{data collection}} methodology for evaluating software development methodologies {{was applied to}} four different software development projects. Goals of the data collection included characterizing changes and errors, characterizing projects and programmers, identifying effective error detection and correction techniques, and investigating ripple effects. The data collected consisted of changes (including error corrections) made to the software after code was written and baselined, but <b>before</b> <b>testing</b> began. <b>Data</b> collection and validation were concurrent with software development. Changes reported were verified by interviews with programmers...|$|R
5000|$|Quasi-experiments {{are also}} {{effective}} because {{they use the}} [...] "pre-post testing". This means that there are <b>tests</b> done <b>before</b> any <b>data</b> are collected {{to see if there}} are any person confounds or if any participants have certain tendencies. Then the actual experiment is done with post test results recorded. This data can be compared as part of the study or the pre-test data can be included in an explanation for the actual experimental data. Quasi experiments have independent variables that already exist such as age, gender, eye color. These variables can either be continuous (age) or they can be categorical (gender). In short, naturally occurring variables are measured within quasi experiments.|$|R
30|$|<b>Data</b> <b>entry</b> and {{analysis}} were done using SPSS and STATA respectively. <b>Before</b> the analysis <b>data</b> editing {{was done to}} get rid of non response items. Data analysis was done using tobit regression model.|$|R
5000|$|<b>Data</b> <b>entry</b> and {{conversion}} services include: academic <b>data</b> <b>entry,</b> database {{content and}} support, survey digitization, and direct marketing support.|$|R
40|$|Routine {{capture of}} patient data for a {{computer-based}} patient record system remains {{a subject of}} study. Time constraints that require fast <b>data</b> <b>entry</b> and maximal expression power {{are in favor of}} free text <b>data</b> <b>entry.</b> However, using patient data directly for decision support systems, for quality assessment, etc. requires structured <b>data</b> <b>entry,</b> which appears to be more tedious and time consuming. In this paper, a prototype clinical <b>data</b> <b>entry</b> application is described that combines free text and structured <b>data</b> <b>entry</b> in one single application and allows clinicians to smoothly switch between these two different input styles. A knowledge base involving a semantic network of clinical <b>data</b> <b>entry</b> terms and their properties and relationships is used by this application to support structured <b>data</b> <b>entry.</b> From structured <b>data,</b> sentences are generated and shown in a text processor together with the free text. This presentation metaphor allows for easy integrated presentation of structured data and free text...|$|R
40|$|Purpose of in this {{research}} is for analyze price influence, brand, image, and relationship effort to evaluation of consumer at brand motorbike Yamaha in Weleri Kendal. Population which will be checked is all public using motor Yamaha in region Weleri Kendal, while sample applied 100 publics motor Yamaha in region Weleri Kendal during the observation. Data analytical method applied in {{this research}} is multiple regression, test t, F test, and determination <b>test.</b> <b>Before</b> done <b>data</b> analysis beforehand is done examination of classic assumption. The conclusion obtained is (1) the regression equation of a line explain that positive change at fourth of independent variable (price, brand, image, and relationship effort) will increase variable dependent evaluation of consumer), (2) variable brand and image has influence significant partially to evaluation of consumer at motor Yamaha in town Weleri Kendal, (3) variable price and relationship effort has no influence significant partially to evaluation of consumer at motor Yamaha in town Weleri Kendal, (4) there was price as variable influence, brand, image, and relationship effort in stimulant to evaluation of consumer at motor Yamaha in town Weleri Kendal, and (5) various change of evaluation of consumer at motor Yamaha in Weleri Kendal is influenced the change of price variable, brand, image, and relationship effort equal to 44, 5...|$|R
50|$|While {{this method}} of quality control clearly is not proof against {{systematic}} errors or operator misread entries from a source document, it is very useful in catching and correcting random miskeyed strokes which occur even with experienced <b>data</b> <b>entry</b> operators. However, {{it proved to be}} a fatally tragic flaw in the Therac 25 incident. This method has survived the keypunch and is available in some currently available <b>data</b> <b>entry</b> programs (e.g. PSPP/SPSS <b>Data</b> <b>Entry).</b> At least one study suggests that single-pass <b>data</b> <b>entry</b> with range checks and skip rules approaches the reliability of two-pass data entry; however, it is desirable to implement both systems in a <b>data</b> <b>entry</b> application.|$|R
5000|$|IBM 3740 <b>Data</b> <b>Entry</b> System was a <b>data</b> <b>entry</b> {{system that}} was {{announced}} by IBM in 1973. It recorded data on a Diskette, a new recording medium from IBM, for fast, flexible, efficient <b>data</b> <b>entry</b> to either high-production, centralized operations or to decentralized, remote operations. The [...] "Diskette" [...] was more commonly known as an 8-inch floppy disk, ...|$|R
40|$|Self-service {{technologies}} have been gaining increasing importance {{in public and}} private organizations over recent years. One of the predominant responsibilities of the customers in the context of self-service is to enter their data on their own. Self-service for <b>data</b> <b>entry</b> can help organizations to further enhance the efficiency of their processes and to make customer experience smoother. However, as self-service for <b>data</b> <b>entry</b> is intended to be handled without intensive employee assistance, inexp erienced <b>data</b> <b>entry</b> can lead to data quality problems. Although academic research has explored several effects of self-service technologies, there is still a lack of research investigating the effect of self-service <b>data</b> <b>entry</b> on <b>data</b> quality. Thus, in an in-depth case study in cooperation with the German Federal Employment Agency we analyzed how customer self-service for <b>data</b> <b>entry</b> affects <b>data</b> quality and found that assisted self-service <b>data</b> <b>entry</b> leads to highest data qualit...|$|R
5000|$|<b>Data</b> <b>entry</b> by geologists in {{the field}} may take less total time than {{subsequent}} <b>data</b> <b>entry</b> in the office, potentially reducing the overall time needed to complete a project.|$|R
40|$|Thesis (M. Consumer Science) [...] North-West University, Potchefstroom Campus, 2006. Background and motivation: Consumers {{are faced}} with choices of textile {{products}} on the market every day, and it should be determined whether labels, being a major source of information at the point of sale, assist consumers in their decision making. Regarding the concept of labelling, the nature of a typical textile label was defined and the information to be included in a label was identified as the size of the product, care instructions, fibre content, country of origin, name of the manufacturer and the brand name. It was discovered from literature that consumer behaviour is influenced by the external factors and determined by the internal factors such as personality and self-concept, attitudes, perceptions, motivation and involvement, memory and learning and emotions. The aim {{of the study was to}} explore consumers' awareness and perceptions of textile product labelling and to determine the extent to which the consumers use the information on the textile product label when making decisions on or purchasing textile goods. Methodology: As very little is known about this phenomenon regarding the South African consumer, a qualitative, explorative, descriptive research strategy was followed. Focus groups were used for data collection and participants were selected by purposive sampling so that only the knowledgeable consumer with reference to the textile label was included. The research was made trustworthy by using the purposive sampling, and conducting a pilot <b>test</b> <b>before</b> <b>data</b> collection. In addition, peers and study leaders monitored all the processes of the research. Data were analysed using a qualitative thematic research analysis method. Identified themes and concepts were then discussed under the pre-determined categories of awareness, perceptions and the influence each of these themes has on the decision-making process when purchasing textile products. Results and discussions: The results revealed that the consumers were very aware of the existence of a label, they were even able to define it, and they could identify it in a number of ways. Consumers were also aware of the type of information provided by the textile product labels. The perceptions that the consumers have about the information found on the label and the nature of the label are discussed at length in the text. It was discovered that textile labels do influence the decision-making process of the consumers when purchasing textile products and the information on size is the most important to consumers. Master...|$|R
30|$|On {{the other}} hand, the {{analyses}} of wet joints {{showed that it}} was hard for the equation to entirely approach the test data simply by adjusting the value of coefficient b because the trends of overestimation and underestimation coexisted when compared to original JSCE equation with b[*]=[*] 0.4. As was mentioned previously, b[*]=[*] 0.4 is not specified in JSCE specifications (2010) but was proposed in some studies (Shin 2016; Watanabe et al. 2007), in which Watanabe et al. (2007) applied UHPC filler with specified compressive strength of 180  MPa. Therefore, it would be desirable to further investigate the validity of b[*]=[*] 0.4 for the wet joint with more accumulated <b>test</b> <b>data</b> <b>before</b> discussing the revision of the coefficient b for UHPC filler.|$|R
5000|$|SuperMICAR automates the MICAR <b>data</b> <b>entry</b> process. This {{program is}} {{designed}} as an enhancement of the earlier PC-MICAR <b>Data</b> <b>Entry</b> program. Super-MICAR is designed to automatically encode cause-of-death data into numeric entity reference numbers.|$|R
40|$|The {{objective}} {{was to determine the}} conditions under which Automatic Speech Recognition (ASR) is an efficient choice for <b>data</b> <b>entry.</b> In particular the focus was on <b>data</b> <b>entry</b> tasks that are part of constructing military messages. The ADF Formatted Messaging System utilises a structured formatting system to constrain the semantics of a message but also includes a field for unlimited and unstructured text. Hence the <b>data</b> <b>entry</b> tasks involved range from form-filling to free dictation of short phrases. In the experiments, ASR and manual input modes are compared for three <b>data</b> <b>entry</b> tasks: textual phrase entry, selection from a list, and numerical <b>data</b> <b>entry.</b> To effect fair comparisons, the tasks minimised the transaction cycle for each input mode and data type and the main comparisons use only times from correct <b>data</b> <b>entry.</b> The results indicate that for inputting short phrases ASR only competes if the typist's speed is below 45 wpm. For selecting an item from a list, ASR offered an advantage only if the list length was greater than 15 items. For entering numerical data, ASR offered no advantage over keypad or mouse. The general conclusion for formatted <b>data</b> <b>entry</b> is that a keyboard/mouse interface designed to match the data to be entered will be more time efficient than any equivalent ASR interface...|$|R
3000|$|... estimations {{made by the}} {{proposed}} MLP-based method for the two platforms (Ekofisk and FINO 1) considered in the study. They are compared with the measurements made by in-situ sensors (buoy). To validate {{the proposed}} method, these results are compared with the ones obtained by the standard method. The comparisons {{are made in the}} designing and testing stages. The aim of comparing the results obtained in both stages is to realize whether the performances obtained during the designing stage are maintained for a data set never processed <b>before</b> (<b>testing</b> <b>data</b> set) or not. In other words, we want to know, once the MLP-based estimator is designed, how the proposed method works from a point of view of performance and time of designing/execution.|$|R
40|$|Introduction: Use of {{electronic}} health record (EHR) systems can place a considerable <b>data</b> <b>entry</b> burden upon {{the emergency department}} (ED) physician. Voice recognition <b>data</b> <b>entry</b> has been proposed as one mechanism to mitigate some of this burden; however, no reports are available specifically comparing emergency physician (EP) time use or number of interruptions between typed and voice recognition data entry-based EHRs. We designed this study to compare physician time use and interruptions between an EHR system using typed <b>data</b> <b>entry</b> versus an EHR with voice recognition. Methods: We collected prospective observational data at 2 academic teaching hospital EDs, one using an EHR with typed <b>data</b> <b>entry</b> {{and the other with}} voice recognition capabilities. Independent raters observed EP activities during regular shifts. Tasks each physician performed were noted and logged in 30 second intervals. We compared time allocated to charting, direct patient care, and change in tasks leading to interruptions between sites. Results: We logged 4, 140 minutes of observation for this study. We detected no statistically significant differences in the time spent by EPs charting (29. 4 % typed; 27. 5 % voice) or the time allocated to direct patient care (30. 7 %; 30. 8 %). Significantly more interruptions per hour were seen with typed <b>data</b> <b>entry</b> versus voice recognition <b>data</b> <b>entry</b> (5. 33 vs. 3. 47; p= 0. 0165). Conclusion: The use of a voice recognition <b>data</b> <b>entry</b> system versus typed <b>data</b> <b>entry</b> did not appear to alter the amount of time physicians spend charting or performing direct patient care in an ED setting. However, we did observe a lower number of workflow interruptions with the voice recognition <b>data</b> <b>entry</b> EHR. Additional research is needed to further evaluate the <b>data</b> <b>entry</b> burden in the ED and examine alternative mechanisms for chart entry as EHR systems continue to evolve. [West J Emerg Med. 2014; 15 (4) : 541 - 547. ]...|$|R
3000|$|<b>Data</b> <b>Entry</b> Methods: The input methods {{available}} for mobile devices {{are different from}} those for desktop computers and require a certain level of proficiency. This problem increases the likelihood of erroneous input and decreases the rate of <b>data</b> <b>entry.</b>|$|R
3000|$|... [*]“… In {{case of a}} <b>data</b> <b>entry</b> zone, a Web page could {{contain more}} than one <b>data</b> <b>entry</b> zone with {{different}} purposes. The technique could allow the verification of all these repeated zones in a particular way.” - Inspector 4.|$|R
