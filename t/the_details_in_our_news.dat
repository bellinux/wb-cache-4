0|10000|Public
3000|$|This {{result can}} be easily {{obtained}} by the standard estimation methods. We omit the proof here and put <b>the</b> <b>details</b> <b>in</b> <b>our</b> technical report (Proposition  3.2) on the website: [URL] [...]...|$|R
60|$|We were obliged {{indeed to}} give up the idea of {{following}} <b>the</b> plan <b>detailed</b> <b>in</b> <b>our</b> book, because we hadn't the sum upon which the furnishing of a small house was therein based.|$|R
6000|$|... "If {{the public}} would only attend," [...] {{observed}} Mr Bright, in commenting on these facts, [...] "to the regulations laid down for their guidance by <b>the</b> Post-Office--as <b>detailed</b> <b>in</b> <b>our</b> Directories and Postal Guides--such errors would seldom occur, for I believe that things of this sort {{are the result of}} ignorance rather than dishonesty." ...|$|R
40|$|We {{present an}} {{efficient}} algorithm for progressive transmission of cutting plane data extracted from large scale Computational Field Simulation (CFS) datasets. Since cutting planes {{are the most}} frequently-used method for examination of 3 D simulation results, efficient compression of both the geometry/topography and the associated field data is important for good visualization performance, especially when the simulation is running on a geographically remote server or the simulation results are stored in a remote repository. Progressive compression is ideal for exploratory visualization since the data can be presented naturally starting with a coarse view and progressing down to <b>the</b> <b>detail.</b> <b>In</b> <b>our</b> algorithm, each cutting plane, which is a triangle mesh, is reduced at the server to triangle strips which contain each contour location. On the local visualization machine (the client), the original surface is reconstructed by filling in the area between the triangle strips using a triangulati [...] ...|$|R
5000|$|In September 2016, the Open Philanthropy Project {{granted the}} panel a $1.3 million grant {{in support of}} the panel's {{influential}} leadership role in the evaluation of the nation’s biodefense systems. Tom Ridge said, [...] "It is troubling that we still do not have a comprehensive approach to preparing for and responding to biological events. That is why this grant from Open Philanthropy is so critical. It will allow us to push forward <b>the</b> recommendations <b>detailed</b> <b>in</b> <b>our</b> National Blueprint and seek to put them into action." ...|$|R
40|$|Figure 1 : A video {{sequence}} (a) is interactively decomposed into temporally consistent components for reflectance (b, top) and illumination (b, bottom). Now, editing the textures in the reflectance image {{does not affect}} the illumination (c) : changes to the brick walls, the roof tiles, and the pathway leading up to the building all maintain the complex illumination of the light through the trees. We encourage readers to zoom into this figure to see <b>the</b> <b>details</b> <b>in</b> <b>our</b> results, and refer them to the accompanying video to see the temporally consistent nature of our decomposition. Separating a photograph into its reflectance and illumination intrinsic images is a fundamentally ambiguous problem, and state-of-the-art algorithms combine sophisticated reflectance and illumination priors with user annotations to create plausible results. However, these algorithms cannot be easily extended to videos for two reasons: first, naı̈vely applying algorithms designed for single images to videos produce results that are temporally incoherent; second, effectively specifying user annotations for a video requires interactive feed-back, and current approaches are orders of magnitudes too slow to support this. We introduce a fast and temporally consistent al...|$|R
40|$|We {{present a}} {{discussion}} of and results from the ESO Remote Galaxy Survey (ERGS), a spectroscopic survey of Lyman-break galaxies with z ~ 5 and above. The survey directly explores the properties of these early star-forming galaxies, increasing <b>the</b> observational <b>detail</b> <b>in</b> <b>our</b> picture of early galaxy evolution. The survey provides a sample of galaxies ideally matched in spatial distribution to the capabilities of current and imminently available instrumentation. We discuss {{the results of the}} first follow-on studies of the sample in the mm/sub-mm that signpost the potential of these facilities for exploring early galaxy evolution...|$|R
30|$|The {{microstructure}} and morphology of the nanostructures {{were characterized}} with a high-resolution {{scanning electron microscope}} (Ultra Plus, Zeiss, Oberkochen, Germany). X-ray diffraction (X'Pert Pro system, PANalytical, Almelo, The Netherlands) data was obtained in grazing incident geometry with fixed angles of 1.5 ° and 0.05 ° step using monochromatic Cu Kα radiation ((λ[*]=[*] 1.5418 Å)). The process steps for preparing <b>the</b> nanostructures were <b>detailed</b> <b>in</b> <b>our</b> previous paper [32] and are described briefly below.|$|R
40|$|This work {{presents}} {{some space}} decomposition algorithms for a convex minimization problem. The algorithms has linear {{rate of convergence}} {{and the rate of}} convergence depends only on four constants. The space decomposition could be a multigrid or domain decomposition method. We explain <b>the</b> <b>detailed</b> procedure <b>in</b> implementing <b>our</b> algorithms for a two-level overlapping domain decomposition method and estimate the needed constants. Numerical tests are reported for linear as well as nonlinear elliptic problems...|$|R
40|$|This report <b>details</b> <b>the</b> changes <b>in</b> <b>our</b> {{transportation}} {{systems that are}} occurring {{and will continue to}} evolve with more advanced car sharing services as ZipCar and Car 2 Go and with transportation innovations which fill niches such as Transportation Network Companies (TNC). We will focus on the Goliath in TNC: Uber. Uber’s technology strategies will be discussed. This report also analyzes the effect on society, employees, and the public of those three services...|$|R
30|$|It is {{worthwhile}} mentioning that there exist sites {{much larger than}} those <b>in</b> <b>our</b> sample, such as, for example, The Pirate Bay. Although using data from one of such sites is desirable, obtaining such data with <b>the</b> level of <b>detail</b> <b>in</b> <b>our</b> traces is arguably infeasible: {{there are millions of}} users and no accessible centralized registry for their state. Furthermore, one should note {{that the vast majority of}} BitTorrent sites with large user bases are not nearly as large as The Pirate Bay [57]. <b>In</b> this perspective, <b>our</b> traces represent the typical large BitTorrent sites [57].|$|R
40|$|This paper {{estimates}} {{the impact of}} road improvements on firm employment and productivity using plant level longitudinal data for Britain. Exposure to transport improvements is measured by changes in employment accessibility along the road network. These changes are constructed using data on employment for small geographical units, <b>details</b> of <b>the</b> main road network and of road construction schemes carried out between 1998 and 2007. We deal with the central problem of endogenous scheme placement by using changes due to new road links and exploiting <b>the</b> spatial <b>detail</b> <b>in</b> <b>our</b> data to focus on accessibility changes close to new schemes. We find substantial effects on employment and numbers of plants for small-scale geographical areas (electoral wards), but no employment response at plant level. This suggests that road construction affects firm entry and exit, but not the employment of existing firms. We also find effects on labour productivity and wages at the firm level, although these results are less robust...|$|R
40|$|This paper {{deals with}} the basic {{approximation}} properties of the h-p version of the boundary element method (BEM) in IR 3. We extend the results on the exponential convergence of the h-p version of the boundary element method on geometric meshes from problems in polygonal domains to problems in polyhedral domains. In 2 D elliptic boundary value problems the solutions have only corner singularities whereas in 3 D problems they contain additional edge and corner-edge singularities. The solutions of the corresponding boundary integral equations inherit those singularities. <b>The</b> <b>detailed</b> investigations <b>in</b> <b>our</b> analysis {{take care of the}} various types of those singularities. While edge singularities can be analyzed using standard one-dimensional approximation results the corner-edge singularities demand a new analysis. Subject Classifications: AMS(MOS) 65 N 38, 65 R 20, 45 L 10 Key Words: boundary element methods, h-p version, exponential rate of convergence 1 Introduction The solutions of three-di [...] ...|$|R
40|$|We study {{dielectric}} breakdown in a semi-classical bond percolation model for nonlinear composite materials introduced by {{us and the}} related breakdown exponent near the percolation threshold in two dimensions. The breakdown exponent after doing finite size scaling analysis {{is found to be}} t_B ≃ 1. 42. We discuss <b>in</b> <b>detail</b> <b>the</b> differences <b>in</b> <b>our</b> model from the traditional models for {{dielectric breakdown}} and argue that our result seems to be different from the standard result of 4 / 3 obtained in the previous models. Comment: 20 pages, LaTex file (6 postscript figures included...|$|R
3000|$|Polysilicon nanoribbon biosensors were {{fabricated}} {{with different}} channel lengths using <b>the</b> TFT process <b>detailed</b> <b>in</b> <b>our</b> previous work [6] and measured in dry and wet ambient. Electrical characterization was performed using an Agilent B 1500 A I/V-based probe-station (Agilent Technologies Singapore (International) Pte. Ltd., Singapore). The sensors were measured in a Faraday cage enclosure box to minimize interference. For {{the characterization of}} the sensors, electrical contact was made on the TiN electrodes using Cascade micropositioner probes. The backside of the substrate was grounded through the probe-station chuck to prevent biasing of the channel through it. Transmission line measurements (TLMs) were performed on test structures, and values of sheet resistance (R [...]...|$|R
40|$|Newcomers to the DIG System often {{inquire about}} the {{possibility}} of performing Northern blot hybridizations with nonradioactive techniques. With the following examples, we would like to share our protocol for performing highly sensitive Northern blots. This procedure strictly adheres to <b>the</b> standard procedures <b>detailed</b> <b>in</b> <b>our</b> manuals and pack inserts, and there are no special “tricks ” required. As a target, we have used total human skeletal muscle RNA (Clontech). We selected two probes: b-actin and a probe comprising the cDNA of the transcription factor CTF 1, which expresses a low abundant mRNA. We used in vitro transcribed RNAs exclusively as probes because, during the development of the DIG System, w...|$|R
40|$|We {{encountered}} {{a patient with}} an adult Bochdalek hernia discovered asymptomatically. A 77 -year-old Japanese woman visited a local clinic with chief complaints of melena and difficulty in defecation. Based {{on the results of}} <b>the</b> <b>detailed</b> examination <b>in</b> <b>our</b> hospital, she was diagnosed with a rectal gastrointestinal stromal tumor (GIST) with a concurrent asymptomatic adult right-sided Bochdalek hernia. Because the tumor was large, laparoscopic abdominoperineal rectal amputation was performed after systemic imatinib therapy. During the surgery, we found a right diaphragmatic defect more than 13 cm in long dia., through which the right hepatic lobe, colon, and greater omentum had prolapsed into the right thoracic cavity. No visceral adhesions were noted. No hernia sac was observed. Adult Bochdalek hernia is a relatively rare condition, and only three (incidentally discovered) cases of asymptomatic Bochdalek hernia, including the present case, have been reported in Japan. Here we provide a case report for the patient, who was followed-up without hernia surgery, plus a review of the literature...|$|R
40|$|The time {{required}} for decay of a metastable state, through tunneling, is a well settled subject. During this lifetime of the initial state, the particle oscillates within its initial well. By contrast, questions about {{the duration of the}} actual tunneling process, when it finally occurs, have led to widely divergent answers. In recent years, Jonson, Stevens, and we, have, through independent approaches, pointed out that there is an effective barrier traversal velocity obtained by dividing the magnitude of the imaginary momentum, under the barrier, by the particle mass. Here we present a fourth approach to this earlier answer, by considering a time modulated stream incident on the barrier. At low modulation frequencies the transmitted beam reproduces the incident modulation without lag, and without change in modulation depth. As the modulation frequency is increased we eventually depart from this simple behavior, and that is taken as a measure of the transversal time. We also provide some of <b>the</b> <b>details</b> omitted <b>in</b> <b>our</b> earlier analysis...|$|R
40|$|Basic {{problems}} of acousto-optic interaction in terahertz region of electromagnetic spectrum are considered. We obtained experimental {{results that are}} fitted by generalized theoretical model developed with the goals to describe <b>the</b> interaction <b>in</b> <b>detail.</b> <b>Our</b> analysis showed that crystalline germanium {{is one of the}} best materials to observe the acousto-optic interaction in the terahertz range. The carried out study proved that a germanium based acousto-optic device could be used for fast and reliable deflection of monochromatic radiation in the terahertz spectral region. DOI: 10. 12693 /APhysPolA. 127. 4...|$|R
40|$|In this study, {{we explore}} {{nucleation}} and the transition state ensemble of the ribosomal protein S 6 using a Monte Carlo (MC) Go model {{in conjunction with}} restraints from experiment. The results are analyzed {{in the context of}} extensive experimental and evolutionary data. The roles of individual residues in the folding nucleus are identified, and the order of events in the S 6 folding mechanism is explored <b>in</b> <b>detail.</b> Interpretation of <b>our</b> results agrees with, and extends the utility of, experiments that shift φ-values by modulating denaturant concentration and presents strong evidence for the realism of <b>the</b> mechanistic <b>details</b> <b>in</b> <b>our</b> MC Go model and the structural interpretation of experimental φ-values. We also observe plasticity in the contacts of the hydrophobic core that support the specific nucleus. For S 6, which binds to RNA and protein after folding, this plasticity may result from the conformational flexibility required to achieve biological function. These results present a theoretical and conceptual picture that is relevant in understanding the mechanism of nucleation in protein folding...|$|R
40|$|Presented {{here are}} <b>the</b> <b>detailed</b> methods {{employed}} <b>in</b> <b>our</b> laboratory for gene mapping and cytogenetic analyses in human beings, {{in the domestic}} cat, and in other mammalian species. Included in the procedures are: 1) establishment of primary fibroblast and lymphoid cell cultures; 2) heterologous cell fusion for production of rapidly proliferating cell hybrids; 3) cellular transformation of primary fibroblasts using an oncogenic retrovirus; 4) cell synchronization for high-resolution banding of prometaphase chromosomes; 5) chromosome-banding procedures, including G-banding, alkaline G- 11, and Q-banding; and 6) in situ hybridization of radiolabeled molecular clones to metaphase chromosomes for regional gene localization...|$|R
40|$|<b>In</b> {{this paper}} <b>detailed</b> {{calculations}} of <b>the</b> complete O(α_s) corrections to top quark decay widths Γ(t→ q+V) are presented (V=g,γ,Z). Besides describing <b>in</b> <b>detail</b> <b>the</b> calculations <b>in</b> <b>our</b> previous paper (arXiv: 0810. 3889), we {{also include the}} mixing effects of the Flavor-Changing Neutral-Current (FCNC) operators for t→ q+γ and t→ q+Z, which were not considered <b>in</b> <b>our</b> previous paper. The results for t→ q+g {{are the same as}} <b>in</b> <b>our</b> previous paper. But the mixing effects can either be large or small, and increase or decrease the branching ratios for t→ q+γ and t→ q+Z, depending on the values of the anomalous couplings (κ^g,γ,Z_tq/Λ, f^g,γ,Z_tq and h^g,γ,Z_tq). Comment: 21 pages, 12 figure...|$|R
40|$|We {{present a}} new {{calculus}} for first-order theorem proving with equality, ME+Sup, which generalizes both the Superposition calculus and the Model Evolution calculus (with equality) by integrating their inference rules and redundancy criteria in a non-trivial way. The main motivation is {{to combine the}} advantageous features of both [...] -rather complementary [...] -calculi in a single framework. For instance, Model Evolution, as a lifted version of the propositional DPLL procedure, contributes a non-ground splitting rule that effectively permits to split a clause into non variable disjoint subclauses. In the paper we present <b>the</b> calculus <b>in</b> <b>detail.</b> <b>Our</b> main result is its completeness under semantically justified redundancy criteria and simplification rules...|$|R
40|$|In this {{longitudinal}} {{case study}} we have followed a small software product {{company that has}} turned from a waterfall-like process to evolutionary project management (Evo). The most prominent feature of the new process is the close engagement of customers. We have interviewed both internals and customers to investigate the practicalities, costs, gains and prerequisites of such a transition. We have gathered data from {{a period of two}} years covering four consecutive release projects using the new process and analyzed <b>the</b> material <b>in</b> <b>detail.</b> <b>Our</b> findings implicate that close customer engagement does give certain benefits but that it comes with a cost and needs careful attention to management...|$|R
40|$|Abstract. This paper {{presents}} a scalable leader election protocol for large process groups {{with a weak}} membership requirement. The underlying network {{is assumed to be}} unreliable but characterized by probabilistic failure rates of processes and message deliveries. The protocol trades correctness for scale, that is, it provides very good probabilistic guarantees on correct termination {{in the sense of the}} classical specification of the election problem, and of generating a constant number of messages, both independent of group size. After formally specifying the probabilistic properties, we describe <b>the</b> protocol <b>in</b> <b>detail.</b> <b>Our</b> subsequent mathematical analysis provides probabilistic bounds on the complexity of the protocol. Finally, the results of simulation show that the performance of the protocol is satisfactory...|$|R
40|$|Abstract. We {{describe}} {{the implementation of}} a 2 D optical flow algorithm published in the European Conference on Computer Vision (ECCV 2004) by Brox et al. [1] (best paper award) and a qualitative and quantitative evaluation of it for a number of synthetic and real image sequences. Their optical flow method combines three assumptions: a brightness constancy assumption, a gradient constancy assumption and a spatio-temporal smoothness constraint. A numerical scheme based on fixed point iterations is used. Their method uses a coarse-to-fine warping strategy to measure larger optical flow vectors. We have investigated <b>the</b> algorithm <b>in</b> <b>detail</b> and <b>our</b> evaluation of the method demonstrates that it produces very accurate optical flow fields from only 2 input images...|$|R
40|$|Dynamic taint {{analysis}} {{is a well-known}} information flow analysis problem with many possible applications. Taint tracking allows for analysis of application data flow by assigning labels to inputs, and then propagating those labels through data flow. Taint tracking systems traditionally compromise among performance, precision, accuracy, and portability. Performance can be critical, as these systems are typically intended to be deployed with software, and hence must have low overhead. To be deployed in security-conscious settings, taint tracking must also be accurate and precise. Dynamic taint tracking must be portable {{in order to be}} easily deployed and adopted for real world purposes, without requiring recompilation of the operating system or language interpreter, and without requiring access to application source code. We present Phosphor, a dynamic taint tracking system for the Java Virtual Machine (JVM) that simultaneously achieves our goals of performance, accuracy, precision, and portability. Moreover, to our knowledge, it is the first portable general purpose taint tracking system for the JVM. We evaluated Phosphor's performance on two commonly used JVM languages (Java and Scala), on two versions of two commonly used JVMs (Oracle's HotSpot and OpenJDK's IcedTea) and on Android's Dalvik Virtual Machine, finding its performance to be impressive: as low as 3 % (53 % on average), using the DaCapo macro benchmark suite. This artifact contains the code needed to reproduce <b>the</b> experiments <b>detailed</b> <b>in</b> <b>our</b> paper...|$|R
40|$|Pacemaker {{activity}} of the sinoatrial node has been studied extensively in various animal species, but is virtually unexplored in man. As such, it is unknown whether the fast sodium current (I Na) {{plays a role in}} the pacemaker {{activity of}} the human sinoatrial node. Recently, we had the unique opportunity to perform patch-clamp experiments on single pacemaker cells isolated from a human sinoatrial node. In 2 out of the 3 cells measured, we observed large inward currents with characteristics of I Na. Although we were unable to analyze <b>the</b> current <b>in</b> <b>detail,</b> <b>our</b> findings provide strong evidence that I Na is present in human sinoatrial node pacemaker cells, and that this I Na is functionally available at potentials negative to - 60 mV. </p...|$|R
40|$|The current {{production}} of the Hungarian agricultural machinery manufacturing sector, which used to see better days, lags behind the {{production of}} the previous years to a great extent. Our study reveals the initial results of longer research work. We examine how the innovation activity of organisations changed during the period between 2007 and 2009. The conclusions of our paper are based on the examination results of questionnaires and in-depth interviews that were carried out at 40 Hungarian agricultural machinery manufacturing companies. The characteristic features of the companies that were involved in the examination reflect the Hungarian conditions properly. Besides the brand-new or higly developed products and technological (procedure) innovations, novelties in organisation and marketing are also paid attention. Furthermore, some of the indicators of the innovation performance of the companies are also presented. The question of cooperation between the organisations taking part in innovation is referred to, as well. The other results of <b>the</b> research are <b>detailed</b> <b>in</b> <b>our</b> further publications...|$|R
40|$|Abstract We {{present a}} new {{calculus}} for first-order theorem proving with equality, ME+Sup, which generalizes both the Superposition calculus and the Model Evolution calculus (with equality) by integrating their inference rules and redundancy criteria in a non-trivial way. The main motivation is {{to combine the}} advantageous features of these two rather complementary calculi in a single framework. In particular, Model Evolution, as a lifted version of the propositional DPLL procedure, contributes a non-ground splitting rule that effectively permits to split a clause into non variable disjoint subclauses. In the paper we present <b>the</b> calculus <b>in</b> <b>detail.</b> <b>Our</b> main result is its completeness under semantically justified redundancy criteria and simplification rules. We also show how under certain assumptions the model representation computed by a (finite and fair) derivation can be queried in an effective way. ...|$|R
40|$|This {{is a case}} of {{idiopathic}} pulmonary calcification and ossification in a 70 {{year old}} with long-standing diabetes and hypertension. Thirteen years prior to her demise, she was first noticed to have multiple calcific deposits in her lungs on a chest X-ray film. She had no risk factors for soft tissue calcification and ossification. Histology of tissue from autopsy showed intraparenchymal pulmonary calcification and ossification with marrow elements. Idiopathic pulmonary calcification and ossification is rare. At autopsy, she was also found to have had bilateral subarachnoid haemorrhage (SAH), a diagnosis missed during clinical evaluation. We highlight <b>the</b> pertinent <b>details</b> <b>in</b> <b>our</b> patient’s management that could have helped to prevent a missed diagnosis of SAH. Even though SAH occurs most commonly following head trauma, the more familiar medical use of SAH is for non-traumatic SAH occurring following a ruptured cerebral aneurysm. This patient had notable risk factors for cerebral aneurysm formation but an aneurysm was not identified at autopsy. The location of the blood high on the cerebral convexities further suggests a traumatic origin rather than a ruptured aneurysm. Heterotopic calcification and ossification (HO) is known to occur in the setting of severe neurologic disorders such as traumatic brain injury {{but the fact that the}} lung calcification <b>in</b> <b>our</b> patient predated the brain injury by over 10 years makes it unlikely for the HO to have been due to the brain trauma. Other organ pathologies found at autopsy include chromophobe renal cell carcinoma, renal papillary necrosis, lymphocytic thyroiditis, and seborrheic keratosis...|$|R
40|$|Independent {{optimization}} for workload {{and power}} management, and active cooling control {{have been studied}} extensively to improve data center energy efficiency. Recently, proposals have started to advocate unified workload, power, and cooling management for further energy savings. In this paper, we study this problem with the objectives of both saving energy and capping power. We present <b>the</b> <b>detailed</b> models derived <b>in</b> <b>our</b> previous work from experiments on an blade enclosure system that can be representative of a data center, discuss the optimization opportunities for coordinated power and cooling management, and the challenges for controller design. We then propose a few design principles and examples for unified workload management, power minimization, and power capping. Our simulation-based evaluation shows that the controllers can cap the total power consumption while maintaining the thermal conditions and improve the overall energy efficiency. We argue that the same opportunities, challenges, and designs are also generally applicable to data center level management. 1...|$|R
40|$|We {{present a}} review of {{elemental}} abundances in the Milky Way stellar disk, bulge, and halo {{with a focus on}} data derived from high-resolution stellar spectra. These data are fundamental in disentangling the formation history and subsequent evolution of the Milky Way. Information from such data is still limited and confined to narrowly defined stellar samples. The astrometric Gala satellite will soon be launched by the European Space Agency. Its final data set will revolutionize information on the motions of a billion stars in the Milky Way. This will be complemented by several ground-based observational campaigns, in particular spectroscopic follow-up to study elemental abundances <b>in</b> <b>the</b> stars <b>in</b> <b>detail.</b> <b>Our</b> review shows <b>the</b> very rich and intriguing picture built from rather small and local samples. The Gaia data deserve to be complemented by data of the same high quality that have been collected for the solar neighborhood. (C) 2013 Elsevier B. V. All rights reserved...|$|R
40|$|In {{this article}} we use a {{combination}} of neural networks with other techniques for the analysis of orthophotos. Our goal is to obtain results that can serve as a useful groundwork for interactive exploration of <b>the</b> terrain <b>in</b> <b>detail.</b> <b>In</b> <b>our</b> approach we split an aerial photo into a regular grid of segments and for each segment we detect a set of features. These features depict the segment from the viewpoint of a general image analysis (color, tint, etc.) {{as well as from the}} viewpoint of the shapes in the segment. We perform clustering based on the Formal Concept Analysis (FCA) and Non-negative Matrix Factorization (NMF) methods and project the results using effective visualization techniques back to the aerial photo. The FCA as a tool allows users to be involved in the exploration of particular clusters by navigation in the space of clusters. In {{this article we}} also present two of our own computer systems that support the process of the validation of extracted features using a neural network and also the process of navigation in clusters. Despite the fact that <b>in</b> <b>our</b> approach we use only general properties of images, the results of our experiments demonstrate the usefulness of our approach and the potential for further development. Web of Science 22212110...|$|R
40|$|We {{propose a}} new {{learning}} method for heterogeneous domain adaptation (HDA), {{in which the}} data from the source domain and the target domain are represented by heterogeneous features with different dimensions. Using two different projection matrices, we first transform the data from two domains into a common subspace in order to measure the similarity between the data from two domains. We then propose two new feature mapping functions to augment the transformed data with their original features and zeros. The existing learning methods (e. g., SVM and SVR) can be readily incorporated with our newly proposed augmented feature representations to effectively utilize the data from both domains for HDA. Using the hinge loss function in SVM as an example, we introduce <b>the</b> <b>detailed</b> objective function <b>in</b> <b>our</b> method called Heterogeneous Feature Augmentation (HFA) for a linear case and also describe its kernelization in order to efficiently cope with the data with very high dimensions. Moreover, we also develop an alternating optimization algorithm to effectively solve the nontrivial optimization problem <b>in</b> <b>our</b> HFA method. Comprehensive experiments on two benchmark datasets clearly demonstrate that HFA outperforms the existing HDA methods. 1...|$|R
40|$|Using a {{national}} panel of housing units, this paper documents {{that the rate}} of nominal rigidity in housing rents is high in Turkey between 2008 and 2011. We find that, on average, 31. 5 percent of the rents did not change from year to year in nominal terms. We then ask if the incidence of nominal rigidity depends on the turnover status of the housing unit. We show that 35. 4 percent of the nonturnover units had rigid rents, while for only 17. 1 percent of the turnover units rents did not change. We also present evidence that grid pricing is associated with {{more than half of the}} nominal rigidity in housing rents <b>in</b> <b>our</b> sample. The household- and individual-level determinants of the nominal rigidity in rents and turnover status are also investigated using <b>the</b> micro-level <b>details</b> available <b>in</b> <b>our</b> dataset. We document that, relative to the low-income tenants, high-income tenants are less likely to have rigid rents and they are also less likely to change units frequently. This finding suggests that search and moving costs impose frictions that amplify the opportunity costs of high-income tenants; thus, they are more likely to agree on reasonable rent increases for the purpose of saving time and reducing emotional stress. ...|$|R
