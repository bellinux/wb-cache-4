9|27|Public
25|$|Xing Yi Quan {{emphasizes}} a {{close relationship}} between the movements of armed/unarmed techniques. This <b>technical</b> <b>overlap</b> aims to produce greater learning efficiency.|$|E
50|$|The term {{astronautics}} (originally astronautique in French) {{was coined}} in the 1920s by J.-H. Rosny, {{president of the}} Goncourt academy, in analogy with aeronautics. Because there is a degree of <b>technical</b> <b>overlap</b> between the two fields, the term aerospace {{is often used to}} describe both at once. In 1930, Robert Esnault-Pelterie published the first book on the new research field.|$|E
5000|$|This {{does not}} mean that the <b>technical</b> <b>overlap</b> of the {{channels}} recommends the non-use of overlapping channels. The amount of interference seen on a configuration using channels 1, 5, 9, and 13 can have very small difference from a three-channel configuration, and in the paper entitled [...] "Effect of adjacent-channel interference in IEEE 802.11 WLANs" [...] by Villegas this is also demonstrated.|$|E
5000|$|Figure out the {{relationship}} among multiple <b>overlapping</b> <b>technical</b> terms ...|$|R
50|$|Computer {{technology}} for developing areas is often through the donation {{of technology to}} developing areas without thought for access to electricity or equipment maintenance. Many institutions, government, charitable, and for-profit organizations require technology development often involving hardware or software design, and the coordination of donors, distributors, and deployers. <b>Technical</b> development <b>overlaps</b> with the fields of technical training, maintenance and support.|$|R
40|$|Abstract—In this paper, we {{integrate}} {{insights from}} diverse islands {{of research on}} electronic privacy to offer a holistic view of privacy engineering and a systematic structure for the discipline’s topics. First, we discuss privacy requirements grounded in both historic and contemporary perspectives on privacy. We use a three-layer model of user privacy concerns to relate them to system operations (data transfer, storage, and processing) and examine their effects on user behavior. In {{the second part of}} this paper, we develop guidelines for building privacy-friendly systems. We distinguish two approaches: “privacy-by-policy ” and “privacy-by-architecture. ” The privacy-by-policy approach focuses on the implementation of the notice and choice principles of fair information practices, while the privacy-by-architecture approach minimizes the collection of identifiable personal data and emphasizes anonymization and client-side data storage and processing. We discuss both approaches with a view to their <b>technical</b> <b>overlaps</b> and boundaries as well as to economic feasibility. This paper aims to introduce engineers and computer scientists to the privacy research domain and provide concrete guidance on how to design privacy-friendly systems. Index Terms—Privacy, security, privacy-enhancing technologies, anonymity, identification. Ç...|$|R
5000|$|The other {{services}} followed during the Vietnam War, creating the counterpart positions of Sergeant Major of the Army in 1966, Master Chief Petty Officer of the Navy and Chief Master Sergeant of the Air Force in 1967, and Master Chief Petty Officer of the Coast Guard in 1969. The positions are generically or collectively {{referred to as}} [...] "senior enlisted advisors" [...] ("SEAs"). Only one Soldier, Sailor, Marine, Airman and Coast Guardsman can hold that rank {{at any one time}} (although they also hold the rank upon retirement per 10 USC § 1406(i)(1); moreover, the position's singularity is not deemed to prevent the <b>technical</b> <b>overlap</b> of a few weeks while a recently [...] "retired" [...] SEA remains legally on active duty for the duration of his accrued back-leave, known as [...] "transitional leave"). Each advises his or her service chief (Chief of Staff of the Army, Commandant of the Marine Corps, Chief of Naval Operations, Chief of Staff of the Air Force, and Commandant of the Coast Guard) and other senior service leaders on all enlisted matters, makes decisions affecting enlisted personnel and their families, and is often invited to testify before Congress.|$|E
40|$|Nuclear power {{generation}} has undergone major expansion and developments in recent years; this third edition contains much revised material in presenting the state-of-the-art {{of nuclear power}} station designs currently in operation throughout the world. The volume covers nuclear physics and basic technology, nuclear station design, nuclear station operation, and nuclear safety. Each chapter is independent but with the necessary <b>technical</b> <b>overlap</b> to provide a complete work on the safe and economic design and operation of nuclear power stations...|$|E
40|$|In this paper, we derive non-asymptotic achievability and {{converse}} bounds on {{the random}} number generation with/without side-information. Our bounds are efficiently computable {{in the sense}} that the computational complexity does not depend on the block length. We also characterize the asymptotic behaviors of the large deviation regime and the moderate deviation regime by using our bounds, which implies that our bounds are asymptotically tight in those regimes. We also show the second order rates of those problems, and derive single letter forms of the variances characterizing the second order rates. Further, we address the equivocation rates for these problems. Comment: There is no <b>technical</b> <b>overlap</b> with the latest version of arXiv: 1309. 752...|$|E
40|$|Abstract –Data fusion is {{a branch}} of applied science that {{processes}} measurements {{from a variety of}} sources and time epochs to provide a consolidated state history of a reality of interest, be it a physical process, an intangible system (e. g. economic, social, …), or a complex entity that includes elements of both (e. g. a set of individuals in physical space). As such, data fusion is an important component in military and security surveillance systems. The technology encompasses stochastic modeling, nonlinear filtering, data correlation, and sensor management. Thus, there are significant <b>technical</b> <b>overlaps</b> with the signal processing, automatic control, information theory, and operations research communities. This paper provides illustrations of some applications of data fusion technology to surveillance systems. The unifying theme of the examples is the use of innovative and flexible multi-stage fusion processing. Additionally, we explore the use of multi-stage processing in the context of some of the datasets from the Multistatic Tracking Working Group (MSTWG), and we describe recent work on performance evaluation of tracking systems...|$|R
30|$|The government’s participation, {{mainly the}} local one, {{needs to be}} active, for {{wherever}} government is not present, maturity levels are low. However, in general, local governments {{do not have the}} ability to deal with an innovation environment, because political aspects <b>overlap</b> <b>technical</b> ones. Leadership and networking are essential to overcome this challenge.|$|R
40|$|This article {{considers}} the contexts and processes of forensic identification in 2004 post-tsunami Thailand {{as examples of}} identity politics. The presence of international forensic teams as carriers of diverse <b>technical</b> expertise <b>overlapped</b> with bureaucratic procedures put {{in place by the}} Thai government. The negotiation of unified forensic protocols and the production of estimates of identified nationals straddle biopolitics and 'thanatocracy'. The immense identification task testified on the one hand to an effort to bring individual bodies back to mourning families and national soils, {{and on the other hand}} to determining collective ethnic and national bodies, making sense out of an inexorable and disordered dissolution of corporeal as well as political boundaries. Individual and national identities were the subject of competing efforts to bring order to the chaos, reaffirming the cogency of the body politic by mapping national boundaries abroad. The overwhelming forensic effort required by the exceptional circumstances also brought forward the socio-economic and ethnic disparities of the victims, whose post-mortem treatment and identification traced an indelible divide between 'us' and 'them'...|$|R
40|$|Domain {{names in}} the Czech legal system The aim of this work is to {{introduce}} readers {{to the topic of}} domain names themselves with a necessary <b>technical</b> <b>overlap,</b> their legal regulation in the Czech legal framework, including a general analysis of particular legal institutes used for their protection (namely trademarks, unfair competition, name of an individual and name of a legal entity), and finally to outline the ways and development of the settlement of disputes arising from them before the judicial and extrajudicial bodies. The first chapter of the thesis covers the concept of domain names as such from a technical point of view. There is described the domain name function in the Internet and the domain name system as well, consisting of top level domains of different types with their specifics. Consequently are there through domain name registration covered domain name disputes in a global context, including the individual causes (types) of these disputes, and finally concludes with the description of a specific form of alternative dispute resolution called UDRP. The second, most extensive chapter is dedicated to the legal regulation of domain {{names in the}} Czech legal framework itself. The chapter begins with theoretical concept of domain names from the legal point of view and its historical [...] ...|$|E
40|$|Sandia {{currently}} lacks a {{high fidelity}} method for predicting loads on and subsequent structural response of earth penetrating weapons. This project seeks to test, debug, improve and validate methodologies for modeling earth penetration. Results {{of this project}} {{will allow us to}} optimize and certify designs for the B 61 - 11, Robust Nuclear Earth Penetrator (RNEP), PEN-X and future nuclear and conventional penetrator systems. Since this is an ASC Advanced Deployment project the primary goal of the work is to test, debug, verify and validate new Sierra (and Nevada) tools. Also, since this project is part of the V&V program within ASC, uncertainty quantification (UQ), optimization using DAKOTA [1] and sensitivity analysis {{are an integral part of}} the work. This project evaluates, verifies and validates new constitutive models, penetration methodologies and Sierra/Nevada codes. In FY 05 the project focused mostly on PRESTO [2] using the Spherical Cavity Expansion (SCE) [3, 4] and PRESTO Lagrangian analysis with a preformed hole (Pen-X) methodologies. Modeling penetration tests using PRESTO with a pilot hole was also attempted to evaluate constitutive models. Future years work would include the Alegra/SHISM [5] and AlegrdEP (Earth Penetration) methodologies when they are ready for validation testing. Constitutive models such as Soil-and-Foam, the Sandia Geomodel [6], and the K&C Concrete model [7] were also tested and evaluated. This report is submitted to satisfy annual documentation requirements for the ASC Advanced Deployment program. This report summarizes FY 05 work performed in the Penetration Mechanical Response (ASC-APPS) and Penetration Mechanics (ASC-V&V) projects. A single report is written to document the two projects because of the significant amount of <b>technical</b> <b>overlap...</b>|$|E
40|$|Most {{transnational}} regulatory problems involve technical systems: extended sets {{of productive}} connections between humans, organized knowledge, and material objects. The functioning and relations between transnational business governance (TBG) schemes {{in any particular}} issue area are usually shaped by these technical systems. These technical systems and the material world that they interact with are not simply exogenous environments for TBG schemes. Individual TBG schemes can enhance their power and influence by expanding their function in a technical system, by incorporating the material aspects of the system into their activities, or by producing the system’s technical knowledge. I hypothesize that where a robust technical system exists, the degree of integration {{and the need for}} coordination of the activities it involves will mean that in most cases that technical system will be coordinated overall by only one TBG scheme. There are two exceptions: where <b>technical</b> systems <b>overlap,</b> or where the system is so weak that competitive pressures outweigh the factors contributing to specialization. The article develops these themes by drawing on theories that have focused on the social and political aspects of technical systems. The article identifies the contributions and limits of these theories and of a focus on technical systems in analyzing interactions among TBG schemes. The relevance of the theoretical points is assessed with regard to the TBG schemes that are active in global finance...|$|R
2500|$|Systems {{engineering}} is an interdisciplinary field {{of engineering and}} engineering management that focuses on how to design and manage complex systems over their life cycles. At its core systems engineering utilizes systems thinking principles to organize this body of knowledge. [...] Issues such as requirements engineering, , logistics, coordination of different teams, testing and evaluation, maintainability and many other disciplines necessary for successful system development, design, implementation, and ultimate decommission become more difficult when dealing with large or complex projects. Systems engineering deals with work-processes, optimization methods, and risk management tools in such projects. It <b>overlaps</b> <b>technical</b> and human-centered disciplines such as industrial engineering, mechanical engineering, manufacturing engineering, control engineering, software engineering, electrical engineering, cybernetics, organizational studies and project management. [...] Systems engineering ensures that all likely aspects of a project or system are considered, and integrated into a whole.|$|R
40|$|Patent {{documents}} are unique external {{sources of information}} that reveal the core technology underlying new inventions. Patents {{also serve as a}} strategic data source that can be mined to discover state-of-the-art technical development and subsequently help guide R&D investments. This research incorporates an ontology schema to extract and represent patent concepts. A clustering algorithm with non-exhaustive overlaps is proposed to overcome deficiencies with exhaustive clustering methods used in patent mining and technology discovery. The non-exhaustive clustering approach allows for the clustering of patent documents with <b>overlapping</b> <b>technical</b> findings and claims, a feature that enables the grouping of patents that define related key innovations. Legal advisors can use this approach to study potential cases of patent infringement or devise strategies to avoid litigation. The case study demonstrates the use of non-exhaustive overlaps algorithm by clustering US and Japan radio frequency identification (RFID) patents and by analyzing the legal implications of automated discovery of patent infringement...|$|R
40|$|Approach to Connect Global Objectives and Local Actions: (1) Articulate global {{objectives}} into {{a hierarchy}} of subsystem requirements and local attributes and measures; (2) Establish a baseline system and viable alternatives through the interactions and relationships (e. g., networks) of local system elements and their options; (3) Evaluate performance of system alternatives and develop improved nuclear material management strategies and technologies; and (4) The need to address greatest concerns first (prioritized or graded approach) and to make tradeoffs among implementation options and competing objectives entails a risk-based approach. IGNMM could provide a systematic understanding of global nuclear materials management and evolutionarily improve and integrate the management through an active architecture, using for example, situation awareness, system models, methods, technologies, and international cooperation. Different tools would be used within the overall framework to address individual issues on the desired geographic scale that could be easily linked to broader analyses. Life-cycle system analyses would allow for evaluating material path alternatives on an integrated global scale. Disconnects, <b>overlaps,</b> <b>technical</b> options, and alternatives for optimizing nuclear materials processes could be evaluated in an integrated manner...|$|R
40|$|Background and {{objective}} It {{has been proven}} that nm 23 -H 1 gene was a tumor metastatic suppressor gene. However, it’s molecular mechanism of suppressing metastasis remains unexplored. There is a closely relationship between the abnormality of stucturs and functions of nm 23 -H 1 gene, and cancer invasion and metastasis. We have constructed the vector with nm 23 -H 1 -shRNA and the vector with nm 23 -H 1 cDNA resistant to the specific shRNA. So, we plan to construct shRNA-resistant eukaryotic expression vector of nm 23 -H 1 gene by site-directed mutagenesis, rescue experiment was performed to verify the nm 23 -H 1 gene expression, and to provide basement for studying the biochemical mechanisms of nm 23 -H 1 gene. Methods Site-directed mutagenesis of nm 23 -H 1 gene was performed by overlap extension PCR method. Pure plasmid containing gene of nm 23 -H 1 (shRNA-resistant) was prepared. The desired five mutations were constructed and cloned into the eukaryotic vector pcDNA 3. 1 Hygro(+). The human lung adenocarcinoma cell A 549 /nm 23 -H 1 -shRNA (stable nm 23 -H 1 gene silencing) was transfected with the five mutants, and {{the expression of the}} mutant proteins was determined by Western blot. Results Five eukaryotic expression vectors (shRNA-resistant) of nm 23 -H 1, nm 23 -H 1 S 44 A, nm 23 -H 1 P 96 S, nm 23 -H 1 H 118 F, nm 23 -H 1 S 120 G, nm 23 -H 1 P 96 S-S 120 G, were successfully constructed. The results of DNA sequencing confirmed that the base sequences of the genes were completely concordant with experiment design. The expression of nm 23 -H 1 mutant proteins was verified by Western blot. Conclusion Five eukaryotic expression vectors (shRNA-resistant) of nm 23 -H 1 gene were successfully constructed, and the mutant proteins were verified. The site-directed mutagenesis <b>technical</b> of <b>overlap</b> extension PCR is a efficient, simple and economical method...|$|R
40|$|In complex technologies, IPR {{is owned}} among several firms and <b>technical</b> {{innovation}} often <b>overlaps.</b> In situations of divided technical leadership cooperative activities {{are crucial to}} influence technology adoption. Standardization is a process to commonly agree on a technology. This article investigates the patent declaration behavior of 250 companies which participate in international standard bodies. Over 60, 000 patent declarations were analyzed on a firm level basis to show how cooperative activities influence the inclusion of patents in technology standards. Our empirical {{results show that the}} involvement in related standards consortia as well as being a member in patent pools favors patent declaration. While the pool variable rather controls for the default mechanism that firms can only join pools when they declare essential IPR, our empirical analysis is able to isolate the positive consortia effect by controlling {{for a wide range of}} firm and standard characteristi cs. We apply panel analysis and find evidence for a positive dynamic effect of entering a standards consortium. We also find significant results for exogenous membership changes in standards consortia. Our findings proof that firms which participate in standards consortia increase their negotiation power in technology selection processes and are thus able to introduce more patents into standards...|$|R
40|$|The {{traceability}} and safeguarding {{of origin}} of organic products shall {{be improved by}} the development of an <b>overlapping</b> <b>technical</b> data base traceability system. This is to contribute to the prevention and mitigation of scandals and strengthen consumers` trust in organic products. Data base systems existing so far or just being developed do serve different purposes depending on the respective fabricator. There are data bases for the acquisition of basic and company related data of the client, systems to delineate the processes of control and certification as well as data bases for the provision of information for the participants of the market, such as information related to products, the products` properties and availability, as well as internal inventory control systems of the company. All of these systems are isolated applications so far, suited for the specific demands of the operator. The precondition for the development of an overlapping system is the integration and linkage of the existing systems. To enable a data exchange among the different data base systems or the concentration of data in a centralised data base a standard of data shall be developed. By this means, data base based traceability along the entire value-added chain would be ensured. ...|$|R
40|$|Previous phylogenetic {{studies in}} oaks (Quercus, Fagaceae) {{have failed to}} resolve the {{backbone}} topology of the genus with strong support. Here, we utilize next-generation sequencing of restriction-site associated DNA (RAD-Seq) to resolve a framework phylogeny of a predominantly American clade of oaks whose crown age is estimated at 23 – 33 million years old. Using a recently developed analytical pipeline for RAD-Seq phylogenetics, we created a concatenated matrix of 1. 40 E 06 aligned nucleotides, constituting 27, 727 sequence clusters. RAD-Seq data were readily combined across runs, with no difference in phylogenetic placement between <b>technical</b> replicates, which <b>overlapped</b> by only 43 – 64 % in locus coverage. 17 % (4, 715) of the loci we analyzed could be mapped with high confidence {{to one or more}} expressed sequence tags in NCBI Genbank. A concatenated matrix of the loci that BLAST to at least one EST sequence provides approximately half as many variable or parsimony-informative characters as equal-sized datasets from the non-EST loci. The EST-associated matrix is more complete (fewer missing loci) and has slightly lower homoplasy than non-EST subsampled matrices of the same size, but there is no difference in phylogenetic support or relative attribution of base substitutions to internal versus terminal branches of the phylogeny. We introduce a partitioned RAD visualization method (implemented in the R package RADami...|$|R
40|$|At the U. S. Department of Energy's Hanford Site {{in southeastern}} Washington State, 216 million liters (57 million gallons) of nuclear waste is {{currently}} stored in aging underground tanks, threatening the Columbia River. The River Protection Project (RPP), a fully integrated system of waste storage, retrieval, treatment, and disposal facilities, is in varying stages of design, construction, operation, and future planning. These facilities face many <b>overlapping</b> <b>technical,</b> regulatory, and financial hurdles to achieve site cleanup and closure. Program execution is ongoing, but completion is currently {{expected to take}} approximately 40 more years. Strategic planning {{for the treatment of}} Hanford tank waste is by nature a multi-faceted, complex and iterative process. To help manage the planning, a report referred to as the RPP System Plan is prepared to provide a basis for aligning the program scope with the cost and schedule, from upper-tier contracts to individual facility operating plans. The Hanford Tank Waste Operations Simulator (HTWOS), a dynamic flowsheet simulation and mass balance computer model, is used to simulate the current planned RPP mission, evaluate the impacts of changes to the mission, and assist in planning near-term facility operations. Development of additional modeling tools, including an operations research model and a cost model, will further improve long-term planning confidence. The most recent RPP System Plan, Revision 4, was published in September 2009...|$|R
500|$|On June 25, 1988, the Hanford {{site was}} divided into four areas and {{proposed}} for inclusion on the National Priorities List. On May 15, 1989, the Washington Department of Ecology, the United States Environmental Protection Agency, and the Department of Energy entered into the Tri-Party Agreement, which provides a legal framework for environmental remediation at Hanford. [...] the agencies are engaged in the world's largest environmental cleanup, with many challenges to be resolved {{in the face of}} <b>overlapping</b> <b>technical,</b> political, regulatory, and cultural interests. The cleanup effort is focused on three outcomes: restoring the Columbia River corridor for other uses, converting the central plateau to long-term waste treatment and storage, and preparing for the future. The cleanup effort is managed by the Department of Energy under the oversight of the two regulatory agencies. A citizen-led Hanford Advisory Board provides recommendations from community stakeholders, including local and state governments, regional environmental organizations, business interests, and Native American tribes. Citing the 2014 Hanford Lifecycle Scope Schedule and Cost report, the 2014 estimated cost of the remaining Hanford clean up is $113.6billion – more than $3billion per year for the next six years, with a lower cost projection of approximately $2billion per year until 2046. About 11,000workers are on site to consolidate, clean up, and mitigate waste, contaminated buildings, and contaminated soil. Originally scheduled to be complete within thirty years, the cleanup was less than half finished by 2008. Of the four areas that were formally listed as Superfund sites on October 4, 1989, only one has been removed from the list following cleanup.|$|R
40|$|This {{dissertation}} defines and {{analyzes the}} infrastructure of classified information including the sites, systems, objects and discourses that enable the creation, maintenance, management and destruction of classified information. Seeking to situate classified information as {{something other than a}} mere absence of information or an impediment to knowledge, this dissertation focuses on what kinds of records, documents, evidence and knowledge are created through the daily practices of official secrecy at the federal level. The increasing complexity of <b>overlapping</b> <b>technical</b> infrastructures and organizational standards requires thinking about records infrastructurally, re-framing individual documents as systems, if we are to begin thinking of future use and access. 	This dissertation examines records within {{the infrastructure of}} classified information by contextualizing the need for research into these records as objects of great complexity that eschew easy distinctions between open and closed. It utilizes a research framework oriented around four elements: Standards, Economies, Rupture and Culture. Using methods from infrastructure studies, archival studies and critical discourse analysis, it analyzes the field of creation within socio-technical systems and identifies materiality as a matter of paramount importance for the maintenance of evidential value within overlapping systems of trust. 	This dissertation illustrates the paucity of nuanced understandings of networked records in federal agencies and exposes a number of areas for further research and challenges for those who work in archival studies and information policy. This dissertation finds that a vital rethinking of the role of archival work and thinking could lead to an integration of archival processes into daily government work instead of traditional modes of custodial transfer...|$|R
40|$|Typically, unit-level data {{take the}} form of {{information}} on specimen labels. However, many organisations with natural history collections house large archives of data that have been compiled on index cards. Before the advent of computerised databases, the card index was one of the major ways by which such organisations used to catalogue information – especially informa-tion that was subject to change over time and which needed to be kept up-to-date. The data in these archives may pertain to specimens, species, or other taxa, and may include information such as nomenclature, type status, type locality, taxonomic history and distribution. Whatever natural history data is held, and usually these are of several kinds within a single archive, each individual card can be regarded as the equivalent of a specimen in a collec-tion. Since these vulnerable archives often represent unique datasets constructed, in many cases, over decades, curators have a responsibility to ensure the protection of data within them. Curators should also aim to improve access to the information held on these cards, which at present is largely trapped within organisations. Fortunately, improving both data protec-tion and data access are highly compatible goals and have <b>overlapping</b> <b>technical</b> requirements. Although card index archives have served the natural history community well in the past (and continue to do so often), they have many disadvantages when compared with computer databases. Cards are all too easy to misfile, misplace or lose. They become degraded with use – characters may fade and cards get torn or bent. Handwriting on cards is often illegible, and archives 10 Computerising unit-level data in naturalhistory card archive...|$|R
40|$|Part 13 : Cyberphysical and Smart SystemsInternational audienceTo {{achieve the}} full {{potential}} of PLM in Systems Engineering tools especially {{in view of}} the complexity of the system in industries such as the aerospace industry {{it is important to have}} a clear understanding of how best to use such systems. Systems Engineering is an interdisciplinary field of engineering that focuses on how to design and manage complex engineering systems over their life cycles. Issues such as reliability, logistics, coordination of different teams (requirements management), evaluation measurements, and other disciplines become more difficult when dealing with large or complex projects. Systems Engineering deals with work-processes, optimization methods and tools in such projects. It <b>overlaps</b> <b>technical</b> and human-centered disciplines such as control engineering, industrial engineering, organizational studies, and project management. Systems Engineering ensures that all likely aspects of a project or system are considered, and integrated into a whole. After a short introduction, this paper, which is based on the results of the accomplished descriptive study and literature survey, presents a generic integrated approach of System Driven Product Development (SDPD) and demonstrates the general requirements of a generic integrated approach during the Engineering Design of Systems. The second section presents a new approach of Systems Engineering, which is based on SDPD and will explain the different phases and sub-phases of the developed approach. By means of designing a Quadrocopter the different phases of the developed generic integrated approach will be demonstrated and presented. Section three will discuss the results of the prescriptive study and address the most important issues. In general this paper presents the prescriptive phase of the design research methodology according to Blessing and Chakrabarti...|$|R
5000|$|On June 25, 1988, the Hanford {{site was}} divided into four areas and {{proposed}} for inclusion on the National Priorities List. On May 15, 1989, the Washington Department of Ecology, the United States Environmental Protection Agency, and the Department of Energy entered into the Tri-Party Agreement, which provides a legal framework for environmental remediation at Hanford. [...] the agencies are engaged in the world's largest environmental cleanup, with many challenges to be resolved {{in the face of}} <b>overlapping</b> <b>technical,</b> political, regulatory, and cultural interests. The cleanup effort is focused on three outcomes: restoring the Columbia River corridor for other uses, converting the central plateau to long-term waste treatment and storage, and preparing for the future. The cleanup effort is managed by the Department of Energy under the oversight of the two regulatory agencies. A citizen-led Hanford Advisory Board provides recommendations from community stakeholders, including local and state governments, regional environmental organizations, business interests, and Native American tribes. Citing the 2014 Hanford Lifecycle Scope Schedule and Cost report, the 2014 estimated cost of the remaining Hanford clean up is $113.6 billion - more than $3 billion per year for the next six years, with a lower cost projection of approximately $2 billion per year until 2046. About 11,000 workers are on site to consolidate, clean up, and mitigate waste, contaminated buildings, and contaminated soil. Originally scheduled to be complete within thirty years, the cleanup was less than half finished by 2008. Of the four areas that were formally listed as Superfund sites on October 4, 1989, only one has been removed from the list following cleanup.|$|R
40|$|AbstractQuantitative PCR (qPCR) is {{the method}} of choice in gene {{expression}} analysis. However, the number of groups or treatments, target genes and technical replicates quickly exceeds the capacity of a single run on a qPCR machine and the measurements have to be spread over more than 1 plate. Such multi-plate measurements often show similar proportional differences between experimental conditions, but different absolute values, even though the measurements were technically carried out with identical procedures. Removal of this between-plate variation will enhance {{the power of the}} statistical analysis on the resulting data. Inclusion and application of calibrator samples, with replicate measurements distributed over the plates, assumes a multiplicative difference between plates. However, random and technical errors in these calibrators will propagate to all samples on the plate. To avoid this effect, the systematic bias between plates can be removed with a correction factor based on all <b>overlapping</b> <b>technical</b> and biological replicates between plates. This approach removes the requirement for all calibrator samples to be measured successfully on every plate. This paper extends an already published factor correction method to the use in multi-plate qPCR experiments. The between-run correction factor is derived from the target quantities which are calculated from the quantification threshold, PCR efficiency and observed Cq value. To enable further statistical analysis in existing qPCR software packages, an efficiency-corrected Cq value is reported, based on the corrected target quantity and a PCR efficiency per target. The latter is calculated as the mean of the PCR efficiencies taking the number of reactions per amplicon per plate into account. Export to the RDML format completes an RDML-supported analysis pipeline of qPCR data ranging from raw fluorescence data, amplification curve analysis and application of reference genes to statistical analysis...|$|R
40|$|Quantification {{of global}} {{transcripts}} expression {{is a key}} step towards developing system-level understanding in biology. Probe independent RNA-seq provides digital estimation of transcript abundance with dynamic range large enough to accurately quantify the majority of complex mammalian transcriptomes. However, a reliable quantification of low abundant transcripts from limited amounts of mRNA has remained a challenge for RNA-seq. The widely used RNA-seq protocol requires 1 - 10 ng of mRNA to generate robust sequencing libraries restricting its application in disciplines where obtaining such amounts of mRNA is challenging, such as in developmental biology, stem cell biology and forensics. To address this issue, we developed a novel RNA-seq methodology (DP-seq) that uses a defined set of 44 heptamer primers to amplify majority of the mammalian transcripts from limiting amounts of mRNA, while preserving their relative abundance. DP-seq reproducibly yields high levels of amplification from as low as 50 pg of mRNA (50 - 100 mammalian cells) with a dynamic range of over five orders of magnitude in RNA concentrations. A novel two-step amplification step utilizing a combination of mesophilic and thermophilic polymerases was devised to achieve efficient amplification from the heptamer primers. Furthermore, we exploited PCR biases observed in our methodology to reduce the representation of highly expressed ribosomal transcripts by more than 70 % in our sequencing libraries. We validated DP-seq on lineage segregation model in early stem cell cultures achieved by modulating TGF[Beta] pathway. DP-seq accurately quantified {{the majority of the}} low expressed transcripts and revealed novel lineage markers and putative TGF[Beta] target genes. Similarly, by using DP- seq we functionally characterized dedifferentiated neurons and astrocytes and found the cell cycle, Wnt signaling and the focal adhesion pathways {{to be involved in the}} maintenance of their undifferentiated state. Finally, we compared DP-seq with other amplification-based strategies and found similar transcriptome coverage and <b>overlapping</b> <b>technical</b> noise. Interestingly, the technical noise increased significantly when ultra-low amount of mRNA (single cell level) was used, irrespectively of the methodology. In conclusion, this study provides an economical and efficient solution for sequencing library generation using low amounts of mRNA thereby increasing the applicability of RNA-seq to a wider spectrum of biological system...|$|R
40|$|Developers {{settle for}} a non-optimal {{solution}} under pressure to meet deadlines and quotas despite the potential pitfalls that might ensue at later stages in development, which {{has been referred to}} as “technical debt. ” And like its financial analogue, if not carefully monitored and mediated, technical debt can compromise the very project it was intended to expedite. Several approaches have been proposed to aid developers in tracking the technical debt they incur. Traditionally, developers have relied on metric-based approaches, which use static analysis tools to identify technical debt based on thresholds defined on object-oriented metrics, e. g. code smells. Another technique, pioneered in a recent study, leverages source code comments to detect (self-admitted) technical debt. Therefore, in this thesis we use empirical studies to examine how self-admitted technical debt and code smells (God Classes) relate to software quality. Preliminarily, we examine the relationship between self-admitted technical debt and software quality for five open-source projects. To measure this, we take into account three criteria commonly associated with quality: (i) on the file level, the relationship between defects and self-admitted technical debt (SATD); (ii) on the change level, the potential of SATD to introduce future defects and (iii) the complexity SATD changes impose on the system. The results of our study indicate that: (i) SATD files tend to have less defects than non-SATD files and (ii) SATD changes make the system less susceptible to future defects than non-SATD changes do, though (iii) SATD changes are more difficult to execute. Until the advent of SATD, god classes were used to detect technical debt, and though others have studied the impact of metric-based approaches on software quality, this work has been limited to a small number of systems. Therefore, we conduct an extensive investigation that compares the relationship between both approaches and software quality on a larger number of projects. We assess how code smells—in particular, god classes (metric-based approach) —and SATD (comment-based approach) are associated with software quality by determining: (i) whether god and SATD files have more defects than non-god and non-SATD files, (ii) whether god and SATD changes induce future defects at a higher rate than non-god and non-SATD changes, (iii) whether god and SATD changes are more difficult to perform than non-god and non-SATD changes and (iv) how much the metric- and comment-based approaches to technical debt file identification overlap. Our results indicate that: (i) neither god nor SATD files are correlated with defects, (ii) introduction of future defects is higher for god- and SATD-related changes, (iii) god- and SATD-related changes are more difficult to perform and (iv) the metric-comment <b>technical</b> debt file <b>overlap</b> ranges from 11...|$|R
40|$|This {{dissertation}} {{considers the}} influence that sixteenth- and early seventeenth-century mathematical thinking exerted on popular drama in the final sixteen years of Elizabeth I’s reign. It concentrates upon six plays by five dramatists, and attempts to analyse how the terms, concepts, and implications of contemporary mathematics impacted upon their vocabularies, forms, and aesthetic and dramaturgical effects and affects. 	Chapter 1 is an introductory chapter, which sets out {{the scope of the}} whole project. It locates the dissertation in its critical and scholarly context, and provides a history of the <b>technical</b> and conceptual <b>overlap</b> between the mathematical and literary arts, before traversing the body of intellectual-historical information necessary to situate contextually the ensuing five chapters. This includes a survey of mathematical practice and pedagogy in Elizabethan England. 	Chapter 2, ‘Algebra and the Art of War’, considers the role of algebra in Marlowe’s Tamburlaine plays. It explores the function of algebraic concepts in early modern military theory, and argues that Marlowe utilised the overlap he found between the two disciplines to create a unique theatrical spectacle. Marlowe’s ‘algebraic stage’, I suggest, enabled its audiences to perceive the enormous scope and aesthetic beauty of warfare within the practical and spatial limitations of the Elizabethan playhouse. 	Chapter 3, ‘Magic, and the Mathematic Rules’, explores the distinction between magic and mathematics presented in Greene’s Friar Bacon and Friar Bungay. It considers early modern debates surrounding what magic is, and how it was often confused and/or conflated with mathematical skill. It argues that Greene utilised the set of difficult, ambiguous distinctions that arose from such debates for their dramatic potential, because they lay also at the heart of similar anxieties surrounding theatrical spectacles. 	Chapter 4, ‘Circular Geometries’, considers the circular poetics effected in Dekker’s Old Fortunatus. It contends that Dekker found an epistemological role for drama by having Old Fortunatus acknowledge a set of geometrical affiliations which it proceeds to inscribe itself into. The circular entities which permeate its form and content are as disparate as geometric points, the Ptolemaic cosmos, and the architecture of the Elizabethan playhouses, and yet, Old Fortunatus unifies these entities to praise God and the monarchy. 	Chapter 5, ‘Infinities and Infinitesimals’, considers how the infinitely large and infinitely small permeate the language and structure of Shakespeare’s Hamlet. It argues that the play is embroiled with the mathematical implications of Copernican cosmography and its Brunian atomistic extension, and offers a linkage between the social circles of Shakespeare and Thomas Harriot. Hamlet, it suggests, courts such ideas at the cutting-edge of contemporary science in order to complicate the ontological context within which Hamlet’s revenge act must take place. 	Chapter 6, ‘Quantifying Death, Calculating Revenge’, proposes that the quantification of death, and the concomitant calculation of an appropriate revenge, are made an explicit component of Chettle’s Tragedy of Hoffman. It suggests that Chettle enters two distinctly mathematical models of revenge into productive counterpoint in the play in order to interrogate the ethics of revenge, and to dramatise attempts at quantifying the parameters of equality and excess, parity and profit...|$|R
40|$|Deming quality methodologies {{applied to}} safety are {{recognized}} with the National Safety Council's annual Robert W. Campbell Award. Over {{the last ten}} years, the implementation of Statistical Process Control and quality methodologies at the U. S. Department of Energy's Hanford Site have contributed to improved safety. Improvements attributed to Statistical Process Control are evidenced in Occupational Safety and Health records and documented through several articles in Quality Progress and the American Society of Safety Engineers publication, Professional Safety. Statistical trending of safety, quality, and occurrence data continues to playa key role in improving safety and quality at {{what has been called}} the world's largest environmental cleanup project. DOE's Hanford Site played a pivotal role in the nation's defense beginning in the 1940 s, when it was established as part of the Manhattan Project. After more than 50 years of producing material for nuclear weapons, Hanford, which covers 586 square miles in southeastern Washington state, is now focused on three outcomes: (1) Restoring the Columbia River corridor for multiple uses; (2) Transitioning the central plateau to support long-term waste management; and (3) Putting DOE assets to work for the future. The current environmental cleanup mission faces challenges of <b>overlapping</b> <b>technical,</b> political, regulatory, environmental, and cultural interests. From Oct. 1, 1996 through Sept. 30, 2008, Fluor Hanford was a prime contractor to the Department of Energy's Richland Operations Office. In this role, Fluor Hanford managed several major cleanup activities that included dismantling former nuclear-processing facilities, cleaning up the Site's contaminated groundwater, retrieving and processing transuranic waste for shipment and disposal off-site, maintaining the Site's infrastructure, providing security and fire protection, and operating the Volpentest HAMMER Training and Education Center. On October 1, 2008, a transition occurred that changed Fluor's role at Hanford. Fluor's work at Hanford was split in two with the technical scope being assumed by the CH 2 M HILL Plateau Remediation Company (CHPRC) CHPRC is now spearheading much of the cleanup work associated with former nuclear-processing facilities, contaminated groundwater, and transuranic waste. Fluor is an integrated subcontractor to CH PRC in this effort. In addition, {{at the time of this}} writing, while the final outcome is being determined for the new Mission Support Contract, Fluor Hanford has had its contract extended to provide site-wide services that include security, fire protection, infrastructure, and operating the HAMMER facility. The emphasis has to be on doing work safely, delivering quality work, controlling costs, and meeting deadlines. Statistical support is provided by Fluor to the PRC, within Fluor Hanford, and to a third contractor, Washington Closure Hanford, which is tasked with cleaning up approximately 210 square miles designated as the Columbia River corridor along the outer edge of the Hanford Site. The closing months of Fluor Hanford's 12 year contract were busy, characterized by special events that capped its work as a prime cleanup contractor, transitions of work scope and personnel, and the completion numerous activities. At this time, Fluor's work and approach to safety were featured in state and national forums. A 'Blockbuster' presentation at the Washington State Governor's Industrial Safety Conference in September 2008 featured Fluor Hanford's Chief Operating Officer, a company Safety Representative, and me. Simultaneously, an award ceremony in Anaheim, Calif. recognized Fluor Hanford as the winner of the 2008 Robert W. Campbell Award. The Robert W. Campbell Award is co-sponsored by Exxon Mobil Corporation and the National Safety Council. Named after a pioneer of industrial safety, the Campbell Award recognizes organizations that demonstrate how integration of environmental, health and safety (EHS) management into business operations is a cornerstone of their corporate success. Fluor Hanford received the award for corporations with more than 1, 000 employees. Campbell Award winners undergo rigorous assessments that include site visits and comprehensive evaluations of their commitment to, and implementation of, EHS practices. Award winners work with an international partnership of 21 organizations to develop case studies that illustrate their superior EHS programs and best practices, for use by top business and engineering schools worldwide. Quality methodologies in place at Fluor Hanford played a key role in the award process. Fluor Hanford's integrated use of Statistical Process Control and Pareto Charts for analyzing and displaying EHS performance were viewed favorably by the award judges...|$|R

