1|40|Public
5000|$|Another early {{example of}} the boss key is in the IBM PC version of Asylum, which clears the screen when F9 is pressed. Certain games have taken {{the idea of the}} boss key and used it to comic effect. Infocom's adult-themed Leather Goddesses of Phobos (only the IBM PC version) had a boss key which would hide the game and show a screen {{designed}} to look like a Cornerstone database view. Upon closer inspection, however, the screen was not exactly boss safe, being populated with order info on rather ridiculous adult items, including an [...] "inflatable milkman". Sierra On-Line's comedy/sci-fi adventure game Space Quest III had a so-called boss key available from the game's pulldown menu. However, when the user selected it, the screen would cut to black and inform the user that his or her boss wouldn't be happy if he or she knew how long the user had been playing the game. It then displayed the total elapsed game time. The first few games in Sierra's Leisure Suit Larry series included a boss key in the pulldown menus (shortcut usually Ctrl+B). However, when this is used, it results in an instantaneous game over with the first game saying [...] "Sorry, but you'll have to restore your game; when you panic, I forget everything!" [...] The computer submarine game, GATO, when ESC was clicked, brought up a Lotus 1-2-3 <b>type</b> <b>spreadsheet</b> screen.|$|E
25|$|The {{visualizations}} and diagrams link to {{a variety}} of assessments, usually handled in Excel <b>type</b> <b>spreadsheets</b> — to increase value outputs, to leverage knowledge and intangibles for improving financial and organizational performance, and to find new value opportunities. When the analysis is complete people gain insights into what is actually happening now, where more value can be realized, and what is required to achieve maximum value benefit across the entire business activity that is the focus of the analysis.|$|R
50|$|Along {{with the}} more {{traditional}} business transactions the critical intangible exchanges are also mapped. Intangible exchanges are those mostly informal knowledge exchanges and benefits or support that build relationships and keep things running smoothly. These informal exchanges are actually the key to creating trust and opening pathways for innovation and new ideas. Traditional business practices ignore these important intangible exchanges, but they are made visible with a value network analysis. The visualizations and diagrams link {{to a variety of}} assessments, usually handled in Excel <b>type</b> <b>spreadsheets</b> — to increase value outputs, to leverage knowledge and intangibles for improving financial and organizational performance, and to find new value opportunities. When the analysis is complete people gain insights into what is actually happening now, where more value can be realized, and what is required to achieve maximum value benefit across the entire business activity that is the focus of the analysis.|$|R
40|$|Students {{using three}} <b>types</b> of <b>spreadsheet</b> calculators for {{understanding}} expected value were observed remotely. This remote observation {{involves the use}} of webcams and application sharing for observing students learning mathematics. The study illustrates how remote observation can be used for collecting mathematical education data and raises questions {{about the extent to which}} such a method can be used in future experiments...|$|R
50|$|Early {{implementations}} of {{the clipboard}} stored data as plain text without meta-information such as typeface, type style or color. More recent implementations support the multiple types of data, allowing complex data structures to be stored. These range from styled text formats such as RTF or HTML, {{through a variety}} of bitmap and vector image formats to complex data <b>types</b> like <b>spreadsheets</b> and database entries.|$|R
40|$|A {{system for}} {{defining}} functions and data <b>types</b> using <b>spreadsheets</b> is presented. Jeksy is spreadsheet application {{which provides a}} way to cre-ate function and <b>type</b> definitions using <b>spreadsheets</b> themselves. These {{are referred to as}} FunctionSheets and TypeSheets. This gives users only fa-miliar with spreadsheets access to some of the power available in classic programming languages. By solving real world problems using Jeksy it was shown how these new features can be used to build spreadsheets with a better structure and as a result reduce the likelihood of errors in the program. 2 Acknowledgements I would like to take this opportunity to thank my supervisor Don Sannella for the guidance and support he has provided throughout the course of the project. 3 Declaration I declare that this thesis was composed by myself, that the work contained herein i...|$|R
40|$|This paper {{describes}} {{a framework for}} a systematic classification of spreadsheet errors. This classification or taxonomy of errors is aimed at facilitating analysis and comprehension of the different <b>types</b> of <b>spreadsheet</b> errors. The taxonomy is an outcome of {{an investigation of the}} widespread problem of spreadsheet errors and an analysis of specific types of these errors. This paper contains a description of the various elements and categories of the classification and is supported by appropriate examples. Comment: 9 Pages, 6 Figure...|$|R
40|$|We {{intend to}} {{demonstrate}} the innate problems with existing spreadsheet products and to show how to tackle these issues using a new <b>type</b> of <b>spreadsheet</b> program called Resolver. It addresses the issues head-on and thereby moves the 1980 's "VisiCalc paradigm" on to match the advances in computer languages and user requirements. Continuous display of the spreadsheet grid and the equivalent computer program, together {{with the ability to}} interact and add code through either interface, provides a number of new methodologies for spreadsheet development. Comment: 12 page...|$|R
40|$|The aim of {{this thesis}} {{is to examine the}} {{dominant}} computer software usability evaluation methods. Four evaluation methods (logged data, questionnaire, interview, and verbal protocol analysis) were used to evaluate three different business software <b>types</b> (<b>spreadsheet,</b> word processor, and database) using a between groups design, involving 148 individuals of both genders. When each evaluation method was examined individually, the results tended to support findings from previous research. Comparisons were made to examine the efficiency of each evaluation method, in terms of its ability to highlight usability problems (both between and within the evaluation strategy). Here support for the efficiency of the verbal protocol analysis method was found. The efficiency of using two evaluation methods was also examined, where it was found that no significant improvement was obtained over the verbal protocol analysis used by itself. A comparison addressing the practicality of using these methods was also conducted. It seems that each method has differing strengths and weaknesses depending on the stage of the evaluation. From these results a theory for the effectiveness of evaluation strategies is presented. Suggestions for improving the methods commonly used, are also made. The thesis concludes by discussing the software evaluation domain and its relationship to the wider evaluation context...|$|R
50|$|The {{domain name}} kosht.com was {{purchased}} on August 16, 1999 {{and the first}} version of the site was launched on September 25. In the first months KOSHT.com was presented as a few <b>spreadsheets</b> <b>typed</b> in Microsoft Excel - price lists consisting of 276 entries from the two companies.|$|R
50|$|The {{first school}} of thought {{believes}} that financial models, if properly constructed, {{can be used to}} predict the future. The focus is on variables, inputs and outputs, drivers and the like. Investments of time and money are devoted to perfecting these models, which are typically held in some <b>type</b> of financial <b>spreadsheet</b> application.|$|R
40|$|Introduction to Alternative Energy Sources Global Warming Pollution Solar Cells Wind Power Biofuels Hydrogen Production and Fuel Cells Introduction to Computer Modeling Brief History of Computer Simulations Motivation and Applications of Computer Models Using <b>Spreadsheets</b> for Simulations <b>Typing</b> Equations into <b>Spreadsheets</b> Functions Available in Spreadsheets Random Numbers Plotting Data Macros and Scripts Interpolation and Extrapolation Numerical Integration and Diff...|$|R
40|$|The {{research}} was purposed {{to find out}} the effectiveness of demonstration and presentation models are able to improve study result of students in operating <b>Spreadsheet</b> <b>type</b> and function by statistics in X Accounting 1 in SMKN 48 at Academic Year 2014 / 2015. The reasearch was conducted on August-November 2014. The method of the {{research was}} Action Research (PTK) which was conducted in two cycles. Demonstration and Presentation were used as learning cycle model. A cycle consisted in three times of meetings and in the third meeting was done Post test. The indicators have been achieved by the result of research. As the expectation, in the second cycle, namely the number of students got the highest points was 97, 45 % and average value was 80, 79 %. In conclusion, demonstration and presentation are able to improve students’ study result in operating <b>spreadsheet</b> <b>type</b> and function by statistics if it was implemented appropriately in X Accounting 1 in SMKN 48 Jakarta...|$|R
40|$|This paper {{describes}} a new <b>type</b> of <b>spreadsheet</b> which mitigates the errors caused by incorrect range referencing in formulae. This spreadsheet {{is composed of}} structured worksheets called tables which contain a hierarchical organization of fields. Formulae are defined at the field-level removing the need for positional references. In addition, relationships can be defined between fields in tables, allowing data to be modeled rather than simply processed and providing a re-usable framework for authoring spreadsheets. We shall describe the key features of tables {{with an emphasis on}} error detection and avoidance. Comment: 6 Pages, 4 colour figures, Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2013, ISBN: 978 - 1 - 9054045 - 1 -...|$|R
40|$|This report {{presents}} the work conducted in Fiscal Year 1994 by the Sludge Pretreatment Chemistry Evaluation Subtask for the Tank Waste Remediation System (TWRS) Tank Waste Treatment Science Task. The {{main purpose of}} this task, is to provide the technical basis and scientific understanding to support TWRS baseline decisions and actions, such as {{the development of an}} enhanced sludge washing process to reduce the volume of waste that will require high-level waste (HLW) vitrification. One objective within the Sludge Pretreatment Chemistry Evaluation Subtask was to establish wash factors for various SST (single-shell tank) sludges. First, analytical data were compiled from existing tank waste characterization reports. These data were summarized on tank-specific worksheets that provided a uniform format for reviewing and comparing data, as well as the means to verify whether the data set for each tank was complete. Worksheets were completed for 27 SST wastes. The analytical water wash data provided tank-specific information about the fraction of each component that dissolves with water, i. e., an estimate of tank-specific wash factors for evaluating tank-by-tank processing. These wash data were then used collectively to evaluate some of the wash factors that are assumed for the overall SST waste inventory; specifically, wash factors for elements that would be found primarily in sludges. The final step in this study was to incorporate the characterization and wash factor data into a spreadsheet that provides insight into the effect of enhanced sludge washing on individual tank sludges as well as for groups of sludges that may be representative of different waste <b>types.</b> <b>Spreadsheet</b> results include the estimated mass and percentage of each element that would be removed with washing and leaching. Furthermore, estimated compositions are given of the final wash and leach streams and residual solids, in terms of both concentration and dry weight percent...|$|R
40|$|Proyecto de Graduación (Licenciatura en Ingeniería en Construcción) Instituto Tecnológico de Costa Rica, Escuela de Ingeniería en Construcción, 2015. This {{document}} {{will focus}} on providing the company AESA, a series of electronic tools, to optimize the design of various structural elements of different materials such as iron, concrete and wood. Spreadsheets were developed for the calculation of different processes of design, using the Excel program (2010 version), and in turn aid also some Visual Basic, to grant a greater versatility to the user in cases where necessary. The purpose of this project is to provide the company AESA with standard spreadsheets for some processes, and that workers use only a single <b>type</b> of <b>spreadsheet</b> for the calculation and design of some structural elements. Instituto Tecnológico de Costa Rica...|$|R
40|$|Like {{any other}} technology, {{bioenergy}} has {{a spectrum of}} advantages and disadvantages associated with it. Biofuels, for instance, are expected to reduce the {{dependence on foreign oil}} considerably, but the energy balance and carbon emission savings achieved by biofuels is still under question. However, amidst all this debate, global bioenergy development continues to take place and is being studied and investigated by scientists, engineers and government agencies worldwide. Simulation, modeling and analysis of bioenergy systems is an inherent and critical part of this process. The first part of this thesis deals with the detection and subcategorization of hard-coding errors (a prominent <b>type</b> of <b>spreadsheet</b> error) in bioenergy-relevant spreadsheets. The second part of this thesis deals with the life cycle analysis of six kinds of cropping systems for bioenergy production, including conventional and alternate; and their assessment from various standpoints of energy, net productivity, and adverse environmental impacts...|$|R
40|$|This paper {{presents}} {{a framework for}} thinking abaout the different <b>types</b> of <b>spreadsheet</b> modeling applications available for teaching economics. Spreadsheet applications are categorized by {{the degree to which}} students are involved in the spreadsheet's construction, {{and the degree to which}} students are involved in the mathematics of the model. Examples of applications from each category call attention to the high degree of flexibility that instructors have in designing spreadsheet applications, and suggest alternative ways that instructors can tailor applications to fit their specific instructional style and setting. While the costs associated with developing spreadsheet applications are significant, we believe that the costs are manageable, even for instructors who are new to spreadsheets. We conclude that the advantages imparted to students by carefully tailored spreadsheet applications are significant, and we hope to encourage the development and use of such applications. economics, teaching, computer software, spreadsheet, Microsoft Excel...|$|R
40|$|Previous {{research}} has shown that spreadsheet errors are common and are not easily detected. In this paper, an experiment was conducted to examine the rate of detection of both quantitative errors and qualitative errors in two domain-free spreadsheets. A detailed list and explanation of the <b>types</b> of common <b>spreadsheet</b> errors are presented. Results showed that the ability to detect various types of errors appears to be dependent on the type and prominence of errors as well as prior incremental practice with spreadsheet error detection. Implications of the findings are discussed. Debugging Detection of errors Qualitative errors Quantitative errors Spreadsheet errors...|$|R
40|$|Use of {{computers}} and computer networks by students and staff members of universities and colleges Scientific and non-scientific staff-members: having teaching tasks or not / faculty respondent is employed. Staff-members and students: use of word-processing software, databases, <b>spreadsheets,</b> <b>type</b> of computer language, type of statistical package / using (international) network facilities / use of ftp-archives / use of e-mail and frequency of use of e-mail / use of discussion lists, Gopher and World Wide Web - WWW / making html pages / use of computer at home / reading and consulting SURFnet publications. Background variables: basic characteristic...|$|R
40|$|With an ever {{growing amount}} of {{information}} that most activities produce today the tradition has been to use different systems to store and manage the data. Depending on its <b>type,</b> the <b>spreadsheets</b> could be stored with FTP on some file server with an obscure directory structure, and every time that new meeting protocol must {{be added to the}} homepage it's added manually, the phone numbers of every employee is written on a long forgotten paper hanging in the corridor, and so forth. Could some sort of an information infrastructure consolidate all data to be easily found, accessed and updated? An infrastructure like that must be able to handle different types of data, it must be based on common technologies for untroubled management from different applications and it must scale well as the data continues to increase. REST or Representation State Transfer is an architecture style as a set of guidelines of how HTTP was originally intended to be used. REST advocates that information should be logically divided into linked resources where each resource is identified by an URI, and where operations on the resources is performed by the methods of HTTP...|$|R
40|$|The National Institute for Occupational Safety and Health (NIOSH) has {{developed}} a computer program called the Air Quantity Estimator (AQE). The purpose {{of the program is}} to provide a starting point for estimating the air quantity needed to dilute diesel particulate matter (DPM) in an underground large opening mine, albeit taking into account various input assumptions. The AQE estimates the total air quantity needed to dilute DPM emissions that enter the main airstream of the mine to current statutory levels. It helps operators pinpoint vehicles that are high DPM contributors. In addition, it allows the user to make what if scenarios by varying the input parameters to achieve the most efficient and practical ventilation system. The AQE is a stand-alone software program that does not require any <b>type</b> of <b>spreadsheet</b> software. Engine data must be known or estimated with some accuracy and assumptions must be made in order to limit the variability of the program output. Program calculations and two case studies, for validation purposes, are included in this report. It {{should be noted that the}} program does not ensure adequate face ventilation and personal exposure compliance...|$|R
40|$|Spreadsheets {{are popular}} {{end-user}} computing applications and one reason behind their popularity {{is that they}} offer a large degree of freedom to their users regarding the way they can structure their data. However, this flexibility also makes spreadsheets difficult to understand. Textual documentation can address this issue, yet for supporting automatic generation of textual documentation, an important pre-requisite is to extract metadata inside spreadsheets. It is a challenge though, to distinguish between data and metadata {{due to the lack}} of universally accepted structural patterns in spreadsheets. Two existing approaches for automatic extraction of spreadsheet metadata were not evaluated on large datasets consisting of user inputs. Hence in this paper, we describe the collection of a large number of user responses regarding identification of spreadsheet metadata from participants of a MOOC. We describe the use of this large dataset to understand how users identify metadata in spreadsheets, and to evaluate two existing approaches of automatic metadata extraction from spreadsheets. The results provide us with directions to follow in order to improve metadata extraction approaches, obtained from insights about user perception of metadata. We also understand what <b>type</b> of <b>spreadsheet</b> patterns the existing approaches perform well and on what type poorly, and thus which problem areas to focus on in order to improve. Software EngineeringSoftware Technolog...|$|R
40|$|The {{purpose of}} this paper is to revisit the Panko-Halverson {{taxonomy}} of spreadsheet errors and suggest revisions. There are several reasons for doing so: First, the taxonomy has been widely used. Therefore, it should have scrutiny; Second, the taxonomy has not been widely available in its original form and most users refer to secondary sources. Consequently, they often equate the taxonomy with the simplified extracts used in particular experiments or field studies; Third, perhaps as a consequence, most users use only a fraction of the taxonomy. In particular, they tend not to use the taxonomy's life-cycle dimension; Fourth, the taxonomy has been tested against spreadsheets in experiments and spreadsheets in operational use. It is time to review how it has fared in these tests; Fifth, the taxonomy was based on the <b>types</b> of <b>spreadsheet</b> errors that were known to the authors in the mid- 1990 s. Subsequent experience has shown that the taxonomy needs to be extended for situations beyond those original experiences; Sixth, the omission category in the taxonomy has proven to be too narrow. Although this paper will focus on the Panko-Halverson taxonomy, this does not mean that that it is the only possible error taxonomy or even the best error taxonomy. Comment: 22 Pages, 17 Figure...|$|R
40|$|Incremental {{computations}} {{can improve}} the performance of interactive programs such as spreadsheet programs, program development environments, text editors, etc. Incremental algorithms describe how to compute a required value depending on the input, after the input has been edited. By considering the possible different edit actions on the data type lists, the basic data <b>type</b> used in <b>spreadsheet</b> programs and text editors, we define incremental algorithms on lists. Some theory {{for the construction of}} incremental algorithms is developed, and we give an incremental algorithm for a more involved example: formatting a text. CR categories and descriptors: D 11 [Software]: Programming Techniques [...] - Applicative Programming, D 43 [Software]: Programming Languages [...] - Language constructs, I 22 [Artificial Intelligence]: Automatic Programming [...] - Program transformation. General terms: algorithm, design, theory. Additional keywords and phrases: Bird-Meertens calculus for program construction, incremen [...] ...|$|R
50|$|In the USA the PCW was {{launched}} {{at a price}} of $799, and its competitors were initially the Magnavox Videowriter and Smith Corona PWP, two word processing systems whose prices also included a screen, keyboard and printer. The magazine Popular Science thought that the PCW could not compete as a general-purpose computer, because its use of non-standard 3-inch floppy disk drives and the rather old CP/M operating system would restrict the range of software available from expanding beyond the <b>spreadsheet,</b> <b>typing</b> tutor and cheque book balancing programs already on sale. However the magazine predicted that the PCW's large screen and easy-to-use word processing software {{would make it a}} formidable competitor for dedicated word processors in the home and business markets. The system was sold in the USA via major stores, business equipment shops and electronics retailers.|$|R
40|$|The {{problem of}} {{constructing}} an automated system {{for use with}} timetabling is a particularly well known one. Many programs exist for this task but perform well only in particular isolated environments. We are currently developing a general system {{able to cope with}} the ever changing requirements of large educational institutions. In this paper, we present the methods and techniques behind such a system. We present graph colouring and room allocation algorithms and show how the two can be combined together to provide the basis of a flexible and widely applicable timetabling system. We also discuss, in some detail, how several common timetabling features can be handled within the system. We intend to overcome the problems of intractability by producing a <b>spreadsheet</b> <b>type</b> system that the user can guide in an informed and useful way. This gives the user control of the search and the possibility of bactracking where no reasonable solution is found, while still letting the heuristic algorithms [...] ...|$|R
40|$|Information {{uncertainty}} {{is inherent in}} many real-world problems and adds a layer of complexity to modeling and visualization tasks. This often causes users to ignore uncertainty, {{especially when it comes}} to visualization, thereby discarding valuable knowledge. A coherent framework for the modeling and visualization of information {{uncertainty is}} needed to address this issue In this work, we have identified four major barriers to the uptake of uncertainty modeling and visualization. Firstly, there are numerous uncertainty modeling tech- niques and users are required to anticipate their uncertainty needs before building their data model. Secondly, parameters of uncertainty tend to be treated at the same level as variables making it easy to introduce avoidable errors. This causes the uncertainty technique to dictate the structure of the data model. Thirdly, propagation of uncertainty information must be manually managed. This requires user expertise, is error prone, and can be tedious. Finally, uncertainty visualization techniques tend to be developed for particular uncertainty types, making them largely incompatible with other forms of uncertainty information. This narrows the choice of visualization techniques and results in a tendency for ad hoc uncertainty visualization. The aim of this thesis is to present an integrated information uncertainty modeling and visualization environment that has the following main features: information and its uncertainty are encapsulated into atomic variables, the propagation of uncertainty is automated, and visual mappings are abstracted from the uncertainty information data <b>type.</b> <b>Spreadsheets</b> have previously been shown to be well suited as an approach to visu- alization. In this thesis, we devise a new paradigm extending the traditional spreadsheet to intrinsically support information uncertainty. Our approach is to design a framework that integrates uncertainty modeling tech- niques into a hierarchical order based on levels of detail. The uncertainty information is encapsulated and treated as a unit allowing users to think of their data model in terms of the variables instead of the uncertainty details. The system is intrinsically aware of the encapsulated uncertainty and is therefore able to automatically select appropriate uncertainty propagation methods. A user-objectives based approach to uncertainty visualization is developed to guide the visual mapping of abstracted uncertainty information. Two main abstractions of uncertainty information are explored for the purpose of visual mapping: the Unified Uncertainty Model and the Dual Uncertainty Model. The Unified Uncertainty Model provides a single view of uncertainty for visual mapping, whereas the Dual Uncertainty Model distinguishes between possibilistic and probabilistic views. Such abstractions provide a buffer between the visual mappings and the uncertainty type of the underly- ing data, enabling the user to change the uncertainty detail without causing the visual- ization to fail. Two main case studies are presented. The first case study covers exploratory and forecasting tasks in a business planning context. The second case study inves- tigates sensitivity analysis for financial decision support. Two minor case studies are also included: one to investigate the relevancy visualization objective applied to busi- ness process specifications, and the second to explore the extensibility of the system through General Purpose Graphics Processor Unit (GPGPU) use. A quantitative anal- ysis compares our approach to traditional analytical and numerical spreadsheet-based approaches. Two surveys were conducted to gain feedback on the from potential users. The significance of this work is that we reduce barriers to uncertainty modeling and visualization in three ways. Users do not need a mathematical understanding of the uncertainty modeling technique to use it; uncertainty information is easily added, changed, or removed at any stage of the process; and uncertainty visualizations can be built independently of the uncertainty modeling technique...|$|R
40|$|Despite the {{validity}} of its theory, the real options analysis has not been widely used in real estate industry. The probable reason is that it requires {{a full understanding of}} academic theory as well as advanced techniques. In order to resolve this discrepancy, many studies have been done to make the academic theory applicable and understandable for developers in the real world. During the MSRED courses at MIT, I have learned two <b>types</b> of Excel <b>spreadsheet</b> based real options models: economics based model and engineering based model. In the MSRED thesis, I am going to examine the procedure of these models and clarify the difference by comparing their results. As a goal, I hope to create a new, more practical real options model by integrating these two models. In this paper, I will examine the engineering based approach, mainly focusing on the demand projection method in the model. In recent years, the real options analysis gains more attention as an innovative decision-making method in real estate industry. However, compared with other financia...|$|R
40|$|In {{previous}} work {{we have developed}} a system that automatically checks for unit errors in spreadsheets. In this paper we describe our experiences using the system in a workshop on spreadsheet safety aimed at high school teachers and students. We present the results from a think-aloud study we conducted with five high school teachers and one high school student as the subjects. The study is the first ever to investigate the usability of a <b>type</b> system in <b>spreadsheets.</b> We discovered that end users can effectively use the system to debug a variety of errors in their spreadsheets. This result is encouraging given that type systems are difficult even for programmers. The subjects had difficulty debugging “non-local” unit errors. Guided {{by the results of}} the study we devised new methods to improve the errorlocation inference. We also extended the system to generate change suggestions for cells with unit errors, which when applied, would correct unit errors. These extensions solved the problem that the study revealed in the original system...|$|R
40|$|The {{objective}} of this {{present study was to}} develop a simple, easily understood methodology for solving biologically based models using a Microsoft Excel spreadsheet. The method involves the use of in-cell formulas in which Rows and Columns of new data are generated from data <b>typed</b> into the <b>spreadsheet,</b> but does not require any programming skills or use of the macro language. The approach involves entering the key parameter values into the spreadsheet and conducting the simulation by solving a set of equations based on these parameter values. The examples used in this paper are firstly, a simple voltage clamp simulation in which initial parameter values are used to calculate a system in steady state. The second example is a current clamp simulation where steady state is not reached and the solution of the equations for each time increment is used as the input for the next time increment in the simulation. The calculations are based on the Hodgkin Huxley mathematical equations that describe the voltage dependence of ion channel behavior. The problems and flexibility of the method are briefly discussed. The methodology developed in this present study should help novice modelers to create simple simulations without the need to learn a programmin...|$|R
40|$|We in the {{educational}} technology field like to use research findings to inform {{our understanding of how}} to design effective learning environments. These learning environments can be student centered or teacher led. This in turn leads us to look for tools that will help us and our students learn. Starting to plan instruction with a tool first is not seen as best way to plan instruction. There are a few exceptions to this rule {{one of them is a}} piece of software called Comic Life [1] which uses the familiar metaphor of a comic as the organizing feature of the software. When educational software was in its infancy, its shape and form came right from the business production model. Word processing and database programs dominated the market. The extension of this model was the electronic facilitation of writing and arithmetic. Thus, the kind of learning that happened was the same; the tools just changed. The learner or teacher were still authors of the same final product, a <b>typed</b> paper or <b>spreadsheet.</b> The irony was that often people composed with pen and paper, typed the paper into a computer, and printed the paper using a printer equipped with a typewriter ribbon and type ball. This seemed logical since these were printers were electronically driven versions of typewriters...|$|R
30|$|Consequently, {{the data}} {{collection}} mechanism {{was replaced by a}} new one built on the following abstractions: data sources, data sinks, and data sets. Data sources are just that, sources of data. A nonaggregate data source receives a single object (e.g., an agent) and returns some data value, typically but not necessarily a property of that agent. An aggregate data source receives multiple objects, and performs some aggregate operation over all of the objects. It then returns the result of that operation. For example, it might calculate the mean value of some property over all of the objects. A data sink writes the output of these data sources, for example, to a file or a chart. A data set manages a collection of sources and sinks, providing the objects on which the data sources operate and directing the results to data sinks. Each data source {{can be thought of as}} a column in a tabular <b>spreadsheet</b> <b>type</b> of format, and each row is the result of recording data from each data source associated with a data set. Crucially, a data set also provides notifications to data sinks, notifying them that a new row of data is being started, that a row has ended, and that the entire record phase for the current time period has ended. It provides similar notifications to the data sources, for example, notifying an aggregate data source that the current row has started and thus any previous aggregate values should be reset. The new code embodying these abstractions is proving to be much more flexible than the previous Log 4 J code and has provided the basis for a more simple and more efficient user interface.|$|R
40|$|Numerous {{studies and}} {{reported}} cases have established {{the seriousness of}} the frequency and impact of user-generated spreadsheet errors. This thesis presents a structured methodology for spreadsheet model development, which enables improved integrity control of the models. The proposed methodology has the potential to ensure consistency in the development process and produce more comprehensible, reliable and maintainable models, which can reduce the occurrence of user-generated errors. An insight into the nature and properties of spreadsheet errors is essential {{for the development of a}} methodology for controlling the integrity of spreadsheet models. An important by-product of the research is the development of a comprehensive classification or taxonomy of the different <b>types</b> of user-generated <b>spreadsheet</b> errors based on a rational taxonomic scheme. Research on the phenomenon of spreadsheet errors has revealed the need to adopt a software engineering based methodology as a framework for spreadsheet development in practical situations. The proposed methodology represents a new approach to the provision of a structured, software engineering based discipline for the development of spreadsheet models. It is established in this thesis that software engineering principles can in fact be applied to the process of spreadsheet model building to help improve the quality of the models. The methodology uses Jackson structures to produce the logical design of the spreadsheet model. This is followed by a technique to derive the physical model, which is then implemented as a spreadsheet. The methodology’s potential for improving the quality of spreadsheet models is demonstrated. In order to evaluate the effectiveness of the proposed framework, the various features of the proposed structured methodology are tested on a range of spreadsheet models through a series of experiments. The results of the tests provide adequate evidence of the methodology’s potential to reduce the occurrence of user-generated errors and enhance the comprehensibility of the models...|$|R
40|$|The overall {{objective}} {{of this study is}} to quantify amounts of recharge to aquifers overlain by a covering of drift and to understand the flow mechanisms by which this occurs. This requires an identification of the key Chalk and Sherwood Sandstone Drift hydrogeological environments, the understanding of recharge processes and a quantification of recharge for each of these domains. The approach adopted by this project is to investigate three aspects: 1. Identify the various Drift domains, develop conceptual models for each domain and with the help of simple <b>spreadsheet</b> <b>type</b> calculations, perform sensitivity analysis to test recharge rates for a range of parameters and boundary conditions within their range of uncertainty. 2. Determine recharge mechanisms and likely rate at field sites in East Anglia and the Tern catchment. In the Tern catchment the study is based on recharge of the Sherwood Sandstone through a thin drift cover. The East Anglia field site has been established at a site where the Chalk is overlain by thick till. The study here will be largely based on water quality data including geochemical residence time indicators and will include the following objectives: • to ascertain whether recharge occurs through the till and to quantify rates. • to establish the degree of importance of (a) fissure-related bypass flow, and (b) flow through higher permeability sand lenses. • to determine the residence times of the various groundwater components in the till and underlying Chalk. • to determine the extent of denitrification within the till. Calculate catchment scale recharge rates using regional Environment Agency data (groundwater levels, hydrological and water quality) and working closely with the Environment Agency. Review recharge models associated with regional groundwater studies in the Environment Agency...|$|R
40|$|This paper {{presents}} the course "Doing Economics with the Computer" we taught since 1999 at the University of Bern, Switzerland. "Doing Economics with the Computer" is a course we designed to introduce sophomores playfully and painlessly into computational economics. Computational methods are usually used in economics to analyze complex problems, which are impossible (or very difficult) to solve analytically. However, our course only looks at economic models, which can (easily) be solved analytically. This approach has two advantages: First, relying on economic theory students have met {{in their first}} year, we can introduce numerical methods at an early stage. This stimulates students to use computational methods later in their academic career when they encounter difficult problems. Second, the confrontation with the analytical analysis shows convincingly both power and limits of numerical methods. Our course introduces students to three <b>types</b> of software: <b>spreadsheet</b> and simple optimizer (Excel with Solver), numerical computation (Matlab) and symbolic computation (Maple). The course consists of 10 sessions, we taught each in a 3 -hour lecture. In the 1 st part of each session we present the economic problem, sometimes its analytical solution and introduce the software used. The 2 nd part, in the computer lab, starts the numerical implementation with step-by-step guidance. In this part, students work on exercises with clearly defined questions and precise guidance for their implementation. The 3 rd part is a workshop, where students work in groups on exercises with still very clear defined questions but no help on their implementation. This part teaches students how to practically handle numerical questions in a well-defined framework. The 4 th part of a session is a graded take home assignment where {{students are asked to}} answer general economic questions. This part teaches students how to translate general economic questions into a numerical task and back into an economically meaningful answer. A short debriefing in the following week is part 5 and completes each session...|$|R
