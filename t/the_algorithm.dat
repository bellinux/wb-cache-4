10000|10000|Public
5|$|If {{there is}} a clause both of whose {{variables}} are already set, {{in a way that}} falsifies the clause, then <b>the</b> <b>algorithm</b> backtracks to its most recent choice point, undoing the assignments it made since that choice, and reverses the decision made at that choice. If there is no choice point, or if <b>the</b> <b>algorithm</b> has already backtracked over the most recent choice point, then it aborts the search and reports that the input 2-CNF formula is unsatisfiable.|$|E
5|$|In {{computer}} science, Putnam {{is known}} for the Davis–Putnam algorithm for the Boolean satisfiability problem (SAT), developed with Martin Davis in 1960. <b>The</b> <b>algorithm</b> finds {{if there is a}} set of true or false values that satisfies a given Boolean expression so that the entire expression becomes true. In 1962, they further refined <b>the</b> <b>algorithm</b> with the help of George Logemann and Donald W. Loveland. It became known as the DPLL algorithm. This algorithm is efficient and still forms the basis of most complete SAT solvers.|$|E
5|$|In {{the uniform}} cost model (suitable for {{analyzing}} {{the complexity of}} gcd calculation on numbers that fit into a single machine word), each step of <b>the</b> <b>algorithm</b> takes constant time, and Lamé's analysis implies that the total running time is also O(h). However, in a model of computation suitable for computation with larger numbers, the computational expense of a single remainder computation in <b>the</b> <b>algorithm</b> can be as large as O(h2). In this case the total time {{for all of the}} steps of <b>the</b> <b>algorithm</b> can be analyzed using a telescoping series, showing that it is also O(h2). Modern algorithmic techniques based on the Schönhage–Strassen algorithm for fast integer multiplication can be used to speed this up, leading to quasilinear algorithms for the GCD.|$|E
30|$|<b>The</b> {{iteration}} <b>algorithms</b> {{are usually}} {{divided into two}} kinds. One is <b>the</b> <b>algorithms</b> with weak convergence, such as <b>the</b> Mann iteration <b>algorithm</b> [15] and <b>the</b> Ishikawa iteration <b>algorithm</b> [16]. <b>The</b> other is <b>the</b> <b>algorithms</b> with strong convergence, such as <b>the</b> Halpern iteration <b>algorithm</b> [17], hybrid <b>algorithms</b> [18], and <b>the</b> shrinking projection <b>algorithm</b> [19].|$|R
50|$|Assume {{that the}} problem is a binary {{decision}} problem. To construct <b>the</b> compound <b>algorithm,</b> a positive weight is given to each of <b>the</b> <b>algorithms</b> in <b>the</b> pool. <b>The</b> compound <b>algorithm</b> then collects weighted votes from all <b>the</b> <b>algorithms</b> in <b>the</b> pool, and gives the prediction that has a higher vote. If <b>the</b> compound <b>algorithm</b> makes a mistake, <b>the</b> <b>algorithms</b> in <b>the</b> pool that contributed to the wrong predicting will be discounted by a certain ratio β where 0<β<1.|$|R
40|$|In the paper, {{we propose}} Tabu Search (TS) based schemes for image {{segmentation}} using Markov Random Field (MRF) model. The segmentation problem is formulated as pixel labeling {{problem and the}} MAP estimates of the labels were obtained by the two proposed TS <b>algorithms.</b> <b>The</b> TS <b>algorithm</b> was parallelized to improve the overall performance of the scheme. The performance of <b>the</b> <b>algorithms</b> was compared with Simulated Annealing (SA) <b>algorithm</b> and <b>the</b> <b>algorithms</b> outperformed <b>the</b> SA <b>algorithm.</b> <b>The</b> <b>algorithms</b> were tested for synthetic as well as real images. 1...|$|R
5|$|Intuitively, <b>the</b> <b>algorithm</b> follows all {{chains of}} {{inference}} after making {{each of its}} choices. This either leads to a contradiction and a backtracking step, or, if no contradiction is derived, {{it follows that the}} choice was a correct one that leads to a satisfying assignment. Therefore, <b>the</b> <b>algorithm</b> either correctly finds a satisfying assignment or it correctly determines that the input is unsatisfiable.|$|E
5|$|Noisy {{binary search}} {{algorithms}} solve {{the case where}} <b>the</b> <b>algorithm</b> cannot reliably compare elements of the array. For each pair of elements, {{there is a certain}} probability that <b>the</b> <b>algorithm</b> makes the wrong comparison. Noisy binary search can find the correct position of the target with a given probability that controls the reliability of the yielded position. The noisy binary search problem can be considered as a case of the Rényi-Ulam game, which Alfréd Rényi introduced in 1961.|$|E
5|$|<b>The</b> <b>algorithm</b> was {{probably}} not discovered by Euclid, who compiled results from earlier mathematicians in his Elements. The mathematician and historian B. L. van der Waerden suggests that Book VII derives from a textbook on number theory written by mathematicians {{in the school of}} Pythagoras. <b>The</b> <b>algorithm</b> {{was probably}} known by Eudoxus of Cnidus (about 375 BC). <b>The</b> <b>algorithm</b> may even pre-date Eudoxus, judging from the use of the technical term ἀνθυφαίρεσις (anthyphairesis, reciprocal subtraction) in works by Euclid and Aristotle.|$|E
40|$|<b>The</b> {{perceptron}} <b>algorithm,</b> {{introduced in}} <b>the</b> late fifties {{in the machine}} learning community, is a simple greedy algorithm for finding a solution to a finite set of linear inequalities. <b>The</b> <b>algorithm’s</b> main advantages are its simplicity and noise tolerance. <b>The</b> <b>algorithm’s</b> main disadvantage is its slow convergence rate. We propose {{a modified version of}} <b>the</b> perceptron <b>algorithm</b> that retains <b>the</b> <b>algorithm’s</b> original simplicity but has a substantially improved convergence rate. perceptron algorithm, smoothing technique, condition num-Key words: ber...|$|R
40|$|Integer Programming {{problems}} {{are difficult to}} solve. The goal is to find an optimal solution that minimizes cost. With the help of Groebner based <b>algorithms</b> <b>the</b> optimal solution can be found if it exists. The application of <b>the</b> Groebner based <b>algorithm</b> and how it works is the topic of research. <b>The</b> <b>Algorithms</b> are <b>The</b> Conti-Traverso <b>Algorithm</b> and <b>the</b> Original Conti-Traverso <b>Algorithm.</b> Examples are given as well as proofs that correspond to <b>the</b> <b>algorithms.</b> <b>The</b> latter <b>algorithm</b> is more efficient as well as user friendly. <b>The</b> <b>algorithms</b> {{are not necessarily the}} best way to solve and integer programming problem, but they do find the optimal solution if it exists...|$|R
30|$|Two {{critical}} point aware data acquisition algorithms are proposed based on numerical analysis [27] and Lagrange interpolation [28] techniques. <b>The</b> <b>algorithms</b> can adjust <b>the</b> sampling {{frequency of the}} sensors automatically according to variation of the physical world. The correctness of <b>the</b> <b>algorithms</b> is proved and the complexities of <b>the</b> <b>algorithms</b> are analyzed.|$|R
5|$|The {{main idea}} of <b>the</b> <b>algorithm</b> {{is to find}} pairs of {{clusters}} to merge by following paths in the nearest neighbor graph of the clusters. Every such path will eventually terminate at a pair of clusters that are nearest neighbors of each other, and <b>the</b> <b>algorithm</b> chooses that pair of clusters as the pair to merge. In order to save work by re-using {{as much as possible}} of each path, <b>the</b> <b>algorithm</b> uses a stack data structure to keep track of each path that it follows. By following paths in this way, the nearest-neighbor chain algorithm merges its clusters in a different order than methods that always find and merge the closest pair of clusters. However, despite that difference, it always generates the same hierarchy of clusters.|$|E
5|$|Using this recursion, Bézout's {{integers}} s and t {{are given}} by s = s'N and t = t'N, where N+1 is the step on which <b>the</b> <b>algorithm</b> terminates with r'N+1 = 0.|$|E
25|$|The {{iteration}} of {{the inner}} loop of <b>the</b> <b>algorithm</b> for v=4 makes a recursive call to <b>the</b> <b>algorithm</b> with R={4}, P={3,5,6}, and X=Ø (although vertex 2 belongs to the set X in the outer call to <b>the</b> <b>algorithm,</b> {{it is not a}} neighbor of v and is excluded from the subset of X passed to the recursive call). This recursive call will end up making three second-level recursive calls to <b>the</b> <b>algorithm</b> that report the three cliques {3,4}, {4,5}, and {4,6}. Then, vertex 4 is added to X and removed from P.|$|E
30|$|In this subsection, we {{describe}} <b>algorithms</b> for estimating <b>the</b> {{position of a}} user with a code sequence s, where <b>the</b> <b>algorithms</b> have different levels of accuracy and numerical complexity. <b>The</b> <b>algorithms</b> are derived for a single-user scenario; however, if the code sequences of the other users are orthogonal to s, <b>the</b> <b>algorithms</b> can also be applied in multi-user settings. If the sequences are not orthogonal and the users are sufficiently separated {{from each other in}} space, <b>the</b> <b>algorithms</b> should still work well.|$|R
30|$|According to the {{analysis}} results given in related references, for calculating the Euler number of an M × N-size binary image, <b>the</b> skeleton-based <b>algorithm</b> will take about 8  M × N pixel accesses (Diaz-de-Leon and Sossa-Azuela 1996), <b>the</b> GRAY <b>algorithm</b> will take 4  M × N pixel accesses, <b>the</b> RUN <b>algorithm</b> will take about 4  M × N pixel accesses in the worst case, and about 3  M × N pixel accesses in average (Bishnu et al. 2005). Moreover, <b>the</b> HCS <b>algorithm</b> will take 2.375  M × N pixel accesses in average (He et al. 2013). Taking advantage of the information obtained during processing the previous bit-quad, <b>the</b> I-GRAY <b>algorithm</b> will only take 2  M × N pixel accesses (Yao et al. 2014). Therefore, <b>the</b> I-GRAY <b>algorithm</b> is better than <b>the</b> skeleton-based <b>algorithm,</b> <b>the</b> GRAY <b>algorithm,</b> <b>the</b> RUN <b>algorithm,</b> and <b>the</b> HCS <b>algorithm.</b>|$|R
30|$|<b>The</b> four <b>algorithms</b> {{included}} in <b>the</b> study are focused either on safety or efficiency. Hence, to evaluate how {{the objectives of}} <b>the</b> <b>algorithms</b> affect <b>the</b> results both safety and efficiency indicators have been included. None of <b>the</b> <b>algorithms</b> have as objective to reduce environmental impacts. Nonetheless, {{it is interesting to}} investigate how <b>the</b> <b>algorithms</b> perform in that respect. Therefore, a simple investigation of the environmental impacts is included.|$|R
25|$|Now, {{the array}} is already sorted, but <b>the</b> <b>algorithm</b> {{does not know}} if it is completed. <b>The</b> <b>algorithm</b> needs one whole pass without any swap to know it is sorted.|$|E
25|$|One {{can show}} that {{allowing}} a single postselection step {{at the end of}} <b>the</b> <b>algorithm</b> (as described above) or allowing intermediate postselection steps during <b>the</b> <b>algorithm</b> are equivalent.|$|E
25|$|If {{the pivot}} is chosen to {{minimize}} the number of recursive calls made by <b>the</b> <b>algorithm,</b> the savings in running time compared to the non-pivoting version of <b>the</b> <b>algorithm</b> can be significant.|$|E
40|$|This project {{presents}} some randomized and deterministic algorithms {{of number}} primality testing, and, {{for some of}} them, their implementation in C++. <b>The</b> <b>algorithms</b> studied here are <b>the</b> naive <b>algorithm</b> (deterministic), <b>the</b> Miller-Rabin <b>algorithm</b> (randomized), <b>the</b> Fermat <b>algorithm</b> (randomized), <b>the</b> Solovay-Strassen <b>algorithm</b> (randomized) and <b>the</b> AKS <b>algorithm</b> (deterministic). These algorithms are presented with the number theory {{they need to be}} understood and with some proofs of the theorems they use...|$|R
40|$|In this paper, {{we present}} two new {{stochastic}} approximation <b>algorithms</b> for <b>the</b> problem of quantile estimation. <b>The</b> <b>algorithms</b> uses <b>the</b> {{characterization of the}} quantile provided {{in terms of an}} optimization problem in 1]. <b>The</b> <b>algorithms</b> take <b>the</b> shape of a stochastic gradient descent which minimizes the optimization problem. Asymptotic convergence of <b>the</b> <b>algorithms</b> to <b>the</b> true quantile is proven using the ODE method. The theoretical results are also supplemented through empirical evidence. <b>The</b> <b>algorithms</b> are shown to provide significant improvement in terms of memory requirement and accuracy...|$|R
40|$|Comparative {{tests were}} {{performed}} on seven signature extension algorithms to evaluate their effectiveness in correcting for changes in atmospheric haze and sun angle in a Landsat scene. Four of <b>the</b> <b>algorithms</b> were cluster matching, and two were maximum likelihood <b>algorithms.</b> <b>The</b> seventh <b>algorithm</b> determined <b>the</b> haze level in both training and recognition segments and used a set of tables calculated from an atmospheric model to determine the affine transformation that corrects the training signatures for changes in sun angle and haze level. Three of <b>the</b> <b>algorithms</b> were tested on a simulated data set, and all of <b>the</b> <b>algorithms</b> were tested on consecutive-day data. The classification performance on the data sets using <b>the</b> <b>algorithms</b> is presented, along with results of statistical tests on the accuracy and proportion estimates. <b>The</b> three <b>algorithms</b> tested on <b>the</b> simulated data produced significant improvements over the results obtained using untransformed signatures. For the consecutive-day data, <b>the</b> tested <b>algorithms</b> produced improvements in most but not all cases. The tests indicated also that {{no statistically significant differences}} were noted among <b>the</b> <b>algorithms...</b>|$|R
25|$|<b>The</b> <b>algorithm</b> below {{uses the}} former strategy.|$|E
25|$|<b>The</b> <b>algorithm</b> is {{composed}} of two parts. The first part of <b>the</b> <b>algorithm</b> turns the factoring problem into the problem of finding the period of a function, and may be implemented classically. The second part finds the period using the quantum Fourier transform, and {{is responsible for the}} quantum speedup.|$|E
25|$|<b>The</b> <b>algorithm</b> may be {{expressed}} in the following pseudocode.|$|E
40|$|Presents a {{new class}} of {{algorithm}} for penalized-likelihood reconstruction of attenuation maps from low-count transmission scans. The authors derive <b>the</b> <b>algorithms</b> by applying to the transmission log-likelihood a variation of the convexity technique developed by De Pierro for the emission case. <b>The</b> new <b>algorithms</b> overcome several limitations associated with previous algorithms. (1) Fewer exponentiations are required than in <b>the</b> transmission EM <b>algorithm</b> or in coordinate-ascent <b>algorithms.</b> (2) <b>The</b> <b>algorithms</b> intrinsically accommodate nonnegativity constraints, unlike many gradient-based methods. (3) <b>The</b> <b>algorithms</b> are easily parallelizable, unlike coordinate-ascent algorithms and perhaps line-search <b>algorithms.</b> <b>The</b> authors show that <b>the</b> <b>algorithms</b> converge faster than several alternatives, even on conventional workstations. They give examples from low-count PET transmission scans and from truncated fan-beam SPECT transmission scans...|$|R
30|$|The {{communication}} overhead of <b>the</b> LAEP <b>algorithm</b> is O(A N) because flooding {{is implemented}} {{only once in}} <b>the</b> LAEP <b>algorithm.</b> Although <b>the</b> proposed <b>algorithm</b> has a twofold communication overhead of <b>the</b> LAEP <b>algorithm,</b> its overhead is insignificant compared to <b>the</b> other <b>algorithms.</b>|$|R
40|$|A {{family of}} {{hierarchical}} algorithms for nonlinear structural equations are presented. <b>The</b> <b>algorithms</b> {{are based on}} the Davidenko-Branin type homotopy and shown to yield consistent hierarchical perturbation equations. <b>The</b> <b>algorithms</b> appear to be particularly suitable to problems involving bifurcation and limit point calculations. An important by-product of <b>the</b> <b>algorithms</b> is that it provides a systematic and economical means for computing the stepsize at each iteration stage when a Newton-like method is employed to solve the systems of equations. Some sample problems are provided to illustrate the characteristics of <b>the</b> <b>algorithms...</b>|$|R
25|$|Time {{complexity}} {{is commonly}} estimated by counting {{the number of}} elementary operations performed by <b>the</b> <b>algorithm,</b> where an elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by <b>the</b> <b>algorithm</b> differ by at most a constant factor.|$|E
25|$|<b>The</b> <b>algorithm</b> is run in phases. Each phase {{consists}} of the following steps.|$|E
25|$|In the {{original}} paper, <b>the</b> <b>algorithm</b> is presented more formally using a substitution style instead {{of side effects}} in the method above. In the latter form, the side effect invisibly takes care of all places where a type variable is used. Explicitly using substitutions not only makes <b>the</b> <b>algorithm</b> hard to read, because the side effect occurs virtually everywhere, but also gives the false impression that the method might be costly. When implemented using purely functional means or {{for the purpose of}} proving <b>the</b> <b>algorithm</b> to be basically equivalent to the deduction system, full explicitness is of course needed and {{the original}} formulation a necessary refinement.|$|E
30|$|It {{is worth}} {{mentioning}} from Tables  2 and 3 that the computational time of <b>the</b> proposed <b>algorithm</b> increases with n and p increasing, {{but not as}} sharply as <b>the</b> <b>algorithms</b> in [19, 21]. For example, in Table  2, the instances cannot be solved by <b>the</b> <b>algorithms</b> in [19, 21] within two hours when p≥ 6 and p≥ 7, respectively, while <b>the</b> presented <b>algorithm</b> can solve all instances with p increasing 2 to 10 {{in less than two}} hours. This {{is due to the fact}} that the main computational cost of <b>the</b> <b>algorithms</b> ([19, 21] and ours) is the solution of linear feasibility problems at the interesting grid points. That is to say, the computational time for solving this kind of problems is directly affected by the number of interesting grid points. We notice that for <b>the</b> <b>algorithms</b> in [19, 21], the number of iterations and interesting grid points checked at each iteration increases with p increasing. However, for <b>the</b> proposed <b>algorithm,</b> p is related to the number of iterations (see Step (k 2) in <b>the</b> proposed <b>algorithm)</b> and independent of the number of interesting grid points checked at each iteration (see Step (k 1) in <b>the</b> proposed <b>algorithm).</b> This means that <b>the</b> proposed <b>algorithm</b> requires fewer interesting grid points considered and less computational time than the ones of <b>the</b> <b>algorithms</b> in [19, 21] for solving this kind of random problems. Moreover, from Table  3, notice that <b>the</b> <b>algorithms</b> in [19, 21] cannot solve the instances within two hours when n≥ 150 and p= 4, but all instances selected can be solved by <b>the</b> proposed <b>algorithm</b> within no more than two hours. This is mainly because the more interesting grid points are considered, the more the feasibility of linear programs with n variables should be checked. On the other hand, note that the interesting grid points considered by <b>the</b> proposed <b>algorithm</b> are much fewer than the ones considered by <b>the</b> <b>algorithms</b> [19, 21]. And so the increase of the computational time of <b>the</b> proposed <b>algorithm</b> is not as sharp as <b>the</b> <b>algorithms</b> [19, 21] with n increasing.|$|R
30|$|Next, we {{describe}} the experimental setting: systems evaluated, evaluation measures, how <b>the</b> <b>algorithms</b> were implemented and configured, and the quality indicators used to compare <b>the</b> <b>algorithms</b> used in MSBA.|$|R
40|$|We present constraint-based {{optimization}} algorithms and a constraintbased approximation <b>algorithm</b> for <b>the</b> job-shop scheduling problem. An empirical {{performance analysis}} shows that both <b>the</b> optimization <b>algorithms</b> and <b>the</b> approximation <b>algorithm</b> perform well. Especially <b>the</b> approximation <b>algorithm</b> is among <b>the</b> best <b>algorithms</b> known to date. We, furthermore, {{show that we}} can improve the performance of <b>the</b> optimization <b>algorithms</b> by combining them with <b>the</b> approximation <b>algorithm...</b>|$|R
