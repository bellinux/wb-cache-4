30|80|Public
50|$|The company offered <b>typical</b> <b>prediction</b> markets, such as {{betting on}} the {{outcomes}} of political races. It attracted some attention for featuring a bet on the Boeing 787 schedule slipping past the announced date.|$|E
5000|$|The basic algorithm, Winnow1, is as follows. The {{instance}} space is , that is, each instance {{is described as}} a set of Boolean-valued features. The algorithm maintains non-negative weights [...] for , which are initially set to 1, one weight for each feature. When the learner is given an example , it applies the <b>typical</b> <b>prediction</b> rule for linear classifiers: ...|$|E
50|$|Thus {{this model}} predicts a global warming of ΔTs = 1.2 K for a {{doubling}} of carbon dioxide. A <b>typical</b> <b>prediction</b> from a GCM is 3 K surface warming, primarily because the GCM allows for positive feedback, notably from increased water vapor. A simple surrogate for including this feedback process is to posit an additional increase of Δε=.02, for a total Δε=.04, to approximate {{the effect of the}} increase in water vapor that would be associated with an increase in temperature. This idealized model then predicts a global warming of ΔTs = 2.4 K for {{a doubling of}} carbon dioxide, roughly consistent with the IPCC.|$|E
40|$|The {{effectiveness}} of the Froelich-Raimes model in treating force constants and alloy behavior is discussed. <b>Typical</b> <b>predictions</b> of trends include the deviation of Vegard's Law and the variation of the bulk modulus with pressure and with alloying elements. The simple model of Koskimaki and Waber using the linear combination of the density of states led to several useful predictions for titanium based alloys...|$|R
40|$|We present {{experimental}} {{evidence that the}} motion of point defects in thermal convection patterns in an inclined fluid layer is well described by Tsallis statistics with an entropic index q ≈ 1. 5. The dynamical properties of the defects (anomalous diffusion, shape of velocity distributions, power-law decay of correlations) are in good agreement with <b>typical</b> <b>predictions</b> of nonextensive models, over a range of driving parameters...|$|R
50|$|Supersymmetric {{extensions}} to the Standard Model, {{such as the}} Minimal Supersymmetric Standard Model, generally lead to a large CP-violation. <b>Typical</b> <b>predictions</b> for {{the neutron}} EDM arising from the theory range between 10−25 e⋅cm and 10−28 e⋅cm. As {{in the case of}} the strong interaction, the limit on the neutron EDM is already constraining the CP violating phases. The fine-tuning is, however, not as severe yet.|$|R
50|$|For the {{simplest}} AH2 molecular system, Walsh produced the first angular correlation diagram by plotting the ab initio orbital energy curves for the canonical molecular orbitals while changing the bond angle from 90° to 180°. As the bond angle is distorted, the energy {{for each of}} the orbitals can be followed along the lines, allowing a quick approximation of molecular energy as a function of conformation. It is still unclear whether or not the Walsh ordinate considers nuclear repulsion, and this remains a topic of debate. A <b>typical</b> <b>prediction</b> result for water is a bond angle of 90°, which is not even close to the experimental derived value of 104°. At best the method is able to differentiate between a bent and linear molecule.|$|E
40|$|Prediction Markets {{have become}} {{efficient}} tools for extracting and aggregating the local private information detained by different individual agents. They function like normal markets where the security that is traded {{depends on the}} realization of a specific future event. <b>Typical</b> <b>prediction</b> markets trade securitie...|$|E
30|$|Several studies use ANN as {{prediction}} model [38, 140, 150]. Although ANN represents a universal approximation, but {{still have the}} drawbacks of in choosing a suitable algorithm, network structure, and initial condition. For butter performance, ANN may be combined with the <b>typical</b> <b>prediction</b> methods such as Sliding Window Method (SWM) [85], Auto-regression model [39], and Fuzzy System (FS) [23, 39, 144].|$|E
40|$|Color {{transparency}} (CT) is {{an effect}} of suppression of nuclear shadowing of hard reactions, {{closely related to the}} color screening. A brief review of theoretical development and experimental search for CT, failed and successful, are presented. A special emphasis is made on a quantum-mechanical nature of CT, as opposed to a wide spread erroneousclassical treatment of this phenomenon. The <b>typical</b> <b>predictions</b> of the classical approach, all contradicting quantum mechanics are: - factorization of cross section of hard reactions on a nucleus;...|$|R
40|$|Pulsed single-frequency CO 2 laser {{oscillators}} {{are often}} used as transmitters for coherent lidar applications. These oscillators suffer from intrapulse chirp, or dynamic frequency shifting. If excessive, such chirp can limit the signal-to-noise ratio of the lidar (by generating excess bandwidth), or limit the velocity resolution if the lidar is of the Doppler type. This paper describes a detailed numerical model that considers all known sources of intrapulse chirp. Some <b>typical</b> <b>predictions</b> of the model are shown, and simple design rules to minimize chirp are proposed...|$|R
40|$|Color {{transparency}} (CT) is {{an effect}} of suppression of nuclear shadowing of hard reactions, {{closely related to the}} color screening. A brief review of theoretical development and experimental search for CT, failed and successful, are presented. A special emphasis is made on a quantum-mechanical nature of CT, as opposed to a wide spread erroneousclassical treatment of this phenomenon. The <b>typical</b> <b>predictions</b> of the classical approach, all contradicting quantum mechanics are:- factorization of cross section of hard reactions on a nucleus;- ”nuclear transparency”, a normalized ratio of nuclear to nucleon cross sections, cannot exceed one;- the larger is a radius of a hadron, the stronger it attenuates in a nucleus;- the higher is the energy of hadrons participating in a hard reaction, the less is the nuclear attenuation;- due to CT hard processes provide a better opportunity to study Fermi-momentum distribution, than soft reactions; etc...|$|R
40|$|This {{study is}} devoted to {{modeling}} dynamical nonlinear non-stationary systems, whose uncertainty lies in a priori known bounds. The convergence and computational properties of a new method for identifying models of such systems is studied. The method provides specified prediction accuracy for dynamic processes {{within the bounds of}} non-stationary and measurement errors. A research for a class of chemical-engineering systems, which are described by models of three levels of difficulty and for <b>typical</b> <b>prediction</b> errors (15 % - 30 %) is conducted...|$|E
40|$|Within the {{composite}} t(R) t(L) BAR {{model for the}} doublet Higgs field in the Weinberg-Salam theory, a generalization involving a complex, triplet Higgs-type field discussed originally by Ross and Veltman, emerges naturally as the effective lagrangian describing the Higgs sector at low energies. The new, triplet Higgs field {{can be interpreted as}} a t(R) t(L) t(R) t(L) BAR composite. The parameters in the model are, however, almost completely determined by the infrared fixed point of the renormalization group equations. Given the <b>typical</b> <b>prediction</b> for the top quark mass, m(t) = 220 - 230 GeV, the most recent LEP data leads to a ratio of the vacuum expectation values of the Higgs triplet versus doublet of the magnitude 0. 07 +/- 0. 02...|$|E
40|$|We examine loop-mediated {{effects of}} new heavy quarks Q=(t',b') on t tbar {{production}} at hadron colliders, using a phenomenological model with flavor off-diagonal couplings of charged and neutral scalars phi=(phi^+-,phi^ 0) to Q. We show that an invariant-mass-dependent asymmetry, in the t tbar center of mass, {{consistent with those}} recently reported by the CDF collaboration can be obtained for quark masses around 350 - 500 GeV, scalar masses of order 100 - 200 GeV, and modest to strong Yukawa couplings. The requisite strong interactions suggest a non-perturbative electroweak symmetry breaking mechanism and composite states at the weak scale. A <b>typical</b> <b>prediction</b> of this framework {{is that the new}} heavy quarks decay dominantly into t phi final states. Comment: 6 pages 6 figures; version published in Physical Review...|$|E
40|$|The nonadditive entropy Sq {{has been}} {{introduced}} in 1988 focusing on a generalization of Boltzmann–Gibbs (BG) statistical mechanics. The aim was to cover a (possibly wide) class of systems among those very many which violate hypothesis such as ergodicity, under which the BG theory {{is expected to be}} valid. It is now known that Sq has a large applicability; more specifically speaking, even outside Hamiltonian systems and their thermodynamical approach. In the present paper we review and comment some relevant aspects of this entropy, namely (i) Additivity versus extensivity; (ii) Probability distributions that constitute attractors in the sense of Central Limit Theorems; (iii) The analysis of paradigmatic low-dimensional nonlinear dynamical systems near the edge of chaos; and (iv) The analysis of paradigmatic long-range-interacting many-body classical Hamiltonian systems. Finally, we exhibit recent as well as <b>typical</b> <b>predictions,</b> verifications and applications of these concepts in natural, artificial, and social systems, as shown through theoretical, experimental, observational and computational results...|$|R
40|$|The PVLAS {{collaboration}} {{has announced}} recently the observation dichroism and birefringence of vacuum permeated by a intense magnetic field. Both are <b>typical</b> <b>predictions</b> {{of the existence}} of light particles that, as the QCD axion, interact with the electromagnetic field. The strength of the interaction as inferred from the PVLAS measurements is however strongly excluded by astrophysical arguments and axion Helioscopes since these particles, as neutrinos, would be copiously produced in the interior of the Sun. In this Thesis several refined models are presented where the astrophysical bounds are severely relaxed, allowing the PVLAS particle interpretation. All these models involve the existence of further new particles, in particular paraphotons and millicharged particles. Also a model-independent study is performed, showing that the reconciliation of the PVLAS particle interpretation implies new low energy physics, usually accessible to precision experiments. Finally, if the PVLAS particle would be parity even, the strongest exclusion bounds would come, not from astrophysics, but from experiments searching for 5 th type forces. Comment: Ph. D. Thesis (Advisor: Eduard Masso) 115 pages, 47 figure...|$|R
3000|$|... {{highlighting}} {{the use of}} self-histories for destination prediction, rather than group-histories (as in <b>typical</b> clustering style <b>prediction</b> approaches), i.e., we predict {{based on what the}} person typically does, rather than what people typically do, [...]...|$|R
40|$|It {{has been}} shown that {{transcription}} factors can {{play a significant role in}} gene expression. The ability to predict the location and clustering of binding sites for transcription factors, and later the confirmation of such sites, can play a pivotal role in medical research. <b>Typical</b> <b>prediction</b> methods produce a high level of false positives, sometimes in the area of 70 % of the hits. These datasets are typically large (approx. 4000 hits for a 1000 bp sequence), and verifying each hit individually can be expensive, and time consuming. TFBS viewer is a display tool to view and navigate these typically large sequences, but more importantly, the viewer provides several tools for comparison with itself and related species. The efficient speed and comparison capabilities of the viewer make it a good candidate for binding site searches...|$|E
30|$|The {{hierarchical}} B-picture (HBP) {{is another}} prediction architecture. It {{is based on}} the IBBP coding structure, which is inspired from the <b>typical</b> <b>prediction</b> architecture of the multi-view coding standard as depicted in Fig.  1 b. This well-known prediction architecture provides efficient coding since it allows inter-picture prediction from all directions for frames, which belong to the odd views. This architecture was used in the context of MSR-MVC to propose a low complexity motion compensation algorithm [15]. Other studies have used this prediction architecture to study the effect of using different inter-view prediction directions (by using full spatial-resolution and low spatial-resolution frames in the base view) upon the coding efficiency of multi-view coding, and to propose different decimation methods for full spatial-resolution frames and to explore the down-sample threshold where suppression theory is valid [16 – 18].|$|E
40|$|Abstract—With the {{motivation}} of using more information to update the parameter estimates to achieve improved tracking performance, composite adaptation that uses both the system tracking errors and a prediction error containing parametric information to drive the update laws, has become widespread in adaptive control literature. However, despite its obvious ben-efits, composite adaptation has not been widely implemented in neural net-work-based control, primarily due to the neural network (NN) reconstruc-tion error that destroys a <b>typical</b> <b>prediction</b> error formulation required for the composite adaptation. This technical note presents a novel approach to design a composite adaptation law for NNs by devising an innovative swap-ping procedure that uses the recently developed robust integral of {{the sign of the}} error (RISE) feedback method. Semi-global asymptotic tracking is proven for a Euler-Lagrange system. Experimental results are provided to illustrate the concept. I...|$|E
40|$|We {{report a}} {{preliminary}} {{search for a}} positively charged lepton (Y+) coupled directly to the μ- and νμ (i. e., muon number = + 1). Using the production process νμ+N→Y++anything, we have looked for the μ+ from the decay Y+→μ++νμ+νμ. The expected number of μ+ events is given {{as a function of}} the heavy-lepton mass, the branching fraction to muons, and the heavy-lepton coupling. For <b>typical</b> gauge-theory <b>predictions,</b> we obtain the 90 % confidence limit MY> 2 GeV/c^ 2...|$|R
40|$|ABSTRACT: Research {{is being}} done to study the details and {{progress}} of soil erosion on levees and dams, including the formation and progression of rills and gullies on the slopes, and eventually to final breaching. These detailed observations of erosion differ from the <b>typical</b> <b>predictions</b> of only the maximum erosion or scour depths, for example around submerged bridge piers. Computer simulations and geotechnical centrifuge modelling will, in the future, be validated using these observations. For testing, single layer sand models were utilized, and will be followed by clayey and mixed soils, and increased number of layers. The computer simulations will incorporate 3 -D Navier-Stokes fluid simulations, and a novel segmented height field extended to allow soil undercuts was developed. The primary intent of the research is to study small-scale erosion on earthen embankments and, ultimately, develop novel and robust erosion software, validated by physical modelling. Figure 2 : Rills form in an earthen embankment. Figure 1 : A levee that was overtopped for several hours during the Katrina hurricane but did not fail. The dramatic gouging/scooping on the lower portions of the levee is due to increased water flow {{at the base of the}} levee and the nonhomogeneous nature of the embankment. ...|$|R
40|$|The Paper {{presents}} a constitutive model for describing the stress-strain behaviour of partially saturated soils. The model is formulated {{within the framework}} of hardening plasticity using two imdependent sets of stress variables: the excess of total stress over air pressure and the suction. The model is able to represent, in a consistent and unified manner, many of the fundamental features of the behaviour of partially saturated soils which had been treated separately by previously proposed models. On reaching saturation, the model becomes a conventional critical state model. Because experimental evidence is still limited, the model has been kept as simple as possible in order to provide a basic framework from which extensions are possible. Tbe mode 1 is intended for partially saturated soils which are slightly or moderately expansive. After formulating the model for isotropic and biaxial stress states, <b>typical</b> <b>predictions</b> are described and compared, in a qualitative way, with characteristic trends of the behaviour of partially saturated soils. Afterwards, the results of a number of suction-controlled laboratory tests on compacted kaolin and a sandy clay are used to evaluate the ability of the model to reproduce, quantitatively, observed behaviour. The agreement between observed and computed results is considered satisfactory and confirms the possibilities of reproducing the most important features of partially saturated soil behaviour using a simple general framework. Peer Reviewe...|$|R
40|$|The {{development}} of robotic devices for {{the rehabilitation of}} gait is a growing area {{of interest in the}} engineering rehabilitation community. The problem with modelling gait dynamics is that everybody walks differently. The approach advocated in this paper addresses this issue by modelling the gait dynamics of individual patients. Specifically, we present a model learner which performs automated system identification of patient gait. The model learner consists of an ensemble of multiple-input-single-output Gaussian Processes which feature automatic relevance determination kernels for automated tuning of parameters. First, the paper presents results for the application of the Gaussian Process ensemble to the learning of a particular patient's gait using a <b>typical</b> <b>prediction</b> configuration. Generalisation of gait prediction is tested with multiple patients and cross-validation. Finally, initial results are presented in which the Gaussian Process ensemble is shown to be capable of learning the mapping between the patient's gait and the therapist-assisted gai...|$|E
40|$|Smart grid is a {{potential}} infrastructure to supply electricity demand for end users in a safe and reliable manner. With the rapid increase of the share of renewable energy and controllable loads in smart grid, the operation uncertainty of smart grid has increased briskly during recent years. The forecast {{is responsible for the}} safety and economic operation of the smart grid. However, most existing forecast methods cannot account for the smart grid due to the disabilities to adapt to the varying operational conditions. In this paper, reinforcement learning is firstly exploited to develop an online learning framework for the smart grid. With the capability of multitime scale resolution, wavelet neural network has been adopted in the online learning framework to yield reinforcement learning and wavelet neural network (RLWNN) based adaptive learning scheme. The simulations on two <b>typical</b> <b>prediction</b> problems in smart grid, including wind power prediction and load forecast, validate the effectiveness and the scalability of the proposed RLWNN based learning framework and algorithm...|$|E
40|$|AbstractBasic chemometric {{methods for}} making {{empirical}} regression models for QSPR/QSAR are briefly described from a user's point of view. Emphasis {{is given to}} PLS regression, simple variable selection and a careful and cautious evaluation {{of the performance of}} PLS models by repeated double cross validation (rdCV). A demonstration example is worked out for QSPR models that predict gas chromatographic retention indices (values between 197 and 504 units) of 209 polycyclic aromatic compounds (PAC) from molecular descriptors generated by Dragon software. Most favorable models were obtained from data sets containing also descriptors from 3 D structures with all H-atoms (computed by Corina software), using stepwise variable selection (reducing 2688 descriptors to a subset of 22). The final QSPR model has <b>typical</b> <b>prediction</b> errors for the retention index of + 12 units (95 % tolerance interval, for test set objects). Programs and data are provided as supplementary material for the open source R software environment...|$|E
40|$|We present {{measurements}} of the B+ meson total cross section and differential cross section dsigma/dp(T). The measurements use a 98 +/- 4 pb(- 1) sample of p (p) over bar collisions at roots = 1. 8 TeV collected by the CDF detector. Charged B meson candidates are reconstructed through the decay B+/- [...] > J/psiK(+/-) with J/psi [...] > mu(+) mu(-). The total cross section, measured in the central rapidity region 6. 0 GeV/c, is 3. 6 +/- 0. 6 (stat + syst) mub. The measured differential cross section is substantially larger than <b>typical</b> QCD <b>predictions</b> calculated to next-to-leading order...|$|R
40|$|Motivated by {{the recent}} CMS excess in a flavor violating Higgs decay h →μτ {{as well as the}} anomaly of muon {{anomalous}} magnetic moment (muon g- 2), we consider a scenario where both the excess in h →μτ and the anomaly of muon g- 2 are explained by the μ-τ flavor violation in a general two Higgs doublet model. We study various processes involving μ and τ, and then discuss the <b>typical</b> <b>predictions</b> and constraints in this scenario. Especially, we find that the prediction of τ→μγ can be within the reach of the Belle II experiment. We also show that the lepton non-universality between τ→μνν̅ and τ→ e νν̅ can be sizable, and hence the analysis of the current Belle data and the future experimental improvement would have an impact on this model. Besides, processes such as τ→μ l^+ l^- (l=e, μ), τ→μη, μ→ e γ, μ→ 3 e, and muon EDM can be accessible, depending on the unknown Yukawa couplings. On the other hand, the processes like τ→ e γ and τ→ e l^+ l^- (l=e, μ) could not be sizable to observe because of the current strong constraints on the e-μ and e-τ flavor violations. Then we also conclude that contrary to h →μτ decay mode, the lepton flavor violating Higgs boson decay modes h→ e μ and h→ eτ are strongly suppressed, and hence {{it will be difficult to}} observe these modes at the LHC experiment. Comment: 35 pages, 17 figures, 1 tabl...|$|R
40|$|Abstract—The {{saturation}} {{of mobile}} phone markets {{has resulted in}} rising costs for operators to obtain new customers. These operators thus focus their energies on identifying users that will churn {{so they can be}} targeted for retention campaigns. <b>Typical</b> churn <b>prediction</b> algorithms identify churners based on service usage metrics, network performance indicators, and demographic information. Social and peer-influence to churn, however, is usually not considered. In this paper, we describe a new churn prediction algorithm that incorporates the influence churners spread to their social peers. Using data from a major service provider, we show that social influence improves churn prediction and is among the most important factors. I...|$|R
30|$|This paper {{presents}} a NIPE embedding technique for reversible watermarking. The NIPE technique can remedy a major drawback of Thodi and Rodriguez’s work (called IPE in this paper) that the predicted values should be rounded to integer number for data embedding. With the NIPE technique, the rounding {{operation in the}} prediction process (that often appears in IPE-based reversible watermarking algorithms to generate integer prediction errors) can be discarded. This is beneficial to use better predictor. In order to prove the advantage, we proposed a prediction model and designed an image predictor for the NIPE. The new predictor can predict pixels with four immediate pixels, and all pixels can be predicted with the original pixels. With the proposed NIPE and predictor, the embedding distortion is smaller than that in [15] at all embedding rates. Experimental results {{have shown that the}} predictor designed in this paper can provide the best performance than several existing <b>typical</b> <b>prediction</b> methods. In comparison with other typical reversible watermarking algorithms, the proposed scheme (combining the NIPE technique and new prediction method) performs better.|$|E
40|$|Abstract. Accurate {{prediction}} of pseudoknotted RNA secondary structure {{is an important}} computational challenge. <b>Typical</b> <b>prediction</b> algorithms aim to find a structure with minimum free energy according to some thermodynamic (“sum of loop energies”) model that is implicit in the recurrences of the algorithm. However, a clear definition of what exactly are the loops and stems in pseudoknotted structures, and their associated energies, has been lacking. We present a comprehensive classification of loops in pseudoknotted RNA secondary structures. Building on an algorithm of Bader et al. [2] we obtain a linear time algorithm for parsing a secondary structures into its component loops. We also give a linear time algorithm to calculate the free energy of a pseudoknotted secondary structure. This is useful for heuristic prediction algorithms which are widely used since (pseudoknotted) RNA secondary structure prediction is NP-hard. Finally, we give a linear time algorithm to test whether a secondary structure is in the class handled by Akutsu’s algorithm [1]. Using our tests, we analyze the generality of Akutsu’s algorithm for real biological structures. ...|$|E
40|$|Two {{valuation}} models {{based on}} accounting concepts and measurements are specified and {{discussed in the}} paper - a "residual income valuation" model and a "value added valuation" model. Given clean surplus accounting, the first model is identical to a model where future expected dividends and a horizon value of owners' equity are discounted to a present value. Similarly, the second model is identical to a free cash flow valuation framework, as specified in the well-known "McKinsey model" (Copeland, Koller & Murrin, 1994). The valuation models being specified in the paper use accounting measures of capital, capital growth and return on capital to calculate a fundamental value of owners' equity. As a certain connection between the required market rate of return ("cost of capital"), the accounting return on capital and capital growth {{can be expected to}} hold in the long run, the valuation models provide for some simplification of <b>typical</b> <b>prediction</b> problems inherent in fundamental valuation analysis. Conservative accounting; economic value added; financial ratios; fundamental analysis; residual income; valuation...|$|E
40|$|In H. 264 /AVC standard, the intra-frame {{prediction}} {{scheme is}} employed for better coding efficiency. Using the already encoded adjacent upper and left pixels, the intra prediction is performed. In this paper, we present an improved intra prediction scheme of H. 264 /AVC. By {{taking into account}} the error of the adjacent bottom and right pixels in intra mode decision, our scheme shows better results than those of the <b>typical</b> intra <b>prediction</b> of H. 264 /AVC in coding efficiency at the expense of slight increase of complexity and bit-rate. Experimental results show that the average PSNR is increased by up to 0. 16 dB...|$|R
40|$|The {{saturation}} {{of mobile}} phone markets {{has resulted in}} rising costs for operators to obtain new customers. These operators thus focus their energies on identifying users that will churn {{so they can be}} targeted for retention campaigns. <b>Typical</b> churn <b>prediction</b> algorithms identify churners based on service usage metrics, network performance indicators, and demographic information. Social and peer-influence to churn, however, is usually not considered. In this paper, we describe a new churn prediction algorithm that incorporates the influence churners spread to their social peers. Using data from a major service provider, we show that social influence improves churn prediction and is among the most important factors...|$|R
40|$|The {{status of}} the {{extended}} solar activity minimum, since {{the second half of}} 2007, has been briefly instructed to the solar-terrestrial community. Cycle 24 has the most spotless days since cycle 16, and probably even since the modern cycles, latitudes of high-latitude (> 35 °) sunspots belonging to a new cycle around the minimum time of the cycle are statistically the lowest at present, compared with those of cycle 12 onwards, and there is only one or no sunspots in a month appearing at high latitudes (> 20 °) for 58 months (from November 2003 to September 2008), which is observed {{for the first time since}} cycle 12 onwards. The solar wind velocity and pressure, 10. 7 cm solar radio flux, the polar solar magnetic field, solar total irradiance, and so on reach their minima during the 23 – 24 cycle minimum time. In order to explain the present extreme low activity, we introduced here one possible mechanism using helio-seismology observations. Viewing, from the long-term running of the time scales of both the Gleissberg period and millenniums, the extended solar activity minimum becomes logical. According to the present observations, the cycle 24 should start in November 2008. Solar activity is predicted at being about 30 % lower in cycle 24 than in cycle 23, synthesizing the <b>typical</b> <b>predictions</b> of solar activity, including those given by NASA and NOAA. The 24 th solar cycle is sluggishly coming and it should be an opportune moment for studying solar physics and solar-terrestrial physics...|$|R
