91|10000|Public
3000|$|In the experiments, {{our local}} <b>transfer</b> <b>learning</b> <b>methods</b> are {{compared}} with non-transfer method, global transfer method and other <b>transfer</b> <b>learning</b> <b>methods.</b> The local <b>transfer</b> <b>learning</b> <b>methods</b> include k-NN transfer learning method, training-test k-NN transfer learning method, adaptive k-NN <b>transfer</b> <b>learning</b> <b>methods</b> and clustering <b>transfer</b> <b>learning</b> <b>methods.</b> The non-transfer method {{does not use}} a transfer learning way and is a traditional method. The global transfer method is also a k-NN transfer learning method, {{but it has a}} k value equalling the number of all test data, i.e. it takes all test data as neighbours. A famous transfer learning method called KMM [10] is also used here as a baseline method. After reweighting importance, we integrate the importance into weighted risk models. We choose weighted risk model MARS, which is open source regression software for Matlab/Octave from ([URL] [...]...|$|E
30|$|In Table 3, {{the result}} {{shows that the}} <b>{{transfer}}</b> <b>learning</b> <b>methods</b> perform much better than non-transfer method MARS. KMM and Clust behave {{a little better than}} other transfer methods. AkNN 1 and AkNN 2 perform nearly equally well to other <b>transfer</b> <b>learning</b> <b>methods.</b>|$|E
30|$|The rest of {{the paper}} is {{organized}} as follows: we present the local regression <b>transfer</b> <b>learning</b> <b>methods</b> in Sect. 2; we then introduce the background of covariate shift and local learning, and propose some local <b>transfer</b> <b>learning</b> <b>methods</b> to reweight the training dataset and build the weighted risk regression model. We perform some experiments of psychological characteristics prediction and analyse the experiment results in Sect. 3. Finally, we conclude the whole work in the last section.|$|E
40|$|This paper {{proposes to}} study the problem of {{identifying}} intention posts in online discussion forums. For example, in a discussion forum, a user wrote “I plan to buy a camera,” which indicates a buying intention. This intention can be easily exploited by advertisers. To {{the best of our}} knowledge, there is still no reported study of this problem. Our research found that this problem is particularly suited to <b>transfer</b> <b>learning</b> because in different domains, people express the same intention in similar ways. We then propose a new <b>transfer</b> <b>learning</b> <b>method</b> which, unlike a general <b>transfer</b> <b>learning</b> algorithm, exploits several special characteristics of the problem. Experimental results show that the proposed method outperforms several strong baselines, including supervised learning in the target domain and a recent <b>transfer</b> <b>learning</b> <b>method.</b> ...|$|R
40|$|The encoder-decoder {{framework}} for {{neural machine translation}} (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a <b>transfer</b> <b>learning</b> <b>method</b> that significantly improves Bleu scores {{across a range of}} low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our <b>transfer</b> <b>learning</b> <b>method</b> we improve baseline NMT models by an average of 5. 6 Bleu on four low-resource language pairs. Ensembling and unknown word replacement add another 2 Bleu which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the <b>transfer</b> <b>learning</b> model for re-scoring, we can improve the SBMT system by an average of 1. 3 Bleu, improving the state-of-the-art on low-resource machine translation. Comment: 8 page...|$|R
40|$|Policy {{advice is}} a <b>transfer</b> <b>learning</b> <b>method</b> where a student agent {{is able to}} learn faster via advice from a teacher. However, both this and other {{reinforcement}} <b>learning</b> <b>transfer</b> <b>methods</b> have little theoretical analysis. This paper formally defines a setting where multiple teacher agents can provide advice to a student and introduces an algorithm to leverage both autonomous exploration and teacher's advice. Our regret bounds justify the intuition that good teachers help while bad teachers hurt. Using our formalization, we are also able to quantify, for the first time, when negative transfer can occur within such a reinforcement learning setting. Comment: 10 pages, 6 figures, IJCAI 2016 conference pape...|$|R
30|$|It can be {{concluded}} {{from the above}} experiments that all our local <b>transfer</b> <b>learning</b> <b>methods</b> work better than non-transfer learning method, because they reduce the prediction bias of model which is trained and tested on different-distribution datasets. Our local k-NN family <b>transfer</b> <b>learning</b> <b>methods</b> perform better than the global k-NN transfer learning method generally, and the {{reason may be that}} an appropriate k value in k-NN methods could reflect more subtle nature in density estimation. All our local <b>transfer</b> <b>learning</b> <b>methods</b> show comparable performance with KMM method in all situations. TTkNN method exceeds kNN and obtains the best performance among all the methods in half of situations. It could be guessed that TTkNN uses both test and training data information, while kNN only uses test data. Clust method performs well in most situations, this proves its applicability, and better density clustering methods may further enhance this method.|$|E
30|$|The study {{proposed}} two <b>transfer</b> <b>learning</b> <b>methods,</b> HeTL and CeHTL, {{on network}} attack detection methods {{to address the}} issues of lacking sufficient labels for new attacks. The results showed that HeTL and CeHTL significantly improved the accuracy compared to the traditional classifiers and other <b>transfer</b> <b>learning</b> <b>methods.</b> Especially, CeHTL performed the best {{in most of the}} tasks, especially in DoS →Probe tasks. One of the reason is DoS had more similarities with Probe than R 2 L, according to the top selected features in Appendix Table 5 and 6. This can improve the accuracy of computing the cluster correspondence, which thus resulted in a better performance.|$|E
30|$|We analyse {{performances}} of all methods. Table 2 shows that all local <b>transfer</b> <b>learning</b> <b>methods</b> perform better than non-transfer method MARS. GkNN behaves unstably: it performs worse than MARS in 2 of all 5 traits, while it performs best in O trait. kNN performs {{no worse than}} GkNN in all five traits. TTkNN {{is still the best}} method for most situations and performs stably. AkNN 1 performs much better than MARS, but much worse in O trait than other local <b>transfer</b> <b>learning</b> <b>methods</b> except AkNN 2. AkNN 2 behaves only a little better than MARS in four traits and weaker in one trait. Clust also beats MARS method in all situations but behaves not so well in O trait.|$|E
40|$|Abstract—Inductive <b>transfer</b> <b>{{learning}}</b> and semi-supervised {{learning are}} two different branches of machine learning. The former tries to reuse knowledge in labeled out-of-domain instances while the later attempts to exploit the usefulness of unlabeled in-domain instances. In this paper, we bridge the two branches {{by pointing out that}} many semi-supervised <b>learning</b> <b>methods</b> can be extended for inductive <b>transfer</b> <b>learning,</b> if the step of labeling an unlabeled instance is replaced by re-weighting a diff-distribution instance. Based on this recognition, we develop a new <b>transfer</b> <b>learning</b> <b>method,</b> namely COITL, by extending the co-training <b>method</b> in semi-supervised <b>learning.</b> Experimental results reveal that COITL can achieve significantly higher generalization and robustness, compared with two state-of-the-art <b>methods</b> in inductive <b>transfer</b> <b>learning.</b> Keywords-Inductive <b>transfer</b> learning; semi-supervised learning; co-training I...|$|R
40|$|International audienceBrain Computer Interfaces {{suffer from}} {{considerable}} cross-session and cross-subject variability, {{which makes it}} hard for classification methods to generalize. We introduce a <b>transfer</b> <b>learning</b> <b>method</b> based on regularized discrete optimal transport with class labels in the interest of enhancing the generalization capacity of state-of-the-art classification methods. We demonstrate the potential of this approach by applying it to offline cross-subject <b>transfer</b> <b>learning</b> for the P 300 -Speller paradigm. We also simulate an online experiment to assess the feasibility of our method. Results show that our method is comparable to-and sometimes even outperforms-session-dependent classification...|$|R
40|$|<b>Transfer</b> <b>learning</b> has {{revolutionized}} computer vision, but existing {{approaches in}} NLP still require task-specific modifications and training from scratch. We propose Fine-tuned Language Models (FitLaM), an effective <b>transfer</b> <b>learning</b> <b>method</b> {{that can be}} applied to any task in NLP, and introduce techniques that are key for fine-tuning a state-of-the-art language model. Our method significantly outperforms the state-of-the-art on five text classification tasks, reducing the error by 18 - 24 % on the majority of datasets. We open-source our pretrained models and code to enable adoption by the community...|$|R
40|$|<b>Transfer</b> <b>learning</b> <b>methods</b> {{address the}} {{situation}} where little labeled training data from the "target" problem exists, but much training data from a related "source" domain is available. However, {{the overwhelming majority of}} <b>transfer</b> <b>learning</b> <b>methods</b> are designed for simple settings where the source and target predictive functions are almost identical, limiting the applicability of <b>transfer</b> <b>learning</b> <b>methods</b> to real world data. We propose a novel, weaker, property of the source domain that can be transferred even when the source and target predictive functions diverge. Our method assumes the source and target functions share a Pairwise Similarity property, where if the source function makes similar predictions on a pair of instances, then so will the target function. We propose Pairwise Similarity Regularization Transfer, a flexible graph-based regularization framework which can incorporate this modeling assumption into standard supervised learning algorithms. We show how users can encode domain knowledge into our regularizer in the form of spatial continuity, pairwise "similarity constraints" and how our method can be scaled to large data sets using the Nystrom approximation. Finally, we present positive and negative results on real and synthetic data sets and discuss when our Pairwise Similarity transfer assumption seems to hold in practice...|$|E
30|$|In this paper, {{based on}} our {{previous}} work [7], we intend to work on more domains of psychological characteristics predictions and propose some new local regression <b>transfer</b> <b>learning</b> <b>methods,</b> including training-test k-NN method and adaptive k-NN methods, which are more effective and can adaptively set the unknown parameter in prediction functions.|$|E
30|$|When {{faced with}} little or no labeled {{training}} data, a model trained on such data will have insufficient discriminatory ability and would be unable to predict accurately. In order to handle this issue, transfer learning techniques have received significant focus in research communities. Specifically, heterogeneous transfer learning has been recently studied as it broadens the application of current <b>transfer</b> <b>learning</b> <b>methods.</b>|$|E
40|$|With {{the design}} and {{development}} of smart cities, opportunities as well as challenges arise at the moment. For this purpose, lots of data need to be obtained. Nevertheless, circumstances vary in different cities due to the variant infrastructures and populations, {{which leads to the}} data sparsity. In this paper, we propose a <b>transfer</b> <b>learning</b> <b>method</b> for urban waterlogging disaster analysis, which provides the basis for traffic management agencies to generate proactive traffic operation strategies in order to alleviate congestion. Existing work on urban waterlogging mostly relies on past and current conditions, as well as sensors and cameras, while there may not be a sufficient number of sensors to cover the relevant areas of a city. To this end, it would be helpful if we could transfer waterlogging. We examine whether it is possible to use the copious amounts of information from social media and satellite data to improve urban waterlogging analysis. Moreover, we analyze the correlation between severity, road networks, terrain, and precipitation. Moreover, we use a multiview discriminant <b>transfer</b> <b>learning</b> <b>method</b> to <b>transfer</b> knowledge to small cities. Experimental results involving cities in China and India show that our proposed framework is effective...|$|R
40|$|Abstract. This paper investigates {{automatic}} wrapper {{generation and}} maintenance for Forums, Blogs and News web sites. Web pages are increasingly dynamically generated using a common template populated {{with data from}} databases. This paper proposes a novel method that uses tree alignment and <b>transfer</b> <b>learning</b> <b>method</b> to generate the wrapper from this kind of web pages. The tree alignment algorithm is adopted {{to find the best}} matching structure of the input web pages. A kind of linear regression method is employed to get the weight of different tag-matching. A <b>transfer</b> <b>learning</b> <b>method</b> is adopted to find the most likely content block. A wrapper built on the most probable content block and the repeating patterns extracts data from web pages. The wrapper maintenance arises because web source may experiment changes that invalidate the current wrappers. This paper presents a wrapper maintenance method using a log likelihood ratio test for detecting the change points on the similarity series which gotten from the wrapper and input web pages. The wrapper generation method is applied to generate a wrapper once the web source change is detected. Experimental results show that the method achieves high accuracy and has steady performanc...|$|R
30|$|<b>Transfer</b> <b>learning</b> {{bridging}} {{tasks in}} such domains to target domain is utilized in machine learning and computer vision. This algorithm where a model {{trained on the}} source domain or data is purposed to refine the target model. It is exploited to assist the generalization in source task to improve a significant performance in target task. The <b>transfer</b> <b>learning</b> is classified to inductive <b>transfer</b> <b>learning,</b> transductive <b>transfer</b> <b>learning,</b> and unsupervised <b>transfer</b> <b>learning</b> based {{on the kind of}} source and target tasks (domains). Multi-task learning is an inductive <b>transfer</b> <b>learning</b> <b>method</b> to solve multiple tasks at the same time. It can result in improving the learning efficiency and prediction accuracy of multiple tasks in the model. Multi-task learning has been widely used in many examples in computer vision (i.e., semantic segmentation [22], classification [23], detection [24, 25], and depth regression). Inspired by these works, we exploit multi-task framework for multiple features learning. We demonstrate that the proposed multi-task <b>learning</b> <b>method</b> would present better constraint compared to the single-task learning.|$|R
30|$|The {{literature}} has proven that the <b>transfer</b> <b>learning</b> <b>methods</b> {{have been successfully}} utilized in various real-world applications like object recognition and classification. These methods propose to use available annotated data and knowledge acquired through some previous tasks relative to source domains so as to improve a learning system of a target task in a target domain [29]. In this section, {{we are interested in}} the work that suggests to develop automatically or with less human effort-specific classifiers or detectors to a target scene.|$|E
40|$|Reinforcement {{learning}} agents typically {{require a}} significant amount of data before performing well on complex tasks. <b>Transfer</b> <b>learning</b> <b>methods</b> have made progress reducing sample complexity, but they have only been applied to model-free learning methods, not more data-efficient model-based learning methods. This paper introduces TIMBREL, a novel method capable of transferring information effectively into a model-based reinforcement learning algorithm. We demonstrate that TIMBREL can significantly improve the sample complexity and asymptotic performance of a model-based algorithm when learning in a continuous state space...|$|E
30|$|We predict two {{personality}} traits, extraversion {{and leadership}} from meeting videos. The lack of sufficiently {{large amounts of}} annotated data necessitates the use of transfer and multi-task learning approaches to augment the feature space and to preprocess the data. Experimental results are presented for multi-task learning and transfer learning frameworks. Various multi-task and <b>transfer</b> <b>learning</b> <b>methods</b> are used to augment the feature space and preprocess the input data. The need for both approaches arises {{from the lack of}} sufficiently large amounts of annotated data.|$|E
40|$|<b>Transfer</b> <b>learning</b> {{significantly}} accelerates the {{reinforcement learning}} process by exploiting relevant knowledge from previous experiences. The problem of optimally selecting source policies during {{the learning process}} is of great importance yet challenging. There has been little theoretical analysis of this problem. In this paper, we develop an optimal online method to select source policies for reinforcement <b>learning.</b> This <b>method</b> formulates online source policy selection as a multi-armed bandit problem and augments Q-learning with policy reuse. We provide theoretical guarantees of the optimal selection process and convergence to the optimal policy. In addition, we conduct experiments on a grid-based robot navigation domain to demonstrate its efficiency and robustness by comparing to the state-of-the-art <b>transfer</b> <b>learning</b> <b>method...</b>|$|R
40|$|A <b>transfer</b> <b>learning</b> <b>method</b> for {{generating}} features suitable for surgical tools and phase recognition from the ImageNet classification features [1] is proposed here. In addition, methods are developed {{for generating}} contextual features and combining them with {{time series analysis}} for final classification using multi-class random forest. The proposed pipeline is tested over the training and testing datasets of M 2 CAI 16 challenges: tool and phase detection. Encouraging results are obtained by leave-one-out cross validation evaluation on the training dataset. Comment: MICCAI M 2 CAI 2016 Surgical tool & phase detection challenge repor...|$|R
40|$|Location-based {{services}} {{often use}} {{only a single}} mobility data source, which typically will be scarce for any new user when the system starts out. We propose a <b>transfer</b> <b>learning</b> <b>method</b> to characterize the temporal distribution of places of individuals by using an external, additional, large-scale check-in data set such as Foursquare data. The method {{is applied to the}} next place prediction problem, and we show that the incorporation of additional data through the proposed method improves the prediction accuracy when there is a limited amount of prior data...|$|R
40|$|As an {{important}} application in text mining and social media, sentiment detection has aroused {{more and more}} research interests, due to the expanding volume of available online information such as microblogging messages and review comments. Many machine learning methods have been proposed for sentiment detection. As a branch of machine learning, transfer learning is {{an important}} technique that tries to transfer knowledge from one domain to another one. When applied to sentiment detection, existing <b>transfer</b> <b>learning</b> <b>methods</b> employ articles with human labeled sentiments from other domains to help the sentiment detection on a target domain. Although most existing <b>transfer</b> <b>learning</b> <b>methods</b> are devoted to handle the data distribution difference between different domains, they only resort to some approximation methods, which may introduce some unnecessary biases. Furthermore, the popular assumption of existing transfer learning techniques on conditional probability is often too strong for practical applications. In this paper, we propose a novel method to model the distribution difference between different domains in sentiment detection by directly modeling the underlying joint distributions for different domains. Some of the important properties of the proposed method, such as the convergence rate and time complexity, are analyzed. The experimental results on the product review dataset and the twitter dataset demonstrate {{the advantages of the}} proposed method over the state-of-the-art methods...|$|E
40|$|In this article, {{we propose}} a {{transfer}} learning method for deep neural networks (DNNs). Deep learning {{has been widely}} used in many applications. However, applying deep learning is problematic when {{a large amount of}} training data are not available. One of the conventional methods for solving this problem is transfer learning for DNNs. In the field of image recognition, state-of-the-art <b>transfer</b> <b>learning</b> <b>methods</b> for DNNs re-use parameters trained on source domain data except for the output layer. However, this method may result in poor classification performance when the amount of target domain data is significantly small. To address this problem, we propose a method called All-Transfer Deep Learning, which enables the transfer of all parameters of a DNN. With this method, we can compute the relationship between the source and target labels by the source domain knowledge. We applied our method to actual two-dimensional electrophoresis image~(2 -DE image) classification for determining if an individual suffers from sepsis; the first attempt to apply a classification approach to 2 -DE images for proteomics, which has attracted considerable attention as an extension beyond genomics. The results suggest that our proposed method outperforms conventional <b>transfer</b> <b>learning</b> <b>methods</b> for DNNs. Comment: Long version of article published at ECAI 2016 (9 pages, 13 figures, 8 tables...|$|E
40|$|A basic {{assumption}} of statistical learning {{theory is that}} train and test data are drawn from the same underlying distribution. Unfortunately, this assumption doesn’t hold in many applications. Instead, ample labeled data might exist in a particular ‘source ’ domain while inference is needed in another, ‘target ’ domain. Domain adaptation methods leverage labeled data from both domains to improve classification on unseen data in the target domain. In this work we survey domain <b>transfer</b> <b>learning</b> <b>methods</b> for various application domains with focus on recent work in Computer Vision. 1...|$|E
40|$|Abstract. We {{consider}} {{the problem of}} learning {{in an environment of}} classi cation tasks. Tasks sampled from the environment are used to im-prove classication performance on future tasks. We consider situations in which the tasks can be divided into groups. Tasks within each group are related by sharing a low dimensional representation, which diers across the groups. We present an algorithm which divides the sampled tasks into groups and computes a common representation for each group. We report experiments on a synthetic and two image data sets, which show the advantage of the approach over single-task learning and a pre-vious <b>transfer</b> <b>learning</b> <b>method.</b> Key words: <b>Learning</b> to <b>learn,</b> multi-task <b>learning,</b> <b>transfer</b> <b>learning.</b> ...|$|R
40|$|We {{consider}} {{the problem of}} learning {{in an environment of}} classification tasks. Tasks sampled from the environment are used to improve classification performance on future tasks. We consider situations in which the tasks can be divided into groups. Tasks within each group are related by sharing a low dimensional representation, which differs across the groups. We present an algorithm which divides the sampled tasks into groups and computes a common representation for each group. We report experiments on a synthetic and two image data sets, which show the advantage of the approach over single-task learning and a previous <b>transfer</b> <b>learning</b> <b>method...</b>|$|R
40|$|Human {{ability of}} both {{versatile}} grasping of given objects and grasping of novel (as of yet unseen) objects is truly remarkable. This probably {{arises from the}} experience infants gather by actively playing around with diverse objects. Moreover, knowledge acquired during this process is reused during learning of how to grasp novel objects. We conjecture that this combined process of active and <b>transfer</b> <b>learning</b> boils down to a random search around an object, suitably biased by prior experience, to identify promising grasps. In this paper we present an active <b>learning</b> <b>method</b> for <b>learning</b> of grasps for given objects, and a <b>transfer</b> <b>learning</b> <b>method</b> for <b>learning</b> of grasps for novel objects. Our <b>learning</b> <b>methods</b> apply a kernel adaptive Metropolis-Hastings sampler that learns an approximation of the grasps' probability density of an object while drawing grasp proposals from it. The sampler employs simulated annealing to search for globally-optimal grasps. Our empirical results show promising applicability of our proposed learning schemes. Comment: 6 pages, 4 figures, 3 tables, technical repor...|$|R
40|$|We {{propose a}} {{probabilistic}} transfer learning model that uses task-level features {{to control the}} task mixture selection in a hierarchical Bayesian model. These task-level features, although rarely used in existing approaches, can provide additional information to model complex task distributions and allow effective transfer to new tasks especially when only limited number of data are available. To estimate the model parameters, we develop an empirical Bayes method based on variational approximation techniques. Our experiments on information retrieval show that the proposed model achieves significantly better performance compared with other <b>transfer</b> <b>learning</b> <b>methods.</b> ...|$|E
40|$|Abstract Learning versatile, {{reusable}} {{skills is}} one of the key prerequisites for autonomous robots. Imitation and reinforcement learning are among the most promi-nent approaches for learning basic robotic skills. How-ever, the learned skills are often very specific and cannot be reused in different but related tasks. In the project BesMan, we develop hierarchical and <b>transfer</b> <b>learning</b> <b>methods</b> which allow a robot to learn a repertoire of versatile skills that can be reused in different situations. The development of new methods is closely integrated with the analysis of complex human behavior...|$|E
40|$|Abstract. Reinforcement {{learning}} agents typically {{require a}} significant amount of data before performing well on complex tasks. <b>Transfer</b> <b>learning</b> <b>methods</b> have made progress reducing sample complexity, but they have primarily been applied to model-free learning methods, not more data-efficient model-based learning methods. This paper introduces timbrel, a novel method capable of transferring information effectively into a model-based reinforcement learning algorithm. We demonstrate that timbrel can significantly improve the sample efficiency and asymptotic performance of a model-based algorithm when learning in a continuous state space. Additionally, we conduct experiments to test the limits of timbrel’s effectiveness. ...|$|E
40|$|We {{present a}} new {{dependency}} parsing method for Korean applying cross-lingual <b>transfer</b> <b>learning</b> and domain adaptation techniques. Unlike existing transfer learn-ing methods relying on aligned corpora or bilingual lexicons, we propose a feature <b>transfer</b> <b>learning</b> <b>method</b> with minimal su-pervision, which adapts an existing parser {{to the target}} language by transferring the features for the source language to the tar-get language. Specifically, we utilize the Triplet/Quadruplet Model, a hybrid pars-ing algorithm for Japanese, and apply a delexicalized feature transfer for Korean. Experiments with Penn Korean Treebank show that even using only the transferred features from Japanese achieves a high accuracy (81. 6 %) for Korean dependency parsing. Further improvements were ob-tained when a small annotated Korean cor-pus was combined with the Japanese train-ing corpus, confirming that efficient cross-lingual <b>transfer</b> <b>learning</b> can be achieved without expensive linguistic resources. ...|$|R
40|$|Abstract—In this paper, we {{introduce}} a prior-based <b>transfer</b> <b>learning</b> <b>method</b> for our statistical machine learning classifier which {{based on the}} logistic regression to detect the phishing sites that relies on our selected features of the URLs. Because of the mismatched distributions of the features in different phishing domains, we employ multiple models for different regions. Since {{it is impossible for}} us to collect enough data from a new region to rebuild the detection model, we adjust the existing models by the <b>transfer</b> <b>learning</b> algorithm to solve these problems. The proposed algorithm was evaluated on a real-world task of detecting the phishing websites. After a number of experiments, our proposed <b>transfer</b> <b>learning</b> algorithm achieves more than 97 % accuracy. The result demonstrates the use of this algorithm in the anti-phishing scenario is feasible and ready for our large scale detection engine. Index Terms—network security, phishing detection, <b>transfer</b> <b>learning,</b> model <b>transfer,</b> logistic regression I...|$|R
40|$|International audienceTo {{solve the}} {{challenging}} task of learning effective visual categories with limited training samples, we propose a new sparse representation classifier based <b>transfer</b> <b>learning</b> <b>method,</b> namely SparseTL, which propagates the crosscategory knowledge from multiple source categories {{to the target}} category. Specifically, we enhance the target classification task in learning a both generative and discriminative sparse representation based classifier using pairs of source categories most positively and most negatively correlated to the target category. We further improve the discriminativeability of the classifier by choosing the most discriminative bins in the feature vector with a feature selection process. The experimental {{results show that the}} proposed methodachieves competitive performance on the NUS-WIDE Scene database compared to several state of the art <b>transfer</b> <b>learning</b> algorithms while keeping a very efficient runtime...|$|R
