56|114|Public
25|$|For <b>time-to-event</b> <b>outcome</b> {{data that}} may be censored, {{survival}} analysis (e.g., Kaplan–Meier estimators and Cox proportional hazards models for time to coronary heart disease after receipt of hormone replacement therapy in menopause) is appropriate.|$|E
40|$|A {{common goal}} of {{longitudinal}} studies is to relate {{a set of}} repeated observations to a time-to-event endpoint. One {{example of such a}} design is in the area of a late-life depression research where repeated measurement of cognitive and functional outcomes can contribute to one's ability to predict whether or not an individual will have a major depressive episode over a period of time. This research proposes a novel model for the relationship between multivariate longitudinal measurements and a <b>time-to-event</b> <b>outcome.</b> The goal of this model is to improve prediction for the <b>time-to-event</b> <b>outcome</b> by considering all longitudinal measurements simultaneously. In this dissertation, we investigate a joint modeling approach for mixed types of multivariate longitudinal outcomes and a <b>time-to-event</b> <b>outcome</b> using a Bayesian paradigm. For the longitudinal model of continuous and binary outcomes, we formulate multivariate generalized linear mixed models with two types of random effects structures: shared random effects and correlated random effects. For the joint model, the longitudinal outcomes and the <b>time-to-event</b> <b>outcome</b> are assumed to be independent conditional on available covariates and the shared parameters, which are associated with the random effects of the longitudinal outcome processes. A Bayesian method using Markov chain Monte Carlo (MCMC) computed in OpenBUGS is implemented for parameter estimation. We illustrate the prediction of future event probabilities within a fixed time interval for patients based on our joint model, utilizing baseline data, post-baseline longitudinal measurements, and the <b>time-to-event</b> <b>outcome.</b> Prediction of event or mortality probabilities allows one to intervene clinically when appropriate. Hence, such methods provide a useful public health tool at both the individual and the population levels. The proposed joint model is applied to data sets on the maintenance therapies in a late-life depression study and the mortality in idiopathic pulmonary fibrosis. The performance of the method is also evaluated in extensive simulation studies...|$|E
40|$|In 2005, Barthel, Royston, and Babiker {{presented}} a menu-driven Stata program under the generic name of ART (assessment of resources for trials) to calculate {{sample size and}} power for complex clinical trial designs with a time-to- event or binary outcome. In this article, we describe a Stata tool called ARTPEP, which is intended to project the power and events of a trial with a <b>time-to-event</b> <b>outcome</b> into the future given patient accrual figures so far and assumptions about event rates and other defining parameters. ARTPEP {{has been designed to}} work closely with the ART program and has an associated dialog box. We illustrate the use of ARTPEP with data from a phase III trial in esophageal cancer. artpep, artbin, artsurv, artmenu, randomized controlled trial, <b>time-to-event</b> <b>outcome,</b> power, number of events, projection, ARTPEP, ART...|$|E
40|$|Abstract Background The number {{needed to}} treat (NNT) is a {{well-known}} effect measure for reporting the results of clinical trials. In the case of <b>time-to-event</b> <b>outcomes,</b> the calculation of NNTs is more difficult than {{in the case of}} binary data. The frequency of using NNTs to report results of randomised controlled trials (RCT) investigating <b>time-to-event</b> <b>outcomes</b> and the adequacy of the applied calculation methods are unknown. Methods We searched in PubMed for RCTs with parallel group design and individual randomisation, published in four frequently cited journals between 2003 and 2005. We evaluated the type of outcome, the frequency of reporting NNTs with corresponding confidence intervals, and assessed the adequacy of the methods used to calculate NNTs in the case of <b>time-to-event</b> <b>outcomes.</b> Results The search resulted in 734 eligible RCTs. Of these, 373 RCTs investigated <b>time-to-event</b> <b>outcomes</b> and 361 analyzed binary data. In total, 62 articles reported NNTs (34 articles with <b>time-to-event</b> <b>outcomes,</b> 28 articles with binary outcomes). Of the 34 articles reporting NNTs derived from <b>time-to-event</b> <b>outcomes,</b> only 17 applied an appropriate calculation method. Of the 62 articles reporting NNTs, only 21 articles presented corresponding confidence intervals. Conclusion The NNT is used as effect measure to present the results from RCTs with binary and <b>time-to-event</b> <b>outcomes</b> in the current medical literature. In the case of time-to-event data incorrect methods were frequently applied. Confidence intervals for NNTs were given in one third of the NNT reporting articles only. In summary, there is much room for improvement in the application of NNTs to present results of RCTs, especially where the outcome is time to an event. </p...|$|R
40|$|BACKGROUND: Joint {{modelling}} of longitudinal and time-to-event data {{is often}} preferred over separate longitudinal or time-to-event analyses {{as it can}} account for study dropout, error in longitudinally measured covariates, and correlation between longitudinal and <b>time-to-event</b> <b>outcomes.</b> The joint modelling literature focuses mainly on the analysis of single studies with no methods currently available for the meta-analysis of joint model estimates from multiple studies. METHODS: We propose a 2 -stage method for meta-analysis of joint model estimates. These methods are applied to the INDANA dataset to combine joint model estimates of systolic blood pressure with time to death, time to myocardial infarction, and time to stroke. Results are compared to meta-analyses of separate longitudinal or time-to-event models. A simulation study is conducted to contrast separate versus joint analyses over a range of scenarios. RESULTS: Using the real dataset, similar results were obtained by using the separate and joint analyses. However, the simulation study indicated a benefit of use of joint rather than separate methods in a meta-analytic setting where association exists between the longitudinal and <b>time-to-event</b> <b>outcomes.</b> CONCLUSIONS: Where evidence of association between longitudinal and <b>time-to-event</b> <b>outcomes</b> exists, results from joint models over standalone analyses should be pooled in 2 -stage meta-analyses...|$|R
40|$|This report {{examines}} the operating characteristics of adaptively randomized trials relative to equally randomized trials {{in regard to}} power and bias. We also examine {{the number of patients}} in the trial assigned to the superior treatment. The effects of prior selection, sample size, and patient prognostic factors are investigated for both binary and <b>time-to-event</b> <b>outcomes.</b> Content...|$|R
40|$|Joint {{modelling}} {{has emerged}} to be a potential tool to analyse data with a <b>time-to-event</b> <b>outcome</b> and longitudinal measurements collected over {{a series of}} time points. Joint modelling involves the simultaneous modelling of the two components, namely the time-to-event component and the longitudinal component. The main challenges of joint modelling are the mathematical and computational complexity. Recent advances in joint modelling have seen the emergence of several software packages which have implemented some of the computational requirements to run joint models. These packages have {{opened the door for}} more routine use of joint modelling. Through simulations and real data based on transition to psychosis research, we compared joint model analysis of <b>time-to-event</b> <b>outcome</b> with the conventional Cox regression analysis. We also compared a number of packages for fitting joint models. Our results suggest that joint modelling do have advantages over conventional analysis despite its potential complexity. Our results also suggest that the results of analyses may depend on how the methodology is implemented...|$|E
40|$|Amulti-armmulti-stage {{clinical}} trial design for binary outcomes with application to tuberculosis Daniel J Bratton*, Patrick PJ Phillips and Mahesh KB Parmar Background: Randomised controlled trials {{are becoming increasingly}} costly and time-consuming. In 2011, Royston and colleagues proposed a particular class of multi-arm multi-stage (MAMS) designs intended {{to speed up the}} evaluation of new treatments in phase II and III {{clinical trial}}s. Their design, which controls the type I error rate and power for each pairwise comparison, discontinues randomisation to poorly performing arms at interim analyses if they fail to show a pre-specified level of benefit over the control arm. Arms in which randomisation is continued to the final stage of the trial are compared against the control on a definitive <b>time-to-event</b> <b>outcome</b> measure. To increase efficiency, interim comparisons can be made on an intermediate <b>time-to-event</b> <b>outcome</b> which is on the causal pathway to the definitive outcome. Methods: We adapt Royston’s MAMS design to binary outcomes observed {{at the end of a}} fixed follow-up period and analysed using an absolute difference in proportions. We apply the design to tuberculosis (TB), an area where man...|$|E
40|$|The data {{presented}} in this article are related to the research article entitled “Measuring differential treatment benefit across marker specific subgroups: the choice of outcome scale” (Satagopan and Iasonos, 2015) [1]. These data were digitally reconstructed from figures published in Larkin et al. (2015) [2]. This article describes the steps to digitally reconstruct patient-level data on <b>time-to-event</b> <b>outcome</b> and treatment and biomarker groups using published Kaplan-Meier survival curves. The reconstructed data set and the corresponding computer programs are made publicly available to enable further statistical methodology research...|$|E
40|$|Cluster {{randomized}} trials (CRTs) involve the random assignment of intact social units rather than independent subjects to intervention groups. <b>Time-to-event</b> <b>outcomes</b> often are endpoints in CRTs where the intracluster correlation coefficient (ICC) {{serves as a}} descriptive parameter to assess the similarity among outcomes in a cluster. However, estimating the ICC in CRTs with <b>time-to-event</b> <b>outcomes</b> is a challenge due {{to the presence of}} censored observations. The ICC is estimated for two CRTs using the censoring indicators and observed outcomes. A simulation study explores the effect of administrative censoring on estimating the ICC. Results show that the ICC estimators derived from censoring indicators and observed outcomes are negatively biased for positively correlated outcomes. Analytic work further supports these results. Censoring indicators may be preferred to estimate the ICC under moderate frequency of administrative censoring while the observed outcomes may be preferred under minimal frequency of administrative censoring...|$|R
40|$|The ROC {{curve and}} the {{corresponding}} AUC are popular tools {{for the evaluation of}} diagnostic tests. They have been recently extended to assess prognostic markers and predictive models. However, due to the many particularities of <b>time-to-event</b> <b>outcomes,</b> various definitions and estimators have been proposed in the literature. This review article aims at presenting the ones that accommodate to right-censoring, which is common when evaluating such prognostic markers...|$|R
40|$|The {{performance}} of different propensity score methods for estimating marginal hazard ratios Peter C. Austina,b,c*† Propensity score methods {{are increasingly being}} used to reduce or minimize the effects of confounding when estimating the effects of treatments, exposures, or interventions when using observational or non-randomized data. Under the assumption of no unmeasured confounders, previous {{research has shown that}} propensity score methods allow for unbiased estimation of linear treatment effects (e. g., differences in means or proportions). However, in biomedical research, <b>time-to-event</b> <b>outcomes</b> occur frequently. There is a paucity of research into the {{performance of}} different propensity score methods for estimating the effect of treatment on <b>time-to-event</b> <b>outcomes.</b> Furthermore, propensity score methods allow for the estimation of marginal or population-average treatment effects. We conducted an extensive series of Monte Carlo simulations to examine the performance of propensity score matching (1 : 1 greedy nearest-neighbor matching within propensity score calipers), stratifi-cation on the propensity score, inverse probability of treatment weighting (IPTW) using the propensity score, and covariate adjustment using the propensity score to estimate marginal hazard ratios. We found that both propensity score matching and IPTW using the propensity score allow for the estimation of marginal hazard ratios with minimal bias. Of these two approaches, IPTW using the propensity score resulted in estimates with lower mean squared error when estimating the effect of treatment in the treated. Stratification on the propensity score and covariate adjustment using the propensity score result in biased estimation of both marginal and con-ditional hazard ratios. Applied researchers are encouraged to use propensity score matching and IPTW using the propensity score when estimating the relative effect of treatment on <b>time-to-event</b> <b>outcomes.</b> Copyright...|$|R
40|$|The joineR package {{implements}} {{methods for}} analysing data from longitudinal {{studies in which}} the response from each subject consists of a time-sequence of repeated measurements and a possibly censored time-toevent outcome. The modelling framework for the repeated measurements is the linear model with random effects and/or correlated error structure. The model for the <b>time-to-event</b> <b>outcome</b> is a Cox proportional hazards model with log-Gaussian frailty. Stochastic dependence is captured by allowing the Gaussian random effects of the linear model to be correlated with the frailty term of the Cox proportional hazards model...|$|E
40|$|Indiana University-Purdue University Indianapolis (IUPUI) Epidemiologic and {{clinical}} studies routinely collect longitudinal measures of multiple outcomes. These longitudinal outcomes {{can be used}} to establish the temporal order of relevant biological processes and their association with the onset of clinical symptoms. In {{the first part of this}} thesis, we proposed to use bivariate change point models for two longitudinal outcomes with a focus on estimating the correlation between the two change points. We adopted a Bayesian approach for parameter estimation and inference. In the second part, we considered the situation when <b>time-to-event</b> <b>outcome</b> is also collected along with multiple longitudinal biomarkers measured until the occurrence of the event or censoring. Joint models for longitudinal and time-to-event data {{can be used to}} estimate the association between the characteristics of the longitudinal measures over time and survival time. We developed a maximum-likelihood method to joint model multiple longitudinal biomarkers and a <b>time-to-event</b> <b>outcome.</b> In addition, we focused on predicting conditional survival probabilities and evaluating the predictive accuracy of multiple longitudinal biomarkers in the joint modeling framework. We assessed the performance of the proposed methods in simulation studies and applied the new methods to data sets from two cohort studies. National Institutes of Health (NIH) Grants R 01 AG 019181, R 24 MH 080827, P 30 AG 10133, R 01 AG 09956...|$|E
40|$|We {{propose a}} joint {{model for a}} <b>time-to-event</b> <b>outcome</b> and a {{quantile}} of a continuous response repeatedly measured over time. The quantile and survival processes are associated via shared latent and manifest variables. Our joint model provides a flexible approach to handle informative drop-out in quantile regression. A general Monte Carlo Expectation Maximization strategy based on importance sampling is proposed, which is directly applicable under any distributional assumption for the longitudinal outcome and random effects, and parametric and non-parametric assumptions for the baseline hazard. Model properties are illustrated through a simulation study and an application to an original data set about dilated cardiomyopathies...|$|E
40|$|As an {{epidemiological}} parameter, {{the population}} attributable fraction {{is an important}} measure to quantify the public health attributable risk of an exposure to morbidity and mortality. In this article, we extend this parameter to the attributable fraction function in survival analysis of <b>time-to-event</b> <b>outcomes,</b> and further establish its estimation and inference procedures based on the widely used proportional hazards models. Numerical examples and simulations studies are presented to validate and demonstrate the proposed methods...|$|R
40|$|We {{consider}} a conceptual {{correspondence between the}} missing data setting, and joint modeling of longitudinal and <b>time-to-event</b> <b>outcomes.</b> Based on this, we formulate an extended shared random effects joint model. Based on this, we provide a characterization of missing at random, which {{is in line with}} that in the missing data setting. The ideas are illustrated using data from a study on liver cirrhosis, contrasting the new framework with conventional joint models. status: publishe...|$|R
40|$|This is a {{compilation}} of current and past work on targeted maximum likelihood estimation. It features the original targeted maximum likelihood learning paper as well as chapters on super (machine) learning using cross validation, randomized controlled trials, realistic individualized treatment rules in observational studies, biomarker discovery, case-control studies, and <b>time-to-event</b> <b>outcomes</b> with censored data, among others. We hope this collection is helpful to the interested reader and stimulates additional research in this important area...|$|R
40|$|Joint {{models are}} {{statistical}} tools for estimating {{the association between}} time-to-event and longitudinal outcomes. One challenge to the application of joint models is its computational complexity. Common estimation methods for joint models include a two-stage method, Bayesian and maximum-likelihood methods. In this work, we consider joint models of a <b>time-to-event</b> <b>outcome</b> and multiple longitudinal processes and develop a maximum-likelihood estimation method using the expectation–maximization algorithm. We assess {{the performance of the}} proposed method via simulations and apply the methodology to a data set to determine the association between longitudinal systolic and diastolic blood pressure measures and time to coronary artery disease...|$|E
40|$|Conducting a {{clinical}} trial at multiple study centres raises {{the issue of whether}} and how to adjust for centre heterogeneity in the statistical analysis. In this paper, weaddress this issue for multicentre clinical trials with a <b>time-to-event</b> <b>outcome.</b> Based on simulations, we show that the current practice of ignoring centre heterogeneity can be seriously misleading, and we illustrate the performances of the frailty modelling approach over competing methods. A special attention is paid to the problem of misspecification of the frailty distribution. The appendix provides sample codes in R and in SAS to perform the analyses in this paper...|$|E
40|$|The {{method of}} {{instrumental}} variable (IV) analysis {{has been widely}} used in economics, epidemiology, and other fields to estimate the causal effects of intermediate covariates on outcomes, {{in the presence of}} unobserved confounders and/or measurement errors in covariates. Consistent estimation of the effect has been developed when the outcome is continuous, while methods for binary outcome produce inconsistent estimation. In this dissertation, we examine two IV methods in the literature for binary outcome and show the bias in parameter estimate by a simulation study. The identifiability problem of IV analysis with binary outcome is discussed. Moreover, IV methods for <b>time-to-event</b> <b>outcome</b> with censored data remain underdeveloped. We propose two Bayesian approaches for IV analysis with censored <b>time-to-event</b> <b>outcome</b> by using a two-stage linear model: One is a parametric Bayesian model with normal and non-normal elliptically contoured error distributions, and the other is a semiparametric Bayesian model with Dirichlet process mixtures for the random errors, in order to relax the parametric assumptions and address heterogeneous clustering problems. Markov Chain Monte Carlo sampling methods are developed for both parametric and semiparametric Bayesian models to estimate the endogenous parameter. Performance of our methods is examined by simulation studies. Both methods largely reduce bias in estimation and greatly improve coverage probability of the endogenous parameter, compared to the regular method where the unobserved confounders and/or measurement errors are ignored. We illustrate our methods on the Women's Health Initiative Observational Study and the Atherosclerosis Risk in Communities Study...|$|E
40|$|Abstract Background In {{systematic}} {{reviews and}} meta-analyses, <b>time-to-event</b> <b>outcomes</b> are most appropriately analysed using hazard ratios (HRs). In {{the absence of}} individual patient data (IPD), methods are available to obtain HRs and/or associated statistics by carefully manipulating published or other summary data. Awareness and adoption of these methods is somewhat limited, perhaps because they are published in the statistical literature using statistical notation. Methods This paper aims to 'translate' the methods for estimating a HR and associated statistics from published time-to-event-analyses into less statistical and more practical guidance and provide a corresponding, easy-to-use calculations spreadsheet, to facilitate the computational aspects. Results A wider audience {{should be able to}} understand published time-to-event data in individual trial reports and use it more appropriately in meta-analysis. When faced with particular circumstances, readers can refer to the relevant sections of the paper. The spreadsheet can be used to assist them in carrying out the calculations. Conclusion The methods cannot circumvent the potential biases associated with relying on published data for systematic reviews and meta-analysis. However, this practical guide should improve the quality of the analysis and subsequent interpretation of systematic reviews and meta-analyses that include <b>time-to-event</b> <b>outcomes.</b> </p...|$|R
30|$|The study {{data were}} meta-analysed where possible. The {{meta-analysis}} of <b>time-to-event</b> <b>outcomes</b> in Review Manager 5.3 (Nordic Cochrane Centre 2014) uses ‘O-E’ and ‘V’ statistics or hazard ratios (HR) for each trial. If {{these were not}} reported in a given trial we calculated them from the available statistics, if possible, using the methods described in Tierney (Tierney et al. 2007). Heterogeneity in meta-analyses was assessed using the I 2 statistic. If the I 2 value was > 50  % we did not pool the effect estimates but used the range of effects from the individual studies instead. <b>Time-to-event</b> <b>outcomes,</b> entered as ‘O–E and Variance’ outcomes, were statistically synthesised using a fixed-effect model and arranged so that HRs >  1 favoured the ALND group and HRs <  1 favoured the comparison group. Dichotomous outcomes were summarised as risk ratios (RR) and analysed using a fixed-effects model according to the Mantel–Haenszel method and arranged so that RRs <  1 favoured the ALND group and RRs >  1 favoured the comparison group. All analyses were conducted in Review Manager 5.3 (Nordic Cochrane Centre 2014). We included only the data available in trial reports or through contact with the trial authors. No data imputation was attempted.|$|R
40|$|In {{longitudinal}} studies {{it is often}} of interest to investigate how a marker that is repeatedly measured in time {{is associated with a}} time to an event of interest, e. g., prostate cancer studies where longitudinal PSA level measurements are collected in conjunction with the time-to-recurrence. Joint Models for Longitudinal and Time-to-Event Data: With Applications in R provides a full treatment of random effects joint models for longitudinal and <b>time-to-event</b> <b>outcomes</b> that can be utilized to analyze such data. The content is primarily explanatory, focusing on applications of joint modeling, bu...|$|R
40|$|A common {{objective}} in longitudinal studies is the joint modelling of a longitudinal response with a <b>time-to-event</b> <b>outcome.</b> Random effects are typically {{used in the}} joint modelling framework to explain the interrelationships between these two processes. However, estimation {{in the presence of}} random effects involves intractable integrals requiring numerical integration. We propose a new computational approach for fitting such models that is based on the Laplace method for integrals that makes the consideration of high dimensional random-effects structures feasible. Contrary to the standard Laplace approximation, our method requires much fewer repeated measurements per individual to produce reliable results. Copyright (c) 2009 Royal Statistical Society. ...|$|E
40|$|We {{present a}} general Bayesian {{framework}} for cost-effectiveness analysis (CEA) from clinical trial data. This framework allows for very flexible modelling of both cost and efficacy related trial data. A common CEA technique is established for this wide class of models through linking mean efficacy and mean {{cost to the}} parameters of any given model. Examples are given in which efficacy may be measured as a continuous, binary, ordinal or <b>time-to-event</b> <b>outcome,</b> and in which costs are modelled as distributed normally, log-normally, as a mixture or non-parametrically. A case study is presented, illustrating the methodology and illuminating the role of prior information. Copyright © 2001 John Wiley & Sons, Ltd. ...|$|E
40|$|Random {{effects or}} shared {{parameter}} models are commonly advocated {{for the analysis}} of combined repeated measurement and event history data, including dropout from longitudinal trials. Their use in practical applications has generally been limited by computational cost and complexity, meaning that only simple special cases can be fitted by using readily available software. We propose a new approach that exploits recent distributional results for the extended skew normal family to allow exact likelihood inference for a flexible class of random-effects models. The method uses a discretization of the timescale for the <b>time-to-event</b> <b>outcome,</b> which is often unavoidable in any case when events correspond to dropout. We place no restriction on the times at which repeated measurements are made. An analysis of repeated lung function measurements in a cystic fibrosis cohort is used to illustrate the method...|$|E
40|$|We {{aimed to}} examine {{the extent to which}} {{inaccurate}} assumptions for nuisance parameters used to calculate sample size can affect the power of a randomized controlled trial (RCT). In a simulation study, we separately considered an RCT with continuous, dichotomous or <b>time-to-event</b> <b>outcomes,</b> with associated nuisance parameters of standard deviation, success rate in the control group and survival rate in the control group at some time point, respectively. For each type of outcome, we calculated a required sample size N for a hypothesized treatment effect, an assumed nuisance parameter and a nominal power of 80 %. We then assumed a nuisance parameter associated with a relative error at the design stage. For each type of outcome, we randomly drew 10, 000 relative errors of the associated nuisance parameter (from empirical distributions derived from a previously published review). Then, retro-fitting the sample size formula, we derived, for the pre-calculated sample size N, the real power of the RCT, taking into account the relative error for the nuisance parameter. In total, 23 %, 0 % and 18 % of RCTs with continuous, binary and <b>time-to-event</b> <b>outcomes,</b> respectively, were underpowered (i. e., the real power was 90 %). Even with proper calculation of sample size, a substantial number of trials are underpowered or overpowered because of imprecise knowledge of nuisance parameters. Such findings raise questions about how sample size for RCTs should be determined...|$|R
40|$|Response-adaptive {{randomization}} {{designs are}} becoming increasingly popular in clinical trial practice. In this paper, we present RARtool, a user interface software developed in MATLAB for designing response-adaptive randomized comparative clinical trials with censored <b>time-to-event</b> <b>outcomes.</b> The RARtool software can compute different types of optimal treatment allocation designs, and it can simulate response-adaptive randomization procedures targeting selected optimal allocations. Through simulations, an investigator can assess design characteristics {{under a variety of}} experimental scenarios and select the best procedure for practical implementation. We illustrate the utility of our RARtool software by redesigning a survival trial from the literature...|$|R
40|$|Evaluating percentiles of {{survival}} was proposed {{as a possible}} method to analyze <b>time-to-event</b> <b>outcomes.</b> This approach sets the cumulative risk of the event of interest to a specific proportion and evaluates the time by which this proportion is attainedIn this context, exposure-outcome associations can be {{expressed in terms of}} differences in survival percentiles, expressing the difference in survival time by which different subgroups of the study population experience the same proportion of events, or in terms of percentile ratios, expressing the strength of the exposure in accelerating the time to the event. Additive models for conditional survival percentiles have been introduced, and their use to estimate multivariable-adjusted percentile differences, and additive interaction on the metric of time has been described. On the other hand, the percentile ratio has never been fully described, neither statistical methods have been presented for its models-based estimation. To bridge this gap, we provide a detailed presentation of the percentile ratio as a relative measure to assess exposure-outcome associations in the context of time-to-event analysis, discussing its interpretation and advantages. We then introduce multiplicative statistical models for conditional survival percentiles, and present their use in estimating percentile ratios and multiplicative interactions in the metric of time. The introduction of multiplicative models for survival percentiles allows researchers to apply this approach in a large variety of context where multivariable adjustment is required, enriching the potentials of the percentile approach as a flexible and valuable tool to evaluate <b>time-to-event</b> <b>outcomes</b> in medical research...|$|R
40|$|Restricted mean {{survival}} time: {{an alternative}} to the hazard ratio for the design and analysis of randomized trials with a <b>time-to-event</b> <b>outcome</b> Patrick Royston * and Mahesh KB Parmar Background: Designs and analyses of clinical trials with a <b>time-to-event</b> <b>outcome</b> almost invariably rely on the hazard ratio to estimate the treatment effect and implicitly, therefore, on the proportional hazards assumption. However, the results of some recent trials indicate that {{there is no guarantee that}} the assumption will hold. Here, we describe the use of the restricted mean survival time as a possible alternative tool in the design and analysis of these trials. Methods: The restricted mean is a measure of average survival from time 0 to a specified time point, and may be estimated as the area under the survival curve up to that point. We consider the design of such trials according to a wide range of possible survival distributions in the control and research arm(s). The distributions are conveniently defined as piecewise exponential distributions and can be specified through piecewise constant hazards and time-fixed or time-dependent hazard ratios. Such designs can embody proportional or non-proportional hazards of the treatment effect. Results: We demonstrate the use of restricted mean survival time and a test of the difference in restricted means as an alternative measure of treatment effect. We support the approach through the results of simulation studies and in real examples from several cancer trials. We illustrate the required sample size under proportional and non-proportional hazards, also the significance level and power of the proposed test. Values are compared with those from the standard approach which utilizes the logrank test. Conclusions: We conclude that the hazard ratio cannot be recommended as a general measure of the treatment effect in a randomized controlled trial, nor is it always appropriate when designing a trial. Restricted mean survival time may provide a practical way forward and deserves greater attention...|$|E
40|$|Abstract: The {{research}} on biomarkers {{has been limited}} in its effectiveness because biomarker levels can only be measured within the thresholds of assays and laboratory instruments, a challenge {{referred to as a}} detection limit (DL) problem. In this paper, we propose a Bayesian approach to the Cox proportional hazards model with explanatory variables subject to lower, upper, or interval DLs. We demonstrate that by formulating the <b>time-to-event</b> <b>outcome</b> using the Poisson density with counting process notation, implementing the proposed approach in the OpenBUGS and JAGS is straightforward. We have conducted extensive simulations to compare the proposed Bayesian approach to the other four commonly used methods and to evaluate its robustness with respect to the distribution assumption of the biomarkers. The proposed Bayesian approach and other methods were applied to an acute lung injury study, in which a panel of cytokine biomarkers was studied for the biomarkers ’ association with ventilation-free survival...|$|E
40|$|The tumor-node-metastasis (TNM) staging {{system has}} been the anchor of cancer diagnosis, treatment, and {{prognosis}} for many years. For meaningful clinical use, an orderly, progressive condensation of the T and N categories into an overall staging system needs to be de ned, usually {{with respect to a}} <b>time-to-event</b> <b>outcome.</b> This can be considered as a cutpoint selection problem for a censored response partitioned with respect to two ordered categorical covariates and their interaction. The aim is to select the best grouping of the TN categories. A novel bootstrap cutpoint/model selection method is proposed for this task by maximizing bootstrap estimates of the chosen statistical criteria. The criteria are based on prognostic ability including a landmark measure of the explained variation, the area under the ROC curve, and a concordance probability generalized from Harrell's c-index. We illustrate the utility of our method by applying it to the staging of colorectal cancer...|$|E
40|$|Background: Patient {{and health}} system {{determinants}} of outcomes following pancreatic cancer resection, particularly {{the relative importance}} of hospital and surgeon volume, are unclear. Our objective was to identify patient, tumour and health service factors related to mortality and survival amongst a cohort of patients who underwent completed resection for pancreatic cancer. Methods: Eligible patients were diagnosed with pancreatic adenocarcinoma between July 2009 and June 2011 and had a completed resection performed in Queensland or New South Wales, Australia, with either tumour-free (R 0) or microscopically involved margins (R 1) (n = 270). Associations were examined using logistic regression (for binary outcomes) and Cox proportional hazards or stratified Cox models (for <b>time-to-event</b> <b>outcomes).</b> Results: Patients treated by surgeons who performe...|$|R
40|$|We develop {{analysis}} methods for clinical trials with <b>time-to-event</b> <b>outcomes</b> which correct for treatment changes during follow-up, yet {{are based on}} comparisons of randomized groups and not of selected groups. A causal model relating observed event times to event times {{that would have been}} observed under other treatment scenarios is fitted using the semi-parametric approach of Robins and Tsiatis (avoiding assumptions about the relationship between treatment changes and prognosis). The methods are applied to the Concorde trial of immediate versus deferred zidovudine, to investigate how the results would have differed if no participant randomized to deferred zidovudine had started treatment before reaching ARC or AIDS. We consider issues relating to model choice, non-constant treatment effects and censoring...|$|R
40|$|We develop {{parametric}} {{maximum likelihood}} methods {{to adjust for}} treatment changes during follow-up {{in order to assess}} the causal effect of treatment in clinical trials with <b>time-to-event</b> <b>outcomes.</b> The accelerated failure time model of Robins and Tsiatis relates each observed event time to the underlying event time that would have been observed if the control treatment had been given throughout the trial. We introduce a bivariate parametric frailty model for time to treatment change and time to trial endpoint. Estimating equations which respect the randomization are constructed and compared to maximum likelihood methods in a simulation study. The Concorde trial of immediate versus deferred zidovudine in HIV infection is used as a motivating example and illustration of the methods...|$|R
