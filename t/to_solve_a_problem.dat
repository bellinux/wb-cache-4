1470|10000|Public
5|$|Traditionally, {{computer}} software {{has been written}} for serial computation. <b>To</b> <b>solve</b> <b>a</b> <b>problem,</b> an algorithm is constructed and implemented as a serial stream of instructions. These instructions are executed on a central processing unit on one computer. Only one instruction may execute at a time—after that instruction is finished, the next one is executed.|$|E
5|$|Parallel computing, on {{the other}} hand, uses {{multiple}} processing elements simultaneously <b>to</b> <b>solve</b> <b>a</b> <b>problem.</b> This is accomplished by breaking the problem into independent parts so that each processing element can execute its part of the algorithm simultaneously with the others. The processing elements can be diverse and include resources such as a single computer with multiple processors, several networked computers, specialized hardware, or any combination of the above.|$|E
5|$|There {{are various}} {{accounts}} of the introduction of polar coordinates {{as part of a}} formal coordinate system. The full history of the subject is described in Harvard professor Julian Lowell Coolidge's Origin of Polar Coordinates. Grégoire de Saint-Vincent and Bonaventura Cavalieri independently introduced the concepts in the mid-seventeenth century. Saint-Vincent wrote about them privately in 1625 and published his work in 1647, while Cavalieri published his in 1635 with a corrected version appearing in 1653. Cavalieri first used polar coordinates <b>to</b> <b>solve</b> <b>a</b> <b>problem</b> relating to the area within an Archimedean spiral. Blaise Pascal subsequently used polar coordinates to calculate the length of parabolic arcs.|$|E
50|$|As {{an example}} in {{applying}} the same logic, {{it may be}} harder <b>to</b> <b>solve</b> <b>a</b> 25-case <b>problem</b> {{than it would be}} <b>to</b> <b>solve</b> <b>an</b> n-case <b>problem,</b> and then apply it to the case where n=25.|$|R
30|$|In {{nonlinear}} analysis, <b>a</b> {{common approach}} <b>to</b> <b>solving</b> <b>a</b> <b>problem</b> with multiple solutions is {{to replace it}} by a family of perturbed <b>problems</b> admitting <b>a</b> unique solution and to obtain a particular solution as the limit of these perturbed solutions when the perturbation vanishes.|$|R
5000|$|The {{criticism}} of professionals or the {{criticism of}} a profession may occur, sometimes in a somewhat humorous, or satirical way. It could be done by professionals themselves, or by amateurs or laypersons. In this case, there is some skepticism about what the status of [...] "being professional" [...] actually adds <b>to</b> <b>solving</b> <b>a</b> <b>problem,</b> or there is skepticism about the claims made by a profession about how it can contribute <b>to</b> <b>solving</b> <b>a</b> <b>problem.</b> It is often implied here, that the standards of professionals do no justice to a specific situation, or {{that there is a}} case of professional cretinism: the professional gets it wrong, because he is unable to think outside of his own profession (he is imprisoned in a framework that does not lead to a solution).|$|R
25|$|The {{relation}} between the complexity classes P and NP is studied in computational complexity theory, {{the part of the}} theory of computation dealing with the resources required during computation to solve a given problem. The most common resources are time (how many steps it takes <b>to</b> <b>solve</b> <b>a</b> <b>problem)</b> and space (how much memory it takes <b>to</b> <b>solve</b> <b>a</b> <b>problem).</b>|$|E
25|$|His normal work habit was <b>to</b> <b>solve</b> <b>a</b> <b>problem</b> {{completely}} in his head, then commit the completed problem to paper.|$|E
25|$|When endeavouring <b>to</b> <b>solve</b> <b>a</b> <b>problem</b> humans at {{an early}} age show {{determination}} while chimpanzees have no comparable facial expression. Researchers suspect the human determined expression evolved because when a human is determinedly working on a problem other people will frequently help.|$|E
5000|$|... risky shift {{phenomenon}} - programmers attempt riskier solutions <b>to</b> <b>solve</b> <b>a</b> software <b>problem</b> ...|$|R
5000|$|... #Caption: [...] The whole-part {{model can}} also be used <b>to</b> <b>solve</b> <b>a</b> {{multiplication}} <b>problem.</b>|$|R
50|$|<b>To</b> <b>solve</b> <b>an</b> {{interesting}} <b>problem,</b> {{start by}} finding <b>a</b> <b>problem</b> that {{is interesting to}} you.|$|R
25|$|The line graphs {{of trees}} {{are exactly the}} claw-free block graphs. These graphs have been used <b>to</b> <b>solve</b> <b>a</b> <b>problem</b> in extremal graph theory, of {{constructing}} a graph with a given number of edges and vertices whose largest tree induced as a subgraph is as small as possible.|$|E
25|$|In 1846, Möbius invited Grassmann {{to enter}} a {{competition}} <b>to</b> <b>solve</b> <b>a</b> <b>problem</b> first proposed by Leibniz: to devise a geometric calculus devoid of coordinates and metric properties (what Leibniz termed analysis situs). Grassmann's Geometrische Analyse geknüpft an die von Leibniz erfundene geometrische Charakteristik, was the winning entry (also the only entry). Möbius, {{as one of the}} judges, criticized the way Grassmann introduced abstract notions without giving the reader any intuition as to why those notions were of value.|$|E
25|$|Convergent {{thinking}} is also linked to knowledge as it involves manipulating existing knowledge {{by means of}} standard procedures. Knowledge is another important aspect of creativity. It {{is a source of}} ideas, suggests pathways to solutions, and provides criteria of effectiveness and novelty. Convergent {{thinking is}} used as a tool in creative problem solving. When an individual is using critical thinking <b>to</b> <b>solve</b> <b>a</b> <b>problem</b> they consciously use standards or probabilities to make judgments. This contrasts with divergent thinking where judgment is deferred while looking for and accepting many possible solutions.|$|E
5000|$|Communities self-organize {{and have}} the human {{resources}} and social assets <b>to</b> <b>solve</b> <b>an</b> agreed-upon <b>problem.</b>|$|R
5000|$|Mace {{was also}} {{actively}} involved in the biodiversity sections of the [...] "Millennium Ecosystem Assessment" [...] which was conducted from 2002 through 2005. Mace continues to work in the field of conservation biology and states that [...] "all the evidence to date is that when societies put their mind <b>to</b> <b>solving</b> <b>a</b> <b>problem,</b> they can generally do it." ...|$|R
5000|$|In order <b>to</b> <b>solve</b> <b>a</b> given <b>problem</b> of {{supervised}} learning, one has {{to perform}} the following steps: ...|$|R
25|$|Claude Malhuret {{was elected}} {{as the new}} {{president}} of Medicins Sans Frontieres in 1977, and soon after debates began over the future of the organisation. In particular, the concept of témoignage ("witnessing"), which refers to speaking out about the suffering that one sees as opposed to remaining silent, was being opposed or played down by Malhuret and his supporters. Malhuret thought MSF should avoid criticism of the governments of countries in which they were working, while Kouchner believed that documenting and broadcasting the suffering in a country was the most effective way <b>to</b> <b>solve</b> <b>a</b> <b>problem.</b>|$|E
25|$|Closely related {{fields in}} {{theoretical}} computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity {{theory is that}} the former is devoted to analyzing the amount of resources needed by a particular algorithm <b>to</b> <b>solve</b> <b>a</b> <b>problem,</b> whereas the latter asks a more general question about all possible algorithms {{that could be used to}} solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kind of problems can, in principle, be solved algorithmically.|$|E
25|$|The {{asymptotic}} computational {{complexity of}} these algorithms may be misleading, {{because they can}} only be run on inputs of very small size. In a 1991 comparison, Hoon Hong estimated that Collins' doubly exponential procedure would be able <b>to</b> <b>solve</b> <b>a</b> <b>problem</b> whose size is described by setting all the above parameters to 2, {{in less than a}} second, whereas the algorithms of Grigoriev, Vorbjov, and Renegar would instead take more than a million years. In 1993, Joos, Roy, and Solernó suggested that {{it should be possible to}} make small modifications to the exponential-time procedures to make them faster in practice than cylindrical algebraic decision, as well as faster in theory. However, as of 2009, it was still the case that general methods for the first-order theory of the reals remained superior in practice to the singly exponential algorithms specialized to the existential theory of the reals.|$|E
50|$|He has {{released}} another preprint in his site that claims <b>to</b> <b>solve</b> <b>a</b> measure <b>problem</b> due <b>to</b> Stefan Banach.|$|R
40|$|Soft {{set theory}} was firstly {{introduced}} by Molodtsov in 1999 {{as a general}} mathematical tool for dealing with fuzzy objects. In this work, t-norm and t-conorm products of fuzzy parameterized soft sets (FP-soft sets) are defined and their properties are investigated. By using these prod-ucts, AND-FP-soft decision making and OR-FP-soft decision making methods are constructed. Finally, the methods are applied <b>to</b> <b>solve</b> <b>a</b> <b>problems</b> which contains uncertainties...|$|R
5000|$|Clients {{being served}} with brief staff-assisted {{services}} {{work with a}} variety of different staff members <b>to</b> <b>solve</b> <b>a</b> career <b>problem.</b>|$|R
25|$|In a population, at most half have values {{strictly}} {{less than}} the median and at most half have values strictly greater than it. If each group contains {{less than half the}} population, then some of the population is exactly equal to the median. For example, if a<nbsp&bnbsp&<nbsp&c, then the median of the list {a,nbsp&b,nbsp&c} is b, and, if anbsp&<nbsp&bnbsp&<nbsp&cnbsp&<nbsp&d, then the median of the list {a,nbsp&b,nbsp&c,nbsp&d} is the mean of b and c; i.e., it is (bnbsp&+nbsp&c)/2. Indeed, as {{it is based on the}} middle data in a group, it is not necessary to even know the value of extreme results in order to calculate a median. For example, in a psychology test investigating the time needed <b>to</b> <b>solve</b> <b>a</b> <b>problem,</b> if a small number of people failed to solve the problem at all in the given time a median can still be calculated.|$|E
25|$|Arizona v. California {{was a set}} of 11 United States Supreme Court cases {{dealing with}} water rights. These cases took place between the years of 1931 and 2006. The initial {{question}} of this case was {{to determine how much}} water from the Colorado river Arizona was entitled to. Many western states became involved in the debate over the rights of the water from the Colorado River, and finally the United States government became involved stating that several federal establishments, including five American Indian reservations, had water rights as defined by Winters v. United States. This United States Supreme Court case helped <b>to</b> <b>solve</b> <b>a</b> <b>problem</b> found in the case Winters v. United States. While the United States Supreme Court case of Winters v. United States held that American Indian Reservations do have reserved water rights equal to the amount of water needed on the reservation to sufficiently irrigate all of the irrigable reservation acreage, there was always {{the question of how to}} decide what amount of water was needed to sufficiently irrigate on the American Indian reservations. Arizona v. California offers the solution of adjudication to help fix this problem.|$|E
500|$|Trygve Lie, {{who after}} the war had become the first Secretary-General of the United Nations, characterised Koht in his memoirs as an expert on foreign affairs, but introverted. He had {{relatively}} little contact with other politicians, kept to himself to study in peace, and spent much time on his extensive writing. Koht reportedly preferred <b>to</b> <b>solve</b> <b>a</b> <b>problem</b> by himself instead of involving co-workers and employees, even the experts in the Ministry of Foreign Affairs. His way of thinking was logical and rational, but he allegedly nurtured an [...] "exaggerated belief in paragraphs" [...] and a [...] "dogmatic belief in international law", and wrongly thought that other countries would obey formal regulations at most times. Koht had few or no alternatives to his neutrality policy, {{and in many ways}} he based his entire career in foreign affairs on that policy. Trygve Lie claimed that before the Second World War, the neutrality policy had [...] "become a religion" [...] for Koht.|$|E
5000|$|... "Theater Arts Center: The Way <b>to</b> <b>Solve</b> <b>a</b> Complex <b>Problem</b> is <b>to</b> the Make it Simple." [...] Architectural Forum (December 1952).|$|R
5000|$|Problem-solving is {{demonstrated}} when children use trial-and-error <b>to</b> <b>solve</b> problems. The ability <b>to</b> systematically <b>solve</b> <b>a</b> <b>problem</b> in <b>a</b> logical and methodical way emerges.|$|R
50|$|Overman {{has also}} worked {{extensively}} on the aza-Cope-Mannich reaction, originally designed <b>to</b> <b>solve</b> <b>a</b> stereoelectronic <b>problem</b> in the total synthesis of gephyrotoxin.|$|R
2500|$|There are {{a number}} of {{technical}} challenges in building a large-scale quantum computer, and thus far quantum computers have yet <b>to</b> <b>solve</b> <b>a</b> <b>problem</b> faster than a classical computer. David DiVincenzo, of IBM, listed the following requirements for a practical quantum computer: ...|$|E
2500|$|Said makes of the Quranic verse (God {{does not}} change a people until they change what is in themselves) a title and a {{starting}} point, to prove that in order <b>to</b> <b>solve</b> <b>a</b> <b>problem</b> or change a situation, the priority of change starts from ourselves.|$|E
2500|$|To {{measure the}} {{difficulty}} of solving a computational problem, one may wish {{to see how much}} time the best algorithm requires to solve the problem. However, the running time may, in general, depend on the instance. In particular, larger instances will require more time to solve. Thus the time required <b>to</b> <b>solve</b> <b>a</b> <b>problem</b> (or the space required, or any measure of complexity) is calculated {{as a function of the}} size of the instance. This is usually taken to be the size of the input in bits. Complexity theory is interested in how algorithms scale with an increase in the input size. For instance, in the problem of finding whether a graph is connected, how much more time does it take <b>to</b> <b>solve</b> <b>a</b> <b>problem</b> for a graph with 2n vertices compared to the time taken for a graph with n vertices? ...|$|E
5000|$|A {{nonce word}} (also called an {{occasion}}alism) is a lexeme created for <b>a</b> single occasion <b>to</b> <b>solve</b> <b>an</b> immediate <b>problem</b> of communication.|$|R
5000|$|The key <b>to</b> <b>solving</b> <b>a</b> <b>problem</b> recursively is <b>to</b> {{recognize}} {{that it can be}} broken down into a collection of smaller sub-problems, to each of which that same general solving procedure that we are seeking applies, and the total solution is then found in some simple way from those sub-problems' solutions. Each of thus created sub-problems being [...] "smaller" [...] guarantees that the base case(s) will eventually be reached. Thence, for the Towers of Hanoi: ...|$|R
3000|$|... “When two rules express {{opposite}} {{points of}} view on how <b>to</b> <b>solve</b> <b>a</b> bankruptcy <b>problem,</b> it is natural to compromise between them by averaging”.|$|R
