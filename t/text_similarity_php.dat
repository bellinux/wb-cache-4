0|401|Public
40|$|Computing <b>text</b> <b>similarity</b> is a {{foundational}} {{technique for}} a wide range of tasks in natural language processing such as duplicate detection, question answering, or automatic essay grading. Just recently, <b>text</b> <b>similarity</b> received wide-spread attention in the research community by the establishment of the Semantic Textual Similarity (STS) Task at the Semantic Evaluation (SemEval) workshop in 2012 [...] -a fact that stresses the importance of <b>text</b> <b>similarity</b> research. The goal of the STS Task is to create automated measures which are able to compute the degree of similarity between two given texts in the same way that humans do. Measures are thereby expected to output continuous <b>text</b> <b>similarity</b> scores, which are then either compared with human judgments or used as a means for solving a particular problem. We start this thesis with the observation that while the concept of similarity is well grounded in psychology, <b>text</b> <b>similarity</b> is much less well-defined in the natural language processing community. No attempt has been made yet to formalize in what way <b>text</b> <b>similarity</b> between two <b>texts</b> can be computed. Still, <b>text</b> <b>similarity</b> is regarded as a fixed, axiomatic notion in the community. To alleviate this shortcoming, we describe existing formal models of similarity and discuss how we can adapt them to texts. We propose to judge <b>text</b> <b>similarity</b> along multiple <b>text</b> dimensions, i. e. characteristics inherent to texts, and provide empirical evidence based on a set of annotation studies that the proposed dimensions are perceived by humans. We continue with a comprehensive survey of state-of-the-art <b>text</b> <b>similarity</b> measures previously proposed in the literature. To the best of our knowledge, no such survey has been done yet. We propose a classification into compositional and non-compositional <b>text</b> <b>similarity</b> measures according to their inherent properties. Compositional measures compute <b>text</b> <b>similarity</b> based on pairwise word similarity scores between all words which are then aggregated to an overall similarity score, while non-compositional measures project the complete texts onto particular models and then compare the texts based on these models. Based on our theoretical insights, we then present the implementation of a <b>text</b> <b>similarity</b> system which composes a multitude of <b>text</b> <b>similarity</b> measures along multiple text dimensions using a machine learning classifier. Depending on the concrete task at hand, we argue that such a system may need to address more than a single text dimension in order to best resemble human judgments. Our efforts culminate in the open source framework DKPro Similarity, which streamlines the development of <b>text</b> <b>similarity</b> measures and experimental setups. We apply our system in two evaluations, for which it consistently outperforms prior work and competing systems: an intrinsic and an extrinsic evaluation. In the intrinsic evaluation, the performance of <b>text</b> <b>similarity</b> measures is evaluated in an isolated setting by comparing the algorithmically produced scores with human judgments. We conducted the intrinsic evaluation in the context of the STS Task as part of the SemEval workshop. In the extrinsic evaluation, the performance of <b>text</b> <b>similarity</b> measures is evaluated with respect to a particular task at hand, where <b>text</b> <b>similarity</b> is a means for solving a particular problem. We conducted the extrinsic evaluation in the text classification task of text reuse detection. The results of both evaluations support our hypothesis that a composition of <b>text</b> <b>similarity</b> measures highly benefits the similarity computation process. Finally, we stress the importance of <b>text</b> <b>similarity</b> measures for real-world applications. We therefore introduce the application scenario Self-Organizing Wikis, where users of wikis, i. e. web-based collaborative content authoring systems, are supported in their everyday tasks by means of natural language processing techniques in general, and <b>text</b> <b>similarity</b> in particular. We elaborate on two use cases where <b>text</b> <b>similarity</b> computation is particularly beneficial: the detection of duplicates, and the semi-automatic insertion of hyperlinks. Moreover, we discuss two further applications where <b>text</b> <b>similarity</b> is a valuable tool: In both question answering and textual entailment recognition, <b>text</b> <b>similarity</b> has been used successfully in experiments and appears to be a promising means for further research in these fields. We conclude this thesis with an analysis of shortcomings of current <b>text</b> <b>similarity</b> research and formulate challenges which should be tackled by future work. In particular, we believe that computing <b>text</b> <b>similarity</b> along multiple <b>text</b> dimensions [...] -which depend on the specific task at hand [...] -will benefit any other task where <b>text</b> <b>similarity</b> is fundamental, as a composition of <b>text</b> <b>similarity</b> measures has shown superior performance in both the intrinsic as well as the extrinsic evaluation...|$|R
40|$|We {{present a}} {{comprehensive}} study of computing <b>similarity</b> between <b>texts.</b> We start from the observation that while the concept of similarity is well grounded in psychology, <b>text</b> <b>similarity</b> is much less well-defined in the natural language processing community. We thus define the notion of <b>text</b> <b>similarity</b> and distinguish it from related tasks such as textual entailment and near-duplicate detection. We then identify multiple text dimensions, i. e. characteristics inherent to texts {{that can be used}} to judge <b>text</b> <b>similarity,</b> for which we provide empirical evidence. We discuss state-of-the-art <b>text</b> <b>similarity</b> measures previously proposed in the literature, before continuing with a thorough discussion of common evaluation metrics and datasets. Based on the analysis, we devise an architecture which combines <b>text</b> <b>similarity</b> measures in a unified classification framework. We apply our system in two evaluation settings, for which it consistently outperforms prior work and competing systems: (a) an intrinsic evaluation in the context of the Semantic Textual Similarity Task as part of the Semantic Evaluation (SemEval) exercises, and (b) an extrinsic evaluation for the detection of text reuse. As a basis for future work, we introduce DKPro Similarity, an open source software package which streamlines the development of <b>text</b> <b>similarity</b> measures and complete experimental setups...|$|R
40|$|Finding {{semantic}} {{similarity between}} short biomedical texts, such as article abstracts or experiment descriptions, may provide important information for health researchers. This paper presents {{a method for}} calculating <b>text</b> <b>similarity</b> in the biomedical context. The method implements a pairwise concept semantic similarity measure that uses concept definitions and ontology structure. The respective results have demonstrated an improved performance in comparison with a previous version of the method using lexical-based measures as similarity function, {{as well as with}} other alternative tools for measuring <b>text</b> <b>similarity.</b> General Terms: Semantic <b>text</b> <b>similarity</b> in biomedicine, <b>text</b> minin...|$|R
30|$|In {{the error}} distribution, we also {{can see that}} IR {{is also the most}} {{significant}} error type in <b>text</b> <b>similarity</b> comparison, 10 incorrect removes of marker, memo, and bookmark in cosine similarity model. This is because in the new version e-book, learning footprints in 10 image pages need to be transferred but the <b>text</b> <b>similarities</b> with other pages will always be zero if a page only contains image content, which makes finding correct target page is impossible. According to the results, this problem can be solved by the proposed method, which is page image & <b>text</b> <b>similarity</b> comparison. Since it will first compare image similarities and find target pages that with 100 % image similarities and transfer learning footprints, then compares <b>text</b> <b>similarities</b> to keep looking for target page. As shown in the results, the error rate can be decreased to 0.111 and the F-measure score can be increased to 0.875.|$|R
5000|$|... eTBLAST - {{a natural}} {{language}} <b>text</b> <b>similarity</b> engine for MEDLINE and other text databases.|$|R
40|$|Background: Computational {{methods have}} been used to find {{duplicate}} biomedical publications in MEDLINE. Full text articles are becoming increasingly available, yet the similarities among them have not been systematically studied. Here, we quantitatively investigated the full <b>text</b> <b>similarity</b> of biomedical publications in PubMed Central. Methodology/Principal Findings: 72, 011 full text articles from PubMed Central (PMC) were parsed to generate three different datasets: full texts, sections, and paragraphs. <b>Text</b> <b>similarity</b> comparisons were performed on these datasets using the <b>text</b> <b>similarity</b> algorithm eTBLAST. We measured the frequency of similar text pairs and compared it among different datasets. We found that high abstract similarity can be used to predict high full <b>text</b> <b>similarity</b> with a specificity of 20. 1 % (95 % CI [17. 3 %, 23. 1 %]) and sensitivity of 99. 999 %. Abstract <b>similarity</b> and full <b>text</b> <b>similarity</b> have a moderate correlation (Pearson correlation coefficient: 20. 423) when the similarity ratio is above 0. 4. Among pairs of articles in PMC, method sections are found to be the most repetitive (frequency of similar pairs, methods: 0. 029, introduction: 0. 0076, results: 0. 0043). In contrast, among a set of manually verified duplicate articles, results are the most repetitive sections (frequency o...|$|R
30|$|In text level analysis, we {{analyze and}} compare two {{different}} <b>text</b> <b>similarity</b> representation models cosine similarity and Jaccard similarity. We compare 42 performances {{according to the}} tested threshold and gold-standard data set, then choose the best one within them. Similar to method 1, for each tested threshold from 0 to 1, if the <b>text</b> <b>similarity</b> between source page and target page {{is higher than the}} tested threshold and meanwhile it is the similarity that most close to the tested threshold, learning footprints on that page will be transferred to its target page in the new revision of learning material. On the other hand, if all the <b>text</b> <b>similarities</b> between source page and other pages in new version e-book are lower than the tested threshold, learning footprints on that page will be removed from the BookRoll.|$|R
40|$|This paper {{presents}} a knowledge-based method {{for measuring the}} semanticsimilarity of texts. While {{there is a large}} body of previous work focused on finding the semantic similarity of concepts and words, the application of these wordoriented methods to <b>text</b> <b>similarity</b> has not been yet explored. In this paper, we introduce a method that combines wordto -word similarity metrics into a text-totext metric, and we show that this method outperforms the traditional <b>text</b> <b>similarity</b> metrics based on lexical matching...|$|R
40|$|Abstract. <b>Text</b> <b>similarity</b> join {{operator}} joins two relations {{if their}} join attributes are textually similar to each other, {{and it has}} a variety of application domains including integration and querying of data from heterogeneous resources; cleansing of data; and mining of data. Although, the <b>text</b> <b>similarity</b> join operator is widely used, its processing is expensive due to the huge number of similarity computations performed. In this paper, we incorporate some short cut evaluation techniques from the Information Retrieval domain, namely Harman, quit, continue, and maximal similarity filter heuristics, into the previously proposed <b>text</b> <b>similarity</b> join algorithms {{to reduce the amount of}} similarity computations needed during the join operation. We experimentally evaluate the original and the heuristic based similarity join algorithms using real data obtained from the DBLP Bibliography database, and observe performance improvements with continue and maximal similarity filter heuristics. ...|$|R
40|$|Abstract. In view of {{the fact}} that {{traditional}} vector space model for <b>text</b> <b>similarity</b> calculation which does not take word order into consideration leads to bias, this paper puts forward a longest common subsequence and the traditional vector space model of combining <b>text</b> <b>similarity</b> calculation. This method takes the word order and word frequency information into account, using the texts of the longest common subsequence and substring of their information from all public records and the use of word order and word frequency in the text. The importance of similarity calculation is acknowledged, and the traditional vector space model in the calculation of the weight is used on the word frequency information. Some of the dataset collected through the web crawler are used in the proposed <b>text</b> <b>similarity</b> calculation method for testing, and the results proved the effectivity of the method...|$|R
40|$|The growing use of cyber-services {{automatically}} impart {{great importance}} to cybersecurity. The Internet is a primary source of information regarding software flaws, vulnerabilities, cyber-attacks and exploits. This information is available through vulnerability databases, news articles, security bulletins and blogs. Variety of applications and security systems like Intrusion Detection Systems (IDS), Intrusion Prevention System (IPS), etc. {{can take advantage of}} this information for consolidating their infrastructure. The lack of availability of ready text corpus of high quality security information from various sources makes it difficult for these applications to use this information. To overcome this problem our work focuses on building a multi-genre corpus of security text using information retrieved from multiple internet based sources; National Vulnerabilities Database, Wikipedia articles, security blogs, security bulletins and scholarly papers. The system builds a text classifier from the initial high quality data which is used to classify and accommodate new data from these sources into the corpus. This corpus can be used by variety of applications like IDS or IPS, in variety of ways like assertion into knowledge base or extraction of named entities. Our work explores one of the applications of generating the semantic <b>text</b> <b>similarity</b> model for cybersecurity text. We use the multi-genre cybersecurity text corpus for creating the word co-occurrence model. This model can extract the synonymity between the different security terms. For example, the words 'virus' and 'malware' that have same context are scored for their degree of similarity. The word co-occurrence model is then extended to generate a semantic <b>text</b> <b>similarity</b> model. The <b>text</b> <b>similarity</b> model extracts the semantic <b>text</b> <b>similarity</b> between different security texts like titles of the papers, vulnerability descriptions, blog paragraphs, etc. The system also develops a combined <b>text</b> <b>similarity</b> model from cybersecurity similarity model and generic <b>text</b> <b>similarity</b> model. This model can be used in document mining for matching security text, clustering documents describing similar vulnerabilities and so on...|$|R
40|$|Abstract – Correct and {{efficient}} text classification {{is a major}} challenge in today’s world of rapidly increasing amount of accessible electronic text data. Kohonen networks have been applied to document classification with comparable success to other document clustering methods. An important challenge is to devise <b>text</b> <b>similarity</b> metrics that can improve the performance of text classification Kohonen networks by integrating more semantic information into the metric. Here we propose an augmented metric for <b>text</b> <b>similarity</b> {{that is based on}} the comparison of word consecutiveness graphs of documents. We show that using the proposed augmented similarity metric Kohonen networks perform better than Kohonen networks using usual Euclidean distance metric comparison of word frequency vectors. Our results indicate that word consecutiveness graph comparison includes more semantic information into the <b>text</b> <b>similarity</b> measure improving <b>text</b> classification performance. Key words – augmented metric, Kohonen network, text classification, word consecutiveness grap...|$|R
40|$|<b>Text</b> <b>similarity</b> join {{operator}} joins two relations {{if their}} join attributes are textually similar to each other, {{and it has}} a variety of application domains including integration and querying of data from heterogeneous resources; cleansing of data; and mining of data. Although, the <b>text</b> <b>similarity</b> join operator is widely used, its processing is expensive due to the huge number of similarity computations performed. In this paper, we incorporate some short cut evaluation techniques from the Information Retrieval domain, namely Harman, quit, continue, and maximal similarity filter heuristics, into the previously proposed <b>text</b> <b>similarity</b> join algorithms {{to reduce the amount of}} similarity computations needed during the join operation. We experimentally evaluate the original and the heuristic based similarity join algorithms using real data obtained from the DBLP Bibliography database, and observe performance improvements with continue and maximal similarity filter heuristics. © Springer-Verlag Berlin Heidelberg 2005...|$|R
40|$|Finding related {{published}} articles {{is an important}} task in any science, but with the explosion of new work in the biomedical domain it has become especially challenging. Most existing methodologies use <b>text</b> <b>similarity</b> metrics to identify whether two articles are related or not. However biomedical knowledge discovery is hypothesis-driven. The most related articles may not be ones with the highest <b>text</b> <b>similarities.</b> In this study, we first develop an innovative crowd-sourcing approach to build an expert-annotated document-ranking corpus. Using this corpus as the gold standard, we then evaluate the approaches of using <b>text</b> <b>similarity</b> to rank the relatedness of articles. Finally, we develop and evaluate a new supervised model to automatically rank related scientific articles. Our results show that authors' ranking differ significantly from rankings by text-similarity-based models. By training a learning-to-rank model on {{a subset of the}} annotated corpus, we found the best supervised learning-to-rank model (SVM-Rank) significantly surpassed state-of-the-art baseline systems. Comment: 12 pages, 1 figur...|$|R
40|$|<b>Text</b> <b>similarity</b> {{calculation}} is {{the basic}} work {{in the application of}} Chinese information processing. A high-quality <b>text</b> <b>similarity</b> calculation method must be accurate and efficient, that is, it can be able to compare texts from the level of text natural language meaning, and arrive at the similarity distinction similar to artificial reading based on a full understanding of the author or text source semantic. At the same time, it should also be an efficient algorithm to save the processing time in facing large amount of text information to be processed. Through the research of many domestic and foreign literature, analysis and further research on current situation of similarity calculation, this paper intended to present a new method to improve the performance of similarity calculation, namely a Chinese <b>text</b> <b>similarity</b> algorithm based on word-number difference, which combined the traditional based on statistics and the narrow semantic method that meant the combination of the statistical efficiency and semantic accuracy. Combining the advantages of statistics and semantic category also means the necessity to face and overcome disadvantages of the two kinds of methods. This paper attempted to take the difference in word-number as the breakthrough point, took advantage of the diversity of Chinese word-number, combining with the word frequency, number and meaning, in order to successfully extend the word similarity calculation to the <b>text</b> <b>similarity</b> calculation. Finally, introduced the self built small text set as test object, compared similarity calculation of different methods in the laboratory environment. It shows that the similarity calculation method based on difference in word-number performances better than the traditional methods based on statistical and semantic. Through artificial comparison of the test results of research on this topic in accuracy and speed of segmentation, provide a new approach for Chinese <b>text</b> <b>similarity</b> calculatio...|$|R
40|$|<b>Text</b> <b>similarity</b> {{detection}} aims at {{measuring the}} degree of similarity between a pair of texts. Corpora available for <b>text</b> <b>similarity</b> detection are designed to evaluate the algorithms to assess the paraphrase level among documents. In this paper we present a textual German corpus for similarity detection. The purpose of this corpus is to automatically assess the similarity between a pair of texts and to evaluate different similarity measures, both for whole documents or for individual sentences. Therefore we have calculated several simple measures on our corpus based on a library of similarity functions. Comment: 1 figure; 13 page...|$|R
30|$|In {{the error}} distribution, {{we can see}} that IR is the most {{significant}} error type in image similarity comparison, 12 incorrect removes of marker, memo, and bookmark in NMSE model, respectively. This is because in image similarity comparison, the image similarities decreased due to the change of location of image contents and paragraphs in a page, and {{it would be hard to}} find correct target page in this situation. According to the results, this problem can be solved by <b>text</b> <b>similarity</b> comparison since it is able to find similar pages based on <b>text</b> <b>similarities</b> instead of image, and the target page can be found even the paragraphs changed.|$|R
40|$|AbstractÐWe {{propose a}} method for text {{retrieval}} from document images {{without the use of}} OCR. Documents are segmented into character objects. Image features, namely, the Vertical Traverse Density (VTD) and Horizontal Traverse Density (HTD), are extracted. An n-gram based document vector is constructed for each document based on these features. <b>Text</b> <b>similarity</b> between documents is then measured by calculating the dot product of the document vectors. Testing with seven corpora of imaged textual documents in English and Chinese as well as images from UW 1 database confirms the validity of the proposed method. Index TermsÐDocument image analysis, document vector, <b>text</b> <b>similarity,</b> <b>text</b> retrieval. æ...|$|R
40|$|In {{order to}} improve the {{accuracy}} of short <b>text</b> <b>similarity</b> calculation, this paper presents the idea that use the history of short text messages to construct semantic feature space, then use the vector in semantic feature space to represent short text and do semantic extension, and finally calculate the short <b>text</b> <b>similarity</b> of corresponding vector in the semantic feature space. This method can represent the semantic information of short text message thoroughly so as to improve the accuracy of similarity calculation. We selected {{a large number of}} problem test sets for experiments. The results show that the method we proposed is reasonable and effective...|$|R
40|$|Thread disentanglement is {{the task}} of sep-arating out conversations whose thread {{structure}} is implicit, distorted, or lost. In this paper, we perform email thread dis-entanglement through pairwise classifica-tion, using <b>text</b> <b>similarity</b> measures on non-quoted texts in emails. We show that i) content <b>text</b> <b>similarity</b> metrics out-perform style and structure text similar-ity metrics in both a class-balanced and class-imbalanced setting, and ii) although feature performance {{is dependent on the}} semantic similarity of the corpus, con-tent features are still effective even when controlling for semantic similarity. We make available the Enron Threads Corpus, a newly-extracted corpus of 70, 178 multi-email threads with emails from the Enron Email Corpus. ...|$|R
40|$|Computational {{methods have}} been used to find {{duplicate}} biomedical publications in MEDLINE. Full text articles are becoming increasingly available, yet the similarities among them have not been systematically studied. Here, we quantitatively investigated the full <b>text</b> <b>similarity</b> of biomedical publications in PubMed Central. 72, 011 full text articles from PubMed Central (PMC) were parsed to generate three different datasets: full texts, sections, and paragraphs. <b>Text</b> <b>similarity</b> comparisons were performed on these datasets using the <b>text</b> <b>similarity</b> algorithm eTBLAST. We measured the frequency of similar text pairs and compared it among different datasets. We found that high abstract similarity can be used to predict high full <b>text</b> <b>similarity</b> with a specificity of 20. 1 % (95 % CI [17. 3 %, 23. 1 %]) and sensitivity of 99. 999 %. Abstract <b>similarity</b> and full <b>text</b> <b>similarity</b> have a moderate correlation (Pearson correlation coefficient: - 0. 423) when the similarity ratio is above 0. 4. Among pairs of articles in PMC, method sections are found to be the most repetitive (frequency of similar pairs, methods: 0. 029, introduction: 0. 0076, results: 0. 0043). In contrast, among a set of manually verified duplicate articles, results are the most repetitive sections (frequency of similar pairs, results: 0. 94, methods: 0. 89, introduction: 0. 82). Repetition of introduction and methods sections {{is more likely to be}} committed by the same authors (odds of a highly similar pair having at least one shared author, introduction: 2. 31, methods: 1. 83, results: 1. 03). There is also significantly more similarity in pairs of review articles than in pairs containing one review and one nonreview paper (frequency of similar pairs: 0. 0167 and 0. 0023, respectively). While quantifying abstract similarity is an effective approach for finding duplicate citations, a comprehensive full text analysis is necessary to uncover all potential duplicate citations in the scientific literature and is helpful when establishing ethical guidelines for scientific publications...|$|R
40|$|Abstract—In {{order to}} improve the {{accuracy}} of short <b>text</b> <b>similarity</b> calculation, this paper presents the idea that use the history of short text messages to construct semantic feature space, then use the vector in semantic feature space to represent short text and do semantic extension, and finally calculate the short <b>text</b> <b>similarity</b> of corresponding vector in the semantic feature space. This method can represent the semantic information of short text message thoroughly so as to improve the accuracy of similarity calculation. We selected {{a large number of}} problem test sets for experiments. The results show that the method we proposed is reasonable and effective. Keywords—short text; semantic feature space; similarity; semantic similarity I...|$|R
40|$|This paper reports {{experiments}} on a corpus of news articles from the Financial Times, comparing different <b>text</b> <b>similarity</b> models. First the Ferret system using a method {{based solely on}} lexical similarities is used, then methods based on semantic similarities are investigated. Different feature string selection criteria are used, for instance with and without synonyms obtained from WordNet, or with noun phrases extracted for comparison. The results indicate that synonyms rather than lexical strings are important for finding similar texts. Hypernyms and noun phrases {{also contribute to the}} identification of <b>text</b> <b>similarity,</b> though they are not better than synonyms. However, precision is a problem for the semantic similarity methods because too many irrelevant texts are retrieved. ...|$|R
40|$|In this paper, {{the authors}} present a {{knowledge-based}} method {{for measuring the}} semantic-similarity of texts. Through experiments performed on two different applications: (1) paraphrase and entailment identification, and (2) word sense similarity, the authors show that this method outperforms the traditional <b>text</b> <b>similarity</b> metrics based on lexical matching...|$|R
40|$|Our system {{combines}} <b>text</b> <b>similarity</b> measures with a textual entailment system. In {{the main}} task, {{we focused on}} the influence of lexicalized versus unlexicalized features, and how they affect performance on unseen questions and domains. We also participated in the pilot partial entailment task, where our system significantly outperforms a strong baseline...|$|R
40|$|This paper {{describes}} the system {{used by the}} LIPN team in the Semantic Textual Similarity task at *SEM 2013. It uses a support vector regression model, combining different <b>text</b> <b>similarity</b> measures that constitute the features. These measures include simple distances like Levenshtein edit distance, cosine, Named Entities overlap and more complex distances lik...|$|R
40|$|We {{present an}} {{approach}} {{for the construction}} of <b>text</b> <b>similarity</b> functions using a parameterized resemblance coefficient in combination with a softened cardinality function called soft cardinality. Our approach provides a consistent and recursive model, varying levels of granularity from sentences to characters. Therefore, our model was used to compare sentences divided into words, and in turn, words divided into q-grams of characters. Experimentally, we observed that a performance correlation function in a space defined by all parameters was relatively smooth and had a single maximum achievable by “hill climbing. ” Our approach used only surface text information, a stop-word remover, and a stemmer to tackle the semantic <b>text</b> <b>similarity</b> task 6 at SEMEVAL 2012. The proposed method ranked 3 rd (average), 5 th (normalized correlation), and 15 th (aggregated correlation) among 89 systems submitted by 31 teams. ...|$|R
40|$|Despite {{lack of any}} {{significant}} quantifiable similarities between documents, people can intuitively compare documents and evaluate their similarity. To understand how people evaluate <b>text</b> <b>similarity,</b> we queried subjects {{about the level of}} content similarity and expression similarity of pairs of documents. Using these judgments on similarity as ground truth, we automated evaluation of similarity...|$|R
40|$|With recent {{research}} {{interest in the}} confounding roles of homophily and contagion in studies of social influence, {{there is a strong}} need for reliable content-based measures of the similarity between people. In this paper, we investigate the use of <b>text</b> <b>similarity</b> measures as a way of predicting the similarity of prolific weblog authors. We describe a novel method of collecting human judgments of overall similarity between two authors, as well as demographic, political, cultural, religious, values, hobbies/interests, personality, and writing style similarity. We then apply a range of automated textual similarity measures based on word frequency counts, and calculate their statistical correlation with human judgments. Our findings indicate that commonly used <b>text</b> <b>similarity</b> measures do not correlate well with human judgments of author similarity. However, various measures that pay special attention to personal pronouns and their context correlate significantly with different facets of similarity...|$|R
40|$|The larger {{amount of}} high quality and {{specialised}} information on the Web is stored in document databases, which is not indexed by general-purpose search engines such as Google and Yahoo. Such information is dynamically generated {{as a result of}} submitting queries to databases — which are referred to as Hidden Web databases. This paper presents a Two-Phase Sampling (2 PS) technique that detects Web page templates from the randomly sampled documents of a database. It generates terms and frequencies that summarise the database content with improved accuracy. We then utilise such statistics to improve the accuracy of <b>text</b> <b>similarity</b> computation in categorisation. Experimental results show that 2 PS effectively eliminates terms contained in Web page templates, and generates terms and frequencies with improved accuracy. We also demonstrate that 2 PS improves the accuracy of <b>text</b> <b>similarity</b> computation required in the process of database categorisation...|$|R
40|$|Early {{work in the}} {{computational}} {{treatment of}} natural language focused on summariza-tion, and machine translation. In my research I have concentrated on the area of summariza-tion of documents in different languages. This thesis presents my work on multi-lingual <b>text</b> <b>similarity.</b> This work enables the identification of short units of text (usually sentences) that contain similar information {{even though they are}} written in different languages. I present my work on SimFinderML, a framework for multi-lingual <b>text</b> <b>similarity</b> computation that makes it easy to experiment with parameters for similarity computation and add support for other languages. An in-depth examination and evaluation of the system is performed using Arabic and English data. I also apply the concept of multi-lingual <b>text</b> <b>similarity</b> to summarization in two different systems. The first improves readability of English sum-maries of Arabic text by replacing machine translated Arabic sentences with highly similar English sentences when possible. The second is a novel summarization system that supports comparative analysis of Arabic and English documents in two ways. First, given Arabic and English documents that describe the same event, SimFinderML clusters sentences to present information that is supported by both the Arabic and English documents. Second, the system provides an analysis of how the Arabic and English documents differ by pre-senting information that is supported exclusively by documents in only one language. This novel form of summarization is a first step at analyzing the difference in perspectives from news reported in different languages...|$|R
30|$|In this paper, we {{identified}} the problem when e-book users {{trying to find}} the learning footprints they made in the new version e-book. To address that, we compared three methods for learning footprint transferring across slide-based e-book revisions in three different levels, page image <b>similarity,</b> page <b>text</b> <b>similarity,</b> and page image & <b>text</b> <b>similarity.</b> We analyzed the optimal threshold and evaluated the performance of learning footprint transferring for each method in different levels, and the error distribution was given for the detail of errors. The performances of learning footprint transferring are presented by F-measure in a confusion matrix. According to the results, the best F-measure scores of methods 1, 2, and 3 are 0.741, 0.759, and 0.875, respectively. According to the F-measure scores, we propose method 3 which is comparing page similarity in image & text level, as the optimal method to automatically transfer learning footprints across e-book revisions. In this method, we used NMSE model and image similarity threshold 0.95 for image content processing, similarity representation, and similar page determination to find similar pages and transfer learning footprints. For the rest of the pages in an old version e-book, we then we used TFIDF weighting method, cosine <b>similarity</b> model, and <b>text</b> <b>similarity</b> threshold 0.7 for <b>text</b> content processing, <b>similarity</b> representation, and similar page determination within a slide-based e-book to find similar pages between versions and transfer learning footprints. According to the evaluation and error analysis, the location of image contents and text contents needs to be considered well; otherwise, learning footprints will be incorrectly transferred to the wrong location even the target page can be successfully found.|$|R
40|$|Speech {{recognition}} {{failures and}} limited vocabulary coverage pose challenges for speech interaction with characters in games. We describe an end-to-end system for automating characters {{from a large}} corpus of recorded human game logs, and demonstrate that inferring utterance meaning {{through a combination of}} plan recognition and surface <b>text</b> <b>similarity</b> compensates for recognition and understanding failures significantly better than relying on surface similarity alone...|$|R
40|$|With {{large number}} of {{documents}} on the web, there is a increasing {{need to be able}} to retrieve the best relevant document. There are different techniques through which we can retrieve most relevant document from the large corpus. Similarity between words, sentences, paragraphs and documents is an important component in various tasks such as information retrieval, document clustering, word-sense disambiguation, automatic essay scoring, short answer grading, machine translation and <b>text</b> summarization. <b>Text</b> <b>similarity</b> means user’s query text is matched with the document text and on the basis on this matching user retrieves the most relevant documents. <b>Text</b> <b>similarity</b> also plays an important role in the categorization of text as well as document. We can measure the similarity between sentences, words, paragraphs and documents to categorize them in an efficient way. On the basis of this categorization, we can retrieve the best relevant document corresponding to user’s query. This paper describes different types of similarity like lexical similarity, semantic similarity etc...|$|R
40|$|In this dissertation, I explore {{unsupervised}} {{techniques for}} the task of automatic short answer grading. I compare a number of knowledge-based and corpus-based measures of <b>text</b> <b>similarity,</b> evaluate the effect of domain and size on the corpus-based measures, and also introduce a novel technique to improve {{the performance of the}} system by integrating automatic feedback from the student answers. I continue to combine graph alignment features with lexical semantic similarity measures and employ machine learning techniques to show that grade assignment error can be reduced compared to a system that considers only lexical semantic measures of similarity. I also detail a preliminary attempt to align the dependency graphs of student and instructor answers in order to utilize a structural component that is necessary to simulate human-level grading of student answers. I further explore the utility of these techniques to several related tasks in natural language processing including the detection of <b>text</b> <b>similarity,</b> paraphrase, and textual entailment...|$|R
40|$|This paper {{studies the}} {{integration}} of lexical semantic knowledge in two related semantic computing tasks: ad-hoc information retrieval and computing <b>text</b> <b>similarity.</b> For this purpose, we compare the performance of two algorithms: (i) using semantic relatedness, and (ii) using a conventional extended Boolean model [13] with additional query expansion. For the evaluation, we use two different test collections in the German language especially suitable to study the vocabulary gap problem: (i) GIRT [5] for the information retrieval task, and (ii) a collection of descriptions of professions built to evaluate a system for electronic career guidance in the information retrieval and <b>text</b> <b>similarity</b> tasks. We found that integrating lexical semantic knowledge increases the performance for both tasks. On the GIRT corpus, the performance is improved only for short queries. The performance on the collection of professional descriptions is improved, but crucially depends on the accurate preprocessing of the natural language essays employed as topics...|$|R
