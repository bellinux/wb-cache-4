44|13|Public
5000|$|Tiboxe: TiBoXe server is a web-application server {{that allows}} {{to work with}} terminologic {{information}} based on translations from open source projects translations. TiBoXe stands on TBX format, a standard that allows to exchange <b>terminological</b> <b>information</b> with XML files. Released in 2009 in GPL v3.|$|E
40|$|The aim of {{this article}} is to present the first results of a project on the {{retrieval}} of <b>terminological</b> <b>information</b> from machine-readable Danish corpora in connection with practical terminology work. Works by Ahmad (1994), Bowker (1996), and Meyer & Mackintosh (1996) about linguistic signals, and especially verbs, inspired me to study the use of such signals to extract <b>terminological</b> <b>information</b> from a Danish corpus. Ahmad uses the term ‘knowledge probe ’ to refer to lexical phrases and verbs which often occur together with terminological data in authentic texts and may thus be used as search patterns for identifying and extracting <b>terminological</b> <b>information.</b> I find his choice of denotation appropriate for terminology work, especially in connection with concept-related information, which can be defined as knowledge-rich information. In order to provide a focussed and fine-grained approach, a valency theory of verbs called the Pronominal Approach (PA) was implemented in my studies. The objective of my studies has been to examine whether the valency patterns of a number of verbs are suitable as knowledge probes and thus also as search patterns in corpus-based terminolog...|$|E
40|$|The {{objective}} {{of this paper is}} to report on the implementation of the BILingual Information Browser (BILIB). BILIB in a greater extend than other systems allow users to retrieve <b>terminological</b> <b>information,</b> explore conceptual knowledge about terms and navigate transparently between a terminological database and a formal conceptual knowledge base. Major emphasis is given to the structure and design of the Knowledge Base, which has been designed and is being developed on the basis of EuroWordNet top ontology and base concepts...|$|E
40|$|The paper {{contains}} {{formulation of}} basic theoretical principles pertaining to thermal protection {{of an oil}} transformer in accordance with classical theory of relay protection and theory of diagnostics {{with the purpose of}} unification of <b>terminological</b> and analytical <b>information</b> which is presently available in respect of this problem. Classification of abnormal thermal modes of an oil transformer and also algorithms and methods for operation of diagnostic thermal protection of a transformer have been proposed...|$|R
40|$|Terminological {{systems such}} as SNOMED CT play an {{increasingly}} important role in contemporary record keeping. This drives the need of assessing the content of these systems, {{as well as the}} content of medical records captured using these systems. In this paper, the use of information content as a measure for the structure of terminological systems and the content in medical records is explored. Two complementary information-content-based measures for terminological systems are proposed: the proportion of concepts with zero information content, and the average information content. The measures are applied to the latest releases of SNOMED CT. The measures are useful as an indicator of the overall structure of terminological systems or parts thereof. Furthermore, two measures are described which can provide an estimate for the content of medical records that is captured using a <b>terminological</b> system. <b>Information</b> content is shown to provide a useful basis for assessing the structure of terminological systems and the content of medical record...|$|R
40|$|The {{purpose of}} this {{research}} was to develop an issue-oriented syllabus retrieval system that combined <b>terminological</b> processing, <b>information</b> retrieval, similarity calculation-based document clustering, and visualization. Recently, scientific knowledge has grown explosively because of rapid advancements that have occurred in academia and society. Because of this dramatic expansion of knowledge, learners and educators sometimes struggle to comprehend the overall aspects of syllabi. In addition, learners may find it difficult to discover appropriate courses of study from syllabi because of the increasing growth of interdisciplinary studies programs. We believe that an issue-oriented syllabus structure might be more efficient because it provides clear directions for users. In this paper, we introduce an issue-oriented automatic syllabus retrieval system that integrates automatic term recognition as an issue extraction, and similarity calculation as terminology-based document clustering. We use automatically-recognized terms to represent each lecture in clustering and visualization. Retrieved syllabi are automatically classified based on their included terms or issues. The main goal of syllabus retrieval and classification is the development of a...|$|R
40|$|In {{this article}} {{we focus on the}} process of {{understanding}} terms in texts. We explain how a method for terminological analysis of specialised texts has been set up in order to develop ontologically-underpinned terminological resources. We will concentrate on the categorisation framework, which is used in termontography for structuring <b>terminological</b> <b>information.</b> This framework is currently implemented in a didactic software tool, called CatTerm. Student translators using CatTerm construct a knowledge model of a given domain while reading a corpus of specialised texts in the target and source languages of the translation exercise...|$|E
40|$|There {{have been}} several {{attempts}} to realize {{the idea of a}} fully automatic translation sy stem for text translation to replace human translators. By contrast, little work has been put into building tools to aid human translators. This report describes the ideas behind such a tool. The tool is intended to aid human translators in achieving higher productivity and better quality, by presenting <b>terminological</b> <b>information</b> extracted from previous translations. The report documents the implementation and evaluation of a prototype. The prototype has been demonstrated to and used by professional translators with promising results...|$|E
40|$|This paper {{discusses}} {{the design and}} implementation of Termino, a large-scale terminological resource for text processing. Dealing with terminology is a difficult but unavoidable task for natural language processing applications, such as information extraction in technical domains. Complex, heterogeneous information must be stored about large numbers of terms. At the same time term recognition must be performed in realistic times. Termino attempts to reconcile this tension by maintaining a flexible, extensible relational database for storing <b>terminological</b> <b>information</b> and compiling finite state machines from this database to do term recognition. 1...|$|E
40|$|We {{examine the}} {{strategies}} of organizing termino-logical information in WordNet, and describe an analogous strategy of adding terminological senses of lexical units to plWordNet, a large Polish word-net. Wordnet builders must cope with differences in lexical and terminological definitions of a term, and with the boundaries between <b>terminological</b> and lexical <b>information.</b> A somewhat adjusted strat-egy is required for Polish, though both WordNet and plWordNet rely mainly on semantic relations in organizing the terminological and general-language units. The proposed guidelines for plWordNet, built on several distinct combinations of denotation and connotation, have a solid theoretical underpinning but will require a large-scale verification of their ef-fectiveness in practice. ...|$|R
40|$|This paper {{describes}} {{work done}} {{to explore the}} common ground between two different ongoing research projects: the standardization of lexical and terminological resources, {{and the use of}} conceptual ontologies for information extraction and data integration. Specifically, this paper explores improving the generation of extraction ontologies through use of a comprehensive terminology database that has been represented in a standardized format for easy tool-based implementation. We show how, via the successful integration of these two distinct efforts, it is possible to leverage large-scale <b>terminological</b> and conceptual <b>information</b> having relationship-rich semantic resources in order to reformulate, match, and merge retrieved information of interest to a user...|$|R
40|$|The {{terminology}} used in biomedicine has lexical characteristics that have required {{the elaboration of}} <b>terminological</b> resources and <b>information</b> retrieval systems with specific functionalities. The main characteristics are the high rates of synonymy and homonymy, due to phenomena such as the proliferation of polysemic acronyms and their interaction with common language. Information retrieval systems in the biomedical domain use techniques oriented {{to the treatment of}} these lexical peculiarities. In this paper we review some of these techniques, such as the application of Natural Language Processing (BioNLP), the incorporation of lexical-semantic resources, and the application of Named Entity Recognition (BioNER). Finally, we present the evaluation methods adopted to assess the suitability of these techniques for retrieving biomedical resources...|$|R
40|$|International audienceIn {{this paper}} we present our ongoing work on {{integrating}} large-scale <b>terminological</b> <b>information</b> into NLP tools. We {{focus on the}} problem of selecting and generating a set of suitable terms from the resources, based on deletion, modification and addition rules. We propose a general framework in which the raw data of the resources are first loaded into a knowledge base (KB). The selection and generation rules are then defined in a declarative way using query templates in the query language of the KB system. We illustrate the use of this framework to select and generate term sets from a UMLS dataset...|$|E
40|$|Abstract: This paper {{proposes a}} model for formalizing concept {{characteristics}} {{in a manner consistent}} with the Terminology theoretical framework. We will focus on the identification of the elements that take part in characteristic formalization and their role, as well as on the double nature of characteristic. Finally, we provide a preliminary outline of characteristics description formalization using an ontology editor called Protégé-OWL. This article shows the preliminary work of a broader project that deals with the representation of characteristics and its <b>terminological</b> <b>information</b> using Protégé-OWL. It is addressed to terminology researchers and developers that may use ontologies in order to represent specific domain conceptual systems...|$|E
40|$|Abstract. Multilingual Information Retrieval usually {{forces a}} choice between free text {{indexing}} or indexing by means of multilingual thesaurus. However, since they share the same objectives, synergy between both approaches is possible. This paper shows a retrieval framework that make use of <b>terminological</b> <b>information</b> in free-text indexing. The Automatic Terminology Extraction task, which is used for thesauri construction, shifts to a searching of terminology and becomes an information retrieval task: Terminology Retrieval. Terminology Retrieval, then, allows cross-language information retrieval through the browsing of morpho-syntactic, semantic and translingual variations of the query. Although terminology retrieval doesn’t make use of them, controlled vocabularies become an appropriate framework for terminology retrieval evaluation. ...|$|E
40|$|Abstract. Recently, {{an overall}} trend towards {{increasing}} complexity of ontologies could be observed, {{not only in}} terms of domain modeling, where the complexity should correspond to the information to be mod-eled, but also as regards the addition of further information, which could be modeled as external resources to the domain model and linked to its relevant elements. This concerns the addition of <b>terminological</b> and lin-guistic <b>information</b> to the description of classes and properties of ontolo-gies. To respond to this development, we propose a functional approach to the modularization of ontologies, based on terminological, linguistic, and conceptual functions each module fulfills. Only the conceptual ele-ments and their structural properties should remain in the domain model, whereas the formalized terminology and linguistics are described in inde-pendent modules referencing the domain models. We provide examples of such complexity in Knowledge Representation systems, discuss related work, and present our approach to modularization in detail...|$|R
40|$|AbstractThe {{versatility}} {{of linguistic}} corpora makes them an excellent conceptual and terminological resource. For their part, knowledge bases are rapidly gathering {{momentum in the}} field of terminology, since they allow for the creation of multidimensional conceptual systems, a significant leap forward from the traditional hierarchical or tree framework. In this paper we propose the use of a Spanish and French comparable corpus to extract <b>terminological</b> and conceptual <b>information</b> on galactosemia, a disorder belonging to the medical sub-domain of rare diseases, followed by the construction of an ontology to create a terminological knowledge base on this pathology 11 This research project was partially funded by the Department of Culture, Education and Universities of the Regional Government of Galicia (Spain) through grants awarded for the Consolidation and Structuring of Competitive Research Units within the Galician University System (ref. CN 2012 / 317) [...] The {{purpose of this study is}} to further the development of resources that provide a satisfactory description of rare diseases, enabling translators, specialised authors, medical professionals and other users to accurately and properly understand and write texts on the topic...|$|R
40|$|Nowadays {{there is}} an {{explosion}} of data repositories available in the Global Information System. In this context, users meet with the problems related to the heterogeneity and distribution of the data repositories. Access to the data may be facilitated by providing semantic views over the repositories that hide all the technical and organizative details associated with them. In this paper it is presented {{the definition of the}} mapping relation that permits one to relate data elements of the semantic views and those of the underlying data repositories. We define the syntax and the semantics of the mapping. From the syntactic point of view, mapping expressions are tuples which main components are relational algebra expressions. From the semantic point of view, the notions of well formed mapping relation and the extension defined by that mapping are introduced. Moreover, we demonstrate the completeness property of the mapping relation {{in the sense that it}} allows for answers to all queries formulated over the semantic view using the data stored in the data repositories. Copyright c fl 1999 Elsevier Science Ltd Key words: <b>Terminological</b> Systems, Mapping <b>Information,</b> Global Information Systems, Data Repositories 1...|$|R
40|$|In this {{communication}} {{we present}} a comparative evaluation {{of the effectiveness of}} search engines (SE) and linguistic tools (LT) to retrieve <b>terminological</b> <b>information</b> from the net, in the context of specialized translation tasks. For achieving that goal, an experiment with translators has been carried out. The results indicate that SE are more effective than LT in situations where the answer is partially ignored by the translator (i. e. the translator is hypothesizing one or several possible answers in the target language before searching). On the other hand, LT have not been either more appropriate in situations where the translator showed total ignorance of the possible answers before searching...|$|E
40|$|In {{this paper}} we discuss the design, implementation, and use of Termino, a large scale terminological {{resource}} for text processing. Dealing with terminology is a difficult but unavoidable task for language processing applications, such as Information Extraction in technical domains. Complex, heterogeneous information must be stored about large numbers of terms. At the same time term recognition must be performed in realistic times. Termino attempts to reconcile this tension by maintaining a flexible, extensible relational database for storing <b>terminological</b> <b>information</b> and compiling finite state machines from this database to do term lookup. While Termino has been developed for biomedical applications, its general design allows it {{to be used for}} term processing in any domain. ...|$|E
40|$|We propose {{applying}} standardized linguistic annotation {{to terms}} included in labels of knowledge representation schemes (taxonomies or ontologies), hypothesizing {{that this would}} help improving ontology-based semantic annotation of texts. We share the view that currently used methods for including lexical and <b>terminological</b> <b>information</b> in such hierarchical networks of concepts are not satisfactory, and thus put forward – as a preliminary step to our annotation goal – a model for modular representation of conceptual, terminological and linguistic information within knowledge representation systems. Our CTL model is based on two recent initiatives that describe the representation of terminologies and lexicons in ontologies: the Terminae method for building terminological and ontological models from text (Aussenac-Gilles et al., 2008), and the LexInfo metamodel for ontology lexica (Buitelaar et al., 2009) ...|$|E
40|$|The {{issue of}} {{effective}} and efficient foreign language teaching is relevant in today’s global world. More and more frequently specialized computer programs for foreign vocabulary training are used. They are not expensive and simple to create and use in comparison with published analogues and not less efficient. The paper considers systemic aspects of information base development for multilingual adaptive training technology, specifically the process of information collection of latent lexical relations and its employment in <b>information</b> <b>terminological</b> basis. The authors offer the system of primary text processing, its algorithms and structure of output data. Creations of computer system aimed at training foreign vocabulary {{is presented in the}} consequence of steps, based on the Markov processes theory. Employment of text processing subsystems, created by integration of subsystem of frequency dictionary generation and subsystem of latent vocabulary relations search is a successful solution in building of information technological basis. Using latent lexical relations increases the efficiency of training system of foreign vocabulary in general. The developed structure and algorithm are nor resource consuming, thus making them economical. The developed structure of output data for preliminary text processing subsystem provides information flexibility and integrity...|$|R
40|$|Background: Most of our {{biomedical}} {{knowledge is}} only accessible through texts. The biomedical literature grows exponentially and PubMed comprises over 18. 000. 000 literature abstracts. Recently much {{effort has been}} put into the creation of biomedical ontologies which capture biomedical facts. The exploitation of ontologies to explore the scientific literature is a new area of research. Motivation: When people search, they have questions in mind. Answering questions in a domain requires {{the knowledge of the}} terminology of that domain. Classical search engines do not provide background knowledge for the presentation of search results. Ontology annotated structured databases allow for data-mining. The hypothesis is that ontology annotated literature databases allow for text-mining. The central problem is to associate scientific publications with ontological concepts. This is a prerequisite for ontology-based literature search. The question then is how to answer biomedical questions using ontologies and a literature corpus. Finally the task is to automate bibliometric analyses on an corpus of scientific publications. Approach: Recent joint efforts on automatically extracting information from free text showed that the applied methods are complementary. The idea is to employ the rich <b>terminological</b> and relational <b>information</b> stored in biomedical ontologies to markup biomedical text documents. Based on established semantic links between documents and ontology concepts the goal is to answer biomedical question on a corpus of documents. The entirely annotated literature corpus allows {{for the first time to}} automatically generate bibliometric analyses for ontological concepts, authors and institutions. Results: This work includes a novel annotation framework for free texts with ontological concepts. The framework allows to generate recognition patterns rules from the <b>terminological</b> and relational <b>information</b> in an ontology. Maximum entropy models can be trained to distinguish the meaning of ambiguous concept labels. The framework was used to develop a annotation pipeline for PubMed abstracts with 27, 863 Gene Ontology concepts. The evaluation of the recognition performance yielded a precision of 79. 9 % and a recall of 72. 7 % improving the previously used algorithm by 25, 7 % f-measure. The evaluation was done on a manually created (by the original authors) curation corpus of 689 PubMed abstracts with 18, 356 curations of concepts. Methods to reason over large amounts of documents with ontologies were developed. The ability to answer questions with the online system was shown on a set of biomedical question of the TREC Genomics Track 2006 benchmark. This work includes the first ontology-based, large scale, online available, up-to-date bibliometric analysis for topics in molecular biology represented by GO concepts. The automatic bibliometric analysis is in line with existing, but often out-dated, manual analyses. Outlook: A number of promising continuations starting from this work have been spun off. A freely available online search engine has a growing user community. A spin-off company was funded by the High-Tech Gründerfonds which commercializes the new ontology-based search paradigm. Several off-springs of GoPubMed including GoWeb (general web search), Go 3 R (search in replacement, reduction, refinement methods for animal experiments), GoGene (search in gene/protein databases) are developed...|$|R
40|$|Technological {{advances}} {{and the increase}} in international communication have led to the emergence of new <b>terminological</b> needs in <b>information</b> technology. The standardization of technical vocabulary is a delicate step in the dissemination of knowledge; it often leads to debate since it does not take adequate stock of the circulation of terms. Standardization means that one term only should designate an entity, thus eliminating alternative terms. However, language users are often seen to ignore the guidelines of standardizing bodies advocating the use of terms univocally designating well-defined concepts. Standardization may contribute to avoid terminological anarchy but it has an essentially persuasive function. The enlargment of the EU has led to an increase in the use of English and a parallel, significant decrease in the prestige and use of French. A comparison between the terms published in the Official Journal (in the ‘IT and Internet’ and ‘email’ sections) starting from 2 December 1997 and the actual use of terms as recorded in the IATE term bank shows that standardizing efforts at national level have had some degree of success, even though not all official terms have been embraced by users at EU level. In giving recommendations based on a constantly updated corpus, IATE appears to be an adequate terminological resource in that it recognizes the authority of terminology standardization bodies but refrains from putting pressure on language users. Recognizing the dynamism of information technology terminology deriving from the use of French and English is an essential step in explaining how users negotiate the choice of terms. The « termoscopy » proposed in this paper aims at {{a better understanding of the}} process of terminological democratization which is taking place in the EU...|$|R
40|$|Abstract: Current search systems fail {{to satisfy}} users when the {{relevant}} information {{is written in}} a foreign language; when the user is {{not aware of the}} relevant-perhaps specialized- terminology for a given topic; or when the user need is fuzzy and requires assisted search once inside an appropriate web portal. This paper describes an interactive multilingual search system that alleviates such limitations, through the browsing of phrases in different languages after being automatically extracted from the text collection. The evaluation of WTB has been focussed in two aspects: the capability to offer translingual terminology to users, and the usefulness of phrase browsing. In this sense, the evaluation shows that users consider the new level of <b>terminological</b> <b>information</b> useful, as it complements the traditional document ranking outcome. ...|$|E
40|$|Levesque and Brachman {{argue that}} in order to provide timely and correct {{responses}} in the most critical applications, general purpose knowledge representation systems should restrict their languages by omitting constructs which require non-polynomial worst-case response times for sound and complete classification. They also separate terminological and assertional knowledge, and restrict classification to purely <b>terminological</b> <b>information.</b> We demonstrate that restricting the terminological language and classifier in these ways limits these "general-purpose" facilities so severely that they are no longer generally applicable. We argue that logical soundness, completeness, and worst-case complexity are inadequate measures for evaluating the utility of representation services, and that this evaluation should employ the broader notions of utility and rationality found in decision theory. We suggest that general purpose representation services should provide fully expressive languages, classi [...] ...|$|E
40|$|Abstract. We {{present an}} {{approach}} to classification of biomedical terms {{based on the information}} acquired automatically from the corpus of relevant literature. The learning phase consists of two stages: acquisition of terminologically relevant contextual patterns (CPs) and selection of classes that apply to terms used with these patterns. CPs represent a generalisation of similar term contexts in the form of regular expressions containing lexical, syntactic and <b>terminological</b> <b>information.</b> The most probable classes for the training terms co-occurring with the statistically relevant CP are learned by a genetic algorithm. Term classification is based on the learnt results. First, each term is associated with the most frequently co-occurring CP. Classes attached to such CP are initially suggested as the term’s potential classes. Then, the term is finally mapped to the most similar suggested class. ...|$|E
40|$|To {{meet the}} {{increasing}} {{demands of the}} complex inter-organizational processes {{and the demand for}} continuous innovation and internationalization, it is evident that new forms of organisation are being adopted, fostering more intensive collaboration processes and sharing of resources, in what can be called collaborative networks (Camarinha-Matos, 2006 : 03). Information and knowledge are crucial resources in collaborative networks, being their management fundamental processes to optimize. Knowledge organisation and collaboration systems are thus important instruments for the success of collaborative networks of organisations having been researched in the last decade in the areas of computer science, information science, management sciences, terminology and linguistics. Nevertheless, research in this area didn’t give much attention to multilingual contexts of collaboration, which pose specific and challenging problems. It is then clear that access to and representation of knowledge will happen more and more on a multilingual setting which implies the overcoming of difficulties inherent to the presence of multiple languages, through the use of processes like localization of ontologies. Although localization, like other processes that involve multilingualism, is a rather well-developed practice and its methodologies and tools fruitfully employed by the language industry in the development and adaptation of multilingual content, it has not yet been sufficiently explored as an element of support to the development of knowledge representations - in particular ontologies - expressed in more than one language. Multilingual knowledge representation is then an open research area calling for cross-contributions from knowledge engineering, terminology, ontology engineering, cognitive sciences, computational linguistics, natural language processing, and management sciences. This workshop joined researchers interested in multilingual knowledge representation, in a multidisciplinary environment to debate the possibilities of cross-fertilization between knowledge engineering, terminology, ontology engineering, cognitive sciences, computational linguistics, natural language processing, and management sciences applied to contexts where multilingualism continuously creates new and demanding challenges to current knowledge representation methods and techniques. In this workshop six papers dealing with different approaches to multilingual knowledge representation are presented, most of them describing tools, approaches and results obtained in the development of ongoing projects. In the first case, Andrés Domínguez Burgos, Koen Kerremansa and Rita Temmerman present a software module that is part of a workbench for terminological and ontological mining, Termontospider, a wiki crawler that aims at optimally traverse Wikipedia in search of domainspecific texts for extracting <b>terminological</b> and ontological <b>information.</b> The crawler is part of a tool suite for automatically developing multilingual termontological databases, i. e. ontologicallyunderpinned multilingual terminological databases. In this paper the authors describe the basic principles behind the crawler and summarized the research setting in which the tool is currently tested. In the second paper, Fumiko Kano presents a work comparing four feature-based similarity measures derived from cognitive sciences. The purpose of the comparative analysis presented by the author is to verify the potentially most effective model that can be applied for mapping independent ontologies in a culturally influenced domain. For that, datasets based on standardized pre-defined feature dimensions and values, which are obtainable from the UNESCO Institute for Statistics (UIS) have been used for the comparative analysis of the similarity measures. The purpose of the comparison is to verify the similarity measures based on the objectively developed datasets. According to the author the results demonstrate that the Bayesian Model of Generalization provides for the most effective cognitive model for identifying the most similar corresponding concepts existing for a targeted socio-cultural community. In another presentation, Thierry Declerck, Hans-Ulrich Krieger and Dagmar Gromann present an ongoing work and propose an approach to automatic extraction of information from multilingual financial Web resources, to provide candidate terms for building ontology elements or instances of ontology concepts. The authors present a complementary approach to the direct localization/translation of ontology labels, by acquiring terminologies through the access and harvesting of multilingual Web presences of structured information providers in the field of finance, leading to both the detection of candidate terms in various multilingual sources in the financial domain that can be used not only as labels of ontology classes and properties but also for the possible generation of (multilingual) domain ontologies themselves. In the next paper, Manuel Silva, António Lucas Soares and Rute Costa claim that despite the availability of tools, resources and techniques aimed at the construction of ontological artifacts, developing a shared conceptualization of a given reality still raises questions about the principles and methods that support the initial phases of conceptualization. These questions become, according to the authors, more complex when the conceptualization occurs in a multilingual setting. To tackle these issues the authors present a collaborative platform – conceptME - where terminological and knowledge representation processes support domain experts throughout a conceptualization framework, allowing the inclusion of multilingual data as a way to promote knowledge sharing and enhance conceptualization and support a multilingual ontology specification. In another presentation Frieda Steurs and Hendrik J. Kockaert present us TermWise, a large project dealing with legal terminology and phraseology for the Belgian public services, i. e. the translation office of the ministry of justice, a project which aims at developing an advanced tool including expert knowledge in the algorithms that extract specialized language from textual data (legal documents) and whose outcome is a knowledge database including Dutch/French equivalents for legal concepts, enriched with the phraseology related to the terms under discussion. Finally, Deborah Grbac, Luca Losito, Andrea Sada and Paolo Sirito report on the preliminary results of a pilot project currently ongoing at UCSC Central Library, where they propose to adapt to subject librarians, employed in large and multilingual Academic Institutions, the model used by translators working within European Union Institutions. The authors are using User Experience (UX) Analysis in order to provide subject librarians with a visual support, by means of “ontology tables” depicting conceptual linking and connections of words with concepts presented according to their semantic and linguistic meaning. The organizers hope that the selection of papers presented here will be of interest to a broad audience, and will be a starting point for further discussion and cooperation...|$|R
40|$|Abstract. Anaphora {{resolution}} in current computer-processable controlled natural languages relies mainly on syntactic information, accessibility constraints {{and the distance}} of the anaphoric expression to its antecedent. This design decision has the advantage that a text can be processed automatically without any additional ontological knowledge, but it has the disadvantage that the author is severely restricted in using anaphoric expressions while writing a text. I will argue that we can allow for a wider range of anaphoric expressions if we consider the anaphora resolution process as an interactive, machine-guided knowledge acquisition process in cases where no suitable antecedent can be found automatically. This knowledge acquisition process relies on the human author who provides additional <b>terminological</b> <b>information</b> for the anaphora resolution algorithm – if required – while a text is written. ...|$|E
40|$|Ontology {{generation}} and population {{is a crucial}} part of knowledge base construction and maintenance that enables us to relate text to ontologies, providing a rich and customised ontology related to the data and domain with which we are concerned. SPRAT combines aspects from traditional named entity recognition, ontology-based information extraction and relation extraction, in order to identify patterns for the extraction of a variety of entity types and relations between them, and to re-engineer them into concepts and instances in an ontology. When augmented with richer knowledge such as WordNet semantic categories and <b>terminological</b> <b>information,</b> the results are greatly improved. SPRAT can either modify an existing ontology or create a new ontology from scratch, which is specific to the corpus of texts processed. Preliminary results are very promising, although more refinement of the patterns is still necessary...|$|E
40|$|In {{this article}} I present an on - going {{study of the}} use of web - based corpora to detect Norwegian {{terminology}} and in particular financial neologisms. With an ever increasing influence from English in the domain, the use of anglicisms such as ‘dobbeldipp’ (‘d ouble dip’) or ‘shortsalg’ (‘short selling’) is increasing rapidly. The study focuses on how to detect these specialised neologisms and also possible Norwegian terms that are formed. Three different corpora are used, i. e., the Norwegian Newspaper Corpus (N NC), Peter Warrens finansblogg (PW) and a blog on a research project which focuses on the financial crisis called Crisis, restructuring and Growth (KOV). The aim is to gather <b>terminological</b> <b>information</b> which can be useful in the NHH Termbase, an online, we b - based termbase to be used by students, lecturers and researchers at the Norwegian School of Economics (NHH) ...|$|E
40|$|Motivation: The {{sheer volume}} of {{textually}} described biomedical knowledge exerts the need for natural language processing (NLP) applications {{in order to allow}} flexible and efficient access to relevant information. Specialised semantic networks (such as biomedical ontologies, terminologies or semantic lexicons) can significantly enhance these applications by supplying the necessary <b>terminological</b> <b>information</b> in a machine-readable form. Due to the explosive growth of bio-literature, new terms (representing newly identified concepts or variations of the existing terms) may not be explicitly described within the network and hence cannot be fully exploited by NLP applications. Linguistic and statistical clues can be used to extract many new terms from free text. The extracted terms still need to be correctly positioned relative to other terms in the network. Classification as a means of semantic typing represents the first step in updating a semantic network with new terms. Results: The MASTERCLASS system implements the case-based reasoning methodology for the classification of biomedical terms. Availability: MASTERCLASS is available a...|$|E
40|$|In {{this paper}} {{we present a}} {{revisited}} classification of term variation {{in the light of}} the Linked Data initiative. Linked Data refers to a set of best practices for publishing and connecting structured data on the Web with the idea of transforming it into a global graph. One of the crucial steps of this initiative is the linking step, in which datasets in one or more languages need to be linked or connected with one another. We claim that the linking process would be facilitated if datasets are enriched with lexical and <b>terminological</b> <b>information.</b> Being that the final aim, we propose a classification of lexical, terminological and semantic variants that will become part of a model of linguistic descriptions that is currently being proposed within the framework of the W 3 C Ontology- Lexica Community Group to enrich ontologies and Linked Data vocabularies. Examples of modeling solutions of the different types of variants are also provided...|$|E
40|$|In {{this article}} we wish to present RICOTERM- 2 [1], a project coordinated by the {{university}} of Santiago de Compostela and the Universitat Pompeu Fabra of Barcelona, in which researchers from the University of The Basque Country and Antwerp University have also participated. The main aim of this applied research project, is the design and development of a prototype queries reelaborator for Internet searches (RECCI). This application consists of an interface located in an economics specialized web portal which will transform a simple monolingual search into a group of multilingual searches expanded towards other concepts related to the initial search. For this project Catalan, Basque and Galitian, in addition to Spanish and English, are being used as working languages. During the first phase of this project our aim is to construct a modular economics knowledge bank, composed of a textual corpus for each of the working languages, a computational dictionary enriched with semantic and <b>terminological</b> <b>information,</b> and an ontology covering a restricted area of economics...|$|E
40|$|This paper {{illustrates}} how efficient text mining may {{be achieved by}} means of syntactic ontology building, associated to morphosyntactic and terminology-based automatic textual analysis software. This paper also gives a factual contribution to the debate on Semantic Web (SW), in a perspective based on the possibility to elaborate ontologies starting from the syntactic structure in which they occur. To deal with these topics, we will here adopt Lexicon-Grammar theoretical and practical analytical framework. Four main themes will be here analyzed and discussed: 1. 	the formalization of semantic predicate properties, based on co-occurrence and selection restriction rules; 2. 	the structure and application of generic and terminological electronic dictionaries; 3. 	the building and application of finite-state automata and transducers; 4. 	the consequent incorporation of all previous lingware elements inside two specific textual analysis software, {{to be used in}} order to achieve text mining, morphosyntactic parsing, and <b>terminological</b> <b>information</b> retrieval. Keywords: data mining, semantic web, lexicon-gramma...|$|E
40|$|This paper {{describes}} a semantic clustering method for data extracted from machine readable dictionaries (MRDs) {{in order to}} build a <b>terminological</b> <b>information</b> retrieval system that finds terms from descriptions of concepts. We first examine approaches based on ontologies and statistics, before introducing our analogy-based approach that lets us extract semantic clusters by aligning definitions from two dictionaries. Evaluation of the final set of clusters for a small set of definitions demonstrates the utility of our approach. 1. Background The majority of lexicographers recognise the need for dictionaries that, contrary to the alphabetical ordering of entries, help users to look for a word that has escaped their memory even though they remember the concept. From a semantic point of view, Baldinger (1980) identifies two kinds of dictionaries. The semasiological one corresponds to the viewpoint of the person interpreting the speaker, thus one starts with the form of the expression to look for the meaning. It i...|$|E
