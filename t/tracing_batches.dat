0|18|Public
30|$|Once transactions’ <b>traces</b> <b>batches</b> {{has been}} collected, {{they must be}} {{analyzed}} somehow. In [50], one can find an extensive overview of currently available tools and utilities that are widely used by researched to data analysis and statistics.|$|R
30|$|We {{designed}} and implemented an extension {{to the existing}} payment application that allows it to record detailed information about each transaction. During the transaction, payment application records all important data and events related to the given transaction and stores it as an entry in transactions’ <b>traces</b> <b>batch</b> file.|$|R
30|$|Each payment {{terminal}} in {{the field}} is managed remotely by the central system called terminal management system (TMS). It is responsible for the management of terminal’s configuration, monitoring its internal state, etc. Most of TMSs have an ability to receives some files from the terminal. Usually, a payment terminal is connecting to its TMS once a day. Because transactions’ <b>traces</b> <b>batch</b> is a single text file, we made a change in the payment application, so that the whole file was uploaded to the TMS each day.|$|R
50|$|Honey {{harvested}} from the incubation farms and procured from partner beekeepers, is transported {{to a central}} processing plant where it is strained, bottled, labelled, packaged and dispatched for distribution. African Honey Bee's Nektar management and traceability technology will allow consumers to <b>trace</b> their <b>batch</b> of honey back to the beekeeper that produced it and see on Google Earth where it comes from.|$|R
50|$|In 2006, a {{short-lived}} scare was caused when some Farley's Rusks {{were found to}} contain traces of the weedkiller chlorpropham. The affected products were recalled and the contamination was <b>traced</b> to a <b>batch</b> of flour used during the manufacturing process. The level of contamination was not high enough {{to be considered a}} health risk.|$|R
40|$|Parallel {{computers}} have matured {{to the point}} where they are capable of running a significant production workload. Characterizing this workload, however, is far more complicated than for the single-processor case. Besides the varying number of processors that may be invoked, the nodes themselves may provide differing computational resources (memory size, for example). In addition, the batch schedulers may introduce further categories of service which must be considered in the analysis. The Cornell Theory Center (CTC) put a 512 -node IBM SP 2 system into production in early 1995. Extended <b>traces</b> of <b>batch</b> jobs began to be collected in mid- 1995 when the usage base became sufficiently large. This paper offers an analysis of this early batch workload...|$|R
40|$|Ray tracing is {{a widely}} used {{algorithm}} to compute images with high visual quality. Mapping ray tracing computations to massively parallel hardware architectures in an efficient manner is a difficult task. Based on an analysis of current ray tracing algorithms on GPUs, a new ray traversal scheme called <b>batch</b> <b>tracing</b> is proposed. It decomposes the task into multiple kernels, {{each of which is}} designed for efficient execution. Our algorithm achieves comparable performance to state-of-the-art approaches and represents a promising avenue for future research...|$|R
40|$|The use of {{synthetic}} DNA as a marking {{system is a}} new traceability concept in the leather industry, especially for supplier and <b>batch</b> <b>tracing.</b> DNA is outstandingly suited for the usage as a marking system because of its code diversity, invisibility and doubtlessness. However, DNA labeling is a great challenge for products exposed to DNA damaging influences during their production, such as acidic pH, elevated temperatures in combination with high humidity or sunlight radiation. Leather is such a product. We attached single-strand DNA (ssDNA) to hydroxyapatite and enhanced the stability of these DNA particles by encapsulation in polystyrene-codivinylbenzene (PS-DVB) microcapsules. Furthermore, the ssDNA containing microcapsules were improved with functional groups {{on the surface of}} the capsule to irreversibly attach them to the collagen matrix of leather by chrome tanning. Laboratory scale tests using acidic conditions as well as elevated temperatures in the presence of high humidity showed that the stability of the leather marking system was enhanced. Marking trials were conducted in crust leathers, and the light fastness of these labeled crusts were tested. The results indicate that encapsulated DNA-hydroxyapatite-particles are more stable at sunlight radiation than non-encapsulated DNA. These marking trials showed that the system could be a suitable leather marking system in the leather industry to establish a powerful supplier and <b>batch</b> <b>tracing...</b>|$|R
40|$|The Cornell Theory Center (CTC) put a 512 -node IBM SP 2 {{system into}} {{production}} in early 1995, and extended <b>traces</b> of <b>batch</b> jobs {{began to be}} collected in June of that year. An analysis of the workload shows {{that it has not}} only grown, but that its characteristics have changed over time. In particular, job duration increased with time, indicative of an expanding production workload. In addition, there was increasing use of parallelism. As the load has increased and larger jobs have become more frequent, the batch management software (IBM's LoadLeveler) has had difficulty in scheduling the requested resources. New policies were established to improve the situation. This paper will profile how the workload has changed over time and give an in-depth look at the maturing workload. It will examine how frequently certain resources are requested and analyze user submittal patterns. It will also describe the policies that were implemented to improve the scheduling situation and their effect on [...] ...|$|R
30|$|The {{measurement}} results were segmented into different numbers of batches based on different mobility level assumptions. The mobile channel went from good (T 1), to medium (T 2), to bad quality (T 3). To emulate different amount of mobility, we {{stayed with the}} same scenario data for different amount of time before changing to the next scenario trace. We used four different PDR change speeds, which we measured in number of batches. A mobility scenario which changes <b>trace</b> every 10 <b>batches,</b> that is, Mobility- 10, is more mobile than one which changes only every 100 batches, that is, Mobility- 100.|$|R
40|$|Ray tracing is a {{very popular}} family of {{algorithms}} {{that are used to}} compute images with high visual quality. One of its core challenges is designing an efficient mapping of ray traversal computations to massively parallel hardware architectures like modern algorithms graphics processing units (GPUs). In this paper we investigate the performance of state-of-the-art ray traversal algorithms on GPUs and discuss their potentials and limitations. Based on this analysis, a novel ray traversal scheme called <b>batch</b> <b>tracing</b> is proposed. It subdivides the task into multiple kernels, each of which is designed for efficient parallel execution. Our algorithm achieves comparable performance to current approaches and represents a promising direction for future research...|$|R
40|$|Images {{with high}} visual quality are often {{generated}} by a ray tracing algorithm. Despite its conceptual simplicity, designing an efficient mapping of ray tracing computations to massively parallel hardware architectures is a challenging task. In this paper we investigate the performance of state-of-the-art ray traversal algorithms for bounding volume hierarchies on GPUs and discuss their potentials and limitations. Based on this analysis, a novel ray traversal scheme called <b>batch</b> <b>tracing</b> is proposed. It decomposes the task into multiple kernels, {{each of which is}} designed for efficient parallel execution. Our algorithm achieves comparable performance to currently prevailing approaches and represents a promising avenue for future research...|$|R
40|$|Abstract. Security {{information}} and event management (SIEM) systems usually {{consist of a}} centralized monitoring server that processes events sent from {{a large number of}} hosts through a potentially slow network. In this work, we discuss how monitoring efficiency can be increased by switching to a model of aggregated traces, where monitored hosts buffer events into lossy but compact <b>batches.</b> In our <b>trace</b> model, such <b>batches</b> retain the number and types of events processed, but not their order. We present an algorithm for automatically constructing, out of a regular finitestate property definition, a monitor that can process such aggregated traces. We discuss the resultant monitor’s complexity and prove that it determines the set of possible next states without producing false negatives and with a precision that is optimal given the reduced information the trace carries. ...|$|R
40|$|This paper {{analyses}} {{the impact}} of the Lisbon Treaty on the institutional architecture of CFSP and the overall external action of the Union. The Lisbon Treaty has introduced some remarkable changes which might substantially influence the (inter-) institutional balance in this policy field. The authors offer two different possible readings of the CFSP provisions of the Lisbon Treaty: they could be interpreted as a major step forward {{in the direction of a}} strengthened, more coherent and more effective international actor with more supranational elements; but they may also be seen as demonstrating an ever-refined mode of ‘rationalised intergovernmentalism’. After an in-depth analysis of the ideas and norms contained in the new treaty, the institutions and the instruments, the authors find more evidence for the second interpretation, but also <b>traces</b> for a <b>‘ratched</b> fusion’ as a third alternative explanation...|$|R
40|$|Nowadays in {{numerous}} electroanalytical laboratories {{many types of}} bismuth-film electrodes (BiFEs) have been proven to be applicable for different target analytes, first of all the heavy metal <b>traces,</b> mostly in <b>batch</b> working regime [1, 2]. Recently, Wang et al. [3], Vytřas et al. [4], Hutton et al. [5], Królicka and Bobrowski [6], Charalambous and Economou [7] and Legeai and Vittori [8] applied BiFEs electrodeposited onto different substrate electrode from plating solutions containing different concentration of bismuth (III) ions without or with complexing agents for use in trace level analysis of target metals, usually by anodic stripping voltammetry (ASV, e. g. Cd, Pb, Zn, In, etc.) or adsorptive stripping voltammetry (AdSV, e. g. Ni, Co). The advantageous analytical properties of BiFEs, roughly {{comparable to those of}} MFEs, are attributed to the property of bismuth to form "fused alloys " with heavy metals, which may b...|$|R
40|$|The {{determination}} of Al, Be, Cd, Co, Cr, Cu, Mn, Ni, Pb, V and Zn in high-saline solutions (urine) by ICP-AES method has been studied. The interference due to individual components (NaCl, KCl, CaCO 3, MgCl 2, NH 4 H 2 PO 4, HCl, H 2 SO 4 and urea) and their concentration levels were evaluated. Suppression of down to 20 % of trace element content was found {{due to the}} main inorganic components of urine. The methods for correction of matrix effect, i. e. internal standardisation with Y, Sc and Ar and the "CAIS" technique were tested and and compared {{with the results of}} direct determination. The organic components of urine had no significant effect. Two approaches of estimation of LOD were used. The influence of urine matrix on LOD was established. Precision (RSD) was about 1 % for internal standardisation and less than 2 % for direct estimation. The accuracy was evaluated with urine control materials Seronom <b>Trace</b> Element Urine, <b>batch</b> 403125 (Nycomed Pharma), and Lyhpochek 69012 (Bio-Rad) ...|$|R
30|$|As {{the target}} {{is to find the}} optimal {{sequence}} of time nodes, the positions in the ACO algorithm can be represented by sequences. A possible time node sequence is necessary since the initial position is very important for the ACO algorithm. Given that all the offloading stations do not offload batches, because the injecting plan is known, the batch interface can be <b>traced</b> and <b>batch’s</b> arrival at stations can be simulated accordingly. Thus, the sequence of time nodes when batches arrive at stations, the injecting flow rate changes, and study horizon’s beginning as well as end can be further calculated. The batch’s offloading starting moment is close to the one when the batch’s head reaches the station, and the finish time is close to one when the next batch’s head reaches the station, providing that the batch’s arrival is within the study horizon. The start of the offloading operation should be close {{to the head of the}} study horizon and the ending moment should be next to the end of the study horizon if the batch’s arriving time is beyond the study horizon. In this way, the initial time node sequence is generated. During each iteration operation, two time nodes will be selected randomly and the firstly chosen one is plugged after the second one, and then we judge whether this changed sequence can meet the formulas (7)–(13). If not, two new time nodes’ orders will be exchanged randomly until they meet those constraints. Then an LP model can be established and solved by the simplex method. Therefore, a detailed scheduling plan and the value of its objective function can be obtained. Sorting all the explored positions on the basis of the value of their objective function, ants are reallocated to a few of the best positions, awaiting the next round of relocation.|$|R
40|$|As {{more and}} more service {{providers}} choose Cloud platforms, a resource provider needs to provision resources and supporting runtime environments (REs) for heterogeneous workloads in different scenarios. Previous work fails {{to resolve this issue}} in several ways: (1) it fails to pay attention to diverse RE requirements, and does not enable creating coordinated REs on demand; (2) few work investigates coordinated resource provisioning for heterogeneous workloads. In this paper, our contributions are three-fold: (1) we present an RE agreement that expresses diverse RE requirements, and build an innovative system PhoenixCloud that enables a resource provider to create REs on demand according to RE agreements; (2) we propose two coordinated resource provisioning solutions for heterogeneous workloads in two typical Cloud scenarios: first, a large organization operates a private Cloud for two heterogeneous workloads; second, a large organization or two service providers running heterogeneous workloads revert to a public Cloud; and (3) A comprehensive evaluation has been performed in experiments. For typical workload <b>traces</b> of parallel <b>batch</b> jobs and Web services, our experiments show that: a) In the first Cloud scenario, when the throughput is almost same like that of a dedicated cluster system, our solution decreases the configuration size of cluster by about 40 %; b) in the second scenario, our solution decreases not only the total resource consumption, but also the peak resource consumption maximally to 31 % with respect to that of EC 2 + RightScale solution. Comment: Submitted to IEEE Transaction on Service Computin...|$|R

