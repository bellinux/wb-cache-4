63|0|Public
6000|$|... 'That's the spishil <b>trustability</b> av a marksman. Train him {{to hit a}} fly wid a stiddy rest {{at seven}} hunder, an' he loose on anythin' he sees or hears up to th' mile. You're well out av that fancy-firin' gang, Jock. Stay here.' ...|$|E
50|$|She has {{published}} seminal works on systems <b>trustability,</b> most recently Intel x86 Considered Harmful and State Considered Harmful - A Proposal for a Stateless Laptop. Rutkowska has been invited as an esteemed presenter at security conferences, such Chaos Computer Conferences, Black Hat Briefings, HITB, RSA Conference, RISK, EuSecWest & Gartner IT Security Summit.|$|E
5000|$|One of the {{benefits}} of the web of trust, such as in PGP, is that it can interoperate with a PKI CA fully trusted by all parties in a domain (such as an internal CA in a company) that is willing to guarantee certificates, as a trusted introducer. If the [...] "web of trust" [...] is completely trusted then, {{because of the nature of}} a web of trust, trusting one certificate is granting trust to all the certificates in that web. A PKI is only as valuable as the standards and practices that control the issuance of certificates and including PGP or a personally instituted web of trust could significantly degrade the <b>trustability</b> of that enterprise's or domain's implementation of PKI.|$|E
5000|$|Advantages: the {{implementation}} of Google Groups comes with its own advantages. For diverse users, it provides the service of interpreting languages widely, which helps present {{a better way to}} communicate effectively with people in different countries. Considering of storage, one group member enjoys [...] "100 megabytes (MB)" [...] while there are no restrictions for the whole group. It delivers convenience for group members work on projects that need considerably more storage than normal files, for example, presentations. Studies conducted by Kushin and Kitchener indicates Facebook provide users in discussion groups with more opportunities to post content that has correlation with [...] "social, political, or sporting issues". For Whatsapp users, the communication service brings enjoyment to share ideas with comparatively low cost. Ideally, it enhanced the quality of communication regarding of its records saving, security and <b>trustability.</b>|$|E
40|$|A {{measure of}} {{software}} dependability called trustabzlzty is described. A program p has trustabil-ity 2 ’ {{if we are}} at least T confident that p M free of faults. <b>Trustability</b> measurement depends on detectabdzt. v. The detectability of a method is the probability that it wdl detect faults, when there are faults present. Detectability research {{can be used to}} characterize conditions under which one testing and analysis method is more effective than another. Several detectability results that were only previously described informally, and Illustrated by example, are proved. Several new detectability results are also proved. The <b>trustability</b> model characterizes the kind of information that is needed to Justify a given level of <b>trustability.</b> When the required information is available, the trustabdity approach can be used to determine strategies in which methods are combined for maximum effectiveness. It can be used to determine the mimmum amount of resources needed to guarantee a required degree of <b>trustability,</b> and the maximum <b>trustability</b> that is achievable with a given amount of resources, Theorems proving several optimization results are given. Apphcatlons of the <b>trustability</b> model are discussed. Methods for the derivation of detectability factors, the relationship between <b>trustability</b> and operational reliability, and the relationship between the software development process and <b>trustability</b> are described...|$|E
40|$|The {{promise of}} search-driven {{development}} is that developers will {{save time and}} resources by reusing external code in their local projects. To efficiently integrate this code, users {{must be able to}} trust it, thus <b>trustability</b> of code search results {{is just as important as}} their relevance. In this paper, we introduce a <b>trustability</b> metric to help users assess the quality of code search results and therefore ease the cost-benefit analysis they undertake trying to find suitable integration candidates. The proposed <b>trustability</b> metric incorporates both user votes and cross-project activity of developers to calculate a "karma" value for each developer. Through the karma value of all its developers a project is ranked on a <b>trustability</b> scale. We present JBender, a proof-of-concept code search engine which implements our <b>trustability</b> metric and we discuss preliminary results from an evaluation of the prototype...|$|E
40|$|The Internet of Things {{is nothing}} new. Yet the {{imminent}} confluence of cyberspace and physical space into one ambient intelligent system still poses fundamental research {{challenges in the}} area of security, privacy and <b>trustability.</b> We discuss these challenges, and present new approaches that may help to overcome them. © 2012 Springer-Verlag...|$|E
40|$|One {{central issue}} in the {{usability}} of answers is in their understandability. We have focused a line of research on explaining answers from heterogeneous distributed systems {{with the goal of}} improving usability and <b>trustability</b> in answers by increasing answer understandability and <b>trustability.</b> We explore ways of providing an interoperable distributed infrastructure that supports explanations containing provenance concerning answers – where information came from, how reliable it is [...] along with information about assumptions relied on, information manipulation techniques, etc. The infrastructure utilizes the Proof Markup Language, encoded in the OWL Web Ontology Language so as to be compatible with web standards and also so as to be able to leverage existing web ontologies. In this paper, we expose and explore some issues related to context as it is used in some of our question answering systems...|$|E
40|$|Web {{provides}} rich {{information about}} a variety of objects. <b>Trustability</b> {{is a major concern}} on the web. Truth establishment is an important task so as to provide the right information to the user from the most trustworthy source. Trustworthiness of information provider and the confidence of the facts it provides are inter-dependent on each other and hence can be expressed iteratively in terms of each other. However, a single information provider {{may not be the most}} trustworthy for all kinds of information. Every information provider has its own area of competence where it can perform better than others. We derive a model that can evaluate <b>trustability</b> on objects and information providers based on clusters (groups). We propose a method which groups the set of objects for which similar set of providers provide “good ” facts, and provides better accuracy in addition to high quality object clusters...|$|E
40|$|Digital image forgery is {{becoming}} easier to perform {{because of the}} rapid development of various manipulation tools. Image splicing {{is one of the}} most prevalent techniques. Digital images had lost their <b>trustability,</b> and researches have exerted considerable effort to regain such <b>trustability</b> by focusing mostly on algorithms. However, most of the proposed algorithms are incapable of handling high dimensionality and redundancy in the extracted features. Moreover, existing algorithms are limited by high computational time. This study focuses on improving one of the image splicing detection algorithms, that is, the run length run number algorithm (RLRN), by applying two dimension reduction methods, namely, principal component analysis (PCA) and kernel PCA. Support vector machine is used to distinguish between authentic and spliced images. Results show that kernel PCA is a nonlinear dimension reduction method that has the best effect on R, G, B, and Y channels and gray-scale images...|$|E
40|$|Abstract. Although the Internet has {{developed}} into a mass-medium for communication and information exchange {{over the last couple}} of years, many problems still exist regarding security and anonymity. One of these Achilles ’ Heels is spam. Electronic mail (e-mail) has become one of the most used communication mechanism. It is absolutely easy to use and cost-effective. Unfortunately, the simplicity and effectiveness of e-mail are also major drawbacks. Without additional effort, Internet users’ mailboxes are flooded with unsolicited, bulk e-mails of many different flavors; mostly without any chance to identify the true origin. Thus, most anti-spam techniques rely on spam detection using large filter data bases and pattern matching functions but cannot identify the <b>trustability</b> of the sender. To overcome this lack of security and <b>trustability,</b> a new concept—Trust-by-Wire—is introduced as well as a mechanism called IPclip, which provides the basic means for enhanced e-mail security. The main idea is to guarantee Trust-by-Wire in packet-switched I...|$|E
40|$|Search is a {{fundamental}} activity in software development. However, to search source code efficiently, it {{is not sufficient to}} implement a traditional full text search over a base of source code, human factors have {{to be taken into account}} as well. We looked into ways of increasing the search results code <b>trustability</b> by providing and analysing a range of meta data alongside the actual search results. 1. RESEARCH PROBLEM AN...|$|E
40|$|The supervision, within socio-educational services, is a {{significant}} need for all professionals working {{with the aim of}} reducing the impact of problems and disabilities on their users’ daily life. A pedagogical approach and a discussion of pedagogical topics within the field are considered in depth. The present role of pedagogy is discussed: present changes and their impact ask pedagogists an innovative and renewed reading of the professional roles of all professionals concerned with socio-educational aims, with a renewed sense of credibility, <b>trustability,</b> educability...|$|E
40|$|AbstractConventional {{approaches}} to the modeling of autonomous agents and agent communication rely heavily on the ascription of mental properties like beliefs and intentions to the individual agents. These “mentalistic” approaches are, when applicable, very powerful, but become problematic in open environments like the Semantic Web, Peer 2 Peer systems and open multiagent systems populated by truly autonomous, self-interested grey- or black-box agents with limited <b>trustability.</b> In this work, we propose communication attitudes in form of dynamic, revisable ostensible beliefs (or opinions) and ostensible intentions as foundational means for the logical, external description of agents obtained from the observation of communication processes, in order to retain the advantages of mentalistic agent models as far as possible, but with verifiable results without the need to speculate about covert agent internals. As potential applications, communication attitudes allow for a simultaneous reasoning about the (possibly inconsistent) "public image(-s) " of a certain agent and her mental properties without blurring interferences, new {{approaches to}} communication language semantics, and a fine-grained, statement-level concept of <b>trustability.</b> As a further application of communicative attitudes, we introduce multi-source opinions and opinion bases. These allow for the computational representation of semantically heterogeneous knowledge, including the representation of inconsistent knowledge in a socially reified form, and a probabilistic weighting of possibly indefinite and inconsistent assertions explicitly attributed to different provenances and social contexts...|$|E
40|$|A Risk Based Inspection (RBI) {{scheme is}} a {{planning}} tool {{used to develop}} the optimum plan for the execution of inspection activities. Organic certification system could benefit from the implementation of RBIs in terms of higher effectiveness, i. e. <b>trustability,</b> and lower transaction costs for organic operators. Data from certification bodies provide basic information about non-compliances and structural aspects of organic operators. Here we propose a methodological approach to risk analysis modelling, based on discrete choice models and Bayesian networks, both aiming at the identification of key risk factor in the organic certification process in the European Union...|$|E
40|$|In open environments {{like the}} Web, and open Multiagent and Peer 2 Peer systems, consent among the autonomous, self-interested {{knowledge}} sources and users very often can-not be established, and {{the estimation of}} <b>trustability</b> and truthfulness of knowledge sources may not be possible. Moreover, competing viewpoints and their communicative contexts even provide valuable meta-knowledge about {{the intentions of the}} participants and their social relationships. As a foundational approach to semantically heterogeneous knowledge perspectives, we introduce a formal framework for the computational representation and integration of multi-source knowledge, which makes explicit heteroge-neous viewpoints, and conflicting opinions and their social contexts, and allows for the rating, generalization and op-tional fusion of knowledge by social choice...|$|E
40|$|Knowledge {{communities}} {{of all kinds}} have social and material practices for deciding what is known and who is to be trusted. In this paper, we address a specific kind of knowledge work, environmental planning, and a particular form of collaboration, the sharing of measurement data sets. We are interested in how trust is created; how <b>trustability</b> is assessed in the arm=s-length collaboration of sharing data sets; and how changes in technology interact with those practices of trust. We look at several elements of scientific practice that facilitate this sharing [...] the publication system, {{communities of}} practice, boundary objects, and assemblages [...] and discuss how a Web-based Digital Library might affect these elements and the knowledge work that they support...|$|E
40|$|Similar to {{the effects}} of {{software}} viruses, hardware can also be compromised by introduction of malicious logic into circuits to cause unwanted system behaviors. This can be done by changing or adding internal logic, {{in such a way that}} it is undetectable using traditional testing and verification tools and techniques. Therefore, the user of the circuit needs to decide whether it can be trusted, i. e., it only performs functions defined in the original circuit specification (no more and no less), before employing it in the system. In this paper, a preliminary methodology is proposed to model potential hardware threats in order to determine a circuit’s <b>trustability</b> and provide guidance to malicious-logic checking tools. 1...|$|E
40|$|Automatic {{verification}} tools, such as model checkers {{and tools}} based on static analysis or on abstract interpretation, have become popular in {{software and hardware}} development. They increase confidence and potentially provide rich feedback. However, with increasing complexity, verification tools themselves {{are more likely to}} contain errors. In contrast to automatic verification tools, higher-order theorem provers use mathematically founded proof strategies checked by a small proof checker to guarantee selected properties. Thus, they enjoy a high level of <b>trustability.</b> Properties of software and hardware systems and their justifications can be encapsulated into a certificate, thereby guaranteeing correctness of the systems, with respect to the properties. These results offer a much higher degree of confidence than results achieved by verification tools. However, higher-order theorem provers are usually slow, due to their general and minimalistic nature. Even for small systems, a lot of human interaction is required for establishing a certificate. In this work, we combine the advantages of automatic verification tools (i. e., speed and automation) with those of higher-order theorem provers (i. e., high level of <b>trustability).</b> The verification tool generates a certificate for each invocation. This is checked by the higher-order theorem prover, thereby guaranteeing the desired property. The generation of certificates is much easier than producing the analysis results of the verification tool in the first place. In our work, we are able to create certificates that come with an algorithmic description of the proof of the desired property as justification. We concentrate on verification tools that generate invariants of systems and certify automatically that these do indeed hold. Our approach is applied to the certification of the verdicts of a deadlock-detection tool for an asynchronous component-based language...|$|E
40|$|Abstract: Fusion is {{a common}} topic {{nowadays}} in Advanced Driver Assistance Systems (ADAS). The demanding requirements of safety applications require trustable sensing technologies. Fusion allows to provide trustable detections by combining different sensor devices, fulfilling the requirements of safety applications. High level fusion scheme is presented; able to improve classic ADAS systems by combining different sensing technologies i. e. laser scanner and computer vision. By means of powerful Data Fusion (DF) algorithms, the performance of classic ADAS detection systems is enhanced. Fusion is performed in a decentralized scheme (high level), allowing scalability; hence new sensing technologies can easily be added to increase the <b>trustability</b> and {{the accuracy of the}} overall system. Present work focus in the Data Fusion scheme used to combine the information of the sensors at high level. Although for completeness some details of the different detection algorithms (low level) of the different sensors is provided. The proposed work adapts a powerful Data Association technique for Multiple Targets Tracking (MTT) : Joint Probabilistic Data Association (JPDA) to improve the <b>trustability</b> of the ADAS detection systems. The final application provides real time detection of road users (pedestrians and vehicles) in real road situations. The tests performed proved the improvement of the use of Data Fusion algorithms. Furthermore, comparison with other classic algorithms such as Global Nearest Neighbors (GNN) proved the performance of the overall architecture. This work was supported by the Spanish Government through the Cicyt projects (GRANT TRA 2010 - 20225 -C 03 - 01) and (GRANT TRA 2011 - 29454 -C 03 - 02). CAM through SEGAUTO-II (S 2009 /DPI- 1509) ...|$|E
40|$|Bibliography: pages 231 - 238. Introduction [...] ch. 1 : Trust as {{a cluster}} concept: sect. 1 : Introduction [...] sect. 2 : Four {{examples}} of trust [...] sect. 3 : Features, types, {{and characteristics of}} trust evident in the phenomena [...] sect. 4 : Influential approaches to the concept if trust and their limitations [...] sect. 5 : Trust as a cluster concept [...] ch. 2 : Trustworthiness, <b>trustability,</b> and mere reliability: sect. 1 : Introduction [...] sect. 2 : From competence and commitment to character [...] sect. 3 : Supplementing trustworthiness with <b>trustability</b> [...] ch. 3 : Institutional trust and trustworthiness: sect. 1 : Introduction [...] sect. 2 : Extending the concept of trust to institutional contexts [...] sect. 3 : <b>Trustability</b> in institutional contexts [...] sect. 4 : Trustworthiness in institutional contexts [...] ch. 4 : Betrayal: sect. 1 : Introduction [...] sect. 2 : Betrayal phenomena [...] sect. 3 : A preliminary analysis of betrayal [...] sect. 4 : Understanding betrayal {{as a type of}} disloyalty [...] sect. 5 : The morality of betrayal [...] sect. 6 : Testing and explaining trust's vulnerability to betrayal [...] ch. 5 : Recovering reasonable trust after betrayal: sect. 1 : Introduction [...] sect. 2 : Three cases of betrayed trust [...] sect. 3 : An account of the damages that betrayal can inflict [...] sect. 4 Recovering reasonable trust after betrayal [...] Conclusion [...] Bibliography. Vulnerability to betrayal has been identified as a distinguishing feature of trust, but there has been little direct analysis of betrayal or its implications for understanding trust. A clear account of betrayal is needed for at least two reasons: to explain the distinction between trust and mere reliance; and to explicate the challenges facing trusters who have been betrayed. If it is true that when we trust we risk betrayal, then every instance of trust involves accepting that others might betray us. The risk of betrayal may fade into the background of most trusters' interactions with others, but it will feature significantly in the cognitive and affective experiences of those who have been betrayed. Betrayal can result in distrust, loss of confidence in knowing who can be trusted, and responses such as resentment and hostile emotions. These effects can inhibit trust after betrayal, but they are not always bad. Distrust, loss of confidence, resentment and hostility may prevent a victim from trusting unwisely or too quickly. That said, these effects of betrayal can also inhibit reasonable placement of trust. And yet some victims do trust after being betrayed. In this thesis I analyse trust and trustworthiness and use distinctions developed in that analysis to explain betrayal, its impact on trust, and the conceptual issues raised by trust after betrayal. Mode of access: World Wide Web. 1 online resource (iv, 238 pages...|$|E
40|$|Dead-Zone {{logic is}} a {{mechanism}} to prevent autonomic managers from unnecessary, inefficient and ineffective control brevity when the system is sufficiently close to its target state. It provides a natural and powerful framework for achieving dependable self-management in autonomic systems by enabling autonomic managers to smartly carry out a change (or adapt) only when it is safe and efficient to do so-within a particular (defined) safety margin. This paper explores and evaluates the performance impact of dead-zone logic in trustworthy autonomic computing. Using two case example scenarios, we present empirical analyses that demonstrate the effectiveness of dead-zone logic in achieving stability, dependability and trustworthiness in adaptive systems. Dynamic temperature target tracking and autonomic datacentre resource request and allocation management scenarios are used. Results show that dead-zone logic can significantly enhance the <b>trustability</b> of autonomic systems...|$|E
40|$|Abstract — This {{paper is}} {{concerned}} with setting {{the groundwork for the}} introduction of standards for Autonomic Computing, in terms of technologies and the composition of functionalities as well as validation methodologies. This is in line with addressing the lack of universal standards for autonomic (self-managing) systems and design methods used for them despite the increasingly pervasiveness of the technology. There are also significant limitations {{to the way in which}} these systems are assessed and validated, with heavy reliance on traditional design-time techniques, despite the highly dynamic behaviour of these systems in dealing with run-time configuration changes and environmental and context changes. These limitations ultimately undermine the <b>trustability</b> of these systems and are barriers to eventual certification. We propose that the first vital step in this chain is to introduce robust techniques by which the systems ca...|$|E
40|$|We {{investigate}} {{the problem of}} autonomous agents processing pieces of information that may be corrupted (tainted). Agents {{have the option of}} contacting a central database for a reliable check of the status of the message, but this procedure is costly and therefore should be used with parsimony. Agents have to evaluate the risk of being infected, and decide if and when communicating partners are affordable. <b>Trustability</b> is implemented as a personal (one-to-one) record of past contacts among agents, and as a mean-field monitoring of the level of message corruption. Moreover, this information is slowly forgotten in time, so that at the end everybody is checked against the database. We explore the behavior of a homogeneous system {{in the case of a}} fixed pool of spreaders of corrupted messages, and in the case of spontaneous appearance of corrupted messages. Comment: 12 page...|$|E
40|$|Published in Evolving and {{adaptive}} intelligent systems. IEEE Conference 2014. (EAIS 2014) Dead-Zone logic is {{a mechanism to}} prevent autonomic managers from unnecessary, inefficient and ineffective control brevity when the system is sufficiently close to its target state. It provides a natural and powerful framework for achieving dependable self-management in autonomic systems by enabling autonomic managers to smartly carry out a change (or adapt) only when it is safe and efficient to do so-within a particular (defined) safety margin. This paper explores and evaluates the performance impact of dead-zone logic in trustworthy autonomic computing. Using two case example scenarios, we present empirical analyses that demonstrate the effectiveness of dead-zone logic in achieving stability, dependability and trustworthiness in adaptive systems. Dynamic temperature target tracking and autonomic datacentre resource request and allocation management scenarios are used. Results show that dead-zone logic can significantly enhance the <b>trustability</b> of autonomic systems...|$|E
40|$|Abstract. Making model transformations trustable is {{an obvious}} target for model-driven {{development}} since they impact on the design process reliability. Ideally, model transformations should be designed and tested {{so that they may}} be used and reused safely as MDA components. We present a method for building trustable MDA components. We first define the notion of MDA component as composed of its specification, one implementation and a set of associated test cases. The testing-for-trust approach checks the consistency between these three facets using the mutation analysis. It points out the lack of efficiency of the tests and the lack of precision of the specification. The mutation analysis thus gives a rate that evaluates: the level of consistency between the component’s facets and the level of trust we can have in a component. Relying on this estimation of the component <b>trustability,</b> developers can consciously trade reliability for resources. ...|$|E
40|$|This paper {{describes}} the AGILE technology for building self-managing systems. AGILE serves {{as both a}} policy expression language and a framework that facilitates the integration and dynamic composition of several autonomic computing techniques within a single deployment technology and supports run-time self-reconfiguration. The paper also discusses the need for self-stabilisation mechanisms for autonomic systems {{in order to reduce}} the reliance of autonomic components on external supervision and extend their behavioural scope and <b>trustability.</b> The self-stabilisation approach taken and the initial support mechanisms in this regard that have been integrated into AGILE are examined. A demonstration application illustrates the powerful dynamic adaptation capabilities of the technology. The self-stabilisation theme is prominent, but other aspects are also demonstrated, including dynamic reconfiguration at the policy level, automated built-in signal processing and trend analysis, the integration of policies and utility functions and the ease with which such advanced self-managing behaviour can be configured using AGILE...|$|E
40|$|In the (Semantic) Web, the {{existence}} or producibility of certain, consensually agreed or authoritative knowledge cannot be assumed, and criteria {{to judge the}} <b>trustability</b> and reputation of knowledge sources may not be given. These issues give rise to formalizations of web information which factor in heterogeneous and possibly inconsistent assertions and intentions, and make such heterogeneity explicit and manageable for reasoning mechanisms. Such approaches call provide valuable meta-knowledge in contemporary application fields, like open or distributed ontologies, social software, ranking and recommender systems, and domains with a high amount of controversies, such as politics and culture. As an approach to this, we introduce a lean formalism for the Semantic Web which allows for the explicit representation of controversial individual and group opinions and goals by means of so-called social contexts, and optionally for the probabilistic belief merging of uncertain or conflicting statements. Doing so, our approach generalizes concepts such as provenance annotation and voting {{in the context of}} ontologies and other kinds of Semantic Web knowledge...|$|E
40|$|In the (Semantic) Web, the {{existence}} or producibility of certain, consensually agreed or authoritative knowledge cannot be assumed, and criteria {{to judge the}} <b>trustability</b> and reputation of knowledge sources may not be given. These issues give rise to the formalization of web information in terms of heterogeneous and possibly inconsistent public assertions and intentions, providing valuable meta-information in contemporary application fields, like open or distributed ontologies, social software, ranking and recommender systems, and domains with a high amount of controversies, such as politics and culture. As an approach towards this, we introduce a lean, intuitive formalism for the Semantic Web which allows for the explicit representation of semantic heterogeneity by means of so-called social contexts, and optionally for the probabilistic aggregation and social rating of possibly uncertain or contradictious assertions. Inter alia, this allows to stochastically generalize multiple assertions (yielding complexity reduction), and generalizes the concept of folksonomies to any ontologies and description logic knowledge bases emergent from social choice processes...|$|E
40|$|The {{prediction}} of protein-protein interactions (PPI) has recently {{emerged as an}} important problem {{in the fields of}} bioinformatics and systems biology, {{due to the fact that}} most essential cellular processes are mediated by these kinds of interactions. In this thesis we focussed in the {{prediction of}} co-complex interactions, where the objective is to identify and characterize protein pairs which are members of the same protein complex. Although high-throughput methods for the direct identification of PPI have been developed in the last years. It has been demonstrated that the data obtained by these methods is often incomplete and suffers from high false-positive and false-negative rates. In order to deal with this technology-driven problem, several machine learning techniques have been employed in the past to improve the accuracy and <b>trustability</b> of predicted protein interacting pairs, demonstrating that the combined use of direct and indirect biological insights can improve the quality of predictive PPI models. This task has been commonly viewed a...|$|E
40|$|Since multi-agent {{systems are}} {{inherently}} complex, there are possibilities that errors related to multi-agent systems interaction could occur. Currently, many verification approaches {{have been proposed}} by focusing on specific properties, using a particular technique and during certain development phase. However, each technique has its limitations. As interaction between agents and multi-agent systems environments evolve during runtime, not all multi-agent systems interaction requirements can be specified and verified during design and development. Thus, some new interaction properties such as agent availability and <b>trustability</b> need to be verified during runtime. In this research, a solution is proposed in which newly defined agents interaction quality requirements are specified, developed into metrics and verified within multi-agent systems runtime verification framework. It is aimed to improve {{the effectiveness of the}} verification of agent interactions during runtime. Finally, an experiment is set up to capture message passing between agents and to gather runtime system profiles to evaluate the proposed solution...|$|E
40|$|The Internet of Things {{is nothing}} new. First {{introduced}} as Ubiquitous Computing by Mark Weiser [49] around 1990, the basic {{concept of the}} “disappearing computer” has been studied as Ambient Intelligence or Pervasive Computing in the decades that followed. Today we witness the first large scale applications of these ideas. We see RFID technology being used in logistics, shopping, public transport and the like. The use of smart phones is soaring. Many of them are able to determine their location using GPS (Global Positioning System). Some phones already have NFC (Near Field Communication) capabilities, allowing them to communicate with objects tagged with RFID directly. Combined with social networking (like Facebook or Twitter), this gives rise to advanced location based services, and augmented reality applications. In fact social networks interconnecting things as well as humans have already emerged. Example are Patchube, a web-based service built to manage the world’s real-time data 1 and Flukso, a web-based community metering application 2. As the full ramifications of the Internet of Things start to unfold, this confluence of cyberspace and physical space is posing interesting new and fundamental research challenges. In particular, as we will argue in this essay, it has a huge impact {{in the area of}} security, privacy and <b>trustability.</b> As Bruce Schneier puts it {{in a recent issue of}} CryptoGram [38] (while discussing IT ingeneral) : “[ [...] . ] it’s not under your control, it’s doing things without your knowledge and consent, and it’s not necessarily acting in your best interests. ” The question then is how to ensure that, despite these adverse conditions, the Internet of Things is a safe, open, supportive and in general pleasant environment for people to engage with, or in fact for people to live in. This essay is structured as follows. We define the Internet of Things in section 2, and describe the main privacy, security and <b>trustability</b> issues associated with it in section 3. Solutions to these problems will have to deal with certain constraints, as explained in section 4. Section 5 discusses classical solutions based on data minimisation techniques, while section 6 discusses more recent alternative approaches. We conclude with an extensive overview of research challenges in section 7...|$|E
30|$|Another {{interesting}} {{report about}} the usage of social media by governmental agencies and operation centers (Lindsay 2010) concludes that social media open new opportunities within the emergency management by establishing a direct communication channel between operators, victims and witnesses in a simple way. From the citizens’ perspective, they can actively participate during the response activities whilst decision makers can use collected information to know more detailed data about human and physical damages (Díaz et al. 2014). Nevertheless, there are some limitations {{to take into account}} like for example the <b>trustability</b> of posted information and the consequent need of specific policies for guaranteeing the privacy (Hiltz and Kushma 2014). Indeed, a study performed with professional EM workers in the area of British Columbia (Canada) and in Spain showed that the main issues that might deter official agencies to use social media is the lack of resources to keep up with such a huge quantity of data, {{and to be able to}} make sense of it (Díaz et al. 2014). Next section reviews some visual analytics tool designed to cope with this problem.|$|E
40|$|International audienceThe {{aim of this}} {{presentation}} is to demonstrate a scalable, modular, refinable methodology to design, assess and improve the <b>trustability</b> of an existing (20 years old), large (500 k lines of C), open source (Eclipse/Polarsys IWG project POP) code generation suite using off-the-shelf, open-source, SAT/SMT verification tools (Yices), by adapting and optimizing the translation validation principle introduced by Pnueli et al. in 1998. This methodology results from the ANR project VERISYNC, in which we aimed at revisiting Pnueli's seminal work on translation validation using off-the-shelf, up-to-date, verification technology. In face of the enormous task at hand, the verification of a compiler infrastructure comprising around 500 000 lines of C code, we devised to narrow down and isolate the problem to the very data-structures manipulated by the infrastructure at the successive steps of code generation, in order to both optimize the whole verification process and make {{the implementation of a}} working prototype at all doable. Our presentation outlines the successive steps of this endeavour, from clock synthesis, static scheduling to target code production...|$|E
40|$|In {{order for}} future {{enterprises}} to make {{effective use of}} the Future Internet, {{it is necessary that}} their Enterprise Architecture aligns with the Future Internet Architecture. An Enterprise Architecture is a comprehensive architecture description that spans enterprise and technology aspects, to allow understanding and analysis of relevant enterprise properties. As such, it is an important instrument to plan processes and resources that are involved in realizing the goals of the enterprise. The Future Internet Architecture comprises a set of design principles and a structure of components and their relationships that together underlie the construction of a ‘better’ Internet, featuring among others the Internet of Things and the Internet of Services. This workshop targets models and mechanisms that make an Enterprise Architecture Future Internet-enabled, extending {{it in such a way}} that Future Internet features can be exploited. In particular, we target dynamic enterprise collaborations with changing partnerships to fulfil emerging customer needs, using real-time monitoring information for decision-making and optimization. Important modelling and analysis goals of an Enterprise Architecture in this context include interoperability, flexibility, <b>trustability,</b> profitability, sustainability, and adaptability...|$|E
40|$|Modern {{search engines}} are {{good enough to}} answer popular {{commercial}} queries with mainly highly relevant documents. However, our experiments show that users behavior on such relevant commercial sites may differ {{from one to another}} web-site with the same relevance label. Thus search engines face the challenge of ranking results that are equally relevant {{from the perspective of the}} traditional relevance grading approach. To solve this problem we propose to consider additional facets of relevance, such as <b>trustability,</b> usability, design quality and the quality of service. In order to let a ranking algorithm take these facets in account, we proposed a number of features, capturing the quality of a web page along the proposed dimensions. We aggregated new facets into the single label, commercial relevance, that represents cumulative quality of the site. We extrapolated commercial relevance labels for the entire learning-to-rank dataset and used weighted sum of commercial and topical relevance instead of default relevance labels. For evaluating our method we created new DCG-like metrics and conducted off-line evaluation as well as on-line interleaving experiments demonstrating that a ranking algorithm taking the proposed facets of relevance into account is better aligned with user preferences...|$|E
