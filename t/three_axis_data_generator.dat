0|1742|Public
30|$|In {{this work}} {{we have used}} two {{databases}} recorded with an inertial motion tracking system, the inertial gyroscopic system (IGS- 190) from Animazoo [23]. The IGS- 190 is a commercial motion capture suit that contains 18 inertial sensors, which each consist of a <b>three</b> <b>axis</b> accelerometer, a <b>three</b> <b>axis</b> gyroscope and a <b>three</b> <b>axis</b> magnetometer. The <b>data</b> from those three sources are integrated and fused directly in the inertial sensor boxes. Angles between the body segments are thus provided straight from the sensors; no mapping is necessary between tracked 3 D positions of markers and joint angles, unlike in optical motion capture systems.|$|R
40|$|ITC/USA 2015 Conference Proceedings / The Fifty-First Annual International Telemetering Conference and Technical Exhibition / October 26 - 29, 2015 / Bally's Hotel & Convention Center, Las Vegas, NVThis paper {{describes}} {{a project to}} implement a body area network to monitor the movements of a human subject. The sensor nodes can measure six degrees of movement by using a <b>three</b> <b>axis</b> accelerometer and <b>three</b> <b>axis</b> gyroscope. The <b>data</b> is transmitted wirelessly from the sensors to a wearable microcontroller. The microcontroller interfaces with a computer application that allows a user to easily analyze and interpret the stored data...|$|R
40|$|This paper {{presents}} the designing of a neural network for {{the classification of}} Human activity. A Triaxial accelerometer sensor, housed in a chest worn sensor unit, {{has been used for}} capturing the acceleration of the movements associated. All the <b>three</b> <b>axis</b> acceleration <b>data</b> were collected at a base station PC via a CC 2420 2. 4 GHz ISM band radio (zigbee wireless compliant), processed and classified using MATLAB. A neural network approach for classification was used with an eye on theoretical and empirical facts. The work shows {{a detailed description of the}} designing steps for the classification of human body acceleration data. A 4 -layer back propagation neural network, with Levenberg-marquardt algorithm for training, showed best performance among the other neural network training algorithms. Comment: 6 pages, 4 figures, 4 Tables, International Conference on Convergence Information Technology, pp. 430 - 435, 2008 Third International Conference on Convergence and Hybrid Information Technology, 200...|$|R
30|$|To {{train the}} algorithms, {{we used the}} open dataset {{provided}} by Bergasa et al. [57]. The dataset provides <b>three</b> <b>axis</b> accelerometer <b>data</b> labeled as cautious and reckless based on thresholds given by Paefgen et al. [54] for acceleration, braking, and turning. This dataset contains driving data for six different drivers and vehicles, D 1 through D 6, for two different routes, one is 25  km in a road with 3 lanes in each direction and a speed limit of 120  km/h, the other is ~ 16  km on a secondary road with one lane on each direction and a 90  km/h speed limit. Each driver drove the same route three times. For each driver’s data, a 3 -fold cross-validation was performed, where each driver’s data are randomly divided into two pieces, one piece of 35 % for training, and one piece of 30 % for testing, and subset of data that generated the best results for each algorithm checked. The training and testing hardware was the smartphone discussed in Section 5.4.|$|R
40|$|Este trabalho tem como objetivo identificar e analisar como se caracterizam as chamadas publicitárias dos anúncios de página inteira e dupla veiculados na revista piauí. O referencial teórico deste trabalho busca definir e contextualizar o anúncio publicitário impresso, abordando as influências históricas e da criatividade, bem como a importância do texto, da chamada e da imagem em sua criação. O corpus é constituído por anúncios de quatro edições da revista piauí ao longo de quatro anos. Para atingir o objetivo proposto, são analisados os dados utilizando-se de três eixos (dados gerais, estrutura e conteúdo), valendo-se da técnica de análise de conteúdo. O trabalho apresenta características marcantes das chamadas e do texto publicitário em geral, como, por exemplo, a sua atual importância e presença em parte significativa dos anúncios veiculados na revista piauí. This study aims to {{identify}} and analyze how the headlines of the full and double page advertisements ran in piauí magazine are characterized. The theoretical framework for this research seeks to define and contextualize printed advertising, addressing how it is influenced by its history and creativity, {{as well as the}} importance of the copywriting, image and headline in its creation. The corpus consists of advertisings of four editions of the magazine over four years. To achieve the proposed objective, the data is analyzed using <b>three</b> <b>axis</b> (general <b>data,</b> structure and content) and the content analysis technique. The paper presents the main characteristics of the headlines and advertising copy in general, such as, for example, its current importance and presence {{in the vast majority of}} the ads running in piauí magazine...|$|R
40|$|Abstract Currently, {{there are}} very few {{programs}} with the capability to create synthetic, but realistic data. This kind of data is useful for testing and evaluating data mining and data matching systems without endangering the rights of actual persons. This paper is a report on efforts to expand upon an existing <b>data</b> <b>generator</b> program such that it can then produce temporal or longitudinal data, i. e. data that follows some timebased or time-related pattern. At the Research School of Computer Science (RSCS) at the Australian National University (ANU), a flexible <b>data</b> <b>generator</b> has been developed for the creation of synthetic personal information. It is for this <b>data</b> <b>generator</b> that the module for longitudinal data generation is created. With this prototype, the <b>data</b> <b>generator</b> can then create personal data that reflects time-related trends, such as credit history, address changes, and transaction records. Keeping true to the programming of the <b>data</b> <b>generator,</b> the longitudinal <b>data</b> <b>generator</b> has been developed using the Python programming language...|$|R
40|$|Abstract [...] Test data {{generation}} {{is basically the}} process of identifying a set of data which satisfy the criteria set for testing. Lot of research have been done by many researchers and they developed many test <b>data</b> <b>generators</b> like random test <b>data</b> <b>generators,</b> symbolic test <b>data</b> <b>generators</b> and dynamic test <b>data</b> <b>generators.</b> This paper applied the optimization study of the test case generation based on the Genetic Algorithm and generates test cases which are far more reliable. 4. [REPLACE] New population {{generation is}} replaced. 5. [TEST] If the specified condition is satisfied stop and return the solution [1]. Keywords [...] Test case generation, Genetic Algorithms, fitness functions, optimizatio...|$|R
40|$|The modular {{attitude}} {{control of a}} large space platform is studied using a <b>three</b> <b>axis</b> five body model of a flexible spacecraft. Three degree of freedom hinges are considered. A <b>three</b> <b>axis</b> four body model of a truss and a <b>three</b> <b>axis</b> ten body model of a space platform are presented. A state variable rotational dynamics model and a multilevel state variable model are shown. A decomposed performance index and decomposed Hamiltonian are also presented. A subproblem hierarchy for hybrid multilevel LQR {{attitude control}} of a <b>three</b> <b>axis</b> model is described...|$|R
5000|$|... foomatic-db-hpijs: Foomatic XML <b>data</b> <b>generators</b> for HP's HPIJS driver.|$|R
5000|$|Pathwise test <b>data</b> <b>generators</b> require two inputs {{from the}} user: ...|$|R
40|$|The Lander Trajectory Reconstruction (LTR) {{computer}} {{program is a}} tool for analysis of the planetary entry trajectory and atmosphere reconstruction process for a lander or probe. The program {{can be divided into}} two parts: (1) the <b>data</b> <b>generator</b> and (2) the reconstructor. The <b>data</b> <b>generator</b> provides the real environment in which the lander or probe is presumed to find itself. The reconstructor reconstructs the entry trajectory and atmosphere using sensor data generated by the <b>data</b> <b>generator</b> and a Kalman-Schmidt consider filter. A wide variety of vehicle and environmental parameters may be either solved-for or considered in the filter process...|$|R
5000|$|Test <b>Data</b> <b>Generators</b> {{based on}} their {{approaches}} are typically classified into ...|$|R
40|$|Generating {{data sets}} for the {{performance}} testing of database systems on a particular hardware configuration and application domain is a very time consuming and tedious process. It is time consuming, {{because of the large}} amount of data that needs to be generated and tedious, because new <b>data</b> <b>generators</b> might need to be developed or existing once adjusted. The difficulty in generating this data is amplified by constant advances in hardware and software that allow the testing of ever larger and more complicated systems. In this paper, we present an approach for rapidly developing customized <b>data</b> <b>generators.</b> Our approach, which is based on the Parallel <b>Data</b> <b>Generator</b> Framework (PDGF), deploys a new concept of so called meta generators. Meta generators extend the concept of column-based generators in PDGF. Deploying meta generators in PDGF significantly reduces the development effort of customized <b>data</b> <b>generators,</b> it facilitates their debugging and eases their maintenance. 1...|$|R
40|$|Abstract. We need to {{generate}} {{various kinds of}} XML data for diverse purposes. Existing XML <b>data</b> <b>generators</b> are developed {{to generate}} XML data that is suitable for particular applications, and their functionalities are limited in terms of generating XML data. This paper introduces a new versatile XML <b>data</b> <b>generator</b> called VeXGene that not only improves the drawbacks of existing XML <b>data</b> <b>generators</b> but also adds new data generation functions. For generating XML data, VeXGene uses raw data files and structure definition files. The raw data file is a text file that has user-supplied data. VeXGene can also generate XML data without accessing raw data files...|$|R
30|$|IMU {{data are}} also {{acquired}} from the Microstrain sensor integrating a GPS receiver, {{in terms of}} absolute position and orientation, together with IMU <b>three</b> <b>axis</b> acceleration and <b>three</b> <b>axis</b> angular velocities. An initialization phase of 3  s is required by the IMU and GPS data fusion algorithm for absolute attitude estimation.|$|R
5000|$|Private label cards (depending on {{the terms}} of {{agreement}} between the company's financial terminal <b>data</b> <b>generator</b> ...|$|R
50|$|She was {{successful}} on several occasions, sinking <b>three</b> <b>Axis</b> vessels during 1944.|$|R
40|$|Abstract—Developing, for example, {{a simple}} booking web service with modern tools {{can be a}} matter of a few weeks work. Testing such a system should not need to take more time than that. Automatically {{generating}} tests from specified properties of the system using the tool QuickCheck provides professional developers with the required test efficiency. But how good is the quality of these automatically generated tests? Do they cover the cases that one would have written in manual tests? The quality depends on the specified properties and <b>data</b> <b>generators</b> and so far there has not been an objective way to evaluate the quality of these QuickCheck generators. In this paper we present a method to assess the quality of QuickCheck test <b>data</b> <b>generators</b> by formulating requirements on them. Using this method we can give feedback to developers of such <b>data</b> <b>generators</b> in an early stage. The method supports developers in improving <b>data</b> <b>generators,</b> which may lead to an increase of the effectiveness in testing while maintaining the same efficiency. I...|$|R
40|$|We {{present a}} novel {{counterexample}} generator for the interactive theorem prover Isabelle {{based on a}} compiler that synthesizes test <b>data</b> <b>generators</b> for functional programming languages (e. g. ML, Haskell) from specifications in Isabelle. In contrast to naive type-based test <b>data</b> <b>generators,</b> the smart generators take the preconditions into account and only generate tests that fulfill the preconditions. The smart generators are constructed by a compiler that reformulates the preconditions as logic programs and analyzes them with an enriched mode inference. From this inference, the compiler can construct the desired generators in the functional programming language. Applying these test <b>data</b> <b>generators</b> reduces the number of tests significantly and enables us to find errors in specifications where naive random and exhaustive testing fail...|$|R
40|$|Synthetically {{generated}} {{data has}} always been important for evaluating and understanding new ideas in database research. In this paper, we describe a <b>data</b> <b>generator</b> for generating synthetic complex-structured XML data that allows for {{a high level of}} control over the characteristics of the generated <b>data.</b> This <b>data</b> <b>generator</b> is certainly not the ultimate {{solution to the problem of}} generating synthetic XML data, but we have found it very useful in our research on XML data management, and we believe that it can also be useful to other researchers. Furthermore, we hope that this paper starts a discussion in the XML community about characterizing and generating XML data, and that it may serve as a first step towards developing a commonly accepted XML <b>data</b> <b>generator</b> for our community. ...|$|R
30|$|Nowadays {{a number}} of {{software}} packages support CityGML, from <b>data</b> <b>generators</b> to visualisers. In this section they are overviewed considering their ADE support.|$|R
40|$|Nineteen bags of Pleistocene Age river {{deposited}} {{samples were}} taken from the Cemex Quarry in Fresno, CA. There are three formations in the area, deposited from rivers derived from glaciers, consisting of the Modesto, Riverbank, and Turlock Lake; youngest deposited to oldest respectively. Phi sizes refer to each of the individual grains diameter, larger sized grains are in the negative spectrum while smaller grains are positive. Each sample has phi size bags that range in sizes from - 5 to 4 +, excluding phi size - 2. For the phi sizes of - 3 to - 5, each individual rock was measured on its <b>three</b> <b>axis</b> and that <b>data</b> was compiled into an excel sheet. This was to determine the textural maturity of the samples which was derived from a number system for the following categories: roundness, rough/smooth, shiny/dull and grain shape. QLF (Quartz, Lithic, and Feldspar) tests were conducted on the phi size 0, - 1, and - 2. This determines the geochemical weathering and its compositional maturity. The QLF graphs indicate that as the phi size increases the grains become more compositionally mature, consisting of mainly quartz while having minute amounts of lithics and feldspars. Respectively, as phi size decreases the grains become less compositionally mature and consist upwards of 90 % lithics. OSL samples {{were taken from}} the quarry. These samples give the exact dating to the given areas sampled from the different formations. Due to the month long process, per sample, these results will be included in future publications of this research...|$|R
40|$|The aim of {{this thesis}} is to {{research}} the current possibilities and limitations of automatic generation of synthetic XML and JSON documents used {{in the area of}} Big Data. The first part of the work discusses the properties of the most used XML <b>data</b> <b>generators,</b> Big <b>Data</b> and JSON <b>generators</b> and compares them. The next part of the thesis proposes an algorithm for data generation of semistructured data. The main focus of the algorithm is on the parallel execution of the generation process while preserving the ability to control the contents of the generated documents. The <b>data</b> <b>generator</b> can also use samples of real data in the generation of the synthetic data and is also capable of automatic creation of simple references between JSON documents. The last part of the thesis provides the results of experiments with the <b>data</b> <b>generator</b> exploited for the purpose of testing database MongoDB, describes its added value and compares it to other solutions. Powered by TCPDF (www. tcpdf. org...|$|R
5000|$|... #Caption: <b>Three</b> <b>axis</b> {{model of}} {{political}} ideologies with both moderate and radical versions and {{the goals of}} their policies ...|$|R
40|$|This paper {{presents}} a synthetic <b>data</b> <b>generator</b> that outputs timestamped transactional data with embedded temporal patterns {{controlled by a}} set of input parameters. In particular, calendar schema, which is determined by a hierarchy of input time granularities, is used as a framework of possible temporal patterns. An example of calendar schema is (year, month, day), which provides a framework for calendar-based temporal patterns of the form - 38352, where each is either an integer or the symbol. For example, is such a pattern, which corresponds to the time intervals consisting of all the 16 th days of all months in year 2000. This paper also evaluates the <b>data</b> <b>generator</b> through a series of experiments. The synthetic <b>data</b> <b>generator</b> is intended to provide support for data mining community in evaluating various aspects (especially the temporal aspects and the scalability) of data mining algorithms...|$|R
5000|$|<b>Three</b> <b>axis</b> control {{version with}} elevator, rudder and ailerons. Standard {{powerplant}} supplied was the Kawasaki 440 snowmobile engine producing [...]|$|R
40|$|In {{this work}} a novel turning process is developed, namely {{simultaneous}} <b>three</b> <b>axis</b> turning, where the tool is {{moved in a}} third rotational (B) axis besides the already existing translational (X, Z) axes. By this the geometrical flexibility of turning processes can be increased and the tool consumption can be decreased significantly. To apply the technology, models and methods are developed to design simultaneous <b>three</b> <b>axis</b> turning processes systematically and stepwise for arbitrary workpieces...|$|R
50|$|The {{first use}} of the <b>three</b> <b>axis</b> ring-coil {{magnetometer}} was on the Apollo 16 moon mission. Subsequently it was used on the Magsat. The MESSENGER mission has triaxial ring-coil magnetometer {{with a range of}} +/- 1000 mT and a sensitivity of 0.02 mT, still in progress, the mission is designed to get detailed information about Mercurian magnetosphere. The {{first use of}} spherical magnetometer in <b>three</b> <b>axis</b> configuration was on the Orsted (satellite).|$|R
40|$|Simulated {{boundary}} potential {{data for}} Electrical Impedance Tomography (EIT) are {{generated by a}} MATLAB based EIT <b>data</b> <b>generator</b> and the resistivity reconstruction is evaluated with Electrical Impedance Tomography and Diffuse Optical Tomography Reconstruction Software (EIDORS). Circular domains containing subdomains as inhomogeneity are defined in MATLAB-based EIT <b>data</b> <b>generator</b> and the boundary data are calculated by a constant current simulation with opposite current injection (OCI) method. The resistivity images reconstructed for different boundary data sets and images are analyzed with image parameters to evaluate the reconstruction...|$|R
50|$|Intelligent Test <b>Data</b> <b>Generators</b> {{depend on}} {{sophisticated}} {{analysis of the}} code to guide the search of the test data. Intelligent Test <b>Data</b> <b>Generators</b> are essentially utilize one of the test data generation method coupled with the {{detailed analysis of the}} code. This approach may generate test data quicker than the other approaches but the analysis required for the utilization of this approach over a wide variety of programs is quite complex and requires a great deal of insight to anticipate the different situations that may arise.|$|R
5000|$|... #Caption: DRO {{providing}} a <b>three</b> <b>axis</b> display with pitch circle calculator, diameter/radius conversion, absolute and incremental toggle, and inch metric toggle ...|$|R
50|$|The Cyclone AX2000 is a British built <b>three</b> <b>axis</b> microlight, first {{flown in}} the 1990s. It seats two in {{side-by-side}} configuration.|$|R
40|$|AbstractDesigning {{test cases}} and {{generating}} data {{are very important}} phases in software engineering these days. In order to generate test <b>data,</b> some <b>generators</b> such as random test <b>data</b> <b>generators,</b> <b>data</b> specification <b>generators</b> and path-oriented (Path-Wise) test <b>data</b> <b>generators</b> are employed. One {{of the most important}} problems in the path-oriented test <b>data</b> <b>generator</b> is the lack of attention given to discovering faults by the test data. In this paper an approach is proposed to generate some test data automatically so that we can realize the goal of discovering more faults in less time. The number of faults near the boundaries of the input domain is more than the center, according to the Pareto 80 – 20 principle the test data of this approach will be generated at 20 % of the allowable area boundary. To do this, we extracted the boundary hypercuboids and then the test data will be generated by exploiting these hypercuboids. The experimental results show that the fault detection probability and the fault detection speed are improved significantly, compared with the previous approaches. By generating data in this way, more faults are discovered {{in a short period of}} time which makes it more possible to deliver products on time...|$|R
40|$|The {{exponential}} {{growth in the}} amount of data retained by today’s systems is fostered by a recent paradigm shift towards cloud computing and the vast deployment of data-hungry applications, such as social media sites. At the same time systems are capturing more sophisticated data. Running realistic benchmarks to test the performance and robustness of these applications is becoming increasingly difficult, because of the amount of data that needs to be generated, the number of systems that need to generate the data and the complex structure of the data. These three reasons are intrinsically connected. Whenever large amounts of data are needed, its generation process needs to be highly parallel, in many cases across-systems. Since the structure of the data {{is becoming more and more}} complex, its parallel generation is extremely challenging. Over the years there have been many papers about <b>data</b> <b>generators,</b> but there has not been a comprehensive overview of the requirements of today’s <b>data</b> <b>generators</b> covering the most complex problems to be solved. In this paper we present such an overview by analyzing the requirements of today’s <b>data</b> <b>generators</b> and either explaining how the problems have been solved in exist-ing <b>data</b> <b>generators,</b> or showing why the problems have not been solved yet...|$|R
40|$|Abstract. Data {{generation}} {{is a key}} issue in big data benchmarking that aims to generate application-specific data sets to meet the 4 V re-quirements of big data. Specifically, big <b>data</b> <b>generators</b> need to generate scalable data (Volume) of different types (Variety) under controllable generation rates (Velocity) while keeping the important characteristics of raw data (Veracity). This gives rise to various new challenges about how we design generators efficiently and successfully. To date, most exist-ing techniques can only generate limited types of data and support spe-cific big data systems such as Hadoop. Hence we develop a tool, called Big <b>Data</b> <b>Generator</b> Suite (BDGS), to efficiently generate scalable big data while employing data models derived from real data to preserve data veracity. The effectiveness of BDGS is demonstrated by developing six <b>data</b> <b>generators</b> covering three representative data types (structured, semi-structured and unstructured) and three data sources (text, graph, and table data) ...|$|R
2500|$|The Cyclone AX2000 is a British built <b>three</b> <b>axis</b> microlight, first {{flown in}} the 1990s. [...] It seats two in {{side-by-side}} configuration.|$|R
