124|1393|Public
2500|$|Investigation In July 2011, the Pennsylvania Department of Education {{released}} a report listing Nescopeck Elementary School among 89 schools with an unusually high level of changes of wrong answers to correct answers on its 2009 5th grade PSSA's. In January 2012, the state {{released a}} list of schools that had been cleared of wrongdoing. Nescopeck Elementary School was not on that list. The Pennsylvania Department of Education issued a formal public reprimand against the Principal for [...] "Allegations that Educator violated the integrity and security of the PSSA by failing to maintain the security of PSSA <b>test</b> <b>documents</b> and by providing improper assistance to a student taking the PSSA." ...|$|E
5000|$|... #Caption: PISA <b>test</b> <b>documents</b> on {{a school}} table (Neues Gymnasium, Oldenburg, Germany, 2006) ...|$|E
5000|$|Test execution: Testers {{execute the}} {{software}} {{based on the}} plans and <b>test</b> <b>documents</b> then report any errors found to the development team.|$|E
50|$|Package <b>testing</b> <b>documents</b> and {{ensures that}} {{packages}} meet regulations and end-use requirements. Manufacturing processes must be controlled and validated to ensure consistent performance.|$|R
40|$|Text {{categorization}} can {{be viewed}} as a process of category search, in which one or more categories for a <b>test</b> <b>document</b> are searched for by using given training documents with known categories. In this paper a cluster-based search with a probabilistic clustering algorithm is proposed and evaluated on two data sets. The efficiency, effectiveness, and noise tolerance of this search strategy were confirmed to be better than those of a full search, a category-based search, and a cluster-based search with nonprobabilistic clustering. 1 Introduction Text categorization {{can be viewed}} as a process of category search: given training documents with known categories, a program searches for one or more categories that a <b>test</b> <b>document</b> is assumed to have. The simplest strategy would be to search the K-nearest training <b>documents</b> to the <b>test</b> <b>document</b> and use the categories assigned to those training documents. This is known as MBR (Memory Based Reasoning) [Stanfill and Waltz, 1986] or K-NN (K-Neares [...] ...|$|R
50|$|Protocatechuic acid (PCA) is {{antioxidant}} and anti-inflammatory. PCA {{extracted from}} Hibiscus sabdariffa protected against chemically induced liver toxicity in vivo. In vitro <b>testing</b> <b>documented</b> antioxidant and anti-inflammatory activity of PCA, while liver protection in vivo {{was measured by}} chemical markers and histological assessment.|$|R
5000|$|Traceability matrix: A {{traceability}} matrix is a table that correlates requirements or design documents to <b>test</b> <b>documents.</b> It {{is used to}} change tests when related source documents are changed, to select test cases for execution when planning for regression tests by considering requirement coverage.|$|E
50|$|While {{automated}} checking {{tools are}} helpful for website development and maintenance, they cannot guarantee that a document will display as intended in all browsers. Developers should always <b>test</b> <b>documents</b> {{in a variety}} of browsers (including mobile browsers) to locate problems that cannot be detected with a computerized checking tool.|$|E
50|$|When a test {{is started}} from WorldBench, the {{benchmark}} manager begins running the test script. This script {{is a series}} of commands—menu requests, keystrokes, and mouse clicks—that cause the application to step through typical tasks. For example, in Microsoft Office, the script starts by unpacking the version of Office embedded on the WorldBench CD. Then it opens the included <b>test</b> <b>documents,</b> copies and pastes text, changes the documents’ formatting, checks spelling, and undertakes other tasks that are likely to be performed using Office. When Office has finished all the steps in the script, the benchmark closes Office.|$|E
40|$|The {{objective}} of the Electromagnetic Borehole Flowmeter (EBF) <b>testing</b> <b>documented</b> in this report is to expand the technology to include simultaneous characterization of conductivity, contaminant concentration, and mass flux profiles. The latter two parameters, especially mass flux, can be valuable information for remedial design...|$|R
40|$|Text {{categorization}} is {{the process}} of grouping documents into categories based on their contents. This process is important to make information retrieval easier, and it became more important due to the huge textual information available online. The main problem in text categorization is how to improve the classification accuracy. Although Arabic text categorization is a new promising field, there are a few researches in this field. This paper proposes a new method for Arabic text categorization using vector evaluation. The proposed method uses a categorized Arabic documents corpus, and then the weights of the <b>tested</b> <b>document's</b> words are calculated to determine the document keywords which will be compared with the keywords of the corpus categorizes to determine the <b>tested</b> <b>document's</b> best category...|$|R
50|$|Through iLearn, DCMST {{teachers}} and other Dearborn Public Schools can give students easy online access to many things for their classes. Common items posted on iLearn by DCMST teachers include weekly syllabi, rubrics, quizzes, <b>tests,</b> <b>documents,</b> school news, individual student grades, and links to helpful sites.|$|R
5000|$|Literature {{from various}} {{studies show that}} {{approximately}} 25-50% of a nurse’s shift is spent on documentation. As most documentation {{is done in the}} traditional manner, that is using paper and pen, enabling a POC documentation device could potentially allow 25-50% more time at the bedside. Using speech recognition and information has been studied [...] as a way to support nurses in POC documentation with encouraging results: 5276 of 7277 test words were recognised correctly and information extraction achieved the F1 of 0.86 in the category for irrelevant text and the macro-averaged F1 of 0.70 over the remaining 35 nonempty categories of the nursing handover form with our 101 <b>test</b> <b>documents.</b>|$|E
5000|$|These new {{standards}} created {{a focus on}} Level 1 at , Level 2 at , Level 3 at [...] protection as tested with the new engineered knives defined in these <b>test</b> <b>documents.</b> The lowest level of this requirement at 25 joules was addressed {{by a series of}} textile products of both wovens, coated wovens and laminated woven materials. All of these materials were based on Para-aramid fiber. The co-efficient of friction for ultra high molecular weigh polyethylene (UHMWPE) prevented its use in this application. The TurtleSkin DiamondCoat and Twaron SRM products addressed this requirement using a combination of Para-Aramid wovens and bonded ceramic grain. These ceramic-coated products do not have the flexibility and softness of un-coated textile materials.|$|E
5000|$|Beginning {{with the}} work of Turney, many {{researchers}} have approached keyphrase extraction as a supervised machine learning problem.Given a document, we construct an example for each unigram, bigram, and trigram found in the text (though other text units are also possible, as discussed below). We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?). We assume there are known keyphrases available for a set of training documents. Using the known keyphrases, we can assign positive or negative labels to the examples. Then we learn a classifier that can discriminate between positive and negative examples {{as a function of the}} features. Some classifiers make a binary classification for a test example, while others assign a probability of being a keyphrase. For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases.After training a learner, we can select keyphrases for <b>test</b> <b>documents</b> in the following manner. We apply the same example-generation strategy to the <b>test</b> <b>documents,</b> then run each example through the learner. We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model. If probabilities are given, a threshold is used to select the keyphrases.Keyphrase extractors are generally evaluated using precision and recall. Precision measures howmany of the proposed keyphrases are actually correct. Recall measures how many of the truekeyphrases your system proposed. The two measures can be combined in an F-score, which is theharmonic mean of the two (F = 2PR/(P + R) [...] ). Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization.|$|E
40|$|The paper {{considers}} {{the possibility of}} using the variation of information function as a quality criterion for categorizing a collection of documents. The performance of the variation of information function is being examined subject to the number of categories and the sample volume of the <b>test</b> <b>document</b> collection...|$|R
40|$|This Functional Design Criteria is {{designed}} to summarize and give guidance during the development of design, manufacturing and <b>testing</b> specification <b>documents.</b> As the overview document bounding parameters are specified with detailed acceptance criteria to be developed in the more detailed and separate design, manufacturing and <b>testing</b> specification <b>documents...</b>|$|R
40|$|Abstract. In this paper, we {{introduce}} Sequential Classifiers Combination (SCC) into text categorization {{to improve}} both the classification effectiveness and classification {{efficiency of the}} combined individual classifiers. We apply two classifiers sequentially for experimental study, where the first classifier (called filtering classifier) is used to generate candidate categories for the <b>test</b> <b>document</b> and the second classifier (called deciding classifier) is used to select a final category for the <b>test</b> <b>document</b> from the candidate categories. Experimental results indicate that when combining boosting and kNN methods, the combined classifier outperforms the best {{one of the two}} individual classifiers, {{and in the case of}} combining Rocchio and kNN methods, the combined classifier performs equally well as kNN while its efficiency is much better than kNN and is close to that of Rocchio. ...|$|R
5000|$|Many {{of extreme}} {{programming}} practices {{have been around}} for some time; the methodology takes [...] "best practices" [...] to extreme levels. For example, the [...] "practice of test-first development, planning and writing tests before each micro-increment" [...] was used as early as NASA's Project Mercury, in the early 1960s [...] To shorten the total development time, some formal <b>test</b> <b>documents</b> (such as for acceptance testing) have been developed in parallel (or shortly before) the software is ready for testing. A NASA independent test group can write the test procedures, based on formal requirements and logical limits, before the software has been written and integrated with the hardware. In XP, this concept is taken to the extreme level by writing automated tests (perhaps inside of software modules) which validate the operation of even small sections of software coding, rather than only testing the larger features.|$|E
5000|$|Eddie {{introduces}} Stevie and Susan Carol to Bob Arciero, {{an orthopedic}} surgeon for the Dreams. Arciero, unlike Dr. Snow, is honest and {{is not part of}} the cover up. He gets them the drug <b>test</b> <b>documents.</b> With the documents at hand, Stevie and Susan Carol write the story. They talk to the newspaper lawyers, who say they need comments from NFL Commissioner Roger Goodell and Don Meeker. Goodell comments on launching an investigation and punishing anyone involved. Meeker, after hearing two paragraphs of the story, goes on a profane tirade. [...] They send the quotes and the story and go out to eat. However, they don’t come back in time and are surrounded by cameras as soon as they step in the hotel. Doncalls them the next day to tell them that he was pegged as the source and that he's not starting. Meeker is about to go on CBS and USTV to lie and try to convince people he did nothing wrong. In the process, he slanders Susan Carol on USTV.|$|E
5000|$|ISO/IEC/IEEE 29119 - 1 {{provides}} {{a common set}} of terms and concepts to support the other parts. The approach basis for ISO/IEC/IEEE 29119-2 is risk-based. In risk-based testing, risks are used to limit and arrange the test effort within project contexts of problem space, cost and schedule. This is done to solve the problem that testing can be viewed as having an infinite problem space (i.e. complete 100% testing is not possible)[...] ISO/IEC/IEEE 29119-3 provides definition of common <b>test</b> <b>documents,</b> which can be selected via tailoring as needed. ISO/IEC/IEEE 29119-4 provides structural and functional test techniques, which can also be selected as part of a tailoring activity. Finally, ISO/IEC/IEEE 29119-5 provides definition of keyword-driven test approaches and concepts, for use in project needing keywords. By the use of ISO 29119 heuristics, the test problem can be managed and solved within context of a project or organizationUse of the standards can be tailored (requirements removed, changed, or added) in support of contracts or projects to fit local context and needs resulting in varying degrees of adoption. For example, users may select only 2 or 3 document outlines from ISO/IEC/IEEE 29119-3. As per common practice, tailoring needs to be agreed to by stakeholders.|$|E
5000|$|... #Caption: The Soviet T-100 Heavy Tank being <b>tested</b> and <b>documented.</b>|$|R
40|$|Abstract In this paper, we {{describe}} {{a modified version}} of the profile-based approach for the Authorship Attribution (AA) task of the PAN 2012 challenge. Our PAN system for AA utilizes the concept of linguistic modalities 1 on profile-based (PB) approaches. We concatenate all the training documents from the same author and build author-specific sub-profiles, one per linguistic modality. Then instead of using all the different types of features to compute the similarity of a <b>test</b> <b>document</b> against an author’s profile in a single step, we compute several similarity scores using one set of features (modality) at a time. Each modality will assign the <b>test</b> <b>document</b> to the author whose sub-profile has the highest cosine similarity in that modality. Final classification decisions are based on the combination of decisions from each modality using majority voting. We achieved competitive results on PAN 2012, with encouraging results on the closed-class authorship attribution...|$|R
50|$|ONI used several {{methods to}} <b>test</b> and <b>document</b> {{internet}} censorship in a country.|$|R
40|$|Data on {{errors and}} error-likely {{situations}} in field tests were gathered from three sources: <b>test</b> <b>documents,</b> interviews with test personnel, and observations {{of a human}} factors specialist who participated in several field tests. This report summarizes the data and makes recommendations on how the likelihood of such errors can be reduced by using human factors observers with duties as described in this report...|$|E
40|$|In {{this paper}} {{we present a}} system to build {{synthetic}} graphic documents for performance evaluation using two main components. The first one performs a vectorial distortion on the objects without using any prior knowledge. The other position the objects in the documents using four filling algorithms allowing to preserve a partitioning between the objects and the document background. Experiments done on more of 300 000 symbols show that our system distort and fill in a logarithmic and gaussian way. A complementary approach, which avoids these difficulties, is to create and use synthetic documents. Here, the <b>test</b> <b>documents</b> are built by an automatic system which combines pre-defined models of document components in a pseudo-random way. <b>Test</b> <b>documents</b> and ground truth can therefore be produced simultaneously. In addition, large numbers of documents can be generated easily and with limited user involvement. This topic is emerging and only [3] [6] [7] [5] exist in the literature. Figure 1 gives examples of the documents produced by these systems. 1...|$|E
40|$|Complex {{document}} formats such as PDF and Microsoft’s Compound File Binary Format {{can contain}} {{information that is}} hidden but recoverable, {{as a result of}} text highlighting, cropping, or the embedding of high-resolution JPEG images. Private information can be released inadvertently if these fi les are distributed in electronic form. Simple experiments involving the creation of <b>test</b> <b>documents</b> can determine whether a particular program embeds hidden information...|$|E
5000|$|Understandability: The {{degree to}} which the {{component}} under <b>test</b> is <b>documented</b> or self-explaining.|$|R
25|$|The {{investigation}} {{could not}} find the functional <b>test</b> result <b>document</b> after theinstallation of the TAWS.|$|R
40|$|This article {{describes}} some approaches to problem of <b>testing</b> and <b>documenting</b> automation in information systems with graphical user interface. Combination of data mining methods and theory of finite state machines {{is used for}} testing automation. Automated creation of software documentation is based on using metadata in documented system. Metadata is built on graph model. Described approaches improve performance and quality of <b>testing</b> and <b>documenting</b> processes...|$|R
40|$|Sentiment Analysis aims at {{determining}} the overall polarity of a document, for instance, identifying whether a movie review appreciates or criticizes a movie. We present a machine learning based {{approach to this}} problem similar to text categorization. The technique is made more effective by incorporating linguistic knowledge gathered through Wordnet 1 synonymy graphs. A method to improve the accuracy of classification over a set of <b>test</b> <b>documents</b> is finally given. 1...|$|E
40|$|We {{participated in}} the triage task of {{biomedical}} documents in the TREC genomic track. In this paper we describe the methods we developed for the four triage 1 subtasks. Logistic regression and support vector machine algorithms were first trained to generate ranked lists of <b>test</b> <b>documents.</b> Then {{a subset of the}} <b>test</b> <b>documents</b> was identified as positive instances by selecting the top-k documents of the ranked lists. Deciding on the ideal value for k requires a good thresholding strategy. In this paper we first describe two thresholding strategies based on i) logistic regression and ii) support vector machines. In addition to these methods, we describe a thresholding method that combines the outputs from logistic regression and support vector machine by applying a joint thresholding strategy. 1. Task Description The goal of Mouse Genome Informatics project [2] is to provide structured, coded annotation of different topics from biological literature. Human curators spend a large amount of effort on documents of specific topics to generate annotated information. To reduce the amount of effort put in by human curators, the triage process can be utilized t...|$|E
40|$|A {{method of}} {{automatic}} document classification is optimized to give best agreement with manually assigned classification {{of a set}} of <b>test</b> <b>documents.</b> The effectiveness of the automatic classification depends on the correlation between the relevance and the mutual keyword content of pairs of documents. An expression is obtained to measure the indexing worth of additional keywords. The resolution of the automatic classification is dependent on the correlation between mutual keyword content and the uniqueness of the association of categories with documents...|$|E
40|$|In {{this note}} we will recall the main <b>tests</b> that <b>document</b> {{robustness}} and {{flexibility of the}} PHOTOS design. This aspect may be of broader use and may find extensions in future applications, also outside the simple case of purely QED bremsstrahlung in decays. The components of the PHOTOS algorithm will be presented using operator language. The results of some flexibility <b>tests,</b> not <b>documented</b> before, will also be presented...|$|R
5000|$|Selecting 15 rain gardens for {{infiltration}} {{testing and}} three of those for additional synthetic drawdown <b>testing</b> and <b>documented</b> the results.|$|R
40|$|This paper {{presents}} the authors recommended practices for spreadsheet <b>testing.</b> <b>Documented</b> spreadsheet error rates are unacceptable in corporations today. Although improvements are needed throughout the systems development life cycle, credible improvement programs must include comprehensive testing. Several forms of testing are possible, but logic inspection {{is recommended for}} module testing. Logic inspection appears to be feasible for spreadsheet developers to do, and logic inspection appears {{to be safe and}} effective. Comment: 12 Pages, Extensive Reference...|$|R
