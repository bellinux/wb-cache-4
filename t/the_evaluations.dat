4601|10000|Public
5|$|Both {{boats were}} {{commissioned}} into the Austro-Hungarian Navy in 1909, with U-4 commissioned in August and U-3 in September. During <b>the</b> <b>evaluations</b> {{conducted by the}} Navy, the U-3 design bested the U-1 (Lake) and U-5 (Holland) classes in reliability and provided the best living conditions. They did, however, have the worst diving abilities of the three designs, and produced excessive exhaust smoke. To remedy the diving problems of the U-3-class, the fins were changed in size and shape several times. Eventually the front diving planes were removed and a stationary stern flap was affixed to the hull.|$|E
5|$|During {{the course}} of the evaluation, the Tri-partite pilots {{developed}} a typical sortie routine for the Kestrel of conducting short take-offs (STO) and returning to base on vertical landings. This manner of operation (STOVL) was judged to be the optimal practice. Operating from rough airstrips was also trialled at nearby RAF Bircham Newton, where the aircraft proved adept at traversing boggy ground and taking off from a variety of temporary ground coverings. During testing one aircraft was lost when a pilot tried to do a rolling take-off with the parking brake on; <b>the</b> <b>evaluations</b> were finalised in November 1965.|$|E
5|$|The {{same basic}} {{procedure}} {{was used by}} all of the above-mentioned psychologists. A participant makes a judgment about some particular issue. At a later time, they judge the same problem again but with information of how certain groups or prestigious people have evaluated the same problem. If the subject changes his judgment {{in the same direction as}} <b>the</b> <b>evaluations</b> of these groups of people or prestigious people, then this is considered a degree of influence that they have exerted on the participant's judgment.|$|E
40|$|<b>The</b> <b>evaluation</b> {{was carried}} out by Particip GmbH within a {{consortium}} led by DRN. <b>The</b> external <b>evaluation</b> team was composed of Landis MacKellar (team leader), Jörn Dosch, Steven Ney, Mely Caballero Anthony, René Madrid, Christopher Veit, Marcel Goeke. Particip GmbH was <b>the</b> <b>evaluation</b> contract manager. <b>The</b> <b>evaluation</b> was managed by <b>the</b> <b>evaluation</b> unit who also chaired the reference group composed of members of EC services (EuropeAid, DG Dev, DG Relex, DG Trade) and the EC Delegation in Jakarta. Full reports of <b>the</b> <b>evaluation</b> can be obtained from <b>the</b> <b>evaluation</b> unit website...|$|R
40|$|In this article, we {{established}} <b>the</b> <b>evaluation</b> index system of manufacturing quality competence, implemented <b>the</b> comprehensive <b>evaluation</b> of quality competence by AHP (Analytic Hierarchy Process), empirically studied <b>the</b> <b>evaluation</b> objects {{including more than}} one thousand manufacturing enterprises, and validated the rationality of <b>the</b> <b>evaluation</b> index system and <b>the</b> <b>evaluation</b> method. <br /...|$|R
30|$|Y, <b>the</b> <b>evaluation</b> {{procedure}} of <b>the</b> model are adequately described; P, <b>the</b> <b>evaluation</b> procedure was described inadequately; N, <b>the</b> <b>evaluation</b> {{procedure of}} <b>the</b> model was not described at all.|$|R
5|$|The next Divine film, Lust in the Dust (1985), reunited {{him with}} Tab Hunter and was Divine's first film not {{directed}} by John Waters. Set in the Wild West {{during the nineteenth}} century, the movie was a sex comedy that starred Divine as Rosie Velez, a promiscuous woman {{who works as a}} singer in saloons and competes for the love of Abel Wood (Tab Hunter) against another woman. A parody of the 1946 western Duel in the Sun, the film was a moderate critical success, with Divine receiving praise from a number of reviewers. Divine followed this production with a very different role, that of gay male gangster Hilly Blue in Trouble in Mind (1985). The script was written with Divine in mind. Although not a major character in the film, Divine had been eager to play the part because he wished to perform in more male roles and leave behind the stereotype of simply being a female impersonator. Reviews of the film were mixed, as were <b>the</b> <b>evaluations</b> of Divine's performance.|$|E
25|$|The San Francisco Wine Tasting of 1978 was {{conducted}} 20 {{months after the}} Paris Wine Tasting of 1976. Steven Spurrier flew in from Paris to participate in <b>the</b> <b>evaluations,</b> which were held at the Vintners Club.|$|E
25|$|These {{tests can}} be used to {{constrain}} deviations between the maximal attainable speed of matter and the speed of light, in particular with respect to the parameters of cμν that are also used in <b>the</b> <b>evaluations</b> of the threshold effects mentioned above.|$|E
30|$|Y, <b>the</b> <b>evaluation</b> {{process is}} fully {{supported}} by a tool; P, <b>the</b> <b>evaluation</b> process is partially {{supported by a}} tool; N no tool support is provided for <b>the</b> <b>evaluation</b> process.|$|R
40|$|In Part 1 {{we looked}} at <b>the</b> {{definition}} of <b>evaluation,</b> its purpose, trends and core ingredients, and at {{the context in which}} it occurs – the project cycle. We also looked briefly at the closely related processes of monitoring and impact assessment. Now, in Part 2, we move on to <b>the</b> <b>evaluation</b> process itself. This process can be grouped into four phases: z preparing <b>the</b> <b>evaluation</b> terms of reference z designing <b>the</b> <b>evaluation</b> z implementing <b>the</b> <b>evaluation</b> z following up <b>the</b> <b>evaluation</b> What are <b>the</b> main issues {{you have to think about}} when organising <b>the</b> <b>evaluation</b> of an information project, and how do you reflect them in your terms of reference? How do these issues affect the way you design <b>the</b> <b>evaluation?</b> How do you select <b>the</b> <b>evaluation</b> tools? Do <b>the</b> tools you select match the objectives set and the resources available? What does <b>evaluation</b> follow-up involve? <b>The</b> process described here is intended mainly for self-evaluation, involving you and key stakeholders in the whole process. As we noted in Part 1, self-evaluation enhances the learning experience and makes it easier to apply the lessons learned to a project, with the overall aim of increasing its impact. We look both at <b>the</b> <b>evaluation</b> process in general and at how it applies, specifically, to information projects, products and services. The four phases in <b>the</b> <b>evaluation</b> process can be subdivided into a clear set of stages: Phase 1 : Preparing <b>the</b> <b>evaluation</b> terms of reference z Define the reasons for and purpose of <b>the</b> <b>evaluation</b> z Define <b>the</b> scope of <b>the</b> <b>evaluation</b> z Organise stakeholder participation z Identify existing data and sources of data z Choose a methodology for data collection and analysis z Formulate a communication strategy z Select <b>the</b> <b>evaluation</b> team z Prepare the work plan and budget z Formulate the terms of reference Phase 2 : Designing <b>the</b> <b>evaluation</b> z Review <b>the</b> project concept and objectives z Determine the data needed to evaluate the project z Determine <b>the</b> <b>evaluation</b> focus, key questions and indicators z Design the data collection methods z Design the data analysis methods z Design the communication plan z Integrate group dynamics issues Phase 3 : Implementing <b>the</b> <b>evaluation</b> z Collect <b>the</b> data z Analyse the data z Review and report the findingsIn Part 2, we move on to <b>the</b> <b>evaluation</b> process itself. This process can be grouped into four phases: preparing <b>the</b> <b>evaluation</b> terms of reference, designing <b>the</b> <b>evaluation,</b> implementing <b>the</b> <b>evaluation,</b> following up <b>the</b> evaluatio...|$|R
3000|$|... {{the third}} stage {{concerns}} the structuring of <b>the</b> <b>evaluation</b> data, input to the MULTIPOL method. In this context, three impact matrixes are created, which contain information relating {{to the impact of}} [12]: a) scenarios with respect to <b>the</b> <b>evaluation</b> criteria; b) policies with respect to <b>the</b> <b>evaluation</b> criteria; and c) policy measures with respect to <b>the</b> <b>evaluation</b> criteria; [...]...|$|R
25|$|SemEval (Semantic Evaluation) is {{an ongoing}} series of {{evaluations}} of computational semantic analysis systems; it evolved from the Senseval word sense evaluation series. <b>The</b> <b>evaluations</b> are intended to explore the nature of meaning in language. While meaning is intuitive to humans, transferring those intuitions to computational analysis has proved elusive.|$|E
25|$|There was {{a report}} that scores on Advanced Placement exams could be helpful in <b>the</b> <b>evaluations</b> process. One report {{suggested}} there was a limit {{on the number of}} AP tests that should be taken, such that taking 12 AP tests was not as helpful as taking five and doing well on those five.|$|E
25|$|The U-3 class {{consisted}} of two U-boats, U-3 and U-4, and was {{also a part of}} the Austro-Hungarian Navy's efforts to competitively evaluate three foreign submarine designs. The U-3-class boats were designed and built by Germaniawerft of Kiel, Germany. During <b>the</b> <b>evaluations</b> conducted by the Navy, the U-3 design bested the U-1 (Lake) and U-5 (Holland) classes in reliability and provided the best living conditions. They did, however, have the worst diving abilities of the three designs, and produced excessive exhaust smoke.|$|E
40|$|Abstract: To {{guarantee}} the run {{effectiveness of the}} quality management system(QMS), a comprehensive <b>evaluation</b> system of <b>the</b> QMS’s effectiveness is brought forward. In <b>the</b> <b>evaluation</b> system, first <b>the</b> <b>evaluation</b> flow chart is set up; Second <b>the</b> <b>evaluation</b> index is given out; Third <b>the</b> <b>evaluation</b> result is given out by the calculate of <b>the</b> grey integration <b>evaluation</b> arithmetic. <b>The</b> organization can find out why {{the effectiveness of the}} QMS is weak and establish improvement measure from <b>the</b> comprehensive <b>evaluation</b> result...|$|R
40|$|<b>The</b> <b>evaluation</b> {{was carried}} out by Particip GmbH within a {{consortium}} led by DRN. <b>The</b> external <b>evaluation</b> team was composed of Max Hennion (team leader), Jörn Dosch, Steven Ney, Florence Burban, René Madrid, Christopher Veit, Marcel Goeke. Particip GmbH was <b>the</b> <b>evaluation</b> contract manager. <b>The</b> <b>evaluation</b> was managed by <b>the</b> joint <b>evaluation</b> unit who also chaired the reference group composed by members o...|$|R
30|$|We {{believe that}} <b>the</b> {{proposed}} <b>evaluation</b> {{system has been}} proven effective that we can understand through <b>the</b> obtained <b>evaluation</b> results and <b>the</b> ability of the results to satisfy the desired interaction characteristics, system behavior and performance. However, to make <b>the</b> <b>evaluation</b> system more effective, the following measures can be taken: (1) add more objective criteria as <b>the</b> <b>evaluation</b> criteria, (2) conduct a short survey with workers/engineers/researchers working with power-assist robots in industries to understand more useful and practical evaluation criteria, (3) conduct another survey based on studies to compare <b>the</b> <b>evaluation</b> scheme proposed in this article with <b>the</b> state-of-the-art <b>evaluation</b> approaches, (4) provide more trainings to the subjects about <b>the</b> <b>evaluation</b> criteria and methods to bring uniformity in <b>the</b> <b>evaluation,</b> etc.|$|R
25|$|Modern {{variants}} of Michelson-Morley and Kennedy–Thorndike experiments {{have been conducted}} {{in order to test}} the isotropy of the speed of light. Contrary to Michelson-Morley, the Kennedy-Thorndike experiments employ different arm lengths, and <b>the</b> <b>evaluations</b> last several months. In that way, the influence of different velocities during Earth's orbit around the sun can be observed. Laser, maser and optical resonators are used, reducing the possibility of any anisotropy of the speed of light to the 10−17 level. In addition to terrestrial tests, Lunar Laser Ranging Experiments have also been conducted as a variation of the Kennedy-Thorndike-experiment.|$|E
25|$|This {{series of}} {{evaluations}} is providing {{a mechanism to}} characterize in more precise terms exactly {{what is necessary to}} compute in meaning. As such, <b>the</b> <b>evaluations</b> provide an emergent mechanism to identify the problems and solutions for computations with meaning. These exercises have evolved to articulate more of the dimensions that are involved in our use of language. They began with apparently simple attempts to identify word senses computationally. They have evolved to investigate the interrelationships among the elements in a sentence (e.g., semantic role labeling), relations between sentences (e.g., coreference), and the nature of what we are saying (semantic relations and sentiment analysis).|$|E
25|$|Up {{to three}} cost-plus {{contracts}} {{were to be}} awarded nine months after the RfP was released. An acquisition decision memorandum on 17 August allowed the program to award technology development contracts. It also initiated two reviews of alternatives including a revised analysis of alternatives and an analysis of non-developmental vehicles. The 18 August, the Army awarded technology development contracts to only BAE and GDLS. BAE was awarded $450 million while GDLS was awarded $440 million. SAIC followed up with a bid protest on 26 August further delaying GCV development. It believed <b>the</b> <b>evaluations</b> process was flawed and the evaluation took factors into consideration that were not stated in the request for proposal.|$|E
40|$|The thesis {{deals with}} a most topical issue which is <b>the</b> staff <b>evaluation</b> {{relating}} to <b>the</b> remuneration. The {{purpose of the}} thesis is to propose a staff <b>evaluation</b> system. <b>The</b> first part gives a definition of <b>the</b> <b>evaluation,</b> a detailed description of the meaning and the goals of <b>the</b> <b>evaluation,</b> <b>the</b> best-known methods of <b>the</b> <b>evaluation</b> and component steps leading {{to the implementation of}} <b>the</b> <b>evaluation</b> system. <b>The</b> research represents a proposed evaluation system which was introduced to a company. The proposal was gradually modified according to the requirements of the owners and managers. Some reactions and pieces of knowledge relating to <b>the</b> <b>evaluation</b> system were found out during the process itself. <b>The</b> <b>evaluation</b> system has been completely worked out and is prepared to be implemented. The thesis is a kind of qualitative research. The design is represented by an action research...|$|R
3000|$|A further {{outstanding}} {{point is}} <b>the</b> <b>evaluation</b> of <b>the</b> matrices by the scenario team. Everyone who takes part in <b>the</b> <b>evaluation</b> process holds another position or perspective to the respective topic. This often causes <b>the</b> <b>evaluation</b> to be subjective. At this point, {{the following questions}} arise: “How can the subjective character be invalidated?” and “How can the uncertainty in <b>the</b> <b>evaluation</b> process be handled?” [...]...|$|R
40|$|Part of the Australian National Continence Management Strategy {{involved}} <b>the</b> <b>evaluation</b> {{of three}} proposed models for continence care, one each in Western Australia, Victoria and New South Wales. A {{team from the}} University of Newcastle, NSW, carried out <b>the</b> <b>evaluation.</b> Within <b>the</b> overall <b>evaluation</b> of <b>the</b> three models of continence care, was <b>the</b> <b>evaluation</b> of <b>the</b> print materials used {{within each of the}} models. It is this aspect of <b>the</b> <b>evaluation</b> that is discussed in this paper...|$|R
25|$|Benjamin Graham is {{regarded}} {{by many to}} be the father of value investing. Along with David Dodd, he wrote Security Analysis, first published in 1934. The most lasting contribution of this book to the field of security analysis was to emphasize the quantifiable aspects of security analysis (such as <b>the</b> <b>evaluations</b> of earnings and book value) while minimizing the importance of more qualitative factors such as the quality of a company's management. Graham later wrote The Intelligent Investor, a book that brought value investing to individual investors. Aside from Buffett, many of Graham's other students, such as William J. Ruane, Irving Kahn, Walter Schloss, and Charles Brandes went on to become successful investors in their own right.|$|E
25|$|Rank Leaders are {{determined}} in phases. Generally each {{year during the}} final stages of the winter semester, band members are invited to nominate people in their own section to be considered for a Rank Leader Candidate spot. Rank Leader Candidates are then selected by the staff and announced during the Spring Meeting or sometimes later. Rank Leader Candidates then participate in a retreat for incoming members, color guard and percussion directly before Band Week begins. Rank Leaders teach the incoming members marching techniques, then have two days of review with all other returning members and the incoming members. Band members are then given the chance to fill out Rank Leader evaluations. The staff then narrows down the potential candidates to the actual Rank Leaders for the year, taking <b>the</b> <b>evaluations</b> into account.|$|E
500|$|<b>The</b> <b>evaluations</b> {{proceeded}} successfully {{through the}} flight envelope without major changes required {{to the basic}} design. The only incident of note was a fire that {{broke out in the}} hangar where the PL.8-01 had been stored. Scorched fabric on the top wing was the result with effective repairs carried out shortly after. On 7 May 1927, after the tests were complete, [...] the aircraft was prepared for its record flight, flying from Villacoublay to Le Bourget Field.|$|E
40|$|This article {{introduces}} <b>the</b> <b>Evaluation</b> Framework EFI for the Impact Measurement of learning, {{education and}} training: <b>The</b> <b>Evaluation</b> Framework for Impact Measurement {{was developed for}} specifying <b>the</b> <b>evaluation</b> phase and its objectives and tasks within the IDEAL Reference Model for the introduction and optimization of quality development within learning, education and training. First, a description of <b>the</b> <b>Evaluation</b> Framework for Impact Measurement will be provided, followed by {{a brief overview of}} the IDEAL Reference Model. Finally, an example for the implementation of <b>the</b> <b>Evaluation</b> Framework for Impact Measurement within the ARISTOTELE project is presented...|$|R
40|$|In {{this paper}} {{we take a}} {{critical}} look at <b>the</b> <b>evaluation</b> method of WebCLEF 2007. The suitability of <b>the</b> <b>evaluation</b> method {{can be seen from}} two sides, namely from a participating system and a non participating system. A participant has the advantage that <b>the</b> <b>evaluation</b> is partly based upon his output. In this paper we will investigate if the size of the pool of snippets, the implementation of <b>the</b> <b>evaluation</b> method and <b>the</b> quality of the assessments is sufficient enough for reliable evaluation. Unfortunately we have to conclude that <b>the</b> <b>evaluation</b> is not suitable. Therefore some alternative evaluation methods will be discussed concluding in a recommendation to improve <b>the</b> <b>evaluation</b> of WebCLEF...|$|R
40|$|The INEX 2010 Focused Relevance Feedback track {{offered a}} refined {{approach}} to <b>the</b> <b>evaluation</b> of Focused Relevance Feedback algorithms through simulated exhaustive user feedback. As in traditional approaches we simulated a user-in-the loop by re-using the assessments of ad-hoc retrieval obtained from real users who assess focused ad-hoc retrieval submissions. <b>The</b> <b>evaluation</b> was extended in several ways: {{the use of}} exhaustive relevance feedback over entire runs; <b>the</b> <b>evaluation</b> of focused retrieval where both the retrieval results and the feedback are focused; <b>the</b> <b>evaluation</b> was performed over a closed set of documents and complete focused assessments; <b>the</b> <b>evaluation</b> was performed over executable implementations of relevance feedback algorithms; and �finally, <b>the</b> entire <b>evaluation</b> platform is reusable. We present <b>the</b> <b>evaluation</b> methodology, its implementation, and experimental results obtained for nine submissions from three participating organisations...|$|R
500|$|Asch, however, reinterprets Lorge's {{findings}} {{and suggests that}} there was [...] "a change in the object of judgment, {{rather than in the}} judgment of the object" [...] (Asch, 1940). He suggests that a person will redefine the object of judgment based on the content of <b>the</b> <b>evaluations.</b> Therefore, the person will base the meaning of the quote in the context of what he/she believes to be true about the person who said the quote, resulting in different meanings of the statements based on the author.|$|E
500|$|The AMX-30 bridgelayer, or Poseur de pont, {{consists}} of the AMX-30's chassis with a box-like superstructure, supporting a scissor-type folding bridge. The [...] bridge can span [...] gaps. The bridge has a width of , but can be increased to [...] {{through the use of}} appliqué panels. It can support weighs of up to [...] Bridgelayer development began as early as 1963, although it was not until June 1967 that development began on a prototype. Although a prototype designated AMX-30H was finished in 1968, it was not until 1971 that the vehicle was evaluated. At the end of <b>the</b> <b>evaluations</b> in September 1971, a pre-series of five vehicles was ordered, resulting in a new period of evaluations beginning on 16 October 1972. In 1975, the AMX-30H was declared standard in the French Army, although none of these vehicles were ever ordered.|$|E
500|$|Evra {{began his}} {{football}} career playing for hometown club CO Les Ulis. After {{playing in the}} streets for years, he {{was brought to the}} club by friend Tshymen Buhanga, who informed the club coach, [...] "I bring you the new Romário." [...] Evra spent one year at the club under the watch of coach Jean-Claude Giordanella, who later became vice-president of the club. Giordanella described the player as [...] "more quiet, almost shy. He was a good kid". Evra originally played football in the striker position and, while training at Les Ulis, underwent trials with professional clubs Rennes and Lens. Following the conclusion of <b>the</b> <b>evaluations,</b> Evra was rejected primarily due to his size. In 1993, he joined amateur club CSF Brétigny based in nearby Brétigny-sur-Orge. Similar to his stint with Les Ulis, Evra went on trials with several clubs, most notably Toulouse and Paris Saint-Germain. He was ultimately signed by the latter and converted into a winger. Evra trained at the Camp des Loges for a few months, but was later released.|$|E
40|$|This diploma work {{completely}} {{deals with}} problems of <b>the</b> <b>evaluation</b> by a teacher. In the introductory part there are described specifics of <b>the</b> school <b>evaluation,</b> its importance and sense. There is also mentioned {{a survey of}} many types, functions of <b>the</b> <b>evaluation</b> and methods, which are mostly being used for <b>the</b> pupils <b>evaluation</b> at school. It also discusses an influence of <b>the</b> <b>evaluation</b> on <b>the</b> pupil. It summarizes principles and processes of <b>the</b> pupils <b>evaluation,</b> which are established and obligatory for schools in the Czech Republic. It analyzes the problems of basic forms of <b>the</b> <b>evaluation,</b> which are mostly being used at our schools; those are <b>the</b> word <b>evaluation</b> and <b>the</b> classification by marks; and it also describes the portfolio that is a new alternative form of <b>the</b> <b>evaluation.</b> It concentrates on important aspects of pupil evaluations, principles, those are important to know {{and they must be}} observed. It devotes to an importance of positive and negative evaluations and also to a smart question of <b>the</b> <b>evaluation</b> objectivity. It refers to mistakes which are important to avoid. It points out the importance of individual approach to pupils, especially to pupils with specific learning difficulties. It analyzes <b>the</b> <b>evaluation</b> problems of <b>the</b> pupils with learning disability and it refers to the mistakes that are necessary to remember in <b>the</b> <b>evaluation</b> of pupils with specific learning difficulties...|$|R
5000|$|An outside {{consultant}} hired by PWGSC advised <b>the</b> <b>evaluation</b> team to destroy all records related to <b>the</b> <b>evaluation.</b>|$|R
40|$|D 5. 2 : Evaluation definition: {{method and}} metrics This {{deliverable}} presents how evaluation {{will be done}} inside the Cooper project. The document presents an evaluation overview and an <b>evaluation</b> methodology. <b>The</b> <b>evaluation</b> in Cooper is performed at the conceptual, technical, and impact levels and this document analyzes how <b>the</b> <b>evaluation</b> will be done from all these points of view. <b>The</b> <b>evaluation</b> considers <b>the</b> aspects described in D 5. 1 “Scenario and Requirements Analysis ” {{and it will be}} the basis for the T 5. 5 which is “Perform <b>the</b> <b>evaluation,</b> analyse <b>the</b> results and elaborate <b>the</b> <b>evaluation</b> report...|$|R
