136|78|Public
2500|$|On CPUs without Second Level Address Translation, {{installation}} of most WDDM accelerated graphics drivers on the primary OS {{will cause a}} dramatic drop in graphic performance. [...] This occurs because the graphics drivers access memory in a pattern that causes the <b>Translation</b> <b>lookaside</b> <b>buffer</b> to be flushed frequently.|$|E
2500|$|Most modern systems divide memory into {{pages that}} are [...] in size, {{often with the}} {{capability}} to use huge pages from [...] to [...] in size. [...] Page translations are cached in a <b>translation</b> <b>lookaside</b> <b>buffer</b> (TLB). [...] Some systems, mainly older RISC designs, trap into the OS when a page translation is {{not found in the}} TLB. [...] Most systems use a hardware-based tree walker. [...] Most systems allow the MMU to be disabled, but some disable the MMU when trapping into OS code.|$|E
2500|$|The Core 2 memory {{management}} unit (MMU) in X6800, E6000 and E4000 processors does not operate to previous specifications [...] implemented in previous generations of x86 hardware. This may cause problems, many of them serious security and stability issues, with existing operating system software. Intel's documentation states that their programming manuals will be updated [...] "in the coming months" [...] with information on recommended methods of managing the <b>translation</b> <b>lookaside</b> <b>buffer</b> (TLB) for Core 2 to avoid issues, and admits that, [...] "in rare instances, improper TLB invalidation may result in unpredictable system behavior, such as hangs or incorrect data." ...|$|E
5000|$|All <b>translation</b> <b>lookaside</b> <b>buffers</b> (TLBs) are 4-way associative.|$|R
5000|$|CPU fully {{associative}} cache controllers and <b>translation</b> <b>lookaside</b> <b>buffers</b> ...|$|R
5000|$|Larger L1 <b>translation</b> <b>lookaside</b> <b>buffers</b> (TLB) and L2 {{efficiency}} improvements ...|$|R
2500|$|Most MMUs use an in-memory {{table of}} items called a [...] "page table", {{containing}} one [...] "page table entry" [...] (PTE) per page, to map virtual page numbers to physical page numbers in main memory. [...] An associative cache of PTEs {{is called a}} <b>translation</b> <b>lookaside</b> <b>buffer</b> (TLB) and is used to avoid the necessity of accessing the main memory every time a virtual address is mapped. [...] Other MMUs may have a private array of memory or registers that hold a set of page table entries. [...] The physical page number is combined with the page offset to give the complete physical address.|$|E
5000|$|Cache space, {{including}} CPU cache and MMU cache (<b>translation</b> <b>lookaside</b> <b>buffer)</b> ...|$|E
50|$|In a Harvard {{architecture}} {{or modified}} Harvard architecture, a separate virtual address space or memory access hardware may exist for instructions and data. This {{can lead to}} distinct TLBs for each access type, an Instruction <b>Translation</b> <b>Lookaside</b> <b>Buffer</b> (ITLB) and a Data <b>Translation</b> <b>Lookaside</b> <b>Buffer</b> (DTLB). Various benefits have been demonstrated with separate data and instruction TLBs.|$|E
5000|$|... flushes {{of memory}} {{management}} unit caches, such as <b>translation</b> <b>lookaside</b> <b>buffers,</b> on other processors when memory mappings are changed by one processor; ...|$|R
50|$|The R8000 {{controlled}} the chip set and executed integer instructions. It contained the integer execution units, integer register file, primary caches and hardware for instruction fetch, branch prediction the <b>translation</b> <b>lookaside</b> <b>buffers</b> (TLBs).|$|R
40|$|Certain {{architectural}} features either constrain or inhibit compiler optimizations. We suggest three hardware changes {{aimed to}} improve the situation, from a compiler's perspective. These changes involve redesigns of <b>translation</b> <b>lookaside</b> <b>buffers,</b> communication in memory hierarchies, and page mapping hardware for caches...|$|R
5000|$|... #Caption: Flowchart [...] {{shows the}} working of a <b>Translation</b> <b>Lookaside</b> <b>Buffer.</b> For simplicity, the page fault routine is not mentioned.|$|E
50|$|<b>Translation</b> <b>lookaside</b> <b>buffer</b> (TLB) entries {{and page}} table entries in PA-RISC 1.1 and PA-RISC 2.0 support read-only, read/write, read/execute, and read/write/execute pages.|$|E
5000|$|... 48-entry fully {{associative}} L1 instruction <b>translation</b> <b>lookaside</b> <b>buffer</b> (TLB) {{with native}} support for 4 KiB, 64 KiB, and 1 MB page sizes ...|$|E
40|$|Some {{architecture}} definitions (e. g. Alpha) {{allows the}} use of multiple virtual page sizes even for a single process. Unfortunately, on current set-associative TLBs (<b>Translation</b> <b>Lookaside</b> <b>Buffers),</b> pages with different sizes can not coexist together. Thus, processors supporting multiple page sizes implement fullyassociative TLBs...|$|R
50|$|Multiple threads can {{interfere}} with each other when sharing hardware resources such as caches or <b>translation</b> <b>lookaside</b> <b>buffers</b> (TLBs). As a result, execution times of a single thread are not improved but can be degraded, even when only one thread is executing, due to lower frequencies or additional pipeline stages {{that are necessary to}} accommodate thread-switching hardware.|$|R
5000|$|In a {{separate}} cache structure, instructions and data are cached separately, meaning that a cache line {{is used to}} cache either instructions or data, but not both; various benefits have been demonstrated with separate data and instruction <b>translation</b> <b>lookaside</b> <b>buffers.</b> [...] In a unified structure, this constraint is not present, and cache lines {{can be used to}} cache both instructions and data.|$|R
5000|$|Virtual memory - Traditional memory {{virtualization}} on {{a single}} computer, typically using the <b>translation</b> <b>lookaside</b> <b>buffer</b> (TLB) to translate between virtual and physical memory addresses ...|$|E
50|$|The CPU's memory {{management}} unit (MMU) stores a cache of recently used mappings from the operating system's page table. This is called the <b>translation</b> <b>lookaside</b> <b>buffer</b> (TLB), which is an associative cache.|$|E
5000|$|Richard Rashid et al. {{described}} a lazy <b>translation</b> <b>lookaside</b> <b>buffer</b> (TLB) implementation that deferred reclaiming virtual-address space until all CPUs flushed their TLB, {{which is similar}} in spirit to some RCU implementations.|$|E
50|$|The I-box {{contains}} two <b>translation</b> <b>lookaside</b> <b>buffers</b> (TLBs) for translating virtual addresses to physical addresses. These TLBs {{are referred to}} as instruction translation buffers (ITBs). The ITBs cache recently used page table entries for the instruction stream. An eight-entry ITB is used for 8 KB pages and a four-entry ITB for 4 MB pages. Both ITBs are fully associative and use a not-last used replacement algorithm.|$|R
40|$|Lowering {{active power}} {{dissipation}} is increasingly important for battery powered embedded microprocessors. Here, power reduction techniques applicable to fully associative <b>translation</b> <b>lookaside</b> <b>buffers,</b> {{as well as}} other associative structures and dynamic register files, are described. Powermill simulations of implementation in a microprocessor on 0. 18 µm process technology demonstrate 42 % power savings. Circuit implementations, as well as architectural simulations demonstrating applicability to typical instruction mixes are shown...|$|R
40|$|Abstract—Efficient {{virtualization}} of <b>translation</b> <b>lookaside</b> <b>buffers</b> (TLBs), a {{core component}} of modern hypervisors, {{is complicated by}} the concurrent, speculative walking of page tables in hardware. We give a formal model of an x 64 -like TLB, criteria for its correct virtualization, and outline the verification of a virtualization algorithm using shadow page tables. The verification is being carried out in VCC, a verifier for concurrent C code. I...|$|R
50|$|Memory-level {{parallelism}} (MLP) {{is a term}} {{in computer}} architecture referring {{to the ability to}} have pending multiple memory operations, in particular cache misses or <b>translation</b> <b>lookaside</b> <b>buffer</b> (TLB) misses, at the same time.|$|E
50|$|The R4200 has a 32-entry <b>translation</b> <b>lookaside</b> <b>buffer</b> (TLB) for data, and a 4-entry TLB for instructions. A 33-bit {{physical}} address is supported. The system bus is 64 bits wide and operates at half the internal clock frequency.|$|E
50|$|A memory {{management}} unit (MMU) that fetches page table entries from main memory has a specialized cache, used for recording the results of virtual address to physical address translations. This specialized cache is called a <b>translation</b> <b>lookaside</b> <b>buffer</b> (TLB).|$|E
50|$|<b>Translation</b> <b>Lookaside</b> <b>Buffers</b> (TLBs) {{have also}} been {{enlarged}} (40 4k/2M/4M entries in L1 cache, 512 4k entries), with reduced latencies and improved branch prediction, with four {{times the number of}} bimodal counters in the global history counter. This and other architectural enhancements, especially as regards SSE implementation, improve instruction per cycle (IPC) performance over the previous Athlon XP generation. To make this easier for consumers to understand, AMD has chosen to market the Athlon 64 using a PR (Performance Rating) system, where the numbers roughly map to Pentium 4 performance equivalents, rather than actual clock speed.|$|R
40|$|AbstractÐWe {{present a}} {{feasibility}} study for performing virtual address translation without specialized translation hardware. Removing address translation hardware and instead managing address translation in software {{has the potential}} to make the processor design simpler, smaller, and more energy-efficient at little or no cost in performance. The {{purpose of this study is}} to describe the design and quantify its performance impact. Trace-driven simulations show that software-managed address translation is just as efficient as hardware-managed address translation. Moreover, mechanisms to support such features as shared memory, superpages, fine-grained protection, and sparse address spaces can be defined completely in software, allowing for more flexibility than in hardware-defined mechanisms. Index TermsÐVirtual memory, virtual address translation, virtual caches, memory management, software-managed address <b>translation,</b> <b>translation</b> <b>lookaside</b> <b>buffers.</b> æ...|$|R
50|$|Unlike {{many other}} micro{{processors}}, the Clipper processors were actually sets of several distinct chips. The C100 and C300 consist of three chips: one {{central processing unit}} containing both an integer unit and a floating point unit, and two cache and memory management units (CAMMUs), one responsible for data and one for instructions. The CAMMUs contained caches, <b>translation</b> <b>lookaside</b> <b>buffers,</b> and support for memory protection and virtual memory. The C400 consists of four basic units: an integer CPU, an FPU, an MMU, and a cache unit. The initial version used one chip each for the CPU and FPU and discrete elements for the MMU and cache unit, but in later versions the MMU and cache unit were combined into one CAMMU chip.|$|R
50|$|On CPUs without Second Level Address Translation, {{installation}} of most WDDM accelerated graphics drivers on the primary OS {{will cause a}} dramatic drop in graphic performance. This occurs because the graphics drivers access memory in a pattern that causes the <b>Translation</b> <b>lookaside</b> <b>buffer</b> to be flushed frequently.|$|E
5000|$|The kernel allocates {{physical}} memory {{pages to}} programs and controls the <b>translation</b> <b>lookaside</b> <b>buffer.</b> A program can share a page with another program by sending it a capability to access that page. The kernel ensures that programs access only pages {{for which they}} have a capability.|$|E
50|$|Systems such as Windows NT and OS/2 {{are said}} to have cheap threads and {{expensive}} processes; in other operating systems there is not so great a difference except the cost of an address space switch which on some architectures (notably x86) results in a <b>translation</b> <b>lookaside</b> <b>buffer</b> (TLB) flush.|$|E
40|$|Typical <b>translation</b> <b>lookaside</b> <b>buffers</b> (TLBs) can map a {{far smaller}} region of memory than {{application}} footprints demand, {{and the cost}} of handling TLB misses therefore limits the performance of an increasing number of applications. This bottleneck can be mitigated by the use of superpages, multiple adjacent virtual memory pages that can be mapped with a single TLB entry, that extend TLB reach without significantly increasing size or cost. We analyze hardware/software tradeoffs for dynamically creating superpages. This study extends previous work by using execution-driven simulation to compare creating superpages via copying with remapping pages within the memory controller, and by examining how the tradeoffs change when moving from a single-issue to a superscalar processor model. We find that remapping-based promotion outperforms copyingbased promotion, often significantly. Copying-based promotion is slightly more effective on superscalar processors than on single-issue processors, and t [...] ...|$|R
40|$|Certain {{architectural}} features either constrain or inhibit compiler optimizations. We suggest three hardware changes {{aimed to}} improve the situation, from a compiler's perspective. These changes involve redesigns of <b>translation</b> <b>lookaside</b> <b>buffers,</b> communication in memory hierarchies, and page mapping hardware for caches. Keywords [...] - cache, compiler, optimization, PlayDoh, prefetch, tiling, TLB I. Introduction Programmers and compilers use models to predict the consequences of programming and optimization choices. To {{take advantage of the}} features of its target, a back end optimizer needs to know various details of the architecture such as register count, instruction interaction and latencies, availability of special-purpose instructions and the ability to mask load latency with operations on independent data. A retargetable back end parameterizes optimizations based on this architectural model. Just as an architectural model lets the back end optimizer predict the performance of cert [...] ...|$|R
40|$|The steady {{reduction}} in physical memory costs, increases in memory density, CPU speed, virtual address size, application complexity and working set size are placing {{pressure on the}} overall performance of computer systems. In particular, pressures are increasing on the address translation mechanism as greater process execution time is being consumed by virtual memory management mechanisms. Many commercial processors have introduced multiple page size (superpage) <b>Translation</b> <b>Lookaside</b> <b>Buffers</b> (TLB) to increase TLB reach {{in an effort to}} reduce average translation costs. Invasive operating system support is necessary to extract the benefits of superpage TLBs. TLB subblocking and partial-subblocking attempt to provide a compromise between chip area and operating system support. In this paper we propose two multi-dimensional TLB designs which provide multiple levels of translation to relax the alignment restrictions of the partial-subblock TLB allowing improved sharing of TLB entries, parti [...] ...|$|R
