37|26|Public
30|$|Examples of {{use that}} {{show how the}} design {{actually}} enables using a <b>toolbox</b> <b>approach</b> despite high latencies.|$|E
30|$|The usage cases {{addressed}} by this work are those where users combine different commands to administrate and/or use distributed resources, i.e., when the <b>toolbox</b> <b>approach</b> {{would be used}} but is not used because of high network latencies. We admit that not all users rely on the UNIX <b>toolbox</b> <b>approach.</b> In fact, just system and application administrators and, so called, power-users {{are the ones who}} prefer to follow it. However, it is desirable to be able to retain such approach (which has worked well for decades) for the distributed environments of today.|$|E
30|$|The problem {{addressed}} by {{this work is}} that it is not reasonable to require multiple RPCs to reach file and data servers when latency is high, and that using the UNIX <b>toolbox</b> <b>approach</b> requires many more RPCs than needed.|$|E
40|$|Many {{application}} domains make {{extensive use}} of spatial and temporal information. However, the numerical approaches employed by most software tools have limitations, particularly when information is vague or incomplete. To address this, alternative qualitative spatial and temporal reasoning (QSTR) methods have been developed, yet few applications have made significant use of these techniques. In response to this we are developing a framework that will support the application of QSTR by allowing software developers to create custom qualitative modelling systems. In this paper we compare our framework to more standard <b>toolbox</b> <b>approaches</b> for QSTR application support. We present fundamental principles of qualitative modelling, and demonstrate, using an architectural lighting example, how these principles {{provide a basis for}} creating qualitative modelling systems that incorporate domain knowledge...|$|R
40|$|Abstract. This paper {{describes}} {{a set of}} verification components that open the way to perform on-the-fly software model checking with the Cadp toolbox, originally designed for verifying the functional correctness of Lotos specifications. Two new tools (named C. Open and Annotator) {{have been added to}} the <b>toolbox.</b> The <b>approach</b> taken fits well within the existing architecture of Cadp which doesn’t need to be altered to enable C program verification. ...|$|R
50|$|He {{gets out}} of the car holding a <b>toolbox,</b> and <b>approaches</b> a house. He knocks on the door, and tells the woman who answers that her {{landlord}} sent him out. He tells the woman that he’s working for his landlord, who is his uncle, and they are changing out all of the shower heads at no charge. The woman asks about her broken dishwasher, but the young man can’t offer any help with it. She tells him if he wants to come in, it’s dishwasher first.|$|R
30|$|In {{this paper}} {{we have shown}} how Clive named channels, {{carrying}} structured data streams, can be efficient and effective for use in distributed systems. We have shown how they permit the construction of tools by combining other tools, following the UNIX <b>toolbox</b> <b>approach.</b>|$|E
30|$|We have {{designed}} and implemented a new system, Clive, for distributed computing environments. Its I/O framework {{can be used}} in UNIX (and other systems) to address the problem stated before, which is further described in the next section. That is, it permits using the UNIX <b>toolbox</b> <b>approach</b> even when latencies get high (which is often the case in Cloud computing environments).|$|E
40|$|In this {{document}} we outline {{what is meant}} by a framework for computational semantics and describe three possible approaches: a logical approach, a conceptual approach and a <b>toolbox</b> <b>approach.</b> Various examples of the approaches are given. For the logical approach we describe a way towards providing a unification of various dynamic semantics. For the conceptual approach we summarize the various kinds of conceptual tools described in D 15. For the <b>toolbox</b> <b>approach</b> we provide an informal specification of the implementation which was built in {{the last part of the}} FraCaS project. This deliverable was originally entitled "The Way Forward" in the Technical Annex. The intention was to describe our strategy for building a framework after having reviewed previous relevant work in the preceding deliverable. It was brought to our attention that the original title suggests that the deliverable should contain material relating to the future development of research in computational semantics after the e [...] ...|$|E
40|$|International audienceWeb {{services}} are increasingly used for building enterprise information systems {{according to the}} Service Oriented Architecture (SOA) paradigm. We propose in this paper a tool-equipped methodology allowing the formal modeling and analysis of Web services described in the BPEL language. The discrete-time transition systems modeling the behavior of BPEL descriptions are obtained by an exhaustive simulation based on a formalization of BPEL semantics using the Algebra of Timed Processes (ATP). These models are then analyzed by model checking value-based temporal logic properties using the CADP <b>toolbox.</b> The <b>approach</b> is illustrated with {{the design of a}} Web service for GPS navigation...|$|R
40|$|Abstract—A new feature-based {{technique}} is introduced {{to solve the}} nonlinear Forward Problem (FP) of the Electrical Capacitance Tomography (ECT) with the target application of monitoring the metal-fill profile in Lost Foam Casting (LFC) process. The new technique to solve the FP is based on key features extracted from the metal distributions and the Correction Factor (CF). The CF is predicted by an Artificial Neural Network (ANN) based on key distribution features. The CF adjusts the linear solution of the FP for nonlinear effects. The data for the ANN training was generated through ANSYS finite element analysis and the codes written in MATLAB. The ANN was implemented using MATLAB Neural Network <b>Toolbox.</b> This <b>approach</b> shows promising results. The ANN was able to learn {{the effect of these}} features on the CF with the % RMS error of 2. 21 for training data. For the previously unseen test metal distributions, the average RMS error was 2. 2 %...|$|R
40|$|The {{ultimate}} {{measure of}} aid effectiveness is how aid affects {{the lives of}} poor people in developing countries. The huge literature on aid 2 ̆ 019 s macroeconomic impact has remarkably little to say on this topic, and less still in terms of practical advice to government officials and aid administrators {{on how to improve}} development effectiveness. But there is an expanding <b>toolbox</b> of <b>approaches</b> to impact evaluation at the field level which can answer both questions of whether aid works, and, properly applied, why it works (or not, as the case may be). This paper lays out these approaches, describing some of their uses by official development agencies. I advocate a theory-based approach to impact evaluation design, as this is most likely to yield policy insights. Academics need to engage in these real world issues and debates if their work is to help alleviate the plight of the world 2 ̆ 019 s poor...|$|R
30|$|The X-kernel [25] {{introduced}} a path abstraction to replace processes, focusing on data paths instead of processing. This {{is similar to}} Clive’s approach in that Clive relies on I/O streams to convey data and permit using different message formats. The main difference is that Clive still preserves the process abstraction to adapt the UNIX <b>toolbox</b> <b>approach</b> to modern distributed scenarios and to interoperate well with existing programs.|$|E
30|$|The Clive I/O {{framework}} is an {{implementation of the}} I/O framework discussed in Section  3. As a result, it enables the UNIX <b>toolbox</b> <b>approach</b> in high-latency environments. One of the reasons it does so is its novel interface. In this section we describe the interface as provided by the Go implementation for the framework, before proceeding {{in the rest of}} the paper with examples of use and evaluation results to support the claims made.|$|E
30|$|One of the {{benefits}} of the UNIX approach is scripting, and combining different programs to build new ones. In many cases, administrators and users may combine existing programs to perform their job, without having to write new software. This is called the <b>toolbox</b> <b>approach</b> and it made UNIX very popular. But, due to high latency in wide area networks, this approach becomes hard to use in distributed environments in use today such as those found in most Cloud computing deployments.|$|E
40|$|Bond graph is an apt {{modelling}} tool for any system working across multiple energy domains. Power electronics system modelling {{is usually the}} study of the interplay of energy in the domains of electrical, mechanical, magnetic and thermal. The usefulness of bond graph modelling in power electronic field has been realised by researchers. Consequently {{in the last couple of}} decades, there has been a steadily increasing effort in developing simulation tools for bond graph modelling that are specially suited for power electronic study. For modelling rotating magnetic fields in electromagnetic machine models, a support for vector variables is essential. Unfortunately, all bond graph simulation tools presently provide support only for scalar variables. We propose an approach to provide complex variable and vector support to bond graph such that it will enable modelling of polyphase electromagnetic and spatial vector systems. We also introduced a rotary gyrator element and use it along with the switched junction for developing the complex/vector variable's <b>toolbox.</b> This <b>approach</b> is implemented by developing a complex S-function tool box in Simulink inside a MATLAB environment This choice has been made so as to synthesise the speed of S-function, the user friendliness of Simulink and the popularity of MATLAB...|$|R
40|$|AbstractExtracting clean fetal {{electrocardiogram}} (fECG) signals from non-invasive abdominal ECG recordings for monitoring {{the health of}} the fetus during pregnancy and labor remains a big challenge. The proposed system for facing extraction, processing and morphological feature estimation was implemented in LabVIEW 2013 with preinstalled Biosignal Filtering, Advanced Signal Processing and Digital Filter Design <b>Toolboxes.</b> The present <b>approach</b> is based on the using of FastICA algorithm for fECG extraction. In order to improve fECG extraction performance, it was applied here a combination of Undecimated Wavelet Transform (UWT) and Fast Fourier Transform (FFT) – Inverse Fast Fourier Transform (IFFT) algorithm as post-processing tool. Fetal ECG morphological indicators like heart rate, T/QRS ratio and QT interval could be estimated from fECG post-processed signals of two patients and some considerations regarding to the fetal stress during labor could be made in these two cases...|$|R
40|$|Abstract—Externally {{recorded}} knee-joint vibroarthrographic (VAG) signals bear {{diagnostic information}} related to degenerative conditions of cartilage disorders in a knee. The VAG technique is passive {{and can be used}} for long term monitoring. In order to improve the diagnostic capabilities of VAG, robust signal processing techniques are needed for de-noising of the signals. Traditional de-noising techniques apply a linear filter to remove the noise and interference from the VAG signals. These methods have certain limitations for the non-stationary VAG signals. In this paper, an improved technique for de-noising of VAG signals is presented. The acquired VAG signals are decomposed, de-noised and reconstructed by utilizing matlab wavelet transform <b>toolbox.</b> The proposed <b>approach</b> improves the signal to noise ratio (SNR) of these signals. The presented technique can be used in pre-processing stage of all VAG based knee joint monitoring and screening of articular cartilage pathology. Index Terms—Wavelets, de-noising, vibroarthrographi...|$|R
30|$|Most {{distributed}} applications and tools used in wide area networks and Cloud computing environments use the UNIX I/O framework. In this framework, processes use file descriptors for standard I/O and file access, {{with the traditional}} open/close/read/write interface. Although this design {{has proven to be}} excellent since the 1970 s, it is not appropriate for today wide-area systems because of the implied RPCs and network latency. There are systems relying on message streams that perform well in such environments, but they depart from the <b>toolbox</b> <b>approach</b> embraced by UNIX, making it harder to combine existing programs to solve new tasks. In this paper we describe the design, implementation, and usage of a new I/O framework, built to enable the construction of services in environments with high latency, while preserving the programmability of the system as a whole and making it convenient to combine existing tools and programs. The framework relies on named channels for I/O. Each channel carries a stream of typed data including directory entries, raw bytes, and other application-specific data. Separate commands using the framework may be combined as in UNIX, but still tolerate high latencies as found in distributed and Cloud computing environments, enabling a <b>toolbox</b> <b>approach</b> in such environments.|$|E
30|$|Erlang [2] is a {{language}} departing from the UNIX interface for I/O. Like Go [7] {{it relies on}} a CSP-like style where processes exchange messages through channels. Unlike Go, it can assign processes to different machines and use Erlang process addresses to deliver messages across the network. The Clive I/O framework presented here also relies on channels, but, unlike in Erlang, separates channels from processes following the Go style of concurrent programming. Furthermore, Clive commands {{can be used as}} tools similar to UNIX commands and can be combined with other UNIX commands. Clive is closer in spirit to the UNIX <b>toolbox</b> <b>approach</b> than Erlang is, because it has been designed as a new system and not as {{a language}} for distributed applications.|$|E
40|$|In this paper, a new {{framework}} for one-dimensional contour extraction from discrete two-dimensional data sets is presented. Contour extraction {{is important in}} many scientific fields such as digital image processing, computer vision, pattern recognition, etc. This novel framework includes (but is not limited to) algorithms for dilated contour extraction, contour displacement, shape skeleton extraction, contour continuation, shape feature based contour refinement and contour simplification. Many of the new techniques depend strongly on {{the application of a}} Delaunay tessellation. In order to demonstrate the versatility of this novel <b>toolbox</b> <b>approach,</b> the contour extraction techniques presented here are applied to scientific problems in material science, biology and heavy ion physics. Comment: 12 pages, 14 figures, updated version including some further, minor correction...|$|E
40|$|Reviews {{the book}} 'Researching society and Culture' by Clive Seale (ed.) (2004). This new edition {{provides}} a multimethod overview and methodological contextualization {{of research for}} students of sociology and other disciplines. In his introduction Seale claims the text avoids the pitfalls of <b>toolbox</b> methods book <b>approaches</b> by addressing both methodology and method and also interlinking this discussion with philosophy, theory and practice in the social sciences. The text provides its own definitions of relevant contexts by including eleven chapters on relevant contexts for research including philosophy of social science, politics and identities, and history. Given the catholic audience of novice researchers of society and culture that the text attempts to address, it is perhaps not surprising that discourse analysis per se does not feature extensively in the text. The collection will provide a good exemplification of what researchers of other methodological persuasions see as relevant in understanding society and culture...|$|R
40|$|Spatial {{econometrics}} {{and also}} multilevel modelling techniques are increasingly {{part of the}} regional scientists‟ <b>toolbox.</b> Both <b>approaches</b> are used to model spatial autocorrelation {{in a wide variety}} of applications. However, it is not always clear on which basis researchers make a choice between spatial econometrics and spatial multilevel modelling. Therefore it is useful to compare both techniques. Spatial econometrics incorporates neighbouring areas into the model design; and thus interprets spatial proximity as defined in Tobler‟s first law of geography. On the other hand, multilevel modelling using geographical units takes a more hierarchical approach. In this case the first law of geography can be rephrased as „everything is related to everything else, but things in the same region are more related than things in different regions‟. The hierarchy (multilevel) and the proximity (spatial econometrics) approach are illustrated using Belgian mobility data and productivity data of European regions. One of the advantages of a multilevel model is that it can incorporate more than two levels (spatial scales). Another advantage is that a multilevel structure can easily reflect an administrative structure with different government levels. Spatial econometrics on the other hand works with a unique set of neighbours which has the advantage that there still is a relation between neighbouring municipalities separated by a regional boundary. The concept of distance can also more easily be incorporated in a spatial econometrics setting. Both spatial econometrics and spatial multilevel modelling proved to be valuable techniques in spatial research but more attention should go to the rationale why one of the two approaches is chosen. We conclude with some comments on models which make a combination of both techniques...|$|R
40|$|Participatory Irrigation Management (PIM) is a {{key term}} in the <b>toolbox</b> of current <b>approaches</b> to improve the {{efficiency}} and performance of water resource management. Experiences from several countries indicate that introducing participatory elements {{in the relationship between}} – mostly governmental- decision makers on water resources and end users of water is an essential, but neither detached nor standardized process in the complex set-up of successful water resources management. The differences between the applied participatory approaches support the assumption that PIM cannot be transferred from one situation to another without modifications. Published guidelines on elements and procedures in participation put their focus on general applicability, but the successful implementation of PIM in a specific case crucially depends on its sensible adaptation to the local situation. Water resource management in the Jordan River's east bank underwent significant changes since the 1960 s and became a highly centralized irrigation system {{under the control of the}} governmental Jordan Valley Authority (JVA). The social structure of the farming population developed alongside the reorganization, but weakened tribal bonds in favour of the economi...|$|R
40|$|Introduction Fine-grain {{scheduling}} {{based on}} software feedback [3] {{was introduced in}} the Synthesis operating system [4, 2] to solve two problems: the dependency between jobs in a pipeline and the low-latency requirements of multimedia type applications. The performance level achieved and the adaptiveness of applications running on Synthesis demonstrated the success of fine-grain scheduling based on software feedback. However, the Synthesis implementation of software feedback is specialized for that particular architecture and a particular application (pipelined process scheduling). Consequently, despite the proven success of fine-grain scheduling, {{it is not easy to}} port it to another operating system or to apply its lessons elsewhere, even within Synthesis. To address the problems of portability and extensibility of software feedback scheduling mechanisms, we have taken a <b>toolbox</b> <b>approach</b> in our current research. Instead of creating a specialized solution for each particular s...|$|E
40|$|Abstract—This paper {{provides}} an overview of the e-SENSE Integrated Project, highlighting its strategic importance with respect to other projects in Mobile and Wireless Systems and Platforms Beyond 3 G (B 3 G). The main focus of the project is to capture context through the use of wireless sensor networks and further integrate the context information through an open gateway architecture into B 3 G. The motivation behind this is to provide real context information for the concept of Ambient Intelligence that is a focal element in many current next generation communication systems, applications and services. e-SENSE approaches this by researching efficient and light weight wireless sensor communication systems including the physical layer up to the transport layer and a distributed processing middleware including distributed services and distributed data processing in a <b>toolbox</b> <b>approach.</b> Further, components of th...|$|E
40|$|We {{present the}} SUPER environment, an {{integrated}} CASE tool whose components cover {{all phases of}} user interaction with a database. The SUPER environment is based on ERC+, an object based extension of the ER model. The proposed architecture can be suitable for all visual environments based on direct manipulation and high-level data models. The architecture is open and built on a generic kernel, providing basic functionalities and independent form any specific tool. We retained the <b>toolbox</b> <b>approach</b> {{for the development of}} the environment, as it appropriately supports both data models and direct manipulation interfaces implementation. In particular, in the paper we present SUPER existing tools: the design and query editors. We focus on functionalities and underlying design choices rather than how they operate. These interfaces provide users with flexibility, simple graphical interactions and consistency of interaction styles over the various tools. Keywords: Database visual environments, CA [...] ...|$|E
40|$|This paper {{presents}} an analysis and selection of various techniques/parameters {{in developing a}} suitable feed forward neural network for performing static voltage stability analysis using MATLAB 5. 2 NEURAL NETWORK <b>TOOLBOX.</b> Initially, an <b>approach</b> based on the input-output relation of real/reactive power and voltage vectors for generator as well as load buses with the voltage stability Index-L is used to train feed forward neural network. Further various toolbox functions such as different types of feed forward neural network, training functions, activation functions, learning functions, initialization functions and performance functions available are tested and the most suitable combination is selected. We summarize the analysis comparing the results with each training function and conclude the suitability of ANN training for Voltage Stability Assessment. Development of the proposed approach shows the ability of ANN in voltage stability assessment and improvement using the well known L-index. This approach {{can be used in}} Energy Management Center and shows the vulnerable buses as indicated by index-L at the same time reduce the computational complexity in mathematical calculations. ...|$|R
40|$|The total {{atomization}} {{energy at}} absolute zero, (TAE 0) of benzene, C 6 H 6, was computed fully ab initio {{by means of}} W 2 h theory as 1306. 6 kcal/mol, to {{be compared with the}} experimentally derived value 1305. 7 ± 0. 7 kcal/mol. The computed result includes contributions from inner-shell correlation (7. 1 kcal/mol), scalar relativistic effects (- 1. 0 kcal/mol), atomic spin-orbit splitting (- 0. 5 kcal/mol), and the anharmonic zero-point vibrational energy (62. 1 kcal/mol). The largest-scale calculations involved are CCSD/cc-pV 5 Z and CCSD(T) /cc-pVQZ; basis set extrapolations account for 6. 3 kcal/mol of the final result. Performance of more approximate methods has been analyzed. Our results suggest that, even for systems the size of benzene, chemically accurate molecular atomization energies can be obtained from fully first-principles calculations, without resorting to corrections or parameters derived from experiment. 1 Computational thermochemistry is coming of age as part of the chemist’s <b>toolbox</b> [1]. Popular <b>approaches</b> (such as G 3 theory [2] and CBS-QB 3 [3]) that can lay claim to ‘chemica...|$|R
40|$|The European Data Portal shows {{a growing}} number of {{governmental}} organisations opening up transport data. As end users need traffic or transit updates on their day-to-day travels, route planners need access to this government data to make intelligent decisions. Developers however, will not integrate a dataset when the cost for adoption is too high. In this paper, the authors study the internal and technological challenges to publish data from the Department of Transport and Public Works in Flanders for maximum reuse. Using the qualitative Engage STakeholdErs through a systEMatic <b>toolbox</b> (ESTEEM) research <b>approach,</b> they interviewed 27 governmental data owners and organised both an internal workshop as a matchmaking workshop. In these workshops, data interoperability was discussed on four levels: legal, syntactic, semantic and querying. The interviews were summarised in ten challenges to which possible solutions were formulated. The effort needed to reuse existing public datasets today is high, yet they see the first evidence of datasets being reused in a legally and syntactically interoperable way. Publishing data so that it is reusable in an affordable way is still challenging...|$|R
40|$|MPEG- 4 is {{currently}} being developed by MPEG to specify the technologies for supporting current and emerging multimedia applications. Because of its object-based features and flexible <b>toolbox</b> <b>approach,</b> {{it is much more}} complex than previous video coding standards. We believe that software-based implementation on parallel and distributed computing systems is a natural and viable option. In this paper, we describe such an approach on the MPEG-LF video encoder using a cluster of workstations. We propose to use hierarchical Petri Nets as a modeling tool to describe the temporal relations and time constrains among various video objects at different levels. This would allow us to perform scheduling with a guarantee of synchronization among multiple objects. A dynamic shape-adaptive data parallel approach is used in the spatial domain for further speed-up gain. Our preliminary results indicate that realtime MPEG- 4 encoding using distributed and parallel computing is achievable...|$|E
40|$|Controlled {{release is}} an {{important}} determinant of the in-vivo performance of drug delivering nanoparticles (NPs). Therefore the control over {{and understanding of the}} release mechanism, e. g. by disassembly or degradation of the carrier, is essential for the optimization of NP formulations. This paper presents a supramolecular <b>toolbox</b> <b>approach</b> for the formation and UV-induced disassembly of supramolecular nanoparticles (SNPs) which are either exclusively stabilized by cucurbit[8]uril (CB[8]) /methyl viologen (MV) /azobenzene (Azo) interactions or CB[8]/MV/naphthol (Np) interactions, or by a combination of both. Photoisomerization of the Azo units enables UV-triggered disassembly of the CB[8]/MV/Azo host-guest complex. Depending on the valency of the electron-rich guest moieties (Np or Azo), either SNPs with a UV-responsive shell or a UV-responsive core were formed by assembling SNPs using a mixture of Azo and Np bearing guest molecules. In contrast, non-responsive SNPs or SNPs which disintegrate at both the core and the shell were formed by using exclusively CB[8]/MV/Np or CB[8]/MV/Azo interactions, respectively...|$|E
40|$|Grasper-CL is {{a system}} for {{manipulating}} and displaying graphs, and for building graph-based user interfaces for application programs. It is implemented in COMMON LISP and CLIM, and has been proven by use {{in a number of}} applications. Grasper-CL includes several advances in graph drawing. It contains a graph abstract datatype plus a comprehensive and novel language of operations on that datatype. The appearance of Grasper-CL graphs can be tailored by a wide variety of shape parameters that allow the application to customize the display of nodes and edges for different domains. Default values for shape parameters can be established at several levels. Grasper-CL employs a <b>toolbox</b> <b>approach</b> to graph layout: the system contains a suite of graph layout algorithms that can be applied individually, or in combination to produce hierarchical graph layouts. The system also contains an interactive graph browser. Keywords: Graphs, Graph drawing, Graph layout 1. Introduction Graphs are virtually ubi [...] ...|$|E
40|$|This policy brief {{provides}} {{a summary of}} the approach and outcomes of a European Parliament Preparatory Action centred on the refinement and implementation of the Research and Innovation Smart Specialisation Strategy (RIS 3) in the region of Eastern Macedonia and Thrace (REMTh). Implemented, mainly during 2015, by the European Commission (the Joint Research Centre in collaboration with the Directorate General for Regional and Urban Policy) and the Managing Authority of the region, this action also had the explicit aim to draw lessons for other low growth and less developed regions in Europe. An essential aspect of the preparatory action was the opportunity it offered for stakeholders, the EC and the regional authority to share experiences and build a common understanding of RIS 3 and the challenges to its implementation. This centred on a series of stakeholder events, critical for the mutual learning process and trust building among stakeholders. Stakeholders have thus worked together to identify and exploit research and innovation based opportunities for the region as well as tackling the challenges to RIS 3 implementation. The various tools developed and applied in the REMTh preparatory action can, taken together, be seen to constitute a <b>toolbox</b> of <b>approaches</b> for RIS 3 implementation. This toolbox and the hands-on approach taken for the implementation of this Preparatory Action, as well as the flexibility to further adapt methodologies to local needs and context, can generate a wide set of tools and lessons on the implementation of regional smart specialisation strategies. These can be of benefit both to less developed regions that have struggled to restructure their economy in spite of considerable investments, and to all regions facing difficulties in implementing RIS 3 as a new and largely unknown governance approach. JRC. B. 3 -Territorial Developmen...|$|R
40|$|Synthetic {{biology is}} a dynamic, young, ambitious, attractive, and {{heterogeneous}} scientific discipline. It is constantly developing and changing, which makes societal evaluation of this emerging new science a challenging task, prone to misunderstandings. Synthetic biology {{is difficult to}} capture, and confusion arises not only regarding which part of synthetic biology the discussion is about, but also {{with respect to the}} underlying concepts in use. This book offers a useful <b>toolbox</b> to <b>approach</b> this complex and fragmented field. It provides a biological access to the discussion using a 'layer' model that describes the connectivity of synthetic or semisynthetic organisms and cells to the realm of natural organisms derived by evolution. Instead of directly reviewing the field as a whole, firstly our book addresses the characteristic features of synthetic biology that are relevant to the societal discussion. Some of these features apply only to parts of synthetic biology, whereas others are relevant to synthetic biology as a whole. In the next step, these new features are evaluated with respect to the different areas of synthetic biology. Do we have the right words and categories to talk about these new features? In the third step, traditional concepts like “life” and “artificiality” are scrutinized with regard to their discriminatory power. This approach may help to differentiate the discussion on synthetic biology. Lastly our refined view is utilized for societal evaluation. We have investigated the public views and attitudes to synthetic biology. It also includes the analysis of ethical, risk and legal questions, posed by present and future practices of synthetic biology. This book contains the results of an interdisciplinary research project and presents the authors’ main findings and recommendations. They are addressed to science, industry, politics and the general public interested in this upcoming field of biotechnology...|$|R
40|$|Development of the Health ↔ Work Toolbox is described. The toolbox aims {{to reduce}} the {{workplace}} impact of common health problems (musculoskeletal, mental health, and stress complaints) by focusing on tackling work-relevant symptoms. Based on biopsychosocial principles this <b>toolbox</b> supplements current <b>approaches</b> by occupying the zone between primary prevention and healthcare. It provides a set of evidence-informed principles and processes (knowledge + tools) for tackling work-relevant common health problems. The toolbox comprises a proactive element aimed at empowering line managers to create good jobs, and a ‘just in time’ responsive element for supporting individuals struggling with a work-relevant health problem. The key intention is helping people with common health problems to maintain work participation. The extensive conceptual and practical development process, including a comprehensive evidence review, produced a functional prototype toolbox that is evidence based and flexible in its use. End-user feedback was mostly positive. Moving the prototype to a fully-fledged internet resource requires specialist design expertise. The Health ↔ Work Toolbox appears to have potential {{to contribute to the}} goal of augmenting existing primary prevention strategies and healthcare delivery by providing a more comprehensive workplace approach to constraining sickness absence...|$|R
