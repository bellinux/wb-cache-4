1|1|Public
40|$|Technology Transfer (TT) is on {{the rise}} in Europe with an {{increased}} focus on cooperation between public research and the business community, as it enables good ideas and inventions to be transformed into products and services for the benefit of society. This project has sought to create the first Nordic Technology Transfer Network, enabling technology transfer professionals from all sectors to engage in the exchange of knowledge through the implementation of an e-based knowledge-exchange platform. The creation of this pan-Nordic <b>TT-network</b> was approached by a series of five regional workshops, with attendance from the local business communities and public research institutions. The network has all along been moderated by a project steering group with representatives from the five Nordic countries. The network&# 8217;s e-tool was further developed and customized during the project lifetime, allowing Nordic TT professionals to seek and exchange knowledge on a global level, to accommodate the Nordic technology transfer sector&# 8217;s expressed need for an international dimension...|$|E
40|$|Deep neural {{networks}} are surprisingly efficient at solving practical tasks, but {{the theory behind}} this phenomenon is only starting {{to catch up with}} the practice. Numerous works show that depth is the key to this efficiency. A certain class of deep convolutional networks [...] namely those that correspond to the Hierarchical Tucker (HT) tensor decomposition [...] has been proven to have exponentially higher expressive power than shallow networks. I. e. a shallow network of exponential width is required to realize the same score function as computed by the deep architecture. In this paper, we prove the expressive power theorem (an exponential lower bound on the width of the equivalent shallow network) for a class of recurrent {{neural networks}} [...] ones that correspond to the Tensor Train (TT) decomposition. This means that even processing an image patch by patch with an RNN can be exponentially more efficient than a (shallow) convolutional network with one hidden layer. Using theoretical results on the relation between the tensor decompositions we compare expressive powers of the HT- and <b>TT-Networks.</b> We also implement the recurrent <b>TT-Networks</b> and provide numerical evidence of their expressivity...|$|R

