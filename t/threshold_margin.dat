6|40|Public
40|$|We {{demonstrate}} polarization-independent simultaneous all-optical phase-preserving amplitude regeneration and wavelength {{conversion of}} NRZ {{differential phase shift}} keying (DPSK) data by four-wave mixing (FWM) in a semiconductor optical amplifier (SOA). The dependence upon polarization state of the signals is eliminated by using a co-polarized dual-pump architecture. Investigation on the regenerative capability vs. pumps detuning shows significant BER <b>threshold</b> <b>margin</b> improvement over 6 nm conversion range...|$|E
40|$|The NOAA 8 SARSAT system {{performance}} was tested at 121. 5 / 243 MHz to measure detection probability, detection threshold, location accuracy, ambiguity resolution, and multiple access capacity. Detection probability exceeds 0. 95 {{for a single}} satellite pass. Detection <b>threshold</b> <b>margin</b> varies from 2 dB (243 MHz, incoherent) to 20 dB (121. 5, coherent). Position location error is 20 km 68 % of the time. Ambiguity is resolved 75 % of the time. The system can locate 10 separate test signals...|$|E
40|$|A {{compact device}} with a {{two-level}} transfer function (TF) implemented with two semiconductor optical amplifier (SOA) -based stages is proposed and characterized. Each stage exploits nonlinear polarization rotation and self-phase modulation. The obtained improved TF with very flat {{top and bottom}} levels makes the scheme suitable for working as a reshaper in all-optical regeneration. The effectiveness of the device is verified in regenerating both nonreturn-to-zero (NRZ) and return-to-zero (RZ) data signals up to 40 Gb/s. Bit error rate measurements demonstrate increased <b>threshold</b> <b>margin</b> and extinction ratio improvement...|$|E
50|$|QMU {{focuses on}} {{quantification}} of {{the ratio of}} design margin to model output uncertainty. The process begins with {{the identification of the}} key performance thresholds for the system, which can frequently be found in the systems requirements documents. These thresholds (also referred to as performance gates) can specify an upper bound of performance, a lower bound of performance, or both in the case where the metric must remain within the specified range. For each of these performance thresholds, the associated performance margin must be identified. The margin represents the targeted range the system is being designed to operate in to safely avoid the upper and lower performance bounds. These margins account for aspects such as the design safety factor the system is being developed to as well as the confidence level in that safety factor. QMU focuses on determining the quantified uncertainty of the simulation results as they relate to the performance <b>threshold</b> <b>margins.</b> This total uncertainty includes all forms of uncertainty related to the computational model as well as the uncertainty in the <b>threshold</b> and <b>margin</b> values. The identification and characterization of these values allows the ratios of margin-to-uncertainty (M/U) to be calculated for the system. These M/U values can serve as quantified inputs that can help authorities make risk-informed decisions regarding how to interpret and act upon results based on simulations.|$|R
5000|$|QMU has the {{potential}} to support improved decision-making for programs that must rely heavily on modeling and simulation. Modeling and simulation results are being used more often during the acquisition, development, design, and testing of complex engineering systems. [...] One of the major challenges of developing simulations is to know how much fidelity should be built into each element of the model. The pursuit of higher fidelity can significantly increase development time and total cost of the simulation development effort. QMU provides a formal method for describing the required fidelity relative to the design <b>threshold</b> <b>margins</b> for key performance variables. This information {{can also be used to}} prioritize areas of future investment for the simulation. Analysis of the various M/U ratios for the key performance variables can help identify model components that are in need of fidelity upgrades to order to increase simulation effectiveness.|$|R
30|$|We {{used these}} {{thresholds}} to construct zones of equivalence and noninferiority for systematic and sporadic errors, respectively. Because systematic errors are meaningful in either direction, {{we used the}} systematic error <b>thresholds</b> as <b>margins</b> both above and below zero, thereby delineating a zone of equivalence around the adjusted mean obtained from the instructor’s images. For sporadic errors, because variance is inherently ‘one-sided’ (as it is a squared quantity), we used sporadic error <b>thresholds</b> as single <b>margins</b> above zero, delineating the upper bound of zones of noninferiority. We considered model-derived errors to be significant if 90 % confidence intervals (CIs) around the point estimates, using two-sided CIs for systematic errors and one-sided CIs for sporadic errors, fell outside zones of equivalence and noninferiority, respectively.|$|R
40|$|We {{investigate}} and experimentally demonstrate a simple method for phase-preserving amplitude regeneration of constant envelope phase-coded signals. This scheme exploits nonlinear interaction between noisy data and a continuous wave beam {{at a different}} wavelength in a saturated semiconductor optical amplifier (SOA). We show that proper balancing of the input signals' power allows us to exploit the amplitude limiting effect of SOA saturated gain without introducing significant excess phase noise due to suppression of the α-factor in the amplifier. In a 10 -Gb/s nonreturn-to-zero differential phase-shift-keying experiment, both four-wave-mixing (FWM) and pass-through signals are remarkably improved with respect to input data in terms of Q-factor and bit error ratio <b>threshold</b> <b>margin,</b> demonstrating wavelength preserving and wavelength converting regeneration. In particular, the FWM signal exhibits better regenerative performance over {{a broader range of}} degraded input data and for lower input overall power levels...|$|E
40|$|The aim of {{this study}} is {{introducing}} an online intelligent method for bidding negotiations in e-marketing. The growth and popularity of internet, increases using of modern techniques to help costumers and sellers in choosing best product and achieve higher benefit. Recommender systems as useful mean have memorable role in permanency customer loyalty. In traditional trade, customer and seller negotiate face to face. But now in online trade, it has changed to negotiation through internet and recommender systems. As a result, paying attention to preferences of both customer and seller in online structure is needed. In this study, we propose a method for making a recommender system for both seller and customer such that the satisfaction level of both be more than a <b>threshold</b> <b>margin.</b> First the needs and preferences of seller and customer are determined and then through the proposed algorithm successive suggestions are made until achieving a point that both sides of the business feel satisfaction...|$|E
40|$|OBJECTIVE: The aim of {{this study}} was to compare the thermal safety of Er:YAG and Er,Cr:YSGG lasers with {{conventional}} multi-use and single-use diamond burs. BACKGROUND DATA: Thermal effect of tooth preparation is mostly evaluated through the pulp chamber because it is difficult to measure the temperature of the preparation surface. A new in vitro method was introduced to simultaneously evaluate the heat increase of the preparation surface together with the pulp chamber. METHODS: Six laser and bur instrument groups were used to make standardized preparations on buccal surfaces of 60 intact third molars. The preparations removed an equal volume of hard tissue from each tooth (4 mm occluso-gingival x 8 mm mesial-distal x 1. 6 mm bucco-lingual). The teeth also included tunnel preparations from the opposite (lingual) surface, exposing the pulpal axial wall (axial dentin wall in contact with the pulp chamber from the preparation surface site). An infrared thermal camera was positioned to capture the preparation surface in direct vision, while the pulpal axial wall was indirectly reflected to the thermal camera via a minimal-energy-loss mirror. Data from both surfaces were analyzed statistically using Nested Least Squares Analysis. RESULTS: The laser groups generated significantly lower heat compared to bur groups on the preparation surfaces. In contrast, both lasers generated greater pulpal heat increase, and the Er:YAG laser group showed significance (p 3 ̆c 0. 0001). CONCLUSIONS: Lasers produced less heat on the preparation surface but more on the pulpal axial wall. However the temperature rise was less than the 5. 5 degrees C <b>threshold</b> <b>margin</b> of safety...|$|E
50|$|The {{system may}} also suffer from {{requirements}} uncertainty {{related to the}} specified <b>thresholds</b> and <b>margins</b> associated with the system requirements. QMU acknowledges that in some situations, the system designer may have high confidence in what the correct value for a specific metric may be, while at other times, the selected value may itself suffer from uncertainty {{due to lack of}} experience operating in this particular regime. QMU attempts to separate these uncertainty values and quantify each of them as part of the overall inputs to the process.|$|R
40|$|Final {{performance}} test {{data for the}} thematic mapper flight model multiplexer are presented in tables. Aspects covered include A/D thresholds for bands 5, 6, and 7; cross talk; the thermistor; bilevel commands signal parameters; A/D <b>threshold</b> ambient, voltage <b>margin</b> low bus; serial data and bit clock parameters; and the wire check. Tests were conducted at ambient temperature...|$|R
30|$|The {{studies in}} [17 – 19] {{have a similar}} focus to our {{approach}} of isolating subsystems of server hardware in order to characterise their resource consumption patterns for SLA and QoS support, the difference being that the strategies presented in all the three contributions consider CPU and memory utilisations associated with processor-bound workloads. Specifically, the strategy in [17] aims to guarantee CPU QoS delivery by overcoming the common problem of runtime interference effects that usually arises when running multiple instances of applications that are derived from virtualisation technologies. The interferences between the active VM application instances are minimised through {{the control of the}} working set sizes of allocated memory pages, thereby ensuring predictability of memory fetch times, CPU utilisations and ultimately, processor QoS support. In [18], the standardised metric, EC 2 Compute Unit (ECU), developed by Amazon is used for rating available computing power on various CPU hardware architectures. Based on the ECU metric, <b>thresholds</b> <b>margins</b> can be defined for identifying the resource utilisation levels, at which SLA violations are approached and the reallocation of the CPU resources can be initiated to protect SLA contracts. The framework presented in [19] features a dynamic SLA template that is designed to deal with changing user requirements by mapping consumer requirements to existing capabilities in the cloud infrastructure, with the focus also being on the allocation of processor cores as the primary resource entities of user interest.|$|R
40|$|We {{analyze the}} {{incumbency}} {{effect on a}} candidate’s electoral prospects using a large data set on Italian municipal elections held from 1993 to 2011. We apply a non-parametric Sharp Regression Discontinuity Design that compares candidates who barely win an election to those who barely lose, exploiting the fact that incumbency status changes discontinuously at the <b>threshold</b> of <b>margin</b> of victory of zero. We find that incumbents {{are more likely to}} win the competition compared to their challengers at the Italian municipal elections. The results are robust to different specifications and estimation strategies with excellent balance in observable characteristics. Also, the effect of interest seems to be larger in magnitude for municipalities located in the North of Italy compared to southern municipalities...|$|R
40|$|Field voles (Microtus agrestis) {{were trapped}} in 14 field margins and their {{behavioural}} and demographic parameters measured. Strong support was found for <b>thresholds</b> in <b>margin</b> width below which vole abundance was extremely low. Narrow margins were male biased with individuals moving greater distances and {{a large proportion of}} males behaved as transient individuals. However, no effect was observed on the age structure or survival of the population. Individuals were able to compensate for the lack of habitat through alterations in their behaviour sufficiently to maintain their survival. Within intensive agro-ecosystems, narrow strips between crops are important links for voles between wider margins and, if available, other more suitable habitats. Maintenance of narrow margins, along with larger areas of suitable habitat, is therefore effective in farmed landscapes for sustaining populations of specialist species where they show sufficient flexibility...|$|R
40|$|In {{this paper}} we combine {{advanced}} STAP algorithms and concepts for adaptive guard channels with subarrayed linear arrays {{together with the}} theoretical results available {{in the field of}} adaptive detection (GLRT, AMF, ACE tests). The aim is to define a robust adaptive sidelobe blanking detector against combined impulse and CW interference as well as strong clutter discretes. For the selection of the <b>thresholds,</b> the detection <b>margin</b> is used as a direction dependent tool instead of the commonly used main and guard channel antenna patterns...|$|R
40|$|Quantification of Margins and Uncertainties (QMU) is a {{decision-support}} {{methodology for}} complex technical decisions centering on performance <b>thresholds</b> and associated <b>margins</b> for engineering systems. Uncertainty propagation {{is a key}} element in QMU process for structure reliability analysis at the presence of both aleatory uncertainty and epistemic uncertainty. In order to reduce the computational cost of Monte Carlo method, a mixed uncertainty propagation approach is proposed by integrated Kriging surrogate model under the framework of evidence theory for QMU analysis in this paper. The approach is demonstrated by a numerical example to show the effectiveness of the mixed uncertainty propagation method...|$|R
40|$|Does trader {{leverage}} {{exacerbate the}} liquidity co-movement that we observe during crises? We exploit the <b>threshold</b> rules governing <b>margin</b> trading eligibility in India {{to identify a}} causal relationship between trader leverage {{and the extent to}} which a stock’s liquidity covaries with the liquidity of other stocks. We find that trader leverage causes sharp increases in comovement during severe market downturns. For our sample of stocks, the estimates suggest that the trader leverage channel explains about one third of the increase in liquidity commonality that we observe during crises. Consistent with downward price pressure due to the deleveraging of traders who rely on borrowing, we also find that leverage causes stocks to exhibit large increases in return comovement during crises...|$|R
30|$|Thirty-two 18 F-FDG PET-CT scans were acquired. The third PET-CT scan {{of patient}} 10 {{could not be}} {{performed}} due to scheduling difficulties. For patient 1, in scan 3 {{it was not possible}} to draw a VOIauto, since the tumor showed an almost complete metabolic response at this treatment stage and it did not meet the <b>margin</b> <b>thresholds</b> to complete the VOIauto. Since it was possible to define the other three types of VOIs, this scan was included in the analyses and a value of zero was given to the metabolic parameters for the VOIauto. The median time between the HILP and scan 2 was 21 [18 – 21] days, whereas the time between the end of EBRT and scan 3 was 3 (1 – 3) days.|$|R
40|$|The {{state of}} {{a flash memory}} cell, {{whether it is a}} logic high or low, is {{determined}} by sensing the current through the cell. [1 - 3] Today 2 ̆ 7 s common design practice is to convert the current to a voltage and then use a differential amplifier or latch to determine whether the flash memory cell has been programmed or erased. [1 - 9] The differential amplifier or latch scheme for data sensing has a major shortcoming; it is sensitive to process variations and noise, demanding wide <b>threshold</b> voltage <b>margins</b> between the programmed and erased memory cell states. As a result, programming and erase times are long, {{and it is difficult to}} store more than one bit of data per memory cell. [10] This thesis presents a new and improved sense amplifier design based on delta sigma modulation. Flash cell current is converted from analog to digital using a one-bit ΔΣ modulator. The delta sigma modulator cancels out noise and outputs an accurate measurement of average cell current. The data conversion circuitry is simple and less sensitive to noise and process variations, with less margin required between programmed and erased states. This new design approach leads to both more precise sensing and more economical memory chips...|$|R
40|$|Initial jobless claims {{provide a}} weekly {{snapshot}} of the labor market. While known for being volatile, when put into the appropriate context initial claims can provide valuable information on the upcoming employment report. This paper introduces a new labor market indicator, {{referred to as the}} threshold of initial jobless claims, that serves as a benchmark of comparison for the weekly reporting of initial jobless claims. The <b>threshold</b> incorporates multiple <b>margins</b> of the labor market such as hires, quits, layoffs, and labor force participation. Deviations of observed initial claims from the threshold are shown to provide accurate estimates of the upcoming change in the unemployment rate. Labor market followers can then make weekly comparisons of observed initial claims to the threshold to gain an updated understanding on {{the current state of the}} labor market...|$|R
40|$|We {{analyze the}} {{counterparty}} risk embedded in CDS contracts, in {{presence of a}} bilateral margin agreement. First, we investigate the pricing of collateralized counterparty risk and we derive the bilateral Credit Valuation Adjustment (CVA), unilateral Credit Valuation Adjustment (UCVA) and Debt Valuation Adjustment (DVA). We propose {{a model for the}} collateral by incorporating all related factors such as the <b>thresholds,</b> haircuts and <b>margin</b> period of risk. We derive the dynamics of the bilateral CVA in a general form with related jump martingales. We also introduce the Spread Value Adjustment (SVA) indicating the counterparty risk adjusted spread. Counterparty risky and the counterparty risk-free spread dynamics are derived and the dynamics of the SVA is found as a consequence. We finally employ a Markovian copula model for default intensities and illustrate our findings with numerical results. ...|$|R
30|$|The MP device {{provides}} real-time intraoperative {{assessment of}} microscopic lumpectomy margins. Published reports {{on use of}} the device have so far been in the settings of clinical studies. In this retrospective case review, we provided the first report on routine use of the device in the USA. The device integrated very well with the workflow in the operating room, taking no more than 5  minutes to use. There were no adverse events associated with device use. Adjunctive use of the device significantly decreases, when compared to corresponding historical series, by 62 % the rate of patients who required re-excision procedures. The <b>margin</b> <b>thresholds</b> for returning a patient {{to the operating room}} were not standardized between surgeons. Still, this relative reduction was similar irrespective of the nominal rates per surgeon. With use of the device, the rate of re-excision procedures was 9.7 %.|$|R
40|$|Abstract — For {{low power}} circuit. Circuit {{work at the}} low voltage supply, but at low voltage supply Integrated circuit {{designers}} start to face a power wall as the most difficult constraints. In new technology and large demand in market of portable electronic equipment are increasing the interest in very low power integrated circuits. Designing very low power integrated circuits requires {{the reduction of the}} internal power supply voltages and interconnection parasitic element should be minimum, as these are the most important parameters associated to power dissipation. For maintain the circuit performance at low voltage supply reduced the threshold voltage of the transistor which impose a large number of serious problem both in circuit design and technology. After decrease the <b>threshold</b> voltage the <b>margin</b> for error decrease and circuit parameter dramatically degrade the performance of original circuit. In this paper we used current compensation technique to minimize the power consumption...|$|R
40|$|Abstract—The {{three-dimensional}} (3 -D) NAND flash {{structure with}} fully charge storage using edge fringing field effect is presented, and its programming characteristic is evaluated. We successfully confirmed that this structure using fringing field effect provides good program characteristics showing sufficient <b>threshold</b> voltage (VT) <b>margin</b> by technology computer-aided design (TCAD) simulation. From the simulation results, {{we expect that}} program speed characteristics of proposed structure have competitive compared to other 3 D NAND flash structure. Moreover, {{it is estimated that}} this structural feature using edge fringing field effect gives better design scalability compared to the conventional 3 D NAND flash structures by scaling of the hole size for the vertical channel. As a result, the proposed structure is one of the candidates of Terabit 3 D vertical NAND flash cell with lower bit cost and design scalability. Index Terms—Novel 3 D NAND flash structure, edge fringing field effect, scalability I...|$|R
40|$|Theoretical {{studies have}} {{proposed}} that a critical threshold occurs below which a small change {{in the amount of}} habitat can cause an abrupt change in population persistence.   I tested the threshold concept using field voles (Microtus agrestis), their predator, the common weasel (Mustela nivalis vulgaris), and their ecto-parasites in a highly fragmented agro-ecosystem. I found strong support for a <b>threshold</b> in <b>margin</b> width, below which vole abundance was extremely low.   I also revealed that changes in the demography and behaviour of vole populations occurred in relation to the detected thresholds.   However, despite these responses, no effect was observed on their survival.   I found no evidence of a threshold response in either predator activity or parasite prevalence in relation to either prey/host abundance or habitat size.   Weasels may have been able to compensate for low vole abundances by consuming other rodents or by moving to areas with higher prey abundances.   The lack of any detectable effect of habitat loss on weasel activity may therefore be related to the scale of this study relative to their home range and their degree of diet specialisation.   The ecto-parasites observed infested all seven of the small mammal species trapped.   The high level of transiency and movement within this multi-host system may have facilitated the spread of parasites between margins.   The susceptibility of predators and parasites to habitat loss due to their trophic position may therefore have been offset by the high vagility of predators and the capability of both predators and parasites to predate/parasitise a range of host species. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
30|$|In {{search of}} a uniform and {{reproducible}} way to calculate changes in metabolic tumor activity in these upfront highly heterogeneous tumors, the use of four different VOI delineation techniques was studied. The VOIman (defined as reference VOI) is the only delineation technique in which the entire tumor is encompassed independently {{of the amount of}} necrosis present in the tumor. Therefore, the VOIman delineation technique seems to be most reliable when used for calculating the metabolic tumor activity. However, the VOIman delineation technique is time-consuming, making it unfit for implementation into daily practice. A high correlation, acceptable level of agreement, and comparable ranking was found between the VOIman and the VOIgrad+ delineation techniques. The differences in ranking between the four VOI delineation techniques are best explained by the high amount of necrosis present in these tumors, as tumor necrosis did not meet the <b>margin</b> <b>thresholds</b> of the VOIauto and VOIgrad. To obtain the VOIgrad+, the necrosis was manually included, and therefore, the ranking of patients was comparable to the ranking according to the VOIman.|$|R
40|$|In {{this paper}} we discuss the issue of {{computation}} of the bilateral credit valuation adjustment (CVA) under rating triggers, and in presence of ratings-linked margin agree-ments. Specifically, we consider collateralized OTC contracts, that are subject to rating triggers, between two parties – an investor and a counterparty. Moreover, we model the margin process as a functional of the credit ratings of the counterparty and the investor. We employ a Markovian approach for modeling of the rating transitions of the two parties to the contract. In this framework, we derive the representation for bilateral CVA. We also introduce a new component in the decomposition of the coun-terparty risky price: namely the rating valuation adjustment (RVA) that accounts for the rating triggers. We give two examples of dynamic collateralization schemes where the <b>margin</b> <b>thresholds</b> {{are linked to the}} credit ratings of the parties. We account for the rehypothecation risk in the presence of independent amounts. Our results are il-lustrated via computation of various counterparty risk adjustments for a CDS contract and for an IRS contract...|$|R
40|$|In {{familiarize}} thresholds {{literary criticism}} {{deals with the}} text as building his entrances and goings out that form the framework ocean, concept of threshold known in the Arab-Islamic culture in the French Western culture, namely the text equivalent of the original text is known only by and through him, and I knew study (platforms) by Jeanette study him in his quest for poetic and I ate before him and he Attoerha talking about scripts {{and their impact on}} the literary text production Almtaagliat, and afternoon term (the title) in the Arab and Western literature is not as one of the platforms only, but is parallel to the term (thresholds) and (platforms), which in its entirety is linked to a correlation entirely by stating and crossing it, and I have numerous job sites or platforms for the purposes of Taanah and descriptive and suggestive and erotic, and I have studied the surrounding text Altoleva in the context of applied studies in the book of miracle signs and found the most important Maver (the author's name, title, start-up, <b>margins)</b> <b>threshold.</b> ...|$|R
40|$|The {{subject of}} this book is to {{introduce}} a model-based quantitative performance indicator methodology applicable for performance, cost and reliability optimization of non-volatile memories. The complex example of flash memories is used to introduce and apply the methodology. It has been developed by the author based on an industrial 2 -bit to 4 -bit per cell flash development project. For the first time, design and cost aspects of 3 D integration of flash memory are treated in this book. Cell, array, performance and reliability effects of flash memories are introduced and analyzed. Key performance parameters are derived to handle the flash complexity. A performance and array memory model is developed {{and a set of}} performance indicators characterizing architecture, cost and durability is defined.    Flash memories are selected to apply the Performance Indicator Methodology to quantify design and technology innovation. A graphical representation based on trend lines is introduced to support a requirement based product development process. The Performance Indicator methodology is applied to demonstrate the importance of hidden memory parameters for a successful product and system development roadmap.   Flash Memories offers an opportunity to enhance your understanding of product development key topics such as: ·        Reliability optimization of flash memories is all about <b>threshold</b> voltage <b>margin</b> understanding and definition; ·        Product performance parameter are analyzed in-depth in all aspects in relation to the threshold voltage operation window; ·        Technical characteristics are translated into quantitative performance indicators; ·        Performance indicators are applied to identify and quantify product and technology innovation within adjacent areas to fulfill the application requirements with an overall cost optimized solution;  ·        Cost, density, performance and durability values are combined into a common factor – performance indicator - which fulfills the application requirements...|$|R
40|$|The {{purpose of}} the study is to analyse tax reform {{measures}} to secure the tax revenue base, in particular the personal income tax structure of South Africa. The main objectives are: firstly, to identify personal income tax reform interventions so as to align the personal income tax structure in South Africa with international best practices. Secondly, the impact of tax reforms on revenue collection, given optimal economic growth levels, is determined. Thirdly, to determine the best tax reform scenario which could minimise the individual tax burden and maximise its efficiency. Lastly, the impact of the suggested tax reforms on fairness as a principle of a good tax system is evaluated. A static micro-simulation model is developed from survey data and used to simulate the proposed tax reforms. Different tax reforms were selected from a study of international tax reform trends and an analysis of the South African personal income tax structure. The literature provides clear margins for the structuring of tax bands and <b>threshold</b> <b>margins.</b> Tax elasticities are estimated in order to explain the methodology for determining the impact of tax reforms. These elasticities include the elasticities for determining the progressiveness of the PIT structure, determining the deadweight loss (tax efficiency) and also to determine the optimal levels of taxes and economic growth and revenue maximisation. The different tax reform scenarios take the economy closer to or further away from optimum growth and optimum revenue. The results show that as far as marginal rates are concerned, a lowering in rates to levels on par with South Africa’s peers offers potential for improved levels of efficiency with the tax burden equal to or even below the optimal tax ratio from an economic growth point of view. Although such a ratio is below the optimal revenue ratio the results suggest that the loss in revenue could be minimised over time through a resultant increase in productivity and economic growth. By adjusting the non-taxable thresholds and taxable income bands according to the algorithm defined in the best practice scenario, more taxpayers will be included into the tax net but with a net decrease in tax liability. As a result the tax/GDP level also declines to a level below the optimal growth level but tax efficiency increases. The resultant loss in revenue will have to be recouped through increases in other than individual income taxes but improved levels of tax morality because of the lower margins for each tax band and increased productivity might also contribute to increased revenue performance. The tax structure is also more progressive which contributes towards the “fairness” of the tax regime. Regarding tax expenditure reforms, the analysis shows that medical tax credits offer a more equitable form of relief than medical deductions which substantiate this kind of reform as already implemented by government and which is to be fully phased in {{over the next couple of}} years. Tax liability is slightly lower in the case of medical credits compared to medical deductions but the difference is only marginal as far as net revenue and optimal growth and efficiency is concerned. However, a medical credit which increases disposable income at the lower end of the scale and discriminates against higher income groups also improves progressiveness of the tax regime and therefore the fairness thereof accordingly. Finally, the demographic impact of the suggested reforms also shows some important trends. Better education improves skills levels which seems to be positively correlated to taxable income levels. As far as age is concerned, the analysis shows that a substantial number of taxpayers in the categories below the age of 24 and above 65 fall within the lower taxable income groups. Those are also the most vulnerable groups from a subsistence point of view. Thus, tax reform that specifically improves their levels of disposable income should be prioritised in order to address equity and fairness as objectives for a “good” tax structure. Thesis (PhD) [...] University of Pretoria, 2014. gm 2014 EconomicsUnrestricte...|$|R
40|$|Abstract: Mobile {{had made}} the life of today’s world easy to {{communicate}} anywhere while Mobile terminals allow users to access service while on the move. Mobile terminals have made a rapid development in the mobile network industry in past decade. However {{as the number of}} cellular subscriber’s increases, the interference that will be experienced by the systems will also increase. This means that many large cellular systems will, sooner or later, have to handle interference problems. In this paper, all handoff effects of mobile Wimax networks are carried out. Performance is evaluated in terms of the expected number of handoffs, the expected handoff delay, standard deviation of handoff location, and the expected link degradation probability as well. The results showed that the <b>threshold</b> and hysteresis <b>margin</b> of the handover should be selected by considering the tradeoffs between the “ping-pang ” effect and the extra interference causing to neighbouring cells due to the poor quality link. After that some work has been done on the concept of smart handover manager. The main goal of this thesis is to reduce the handover latency. The handover latency of mobile Wimax is below 50 ms with the travelling speed of mobile station...|$|R
40|$|Mobile {{terminals}} allow {{users to}} access service {{while on the}} move. This unique feature has driven the rapid growth in the mobile network industry, changing it from a new technology into a massive industry {{in less than two}} decades. In this thesis, an in-depth study of the handover effects of mobile WiMAX networks is carried out. The mobile WiMAX technology is first presented as literature study and then the technologies of handovers for previous generations are introduced in detail. Further, the hard handover of the mobile WiMAX is simulated by Network Simulator- 2 (NS- 2). In addition, the 2 ̆ 2 ping-pang 2 ̆ 2 effect of handover was investigated and the call blocking and dropping probabilities are implemented using MATLAB. The goal is to find out which parameters have the significant impact on the handover performance. The results showed that the <b>threshold</b> and hysteresis <b>margin</b> of the handover should be selected by considering the tradeoff between the 2 ̆ 2 ping-pang 2 ̆ 2 effect and the extra interference causing to neighboring cells due to the poor quality link. The handover latency of mobile WiMAX is below 50 ms with the traveling speed of mobile station up to 20 m/s...|$|R
40|$|Abstract. We {{reconsider the}} {{stochastic}} (sub) gradient {{approach to the}} unconstrained primal L 1 -SVM optimization. We observe that if the learning rate is inversely proportional {{to the number of}} steps, i. e., the number of times any training pattern is presented to the algorithm, the update rule may be transformed into the one of the classical perceptron with margin in which the <b>margin</b> <b>threshold</b> increases linearly with the number of steps. Moreover, if we cycle repeatedly through the possibly randomly permuted training set the dual variables defined naturally via the expansion of the weight vector as a linear combination of the patterns on which margin errors were made are shown to obey {{at the end of each}} complete cycle automatically the box constraints arising in dual optimization. This renders the dual Lagrangian a running lower bound on the primal objective tending to it at the optimum and makes available an upper bound on the relative accuracy achieved which provides a meaningful stopping criterion. In addition, we propose a mechanism of presenting the same pattern repeatedly to the algorithm which maintains the above properties. Finally, we give experimental evidence that algorithms constructed along these lines exhibit a considerably improved performance. ...|$|R
40|$|An {{algorithm}} {{for learning}} fast multiclass object detection cascades is introduced. It produces multi-resolution (MRes) cascades, whose early stages are binary target vs. non-target detectors that eliminate false positives, late stages multiclass clas-sifiers that finely discriminate target classes, and middle stages have intermediate numbers of classes, determined in a data-driven manner. This MRes structure is achieved {{with a new}} structurally biased boosting algorithm (SBBoost). SBBost extends previous multiclass boosting approaches, whose boosting mechanisms are shown to implement two complementary data-driven biases: 1) the standard bias towards examples difficult to classify, and 2) a bias towards difficult classes. It is shown that structural biases can be implemented by generalizing this class-based bias, so as to encourage the desired MRes structure. This is accomplished through a generalized definition of multiclass margin, which includes a set of bias pa-rameters. SBBoost is a boosting algorithm for maximization of this margin. It can also be interpreted as standard multiclass boosting algorithm augmented with <b>margin</b> <b>thresholds</b> or a cost-sensitive boosting algorithm with costs defined by the bias parameters. A stage adaptive bias policy is then introduced to determine bias parameters in a data driven manner. This is shown to produce MRes cascades that have high detection rate and are computationally efficient. Experiments on multiclass object detection show improved performance over previous solutions. ...|$|R
40|$|Leakage {{detection}} from Liquid transmission pipeLines using improved {{pressure wave}} technique diagnozowanie nieszczeLności w rurociągach przesyłowych cieczy z wykorzystaniem zmodyfikowanej metody opartej na detekcji faL ciśnienia* This paper deals with leak detection in liquid transmission pipelines. It focuses on improving {{the efficiency of}} a method based on negative pressure wave detection. A new algorithm for pressure wave monitoring has been proposed. The algorithm is aimed to precisely capture the corresponding characteristic points in the signal sequence of negative pressure waves caused by leakage. It uses median filtering of the calculating deviations of pressure signals measured along the pipeline. Adaptive alarm <b>thresholds</b> with reduced <b>margins,</b> based on statistical analysis of the calculating deviations of pressure signals, were used. Additionally, the algorithm {{is supported by a}} set of functions base on the calculation of the cross-correlation of the deviations which represent pres-sure signals from neighboring transducers. The developed technique has been tested on a physical model of pipeline. The pipeline is 380 meters long and 34 mm in internal diameter, and is made of polyethylene (PEHD) pipes. The medium pumped through the pipeline was water. Tests proved, that the proposed solution is sensitive to small leaks and resistant for false alarm (occurring disturbances). It is also capable of localizing the leak point with satisfactory accuracy, without significant delay...|$|R
40|$|This {{dissertation}} {{focuses on}} option-based risk management from corporate finance and investment perspectives. ^ The first chapter {{focuses on a}} corporate finance issue: understanding dynamic hedge ratios of gold mining firms. An understanding of the hedging activities of a firm is critical to developing a thorough picture of risk exposures. In this paper we provide an alternative explanation for the dynamic nature of hedge ratios of mining firms. While the extant hedging literature is voluminous, it often errs by directly or indirectly assuming that Selective Hedging (in which managers base their hedging decisions on their expectations of future prices) {{is the source of}} dynamic hedge ratios. In contrast, we argue that the dynamic hedging activities of gold producers may not be indicative of Selective Hedging, but rather a form of hedging driven by the firm’s cost structure whereby the managers attempt to dynamically replicate a protective put position on the value of their future production to ensure a minimum <b>threshold</b> profit <b>margin</b> (similar to Portfolio Insurance). Informational asymmetries relating to the details of the firms cost structure may allow such managers to add value to the firm through their hedging activities. Thus, this model of hedging activity does not suffer from the persistence paradox of the Selective Hedging model. ^ The second essay focuses on option-based risk management from an investment perspective. The majority of the extant literature on option-based risk management focuses on equity investments. This is the first study to examine collar strategies on such a wide range of asset classes (17 exchange traded funds representing currencies, commodities, fixed income, and real estate). By considering the performance of collar strategies on a diverse set of assets, we draw insights into the unique characteristics of each asset class and the implications of these differences for risk management. We find significant differences across asset classes with respect to implied and realized volatility levels, volatility skew, transaction costs, liquidity and the effectiveness of collar strategies. This research provides valuable insight into the implementation of risk management strategies for retail and institutional investors in the rapidly evolving financial markets. ...|$|R
