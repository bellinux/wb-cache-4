444|2259|Public
25|$|Most {{software}} applications will compute small factorials by direct multiplication or <b>table</b> <b>lookup.</b> Larger factorial values can be approximated using Stirling's formula. Wolfram Alpha can calculate exact {{results for the}} ceiling function and floor function applied to the binary, natural and common logarithm of n! for values of n up to 249999, and up to 20,000,000! for the integers.|$|E
25|$|In {{computer}} science, tabulation hashing is {{a method}} for constructing universal families of hash functions by combining <b>table</b> <b>lookup</b> with exclusive or operations. It was first studied {{in the form of}} Zobrist hashing for computer games; later work by Carter and Wegman extended this method to arbitrary fixed-length keys. Generalizations of tabulation hashing have also been developed that can handle variable-length keys such as text strings.|$|E
25|$|The {{best way}} {{to speed up the}} baby-step giant-step {{algorithm}} is to use an efficient <b>table</b> <b>lookup</b> scheme. The best in this case is a hash table. The hashing is done on the second component, and to perform the check in step 1 of the main loop, γ is hashed and the resulting memory address checked. Since hash tables can retrieve and add elements in O(1) time (constant time), this does not slow down the overall baby-step giant-step algorithm.|$|E
40|$|Encryption {{algorithms}} commonly use <b>table</b> <b>lookups</b> {{to perform}} substitution, {{which is a}} confusion primitive. The use of <b>table</b> <b>lookups</b> {{in this way is}} especially common in the more recent encryption algorithms, such as the AES finalists like MARS and Twofish, and the AES winner, Rijndael. Workload characterization studies indicate that these algorithms spend a significant fraction of their execution cycles on performing these <b>table</b> <b>lookups,</b> more specifically on effective address calculations. This study.. ...|$|R
40|$|Symmetric table {{addition}} methods (STAMs) approximate functions {{by performing}} parallel <b>table</b> <b>lookups,</b> followed by multioperand addition. STAMs require signi cantly less memory than direct <b>table</b> <b>lookups</b> and are faster than piecewise linear approximations. This paper investigates {{the application of}} STAMs to the sigmoid function and its derivative, which are commonly used in arti cial neural networks. Compared to direct <b>table</b> <b>lookups,</b> STAMs require between 23 and 41 times less memory for sigmoid and between 24 and 46 times less memory for sigmoid's derivative, when the input operand size is 16 bits and the output precision is 12 bits...|$|R
5000|$|IP router tables {{can become}} bloated with routes to these ephemeral users slowing down <b>table</b> <b>lookups.</b>|$|R
25|$|Modern {{computers}} use {{a variety}} of techniques. One common method, especially on higher-end processors with floating point units, is to combine a polynomial or rational approximation (such as Chebyshev approximation, best uniform approximation, and Padé approximation, and typically for higher or variable precisions, Taylor and Laurent series) with range reduction and a table lookup—they first look up the closest angle in a small table, and then use the polynomial to compute the correction. Devices that lack hardware multipliers often use an algorithm called CORDIC (as well as related techniques), which uses only addition, subtraction, bitshift, and <b>table</b> <b>lookup.</b> These methods are commonly implemented in hardware floating-point units for performance reasons.|$|E
500|$|The Mark 1A Fire Control Computer (pictured) was an {{electro-mechanical}} analog ballistic computer. Its {{function was}} to automatically aim the guns {{so that a}} fired projectile would collide with the target. This was the same function as the main battery's Mk 8 Rangekeeper above except {{that some of the}} targets the Mark 1A had to deal with also moved in elevation — and much faster. For a surface target, the Secondary Battery's Fire Control problem {{is the same as the}} Main Battery's with the same type inputs and outputs. The major difference between the two computers was their ballistics calculations. The amount of gun elevation needed to project a 5-inch (127mm) shell [...] is different than the elevation needed to project a 16-inch shell the same distance. The ballistics calculations in these mechanical analog computers were performed by mechanisms like differential gears, levers, and small rods riding on the surface of three-dimensional cams. These mechanical adders, multipliers, and <b>table</b> <b>lookup</b> devices were handmade at the factory, and were buried deep in the workings of the computer. It was not possible to change a computer's ballistics at sea until the advent of fast digital computers. The anti-aircraft fire control problem was more complicated because it had the additional requirement of tracking the target in elevation and making target predictions in three dimensions. The outputs of the Mk 1A were the same (gun bearing and elevation), except fuze time was added. The fuze time was needed because the ideal of directly hitting the fast moving aircraft with the projectile was impractical. With fuze time set into the shell, it was hoped that it would explode near enough to the target to destroy it with the shock wave and shrapnel. Towards the end of World War II, the invention of the VT proximity fuze eliminated the need to use the fuze time calculation and its possible error. This greatly increased the odds of destroying an air target.|$|E
2500|$|Conical {{micrometer}} anvils, specifically {{suited to}} {{resting on the}} sides of the thread, are made for various thread angles, with 60° being the most common. Mics with such anvils are usually called [...] "thread mics" [...] or [...] "pitch mics" [...] (because they directly measure the pitch diameter). Users who lack thread mics rely instead on the [...] "3-wire method", which involves placing 3 short pieces of wire (or gauge pins) of known diameter into the valleys of the thread and then measuring from wire to wire with standard (flat) anvils. A conversion factor (produced by a straightforward trigonometric calculation) is then multiplied with the measured value to infer a measurement of the thread's pitch diameter. Tables of these conversion factors were established many decades ago for all standard thread sizes, so today a user need only take the measurement and then perform the <b>table</b> <b>lookup</b> (as opposed to recalculating each time). The 3-wire method is also used when high precision is needed to inspect a specific diameter, commonly the pitch diameter, or on specialty threads such as multi-start or when the thread angle is not 60°. [...] Ball-shaped micrometer anvils can be used in similar fashion (same trigonometric relationship, less cumbersome to use). Digital calipers and micrometers can send each measurement (data point) as it occurs to storage or software through an interface (such as USB or RS-232), in which case the <b>table</b> <b>lookup</b> is done in an automated way, and quality assurance and quality control can be achieved using statistical process control.|$|E
5000|$|The shift {{rules are}} {{implemented}} as constant-time <b>table</b> <b>lookups,</b> using <b>tables</b> generated during the preprocessing of [...]|$|R
5000|$|Blue boxes: The blue boxes {{represent}} so-called <b>lookup</b> <b>tables.</b> A <b>lookup</b> <b>table</b> {{consists of}} two columns (key and value) containing information used for access control, e-mail routing etc.|$|R
50|$|This example {{does not}} use any {{branches}} or <b>table</b> <b>lookups</b> {{in order to avoid}} side channels and is therefore suitable for use in cryptography.|$|R
2500|$|Another common {{feature is}} the use of the 'z' key as a wildcard. [...] The Wubi method was {{actually}} designed with this feature in mind; this is why no components are assigned to the z key. [...] Basically, one can type a z when unsure what the component should be, and the input method will help complete it. [...] If one knew, for example, that the character ought to start with [...] "kt", but was unsure what the next component should be, typing [...] "ktz" [...] would produce a list of all characters starting with [...] "kt". [...] In practice though, many input method engines use a tabular lookup method for all table based input systems, including for Wubi. This means that they simply have a large table in memory, associating different characters to their respective representations. [...] The input method then simply becomes a <b>table</b> <b>lookup.</b> [...] In such an implementation, the z key breaks the paradigm and as such is not found in much generalized software (although the Wubi input method commonly found in Chinese Windows implements the feature). For this same reason, the multiple character optimization described in the previous paragraph is also relatively rare.|$|E
2500|$|... {{observed}} that the separator theorem {{may be used to}} obtain polynomial time approximation schemes for NP-hard optimization problems on planar graphs such as finding the maximum independent set. Specifically, by truncating a separator hierarchy at an appropriate level, one may find a separator of size O(n/√lognbsp&n) the removal of which partitions the graph into subgraphs of size cnbsp&lognbsp&n, for any constant c. By the four-color theorem, there exists an independent set of size at least n/4, so the removed nodes form a negligible fraction of the maximum independent set, and the maximum independent sets in the remaining subgraphs can be found independently in time exponential in their size. By combining this approach with later linear-time methods for separator hierarchy construction and with <b>table</b> <b>lookup</b> to share the computation of independent sets between isomorphic subgraphs, it can be made to construct independent sets of size within a factor of 1nbsp&minus&nbsp&O(1/√lognbsp&n) of optimal, in linear time. However, for approximation ratios even closer to 1 than this factor, a later approach of [...] (based on tree-decomposition but not on planar separators) provides better tradeoffs of time versus approximation quality.|$|E
50|$|Soon {{after the}} first version, Maven {{acquired}} rack evaluation terms for vowel/consonant balance and Q/U distribution. Vowel/consonant balance was a <b>table</b> <b>lookup</b> based on the count of vowels and consonants left in the rack. Q/U distribution varied the values of Q and U using a <b>table</b> <b>lookup</b> indexed by how many of each remained in the bag.|$|E
50|$|On {{systems with}} 32-bit or larger words, it is {{possible}} to speed up execution of this cipher by combining the SubBytes and ShiftRows steps with the MixColumns step by transforming them into a sequence of <b>table</b> <b>lookups.</b> This requires four 256-entry 32-bit tables, and utilizes a total of four kilobytes (4096 bytes) of memory—one kilobyte for each table. A round can then be done with 16 <b>table</b> <b>lookups</b> and 12 32-bit exclusive-or operations, followed by four 32-bit exclusive-or operations in the AddRoundKey step.|$|R
40|$|This paper {{presents}} a high-speed method for computing elementary functions using parallel <b>table</b> <b>lookups</b> and multi-operand addition. Increasing {{the number of}} tables and inputs to the multi-operand adder significantly reduces the amount of memory required. Symmetry and leading zeros in the table coefficients are used {{to reduce the amount}} of memory even further. This method has a closed-form solution for the table entries and can be applied to any differentiable function. For 24 -bit operands, this method requires two to three orders of magnitude less memory than conventional <b>table</b> <b>lookups.</b> Keywords: Elementary functions, <b>table</b> <b>lookups,</b> approximations, multi-operand addition, computer arithmetic, hardware design. 1. Introduction Elementary function approximations are important in scientific computing, computer graphics, and digital signal processing applications. In the systolic array implementation of Cholesky decomposition, presented in [1], 30 % of the cells approximate reciprocals [...] ...|$|R
40|$|Abstract. This paper {{presents}} a high-speed method for computing elementary functions using parallel <b>table</b> <b>lookups</b> and multi-operand addition. Increasing {{the number of}} tables and inputs to the multi-operand adder significantly reduces the amount of memory required. Symmetry and leading zeros in the table coefficients are used {{to reduce the amount}} of memory even further. This method has a closed-form solution for the table entries and can be applied to any differentiable function. For 24 -bit operands, this method requires two to three orders of magnitude less memory than conventional <b>table</b> <b>lookups...</b>|$|R
5000|$|... a <b>table</b> <b>lookup</b> is {{generally}} faster than an addition and a shift, and ...|$|E
50|$|Using {{a trivial}} hash function, in a non-iterative <b>table</b> <b>lookup,</b> can {{eliminate}} conditional testing and branching completely, reducing the instruction path {{length of a}} computer program.|$|E
50|$|The <b>Table</b> <b>lookup</b> (TLU) {{instruction}} could high-equal compare a referenced 10-digit {{word with}} 48 consecutive {{words on the}} same drum band in one 5ms revolution and then switch to the next band {{in time for the}} next 48 words. This feat was about one-third the speed of a one-thousand times faster binary machine in 1963 (1500 microseconds on the IBM 7040 to 5000 microseconds on the 650) for looking up 46 entries as long as both were programmed in assembler. There was an optional <b>Table</b> <b>lookup</b> Equal instruction, with the same performance.|$|E
40|$|This paper {{presents}} a methodology for designing bipartite tables for accurate function approximation. Bipartite tables use two parallel <b>table</b> <b>lookups</b> {{to obtain a}} carry-save (borrow-save) function approximation. A carry propagate adder can then convert this approximation to a two's complement number or the approximation can be directly Booth encoded. Our method for designing bipartite tables, called the Symmetric Bipartite Table Method, utilizes symmetry in the table entries to reduce the overall memory requirements. It has several advantages over previous bipartite table methods in that it (1) provides a closed form solution for the table entries, (2) has tight bounds on the maximum absolute error, (3) requires smaller <b>table</b> <b>lookups</b> to achieve a given accuracy, and (4) {{can be applied to}} a wide range of functions. Compared to conventional <b>table</b> <b>lookups,</b> the symmetric bipartite tables presented in this paper are 15. 0 to 41. 7 times smaller when the operand size is 16 bits and 99. 1 to 273 [...] . ...|$|R
40|$|The {{need for}} fast {{parallel}} <b>table</b> <b>lookups</b> {{is evident in}} many modern hardware applications, such as network switches, hard disk controllers, and encryption devices. Typically, most of these <b>table</b> <b>lookups</b> are performed in fast and expensive on-board SRAMs {{in order to reduce}} latency. These SRAMs frequently provide dual-ported access at speeds of up to 20 ns. However, for applications demanding many large look-up tables, SRAM's physical size, density, power requirements, and cost are prohibitive. In this paper, we address this problem through one particularly demanding example: the routing control in a sophisticated ATM switch. We present a design that uses merged memory and logic (MML, a modified form of DRAM) to simulate dual-ported SRAM in performing tens of <b>table</b> <b>lookups</b> in parallel. Our solution fits on one chip instead of over 300 required by an existing design, providing an integrated, low-power solution while still meeting the rigorous timing constraints of the application...|$|R
50|$|Unlike {{the above}} example, the {{character}} classification routines are not written as comparison tests. In most C libraries, they are written as static <b>table</b> <b>lookups</b> instead of macros or functions.|$|R
50|$|If the {{resulting}} four-kilobyte table size {{is too large}} for a given target platform, the <b>table</b> <b>lookup</b> operation can be performed with a single 256-entry 32-bit (i.e. 1 kilobyte) table {{by the use of}} circular rotates.|$|E
5000|$|The {{complexity}} of these algorithms {{is influenced by}} the sophistication of the language. For example, name resolution in assembly language usually involves only a single simple <b>table</b> <b>lookup,</b> while name resolution in C++ is extremely complicated as it involves: ...|$|E
5000|$|The {{adjacency}} table maintains layer 2 or {{switching information}} {{linked to a}} particular FIB entry, avoiding {{the need for an}} Address Resolution Protocol (ARP) request for each <b>table</b> <b>lookup.</b> There are several types of adjacencies. Some are listed below: ...|$|E
5000|$|In some cases, <b>lookup</b> <b>tables</b> {{are more}} {{efficient}} than non-optimized [...] statements since many languages can optimize <b>table</b> <b>lookups,</b> whereas switch statements are not optimized unless the range of values is small with few gaps. A non-optimized, non-binary search lookup, however, {{will almost certainly be}} slower than either a non-optimized switch or the equivalent multiple if-else statements.|$|R
50|$|Tabulation hashing is a {{technique}} for mapping keys to hash values by partitioning each key into bytes, using each byte as the index into a table of random numbers (with a different table for each byte position), and combining {{the results of these}} <b>table</b> <b>lookups</b> by a bitwise exclusive or operation. Thus, it requires more randomness in its initialization than the polynomial method, but avoids possibly-slow multiplication operations. It is 3-independent but not 4-independent. Variations of tabulation hashing can achieve higher degrees of independence by performing <b>table</b> <b>lookups</b> based on overlapping combinations of bits from the input key, or by applying simple tabulation hashing iteratively.|$|R
40|$|This paper {{treats the}} problem of {{designing}} an optimal size for a <b>lookup</b> <b>table</b> used for sensor linearization. In small embedded systems the <b>lookup</b> <b>table</b> must {{be reduced to a}} minimum {{in order to reduce the}} memory footprint and intermediate table values are estimated by linear interpolation. Since interpolation introduces an estimation uncertainty that increases with the sparseness of the <b>lookup</b> <b>table</b> there is a trade-off between <b>lookup</b> <b>table</b> size and estimation precision. This work will present a theory for finding the minimum allowed size of a <b>lookup</b> <b>table</b> that does not affect the overall precision, i. e. the overall precision is determined by the <b>lookup</b> <b>table</b> entries’ precision, not by the interpolation error...|$|R
50|$|As in above examples, it is {{possible}} to very efficiently translate the potential ASCII input values (A,S,M,D or unknown) into a pointer array index without actually using a <b>table</b> <b>lookup,</b> but is shown here as a table for consistency with the first example.|$|E
50|$|In many situations, {{hash tables}} {{turn out to}} be more {{efficient}} than search trees or any other <b>table</b> <b>lookup</b> structure. For this reason, they are widely used in many kinds of computer software, particularly for associative arrays, database indexing, caches, and sets.|$|E
50|$|In this example, {{database}} fields STATE, CUST_ID, NAME, PHONE, STATUS, and BALANCE {{are laid}} out on a grid, with two sort breaks (via BY), generated columns based on data values (via ACROSS), and data selection (via WHERE). Additional keywords could control subtotals, titles, footers, <b>table</b> <b>lookup,</b> and myriad reporting details.|$|E
40|$|Abstract — A Parallel {{algorithm}} and its hardware {{implementation of}} Inverse Halftone operation is proposed in this paper. The algorithm {{is based on}} <b>Lookup</b> <b>Tables</b> from which the inverse halftone value of a pixel is directly determined using a pattern of pixels. A method has been developed that allows accessing more than one value from the <b>lookup</b> <b>table</b> at any time. The <b>lookup</b> <b>table</b> is divided into smaller <b>lookup</b> <b>tables,</b> such that each pattern selected at any time goes to a separate smaller <b>lookup</b> <b>table.</b> The 15 -pixel parallel version of the algorithm was tested on sample images and a simple and effective method {{has been used to}} overcome quality degradation due to pixel loss in the proposed algorithm. It can provide at least 4 times decrease in <b>lookup</b> <b>table</b> size when compared with serial <b>lookup</b> <b>table</b> method implemented multiple times for same number of pixels. I...|$|R
40|$|This paper {{describes}} {{the use of}} an evolutionary algorithm to develop <b>lookup</b> <b>tables</b> which consist of an ordered list of regions, each of which encloses training examples of only one category. Compared to a simpler type of <b>lookup</b> <b>table</b> which consists of an unordered list of the training points and their categories, region based tables are smaller and, in general, faster to use. The development of a region based <b>lookup</b> <b>table</b> for the Frey and Slate character recognition problem is described and the size and accuracy achieved are compared with the original Frey and Slate point based <b>lookup</b> <b>table.</b> The reasons why it outperforms the original <b>lookup</b> <b>table</b> are discussed...|$|R
2500|$|Different {{algorithms}} may {{complete the}} same task {{with a different}} set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n) [...] ) outperforms a sequential search (cost O(n) [...] ) when used for <b>table</b> <b>lookups</b> on sorted lists or arrays.|$|R
