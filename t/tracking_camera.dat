120|990|Public
25|$|Video {{taken during}} {{lift-off}} of STS-107 was routinely reviewed {{two hours later}} and revealed nothing unusual. The following day, higher-resolution film that had been processed overnight revealed the foam debris striking the left wing, potentially damaging the thermal protection on the Space Shuttle. At the time, the exact location where the foam struck the wing could not be determined due to the low resolution of the <b>tracking</b> <b>camera</b> footage.|$|E
25|$|Guardian is {{designed}} to operate autonomously, without input from the flight crew. An array of sensors detects a missile approaching the aircraft and passes this information to an infrared <b>tracking</b> <b>camera.</b> The system's computer analyses the input signals, {{to confirm that the}} threat is real, and then directs a beam of an eye-safe infrared laser at the incoming object. The laser is intended to introduce a false target into the missile's guidance system, causing it to turn away from the aircraft. The detect-track-jam process lasts for two to three seconds. The system then automatically notifies the pilot and air traffic control that a threat has been jammed.|$|E
25|$|According to some {{conspiracy}} theorists, {{during the}} military investigation of green fireballs in New Mexico, UFOs were photographed by a <b>tracking</b> <b>camera</b> over White Sands Proving Grounds on April 27, 1949. They {{claim that the}} final report in 1951 on the green fireball investigation claimed there was insufficient data to determine anything. Conspiracy theorists claim that documents later uncovered by Dr. Bruce Maccabee indicate that triangulation was accomplished. The conspiracy theorists also claim that the data reduction and photographs showed four objects about 30 feet in diameter flying in formation at high speed {{at an altitude of}} about 30 miles. According to conspiracy theorists, Maccabee says this result was apparently suppressed from the final report.|$|E
5000|$|Baker-Nunn {{satellite}} <b>tracking</b> <b>cameras</b> (January 1967-October 1975) ...|$|R
30|$|We divide human <b>tracking</b> over <b>camera</b> {{networks}} {{into two}} inter-related modules: human <b>tracking</b> within a <b>camera</b> and human <b>tracking</b> across non-overlapping <b>cameras.</b>|$|R
50|$|The main news {{studio is}} located {{adjacent}} to the newsroom, with a <b>track</b> <b>camera</b> connecting the two for newscast openings.|$|R
500|$|As {{the name}} says, a <b>tracking</b> <b>camera</b> follows the {{characters}} from behind. The player does {{not control the}} camera in any way - he/she cannot for example rotate it or move it to a different position. This type of camera system was very common in early 3D games such as Crash Bandicoot or Tomb Raider since it is very simple to implement. However, {{there are a number}} of issues with it. In particular, if the current view is not suitable (either because it is occluded by an object, or because it is not showing what the player is interested in), it cannot be changed since the player does not control the camera. Sometimes this viewpoint causes [...] difficulty when a character turns or stands face out against a wall. The camera may jerk or end up in awkward positions.|$|E
500|$|This type {{of camera}} system is an {{improvement}} over the <b>tracking</b> <b>camera</b> system. While the camera is still tracking the character, some of its parameters, such as its orientation or distance to the character, can be changed. On video game consoles, the camera is often controlled by an analog stick to provide a good accuracy; whereas on PC games it is usually controlled by the mouse. This {{is the case in}} games such as Super Mario Sunshine or [...] Fully interactive camera systems are often difficult to implement in the right way. Thus GameSpot argues that much of the Super Mario Sunshine difficulty comes from having to control the camera. The Legend of Zelda: The Wind Waker was more successful at it - IGN called the camera system [...] "so smart that it rarely needs manual correction".|$|E
500|$|One of {{the first}} games to offer an {{interactive}} camera system was Super Mario 64. The game had two types of camera systems between which the player could switch at any time. The first one was a standard <b>tracking</b> <b>camera</b> system {{except that it was}} partly driven by artificial intelligence. Indeed, the system was [...] "aware" [...] {{of the structure of the}} level and therefore could anticipate certain shots. For example, in the first level, when the path to the hill is about to turn left, the camera automatically starts looking towards the left too, thus anticipating the player's movements. The second type allows the player to control the camera relatively to Mario's position. By pressing on the left or right buttons, the camera rotates around Mario, while pressing up or down moves the camera closer or away from Mario.|$|E
50|$|The movie Koyaanisqatsi (1983) shows {{footage of}} Atlas-Centaur 1's launch, {{including}} the <b>tracking</b> <b>cameras</b> following the Atlas's sustainer engine down to impact with the ocean.|$|R
30|$|In recent years, {{automated}} human <b>tracking</b> over <b>camera</b> networks {{is getting}} essential for video surveillance. The tasks of <b>tracking</b> human over <b>camera</b> networks {{are not only}} inherently challenging due to changing human appearance, but also have enormous potentials {{for a wide range}} of practical applications, ranging from security surveillance to retail and health care. This review paper surveys the most widely used techniques and recent advances for human <b>tracking</b> over <b>camera</b> networks. Two important functional modules for the human <b>tracking</b> over <b>camera</b> networks are addressed, including human <b>tracking</b> within a <b>camera</b> and human <b>tracking</b> across non-overlapping <b>cameras.</b> The core techniques of human <b>tracking</b> within a <b>camera</b> are discussed based on two aspects, i.e., generative trackers and discriminative trackers. The core techniques of human <b>tracking</b> across non-overlapping <b>cameras</b> are then discussed based on the aspects of human re-identification, camera-link model-based tracking and graph model-based tracking. Our survey aims to address existing problems, challenges, and future research directions based on the analyses of the current progress made toward human <b>tracking</b> techniques over <b>camera</b> networks.|$|R
40|$|During {{the last}} years the use of <b>tracking</b> <b>cameras</b> for SLR {{observations}} became less important due to the high accuracy of the predicted orbits. Upcoming new targets like satellites in eccentric orbits and space debris objects, however, require <b>tracking</b> <b>cameras</b> again. In 2013 the interline CCD camera was replaced at the Zimmerwald Observatory with a so called scientific CMOS camera. This technology promises a better performance for this application than all kinds of CCD cameras. After the comparison of the different technologies the {{focus will be on}} the integration in the Zimmerwald SLR system...|$|R
5000|$|... #Subtitle level 3: Baker-Nunn {{satellite}} <b>tracking</b> <b>camera</b> station ...|$|E
50|$|It {{was home}} to the world's most {{accurate}} satellite <b>tracking</b> <b>camera.</b> The 8.5 ton Hewitt Camera.|$|E
5000|$|... #Caption: Disintegration of the SpaceX CRS-7 {{launch vehicle}} {{approximately}} two minutes after liftoff {{as seen from}} a NASA <b>tracking</b> <b>camera.</b>|$|E
50|$|Optical sensors {{included}} the twelve Baker-Nunn satellite <b>tracking</b> <b>cameras</b> operated for NASA by the Smithsonian Astrophysical Observatory (SAO), three Baker-Nunn cameras {{operated by the}} USAF, and the Boston University camera at Patrick Air Force Base operated by Walter Manning.|$|R
50|$|Face <b>tracking</b> with <b>camera.</b>|$|R
40|$|This paper {{presents}} the realtime processing of opti-cal motion capture with pan-tilt <b>camera</b> <b>tracking.</b> Pan-tilt <b>camera</b> <b>tracking</b> expands {{the range of}} capturing eld dynamically. Asymmetrical marker distribution and polyhedra search algorithm realize robust labeling against marker missing. The algorithm is developed for parallel cluster computation and enables realtime data processing. Experimental results demonstrate ef-fectiveness of the system...|$|R
5000|$|... #Caption: The intact {{crew cabin}} was seen exiting the cloud by a <b>tracking</b> <b>camera</b> after its {{trajectory}} carried it across an adjacent contrail.|$|E
5000|$|... #Caption: An {{illustration}} of a protagonist whom a player controls and a <b>tracking</b> <b>camera</b> just behind, slightly above, and slightly facing down towards that character.|$|E
5000|$|... #Caption: The Bolt <b>tracking</b> <b>camera</b> (on left) {{was used}} to capture the other band actors and {{audience}} members to create the live-action footage for the game.|$|E
50|$|Prior to {{the launch}} of the Sputnik I spacecraft, Baker collaborated with Joseph Nunn to build a series of 12 {{satellite}} <b>tracking</b> <b>cameras</b> that would be called the Baker-Nunn camera. Dr. Baker designed the optical system for the cameras, which were fabricated by Perkin-Elmer Corporation.|$|R
5000|$|In September 2010, Flying Lotus {{released}} [...] "Pattern+Grid World", a 8 track EP featuring Thundercat on bass & art by Theo Ellsworth. The <b>Track</b> <b>Camera</b> Day {{was used}} in the Killer Mike song Swimming, which was released as part of the Adult Swim Singles Series.|$|R
25|$|BWF {{introduced}} Hawk-Eye {{technology in}} 2014 after testing other instant review technologies for line call decision in BWF major events. Hawk-Eye's <b>tracking</b> <b>cameras</b> {{are also used}} to provide shuttlecock speed and other insight in badminton matches. Hawk-Eye is formally introduced in 2014 India Super Series tournament.|$|R
50|$|In the 1950s, Boller and Chivens {{collaborated with}} Perkin-Elmer {{to develop and}} {{manufacture}} the large-aperture Baker-Nunn satellite <b>tracking</b> <b>camera</b> for the United States Vanguard space satellite program.|$|E
50|$|The {{programme}} sometimes satirised {{current events}} but the mainstay was simple observational comedy and frequently employed base humour (for example, the <b>tracking</b> <b>camera</b> {{shot in the}} title sequence showed a drunk who had urinated in his trousers).|$|E
5000|$|... #Caption: A <b>tracking</b> <b>camera</b> located {{north of}} the pad {{captured}} the SRB plume as it burned through the external tank. The damaged SRB was seen exiting the vapor cloud with clear signs of O-ring failure on one of its segments.|$|E
50|$|In a new {{innovation}} the BBC introduced interactive services, {{which enabled}} UK viewers to access {{features such as}} a statistical predictor, archive footage of previous Nationals and a split-screen view of the race itself to enable viewers to watch the race from the air {{as well as the}} normal <b>tracking</b> <b>cameras.</b>|$|R
30|$|Effective {{global data}} {{association}} for human <b>tracking</b> over <b>camera</b> networks.|$|R
30|$|Motion <b>tracking</b> <b>cameras</b> (Optotrak Certus, Northern Digital Inc., Waterloo, Ontario, Canada) {{were used}} to {{validate}} proper angles of the tibia relative to the fixed femur prior to loading. Both tibial and femoral anatomical axes were pre-defined using a digitizing probe. Two infrared diode sensors were placed on both the femur and tibia to track their relative 3 D motion.|$|R
50|$|On 2 October 1976 Greenamyer flew {{an average}} 1010 mph at Mud Lake near Tonopah, Nevada. A <b>tracking</b> <b>camera</b> {{malfunction}} eliminated the necessary proof for formal records. On 24 October 1977 Greenamyer flew a 3 km official FAI record flight of 988.26 mph.|$|E
50|$|The SR-71 {{originally}} included optical/infrared imagery systems; side looking {{airborne radar}} (SLAR); electronic intelligence (ELINT) gathering systems; defensive systems for countering missile and airborne fighters; and recorders for SLAR, ELINT and maintenance data. The SR-71 carried a Fairchild <b>tracking</b> <b>camera</b> and an HRB Singer infrared camera, {{both of which}} ran during the entire mission.|$|E
50|$|Your Shape was {{released}} to mixed reviews by critics. Its motion <b>tracking</b> <b>camera</b> {{was met with}} positive reception for its ease of setup and accuracy; allowing Your Shape to not require any special accessories or controllers for use. However, reviewers also noted that a large play area was required to make the camera function with the game properly.|$|E
40|$|Abstract. During {{the last}} years the use of <b>tracking</b> <b>cameras</b> for SLR {{observations}} became less important due to the high accuracy of the predicted orbits. Upcoming new targets like satellites in eccentric orbits and space debris objects, however, require <b>tracking</b> <b>cameras</b> again. In 2013 the interline CCD camera was replaced at the Zimmerwald Observatory with a so called scientific CMOS camera. This technology promises a better performance for this application than all kinds of CCD cameras. After the comparison of the different technologies the {{focus will be on}} the integration in the Zimmerwald SLR system. Zimlat The 1 ‐meter Zimmerwald Laser and Astronometry Telescope (ZIMLAT) was installed in 1997. It allows for state‐of‐the‐art satellite laser ranging (SLR) and also serves as astronomical telescope for the optical observation of astrometric positions and magnitudes of near‐Earth objects, such as space debris, using Charge Coupled Device (CCD) or Complementary Metal Oxide Semiconductor (CMOS) cameras. The telescope is monostatic w. r. t. SLR (transmit and receive paths are identical between the primary mirror and the transmit/receive mirror located {{at the lower end of}} the Coudé path). The dichroic mirror (DBS) located in the fork of the mount (Figure 1) allows for the use of <b>tracking</b> <b>cameras</b> simultaneously with SLR observations. The Deflection Mirror (DM) is used to select one of 4 corrector lenses and cameras. The focal length varies between 1 m, 4 m and 8 m. Figure 1. Drawing of the ZIMLAT telescope and the derotator platform...|$|R
50|$|The digital eye <b>tracking</b> <b>cameras</b> - {{designed}} around state-of-the-art CMOS image sensors - are interfaced to {{a dedicated}} processor {{board in the}} host PC via bi-directional, high speed digital transmission links (400 Mbit/s). This PCI plug-in board carries the front-end processing architecture, consisting of digital signal processors (DSP) and programmable logic devices (FPGA) for binocular, online image and signal acquisition.|$|R
40|$|Marker-based optical outside-in {{tracking}} is {{a mature}} and robust technology {{used by many}} AR, VR and motion capture applications. However, in small environments the <b>tracking</b> <b>cameras</b> are often dif-ficult to install. An example scenario are ergonomic studies in car manufacturing, where the motion of a worker needs to be tracked in small spaces such as {{the trunk of a}} car. In this paper, we describe how to extend the tracking volume in small, cluttered environments using small and flexible wireless cameras in form of unmodified mobile phones that can quickly be installed. Since the mobile phones are not synchronized with the main <b>tracking</b> <b>cameras,</b> we describe several modifications to the tracking algorithms, such as inter-frame interpolation, the replace-ment of the least-squares adjustment by a Kalman filter and the integration of rolling-shutter compensation. To support the quick setup of mobile phones while the tracking system is running, the system is extended by an on-line calibration technique that determines the extrinsic camera parameters without requiring a dedicated calibration step...|$|R
