470|10000|Public
5|$|As {{the largest}} Canadian cities {{grew in the}} 1950s and 1960s, the volume of mail passing through the country's postal system also grew, to billions of items by the 1950s and <b>tens</b> <b>of</b> <b>billions</b> <b>of</b> items by the mid-1960s. Consequently, it became {{progressively}} more difficult for employees who handsorted mail to memorize and keep track of all the individual letter-carrier routes within each city. New technology that allowed mail to be delivered faster {{also contributed to the}} pressure for these employees to properly sort the mail.|$|E
5|$|Upon {{taking office}} in 2001, Bush stated his {{opposition}} to the Kyoto Protocol, an amendment to the United Nations Framework Convention on Climate Change which seeks to impose mandatory targets for reducing greenhouse gas emissions, citing that the treaty exempted 80percent of the world's population and would have cost <b>tens</b> <b>of</b> <b>billions</b> <b>of</b> dollars per year. He also cited that the Senate had voted 95–0 in 1997 on a resolution expressing its disapproval of the protocol.|$|E
5|$|In roughly 5billion years, the Sun will {{cool and}} expand outward to many times its current {{diameter}} (becoming a red giant), before casting off its outer layers as a planetary nebula and {{leaving behind a}} stellar remnant known as a white dwarf. In the far distant future, the gravity of passing stars will gradually reduce the Sun's retinue of planets. Some planets will be destroyed, others ejected into interstellar space. Ultimately, {{over the course of}} <b>tens</b> <b>of</b> <b>billions</b> <b>of</b> years, {{it is likely that the}} Sun will be left with none of the original bodies in orbit around it.|$|E
40|$|Observations of the {{expansion}} rate of the universe at late times disagree {{by a factor of}} 1. 5 - 2 with the prediction of homogeneous and isotropic models based on ordinary matter and gravity. We discuss how the departure from linearly perturbed homogeneity and isotropy due to structure formation could explain this discrepancy. We evaluate {{the expansion}} rate in a dust universe which contains non-linear structures with a statistically homogeneous and isotropic distribution. The expansion rate is found to increase relative to the exactly homogeneous and isotropic case by a factor of 1. 1 - 1. 3 at some <b>tens</b> <b>of</b> <b>billion</b> <b>of</b> years. The timescale follows from the cold dark matter transfer function and the amplitude of primordial perturbations without additional free parameters. Comment: 6 pages, 1 figure. Awarded Honorable Mention in the 2008 Gravity Research Foundation essay competition. More extended treatment of the topics can be found in arXiv: 0801. 2692 v...|$|R
50|$|Whatever the {{proportion}} of stars with planets, {{the total number of}} exoplanets must be very large. Because the Milky Way has at least 200 billion stars, it must also contain <b>tens</b> or hundreds <b>of</b> <b>billions</b> <b>of</b> planets.|$|R
30|$|We {{validate}} the proposed algorithms using two real-world datasets on a distributed shared-memory computation platform. Numerical results {{demonstrate that the}} asynchronous distributed stochastic gradient descent algorithms achieve nearly linear computational speedups {{with respect to the}} number of computational threads and are able to complete an adjacency matrix <b>of</b> <b>ten</b> <b>billions</b> <b>of</b> entries within 10 s.|$|R
25|$|According to the EPA, mining has {{contaminated}} {{portions of}} the headwaters of over 40 percent of watersheds in the western continental U.S., and reclamation of 500,000 abandoned mines in 32 states could cost <b>tens</b> <b>of</b> <b>billions</b> <b>of</b> dollars.|$|E
25|$|Worlds {{represent}} {{a wide spectrum}} of conditions, from barren planetoid moons to large gas giant worlds, from uncolonized territory to planets with <b>tens</b> <b>of</b> <b>billions</b> <b>of</b> people. Most worlds tend to be only modestly colonized, though some worlds may be dangerously overcrowded.|$|E
25|$|Big FFTs: With the {{explosion}} of big data in fields such as astronomy, the need for 512k FFTs has arisen for certain interferometry calculations. The data collected by projects such as MAP and LIGO require FFTs of <b>tens</b> <b>of</b> <b>billions</b> <b>of</b> points. As this size does not fit into main memory, so called out-of-core FFTs are an active area of research.|$|E
5|$|The {{more massive}} the star, the shorter its lifespan, {{primarily}} because massive stars have greater pressure on their cores, {{causing them to}} burn hydrogen more rapidly. The most massive stars last an average of a few million years, while stars of minimum mass (red dwarfs) burn their fuel very slowly and can last <b>tens</b> to hundreds <b>of</b> <b>billions</b> <b>of</b> years.|$|R
40|$|Media {{applications}} such as image processing, signal processing, and graphics require <b>tens</b> to hundreds <b>of</b> <b>billions</b> <b>of</b> arithmetic operations per second of sustained performance for real-time application rates, yet also have tight power constraints in many systems. For this reason, these applications often use special-purpose (fixed-function) processors, such as graphics processors in desktop systems. These processors provide several orders of magnitude higher performance efficiency (performance per unit area and performance per unit power) than conventional programmable processors...|$|R
50|$|So for example, if the {{population}} {{be on the}} order <b>of</b> <b>tens</b> <b>of</b> <b>billions,</b> so that k = 10, and we observe n = 10 results without success, then the expected proportion in {{the population}} is approximately 0.43%. If the population is smaller, so that n = 10, k = 5 (<b>tens</b> <b>of</b> thousands), the expected proportion rises to approximately 0.86%, and so on. Similarly, {{if the number of}} observations is smaller, so that n = 5, k = 10, the proportion rise to approximately 0.86% again.|$|R
25|$|There {{are many}} {{different}} types of energy efficiency innovation, including efficient water heaters; improved refrigerators and freezers; advanced building control technologies and advances in heating, ventilation, and cooling (HVAC); smart windows that adapt to maintain a comfortable interior environment; new building codes to reduce needless energy use; and compact fluorescent lights. Improvements in buildings alone, where over sixty percent of all energy is used, can save <b>tens</b> <b>of</b> <b>billions</b> <b>of</b> dollars per year.|$|E
25|$|Vinyl {{chloride}} {{liquid is}} fed to polymerization reactors {{where it is}} converted from a monomer to a polymer PVC. The final product of the polymerization process is PVC in either a flake or pellet form. From its flake or pellet form PVC is sold to companies that heat and mold the PVC into end products such as PVC pipe and bottles. <b>Tens</b> <b>of</b> <b>billions</b> <b>of</b> pounds of PVC are sold on the global market each year.|$|E
25|$|In 2015, the UK Government {{introduced}} a new law intended to penalise Google and other large multinational corporations' artificial tax avoidance. Google is accused of avoiding paying <b>tens</b> <b>of</b> <b>billions</b> <b>of</b> dollars of tax through a convoluted scheme of inter-company licensing agreements and transfers to tax havens. Schmidt was also criticised for his inaccurate {{use of the term}} 'capitalism' to describe billions of dollars being transferred into tax havens where no economic activity was actually taking place.|$|E
5000|$|Cemp Investments Ltd. was {{the primary}} holding company and {{investment}} vehicle for the four children of Samuel Bronfman: Charles Bronfman, Edgar Bronfman, Aileen [...] "Minda" [...] Bronfman de Gunzburg, and Phyllis Lambert, {{also known as the}} Montreal branch of the Bronfman family. During its five decade existence, Cemp became one of the largest privately owned companies in Canada. At its peak it controlled assets worth <b>tens</b> <b>of</b> <b>billions</b> in dollars <b>of</b> assets in major distilling, commercial real estate development, oil and gas, and entertainment companies across North America.|$|R
40|$|LT) (Bohm-Vitense 1958), and the Full Spectrum of Turbulence (FST) models by Canuto and Mazzitelli (1991). Other {{attempts}} to model convection more in detail (e. g. {{in two or}} three dimensions, with consideration of rotation and/or magnetic fields) are limited to the solar case, while the more widely used Large Eddy Simulations (LES) are not yet able to deal with more than a few <b>tens</b> <b>of</b> the <b>billions</b> <b>of</b> scales of turbulence present in a star (see Canuto and Christensen Dalsgaard 1997). The first main input of a convection model is the convective flux: the MLT is a "one eddy" theory: it adopts a phenomenological description of convection based on the hypothesis that the convective energy is carried by convective elements of a unique size, that in the end dissolve into their surroundings. The FST employs the results of the computation of the whole energy distribution of the convective eddies. The MLT fluxes are much smaller than the experimental ones (Castai...|$|R
30|$|This paper proposes two {{asynchronous}} distributed stochastic {{gradient descent}} algorithms {{to solve the}} link prediction problem defined over a large-scale signed network. The link prediction problem is formulated a matrix factorization model, which aims to complete the low-rank adjacency matrix. The two proposed distributed stochastic gradient descent algorithms, one is fully asynchronous {{and the other is}} partially asynchronous, are shown to be powerful tools to solve this problem on a shared-memory computation platform. Numerical experiments on two real-world large-scale datasets demonstrate that the two proposed asynchronous distributed algorithms have nearly linear computational speedups, and are able to complete an adjacency matrix <b>of</b> <b>ten</b> <b>billions</b> <b>of</b> entries within 10 s.|$|R
25|$|All {{proposed}} {{solar radiation}} management techniques require implementation on a relatively large scale, in order to impact the Earth's climate. The least costly proposals are budgeted at <b>tens</b> <b>of</b> <b>billions</b> <b>of</b> US dollars annually. Space sunshades would cost far more. Who was to bear the substantial costs of some climate engineering techniques {{may be hard to}} agree. However, the more effective {{solar radiation management}} proposals currently appear to have low enough direct implementation costs that {{it would be in the}} interests of several single countries to implement them unilaterally.|$|E
25|$|Following the 2011 Japanese Fukushima nuclear disaster, {{authorities}} {{shut down}} the nation's 54 nuclear power plants, {{but it has been}} estimated that if Japan had never adopted nuclear power, accidents and pollution from coal or gas plants would have caused more lost years of life. As of 2013, the Fukushima site remains highly radioactive, with some 160,000 evacuees still living in temporary housing, and some land will be unfarmable for centuries. The difficult Fukushima disaster cleanup will take 40 or more years, and cost <b>tens</b> <b>of</b> <b>billions</b> <b>of</b> dollars.|$|E
25|$|The loss of 30% of the country's {{generating}} capacity led to much greater reliance on {{liquified natural gas}} and coal. Unusual conservation measures were undertaken. In the immediate aftermath, nine prefectures served by TEPCO experienced power rationing. The government asked major companies to reduce power consumption by 15%, and some shifted their weekends to weekdays to smooth power demand. Converting to a nuclear-free gas and oil energy economy would cost <b>tens</b> <b>of</b> <b>billions</b> <b>of</b> dollars in annual fees. One estimate is that even including the disaster, more lives would have been lost if Japan had used coal or gas plants instead of nuclear.|$|E
25|$|Stars {{greater than}} 8solar masses (M⊙) will likely end {{their lives in}} {{dramatic}} supernovae explosions, while planetary nebulae seemingly only occur {{at the end of}} the lives of intermediate and low mass stars between 0.8M⊙ to 8.0M⊙. Progenitor stars that form planetary nebulae will spend most of their lifetimes converting their hydrogen into helium in the star's core by nuclear fusion at about 15million K. This generated energy creates outward pressure from fusion reactions in the core, equally balancing the crushing inward pressures of the star's gravity. Hence, all single intermediate to low-mass stars on the main sequence can last for <b>tens</b> <b>of</b> millions to <b>billions</b> <b>of</b> years.|$|R
40|$|Maximum Entropy (MaxEnt) {{language}} models [1, 2] are {{linear models}} that are typically regularized via well-known L 1 or L 2 {{terms in the}} likelihood objective, hence avoiding {{the need for the}} kinds of backoff or mixture weights used in smoothed n-gram language models using Katz backoff [3] and similar tech-niques. Even though backoff cost is not required to regularize the model, we investigate the use of backoff features in Max-Ent models, as well as some backoff-inspired variants. These features are shown to improve model quality substantially, as shown in perplexity and word-error rate reductions, even in very large scale training scenarios <b>of</b> <b>tens</b> or hundreds <b>of</b> <b>billions</b> <b>of</b> words and hundreds of millions of features. Index Terms: maximum entropy modeling, language model-ing, n-gram models, linear model...|$|R
40|$|Current {{predictions}} {{from industry}} envision {{that within a}} decade, the Internet will bepopulated by <b>tens</b> <b>of</b> <b>billion</b> <b>of</b> devices. Already today, smart Internet devices havesensors that provide an enormous potential for creating new applications. The chal-lenge at hand is how this information can be shared on the future Internet in order tounlock the full capability of applications {{to interact with the}} real world. Therefore,there is an urgent need for scalable and agile support for connecting people, placesand artifacts in applications via a vast number of devices and sensors on the futureInternet. Clearly, this poses a challenge of sharing and thus storage of so-called con-text information. Beyond scalable context storage lays another challenge to identifyand locate devices that are important to the user. In a support for <b>billion</b> <b>of</b> contin-uously changing sensors and actuators, a search engine would not work. Thereforean intelligent way to group devices is required. This thesis deals with mainly three issues: Firstly, propose a method for devicesto be reachable and thus addressable independent of their location in the infrastruc-ture. Secondly, how can the proposed method be used to insure automatic connectiv-ity anywhere between clients and services offered by the device, in particular associ-ated sensors and actuators. Thirdly, how can the grouping and support be combinedand used to dynamically associate sensors from across the Internet with applications,assuming that the aforementioned grouping exists. The proposed solution to the firstissue is to store identifier-locator pairs in an overlay. For the second issue we pro-pose a sensor socket introduced which exploits the identifier/locator pairs to enabledevice mobility. The third issue is addressed by providing a group-cast operation inthe sensor socket. This arrangement allows communication with peers determinedby a grouping algorithm which operates on context information on the context over-lay. Thus we have enabled the creation of automated dynamic clustering of sensorsand actuators in the Internet of Things. The sensor socket is designed as a stand-alone module to support any contextoverlay that provides the same basic functionality. The sensor socket embodiesa support to automatically interconnect and communicate with devices. Using abridging software, remote devices can be dynamically found and inserted into legacylocal area network where current devices can benefit from the connectivity. For fu-ture work the bridge can be extended to actively locate and identify nearby sensorsthat are unable to participate in the overlay network otherwise...|$|R
25|$|With {{the help}} of {{increasing}} oil revenues, Saddam diversified the largely oil-based Iraqi economy. Saddam implemented a national infrastructure campaign that made great progress in building roads, promoting mining, and developing other industries. The campaign helped Iraq's energy industries. Electricity was brought to nearly every city in Iraq, and many outlying areas. Before the 1970s, most of Iraq's people lived in the countryside and roughly two-thirds were peasants. This number would decrease quickly during the 1970s as global oil prices helped revenues to rise from less than a half billion dollars to <b>tens</b> <b>of</b> <b>billions</b> <b>of</b> dollars and the country invested into industrial expansion.|$|E
25|$|Li began two megaprojects {{when he was}} the Premier. He {{initiated}} the construction of the Three Gorges Dam on 14 December 1994, and later began preparations for the Shenzhou Manned Space Program. Both programs were subject to much controversy within China and abroad. The Shenzhou program was especially criticized due to its extraordinary cost (<b>tens</b> <b>of</b> <b>billions</b> <b>of</b> dollars) in a country that sometimes referred to itself as a Third World nation. Many economists and humanitarians suggested that those billions in capital might be better invested in helping the Chinese population deal with economic hardships and improvement in the China's education, health services, and legal system.|$|E
25|$|After the May 2011 {{death of}} Osama bin Laden in Pakistan, many {{prominent}} Afghan figures were assassinated. Afghanistan–Pakistan border skirmishes intensified and many large scale attacks by the Pakistan-based Haqqani Network also took place across Afghanistan. The United States blamed rogue {{elements within the}} Pakistani government for the increased attacks. The U.S. government spent <b>tens</b> <b>of</b> <b>billions</b> <b>of</b> dollars on development aid over 15 years and over a trillion dollars on military expenses during the same period. Corruption by Western defense and development contractors and associated Afghans reached unprecedented levels {{in a country where}} the national GDP was often only a small fraction of the U.S. government's annual budget for the conflict.|$|E
50|$|Stars {{greater than}} 8 solar masses (M⊙) will likely end {{their lives in}} {{dramatic}} supernovae explosions, while planetary nebulae seemingly only occur {{at the end of}} the lives of intermediate and low mass stars between 0.8 M⊙ to 8.0 M⊙. Progenitor stars that form planetary nebulae will spend most of their lifetimes converting their hydrogen into helium in the star's core by nuclear fusion at about 15 million K. This generated energy creates outward pressure from fusion reactions in the core, equally balancing the crushing inward pressures of the star's gravity. Hence, all single intermediate to low-mass stars on the main sequence can last for <b>tens</b> <b>of</b> millions to <b>billions</b> <b>of</b> years.|$|R
40|$|SUMMARY: Many {{landfills}} in Sweden {{as well as}} in Europe have to {{be closed}} in the near future. Apart from material costs in the order <b>of</b> <b>tens</b> <b>of</b> <b>billions</b> Euro, this puts a strain on the environ-ment through the exploitation of virgin materials. Many landfill operators are considering alter-native cover constructions in order to reduce resource spending. In this study, the authors are looking at a construction including slags from a steel mill. Four electric arc furnace (EAF) slags and one ladle slag from Uddeholm Tooling AB at Hagfors, about 270 km Northwest of Stock-holm, were investigated with regard to their physical properties. A full scale field test on an area of about 5, 000 m 2 will be started at the local municipal landfill in Hagfors in August 2005. 1...|$|R
40|$|Cascading {{failures}} {{are one of}} {{the main}} reasons for blackouts in electric power transmission grids. The economic cost of such failures is in the order <b>of</b> <b>tens</b> <b>of</b> <b>billion</b> dollars annually. The loading level of power system is a key aspect to de-termine the amount of the damage caused by cascading failures. Existing studies show that the blackout size exhibits phase transitions as the loading level increases. This paper investigates the impact of the topology of a power grid on phase transi-tions in its robustness. Three spectral graph metrics are considered: spectral radius, effective graph resistance and algebraic connectivity. Experimental results from a model of cascading failures in power grids on the IEEE power systems demon-strate the applicability of these metrics to design/optimise a power grid topology for an enhanced phase transition behaviour of the system. ...|$|R
25|$|The {{degree and}} extent of damage caused by the {{earthquake}} and resulting tsunami were enormous, {{with most of the}} damage being caused by the tsunami. Video footage of the towns that were worst affected shows little more than piles of rubble, with almost no parts of any structures left standing. Estimates {{of the cost of the}} damage range well into the <b>tens</b> <b>of</b> <b>billions</b> <b>of</b> US dollars; before-and-after satellite photographs of devastated regions show immense damage to many regions. Although Japan has invested the equivalent of billions of dollars on anti-tsunami seawalls which line at least 40% of its 34,751km (21,593mi) coastline and stand up to 12m (39ft) high, the tsunami simply washed over the top of some seawalls, collapsing some in the process.|$|E
25|$|The {{early history}} of New Orleans was one of {{uninterrupted}} growth. In the 1850 census, New Orleans ranked as the 6th largest city in the United States, with a population reported as 168,675. It was the only city in the South with over 100,000 people. By 1840 New Orleans had the largest slave market in the nation, which contributed greatly to its wealth. During the antebellum years, two-thirds {{of the more than}} one million slaves who moved from the Upper South in forced migration to the Deep South were taken in the slave trade. Estimates are that the slaves generated an ancillary economy valued at 13.5 percent of the price per person, generating <b>tens</b> <b>of</b> <b>billions</b> <b>of</b> dollars through the years.|$|E
25|$|Saddam {{borrowed}} <b>tens</b> <b>of</b> <b>billions</b> <b>of</b> {{dollars from}} other Arab {{states and a}} few billions from elsewhere during the 1980s to fight Iran, mainly to prevent the expansion of Shi'a radicalism. However, this had proven to completely backfire both on Iraq and {{on the part of}} the Arab states, for Khomeini was widely perceived as a hero for managing to defend Iran and maintain the war with little foreign support against the heavily backed Iraq and only managed to boost Islamic radicalism not only within the Arab states, but within Iraq itself, creating new tensions between the Sunni Ba'ath Party and the majority Shi'a population. Faced with rebuilding Iraq's infrastructure and internal resistance, Saddam desperately re-sought cash, this time for postwar reconstruction.|$|E
40|$|Cascading {{failures}} are {{the main}} reason blackouts occur in power networks. The economic cost of such failures is in the order <b>of</b> <b>tens</b> <b>of</b> <b>billion</b> dollars annually. In a power network, the cascading failure phenomenon is related to both topological properties (number and types of buses, density of transmission lines and interconnection of components) and flow dynamics (load distribution and loading level). Existing studies most often focus on network topology, and not on flow dynamics. This paper proposes a new metric to assess power network robustness with respect to cascading failures, in particular for cascading effects due to line overloads and caused by targeted attacks. The metric takes both the effect of topological features {{and the effect of}} flow dynamics on network robustness into account, using an entropy-based approach. Experimental verification shows that the proposed robustness metric quantifies a power grid robustness with respect to cascading failures. © 2013 IEEE...|$|R
40|$|Astrophysical {{black holes}} (BHs) operate as engines that convert gravita-tional binding energy of accreting plasmas into intense {{radiation}} (1) and release BH spin energy (2 – 4) into powerful relativistic jets (5, 6). Rela-tivistic jets from accreting BHs are commonly observed {{to emerge from}} active galactic nuclei (AGN) or quasars, x-ray binaries as microquasars, and gamma-ray burst (GRB) events. GRB jets allow one to probe the earliest epochs of star formation, whereas radiation and jets from AGN play a direct dynamical role via feedback that suppresses star formation in their host galaxies (7). BHs are also intrinsically interesting because they act as laboratories for probing Einstein’s general relativity theory and for testing theories about accreting BHs and jets. Astrophysical BHs are characterized pri-marily by their mass (M) and dimensionless spin angular momentum (j). BHs have been measured to have masses <b>of</b> <b>tens</b> to <b>billions</b> <b>of</b> solar masses, like M 87 ’s BH with M ∼ 6 × 109 M ◉ (8). Spins have also bee...|$|R
40|$|Triadic {{analysis}} {{encompasses a}} useful set of graph mining methods that are {{centered on the}} concept of a triad, which is a subgraph of three nodes. Such methods are often applied in the social sciences as well as many other diverse fields. Triadic methods commonly operate on a triad census that counts the number of triads of every possible edge configuration in a graph. Like other graph algorithms, triadic census algorithms do not scale well when graphs reach <b>tens</b> <b>of</b> millions to <b>billions</b> <b>of</b> nodes. To enable the triadic analysis of large-scale graphs, we developed and optimized a triad census algorithm to efficiently execute on shared memory architectures. We then conducted performance evaluations of the parallel triad census algorithm on three specific systems: Cray XMT, HP Superdome, and AMD multi-core NUMA machine. These three systems have shared memory architectures but with markedly different hardware capabilities to manage parallelism...|$|R
