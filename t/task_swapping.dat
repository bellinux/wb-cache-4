11|45|Public
40|$|In {{this paper}} we propose <b>task</b> <b>swapping</b> {{networks}} for task reassignments by using task swappings in distributed systems. Some classes of task reassignments are achieved by using iterative local task swappings between software agents in distributed systems. We use group-theoretic methods to find a minimum-length sequence of adjacent task swappings needed from a source task assignment to a target task assignment in a <b>task</b> <b>swapping</b> network of several well-known topologies. Comment: This is a preprint of a paper whose final and definite form is published in: Int. J. Comput. Math. 90 (2013), 2221 - 2243 (DOI: 10. 1080 / 00207160. 2013. 772985...|$|E
40|$|In {{this paper}} we analyze and extend a {{recently}} developed "task-swapping procedure" for improving schedules in oversubscribed situations. In such situations, there are tasks which cannot be directly {{added to the}} current schedule without introducing capacity conflicts. A schedule is improved if {{one or more of}} these tasks can be feasibly included, and the goal of <b>task</b> <b>swapping</b> is to rearrange some portion of the current schedule to make this possible. Key to effective <b>task</b> <b>swapping</b> is an ability to exploit the scheduling flexibility inherent in the constraints associated with various scheduled tasks, and previous work has shown that the use of retraction heuristics that favor tasks with greater rescheduling flexibility can give rise to strong schedule improvement capabilities...|$|E
40|$|In {{this paper}} we propose a market-like {{negotiation}} protocol which extends the wellknown Contract Net Protocol to cope with domains where task swap is the only possible type of contract. The agents we consider are heterogeneous and self-interested. The absence of an explicit support for utility transfer determines interesting implications on efficiency and stability; in particular, we propose different design alternatives induced by different compositions of the announcement and the bid, and discusses the strategies supported. <b>Task</b> <b>swapping</b> is more feasible than task selling for many realistic applications; symbolic path planning in autonomous robotic agents is the application on which we focus the description of our approach. ...|$|E
50|$|Detour: A {{choice between}} two tasks. Teams {{are free to}} choose either <b>task</b> or <b>swap</b> <b>tasks</b> if they find one option too difficult.|$|R
5000|$|Detours: A {{choice of}} two tasks. Teams {{are free to}} choose either <b>task</b> or <b>swap</b> <b>tasks</b> if they find one option too difficult. There is {{generally}} one Detour present on each leg of the race.|$|R
40|$|Symbolic path {{planning}} in autonomous robotic agents is the search, {{on a network}} of places and routes modeling the environment, for a sequence of routes which enables an agent to execute a set of inter-related tasks. When several agents {{are present in the}} environment, each having been assigned a set of tasks, the adoption of coordination mechanisms allows the task-execution costs to be decreased. The formulation of tasks includes resources, resource types, temporal and precedence constraints, production and use of objects. In this paper we propose a market-like negotiation protocol which extends the well-known Contract Net Protocol to domains where <b>task</b> <b>swap</b> is the only possible type of contract. After planning an initial path to execute its tasks, each agent uses this protocol to progressively decrease the path cost by <b>swapping</b> <b>tasks</b> with other agents. The agents we consider are heterogeneous and self-interested, and communicate with each other by broadcasting messages with a limited radius...|$|R
40|$|Walker, J., Wilson, M. S. (2011). Task Allocation for Robots using {{inspiration}} from hormones. Adaptive Behavior, 19 (3), 208 - 224. This paper describes a robotic system which uses evolution to continuously adapt {{a group of}} heterogeneous robots to their current environment while assigning tasks to these robots using an endocrine based system. The tasks are allocated dependent on the robots' current ability to perform the task and whether the task is being done by another robot. A series of experiments is presented taking the work from an evolutionary training phase, through simulation trials, to experiments on real robots. The real robot trials show <b>task</b> <b>swapping</b> dependent on the robots' ability to perform each task. Peer reviewe...|$|E
40|$|Abstract—This paper {{describes}} Pica, a fine-grain, message-passing architecture {{designed to}} efficiently support high-throughput, low-memory parallel applications, such as image processing, object recognition, and data compression. By specializing the processor and reducing local memory (4, 096 36 -bit words), multiple nodes {{can be implemented}} on a single chip. This allows highperformance systems for high-throughput applications to be realized at lower cost. The architecture minimizes overhead for basic parallel operations. An operand-addressed context cache and round-robin task manager support fast <b>task</b> <b>swapping.</b> Fixed-sized activation contexts simplify storage management. Word-tag synchronization bits provide low-cost synchronization. Several applications {{have been developed for}} this architecture, including thermal relaxation, matrix multiplication, JPEG image compression, and Positron Emission Tomography image reconstruction. These applications have been executed using an instrumented instruction-level simulator. The results of these experiments and an evaluation of Pica’s architectural features are presented. Index Terms—Fine-grain parallelism, image processing architectures, through-wafer interconnects, MIMD architectures. ...|$|E
40|$|Abstract — Until recently, {{there has}} been a lack of methods to {{trade-off}} energy use for quality of service at run-time in stand-alone embedded systems. Such systems are motivated by the need to increase the apparent available battery energy of portable devices, with minimal compromise in quality. The available systems either drew too much power or added considerable overheads due to <b>task</b> <b>swapping.</b> In this paper, we demonstrate a feasible method to perform these trade-offs. This work has been enabled by a lowimpact power/energy estimating processor which utilizes counters to estimate power and energy consumption at run-time. Techniques are shown that modify multimedia applications to differ the fidelity of their output to optimize the energy/quality trade-off. Two adaptation algorithms are applied to multimedia applications demonstrating the efficacy of the method. The method increases code size by 1 % and execution time by 0. 02 %, yet is able to produce an output which is acceptable and processes up to double the number of frames...|$|E
50|$|On 26 February 2004, Kaleri and NASA {{astronaut}} Michael Foale floated {{outside the}} ISS from the Pirs Docking Compartment airlock. The spacewalk began at 4:17 EST, and Foale and Kaleri {{were able to}} complete {{all but two of}} their planned <b>tasks,</b> <b>swapping</b> out experiment packages and mounting an instrumented dummy torso on the station's hull to measure the radiation environment faced by spacewalking astronauts. Although the spacewalk was originally planned for 5 hours and 30 minutes, it was cut short due to a cooling system malfunction in Kaleri's spacesuit. Kaleri reported seeing drops of water on the inside of his spacesuit's visor. The spacewalk lasted 3 hours and 55 minutes. The excursion was the fifth career spacewalk for Kaleri.|$|R
40|$|This paper {{introduces}} Pica, a fine-grain, {{message passing}} architecture designed to efficiently support high-throughput parallel applications. This focus on high-throughput applications allows a small local memory of 4096 36 -bit words. The architecture minimizes overhead for basic parallel operations. An operand-addressed context cache and round-robin task manager allow single cycle <b>task</b> <b>swaps.</b> Fixed-sized activation contexts simplify storage management. Word-tag synchronization bits provide low-cost synchronization. Several applications {{have been developed}} for this architecture including thermal relaxation, matrix multiplication, JPEG image compression, and positron emission tomography image reconstruction. These applications have been implemented and executed on the Pica architecture using an instrumented instruction-level simulator. Using these results, the architectural features of Pica are evaluated. 1 Introduction There are many approaches to exploiting fine-grain parallelism: in [...] ...|$|R
40|$|This paper {{introduces}} Pica, a fine-grain, {{message passing}} architecture designed to efficiently support high-throughput parallel applications. The architecture minimizes overhead for basic parallel operations. An operand-addressed context cache and round-robin task manager allow single cycle <b>task</b> <b>swaps.</b> Fixed-sized activation contexts simplify storage management. Wordtag synchronization bits provide low-cost synchronization. The focus on high-throughput applications allows a small local memory (1024 36 -bit words). A complete node (including memory) {{can be implemented}} using {{a fraction of a}} chip. A multi-node chip prototype (four nodes/chip) is being designed. In order to meet chip I/O requirements, a high-bandwidth, threedimensional optical network is also being designed. Using recent developments in epitaxial liftoff of optoelectronic devices and through-chip transmission, a network is presented that provides 3. 2 Gbits/sec offchip bandwidth. 1 High-Bandwidth Parallelism There are many [...] ...|$|R
40|$|This paper {{introduces}} {{an extension}} to the RMS scheduling technique {{that we call}} "Hot Swapping". Hot Swapping enables a system to choose between various selected implementations of one task on-the-fly and thus to optimize the system's cost (e. g. power savings). The on-the-fly swapping between those implementations requires extra time to save and/or transform states of a certain task implementation. Even if the two steady-state schedules {{before and after the}} swapping are feasible, the transient schedule with the additional swapping computation time may exceed the system's capacity. Our technique is an extension to Rate Monotonic Scheduling (RMS). While maintaining and meeting performance requirements, our technique shows an average reduction of 31 % in power consumption compared to systems using a pure static scheduling approach (RMS) that cannot make use of <b>task</b> <b>swapping.</b> We have evaluated our algorithm through simulation of five real-world task sets and in addition by use {{of a large number of}} generated task sets...|$|E
40|$|Until recently, {{there has}} been a lack of methods to {{trade-off}} energy use for quality of service at run-time in stand-alone embedded systems. Such systems are motivated by the need to increase the apparent available battery energy of portable devices, with minimal compromise in quality. The available systems either drew too much power or added considerable overheads due to <b>task</b> <b>swapping.</b> In this paper we demonstrate a feasible method to perform these trade-offs. This work has been enabled by a low-impact power/energy estimating processor which utilizes counters to estimate power and energy consumption at run-time. Techniques are shown that modify multimedia applications to differ the fidelity of their output to optimize the energy/quality trade-off. Two adaptation algorithms are applied to multimedia applications demonstrating the efficacy of the method. The method increases code size by 1 % and execution time by 0. 02 %, yet is able to produce an output which is acceptable and processes up to double the number of frames. 1...|$|E
40|$|In {{this paper}} we reconsider a “task-swapping ” {{procedure}} for improving schedules {{in the face}} of resource oversubscription. Prior work has demonstrated that use of a retraction heuristic to determine which tasks to rearrange in an existing schedule allows for addition of new tasks which would otherwise fail to be scheduled. The existing task swap procedure employs a variable ordering heuristic for task insertion that {{is the same as the}} retraction heuristic, but scored in the reverse. That is, the least constrained tasks are retracted, and of these the most constrained are committed first. Value selection for commitment is defaulted to a task’s earliest feasible start time. We have found that by applying a value selection heuristic, maxavailability, to the choice of where to assign the retracted tasks, both solution quality and runtime performance of task swap can be improved greatly. Max-availability considers resource contention for an unassigned task and places it where availability is predicted to be maximal over the range of that task. This heuristic is applicable not only in a repair context, but can also promote resource levelling in the context of constructive task allocation. Finally, we show that use of max-availability in <b>task</b> <b>swapping</b> promotes schedule stability when compared to the prior greedy task insertion policy...|$|E
40|$|Sharing {{capacity}} across decentralized {{service providers}} {{can lead to}} improvements in logistics efficiency. However, sharing the information needed to determine if capacity can be shared poses interesting problems. We focus on a problem faced by independent trucking companies that have separate pickup and delivery tasks; the identification of potential efficiency enhancing <b>task</b> <b>swaps</b> between these companies. A second goal is to limit the information the parties must reveal to identify these swaps. By integrating state-of-the-art techniques from cryptography into an operations context we present an algorithm that finds opportunities to swap loads without revealing any information except the loads swapped, along with proofs of the security. We then apply this algorithm to an empirical dataset from a large transportation company and present results that suggest significant opportunities to improve efficiency through Pareto improving swaps. 1...|$|R
25|$|The third series {{consists}} of sixteen candidates in total. As {{with the two}} previous series, the candidates were inititally separated into two teams based on gender. The women chose the name 'Fusion', while the men chose the name 'Elev8'. In the first <b>task,</b> Bill <b>swapped</b> the project manager of the women's team {{with that of the}} men's before commencing the task.|$|R
5000|$|In {{the year}} 2000, Eclipse Press, the {{publishing}} {{arm of the}} industry magazine The Blood-Horse {{began a series of}} books about the greatest horses in the history of American racing. This was called the Thoroughbred Legends Series. In 2002, Barry Irwin was invited to write the Legends Series book [...] "Swaps; the California Comet" [...] which told the story of the 1955 Kentucky Derby winner. Irwin eagerly accepted the <b>task</b> since <b>Swaps</b> was his favorite racehorse of all time.|$|R
40|$|We {{consider}} a multiagent system {{that consists of}} heterogeneous groups of homogeneous agents. Instead of defining a global task for the whole team, each agent is assigned a local task as syntactically cosafe linear temporal logic formulas that specify both motion and action requirements. Interagent dependence is introduced by collaborative actions, of which the execution requires multiple agents' collaboration. To ensure the satisfaction of all local tasks without central coordination, we propose a bottom-up motion and task coordination strategy that contains an off-line initial plan synthesis and an online coordination scheme based on real-time exchange of request and reply messages. It facilitates not only the collaboration among heterogeneous agents but also the <b>task</b> <b>swapping</b> between homogeneous agents to reduce the total execution cost. It is distributed as any decision is made locally by each agent based on local computation and communication within neighboring agents. It is scalable and resilient to agent failures as the dependence is formed and removed dynamically based on agent capabilities and their plan execution status, instead of preassigned agent identities. The overall scheme is demonstrated by a simulated scenario of 20 agents with loosely coupled local tasks. the Swedish Research Council (VR) H 2020 ERC Starting Grant BUCOPHSYS EU STREP RECONFIG: FP 7 -ICT- 2011 - 9 - 600825. QC 20170614 </p...|$|E
40|$|Several {{objective}} functions {{have been}} proposed in the literature to smooth (equalize) work load variations in mixed-model serial assembly lines. In this paper we modified and tested three mixed-model serial line objective functions for the mixed-model, U-shaped assembly line. These objective functions are the absolute deviation from cycle time (ADC), the maximum deviation from cycle time (MDC) and {{the sum of the}} cycle time violations (SCV). We compared these objective functions using the well-known smoothness index. We employed a four step smoothing process from the literature and swapped tasks between workstations to minimize our objective functions. Our <b>task</b> <b>swapping</b> procedure was guided by the great deluge algorithm heuristic. We tested three problem sizes of 19, 61 and 111 tasks, and a variety of cycle times and model sequences for a total of 68 problems. We analyzed our results using the non-parametric Friedman?s test in conjunction with the multiple-comparison for use with Friedman test. Our results indicated that across all problems the ADC and the MDC performed significantly better than the MDC. But, when we controlled for high and low cycle times, the MDC was significantly better than both ADC and SCV at low cycle times, while the ADC and SCV were significantly better than the MDC at high cycle times. From the results of this experiment we can conclude that the smoothness index is influenced by both the objective function and the cycle time in the mixed-model, U-shaped assembly line balancing problem. (Assembly Line Balancing, Mixed-Model Production, U-Lines, Layout...|$|E
40|$|Topology aware mapping {{has started}} to attain {{interest}} again {{by the development of}} supercomputers whose topologies consist of thousands of processors with large diameters. In such parallel architectures, it is possible to obtain performance improvements for the executed parallel programs via careful mapping of tasks to processors by considering properties of the underlying topology and the communication pattern of the mapped program. One of the most widely used metric for capturing a parallel program’s communication overhead is the hop-bytes metric which takes the processor topology into account which is in contrast to the assumptions made by the wormhole routing. In this work, we propose a KL-based iterative improvement heuristic for mapping tasks of a given program to the processors of the parallel architecture where the objective is the reduction of the communication volume that is modeled with the hop-bytes metric. We assume that the communication pattern of the program is known beforehand and the processor topology information is available. The algorithm basically tries to improve a given initial mapping with a number of successive <b>task</b> <b>swaps</b> defined within a given processor neighborhood. We test our algorithm for different number of tasks and processors and demonstrate its results by comparing it to random mapping, which is widely used in recent supercomputers...|$|R
40|$|The multi-agent path-finding (MAPF) {{problem has}} {{recently}} {{received a lot}} of attention. However, it does not capture important characteristics of many real-world domains, such as automated warehouses, where agents are constantly engaged with new tasks. In this paper, we therefore study a lifelong version of the MAPF problem, called the multi-agent pickup and delivery (MAPD) problem. In the MAPD problem, agents have to attend to a stream of delivery tasks in an online setting. One agent has to be assigned to each delivery task. This agent has to first move to a given pickup location and then to a given delivery location while avoiding collisions with other agents. We present two decoupled MAPD algorithms, Token Passing (TP) and Token Passing with <b>Task</b> <b>Swaps</b> (TPTS). Theoretically, we show that they solve all well-formed MAPD instances, a realistic subclass of MAPD instances. Experimentally, we compare them against a centralized strawman MAPD algorithm without this guarantee in a simulated warehouse system. TP can easily be extended to a fully distributed MAPD algorithm and is the best choice when real-time computation is of primary concern since it remains efficient for MAPD instances with hundreds of agents and tasks. TPTS requires limited communication among agents and balances well between TP and the centralized MAPD algorithm. Comment: In AAMAS 201...|$|R
40|$|The {{problem of}} sharing manufacturing, {{inventory}} or capacity {{to improve performance}} is applicable in many decentralized operational contexts. However, solution of such problems commonly requires an intermediary or a broker to manage information security concerns of individual participants. Our goal is to examine use of cryptographic techniques to attain the same result {{without the use of}} a broker. To illustrate this approach, we focus on a problem faced by independent trucking companies that have separate pickup and delivery tasks and wish to identify potential efficiency enhancing <b>task</b> <b>swaps</b> while limiting the information the companies must reveal to identify these swaps. We present an algorithm that finds opportunities to swap loads without revealing any information except the loads swapped, along with proofs of the security of the protocol. We also show that it is incentive compatible for each company to both follow the protocol correctly as well as provide their true data. We apply this algorithm to an empirical dataset from a large transportation company and present results that suggest significant opportunities to improve efficiency through Pareto improving swaps. This paper uses cryptographic arguments in an operations management problem context to show how an algorithm can be proven incentive compatible as well as demonstrate the potential value of its use on an empirical dataset. 1...|$|R
30|$|From {{the point}} of view of task workload, Taskpm[*]>[*]Taskp 1. Since w 1 is the fastest worker, wm is the slowest worker and worker w 1 and wm <b>swap</b> <b>tasks.</b> In essence, the {{switching}} task increases the computation cost of w 1 and reduces the workload of wm, so task allocation is skewed.|$|R
30|$|For example, {{a teacher}} might draw {{an object in}} a {{horizontal}} layout when the instruction target is to sort an array, whereas the teacher might draw the array in a vertical layout for a stack. Changes in visualization policy such as this are derived by fitting the instruction content to the learners’ background knowledge. For example, if the learners sufficiently understand a stack, drawing either object in a horizontal or vertical layout would be acceptable to the learners. Similarly, the teacher would not need to draw the temporary variable in a <b>task</b> that <b>swaps</b> the values of two variables for non-novice learners.|$|R
50|$|Group A housemates are housemates {{initially}} from House A while Group B housemates are housemates initially from House B, {{with the}} exception of Tom, originally part of House A, and Melisa, originally part of House B, who were swapped by Big Brother {{before the end of the}} first weekly task. For the fourth weekly <b>task,</b> the housemates <b>swapped</b> houses, and Rob and Carol swapped groups.|$|R
5000|$|...: No {{nominations}} {{were held}} in Week 4 as all Housemates faced the public vote.*: During Week 6, Gaetano was {{taking part in the}} Big brother swap after winning a <b>task.</b> He was <b>swapped</b> with Cameron from the Big Brother UK House. Because of this, Gaetano was exempt from nominations this week.*: During the Final Week, the public voted to win, rather than to evict.|$|R
40|$|Recent {{interest}} in distributed agent-based control systems for dynamic, unpredictable application domains has been motivated {{largely by the}} increased responsiveness of independent entities that can react locally {{to changes in the}} operating environment. The agent-oriented paradigm for software engineering provides a basis for the construction of extremely large, complex systems, in which components can be naturally distributed across a network of heterogeneous computers, without requiring a complete analysis of their interactions. For all their advantages, such systems are difficult to optimize. Localization of information and computation, limited communication, and diversified goals present significant challenges to the design of systems capable of achieving high levels of global utility. The most common approach to optimizing agent-based systems is some sort of interagent exchange mechanism, usually pair-wise, in which <b>tasks</b> are <b>swapped</b> between agents, subject to the constraint that s [...] ...|$|R
40|$|This paper {{examines}} {{the effects of}} complexity-enhancing manipulations of two cognitive <b>tasks</b> – <b>Swaps</b> and Triplet Numbers tests (Stankov, 2000) – on their relationship with Raven’s Progressive Matrices test representing aspects of fluid intelligence. The complexity manipulations involved four treatment levels, each requiring {{an increasing number of}} components and relationships among these components. The accuracy, speed of processing, and confidence measures were decomposed into experimental and non-experimental parts and represented by the latent variables within a structural equation model. In the fitted model, four latent predictor variables had substantial path coefficients to Raven’s Progressive Matrices test. Experimental accuracy scores for both Swaps and Triplet Numbers tests have significant predictive validity. Thus, complexity-enhancing manipulations affect correlations fluid intelligence captured by the Raven’s test. In addition, two non-experimental latent variables (speed from Triplet Numbers and confidence from Swaps) have significant path coefficients...|$|R
40|$|This paper {{describes}} the scheduling issues speci c to the Tera MTA high performance shared memory multithreaded multiprocessor and presents solutions to classic scheduling problems. The Tera MTA exploits parallelism at all levels, from ne-grained instruction-level parallelism {{within a single}} processor to parallel programming across processors, to multiprogramming among several applications simultaneously. Consequently, scheduling of resources occurs at many levels, and managing these resources poses unique and challenging scheduling concerns. This paper outlines the scheduling algorithms of the user level runtime and operating system and {{describes the}} issues relevant to each. Many of the issues encountered and solutions proposed are novel, given the multithreaded, multiprogrammed nature of our architecture. In particular, we present an algorithm for swapping a set of tasks to and from memory that achieves minimal overhead, largely independent of {{the order in which}} <b>tasks</b> are <b>swapped...</b>|$|R
50|$|Curse of the Azure Bonds follows {{along the}} same style as Pool of Radiance, with the main adventuring action using a first person perspective. The player uses the top left window to view the current location, with the status panel {{on the right and}} the {{commands}} along the bottom. Through these commands, the player can select a wide range of actions and <b>tasks</b> including spell-casting, <b>swapping</b> weapons, or resting and memorizing spells. The player creates an icon for each character, which can be customized to taste.|$|R
40|$|This paper aims {{to explore}} the {{complexities}} of students’ interaction in a collaborative learning task set within a CLIL unit in the EFL classroom. By way of illustration, a sample from a 4 th year classroom grade in a secondary school of Catalonia has been collected. The paper examines, through the students’ outcome, the impact of cooperative learning in a content-rich information <b>swap</b> <b>task</b> and discusses the role of content and language integrated learning in the classroom. This paper concludes with several implications for research and practic...|$|R
5000|$|Multitasking, most {{commonly}} cooperative round-robin scheduling, is normally available (although multitasking words and support {{are not covered}} by the ANSI Forth Standard). The word [...] is used to save the current task's execution context, to locate the next task, and restore its execution context. Each task has its own stacks, private copies of some control variables and a scratch area. <b>Swapping</b> <b>tasks</b> is simple and efficient; as a result, Forth multitaskers are available even on very simple microcontrollers, such as the Intel 8051, Atmel AVR, and TI MSP430.|$|R
40|$|With {{the arrival}} of partial {{reconfiguration}} technology, modern FPGAs support <b>swapping</b> <b>tasks</b> in or out individually at run-time without interrupting other tasks running on the same FPGA. Although, implementing this feature achieves much better flexibility and device utilization, the challenge remains to quickly and efficiently place tasks arriving at run-time on such partially reconfigurable systems. In this paper, we propose an algorithm to handle this on-line, run-time task placement problem. By adding logical constraints on the FPGA and introducing our resources management solution, the simulation results show our algorithm has better overall performances compared with previous reported methods in terms of task rejection number, placement quality 1 and execution time. ...|$|R
50|$|In the mid-to-late 1960s, {{mainframe}} computers, {{such as the}} IBM System/360, had memory sizes from 8 KB to 512 KB. In {{order to}} conserve memory, transients were small modules that supported a specific <b>task,</b> and were <b>swapped</b> {{in and out of}} memory. The operating system for the 360 had two areas reserved for transients that supported input/output operations. These were referred to as the “logical transient area,” and the “physical transient area.” If an application program, for example, needed to use the printer, transients that supported printing were brought into the transient areas. If an application needed to use tape drives, transients that supported tape drive access were brought into the transient areas.|$|R
40|$|Swarm of vehicles, {{instead of}} single or {{small groups of}} vehicles, are {{becoming}} of large interest {{in a variety of}} applications for their intrinsic robustness and flexibility. Probably, the most difficult issue is coordination and control {{of a large number of}} small vehicles with limited processing, communication and power capabilities. A large number of solutions for practical problems have been studied and proposed by researchers worldwide; they are often specific solutions to specific problems that are difficult to generalize. Most of them cannot manage the natural heterogeneity of a large group of vehicles: it is often desired that a swarm performs a complex mission in different phases that may require different specialized vehicles instead of a multi role one. The aim of this thesis is to develop a framework for swarm modeling, decentralized estimation and control capable of managing many of the the application fields proposed and studied in the literature. The environment and mission(s) under consideration are as general as possible, as well as the characteristics of the agents which may be all identical or heterogeneous. The Descriptor Functions Framework, the main topic and contribution of this thesis, associates a specific function to each agent, and one or more functions to the mission and uses an analytical framework for generation of the agents control law. Decentralized control and estimation of the relevant variables needed as feedback are cast into an optimization problem; analysis of equilibria and formal proof of convergence of the estimation and control law proposed are derived. Then, a bio-inspired method for task self-assignment is applied to the Framework; swarm vehicles are given the capability to select the best <b>task</b> to perform, <b>swap</b> <b>task</b> with other agents during the mission, and assess the degree of completeness of the various tasks in order to balance the swarm capabilities among them. Finally an hardware test bed, together with experimental results, is be presented...|$|R
