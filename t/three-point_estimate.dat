5|7|Public
40|$|Abstract: This {{research}} article presents {{the development process}} of a cost-estimating model for railway renewal projects at the early stage of a project life cycle. The model comprises four main stages: creating a project structure that composes the goal, project criteria, and alternatives; col-lecting the necessary data {{in the form of}} pairwise comparisons made by a domain expert; pro-ducing alternative weights using a geometric mean; finally, employing an algorithmic method using the produced alternative weights and the known cost of one alternative per criteria. The practical implications of the developed model are its ability to estimate renewal project costs of railway assets when {{there is a lack of}} quantitative data and detailed project definition. The process provides a transparent and a structured way of formalizing subjective judgments, which produces a <b>three-point</b> <b>estimate</b> as the resulting output. The model is described and vali-dated using a switch and crossing case study...|$|E
40|$|The rapid {{increase}} in volume of digital information can cause concern among organisations regarding manageability, costs and security of their information in the long-term. As cloud computing technology is often used for digital preservation purposes and is still evolving, there is difficulty in determining its long-term costs. This paper presents {{the development of a}} generic cost model for public and private clouds utilisation in long term digital preservation (LTDP), considering the impact of uncertainties and obsolescence issues. The cost model consists of rules and assumptions and was built using a combination of activity based and parametric cost estimation techniques. After generation of cost breakdown structures for both clouds, uncertainties and obsolescence were categorised. To quantify impacts of uncertainties on cost, <b>three-point</b> <b>estimate</b> technique was employed and Monte Carlo simulation was applied to generate the probability distribution on each cost driver. A decision support cost estimation tool with dashboard representation of results was developed...|$|E
40|$|Most {{methods for}} {{estimating}} the total effort {{required for a}} software project (to decide on schedule, staffing, and feasibility) depend {{on the size of}} the software project. Unfortunately, it is difficult to measure size meaningfully, it is difficult to estimate size in advance, and it is difficult to extrapolate from size to what we are really interested in. We will first look at methods for estimating size, then at how size can be used to estimate effort (e. g. using COCOMO). SAPM Spring 2007 : Estimation 2 Three-Point Estimates If you just ask someone for an estimate of how long a task will take, the answers you get will vary enormously, depending on how they interpret the question. Worst case? Best case? Etc. Instead, estimates are typically done using three points: o = Optimistic estimate (best 2. 5 %) m = Most likely estimate p = Pessimistic estimate (worst 2. 5 %) SAPM Spring 2007 : Estimation 3 Using Three-Point Estimates A single estimate can be computed from a <b>three-point</b> <b>estimate</b> as...|$|E
40|$|The main {{objective}} of this thesis was to identify potential improvements of the risk management process to DNV GL {{so that it is}} best fit at identifying risk of time overruns and estimating project duration. Focus was placed on studying the following three aspects of risk management: 1) Scheduling method used to estimate project duration, 2) Quantification of uncertainties and 3) Monte Carlo simulation. The theoretical framework underlying the DNV GL method was identified by comparing the theoretical framework of stochastic scheduling and the DNV GL method. The findings suggest that DNV GL uses a static stochastic scheduling method in order to estimate project duration, which is well suited when uncertainties are inherent in projects. A Bayesian estimation method is used to quantify uncertainties by establishing <b>three-point</b> <b>estimates</b> that define the best, worst and most likely case for uncertain input variables. This method is efficient when there is limited amount of data available to base estimates on. Four risk analyses were carried out for a case study of a navy vessel. The main finding was that the estimate for project duration is likely to be too optimistic, because the schedule is approximately deterministic. When stochastic uncertainties were added to the deterministic input variables in the baseline model, the outcome was stochastic input variables with a variance, which was far too small. This also limited the impact of integrating correlation coefficients into the model. However, the establishment of uncertainties was effective as it caused the project to have a mean delay of about 10, 7 months. The sensitivity to choice of probability distributions used to characterize uncertainties was found to be low, {{with the exception of the}} Trigen distribution. Sensitivity to errors in <b>three-point</b> <b>estimates</b> was found to be significant for extreme values. Due to subjective errors in the assessment of <b>three-point</b> <b>estimates,</b> a Trigen distribution was suggested to characterize these estimates. This probability distribution generated the highest standard deviation amongst the distributions in the case study. DNV GL runs an efficient risk management process with a theoretical framework well fitted for identifying risk and estimating project duration. However, the following recommendations are given: 1. Recommend the customer to establish a stochastic baseline schedule 2. A Trigen distribution (P 10 /P 90) should be used to characterize <b>three-point</b> <b>estimates</b> 3. Introduce a 15 -minute exercise in estimation technique in the workshop 4. Consistency in establishment of correlation coefficient should be a requirement 5. Establish a database and compare estimates to actual result...|$|R
40|$|Aluminium is {{extensively}} {{applied in}} aerospace industry, however, uncertainties lie in all activities of its production. In this paper, risk assessment for cost estimates in aluminium lifecycle is investigated. Through the in-house module, COSTLIPS (COSTing Life-cycle In material ProcesseS), probabilities of typical processes for aluminium production are estimated. Approach of probability distribution functions and <b>three-point</b> <b>estimates</b> {{is used to}} estimate statistical parameters of the costs for different processes. Example {{results show that the}} COSTLIPS can be used for costing of aluminium production processes including risk analysis. This would be useful for material selection and evaluation in aircraft design and manufacture in term of cost estimation. Copyright © 2008 by the American Institute of Aeronautics and Astronautics, Inc...|$|R
40|$|A new {{approach}} to determining the solar galactocentric distance, R_ 0, from the geometry of spiral-arm segments is proposed. Geometric aspects of the problem are analyzed and a simplified <b>three-point</b> method for <b>estimating</b> R_ 0 from objects in a spiral segment is developed {{in order to test}} the proposed approach. An estimate of R_ 0 = 8. 44 ± 0. 45 kpc is obtained by applying the method to masers with measured trigonometric parallaxes, and statistical properties of the R_ 0 estimation from spiral segments are analyzed. Comment: 8 pages, 7 figures. Published in Baltic Astronomy (BA...|$|R
30|$|The {{increasing}} {{adoption of}} gas-fired power plants directly strengthens the coupling between electric power {{and natural gas}} systems. Current industrial practice in optimal power flow for electric power systems has not taken the security constraints of gas systems into consideration, resulting in an overly-optimistic solution. Meanwhile, the operation of electric power and natural gas systems is coupled over multiple periods because of the ramp rate limits of power generators and the slow dynamical characteristics of gas systems. Based on these motivations, we propose a multi-period integrated natural gas and electric power system probabilistic optimal power flow (M-GEPOPF) model, which includes dynamic gas flow models. To address the uncertainties originating from wind power and load forecasting, a probabilistic optimal power flow (POPF) calculation based on a <b>three-point</b> <b>estimate</b> method (3 PEM) is adopted. Moreover, power-to-gas (PtG) units are employed to avoid wind power curtailment and enable flexible bi-directional energy flows between the coupled energy systems. An integrated IEEE RTS 24 -bus electric power system and the Belgian 20 -node natural gas system are employed as a test case to verify {{the applicability of the}} proposed M-GEPOPF model, and to demonstrate the potential economic benefits of PtG units.|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedDecision making in engineering development projects and programs relies on numbers. This quantitative support can involve uncertainty that is frequently characterized by three-point estimates of decision variables. Modeling of these estimates for analysis commonly utilizes the triangular distribution for its simplicity, but errors could be introduced if another distribution model is {{more appropriate for}} the data. This study measures statistics from distribution types ranging from fully flat to narrowly peaked, fitting estimates for all sizes of minimum to maximum ranges and spanning the complete spectrum of asymmetry. The study compares common statistical values for each distribution to an equivalent triangular distribution. It calculates the error size for the mean, high-confidence interval, and coefficient of variation. The study then provides recommendations for when to use a triangular distribution or a different model. The guidelines {{are based on a}} weight factor of the distribution mode and the estimate’s maturity to produce an objective set of guidelines for selecting distribution shapes best suited to model any given <b>three-point</b> <b>estimate.</b> With these guidelines, estimators and modelers can quickly and easily provide a more accurate uncertainty analysis to support decision makers. Civilian, National Aeronautics and Space Administratio...|$|E
40|$|We {{present a}} new method to <b>estimate</b> <b>three-point</b> {{correlations}} in Cosmic Microwave Background maps. Our Fast Fourier Transform based implementation <b>estimates</b> <b>three-point</b> functions using all possible configurations (triangles) at a controlled resolution. The {{speed of the}} technique depends both on the resolution and {{the total number of}} pixels N. The resulting N log N scaling is substantially faster than naive methods with prohibitive N 3 scaling. As an initial application, we measure three-point correlation functions in the First Year Data Release of the Wilkinson Anisotropy Probe. We estimate 336 cross-correlations of any triplet of maps from the 8 differential assemblies, scanning altogether 2. 6 million triangular configurations. Estimating covariances from Gaussian signal plus realistic noise simulations, we perform a null-hypothesis testing with regards to the Gaussianity of the Cosmic Microwave Background. Our main result is that at the three-point level WMAP is fully consistent with Gaussianity. To quantify the level of possible deviations, we introduce false discovery rate analysis, a novel statistical technique to analyze for three-point measurements. This confirms that the data are consistent with Gaussianity at better than 1 -σ level when jointly considering all configurations. We constrain a specific non-Gaussian model using the quadratic approximation of weak non-Gaussianities in terms of the fNLT parameter, for which we construct an estimator from the the three-point function. We find that using the skewness alone is more constraining than a heuristic suboptimal combination of all our results; our best estimate is fNLT = − 110 ± 150 assuming a ΛCDM concordance model. Subject headings: cosmic microwave background — cosmology: theory — methods: statistical...|$|R
40|$|Research in bone {{diseases}} like osteoporosis {{is motivated by}} its immense social impact and health care costs. While there are numerous studies {{about the influence of}} bone mineral density and microarchitectural properties on the mechanical properties of trabecular bone, {{little is known about the}} influence of bone matrix material properties. In this communication, we present novel ways for combining mechanical testing of single trabeculae with imaging on both the micro- and nanoscale to further investigate these material properties. Our results indicate microdamage in an ellipsoid zone on the tension side of the trabeculae tested in <b>three-point</b> bending. We <b>estimated</b> the highest tensile strains in this region to be about 3. 5 %. Quantitatively global whitening versus distance curves correlate well with retrieved force-distance data. Scanning electron microscopy investigations of the microdamaged and optically whitened zones suggest that damage formation happens primarily in the bone and not on the surface. In addition to whitening/microdamage assessment on the microscale, we used atomic force microscopy together with a custom made three-point bending device and a region based digital image correlation tool to obtain quantitative local surface displacements on the bending side of single trabeculae. We found that bone deformation is heterogeneous with different surface domains shearing off each other. The two novel methods presented make it possible to investigate the dynamics of plasticity and failure of bone both on the micro- and on the nanoscale...|$|R
40|$|We present new {{results for}} the three-point {{correlation}} function, ζ, measured {{as a function of}} scale, luminosity and colour from {{the final version of the}} 2 dF Galaxy Redshift Survey (2 dFGRS). The reduced three-point correlation function, Q 3 ~ζ/ξ 2, is estimated for different triangle shapes and sizes, employing a full covariance analysis. The form of Q 3 is consistent with the expectations for the Λ cold dark matter model, confirming that the primary influence shaping the distribution of galaxies is gravitational instability acting on Gaussian primordial fluctuations. However, we find a clear offset in amplitude between Q 3 for galaxies and the predictions for the dark matter. We are able to rule out the scenario in which galaxies are unbiased tracers of the mass at the 9 σ level. On weakly non-linear scales, we can interpret our results in terms of galaxy bias parameters. We find a linear bias term that is consistent with unity, b 1 = 0. 93 + 0. 10 - 0. 08 and a quadratic bias c 2 =b 2 /b 1 =- 0. 34 + 0. 11 - 0. 08. This is the first significant detection of a non-zero quadratic bias, indicating a small but important non-gravitational contribution to the <b>three-point</b> function. Our <b>estimate</b> of the linear bias from the three-point function is independent of the normalization of underlying density fluctuations, so we can combine this with the measurement of the power spectrum of 2 dFGRS galaxies to constrain the amplitude of matter fluctuations. We find that the rms linear theory variance in spheres of radius 8 h− 1 Mpc is σ 8 = 0. 88 + 0. 12 - 0. 10, providing an independent confirmation of values derived from other techniques. On non-linear scales, where ξ > 1, we find that Q 3 has a strong dependence on scale, colour and luminosit...|$|R
40|$|A {{proposed}} {{method of}} measuring {{the size of}} particles entrained in a flow of a liquid or gas would involve utilization of data from digital particle-image velocimetry (DPIV) of the flow. That is to say, with proper design and operation of a DPIV system, the DPIV data could be processed according to the proposed method to obtain particle sizes in addition to particle velocities. As an additional benefit, one could then compute the mass flux of the entrained particles from the particle sizes and velocities. As in DPIV as practiced heretofore, a pulsed laser beam would be formed into a thin sheet to illuminate a plane of interest in a flow field and the illuminated plane would be observed {{by means of a}} charge-coupled device (CCD) camera aimed along a line perpendicular to the illuminated plane. Unlike in DPIV as practiced heretofore, care would be taken to polarize the laser beam so that its electric field would lie in the illuminated plane, for the reason explained in the next paragraph. The proposed method applies, more specifically, to transparent or semitransparent spherical particles that have an index of refraction {{different from that of the}} fluid in which they are entrained. The method is based on the established Mie theory, which describes the scattering of light by diffraction, refraction, and specular reflection of light by such particles. In the case of a particle illuminated by polarized light and observed in the arrangement described in the preceding paragraph, the Mie theory shows that the image of the particle on the focal plane of the CCD camera includes two glare spots: one attributable to light reflected toward the camera and one attributable to light refracted toward the camera. The distance between the glare spots is a known function of the size of the particle, the indices of refraction of the particle material, and design parameters of the camera optics. Hence, the size of a particle can be determined from the distance between the glare spots. The proposed method would be implemented in an algorithm that would automatically identify, and measure the distance between, the glare spots for each particle for which a suitable image has been captured in a DPIV image frame. The algorithm (see figure) would begin with thresholding of data from the entire image frame to reduce noise, thereby facilitating discrimination of particle images from the background and aiding in the separation of overlapping particles. It is important not to pick a threshold level so high that the light intensity between a given pair of glare spots does not fall below the threshold value, leaving the glare spots disconnected. The image would then be scanned in a sequence of rows and columns of pixels to identify groups of adjacent pixels that contain nonzero brightnesses and that are surrounded by pixels of zero brightness. Each such group would be assumed to constitute the image of one particle. Each such group would be further analyzed to determine whether the image was saturated; saturated particle images must be rejected because the locations of glare spots in saturated images cannot accurately be determined. Within each unsaturated particle image, the centroids (deemed to be the locations) of the glare spots would be determined by means of gradients of brightness distributions and three-point horizontal and <b>three-point</b> vertical Gaussian <b>estimates</b> based on the brightness values of the brightest pixels and the pixels adjacent to them. If the brightness of a given particle image contained only one peak, then it would be assumed that a second glare spot did not exist and that image would be rejected...|$|R

