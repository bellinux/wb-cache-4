33|10000|Public
5000|$|<b>Transfer</b> <b>Learning</b> <b>with</b> an Ensemble of Background Tasks, Z. Marx, M. T. Rosenstein, L. P. Kaelbling, and T. G. Dietterich. NIPS 2005 Workshop on Transfer Learning, Whistler, BC.|$|E
40|$|Abstract—We analyze <b>transfer</b> <b>learning</b> <b>with</b> Deep Neural Networks (DNN) {{on various}} {{character}} recognition tasks. DNN trained on digits are {{perfectly capable of}} recognizing uppercase letters with minimal retraining. They are on par with DNN fully trained on uppercase letters, but train much faster. DNN trained on Chinese characters easily recognize uppercase Latin letters. Learning Chinese characters is accelerated by first pretraining a DNN on a small subset of all classes and then continuing to train on all classes. Furthermore, pretrained nets consistently outperform randomly initialized nets on new tasks with few labeled data. I...|$|E
40|$|Relational {{reinforcement}} learning has allowed results from {{reinforcement learning}} tasks to be re-used in other, closely related, tasks. This {{transfer of knowledge}} is {{made possible by the}} use of parameters in the representations of the task-description and the learned policy. In this paper, we will give a description of {{the current state of the}} art of <b>transfer</b> <b>learning</b> <b>with</b> relational reinforcement learning, make some observations about the usefulness and limitations of this current state and discuss some directions for future research. We also present a first small step along one of those directions. 1...|$|E
40|$|We {{describe}} an under-studied problem in language resource management: that of providing automatic assistance to annotators working in exploratory settings. When no satisfactory tagset already exists, {{such as in}} under-resourced or undocumented languages, it must be developed iteratively while annotating data. This process naturally {{gives rise to a}} sequence of datasets, each annotated differently. We argue that this problem is best regarded as a <b>transfer</b> <b>learning</b> problem <b>with</b> multiple source tasks. Using part-of-speech tagging data with simulated exploratory tagsets, we demonstrate that even simple <b>transfer</b> <b>learning</b> techniques can significantly improve the quality of pre-annotations in an exploratory annotation...|$|R
40|$|Abstract. The rapid {{development}} of social video sharing platforms {{has created a}} huge demand for automatic video classification and annotation techniques, in particular for videos containing social activities {{of a group of}} people (e. g. YouTube video of a wedding reception). Recently, attribute learning has emerged as a promising paradigm for <b>transferring</b> <b>learning</b> to sparsely labelled classes in object or single-object short action classification. In contrast to existing work, this paper for the first time, tackles the problem of attribute learning for understanding group social activities with sparse labels. This problem is more challenging because of the complex multi-object nature of social activities, and the unstructured nature of the activity context. To solve this problem, we (1) contribute an unstructured social activity attribute (USAA) dataset with both visual and audio attributes, (2) introduce the concept of semi-latent attribute space and (3) propose a novel model for learning the latent attributes which alleviate the dependence of existing models on exact and exhaustive manual specification of the attribute-space. We show that our framework is able to exploit latent attributes to outperform contemporary approaches for addressing a variety of realistic multi-media sparse data learning tasks including: multi-task <b>learning,</b> N-shot <b>transfer</b> <b>learning,</b> <b>learning</b> <b>with</b> label noise and importantly zero-shot learning. ...|$|R
40|$|We {{present a}} new {{dependency}} parsing method for Korean applying cross-lingual <b>transfer</b> <b>learning</b> and domain adaptation techniques. Unlike existing transfer learn-ing methods relying on aligned corpora or bilingual lexicons, we propose a feature <b>transfer</b> <b>learning</b> method <b>with</b> minimal su-pervision, which adapts an existing parser {{to the target}} language by transferring the features for the source language to the tar-get language. Specifically, we utilize the Triplet/Quadruplet Model, a hybrid pars-ing algorithm for Japanese, and apply a delexicalized feature transfer for Korean. Experiments with Penn Korean Treebank show that even using only the transferred features from Japanese achieves a high accuracy (81. 6 %) for Korean dependency parsing. Further improvements were ob-tained when a small annotated Korean cor-pus was combined with the Japanese train-ing corpus, confirming that efficient cross-lingual <b>transfer</b> <b>learning</b> can be achieved without expensive linguistic resources. ...|$|R
40|$|When {{a series}} of {{problems}} are related, representations derived from learning earlier tasks {{may be useful in}} solving later problems. In this paper we propose a novel approach to <b>transfer</b> <b>learning</b> <b>with</b> low-dimensional, non-linear latent spaces. We show how such representations can be jointly learned across multiple tasks in a discriminative probabilistic regression framework. When transferred to new tasks with relatively few training examples, learning can be faster and/or more accurate. Experiments on a digit recognition task show significantly improved performance when compared to baseline performance with the original feature representation or with a representation derived from a semi-supervised learning approach. ...|$|E
40|$|Transfer {{learning}} {{addresses the}} problem of how to leverage knowledge acquired in a source domain to improve the accuracy and speed of learning in a related target domain. This paper considers <b>transfer</b> <b>learning</b> <b>with</b> Markov logic networks (MLNs), a powerful formalism for learning in relational domains. We present a complete MLN transfer system that first autonomously maps the predicates in the source MLN to the target domain and then revises the mapped structure to further improve its accuracy. Our results in several real-world domains demonstrate that our approach successfully reduces the amount of time and training data needed to learn an accurate model of a target domain over learning from scratch...|$|E
40|$|We {{present a}} general {{framework}} to learn functions in tensor product reproducing kernel Hilbert spaces (TP-RKHSs). The methodology {{is based on}} a novel representer theorem suitable for existing as well as new spectral penalties for tensors. When the functions in the TP-RKHS are defined on the Cartesian product of finite discrete sets, in particular, our main problem formulation admits as a special case existing tensor completion problems. Other special cases include <b>transfer</b> <b>learning</b> <b>with</b> multimodal side information and multilinear multitask learning. For the latter case, our kernel-based view is instrumental to derive nonlinear extensions of existing model classes. We give a novel algorithm and show in experiments the usefulness of the proposed extensions. ...|$|E
40|$|Many {{behavior}} recognition {{systems are}} trained and tested on single datasets limiting their application to comparable datasets. While retraining the {{system with a}} novel dataset is possible, it involves laborious annotation effort. We propose to minimize the annotation effort by reusing the knowledge obtained from previous datasets and adapting the recognition system to the novel data. To this end, we investigate the use of <b>transfer</b> <b>learning</b> {{in the context of}} rodent behavior recognition. Specifically, we look at two <b>transfer</b> <b>learning</b> methods <b>with</b> two different approaches and examine the implications of their respective assumptions on synthetic data. We further illustrate their performance in transferring a rat action classifier to a mouse action classifier. The performance results in the transfer task are promising. The classification accuracy improves substantially with only very few labeled examples from the novel dataset...|$|R
40|$|Deep {{learning}} {{emerges as}} {{a powerful tool for}} analyzing medical images. Retinal disease detection by using computer-aided diagnosis from fundus image has emerged as a new method. We applied deep learning convolutional neural network by using MatConvNet for an automated detection of multiple retinal diseases with fundus photographs involved in STructured Analysis of the REtina (STARE) database. Dataset was built by expanding data on 10 categories, including normal retina and nine retinal diseases. The optimal outcomes were acquired by using a random forest <b>transfer</b> <b>learning</b> based on VGG- 19 architecture. The classification results depended greatly on the number of categories. As the number of categories increased, the performance of deep learning models was diminished. When all 10 categories were included, we obtained results with an accuracy of 30. 5 %, relative classifier information (RCI) of 0. 052, and Cohen's kappa of 0. 224. Considering three integrated normal, background diabetic retinopathy, and dry age-related macular degeneration, the multi-categorical classifier showed accuracy of 72. 8 %, 0. 283 RCI, and 0. 577 kappa. In addition, several ensemble classifiers enhanced the multi-categorical classification performance. The <b>transfer</b> <b>learning</b> incorporated <b>with</b> ensemble classifier of clustering and voting approach presented the best performance with accuracy of 36. 7 %, 0. 053 RCI, and 0. 225 kappa in the 10 retinal diseases classification problem. First, due to the small size of datasets, the deep learning techniques in this study were ineffective to be applied in clinics where numerous patients suffering from various types of retinal disorders visit for diagnosis and treatment. Second, we found that the <b>transfer</b> <b>learning</b> incorporated <b>with</b> ensemble classifiers can improve the classification performance in order to detect multi-categorical retinal diseases. Further studies should confirm the effectiveness of algorithms with large datasets obtained from hospitals...|$|R
30|$|In this study, {{source and}} target domain data {{refers to the}} same network {{environment}} at a different time. We assumed that attacks in a source domain are already known and labeled, and attacks in a target domain are new and different than the source. We formularized the problem by using source domain data to differentiate new attacks in the target domain. Previously, we developed a transfer learning-enabled detection framework and proposed a feature-based heterogeneous <b>transfer</b> <b>learning,</b> called HeTL [6], to detect unseen variants of attacks. HeTL can find new feature representations for source and target domain by transforming them on a common latent space. Nevertheless, we observed that the performance of HeTL depended on manual pre-settings of a hyper-parameter: relevance between the source and target domain [6]. In this paper, we proposed another approach—a hierarchical <b>transfer</b> <b>learning</b> algorithm <b>with</b> clustering enhancement, called CeHTL, which can cluster source and target domain and compute the relevance between them.|$|R
40|$|An {{automated}} {{technique has}} recently been proposed to transfer learning in the hierarchical Bayesian optimization algorithm (hBOA) based on distance-based statistics. The technique enables practitioners to improve hBOA efficiency by collecting statistics from probabilistic models obtained in previous hBOA runs and using the obtained statistics to bias future hBOA runs on similar problems. The {{purpose of this paper}} is threefold: (1) test the technique on several classes of NP-complete problems, including MAXSAT, spin glasses and minimum vertex cover; (2) demonstrate that the technique is effective even when previous runs were done on problems of different size; (3) provide empirical evidence that combining <b>transfer</b> <b>learning</b> <b>with</b> other efficiency enhancement techniques can often provide nearly multiplicative speedups...|$|E
40|$|International audienceIn recent years, deep {{architectures}} {{have been}} used for <b>transfer</b> <b>learning</b> <b>with</b> state-of-the-art performance in many data-sets. The properties of their features remain, however, largely unstudied under the transfer perspective. In this work, we present an extensive analysis of the resiliency of feature vectors extracted from deep models, with special focus on the trade-off between performance and compression rate. By introducing perturbations to image descriptions extracted from a deep convolutional neural network, we change their precision and number of dimensions, measuring how it affects the final score. We show that deep features are more robust to these disturbances when compared to classical approaches, achieving a compression rate of 98. 4 %, while losing only 0. 88 % of their original score for Pascal VOC 2007...|$|E
30|$|Although {{there are}} several methods that {{facilitate}} learning on smaller datasets as described above, well-annotated large medical datasets are still needed {{since most of the}} notable accomplishments of deep learning are typically based on very large amounts of data. Unfortunately, building such datasets in medicine is costly and demands an enormous workload by experts, and may also possess ethical and privacy issues. The goal of large medical datasets is the potential to enhance generalizability and minimize overfitting, as discussed previously. In addition, dedicated medical pretrained networks can probably be proposed once such datasets become available, which may foster deep learning research on medical imaging, though whether <b>transfer</b> <b>learning</b> <b>with</b> such networks improves the performance in the medical field compared to that with ImageNet pretrained models is not clear and remains an area of further investigation.|$|E
30|$|Throughout the {{literature}} on <b>transfer</b> <b>learning,</b> {{there are a number}} of terminology inconsistencies. Phrases such as <b>transfer</b> <b>learning</b> and domain adaptation are used to refer to similar processes. The following definitions will be used in this paper. Domain adaptation, as it pertains to <b>transfer</b> <b>learning,</b> is the process of adapting one or more source domains for the means of transferring information to improve the performance of a target learner. The domain adaptation process attempts to alter a source domain in an attempt to bring the distribution of the source closer to that of the target. Another area of literature inconsistencies is in characterizing the <b>transfer</b> <b>learning</b> process <b>with</b> respect to the availability of labeled and unlabeled data. For example, Daumé [22] and Chattopadhyay [14] define supervised <b>transfer</b> <b>learning</b> as the case of having abundant labeled source data and limited labeled target data, and semi-supervised <b>transfer</b> <b>learning</b> as the case of abundant labeled source data and no labeled target data. In Gong [42] and Blitzer [5], semi-supervised <b>transfer</b> <b>learning</b> is the case of having abundant labeled source data and limited labeled target data, and unsupervised <b>transfer</b> <b>learning</b> is the case of abundant labeled source data and no labeled target data. Cook [19] and Feuz [36] provide a different variation where the definition of supervised or unsupervised refers to the presence or absence of labeled data in the source domain and informed or uninformed refers to the presence or absence of labeled data in the target domain. With this definition, a labeled source and limited labeled target domain is referred to as informed supervised <b>transfer</b> <b>learning.</b> Pan [84] refers to inductive <b>transfer</b> <b>learning</b> as the case of having available labeled target domain data, transductive <b>transfer</b> <b>learning</b> as the case of having labeled source and no labeled target domain data, and unsupervised <b>transfer</b> <b>learning</b> as the case of having no labeled source and no labeled target domain data. This paper will explicitly state when labeled and unlabeled data are being used in the source and target domains.|$|R
40|$|Disturbed intermanual <b>transfer</b> of tactile <b>learning</b> in callosal {{agenesis}} {{has been}} interpreted {{as a sign of}} disconnection syndrome. We observed this sign in one of four acallosal patients with a conventional form-board task, and tried to elucidate the nature of the deficit. The form-board performance of the patient <b>with</b> disturbed <b>transfer</b> of <b>learning</b> totally depended on motor skill, while the other acallosals and normal controls executed the task based on spatial and somesthetic information. All acallosals and normals, however, failed to show <b>transfer</b> of <b>learning</b> <b>with</b> another tactile task which needed motor skill but not spatial-somesthetic information. These findings suggest that the task-performing strategies in form-board learning change the state of interhemispheric <b>transfer.</b> Unimanual <b>learning</b> effect is <b>transferred</b> if spatial-somesthetic information is acquired in the process of learning, but is not transferred if motor skill is the exclusive content of learning. We conclude that disturbed “transfer” of learning in some acallosals is not a true disconnection sign. It should be attributed to a lack of appropriate strategy, as a result of ineffective problem solving in tactile tasks...|$|R
40|$|Different from a {{large body}} of {{research}} on social networks that almost exclusively focused on positive relationships, we study signed social networks with both positive and negative links. Specifically, we focus on how to reliably and effectively predict the signs of links in a signed social network (called a target network), where a very small amount of edge sign information is available as the training data. To train a good classifier, we adopt the <b>transfer</b> <b>learning</b> approach to leverage the abundant edge signs from another signed social network (called a source network) which may have a different joint distribution of the observed instance and the class label. As there is no predefined feature vector for the edge instances ii in a signed network, we construct generalizable features that can transfer the topological knowledge from the source network to the target. With the extracted features, we adopt an AdaBoost-like <b>transfer</b> <b>learning</b> algorithm <b>with</b> instance weighting to utilize more useful training instances in the source network for model learning. Experimental results on two real large signed social networks demonstrate that our <b>transfer</b> <b>learning</b> algorithm can improve the prediction accuracy by 40 % over baseline schemes. ii...|$|R
40|$|Artificial neural {{networks}} {{learn how to}} solve new problems through a computationally intense and time consuming process. One {{way to reduce the}} amount of time required is to inject preexisting knowledge into the network. To make use of past knowledge, we can take advantage of techniques that transfer the knowledge learned from one task, and reuse it on another (sometimes unrelated) task. In this paper we propose a novel selective breeding technique that extends the <b>transfer</b> <b>learning</b> <b>with</b> behavioural genetics approach proposed by Kohli, Magoulas and Thomas (2013), and evaluate its performance on financial data. Numerical evidence demonstrates the credibility of the new approach. We provide insights on the operation of transfer learning and highlight the benefits of using behavioural principles and selective breeding when tackling a set of diverse financial applications problems...|$|E
40|$|Abstract—Artificial neural {{networks}} {{learn how to}} solve new problems through a computationally intense and time consuming process. One {{way to reduce the}} amount of time required is to inject pre-existing knowledge into the network. To make use of past knowledge, we can take advantage of techniques that transfer the knowledge learned from one task, and reuse it on another (sometimes unrelated) task. In this paper we propose a novel selective breeding technique that extends the <b>transfer</b> <b>learning</b> <b>with</b> behavioural genetics approach proposed by Kohli, Magoulas and Thomas (2013), and evaluate its performance on financial data. Numerical evidence demonstrates the credibility of the new approach. We provide insights on the operation of transfer learning and highlight the benefits of using behavioural principles and selective breeding when tackling a set of diverse financial applications problems. Keywords—transfer learning, artificial {{neural networks}}, ge-netic algorithms, population studies, behavioural genetics, selectiv...|$|E
40|$|This paper proposes an Extended Minimum Description Length Principle (EMDLP) for feature-based {{inductive}} transfer learning, {{in which}} both the source and the target data sets contain class labels and relevant features are transferred from the source domain to the target one. Despite numerous works on this topic, few {{of them have a}} solid theoretical framework and are parameter-free. Our EMDLP overcomes these flaws and allows us to evaluate the inferiority of the results of <b>transfer</b> <b>learning</b> <b>with</b> the add-sum of the code lengths of five components: the corresponding two hypotheses, the two data sets {{with the help of the}} hypotheses, and the set of the transferred features. We design a code book to build the connections between the source and the target tasks. Extensive experiments using both real and artificial data sets show that EMDLP is robust against noise and performs better on the classification accuracy than the state-of-the-art methods...|$|E
40|$|Dynamic {{transfer}} 2 We contrast previous {{views of}} <b>transfer</b> of <b>learning</b> <b>with</b> emerging perspectives in the field. Based on the latter, we have adapted our previously developed analytical framework to characterize transfer as it occurs dynamically in an interview. Our adapted framework is {{also consistent with}} a theoretical framework proposed by Redish (in press) that addresses several cognitive and epistemological issues. In light of Redish’s framework and contemporary transfer models, we have demonstrated how our analytical framework can help identify and characterize transfer as it occurs in an interview. We describe instances in which students <b>transfer</b> their <b>learning</b> spontaneously in an interview as well as those in which transfer is promoted by scaffolding provided by the interviewer. In connection with the latter, we describe yet another research methodology—the teaching interviews that can allow us to investigate dynamic scaffolded transfer. Dynamic transfer...|$|R
40|$|International audienceThe {{main purpose}} of <b>transfer</b> <b>learning</b> is to resolve the problem of {{different}} data distribution, generally, when the training samples of source domain {{are different from the}} training samples of the target domain. Prediction of salient areas in natural video suffers from the lack of large video benchmarks with human gaze fixations. Different databases only provide dozens up to one or two hundred of videos. The only public large database is HOLLYWOOD with 1707 videos available with gaze recordings. The main idea {{of this paper is to}} <b>transfer</b> the knowledge <b>learned</b> <b>with</b> the deep network on a large dataset to train the network on a small dataset to predict salient areas. The results show an improvement on two small publicly available video datasets...|$|R
40|$|In this paper, {{we propose}} {{to study the}} problem of {{heterogeneous}} transfer ranking, a <b>transfer</b> <b>learning</b> problem <b>with</b> heterogeneous features in order to utilize the rich large-scale labeled data in popular languages to help the ranking task in less popular languages. We develop a large-margin algorithm, namely LM-HTR, {{to solve the problem}} by mapping the input features in both the source domain and target domain into a shared latent space and simultaneously minimizing the feature reconstruction loss and prediction loss. We analyze the theoretical bound of the prediction loss and develop fast algorithms via stochastic gradient descent so that our model can be scalable to large-scale applications. Experiment results on two application datasets demonstrate the advantages of our algorithms over other state-of-the-art methods. 1...|$|R
40|$|Deep {{learning}} {{has dominated the}} computer vision field since 2012, but a common criticism of deep learning methods is their dependence on large amounts of data. To combat this criticism research into data-efficient deep learning is growing. The foremost success in data-efficient deep learning is <b>transfer</b> <b>learning</b> <b>with</b> networks pre-trained on the ImageNet dataset. Pre-trained networks have achieved state-of-the-art performance on many tasks. We consider the pre-trained network method for a new task {{where we have to}} collect the data. We hypothesize that the data efficiency of pre-trained networks can be improved through informed data collection. After exhaustive experiments on CaffeNet and VGG 16, we conclude that the data efficiency indeed can be improved. Furthermore, we investigate an alternative approach to data-efficient learning, namely adding domain knowledge {{in the form of a}} spatial transformer to the pre-trained networks. We find that spatial transformers are difficult to train and seem to not improve data efficiency...|$|E
40|$|The aim of {{this paper}} is to give an {{overview}} of domain adaptation and <b>transfer</b> <b>learning</b> <b>with</b> a specific view on visual applications. After a general motivation, we first position domain adaptation in the larger transfer learning problem. Second, we try to address and analyze briefly the state-of-the-art methods for different types of scenarios, first describing the historical shallow methods, addressing both the homogeneous and the heterogeneous domain adaptation methods. Third, we discuss the effect of the success of deep convolutional architectures which led to new type of domain adaptation methods that integrate the adaptation within the deep architecture. Fourth, we overview the methods that go beyond image categorization, such as object detection or image segmentation, video analyses or learning visual attributes. Finally, we conclude the paper with a section where we relate domain adaptation to other machine learning solutions. Comment: Book chapter to appear in "Domain Adaptation in Computer Vision Applications", Springer Series: Advances in Computer Vision and Pattern Recognition, Edited by Gabriela Csurk...|$|E
30|$|Data meaning perspective: Due to {{the fact}} that, nowadays, most data are {{dispersed}} to different regions, systems, or applications, the “meaning” of the collected data from various sources may not be exactly the same, which may significantly impact {{the quality of the}} machine learning results. Although the previous mentioned techniques such as <b>transfer</b> <b>learning</b> <b>with</b> the power of knowledge transfer and the cognition-assisted learning methods provide some possible solutions to this problem, it is obvious that they are absolutely not catholicons owing to the limitations of these techniques for achieving context-aware. Ontology, semantic web, and other related technologies seem to be preferred on this issue. Based on ontology modeling and semantic derivation, some valuable patterns or rules can be discovered as knowledge as well, which is a necessity for learning systems to be, or appear to be intelligent. But the problem that arises now is, although the ontology and semantic web technologies can benefit the big data analysis, these two technologies are not mature enough, thus how to employ them in machine learning methods to process big data will be a meaningful research.|$|E
40|$|This paper summarises and {{analyses}} the cross-dataset recognition <b>transfer</b> <b>learning</b> techniques <b>with</b> {{the emphasis on}} what kinds of methods can be used when the available source and target data are presented in different forms for boosting the target task. This paper {{for the first time}} summarises several transferring criteria in details from the concept level, which are the key bases to guide what kind of knowledge to transfer between datasets. In addition, a taxonomy of cross-dataset scenarios and problems is proposed according the properties of data that define how different datasets are diverged, thereby review the recent advances on each specific problem under different scenarios. Moreover, some real world applications and corresponding commonly used benchmarks of cross-dataset recognition are reviewed. Lastly, several future directions are identified...|$|R
40|$|Worked {{examples}} {{have been}} effective in enhancing <b>learning</b> outcomes, especially <b>with</b> novice learners. Most of this {{research has been conducted}} in laboratory settings. This study examined the impact of embedding elaborated worked example modeling in a computer simulation practice activity on learning achievement among 39 undergraduate students within a classroom environment. The students from one introductory forensic science course were {{randomly assigned to one of}} two groups that worked through computer-based simulations containing worked example modeling conditions presented in varied order. The computer software administered the modeled simulations, prior knowledge test, pretest, posttests, and a second domain test. Findings from this study suggest that embedded worked example modeling within practice simulations can be an effective method for <b>transfer</b> of <b>learning</b> <b>with</b> novice learners...|$|R
50|$|Roxbury Community College is {{accredited}} by the New England Association of Schools and Colleges, Inc. {{through its}} Commission on Institutions of Higher Education. RCC affords its students {{a solid foundation}} for employment, professional advancement, college <b>transfer</b> and lifelong <b>learning.</b> <b>With</b> over 300 programs of study, the post-secondary education offered at Roxbury Community College helps students to foster developmental academic skills through career and transfer programs, workforce development, and private and public sector training.|$|R
40|$|This paper {{focuses on}} the problem of {{explaining}} predictions of psychological attributes such as attractiveness, happiness, confidence and intelligence from face photographs using deep neural networks. Since psychological attribute datasets typically suffer from small sample sizes, we apply <b>transfer</b> <b>learning</b> <b>with</b> two base models to avoid overfitting. These models were trained on an age and gender prediction task, respectively. Using a novel explanation method we extract heatmaps that highlight the parts of the image most responsible for the prediction. We further observe that the explanation method provides important insights into the nature of features of the base model, which allow one to assess the aptitude of the base model for a given transfer learning task. Finally, we observe that the multiclass model is more feature rich than its binary counterpart. The experimental evaluation is performed on the 2222 images from the 10 k US faces dataset containing psychological attribute labels as well as on a subset of KDEF images. Comment: 12 pages, 7 figures, Paper accepted for GCPR 201...|$|E
40|$|We {{explore the}} problem of {{classification}} within a medical image data-set based on a feature vector extracted from the deepest layer of pre-trained Convolution Neural Networks. We have used feature vectors from several pre-trained structures, including networks with/without transfer learning to evaluate the performance of pre-trained deep features versus CNNs which have been trained by that specific dataset {{as well as the}} impact of <b>transfer</b> <b>learning</b> <b>with</b> a small number of samples. All experiments are done on Kimia Path 24 dataset which consists of 27, 055 histopathology training patches in 24 tissue texture classes along with 1, 325 test patches for evaluation. The result shows that pre-trained networks are quite competitive against training from scratch. As well, fine-tuning does not seem to add any tangible improvement for VGG 16 to justify additional training while we observed considerable improvement in retrieval and classification accuracy when we fine-tuned the Inception structure. Comment: To appear in proceedings of the 7 th International Conference on Image Processing Theory, Tools and Applications (IPTA 2017), Nov 28 -Dec 1, Montreal, Canad...|$|E
40|$|Abstract. Knowledge {{transfer}} from multiple source domains to a target domain {{is crucial in}} transfer learning. Most existing methods are focused on learning weights for different domains based on the similarities between each source do-main and the target domain or learning more precise classifiers from the source domain data jointly by maximizing their consensus of predictions on the target domain data. However, these methods only consider measuring similarities or building classifiers on the original data space, and fail to discover a more power-ful feature representation of the data when transferring knowledge from multiple source domains to the target domain. In this paper, we propose a new framework for <b>transfer</b> <b>learning</b> <b>with</b> multiple source domains. Specifically, in the proposed framework, we adopt autoencoders to construct a feature mapping from an orig-inal instance to a hidden representation, and train multiple classifiers from the source domain data jointly by performing an entropy-based consensus regular-izer on the predictions on the target domain. Based on the framework, a particular solution is proposed to learn the hidden representation and classifiers simultane-ously. Experimental results on image and text real-world datasets demonstrate the effectiveness of our proposed method compared with state-of-the-art methods...|$|E
40|$|<b>Transfer</b> <b>Learning</b> is {{concerned}} <b>with</b> {{the application of}} knowledge gained from solving a problem to a different but related problem domain. In this paper, we propose a method and efficient algorithm for ranking and selecting representations from a Restricted Boltzmann Machine trained on a source domain to be transferred onto a target domain. Experiments carried out using the MNIST, ICDAR and TiCC image datasets show that the proposed adaptive feature ranking and <b>transfer</b> <b>learning</b> method offers statistically significant improvements on the training of RBMs. Our method is general in that the knowledge chosen by the ranking function {{does not depend on}} its relation to any specific target domain, and it works <b>with</b> unsupervised <b>learning</b> and knowledge-based <b>transfer.</b> Comment: 9 pages 7 figures, new experimental results on ranking and transfer have been added, typo fixe...|$|R
40|$|In this paper, {{we propose}} a {{boosting}} based tracking framework using <b>transfer</b> <b>learning.</b> To deal <b>with</b> complex appearance variations, the proposed tracking framework tries to utilize discriminative information from previous frames {{to conduct the}} tracking task in the current frame, and thus transfers some prior knowledge from the previous source data domain to the current target data domain, resulting in a high discriminative tracker for distinguishing the object from the background. The proposed tracking system has been tested on several challenging sequences. Experimental results demonstrate {{the effectiveness of the}} proposed tracking framework. Wenhan Luo, Xi Li, Wei Li, Weiming H...|$|R
30|$|In {{the last}} few years, visual {{recognition}} community has shown a growing interest in <b>transfer</b> <b>learning</b> algorithms [30, 31]. <b>Transfer</b> subspace <b>learning</b> (TSL) is effectively used in understanding kin relationships in the photo [32]. Classification under covariate shift is been solved by <b>transfer</b> <b>learning</b> [33]. Features <b>with</b> meta-features {{that can be used}} in prediction task is studied in [34]. Building classifiers for text classification by extracting positive examples from unlabeled examples for improving performance of the system are highlighted in [10]. <b>Transfer</b> subspace <b>learning</b> that can reduce time and space cost is proposed in [35]. Enhanced subspace clustering algorithms [36, 37] are used to handle complex data and to improve clustering results. Cross-domain discriminative locally linear embedding (CDLLE) can be used to reduce the human labeling efforts for social image annotation problem [38]. Robust framework against noise in the <b>transfer</b> <b>learning</b> setting is proposed in [39]. Semisupervised clustering algorithm with domain adaptation and the constraint knowledge with transferred centroid regularization is proposed in [40]. Xiaoxin Yin et al. have proposed [41] efficient classification across multiple database relations. Performance improvement is seen when <b>transfer</b> <b>learning</b> is used in medical image segmentation followed by classification [42]. Low-resolution face images are matched with the high-resolution gallery images using <b>transfer</b> <b>learning</b> which improved cross-resolution face matching [43]. <b>Transfer</b> <b>learning</b> using Bayesian model was used in [44] for face verification application. Ensemble-based <b>transfer</b> <b>learning</b> was used in text classification [45]. Knowledge was transferred between text and images using matrix factorization approach by Zhu et al. [46]. Geng et al. used domain adaptation metric learning for face recognition and web image annotation [47]. Server-based spam filter learned from public sources was designed and applied to individual users with the help of <b>transfer</b> <b>learning</b> [48].|$|R
