28|10000|Public
5000|$|... {{that it gave}} in {{condensed}} form <b>the</b> <b>information</b> <b>in</b> <b>that</b> work, {{and took}} such matter directly from the German work in most instances, although occasionally while the topic was the same the treatment was independent of the German original; ...|$|E
50|$|The {{content of}} one's {{religious}} knowledge may vary {{from person to}} person, as will {{the degree to which}} it may occupy the person's mind (frequency), the intensity of the knowledge, and the centrality of <b>the</b> <b>information</b> (<b>in</b> <b>that</b> religious tradition, or to that individual).|$|E
50|$|Kennedy was {{the author}} of The Official Underground and Newave Comix Price Guide (Boatner Norton Press, 1982), the first price guide to the {{underground}} comix of the 1960s and 1970s. Much of <b>the</b> <b>information</b> <b>in</b> <b>that</b> book was based on Kennedy's personal collection of more than 9,500 comix.|$|E
50|$|It is {{important}} to differentiate between problem analysis and decision-making. Traditionally, {{it is argued that}} problem analysis must be done first, so <b>that</b> <b>the</b> <b>information</b> gathered <b>in</b> <b>that</b> process may be used towards decision-making.|$|R
5000|$|A “regular” JSON {{transformation}} {{produces a}} more compact representation, but loses some of <b>the</b> document structural <b>information,</b> <b>in</b> <b>that</b> {{it does not}} define whether a key-value pair is an attribute or a node: ...|$|R
30|$|From {{the above}} {{relation}} (41) producer or customer have <b>the</b> required <b>information,</b> <b>in</b> such <b>that</b> <b>the</b> other party {{has to decide}} the offer done based on the working time per individual on their availability.|$|R
50|$|Early in my premedical training, I read a {{book called}} Sugar Blues by William Dufty, and <b>the</b> <b>information</b> <b>in</b> <b>that</b> book {{actually}} changed by thought belief system and patterns of eating so drastically that it, essentially, changed my life...The drastic improvements in my health changed my attitudes towards the way I was living at the time...|$|E
5000|$|Information {{distance}} is {{the distance between}} two finite objects (represented as computer files) expressed {{as the number of}} bits in the shortest program which transforms one object into the other one or vice versa on a universal computer. This is an extension of Kolmogorov complexity. The Kolmogorov complexity of a single finite object is <b>the</b> <b>information</b> <b>in</b> <b>that</b> object; the information distance between a pair of finite objects is the minimum information required to go from one object to the other or vice versa.Information distance was first defined and investigated in [...] based on thermodynamic principles, see also. Subsequently, it achieved final form in. It is applied in the normalized compression distance and the normalized Google distance.|$|E
5000|$|There {{are many}} subtle {{differences}} between CA signed and self-signed certificates, {{especially in the}} amount of trust that can be placed in the security assertions of the certificate. Some CAs can verify the identity of the person to whom they issue a certificate; for example the US military issues their Common Access Cards in person, with multiple forms of other ID. The CA can attest identity values like these by including them in the signed certificate. The entity that validates the certificate can trust <b>the</b> <b>information</b> <b>in</b> <b>that</b> certificate, to the same extent that they trust the CA that signed it (and by implication, the security procedures the CA used to verify the attested information).|$|E
2500|$|The Sumerian word kur for hill {{could also}} mean {{mountain}} or country or land, depending on context. [...] The kur sign (shadu in Akkadian) also meant hill or mountain, and therefore <b>the</b> <b>information</b> <b>in</b> Gilgamesh <b>that</b> <b>the</b> craft {{came to rest}} upon a mountain, could alternatively mean it {{came to rest on}} a hill at the west end of the Persian Gulf. [...] This hill or mound may have been underwater at high tide when the ark grounded there.|$|R
40|$|The first {{libraries}} in Brazil {{belonged to}} religious orders, {{such as the}} Companhia de Jesus (Company of Jesus/Society of Jesus). Little is know about them except for the Jesuits libraries which are {{the best known of}} them, due to the work História da Companhia de Jesus no Brasil by Serafim Leite. The present article aims to review and collect <b>the</b> <b>information</b> contained <b>in</b> <b>that</b> work, and thus offer an overview of the theme...|$|R
40|$|The {{purpose of}} the paper is to present the {{fundamental}} elements used in entity's valuation, starting with the most laborious step in practical activity of valuation which {{is the process of}} collecting and processing all <b>the</b> <b>information</b> necessary <b>in</b> <b>that</b> process and <b>the</b> analysis of <b>that</b> <b>information</b> <b>in</b> order to choose the adequate valuation method. Diagnostic analysis provides the main and essential information for decision making process making facile the process of establish the market value of an entity...|$|R
50|$|One {{important}} rule of any {{forensic analysis}} {{is that the}} objective facts must be considered first. Reviewing comments in the code or searching the Internet to find information about the companies that distribute the code and the programmers who wrote the code are useful only after the objective facts regarding correlation have been considered. Once an analysis has been performed using forensic tools and procedures, analysts can then begin looking at subjective evidence like comments in the code. If <b>the</b> <b>information</b> <b>in</b> <b>that</b> subjective evidence conflicts with the objective analysis, analysts need to doubt the subjective evidence. Fake copyright notices, open source notifications, or programmer names that were added to source code after copying took place, in order to disguise the copying, are not uncommon in real-world cases of code theft.|$|E
5000|$|The HEVC video coding layer {{uses the}} same [...] "hybrid" [...] {{approach}} used in all modern video standards, starting from H.261, in that it uses inter-/intra-picture prediction and 2D transform coding. A HEVC encoder first proceeds by splitting a picture into block shaped regions for the first picture, or the first picture of a random access point, which uses intra-picture prediction. Intra-picture prediction is when the prediction of the blocks in the picture is based only on <b>the</b> <b>information</b> <b>in</b> <b>that</b> picture. For all other pictures, inter-picture prediction is used, in which prediction information is used from other pictures. After the prediction methods are finished and the picture goes through the loop filters, the final picture representation is stored in the decoded picture buffer. Pictures stored in the decoded picture buffer {{can be used for}} the prediction of other pictures.|$|E
50|$|A modern {{conceptualization}} by Wickens et al. (2008) is the salience, effort, expectancy, {{and value}} (SEEV) model. It {{was developed by}} the researchers (Wickens et al., 2001) as a model of scanning behavior describing the probability that a given area of interest will attract attention (AOI). The SEEV model is described by p(A) = sS - efEF + (exEX)(vV), in which p(A) is the probability a particular area will be samples, S is the salience for that area; EF represents the effort required in reallocating attention to a new AOI, related to the distance from the currently attended location to the AOI; EX (expectancy) is the expected event rate (bandwidth), and V is the value of <b>the</b> <b>information</b> <b>in</b> <b>that</b> AOI, represented as the product of Relevance and Priority (R*P). The lowercase values are scaling constants. This equation allows for the derivation of optimal and normative models for how an operator should behave, and to characterize how they behave. Wickens et al., (2008) also generated a version of the model that does not require absolute estimation of the free parameters for the environment - just the comparative salience of other regions compared to region of interest.|$|E
50|$|Public social {{activity}} from the interest graph {{is the primary}} data source 140 Proof uses to make its ads more relevant.Apps using 140 Proof give the company a user ID list stripped of names, along with <b>the</b> public <b>information</b> <b>in</b> <b>that</b> user’s profile. 140 Proof’s algorithms assemble ‘personas’ of users based on keywords in users’ posts and who users are following. By combining information on several of a user’s stated interests, interest graphs allow 140 Proof to infer further about the user’s interests.|$|R
40|$|We derive an {{explicit}} lower bound on {{the capacity of}} the discrete amplitude–constrained Gaussian channel by proving the existence of tight frames that permit redundant vector representations with small coefficients. Our method encodes <b>the</b> <b>information</b> <b>in</b> subspaces <b>that</b> are optimal <b>in</b> terms of the power to amplitude ratio. In a recent paper, Lyubarskii and Vershynin discuss how the work of Kashin (1977) implies the existence of such representations, and they term them Kashin respresentations. We use this work from frame theory to address the relationship between signal redundancy, peak–to–average power ratio and achievable data rates. 1...|$|R
40|$|<b>The</b> <b>information</b> <b>in</b> bug reports {{influences}} {{the speed at}} which bugs are fixed. However, bug reports differ in their quality of information. We conducted a survey among ECLIPSE developers to determine <b>the</b> <b>information</b> <b>in</b> reports <b>that</b> they widely used and the problems frequently encountered. Our results show that steps to reproduce and stack traces are most sought after by developers, while inaccurate steps to reproduce and incomplete <b>information</b> pose <b>the</b> largest hurdles. Surprisingly, developers are indifferent to bug duplicates. Such insight is useful to design new bug tracking tools that guide reporters at providing more helpful information. We also present a prototype of a quality-meter tool that measures the quality of bug reports by scanning its content. 1...|$|R
50|$|Note {{that once}} a wheel spans the desired upper limit of the sieving range, one can stop {{generating}} further wheels and use <b>the</b> <b>information</b> <b>in</b> <b>that</b> wheel to cull the remaining composite numbers from that last wheel list using a Sieve of Eratosthenes type technique but using the gap pattern inherent to the wheel to avoid redundant culls; some optimizations {{may be able to}} be made {{based on the fact that}} (will be proven in the next section) that there will be no repeat culling of any composite number: each remaining composite will be culled exactly once. Alternatively, one can continue to generate truncated wheel lists using primes up to the square root of the desired sieve range, in which case all remaining number representations in the wheel will be prime; however, although this method is as efficient as to never culling composite numbers more than once, it loses much time external to the normally considered culling operations in processing the successive wheel sweeps so as to take much longer.The elimination of composite numbers by a factorization wheel is based on the following: Given a number k > n, we know that k is not prime if k mod n and n are not relatively prime. From that, the fraction of numbers that the wheel sieve eliminates can be determined (although not all need be physically struck off; many can be culled automatically in the operations of copying of lesser wheels to greater wheels) as 1 - phi (n) / n, which is also the efficiency of the sieve.|$|E
40|$|During {{the past}} weekend hail storms once again wreaked havoc on corn and soybean fields across Iowa. Much like hail damage {{last month and}} last week, growers have an option to spray {{fungicides}} to protect remaining leaf tissue. We wrote an article last month walking through a fungicide decision on hail-damaged crops {{at that point in}} the growing season. Much of <b>the</b> <b>information</b> <b>in</b> <b>that</b> article remains relevant...|$|E
40|$|Multimedia data formats axe the {{permanent}} {{representation of a}} multimedia document and thus encode <b>the</b> <b>information</b> <b>in</b> <b>that</b> document. Users are limited in their expression ultimately by what the data format permits the application to store. Therefore, it behooves the system designer {{to be aware of}} the issues involved in creating and using data formats. This paper sketches some of the user requirements of multimedia, discusses the variety of issues concerning data formats (both common and media specific), and examines a few key formats in use today. Copyright (c) 199...|$|E
40|$|We {{show that}} the `standard' quantum {{algorithm}} for the abelian hidden subgroup problem is not only e#cient but is optimal <b>in</b> <b>the</b> <b>information</b> theoretic sense, <b>in</b> <b>that</b> it maximizes the probability of correctly identifying the hidden subgroup. The proof uses semidefinite programming to {{show that the}} standard algorithm implements the optimal set of measurements...|$|R
40|$|Authors and Contributors The FVS {{staff has}} {{maintained}} model documentation for this variant {{in the form}} of a variant overview since its release in 1982. The original author was Gary Dixon. In 2008, the previous document was replaced with an updated variant overview. Gary Dixon, Christopher Dixon, Robert Havis, Chad Keyser, Stephanie Rebain, Erin Smith-Mateja, and Don Vandendriesche were involved with that update. Don Vandendriesche cross-checked <b>the</b> <b>information</b> contained <b>in</b> <b>that</b> variant overview update with the FVS source code. In 2010, Gary Dixon expanded the species list and made significant updates to this variant overview. Current maintenance is provided by Cha...|$|R
40|$|Nowadays, data {{handled by}} an {{institution}} or company is spread out {{by more than}} one database and lots of documents of different types. To extract <b>the</b> <b>information</b> implicit <b>in</b> <b>that</b> data, it is necessary to pick parts from those various archives. To obtain a general overview, those information slices should be gather. Different approaches can be followed to achieve that integration, ranging from the merge of resources till the fusion of the extracted parts. In this paper, we introduce Metamorphosis – a Topic Maps oriented environment to generate conceptual navigators for heterogenous information systems – and we argue that Metamorphosis can be used to achieve the referred interoperability...|$|R
40|$|In this paper, We {{study the}} problem of {{learning}} a controllable representation for high-dimensional observations of dynamical systems. Specifically, we consider {{a situation where there}} are multiple sets of observations of dynamical systems with identical underlying dynamics. Only one of these sets has information about the effect of actions on the observation and the rest are just some random observations of the system. Our goal is to utilize <b>the</b> <b>information</b> <b>in</b> <b>that</b> one set and find a representation for the other sets {{that can be used for}} planning and ling-term prediction. Comment:...|$|E
40|$|In 1999, USU’s Extension Forester and Horticulturist {{published}} an extension publication titled “Selecting and Planting Landscape Trees”. The publication allowed readers to select trees with various growth related, ornamental and cultural characteristics. <b>The</b> <b>information</b> <b>in</b> <b>that</b> publication was enhanced by developing {{it into a}} searchable database software program, TreeBrowser a computer program containing native and introduced trees growing in Utah and the Intermountain West. In 2009, USU Tree Browser migrated into an interactive website, [URL] containing information on 241 native and introduced trees growing in Utah and the Intermountain West, including 1, 070 full color photographs. Users browse through {{a complete list of}} trees or can narrow their choices by selecting from 21 general, growth-related, cultural, and ornamental characteristics...|$|E
40|$|Medical {{language}} {{is at the}} heart of the electronic health record (EHR), with up to 70 percent of <b>the</b> <b>information</b> <b>in</b> <b>that</b> record being recorded in the natural language, free-text portion. In moving from paper medical records to EHRs, we have opened up opportunities for the reuse of this clinical information through automated search and analysis. Natural language, however, is challenging for computational methods. This paper examines the tension between the nuanced, qualitative nature of medical language and the logical, structured nature of computation as well as the way in which these have interacted with each other through the medium of the EHR. The paper also examines the potential for the computational analysis of natural language to overcome this tension...|$|E
40|$|The {{purpose of}} this survey is to gather {{gender-disaggregated}} data on agricultural activities, decision-making, weather information, risk-perception and values from rural households in Kenya, Bangladesh, Uganda and Senegal. The survey ultimately will cover 200 households in each site - the same households that were sampled for the IMPACT Lite Surveys. This survey builds on <b>the</b> <b>information</b> collected <b>in</b> <b>that</b> round, supplementing the detailed productivity related information with data on decision-making and other topics. This information {{will be used for}} researchers, policy-makers and development practitioners to better understand the vulnerabilities of men and women to climate change, how they differ, and what actions can be taken to reduce that vulnerability...|$|R
40|$|Structural <b>information</b> is <b>the</b> most {{important}} content of seismic images. I introduce a numerical algorithm for spreading <b>information</b> <b>in</b> 3 -D volumes {{according to the}} local structure of seismic events. The algorithm consists of two steps. First, local spatially-variable inline and crossline slopes of seismic events are estimated by the plane-wave-destruction method. Next, a seed trace is inserted in the volume, and <b>the</b> <b>information</b> contained <b>in</b> <b>that</b> trace is spread inside the volume, thus automatically “painting ” the data space. Immediate applications of this technique include automatic horizon picking and flattening in applications to both prestack and post-stack seismic data analysis. Synthetic and field data tests demonstrate the effectiveness of predictive painting...|$|R
40|$|The {{first goal}} of the {{proposed}} project is to produce micro samples of two Puerto Rican censuses, for 1910 and for 1920. Both of these censuses are comparable to those fielded in the US during the same years. The most important {{difference is that the}} 1910 Puerto Rican census contains information on duration of union, not to be found in US or, with a handful of exceptions, any other censuses for that matter. As we show below, information on union duration is useful to apply robust tests on the nature of fertility changes. We propose to collect and organize <b>the</b> <b>information</b> <b>in</b> ways <b>that</b> are strictly comparable to those used to collect and organize the US based IPUMS data (Ruggles and Sobeck, 1995). The tw...|$|R
40|$|Good {{translation}} requires {{writing skills}} {{in each of}} its three stages: decoding the original text; transferring its cultural and linguistic element into {{the context of the}} target language; and encoding <b>the</b> <b>information</b> <b>in</b> <b>that</b> context. During decoding, the translator must be conscious of speech level, word usage, cultural references, syntactic devices used for stylistic effect, connotation as well as denotation, and writing skills. The second stage of translation requires making cultural and linguistic elements recognizable in both linguistic communities. Often, this requires some research, and may mean inserting footnotes for clarification. In the third stage, encoding text into the new language and context, writing skills are most clearly needed to make both style and context of the target language text faithful to the original. Translation from Spanish to English is seen to involv...|$|E
40|$|This manual {{was created}} for the {{practicing}} counselor {{as a supplement to}} whatever formal counselor training he or she may have received. It contains information regarding some of the most critical competency areas as well as a variety of other resources upon which the counselor may draw for further study or assistance. The manual is divided into 12 sections which represent the following major competency areas: individual counseling techniques, group counseling techniques, the counseling of culturally diverse clients, drug and alcohol counseling, interviewing skills, psychological assessment, career development, economic self-sufficiency planning, referral strategy, accountability methods and information resources. Each section contains a statement(s) of learning objective(s) followed by a pre-test covering <b>the</b> <b>information</b> <b>in</b> <b>that</b> section. This manual is a resource for increasing counselor awareness while providing a quick information source. (Author...|$|E
40|$|We {{present a}} O(n) {{algorithm}} for generalizing a database relation using concept hierarchies, where n {{is the number}} of tuples in the input relation. The algorithm is based on a variant of Han et al. 's attribute-oriented O(n log n) algorithm. Our algorithm is an on-line algorithm; fast performance is achieved because after encountering a tuple and generalizing it, the location of the appropriate counter to increment is calculated instead of searched for. 1. INTRODUCTION Techniques for knowledge discovery from databases (KDD) attempt to extract implicit, previously unknown, and potentially useful information from large databases [4]. Attribute-oriented generalization is a KDD method which, given a relation retrieved from a relational database, processes it on an attribute by attribute basis and achieves a relatively efficient generalization of <b>the</b> <b>information</b> <b>in</b> <b>that</b> relation [5]. Generalization refers to the replacement of specific attribute values found in the data with more general co [...] ...|$|E
50|$|Besides comprehensiveness, encyclopedic {{writing is}} {{distinguished}} by {{its lack of}} a specific audience or practical application. The author explains facts concisely {{for the benefit of}} a reader who will then use <b>the</b> <b>information</b> <b>in</b> a way <b>that</b> <b>the</b> writer does not try to anticipate. Early examples of encyclopedic writing include discussions of agriculture and craft by Roman writers such as Pliny the Elder and Varro - discussions presumably not intended as practical advice to farmers or craftsmen.|$|R
50|$|In October 2001, the Secretary-General asked Desai {{to act as}} Secretary-General of the World Summit on Sustainable Development (2002 Johannesburg Summit).A {{special feature}} of this Summit was {{the focus on the}} {{developmental}} and environmental aspects of water, energy, agriculture, health and bio-diversity. The Summit also led to practical partnerships between governments, international organisations, the private sector and NGOs to address these concerns. Desai undertook this task in addition to his existing responsibilities. At the end of August 2003, Desai retired from the UN but continued his association with the UN as a Special Adviser to the Secretary General for the World Summit on <b>the</b> <b>Information</b> Society. <b>In</b> <b>that</b> capacity he chaired an international multi-stakeholder Working Group on Internet Governance.|$|R
5000|$|An American {{invention}} {{that was}} barely noticed in 1947 {{went on to}} usher <b>in</b> <b>the</b> <b>Information</b> Age. <b>In</b> <b>that</b> year John Bardeen, William Shockley, and Walter Brattain of Bell Laboratories drew upon highly sophisticated principles of quantum physics to invent the transistor, a small substitute for the bulky vacuum tube. This, and a device invented 10 years later, the integrated circuit, {{made it possible to}} package enormous amounts of electronics into tiny containers. As a result, book-sized computers of today can outperform room-sized computers of the 1960s, {{and there has been a}} revolution in the way people live [...] - [...] in how they work, study, conduct business, and engage in research.|$|R
