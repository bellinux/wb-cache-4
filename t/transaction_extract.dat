0|41|Public
30|$|The ACM-MS {{algorithm}} is a modified CMAR algorithm (Li et al. 2001), a recently developed associative classification algorithm. The data structure CR-tree used in CMAR {{is used to}} store compact <b>transactions</b> <b>extracted</b> from the ADR warehouse and then to generate all of the multiple-drug-single-symptom rules that satisfy the user specified measure.|$|R
40|$|In this paper, {{we propose}} an {{automated}} method {{to decide on}} the number of fuzzy sets and for the autonomous mining of both fuzzy sets and fuzzy association rules. We compare the proposed multi-objective GA based approach with: 1) CURE based approach; 2) Chien et al clustering approach. Experimental results on 100 K <b>transactions</b> <b>extracted</b> from the adult data of United States census in year 2000 show that the proposed method exhibits good performance over the other two approaches in terms of runtime, number of large itemsets and number of association rules. 1...|$|R
40|$|A {{number of}} {{empirical}} analyses of interbank lending rely on indirect inferences from individual interbank <b>transactions</b> <b>extracted</b> from payments data using algorithms. In this paper, we conduct an evaluation {{to assess the}} ability of identifying overnight U. S. fed funds activity from Fedwire payments data. We find evidence that the estimates extracted from the data are statistically significantly correlated with banks' fed funds borrowing as reported on the FRY- 9 C. We find similar associations for fed funds lending, although the correlations are lower. To be conservative, {{we believe that the}} estimates are best interpreted as measures of overnight interbank activity rather than fed funds activity specifically. We also compare the estimates provided by Armantier and Copeland (2012) to the Y- 9 C fed funds amounts...|$|R
40|$|In {{this work}} we propose, gRegress, a new {{algorithm}} which given set of graph transactions {{and a real}} value associated with each graph <b>transaction</b> <b>extracts</b> the complete set of subgraphs such that a) each subgraph in this set has correlation with the real value above a user-specified threshold and b) each subgraph in this set has correlation with any other subgraph in the set below a user-specified threshold. gRegress incorporates novel pruning mechanisms based on correlation of a subgraph feature with the output and correlation with other subgraph features. These pruning mechanisms lead to significant speedup. Experimental results indicate {{that in terms of}} runtime, gRegress substantially outperforms gSpan, often by an order of magnitude while the regression models produced by both approaches have comparable accuracy. ...|$|R
2500|$|Sage {{resolved}} {{to stop him}} but {{had no way of}} going after Dr. Twain without exposing himself. Rodor suggested that Sage use a mask made of Pseudoderm to cover his famous features. [...] Armed with information, and more importantly a disguise, Sage eventually caught up with Dr. Twain, stopping the <b>transaction</b> and <b>extracting</b> a confession, then leaving Twain bound in Pseudoderm. [...] On television, Sage reported on Dr. Twain's illegal activities.|$|R
40|$|We present GHTraffic, a dataset of {{significant}} size comprising HTTP <b>transactions</b> <b>extracted</b> from GitHub data and augmented with synthetic transaction data. This dataset facilitates reproducible research on {{many aspects of}} service-oriented computing. GHTraffic comprises three different editions: Small (S), Medium (M) and Large (L). The S dataset includes HTTP records created from google/guava repository. Guava is a popular Java library containing utilities and data structures. The M dataset includes records from the npm/npm project. It is the popular de-facto standard package manager for JavaScript. The L dataset contains data that were created by selecting eight repositories containing large and very active projects, including twbs/bootstrap, symfony/symfony, docker/docker, Homebrew/homebrew, rust-lang/rust, kubernetes/kubernetes, and angular/angular. js. We also provide access to the scripts used to generate GHTraffic. Using these scripts, the user can modify the configuration properties in the config. properties file {{in order to create}} a customised version of GHTraffic datasets for thier own use. Scripts can be accessed by downloading the pre-configured VirtualBox image (ghtraffic-artifact- 1. 0. 0. ova) or by cloning the repository from [URL]...|$|R
40|$|Abstract—In the Association rule mining, {{originally}} proposed form market basket data, {{has potential}} applications in many areas. Spatial data, such as remote sensed imagery (RSI) data, {{is one of}} the promising application areas. Association Rule mining {{is one of the}} most popular data mining techniques which can be defined as extracting the interesting correlation and relation among large volume of <b>transactions.</b> <b>Extracting</b> interesting patterns and rules from spatial data sets, composed of images and associated ground data, can be of importance in precision agriculture, resource discovery, and other areas. However, in most cases, the sizes of the spatial data sets are too large to be mined in a reasonable amount of time using existing algorithms. In this paper, we propose an efficient approach to derive association rules from spatial data using Peano Count Tree (P-tree) structure. P-tree structure provides a lossless and compressed representation of spatial data. Based on P-trees, an efficient association rule mining algorithm with fast support calculation and significant pruning techniques is introduced to improve the efficiency of the rule mining process. Keywords- Association rule mining, Data mining, Remot...|$|R
40|$|Objectives: The {{current study}} {{contributes}} to the literature through a systematic social observation of the defensive actions of drug sellers within open-air retail markets. The study expands upon previous literature by incorporating a novel data collection and coding method. Methods: Video footage of narcotics <b>transactions</b> was <b>extracted</b> from the closed-circuit television (CCTV) system of the Newark, NJ Police Department. Research-ers transcribed and coded the footage to measure the frequency of defen-sive actions incorporated by drug sellers. Fisher’s exact tests measured whether the frequency of each defensive action significantly differed across geographic setting or time of day. Results: The frequency of many defensiv...|$|R
40|$|Abstract — This paper {{presents}} a debug method for system communications in post-silicon verification. First, we <b>extract</b> <b>transaction</b> sequences at run-time using on-chip circuits and {{store them in}} a trace buffer. Then, we read the stored transactions and analyze them with software. The analysis software tries to find certain patterns in the <b>extracted</b> <b>transactions</b> that are defined by our transaction debug pattern specification language (TDPSL). We have also defined a number of standard patterns for common communication problems such as race and deadlock in TDPSL. To show the feasibility of the method, it is applied {{to a number of}} on chip buses. It is shown that the area overhead of the method is very low. Also we have implemented the analysis software and shown that it is memory efficient, scalable and effective to find bugs. The proposed method can also be applied to fault analysis including transient faults. I...|$|R
40|$|Transaction {{data can}} {{arrive at a}} ferocious rate in the or-der that {{transactions}} are completed. The data contain {{an enormous amount of}} information about customers, not just <b>transactions,</b> but <b>extracting</b> up-to-date customer informa-tion from an ever changing stream of data and mining it in real-time is a challenge. This paper describes a statistically principled approach to designing short, accurate summaries or signatures of high dimensional customer behavior that can be kept current with a stream of transactions. A signature database can then be used for data mining and to provide approximate answers to many kinds of queries about current customers quickly and accurately, as an empirical study of the calling patterns of 96, 000 wireless customers who maxie about 18 million wireless calls over a three month period shows...|$|R
30|$|An {{association}} rule has the form X → Y where X {{is a set}} of antecedent items and Y is the consequent item. The goal is to discover important {{association rule}}s within a corpus such that the presence of a set of terms in an article implies the presence of another term. An association based topic discovery method was proposed in [24] which represents each document in a vector format, where each document vector is considered a transaction. The association rule mining algorithms can then be applied to the <b>transaction</b> set to <b>extract</b> patterns from the corpus.|$|R
40|$|As {{the result}} of {{interactions}} between visitors and a web site, an http log file contains very rich knowledge about users on-site behaviors, which, if fully exploited, can better customer services and site performance. Different {{to most of the}} existing log analysis tools which use statistical counting summaries on pages, hosts, etc., we propose a transaction model to represent users access history and a framework to adapt data mining techniques such as sequence and association rule mining to these transactions. In this framework, all <b>transactions</b> are <b>extracted</b> from the raw log file though a series of step by step data preparation phases. We discuss different methods to identify a user, and separate long convoluted sequences into semantically meaningful sessions and transactions. A new feature called interestingness is defined to model user interests in different web sections. With all the transactions being imported into an adapted cube structure with a concept hierarchy attached to each dimension of it, it is possible to carry out multi-dimensional data mining at multi-abstract levels. Using interest context rules, we demonstrate the potentially significant meaning of this system prototype...|$|R
40|$|AbstractObjectivesThis study {{analyzes}} {{the influence of}} the financial structure of pharmaceutical companies on R&D investment to create a next-generation profit source or develop relatively cost-effective drugs to maximize enterprise value. MethodsThe period of the empirical analysis is from 2000 to 2012. Financial statements and comments in general and internal <b>transactions</b> were <b>extracted</b> from TS- 2000 of the Korea Listed Company Association (KLCA), and data related to stock price is extracted from KISVALUE-Ⅲ of NICE Information Service Co., Ltd. Stata 12. 0 was used as the statistical package for panel analysis. ResultsThe current ratio had a positive influence on R&D investment, the debt ratio had a negative influence on R&D investment, and return on investment and net sales growth rate did not have a significant influence on R&D investment. ConclusionIt was found in this study that the higher liquidity ratio, the greater the R&D investment. The stability of pharmaceutical companies has a negative influence on R&D investment. This finding is consistent with the prediction that if a company faces a financial risk, it will be passive in R&D investment due to its financial difficulties...|$|R
40|$|This paper {{introduces}} a new {{framework for the}} dynamic modelling of univariate and multivariate point processes. The so-called latent factor intensity (LFI) model {{is based on the}} assumption that the intensity function consists of univariate or multivariate observation driven dynamic components and a univariate dynamic latent factor. In this sense, the model corresponds to a dynamic extension of a doubly stochastic Poisson process. We illustrate alternative parameterizations of the observation driven component based on autoregressive conditional intensity (ACI) specifications, as well as Hawkes types models. Based on simulation studies, it is shown that the proposed model provides a flexible tool to capture the joint dynamics of multivariate point processes. Since the latent component has to be integrated out, the model is estimated by simulated maximum likelihood based upon efficient importance sampling techniques. Applications of univariate and bivariate LFI models to <b>transaction</b> data <b>extracted</b> from the German XETRA trading system provide evidence for an improvement of the econometric specification when observable as well as unobservable dynamic components are taken into account. multivariate point process, latent factor, transaction durations, efficient importance sampling...|$|R
50|$|Telecommunication {{networks}} {{can generate}} {{a vast amount}} of transactions, each transaction containing information about a particular subscriber's activity. Telecommunication network consist of various interacting devices and platforms, any transaction done by a subscriber is often recorded in multiple devices as it passes through the network. Telecommunication organizations {{need to be able to}} <b>extract</b> <b>transaction</b> information from these various network elements in order to correctly bill subscribers for the usage on the network. Transaction processing system is a subset of information systems, and in the telecommunications industry, forms an integral part of the management information system. TPS can be regarded as the link between the various network elements and platforms and the information management uses to drive the business.|$|R
40|$|The {{possibility}} to analyze everyday monetary transactions {{is limited by}} the scarcity of available data, as this kind of information is usually considered highly sensitive. Present econophysics models are usually employed on presumed random networks of interacting agents, and only macroscopic properties (e. g. the resulting wealth distribution) are compared to real-world data. In this paper, we analyze BitCoin, which is a novel digital currency system, where the complete list of transactions is publicly available. Using this dataset, we reconstruct the network of <b>transactions,</b> and <b>extract</b> the time and amount of each payment. We analyze the structure of the transaction network by measuring network characteristics over time, such as the degree distribution, degree correlations and clustering. We find that linear preferential attachment drives the growth of the network. We also study the dynamics taking place on the transaction network, i. e. the flow of money. We measure temporal patterns and the wealth accumulation. Investigating the microscopic statistics of money movement, we find that sublinear preferential attachment governs the evolution of the wealth distribution. We report a scaling relation between the degree and wealth associated to individual nodes. Comment: Project website: [URL] updated after publicatio...|$|R
40|$|Purpose – Big data {{produced}} by mobile apps contains valuable knowledge about customers and markets {{and has been}} viewed as productive resources. This study proposes a multiple methods approach to elicit intelligence and value from big data by analysing customer behaviour in mobile app usage. Design/methodology/approach – The big data analytical approach is developed using three data mining techniques: RFM (Recency, Frequency, Monetary) analysis, link analysis, and association rule learning. We then conduct a case study to apply the approach to analyse the <b>transaction</b> data <b>extracted</b> from a mobile app. Findings – The approach can identify high-value and mass customers, and understand their patterns and preferences in using {{the functions of the}} mobile app. Such knowledge enables the developer to capture the behaviour of large pools of customers and to improve products and services by mixing and matching functions and offering personalised promotions and marketing information. Originality/value – The approach used in this study balances complexity with usability, thus facilitating corporate use of big data in making product improvement and customisation decisions. The approach allows developers to gain insights into customer behaviour and function usage preferences by analysing big data. The identified associations between functions can also help developers improve existing, and design new, products and services to satisfy customers’ unfulfilled requirement...|$|R
40|$|AbstractObjectivesThe {{purpose of}} this study is to analyze the {{influence}} of the corporate governance of pharmaceutical companies on research and development (R&D) investment. MethodsThe period of the empirical analysis is from 2000 to 2012. Financial statements and comments in general, and internal <b>transactions</b> were <b>extracted</b> from TS- 2000 of the Korea Listed Company Association. Sample firms were those that belong to the medical substance and drug manufacturing industries. Ultimately, 786 firm-year data of 81 firms were included in the sample (unbalanced panel data). ResultsThe shareholding ratio of major shareholders and foreigners turned out to have a statistically significant influence on R&D investment (p <  0. 05). No statistical significance was found in the shareholding ratio of institutional investors and the ratio of outside directors. ConclusionThe higher the shareholding ratio of the major shareholders, the greater the R&D investment. There will be a need to establish (or switch to) a holding company structure. Holding companies can directly manage R&D in fields with high initial risks, and they can diversify these risks. The larger the number of foreign investors, the greater the R&D investment, indicating that foreigners directly or indirectly impose pressure on a manager to make R&D investments that bring long-term benefits...|$|R
40|$|Analytics-driven {{solutions}} for customer targeting and sales-force allocation Sales professionals {{need to identify}} new sales prospects, and sales executives need to deploy the sales force against the sales accounts with the best potential for future revenue. We describe two analytics-based solutions developed within IBM to address these related issues. The Web-based tool OnTARGET provides a set of analytical models to identify new sales opportunities at existing client accounts and noncustomer companies. The models estimate the probability of purchase at the product-brand level. They use training examples drawn from historical <b>transactions</b> and <b>extract</b> explanatory features from transactional data joined with company firmographic data (e. g., revenue and number of employees). The second initiative, the Market Alignment Program, supports sales-force allocation based on field-validated analytical estimates of future revenue opportunity in each operational market segment. Revenue opportunity estimates are generated by defining the opportunity as a high percentile of a conditional distribution of the customer’s spending, that is, what we could realistically hope to sell to this customer. We describe the development of both sets of analytical models, the underlying data models, and the Web sites used to deliver the overall solution. We conclude {{with a discussion of}} the business impact of both initiatives...|$|R
40|$|Data {{recovery}} for malicious committed transactions af-ter attacks increasingly becomes an important issue. Dam-age assessment for data recovery requires a transaction log which record data items read or written by all mali-cious and benign transactions. Unfortunately, conventional undo/redo log could not record read operations for trans-actions; and existing auditing mechanisms in DBMS could not capture operations for data items. In this paper, we introduce {{a concept of}} “Extended Read Operations ” and illustrate how the Extended Read Operations would cause the damage spreading, and then a Fine Grained Transac-tion Log (FGTL) is proposed. The log records all the data items of the read only and update-involved operations (read and write) for the committed <b>transactions,</b> and even <b>extracts</b> data items read by the subqueries in the SQL statements. A prototype system denoted FGTL Generator is developed to generate the FGTL. Experiments based on TPC-W Bench-mark show the availability for FGTL Generator...|$|R
40|$|The {{association}} rule mining {{is one of}} the important area for research in data mining. In {{association rule}} mining online association rule mining {{is one of the}} hottest area due to the reason that the knowledge embedded in the data stream {{is more likely to be}} changed as time goes by. This paper proposes an algorithm as well as a datastructure for online data mining. In this method the pruning in the data structure as well as the frequent itemset generation will be based on the request. The data structure which we introducing will have the capability to maintain the transactions in the sorted order. Every <b>transaction</b> can be <b>extracted</b> from the item-Order-Tree as by doing the traversal in depth. Frequent itemset can be generated as by do the traversal from the parent node that the user requested for. This ItemOrder-Tree improves the performance of the online association rule mining...|$|R
40|$|AbstractObjectivesThe aim of {{this study}} is to analyze the {{influence}} of the research and development (R&D) investment of pharmaceutical companies on enterprise value. MethodsThe period of the empirical analysis is from 2000 to 2012, considering the period after the influence of the financial crisis. Financial statements and comments in general and internal <b>transactions</b> were <b>extracted</b> from TS- 2000 of the Korea Listed Company Association, and data related to stock price were extracted from KISVALUE-III of National Information and Credit Evaluation Information Service Co., Ltd. STATA 12. 0 was used as the statistical package for panel analysis. ResultsIn the pharmaceutical firms, the influence of the R&D intensity with regard to Tobin's q was found to be positive. However, only the R&D expenditure intensities of previous years 2 and 5 (t– 2 and t– 5, respectively) were statistically significant (p <  0. 1), whereas those of previous years 1, 3, and 4 years (t– 1, t– 3, and t– 4, respectively) were not statistically significant. ConclusionR&D investment not only affects the enterprise value but is also evaluated as an investment activity that raises the long-term enterprise value. The research findings will serve as valuable data to understand the enterprise value of the Korea pharmaceutical industry and to strengthen reform measures. Not only should new drug development be made, but also investment and support should be provided according to the specific factors suitable to improve the competitiveness of each company, such as generic, incrementally modified drugs, and biosimilar products...|$|R
30|$|The data {{processing}} procedure is as follows. First, we <b>extract</b> <b>transactions</b> made between September 4, 2000 and December 31, 2015. The {{choice of the}} initial date {{is based on the}} introduction of the ONL category [13]. This leaves us with 1, 119, 258 ON and 73, 480 ONL transactions, which comprise 86 % of all the transactions during that period. Next, we transform all the ON and ONL transactions into a sequence of daily networks by applying the daily time window of 8 : 00 – 18 : 00  [28]. We then <b>extract</b> the <b>transactions</b> that belong to the largest weakly connected component of each daily network, which account for 99.3 % of all the daily transactions on average (the minimum is 78 %). We referred to this component as daily network throughout the analysis. Multiple edges between two banks are simplified. In the end, we have 1, 187, 415 transactions conducted by 308 financial institutions over 3922 business days.|$|R
40|$|Web access {{patterns}} {{can provide}} valuable information for website designers in making website-based communication more efficient. To extract interesting or useful web access patterns, we use data mining techniques which analyze historical web access logs. In this paper, we present an efficient approach to mine {{the most interesting}} web access associations, where the word "interesting" denotes patterns that are supported by a high fraction of access activities with strong confidence. Our approach consists of three steps: 1) transform raw web logs to a relational table; 2) convert the relational table to a collection of access transactions; 3) mine the <b>transaction</b> collection to <b>extract</b> associations and rules. In both step 1 and step 2, we provide users with an effective mechanism to help them generate only "interesting" access records and transactions for mining. In the third step, we present a new efficient data mining algorithm {{to find the most}} interesting web access associat [...] ...|$|R
40|$|Abstract- The {{association}} rule mining {{is one of}} the important area for research in data mining. In {{association rule}} mining online association rule mining {{is one of the}} hottest area due to the reason that the knowledge embedded in the data stream {{is more likely to be}} changed as time goes by. This paper proposes an algorithm as well as a data structure for online data mining. In this method the pruning in the data structure as well as the frequent itemset generation will be based on the request. The data structure which we introducing will have the capability to maintain the transactions in the sorted order. Every <b>transaction</b> can be <b>extracted</b> from the item-Order-Tree as by doing the traversal in depth. Frequent itemset can be generated as by do the traversal from the parent node that the user requested for. This ItemOrder-Tree improves the performance of the online association rule mining...|$|R
40|$|Data {{recovery}} {{techniques for}} malicious transactions are increasingly becoming {{an important issue}} since the security for DBMSs are mainly prevention based, and they cannot defend systems from unknown attacks. Survivability and availability are essential for modern DBMSs, which require the database provide continuous services {{in the period of}} recovery, namely dynamic recovery. In this paper, we presented a data recovery model and introduce extended read-write dependency and phantoms dependency to the model. A fine grained transaction log is proposed for data recovery. The log records all the data items of the read and update-involved operations for the committed <b>transactions,</b> and even <b>extracts</b> data items read by the subqueries in the SQL statements. Based on the log, we develop a dynamic recovery system to implement the data recovery model. The system could provide continuous services while the recovery is processing. Experiments based on TPC-W benchmark show that the dynamic recovery system is high-efficient and reliable...|$|R
40|$|Abstract In web context, a {{self-healing}} database {{system which}} {{has the ability to}} automatically locate and undo the set of transactions that are corrupted by malicious attacks is in urgent need. The metrics of survivability and availability require the database provide continuous services in the period of recovery, which is referred to as dynamic recovery. In this paper, we present a Fine Grained Transaction Log to serve for the damage assessment. The log records all the data items of the read only and update-involved operations for the committed <b>transactions,</b> and even <b>extracts</b> data items read by the subqueries in the SQL statements. Based on the log, we propose a Dynamic Recovery System to implement the damage repair. The system retains the execution results for blind write transactions and gives a complete solution to the issues of recovery conflicts caused by Forward Recovery. Moreover, a confinement activity is imposed on the in-repairing data to prevent a further damage propagation while the data recovery is processing. The integrity measurement and performance evaluation in our experiments show that the system is reliable and high-efficient...|$|R
40|$|The {{similarities}} between the viatical settlement market and the standard market for insurance evoke {{the question of what}} effect asymmetric information, an oft discussed topic surrounding standard markets, might have in the viatical settlement market. In order to examine the possibility of the existence of adverse selection in this market, as well as {{to gain a better understanding}} of the dynamics of the market in general, I analyze a model describing consumer behavior in this market. Applying the results of my analysis to the logic of a classic adverse selection model, I develop and implement a test of adverse selection in the viatical settlement market using nationally representative data on viatical <b>transactions.</b> Additionally, I <b>extract</b> a new prediction from the consumer behavior model relating the probability of selling life insurance to both the policy’s face value and the policyholder’s mortality risk, and I test this prediction against data on HIV+ individuals. In the end I find that there is no empirical evidence for adverse selection in the viatical settlement market, and that the consumer behavior model is largely consistent with the data...|$|R
40|$|Online {{organizations}} {{are always in}} search for innovative marketing strategies to better satisfy their current website users and lure new ones. Thus, recently, many organizations have started to retain all transactions taking place on their website, and tried to utilize this information to better understand and satisfy their users. However, due to the huge amount of transaction data, traditional methods are neither possible nor cost-effective. Hence, the use of effective and automated methods to handle these transactions became imperative. Web Usage Mining {{is the process of}} applying data mining techniques on web log data (<b>transactions)</b> to <b>extract</b> the most interesting usage patterns. The usage patterns are stored as profiles (a set of URLs) {{that can be used in}} higher-level applications, e. g. a recommendation system, to meet the company 2 ̆ 7 s business goals. A lot of research has been conducted on Web Usage Mining, however, little has been done to handle the dynamic nature of web content, the spontaneous changing behavior of users, and the need for scalability in the face of large amounts of data. This thesis proposes a framework that helps capture the changing nature of user behavior on a website. The framework is designed to be applied periodically on incoming web transactions, with new usage data that is similar to older profiles used to update these old profiles, and distinct transactions subjected to a new pattern discovery process. The result of this framework is a set of evolving profiles that represent the usage behavior at any given period of time. These profiles can later be used in higher-level applications, for instance to predict the evolving user 2 ̆ 7 s interest as part of an intelligent web personalization framework...|$|R
40|$|In recent times, {{the mining}} of {{association}} rules from XML databases has received attention {{because of its}} wide applicability and flexibility. Many mining methods have been proposed. Because of the inherent flexibility of the structures and the semantics of the documents, however, these methods are challenging to use. In order to accomplish the mining, an XML document must first be converted into a relational dataset, and an index table with node encoding is created to <b>extract</b> <b>transactions</b> and interesting items. In this paper, we propose a new method to mine association rules from XML documents using {{a new type of}} node encoding scheme that employs a Unique Identifier (UID) to extract the important items. The node scheme modified with UID encoding speeds up the mining process. A significance measure is used to identify the important rules found in the XML database. Finally, the mining procedure calculates the confidence that the identified rules are indeed meaningful. Experiments are conducted using XML databases available in the XML data repository. The results illustrate that the proposed method is efficient in terms of computation time and memory usage...|$|R
40|$|Working Paper: Comments {{most welcome}} Yes. I aim to {{establish}} empirically that the “equity premium ” puzzle, with its 6 % excess return pa over Treasury bills {{for the last}} 100 years on the NYSE, is explained once the value of endogenous stock market trading is incorporated into investor preferences. According to my model, the “investor surplus ” from trading liquid Treasury bills relative to illiquid equity is exactly compensated for by the expected equity premium. Observed transaction cost and liquidity differentials between equity and bills {{are consistent with the}} premium. Extensive tests are carried out on Australian and US NYSE data for 1955 - 98. The puzzle concerning the volatility of the stochastic discount factor also appears to be explained by trading behavior, which is of comparable volatility. Reasonably accurate estimates of <b>transactions</b> costs are <b>extracted</b> just from daily dividend yields and turnover. Transaction costs would need to be 400 % higher to explain the premium by the “amortized spread”, together with exogenous trading and habit formation. An ability to create unlimited wealth, implicit in some existing models, no longer applies. Additionally, the model explains the further 15 - 20 % pa discount on illiquid “letter ” stock. Key words: equity premium, asset prices, liquidity, trading, transaction cost, amortized spread...|$|R
40|$|Yes. I aim to {{establish}} empirically that the “equity premium ” puzzle, with its 6 % excess return per annum over Treasury bills {{for the last}} 100 years on the NYSE, can be explained once the value of endogenous stock market trading is incorporated into investor preferences. Within my framework, investors enjoy trading. According to my model, the “investor surplus ” from trading liquid Treasury bills relative to illiquid equity is exactly compensated for by the expected equity premium. Observed transaction cost and liquidity differentials between equity and bills {{are consistent with the}} premium. Extensive tests are carried out on Australian and US NYSE data for 1955 - 98. The puzzle concerning the volatility of the stochastic discount factor also appears to be explained by trading behavior, which is of comparable volatility. Reasonably accurate estimates of <b>transactions</b> costs are <b>extracted</b> just from daily dividend yields and turnover. Transaction costs would need to be 400 % higher to explain the premium by the “amortized spread”, together with exogenous trading and habit formation. An ability to create unlimited wealth, implicit in some existing models, no longer applies. Additionally, the model explains the further 15 - 20 % pa discount on illiquid “letter ” stock...|$|R
40|$|As {{our first}} step in {{compiling}} the data on sovereign wealth funds, we created a preliminary sample by combining the profiles of the funds published by J. P. Morgan (Fernandez and Eschweiler 2008) and Preqin (Friedman 2008). In the cases where the two databases use different names for the same sovereign wealth fund, we employ the fund address and related information to eliminate duplicates. We add five funds to the sample that {{were not included in}} these two compilations but are frequently described as sovereign wealth funds in {{at least one of the}} investment datasets noted below. This initial search yields a population of 69 institutions, including some sovereign wealth funds that have been announced but are not yet active. We then merge this initial sample of funds with the available data on characteristics of such funds and on their direct investments (and the investments of subsidiaries in which the fund has at least a 50 percent ownership stake). To <b>extract</b> <b>transactions</b> involving subsidiaries of sovereign wealth fund, we supplement our list of subsidiaries by employing ownership data in the Directory of Corporate Affiliations and Bureau van Dijk’s Orbis. Second, we track the investments of sovereign wealth funds. We sought such information in Dealogic’s M&A Analytics, SDC’s Platinum M&A, and Bureau van Dijk’s Zephyr...|$|R
40|$|Politeness is an {{important}} social element in the Malaysian society and it is gauged by the way people behave toward each other during interactions. In this context, politeness is taken to mean good manners such as greeting, acknowledging and thanking others. Taking the cue from the Malaysian government which emphasizes on showing good manners, this paper examines the public transactions of front counter staff and patients in nine Malaysian private hospitals. Focus {{was given to the}} use of openings and closings in 158 <b>transactions</b> which were <b>extracted</b> over a period of three months via close observations which were allowed by the gatekeepers manning the front counters. Data were then orthographically transcribed. Brown and Levinson's (1987) notion of politeness and the Malaysian concept of good manners such as greeting and thanking were applied as a framework. Our analysis indicates that front counter staff in private hospitals employed more impolite openings {{but at the end of}} the transactions, they used more polite closings. A closer analysis of the data indicates that these polite closings were often given in response to patients' initiations. Although our findings are small in comparison, we believe they will benefit researchers of communication, curriculum designers and practitioners as these findings clearly indicate that there is a need for professional communication skills to be taught and implemented in service industries...|$|R
40|$|Generalized {{association}} rule mining {{is an extension}} of traditional {{association rule}} mining to discover more informative rules, given a taxonomy. In this pisB, we describe a formal framework for thep oblem of mining generalized association rules. In the framework, The subset-supj set and thepF ent-child relationship among generalized itemsets are introduced top esent the different views of generalized itemsets, i. e. the lattice of generalized itemsets and the taxonomies of k-generalized itemsets,respBj@zDB(@ Wep esent an op timization technique to reduce the time consuming byap@GqF@ two constraints each of which correspFjD to each view of generalized itemsets. In the miningp ocess, a new set enumeration algorithm, named SET, that utilizes these constraints to fasten mining all generalized frequent itemsets isp ropF ed. ByexpW iments on synthetic data, the results show that SET outpt forms the current most efficient algorithm, Prutax, by an order of magnitude or more. 1. Introd 6 %P 5 P 6 In the areaof Knowledge Discovery in Databases (KDD), association rule mining is one of the important tasks. It wasf irst introduced in [1] tof ind the set of all subsets of items (called itemsets) thatf requently occur in many database records or <b>transactions,</b> and to <b>extract</b> the rules telling us how a subset of items inf luences the presenceof another subset [2]. Nevertheless, association rules may not provide desired knowledge in the database. It may be limited with the granularityf actors over the items. For example, suppose that the database keeps a set of transactions, where chocolate milk tends to be purchased together with wheat bread. We may obtain a ruleof " 5 % of customers who buy wheat breads, also buy chocolate milk". At this point, it is more intuitive or more inf ormative [...] ...|$|R
40|$|Frequent Itemsets (FIs) mining is a {{fundamental}} primitive in data mining that requires to identify all itemsets appearing in a fraction at least θ of a transactional dataset D. Often though, {{the ultimate goal of}} mining D is not an analysis of the dataset per se, but the understanding of the underlying process that generated D. Specifically, in many applications D is a collection of samples obtained from an unknown probability distribution pi on <b>transactions,</b> and by <b>extracting</b> the FIs in the dataset D one attempts to infer itemsets that are frequently generated by pi, which we call the True Frequent Itemsets (TFIs). Due to the inherently random nature of the generative process, the set of FIs is only a rough approximation to the set of TFIs, as it often contains a huge number of spurious itemsets, i. e., itemsets that are not among the TFIs. In this work we present two methods to identify a collection of itemsets that contains only TFIs with probability at least 1 − δ (i. e., the methods have Family-Wise Error Rate bounded by δ), for some user-specified δ, without imposing any restriction on pi. Our methods are distribution-free and make use of results from statistical learning theory involving the (empirical) VC-dimension of the problem at hand. This allows us to identify a larger fraction of the TFIs (i. e., to achieve higher statistical power) than what could be done using traditional multiple hypothesis testing corrections. In the experimental evaluation we compare our methods to established techniques (Bonferroni correction, holdout) and show that they return a very large subset of the TFIs, achieving a very high statistical power, while controlling the Family-Wise Error Rate. ...|$|R
