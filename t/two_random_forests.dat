0|10000|Public
40|$|This paper {{presents}} an impact assessment for the imputation of missing data. The data set used is HIV Seroprevalence data from an antenatal clinic study survey performed in 2001. Data imputation is performed through five methods: <b>Random</b> <b>Forests,</b> Autoassociative Neural Networks with Genetic Algorithms, Autoassociative Neuro-Fuzzy configurations, and <b>two</b> <b>Random</b> <b>Forest</b> and Neural Network based hybrids. Results indicate that <b>Random</b> <b>Forests</b> are superior in imputing missing data in terms both of accuracy and of computation time, with accuracy increases {{of up to}} 32 % on average for certain variables when compared with autoassociative networks. While the hybrid systems have significant promise, they are hindered by their Neural Network components. The imputed data is used to test for impact in three ways: through statistical analysis, HIV status classification and through probability prediction with Logistic Regression. Results indicate that these methods are fairly immune to imputed data, and that the impact is not highly significant, with linear correlations of 96 % between HIV probability prediction {{and a set of}} two imputed variables using the logistic regression analysis...|$|R
40|$|We propose {{novel model}} transfer-learning methods that refine a {{decision}} forest model M learned within a "source" domain using a training set sampled from a "target" domain, {{assumed to be}} a variation of the source. We present <b>two</b> <b>random</b> <b>forest</b> transfer algorithms. The first algorithm searches greedily for locally optimal modifications of each tree structure by trying to locally expand or reduce the tree around individual nodes. The second algorithm does not modify structure, but only the parameter (thresholds) associated with decision nodes. We also propose to combine both methods by considering an ensemble that contains the union of the two forests. The proposed methods exhibit impressive experimental results over a range of problems. Comment: 2 columns, 14 pages, TPAMI submitte...|$|R
40|$|A {{search for}} new {{phenomena}} in final states {{with one or more}} energetic jets and large missing transverse momentum is presented. An integrated luminosity of 20 fb^ 1 is collected from √ = 8 TeV collisions at the LHC with the ATLAS detector during 2012 operations. Standard Model backgrounds and systematics uncertainties are estimated using a maximum likelihood procedure. The number of events passing this selection criteria is in good agreement with Standard Model expectations. These events are then divided up into three orthogonal signal regions based on the outputs of <b>two</b> <b>Random</b> <b>Forest</b> classifiers trained to classify invisible decays of the Higgs boson produced through the vector boson fusion and Higgs-strahlung production modes. These results are then interpreted in terms of three different Higgs-portal models and translated into upper limits on WIMP-nucleon cross-sections for comparison with direct detection experiments...|$|R
40|$|A {{system that}} can {{automatically}} detect nodules within lung images may assist expert radiologists in interpreting the abnormal patterns as nodules in 2 D CT lung images. A system is presented that can automatically identify nodules of various sizes within lung images. The pattern classification method is employed to develop the proposed system. A <b>random</b> <b>forest</b> ensemble classifier is formed consisting of many weak learners that can grow decision trees. The forest selects the decision that has the most votes. The developed system consists of <b>two</b> <b>random</b> <b>forest</b> classifiers connected in a series fashion. A subset of CT lung images from the LIDC database is employed. It consists of 5721 images to train and test the system. There are 411 images that contained expert- radiologists identified nodules. Training sets consisting of nodule, non-nodule, and false-detection patterns are constructed. A collection of test images are also built. The first classifier is developed to detect all nodules. The second classifier is developed to eliminate the false detections produced by the first classifier. According to the experimental results, a true positive rate of 100 %, and false positive rate of 1. 4 per lung image are achieved. <br /...|$|R
40|$|Monitoring of {{dialysis}} sessions {{is crucial}} as different stress factors can yield suffering or critical situations. Specialized personnel is usually {{required for the}} administration of this medical treatment; nevertheless, subjects whose clinical status can be considered stable require different monitoring strategies when compared with subjects with critical clinical conditions. In this case domiciliary treatment or monitoring can substantially {{improve the quality of}} life of patients undergoing dialysis. In this work, we present a Computer Aided Detection (CAD) system for the telemonitoring of patients' clinical parameters. The CAD was mainly designed to predict the insurgence of critical events; it consisted of <b>two</b> <b>Random</b> <b>Forest</b> (RF) classifiers: the first one (R F 1) predicting the onset of any malaise one hour after the treatment start and the second one (R F 2) again two hours later. The developed system shows an accurate classification performance in terms of both sensitivity and specificity. The specificity in the identification of nonsymptomatic sessions and the sensitivity in the identification of symptomatic sessions for R F 2 are equal to 86. 60 % and 71. 40 %, respectively, thus suggesting the CAD as an effective tool to support expert nephrologists in telemonitoring the patients...|$|R
40|$|Climate {{change will}} impact {{bioclimatic}} drivers that regulate the geospatial distribution of dryland agro-ecological classes (AECs). Characterizing the geospatial relationship between present AECs and their bioclimatic controls will provide insights into potential future shifts in AECs as climate changes. The major objectives {{of this study}} are to quantify empirical relationships between bioclimatic variables and the current geospatial distribution of six dryland AECs of the inland Pacific Northwest (iPNW) of the United States; and apply bioclimatic projections from downscaled climate models to assess geospatial shifts of AECs under current production practices. <b>Two</b> <b>Random</b> <b>Forest</b> variable selection algorithms, VarSelRF and Boruta, were used to identify relevant bioclimatic variables. Three bioclimatic variables were identified by VarSelRF as useful for predictive <b>Random</b> <b>Forest</b> modeling of six AECs: (1) Holdridge evapotranspiration index; (2) spring precipitation (March, April, and May); and (3) precipitation of the warmest 4 -month season (June, July, August, and September). Super-imposing future climate scenarios onto current agricultural production systems resulted in significant geospatial shifts in AECs. The <b>Random</b> <b>Forest</b> model projected a 58 and 63 % increase in area under dynamic annual crop-fallow-transition (AC-T) and dynamic grain-fallow (GF) AECs, respectively. By contrast, a 46 % decrease in area was projected for stable AC-T and dynamic annual crop (AC) AECs across all future time periods for Representative Concentration Pathway (RCP) 8. 5. For the same scenarios, the stable AC and GF AECs showed the least declines in area (8 and 13 %, respectively), compared to other AECs. Future spatial shifts from stable to dynamic AECs, particularly to dynamic AC-T and dynamic GF AECs would result in more use of fallow, a greater hazard for soil erosion, greater cropping system uncertainty, and potentially less cropping system flexibility. These projections are counter to cropping system goals of increasing intensification, diversification, and productivity...|$|R
40|$|Genetic {{analysis}} of complex diseases demands novel analytical methods to interpret data collected {{on thousands of}} variables by genome-wide association studies. The complexity of such analysis is multiplied when one has to consider interaction effects, be they among the genetic variations (G × G) or with environment risk factors (G × E). Several statistical learning methods seem quite promising in this context. Herein we consider applications of <b>two</b> such methods, <b>random</b> <b>forest</b> and Bayesian networks, to the simulated dataset for Genetic Analysis Workshop 16 Problem 3. Our evaluation study showed that an iterative search based on the <b>random</b> <b>forest</b> approach has the potential in selecting important variables, while Bayesian networks can capture some of the underlying causal relationships...|$|R
40|$|International audienceHuman action {{recognition}} is a challenging field {{that have been}} addressed with many different classification techniques such as SVM or <b>Random</b> Decision <b>Forests</b> and by considering {{many different kinds of}} information joints, key poses, joints rotation matrix, angles for example. This paper presents our approach for action recognition that considers only information given by the 3 D joints from the skeleton and trains a <b>two</b> stage <b>random</b> <b>forest</b> to classify them. We extract skeletal features by computing all angles between any triplet of joints and all distances between any pair of joints then organizing them into a feature vector for each static pose. Complex dynamic actions are then described by sequences of such feature vectors. We evaluate our approach on the most recent and the largest benchmark, MSRC- 12 Kinect Gesture Dataset, and compare our results with the state-of-the-art methods on this dataset...|$|R
40|$|Aluminium/alumina/graphite hybrid {{metal matrix}} {{composites}} manufactured using stir casting technique {{was subjected to}} machining studies to predict tool condition during machining. Fresh tool as well as tools with specific amount of wear deliberately created prior to machining experiments was used. Vibration signals were acquired using an accelerometer for each tool condition. These signals were then processed to extract statistical and histogram features to predict the tool condition during machining. <b>Two</b> classifiers namely, <b>Random</b> <b>Forest</b> and Classification and Regression Tree (CART) were used to classify the tool condition. Results showed that histogram features with <b>Random</b> <b>Forest</b> classifier yielded maximum efficiency in predicting the tool condition. This machine learning approach enables the prediction of tool failure in advance, thereby minimizing the unexpected breakdown of tool and machine...|$|R
40|$|Abstract Genetic {{analysis}} of complex diseases demands novel analytical methods to interpret data collected {{on thousands of}} variables by genome-wide association studies. The complexity of such analysis is multiplied when one has to consider interaction effects, be they among the genetic variations (G × G) or with environment risk factors (G × E). Several statistical learning methods seem quite promising in this context. Herein we consider applications of <b>two</b> such methods, <b>random</b> <b>forest</b> and Bayesian networks, to the simulated dataset for Genetic Analysis Workshop 16 Problem 3. Our evaluation study showed that an iterative search based on the <b>random</b> <b>forest</b> approach has the potential in selecting important variables, while Bayesian networks can capture some of the underlying causal relationships. </p...|$|R
40|$|Relative {{importance}} of regressor variables {{is an old}} topic that still awaits a satisfactory solution. When interest is in attributing importance in linear regression, averaging over orderings methods for decomposing R 2 are among the state-of-theart methods, although the mechanism behind their behavior is not (yet) completely understood. Random forests—a machinelearning tool for classification and regression proposed a few years ago—have an inherent procedure of producing variable importances. This article compares the two approaches (linear model {{on the one hand}} and <b>two</b> versions of <b>random</b> <b>forests</b> on the other hand) and finds both striking similarities and differences, some of which can be explained whereas others remain a challenge. The investigation improves understanding of the nature of variable importance in <b>random</b> <b>forests.</b> This article has supplementary material online...|$|R
40|$|The {{motivation}} of my dissertation {{is to improve}} <b>two</b> weaknesses of <b>Random</b> <b>Forests.</b> One, the failure to detect genetic interactions between two single nucleotide polymorphisms (SNPs) in higher dimensions when the interacting SNPs both have weak main effects and two, the difficulty of interpretation in comparison to parametric methods such as logistic regression, linear discriminant analysis, and linear regression. We focus on detecting pairwise SNP interactions in genome case-control studies. We determine the best parameter settings to optimize the detection of SNP interactions and improve the efficiency of <b>Random</b> <b>Forests</b> and present an efficient filtering method. The filtering method is compared to leading methods and is shown that it is computationally faster with good detection power. <b>Random</b> <b>Forests</b> allows us to identify clusters, outliers, and important features for subgroups of observations through the visualization of the proximities. We improve the interpretation of <b>Random</b> <b>Forests</b> through the proximities. The result of the new proximities are asymmetric, and the appropriate visualization requires an asymmetric model for interpretation. We propose a new visualization technique for asymmetric data and compare it to existing approaches...|$|R
40|$|The growing {{importance}} and utilization of measuring brain waves (e. g. EEG signals of eye state) in brain-computer interface (BCI) applications highlighted {{the need for}} suitable classification methods. In this paper, a comparison between three of well-known classification methods (i. e. support vector machine (SVM), hidden Markov map (HMM), and radial basis function (RBF)) for EEG based eye state classification was achieved. Furthermore, a suggested method {{that is based on}} ensemble model was tested. The suggested (ensemble system) method based on a voting algorithm with <b>two</b> kernels: <b>random</b> <b>forest</b> (RF) and Kstar classification methods. The performance was tested using three measurement parameters: accuracy, mean absolute error (MAE), and confusion matrix. Results showed that the proposed method outperforms the other tested methods. For instance, the suggested method's performance was 97. 27 % accuracy and 0. 13 MAE. Comment: 7 page...|$|R
40|$|Climate {{envelope}} {{models are}} widely used to forecast potential {{effects of climate change}} on species distributions. A key issue in climate envelope modeling is the selection of predictor variables that most directly influence species. To determine whether model performance and spatial predictions were related to the selection of predictor variables, we compared models using bioclimate variables with models constructed from monthly climate data for twelve terrestrial vertebrate species in the southeastern USA using <b>two</b> different algorithms (<b>random</b> <b>forests</b> or generalized linear models), and two model selection techniques (using uncorrelated predictors or a subset of user-defined biologically relevant predictor variables). There were no differences in performance between models created with bioclimate or monthly variables, but one metric of model performance was significantly greater using the <b>random</b> <b>forest</b> algorithm compared with generalized linear models. Spatial predictions between maps using bioclimate and monthly variables were very consistent using the <b>random</b> <b>forest</b> algorithm with uncorrelated predictors, whereas we observed greater variability in predictions using generalized linear models...|$|R
40|$|<b>Random</b> <b>forests</b> {{have been}} {{introduced}} by Leo Breiman (2001) as a new learning algorithm, extend-ing the capabilities of decision trees by aggregating and randomising them. We explored {{the effects of the}} introduction of noise and irrelevant variables in the training set on the learning curve of a ran-dom forest classifier and compared them to the results of a classical decision tree algorithm inspired by Breiman's CART (1984). This study was realized by simulating 23 artificial binary concepts pre-senting a wide range of complexity and dimension (4 to 10 relevant variables), adding different noise and irrelevant variables rates to learning samples of various sizes (50 to 5000 examples). It ap-peared that <b>random</b> <b>forests</b> and individual decision trees have different sensitivities to those pertur-bation factors. The initial slope of the learning curve is more affected by irrelevant variables than by noise on both algorithms, but counterintuitively <b>random</b> <b>forests</b> show a greater sensitivity to noise than decision trees for this parameter. Globally, average learning speed is quite similar between the <b>two</b> algorithms but <b>random</b> <b>forests</b> better exploit both small and big samples: their learning curve starts lower and is not affected by the asymptotical limitation showed by single decision trees...|$|R
40|$|Abstract Background Population-based {{investigations}} {{aimed at}} uncovering genotype-trait associations often involve high-dimensional genetic polymorphism data {{as well as}} information on multiple environmental and clinical parameters. Machine learning (ML) algorithms offer a straightforward analytic approach for selecting subsets of these inputs that are most predictive of a pre-defined trait. The performance of these algorithms, however, in the presence of covariates is not well characterized. Methods and Results In this manuscript, we investigate <b>two</b> approaches: <b>Random</b> <b>Forests</b> (RFs) and Multivariate Adaptive Regression Splines (MARS). Through multiple simulation studies, the performance under several underlying models is evaluated. An application to a cohort of HIV- 1 infected individuals receiving anti-retroviral therapies is also provided. Conclusion Consistent with more traditional regression modeling theory, our findings highlight the importance of considering the nature of underlying gene-covariate-trait relationships before applying ML algorithms, particularly when there is potential confounding or effect mediation. </p...|$|R
40|$|Tropical dry and {{deciduous}} forest comprises {{as much as}} 42 % of the world’s tropical forests, but hasreceived far less attention than forest in wet tropical areas. Land use change threatens to greatly reducethe extent of dry forest that is known to contain high levels {{of plant and animal}} diversity. Forest fragmentationmay further endanger arboreal mammals that play principal role in the dispersal of large seeded fruits, plantcommunity assembly and diversity in these systems. Data on the spatial arrangement and extent of dryforest and other land cover types is greatly needed to enhance studies of forest fragmentation effects onanimal populations. To address this issue, we compared <b>two</b> <b>Random</b> <b>Forest</b> decision tree models forland cover classification in a Nicaraguan tropical dry forest landscape with and without the use of terrainvariables derived from Space Shuttle Radar and Topography Mission digital elevation data (SRTM-DEM). Landsat Enhanced Thematic Mapper (ETM+) bands and vegetation indices were the principle source ofspectral variables used. Overall classification accuracy for nine land cover types improved from 82. 4 % to 87. 4 % once terrain and spectral predictor variables were combined. Error matrix comparisons showedthat class accuracy was significantly greater (z = 2. 57, p-value < 0. 05) with the inclusion of terrain variables(e. g., slope, elevation and topographic wetness index) in decision tree models. Variable importance metricsindicated that a corrected Normalized Difference Vegetation Index (NDVIc) and terrain variables improveddiscrimination of forest successional types and wetlands in the study area. Results from this study demonstratethe capability of terrain variables to enhance land cover classification and habitat mapping useful tobiodiversity assessment in tropical dry forest...|$|R
40|$|Accurate and spatially-explicit maps of {{tropical}} forest carbon stocks {{are needed to}} implement carbon offset mechanisms such as REDD+ (Reduced Deforestation and Degradation Plus). The <b>Random</b> <b>Forest</b> machine learning algorithm may aid carbon mapping applications using remotely-sensed data. However, <b>Random</b> <b>Forest</b> has never been compared to traditional and potentially more reliable techniques such as regionally stratified sampling and upscaling, and it has rarely been employed with spatial data. Here, we evaluated the performance of <b>Random</b> <b>Forest</b> in upscaling airborne LiDAR (Light Detection and Ranging) -based carbon estimates compared to the stratification approach over a 16 -million hectare focal area of the Western Amazon. We considered <b>two</b> runs of <b>Random</b> <b>Forest,</b> both with and without spatial contextual modeling by including [...] {{in the latter case}} [...] x, and y position directly in the model. In each case, we set aside 8 million hectares (i. e., half of the focal area) for validation; this rigorous test of <b>Random</b> <b>Forest</b> went above and beyond the internal validation normally compiled by the algorithm (i. e., called "out-of-bag"), which proved insufficient for this spatial application. In this heterogeneous region of Northern Peru, the model with spatial context was the best preforming run of <b>Random</b> <b>Forest,</b> and explained 59 % of LiDAR-based carbon estimates within the validation area, compared to 37 % for stratification or 43 % by <b>Random</b> <b>Forest</b> without spatial context. With the 60 % improvement in explained variation, RMSE against validation LiDAR samples improved from 33 to 26 Mg C ha(- 1) when using <b>Random</b> <b>Forest</b> with spatial context. Our results suggest that spatial context should be considered when using <b>Random</b> <b>Forest,</b> and that doing so may result in substantially improved carbon stock modeling for purposes of climate change mitigation...|$|R
30|$|In {{the case}} of Ensemble Methods {{designed}} to address the class imbalance problem, generally Bagging, AdaBoost, and <b>Random</b> <b>Forest</b> are popular approaches [42, 43]. Several variants of Bagging have arisen from the original technique, such as Asymmetric Bagging, SMOTEBagging, ROSBagging, and RUSBagging. Adaptive Boosting (or AdaBoost) takes an iterative Boosting approach with the general goal of improving the classification performance of weak learners; some of its variants include RUSBoost [24] and ROSBoost [42]. A commonly used classifier, <b>Random</b> <b>Forest</b> is comprised of a bagging technique and random feature subspace selection that grows each tree in a Decision <b>Forest</b> [27]. Balanced <b>Random</b> <b>Forest</b> and Weighted <b>Random</b> <b>Forest</b> are <b>two</b> variants of <b>Random</b> <b>Forest</b> that have been proposed to use RF {{to address the problem}} of class imbalance [44]. Galar et al. [45] suggest that under class imbalance conditions ensemble-based techniques generally produce better classification results than data sampling methods. However, the authors also conclude that SMOTEBagging, RUSBoost, and UnderBagging outperform other ensemble classifiers. Among these three methods there was no statistical difference; however, SMOTEBagging yielded slightly better classification performance. As mentioned in {{the case of}} hybrid methods, it should be noted that ensemble methods (especially variants of basic ensemble learners) also have the burden to ensure that the differences in the individual approaches properly complement each other, and together yield better performance compared to the individual methods alone.|$|R
40|$|AbstractA {{test of the}} {{performance}} of <b>two</b> probabilistic classifiers (<b>random</b> <b>forests</b> and multinomial logit models) in automatically defining cancer cases has been carried out on 5608 subjects, registered by the Venetian Tumour Registry (RTV) during the years 1987 – 1996 and manually checked for possible second cancers that occurred during the 1997 – 1999 period. An eightfold cross-validation was performed to estimate the classification error; 63 predictive variables were entered into the model fitting. The <b>random</b> <b>forest</b> allows to automatically classify 45 % of subjects with a classification error lower than 5 %, while the corresponding error is 31 % for the multilogit model. The performance of the former classifier is appealing, indicating a potential drop of manually checked cases from 1750 to 960 per incidence year with a moderate error rate. This result suggests to refine the approach and extend it to other categories of manually treated cases...|$|R
40|$|Previous {{studies have}} shown that DNA inethylation may 1 e {{associated}} with disease, aging, the rate of aging and genetics. In this thesis, age is accurately predicted from DNA methylation in brain and blood tissues using <b>two</b> different algorithms, <b>Random</b> <b>Forest</b> and Linear Regression. Relationships between DNA methylation, genetics, disease and aging rate were also identified. Furthermore, the differences between the real ages and the predicted ages were found {{to be associated with the}} age of onset of aging related disease. The findings presented in this thesis take us a step further in understanding the causes of aging and age related disease and further prove the theory that methylation is related to our internal biological clock and disease...|$|R
40|$|BackgroundGenomic sequence-based {{deduction}} of antibiotic {{minimum inhibitory concentration}} (MIC) has great potential to enhance the speed and sensitivity of antimicrobial susceptibility testing. We previously developed a penicillin-binding protein (PBP) typing system and <b>two</b> methods (<b>Random</b> <b>Forest</b> (RF) and Mode MIC (MM)) that accurately predicted 2 -lactam MICs for pneumococcal isolates carrying a characterized PBP sequence type (phenotypic 2 -lactam MICs known {{for at least one}} isolate of this PBP type). This study evaluates the prediction performance for previously uncharacterized (new) PBP types and the probability of encountering new PBP types, both of which impact the overall prediction accuracy. ResultsThe MM and RF methods were used to predict MICs of 4309 previously reported pneumococcal isolates in 2 datasets and the results were compared to the known broth microdilution MICs to 6 2 -lactams. Based on a method that specifically evaluated predictions for new PBP types, the RF results were more accurate than MM results for new PBP types and showed percent essential agreement (MICs agree within 211 dilution) > 97...|$|R
40|$|The {{onset of}} fetal pathologies can be {{screened}} during pregnancy {{by means of}} Fetal Heart Rate (FHR) monitoring and analysis. Noticeable advances in understanding FHR variations were obtained in the last twenty years, thanks {{to the introduction of}} quantitative indices extracted from the FHR signal. This study searches for discriminating Normal and Intra Uterine Growth Restricted (IUGR) fetuses by applying data mining techniques to FHR parameters, obtained from recordings in a population of 122 fetuses (61 healthy and 61 IUGRs), through standard CTG non-stress test. We computed N= 12 indices (N= 4 related to time domain FHR analysis, N= 4 to frequency domain and N= 4 to non-linear analysis) and normalized them with respect to the gestational week. We compared, through a 10 -fold crossvalidation procedure, 15 data mining techniques in order to select the more reliable approach for identifying IUGR fetuses. The results of this comparison highlight that <b>two</b> techniques (<b>Random</b> <b>Forest</b> and Logistic Regression) show the best classification accuracy and that both outperform the best single parameter in terms of mean AUROC on the test sets...|$|R
40|$|Abstract—There {{has been}} much recent {{research}} into the connection between Parkinson’s disease (PD) and speech impairment. Recently, {{a wide range of}} speech signal processing algorithms (dysphonia measures) aiming to predict PD symptom severity using speech signals was introduced. In this paper, we test how accurately these novel algorithms can be used to discriminate PD subjects from healthy controls. In total, we compute 132 dysphonia measures from sustained vowels. Then, we select four parsimonious subsets of these dysphonia measures using four feature selection algorithms, and map these feature subsets to a binary classification response using <b>two</b> statistical classifiers: <b>random</b> <b>forests</b> and support vector machines. We use an existing database consisting of 263 samples from 43 subjects, and demonstrate that these new dysphonia measures can outperform state of the art results, reaching almost 99 % overal...|$|R
40|$|ABSTRACT: Using a {{benchmark}} Ames mutagenicity data set, we evaluated {{the performance of}} molecular fingerprints as descriptors for developing quantitative structure−activity relationship (QSAR) models and defining applicability domains with <b>two</b> machine-learning methods: <b>random</b> <b>forest</b> (RF) and variable nearest neighbor (v-NN). The two methods focus on complementary aspects of chemical mutagenicity and use different characteristics of the molecular fingerprints to achieve high levels of prediction accuracies. Thus, while RF flags mutagenic compounds using {{the presence or absence}} of small molecular fragments akin to structural alerts, the v-NN method uses molecular structural similarity as measured by fingerprint-based Tanimoto distances between molecules. We showed that the extended connectivity fingerprints could intuitively be used to define and quantify an applicability domain for either method. The importance of using applicability domains in QSAR modeling cannot be understated; compounds that are outside the applicability domain do not have any close representative in the training set, and therefore, we cannot make reliable predictions. Using either approach, we developed highl...|$|R
5000|$|Leo Breiman was {{the first}} person to notice the link between <b>random</b> <b>forest</b> and kernel methods. He pointed out that <b>random</b> <b>forests</b> which are grown using i.i.d random vectors in the tree {{construction}} are equivalent to a kernel acting on the true margin. Lin and Jeon [...] established the connection between <b>random</b> <b>forests</b> and adaptive nearest neighbor, implying that <b>random</b> <b>forests</b> can be seen as adaptive kernel estimates. Davies and Ghahramani proposed <b>Random</b> <b>Forest</b> Kernel and show that it can empirically outperform state-of-art kernel methods. Scornet first defined KeRF estimates and gave the explicit link between KeRF estimates and <b>random</b> <b>forest.</b> He also gave explicit expressions for kernels based on centred <b>random</b> <b>forest</b> and uniform <b>random</b> <b>forest,</b> <b>two</b> simplified models of <b>random</b> <b>forest.</b> He named these two KeRFs by Centred KeRF and Uniform KeRF,and proved upper bounds on their rates of consistency.|$|R
40|$|<b>Random</b> <b>forests</b> are {{a method}} for {{predicting}} numerous ensemble learning tasks. Prediction variability can illustrate how influential the training set is for producing the observed <b>random</b> <b>forest</b> predictions and provides additional information about prediction accuracy. forest-confidence-interval is a Python module for calculating variance and adding confidence intervals to scikit-learn <b>random</b> <b>forest</b> regression or classification objects. The core functions calculate an in-bag and error bars for <b>random</b> <b>forest</b> objects. Our software is designed for individuals using scikit-learn <b>random</b> <b>forest</b> objects that want to add estimates of uncertainty to <b>random</b> <b>forest</b> predictors...|$|R
40|$|<b>Random</b> <b>forests,</b> {{introduced}} by Leo Breiman in 2001, {{are a very}} effective statistical method. The complex mechanism of the method makes theoretical analysis difficult. Therefore, a simplified version of <b>random</b> <b>forests,</b> called purely <b>random</b> <b>forests,</b> which can be theoretically handled more easily, has been considered. In this paper we introduce a variant {{of this kind of}} <b>random</b> <b>forests,</b> that we call purely uniformly <b>random</b> <b>forests.</b> In the context of regression problems with a one-dimensional predictor space, we show that both random trees and <b>random</b> <b>forests</b> reach minimax rate of convergence. In addition, we prove that compared to <b>random</b> trees, <b>random</b> <b>forests</b> improve accuracy by reducing the estimator variance by a factor of three fourths...|$|R
30|$|We {{employed}} <b>Random</b> <b>Forest</b> algorithm as a weak classifier in the AdaBoost algorithm {{to predict}} asthma lung sounds. Boosting <b>Random</b> <b>Forest</b> algorithm is somewhat similar to employing <b>forests</b> of <b>random</b> <b>forests.</b>|$|R
50|$|In machine learning, kernel <b>random</b> <b>forests</b> {{establish}} {{the connection between}} <b>random</b> <b>forests</b> and kernel methods. By slightly modifying their definition, <b>random</b> <b>forests</b> can be rewritten as kernel methods, which are more interpretable and easier to analyze.|$|R
40|$|<b>Random</b> <b>forests</b> are {{ensemble}} methods which grow {{trees as}} base learners and combine their predictions by averaging. <b>Random</b> <b>forests</b> {{are known for}} their good practical performance, particularly in high dimensional set-tings. On the theoretical side, several studies highlight the potentially fruitful connection between <b>random</b> <b>forests</b> and kernel methods. In this paper, we work out in full details this connection. In particular, we show that by slightly modifying their definition, <b>random</b> <b>forests</b> can be rewrit-ten as kernel methods (called KeRF for Kernel based on <b>Random</b> <b>Forests)</b> which are more interpretable and easier to analyze. Explicit expressions of KeRF estimates for some specific <b>random</b> <b>forest</b> models are given, together with upper bounds on their rate of consistency. We also show empirically that KeRF estimates compare favourably to <b>random</b> <b>forest</b> estimates...|$|R
40|$|There {{has been}} {{considerable}} recent {{research into the}} connection between Parkinson's disease (PD) and speech impairment. Recently, {{a wide range of}} speech signal processing algorithms (dysphonia measures) aiming to predict PD symptom severity using speech signals have been introduced. In this paper, we test how accurately these novel algorithms can be used to discriminate PD subjects from healthy controls. In total, we compute 132 dysphonia measures from sustained vowels. Then, we select four parsimonious subsets of these dysphonia measures using four feature selection algorithms, and map these feature subsets to a binary classification response using <b>two</b> statistical classifiers: <b>random</b> <b>forests</b> and support vector machines. We use an existing database consisting of 263 samples from 43 subjects, and demonstrate that these new dysphonia measures can outperform state-of-the-art results, reaching almost 99 % overall classification accuracy using only ten dysphonia features. We find that some of the recently proposed dysphonia measures complement existing algorithms in maximizing the ability of the classifiers to discriminate healthy controls from PD subjects. We see these results as an important step toward noninvasive diagnostic decision support in PD...|$|R
30|$|In this section, we {{introduce}} the <b>random</b> <b>forest</b> classifier [8, 11]. The <b>random</b> <b>forest</b> is a bagging algorithm, {{which means that}} the <b>random</b> <b>forest</b> consists of a number of weak classifiers [12], which has zero bias but high variance of the true value. The weak classifiers are decision trees [9]. We start this section by describing how to grow a decision tree and then move on to the <b>random</b> <b>forest.</b>|$|R
5000|$|As part {{of their}} construction, <b>random</b> <b>forest</b> {{predictors}} naturally lead to a dissimilarity measure between the observations. One can also define a <b>random</b> <b>forest</b> dissimilarity measure between unlabeled data: {{the idea is to}} construct a <b>random</b> <b>forest</b> predictor that distinguishes the “observed” data from suitably generated synthetic data.The observed data are the original unlabeled data and the synthetic data are drawn from a reference distribution. A <b>random</b> <b>forest</b> dissimilarity can be attractive because it handles mixed variable types well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. The <b>random</b> <b>forest</b> dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the [...] "Addcl 1" [...] <b>random</b> <b>forest</b> dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. The <b>random</b> <b>forest</b> dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.|$|R
40|$|International audienceRandom {{forests are}} {{ensemble}} methods which grow trees as base learners and combine their predictions by averaging. <b>Random</b> <b>forests</b> {{are known for}} their good practical performance, particularly in high-dimensional settings. On the theoretical side, several studies highlight the potentially fruitful connection between <b>random</b> <b>forests</b> and kernel methods. In this paper, we work out in full details this connection. In particular, we show that by slightly modifying their definition, <b>random</b> <b>forests</b> can be rewritten as kernel methods (called KeRF for Kernel based on <b>Random</b> <b>Forests)</b> which are more interpretable and easier to analyze. Explicit expressions of KeRF estimates for some specific <b>random</b> <b>forest</b> models are given, together with upper bounds on their rate of consistency. We also show empirically that KeRF estimates compare favourably to <b>random</b> <b>forest</b> estimates...|$|R
40|$|<b>Random</b> <b>Forests</b> is {{a useful}} data mining tool that is quite popular in finding {{variable}} importance. However, many people don’t {{make use of the}} <b>Random</b> <b>Forests</b> results in interactive graphs. Partly, this is because software packages that can do interactive graphs can’t handle large data sets and those that use <b>Random</b> <b>Forests</b> have large data sets or many variables. A new software package in R, known as iPlots eXtreme, that is still in development makes it simple to explore large data sets interactively. I have created a function, called irfplot (interactive <b>random</b> <b>forests</b> plot) that specifically uses <b>Random</b> <b>Forests</b> to produce interactive graphs that are more informative than using raw values. I will use the interactive <b>Random</b> <b>Forests</b> plot that I’ve created to explore the nutrition data set from the Cache County Memory Study...|$|R
