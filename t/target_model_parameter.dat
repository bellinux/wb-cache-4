2|10000|Public
40|$|We {{propose a}} method {{for the design of}} seismic observables with maximum {{sensitivity}} to a <b>target</b> <b>model</b> <b>parameter</b> class, and minimum sensitivity to all remaining parameter classes. The resulting optimal observables thereby minimize interparameter trade-offs in multiparameter inverse problems. Our method is based on the linear combination of fundamental observables that can be any scalar measurement extracted from seismic waveforms. Optimal weights of the fundamental observables are determined with an efficient global search algorithm. While most optimal design methods assume variable source and/or receiver positions, our method has the flexibility to operate with a fixed source-receiver geometry, making it particularly attractive in studies where the mobility of sources and receivers is limited. In a series of examples we illustrate the construction of optimal observables, and assess the potentials and limitations of the method. The combination of Rayleigh-wave traveltimes in four frequency bands yields an observable with strongly enhanced sensitivity to 3 -D density structure. Simultaneously, sensitivity to S velocity is reduced, and sensitivity to P velocity is eliminated. The original three-parameter problem thereby collapses into a simpler two-parameter problem with one dominant parameter. By defining parameter classes to equal earth model properties within specific regions, our approach mimics the Backus-Gilbert method where data are combined to focus sensitivity in a target region. This concept is illustrated using rotational ground motion measurements as fundamental observables. Forcing dominant sensitivity in the near-receiver region produces an observable that is insensitive to the Earth structure at more than a few wavelengths' distance from the receiver. This observable may be used for local tomography with teleseismic data. While our test examples use a small number of well-understood fundamental observables, few parameter classes and a radially symmetric earth model, the method itself does not impose such restrictions. It can easily be applied to large numbers of fundamental observables and parameters classes, as well as to 3 -D heterogeneous earth model...|$|E
40|$|The {{scientific}} {{investigation of}} the solid Earth's complex processes, including their interactions with the oceans and the atmosphere, is an interdisciplinary field in which seismology has one key role. Major contributions of modern seismology are (1) the development of high-resolution tomographic images of the Earth's structure and (2) the investigation of earthquake source processes. In both disciplines the challenge lies in solving a seismic inverse problem, i. e. in obtaining information about physical parameters that are not directly observable. Seismic inverse studies usually aim to find realistic models through the minimization of the misfit between observed and theoretically computed (synthetic) ground motions. In general, this approach depends on the numerical simulation of seismic waves propagating in a specified Earth model (forward problem) and the acquisition of illuminating data. While the former is routinely solved using spectral-element methods, many seismic inverse problems still suffer {{from the lack of}} information typically leading to ill-posed inverse problems with multiple solutions and trade-offs between the model parameters. Non-linearity in forward modeling and the non-convexity of misfit functions aggravate the inversion for structure and source. This situation requires an efficient exploitation of the available data. However, a careful analysis of whether individual models can be considered a reasonable approximation of the true solution (deterministic approach) or if single models should be replaced with statistical distributions of model parameters (probabilistic or Bayesian approach) is inevitable. Deterministic inversion attempts to find the model that provides the best explanation of the data, typically using iterative optimization techniques. To prevent the inversion process from being trapped in a meaningless local minimum an accurate initial low frequency model is indispensable. Regularization, e. g. in terms of smoothing or damping, is necessary to avoid artifacts from the mapping of high frequency information. However, regularization increases parameter trade-offs and is subjective to some degree, which means that resolution estimates tend to be biased. Probabilistic (or Bayesian) inversions overcome the drawbacks of the deterministic approach by using a global model search that provides unbiased measures of resolution and trade-offs. Critical aspects are computational costs, the appropriate incorporation of prior knowledge and the difficulties in interpreting and processing the results. This work studies both the deterministic and the probabilistic approach. Recent observations of rotational ground motions, that complement translational ground motion measurements from conventional seismometers, motivated the research. It is investigated if alternative seismic observables, including rotations and dynamic strain, have the potential to reduce non-uniqueness and parameter trade-offs in seismic inverse problems. In the framework of deterministic full waveform inversion a novel approach to seismic tomography is applied {{for the first time to}} (synthetic) collocated measurements of translations, rotations and strain. The concept is based on the definition of new observables combining translation and rotation, and translation and strain measurements, respectively. Studying the corresponding sensitivity kernels assesses the capability of the new observables to constrain various aspects of a three-dimensional Earth structure. These observables are generally sensitive only to small-scale near-receiver structures. It follows, for example, that knowledge of deeper Earth structure are not required in tomographic inversions for local structure based on the new observables. Also in the context of deterministic full waveform inversion a new method for the design of seismic observables with focused sensitivity to a <b>target</b> <b>model</b> <b>parameter</b> class, e. g. density structure, is developed. This is achieved through the optimal linear combination of fundamental observables that can be any scalar measurement extracted from seismic recordings. A series of examples illustrate that the resulting optimal observables are able to minimize inter-parameter trade-offs that result from regularization in ill-posed multi-parameter inverse problems. The inclusion of alternative and the design of optimal observables in seismic tomography also affect more general objectives in geoscience. The {{investigation of the}} history and the dynamics of tectonic plate motion benefits, for example, from the detailed knowledge of small-scale heterogeneities in the crust and the upper mantle. Optimal observables focusing on density help to independently constrain the Earth's temperature and composition and provide information on convective flow. Moreover, the presented work analyzes for the first time if the inclusion of rotational ground motion measurements enables a more detailed description of earthquake source processes. The complexities of earthquake rupture suggest a probabilistic (or Bayesian) inversion approach. The results of the synthetic study indicate that the incorporation of rotational ground motion recordings can significantly reduce the non-uniqueness in finite source inversions, provided that measurement uncertainties are similar to or below the uncertainties of translational velocity recordings. If this condition is met, the joint processing of rotational and translational ground motion provides more detailed information about earthquake dynamics, including rheological fault properties and friction law parameters. Both are critical e. g. for the reliable assessment of seismic hazards...|$|E
40|$|This paper {{presents}} {{the theory and}} examples of performance for a new algorithm that initiates tracks using a multiple model Probabilistic Data Association (PDA) filter. The analysis is generalised for the case of multiple non-uniform clutter regions within the measurement data that updates the filter. The algorithm starts multiple parallel PDA filters from a single sensor measurement. Each filter is assigned one {{of a range of}} possible <b>target</b> <b>model</b> <b>parameters.</b> To reduce the possibility of clutter measurements forming established tracks, the solution includes a model for a visible target. That is, a target that gives sensor measurements that satisfy one of the <b>target</b> <b>models.</b> Other features included in the algorithm are the selection of a fixed number of nearest measurements and the addition of signal amplitude to the target state vector. The inclusion of signal amplitude is one coordinate that is applicable to the non-uniform clutter model developed in this paper...|$|R
40|$|Machine {{learning}} {{models are}} powerful but fallible. Generating adversarial examples - inputs deliberately crafted to cause model misclassification or other errors - can yield important insight into model assumptions and vulnerabilities. Despite significant recent work on adversarial example generation targeting image classifiers, relatively little work exists exploring adversarial example generation for text classifiers; additionally, many existing adversarial example generation algorithms require {{full access to}} <b>target</b> <b>model</b> <b>parameters,</b> rendering them impractical for many real-world attacks. In this work, we introduce DANCin SEQ 2 SEQ, a GAN-inspired algorithm for adversarial text example generation targeting largely black-box text classifiers. We recast adversarial text example generation as a reinforcement learning problem, and demonstrate that our algorithm offers preliminary but promising steps towards generating semantically meaningful adversarial text examples in a real-world attack scenario...|$|R
40|$|Model {{transfer}} {{refers to}} the process of adjusting a model that was previously identified for one task (source) {{so that it can be}} used for a new, related task (target). For decision tasks in unknown Markov environments, we profit through model transfer by using information from related tasks, e. g. transition knowledge and solution (policy) knowledge, to quickly determine an appropriate model of the new task environment. A difficulty with such transfer is typically the non-linear and indirect relationship between the available source knowledge and the <b>target’s</b> working prior <b>model</b> of the unknown environment, provided through a complex multidimensional transfer function. In this paper, we take a Bayesian view and present a probability perturbation method that conditions the <b>target’s</b> <b>model</b> <b>parameters</b> to a variety of source knowledge types. The method relies on pre-posterior distributions, which specifies the distribution of the target’s parameter set given each individual knowledge types. The pre-posteriors are then combined to obtain a posterior distribution for the parameter set that matches all the available knowledge. The method is illustrated with an example...|$|R
40|$|Adversarial {{examples}} are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without {{knowledge of the}} <b>target</b> <b>model's</b> <b>parameters.</b> Adversarial training {{is the process of}} explicitly training a model on adversarial examples, {{in order to make it}} more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a "label leaking" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process. Comment: 17 pages, 5 figure...|$|R
40|$|Thermal {{models of}} {{buildings}} {{are often used}} to identify energy savings within a building. Given that {{a significant proportion of}} that energy is typically used to maintain building temperature, establishing the optimal control of the buildings thermal system is important. This requires an understanding of the thermal dynamics of the building, which is often obtained from physical thermal models. However, these models require detailed building parameters to be specified and these can often be difficult to determine. In this paper, we propose an evolutionary approach to parameter identification for thermal models that are formulated as an optimization task. A state-of-the-art evolutionary algorithm, i. e., SaNSDE+, has been developed. A fitness function is defined, which quantifies the difference between the energy-consumption time-series data that are derived from the identified parameters and that given by simulation with a set of predetermined <b>target</b> <b>model</b> <b>parameters.</b> In comparison with a conventional genetic algorithm, fast evolutionary programming, and two state-of-the-art evolutionary algorithms, our experimental results show that the proposed SaNSDE+ has significantly improved both the solution quality and the convergence speed, suggesting this is an effective tool for parameter identification for simulated building thermal models...|$|R
40|$|To {{obtain a}} robust {{acoustic}} {{model for a}} certain speech recognition task, {{a large amount of}} speech data is necessary. However, the preparation of speech data including recording and transcription is very costly and time-consuming. Although there are attempts to build generic acoustic models which are portable among different applications, speech recognition performance is typically task-dependent. This paper introduces a method for automatically building task-dependent acoustic models based on selective training. Instead of setting up a new database, only a small amount of task-specific development data needs to be collected. Based on the likelihood of the <b>target</b> <b>model</b> <b>parameters</b> given this development data, utterances which are acoustically close to the development data are selected from existing speech data resources. Since there are too many possibilities for selecting a data subset from a larger database in general, a heuristic has to be employed. The proposed algorithm deletes single utterances temporarily or alternates between successive deletion and addition of multiple utterances. In order to make selective training computationally practical, model retraining and likelihood calculation need to be fast. It is shown, that the model likelihood can be calculated fast and easily based on sufficient statistics without the need for explicit reconstruction of <b>model</b> <b>parameters.</b> The algorithm is applied to obtain an infant- and elderly-dependent acoustic model with only very few development data available. There is an improvement in word accuracy of up to 9 % in comparison to conventional EM training without selection. Furthermore, the approach was also better than MLLR and MAP adaptation with the development data...|$|R
40|$|The EIRENE Monte-Carlo neutral code {{is used to}} {{calculate}} neutral pressures throughout the divertor region of the Alcator C-Mod tokamak. A semi-empirical onion-skin method (OSM) is used {{to calculate}} the hydrogenic background plasma for a moderate density discharge, = 1. 5 x 10 (20) m(- 3), with a partially detached inner target and an attached outer <b>target.</b> The <b>model</b> <b>parameters</b> are adjusted until there is agreement with the Langmuir probe and spectroscopic data for the divertor region. The measured divertor pressure is 30 mTorr, and for C-Mod divertor dimensions this implies the transition flow regime between free molecular and viscous neutral transport. Therefore, a non-linear BGK-model Boltzmann collision term is included in EIRENE to allow for interactions between neutral particles. For the standard <b>model</b> <b>parameters,</b> the calculated divertor pressure is 7 mTorr. The <b>model</b> <b>parameters</b> are adjusted to reduce the discrepancy, and the plausibility of each variation is discussed. (C) 2003 Elsevier Science B. V. All rights reserved...|$|R
40|$|The paper {{outlines}} a programme {{of research}} funded under the Rural Economy and Land Use (RELU) programme. The proposed research {{will examine the}} likely effects of the Water Framework Directive (WFD) {{in terms of both}} its impacts upon the farming sector and the non-market benefits it may generate. From an agricultural perspective the WFD will impose a substantial extension of controls upon diffuse pollution from farms. A major objective of the research will be to assess the likely response and consequent economic costs to an already fragile farming sector. This objective will be addressed via a highly interdisciplinary methodology combining hydrological and other physical sciences with quantitative and qualitative socio-economic analyses to generate an integrated hydrological-economic model of farm activities and incomes. This will dynamically link farm local and regional activities to water standards, allowing feedback loops to indicate the impacts of altering farm activity and changing water quality <b>targets.</b> <b>Model</b> <b>parameters</b> and response scenarios will initially be established via quantitative estimation and then refined through a series of farm attitude and behaviour surveys. This cost-effectiveness analysis will be complemented by an assessment of the benefits arising from the WFD and an aggregation and equity analysis of the distribution of both costs and benefits. Planned deliverables include assessments of the impact of alternative WFD implementation strategies allowing policy makers to inspect effects upon farmer and the wider community at a variety of spatial scales...|$|R
40|$|Abstract—Signal {{processing}} algorithms for hand-held mine detection sensors are described. The {{goals of}} the algorithms are to provide alarms to a human operator indicating {{the likelihood of the}} presence of a buried mine. Two modes of operations are considered: search mode and discrimination mode. Search mode generates an initial detection at a suspected location and discrimination mode confirms that the suspected location contains a land mine. Search mode requires that the signal processing algorithm generate a detection confidence value immediately at the current sample location and no delay in producing an alarm confidence is tolerable. Search mode detection has a high false-alarm rate. Discrimination mode allows the operator to interrogate the entire suspected location to eliminate false alarms. It does not require that the signal processing algorithm produce an alarm confidence immediately for the current sample location, but rather allows the system to process all the data acquired over the region before producing an alarm. This paper proposes discrimination mode processing algorithms for metal detectors (MDs), or electromagnetic induction sensors (EMIs), ground-penetrating radars (GPRs), and their fusion. The MD discrimination mode algorithm employs a model-based approach and uses the <b>target</b> <b>model</b> <b>parameters</b> to discriminate between mines and clutter objects. The GPR discrimination mode algorithm uses the consistency of detection as well as the shape of the detection peaks over several sweeps to improve the discrimination accuracy. The performances of the proposed algorithms were examined on a dataset collected at a government test site, and performance was compared with baseline techniques. Experimental results showed that the proposed method can reduce the probability of false alarm by as much as 70 % at a 100 % correct detection rate and performed comparable to the best human operator on a blind test with data collected at approximately 1000 locations. Index Terms—Electromagnetic induction (EMI), ground-penetrating radar (GPR), hand-held mine detector, land mine...|$|R
40|$|International audiencePolarimetric {{incoherent}} target decomposition aims in accessing physical {{parameters of}} illuminated scatters through {{the analysis of}} target coherence or covariance matrix. In this framework, Independent Component Analysis (ICA) was recently proposed as an alternative method to eigenvector decomposition to better interpret non-Gaussian heterogeneous clutter (inherent to high resolution SAR systems). In this paper a Monte Carlo approach is performed in order to investigate the bias in estimating Touzi's <b>Target</b> Scattering Vector <b>Model</b> <b>parameters</b> when ICA is employed. Simulated data and data from the P-band airborne dataset acquired by the Office National d'tudes et de Recherches Arospatiales (ON-ERA) over the French Guiana in 2009 {{in the frame of}} the European Space Agency campaign TropiSAR are taken into consideration...|$|R
40|$|A compartmental {{model for}} the in vitro uptake {{kinetics}} of the anti-cancer agent topotecan is proposed. This model provides {{a description of the}} activity of the drug, and subsequent delivery of active form to the nuclear DNA <b>target.</b> The unknown <b>model</b> <b>parameters</b> are estimated from two-photon laser-scanning microscopy data, which provide concentrations of topotecan (active plus inactive forms) in the extracellular region containing live human breast tumour cells (MCF- 7 cell line), the cytoplasm and the nucleus. This determines an output structure for which the model is uniquely identifiable, that is, the unknown parameters are uniquely determined from noise-free, continuous and perfect data. The model allows in silico predictions of the dose dependence of target binding. Copyright © 2005 John Wiley & Sons, Ltd...|$|R
30|$|In this paper, a {{time domain}} stripmap mode Synthetic Aperture Radar (SAR) raw data {{simulation}} including both the terrain and the targets is proposed. The simulator generates SAR raw data of a scene, involving both single and double reflections in a computationally efficient manner. The inputs of the simulator are the {{digital elevation model}} of the terrain, the 3 D <b>target</b> <b>model,</b> and the <b>parameters</b> of the SAR system. The simulator extracts a geometrically accurate reflectivity map and generates the SAR raw data in time domain. The disadvantage of time domain method is justified to be tolerable by presenting experiments on modularity performance of the simulator. Also a novel method to decrease the time domain computational complexity of the SAR raw data generation is proposed. Our method has showed very promising results in representing the scattering characteristics, the raw data, and the time domain simulation flexibility.|$|R
40|$|License, which permits {{unrestricted}} use, distribution, {{and reproduction}} in any medium, provided the original work is properly cited. In this paper, a time domain stripmap mode Synthetic Aperture Radar (SAR) raw data simulation including both the terrain and the targets is proposed. The simulator generates SAR raw data of a scene, involving both single and double reflections in a computationally efficient manner. The inputs of the simulator are the {{digital elevation model}} of the terrain, the 3 D <b>target</b> <b>model,</b> and the <b>parameters</b> of the SAR system. The simulator extracts a geometrically accurate reflectivity map and generates the SAR raw data in time domain. The disadvantage of time domain method is justified to be tolerable by presenting experiments on modularity performance of the simulator. Also a novel method to decrease the time domain computational complexity of the SAR raw data generation is proposed. Our method has showed very promising results in representing the scattering characteristics, the raw data, and the time domain simulation flexibility. 1...|$|R
40|$|In this study, {{we present}} a {{collection}} of local models, termed geographically weighted (GW) models, {{that can be found}} within the GWmodel R package. A GW model suits situations when spatial data are poorly described by the global form, and for some regions the localised fit provides a better description. The approach uses a moving window weighting technique, where a collection of local models are estimated at <b>target</b> locations. Commonly, <b>model</b> <b>parameters</b> or outputs are mapped so that the nature of spatial heterogeneity can be explored and assessed. In particular, we present case studies using: (i) GW summary statistics and a GW principal components analysis; (ii) advanced GW regression fits and diagnostics; (iii) associated Monte Carlo significance tests for non-stationarity; (iv) a GW discriminant analysis; and (v) enhanced kernel bandwidth selection procedures. General Election data sets from the Republic of Ireland and US are used for demonstration. This study is designed to complement a companion GWmodel study, which focuses on basic and robust GW models...|$|R
40|$|This thesis {{investigates the}} {{implications}} of explicitly modeling the monetary policy of the Central Bank within a Dynamic Term Structure Model (DTSM). We follow Piazzesi (2005) and implement monetary policy by including the Fed target rate as a state variable. The discontinuous target dynamics are accurately modeled via a non-linear switching process, while still maintaining affine requirements under the pricing measure ensuring tractability. To ensure a flexible risk specification {{we turn to the}} parametrization of Cheridito et al (2007), with extensions to the <b>target</b> jump process. <b>Model</b> <b>parameters</b> are estimated via a simulated maximum likelihood es-timation scheme with importance sampling. A Bayesian particle filter is used as a robustness check, and it's use for static parameter estimation in a DTSM framework is explored. Our results support those in Piazzesi (2005), revealing a substantial improvement in pricing errors especially on the short end of the yield curve. The model construction provides a natural framework to inspect monetary policy information embedded i...|$|R
40|$|In {{this thesis}} the {{possibility}} for automatic classifications of radar images has been evaluated. The topic automatic classification has always been {{of interest in the}} radar community, and the improvements both of hardware and software during the years today presents a new field for engineers. This new field of classification opens up opportunities no one would have believed {{in the early days of}} radar. One possible approach classifying radar images is using radar range profiles. Feature extraction of the peak amplitudes and their position in the profiles is a common method. In this thesis, a classification of pure range profiles without extraction of above mentioned features was evaluated. The range profiles were obtained through software simulations. The software was used to both <b>model</b> ground <b>targets</b> and simulating the range profiles. Five <b>target</b> <b>models</b> were simulated, four were modeled and one already existed in the software. All <b>target</b> <b>models</b> in the software were simplifications of real targets. Real target simplifications are made possible through the use of common geometrical figures assembled to form a more complex, but still simplified <b>target</b> <b>model.</b> Common geometrical figures used for modelling were: spheres, elliptical cylinders, rectangular plates etc. The simulation software did not include any visual confirmation of the <b>modeled</b> <b>targets.</b> A method using sinograms and the inverse radon transform was developed. This method made it possible to visually confirm the <b>modeled</b> <b>targets</b> and prevented incorrect placement or orientation of the geometrical figures. The selected classification algorithm was an artificial neural network. A neural network is a mathematical simulation of the biological nervous system. The core of a neural network consists of hidden neurons containing threshold functions performing the calculations. The network implemented was a standard 2 -layer network, with non-linear threshold functions for the neurons in the hidden layer and linear threshold functions for neurons in the output layer. The range profiles were feed to the network as inputs and the outputs were vectors with five values, one value for each <b>target</b> <b>model.</b> The <b>parameter</b> settings of the neural network were evaluated. The neural network classification algorithm was written in the MATLAB program language. The classification results revealed that it is possible to classify pure range profiles with and without noise. The classification rates for profiles without noise were above 70 % for all <b>target</b> <b>models</b> (worst case scenario with respect to the parameter settings of the neural network). Range profiles containing noise showed for a SNR level of 15 dB similar classification rates as range profiles without noise. Validerat; 20101217 (root...|$|R
40|$|In Bayesian multi-target {{filtering}} {{knowledge of}} parameters such as clutter intensity and detection probability profile are of critical importance. Significant mismatches in clutter and detection <b>model</b> <b>parameters</b> results in biased estimates. In this {{paper we propose}} a multi-target filtering solution that can accommodate non-linear <b>target</b> <b>models</b> and an unknown non-homogeneous clutter and detection profile. Our solution is based on the multi-target multi-Bernoulli filter that adaptively learns non-homogeneous clutter intensity and detection probability while filtering...|$|R
40|$|In {{this paper}} a process <b>targeting</b> <b>model</b> for a three class {{screening}} problem is developed. The model developed, extends {{the work in}} the literature by incorpo-rating product uniformity. The product uniformity is introduced via a Taguchi type quadratic loss function. Two cases for the process targeting are considered. In addition, an illustrative example is presented. Sensitivity analysis is also con-ducted to study the effect of <b>model</b> <b>parameters</b> on expected profit and optimal process mean...|$|R
40|$|Tidal {{harmonic}} analysis simulations along with simulations spanning four specific historical time periods in 2003 and 2004 {{were conducted to}} test {{the performance of a}} northern Gulf of Mexico tidal model. A recently developed method for detecting inundated areas based on integrated remotely sensed data (i. e., Radarsat- 1, aerial imagery, LiDAR, Landsat 7 ETM+) was applied to assess the performance of the tidal model. The analysis demonstrates the applicability of the method and its agreement with traditional performance assessment techniques such as harmonic resynthesis and water level time series analysis. Based on the flooded/non-flooded coastal areas estimated by the integrated remotely sensed data, the model is able to adequately reproduce the extent of inundation within four sample areas from the coast along the Florida panhandle, correctly identifying areas as wet or dry over 85 % of the time. Comparisons of the tidal model inundation to synoptic (point-in-time) inundation areas generated from the remotely sensed data generally agree with the results of the traditional performance assessment techniques. Moreover, this approach is able to illustrate the spatial distribution of model inundation accuracy allowing for <b>targeted</b> refinement of <b>model</b> <b>parameters...</b>|$|R
40|$|The optimal {{selection}} of experimental conditions {{is essential to}} maximizing the value of data for inference and prediction. We propose an information theoretic framework and algorithms for robust optimal experimental design with simulation-based models, {{with the goal of}} maximizing information gain in <b>targeted</b> subsets of <b>model</b> <b>parameters,</b> particularly in situations where experiments are costly. Our framework employs a Bayesian statistical setting, which naturally incorporates heterogeneous sources of information. An objective function reflects expected information gain from proposed experimental designs. Monte Carlo sampling is used to evaluate the expected information gain, and stochastic approximation algorithms make optimization feasible for computationally intensive and high-dimensional problems. A key aspect of our framework is the introduction of model calibration discrepancy terms that are used to "relax" the model so that proposed optimal experiments are more robust to model error or inadequacy. We illustrate the approach via several model problems and misspecification scenarios. In particular, we show how optimal designs are modified by allowing for model error, and we evaluate the performance of various designs by simulating "real-world" data from models not considered explicitly in the optimization objective. by Chi Feng. Thesis: S. M., Massachusetts Institute of Technology, Computation for Design and Optimization Program, 2015. Cataloged from PDF version of thesis. Includes bibliographical references (pages 87 - 90) ...|$|R
40|$|Abstract—According to the sensing {{principle}} of coded structured light, and {{the characteristics of}} broad complication and high parameters request, a new coded structured light mathematics model and measurement method is proposed, which can simply and quickly obtain depth information of <b>targets.</b> <b>Model</b> is simulated by computer. It can gain the offset and deformation of pattern images based on different <b>model</b> <b>parameters.</b> For the deformation of images, system parameters can be changed correspondingly. So, whole system can be fixed on using optimal system parameters. The experiment results reveal that the method of structured light model and simulation proposed are reasonable and effective, which have important theoretical value and practical signification. Index Terms—coded structure light, model, object detecting, reconstruction I...|$|R
40|$|In this paper, a Process <b>Targeting</b> <b>model</b> for a three-class {{screening}} {{problem is}} developed. The model extends {{the work in}} the literature by incorporating a measurement error present in inspection systems. The strategy adopted, to nullify the effect of measurement error, is to introduce the concept of 'cut-off points', i. e. the decision during inspection is based on these cut-off points rather than specification limits of the various product grades. This model considers cut-off points as decision variables. In addition, an illustrative example is presented that compares model performance with the model presented in Min and Jang (1997). Sensitivity analysis is also conducted to study the effect of various <b>model</b> <b>parameters,</b> particularly measurement error on expected profit, optimal process mean and cut-off points...|$|R
5000|$|Swerling <b>target</b> <b>models</b> {{are special}} {{cases of the}} Chi-Squared <b>target</b> <b>models</b> with {{specific}} degrees of freedom. There are five different Swerling models, numbered I through V: ...|$|R
40|$|Document {{images are}} {{degraded}} through bilevel {{processes such as}} scanning, printing, and photocopying. The resulting image degradations can be categorized based either on observable degradation features or on degradation <b>model</b> <b>parameters.</b> The degradation features can be related mathematically to <b>model</b> <b>parameters.</b> In this paper we statistically compare pairs of populations of degraded character images created with different <b>model</b> <b>parameters.</b> The changes in {{the probability that the}} characters are from different populations when the <b>model</b> <b>parameters</b> vary correlate with the relationship between observable degradation features and the <b>model</b> <b>parameters.</b> The paper also shows which features have the largest impact on the image...|$|R
50|$|Special {{versions}} {{were made}} for the United States Marine Corps (100 Match <b>Target</b> <b>Models</b> and 2500 Sport Models); United States Air Force (925 <b>Target</b> <b>Models)</b> and 75 Match <b>Target</b> <b>Models</b> for the United States Coast Guard. The Air Force models had no special markings and most were sold as surplus through the Director of Civilian Marksmanship Program. The bulk of the Marine and Coast Guard versions were destroyed and sold as scrap metal.|$|R
40|$|Document {{image quality}} is {{degraded}} through {{processes such as}} scanning, printing, and photocopying. The resulting bilevel image degradations can be categorized based either on observable degradation features or on degradation <b>model</b> <b>parameters.</b> The image degradation features can be related mathematically to <b>model</b> <b>parameters.</b> In this paper we statistically compare pairs of populations of degraded character images created with different <b>model</b> <b>parameters.</b> The probability that the character populations were degraded by the same <b>model</b> <b>parameters</b> correlates with the relationship between observable degradation features and the <b>model</b> <b>parameters.</b> Two metrics of character difference are used: Hamming distance and moment feature distance. Knowledge about {{the conditions under which}} characters will be similar and when they will be different can influence the choice of parameters for future experiments...|$|R
40|$|Accurate {{estimation}} of <b>model</b> <b>parameters</b> and {{state of charge}} (SoC) is crucial for the lithium-ion battery management system (BMS). In this paper, {{the stability of the}} <b>model</b> <b>parameters</b> and SoC estimation under measurement uncertainty is evaluated by three different factors: (i) sampling periods of 1 / 0. 5 / 0. 1 s; (ii) current sensor precisions of ± 5 /± 50 /± 500 mA; and (iii) voltage sensor precisions of ± 1 /± 2. 5 /± 5 mV. Firstly, the numerical model stability analysis and parametric sensitivity analysis for battery <b>model</b> <b>parameters</b> are conducted under sampling frequency of 1 – 50 Hz. The perturbation analysis is theoretically performed of current/voltage measurement uncertainty on <b>model</b> <b>parameter</b> variation. Secondly, the impact of three different factors on the <b>model</b> <b>parameters</b> and SoC estimation was evaluated with the federal urban driving sequence (FUDS) profile. The bias correction recursive least square (CRLS) and adaptive extended Kalman filter (AEKF) algorithm were adopted to estimate the <b>model</b> <b>parameters</b> and SoC jointly. Finally, the simulation results were compared and some insightful findings were concluded. For the given battery <b>model</b> and <b>parameter</b> estimation algorithm, the sampling period, and current/voltage sampling accuracy presented a non-negligible effect on the estimation results of <b>model</b> <b>parameters.</b> This research revealed the influence of the measurement uncertainty on the <b>model</b> <b>parameter</b> estimation, which will provide the guidelines to select a reasonable sampling period and the current/voltage sensor sampling precisions in engineering applications...|$|R
30|$|Future {{work will}} deal with more complex <b>target</b> <b>models</b> and the {{development}} of associated imaging algorithms based on the monostatic SAR processors presented in [19, 20] where subspace approaches are used to integrate complex <b>target</b> <b>model</b> in monostatic SAR processing.|$|R
40|$|Deformation {{transfer}} is {{to transfer the}} deformation of a source deforming <b>model</b> to a <b>target</b> <b>model.</b> Not only the pose but the detailed deformations of a source model are transferred to a <b>target</b> <b>model,</b> causing the characteristic deformations of the <b>target</b> <b>model</b> are mixed {{with those of the}} source model. This leads to unnatural results. In this paper, we present a novel example-based deformation transfer approach to solve this problem. With the aid of a few target examples, the characteristic deformations of transferred <b>target</b> <b>models</b> are recovered. We evaluate our approach with several full-body articulated models. The experimental results show that our approach can generate more natural and convincing deformation transfer results than other approaches...|$|R
40|$|The {{purpose of}} this {{research}} is to propose a robust estimate for the parameters of a nonlinear regression model and its residual variance <b>model</b> <b>parameters,</b> when the residuals follow a heteroscedastic parametric model function. The classic estimate is based on the least squares estimation error for the <b>model</b> <b>parameters</b> and the least square estimate error between sample variance and variance <b>model,</b> for the <b>parameters</b> of variance function model. The sample variance that are computed from the data set, are used as the initial estimates of variance model. In the presence of outliers these estimators are not Robust, and tends to infinity. Both function <b>model</b> <b>parameter</b> estimates and variance <b>model</b> <b>parameter</b> estimates must be robustified to solve the outlier effect problems. In this research the MM-estimator is applied for robust estimating the function <b>model</b> <b>parameters</b> and M-estimator is applied for robust estimating of variance function <b>model</b> <b>parameters.</b> These estimators  finally combined and the Extended Generalized Estimator is calculated.  Robustifying the Least square estimate of <b>parameter</b> of variance <b>model</b> functions in nonlinear regression with heterogeneous variance, Islamic Azad Lamerd universit...|$|R
3000|$|The second-stage gird {{search was}} further adopted with smaller steps, {{so as to}} {{determine}} the “optimal” ESN <b>model</b> <b>parameters.</b> For each <b>model</b> <b>parameter,</b> five grid points were equally spaced within the corresponding promising region. Then, all the 56 possible combinations of the six ESN <b>model</b> <b>parameters</b> were evaluated, and the lowest estimation error (12.4961) of the HF spectral envelope was obtained at {a [...]...|$|R
50|$|Lake <b>model</b> <b>parameters</b> were {{provided}} by Dave Kramer in 2004. Since then, Luke Hendrickson has provided simplified lake <b>model</b> <b>parameters</b> that allow {{the resurrection of the}} parameter NPQ, from the puddle model, back into the lake model. This is valuable because {{there have been so many}} scientific papers that have used NPQ for plant stress measurement, as compared to papers that have used lake <b>model</b> <b>parameters.</b>|$|R
40|$|This study {{examines}} applicability of Normal Turbulence Model (NTM) in IEC 61400 - 1 for offshore conditions using wind records obtained at an offshore site. The <b>model</b> <b>parameters</b> for {{estimation of the}} standard deviation of longitudinal fluctuating velocity, σ 1, in NTM are presented. The identified <b>model</b> <b>parameters</b> for the mean value of σ 1 agree well with those of IEC Normal Turbulence <b>Model,</b> but <b>parameters</b> for the standard deviation of σ 1 {{are found to be}} larger than those used in IEC. As a result, the standard deviation of σ 1 obtained by IEC Normal Turbulence <b>Model</b> <b>parameters</b> is underestimated and predicted that by the proposed <b>model</b> <b>parameters</b> shows a good agreement with the observation...|$|R
50|$|The <b>model</b> <b>parameters</b> are {{confidential}} {{and only}} accessible to consortium members {{for at least}} two and a half year after the first delivery. After this time the university can publish the <b>model</b> <b>parameters.</b>|$|R
