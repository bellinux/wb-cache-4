663|10000|Public
25|$|A Type I error would falsely {{indicate}} that treatment A {{is more effective}} than the placebo, whereas a <b>Type</b> <b>II</b> <b>error</b> would be a failure to demonstrate that treatment A {{is more effective than}} placebo even though it actually is more effective.|$|E
25|$|A typeII error {{occurs when}} the null {{hypothesis}} is false, but erroneously fails to be rejected. It is failing to assert what is present, a miss. A typeII error may be compared with a so-called false negative (where an actual 'hit' was disregarded by the test and seen as a 'miss') in a test checking for a single condition with a definitive result of true or false. A <b>Type</b> <b>II</b> <b>error</b> is committed when we fail to believe a true alternative hypothesis. In terms of folk tales, an investigator may fail to see the wolf ("failing to raise an alarm"). Again, H0: no wolf.|$|E
25|$|The {{standard}} {{approach is to}} test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore {{the probability that the}} estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of <b>type</b> <b>II</b> <b>error</b> is the probability that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.|$|E
5000|$|... {{avoiding}} the <b>type</b> <b>II</b> <b>errors</b> (or false negatives) that classify imposters as authorized users.|$|R
30|$|The {{total error}} {{minimization}} principle {{is applied to}} obtain cutoff value. Various cutoff values are tested and the final cutoff value is decided where the sum of Type I and <b>Type</b> <b>II</b> <b>errors</b> are minimized. <b>Type</b> I errors occur when a model incorrectly classifies a distressed company as non-distressed, while <b>Type</b> <b>II</b> <b>errors</b> occur when a model incorrectly classifies a non-distressed company as distressed.|$|R
5000|$|... {{while the}} {{probability}} of <b>type</b> <b>II</b> <b>errors</b> is called the [...] "false accept rate" [...] (FAR) or false match rate (FMR).|$|R
25|$|There are {{a variety}} of {{theories}} about the cause of this gap. However, it has been well established that one of the main issues is that the questions studied by restoration ecologists are frequently not found useful or easily applicable by land managers. For instance, many publications in restoration ecology characterize the scope of a problem in depth, without providing concrete solutions. Additionally many restoration ecology studies are carried out under controlled conditions and frequently at scales much smaller than actual restorations. Whether or not these patterns hold true in an applied context is often unknown. There is evidence that these small scale experiments inflate <b>type</b> <b>II</b> <b>error</b> rates and differ from ecological patterns in actual restorations.|$|E
25|$|All {{statistical}} hypothesis tests have a probability of making type I and type II errors. For example, all blood tests for a disease will falsely detect {{the disease in}} some proportion {{of people who do}}n't have it, and will fail to detect the disease in some proportion of people who do have it. A test's probability of making a type I error is denoted by α. A test's probability of making a <b>type</b> <b>II</b> <b>error</b> is denoted by β. These error rates are traded off against each other: for any given sample set, the effort to reduce one type of error generally results in increasing the other type of error. For a given test, the only way to reduce both error rates is to increase the sample size, and this may not be feasible.|$|E
2500|$|In {{statistical}} hypothesis testing, a type I error is the incorrect rejection {{of a true}} null hypothesis (also known as a [...] "false positive" [...] finding), while a <b>type</b> <b>II</b> <b>error</b> is incorrectly retaining a false null hypothesis (also known as a [...] "false negative" [...] finding). More simply stated, a type I error is to falsely infer the existence of {{something that is not}} there, while a <b>type</b> <b>II</b> <b>error</b> is to falsely infer the absence of something that is.|$|E
40|$|This {{technical}} report describes a simple method for using inertial data to facilitate locomotion in virtual environments. The means of locomotion for virtual environments contributes {{greatly to the}} users’ sense of presence in the environment. Although walking-in-place is not so realistic as real walking, {{it is a very}} cost-effective alternative. Using a simple version of this algorithm both Type I and <b>Type</b> <b>II</b> <b>errors</b> were decreased compared to a previous walking-in-place algorithm. Type I errors occur when the algorithm detects a step when none occurred. <b>Type</b> <b>II</b> <b>errors</b> occur when the algorithm does not detect a step when one did occur. Type I errors decreased from 3 % to 1 % and <b>Type</b> <b>II</b> <b>errors</b> decreased from 32 % to 11 %. The more complex version of this algorithm promises even better results...|$|R
50|$|<b>Type</b> <b>II</b> <b>errors</b> which {{consist of}} failing to reject a null {{hypothesis}} that is false; this amounts to a false negative result.|$|R
2500|$|<b>Type</b> <b>II</b> <b>errors</b> {{where the}} null {{hypothesis}} fails to be rejected and an actual difference between populations is missed giving a [...] "false negative".|$|R
2500|$|From all {{the numbers}} c, with this property, we choose the smallest, in order to {{minimize}} the probability of a <b>Type</b> <b>II</b> <b>error,</b> a false negative. For the above example, we select: [...]|$|E
2500|$|Fisher popularized the [...] "significance test". He {{required}} a null-hypothesis (corresponding to a population frequency distribution) and a sample. His (now familiar) calculations determined whether {{to reject the}} null-hypothesis or not. Significance testing did not utilize an alternative hypothesis {{so there was no}} concept of a <b>Type</b> <b>II</b> <b>error.</b>|$|E
2500|$|When {{comparing}} two means, concluding {{the means}} were different when in reality {{they were not}} different would be a Type I error; concluding the means were not different when in reality they were different would be a <b>Type</b> <b>II</b> <b>error.</b> Various extensions have been suggested as [...] "Type III errors", though none have wide use.|$|E
30|$|In {{order to}} satisfy {{underlying}} assumptions of statistical analyses and avoid type I and <b>type</b> <b>II</b> <b>errors,</b> {{the data sets}} were subjected to transformations prior to multivariate analyses.|$|R
50|$|In statistics, if {{the null}} {{hypothesis}} is that all items are irrelevant (where the hypothesis is accepted or rejected {{based on the number}} selected compared with the sample size), absence of type I and <b>type</b> <b>II</b> <b>errors</b> corresponds respectively to maximum precision (no false positive) and maximum recall (no false negative). The above pattern recognition example contained 8 &minus; 5 = 3 type I errors and 12 &minus; 5 = 7 <b>type</b> <b>II</b> <b>errors.</b> Precision {{can be seen as a}} measure of exactness or quality, whereas recall is a measure of completeness or quantity.|$|R
40|$|Drug {{testing in}} the United States is {{currently}} biased toward the minimization of "Type I" error, that is, toward minimizing the chance of approving drugs that are unsafe or ineffective. This regulatory focus of the Food and Drug Administration (FDA) ignores the potential for committing the alternative "Type II" error, that is, the error of not approving drugs that are, in fact, safe and effective. Such <b>Type</b> <b>II</b> <b>errors</b> can result {{in the loss of}} significant benefits to society when the sale of drugs that are safe and effective is prohibited. The present drug approval system puts enormous stress on Type I errors and largely ignores <b>Type</b> <b>II</b> <b>errors,</b> thereby raising the cost of drug testing and delaying the availability of safe and effective drugs. A more balanced set of FDA drug approval standards, accounting for the consequences of both Type I and <b>Type</b> <b>II</b> <b>errors,</b> could result in better outcomes, as compared to the present system...|$|R
2500|$|In {{terms of}} false {{positive}}s and false negatives, a positive result corresponds to rejecting the null hypothesis, while a negative result corresponds to failing {{to reject the}} null hypothesis; [...] "false" [...] means the conclusion drawn is incorrect. Thus a type I error is a false positive, and a <b>type</b> <b>II</b> <b>error</b> is a false negative.|$|E
2500|$|A {{statistical}} hypothesis test compares a test statistic (z or t for examples) to a threshold. The test statistic (the formula {{found in the}} table below) is based on optimality. For a fixed level of Type I error rate, use of these statistics minimizes <b>Type</b> <b>II</b> <b>error</b> rates (equivalent to maximizing power). The following terms describe tests in terms of such optimality: ...|$|E
50|$|In statistics, beta may {{represent}} <b>type</b> <b>II</b> <b>error,</b> or regression slope.|$|E
50|$|While {{performing}} the McDonald-Kreitman test, scientists {{also have to}} avoid making too numerous <b>type</b> <b>II</b> <b>errors.</b> Otherwise, a test's results may be too flawed and its results will be termed useless.|$|R
3000|$|In ([14], p. 114), {{the author}} {{provides}} an interesting inequality relating the discrimination to type I and <b>type</b> <b>II</b> <b>errors</b> in hypothesis testing. This relation is stated {{by the following}} lemma: [...]...|$|R
40|$|On {{grounds of}} {{inclusion}} of undesirable votes (type I errors) and exclusion of desirable votes (<b>type</b> <b>II</b> <b>errors),</b> we question the convention of selecting only finalpassage votes for roll call analysis. We propose an alternative selection method {{based on the}} estimated salience and strategic significance of roll calls and argue that this method reduces type I and <b>type</b> <b>II</b> <b>errors.</b> We demonstrate that selection of roll calls based on alternative criteria has a major bearing on the asymmetry of partisan roll rates and, we conjecture that its application will also be substantively significant in other modes of inquiry. ...|$|R
50|$|A <b>type</b> <b>II</b> <b>error</b> {{occurs when}} the null {{hypothesis}} is false, but erroneously fails to be rejected. It is failing to assert what is present, a miss. A <b>type</b> <b>II</b> <b>error</b> may be compared with a so-called false negative (where an actual 'hit' was disregarded by the test and seen as a 'miss') in a test checking for a single condition with a definitive result of true or false. A <b>Type</b> <b>II</b> <b>error</b> is committed when we fail to believe a truth. In terms of folk tales, an investigator may fail to see the wolf ("failing to raise an alarm"). Again, H0: no wolf.|$|E
5000|$|There are {{two types}} of error that can occur using {{hypothesis}} testing, rejecting a valid model called type I error or [...] "model builders risk" [...] and accepting an invalid model called <b>Type</b> <b>II</b> <b>error,</b> β, or [...] "model user's risk". The level of significance or α is equal the probability of type I error. If α is small then rejecting the null hypothesis is a strong conclusion. For example, if α = 0.05 and the null hypothesis is rejected there is only a 0.05 probability of rejecting a model that is valid. Decreasing the probability of a <b>type</b> <b>II</b> <b>error</b> is very important. The probability of correctly detecting an invalid model is 1 - β. The probability of a <b>type</b> <b>II</b> <b>error</b> is dependent of the sample size and the actual difference between the sample value and the observed value. Increasing the sample size decreases the risk of a <b>type</b> <b>II</b> <b>error.</b>|$|E
5000|$|In {{statistical}} hypothesis testing, a type I error is the incorrect rejection {{of a true}} null hypothesis (a [...] "false positive"), while a <b>type</b> <b>II</b> <b>error</b> is incorrectly retaining a false null hypothesis (a [...] "false negative"). More simply stated, a type I error is the (false) detection of an effect that is not present, while a <b>type</b> <b>II</b> <b>error</b> is the failure to detect an effect that is present.|$|E
3000|$|... {{some of the}} {{outliers}} in q don’t have a correspondence in q^aux, i.e., {{we cannot}} violate anonymity {{of some of the}} outliers (<b>type</b> <b>II</b> <b>errors).</b> We will call such outliers undisclosed outliers; [...]...|$|R
5000|$|Biometric matching, such as for {{fingerprint}} recognition, {{facial recognition}} or iris recognition, {{is susceptible to}} type I and <b>type</b> <b>II</b> <b>errors.</b> The null hypothesis is that the input does identify someone in the searched list of people, so: ...|$|R
30|$|As a {{convenient}} way, {{we can find}} type I errors and their counts in the unigram frequency list of the segmentation results, and find <b>type</b> <b>II</b> <b>errors</b> and their counts in the bigram frequency list of the segmentation results.|$|R
5000|$|The {{statistical}} power desired is estimated by one minus beta, where beta {{is equal to}} the probability of making a <b>Type</b> <b>II</b> <b>error.</b> A <b>Type</b> <b>II</b> <b>error</b> is failing to reject that statistical null hypothesis (i.e., rho or delta equals zero), when in fact the null hypothesis is false in the population and should be rejected. Cohen (1977) recommends using power equal to [...]80 or 80%, for a beta = [...]20.|$|E
5000|$|A {{false sense}} of {{security}} caused by false negatives, which may delay final diagnosis (namely misdiagnosis with <b>Type</b> <b>II</b> <b>error).</b>|$|E
5000|$|We notice that, the Type I error [...] and <b>Type</b> <b>II</b> <b>error</b> [...] for {{identifying}} a correct Pareto set are respectively ...|$|E
30|$|If RCTs {{are clearly}} the {{preferred}} study design to assess treatment efficacy, {{the small number}} of patients in these RCTs may unbalance the baseline characteristics and, thus, influence the results. The addition of observational studies can increase greatly the sample size, thereby reducing the chance of <b>type</b> <b>II</b> <b>errors.</b>|$|R
40|$|Genetic {{parentage}} analyses {{provide a}} practical means {{with which to}} identify parent-offspring relationships in the wild. In Harrison et al. 's study (2013 a), we compare three methods of parentage analysis and showed that the number and diversity of microsatellite loci {{were the most important}} factors defining the accuracy of assignments. Our simulations revealed that an exclusion-Bayes theorem method was more susceptible to false-positive and false-negative assignments than other methods tested. Here, we analyse and discuss the trade-off between type I and <b>type</b> <b>II</b> <b>errors</b> in parentage analyses. We show that controlling for false-positive assignments, without reporting <b>type</b> <b>II</b> <b>errors,</b> can be misleading. Our findings illustrate the need to estimate and report both the rate of false-positive and false-negative assignments in parentage analyses. © 2013 John Wiley & Sons Ltd...|$|R
50|$|In the {{traditional}} language of statistical hypothesis testing, {{the sensitivity of}} a test is called the statistical power of the test, although the word power in that context has a more general usage that is not applicable in the present context. A sensitive test will have fewer <b>Type</b> <b>II</b> <b>errors.</b>|$|R
