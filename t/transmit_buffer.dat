47|48|Public
5000|$|The <b>transmit</b> <b>buffer</b> in A will {{load the}} first flit Z {{and send it}} to B.|$|E
50|$|The <b>transmit</b> <b>buffer</b> in A {{will then}} load the next flit Y {{and send it}} to B.|$|E
50|$|Until the mid-1990s, all of TCP's set timeouts and {{measured}} round-trip delays were based upon {{only the last}} transmitted packet in the <b>transmit</b> <b>buffer.</b> University of Arizona researchers Larry Peterson and Lawrence Brakmo introduced TCP Vegas, named after the largest Nevada city, in which timeouts were set and round-trip delays were measured for every packet in the <b>transmit</b> <b>buffer.</b> In addition, TCP Vegas uses additive increases in the congestion window. This variant was not widely deployed outside Peterson's laboratory. In a comparison study of various TCP congestion control algorithms, TCP Vegas {{appeared to be the}} smoothest followed by TCP CUBIC.|$|E
40|$|The {{analysis}} of the real-time properties of an embedded communication system relies on finding upper bounds on the Worst-Case Response Time (WCRT) of the messages that are exchanged among the nodes on the network. The classical WCRT {{analysis of}} Controller Area Network (CAN) implicitly assumes that at any given time, each node is able to enter its highest priority ready message into arbitration. However, in reality, CAN controllers may have some characteristics, such as non-abortable <b>transmit</b> <b>buffers,</b> which may break this assumption. This paper provides analysis for networks that contain nodes with non-abortable <b>transmit</b> <b>buffers,</b> as well as nodes that {{meet the requirements of}} the classical analysis. The impact on message WCRTs due to a limited number of transmission buffers with non-abortable behaviour is examined via two case-studies. I...|$|R
40|$|Abstractâ€”DVFS {{circuits}} {{are applied}} to network-controllers inorder to reduce power dissipation in networks. When receive and <b>transmit</b> <b>buffers</b> are relatively empty, energy is saved by lowering the packet processing rate. Simulations show an average energy savings of 29 % across various traffic loads and 19 % across various congestion levels. I...|$|R
3000|$|... are the [...] "QoS rate", the {{required}} rate to <b>transmit</b> the <b>buffer</b> data, {{and the rate}} to transmit the emergency data for the connection C [...]...|$|R
50|$|Active queue {{management}} (AQM) is the reorder or drop {{of network}} packets inside a <b>transmit</b> <b>buffer</b> that {{is associated with}} a network interface controller (NIC). This task is performed by the network scheduler.|$|E
50|$|No {{capability}} for <b>transmit</b> <b>buffer</b> byte count of zero.Receive lockup may occur if bus latency is large.External loopback on a live network may cause reception of invalid loopback failure indications.Receive descriptor zero byte count buffer interpreted as 4096 available bytes.Will poll computer memory every 1.6 ms for new packets to transmit.|$|E
5000|$|An [...] "underrun error" [...] {{occurs when}} the UART {{transmitter}} has completed sending a character and the <b>transmit</b> <b>buffer</b> is empty. In asynchronous modes this is treated as an indication that no data remains to be transmitted, rather than an error, since additional stop bits can be appended. This error indication is commonly found in USARTs, since an underrun is more serious in synchronous systems.|$|E
30|$|The {{analyses}} with NC use opportunistic two-way network coding [20] where a {{relay node}} XORs two packets intended for exchange between two neighbouring nodes. Neighbouring nodes {{do not need}} to communicate which packets each received, neither is opportunistic listening required with two-way coding, and the throughput gain with n-way coding is maximised when n = 2. First-in-first-out (FIFO) <b>transmit</b> <b>buffers</b> are employed and with NC the buffer is checked in a FIFO manner to obtain the first two packets that can be coded and transmitted over the scheduled temporal reuse set.|$|R
40|$|The FlexRay {{controller}} {{supports a}} single communication channel, {{and can be}} configured for either Channel A or Channel B. The Cyclic Redundancy Check (CRC) seed used to generate the Header CRC is different for Channel A and for Channel B. Therefore, connecting the FlexRay controller that is configured for Channel A to Channel B in the cluster (or vice-versa) is not allowed. Configurable Payload Length The Max Payload that can be supported by the controller during synchronous or aysnchronous transmission is user-configurable, depending upon specific design requirements. Configurable number of <b>Transmit</b> <b>Buffers</b> The number of <b>transmit</b> <b>buffers</b> in the FlexRay controller can be user-configured from {{a minimum of two}} to a maximum of 128 in powers of two. The block RAM resource utilization varies, {{based on the number of}} <b>Transmit</b> <b>buffers</b> and the configurable payload length. Configurable number of Receive Buffers The number of Receive buffers in the FlexRay controller can be user-configured from a minimum of two to a maximum of 128 in powers of two. The block RAM resource utilization varies, based on the number of Receive buffers and the configurable payload length. Configurable Receive FIFO depth The depth of the Receive FIFO in the FlexRay controller can be user-configured from a minimum of two to a maximum of 128 in powers of two. The block RAM resource utilization varies, based on the depth of the Receive FIFO and the configurable payload length. Frame ID, Cycle Counter and Message ID based Receive Filtering The FlexRay controller supports filtering of receive messages based on Frame ID, cycle counter and message ID. Any or all of these combinations can be used for filtering the receive messages. Applications The FlexRay controller is typically used in automotive networked applications, offering high fault tolerant synchronous and asynchronous transfer capabilities. It can function as a stand-alone FlexRay controller or it can be integrated with other Xilinx LogiCORE and EDK cores to build a variety of embedded systems. The FlexRay solution allows for flexible partioning between software and hardware functions by providing a customized, scalable controller that off loads network specific overhead from the host processor. This is ideal for applications demanding maximum host application performance...|$|R
30|$|The {{shortened}} list is then {{forwarded to}} the frequency domain scheduler where the frequency chunks are allocated to the UEs: in this process, all the frequency resource units are visited {{one after the other}} and for each UE in the TD list, the QoS metric is computed. A resource unit is allocated to the user with the highest QoS metric given by (12). After each resource unit is allocated, the average throughput of the chosen UE is updated and the next resource unit is visited. The scheduling process is repeated for the other resource units until all resources have been allocated or no data are left in the <b>transmitting</b> <b>buffers.</b>|$|R
5000|$|Some voice modems offer a {{very large}} <b>transmit</b> <b>buffer</b> (for example, 4 seconds worth of audio) coupled with a bug that {{prevents}} the host from requesting an [...] "abort playback". The result {{is that if a}} caller presses a touch-tone that's supposed to interrupt a message, and the host is providing unlimited audio data mediated by CTS alone, the end result is that the message can't be interrupted for at least 4 seconds.|$|E
50|$|Because the timeout timer is reset {{whenever}} there is {{progress in the}} <b>transmit</b> <b>buffer,</b> this allows New Reno to fill large holes, or multiple holes, in the sequence space much like TCP SACK. Because New Reno can send new packets {{at the end of}} the congestion window during fast recovery, high throughput is maintained during the hole-filling process, even when there are multiple holes, of multiple packets each. When TCP enters fast recovery it records the highest outstanding unacknowledged packet sequence number. When this sequence number is acknowledged, TCP returns to the congestion avoidance state.|$|E
50|$|Like random early {{detection}} (RED), Blue operates by randomly dropping or marking packet with {{explicit congestion notification}} mark before the <b>transmit</b> <b>buffer</b> of the network interface controller overflows. Unlike RED, however, it requires little or no tuning to be performed by the network administrator. A Blue queue maintains a drop/mark probability p, and drops/marks packets with probability p {{as they enter the}} queue. Whenever the queue overflows, p is increased by a small constant pi, and whenever the queue is empty, p is decreased by a constant pd < pi.|$|E
3000|$|As stated previously, {{depending}} on which relay receives or <b>transmits</b> data, the <b>buffers</b> move from the current state s [...]...|$|R
30|$|Upon {{finishing}} the first-round reservation for each node, the hub transmits beacon frame {{to start the}} data transmission. When the allocated interval commences, each node <b>transmits</b> the <b>buffered</b> data to the hub. The hub receives the uplink data from each node in each allocation interval. All nodes and hub turn into the sleep mode {{in the rest of}} the time to save energy.|$|R
30|$|The work in [21], {{proposed}} a seamless handover {{scheme based on}} proactive caching of data packets. Here, when an OBU is about to leave the coverage area of an RSU, the buffered packets will be forwarded to the entire candidate RSUs. The new RSU {{which is one of}} the candidates will <b>transmit</b> the <b>buffered</b> packets to the OBU and a message is sent to the rest of the candidate RSUs to discard the cached packets.|$|R
5000|$|Transmission {{operation}} is simpler as the timing {{does not have}} to be determined from the line state, nor is it bound to any fixed timing intervals. As soon as the sending system deposits a character in the shift register (after completion of the previous character), the UART generates a start bit, shifts the required number of data bits out to the line, generates and sends the parity bit (if used), and sends the stop bits. Since full-duplex operation requires characters to be sent and received at the same time, UARTs use two different shift registers for transmitted and received characters. High performance UARTs could contain a transmit FIFO (first in first out) buffer to allow a CPU or DMA controller to deposit multiple characters in a burst into the FIFO rather than have to deposit one character at a time into the FIFO. Since transmission of a single or multiple characters may take a long time relative to CPU speeds, a UART maintains a flag showing busy status so that the host system knows if there is at least one character in the <b>transmit</b> <b>buffer</b> or shift register; [...] "ready for next character(s)" [...] may also be signaled with an interrupt.|$|E
3000|$|The second paper, [...] "Exploiting <b>transmit</b> <b>buffer</b> {{information}} at the receiver in block-fading channels" [...] by Dinesh Rajan considers an interesting problem: {{how to use the}} <b>transmit</b> <b>buffer</b> {{information at}} the receiver (TBIR) when the transmitter has a partial channel state information. The author develops a design framework for systems that utilize feedback and feed-forward information in block fading channels. TBIR is used at the receiver to efficiently quantize and report the state of the fading channel back to the transmitter. The results show that this innovative approach can lead to a reduction of the packet loss as well as power savings.|$|E
40|$|This paper {{presents}} and analyzes a point-tomultipoint (P 2 MP) network {{that uses a}} number of freespace optical (FSO) links for data transmission from the central node to the different remote nodes of the network. A common backup radio frequency (RF) link {{can be used by}} the central node for data transmission to any remote node in case any one of the FSO links fails. Each remote node is assigned a <b>transmit</b> <b>buffer</b> at the central node. Considering the transmission link from the central node to a tagged remote node, we study various performance metrics. Specifically,we study the throughput from the central node to the tagged node, the average <b>transmit</b> <b>buffer</b> size, the symbol queuing delay in the <b>transmit</b> <b>buffer,</b> the efficiency of the queuing system, the symbol loss probability, and the RF link utilization. Numerical examples are presented to compare the performance of the proposed P 2 MP hybrid FSO/RF network with that of a P 2 MP FSO-only network and show that the P 2 MP hybrid FSO/RF network achieves considerable performance improvement over the P 2 MP FSO-only network...|$|E
3000|$|... [...]) is {{the average}} waiting time of user i in the queue plus the service time. In the {{optimization}} problem, {{it is assumed that}} users have enough traffic waited in the queue and ready to be <b>transmitted</b> (i.e., full <b>buffer</b> assumption).|$|R
40|$|Abstractâ€”This paper {{models and}} {{analyzes}} {{the performance of}} an amplify-and-forward cooperative diversity wireless network. We propose a Markov-based model, which encompasses the following aspects: 1) the transmission using amplify-and-forward cooperative diversity at the physical layer; 2) a flow control pro-tocol, finite and infinite <b>transmitting</b> <b>buffers,</b> and an ARQ-based error recovery mechanism at the radio link layer; and 3) a bursty traffic pattern at the application layer. We derive expressions for packet delivery probability and distribution of packet delivery delay. We numerically quantify improvement in terms of packet delivery probability and packet delivery delay for increasing SNR and/or cooperative nodes. For an additional cooperative node, we quantify the amount of SNR which can be reduced (i. e., SNR saving) without degrading the system performance. Also, the minimum SNR and cooperative nodes which satisfy a probabilistic delay bound are computed. We then derive a sufficient condition that ensures an increase in packet delivery probability. Unlike numerical evaluation of the model, this sufficient condition does not require computation of stationary distribution of the Markov chain. It only involves parameter adjustment at physical, radio link, and application layers, hence substantially reducing the com-putation effort. Based on the developed model, we design a power allocation algorithm, which computes the minimum transmission power under a packet delivery probability constraint. We then use the derived sufficient condition to reduce complexity of the power allocation algorithm. Index Termsâ€”Amplify-and-forward (AF), cooperative diversity (CD), Markov chain, monotonicity, stochastic dominance. I...|$|R
2500|$|New Horizons {{recorded}} scientific instrument data to its solid-state memory buffer at each encounter, then transmitted {{the data}} to Earth. Data storage is done on two low-power solid-state recorders (one primary, one backup) holding up to s each. Because of the extreme distance from Pluto and the Kuiper belt, only one buffer load at those encounters can be saved. This is because New Horizons will require approximately 16 months after it has left the vicinity of Pluto to <b>transmit</b> the <b>buffer</b> load back to Earth. [...] At Pluto's distance, transmissions from the space probe back to Earth took four hours and 25 minutes to traverse 4.7 billion km of space.|$|R
40|$|Abstract â€“ Resilient Packet Ring (RPR) is an {{emerging}} {{standard for the}} construction of local and metropolitan area networks. The Priority Queue (PQ) algorithm, which is recommended as the scheduling scheme for RPR always gives priority to the transit buffer. Using this scheduling scheme in single transit buffer RPR, the high priority traffic, such as video packets waiting to access the ring at congested node in the <b>transmit</b> <b>buffer</b> will suffer large delays and unsteady delay jitters. In this paper, we propose a new scheduling scheme for RPR {{to improve the quality of}} service for video traffic transmission. The proposed scheduling scheme alternately selects packets from the single transit buffer and the high priority <b>transmit</b> <b>buffer</b> using Deficit Round-Robin (DRR) algorithm. If there is no packet in the above two buffers, low priority <b>transmit</b> <b>buffer</b> is then served. We investigate the system performance for transmission of MPEG- 4 encoded bitstream as high priority traffic in an RPR network in scenario that all traffic is forwarded to a common node. We report end-to-end delay and delay jitter for I, P and B frames of several encoded video streams. Simulation results show certain improvement on overall delay and delay jitter performance for all types of video frames, especially for I frames which are the most important frames for video reconstruction...|$|E
40|$|Abstractâ€”In hardware, {{packet loss}} may happen due to {{overflow}} from a finite-depth <b>transmit</b> <b>buffer.</b> To prevent such losses and further improve rate selection, we exploit statistical knowledge of <b>transmit</b> <b>buffer</b> occupancy and source packet distribution in IEEE 802. 11 -based systems, which have variable frame slots. We consider a traditional method of rate adaptation based on channel quality information {{and evaluate the}} throughput gain in hardware when the buffer occupancy and source packet distribution information are known. Our optimization objective is to maximize the throughput with constant transmit power since most IEEE 802. 11 APs and nodes operate in this manner. We also derive an upper bound of the improvement introduced by exploiting the offered load distribution and buffer status information. By evaluating the effect of diverse buffer sizes with different packet arrival distributions, both our theoretical analysis and our experimental {{results show that the}} throughput can be greatly improved in many cases when the source packet distribution and buffer status information are exploited...|$|E
30|$|However, when {{offered load}} {{is higher than}} 40 Mbps, the {{guaranteed}} user throughput using the BB protocol degrades {{with an increase in}} the offered load. By contrast, the guaranteed user throughput with static chunk allocation increases until a peak is reached and roughly the same level of throughput is maintained. Note that the system bandwidth is only 20 MHz, out of which only 10 MHz can be allocated independently. The remaining 10 MHz of the system bandwidth is used to transmit the complex conjugate of data symbols in order to maintain a real valued signal. With these parameters, the peak raw data rate in an isolated cell would be 10 Mbps assuming a binary phase shift keying (BPSK) modulation and 80 Mbps assuming 256 -quadrature amplitude modulation (QAM) modulation format. Therefore, this reflects a scenario where a user with heavy traffic demand competes with other users that may possibly have heavy or light traffic demands. Provided that the user with heavy traffic demand has successfully accessed the chunk and reserved it by transmitting a BB signal, the chunks available at the AP are exclusively assigned to that user. Such chunks appear unavailable to the user that has just entered the network or switched from idle (empty <b>transmit</b> <b>buffer)</b> to active (containing at least a protocol data unit (PDU) queued in the <b>transmit</b> <b>buffer).</b> Likewise, assuming that an active user releases a chunk when its <b>transmit</b> <b>buffer</b> is empty, it will find that the chunks are all occupied at a later point in time when such user attempts to transmit data again. Moreover, increasing the traffic load increases the number of frames that a chunk reserved by a user appears unavailable to other users. Thus, the ability of a user to reacquire the released chunks decrease with an increase in traffic load. Therefore, the guaranteed user throughput decreases.|$|E
30|$|In the following, the {{performance}} of BB signalling is compared against a cluster-based resource partitioning approach as well as against a full chunk reuse approach. For the results presented in this section, the chunk reservation policy is that a user that has transmitted a BB is assigned a chunk {{as long as it}} has additional data to <b>transmit</b> in its <b>buffer.</b>|$|R
40|$|A new message {{handling}} concept {{does not require}} buffer locking and assures data integrity for all message buffers. In an event driven communication system, the application program of each node in the system may autonomously decide when the transmission of a specific message is to be started, usually when the data content of a message is updated, or when data from remote nodes is requested. In FlexRay, as in other time triggered communication systems, the schedule, when which message is to be transmitted, is decided a priori for all nodes and all messages by the system designer. The application program cannot influence the point of time when a message is transmitted, at the most it may prevent a message from being transmitted at all. While the application program controls {{the operation of the}} communication controller in event driven systems, the application program has to be synchronized to the communication schedule in time triggered systems, updating the messageâ€™s data content in time before the messageâ€™s transmission time. This fundamental difference in how application program and communication controller control the transfer of messages requires different concepts for the event driven and for the time triggered communication controllers how they handle the messages internally. The time triggered communication controller operates on an excerpt of the message schedule, leaving aside those messages that are neither transmitted nor used by the particular node. It transmits messages that are stored in the <b>transmit</b> <b>buffers</b> at their dedicated time slots (provided that the application has updated them on time) and sorts received messages into their dedicated receive buffers (provided that they Â© automotiv...|$|R
40|$|Abstract â€” This paper {{describes}} {{the benefit of}} control systems with network architecture over traditional systems with a central processor. A suitable standard protocol, CAN, is briefly presented and its current and future use in automobile machines is discussed. An important task {{is to find a}} way to make it possible to use standard network modules from different producers in a network specially designed for a specific machine. A solution to this problem is the design rules "CAN Kingdom" and the basics for this are presented. This model consists of the collection and display of data which are independent of each other and remotely executed. The data collection module receives data through CAN bus, and the data display module display these data via GUI designed by Qt/Embedded in Microcontroller. Temperature sensor and IR sensors are used as data collection agents and LCD and motor is used as output agents for an application here to demonstrate the effectiveness of CAN. To avoid data corruption, redundancy and reduce the complexity of circuit, MCP 2510 is used for extending CAN. The whole CAN bus system is made up of the MCP 2510 which is a stand-alone CAN controller with SPI Interface and the MCP 2551 which acts as an interface between the CAN controller and the physical bus which carries data. MCP 2510 is capable of both acceptance filtering and message management. It includes three <b>transmit</b> <b>buffers</b> and two receive buffers that reduce the amount of microcontroller (MCU) management required. The MCU communication is implemented via an industry standard Serial Peripheral Interface (SPI) with data rates up to 5 Mb/s...|$|R
40|$|Network {{processors}} today {{consist of}} multiple parallel processors (micro engines) with support for multiple threads to exploit packet level parallelism inherent in network workloads. With such concurrency, packet ordering at {{the output of}} the network processor cannot be guaranteed. This paper studies the effect of concurrency in network processors on packet ordering. We use a validated Petri net model of a commercial network processor, Intel IXP 2400, {{to determine the extent of}} packet reordering for IPv 4 forwarding application. Our study indicates that in addition to the parallel processing in the network processor, the allocation scheme for the <b>transmit</b> <b>buffer</b> also adversely impacts packet ordering. In particular, our results reveal that these packet reordering results in a packet retransmission rate of up to 61 %. We explore different <b>transmit</b> <b>buffer</b> allocation schemes namely, contiguous, strided, local, and global which reduces the packet retransmission to 24 %. We propose an alternative scheme, packet sort, which guarantees complete packet ordering while achieving a throughput of 2. 5 Gbps. Further, packet sort outperforms the in-built packet ordering schemes in the IXP processor by up to 35 %...|$|E
30|$|The {{assumption}} that the reserved chunk will be allocated to the same user in the next slot ensures that the a priori knowledge {{of the amount of}} CCI caused to the user served by a neighbouring cell is valid. However, the results presented in Figure 6 have demonstrated that allowing for the reservation of chunks until the <b>transmit</b> <b>buffer</b> is emptied deteriorates the system performance once the system is overloaded. To address this shortcoming, a fair chunk reservation mechanism was proposed in Section 3.4, whose performance is analysed in the next section.|$|E
40|$|Abstractâ€”In this paper, we {{show that}} packet {{blocking}} probability at the transmitter can be traded off with average queue delay for fading channels in environmentally powered wireless networks with bursty packet arrivals. Varying {{the size of the}} energy storage unit {{as well as that of}} the <b>transmit</b> <b>buffer</b> queue aids in the trade off. We cast the problem into a Markov Decision Process (MDP) and use Dynamic Programming to obtain the optimal scheduler. Further, we develop two different low complexity schedulers that achieve near optimal performance in different system settings. I...|$|E
30|$|In {{time slot}} t= 2, the channel {{conditions}} are as BG. Therefore, {{in the first}} subslot, {{there will not be}} any transmission from the BS to relay and the overall waiting time of the packets 2,â€¦,N will be increased by one slot. However, due to good condition of the relay channel, packet 1 will be <b>transmitted</b> from the <b>buffer</b> of the relay to the user in the second subslot.|$|R
40|$|Earlier {{studies have}} {{exploited}} statistical multiplexing of flows {{in the core}} of the Internet to reduce the buffer requirement in routers. Reducing the memory requirement of routers is important as it enables an improvement in performance {{and at the same time}} a decrease in the cost. In this paper, we observe that the links {{in the core of}} the Internet are typically over-provisioned and this can be exploited to reduce the buffering requirement in routers. The small on-chip memory of a network processor (NP) can be effectively used to buffer packets during most regimes of traffic. We propose a dynamic buffering strategy which buffers packets in the receive and <b>transmit</b> <b>buffers</b> of a NP when the memory requirement is low. When the buffer requirement increases due to bursts in the traffic, memory is allocated to packets in the off-chip DRAM. This scheme effectively mitigates the DRAM access bottleneck, as only a part of the traffic is stored in the DRAM. We build a Petri net model and evaluate the proposed scheme with core Internet like traffic. At 77 % link utilization, the dynamic buffering scheme has a drop rate of just 0. 65 %, whereas the traditional DRAM buffering has 4. 64 % packet drop rate. Even with a high link utilization of 90 %, which rarely happens in the core, our dynamic buffering results in a packet drop rate of only 2. 17 %, while supporting a throughput of 7. 39 Gbps. We study the proposed scheme under different conditions to understand the provisioning of processing threads and to determine the queue length at which packets must be buffered in the DRAM. We show that the proposed dynamic buffering strategy drastically reduces the buffering requirement while still maintaining low packet drop rates...|$|R
30|$|CTN: A CTN emulates a CAN-based device. Each of the CTN is {{realized}} {{by a small}} Nios II softcore CPU system and a CAN controller to generate or consume CAN messages. The 8 byte payload of each generated CAN message contains a timestamp that is set shortly before the CAN message is transferred to the CAN controller. In case a CAN message transmit attempt fails (i.e., another CAN message with a higher priority is sent simultaneously), the CAN controller will retry sending indefinitely until it succeeds. Each CAN controller has a <b>transmit</b> FIFO <b>buffer</b> with a length of 30 messages. In case the FIFO buffer is overrun, new messages are lost. All CTNs are also connected to the TTNoC; specifically they can receive configuration messages from the MU.|$|R
