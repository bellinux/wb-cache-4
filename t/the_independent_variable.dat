4080|10000|Public
25|$|Here {{there is}} no ∂f/∂t term since f itself {{does not depend on}} <b>the</b> <b>independent</b> <b>variable</b> t directly.|$|E
25|$|Polynomial {{least squares}} {{describes}} {{the variance in}} a prediction of the dependent variable {{as a function of}} <b>the</b> <b>independent</b> <b>variable</b> and the deviations from the fitted curve.|$|E
25|$|N.B. The reader {{should be}} warned {{here that the}} order of the {{variables}} are reversed! y is <b>the</b> <b>independent</b> <b>variable</b> and x is the dependent variable, e.g., x = sin(y).|$|E
50|$|If both of <b>the</b> <b>independent</b> <b>variables</b> are continuous, it {{is helpful}} for {{interpretation}} to either center or standardize <b>the</b> <b>independent</b> <b>variables,</b> X and Z. (Centering involves subtracting the overall sample mean score from the original score; standardizing does the same followed by dividing by the overall sample standard deviation.) By centering or standardizing <b>the</b> <b>independent</b> <b>variables,</b> <b>the</b> coefficient of X or Z {{can be interpreted as}} the effect of that variable on Y at the mean level of <b>the</b> other <b>independent</b> <b>variable.</b>|$|R
50|$|In statistics, {{regression}} analysis is a statistical process for estimating {{the relationships among}} variables. It includes many techniques for modeling and analyzing several variables, when {{the focus is on}} the relationship between a dependent variable and one or more <b>independent</b> <b>variables.</b> More specifically, {{regression analysis}} helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of <b>the</b> <b>independent</b> <b>variables</b> is varied, while <b>the</b> other <b>independent</b> <b>variables</b> are held fixed. Most commonly, regression analysis estimates the conditional expectation of the dependent <b>variable</b> given <b>the</b> <b>independent</b> <b>variables</b> - that is, the average value of the dependent <b>variable</b> when <b>the</b> <b>independent</b> <b>variables</b> are fixed. Less commonly, the focus is on a quantile, or other location parameter of the conditional distribution of the dependent <b>variable</b> given <b>the</b> <b>independent</b> <b>variables.</b> In all cases, the estimation target is a function of <b>the</b> <b>independent</b> <b>variables</b> called <b>the</b> regression function. In regression analysis, it is also of interest to characterize the variation of the dependent variable around the regression function which can be described by a probability distribution.|$|R
30|$|In the {{examples}} of [15], the index of <b>the</b> <b>independent</b> <b>variables</b> of h cannot be 1, but the index of <b>the</b> <b>independent</b> <b>variables</b> of h can be 1 in this paper because _x→∞h(x,x,...,x)/x= 0 is replaced by _x→∞h(x,x,...,x)/x=λ<Γ (n- 1 -μ_n- 2)/eγ_ 1.|$|R
25|$|A complex {{function}} {{is one in}} which <b>the</b> <b>independent</b> <b>variable</b> and the dependent variable are both complex numbers. More precisely, a complex {{function is}} a function whose domain and range are subsets of the complex plane.|$|E
25|$|The {{differential}} equation F(x) = (x) {{has a special}} form: the right-hand side contains only the dependent variable (here x) and not <b>the</b> <b>independent</b> <b>variable</b> (here F). This simplifies the theory and algorithms considerably. The problem of evaluating integrals is thus best studied in its own right.|$|E
25|$|Measuring the pressure-volume-temperature {{state of}} a material. In DAC work, {{this is done}} by {{applying}} pressure with the diamond anvils, applying temperature with lasers/resistive heaters, and measuring the volume response with X-ray diffraction. The thermal expansion and compressibility can then be defined in an equation of state with <b>the</b> <b>independent</b> <b>variable</b> of volume.|$|E
3000|$|... 2  =  220.03 (44), p < . 001); thus, <b>the</b> {{model with}} <b>independent</b> <b>variables</b> was appropriated. <b>The</b> <b>independent</b> <b>variables,</b> gender (− 2 LL =  1320.35, χ [...]...|$|R
50|$|<b>The</b> <b>independent</b> <b>variables</b> are <b>the</b> intensities.|$|R
3000|$|... d Multicollinearity is {{observed}} when {{two or more}} <b>independent</b> <b>variables</b> are highly correlated, i.e., {{at least one of}} <b>the</b> <b>independent</b> <b>variables</b> can be computed as a linear combination of the rest to a statistically significant degree. If <b>the</b> <b>independent</b> <b>variables</b> are multicollinear, then the results for the ordinary least squared regression may be computed incorrectly.|$|R
25|$|More generally, {{the shape}} of the {{resulting}} curve, especially for very high or low values of <b>the</b> <b>independent</b> <b>variable,</b> may be contrary to commonsense, i.e. to what is known about the experimental system which has generated the data points. These disadvantages can be reduced by using spline interpolation or restricting attention to Chebyshev polynomials.|$|E
25|$|Homogeneous linear {{differential}} equations are a subclass of linear {{differential equations}} {{for which the}} space of solutions is a linear subspace i.e. the sum of any set of solutions or multiples of solutions is also a solution. The coefficients of the unknown function and its derivatives in a linear differential equation are allowed to be (known) functions of <b>the</b> <b>independent</b> <b>variable</b> or variables; if these coefficients are constants then one speaks of a constant coefficient linear differential equation.|$|E
25|$|Repeated-measures {{experiments}} are those which take place through intervention on multiple occasions. In {{research on the}} effectiveness of psychotherapy, experimenters often compare a given treatment with placebo treatments, or compare different treatments against each other. Treatment type is <b>the</b> <b>independent</b> <b>variable.</b> The dependent variables are outcomes, ideally assessed in several ways by different professionals. Using crossover design, researchers can further increase the strength of their results by testing both of two treatments on two groups of subjects.|$|E
40|$|The {{techniques}} {{discussed in}} our series, thus far, examine unidirectional relationships – i. e. how <b>the</b> <b>independent</b> <b>variables</b> affect <b>the</b> dependent variable. The assumptions {{were that the}} dependent response is random and subject to error whereas <b>the</b> <b>independent</b> <b>variables</b> could be measured directly (error-free), interdependency or simultaneous causation among these <b>independent</b> <b>variables</b> were not modelled. Multicolinearity among <b>the</b> <b>independent</b> <b>variables</b> is an issue which we could resolve using PCA or Factor analysis(2) to derive independent components/ factors for modelling purposes, given that meaningful interpretations are feasible. Structural equation model (SEM) is used to examine multiple and interrelated dependenc...|$|R
30|$|The {{validated}} {{model was}} used to optimize the process using the tool response/desirability profiling of Statistica 8.0. The desirability function allows the response surface produced be inspected by fitting the observed responses using the above mentioned equation based on levels of <b>the</b> <b>independent</b> <b>variables.</b> This equation {{was used to}} predict values for response (inulinase immobilization yield) at different combinations of levels of <b>the</b> <b>independent</b> <b>variables,</b> specify desirability functions for the dependent variables, and {{to search for the}} levels of <b>the</b> <b>independent</b> <b>variables</b> that produce <b>the</b> most desirable responses for the dependent variables (immobilization yield) [13].|$|R
30|$|All <b>the</b> <b>independent</b> <b>variables</b> {{are also}} dichotomous or categorical.|$|R
25|$|Cauchy {{makes some}} general remarks about {{functions}} in Chapter I, Section 1 of his Analyse algébrique (1821). From {{what he says}} there, {{it is clear that}} he normally regards a function as being defined by an analytic expression (if it is explicit) or by an equation or a system of equations (if it is implicit); where he differs from his predecessors is that he is prepared to consider the possibility that a function may be defined only for a restricted range of <b>the</b> <b>independent</b> <b>variable.</b>|$|E
25|$|This {{highlights}} {{a common}} error: this example is {{an abuse of}} OLS which inherently requires that the errors in <b>the</b> <b>independent</b> <b>variable</b> (in this case height) are zero or at least negligible. The initial rounding to nearest inch plus any actual measurement errors constitute a finite and non-negligible error. As a result, the fitted parameters are not the best estimates they are presumed to be. Though not totally spurious the error in the estimation will depend upon relative size of the x and y errors.|$|E
25|$|Reeves and Nass {{established}} two rules {{before the}} test- when a computer asks a user about itself, the user will give more positive responses than when a different computer asks the same questions. They expected {{people to be}} less variable with their responses when they took a test and then answered a questionnaire on the same computer. They wanted to see that computers, although not human, can implement social responses. <b>The</b> <b>independent</b> <b>variable</b> was the computer (there are 2 in the test), and the dependent variable was the evaluation responses. The control was a pen-and-paper questionnaire.|$|E
5000|$|<b>The</b> <b>independent</b> <b>variables</b> in <b>the</b> {{estimating}} equation for 2006 include: ...|$|R
30|$|<b>The</b> <b>independent</b> <b>variables</b> in <b>the</b> {{study are}} power {{distance}} orientation and CSE.|$|R
50|$|The null {{hypothesis}} {{for the overall}} model fit: The overall model does not predict re-arrest. OR, <b>the</b> <b>independent</b> <b>variables</b> as a group {{are not related to}} being re-arrested. (And For <b>the</b> <b>Independent</b> variables: any of <b>the</b> separate <b>independent</b> <b>variables</b> is not related to the likelihood of re-arrest).|$|R
25|$|There {{has been}} some {{controversy}} over the relative strengths {{of different types of}} research. Because randomized trials provide clear, objective evidence on “what works”, policy makers often take only those studies into consideration. Some scholars have pushed for more random experiments in which teaching methods are randomly assigned to classes. In other disciplines concerned with human subjects, like biomedicine, psychology, and policy evaluation, controlled, randomized experiments remain the preferred method of evaluating treatments. Educational statisticians and some mathematics educators have been working to increase the use of randomized experiments to evaluate teaching methods. On the other hand, many scholars in educational schools have argued against increasing the number of randomized experiments, often because of philosophical objections, such as the ethical difficulty of randomly assigning students to various treatments when the effects of such treatments are not yet known to be effective, or the difficulty of assuring rigid control of <b>the</b> <b>independent</b> <b>variable</b> in fluid, real school settings.|$|E
25|$|Early {{evidence}} relating tobacco smoking to {{mortality and}} morbidity came from observational studies employing regression analysis. In {{order to reduce}} spurious correlations when analyzing observational data, researchers usually include several variables in their regression models {{in addition to the}} variable of primary interest. For example, suppose we have a regression model in which cigarette smoking is <b>the</b> <b>independent</b> <b>variable</b> of interest, and the dependent variable is lifespan measured in years. Researchers might include socio-economic status as an additional independent variable, to ensure that any observed effect of smoking on lifespan is not due to some effect of education or income. However, it is never possible to include all possible confounding variables in an empirical analysis. For example, a hypothetical gene might increase mortality and also cause people to smoke more. For this reason, randomized controlled trials are often able to generate more compelling evidence of causal relationships than can be obtained using regression analyses of observational data. When controlled experiments are not feasible, variants of regression analysis such as instrumental variables regression may be used to attempt to estimate causal relationships from observational data.|$|E
500|$|If [...] is a {{dependent}} variable, then often the subscript x {{is attached to}} the D to clarify <b>the</b> <b>independent</b> <b>variable</b> x.|$|E
2500|$|... <b>the</b> <b>in{{dependent}}</b> <b>variables</b> are a {{cause of}} the changes in the dependent variable; ...|$|R
5000|$|Do {{changes in}} <b>the</b> <b>independent</b> <b>variable(s)</b> have {{significant}} {{effects on the}} dependent variables? ...|$|R
5000|$|Gather all <b>the</b> <b>independent</b> <b>variables</b> {{that are}} likely to {{influence}} the dependent variable.|$|R
500|$|The {{derivative}} of {{a function of}} a single variable at a chosen input value, when it exists, is {{the slope of the}} tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value. [...] For this reason, the derivative is often described as the [...] "instantaneous rate of change", the ratio of the instantaneous change in the dependent variable to that of <b>the</b> <b>independent</b> <b>variable.</b>|$|E
500|$|The {{vertical}} coordinate {{is handled}} in various ways. Lewis Fry Richardson's 1922 model used geometric height (...) as the vertical coordinate. Later models substituted the geometric [...] coordinate with a pressure coordinate system, {{in which the}} geopotential heights of constant-pressure surfaces become dependent variables, greatly simplifying the primitive equations. [...] This correlation between coordinate systems can be made since pressure decreases with height through the Earth's atmosphere. The first model used for operational forecasts, the single-layer barotropic model, used a single pressure coordinate at the 500-millibar (about [...] ) level, and thus was essentially two-dimensional. High-resolution models—also called mesoscale models—such as the Weather Research and Forecasting model tend to use normalized pressure coordinates referred to as sigma coordinates. [...] This coordinate system receives its name from <b>the</b> <b>independent</b> <b>variable</b> [...] used to scale atmospheric pressures {{with respect to the}} pressure at the surface, and in some cases also with the pressure {{at the top of the}} domain.|$|E
2500|$|In {{engineering}} and science, one often {{has a number}} of data points, obtained by sampling or experimentation, which represent the values of a function for a limited number of values of <b>the</b> <b>independent</b> <b>variable.</b> It is often required to interpolate (i.e., estimate) the value of that function for an intermediate value of <b>the</b> <b>independent</b> <b>variable.</b>|$|E
5000|$|The jth parameter, , of the {{distribution}} depends on <b>the</b> <b>independent</b> <b>variables,</b> [...] through ...|$|R
5000|$|... <b>the</b> <b>in{{dependent}}</b> <b>variables</b> are a {{cause of}} the changes in the dependent variable; ...|$|R
30|$|Xi’s, <b>the</b> <b>independent</b> <b>{{variables}}</b> {{consist of}} variables associated with Household characteristics and Individual characteristics.|$|R
