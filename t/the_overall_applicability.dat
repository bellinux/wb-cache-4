42|10000|Public
25|$|As the price/performance {{of general}} purpose graphic {{processors}} (GPGPUs) has improved, {{a number of}} petaflop supercomputers such as Tianhe-I and Nebulae have started to rely on them. However, other systems such as the K computer continue to use conventional processors such as SPARC-based designs and <b>the</b> <b>overall</b> <b>applicability</b> of GPGPUs in general purpose high performance computing applications {{has been the subject}} of debate, in that while a GPGPU may be tuned to score well on specific benchmarks its overall applicability to everyday algorithms may be limited unless significant effort is spent to tune the application towards it. However, GPUs are gaining ground and in 2012 the Jaguar supercomputer was transformed into Titan by replacing CPUs with GPUs.|$|E
50|$|A context {{adaptive}} system typically enables {{the user to}} maintain a certain application (in different forms) while roaming between different wireless access technologies, locations, devices and even simultaneously executing everyday tasks like meetings, driving a car etc. For example a context adaptive and hence ubiquitous navigation system would offer navigation support in the situations at home, indoor, outdoor, and in car. This involves making the navigation functionality available for different availability of output devices, input devices and location sensors as well as adapting the user interaction operability to the current speed, noise or operator handicaps while keeping in mind <b>the</b> <b>overall</b> <b>applicability</b> depending on the user preferences, his knowledge, current task etc.|$|E
50|$|As the price, {{performance}} and energy efficiency of general purpose graphic processors (GPGPUs) have improved, {{a number of}} petaFLOPS supercomputers such as Tianhe-I and Nebulae have started to rely on them. However, other systems such as the K computer continue to use conventional processors such as SPARC-based designs and <b>the</b> <b>overall</b> <b>applicability</b> of GPGPUs in general-purpose high-performance computing applications {{has been the subject}} of debate, in that while a GPGPU may be tuned to score well on specific benchmarks, its overall applicability to everyday algorithms may be limited unless significant effort is spent to tune the application towards it. However, GPUs are gaining ground and in 2012 the Jaguar supercomputer was transformed into Titan by retrofitting CPUs with GPUs.|$|E
40|$|Energy Conversion Statics {{deals with}} {{equilibrium}} situations and processes linking equilibrium states. A {{development of the}} basic theory of energy conversion statics and its applications is presented. In the applications {{the emphasis is on}} processes involving electrical energy. The text commences by introducing the general concept of energy with a survey of primary and secondary energy forms, their availability, and use. The second chapter presents the basic laws of energy conversion. Four postulates defining <b>the</b> <b>overall</b> range of <b>applicability</b> of the general theory are set out, demonstrating t...|$|R
40|$|Calculations of helium and the {{negative}} hydrogen ion are presented using the pseudospectral method. The fundamental analytic properties, including the presence of Kato cusps and logarithmic terms of the solutions to the Schrodinger equation, are explored and their effect on the convergence properties analyzed. We find that by most measures of error the pseudospectral method converges at an exponential rate. With this method, we calculate energies and perturbations due to the finite nuclear mass and relativity and interactions with the electromagnetic field. The value calculated for the absorption oscillator strength of the 1 [EXP] 1 S [RIGHTWARDS ARROW] 2 [EXP] 1 P transition is about as accurate as {{the best in the}} literature. A general prescription is given for choosing subdomains needed for exponential convergence. With this prescription and <b>the</b> <b>overall</b> general <b>applicability</b> of <b>the</b> method, we conclude pseudospectral methods can be applied to general fewelectron problems...|$|R
40|$|Laser induced {{fluorescence}} of the OH radical {{has been}} used for planar temperature measurement by means of a two-level vibrational model to describe the relationship between the population ratio of excited states and temperature. The quenching to vibrational transfer ratio between the excited states was used as a calibration parameter to fit the thermally assisted fluorescence (THAF) measurements to known temperature data. A rectangular shaped burner allowed for calibration and comparison of the thermally assisted temperature results with sodium line reversal data available in the literature for premixed methane-air flames. Excellent agreement between the two techniques was achieved for three different equivalence ratios. <b>Overall,</b> <b>the</b> <b>applicability</b> of THAF to planar temperature measurement was positively demonstrated in the present work...|$|R
30|$|The {{applicability}} of the evidence retrieved is questionable; most trials were conducted in a target population including—but not limited to—women suffering from infertility: Two trials [36, 38] included variable proportions of women suffering from infertility, miscarriage, or risk of preterm delivery; for two studies [37, 39], {{it is unclear whether}} and how many participants suffered from infertility while the fifth study [40] included a source population of women with proven fertility trying to conceive after miscarriage. It is unlikely that the mechanisms whereby any of the studied interventions might decrease de novo adhesion formation might differ in infertile versus fertile target populations; nevertheless, we judge <b>the</b> <b>overall</b> <b>applicability</b> of the retrieved best available evidence in a more general source population to a target population of women suffering from infertility to be limited at the best.|$|E
40|$|We {{investigate}} a binary mixture of bosonic atoms loaded into a state-dependent honeycomb lattice. For this system, {{the emergence of}} a so-called twisted-superfluid ground state was experimentally observed in [Soltan-Panahi et al., Nat. Phys. 8, 71 (2012) ]. Theoretically, the origin of this effect is not understood. We perform numerical simulations of an extended Bose-Hubbard model adapted to the experimental parameters employing the Multi-Layer Multi-Configuration Time-Dependent Hartree method for Bosons. Our results confirm <b>the</b> <b>overall</b> <b>applicability</b> of mean-field theory within the relevant parameter range. Beyond this, we provide a detailed analysis of correlation effects correcting the mean-field result. These have the potential to induce asymmetries in single shot time-of-flight measurements, but we find no indication of the patterns characteristic of the twisted superfluid. We comment on the restrictions of our model and possible extensions...|$|E
40|$|Abstract. Productivity is {{the ability}} to create a quality {{software}} product in a limited period with limited resources. The software engineering community advocates that the future of productivity lies in the field of domain engineering. However, existing domain engineering approaches suffer from the tension between productivity and applicability. In this paper we propose an approach that reduces this tension by adopting a domain engineering method called Application-based DOmain Modeling (ADOM) as an infrastructure for a new programming approach. The adopted ADOM is applied on Java as its underlying language. This approach will offer guidance and validation for application developers as mechanisms for improving their productivity. This is done by keeping the regular Java development environment and thus maintaining the developer's expressiveness and not compromising <b>the</b> <b>overall</b> <b>applicability</b> of the approach...|$|E
40|$|Abstract. —Methods of {{polarity}} determination {{are governed}} by both the basic assumptions of the analysis and particular auxiliary assumptions. Differences in basic assumptions between phylogenetic systematics and pattern cladistics result in different approaches to polarity deter-mination. In phylogenetic systematics outgroup comparison, the ontogenetic method and the paleontological method are valid procedures for determining polarity {{because they can be}} de-rived from basic evolutionary axioms. The applicability of each method of polarization in par-ticular instances depends on the veracity of auxiliary assumptions. Auxiliary assumptions entail necessary conditions implicit in the rationale of the method or the adequacy of available infor-mation. Each method has a unique combination of auxiliary assumptions and is not a special case of, or dependent on, other methods. Much of the recent debate regarding <b>the</b> <b>overall</b> rela-tive <b>applicability</b> of <b>the</b> three methods suffers from the failure to properly consider auxiliary assumptions. Outgroup comparison has the broadest utility, but the applicability and utility of all methods should be considered in each instance. [Phylogenetic systematics; character trans-formation; axioms; auxiliary assumptions; outgroup comparison; ontogenetic method; paleon-tological method. ] In phylogenetic systematics (Hennig, 1966) outgroup comparison, the ontoge-netic method and, to a lesser extent, the paleontological method have been widely accepted as appropriate methods of deter-mining the polarity of character transfor-mations (e. g., Eldredge and Cracraft, 1980...|$|R
40|$|Feature {{selection}} {{is an effective}} technique in reducing the dimensionality of features in many applications where datasets involve {{hundreds or thousands of}} features. The objective of feature {{selection is}} to find an optimal subset of relevant features such that the feature size is reduced and understandability of a learning process is improved without significantly decreasing <b>the</b> <b>overall</b> accuracy and <b>applicability.</b> This thesis focuses on the consistency measure where a feature subset is consistent if there exists a set of instances of length more than two with the same feature values and the same class labels. This thesis introduces a new consistency-based algorithm, Automatic Hybrid Search (AHS) and reviews several existing feature selection algorithms (ES, PS and HS) which are based on the consistency rate. After that, we conclude this work by conducting an empirical study to a comparative analysis of different search algorithms...|$|R
40|$|Masonry is {{characterized}} by the large variability of its components. Parameters like strength, bond and workmanship defects strongly influence the performance of <b>the</b> <b>overall</b> structure. <b>The</b> <b>applicability</b> of different computational modelling approaches to assess the structural behaviour of masonry has been studied. Two of the most relevant computational modelling approaches have been considered namely: finite element method (FEM) and distinct element method (DEM). In order to validate the numerical outcomes, comparisons with the experimental results have been undertaken. The aim {{of this paper is to}} contribute to the knowledge and selection of a suitable modelling approach for modelling low unit strength masonry structures. The results showed that in the case of low unit strength masonry, FEM is a more suitable approach to use. In fact, since in the considered case, the block is the weak component, it is not possible to assume the brick units as a rigid block. Therefore an accurate plasticity and cracking model for the brick is required...|$|R
40|$|The {{software}} development industry {{is dominated by}} a myriad of smaller organizations world-wide, including very small entities (VSEs), which have up to 25 people. Managing software process is a big challenge for practitioners. In 2011, due to the VSEs’ increasing importance, a set of ISO/IEC 29110 standards and guides were released. Although other initiatives are devoted to small entities, ISO/IEC 29110 is becoming the widely adopted standard. But it is an emerging standard and practitioners need to be actively engaged in their learning. In this sense, serious games offer the potential to entertain and educate. This study shows empirical evidence to support <b>the</b> <b>overall</b> <b>applicability</b> of the game proposed as learning tool. Moreover, {{the results indicate that}} the learning tool creates a positive experience, and therefore {{could be used as a}} strategy to promote the standard...|$|E
40|$|An {{introductory}} {{conceptual and}} empirical review stresses {{the need for}} a stable theoretical basis for union commitment research. The {{purpose of this paper is}} to develop a new conceptualization of union commitment based on the integration of two theories (the theory of reasoned action and the rationalistic approach to commitment). The integrated theory suggests that union commitment is composed of two dimensions, one based on instrumentality and one based on ideology, which are causally related to pro-union behavioral intentions and, in turn, to union participation. Propositions derived from the integrated theory are tested using data on 1486 blue-collar workers in Sweden. Results of linear structural equation modelling with latent variables and of multiple regression analyses provide strong support for the construct validity of the commitment dimensions and <b>the</b> <b>overall</b> <b>applicability</b> of the integrated theory. The central findings, their conceptual implications for the understanding of union commitment, and their practical implications for unions are discussed...|$|E
40|$|Implementation of {{automatic}} repeat request strategies {{is the main}} reliability option available in the Consultative Committee for Space Data Systems protocols to support data communications in deep-space missions. The large latency often experienced in such scenarios, however, penalizes the efficiency of retransmissions, making <b>the</b> <b>overall</b> <b>applicability</b> {{of automatic}} repeat request problematic. On the contrary, the use of erasure codes is a more appealing solution to mitigate packet losses. In this regard, preliminary studies {{on the use of}} binary low-density parity-check codes under maximum likelihood/iterative decoding have already shown the performance benefit they can bring with respect to traditional schemes based on retransmissions. This paper extends the analysis conducted in previous studies toward non-binary low-density parity-check codes. Performance assessment is carried out with respect to reliability metrics (codeword error rate) and encoding/decoding complexity. Finally, the integration of erasure codes into the Consultative Committee for Space Data Systems protocol stack is discussed, by presenting the implementation in the Licklider Transmission Protocol...|$|E
40|$|Various {{environmental}} regulations imply {{that it is}} important to minimize the cost associated with treatment of different industrial wastes prior to its discharge to the environment. In this paper, an algebraic methodology, based on the principles of process integration, is proposed to target the minimum waste treatment flow rate to satisfy an environmentally acceptable discharge limit. In the proposed methodology, treatment units with fixed outlet concentrations are considered. It is observed that it is not necessary to maximize the usage of the treatment unit with the minimum specific cost. Hence, a prioritized cost for each treatment unit is devised to select appropriate treatment units that minimize <b>the</b> <b>overall</b> operating cost. <b>Applicability</b> of the proposed methodology is demonstrated through examples. Thanks to the Department of Science and Technology (DST), India and National Research Foundation, South Africa for supporting the collaboration between IIT Bombay, India and University of Pretoria, South Africa under ‘Indian/ South African Science and Technology Cooperation Programme. [URL]...|$|R
40|$|International Telemetering Conference Proceedings / September 28 - 30, 1982 / Sheraton Harbor Island Hotel and Convention Center, San Diego, CaliforniaClassical {{and public}} key {{cryptography}} for communications privacy are discussed regarding their relative implementation complexity and <b>overall</b> <b>applicability...</b>|$|R
40|$|The {{present study}} seeks to {{demonstrate}} the use of vibrational thermallyassistedlaserinducedfluorescence of the hydroxyl radical to obtain planartemperature measurements. This technique utilizes a simple two-level vibrational model to describe {{the relationship between the}} population ratio of excited states and temperature. The quenching to vibrational transfer ratio between the excited states was used as a calibration parameter to fit the thermallyassistedfluorescence measurements to known temperature data. The measurements presented here are from a premixed methane–airflame. A rectangular shaped burner allowed for calibration and comparison of the thermallyassistedtemperature results with sodium line reversal data available in literature. Excellent agreement between the two approaches was achieved for three different equivalence ratios. A single calibration was sufficient for the range of conditions tested in the present work. Two detection schemes were also tested, the first using the (0 – 0) and (1 – 0) vibrational bands and the second substituting the (0 – 1) fluorescence in place of the (0 – 0) band. The weakness of the fluorescence signal from the (0 – 1) band was very restrictive to temperatureimaging with <b>the</b> current setup. <b>Overall,</b> <b>the</b> <b>applicability</b> of the thermallyassisted technique to temperatureimaging was positively demonstrated from this work...|$|R
40|$|Much {{has been}} written about the {{advantages}} and disadvantages of high efficiency electric motors. For a given motor application it is possible to find literature that enables a plant engineer to make an informed choice between a standard efficiency and a high efficiency motor; however, few plant engineers have the time to perform a detailed analysis for each motor in their facility. A technique is needed to reduce the analysis to manageable proportions. This paper looks at efforts to identify high efficiency electric motor applications at two manufacturing facilities. It describes a technique that was used to assemble available data in a form that helped prioritize motors in terms of suitability for retrofit with high efficiency models. The technique addresses the problems of limited time and missing data, and suggests ways for quickly filling in data gaps. The motors in the studies spanned a range of 7. 5 to 250 hp. The prioritization was performed primarily on the basis of simple payback. The study results are of potential interest to persons interested in <b>the</b> <b>overall</b> <b>applicability</b> of high efficiency motors in manufacturing...|$|E
40|$|Verifying multi-threaded {{programs}} {{is becoming more}} and more important, because of the strong trend to increase the number of processing units per CPU socket. We introduce a new configurable program analysis for verifying multi-threaded programs with a bounded number of threads. We present a simple and yet efficient implementation as component of the existing program-verification framework CPAchecker. While CPAchecker is already competitive on a large benchmark set of sequential verification tasks, our extension enhances <b>the</b> <b>overall</b> <b>applicability</b> of the framework. Our implementation of handling multiple threads is orthogonal to the abstract domain of the data-flow analysis, and thus, can be combined with several existing analyses in CPAchecker, like value analysis, interval analysis, and BDD analysis. The new analysis is modular and can be used, for example, to verify reachability properties as well as to detect deadlocks in the program. This paper includes an evaluation of the benefit of some optimization steps (e. g., changing the iteration order of the reachability algorithm or applying partial-order reduction) as well as the comparison with other state-of-the-art tools for verifying multi-threaded programs. Comment: In Proceedings MEMICS 2016, arXiv: 1612. 0403...|$|E
40|$|Several {{techniques}} {{have been developed}} for detection and quantification of genetically modified organisms, but quantitative real-time PCR {{is by far the most}} popular approach. Among the most commonly used real-time PCR chemistries are TaqMan probes and SYBR green, but many other detection chemistries have also been developed. Because their performance has never been compared systematically, here we present an extensive evaluation of some promising chemistries: sequence-unspecific DNA labeling dyes (SYBR green), primer-based technologies (AmpliFluor, Plexor, Lux primers), and techniques involving double-labeled probes, comprising hybridization (molecular beacon) and hydrolysis (TaqMan, CPT, LNA, and MGB) probes, based on recently published experimental data. For each of the detection chemistries assays were included targeting selected loci. Real-time PCR chemistries were subsequently compared for their efficiency in PCR amplification and limits of detection and quantification. <b>The</b> <b>overall</b> <b>applicability</b> of the chemistries was evaluated, adding practicability and cost issues to the performance characteristics. None of the chemistries seemed to be significantly better than any other, but certain features favor LNA and MGB technology as good alternatives to TaqMan in quantification assays. SYBR green and molecular beacon assays can perform equally well but may need more optimization prior to use. © 2010 Springer-Verlag. Peer Reviewe...|$|E
40|$|The {{public sector}} in the United States {{continues}} to wrestle with the challenges of complying with legislation such as the Information Technology Reform Act and the Government Perfor mance and Results Act that requires federal agencies to align their programs with <b>the</b> <b>overall</b> mission of <b>the</b> agency and to report their success and failures {{on an annual basis}} to Congress. The results unfortunately have not been very good. The Office of Management and Budget and the General Accounting Office have found in their reviews that the federal government as a whole has not been {{doing a very good job}} aligning their programs with the goals and establishing effective performance measures. The Balanced Scorecard (BSC) offers a methodology that can be applied to the federal sec tor to enable more effective alignment of programs to mission and a tangible means for measur ing the results. Some agencies, particularly within the Department of Defense, have embraced the Balanced Scorecard as a means for achieving these objectives, and have successfully trans formed their programs to achieve higher productivity, aligning with the mission of the agency and resulting in higher efficiencies. This paper discusses <b>the</b> <b>overall</b> effectiveness and <b>applicability</b> of the Balanced Scorecard to the federal sector of the US Government, including a specific real-world application by the De fense Finance and Accounting Services (DFAS). JEL Classification: Lll, L 86, M 31...|$|R
40|$|The {{suitability}} of metallothioneins (MT) in fish as {{biomarker of exposure}} to mercury has been questioned. Therefore, this study aimed at investigating the relationship between external levels of exposure, mercury accumulation and MT content, assessing species and tissue specificities. Two ecologically different fish species – Dicentrarchus labrax and Liza aurata – were surveyed in an estuary historically affected by mercury discharges. Total mercury (T-Hg) and MT content were determined in gills, blood, liver, kidney, muscle and brain. All tissues reflected differences in T-Hg accumulation in both species, although D. labrax accumulated higher levels. Regarding MT, D. labrax revealed a depletion in brain MT content and an incapacity to induce MT synthesis in all the other tissues, whereas L. aurata showed the ability to increase MT in liver and muscle. Tissue-specificities were exhibited in the MT inducing potential and in the susceptibility to MT decrease. L. aurata results presented muscle as the most responsive tissue. None of the investigated tissues displayed significant correlations between T-Hg and MT levels. <b>Overall,</b> <b>the</b> <b>applicability</b> of MT content in fish tissues as biomarker of exposure to mercury was uncertain, reporting limitations in reflecting the metal exposure levels and the subsequent accumulation extent...|$|R
40|$|This {{grounded}} theory driven study explored the predominant categories and concepts involved with perceptions of exercise among school aged children with asthma. Ten children (five males, five females), ages 8 - 12, with various asthma disease severity, were interviewed in their homes. In addition, nine parents completed a health history questionnaire. The emergent {{grounded theory}}: The {{process of creating}} perceptions of exercise was identified from the data. The ongoing creation of perceptions of exercise was influenced by four predominant categories: perceived benefits, striving for normalcy, exercise influences, and asthma's influence. Because process is an ongoing occurrence, the four predominant categories may influence the creation of exercise perceptions simultaneously, or at different times and in various ways dependent upon {{the characteristics of the}} child and their unique situations and experiences (context). Perceived benefits, striving for normalcy, exercise influences, and asthma's influence were identified categories involved with the interactions, actions, and consequences interwoven throughout the creation of perceptions of exercise process. These categories help explain how exercise perceptions are developed from the participants' perspective. The process of creating perceptions of exercise is a continuous, circular, happening with the consequences leading to the development of exercise perceptions. The context may change but <b>the</b> <b>overall</b> process retains <b>applicability</b> to creating perceptions of exercise. The subjective insight gained throughout the development of the theory: the creation of perceptions of exercise, gives light to numerous areas for future nursing research and practice in hopes of improving <b>the</b> <b>overall</b> quality of life among this population...|$|R
40|$|The CarboEurope Regional Experiment Strategy (CERES) was {{designed}} to develop and test a range of methodologies to assess regional surface energy and mass exchange of a large study area in the south-western part of France. This paper describes a methodology to estimate sensible and latent heat fluxes {{on the basis of}} net radiation, surface radiometric temperature measurements and information obtained from available products derived from the Meteosat Second Generation (MSG) geostationary meteorological satellite, weather stations and ground-based eddy covariance towers. It is based on a simplified bulk formulation of sensible heat flux that considers the degree of coupling between the vegetation and the atmosphere and estimates latent heat as the residual term of net radiation. Estimates of regional energy fluxes obtained in this way are validated at the regional scale by means of a comparison with direct flux measurements made by airborne eddy-covariance. The results show an overall good matching between airborne fluxes and estimates of sensible and latent heat flux obtained from radiometric surface temperatures that holds for different weather conditions and different land use types. <b>The</b> <b>overall</b> <b>applicability</b> of the proposed methodology to regional studies is discusse...|$|E
40|$|Within {{the marine}} environment, aerial surveys have {{historically}} centred on apex predators, such as pinnipeds, cetaceans and sea birds. However, {{it is becoming}} increasingly apparent that the utility of this technique may also extend to subsurface species such as pre-spawning fish stocks and aggregations of jellyfish that occur close to the surface. In light of this, we tested the utility of aerial surveys to provide baseline data for 3 poorly understood scyphozoan jellyfish found throughout British and Irish waters: Rhizostoma octopus, Cyanea capillata and Chrysaora hysoscella. Our principal objectives were to develop a simple sampling protocol to identify and quantify surface aggregations, assess their consistency in space and time, and consider <b>the</b> <b>overall</b> <b>applicability</b> of this technique to the study of gelatinous zooplankton. This approach provided a general understanding of range and relative abundance for each target species, with greatest suitability to the study of R. octopus. For this species it was possible to identify and monitor extensive, temporally consistent and previously undocumented aggregations throughout the Irish Sea, an area spanning thousands of square kilometres. This finding has pronounced implications for ecologists and fisheries managers alike and, moreover, draws attention to the broad utility of aerial surveys for the study of gelatinous aggregations beyond the range of conventional ship-based techniques...|$|E
40|$|Approved {{for public}} release; {{distribution}} unlimitedAs the United States transforms from threat-based to capabilities-based combat operations, one must examine {{the ability of}} existing international laws, domestic directives, and Service regulations and training programs to protect American military and civilian prisoners of war, detainees, and hostages while under enemy control. This thesis explores the impact of The National Military Strategy of the United States of America 2004 (NMS) security environment on existing Code of Conduct (CoC) training. A thorough examination and comparison of the existing legal framework to the future components of warfare provides a new context through which to evaluate existing CoC training programs and determine <b>the</b> <b>overall</b> <b>applicability</b> of the course content to the expanded spectrum of captivity. The Department of Defense must compensate {{for the lack of}} effective international protection by designing a conduct-after-capture program that addresses the rapidly changing conditions of different captivity situations. This thesis reveals that the existing CoC training programs and SERE skill sets lack the flexibility to enable the isolated person to rapidly adjust to changes in the future captivity environment and proposes a core captivity curriculum that provides an adaptable set of skills designed to enable the captive to survive and return with honor regardless of the captor or location of captivity. Major, United States Air Forc...|$|E
40|$|This article aims {{to analyze}} and assess the {{applicability}} of widely employed frameworks in cross-cultural management research. First, some criteria are conceptualized and then, eight cultural frameworks are examined and their relevance with respect to defined criteria is determined. At the end, all cultural frameworks are compared, their <b>overall</b> <b>applicability</b> is assessed, and suggestions for empirical research are presented. Results and discussion might be useful not only in applying cultural typologies, but also in improving existing frameworks. Culture, Cross Cultural Management, Cultural Dimension, Cultural Frameworks...|$|R
40|$|Abstract: The {{evapotranspiration}} (ET) {{is one of}} {{the most}} important factor in the hydrological cycle. In this study, remote sensing based ET algorithm using Moderate Resolution Imaging Spectroradiometer (MODIS) was considered. Then, Priestley-Taylor algorithm was used for estimation of potential evapotranspiration in South Korea, and its spatial distribution was analyzed. <b>Overall</b> <b>applicability</b> between estimated potential evapotranspiration and weather station pan evaporation in Nakdong river basin was represented. The results using small pan showed that correlation coefficient in Pohang and Moonkyung Station was 0. 70 and 0. 55, respectively. However, the results using large pa...|$|R
40|$|In {{support of}} the U. S. Army`s efforts to {{determine}} the best technologies for remediation of soils, water, and structures contaminated with pesticides and chemical agents, Argonne National Laboratory has reviewed technologies for treating soils contaminated with mustard, lewisite, sarin, o-ethyl s-(2 - (diisopropylamino) ethyl) methyl-phosphonothioate (VX), and their breakdown products. This report focuses on assessing alternatives to incineration for dealing with these contaminants. For each technology, a brief description is provided, its suitability and constraints on its use are identified, and its <b>overall</b> <b>applicability</b> for treating <b>the</b> agents of concern is summarized. Technologies that merit further investigation are identified...|$|R
40|$|Illegal logging {{is one of}} {{the main}} causes of ongoing {{worldwide}} deforestation and needs to be eradicated. The trade in illegal timber and wood products creates market disadvantages for products from sustainable forestry. Although various measures have been established to counter illegal logging and the subsequent trade, there is a lack of practical mechanisms for identifying the origin of timber and wood products. In this study, six nuclear microsatellites were used to generate DNA fingerprints for a genetic reference database characterising the populations of origin of a large set of mahogany (Swietenia macrophylla King, Meliaceae) samples. For the database, leaves and/or cambium from 1, 971 mahogany trees sampled in 31 stands from Mexico to Bolivia were genotyped. A total of 145 different alleles were found, showing strong genetic differentiation (= 0. 52, = 0. 18, = 0. 65) and clear correlation between genetic and spatial distances among stands (r= 0. 82, P< 0. 05). We used the genetic reference database and Bayesian assignment testing to determine the geographic origins of two sets of mahogany wood samples, based on their multilocus genotypes. In both cases the wood samples were assigned to the correct country of origin. We discuss <b>the</b> <b>overall</b> <b>applicability</b> of this methodology to tropical timber trading...|$|E
40|$|This thesis {{entails the}} {{development}} of concurrent algorithms and specific architectures for the Protocol Data Unit (PDU) encoding/decoding functionality of communication protocol implementations. The main motivation in {{the development of}} these algorithms and architectures is to increase the end-to-end performance of multilayered communication architectures where high computational costs associated with PDU encoding/decoding directly affect the performance. In order to construct the formal basis of this development, the notions of type and value of PDUs are initially defined. Based on the type-value duality and different forms of values, PDU encoding and decoding are divided into phases, which are then identified as mappings between different value forms. Founded on this formalism, concurrent algorithms for PDU encoding/decoding are developed. A distributed implementation model for these algorithms is then conceptualized, where the Communicating Sequential Processes (CSP) notation is used as the basis of formalism to specify the model. Using the distributed implementation model, a taxonomy for different architectural models for PDU encoders/decoders is presented. A customized performance evaluation methodology for measuring the effectiveness of the architectures is proposed. <b>The</b> <b>overall</b> <b>applicability</b> of all the above concepts is demonstrated through the development, implementation, and performance evaluation of an Abstract Syntax Notation-One (ASN. 1) encoder/decoder used in Open System Interconnection (OSI) implementations...|$|E
40|$|Artificial neural {{networks}} {{have proven to}} be an attractive mathematical tool to represent complex relationships in many branches of hydrology. Due to this attractive feature, {{neural networks}} are increasingly being applied in subsurface modeling where intricate physical processes and lack of detailed field data prevail. In this paper, a methodology using modular neural networks (MNN) is proposed to simulate the nitrate concentrations in an agriculture-dominated aquifer. The methodology relies on geographic information system (GIS) tools in the preparation and processing of the MNN input–output data. The basic premise followed in developing the MNN input–output response patterns is to designate the optimal radius of a specified circular-buffered zone centered by the nitrate receptor so that the input parameters at the upgradient areas correlate with nitrate concentrations in ground water. A three-step approach that integrates the on-ground nitrogen loadings, soil nitrogen dynamics, and fate and transport in ground water is described and the critical parameters to predict nitrate concentration using MNN are selected. The sensitivity of MNN performance to different MNN architecture is assessed. The applicability of MNN is considered for the Sumas-Blaine aquifer of Washington State using two scenarios corresponding to current land use practices and a proposed protection alternative. The results of MNN are further analyzed and compared to those obtained from a physically-based fate and transport model to evaluate <b>the</b> <b>overall</b> <b>applicability</b> of MNN...|$|E
40|$|Abstract. The {{purpose of}} this study was to {{evaluate}} <b>the</b> <b>overall</b> sensitivity and <b>applicability</b> of a number of bioassays representing multiple trophic levels, for the preliminary ecotoxicological screening (Tier I) of estuarine sediments. Chemical analyses were conducted on sediments from all sampling sites to assist in interpreting results. As sediment is an inherently complex, heterogeneous geological matrix, the toxicity associated with different exposure routes (solid, porewater and elutriate phases) was also assessed. A stimulatory response was detected following exposure of some sediment phases to both the Microtox and algal bioassays. Of the bioassays and endpoints employed in this study, the algal test was the most responsive to both elutriates and porewaters. Salinity controls, which corresponded to the salinity of the neat porewater samples, were found to have significant effects on the growth of the algae. To our knowledge, this is the first report of the inclusion of a salinity control in algal toxicity tests, the results of which emphasise the importance of incorporating appropriate controls in experimental design. While differential responses were observed, the site characterised as the most polluted on the basis of chemical analysis was consistently ranked the most toxic with all test species and all test phases. In terms of identifying appropriate Tier I screening tests for sediments, this study demonstrated both the Microtox and algal bioassays to be more sensitive than the bacterial enzyme assays and the invertebrate lethality assay employing Artemia salina. The findings of this study highlight that salinity effects and geophysical properties need to be taken into account when interpreting the results of the bioassays...|$|R
40|$|We {{consider}} {{the problem of}} conducting large experimental campaigns in programming languages research. Most research efforts require {{a certain level of}} bookkeeping of results. This is manageable via quick, on-the-fly infrastructure implementations. However, it becomes a problem for large-scale testing initiatives, especially as the needs of the project evolve along the way. We look at how the Collective Knowledge generalized testing framework can help with such a project and its <b>overall</b> <b>applicability</b> and ease of use. The project in question is an OpenCL compiler testing campaign. We investigate how to use the Collective Knowledge framework to lead the experimental campaign, by providing storage and representation of test cases and their results. We also provide an initial implementation, publicly available. Comment: 7 pages, 4 figures, Adapt 2016 workshop (co-located with HiPEAC 2016...|$|R
40|$|Abstract in Undetermined Suitable {{methods for}} testing alleged mediums are still debated after {{a century of}} research. In this study a {{professional}} medium was tested using a double-masked, long distance protocol with seven male sitters who rated how each statement and overall readings applied to them; they also completed a measure of paranormal belief. The experimenters rated the specificity of the statements. Statement specificity was negatively correlated with applicability, whereas paranormal belief {{was positively related to}} <b>overall</b> <b>applicability</b> ratings, but not to sitters’ ratings of their target reading. No sitter rated his target reading as the most applicable and the statistical analysis based on the Pratt and Birge (1948) technique did not support the hypothesis of genuine mediumistic ability. Possible reasons for these results are discussed as are methodological issues in the quantitative assessment of mediumship...|$|R
