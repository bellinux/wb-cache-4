10000|10000|Public
5|$|Sixteen {{isotopes}} of americium {{are known}} with mass numbers from 232 to 248. The most important are 241Am and 243Am, which are alpha-emitters and also emit soft, but intense γ-rays; {{both of them}} can be obtained in an isotopically pure form. Chemical properties of americium were first studied with 241Am, but later shifted to 243Am, which is almost 20 times less radioactive. The disadvantage of 243Am is production of the short-lived daughter isotope 239Np, which has to be considered in <b>the</b> <b>data</b> <b>analysis.</b>|$|E
5|$|X-rays are {{generally}} filtered (by use of X-Ray Filters) {{to a single}} wavelength (made monochromatic) and collimated to a single direction before {{they are allowed to}} strike the crystal. The filtering not only simplifies <b>the</b> <b>data</b> <b>analysis,</b> but also removes radiation that degrades the crystal without contributing useful information. Collimation is done either with a collimator (basically, a long tube) or with a clever arrangement of gently curved mirrors. Mirror systems are preferred for small crystals (under 0.3 mm) or with large unit cells (over 150 Å).|$|E
25|$|Business Intelligence (BI) {{comprises}} the strategies and technologies used by enterprises for <b>the</b> <b>data</b> <b>analysis</b> of business information.|$|E
40|$|With the {{development}} of society, the relational database facing to the great opportunities and challenges, how to store big <b>data,</b> <b>analysis</b> big <b>data</b> is become a hot issue. This article from <b>the</b> traditional <b>data</b> <b>analysis</b> start, find out <b>the</b> traditional <b>data</b> <b>analysis</b> situation and <b>the</b> trend of <b>data</b> <b>analysis.</b> Big <b>data</b> is facing a lot of issues, such as architecture, analysis technical, storage, privacy and security. Due to the method of analysis, the article mainly introduced to <b>the</b> structure <b>data</b> <b>analysis,</b> uncertainty analysis and others. The aim is {{for the study of}} big data prepare. To improve the ability of big <b>data</b> <b>analysis</b> and let <b>the</b> big <b>data</b> service for us...|$|R
5000|$|The most {{important}} distinction between <b>the</b> initial <b>data</b> <b>analysis</b> phase and <b>the</b> main analysis phase, is that during initial <b>data</b> <b>analysis</b> one refrains from any analysis that {{is aimed at}} answering the original research question. <b>The</b> initial <b>data</b> <b>analysis</b> phase is guided by the following four questions: ...|$|R
30|$|WM {{created the}} image {{acquisition}} protocols, acquired images, carried out <b>the</b> <b>data</b> collection and <b>analysis,</b> and drafted manuscript. RM {{assisted in the}} interpretation of <b>the</b> <b>data</b> and <b>the</b> revising of the manuscript. AA assisted {{in the interpretation of}} <b>the</b> <b>data,</b> <b>data</b> <b>analysis</b> of <b>the</b> Cobalt 57 <b>data,</b> and <b>the</b> revising of the manuscript. RH assisted in the revising of the manuscript. TL assisted in <b>the</b> CT <b>data</b> <b>analysis</b> and <b>the</b> revising of the manuscript. All authors participated in the conception of the study and design. All authors read and approved the final manuscript.|$|R
25|$|If a {{data point}} (or points) is {{excluded}} from <b>the</b> <b>data</b> <b>analysis,</b> {{this should be}} clearly stated on any subsequent report.|$|E
25|$|In 2012, {{a planet}} around Alpha Centauri B was announced, but in 2015 a new {{analysis}} {{concluded that it}} almost certainly does not exist and was just a spurious artefact of <b>the</b> <b>data</b> <b>analysis.</b>|$|E
25|$|In December 2010, a claimed {{methodological error}} was reported—by a {{group led by}} Rene Andrae of the Max Planck Institute for Astronomy—in <b>the</b> <b>data</b> <b>analysis</b> {{that led to the}} {{discovery}} of Gliese 581f and g.|$|E
30|$|Two major <b>data</b> <b>analysis</b> methods namely GIS Analysis and Descriptive <b>Analysis</b> were <b>the</b> major employed <b>data</b> <b>analysis.</b>|$|R
30|$|<b>The</b> {{proposed}} <b>data</b> <b>analysis</b> method {{consists of}} three concrete steps.|$|R
40|$|In {{this report}} a {{supplementary}} technique to improve modal results of overview identification is presented. This new technique, called <b>the</b> Emphasized <b>Data</b> <b>Analysis,</b> combines <b>the</b> basic {{idea of the}} traditional phase resonance method with <b>the</b> fast <b>data</b> acquisition andanalyses of multiple input-multiple output techniques (MIMO) in the time domain. To apply <b>the</b> Emphasized <b>Data</b> <b>Analysis,</b> a priori knowledge about the test specimen is required, which usually is provided by a standard MIMO identification, taking into account all available inputs and outputs. While low global accuracy indicators highlight candidate modes for improvement, local accuracy indicators are used for selection of optimal inputs and outputs. The successful application of <b>the</b> Emphasized <b>Data</b> <b>Analysis</b> to <b>the</b> CLUMOD structure is illustrated in this report. Accuracy and stability of modal parameters of six target modes could considerably be improved. In addition, the computation time of <b>the</b> Emphasized <b>Data</b> <b>Analysis</b> is much shorter compared to the standard MIMO approach, {{due to the fact}} that only a subset of all available inputs and outputs is used. <b>The</b> Emphasized <b>Data</b> <b>Analysis</b> is very promising, especially for modal testing with large numbers of inputs and outputs...|$|R
25|$|However, {{the main}} {{limitation}} of electron diffraction in TEM remains the comparatively {{high level of}} user interaction needed. Whereas both the execution of powder X-ray (and neutron) diffraction experiments and <b>the</b> <b>data</b> <b>analysis</b> are highly automated and routinely performed, electron diffraction requires a much higher level of user input.|$|E
25|$|In {{addition}} to managing JPL, Caltech also operates the Palomar Observatory in San Diego County, the Owens Valley Radio Observatory in Bishop, California, the Submillimeter Observatory and W. M. Keck Observatory at the Mauna Kea Observatory, the Laser Interferometer Gravitational-Wave Observatory at Livingston, Louisiana and Richland, Washington, and Kerckhoff Marine Laboratory in Corona del Mar, California. The Institute launched the Kavli Nanoscience Institute at Caltech in 2006, the Keck Institute for Space Studies in 2008, {{and is also}} the current home for the Einstein Papers Project. The Spitzer Science Center (SSC), part of the Infrared Processing and Analysis Center located on the Caltech campus, is <b>the</b> <b>data</b> <b>analysis</b> and community support center for NASA's Spitzer Space Telescope.|$|E
500|$|Despite {{finding that}} some of the forms Barthel had assumed were allographs {{appeared}} instead to be independent glyphs, such as the two orientations of his glyph 27, [...] the overall conflation of allographs and ligatures greatly reduced the size of Barthel's published 600-glyph inventory. [...] By recoding the texts with these findings and then recomparing them, Pozdniakov was able to detect twice as many shared phrases, which enabled him to further consolidate the inventory of glyphs. By 2007, he and his father, a pioneer in Russian computer science, had concluded that 52 glyphs accounted for 99.7% of the corpus. From this he deduced that rongorongo is essentially a syllabary, though mixed with non-syllabic elements, possibly determinatives or logographs for common words (see below). <b>The</b> <b>data</b> <b>analysis,</b> however, has not been published.|$|E
30|$|Organize {{and prepare}} <b>the</b> <b>data</b> for <b>analysis.</b>|$|R
5000|$|Several {{analyses}} can be {{used during}} <b>the</b> initial <b>data</b> <b>analysis</b> phase: ...|$|R
5000|$|... dropout (this {{should be}} {{identified}} during <b>the</b> initial <b>data</b> <b>analysis</b> phase) ...|$|R
2500|$|There are two {{applications}} for machine learning and data mining: data management and data analysis. [...] Statistics tools {{are necessary for}} <b>the</b> <b>data</b> <b>analysis.</b>|$|E
2500|$|<b>The</b> <b>Data</b> <b>Analysis</b> Core: offers Turnkey {{service to}} analyze -omics {{and other data}} from raw data in to {{manuscript}} ready figures and text out. It also provides [...] Nexgen sequence assembly and annotation; microarray design, analysis and interpretation; mass spec data analysis; data QC; hypothesis generation; experimental design; statistical data analysis ...|$|E
2500|$|Five {{years after}} the Munich study was published, Jim T. Enright, a {{professor}} of physiology who emphasised correct data analysis procedure, contended that the study's results are merely consistent with statistical fluctuations and not significant. He believed the experiments provided [...] "the most convincing disproof imaginable that dowsers can do what they claim", stating that <b>the</b> <b>data</b> <b>analysis</b> was [...] "special, unconventional and customized". Replacing it with [...] "more ordinary analyses", {{he noted that the}} best dowser was on average [...] out of [...] closer to a mid-line guess, an advantage of 0.04%, and that the five other [...] "good" [...] dowsers were on average farther than a mid-line guess. Enright emphasized that the experimenters should have decided beforehand how to statistically analyze the results; if they only afterward chose the statistical analysis that showed the greatest success, then their conclusions would not be valid until replicated by another test analyzed by the same method. He further pointed out that the six [...] "good" [...] dowsers did not perform any better than chance in separate tests. Another study published in Pathophysiology hypothesized that such experiments as this one that were carried out in the 20th century could have been interfered with by man-made radio frequency radiation, as test subjects' bodies absorbed the radio waves and unconscious hand movement reactions took place following the standing waves or intensity variations.|$|E
5000|$|In 2008, it {{purchased}} Insightful Corporation, including <b>the</b> S-PLUS <b>data</b> <b>analysis</b> programming language.|$|R
3000|$|... {{from the}} Earth in the sunward direction. <b>The</b> <b>data</b> from <b>the</b> {{satellite}} {{was taken from}} <b>the</b> Coordinated <b>Data</b> <b>Analysis</b> (CDA) web.|$|R
50|$|<b>The</b> Climate <b>Data</b> <b>Analysis</b> Tool (CDAT) is {{plotting}} software used in {{atmospheric sciences}} and climatology.|$|R
50|$|Datacopia {{attempts}} {{to resolve these}} difficulties by automating both <b>the</b> <b>data</b> <b>analysis</b> and chart selection processes.|$|E
5000|$|Establishment of <b>the</b> <b>Data</b> <b>Analysis</b> and Coordination Center (DACC), {{which serves}} as the central {{repository}} for all HMP data.|$|E
50|$|If a {{data point}} (or points) is {{excluded}} from <b>the</b> <b>data</b> <b>analysis,</b> {{this should be}} clearly stated on any subsequent report.|$|E
3000|$|... ● Each virtual service cell is {{equipped}} with basic service functionalities suitable for <b>the</b> microarray <b>data</b> <b>analysis.</b>|$|R
5000|$|Item nonresponse (whether this is random or not {{should be}} {{assessed}} during <b>the</b> initial <b>data</b> <b>analysis</b> phase) ...|$|R
40|$|In {{this paper}} a {{supplementary}} technique to improve modal results is presented, called <b>the</b> Emphasized <b>Data</b> <b>Analysis.</b> A priori {{knowledge of the}} test specimen is required, which usually is provided by a standard multiple input-multiple output identification, taking into account all available inputs and outputs. While low global accuracy indicators highlight candidate modes for improvement, local accuracy indicators are used for selection of optimal inputs and outputs. The successful application of <b>the</b> Emphasized <b>Data</b> <b>Analysis</b> to <b>the</b> CLUMOD structure is illustrated in this report. Accuracy and stability of modal parameters of six target modes could be improved considerably. In addition, the computation time of <b>the</b> Emphasized <b>Data</b> <b>Analysis</b> is much shorter compared to the standard MIMO approach, {{due to the fact}} that <b>the</b> <b>data</b> input matrices are drastically reduced...|$|R
50|$|Data {{from both}} {{measurement}} facilities underwent necessary processing at the Riverton, New Jersey, data reduction center, {{a part of}} <b>the</b> <b>Data</b> <b>Analysis</b> Laboratory.|$|E
50|$|There are two {{applications}} for machine learning and data mining: data management and data analysis. Statistics tools {{are necessary for}} <b>the</b> <b>data</b> <b>analysis.</b>|$|E
50|$|Between May 1979 and October 1988, Sach {{served in}} the Budget Division as a Budget Officer. Later on, he became Chief of <b>the</b> <b>Data</b> <b>Analysis</b> and Systems Control Unit.|$|E
40|$|The {{purpose of}} the study was to develop a {{substantive}} grounded theory on the inter-profession career transition phenomenon as experienced by eight transitioning individuals. With an interpretive qualitative approach, data was collected through memoirs, interviews and literature. <b>The</b> Straussarian <b>data</b> <b>analysis</b> entailed open coding, axial coding, and selective coding. Each new participant analysed was chosen following the theoretical sampling technique. <b>The</b> qualitative <b>data</b> <b>analysis</b> software programme called ATLAS. ti was utilized to store all <b>the</b> <b>data</b> collected. Five categories were identified through th...|$|R
3000|$|... {{compensated}} for by employing combined analyses of both <b>the</b> tsunami <b>data</b> <b>analysis</b> and <b>the</b> operational seismic/geodetic coseismic source estimation.|$|R
5000|$|<b>The</b> Resource <b>Data</b> <b>Analysis</b> Branch (RDAB) is {{mandated}} to conduct land use assessment/evaluation and land classification. Its functions are: ...|$|R
