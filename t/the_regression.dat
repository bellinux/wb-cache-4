10000|10000|Public
5|$|A severe {{regression}} {{would have}} greatly reduced the continental shelf area, {{which is the}} most species-rich part of the sea, and therefore could have been enough to cause a marine mass extinction, however, research concludes that this change would have been insufficient to cause the observed level of ammonite extinction. <b>The</b> <b>regression</b> would also have caused climate changes, partly by disrupting winds and ocean currents and partly by reducing the Earth's albedo and therefore, increasing global temperatures.|$|E
5|$|In {{especially}} {{severe cases}} of OFC, parathyroidectomy, or the full {{removal of the}} parathyroid glands, is the chosen route of treatment. Parathyroidectomy {{has been shown to}} result in the reversal of bone resorption and the complete regression of brown tumors. In situations where parathyroid carcinoma is present, surgery to remove the tumors has also led to <b>the</b> <b>regression</b> of hyperparathyroidism as well as the symptoms of OFC.|$|E
5|$|Most of {{the fossils}} {{discovered}} in the Mussentuchit are scattered and disarticulated, with intact skeletons being relatively rare. Although the lakes did not actively destroy bones, it was not particularly conducive to good preservation either due to the long duration of the burial process. Before burial, scavengers could have disrupted the skeletal material, which {{is evident in the}} severed Eolambia tibiae bearing tooth marks {{discovered in the}} Cifelli #2 quarry. <b>The</b> <b>regression</b> of the lake could have occasionally created bogs, but they would not have been permanent enough to entrap and preserve larger vertebrates. Action by water currents seems to {{have played a role in}} the deposition of Eolambia bones in the Cifelli #2 quarry, where the bones are largely deposited at 28°, 69°, 93°, 131° and 161° azimuth (i.e. relative to north). These directions would have been parallel or sub-parallel to the edges of the lake, indicating orientation by lake currents, except for the 69° and 93° azimuths, which probably represent river currents.|$|E
30|$|In <b>the</b> <b>regressions,</b> <b>the</b> omitted {{variable}} is crop 2.|$|R
3000|$|... 26 In {{addition}} to the continuous time analysis, I run <b>the</b> <b>regressions</b> based on discrete time analysis and use <b>the</b> complementary log-log <b>regression</b> (<b>the</b> discrete-time proportional hazards model) to compare if results are similar: <b>the</b> cloglog <b>regressions</b> return qualitatively comparable results, where the UISA variable is statistically significant at a 1 percent level throughout <b>the</b> <b>regressions</b> and equally increases the hazard of leaving employment. Coefficients are quantitatively above the results of continuous time analysis, the difference is however minor.|$|R
30|$|In <b>the</b> wage <b>regressions,</b> <b>the</b> same {{explanatory}} {{variables as}} in <b>the</b> layoff <b>regression</b> are used with two exceptions. First, {{the interaction between}} non-union status and a proxy of p in <b>the</b> wage <b>regressions</b> is included. Second, the CPI used in <b>the</b> wage <b>regressions</b> is <b>the</b> annual CPI (by industry) rather than the CPI during January used in Table 6.|$|R
25|$|R2 is a {{statistic}} {{that will give}} some information about the goodness of fit of a model. In regression, the R2 coefficient of determination is {{a statistic}}al measure of how well <b>the</b> <b>regression</b> line approximates the real data points. An R2 of 1 indicates that <b>the</b> <b>regression</b> line perfectly fits the data.|$|E
25|$|SPSS: Included as {{an option}} in <b>the</b> <b>Regression</b> function.|$|E
25|$|Total sum of squares, model sum of squared, and {{residual}} sum of squares tell us {{how much}} of the initial variation in the sample were explained by <b>the</b> <b>regression.</b>|$|E
50|$|Adapted from <b>the</b> <b>Regressions</b> liner notes.|$|R
3000|$|... 11 In <b>the</b> <b>regressions,</b> <b>the</b> LEP {{percentages}} are lagged, {{in keeping}} with other peer variables. However, for explanatory purposes, this example uses current LEP percentages.|$|R
30|$|In a final {{auxiliary}} analysis, <b>the</b> <b>regressions</b> from <b>the</b> main {{analysis were}} run using {{the outcome of}} whether individuals voted, rather than whether they volunteered. In this case, the immigrant location selection correction was the 1990 average voting in the community. Since the 1990 sample used for the voting selection correction did not suffer from {{the same sort of}} sample size issues as the 1989 supplements, <b>the</b> linearized <b>regression</b> was not employed for <b>the</b> voting <b>regressions.</b> There was only one “type” of voting, however, it was disaggregated by city size as well as employing or not employing various area level characteristics in <b>the</b> <b>regressions.</b>|$|R
25|$|The {{critical}} values, dL,α and dU,α, vary by {{level of}} significance (α), the number of observations, {{and the number of}} predictors in <b>the</b> <b>regression</b> equation. Their derivation is complex—statisticians typically obtain them from the appendices of statistical texts.|$|E
25|$|Another way {{of looking}} at it is to {{consider}} <b>the</b> <b>regression</b> line to be a weighted average of the lines passing through the combination of any two points in the dataset. Although this way of calculation is more computationally expensive, it provides a better intuition on OLS.|$|E
25|$|When this {{assumption}} is violated the regressors are called linearly dependent or perfectly multicollinear. In such case {{the value of}} <b>the</b> <b>regression</b> coefficient β cannot be learned, although prediction of y values is still possible for new values of the regressors that lie in the same linearly dependent subspace.|$|E
40|$|An {{experiment}} {{was conducted to}} investigate <b>the</b> offspring-parent <b>regression</b> for three quantitative traits (weight, abdominal bristles and wing length) in Drosophila melanogaster. Linear and polynomial models were fitted for <b>the</b> <b>regressions</b> of a character in offspring on both parents. It is demonstrated that responses by the characters to selection predicted by <b>the</b> nonlinear <b>regressions</b> may differ substantially from those predicted by <b>the</b> linear <b>regressions.</b> This is true even, and especially, if selection is weak. The realized heritability for a character under selection is shown to be determined not only by <b>the</b> offspring-parent <b>regression</b> {{but also by the}} distribution of the character and by the form and strength of selection...|$|R
3000|$|... 16 <b>The</b> {{separate}} <b>regressions</b> {{also indicate}} gender-related wage discrimination in both sectors in Syria. Further, <b>the</b> <b>regressions</b> for Lebanon show that age {{is an important}} factor in determining wages in the formal sector but not the informal one.|$|R
30|$|Including {{interaction}} terms {{between gender}} and education in <b>the</b> <b>regressions</b> do not change this result.|$|R
25|$|The theorem {{can be used}} to {{establish}} a number of theoretical results. For example, having a regression with a constant and another regressor is equivalent to subtracting the means from the dependent variable and the regressor and then running <b>the</b> <b>regression</b> for the demeaned variables but without the constant term.|$|E
25|$|A {{method to}} {{compensate}} for both sources of inaccuracy above is to establish the relative risks by multivariate regression analysis. However, to retain its validity, relative risks established as such must be multiplied {{with all the other}} risk factors in the same regression analysis, and without any addition of other factors beyond <b>the</b> <b>regression</b> analysis.|$|E
25|$|R-squared is the {{coefficient}} of determination indicating goodness-of-fit of <b>the</b> <b>regression.</b> This statistic will be equal to one if fit is perfect, and to zero when regressors X have no explanatory power whatsoever. This is a biased estimate of the population R-squared, and will never decrease if additional regressors are added, {{even if they are}} irrelevant.|$|E
30|$|Once {{the regions}} were {{selected}} based on PTAs, the PTA depth was dropped from <b>the</b> <b>regressions.</b>|$|R
3000|$|... 9 {{includes}} {{summary statistics}} from <b>the</b> first-stage <b>regressions</b> that {{are useful for}} identifying weak instruments. The R-squared and the adjusted R-squared are only 0.09 and 0.08 in <b>the</b> IV <b>regressions</b> for men and women. This result suggests a loss of precision associated with the IV estimation, which is expected. The partial R-squared is only 0.02 in <b>the</b> <b>regressions</b> for men and 0.03 in <b>the</b> <b>regressions</b> for women. <b>The</b> F-statistic values are 23.92 and 30.10 for the full sample (Panel A) {{of men and women}} and 11.2 and 15.07 for the ‘conflict region’ sub-sample (Panel B) which is greater than the rule-of-thumb value of 10. The test of Stock and Yogo ([...] [...]...|$|R
3000|$|... 13 Descriptive {{statistics}} for variables used in <b>the</b> <b>regressions</b> for Lebanon and Syria are available upon request.|$|R
25|$|While the δD of {{source water}} is the biggest {{influence}} on the δD of lipids, discrepancies between fractionation factor values obtained from the slope and from the intercept of <b>the</b> <b>regression</b> suggest that the relationship {{is more complex than}} a two-pool fractionation. In other words, there are multiple fractionation steps that {{must be taken into account}} in understanding the isotopic composition of lipids.|$|E
25|$|Another {{variation}} of <b>the</b> <b>regression</b> hypothesis {{is the best}} learned-last-forgotten hypothesis, which emphasizes the intensity {{and quality of the}} acquired knowledge, not the order in which it is learned. Therefore, the better something is learned, the longer it will remain. Because the language component is repeated again and again, it becomes automated and increases the probability that it will last in the memory (Schöpper-Grabe 1998: 241).|$|E
25|$|The numerator, n−p, is the {{statistical}} degrees of freedom. The first quantity, s2, is the OLS estimate for σ2, whereas the second, , is the MLE estimate for σ2. The two estimators are quite similar in large samples; {{the first one}} is always unbiased, while the second is biased but minimizes the mean squared error of the estimator. In practice s2 is used more often, since it is more convenient for the hypothesis testing. The square root of s2 is called the standard error of <b>the</b> <b>regression</b> (SER), or standard error of the equation (SEE).|$|E
30|$|<b>The</b> first-stage <b>regression</b> is {{estimated}} by OLS and corrects for possible heteroscedasticity and serial correlation by using Newey-West standard errors with the maximum lag order set equal to T 1 / 2, i.e. the square {{root of the}} sample size (16 months for the full sample, 10 {{for the first and}} 12 for the second subsample). <b>The</b> zero-stage <b>regression</b> and second-stage regression are estimated jointly by GMM which allows the standard errors of <b>the</b> second-stage <b>regression</b> to incorporate not only the uncertainty deriving from <b>the</b> first-stage <b>regression</b> (as is common IV setups), but also the one from <b>the</b> zero-stage <b>regression.</b> 8 Standard errors are based on the Newey-West estimate of the covariance matrix, with the maximum number of lags corresponding to the values of <b>the</b> first-stage <b>regression.</b>|$|R
30|$|The {{full set}} of {{coefficient}} estimates for <b>the</b> <b>regressions</b> in Table  4 {{can be found in}} Appendix Table 6.|$|R
40|$|This paper studies {{what happens}} when we move from a short {{regression}} to a long regression (or vice versa), when <b>the</b> long <b>regression</b> is shorter than the data-generation process. In the special case where <b>the</b> long <b>regression</b> equals <b>the</b> data-generation process, the least-squares estimators have smaller bias (in fact zero bias) but larger variances in <b>the</b> long <b>regression</b> than in <b>the</b> short <b>regression.</b> But if <b>the</b> long <b>regression</b> is also misspecified, the bias may not be smaller. We provide bias and mean squared error comparisons and study the dependence of the differences on the misspecification parameter...|$|R
25|$|Regression {{analysis}} {{and in particular}} ordinary least squares specifies that a dependent variable depends according to some function upon one or more independent variables, with an additive error term. Various types of statistical inference on <b>the</b> <b>regression</b> assume that the error term is normally distributed. This assumption can be justified by assuming that the error term is actually the sum {{of a large number}} of independent error terms; even if the individual error terms are not normally distributed, by the central limit theorem their sum can be well approximated by a normal distribution.|$|E
25|$|Two {{hypothesis}} {{tests are}} particularly widely used. First, {{one wants to}} know if the estimated regression equation is any better than simply predicting that all values of the response variable equal its sample mean (if not, it is said to have no explanatory power). The null hypothesis of no explanatory value of the estimated regression is tested using an F-test. If the calculated F-value is found to be large enough to exceed its critical value for the pre-chosen level of significance, the null hypothesis is rejected and the alternative hypothesis, that <b>the</b> <b>regression</b> has explanatory power, is accepted. Otherwise, the null hypothesis of no explanatory power is accepted.|$|E
25|$|In statistics, {{ordinary}} {{least squares}} (OLS) or linear least squares is a method for estimating the unknown parameters in a linear regression model, {{with the goal of}} minimizing the sum of the squares {{of the differences between the}} observed responses (values of the variable being predicted) in the given dataset and those predicted by a linear function of a set of explanatory variables. Visually this is seen as the sum of the squared vertical distances between each data point in the set and the corresponding point on <b>the</b> <b>regression</b> line – the smaller the differences, the better the model fits the data. The resulting estimator can be expressed by a simple formula, especially in the case of a single regressor on the right-hand side.|$|E
40|$|<b>The</b> reverse <b>regression</b> {{method of}} {{measuring}} wage discrimination {{is the main}} challenge to <b>the</b> dominant direct <b>regression</b> method based on the Oaxaca/Blinder approach. In this article, {{it is argued that}} the choice between the two methods is fundamentally a choice of assumptions regarding the nature of the wage determination process and the nature of <b>the</b> unexplained <b>regression</b> residual of <b>the</b> wage <b>regression</b> equation. In particular, this article concludes that <b>the</b> reverse <b>regression</b> method is more likely to produce the correct wage discrimination measure if any of the following three assumptions is correct: (a) qualifications do not determine how much individuals earn (as <b>the</b> direct <b>regression</b> method assumes) but, instead, determine which candidates are selected for existing jobs with fixed wages; (b) errors in the measurement of qualifications are larger than errors in the measurement of wages, in which case <b>the</b> direct <b>regression</b> method would understate the importance of differences in qualifications; and (c) differences in unobserved qualifications (e. g., importance of job flexibility; relevance of past work experience) between two groups are not zero (as <b>the</b> direct <b>regression</b> method assumes) but tend to favour the group with the better observed qualifications. Finally, this article shows that application of <b>the</b> reverse <b>regression</b> technique simply requires the augmentation of the qualification component of <b>the</b> direct <b>regression</b> method by dividing it by the R 2 coefficient. ...|$|R
30|$|In general, {{statistical}} significance of <b>the</b> <b>regressions</b> of females is {{low compared to}} that of males, but the signs coincide.|$|R
30|$|Table A 1 {{lists the}} {{variables}} in the dataset used to carry out <b>the</b> <b>regressions.</b> All data are from Eurostat.|$|R
