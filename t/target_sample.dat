579|1805|Public
5|$|The {{choice of}} method often depends largely on what the {{researcher}} intends to investigate. For example, a researcher concerned with drawing a statistical generalization across an entire population may administer a survey questionnaire to a representative sample population. By contrast, a researcher who seeks full contextual understanding of an individual's social actions may choose ethnographic participant observation or open-ended interviews. Studies will commonly combine, or 'triangulate', quantitative and qualitative methods {{as part of a}} 'multi-strategy' design. For instance, a quantitative study may be performed to gain statistical patterns or a <b>target</b> <b>sample,</b> and then combined with a qualitative interview to determine the play of agency.|$|E
2500|$|The core {{principle}} behind microarrays is hybridization {{between two}} DNA strands, {{the property of}} complementary nucleic acid sequences to specifically pair with each other by forming hydrogen bonds between complementary nucleotide base pairs. A high number of complementary base pairs in a nucleotide sequence means tighter non-covalent bonding between the two strands. After washing off non-specific bonding sequences, only strongly paired strands will remain hybridized. Fluorescently labeled target sequences that bind to a probe sequence generate a signal {{that depends on the}} hybridization conditions (such as temperature), and washing after hybridization. Total strength of the signal, from a spot (feature), depends upon the amount of <b>target</b> <b>sample</b> binding to the probes present on that spot. Microarrays use relative quantitation in which the intensity of a feature is compared to the intensity of the same feature under a different condition, [...] and the identity of the feature is known by its position.|$|E
5000|$|... #Caption: In {{the sequel}} Q*bert's Qubes, the player must rotate cubes {{in a line}} to match the <b>target</b> <b>sample</b> in the top left corner.|$|E
30|$|For all the <b>target</b> <b>samples</b> {{we found}} that {{divergence}} from nadir-like measurement becomes significant for emergence angles θ ≥  40 °.|$|R
3000|$|... where X̆^*(n)_k+ 1 and X̆^*'(n)_k+ 1 are the {{selected}} <b>target</b> <b>samples</b> {{for the next}} iteration (k+ 1) from subsets 2 and 3, respectively.|$|R
50|$|Certain {{kinds of}} A/D {{converters}} known as delta-sigma converters produce disproportionately more quantization {{noise in the}} upper portion of their output spectrum. By running these converters at some multiple of the <b>target</b> <b>sampling</b> rate, and low-pass filtering the oversampled signal down to half the <b>target</b> <b>sampling</b> rate, a final result with less noise (over the entire band of the converter) can be obtained. Delta-sigma converters use a technique called noise shaping to move the quantization noise to the higher frequencies.|$|R
5000|$|The {{objective}} is to reweight the source labeled sample such that it [...] "looks like" [...] the <b>target</b> <b>sample</b> (in term of the error measure considered) ...|$|E
50|$|Once the {{ion beam}} has ionized <b>target</b> <b>sample</b> atoms, the sample ions are recoiled toward the detector. The beam ions are {{scattered}} {{at an angle}} that does not permit them to reach the detector. The sample ions pass through an entrance window of the detector, and {{depending on the type}} of detector used, the signal is converted into a spectrum.|$|E
5000|$|The Fundamental aspects {{in dealing}} with recoil {{spectroscopy}} involves electron back scattering process of matter such as thin films and solid materials. Energy loss of particles in target materials is evaluated by assuming that the <b>target</b> <b>sample</b> is laterally uniform and constituted by a mono isotopic element. This allows a simple relationship between that of penetration depth profile and elastic scattering yield ...|$|E
40|$|Background:  Full axon {{counting}} of {{optic nerve}} cross-sections represents {{the most accurate}} method to quantify axonal damage, but such analysis is very labour intensive. Recently, a new method has been developed, termed <b>targeted</b> <b>sampling,</b> which combines the salient features of a grading scheme with axon counting. Preliminary findings revealed the method compared favourably with random sampling. The aim {{of the current study}} was to advance our understanding of the effect of sampling patterns on axon counts by comparing estimated axon counts from <b>targeted</b> <b>sampling</b> with those obtained from fixed-pattern sampling in a large collection of optic nerves with different severities of axonal injury. Methods:  Chronic ocular hypertension was induced in adult Sprague-Dawley rats for 1 – 7 weeks by translimbal laser photocoagulation of the trabecular meshwork. Axonal damage on resin-embedded cross-sections was estimated using three different methods: (i) semi-quantitative damage grading; (ii) semi-quantitative, automated axon counting using targeted sampling; and (iii) semi-quantitative, automated axon counting using fixed-pattern sampling. Results:  Estimated axon counts, as generated by <b>targeted</b> <b>sampling</b> and fixed-pattern sampling, correlated equally well with the semi-quantitative grading scheme. Estimated counts obtained with <b>targeted</b> <b>sampling</b> were not statistically different from those yielded by fixed-pattern sampling. Bland–Altman analysis showed a good agreement between the two methods. Conclusions:  The results of our study validate the use of both fixed-pattern <b>sampling</b> and <b>targeted</b> <b>sampling</b> for estimation of axonal damage but do not indicate that the latter method is superior for detection of axon loss in animals with minor damage. Andreas Ebneter, Robert J Casson, John PM Wood and Glyn Chidlo...|$|R
30|$|Mao 2015 [19]: A {{detector}} {{was trained}} on <b>target</b> <b>samples</b> labeled automatically by using tracklets and by information propagation from labeled tracklets to uncertain ones.|$|R
40|$|Those {{who engage}} in illegal or stigmatized behaviors, which put them at risk of HIV infection, are largely {{concentrated}} in urban centers. Owing to their illegal and/ or stigmatized behaviors, they are difficult to reach with public health surveillance and prevention programs. 1 These populations include illicit drug users, sex workers and {{men who have sex}} with men. Development and implementation of adequate prevention services targeting hidden populations requires data on risk behaviors and disease prevalence from non-biased samples. In the last two decades, a number of sampling methods have been used to collect risk behavior and disease prevalence data from highly at-risk populations and to direct survey participants to prevention services. These include venue-based time–space <b>sampling,</b> <b>targeted</b> <b>sampling,</b> and snowball sampling. Time–space (also called time–location or venue–day–time) and <b>targeted</b> <b>sampling</b> provide coverage limited to population members who are readily accessible; those who are missed may differ from those who are Bcaptured[. 2 <b>Targeted</b> <b>sampling</b> fares well when compared to other forms of convenienc...|$|R
5000|$|... #Caption: Schematic of {{a typical}} dynamic SIMS instrument. High energy (usually several keV) ions are {{supplied}} by an ion gun (1 or 2) and focused on to the <b>target</b> <b>sample</b> (3), which ionizes and sputters some atoms off the surface (4). These secondary ions are then collected by ion lenses (5) and filtered according to atomic mass (6), then projected onto an electron multiplier (7, top), Faraday cup (7, bottom), or CCD screen (8).|$|E
50|$|The Yutu rover has {{a mass of}} 140 kg, with a payload {{capacity}} of 20 kg. It is smaller than the Mars Exploration Rovers, Spirit and Opportunity, and carries similar instruments: panoramic cameras, an infrared spectrometer and an alpha particle X-ray spectrometer (APXS). Yutu is also equipped with a robotic arm to position its APXS near a <b>target</b> <b>sample.</b> In addition, the rover could transmit live video, and has automatic sensors {{to prevent it from}} colliding with other objects.|$|E
50|$|Another {{application}} of sputtering is to etch away the target material. One such example occurs in secondary ion mass spectrometry (SIMS), where the <b>target</b> <b>sample</b> is sputtered {{at a constant}} rate. As the target is sputtered, the concentration and identity of sputtered atoms are measured using Mass Spectrometry. In this way {{the composition of the}} target material can be determined and even extremely low concentrations (20 µg/kg) of impurities detected. Furthermore, because the sputtering continually etches deeper into the sample, concentration profiles as a function of depth can be measured.|$|E
40|$|Deep-layered models {{trained on}} {{a large number of}} labeled samples boost the {{accuracy}} of many tasks. It is important to apply such models to different domains because collecting many labeled samples in various domains is expensive. In unsupervised domain adaptation, one needs to train a classifier that works well on a target domain when provided with labeled source <b>samples</b> and unlabeled <b>target</b> <b>samples.</b> Although many methods aim to match the distributions of source and <b>target</b> <b>samples,</b> simply matching the distribution cannot ensure accuracy on the target domain. To learn discriminative representations for the target domain, we assume that artificially labeling <b>target</b> <b>samples</b> can result in a good representation. Tri-training leverages three classifiers equally to give pseudo-labels to unlabeled samples, but the method does not assume labeling samples generated from a different domain. In this paper, we propose an asymmetric tri-training method for unsupervised domain adaptation, where we assign pseudo-labels to unlabeled samples and train neural networks as if they are true labels. In our work, we use three networks asymmetrically. By asymmetric, we mean that two networks are used to label unlabeled <b>target</b> <b>samples</b> and one network is trained by the samples to obtain target-discriminative representations. We evaluate our method on digit recognition and sentiment analysis datasets. Our proposed method achieves state-of-the-art performance on the benchmark digit recognition datasets of domain adaptation. Comment: TBA on ICML 201...|$|R
40|$|Wildlife {{biologists}} {{often use}} grid-based designs to sample animals and generate abundance estimates. Although sampling in grids is theoretically sound, in application, the method can be logistically difficult and expensive when sampling elusive species inhabiting extensive areas. These factors make it challenging to sample animals {{and meet the}} statistical assumption of all individuals having an equal probability of capture. Violating this assumption biases results. Does an alternative exist? Perhaps by sampling only where resources attract animals (i. e., <b>targeted</b> <b>sampling),</b> it would provide accurate abundance estimates more efficiently and affordably. However, biases from this approach would also arise if individuals have an unequal probability of capture, especially if some failed to visit the sampling area. Since most biological programs are resource limited, and acquiring abundance data drives many conservation and management applications, it becomes imperative to identify economical and informative sampling designs. Therefore, we evaluated abundance estimates generated from grid and <b>targeted</b> <b>sampling</b> designs using simulations based on geographic positioning system (GPS) data from 42 Alaskan brown bears (Ursus arctos). Migratory salmon drew brown bears from the wider landscape, concentrating them at anadromous streams. This provided a scenario for testing the targeted approach. Grid and <b>targeted</b> <b>sampling</b> varied by trap amount, location (traps placed randomly, systematically or by expert opinion), and traps stationary or moved between capture sessions. We began by identifying when to sample, and if bears had equal probability of capture. We compared abundance estimates against seven criteria: bias, precision, accuracy, effort, plus encounter rates, and probabilities of capture and recapture. One grid (49 km 2 cells) and one targeted configuration provided the most accurate results. Both placed traps by expert opinion and moved traps between capture sessions, which raised capture probabilities. The grid design was least biased (− 10. 5 %), but imprecise (CV 21. 2 %), and used most effort (16, 100 trap-nights). The targeted configuration was more biased (− 17. 3 %), but most precise (CV 12. 3 %), with least effort (7, 000 trap-nights). <b>Targeted</b> <b>sampling</b> generated encounter rates four times higher, and capture and recapture probabilities 11 % and 60 % higher than grid sampling, in a sampling frame 88 % smaller. Bears had unequal probability of capture with both sampling designs, partly because some bears never had traps available to sample them. Hence, grid and <b>targeted</b> <b>sampling</b> generated abundance indices, not estimates. Overall, <b>targeted</b> <b>sampling</b> provided the most accurate and affordable design to index abundance. <b>Targeted</b> <b>sampling</b> may offer an alternative method to index the abundance of other species inhabiting expansive and inaccessible landscapes elsewhere, provided their attraction to resource concentrations...|$|R
3000|$|Even this {{strategy}} {{was not sufficient}} to attain the <b>targeted</b> <b>sample</b> size within the expected calendar, so eventually spatial sampling and random routes were also used to complete the sample (see [...]...|$|R
50|$|The main {{principle}} of SALDI {{relies on a}} medium that absorbs energy from a laser and then transfers the energy to the <b>target</b> <b>sample.</b> This class of techniques where the bulk of energy goes to the substrate instead of the sample molecules is known as soft ionization techniques. The development of SALDI started as a modification of matrix-assisted laser desorption/ionization (MALDI). The former technique suffered from ionization interference from the matrix molecules of MALDI. SALDI substituted an active surface of specific substrates, usually made of inorganic components, for the organic matrix of MALDI.|$|E
50|$|Electron {{tomography}} (ET) is a tomography {{technique for}} obtaining detailed 3D structures of sub-cellular macro-molecular objects. Electron tomography {{is an extension}} of traditional transmission electron microscopy and uses a transmission electron microscope to collect the data. In the process, a beam of electrons is passed through the sample at incremental degrees of rotation around the center of the <b>target</b> <b>sample.</b> This information is collected and used to assemble a three-dimensional image of the target. For biological applications, the typical resolution of ET systems are in the 5-20 nm range, suitable for examining supra-molecular multi-protein structures, although not the secondary and tertiary structure of an individual protein or polypeptide.|$|E
50|$|The {{choice of}} method often depends largely on what the {{researcher}} intends to investigate. For example, a researcher concerned with drawing a statistical generalization across an entire population may administer a survey questionnaire to a representative sample population. By contrast, a researcher who seeks full contextual understanding of an individual's social actions may choose ethnographic participant observation or open-ended interviews. Studies will commonly combine, or 'triangulate', quantitative and qualitative methods {{as part of a}} 'multi-strategy' design. For instance, a quantitative study may be performed to gain statistical patterns or a <b>target</b> <b>sample,</b> and then combined with a qualitative interview to determine the play of agency.|$|E
40|$|Full axon {{counting}} of {{optic nerve}} cross-sections represents {{the most accurate}} method to quantify axonal damage, but such analysis is very labour intensive. Recently, a new method has been developed, termed <b>targeted</b> <b>sampling,</b> which combines the salient features of a grading scheme with axon counting. Preliminary findings revealed the method compared favourably with random sampling. The aim {{of the current study}} was to advance our understanding of the effect of sampling patterns on axon counts by comparing estimated axon counts from <b>targeted</b> <b>sampling</b> with those obtained from fixed-pattern sampling in a large collection of optic nerves with different severities of axonal injury...|$|R
40|$|The {{analysis}} of concentrations of {{persistent organic pollutants}} (POPs) in ambient air is costly and can only be done for {{a limited number of}} samples. It is thus beneficial to maximize the information content of the samples analyzed via a targeted observation strategy. Using polychlorinated biphenyls (PCBs) as an example, a forecasting system to predict and evaluate long-range atmospheric transport (LRAT) episodes of POPs at a remote site in southern Norway has been developed. The system uses the Lagrangian particle transport model FLEXPART, and can be used for triggering extra (“targeted”) sampling when LRAT episodes are predicted to occur. The system was evaluated by comparing <b>targeted</b> <b>samples</b> collected over 12 - 25 h during individual LRAT episodes with monitoring samples regularly collected over one day per week throughout a year. Measured concentrations in all <b>targeted</b> <b>samples</b> were above the 75 th percentile of the concentrations obtained from the regular monitoring program and included the highest measured values of all samples. This clearly demonstrates the success of the <b>targeted</b> <b>sampling</b> strategy...|$|R
40|$|The {{effect of}} the {{composition}} of the used standard reference material (SRM) on results of determination of fallout radionuclides in soil samples was studied. Using five soil types as SRMs, we measured the specific activity of Pb- 210 and Cs- 137 in six <b>target</b> <b>samples</b> of Chestnut soil. It was observed that the determination of the Pb- 210 activity in the samples depended on the chemical composition of SRMs used to create the efficiency curves. Thus, using SRMs similar in chemical composition to the <b>target</b> <b>samples</b> should improve accuracy in the determination of Pb- 210 in environmental samples. (C) 2010 Elsevier Ltd. All rights reserve...|$|R
5000|$|Ion Gun, used {{to direct}} a beam of ions at a <b>target</b> <b>sample.</b> An {{electron}} ionization ion source is typically used to ionize noble gas atoms such as He, Ne or Ar, while heating of wafers containing alkali atoms is used to create an alkali ion beam. The ions thus created hold a positive charge, typically +1, due to the ejection of electrons from the atoms. The range of energies used most often in LEIS is 500 eV to 20 keV. In order to attain good experimental resolution {{it is important to}} have a narrow energy spread (ΔE/E < 1%) in the outgoing ion beam.|$|E
5000|$|... 8,813 {{people were}} {{screened}} for the study, {{out of which}} were ultimately chosen 459 participants whose demographic characteristics most closely resembled the target population and study requirements. The sample population consisted of healthy {{men and women with}} an average age of 46, with systolic blood pressures of less than 160 mm Hg and diastolic blood pressures within 80 to 95 mm Hg. African-American and other minority groups were planned to comprise 67% of the study sample, with 49% of the sample being female. Indeed, due to the exceptional burden of high blood pressure in minority populations, especially among African-Americans, a major goal of the trial was to recruit enough ethnic minorities to constitute two thirds of the <b>target</b> <b>sample.</b>|$|E
50|$|The survey had an {{original}} <b>target</b> <b>sample</b> size of 10,080 households. Interviewers visited 89.4% of 1086 household clusters {{during the study}} period and the household response rate was 96.2%. 115 clusters (10.6%) were not visited due to security problems. The IFHS argues that past mortality is likely to be higher in these missed clusters. Ratios derived from comparing their surveyed clusters to corresponding data from Iraq Body Count were used to impute elevated mortality rates to these missed clusters. Using this method, the IFHS derives death rates for the missed clusters in Baghdad that are 4.0 times as high as the clusters visited by the survey in Baghdad. The same procedure in Anbar derived rates that are 1.7 times as high as clusters visited in Anbar.|$|E
50|$|Articles {{on quality}} {{assurance}} in cytology published by Dr Laverty {{and his team}} concerned the importance of <b>targeted</b> <b>sampling</b> of the transformation zone (where most precancer and cancer occurs), sampling implement choice, reporting terminology and management recommendations.|$|R
40|$|Abstract—The {{performance}} of a generic pedestrian detector may drop significantly when it is applied to a specific scene due to the mismatch between the source training set and <b>samples</b> from the <b>target</b> scene. We propose a new approach of automatically transferring a generic pedestrian detector to a scene-specific detector in static video surveillance without manually labeling <b>samples</b> from the <b>target</b> scene. The proposed transfer learning framework consists of four steps. (1) Through exploring the indegrees from <b>target</b> <b>samples</b> to source samples on a visual affinity graph, the source samples are weighted to match the distribution of <b>target</b> <b>samples.</b> (2) It explores a set of context cues to automatically select <b>samples</b> from the <b>target</b> scene, predicts their labels, and computes confidence scores to guide transfer learning. (3) The confidence scores propagate among <b>target</b> <b>samples</b> according to their underlying visual structures. (4) <b>Target</b> <b>samples</b> with higher confidence scores have larger influence on training scene-specific detectors. All these considerations are formulated under a single objective function called Confidence-Encoded SVM, which avoids hard thresholding on confidence scores. During test, only the appearance-based detector is used without context cues. The effectiveness is demonstrated through experiments on two video surveillance datasets. Compared with a generic detector, it improves the detection rates by 48 % and 36 % at one false positive per image (FPPI) on the two datasets respectively. The training process converges after one or two iterations on the datasets in experiments. Index Terms—Pedestrian detection, transfer learning, confidence-encoded SVM, domain adaptation, video surveillance F...|$|R
40|$|Proceedings of the 2007 Georgia Water Resources Conference, March 27 - 29, 2007, Athens, Georgia. In a {{continuing}} {{effort to develop}} inexpensive source tracking methods to detect human fecal contamination in environmental waters, we combined <b>targeted</b> <b>sampling</b> with fluorometry. <b>Targeted</b> <b>sampling</b> works by identifying hotspots of fecal contamination through multiple samplings over ever decreasing distances. Fluorometry identifies human fecal contamination by detecting optical brighteners, primarily from laundry detergents. On St. Simons Island, <b>targeted</b> <b>sampling</b> and fluorometry identified two hotspots of fecal contamination. One hotspot was confirmed as fecal contamination from humans, but the other was not, most likely because of background organic matter fluorescence. Adding a 436 -nm emission filter to the fluorometer reduced this background fluorescence by > 50 %, and with this filter in place, the second hotspot was identified as fecal contamination from birds. As long as a fluorometer {{is equipped with a}} 436 -nm emission filter, <b>targeted</b> <b>sampling</b> combined with fluorometry may be a relatively inexpensive method to identify human fecal contamination in water. Sponsored and Organized by: U. S. Geological Survey, Georgia Department of Natural Resources, Natural Resources Conservation Service, The University of Georgia, Georgia State University, Georgia Institute of TechnologyThis book was published by the Institute of Ecology, The University of Georgia, Athens, Georgia 30602 - 2202. The views and statements advanced in this publication are solely those of the authors and do not represent official views or policies of The University of Georgia, the U. S. Geological Survey, the Georgia Water Research Institute as authorized by the Water Resources Research Act of 1990 (P. L. 101 - 397) or the other conference sponsors...|$|R
50|$|Elastic Recoil Detection Analysis (ERDA), also {{referred}} to as forward recoil scattering (or, contextually, spectrometry), is an Ion Beam Analysis technique in materials science to obtain elemental concentration depth profiles in thin films. This technique is known by several different names. These names are listed below. In the technique of ERDA, an energetic ion beam is directed at a sample to be characterized and (as in Rutherford backscattering) there is an elastic nuclear interaction between the ions of beam and the atoms of the <b>target</b> <b>sample.</b> Such interactions are commonly of Coulomb nature. Depending on the kinetics of the ions, cross section area, and the loss of energy of the ions in the matter, Elastic Recoil Detection Analysis helps determine the quantification of the elemental analysis. It also provides information about the depth profile of the sample.|$|E
50|$|Weighted {{data for}} China, Ghana, India, Mexico, Russian Federation and South Africa {{are in the}} public domain (see Study on global AGEing and Adult Health). SAGE's first full round of data {{collection}} included both follow-up and new respondents in four participating countries. The goal of the sampling design was to obtain a nationally representative cohort of persons aged 50 years and older, with a smaller cohort of persons aged 18 to 49 for comparison purposes. The <b>target</b> <b>sample</b> size was 5000 households with at least one person aged 50+ years and 1000 households with an 18- to 49-year-old respondent. In the older households, all persons aged 50+ years (for example, spouses and siblings) were invited to participate. Proxy respondents were identified for respondents who were unable to respond for themselves. The pooled data set will include over 43,000 respondents (see table below).|$|E
50|$|The core {{principle}} behind microarrays is hybridization {{between two}} DNA strands, {{the property of}} complementary nucleic acid sequences to specifically pair with each other by forming hydrogen bonds between complementary nucleotide base pairs. A high number of complementary base pairs in a nucleotide sequence means tighter non-covalent bonding between the two strands. After washing off non-specific bonding sequences, only strongly paired strands will remain hybridized. Fluorescently labeled target sequences that bind to a probe sequence generate a signal {{that depends on the}} hybridization conditions (such as temperature), and washing after hybridization. Total strength of the signal, from a spot (feature), depends upon the amount of <b>target</b> <b>sample</b> binding to the probes present on that spot. Microarrays use relative quantitation in which the intensity of a feature is compared to the intensity of the same feature under a different condition, and the identity of the feature is known by its position.|$|E
40|$|Watch-list-based {{classification}} and verification is advantageous in {{a variety}} of surveillance applications. In this thesis, we present an approach for verifying if a query image lies in a predefined set of <b>target</b> <b>samples</b> (the watch-list) or not. This approach is particularly useful at identifying a small set of target subjects and therefore can render high levels of accuracy. Further, this approach can also be extended to identify the query image exactly out of the <b>target</b> <b>samples.</b> The three- stages approach proposed here consists of using a combination of color and texture features to represent the image and further using, Kernel Partial Least Squares for dimensionality reduction followed by a classifier. This approach provides improved accuracy as shown by experiments on two datasets...|$|R
40|$|In April 2004, {{high numbers}} of fecal enterococci {{triggered}} a beach advisory on Sea Island, GA. <b>Targeted</b> <b>sampling,</b> which finds fecal contamination much like the children’s game of “hot” and “cold,” was combined with three bacterial source tracking (BST) methods: Enterococcus speciation, {{the presence or absence}} of a human virulence factor in Enterococcus faecium, and fluorometry. During calm (i. e non-runoff) conditions, the likely contamination sources were wildlife feces and leaking sewer lines located on a creek of St. Simons Island, GA. Fluorometry quickly identified malfunctioning sewer lines. A test for human virulence factor was positive. During stormflow (i. e. runoff) conditions, the likely contamination sources were wildlife feces and effluent from two pipes. A test for human virulence factor was negative. Because the percentage of Ent. faecalis from the pipes was high (> 30 %), fecal contamination from wild birds was likely. This is the first report of <b>targeted</b> <b>sampling</b> during stormy conditions, and the first time fluorometry has been combined with <b>targeted</b> <b>sampling.</b> Sponsored by: Georgia Environmental Protection Division U. S. Geological Survey, Georgia Water Science Center U. S. Department of Agriculture, Natural Resources Conservation Service Georgia Institute of Technology, Georgia Water Resources Institute The University of Georgia, Water Resources Facult...|$|R
30|$|All of {{the results}} {{obtained}} from the human scent identifications using specially trained canines are summarized in Table  3. In these line-up identifications, the trained canines compared the fractioned starting scent (SS) samples with the “full” <b>target</b> <b>samples</b> (TS) and other distracting scents (DS) in the line-up.|$|R
