23|158|Public
30|$|The {{error is}} {{decreasing}} after 300 <b>training</b> <b>epoch</b> using the Levenberg-Marquardt method.|$|E
40|$|Existing metrics for the {{learning}} performance of feed-forward neural networks {{do not provide}} a satisfactory basis for comparison because {{the choice of the}} <b>training</b> <b>epoch</b> limit can determine the results of the comparison. I propose new metrics which have the desirable property of being independent of the <b>training</b> <b>epoch</b> limit. The efficiency measures the yield of correct networks in proportion to the training effort expended. The optimal epoch limit provides the greatest efficiency. The learning performance is modelled statistically, and asymptotic performance is estimated. Implementation details may be found in (Harney, 1992). ...|$|E
30|$|Both ANN and ANFIS {{model were}} {{developed}} using MATLAB (The Mathwork Inc.). The maximum <b>training</b> <b>epoch</b> was 1500. In the post-processing step, the decimal values {{were removed from}} the output by rounding the values to the integer number.|$|E
40|$|In this paper, Short Term Load Forecasting (STLF) can {{be applied}} using Generalized Neuron Model (GNM) for under sum square error {{gradient}} function for different learning rates, with various <b>training</b> <b>epochs</b> and constant leaning rate, by having 30, 000 <b>training</b> <b>epochs.</b> The simulation results were the root mean square testing error, maximum testing error, minimum testing error were predicted...|$|R
30|$|Repeat Steps 2 to 3 {{until the}} maximum number of <b>training</b> <b>epochs</b> is reached or the error is below a {{predefined}} limit.|$|R
40|$|Abstract. In {{the course}} of the face feature match, many {{classifiers}} have been designed. The neural network is usually selected as a classifier because of its validity and universality, whereas its <b>training</b> time, <b>training</b> <b>epochs,</b> and its convergence, all are not satisfied to us. It is often influenced by the author’s experience. In the case, a collaborative genetic algorithm and neural network is presented as a new face recognition classifier. The one thing is to train the NN weights by the GA until the stopping criterion is met, and the next thing is to use the BP algorithm to continue to train the network. The training time and <b>training</b> <b>epochs</b> have been improved in the experiment of the face recognition on ORL face database. The simulation shows the validity of methods...|$|R
30|$|Here the {{learning}} rate η, the momentum coefficient vector of the n-th training α _W^n and other coefficients {{are the same}} as the description of algorithm BGAM. For each α _w_i^n, after each <b>training</b> <b>epoch</b> it is chosen as (10).|$|E
30|$|Both grid {{partitioning}} and subtractive clustering {{techniques were}} applied for ANFIS modeling. Both kinds of ANFIS models were trained for 30 epochs. The value of error drops with each <b>training</b> <b>epoch.</b> The models trained using the training set were validated by the test set. The value of error goes {{down for the}} test set too {{with the number of}} <b>training</b> <b>epoch</b> till it finally converges to a constant value. The RMSE values converged at 0.109 and 0.105 for training set and the test set, respectively, for the grid partition-based ANFIS model. For the subtractive clustering based model, the RMSE values converged at 0.0539 and 0.0672 for training set and the test set, respectively.|$|E
30|$|Learning rate: the {{learning}} rate is very important. If it is big, {{the system will}} become unstable. Otherwise the <b>training</b> <b>epoch</b> will become too long. Generally, a relatively small learning rate will make the error converge asymptotically. At the same time, because the network size is different, {{the learning}} rate should be adjusted according to the network size. In this experiment, the learning rate is set to be 0.05.|$|E
30|$|In our audio source {{separation}} experiments, {{the number}} of discriminative filters is set to be 64, other parameters are set as described in Section 4.1. When the spectrum reconstruction is needed, the regularization coefficient is set to be 0.0001. Training is done using the Adam [54] update method and is stopped after 500 <b>training</b> <b>epochs.</b>|$|R
50|$|Spatz has {{published}} short {{stories in the}} New England Review, Glimmer <b>Train,</b> <b>Epoch,</b> The Kenyon Review, The New Yorker and in other literary journals. In 2003, Spatz won the Washington State Book Award for Wonderful Tricks {{and he was the}} recipient of the 2012 NEA Literature Fellowship. He has also won numerous grants from the Washington State Artist Trust.|$|R
40|$|Artificial Neural Networks (ANN) {{have gained}} {{increasing}} popularity {{as an alternative}} to statistical methods for classification of Remote Sensing Data. Their superiority to some of the classical statistical methods has been shown in the literature. Therefore, ANNs are commonly used for segmentation and classification purposes. In this paper we address a land cover classification problem using multi [...] spectral Landsat Thematic Mapper (TM) data employing ANNs. We concentrate on the search for the problem [...] adapted network topology and the appropriate number of <b>training</b> <b>epochs</b> for Multi [...] Layer Feed [...] Forward ANNs. To prevent the ANN from overfitting the number of <b>training</b> <b>epochs</b> is essential. For the automatic generation of problem [...] adapted topologies a method based on Genetic Algorithms (GA) is employed. With this approach populations of ANNs are generated, trained, and evaluated for the land [...] cover classification task. Individuals which solve the given task well receive a high fitness value [...] ...|$|R
30|$|A {{multilayer}} perceptron network trained {{with a highly}} popular algorithm known as the error back-propagation (BP) has been dominating in the neural network literature for over two decades (Haykin 2008). BP uses two practical ways to implement the gradient method: the batch updating approach that accumulates the weight corrections over the <b>training</b> <b>epoch</b> before performing the update, while the online learning approach updates the network weights immediately after each training sample is processed (Wilson and Martinez 2003).|$|E
40|$|The {{ice cover}} on lakes {{is one of}} the most {{influential}} factors in the lakes’ winter aquatic ecosystem. The paper presents a method for predicting ice coverage of lakes by means of multilayer perceptrons. This approach is based on historical data on the ice coverage of lakes taking Lake Onega as an example. The daily time series of ice coverage of Lake Onega for 2004 – 2017 was collected by means of satellite data analysis of snow and ice cover of the Northern Hemisphere. Input signals parameters for the multilayer perceptrons aimed at predicting ice coverage of lakes are based on the correlation analysis of this time series. The results of training of multilayer perceptrons showed that perceptrons with architectures of 3 - 2 - 1 within the Freeze-up phase (arithmetic mean of the mean square errors for <b>training</b> <b>epoch</b> MSE¯= 0. 0155 MSE = 0. 0155) and 3 - 6 - 1 within the Break-up phase (MSE¯= 0. 0105 MSE = 0. 0105) have the least mean-squared error for the last <b>training</b> <b>epoch.</b> Tests within the holdout samples prove that multilayer perceptrons give more adequate and reliable prediction of the ice coverage of Lake Onega (mean-squared prediction error MSPE = 0. 0076) comparing with statistical methods such as linear regression, moving average and autoregressive analyses of the first and second order...|$|E
40|$|This paper {{addresses}} {{the problem of}} deadbeat control in fully controlled high-power-factor rectifiers. Improved deadbeat control can be achieved {{through the use of}} neuralnetwork-based predictors for the input-current reference to the rectifier. In this application, on-line training is absolutely required. In order to achieve sufficiently fast on-line training, a new random-search algorithm is presented and evaluated. Simulation results show that this type of network training yields equivalent performance to standard backpropagation training. Unlike backpropagation, however, the random weight change (RWC) method can be implemented in mixed digital/analog hardware for this application. The paper proposes a very large-scale integration (VLSI) implementation which achieves a <b>training</b> <b>epoch</b> as low as 8 µs...|$|E
40|$|Denoising auto-encoders (DAEs) {{have been}} suc-cessfully used {{to learn new}} {{representations}} {{for a wide range}} of machine learning tasks. During training, DAEs make many passes over the train-ing dataset and reconstruct it from partial cor-ruption generated from a pre-specified corrupting distribution. This process learns robust represen-tation, though at the expense of requiring many <b>training</b> <b>epochs,</b> in which the data is explicitly corrupted. In this paper we present the marginal-ized Denoising Auto-encoder (mDAE), which (approximately) marginalizes out the corruption during training. Effectively, the mDAE takes into account infinitely many corrupted copies of the training data in every epoch, and therefore is able to match or outperform the DAE with much fewer <b>training</b> <b>epochs.</b> We analyze our proposed algorithm and show that it can be understood as a classic auto-encoder with a special form of reg-ularization. In empirical evaluations we show that it attains 1 - 2 order-of-magnitude speedup in training time over other competing approaches. 1...|$|R
30|$|Our work {{is based}} on two {{baseline}} models: Paragraph Vector and GloVe. In this paper, the hyper-parameter settings of two baselines follow the [18] 2 (an implementation of PV) and [22] 3 (an implementation of GloVe) respectively. The number of <b>training</b> <b>epochs</b> is the only hyper-parameter that is determined by validation data. The trained text embeddings are then fed into logistic regression classifier [5] for classification.|$|R
40|$|This paper {{presents}} a constructive algorithm for training cooperative neural-network ensembles (CNNEs). CNNE combines ensemble architecture design with cooperative training for individual neural networks (NNs) in ensembles. Unlike most previous studies on training ensembles, CNNE puts emphasis on both accuracy and diversity among individual NNs in an ensemble. In {{order to maintain}} accuracy among individual NNs, the number of hidden nodes in individual NNs are also determined by a constructive approach. Incremental training based on negative correlation is used in CNNE to train individual NNs for different numbers of <b>training</b> <b>epochs.</b> The use of negative correlation learning and different <b>training</b> <b>epochs</b> for <b>training</b> individual NNs reflect CNNEs emphasis on diversity among individual NNs in an ensemble. CNNE has been tested extensively {{on a number of}} benchmark problems in machine learning and neural networks, including Australian credit card assessment, breast cancer, diabetes, glass, heart disease, letter recognition, soybean, and Mackey [...] Glass time series prediction problems. The experimental results show that CNNE can produce NN ensembles with good generalization ability...|$|R
40|$|Performance metrics are {{a driving}} force in many fields of work today. The field of {{constructive}} neural networks is no different. In this field, the popular measurement metrics (resultant network size, test set accuracy) are difficult to maximise, given their dependence on several varied factors, of which the mostimportant is the dataset to be applied. This project set out with the intention to minimise the number of hidden units installed into a resource allocating network (RAN) (Platt 1991), whilst increasing the accuracy by means of application of competitive learning techniques. Three datasets were used for evaluation of the hypothesis, one being a time-series set, {{and the other two}} being more general regression sets. Many trials were conducted during the period of this work, {{in order to be able}} to prove conclusively the discovered results. Each trial was different in only one respect from another in an effort to maximise the comparability of the results found. Four metrics were recorded for each trial- network size (per <b>training</b> <b>epoch,</b> and final), test and training set accuracy (again, per <b>training</b> <b>epoch</b> and final), and overall trial runtime. The results indicate that the application of competitive learning algorithms to the RAN results in a considerable reduction in network size (and therefore the associated reduction in processing time) across the vast majority of the trials run. Inspection of the accuracy related metrics indicated that using this method offered no real difference to that of the originalimplementation of the RAN. As such, the positive network-size results found are only half of the bigger picture, meaning there is scope for future work to be done to increase the test set accuracy...|$|E
40|$|This paper {{proposes a}} new {{framework}} for adapting regularization parameters {{in order to}} minimize validation error during the training of feedforward neural networks. A second derivative of validation error based regularization algorithm (SDVR) is derived using the Gauss-Newton approximation to the Hessian. The basic algorithm, which uses incremental updating, allows the regularization parameter α to be recalculated in each <b>training</b> <b>epoch.</b> Two variations of the algorithm, called convergent updat-ing and conditional updating, enable α to be updated over a variable interval according to the specified control criteria. Simulations on a noise-corrupted parabolic function with two-inputs and a single output are investigated. The results demonstrate that the SDVR framework is very promising for adaptive regularization and can be costeffectively applied to a variety of different problems. 1...|$|E
40|$|In this paper; we {{introduce}} {{an enhanced}} electromyography (EMG) pattern recognition algorithm {{based on a}} split-and-merge deep belief network (SM-DBN). Generally, {{it is difficult to}} classify the EMG features because the EMG signal has nonlinear and time-varying characteristics. Therefore, various machine-learning methods have been applied in several previously published studies. A DBN is a fast greedy learning algorithm that can identify a fairly good set of weights rapidly—even in deep networks with a large number of parameters and many hidden layers. To reduce overfitting and to enhance performance, the adopted optimization method was based on genetic algorithms (GA). As a result, the performance of the SM-DBN was 12. 06 % higher than conventional DBN. Additionally, SM-DBN results in a short convergence time, thereby reducing the <b>training</b> <b>epoch.</b> It is thus efficient in reducing the risk of overfitting. It is verified that the optimization was improved using GA...|$|E
40|$|Abstract: The {{difficulties}} of tuning parameters of MLP classifiers are well known. In this paper, a measure is described that {{is capable of}} predicting the number of classifier <b>training</b> <b>epochs</b> for achieving optimal performance in an ensemble of MLP classifiers. The measure is computed between pairs of patterns on the training data, and is based on a spectral representation of a Boolean function. This representation characterises the mapping from classifier decisions to target label, and allows accuracy and diversity to be incorporated within a single measure. Results on many benchmark problems, including the ORL face database demonstrate that the measure is well correlated with base classifier test error, and may be used to predict the optimal number of <b>training</b> <b>epochs.</b> While correlation with ensemble test error is not quite as strong, it is shown in this paper that the measure may be used to predict number of epochs for optimal ensemble performance. Although the technique is only applicable to two-class problems, it is extended here to multi-class through Output Coding. For the Output Coding technique, a random code matrix is shown to give better performance than One-per-class code, even when the base classifier is well-tuned...|$|R
40|$|ABSTRACT- This paper {{presents}} an results of experimental investigation conducted {{to evaluate the}} possibilities of adopting Genetic Algorithm (GA) based Artificial Neural Networks (ANN) to predict the workability and strength characteristics of High Performance Concrete (HPC) with different water-binder ratios (0. 3, 0. 325, 0. 35, 0. 375, 0. 4, 0. 425, 0. 45, 0. 475 & 0. 5) and different aggregate binder ratios (2, 2. 5 & 3) and different percentage replacement of cement by mineral admixtures such as Flyash, Metakaolin and Silicafume (0, 10, 20 & 30 %) as input vectors. The network has been trained with experimental data obtained from laboratory experimentation. The Artificial Neural Network learned the relationship for predicting the Compaction factor, Vee-bee time, Compressive of HPC in 1300 <b>training</b> <b>epochs.</b> The Artificial Neural Network learned the relationship for predicting the Compressive strength, Tensile strength, Flexural strength and Young’s Modulus of HPC in 2000 <b>training</b> <b>epochs.</b> After successful learning the GA based ANN models predicted the workability and strength characteristics satisfying all the constraints with an accuracy of about 95 %. The various stages involved {{in the development of}} genetic algorithm based neural network models are addressed at length in this paper...|$|R
30|$|The {{characteristics}} are plotted {{two on the}} same graph, {{to show that there}} is no significant difference. The characteristic of the training data is plotted with °. The characteristic of the FIS output is plotted with *. The difference between the training case and the testing case is very small. The plotting signs ° and * are on the same points for both characteristics. The average testing error is 2, 017.10 - 5. The number of <b>training</b> <b>epochs</b> was 3.|$|R
40|$|We {{study the}} problem of {{hierarchical}} classification when labels corre-sponding to partial and/or multiple paths in the underlying taxonomy are allowed. We introduce a new hierarchical loss function, the H-loss, im-plementing the simple intuition that additional mistakes in the subtree of a mistaken class should not be charged for. Based on a probabilistic data model introduced in earlier work, we derive the Bayes-optimal classifier for the H-loss. We then empirically compare two incremental approx-imations of the Bayes-optimal classifier with a flat SVM classifier and with classifiers obtained by using hierarchical versions of the Perceptron and SVM algorithms. The experiments show that our simplest incremen-tal approximation of the Bayes-optimal classifier performs, after just one <b>training</b> <b>epoch,</b> nearly {{as well as the}} hierarchical SVM classifier (which performs best). For the same incremental algorithm we also derive an H-loss bound showing, when data are generated by our probabilistic dat...|$|E
40|$|The basic {{back-propagation}} learning {{law is a}} gradient-descent algorithm based on the estimation of the gradient of the instantaneous sum-squared error for each layer: fiffffifl (7. 1) Such an algorithm is slow for three basic reasons: It uses an instantaneous sum-squared error to minimise the mean squared error, %, over the <b>training</b> <b>epoch.</b> The gradient of the instantaneous sum-squared error {{is not a good}} estimate of the gradient of the mean squared error. Therefore, satisfactory minimisation of this error typically requires many repetitions of the training epochs. It is a first-order minimisation algorithm which is based on the first-order derivatives (a gradient). Faster algorithms utilise also the second derivatives (the Hessian matrix) The error propagation, which is conceptually very interesting, serialises computations on the layer by layer basis. The mean squared error, %, is a relatively complex surface in the weight space, possibly with many local minima, flat sections, narrow irregular valleys, and saddle points...|$|E
40|$|Abstract: Autoregressive (AR) {{model is}} a common {{predictor}} that has been extensively used for time series forecasting. Many training methods can used to update AR model parameters, for instance, least square estimate and maximum likelihood estimate; however, both techniques are sensitive to noisy samples and outliers. To deal with the problems, an evolving AR predictor, EAR, is developed in this study to enhance prediction accuracy and mitigate the effect of noisy samples and outliers. The model parameters of EAR are trained with an Adaptive Least Square Estimate (ALSE) method, which can learn samples characteristics more effectively. In each <b>training</b> <b>epoch,</b> the ALSE weights the samples by their fitting accuracy. The samples with larger fitting errors {{will be given a}} larger penalty value in the cost function; however the penalties of difficult-to-predict samples will be adaptively reduced to enhance the prediction accuracy. The effectiveness of the developed EAR predictor is verified by simulation tests. Test results show that the proposed EAR predictor can capture the dynamics of the time series effectively and predict the future trend accurately...|$|E
50|$|A Word2vec {{model can}} be trained with {{hierarchical}} softmax and/or negative sampling. To approximate the conditional log-likelihood a model seeks to maximize, the hierarchical softmax method uses a Huffman tree to reduce calculation. The negative sampling method, on the other hand, approaches the maximization problem by minimizing the log-likelihood of sampled negative instances. According to the authors, hierarchical softmax works better for infrequent words while negative sampling works better for frequent words and better with low dimensional vectors. As <b>training</b> <b>epochs</b> increase, hierarchical softmax stops being useful.|$|R
40|$|In {{this paper}} we {{describe}} CB 513 a non-redundant dataset, suitable {{for development of}} algorithms for prediction of secondary protein structure. A program was made in Borland Delphi for transforming data from our dataset to make it suitable for learning of neural network for prediction of secondary protein structure implemented in MATLAB Neural-Network Toolbox. Learning (training and testing) of neural network is researched with different sizes of windows, different number of neurons in the hidden layer and different number of <b>training</b> <b>epochs,</b> while using dataset CB 513...|$|R
30|$|In our audio scene {{classification}} experiments, {{the number}} of discriminative filters is also set to be 64. For both LITIS ROUEN and DCASE 2016 datasets, we use rectified linear units; the window sizes of convolutional layers are 64 × 2 × 64, 64 × 3 × 64, and 64 × 4 × 64, and the fully connected layers are 196 × 128 × 19 (15). For DCASE 2016 dataset, we use the dropout rate of 0.5. Training is done using the Adam update method and is stopped after 100 <b>training</b> <b>epochs.</b>|$|R
40|$|Most {{parts of}} the urban areas {{are faced with the}} problem of {{floating}} fine particulate matter. Therefore, it is crucial to estimate the amounts of fine particulate matter concentrations through the urban atmosphere. In this research, an artificial neural network technique was utilized to model the PM 2. 5 dispersion in Tehran City. Factors which are influencing the predicted value consist of weather-related and air pollution-related data, i. e. wind speed, humidity, temperature, SO 2, CO, NO 2, and PM 2. 5 as target values. These factors have been considered in 19 measuring stations (zones) over urban area across Tehran City during four years, from March 2011 to March 2015. The results indicate that the network with hidden layer including six neurons at <b>training</b> <b>epoch</b> 113, has the best performance with the lowest error value (MSE= 0. 049438) on considering PM 2. 5 concentrations across metropolitan areas in Tehran. Furthermore, the “R” value for regression analysis of training, validation, test, and all data are 0. 65898, 0. 6419, 0. 54027, and 0. 62331, respectively. This study also represents the artificial neural networks have satisfactory implemented for resolving complex patterns in the field of air pollution...|$|E
40|$|Identifying DNA {{sequences}} is {{very useful}} in forensic area. Currently, {{there are a}} lot of computational biology approaches (bioinformatics) in solving the molecular biology. The variation, complexity, and incompletely-understood nature of sequences make it impractical to hand-code algorithm by applying the human ability and laboratory equipments in identifying the sequences. Artificial neural network (ANN), which is one of the commonly used machine learning technique, might be preferable to form its own descriptions of genetic concepts. Thus, it is applied in developing a prototype to identify the living organism whether it is human (Malay or India) or non-human. A multi-layer backpropagation algorithm of one hidden layer with 5 neurons was used. It is the constant representation whereby it produces one output. The training set was composed of 7 types of organisms from randomly selected DNA nucleotide sequences. The result of this prototype shows that it successfully can train the sequence of non-human. The reason it cannot train the human sequence probably because the way of massaging the data. By using the different sequences from the same types of organisms, the network successfully can identify. The <b>training</b> <b>epoch</b> and time can be accelerated if the network is included with the momentum...|$|E
40|$|In this paper, {{we present}} a simple and {{efficient}} method for training deep neural networks in a semi-supervised setting where {{only a small portion}} of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent <b>training</b> <b>epoch,</b> and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18. 44 % to 7. 05 % in SVHN with 500 labels and from 18. 63 % to 16. 55 % in CIFAR- 10 with 4000 labels, and further to 5. 12 % and 12. 16 % by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR- 100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels. Comment: Final ICLR 2017 version. Includes new results for CIFAR- 100 with additional unlabeled data from Tiny Images datase...|$|E
40|$|Abstract- In this paper, {{we study}} the {{properties}} {{of a new kind}} of complex domain artificial neural networks called complex adaptive spline neural networks (CASNN), which are able to adapt their activation functions by varying the control points of a Catmull-Rom cubic spline. This new kind of neural network can be implemented as a very simple structure being able to improve the generalization capabilities using few <b>training</b> <b>epochs.</b> Due to its low architectural complexity this network can be used to cope with several nonlinear DSP problem at high throughput rate. 1...|$|R
40|$|Abstract. In {{this paper}} we conduct a {{comparative}} study between hybrid methods to optimize multilayer perceptrons: {{a model that}} optimizes the architecture and initial weights of multilayer perceptrons; a parallel approach to optimize the architecture and initial weights of multilayer perceptrons; a method that searches for {{the parameters of the}} training algorithm, and an approach for cooperative co-evolutionary optimization of multilayer perceptrons. Obtained results show that a co-evolutionary model obtains similar or better results than specialized approaches, needing much less <b>training</b> <b>epochs</b> and thus using much less simulation time. ...|$|R
30|$|A {{weakness}} of the neural network approach {{is that it can}} be easily overfitted, namely, the net steadily improves its fitting with the training patterns over the epochs, at the cost of diminishing the ability to generalize to patterns never seen during the training. The overfitting, therefore, causes an error rate on validation data larger than the error rate on the training data. To avoid the overfitting, a careful choice of the training set, the number of neurons in the hidden layer, and the number of <b>training</b> <b>epochs</b> must be performed.|$|R
