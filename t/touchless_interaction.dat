33|12|Public
25|$|The aim of {{the project}} then is to explore the use of <b>touchless</b> <b>interaction</b> within {{surgical}} settings, allowing images to be viewed, controlled and manipulated without contact {{through the use of}} camera-based gesture recognition technology. In particular, the project seeks to understand the challenges of these environments for the design and deployment of such systems, as well as articulate the ways in which these technologies may alter surgical practice. While our primary concerns here are with maintaining conditions of asepsis, the use of these touchless gesture-based technologies offers other potential uses.|$|E
5000|$|Crunchfish is a Swedish {{technology}} company in Malmö that develops gesture recognition {{software for the}} mobile phone and tablet market. Crunchfish was founded in 2010 with an initial focus to create innovations for the iOS and Android app markets. Gesture recognition using a standard webcam as main gesture sensor {{was one of their}} core innovations and the company is now focusing on <b>touchless</b> <b>interaction</b> based on camera based gestures. In 2013, April, the company was selected a '2013 Red Herring Top 100' company by Red Herring (magazine). Crunchfish produces gesture sensing software, a set of customized mid-air gesture recognition solutions, named A3D™, to global mobile device manufacturers and app developers. Crunchfish cooperates with smartphone manufacturers to enable Crunchfish gesture sensing technology in their partners mobile devices. Crunchfish developed the touchless functions in Chinese Gionee's smartphone Elife E6, launched in China, July, 2013 and in India and Africa in August, 2013 ...|$|E
40|$|Within medical {{settings}} {{there is}} a growing interest in exploring <b>touchless</b> <b>interaction</b> technologies. The primary motivation here is to avoid contact during interaction with data so as to maintain asepsis. However, {{there is a}}nother important property of <b>touchless</b> <b>interaction</b> that has significant implications for their use within such settings – namely that interaction behaviour is spatially distal from the device being interacted with. To further understand these implications we present fieldwork observations of work practice in neurosurgery theatres. Drawing on the notion of interaction proxemics and the theory of F-formations, our analysis articulates the spatial organization of collaborative work practices and interaction in these settings. From this understanding of spatial practices, we discuss opportunities and difficulties relating to the design of <b>touchless</b> <b>interaction</b> technologies for in surgical settings. Author Keywords Gestural, <b>touchless</b> <b>interaction,</b> space, proxemics, health...|$|E
40|$|<b>Touchless</b> <b>interactions</b> {{synthesize}} {{input and}} output from physically disconnected motor and display spaces without any haptic feedback. In {{the absence of any}} haptic feedback, <b>touchless</b> <b>interactions</b> primarily rely on visual cues, but properties of visual feedback remain unexplored. This paper systematically investigates how large-display <b>touchless</b> <b>interactions</b> are affected by (1) types of visual feedback—discrete, partial, and continuous; (2) alternative forms of touchless cursors; (3) approaches to visualize target-selection; and (4) persistent visual cues to support out-of-range and drag-and-drop gestures. Results suggest that continuous was more effective than partial visual feedback; users disliked opaque cursors, and efficiency did not increase when cursors were larger than display artifacts’ size. Semantic visual feedback located at the display border improved users’ efficiency to return within the display range; however, the path of movement echoed in drag-and-drop operations decreased efficiency. Our findings contribute key ingredients to design suitable visual feedback for large-display touchless environments. This work was partially supported by an IUPUI Research Support Funds Grant (RSFG) ...|$|R
40|$|To {{facilitate}} {{interaction and}} collaboration around ultra-high-resolution, Wall-Size Displays (WSD), post-WIMP <b>interaction</b> modes like <b>touchless</b> and multi-touch {{have opened up}} new, unprecedented opportunities. Yet to fully harness this potential, {{we still need to}} understand fundamental design factors for successful WSD experiences. Some of these include visual feedback for <b>touchless</b> <b>interactions,</b> novel interface affordances for at-a-distance, high-bandwidth input, and the techno-social ingredients supporting laid-back, relaxed collaboration around WSDs. This position paper highlights our progress in a long-term research program that examines these issues and spurs new, exciting research directions. We recently completed a study aimed at investigating the properties of visual feedback in <b>touchless</b> WSD <b>interaction,</b> and we discuss some of our findings here. Our work exemplifies how research in WSD interaction calls for re-conceptualizing basic, first principles of Human-Computer Interaction (HCI) to pioneer a suite of next-generation interaction environments...|$|R
40|$|Everybody {{was amazed}} {{a few years}} ago. How could Amazon patent one click? Isn’t that obvious? Haven’t we been single and double {{clicking}} for some time? The US Patent and Trademark office decides what is patentable, which they define as things that are new, useful, and nonobvious. If that all checks out, the patent office clicks ‘‘approve. ’’ Since then, the landscape has changed considerably. Single clicks, touches, and gestures are now powerful, often complex interactions {{that are part of}} our everyday vernacular. Currently, <b>touchless</b> <b>interactions</b> and zero-click interfaces bring us content, push notifications, and recommended playlists (see the sidebar fo...|$|R
40|$|The {{growth of}} image-guided {{procedures}} in surgical settings {{has led to}} an increased need to interact with digital images under sterile conditions. Traditional touch-based interaction techniques present challenges for managing asepsis in these environments leading to suggestions that new <b>touchless</b> <b>interaction</b> techniques may provide a compelling set of alternatives. In this paper we explore the potential for <b>touchless</b> <b>interaction</b> in image-guided Interventional Radiology (IR) through an ethnographic study. The findings highlight how the distribution of labour and spatial practices of this work are organised with respect to concerns about asepsis and radiation exposure, the physical and cognitive demands of artefact manipulation, patient management, and the construction of “professional vision”. We discuss the implications of these key features of the work for <b>touchless</b> <b>interaction</b> technologies within IR and suggest that such issues will be of central importance in considering new input techniques in other medical settings. Author Keywords Image use, ethnography, medical practice, touchles...|$|E
40|$|Motion-based <b>touchless</b> <b>interaction</b> {{empowers}} {{users to}} interact using movements and gestures, {{and without the}} burden of physical contact with technology (e. g., data gloves, body markers, or remote controllers). Most motion-based touchless applications are designed for interaction "in-the-large", where users engage with medium or large displays. Our research explores motion-based <b>touchless</b> <b>interaction</b> "in-the- small" that involves only small displays (of the size, for example, of smart phone screens). We have applied this novel paradigm to develop interactive applications for household appliances, discovering that the "in-the-small" feature raises a number of challenging design issues, exemplified in the paper through a case study...|$|E
40|$|Our {{research}} {{explores the}} integration of motion-based <b>touchless</b> <b>interaction</b> with human-robots interaction to support game-based learning for children with intellectual disability. The paper discusses the design challenges of this novel approach and presents the design concepts of our initial prototypes. © 2014 Authors...|$|E
50|$|Researchers at Deutsche Telekom {{have used}} magnetometers {{embedded}} in mobile devices to permit <b>touchless</b> 3D <b>interaction.</b> Their interaction framework, called MagiTact, tracks {{changes to the}} magnetic field around a cellphone to identify different gestures made by a hand holding or wearing a magnet.|$|R
40|$|Natural User Interfaces (NUI) offer rich {{ways for}} {{interacting}} with the digital world that make innovative use of existing human capabilities. They include and often combine different input modalities such as voice, gesture, eye gaze, body <b>interactions,</b> touch and <b>touchless</b> <b>interactions.</b> However much of the focus of NUI research and development has been on enhancing the experience of individuals interacting with technology. Effective NUIs must also acknowledge our innately social characteristics, and support how we communicate with each other, play together, learn together and collaboratively work together. This workshop concerns the social aspects of NUI. The workshop seeks {{to better understand the}} social uses and applications of these new NUI technologies [...] how we design these technologies for new social practices and how we understand the use of these technologies in key social contexts...|$|R
40|$|Public {{displays}} have lately become ubiquitous {{thanks to}} the decreasing cost of such technology and public policies supporting the development of smart cities. Depending on form factor, those displays might use touchless gestural interfaces that therefore are becoming more often the subject {{of public and private}} research. In this paper, we focus on <b>touchless</b> <b>interactions</b> with situated public displays, and introduce a pilot study on comparing two interfaces: An interface based on the Microsoft Human Interface Guidelines (HIG), a de facto standard in the field, and a novel interface, designed by us. Differently from the HIG-based one, our interface displays an avatar, which does not require any activation gestures to trigger actions. Our aim is to study how the two interfaces address the so-called interaction blindness - the inability of the users to recognize the interactive capabilities of those displays. According to our pilot study, although providing a different approach, both interfaces has proven effective in the proposed scenario: A public display in a hall inside a University campus building...|$|R
40|$|Indiana University-Purdue University Indianapolis (IUPUI) We use {{gestures}} {{frequently in}} daily life—to interact with people, pets, or objects. But interacting with computers using mid-air gestures continues {{to challenge the}} design of touchless systems. Traditional approaches to <b>touchless</b> <b>interaction</b> focus on exploring gesture inputs and evaluating user interfaces. I shift the focus from gesture elicitation and interface evaluation to <b>touchless</b> <b>interaction</b> mechanics. I argue for a novel approach to generate design guidelines for touchless systems: to use fundamental interaction principles, instead of a reactive adaptation to the sensing technology. In five sets of experiments, I explore visual and pseudo-haptic feedback, motor intuitiveness, handedness, and perceptual Gestalt effects. Particularly, I study the interaction mechanics in touchless target selection. To that end, I introduce two novel interaction techniques: touchless circular menus that allow command selection using directional strokes and interface topographies that use pseudo-haptic feedback to guide steering–targeting tasks. Results illuminate different facets of <b>touchless</b> <b>interaction</b> mechanics. For example, motor-intuitive touchless interactions explain how our sensorimotor abilities inform touchless interface affordances: we often make a holistic oblique gesture instead of several orthogonal hand gestures while reaching toward a distant display. Following the Gestalt theory of visual perception, we found similarity between user interface (UI) components decreased user accuracy while good continuity made users faster. Other findings include hemispheric asymmetry affecting transfer of training between dominant and nondominant hands and pseudo-haptic feedback improving touchless accuracy. The results of this dissertation contribute design guidelines for future touchless systems. Practical applications of this work {{include the use of}} <b>touchless</b> <b>interaction</b> techniques in various domains, such as entertainment, consumer appliances, surgery, patient-centric health settings, smart cities, interactive visualization, and collaboration...|$|E
40|$|To design intuitive, {{interactive}} {{systems in}} various domains, such as health, entertainment, or smart cities, researchers are exploring <b>touchless</b> <b>interaction.</b> Touchless systems allow individuals to interact without any input device—using freehand gestures in midair. Gesture-elicitation studies focus on generating user-defined interface controls to design touchless systems. Interface controls, however, {{are composed of}} primary units called interaction primitives—which remain little explored. For example, what touchless primitives are motor-intuitive and can unconsciously use our pre-existing sensorimotor knowledge (such as visual perception or motor skills) ? Drawing on the disciplines of cognitive science and motor behavior, my research aims to understand the perceptual and motor factors in <b>touchless</b> <b>interaction</b> with 2 D user interfaces (2 D UIs). I then aim to apply this knowledge to design a set of touchless interface controls for large displays. Author Keywords Gesture-based interfaces; novel interaction techniques; touchless interaction; motor-intuitive interaction. ACM Classification Keyword...|$|E
40|$|Smart cities large {{events have}} led to the {{ubiquitous}} deployment of public display systems, supporting the ambition to improve visitors experience and providing services which fit users' individual needs. In this context, we developed a personalized information service that integrates touch and <b>touchless</b> <b>interaction,</b> respectively on personal devices and large public screens, transforming public displays to something that makes a step towards innovation...|$|E
40|$|In this paper, we {{will explore}} the {{potentials}} of low-cost portable immersive environments that combine textile structures, gesture-based interfaces and multiple projections. Our {{aim is to}} develop affordable, easy to set up, portable and inviting immersive spaces that can serve as an interface between a web-based geographic virtual environment, experts and lay people. In this context, after the introduction, we will review a variety of methods, conceptual tools and materials related to textile tectonics and techniques which can be individually used or combined for the development and construction of portable immersive spaces. In the next section, we will discuss the opportunities and challenges of using a low-cost gesture-based interface (Kinect) to support <b>touchless</b> <b>interactions.</b> Consequently, we will present the design alternatives of low-cost portable immersive spaces that we have synthesized from our background studies. This {{will be followed by}} the observations and findings from our prototype development, implementation and preliminary testing processes. In conclusion, we will discuss our conclusions and recommendations regarding the future development of low-cost portable immersive spaces...|$|R
40|$|Abstract Contemporary art {{nowadays}} {{tries to}} exploit modern technology to better address and enlighten specific problems and ideas of our time. Our interest in wider impact {{of modern technology}} on society and the interest in contemporary art, brought our attention also to the applicative field of use of computer vision methods in art. This chapter walks us through a few projects, proving that art is definitely a perfect testbed for our research: 15 Seconds of Fame, Dynamic Anamorphosis, Virtual Skiing, Smart Wall, Virtual Dance and Virtual Painter, from face detection, motion following, depth recovery, <b>touchless</b> human-computer <b>interaction</b> to popart, constant eye gaze of a person on the portrait, regardless of where the spectator stands, immersion into different virtual worlds {{without the need for}} any special equipment. ...|$|R
40|$|Within this masters thesis a Location Based Service is designed, {{that aims}} {{mobility}} impaired people. It is developed on a mobile, view controlled Augmented Reality platform, which implements the See-Through technology. The requirements to {{graphical user interfaces}} such a system arises are analysed, in order to asure the readable and noticable presentation of informations. The Location Based Service uses geoinformations from the OpenStreetMap project, which are adapted {{to the requirements of}} the system. Therefore a comparison between different rendering tools is done. To asure the portability of the service, the communication among eyetracking software and application is reduced. In addition widget libraries that are based on DirectFB are compared. They provide elements for graphical user interaction and perfom a further decouple of location based service and eyetracking software. Furthermore possibilities of <b>touchless,</b> eyebased <b>interactions</b> are analyzed and the most practicably is put into practice...|$|R
40|$|Markerless motion-sensing {{promises}} to position touchless interactions successfully in various domains (e. g., entertainment or surgery) {{because they are}} deemed natural. This naturalness, however, depends upon the mechanics of <b>touchless</b> <b>interaction</b> that remains largely unexplored. My dissertation first aims to deconstruct the interaction mechanics of touchless, especially its device-less property, from an embodied perspective. Grounded in this analysis, I then plan to investigate how visual perception affects <b>touchless</b> <b>interaction</b> with distant, 2 D displays. Preliminary findings suggest that Gestalt principles in visual perception and motor action affect the touchless user experience. User interface elements demonstrating perceptual-grouping principles, such as similarity of orientation decreased users’ efficiency, while continuity of UI elements forming a perceptual whole increased users ’ effectiveness. Moreover, following the law of Prägnanz, users often gestured to minimize their energy expenditure. This work can inform the design of touchless UX by uncovering relations between perceptual and motor gestalt in touchless interactions...|$|E
40|$|The Human-Computer Interaction {{research}} {{field is}} ever evolving, producing new interac-tion methods, always seeking efficiency, innovation and intuitiveness. Among these inno-vations {{we have the}} concern with reducing the distance between natural human behavior and human-computer interaction. Interfaces which address this concern are called natural user interfaces. One particular field of these interfaces, <b>touchless</b> <b>interaction,</b> has been the object of new advancements that provide a new spot {{on the market for}} this concept. Devices, such as, depth cameras, that enable this type of interaction have been flooding the market at a fast pace, with developers and hackers trying to harness its capabilities. However, developing an application based on <b>touchless</b> <b>interaction</b> may be a cumbersome process, requiring a considerably good understanding on the subject, including frame-works and how to properly exploit their capabilities. As the interactive process does not require contact with any device it makes sense to incorporate gesture recognition, which can also be difficult. Therefore, currently, there exists the lack of a tool that enables a rapid and easy appli...|$|E
40|$|<b>Touchless</b> <b>interaction</b> has {{recently}} gained considerable attention by researchers {{as well as}} industry. Different domains are interested in implementing this technology in their solutions. Medical visualization has {{a special interest in}} this technology due to the sterile conditions in operating rooms. Exploration and detailed inspection of the scanned objects are among the most common interactions performed by professionals. These operations become more challenging when combined with touchless input. Context-aware methods exist, which facilitate navigation, but these methods are made for meshes and not for volume renderings. Hence the research question: Can these methods be extended to volume renderings and how well will they perform with <b>touchless</b> <b>interaction</b> metaphors? Metaphor and underlying VolCam algorithm are presented in this work. The metaphor allows users to perform exploration and inspection tasks on medical volume data using touchless input device - LeapMotion. The VolCam - an extension of the ShellCam algorithm, automatically maps the user input to distinct camera movements based on the current scene view by sampling the visible part of the volume. Interactive frame rates are achieved by performing computations on GPU. No pre-processing or specialized data structures are required which makes the technique directly applicable to wide-range of volume datasets. 3 JECTO...|$|E
40|$|Our {{research}} aims at promoting {{communication skills}} of medium functioning autistic children by combining four main ingredients: multimedia storytelling, <b>touchless</b> interfaces, human-robots <b>interaction,</b> and Augmentative Alternative Communication (AAC). Author Keywords Children; complex communication needs; autism; touchless interaction; robots Context Autism is a pervasive developmental disorder that {{is marked by}} the presence of impaired social interaction and communication and a restricted repertoire of activities and interests. The prevalence of autism has been estimated to affect 1 of every 88 children and is five times more common in boys than in girls (CDC, 2013). Children with autism show a great variance of symptoms, ranging from severe impairment in the use of nonverbal behaviors that regulate social interaction to a failure to develop peer relationships appropriate to age. Their impairment in communication is also marked and sustained and can affect both verbal and nonverbal skills. Children with autism may have a delay in or a total lack of spoken language. In children who d...|$|R
40|$|This {{special issue}} {{includes}} eight original works that detail the further developments of ELMs in theories, applications, and hardware implementation. In "Representational Learning with ELMs for Big Data," Liyanaarachchi Lekamalage Chamara Kasun, Hongming Zhou, Guang-Bin Huang, and Chi Man Vong propose using the ELM as an auto-encoder for learning feature representations using singular values. In "A Secure and Practical Mechanism for Outsourcing ELMs in Cloud Computing," Jiarun Lin, Jianping Yin, Zhiping Cai, Qiang Liu, Kuan Li, and Victor C. M. Leung propose {{a method for}} handling large data applications by outsourcing to the cloud that would dramatically reduce ELM training time. In "ELM-Guided Memetic Computation for Vehicle Routing," Liang Feng, Yew-Soon Ong, and Meng-Hiot Lim consider the ELM as an engine for automating the encapsulation of knowledge memes from past problem-solving experiences. In "ELMVIS: A Nonlinear Visualization Technique Using Random Permutations and ELMs," Anton Akusok, Amaury Lendasse, Rui Nian, and Yoan Miche propose an ELM method for data visualization based on random permutations to map original data and their corresponding visualization points. In "Combining ELMs with Random Projections," Paolo Gastaldo, Rodolfo Zunino, Erik Cambria, and Sergio Decherchi analyze the relationships between ELM feature-mapping schemas and the paradigm of random projections. In "Reduced ELMs for Causal Relation Extraction from Unstructured Text," Xuefeng Yang and Kezhi Mao propose combining ELMs with neuron selection to optimize the neural network architecture and improve the ELM ensemble's computational efficiency. In "A System for Signature Verification Based on Horizontal and Vertical Components in Hand Gestures," Beom-Seok Oh, Jehyoung Jeon, Kar-Ann Toh, Andrew Beng Jin Teoh, and Jaihie Kim propose a novel paradigm for hand signature biometry- for touchless applications {{without the need for}} handheld devices. Finally, in "An Adaptive and Iterative Online Sequential ELM-Based Multi-Degree-of-Freedom Gesture Recognition System," Hanchao Yu, Yiqiang Chen, Junfa Liu, and Guang-Bin Huang propose an online sequential ELM-based efficient gesture recognition algorithm for <b>touchless</b> human-machine <b>interaction...</b>|$|R
40|$|Electron Microscopes are {{sensitive}} to noise of all kinds : Alternating Current (AC) Magnetic Fields, acoustic disturbances, temperature fluctuations and/or disturbances caused by rapid air flow in clean rooms. Therefore, existing and potential room locations need quick and reliable characterization methods for all these parameters. AC noise, in particular, is characterized today by either specialized instruments and handheld meters dedicated to the purpose. Our aim {{is to find a}} suitable substitute for these instruments by making an application that uses the in-built magnetometer in smartphones to measure electromagnetic fields. Our research is motivated by the fact that smartphones with considerable computing ability have become ubiquitous enough that they become a viable alternative to some dedicated characterization instruments. Furthermore, a method to characterize these fields can also extend to other applications, such as finding and mapping electrical faults, and <b>touchless</b> 3 -D <b>interaction.</b> We will discuss the development process of an iPhone application for quick analysis and recording of electromagnetic noise in equipment rooms for the installation of sensitive equipment. This will cover the suitability of the hardware to the purpose, the sensitivity of the hardware to our measurement criteria, our methods for data analysis and the associated software visualization techniques, and comparison to existing methods. Finally, we cover the limitations and potential applications of the software developed. 2018 - 05 - 2...|$|R
30|$|Interventional {{radiology}} is {{a specific}} process with specific needs, {{the most important of}} which is maintaining the sterility of the operating site while manipulating the images. Ideally, the operation has to be autonomic for at least basic features such as selecting series, reformatting, slicing, and pan and zoom manipulation. Some have proposed taking a mouse or trackpad inside the sterile protected area, or even using a tablet PC to visualize images. However, the most efficient setup in these conditions is <b>touchless</b> <b>interaction</b> [14], which will minimize the risk of contamination.|$|E
40|$|While {{surgical}} {{practices are}} increasingly reliant {{on a range}} of digital imaging technologies, the ability for clinicians to interact and manipulate these digital representations in the operating theatre using traditional touch based interaction devices is constrained by the need to maintain sterility. To overcome these concerns with sterility, a number of researchers are have been developing ways of enabling interaction in the operating theatre using <b>touchless</b> <b>interaction</b> techniques such as gesture and voice to allow clinicians control of the systems. While there have been important technical strides in the area, there has been {{little in the way of}} understanding the use of these touchless systems in practice. With this in mind we present a touchless system developed for use during vascular surgery. We deployed the system in the endovascular suite of a large hospital for use in the context of real procedures. We present findings from a study of the system in use focusing on how, with <b>touchless</b> <b>interaction,</b> the visual resources were embedded and made meaningful in the collaborative practices of surgery. In particular we discuss the importance of direct and dynamic control of the images by the clinicians in the context of talk and in the context of other artefact use as well as the work performed by members of the clinical team to make themselves sensable by the system. We discuss the broader implications of these findings for how we think about the design, evaluation and use of these systems...|$|E
40|$|Large, {{high-resolution}} displays enable efficient {{visualization of}} large datasets. To interact with these large datasets, touchless interfaces can support fluid interaction at different distances from the display. Touchless gestures, however, lack haptic feedback. Hence, users ’ gestures may unintentionally move off the interface elements and require additional physical effort to perform intended actions. To address this problem, we propose data-morphed topographies for touchless interactions: constraints on users ’ cursor movements that guide <b>touchless</b> <b>interaction</b> along {{the structure of}} the visualized data. To exemplify the potential of our concept, we envision applying three data-morphed topographies—holes, pits, and valleys— to common problem-solving tasks in visual analytics...|$|E
40|$|Touchless user {{interfaces}} are a promising {{way to let}} passers-by interact with public displays, {{but they have to}} cope with complex social constellations of multiple and novice users. In this paper we discuss how public displays offering <b>touchless</b> <b>interaction</b> can subtly direct individual users and thereby actively shape audience constellations by visual stimuli, thus improving parallel usage and the dynamic of interaction. As a starting point we present our findings from a field study on how a display can direct its users and dissolve crowds by using dynamic visual signifiers. At the workshop we would like to discuss how further touchless applications could benefit from visually moderating their audience...|$|E
40|$|Abstract. In {{this paper}} {{we present a}} case study of context aware, on-site {{information}} retrieval using a computervision-based interaction on mobile phones with the goal of facilitating information access for urban commuters that use public transport. Our focus is on intuitive <b>touchless</b> <b>interaction</b> using contextual recognition of existing visual objects, such as signs and information plates. Object detection and localization are based on fast matching of local image features represented as Histogrammed Intensity Patches. We show that low-level image features can be learned, organized, and optimized for discrimination between similar, poorly textured object images. The system is integrated with existing software for information retrieval and experiments show that it achieves good performance in real-life situations...|$|E
40|$|Recent {{years have}} seen the {{possibilities}} of new imaging and interaction technologies for minimally invasive surgery such as <b>touchless</b> <b>interaction</b> and high definition renderings of three-dimensional anatomy. With this paper we {{take a step back}} to review the historical introduction and assimilation of imaging technologies in the surgical theatre in parallel with the productive and cross-referential nature of surgical practice and image use. We present findings from a field study of image use during neurosurgery where we see that the work to see medical images is highly constructed and embodied with the action of manipulating the body. This perspective lends itself to a discussion of the directions for new imaging interaction technologies...|$|E
40|$|This {{contribution}} {{deals with}} preliminary studies for <b>touchless</b> <b>interaction</b> based on capacitive sensor technologies. An existing approach {{in combination with}} additional capacitance measurements is used for position estimation. For that, a measurement system is utilized which is able to measure capacitances {{in a range of}} a few fF to several pF. The main focus is the 3 D position estimation of a human finger through measuring the spatial capacitance distributions caused by its movements. A grounded metallic test object is used as an abstract model of a finger. The capacitance modeling is based on a 2 D simplification by considering properties of symmetry. This new model, combined with complex data processing algorithms, ensures precise finger gesture recognition in future research...|$|E
40|$|Touchless or empty-handed gestural input has {{received}} considerable attention {{during the last}} years because of such benefits as removing the burden of physical contact with an interactive system and making the interaction pleasurable. What is often overlooked is that those special forms Of <b>touchless</b> <b>interaction</b> which employ genuine gestures - defined as movements that have a meaning - {{are associated with the}} danger of suffering from the same drawbacks as command based interfaces do, which have been widely abandoned in favor of direct manipulation interfaces. Touchless direct manipulation, however, is about to reach maturity in certain application fields. In our paper WC try to point out why and Under which conditions this is going to happen, and how we are working to optimize the interfaces through user tests...|$|E
40|$|This {{proposal}} {{is aimed at}} illustrating a low cost exhibit of a projection-based city atlas. The exhibit could be replicated and reconfigured {{in accordance with the}} size of the mock-up and the spatial arrangement of the place where it will be located. The paper will be divided into different sections in order to explain and illustrate: {{the state of the art}} and previous experiences about spatial augmented geographycal representation; the technical issues regarding the projection set-up and the technological framework related to the <b>touchless</b> <b>interaction</b> system; the content design process. Within this framework, the experimentation has been carried out on the urban area of Ascoli Piceno (Italy). Based on the description of events occurring over time, the interactive virtual tour aims to highlight how the urban fabric has changed throughout the historical and political periods that have affected the city...|$|E
40|$|This paper {{presents}} the first very positive findings from an empirical study {{about the effectiveness}} of the use of a Kinect learning game for children with gross motor skills problems and motor impairments. This game follows the principles of a newly presented approach, called Kinems, which advocates that special educators and therapists should use learning games that via embodied <b>touchless</b> <b>interaction</b> – thanks to the Microsoft Kinect camera- children with dyspraxia and other related disorders such as autism, Asperger's Syndrome, and Attention Deficit Disorder, can improve related skills. Several Kinems games have been proposed ([URL] These games are innovative and are played with hand and body gestures. Kinems suggests that games should be highly configurable so that a teacher can modify the settings (e. g. difficult level, time settings, etc.) for the individual needs of each child. Also, a teacher should have access to kinetic and learning analytics of the child’s interaction progress and achievements should be safely stored and vividly presented...|$|E
40|$|Touchless {{interfaces}} allow {{users to}} view, control and manipulate digital content without physically touching an interface. They are being explored {{in a wide}} range of application scenarios from medical surgery to car dashboard controllers. One aspect of <b>touchless</b> <b>interaction</b> that has not been explored to date is the Sense of Agency (SoA). The SoA refers to the subjective experience of voluntary control over actions in the external world. In this paper, we investigated the SoA in touchless systems using the intentional binding paradigm. We first compare touchless systems with physical interactions and then augmented different types of haptic feedback to explore how different outcome modalities influence users’ SoA. From our experiments, we demonstrated that an intentional binding effect is observed in both physical and touchless interactions with no statistical difference. Additionally, we found that haptic and auditory feedback help to increase SoA compared with visual feedback in touchless interfaces. We discuss these findings and identify design opportunities that take agency into consideration...|$|E
40|$|The {{number and}} quality of {{smartphones}} on the market has dramatically raised lately. Researchers and developers are, thus, more and more pushed to bring algorithms and techniques from desktop environments to mobile platforms. One of the biggest constraints in mobile applications is the fine control of computing power and the relative power consumption. Although smartphones' manufactures are offering better computing performance and longer battery life, the mobile architecture is not always powerful enough. Furthermore, nowadays, the <b>touchless</b> <b>interaction</b> (e. g., the usage of voice commands) on mobile devices is particularly attractive. The device can also possibly answer to our questions (e. g., Siri-Speech Interpretation and Recognition Interface, which according to Apple is "the intelligent personal assistant that helps you get things done just by asking"). The use of talking avatars can {{improve the quality of}} the interaction and make it more useful and pleasant. Since avatars are static models, but the interaction requires dynamics, it is almost obliged to introduce avatars' animations...|$|E
