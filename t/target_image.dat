1159|1744|Public
5|$|The TLAM {{could be}} {{equipped}} with an inertial and terrain contour matching (TERCOM) radar guidance package to find and destroy its target. The TERCOM radar used a stored map reference to compare with the actual terrain to determine the missile's position. If necessary, a course correction was then made to place the missile on course to the target. Terminal guidance in the target area {{was provided by the}} optical Digital Scene Matching Area Correlation (DSMAC) system, which compared a stored image of target with the actual <b>target</b> <b>image.</b>|$|E
25|$|Content-based image retrievalfinding all {{images in}} a larger set of images which have a {{specific}} content. The content can be specified in different ways, for example in terms of similarity relative a <b>target</b> <b>image</b> (give me all images similar to image X), or in terms of high-level search criteria given as text input (give me all images which contains many houses, are taken during winter, and have no cars in them).|$|E
50|$|There are {{two kinds}} of mosaic, {{depending}} on how the matching is done. In the simpler kind, each part of the <b>target</b> <b>image</b> is averaged down to a single color. Each of the library images is also reduced to a single color. Each part of the <b>target</b> <b>image</b> is then replaced with one from the library where these colors are as similar as possible. In effect, the <b>target</b> <b>image</b> is reduced in resolution (by downsampling), and then each of the resulting pixels is replaced with an image whose average color matches that pixel.|$|E
5000|$|... #Caption: Spoke <b>target</b> <b>imaged</b> by a {{diffraction}} limited imaging system.|$|R
40|$|Abstract-An affine {{transformation}} model is established for ground-based photoelectric imaging and detecting systems. This model shows that affine transform consists of five basic transformations, which are translation, scale, slant and rotation transformations. Six affine invariant moments are given with geometrical moments. Furthermore. affine invariant moment vectors of training data are calculated for four-class simulated space <b>target</b> <b>images.</b> For different viewpoint angle <b>target</b> <b>images,</b> the affine invariant moments are calculated. Simulation experiments are performed for space targets classification and recognition through their eigenvectors comparing. Simulated {{results show that}} the method of <b>target</b> <b>images</b> classification recognition based on affine invariant moments is effective. Keywords-Space targets, Affine transform model, Affine invariant moments, Classification, Recognitio...|$|R
3000|$|... [...]) and one {{specifically}} for testing. At each sonar resolution {{and for each}} target, 80 synthetic <b>target</b> <b>images</b> at random ranges, random altitude and with a randomly selected seafloor {{have been used for}} training. A larger set of 40000 synthetic <b>target</b> <b>images</b> are used to test the classifier. The classifier is trained and tested according to the algorithm described in Section 3.2.|$|R
5000|$|In {{computer}} vision, the Hausdorff distance {{can be used}} to find a given template in {{an arbitrary}} <b>target</b> <b>image.</b> The template and image are often pre-processed via an edge detector giving a binary image. Next, each 1 (activated) point in the binary image of the template is treated as a point in a set, the [...] "shape" [...] of the template. Similarly, an area of the binary <b>target</b> <b>image</b> is treated as a set of points. The algorithm then tries to minimize the Hausdorff distance between the template and some area of the <b>target</b> <b>image.</b> The area in the <b>target</b> <b>image</b> with the minimal Hausdorff distance to the template, can be considered the best candidate for locating the template in the target.In computer graphics the Hausdorff distance is used to measure the difference between two different representations of the same 3D object particularly when generating level of detail for efficient display of complex 3D models.|$|E
50|$|In {{the more}} {{advanced}} kind of photographic mosaic, the <b>target</b> <b>image</b> is not downsampled, and the matching is done by comparing each pixel in the rectangle to the corresponding pixel from each library image. The rectangle in the target is then replaced with the library image that minimizes the total difference. This requires much more computation than the simple kind, but {{the results can be}} much better since the pixel-by-pixel matching can preserve the resolution of the <b>target</b> <b>image.</b>|$|E
5000|$|... more {{detailed}} <b>target</b> <b>image</b> allows targeting of more vulnerable parts of aircraft {{instead of just}} homing in on the brightest infrared source (exhaust).|$|E
5000|$|The jigdo-file utility is {{generally}} {{used to create}} the [...] ".jigdo" [...] and [...] ".template" [...] files needed to create <b>target</b> <b>images</b> using Jigdo.|$|R
40|$|Relevance {{feedback}} schemes using linear/quadratic estimators {{have been}} applied in content-based image retrieval to significantly improve retrieval performance. One major difficulty in relevance feedback is to estimate the support of <b>target</b> <b>images</b> in high dimensional feature space with {{a relatively small number}} of training samples. In this paper, we develop a novel scheme based on oneclass SVM, which fits a tight hyper-sphere in the nonlinearly transformed feature space to include most of the <b>target</b> <b>images</b> based on the positive examples. The use of kernel provides us an elegant way to deal with nonlinearity in the distribution of the <b>target</b> <b>images,</b> while the regularization term in SVM provides good generalization ability. To validate the efficacy of the proposed approach, we test it on both synthesized data and real-world images. Promising results are achieved in both cases. 1. 1 Background 1...|$|R
30|$|Interpolating between <b>target</b> <b>images</b> [9, 31]: {{the shape}} model {{is often used}} to {{regularize}} the computation of the optical flow between pixels of key images.|$|R
50|$|In Experiment 1 of Arnold et al., (2008), a {{white noise}} pattern and a <b>target</b> <b>image</b> {{switched}} between both eyes at selected constant rates. To test for optimal rates of BSS, images were switched between the both eyes at speeds ranging from 0.5 - 15 Hertz {{and the degree}} of suppression was measured. Participants were asked to indicate their response when they detect any part of the <b>target</b> <b>image.</b> Results showed that the optimal switch rates for BSS was of 1 Hertz and this is the rate which suppression was the longest.|$|E
50|$|Image {{registration}} {{is a process}} that searches for the correct alignment of images. In the simplest case, two images are aligned. Typically, one image is treated as the <b>target</b> <b>image</b> and the other is treated as a source image; the source image is transformed to match the <b>target</b> <b>image.</b> The optimization procedure updates the transformation of the source image based on a similarity value that evaluates the current quality of the alignment. This iterative procedure is repeated until a (local) optimum is found. An example is the registration of CT and PET images to combine structural and metabolic information (see figure).|$|E
5000|$|Image {{registration}} or image alignment algorithms can {{be classified}} into intensity-based and feature-based. One of the images {{is referred to as}} the reference or source and the others are referred to as the target, sensed or subject images. Image registration involves spatially transforming the source/reference image(s) to align with the <b>target</b> <b>image.</b> The reference frame in the <b>target</b> <b>image</b> is stationary, while the other datasets are transformed to match to the target. [...] Intensity-based methods compare intensity patterns in images via correlation metrics, while feature-based methods find correspondence between image features such as points, lines, and contours. Intensity-based methods register entire images or sub-images. If sub-images are registered, centers of corresponding sub images are treated as corresponding feature points. Feature-based methods establish a correspondence between a number of especially distinct points in images. Knowing the correspondence between a number of points in images, a geometrical transformation is then determined to map the <b>target</b> <b>image</b> to the reference images, thereby establishing point-by-point correspondence between the reference and target images.|$|E
5000|$|Steve Jiang, Program Manager, who was {{in charge}} of the first version of the Target Design, which is the product that enabled {{developers}} to build embedded <b>target</b> <b>images</b> ...|$|R
40|$|We {{propose a}} {{framework}} incorporating aspects of image classification {{to aid the}} matching of a reference <b>image</b> to <b>target</b> <b>images.</b> The framework involves an image representation based {{on a set of}} feature vectors, and a parametric distance measure on any two such vector sets. The distance measure may be optimized to provide maximum discrimination between the matching <b>target</b> <b>images</b> and the background images, when compared to the reference image. Preliminary results indicate that the new distance measure performs substantially better than the traditional SSD and the Bhattacharyya histogram measures in classification and tracking tasks. 1...|$|R
40|$|Abstract—Method for noise suppressing edge {{enhancement}} {{based on}} genetic algorithm {{taking into account}} complexity of <b>target</b> <b>images</b> measured with fractal dimension is proposed. Through experiments with satellite remote sensing imagery data with additive noise, {{it is found that}} the proposed method shows appropriate edge enhancing performance with suppressing the additive noise in accordance with complexity of <b>target</b> <b>images.</b> It is also found that the proposed method requires a small computer resources in comparison to the method based on Simulated Annealing: SA. Keywords—edge enhancement; fractal dimension; genetic algorithm; simulated annealing; remote sensing satellite imagery I...|$|R
50|$|The {{algorithm}} {{uses the}} difference between the current estimate of appearance and the <b>target</b> <b>image</b> to drive an optimization process.By taking advantage of the least squares techniques, it can match to new images very swiftly.|$|E
50|$|Continuous flash {{suppression}} (CFS), {{a technique}} which {{was developed by}} Tsuchiya and Koch in 2005, combined the effectiveness of binocular rivalry and flash suppression to minimise the randomness in rivalry during visual perception and suppression. In a typical CFS trial, the participants are shown two contrasting images, one to each eye {{and the image of}} greater signal strength will flash at a constant rate (7-10 Hertz). The <b>target</b> <b>image</b> will be shown to the other eye. Unlike Binocular Switch Suppression (BSS), the <b>target</b> <b>image</b> will remain stationary at the same spot on the screen and will not switch nor flash during the presentation period.|$|E
50|$|Outdoors, {{variables}} such as light, wind, temperature, humidity and mirage affect the <b>target</b> <b>image</b> and bullet trajectory. To help shooters, most ranges have wind flags placed at useful positions around the range to display the wind conditions.|$|E
40|$|This paper {{presents}} a novel image sequence color transfer algorithm (ISCT). It {{is able to}} render an image sequence with color characteristics borrowed from three user-given <b>target</b> <b>images.</b> The input of this algorithm consists of a single input image (I 1) and three <b>target</b> <b>images</b> (T 1, T 2, T 3). The output of the algorithm is an image sequence {Si} with N images that embeds itself with color mood variations. A user selects necessary parameters, such as N and a color variation curve (CVC), through a user friendly interface we have developed. The algorithm completes the task in three steps. First, it executes a color transfer algorithm using the input <b>image</b> and three <b>target</b> <b>images,</b> producing three basis images (B 1, B 2, B 3). Their color features are thus borrowed from the three <b>target</b> <b>images</b> (T 1, T 2, T 3). Given the user-defined CVCs and three basis images generated in the first step, the ISCT algorithm then interpolates the mean and variance values for N images. Finally, referring to the input image and values interpolated in the second step, the algorithm automatically generates an image sequence. This is achieved in several seconds by applying the color transfer algorithm individually for every interpolated value. In addition, we have developed a user interface which {{makes it possible to}} view the rendered image sequence. Experimental results show that our algorithm swiftly produces an image sequence containing color variation with little user intervention. Given only three <b>target</b> <b>images,</b> the novel ISCT algorithm can demonstrate its ability to produce an image sequence with color mood variation as well as visually plausible effects. This algorithm is automatic, effective, and expeditious, and is appropriate for many applications...|$|R
3000|$|In this section, {{we define}} source and <b>target</b> <b>images</b> as random {{variables}} and denote them as S and T instead of S(x) and T(x). The following notation is used: if μ [...]...|$|R
40|$|There are 23 <b>target</b> and clutter <b>images</b> {{which are}} saved in target. zip and clutter. zip, respectively. The size of each image is 128 × 128 pixels. You can choose any N(N = 6) images {{out of the}} given 23 <b>target</b> <b>images</b> as {{training}} images for the filter construction. The N images should cover all the range. Fig. 1 shows an example of picking 5 <b>target</b> training <b>images.</b> Fig. 2 shows the sample of clutter images. The process of creating LPCCF: Figure 1 : Training set of target face...|$|R
5000|$|... #Caption: In {{evaluating}} the ESF, an operator defines a box area equivalent to 10% {{of the total}} frame area of a knife-edge test target back-illuminated by a blackbody. The area is defined to encompass {{the edge of the}} <b>target</b> <b>image.</b>|$|E
50|$|Unknown target or antenna motion: Unmodeled motion {{will cause}} the <b>target</b> <b>image</b> to defocus and be at an {{incorrect}} location. This error is controlled by suitable mechanical design or {{by the use of}} auto-focus techniques. This error can be measured by the analytic signal phase measurement method described earlier.|$|E
50|$|Template {{matching}} is {{a central}} tool in Computational anatomy (CA).The deformable template model models the space of human anatomies is an orbit under the group action of diffeomorphisms.Template matching arise as a problem in matching the unknown diffeomorphism that acts on the template to match the <b>target</b> <b>image.</b>|$|E
40|$|The attentional biases of {{individuals}} with high {{and low levels of}} depression and anxiety were tested using the Attentional Blink paradigm. A rapid serial visual presentation (RSVP) task was used to detect biases in identification of emotionally valenced <b>target</b> <b>images.</b> The independent variables were depression, anxiety, lag of target stimulus, and emotional valence of <b>target</b> <b>images.</b> The dependent variables were accuracy, reaction times, and pupil dilation. As predicted, attentional biases were found for symptoms of both depression and anxiety, independently and co-morbidly, for dependent variables. The data suggest that there are both differences and similarities in the effects of symptoms of anxiety and depression on attentional biases around emotional stimuli...|$|R
40|$|Color {{transfer}} between images {{uses the}} statistics information of image effectively. We present {{a novel approach}} of local color transfer between images based on the simple statistics and locally linear embedding. A sketching interface is proposed for quickly and easily specifying the color correspondences between <b>target</b> and source <b>image.</b> The user can specify the correspondences of local region using scribes, which more accurately transfers the target color to the source image while smoothly preserving the boundaries, and exhibits more natural output results. Our algorithm is not restricted to one-to-one image color transfer and can make use {{of more than one}} <b>target</b> <b>images</b> to transfer the color in different regions in the source image. Moreover, our algorithm does not require to choose the same color style and image size between source and <b>target</b> <b>images.</b> We propose the sub-sampling to reduce the computational load. Comparing with other approaches, our algorithm is much better in color blending in the input data. Our approach preserves the other color details in the source image. Various experimental results show that our approach specifies the correspondences of local color region in source and <b>target</b> <b>images.</b> And it expresses the intention of users and generates more actual and natural results of visual effect. Comment: arXiv admin note: text overlap with arXiv: 1610. 0486...|$|R
40|$|We {{present a}} novel {{approach}} of color transfer between images by exploring their high-level semantic information. First, {{we set up a}} database which consists of the collection of downloaded images from the internet, which are segmented automatically by using matting techniques. We then, extract image foregrounds from both source and multiple <b>target</b> <b>images.</b> Then by using image matting algorithms, the system extracts the semantic information such as faces, lips, teeth, eyes, eyebrows, etc., from the extracted foregrounds of the source image. And, then the color is transferred between corresponding parts with the same semantic information. Next we get the color transferred result by seamlessly compositing different parts together using alpha blending. In the final step, we present an efficient method of color consistency to optimize the color of a collection of images showing the common scene. The main advantage of our method over existing techniques {{is that it does not}} need face matching, as one could use more than one <b>target</b> <b>images.</b> It is not restricted to head shot images as we can also change the color style in the wild. Moreover, our algorithm does not require to choose the same color style, same pose and image size between source and <b>target</b> <b>images.</b> Our algorithm is not restricted to one-to-one image color transfer and can make use of more than one <b>target</b> <b>images</b> to transfer the color in different parts in the source image. Comparing with other approaches, our algorithm is much better in color blending in the input data. Comment: 10 pages, 11 figure...|$|R
5000|$|The second {{category}} of transformations allow 'elastic' or 'nonrigid' transformations. These transformations {{are capable of}} locally warping the <b>target</b> <b>image</b> to align with the reference image. Nonrigid transformations include [...] radial basis functions (thin-plate or surface splines, multiquadrics, and compactly-supported transformations), physical continuum models (viscous fluids), and large deformation models (diffeomorphisms).|$|E
50|$|First, {{there is}} pure CCD or IIR (for low {{lighting}} conditions) image matching, when the guidance section uses algorithms {{in order to}} match the <b>target</b> <b>image</b> in its memory with the image provided by the seeker, and align {{the center of the}} seeker's FOV with the desired image (guidance method known as DSMAC).|$|E
50|$|Discrete {{optimization}} methods - {{the search}} space is quantized, and then image matching is addressed through label assignment at every pixel, {{such that the}} corresponding deformation minimizes {{the distance between the}} source and the <b>target</b> <b>image.</b> The optimal solution is often recovered through Max-flow min-cut theorem algorithms, linear programming or belief propagation methods.|$|E
25|$|On May 3, 2011, Dawn {{acquired}} {{its first}} <b>targeting</b> <b>image,</b> 1,200,000km from Vesta, and began its approach phase to the asteroid. On June 12, Dawn's speed relative to Vesta was slowed {{in preparation for}} its orbital insertion 34 days later.|$|R
40|$|International audienceHere {{we present}} a new {{approach}} of multiplexing and simultaneous encoding of <b>target</b> <b>images.</b> Our approach can enhance the encryption level of a classical double random phase (DRP) encryption system by adding a supplementary security layer. The new approach {{can be divided into}} two security layers. The first layer is called the multiplexing level, which consists in using iterative Fourier transformations along with several encryption key images. These latter can be a set of biometric images. At the second layer, we use a classical DRP system. The two layers enable us to encode several <b>target</b> <b>images</b> (multiencryption) and to reduce, at the same time, the requested decoded information (transmitted or storage information) ...|$|R
50|$|The seeker stays {{focused on}} the <b>target’s</b> <b>image</b> {{continuing}} to track it as the target moves or the missile’s flight path alters or as attack angles change. The seeker has three main components: focal plane array (FPA), cooling and calibration and stabilization.|$|R
