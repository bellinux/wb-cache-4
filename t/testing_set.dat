701|10000|Public
25|$|The GLM model {{does not}} take into account the {{contribution}} of relationships between multiple voxels. Whereas GLM analysis methods assess whether a voxel or region's signal amplitude is higher or lower for one condition than another, newer statistical models such as multi-voxel pattern analysis (MVPA), utilize the unique contributions of multiple voxels within a voxel-population. In a typical implementation, a classifier or more basic algorithm is trained to distinguish trials for different conditions within a subset of the data. The trained model is then tested by predicting the conditions of the remaining (independent) data. This approach is most typically achieved by training and testing on different scanner sessions or runs. If the classifier is linear, then the training model is a set of weights used to scale the value in each voxel before summing them to generate a single number that determines the condition for each <b>testing</b> <b>set</b> trial. More information on training and testing classifiers is at statistical classification.|$|E
50|$|Britain's DNA {{is one of}} a {{group of}} {{commercial}} companies providing DNA ancestry <b>testing,</b> <b>set</b> up by Moffat in 2012 and 2013; Scotland's DNA (the first), Ireland's DNA and Yorkshire's DNA.|$|E
50|$|In practice, early {{stopping}} {{is implemented}} by training on a training set and measuring accuracy on a statistically independent validation set. The model is trained until {{performance on the}} validation set no longer improves. The model is then tested on a <b>testing</b> <b>set.</b>|$|E
40|$|Building machine {{translation}} (MT) <b>test</b> <b>sets</b> {{is a relatively}} expensive task. As MT becomes increasingly desired {{for more and more}} language pairs and more and more domains, it becomes necessary to build <b>test</b> <b>sets</b> for each case. In this paper, we investigate using Amazon’s Mechanical Turk (MTurk) to make MT <b>test</b> <b>sets</b> cheaply. We find that MTurk {{can be used to make}} <b>test</b> <b>sets</b> much cheaper than professionally-produced <b>test</b> <b>sets.</b> More importantly, in experiments with multiple MT systems, we find that the MTurk-produced <b>test</b> <b>sets</b> yield essentially the same conclusions regarding system performance as the professionally-produced <b>test</b> <b>sets</b> yield. ...|$|R
40|$|We {{provide an}} {{empirical}} evaluation {{of one of}} the main <b>test</b> <b>sets</b> that is currently in use for testing modal satisfiability solvers, viz. the random modal QBF <b>test</b> <b>set.</b> We first discuss some of the background underlying the <b>test</b> <b>set,</b> and then evaluate the <b>test</b> <b>set</b> using criteria set forth by Horrocks, Patel-Schneider, and Sebastiani. We also present some guidelines {{for the use of the}} <b>test</b> <b>set...</b>|$|R
40|$|When a <b>test</b> <b>set</b> size {{is larger}} than desired, some {{patterns}} must be dropped. This paper presents a systematic method to reduce <b>test</b> <b>set</b> size; the method reorders a <b>test</b> <b>set</b> using the gate exhaustive test metric and truncates the <b>test</b> <b>set</b> to the desired size. To determine {{the effectiveness of the}} method, <b>test</b> <b>sets</b> with 1, 556 test patterns were applied to 140 defective Stanford ELF 18 test cores. The original <b>test</b> <b>set</b> required 758 <b>test</b> patterns to detect all defective cores, while the <b>test</b> <b>set</b> reordered using the presented method required 286 test patterns. The method also reduces the test application time for defective cores. 1...|$|R
50|$|One {{round of}} {{cross-validation}} involves partitioning {{a sample of}} data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or <b>testing</b> <b>set).</b> To reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to estimate a final predictive model.|$|E
5000|$|With {{construction}} {{completed and}} <b>testing</b> <b>set</b> to commence, the station applied for its license to cover on March 2, 2011. , this application was accepted for filing by the FCC and is awaiting further {{action by the}} commission. After several days of equipment testing and broadcasting under program test authority, the station began regular broadcasting as [...] "93-7 KHF; Your Music" [...] at 7:00 P.M. on March 25, 2011. The station was simulcast on WZZI, from June 2012 until November 2, 2012.|$|E
50|$|Set 3313/14 {{was used}} during {{acceptance}} testing on section 1 of High Speed 1 {{and in the}} process of over-speed <b>testing,</b> <b>set</b> a new UK rail speed record of 334.7 km/h in 2003. The set is named Entente Cordiale and has seen use as a VIP charter train, having transported the Queen on a state visit to France and to the Entente Cordiale anniversary celebrations in 2004. On 12 June 2007 the unit was used to carry International Olympic Committee inspectors from Stratford International to London St Pancras, as a demonstration for Olympic Javelin services in 2012.|$|E
40|$|We {{propose a}} new static <b>test</b> <b>set</b> {{compaction}} method {{based on a}} careful examination of attributes of fault coverage curves. Our method is based on two key ideas: (1) fault-list and testset partitioning, and (2) vector re-ordering. Typically, the first few vectors of a <b>test</b> <b>set</b> detect {{a large number of}} faults. The remaining vectors usually constitute a large fraction of the <b>test</b> <b>set,</b> but these vectors are included to detect relatively few hard faults. We show that significant compaction can be achieved by partitioning faults into hard and easy faults. This significantly reduces the computational cost for static <b>test</b> <b>set</b> compaction without affecting quality of compaction. The second technique re-orders vectors in a <b>test</b> <b>set</b> by moving sequences that detect hard faults {{to the beginning of the}} <b>test</b> <b>set.</b> Fault simulation of the newly concatenated re-ordered <b>test</b> <b>set</b> results in the omission of several vectors so that the compact <b>test</b> <b>set</b> is smaller than the original <b>test</b> <b>set.</b> Experiments on sev [...] ...|$|R
40|$|AbstractWe {{consider}} discrete matrices with distinct rows. A <b>test</b> <b>set</b> of {{a matrix}} is {{a subset of}} columns such that all the corresponding subrows are distinct. The essential <b>test</b> <b>set</b> of a matrix is the intersection of all the <b>test</b> <b>sets.</b> A relationship between {{the size of a}} matrix and the cardinality of the essential <b>test</b> <b>set</b> is derived. Also, we investigate matrices having essential <b>test</b> <b>sets</b> of maximum cardinality, and characterize a relationship of such matrices with trees...|$|R
40|$|Building machine {{translation}} (MT) <b>test</b> <b>sets</b> {{is a relatively}} expensive task. As MT becomes increasingly desired {{for more and more}} language pairs and more and more domains, it becomes necessary to build <b>test</b> <b>sets</b> for each case. In this paper, we investigate using Amazon's Mechanical Turk (MTurk) to make MT <b>test</b> <b>sets</b> cheaply. We find that MTurk {{can be used to make}} <b>test</b> <b>sets</b> much cheaper than professionally-produced <b>test</b> <b>sets.</b> More importantly, in experiments with multiple MT systems, we find that the MTurk-produced <b>test</b> <b>sets</b> yield essentially the same conclusions regarding system performance as the professionally-produced <b>test</b> <b>sets</b> yield. Comment: 4 pages, 2 tables; appeared in Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk, June 201...|$|R
5000|$|The {{baseline}} Leopard 2, sometimes informally {{called the}} [...] "A0" [...] to differentiate it from later versions, {{was the first}} series manufactured version. The vehicles were manufactured from October 1979 until March 1982, altogether 380 vehicles. 209 were built by Krauss Maffei and 171 by MaK. The basic equipment consisted of electrical-hydraulic stabilizer WNA-H22, a fire control computer, a laser rangefinder, a wind sensor, a general purpose telescope EMES 15, a panorama periscope PERI R17, the tower sight FERO Z18, on the tower roof {{as well as a}} computer controlled tank <b>testing</b> <b>set</b> RPP 1-8. 200 of the vehicles had a low-light enhancer (PZB 200) instead of a thermal imaging. Two chassis served as driver training vehicles.|$|E
50|$|The GLM model {{does not}} take into account the {{contribution}} of relationships between multiple voxels. Whereas GLM analysis methods assess whether a voxel or region's signal amplitude is higher or lower for one condition than another, newer statistical models such as multi-voxel pattern analysis (MVPA), utilize the unique contributions of multiple voxels within a voxel-population. In a typical implementation, a classifier or more basic algorithm is trained to distinguish trials for different conditions within a subset of the data. The trained model is then tested by predicting the conditions of the remaining (independent) data. This approach is most typically achieved by training and testing on different scanner sessions or runs. If the classifier is linear, then the training model is a set of weights used to scale the value in each voxel before summing them to generate a single number that determines the condition for each <b>testing</b> <b>set</b> trial. More information on training and testing classifiers is at statistical classification.|$|E
30|$|The {{data set}} {{was divided into}} 2 subsets i.e., {{training}} set and <b>testing</b> <b>set</b> by means of leave-one-out cross validation (LOO-CV). Conceptually, one sample {{was removed from the}} whole data set (N) and were used as the <b>testing</b> <b>set</b> while the remaining samples (N- 1) were used as the training set. The same process was continued until every sample in the data set was iteratively used as the <b>testing</b> <b>set</b> to predict Y variable.|$|E
40|$|We {{consider}} {{the relationship between}} test data compression {{and the ability to}} perform comprehensive testing of a circuit under an n-detection <b>test</b> <b>set.</b> The size of an n-detection <b>test</b> <b>set</b> grows approximately linearly with n. Therefore, one may expect a decompresser that can decompress a compressed n-detection <b>test</b> <b>set</b> to be larger than a decompresser required for a compact conventional <b>test</b> <b>set.</b> The results presented in this work demonstrate {{that it is possible to}} use a decompresser designed based on a compact one-detection <b>test</b> <b>set</b> in order to apply an n-detection <b>test</b> <b>set.</b> Thus, the design of the decompresser does not have to be changed as n is increased. We describe a procedure that generates an n-detection <b>test</b> <b>set</b> to achieve this result...|$|R
40|$|An {{important}} question in software testing {{is whether it}} is reasonable to apply coverage based criteria as a filter {{to reduce the size of}} a <b>test</b> <b>set.</b> An empirical study was conducted using a <b>test</b> <b>set</b> minimization technique to explore the effect of reducing the size of a <b>test</b> <b>set,</b> while keeping block coverage constant, on the fault detection strength of the resulting minimized <b>test</b> <b>set.</b> Two types of <b>test</b> <b>sets</b> were examined. For those with respect to a fixed size, no test case screening was conducted during the generation, whereas for those with respect to a fixed coverage, each subsequent test case had to improve the overall coverage in order to be included. The study reveals that no matter how a <b>test</b> <b>set</b> is generated (with or without any test case screening) block minimized <b>test</b> <b>sets</b> have a size/effectiveness advantage, in terms of a significant reduction in <b>test</b> <b>set</b> size but with almost the same fault detection effectiveness, over the original non-minimized <b>test</b> <b>sets.</b> Keywords: Block c [...] ...|$|R
30|$|All {{the models}} were then divided into {{training}} and <b>test</b> <b>set</b> by randomly selecting around 20 % of the compounds in the <b>test</b> <b>set.</b> Two independent <b>test</b> <b>sets</b> were constructed {{to rule out}} chance correlation (statistical data for the second <b>test</b> <b>set</b> is reported in Additional file 1 Table S 83). Both the <b>test</b> <b>sets</b> showed the similar statistical performance indicating that the developed models are adequate. Final QSAR models were generated within the training set, and {{they were used to}} predict the activity of <b>test</b> <b>set</b> of compounds. The lower average residual obtained in both the training and <b>test</b> <b>set</b> of compounds in all the models indicate that the developed models are valuable and have capability to establish the relationship between the structure and activity for various anti-cancer scaffolds used in this study.|$|R
3000|$|... -test. We {{afterwards}} randomly draw {{a number}} of samples {{to be used as}} the training set and employed the rest as a <b>testing</b> <b>set.</b> The number of training points are chosen to be small to keep the small sample setting, and to have a large enough <b>testing</b> <b>set.</b> This was repeated [...]...|$|E
30|$|As {{mentioned}} in Section 3, we considered that the adversary has already built a user profile for each user. Consequently, we split each dataset in two parts: a training set {{used to build}} the user profiles, and a <b>testing</b> <b>set</b> {{used to assess the}} robustness of the considered privacy-preserving mechanism. We used two third of user queries to create the training set and the remaining third of queries to create the <b>testing</b> <b>set.</b> We used two third of user queries to create the training set, and the remaining third of queries to create the <b>testing</b> <b>set.</b>|$|E
30|$|The {{data from}} January 2013 to December 2013 are {{employed}} as first fold of <b>testing</b> <b>set.</b>|$|E
40|$|An {{experimental}} {{comparison of}} the effectiveness of the all-uses and all-edges test data adequacy criteria was performed. A large number of <b>test</b> <b>sets</b> was randomly generated for each of nine subject programs with sub-tle errors. For each <b>test</b> <b>set,</b> the percentages of (exe-cutable) edges and definition-use associations covered were measured and it was determined whether the <b>test</b> <b>set</b> exposed an error. Hypothesis testing was used to investigate whether all-uses adequate <b>test</b> <b>sets</b> are more likely to expose errors than are all-edges adequate <b>test</b> <b>sets.</b> All-uses was shown to be significantly more effec-tive than all-edges for five of the subjects; moreover, for four of these, all-uses appeared to guarantee detection of the error. Further analysis showed that in four subjects, all-uses adequate <b>test</b> <b>sets</b> appeared to be more effective than all-edges adequate <b>test</b> <b>sets</b> of the same size. Lo-gistic regression showed that in some, but not all of the subjects there was a strong positive correlation between the percentage of definition-use associations covered by a <b>test</b> <b>set</b> and its error-exposing ability. 1...|$|R
3000|$|... stage II: <b>test</b> <b>set</b> → {{preprocessing}} → {{feature extraction}} → Recognition using trained model → output classes of <b>test</b> <b>set.</b>|$|R
40|$|It was {{recently}} observed that, {{in order to}} improve the defect coverage of a <b>test</b> <b>set,</b> <b>test</b> generation based on fault models such as the single-line stuck-at model may need to be augmented so as to derive <b>test</b> <b>sets</b> that detect each modeled fault more than once. In this work, we report on test pattern generators for combinational circuits that generate <b>test</b> <b>sets</b> to detect each single line stuck-at fault a given number of times. Additionally, we study the effects of <b>test</b> <b>set</b> compaction on the defect coverage of such <b>test</b> <b>sets.</b> For the purpose of experimentation, defect coverage is measured by the coverage of surrogate faults, using a framework proposed earlier. Within this framework, we show that the defect coverage {{does not have to be}} sacrificed by test compaction if the <b>test</b> <b>set</b> is computed using appropriate test generation objectives. Moreover, two <b>test</b> <b>sets</b> generated using the same test generation objectives, except that compaction heuristics were used during the generation of one but not the other, typically have similar defect coverages, even if the compacted <b>test</b> <b>set</b> is significantly smaller than the noncompacted one. Test generation procedures and experimental results to support these claims are presented...|$|R
30|$|The {{data from}} January 2013 to July 2016 are {{employed}} as fourth fold of <b>testing</b> <b>set.</b>|$|E
30|$|We {{randomly}} {{divided the}} entire dataset into two disjoint subsets {{with the same}} size (i.e., 31, 923 subjects), that is, a training set and <b>testing</b> <b>set,</b> where the training set contains 15, 596 males and 16, 327 females, whereas the <b>testing</b> <b>set</b> contains 15, 497 males and 16, 426 females 2, with a similar distribution {{as that of the}} entire dataset for both subsets.|$|E
30|$|As a consequence, the {{database}} consists of 4000 instances, each with the post-disturbance generator voltage trajectories as input and the transient stability status as output. The 4000 instances are then randomly {{divided into two}} different sets, one serves as training set, whereas the other as <b>testing</b> <b>set.</b> The training set occupies 87.5 % and the <b>testing</b> <b>set</b> occupies 12.5 % of all the instances.|$|E
30|$|Figure 4 reports Kappa index {{calculated}} for K-fold <b>test</b> <b>set,</b> validation <b>test</b> <b>set</b> and full <b>test</b> <b>sets.</b> Comparing the boxplots {{of the three}} validation methods, it is clear how their values grow proportionally with the training subset size from 2 % to 20 %. The variance decreases {{with the increase of}} the training subset size. In the K-fold cross validation <b>test</b> <b>set,</b> the results range from 80 to 84, and the SVM performs better than RF. Using the validation <b>test</b> <b>set,</b> the results are similar, but values are, as expected, lower, ranging from 48 to 49.5. In addition, in this case the SVM is better in comparison with RF. When validating against the full <b>test</b> <b>set,</b> the RF returns a better result than SVM when training with more than 10 % of the full <b>test</b> <b>set.</b> The Kappa values range from 48 to 51. RF rises gradually from 48.5 to 50.5, whereas the SVM remains stable around 49.5.|$|R
30|$|For {{the adult}} <b>test</b> <b>set,</b> speech utterances from 27 {{speakers}} (23 Males and 4 Females) {{in an age}} range of 30 – 45 (Average: 41.3) were chosen. For the older <b>test</b> <b>set,</b> speech data from 12 speakers (10 Males and 2 Females) in the age range of 60 – 85 (Average: 68.4) were used. The speaker <b>set</b> used for <b>testing</b> is disjoint from the training set speakers. 10 utterances (about 130 seconds on average) for each test speaker were kept aside for speaker adaptation and the remaining utterances formed the <b>test</b> <b>set.</b> In all the adult <b>test</b> <b>set</b> comprises of 4323 utterances (12.5 hours) and the older <b>test</b> <b>set</b> comprises of 6410 utterances (18 hours). The perplexity [22] of the language model on the adult <b>test</b> <b>set</b> is 178.3 with Out Of Vocabulary (OOV) rate of 3.8 % and on the older <b>test</b> <b>set</b> is 169.7 with OOV rate of 4.3 %.|$|R
40|$|This paper {{describes}} an experiment {{to measure the}} playing strength of Shogi players, man or machine, using a <b>test</b> <b>set</b> with 48 next-move problems. Each problem in the <b>test</b> <b>set</b> has been selected from games played by grandmasters, using some specific criteria. In Shogi {{there is no such}} rating system as the ELO system in chess to measure the playing strength of players and there was hitherto no <b>test</b> <b>set</b> for Shogi computers like the Bratko-Kopec <b>test</b> <b>set</b> for chess computers. In this paper, we first give a short sketch of <b>test</b> <b>sets</b> in use for chess computers, focusing on our interest in obtaining a similar <b>test</b> <b>set</b> for Shogi computers. Then our recent work on measuring the playing strength of Shogi computers using a newly devised <b>test</b> <b>set</b> is described, including some experiments performed with computer and human subjects. 1 Introduction It is very important to measure objectively the playing strength of computer programs, enabling easy comparison between computers and human players, encoura [...] ...|$|R
30|$|Generate a new Random Forest {{tree and}} {{supplementary}} analyze it using <b>testing</b> <b>set</b> (cross validation) {{to predict the}} model and its precision.|$|E
30|$|The patient groups (A and B) were {{partitioned}} randomly {{into two}} sets: training set (15 from A[*]+[*] 15 from B) and <b>testing</b> <b>set</b> (15 [*]+[*] 15). The <b>testing</b> <b>set</b> was {{separate from the}} training phase and was used only in the reporting of the results. An intracranial mask was used to limit the investigative volume. The manually drawn stroke volumes were considered as ground truths. The output consisted of a single label, i.e., the estimated infarct lesion.|$|E
30|$|After {{decreasing}} {{the dimensions of}} the features through the SFS, the new feature set is forwarded to the LS_SVM classifier. In this study, we obtain a feature set that has 2000 data points of 35 dimensions. These features are divided into two groups, which are the training set and the <b>testing</b> <b>set.</b> The training set is directed to train a classifier. The <b>testing</b> <b>set</b> is employed to evaluate the performance of the methodology and it is utilized as the input of the classifier.|$|E
40|$|In this paper, a {{combinatorial}} auction {{problem is}} modeled as a NP-complete set packing problem and a Lagrangian relaxation based heuristic algorithm is proposed. Extensive experiments are conducted using benchmark CATS <b>test</b> <b>sets</b> {{and more complex}} <b>test</b> <b>sets.</b> The algorithm provides optimal solutions for most <b>test</b> <b>sets</b> and is always 1 % from the optimal solutions for all CATS <b>test</b> <b>sets.</b> Comparisons with CPLEX 8. 0 are also provided, which show that the algorithm provides good solutions. ...|$|R
40|$|Redundant <b>test</b> <b>set</b> {{is one of}} focuses {{in recent}} {{bioinformatics}} research. The author investigates the phenomenon of ”differentiation repetition ” of (redundant) <b>test</b> <b>set</b> and utilizes derandomization method to give deeper approximability results of redundant <b>test</b> <b>set</b> conditionally better than directly derived from set cover. Set cover greedy algorithm (SGA for short) is a common used algorithm for (redundant) <b>test</b> <b>set.</b> This paper proves approximation ratio of SGA for odd redundancy r ≥ 3 can be (2 −...|$|R
30|$|The {{training}} material comes from 8 male and 8 female speakers {{and consists of}} 6664 clean tokens, after removing unusable tokens identified during postprocessing. The tokens from the remaining 8 speakers are provided in 7 <b>test</b> <b>sets</b> employing different types of noise. We combined all <b>test</b> <b>sets</b> as one large <b>test</b> <b>set</b> of clean tokens. For this, the clean speech signals were extracted from the two-channel material that contains speech in one channel and noise in the other channel. Each of the 7 <b>test</b> <b>sets</b> contains 16 instances {{of each of the}} 24 consonants, giving a total of 2688 tokens in the combined <b>test</b> <b>set.</b>|$|R
