144|70|Public
5000|$|To {{test their}} hypothesis, McRae and Hetherington (1993) {{compared}} {{the performance of}} a naïve and pre-trained auto-encoder backpropagation network on three simulations of verbal learning tasks. <b>The</b> <b>pre-trained</b> network was trained using letter based representations of English monosyllabic words or English word pairs. All three tasks involved the learning of some consonant-vowel-consonant (CVC) strings or CVC pairs (list A), followed by training on a second list of these items (list B). Afterwards, the distributions of the hidden node activations were compared between the naïve and pre-trained network. In all three tasks, the representations of a CVC in the naïve network tended to be spread fairly evenly across all hidden nodes, whereas most hidden nodes were inactive in <b>the</b> <b>pre-trained</b> network. Furthermore, in <b>the</b> <b>pre-trained</b> network the representational overlap between CVCs was reduced compared to the naïve network. <b>The</b> <b>pre-trained</b> network also retained some similarity information as the representational overlap between similar CVCs, like [...] "JEP" [...] and [...] "ZEP", was greater than for dissimilar CVCs, such as [...] "JEP" [...] and [...] "YUG". This suggests that <b>the</b> <b>pre-trained</b> network had a better ability to generalize, i.e. notice the patterns, than the naïve network. Most importantly, this reduction in hidden unit activation and representational overlap resulted in significantly less forgetting in <b>the</b> <b>pre-trained</b> network than the naïve network, essentially eliminating catastrophic interference. Essentially, the pre-training acted to create internal orthogonalization of the activations at the hidden layer, which reduced interference. Thus, pre-training is a simple way to reduce catastrophic forgetting in standard backpropagation networks.|$|E
50|$|Related to {{multi-task}} {{learning is}} the concept of knowledge transfer. Whereas traditional multi-task learning implies that a shared representation is developed concurrently across tasks, transfer of knowledge implies a sequentially shared representation. Large scale machine learning projects such as the deep convolutional neural network GoogLeNet, an image-based object classifier, can develop robust representations which may be useful to further algorithms learning related tasks. For example, <b>the</b> <b>pre-trained</b> model {{can be used as a}} feature extractor to perform pre-processing for another learning algorithm. Or <b>the</b> <b>pre-trained</b> model can be used to initialize a model with similar architecture which is then fine-tuned to learn a different classification task.|$|E
30|$|Step 1 : Randomly {{select a}} batch of {{training}} data as input to <b>the</b> <b>pre-trained</b> architecture.|$|E
3000|$|... where A_ν^f∈R^I'× I' and b_ν^ f∈R^I' are <b>the</b> νth <b>pre-trained</b> {{basis of}} an fMLLR {{transform}} matrix and bias term, respectively, which are estimated from entire training data. For evaluation, only their weights π [...]...|$|R
40|$|In this work, we {{implement}} a {{deep neural network}} for the text-to-speech system. We have tried different parameter settings for the DNN layers and units, and find that the three-layer DNN works better than the four-layer ones. We also <b>pre-trained</b> <b>the</b> best three-layer system (1000 - 1000 - 1000), and both objective and subjective test results show significant improvement in the synthesizing quality after pre-training. <b>The</b> final <b>pre-trained</b> system obtains an average linear spectral pair (LSP) {{root mean square error}} (RMSE) of 0. 179096, beating the DNN-TTS benchmark of 0. 187225...|$|R
30|$|Each of <b>the</b> four <b>pre-trained</b> {{classifiers}} of <b>the</b> OpenCV and Viola-Jones algorithm [16] {{were applied}} {{to each of the}} data sets: OpenCV 1 (uses the haarcascade_frontalface_default.xml), OpenCV 2 (haarcascade_frontalface_alt.xml), OpenCV 3 : (haarcascade_frontalface_alt 2.xml), and OpenCV 4 (haarcascade_profileface.xml. The profile face detector (OpenCV 4) was not applied to the BioID and MUCT datasets as these images only contain front-on faces. As per the OpenCV documentation, the default parameters for each detector were used. The Dlib [35] HOG-based face detector was also applied to each of the datasets using default values as per the Dlib documentation.|$|R
30|$|Step 2 : Remove the {{classification}} layer from <b>the</b> <b>pre-trained</b> off-the-shelf deep model {{and preserve the}} rest parts as the initial model of RGB and NIR data.|$|E
30|$|During testing we {{evaluate}} <b>the</b> <b>pre-trained</b> classifier with test examples. The average precision {{result for}} this experiment over all crowd behavior classes {{is presented in}} second column of Table  5.|$|E
40|$|In this paper, {{we present}} a new {{automatic}} diagnosis method of facial acne vulgaris based on convolutional neural network. This method is proposed to overcome the shortcoming of classification types in previous methods. The core of our method is to extract features of images based on convolutional neural network and achieve classification by classifier. We design a binary classifier of skin-and-non-skin to detect skin area and a seven-classifier to achieve the classification of facial acne vulgaris and healthy skin. In the experiment, we compared the effectiveness of our convolutional neural network and <b>the</b> <b>pre-trained</b> VGG 16 neural network on the ImageNet dataset. And we use the ROC curve and normal confusion matrix to evaluate {{the performance of the}} binary classifier and the seven-classifier. The results of our experiment show that <b>the</b> <b>pre-trained</b> VGG 16 neural network is more effective in extracting image features. The classifiers based on <b>the</b> <b>pre-trained</b> VGG 16 neural network achieve the skin detection and acne classification and have good robustness. Comment: 12 pages, 7 figures, 5 table...|$|E
30|$|A more {{complicated}} simulated environment was also constructed {{as shown in}} Fig. 7. More obstacles were located to this simulated environment. However, {{no matter how long}} the control network was trained in this new environment, the exploration could not be accomplished. Even several more layers were added to the control network to improve the nonlinearity for complicated models, it sill could not converge. It is possible that <b>the</b> fixed <b>pre-trained</b> perception network limits the extension of the whole model. As mentioned before, the overfitting of the supervised learning model might not represent the new environment in Fig. 7 good enough.|$|R
3000|$|In {{contrast}} to the other components of the SE front-end, DOLPHIN is based on non-linear processing, i.e., it processes speech based on frame-wise spectral modification possibly introducing time-varying distortions that {{may be difficult for}} the ASR back-end to cope with. However, <b>the</b> use of <b>pre-trained</b> spectral models of speech ensures that the characteristics of the processed speech remain close to that of clean speech. 2 [...]...|$|R
40|$|Abstract — speech {{separation}} as {{a binary}} classification {{problem has been}} shown to be effective. A good separation performance is achieved in matched test conditions using kernel SVM it does not perform mismatch conditions, it support small data sets only. Linear SVM perform both matched and unmatched conditions. DNN-SVM system separates the speech in variety acoustic conditions within a reasonable amount of time and also supports large data sets. <b>The</b> standard <b>pre-trained</b> deep neural networks are used for feature learning. Systematic evaluations show that the system produces very promising results under various test mixture show good generalization to new utterances, background noise and unseen speaker. Index Terms — support vector machines, DNN-SVM, computational auditory scene analysis (CASA) I...|$|R
3000|$|... {{which is}} a set of {{coefficients}} learned from the raw feature x. In our implementation, we use this observation and represent φ(x) as the score output of <b>the</b> <b>pre-trained</b> linear SVM instead of keeping it as a high-dimensional feature vector. As a result, W [...]...|$|E
30|$|Generally, since a {{large number}} of trainable {{parameters}} should be learned for CNN, an effective model requires lots of training samples. If we train a CNN model with insufficient training samples, it would lead to overfitting. To address the problem, we train the siamese network by fine-tuning <b>the</b> <b>pre-trained</b> CNN model.|$|E
30|$|Where the ϕl {{denotes the}} {{activation}} at the lth layer of <b>the</b> <b>pre-trained</b> feature extracting network. In the paper, we choose the Conv 1 - 2, Conv 2 - 2, Conv 3 - 2, Conv 4 - 2, and Conv 5 - 2 layers {{to acquire the}} features and compute the perceptual loss.|$|E
40|$|We {{investigate}} {{the task of}} assessing sentence-level prompt relevance in learner essays. Various systems using word overlap, neural embeddings and neural compositional models are evaluated on two datasets of learner writing. We propose a new method for sentence-level similarity calculation, which learns to adjust <b>the</b> weights of <b>pre-trained</b> word embeddings for a specific task, achieving substantially higher accuracy compared to other relevant baselines. Comment: Accepted for publication at BEA- 201...|$|R
40|$|Deep belief network (DBN) {{has been}} shown to be a good {{generative}} model in tasks such as hand-written digit image generation. Previous work on DBN in the speech community mainly focuses on using <b>the</b> generatively <b>pre-trained</b> DBN to initialize a discriminative model for better acoustic modeling in speech recognition (SR). To fully utilize its generative nature, we propose to model the speech parameters including spectrum and F 0 simultaneously and generate these parameters from DBN for speech synthesis. Compared with the predominant HMM-based approach, objective evaluation shows that the spectrum generated from DBN has less distortion. Subjective results also confirm the advantage of the spectrum from DBN, and the overall quality is comparable to that of context-independent HMM. Index Terms — Speech synthesis, Deep belief network 1...|$|R
40|$|In this paper, {{instead of}} {{designing}} new fea-tures based on intuition, linguistic knowl-edge and domain, we learn some new and effective features using the deep auto-encoder (DAE) paradigm for phrase-based translation model. Using <b>the</b> unsupervised <b>pre-trained</b> deep belief net (DBN) to ini-tialize DAE’s parameters {{and using the}} in-put original phrase features as a teacher for semi-supervised fine-tuning, we learn new semi-supervised DAE features, which are more effective and stable than the unsuper-vised DBN features. Moreover, to learn high dimensional feature representation, we introduce a natural horizontal compo-sition of more DAEs for large hidden lay-ers feature learning. On two Chinese-English tasks, our semi-supervised DAE features obtain statistically significant im-provements of 1. 34 / 2. 45 (IWSLT) and 0. 82 / 1. 52 (NIST) BLEU points over the unsupervised DBN features and the base-line features, respectively. ...|$|R
40|$|This paper {{proposes a}} {{learning}} strategy that extracts object-part concepts from a pre-trained {{convolutional neural network}} (CNN), {{in an attempt to}} 1) explore explicit semantics hidden in CNN units and 2) gradually grow a semantically interpretable graphical model on <b>the</b> <b>pre-trained</b> CNN for hierarchical object understanding. Given part annotations on very few (e. g., 3 - 12) objects, our method mines certain latent patterns from <b>the</b> <b>pre-trained</b> CNN and associates them with different semantic parts. We use a four-layer And-Or graph to organize the mined latent patterns, so as to clarify their internal semantic hierarchy. Our method is guided by a small number of part annotations, and it achieves superior performance (about 13 %- 107 % improvement) in part center prediction on the PASCAL VOC and ImageNet datasets. Comment: in the Thirty-First AAAI Conference on Artificial Intelligence (AAAI- 17...|$|E
30|$|The {{training}} of the generator in the GAN architecture ensures that {{the output of the}} generator is located on the manifold of appliance load sequences with high probability. As we integrate <b>the</b> <b>pre-trained</b> generator to the disaggregation process, we force the output of the disaggregator to be located on the manifold of reasonably-shaped load sequences.|$|E
3000|$|... and [...] (s_ℓ^a=s^i,a,s_ℓ^b=s^j,b) [...]. The {{iterative}} algorithm of Hu and Wang also utilizes a {{more sophisticated}} soft masking procedure (with respect to the procedure discussed in Section 4.3) and hence yields good SAR and SDR. The iterative adaptation of <b>the</b> <b>pre-trained</b> HMMs of the speakers might explain the good SIR performance of Hu and Wang algorithm.|$|E
40|$|The {{focus of}} past machine {{learning}} research for Reading Comprehension tasks has been {{primarily on the}} design of novel deep learning architectures. Here we show that seemingly minor choices made on (1) <b>the</b> use of <b>pre-trained</b> word embeddings, and (2) the representation of out-of-vocabulary tokens at test time, can turn out to have a larger impact than architectural choices on the final performance. We systematically explore several options for these choices, and provide recommendations to researchers working in this area...|$|R
30|$|Current {{automatic}} {{speech recognition}} (ASR) systems achieve over 90 – 95 % accuracy, depending on the methodology applied and datasets used. However, the level of accuracy decreases significantly when the same ASR system is used by a non-native speaker of the language to be recognized. At the same time, the volume of labeled datasets of non-native speech samples is extremely limited both in size and {{in the number of}} existing languages. This problem makes it difficult to train or build sufficiently accurate ASR systems targeted at non-native speakers, which, consequently, calls for a different approach that would make use of vast amounts of large unlabeled datasets. In this paper, we address this issue by employing dual supervised learning (DSL) and reinforcement learning with policy gradient methodology. We tested DSL in a warm-start approach, with two models trained beforehand, and in a semi warm-start approach with only one of <b>the</b> two models <b>pre-trained.</b> <b>The</b> experiments were conducted on English language pronounced by Japanese and Polish speakers. The results of our experiments show that creating ASR systems with DSL can achieve an accuracy comparable to traditional methods, while simultaneously making use of unlabeled data, which obviously is much cheaper to obtain and comes in larger sizes.|$|R
40|$|Glomerulus {{classification}} and detection in kidney tissue segments are key {{processes in}} nephropathology {{used for the}} correct diagnosis of the diseases. In this paper, {{we deal with the}} challenge of automating Glomerulus classification and detection from digitized kidney slide segments using a deep learning framework. The proposed method applies Convolutional Neural Networks (CNNs) between two classes: Glomerulus and Non-Glomerulus, to detect the image segments belonging to Glomerulus regions. We configure the CNN with <b>the</b> public <b>pre-trained</b> AlexNet model and adapt it to our system by learning from Glomerulus and Non-Glomerulus regions extracted from training slides. Once the model is trained, labeling is performed by applying the CNN classification to the image blocks under analysis. The results of the method indicate that this technique is suitable for correct Glomerulus detection in Whole Slide Images (WSI), showing robustness while reducing false positive and false negative detections...|$|R
40|$|Deep {{learning}} {{has dominated the}} computer vision field since 2012, but a common criticism of deep learning methods is their dependence on large amounts of data. To combat this criticism research into data-efficient deep learning is growing. The foremost success in data-efficient deep learning is transfer learning with networks pre-trained on the ImageNet dataset. Pre-trained networks have achieved state-of-the-art performance on many tasks. We consider <b>the</b> <b>pre-trained</b> network method for a new task {{where we have to}} collect the data. We hypothesize that the data efficiency of pre-trained networks can be improved through informed data collection. After exhaustive experiments on CaffeNet and VGG 16, we conclude that the data efficiency indeed can be improved. Furthermore, we investigate an alternative approach to data-efficient learning, namely adding domain knowledge {{in the form of a}} spatial transformer to <b>the</b> <b>pre-trained</b> networks. We find that spatial transformers are difficult to train and seem to not improve data efficiency...|$|E
40|$|Relatively small {{data sets}} {{available}} for expression recognition research make {{the training of}} deep networks for expression recognition very challenging. Although fine-tuning can partially alleviate the issue, the performance is still below acceptable levels as the deep features probably contain redun- dant information from <b>the</b> <b>pre-trained</b> domain. In this paper, we present FaceNet 2 ExpNet, a novel idea to train an expression recognition network based on static images. We first propose a new distribution function to model the high-level neurons of the expression network. Based on this, a two-stage training algorithm is carefully designed. In the pre-training stage, we train the convolutional layers of the expression net, regularized by the face net; In the refining stage, we append fully- connected layers to <b>the</b> <b>pre-trained</b> convolutional layers and train the whole network jointly. Visualization shows that the model trained with our method captures improved high-level expression semantics. Evaluations on four public expression databases, CK+, Oulu-CASIA, TFD, and SFEW demonstrate that our method achieves better results than state-of-the-art...|$|E
40|$|Automatically detecting, labeling, and {{tracking}} objects in videos depends {{first and foremost}} on accurate category-level object detectors. These might, however, not always be available in practice, as acquiring high-quality large scale labeled training datasets is either too costly or impractical for all possible real-world application scenarios. A scalable solution consists in re-using object detectors pre-trained on generic datasets. This work {{is the first to}} investigate the problem of on-line domain adaptation of object detectors for causal multi-object tracking (MOT). We propose to alleviate the dataset bias by adapting detectors from category to instances, and back: (i) we jointly learn all target models by adapting them from <b>the</b> <b>pre-trained</b> one, and (ii) we also adapt <b>the</b> <b>pre-trained</b> model on-line. We introduce an on-line multi-task learning algorithm to efficiently share parameters and reduce drift, while gradually improving recall. Our approach is applicable to any linear object detector, and we evaluate both cheap "mini-Fisher Vectors" and expensive "off-the-shelf" ConvNet features. We quantitatively measure the benefit of our domain adaptation strategy on the KITTI tracking benchmark and on a new dataset (PASCAL-to-KITTI) we introduce to study the domain mismatch problem in MOT. Comment: To appear at BMVC 201...|$|E
40|$|Designed as {{extremely}} deep architectures, deep residual networks {{which provide}} a rich visual representation and offer robust convergence behaviors have recently achieved exceptional performance in numerous computer vision problems. Being directly {{applied to a}} scene labeling problem, however, they were limited to capture long-range contextual dependence, which is a critical aspect. To address this issue, we propose a novel approach, Contextual Recurrent Residual Networks (CRRN) which is able to simultaneously handle rich visual representation learning and long-range context modeling within a fully end-to-end deep network. Furthermore, our proposed end-to-end CRRN is completely trained from scratch, without using any pre-trained models in contrast to most existing methods usually fine-tuned from <b>the</b> state-of-the-art <b>pre-trained</b> models, e. g. VGG- 16, ResNet, etc. The experiments are conducted on four challenging scene labeling datasets, i. e. SiftFlow, CamVid, Stanford background and SUN datasets, and compared against various state-of-the-art scene labeling methods...|$|R
40|$|This paper {{presents}} an image classification based approach for skeleton-based video action recognition problem. Firstly, A dataset independent translation-scale invariant image mapping method is proposed, which transformes the skeleton videos to colour images, named skeleton-images. Secondly, A multi-scale deep {{convolutional neural network}} (CNN) architecture is proposed which could be built and fine-tuned on <b>the</b> powerful <b>pre-trained</b> CNNs, e. g., AlexNet, VGGNet, ResNet etal [...] Even though the skeleton-images are very different from natural images, the fine-tune strategy still works well. At last, we prove that our method could also work well on 2 D skeleton video data. We achieve the state-of-the-art results on the popular benchmard datasets e. g. NTU RGB+D, UTD-MHAD, MSRC- 12, and G 3 D. Especially on the largest and challenge NTU RGB+D, UTD-MHAD, and MSRC- 12 dataset, our method outperforms other methods by a large margion, which proves the efficacy of the proposed method...|$|R
40|$|We review some of {{the most}} recent {{approaches}} to colorize gray-scale images using deep learning methods. Inspired by these, we propose a model which combines a deep Convolutional Neural Network trained from scratch with high-level features extracted from <b>the</b> Inception-ResNet-v 2 <b>pre-trained</b> model. Thanks to its fully convolutional architecture, our encoder-decoder model can process images of any size and aspect ratio. Other than presenting the training results, we assess the "public acceptance" of the generated images by means of a user study. Finally, we present a carousel of applications on different types of images, such as historical photographs. Comment: 12 page...|$|R
40|$|Given a {{convolutional}} {{neural network}} (CNN) that is pre-trained for object classification, this paper proposes to use active question-answering to semanticize neural patterns in conv-layers of the CNN and mine part concepts. For each part concept, we mine neural patterns in <b>the</b> <b>pre-trained</b> CNN, which {{are related to the}} target part, and use these patterns to construct an And-Or graph (AOG) to represent a four-layer semantic hierarchy of the part. As an interpretable model, the AOG associates different CNN units with different explicit object parts. We use an active human-computer communication to incrementally grow such an AOG on <b>the</b> <b>pre-trained</b> CNN as follows. We allow the computer to actively identify objects, whose neural patterns cannot be explained by the current AOG. Then, the computer asks human about the unexplained objects, and uses the answers to automatically discover certain CNN patterns corresponding to the missing knowledge. We incrementally grow the AOG to encode new knowledge discovered during the active-learning process. In experiments, our method exhibits high learning efficiency. Our method uses about 1 / 6 - 1 / 3 of the part annotations for training, but achieves similar or better part-localization performance than fast-RCNN methods. Comment: Published in CVPR 201...|$|E
30|$|In this paper, the {{integration}} of the deep visual features and the multimodal information has been proposed for ground-based cloud classification in weather station networks. We first fine-tune <b>the</b> <b>pre-trained</b> deep CNN model using the cloud images, followed by extraction of deep visual features and then fused with the multimodal information. A series of comparative experiments have been conducted to test the effectiveness of the proposed DMF, and the results show that the accuracy of the proposed DMF is higher than the state-of-the-art methods.|$|E
40|$|Learning {{powerful}} feature representations for image retrieval {{has always}} been a challenging task in the field of remote sensing. Traditional methods focus on extracting low-level hand-crafted features which are not only time-consuming but also tend to achieve unsatisfactory performance due to the complexity of remote sensing images. In this paper, we investigate how to extract deep feature representations based on convolutional neural networks (CNNs) for high-resolution remote sensing image retrieval (HRRSIR). To this end, several effective schemes are proposed to generate powerful feature representations for HRRSIR. In the first scheme, a CNN pre-trained on a different problem is treated as a feature extractor since there are no sufficiently-sized remote sensing datasets to train a CNN from scratch. In the second scheme, we investigate learning features that are specific to our problem by first fine-tuning <b>the</b> <b>pre-trained</b> CNN on a remote sensing dataset and then proposing a novel CNN architecture based on convolutional layers and a three-layer perceptron. The novel CNN has fewer parameters than <b>the</b> <b>pre-trained</b> and fine-tuned CNNs and can learn low dimensional features from limited labelled images. The schemes are evaluated on several challenging, publicly available datasets. The results indicate that the proposed schemes, particularly the novel CNN, achieve state-of-the-art performance...|$|E
40|$|The {{results of}} two {{experiments}} showed that observation of a trained conspecific Atlantic salmon Salmo salar significantly increased {{the rate at}} which naïve hatchery-reared fish accepted novel, live prey items, whereas the presence of an untrained conspecific actually decreased learning rates due to social inhibition. Pre-release training involving exposure of hatchery-reared fish to live prey items in <b>the</b> presence of <b>pre-trained</b> demonstrators would result in a significant enhancement in their foraging success on release and help prevent starvation, which is thought {{to be one of the}} principal causes of post-release mortality in hatchery-reared fishes. 12 page(s...|$|R
40|$|Recently, neuron activations {{extracted}} from a pre-trained {{convolutional neural network}} (CNN) show promising performance in various visual tasks. However, due to the domain and task bias, using the features generated from <b>the</b> model <b>pre-trained</b> for image classification as image representations for instance retrieval is problematic. In this paper, we propose quartet-net learning to improve the discriminative power of CNN features for instance retrieval. The general idea is to map the features into a space where the image similarity can be better evaluated. Our network differs from the traditional Siamese-net in two ways. First, we adopt a double-margin contrastive loss with a dynamic margin tuning strategy to train the network which leads to more robust performance. Second, we introduce in the mimic learning regularization to improve the generalization ability of the network by preventing it from overfitting to the training data. Catering for the network learning, we collect a large-scale dataset, namely GeoPair 1, which consists of 68 k matching image pairs and 63 k non-matching pairs. Experiments on several standard instance retrieval datasets demonstrate the effectiveness of our method...|$|R
30|$|Pre-training, or the {{warm-start}} {{approach in}} a chosen methodology, is helpful for preventing models from learning incorrect associations between speech features and text sentences. It is very useful for {{speeding up the}} learning process and increases the chance of achieving a desired convergence point as it provides a good starting position for the optimization algorithm. Due to <b>the</b> application of <b>pre-trained</b> MSTT and MTTS models, we start the DSL process from the point where distributions of P(TTS(t)|t) and P(STT(s)|s) are partially learned from the labeled dataset. Assuming the correctness of the dataset itself, the distributions are correct, but do not represent the full feature space yet.|$|R
