4|7|Public
40|$|The sixth {{edition of}} Where the Green Grants Went {{provides}} a comprehensive overview of grants to environmental initiatives from UK foundations, the National Lottery, and public sector funding programmes. The report focuses on 5, 857 grants from foundations and the lottery which together were worth £ 383 million {{across the two}} financial years 2010 / 11 and 2011 / 12. Highlights include: detailed analysis of environmental grants from 180 foundations, with UK environmental philanthropy reaching £ 112 million in 2011 / 12, its highest ever level insights into the types of funding most needed by environmental groups, and the comparative advantages of foundation grants relative to other income sources perspectives from more than 100 chief executives on why they value philanthropic funding interviews with four experienced environmental philanthropists {{a list of the}} 100 environmental organisations receiving the most money from UK foundations analysis of environmental grants from 31 lottery programmes, together worth £ 182. 9 million across 2010 / 11 and 2011 / 12 an overview of public sector grants programmes, and <b>top-level</b> <b>analysis</b> of their thematic and geographic focu...|$|E
40|$|Sponsored Report (for Acquisition Research Program) This {{exploratory}} study provides {{a detailed analysis}} of ground support equipment (GSE) maintenance and operations at the Oklahoma City Air Logistics Center (OC-ALC) to support future contract negotiations. The United States has been engaged in combat operations for over ten years. A key component of these combat operations has been airpower. A high wartime operational tempo and the use of aging airframes, some of which were scheduled to be retired decades ago, have solidified the need for a refined depot-level maintenance system designed to quickly and effectively dismantle, rebuild, and reconstitute combat and support aircraft. A critical part of the foundation that this system is built upon is the management of critical GSE, which is essential to depot-level maintenance operations. The {{purpose of this study is}} to provide a <b>top-level</b> <b>analysis</b> of current GSE management processes in order to better understand the effectiveness of current contractor logistics support, estimate the operational availability of ten categories of GSE, and provide specific findings and recommendations for the leadership at the OC-ALC. Acquisition Research Progra...|$|E
40|$|PT OneJect Indonesia {{faces the}} high level of defect on its barrel products. To {{overcome}} this problem, this study proposes an application quality control based on information system. We implement requirement analysis based on the needs of quality control methods, while to lower the defect rate we utilize Six Sigma method with DMAIC (define, measure, analyze, improve, control) phases. In define phase we determine the type of product to be investigated using Critical to Quality, as well as the problems and objectives. In measure phase we calculate DPMO value and sigma level; sigma value of 3. 520 is obtained. Then we calculate P control chart to see the production process control. In analyze phase we create pareto diagram to see the types of dominant defects. Then we create cause-and-effect diagram to determine the cause of defects, and five whys diagram to determine the root causes of the defects. In improve phase, we do improvement analysis through FMEA table by calculating the RPN value to determine the failure modes to be controlled. In control phase, we implement the proposal and simulation of sigma value increase. The analysis and design uses object-oriented information system by Lars Mathiasen, and UML as the requirement modeling and the design system. The results obtained are the documents of <b>top-level</b> <b>analysis</b> and design of information system as a quality control material for the actual system development. </p...|$|E
50|$|<b>Top-level</b> reviews - <b>analysis</b> {{of actual}} results versus {{organizational}} goals or plans, periodic and regular operational reviews, metrics, {{and other key}} performance indicators (KPIs).|$|R
50|$|The <b>top-level</b> unit of <b>analysis</b> in {{functional}} {{discourse grammar}} is the discourse move, not the sentence or the clause. This is a principle that sets functional discourse grammar apart from many other linguistic theories, including its predecessor functional grammar.|$|R
40|$|The {{significant}} increase in natural/shale gas production in the US is causing {{major changes in the}} chemical and petrochemical markets. These changes include the increased supply of methanol and the decreased supply of propylene. As such, there are promising opportunities for methanol-to-propylene processes in the US. This paper provides a <b>top-level</b> techno-economic <b>analysis</b> of two pathways: methanol to olefins (MTO) and methanol to propylene (MTP). Base-case scenarios are simulated using ASPEN Plus to obtain the key mass and energy balances as well as design data. For each process, two scenarios are considered for the feedstock: buying methanol versus making it from natural gas. The return on investment (ROI) is calculated for both processes under broad ranges of the prices of natural gas, methanol, and products. In addition to the techno-economic analysis, the CO 2 emissions are evaluated and compared...|$|R
40|$|The fifth World Internet Project New Zealand (WIPNZ) survey {{continues}} our biennial {{analysis of}} New Zealanders’ usage of, and attitudes towards, the internet. It follows {{on from the}} surveys undertaken in 2007, 2009, 2011 and 2013. In this report, we present <b>top-level</b> <b>analysis</b> of data from the survey carried out between September and November 2015. Both telephone and online interviews were conducted, together with {{a small sample of}} face-to-face interviews, and extensive material on the use of and attitudes towards Ultra-fast broadband (UFB) was collected. The inclusion, again, of online interviews has resulted in what we believe to be a more representative sample, since some of the growing group of New Zealanders who do not have landlines are now covered in the sample. The face-to-face interviewing tapped the views of otherwise difficult-to-interview groupings. Comparative findings with our earlier surveys will be presented in a later report. The report is divided into three sections: • Section 1 : Key Findings shows selected results from the survey for the full sample and is structured according to the various themes of the questionnaire. • Section 2 : The Diversity of Internet Users looks in more detail at how responses to the survey differ according to age, gender, ethnicity, household income and area, and is structured in terms of these social groupings. For {{the first time in the}} survey, we include people with disabilities to illustrate further the diversity of internet users. • Section 3 : Digital Disadvantage in 2015 looks at the sample from the perspective of different types of user – from the highly engaged to the low-level user. Section 3 also presents, in more detail, the characteristics and opinions of internet non-users in parameters such as age and gender...|$|E
40|$|Interpretation Table 1. 2 : Annotations in the Thesis In Chapter 2 {{we present}} a {{combined}} strictness and totality analysis. We are specifying the analysis as an annotated type system. The type system allows conjunctions of annotated types, but only at the <b>top-level.</b> The <b>analysis</b> is somewhat {{more powerful than the}} strictness analysis by Kuo and Mishra [KM 89] due to the conjunctions and in that we also consider totality. The analysis is shown sound with respect to a natural-style operational semantics. The analysis is not immediately extendable to full conjunction. The analysis of Chapter 3 is also a combined strictness and totality analysis, however with "full" conjunction. Soundness of the analysis is shown with respect to a denotational semantics. The analysis is more powerful than the strictness analyses by Jensen [Jen 92 a] and Benton [Ben 93] in that it in addition to strictness considers totality. So far we have only specified the analyses, however in order for the analyses to be pract [...] ...|$|R
40|$|It {{is widely}} {{understood}} that protein functions can be exhaustively {{described in terms}} of no single parameter, whether this be amino acid sequence or the three-dimensional structure of the underlying protein molecule. This means that a number of different attributes must be used to create an ontology of protein functions. Certainly much of the required information is already stored in databases such as Swiss-Prot, Protein Data Bank, SCOP and MIPS. But the latter have been developed for different purposes and the separate data-structures which they employ are not conducive to the needed data integration. When we attempt to classify the entities in the domain of proteins, we find ourselves faced with a number of cross-cutting principles of classification. Our question here is: how can we bring together these separate taxonomies in order to describe protein functions? Our proposed answer is: via a careful <b>top-level</b> ontological <b>analysis</b> of the relevant principles of classification, combined with a new framework for the simultaneous manipulation of classifications constructed for different purposes...|$|R
40|$|The Constellation Program (CxP) is NASA's {{effort to}} replace the Space Shuttle, return humans to the moon, and prepare for a human mission to Mars. The major {{elements}} of the Constellation Lunar sortie design reference mission architecture are shown. Unlike the Apollo Program of the 1960 's, affordability {{is a major concern}} of United States policy makers and NASA management. To measure Constellation affordability, a total ownership cost life-cycle parametric cost estimating capability is required. This capability is being developed by the Constellation Systems Engineering and Integration (SE&I) Directorate, and is called the Lifecycle Cost Analysis Model (LCAM). The requirements for LCAM are based on the need to have a parametric estimating capability in order to do <b>top-level</b> program <b>analysis,</b> evaluate design alternatives, and explore options for future systems. By estimating the total cost of ownership {{within the context of the}} planned Constellation budget, LCAM can provide Program and NASA management with the cost data necessary to identify the most affordable alternatives. LCAM is also a key component of the Integrated Program Model (IPM), an SE&I developed capability that combines parametric sizing tools with cost, schedule, and risk models to perform program analysis. LCAM is used in the generation of cost estimates for system level trades and analyses. It draws upon the legacy of previous architecture level cost models, such as the Exploration Systems Mission Directorate (ESMD) Architecture Cost Model (ARCOM) developed for Simulation Based Acquisition (SBA), and ATLAS. LCAM is used to support requirements and design trade studies by calculating changes in cost relative to a baseline option cost. Estimated costs are generally low fidelity to accommodate available input data and available cost estimating relationships (CERs). LCAM is capable of interfacing with the Integrated Program Model to provide the cost estimating capability for that suite of tools...|$|R
40|$|Reliability {{analyses}} are {{performed on the}} field failure data of various drive families from different drive manufacturers to gain insight {{into the nature of}} the underlying failure mechanisms and their contribution to the overall failure rate of the disk drive. The effect of various disk drive firmware changes implemented throughout the operating life of the disk drive and the capacity dependent failure mechanisms are studied in detail. The results obtained from the analysis of three disk drive families shows that a <b>top-level</b> failure rate <b>analysis</b> is not adequate to understand and improve the reliability of disk drives. Detailed hazard rate analyses of all failure mechanisms should be performed to understand the top contributors to the overall hazard rate. The results obtained from the analysis of one drive family shows that the composite hazard rate obtained from the combination of various failure mechanisms depends on which failure mechanism is dominating at what point during the operating life. On this particular drive, the authors conclude that in early stages {{of the life of the}} drive, one failure mechanism is dominating while a second failure mechanism starts dominating towards the end. The rest of the failure mechanisms are either decreasing or constant and have a minimal effect on the overall hazard rate. Analyses of a second drive family show that a firmware change implemented on the drive to fix a particular problem in the field inadvertently accelerated some hardware failure mechanisms resulting in the increase in overall hazard rate of drive. The third drive family analyses show that the capacity dependent failure mechanisms on the higher capacity disk drive contributed significantly to the increase in the overall hazard rate. 1...|$|R

