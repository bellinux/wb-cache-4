0|17|Public
2500|$|However, {{recent work}} by {{engineer}} James O'Kon suggests the Mesoamerican [...] "arch" [...] is technically not a corbelled arch {{at all but}} a trapezium truss system. Moreover, unlike a corbelled arch, it does not rely on overlapping layers of blocks but cast-in-place concrete often supported by timber <b>thrust</b> beams. <b>Computer</b> analysis reveals this to be structurally superior to a curved arch ...|$|R
5000|$|The Coordinated Science Laboratory (CSL) {{is a major}} {{scientific}} research laboratory at the University of Illinois at Urbana-Champaign. With deep roots in information technology, CSL has invented and deployed many landmark innovations, such as the electric vacuum gyroscope, the first computer-assisted instructional program and the plasma TV. Today, research <b>thrusts</b> include <b>computer</b> vision, economics and energy systems, information trust, neuroengineering, parallel computing, robotics and more.|$|R
40|$|We briefly {{survey the}} major <b>thrusts</b> of <b>computer</b> {{graphics}} activities, examining trends and topics rather than offering a comprehensive survey {{of all that}} is happening. The directions of professional activities, hardware, software, and algorithms are outlined. Within hardware we examine workstations, personal graphics systems, high performance systems, and low level VLSI chips; within software, standards and interactive system design; within algorithms, visible surface rendering and shading, three-dimensional modeling techniques, and animation. Note: This paper was presented at Eurographics 2 ̆ 784 in Copenhagen, Denmark...|$|R
40|$|This thesis {{presents}} the results of an investigation into the transient compressible viscous three-dimensional fluid flow in a propulsive nozzle. The governing equations for the problem are derived and applied in a computer code utilizing the VALE (Viscous Arbitrary Lagrangian-Eulerian) method. The computer code was verified using the nozzles of Cuffle, Back, and Massier (1969) and Hoffman (1988). Two test cases were utilized to demonstrate the utility of the code for this type of work. A transient propulsive nozzle was examined. Viscous effects are apparent during the nozzle start-up. Asymmetric stagnation inlet properties in propulsive nozzles were analyzed. The asymmetric inlet conditions were found to produce asymmetric <b>thrust.</b> The <b>computer</b> code developed for this project proved to be a useful tool, which should have lasting utility. ...|$|R
40|$|The present paper gives a {{statistical}} adventure towards exploring the average case complexity behavior of computer algorithms. Rather than following the traditional count based analytical (pen and paper) approach, we instead talk {{in terms of}} the weight based analysis that permits mixing of distinct operations into a conceptual bound called the statistical bound and its empirical estimate, the so called "empirical O". Based on careful analysis of the results obtained, we have introduced two new conjectures in the domain of algorithmic analysis. The analytical way of average case analysis falls flat {{when it comes to a}} data model for which the expectation does not exist (e. g. Cauchy distribution for continuous input data and certain discrete distribution inputs as those studied in the paper). The empirical side of our approach, with a <b>thrust</b> in <b>computer</b> experiments and applied statistics in its paradigm, lends a helping hand by complimenting and supplementing its theoretical counterpart. Computer science is or at least has aspects of an experimental science as well, and hence hopefully, our statistical findings will be equally recognized among theoretical scientists as well...|$|R
40|$|The {{design of}} coolant {{passages}} in regeneratively cooled thrust chambers {{is critical to}} the operation and safety of a rocket engine system. Designing a coolant passage is a complex thermal and hydraulic problem requiring an accurate understanding of the heat transfer between the combustion gas and the coolant. Every major rocket engine company has invested in the development of <b>thrust</b> chamber <b>computer</b> design and analysis tools; two examples are Rocketdyne's REGEN code and Aerojet's ELES program. In an effort to augment current design capabilities for government and industry, the NASA Lewis Research Center is developing a computer model to design coolant passages for advanced regeneratively cooled thrust chambers. The RECOP code incorporates state-of-the-art correlations, numerical techniques and design methods, certainly minimum requirements for generating optimum designs of future space chemical engines. A preliminary version of the RECOP model was recently completed and code validation work is in progress. This paper introduces major features of RECOP and compares the analysis to design points for the first test case engine; the Pratt & Whitney RL 10 A- 3 - 3 A thrust chamber...|$|R
40|$|Logical {{flow and}} {{guidelines}} are {{provided for the}} construction of a low <b>thrust</b> orbit determination <b>computer</b> program. The program, tentatively called FRACAS (filter response analysis for continuously accelerating spacecraft), is capable of generating a reference low thrust trajectory, performing a linear covariance analysis of guidance and navigation processes, and analyzing trajectory nonlinearities in Monte Carlo fashion. The choice of trajectory, guidance and navigation models has been made after extensive literature surveys and investigation of previous software. A key part of program design relied upon experience gained in developing and using Martin Marietta Aerospace programs: TOPSEP (Targeting/Optimization for Solar Electric Propulsion), GODSEP (Guidance and Orbit Determination for SEP) and SIMSEP (Simulation of SEP) ...|$|R
40|$|One of {{the major}} current <b>thrust</b> areas for <b>computer</b> {{software}} development is artificial intelligence and particularly expert systems. Several {{attempts have been made}} to implement cartographic design expert systems. None of these, however, can either understand why particular decisions are reached, or explain the reasoning to the user. This self-knowledge is one of the principle properties of any expert system and so it is doubtful whether any of the systems reported to date deserve the epithet "expert". This omission is not the fault of the system developers, but is caused by a lack of any systematised and accepted methodology for cartographic assessment. The cartographic community is urged to address this problem expeditiously...|$|R
50|$|The MAK's {{preliminary}} report, {{issued the}} week of 25 September 2006, attributed the accident to pilot error, while finding {{that there was no}} problem with the engines or the aircraft.The autopilot and thrust controller were switched off by the crew at a height of 100m and could not have any effect on the landing and landing run. The couplings linking the thrust controller to the engine control linkage were uncoupled and were not connected to the linkage any more. No steering commands given by the <b>thrust</b> controller <b>computer</b> were recorded. The results of the investigation of the electronic guidance and engine control system (FADEС) testify to the efficiency of the system right up to when the airplane collided with the barriers. The airplane landed in the touchdown zone at Irkutsk airport at 22:43:40 in wheel control mode with engines running at idle. After landing, the spoilers were automatically deployed and the automatic braking system (ABS) was automatically switched on in LOW mode. 1.5 seconds after touchdown, the airplane captain set the reverse thrust lever for the right engine to the reverse mode. The right engine correctly went into reverse thrust mode. The reverse thrust lever for the left engine was not applied.|$|R
40|$|We {{present a}} new {{rendering}} technique, termed LOD-sprite render-ing, {{which uses a}} combination of a level-of-detail (LOD) repre-sentation of the scene together with reusing image sprites (previ-ously rendered images). Our primary application is an accelera-tion technique for virtual environment navigation. The LOD-sprite technique renders an initial frame using a full-resolution model of the scene geometry. It renders subsequent frames with a much lower-resolution model of the scene geometry and texture-maps each polygon with the image sprite from the initial full-resolution frame. As it renders these subsequent frames the technique mea-sures the error associated with each low-resolution polygon, and uses this to decide when to re-render the scene using the full-resolution model. The LOD-sprite technique can be efficiently im-plemented in texture-mapping graphics hardware. The LOD-sprite technique is thus a combination of two currently very active <b>thrusts</b> in <b>computer</b> graphics: level-of-detail representa-tions and image-based modeling and rendering (IBMR) techniques. The LOD-sprite technique is different from most previous IBMR techniques in that they typically model the texture-map as a quadri-lateral, as opposed to a lower-resolution scene model. This scene model, even if only composed of a few polygons, greatly increases the range of novel views that can be interpolated before unaccept-able distortions arise. Also unlike previous LOD techniques, the LOD-sprite algorithm dynamically updates the image sprite every several frames. The LOD-sprite technique can be implemented with any LOD decomposition...|$|R
40|$|The revolutionary ultrafast {{passenger}} {{transportation system}} SpaceLiner {{is under investigation}} at DLR in the EU-funded study Future high-Altitude high-Speed Transport 20 XX. SpaceLiner’s configuration is being amended continuously, and SpaceLiner 7 is the brand new version {{at the point of}} April in 2013. SpaceLiner 7 is two staged reusable launch vehicle with liquid rocket engines. SpaceLiner Main Engine (SLME) is required to have high performance for the total system to be feasible, and also to be easy on the environment for frequent launches. Therefore staged combustion cycle (SC) rocket engine with liquid hydrogen and liquid oxygen (LH 2 /LOX) is accounted to be promising for SLME. The engine cycle analysis and the component predesign of SLME are performed with DLR developed codes and NASA developed Two-Dimensional Kinetic <b>Thrust</b> Chamber Analysis <b>Computer</b> Program (TDK). They show SLME’s feasibility and subject to be researched in the future...|$|R
40|$|Abstract]: Agile methodologies such as Extreme Programming (XP) {{have emerged}} as a major <b>thrust</b> in the <b>computer</b> {{software}} development arena. Almost contemporaneously, an organisational theory highly aligned with the values of agile methodologies has mutated from the ‘restructuring’ paradigm. This paper presents an overview of agile methodologies and the key features of the agile corporation, exploring the synergistic relationship they engender. The paper examines whether agile methodologies are used in practice and what organisational factors facilitate or inhibit their adoption. The paper concludes that agile methodologies are gaining widespread acceptance but {{there is often a}} misalignment with organisational culture and values. The importance of web systems have helped agile methods forge a place for themselves but there are still a number of factors mitigating against their large-scale success. In some corporations, agile methods need to be adapted to survive. In many others, their time will only come when changes in structure, culture and values have occurred...|$|R
50|$|An {{attempted}} {{launch of}} a military radar calibration satellite on 25 January 1983 suffered another first stage failure about 40 seconds into launch when the RD-219 started losing <b>thrust.</b> The onboard <b>computer</b> automatically shut the engine off and the rocket fell into the Northern Dvina River. Due to the tense relations between the US and Soviet Union at this time, the US military was widely suspected of having shot down the launch vehicle and General Secretary Yuri Andropov personally informed of this possibility. However, a group of locals ice fishing in the Dvina had witnessed the booster plunge into the river and reported what they'd seen to authorities. After this and a quick examination of telemetry, sabotage was ruled out. The failure was traced to high-frequency combustion instability which had been {{a problem with the}} RD-219 engine and was also responsible for the 1970 and 1975 Kosmos 3M failures. The engine was redesigned and no further launches were lost due to first stage engine failures.|$|R
40|$|In {{the central}} Canadian Rocky Mountain thrust-fold belt, {{three types of}} first order decollements are {{outlined}} within or along {{the border of the}} present orogenic wedge: one basal decollement, three intermediate or internal decollements and one upper decollement. Structural relationships suggest that each internal decollement is the result of one of the successive forward shifts of early basal decollements to new positions within the stratigraphic pile. Two computer programs have been developed to analyze the propagation of multiple thrust faults and their influence on the geometry of a <b>thrust</b> belt. The <b>computer</b> programs generate graphical simulations used to demonstrate a model of thrust propagation and thrust belt development that fits current knowledge about fault propagation and can replace the thrust transfer zone concept. A structural analysis of mesoscale structures in two thrust sheets indicates that a thrust sheet consist of a series of elongated blocks separated by subtle brittle-ductile shear zones along which differential motion occurred. These shear zones are oriented perpendicular to the mean strike of the thrust faults...|$|R
40|$|This thesis {{investigates the}} concept of thrust rating as a means towards {{reducing}} the life cycle costs of engine ownership. Towards this end, this thesis has discussed {{the concept of}} <b>thrust</b> rating, developed <b>computer</b> programs for mechanical load type failures, which include creep, LCF, and combinations thereof, and conducted simulations of improving life usage and reducing life cycle costs. A study was performed on a military engine, under an original design mission mix, that showed significant gains in creep-LCF life of the HPT blade could be achieved, especially With the recently proposed and presumably more accurate criterion- ductility exhaustion, by thrust rating. The savings were {{expressed in terms of}} an approximate reduced life accumulation rates and life cycle costs. The net result was a 50 % increase in creep-LCF life with a savings of $ 50. 4 million. These calculations were based on a Feet of 300 engines having the designed lifetime of 8, 000 operating hours per engine. Throughout the thesis, mention is also made of employing the thrust rating concept on other engines. To this end, the thesis will also give a blueprint for conducting a feasibility study to employ thrust rating as a maintenance tool. In addition to the technical aspects, the role of maintenance and aircraft operations policy will also be studied to determine the interrelationships that exist between thrust rating technology and its practical application...|$|R
40|$|The Petascale Computing Enabling Technologies (PCET) project {{addressed}} challenges {{arising from}} current trends in computer architecture {{that will lead}} to large-scale systems with many more nodes, each of which uses multicore chips. These factors will soon lead to systems that have over one million processors. Also, the use of multicore chips will lead to less memory and less memory bandwidth per core. We need fundamentally new algorithmic approaches to cope with these memory constraints and the huge number of processors. Further, correct, efficient code development is difficult even with the number of processors in current systems; more processors will only make it harder. The goal of PCET was to overcome these challenges by developing the computer science and mathematical underpinnings needed to realize the full potential of our future large-scale systems. Our research results will significantly increase the scientific output obtained from LLNL large-scale computing resources by improving application scientist productivity and system utilization. Our successes include scalable mathematical algorithms that adapt to these emerging architecture trends and through code correctness and performance methodologies that automate critical aspects of application development as well as the foundations for application-level fault tolerance techniques. PCET's scope encompassed several research <b>thrusts</b> in <b>computer</b> science and mathematics: code correctness and performance methodologies, scalable mathematics algorithms appropriate for multicore systems, and application-level fault tolerance techniques. Due to funding limitations, we focused primarily on the first three thrusts although our work also lays the foundation for the needed advances in fault tolerance. In the area of scalable mathematics algorithms, our preliminary work established that OpenMP performance of the AMG linear solver benchmark and important individual kernels on Atlas did not match the predictions of our simple initial model. Our investigations demonstrated that a poor default memory allocation mechanism degraded performance. We developed a prototype NUMA library to provide generic mechanisms to overcome these issues, resulting in significantly improved OpenMP performance. After additional testing, we will make this library available to all users, providing a simple means to improve threading on LLNL's production Linux platforms. We also made progress on developing new scalable algorithms that target multicore nodes. We designed and implemented a new AMG interpolation operator with improved convergence properties for very low complexity coarsening schemes. This implementation will also soon be available to LLNL's application teams as part of the hypre library. We presented results for both topics in an invited plenary talk entitled 'Efficient Sparse Linear Solvers for Multi-Core Architectures' at the 2009 HPCMP Institutes Annual Meeting/CREATE Annual All-Hands Meeting. The interpolation work was summarized in a talk entitled 'Improving Interpolation for Aggressive Coarsening' at the 14 th Copper Mountain Conference on Multigrid Methods and in a research paper that will appear in Numerical Linear Algebra with Applications. In the area of code correctness, we significantly extended our behavior equivalence class identification mechanism. Specifically, we not only demonstrated it works well at very large scales but we also added the ability to classify MPI tasks not only by function call traces, but also by specific call sites (source code line numbers) being executed by tasks. More importantly, we developed a new technique to determine relative logical execution progress of tasks in the equivalence classes by combining static analysis with our original dynamic approach. We applied this technique to a correctness issue that arose at 4096 tasks during the development of the new AMG interpolation operator discussed above. This scale isat the limit of effectiveness of production tools, but our technique quickly located the erroneous source code, demonstrating the power of understanding relationships between behavioral equivalence classes. This work {{is the subject of a}} paper recently accepted to SC 09, as well as a presentation entitled 'Providing Order to Extreme Scale Debugging Chaos' given at the ParaDyn Week annual conference in College Park, MD. In addition to this theoretical extension, we have made significant progress in developing a front end for this tool set, and the front-end is now available on several of LLNL's largescale computing resources. In addition, we explored mechanisms to identify exact locations of erroneous MPI usage in application source code. In this work, we developed a new model that led to a highly efficient algorithm for detecting deadlock during dynamic software testing. This work was the subject of a well-received paper at ICS 2009 [4]...|$|R

