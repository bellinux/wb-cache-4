1|10000|Public
40|$|The BabyExp {{project is}} {{collecting}} very dense {{audio and video}} recordings of the first 3 years of life of a baby. <b>The</b> <b>corpus</b> <b>constructed</b> in this way will be transcribed with automated techniques and {{made available to the}} research community. Moreover, techniques to extract commonsense conceptual knowledge incrementally from these multimodal data are also being explored within the project. The current paper describes BabyExp in general, and presents pilot studies on the feasability of the automated audio and video transcriptions...|$|E
40|$|Abstract The {{role of the}} Web for text corpus {{construction}} is becoming increas-ingly significant. However, {{the contribution of the}} Web is largely confined to building a general virtual corpus or low quality specialised corpora. In this paper, we introduce a new technique called SPARTAN for constructing specialised cor-pora from the Web by systematically analysing website contents. Our evaluations show that <b>the</b> <b>corpora</b> <b>constructed</b> using our technique are independent of the search engines employed. In particular, SPARTAN-derived corpora outperform all corpora based on existing techniques for the task of term recognition...|$|R
40|$|Parallel corpus is a {{valuable}} resource for cross-language information retrieval and data-driven natural language processing systems, especially for Statistical Machine Translation (SMT). However, most existing parallel corpora to Chinese are subject to in-house use, while others are domain specific and limited in size. To a certain degree, this limits the SMT research. This paper describes the acquisition of a large scale and high quality parallel corpora for English and Chinese. <b>The</b> <b>corpora</b> <b>constructed</b> in this paper contain about 15 million English-Chinese (E-C) parallel sentences, and more than 2 million training data and 5, 000 testing sentences are made publicly available. Different from previous work, <b>the</b> <b>corpus</b> is designed to embrace eight different domains. Some of them are further categorized into different topics. <b>The</b> <b>corpus</b> will be released to the research community, which {{is available at the}} NLP 2 CT 1 website...|$|R
5000|$|<b>The</b> <b>Corpus</b> Christi Church (<b>constructed</b> 1587-1593) {{is one of}} the {{earliest}} Jesuit churches in the world {{and one of the first}} baroque buildings in the Polish-Lithuanian Commonwealth, influencing the later architecture of Belarus, Poland and Lithuania.|$|R
40|$|This paper tackles the {{computational}} {{problems of}} Croatian verbal idioms. Croatian language has very rich phraseme structure, {{as described in}} Matešić (1982), Menac et. al. (2003; 2007) and Menac-Mihalić (2004), {{as well as many}} others. This work {{is one of the few}} attempts of computational analyis of idioms in Croatian language as multi-word units. We used rule-based approach and NooJ syntactic grammars in order to recognize any verb based idiom (of the ~ 1500 analyzed) in any syntactic position. The Croatian Dictionary of Idioms (Menac et al., 2003) was used for the initial list, which was implemented with new additions during training phase. Grammars were tested within <b>the</b> <b>corpora</b> <b>constructed</b> specifically for this work, and used to calculate statistical measures of recall, precision and f-measure for our grammars. With the final results of recall < 98 %, precision < 96 % and f-measure < 97 %, we consider this a successful attempt in the recognition of verb based idioms in Croatian language...|$|R
40|$|In this paper, we {{base on the}} {{syntactic}} structural Chinese Treebank <b>corpus,</b> <b>construct</b> <b>the</b> Chinese Opinon Treebank for {{the research}} of opinion analysis. We introduce the tagging scheme and develop a tagging tool for <b>constructing</b> this <b>corpus.</b> Annotated samples are described. Information including opinions (yes or no), their polarities (positive, neutral or negative), types (expression, status, or action), is defined and annotated. In addition, five structure trios are introduced according to the linguistic relations between two Chinese words. Four of them that are possibly related to opinions are also annotated in <b>the</b> <b>constructed</b> <b>corpus</b> to provide <b>the</b> linguistic cues. The number of opinion sentences together {{with the number of}} their polarities, opinion types, and trio types are calculated. These statistics are compared and discussed. To know the quality of the annotations in this <b>corpus,</b> <b>the</b> kappa values of the annotations are calculated. The substantial agreement between annotations ensures the applicability and reliability of <b>the</b> <b>constructed</b> <b>corpus.</b> 1...|$|R
40|$|We {{describe}} {{the construction of}} <b>the</b> CODA <b>corpus,</b> a parallel corpus of monologues and expository dialogues. The dialogue part of <b>the</b> <b>corpus</b> consists of expository, i. e., information-delivering rather than dramatic, dialogues written by several acclaimed authors. The monologue part of <b>the</b> <b>corpus</b> is a paraphrase in monologue form of these dialogues by a human annotator. <b>The</b> <b>corpus</b> was <b>constructed</b> {{as a resource for}} extracting rules for automated generation of dialogue from monologue. Using authored dialogues allows us to analyse the techniques used by accomplished writers for presenting information in the form of dialogue. The dialogues are annotated with dialogue acts and the monologues with rhetorical structure. We developed annotation and translation guidelines together with a custom-developed tool for carrying out translation, alignment and annotation. 1...|$|R
40|$|This paper {{describes}} {{a collection of}} multimodal corpora of referring expressions, <b>the</b> REX <b>corpora.</b> <b>The</b> <b>corpora</b> have two notable features, namely (1) they include time-aligned extra-linguistic information such as participant actions and eye-gaze on top of linguistic information, (2) dialogues were collected with various configurations {{in terms of the}} puzzle type, hinting and language. After describing how <b>the</b> <b>corpora</b> were <b>constructed</b> and sketching out each, we present an analysis of various statistics for <b>the</b> <b>corpora</b> with respect to the various configurations mentioned above. The analysis showed that <b>the</b> <b>corpora</b> have different characteristics in the number of utterances and referring expressions in a dialogue, the task completion time and the attributes used in the referring expressions. In this respect, we succeeded in constructing a collection of corpora that included a variety of characteristics by changing the configurations for each set of dialogues, as originally planned. <b>The</b> <b>corpora</b> are now under preparation for publication, to be used for research on human reference behaviour...|$|R
40|$|Abstract—The {{principle}} {{objective of}} this work is to build multi dialect Arabic texts corpora using a web corpus as a resource. A survey has been conducted to categorise distinct words and phrases that are common to a specific dialect only, and not used in other dialects, the purpose being to download a specific dialect text corpus. From this experiment we obtained 48 M tokens from different Arabic dialects. These dialects were categorised into four main dialects Gulf, Levantine, Egyptian and North African, resulting in 14. 5 M, 10. 4 M, 13 M and 10. 1 M tokens being obtained respectively. The total number of distinct types in all <b>the</b> <b>corpora</b> is 2 M types. In this paper we describe how <b>the</b> <b>corpora</b> were <b>constructed</b> by using distinct words...|$|R
40|$|Since the {{machines}} {{become more and}} more intelligent, it is reasonable to expect the automatic construction of text classi-fiers by given just the objective categories. As trade-off solutions, existing researches usually provide additional information to the category terms to enhance the perfor-mance of a classifier. Unique from them, in this paper, we <b>construct</b> <b>the</b> standard <b>corpora</b> from <b>the</b> web by just providing text categories. Since there are million-s of manually constructed websites, it is hopeful to find out proper text categoriza-tion (TC) knowledge. So we directly go to the web and use the hierarchies implied in navigation bars to extract and verify TC re-sources. By addressing the issues of nav-igation bar recognition and text filtering, <b>the</b> <b>corpora</b> are <b>constructed</b> for given tex-t categories and the classifiers are trained based on them. We conduct our experi-ments on the large scale of webpages col-lected from the 500 top English websites on Alexa. The Open Directory Project (ODP) is used as testing corpus. Experi-mental results show that, being compared with the classifier based on manually la-beled <b>corpus,</b> <b>the</b> classifier trained on au-to <b>constructed</b> <b>corpora</b> reaches compara-ble performance for the categories that are well covered by <b>the</b> training <b>corpus.</b> ...|$|R
40|$|Abstract — We {{present in}} this paper a new {{multimodal}} corpus of spon-taneous collaborative and affective interactions in French: RECOLA, which is being {{made available to the}} research community. Participants were recorded in dyads during a video conference while completing a task requiring collaboration. Different multimodal data, i. e., audio, video, ECG and EDA, were recorded continuously and synchronously. In total, 46 participants took part in the test, for which the first 5 minutes of interaction were kept to ease annotation. In addition to these recordings, 6 annotators measured emotion continuously on two dimensions: arousal and valence, as well as social behavior labels on five dimensions. <b>The</b> <b>corpus</b> allowed us to take self-report measures of users during task completion. Methodologies and issues related to affective corpus construction are briefly reviewed {{in this paper}}. We further detail how <b>the</b> <b>corpus</b> was <b>constructed,</b> i. e., participants, procedure and task, the multimodal recording setup, the annotation of data and some analysis of the quality of these annotations. I...|$|R
40|$|This paper {{describes}} the automatic {{building of a}} corpus of short Swedish news texts from the Internet, its application and possible future use. <b>The</b> <b>corpus</b> is aimed at research on Information Retrieval, Information Extraction, Named Entity Recognition and Multi Text Summarization. <b>The</b> <b>corpus</b> has been <b>constructed</b> by using an Internet agent, the so called newsAgent, downloading Swedish news text from various sources. A small part of this corpus has then been manually tagged with keywords and named entities. The newsAgent is also used as a workbench for processing the abundant flows of news texts for various users in a customized format in the application Nyhetsguiden...|$|R
40|$|This paper {{presents}} a new corpus project, aiming at building a national corpus of Polish. What makes it {{different from a}} typical YACP (Yet Another Corpus Project) is 1) {{the fact that all}} four partners in the project have in <b>the</b> past <b>constructed</b> <b>corpora</b> of Polish, sometimes in the spirit of collaboration, at other times — in the spirit of competition, 2) the partners bring into the project varying areas of expertise and experience, so the synergy effect is anticipated, 3) <b>the</b> <b>corpus</b> will be built with an eye on specific applications in various fields, including lexicography (<b>the</b> <b>corpus</b> will be <b>the</b> empirical basis of a new large general dictionary of Polish) and natural language processing (...|$|R
40|$|We {{present an}} {{approach}} to language-specific query-based sampling which, given a single document in a target language, can find many more examples of documents in that language, by automatically constructing queries to access such documents {{on the world wide}} web. We propose a number of methods for building search queries to quickly obtain documents in the target language. They perform accurately and efficiently for building a corpus of documents in Tagalog starting from a single seed document, when these documents are only 2. 5 % of the documents in a collection. We found that a simple approach [...] of sampling with a query consisting of the most frequent word from <b>the</b> minority language <b>corpus</b> <b>constructed</b> so far [...] was very successful. This method built a corpus of documents with word frequencies similar to those in <b>the</b> <b>corpus</b> based on all Tagalog documents in our collection, and required {{a relatively small number of}} search queries. It also quickly acquired a go [...] ...|$|R
40|$|This thesis {{investigates the}} social {{construction}} of an increasingly relevant aspect of social life, namely mental depression, in British and Chinese news media over the last two decades, aiming at delivering a contribution to people’s understanding of the link between discourse and the social reality of depression. A discourse is understood as the totality of all the texts that have been produced within a particular discourse community. The special discourse analysed consists of two diachronic corpora including articles in which the lexical item depression or 抑郁症 (yiyuzheng, ‘depression’) occurs in British and Chinese national newspapers from 1984 to 2009. Corpus analysis is complemented by a targeted paraphrase analysis of the paraphrastic content expressed in the context of relevant keywords. My findings suggest that in <b>the</b> British <b>corpus,</b> there has been a circular movement {{in the construction of the}} meanings of depression, swinging between a psychological problem that needs psychotherapy and a biochemical condition that needs pharmaceutical intervention. <b>The</b> Chinese <b>corpus</b> <b>constructs</b> ‘抑郁症’ (yiyuzheng, ‘depression’) as a problem that is normally caused by external social factors, and therefore psychological support and improvement of the social environment have been represented as more helpful than medical treatment...|$|R
40|$|This {{report has}} two aims ffl To give {{information}} about <b>the</b> issues behind <b>corpus</b> alignment and <b>the</b> techniques currently used. ffl To describe a particular corpus {{which members of}} CCL were involved in <b>constructing</b> - <b>the</b> Asahi <b>corpus.</b> <b>The</b> subject of aligning parallel corpora is expanding rapidly, particularly because the bottom-up machine translation (MT) paradigms such as Example-based MT and Statistics-based MT are looking for large knowledge sources. However, most {{work has been done}} on aligning European language <b>corpora</b> such as <b>the</b> Canadian Hansard and this necessarily ignores many of the difficult issues we face when aligning more semantically distant languages such as English and Japanese. <b>The</b> Asahi <b>corpus</b> was <b>constructed</b> from a CD-ROM of newspaper editorials which were automatically aligned using a hybrid statistical-linguistic approach at the Nara Advanced Institute of Science and Technology (NAIST) in Japan. The Asahi editorials appear daily in a national, broadsheet newspaper [...] ...|$|R
40|$|In <b>the</b> {{development}} of <b>corpus</b> linguistics, <b>the</b> creation of <b>corpora</b> {{has had a}} critical role in corpus-based studies. The majority of created corpora have been associated with English and native languages, while other languages and types of corpora have received relatively less attention. Because an increasing number of <b>corpora</b> have been <b>constructed,</b> and each <b>corpus</b> is <b>constructed</b> for a definite purpose, this study identifies <b>the</b> functions of <b>corpora</b> and combines <b>the</b> values of various types of corpora for auto-learning based on <b>the</b> existing <b>corpora.</b> Specifically, <b>the</b> following three <b>corpora</b> are adopted: (a) <b>the</b> <b>Corpus</b> o...|$|R
40|$|We are {{presenting}} {{the construction of}} a Swedish corpus aimed at research 1 on Information Retrieval, Information Extraction, Named Entity Recognition and Multi Text Summarization, we will also present the results on evaluating our Swedish text summarizer SweSum with this <b>corpus.</b> <b>The</b> <b>corpus</b> has been <b>constructed</b> by using Internet agents downloading Swedish newspaper text from various sources. A small part of this corpus has then been manually annotated. To evaluate our text summarizer SweSum we let ten students execute our text summarizer with increasing compression rate on the 100 manually annotated texts to find answers to questions. The results showed that at 40 percent summarization/compression rate the correct answer rate was 84 percent...|$|R
40|$|With {{the rise}} of cloud computing, it is {{increasingly}} attractive for end-users (organizations and individuals) to outsource the management of their data to {{a small number of}} largescale service providers. In this paper, we consider a user who wants to outsource storage and search for a corpus of web documents (e. g., an intranet). At the same time, <b>the</b> <b>corpus</b> may contain confidential documents that the organization does not want to reveal to the service provider. While past work has considered the problems of secure keyword search and secure indexing, all of the proposed tools require significant modifications to existing search engines and infrastructure. In this paper, we propose a system called PrivatePond, which allows confidential outsourced web search using an unmodified search engine. The system is built around the central idea of a secure indexable representation, which is attached to each document in <b>the</b> <b>corpus,</b> and <b>constructed</b> with <b>the</b> goal of balancing confidentiality and searchability. In addition, a secure local proxy is used to provide transparency to the end-user. While the idea of a secure indexable representation is very general, we propose a preliminary instantiation of this idea, which provides practical confidentiality. In addition, an experimental evaluation indicates that this indexable representation can provide high-quality search and ranking, similar to what is available using <b>the</b> unmodified <b>corpus.</b> 1...|$|R
40|$|This {{exploratory}} study describes {{a framework for}} data-driven learning (DDL), inGeneral (non-major) English university classes, in which learners <b>construct</b> linguistic <b>corpora</b> instead of merely consulting them. Prior related work has addressed the needs of language specialists, in particular trainee translators who are learning how to compile glossaries, rather than non-major students of English. It is argued in this article {{that the process of}} creating a corpus inculcates a sense of ownership in the learner and therefore has a motivational impetus. This is especially true, it is claimed here, when the topic of <b>the</b> <b>corpus</b> is of personal interest to the learner, or coincides with their major field of study. Learners may pursue language study for only a short period of their university career, but once <b>the</b> <b>corpus</b> is <b>constructed,</b> some students may be sufficiently motivated to consult it and add to it when needed. Moreover, the process of compiling <b>the</b> <b>corpus</b> may lead to the acquisition of not only language but also useful transferable skills, including information technology and problem-solving competencies. This study presents some of the motivational issues surrounding DDL in Asia and suggests corpus construction as a solution. Previous research on corpus construction by learners is reviewed. Inthe experiment which forms the core of this study, 90 freshmen compiled andanalyzed corpora as part of a General English course in Taiwan. Of these, 19 students completed final projects based on corpora they had compiled. Their findings – and reactions to <b>the</b> use of <b>corpora</b> compilation as a language learning tool – are reported in a qualitative data analysis...|$|R
40|$|Recently, the {{credibility}} {{of information on the}} Web has become an important issue. In addition to telling about content of source documents, indicating how to interpret the content, especially showing interpretation of the relation between statements appeared to contradict each other, is important for helping a user judge {{the credibility}} of information. In this paper, we will describe the purpose and the way in the construction of a text summarization corpus. Our purpose in the construction of <b>the</b> <b>corpus</b> includes <b>the</b> following three points; to collect Web documents relevant to several query sentences, to prepare gold standard data to evaluate smaller sub-processes in the extraction process and the summary generation process, to investigate the summaries made by human summarizers. <b>The</b> <b>constructed</b> <b>corpus</b> contains six query sentences, 24 manually-constructed summaries, and 24 collections of source Web documents. We also investigated how the descriptions of interpretation, which help a user judge {{the credibility of}} other descriptions in the summary, appear in <b>the</b> <b>corpus.</b> As a result, we confirmed that showing interpretation on conflicts is important for helping a user judge the credibility of information. 1...|$|R
40|$|Machine {{translation}} (MT) {{technology has}} made significant progress {{over the last decade}} and now offers the potential for Arabic sign language (ArSL) signers to access text published in Arabic. The dominant model of MT is now corpus based. In this model, the accuracy of translation correlates directly with size and coverage of <b>the</b> <b>corpus.</b> <b>The</b> <b>corpus</b> is a collection of translation examples constructed from existing documents such as books and newspapers; however, no written system for sign language (SL) comparable to that used for natural language has yet been developed. Hence, no SL documents exist, complicating the procedure for <b>constructing</b> an SL <b>corpus.</b> In countries such as Ireland and Germany, a number of corpora have already been developed from scratch and used for MT. There is no ArSL corpus for MT, requiring {{the creation of a new}} ArSL corpus for language instruction. The goal of building this corpus is to develop an automatic translation system from Arabic text to ArSL. This paper presents <b>the</b> ArSL <b>corpus</b> for instructional language constructed for use in schools, and the methodology used to create it. <b>The</b> <b>corpus</b> was collected at the College of Computer and Information Sciences at Imam Muhammad bin Saud University in Riyadh, Saudi Arabia. A group of interpreters and native signers with backgrounds in education were involved in this work. <b>The</b> <b>corpus</b> was <b>constructed</b> by collecting instructional sentences used daily in schools for the deaf. The syntax and morphology of each sentence were then manually analysed. Each sentence was individually translated, recorded on video, and stored in MPEG format. <b>The</b> <b>corpus</b> contains video data from three native signers. The videos were then annotated using an ELAN annotation tool. The annotated video data contain isolated signs accompanied by detailed information, such as manual and non-manual features. The last procedure in <b>constructing</b> <b>the</b> <b>corpus</b> was to create a bilingual dictionary from the annotated videos. <b>The</b> <b>corpus</b> comprises two main parts. The first part is the annotated video data, comprising isolated signs with detailed information, accompanied by manual and non-manual features. It also contains the Arabic translation script, including syntax and morphology details. The second part is the bilingual dictionary, delivered with the annotated videos...|$|R
40|$|Thanks to the {{development}} of the concept of metadiscourse, it is now widely acknowledged that academic/scientific writing is not only concerned with communicating purely propositional meanings: what is communicated through academic/scientific communication is seen to be intertwined with the negotiation of social and interpersonal meanings. While a large number of so called metadiscoursal resources contribute to the simultaneous negotiation of propositional and interpersonal meanings, the present study aimed at investigating the functions self-mention forms can fulfill in academic/scientific communication. Two of Stephen Hawking's scientific books were selected as <b>the</b> <b>corpus</b> of <b>the</b> research, and based on Tang and John's (1999) model, <b>the</b> <b>constructed</b> <b>corpus</b> was analyzed in terms of the functions self-mention forms can fulfill in academic/scientific writing. The findings revealed that from among the different roles identified by Tang and John, the representative role constituted the most frequent self-mention function in <b>the</b> <b>corpus.</b> <b>The</b> remarkably heavy presence of representative role in Hawking's scientific prose was interpreted as a further evidence for the claim that scientists are more likely to persuade readers of their ideas if they frame their messages in ways which appeal to appropriate community-recognized relationships...|$|R
40|$|We {{describe}} a methodology for constructing a {{gold standard for}} the automatic evaluation of term extractors, an important step toward establishing a much-needed evaluation protocol for term extraction systems. The gold standard proposed is a fully annotated <b>corpus,</b> <b>constructed</b> in accordance with a specific terminological setting (i. e. the compilation of a specialized dictionary of automotive mechanics), and accounting for {{the wide variety of}} realizations of terms in context. A list of all the terminological units in <b>the</b> <b>corpus</b> is extracted, and may be compared to the output of a term extractor, using a set of metrics to assess its performance. Subsets of terminological units may also be extracted, due to the use of XML for annotation purposes, providing a level of customization. Particular attention is paid to the criteria used to select terminological units in <b>the</b> <b>corpus,</b> and <b>the</b> protocol established to account for terminological variation within <b>the</b> <b>corpus...</b>|$|R
40|$|International audienceWhile human {{communication}} involves rich, {{complex and}} expressive gestures, available corpora of captured motions {{used for the}} animation of virtual characters contain actions ranging from locomotion to everyday life motions. We aim at creating a novel corpus of expressive and meaningful gestures, and we focus on body movements and gestures involved in theatrical scenarios. In this {{paper we propose a}} methodology for building a corpus of full-body theatrical gestures based on a magician show enriched with affective content. We then validate <b>the</b> <b>constructed</b> <b>corpus</b> of theatrical gestures and sequences through several perceptual studies focusing on the complexity of the produced movements as well as the recognizability of the additional affective content...|$|R
40|$|Based on {{a series}} of <b>corpora</b> <b>constructed</b> from {{internet}} live sports pages, the chapter seeks to answer the question whether English language use is being affected significantly by the advent of new media (the internet, sms messages, twitter and e-mails). <b>The</b> <b>corpora</b> offer <b>the</b> chance to take a snapshot of present English usage in a context that is at once intended to be informative and enjoyable, {{and at the same time}} promise the benefit of including many sources of language use in three manageable corpora. Problems both of collection and interpretation are discussed, not least the risk of the effects of normalisation in <b>corpora</b> <b>constructed</b> from internet sites designed to be understood by a wide readership. The concluson is reached that widely-held assumptions about language change tend to be simplistic and English is found to be altered rather less than one might expect by the new media. The author also notes the need for a multi-disciplinary approach to linguistics and lexicograph...|$|R
40|$|Developing corpora {{from social}} media content {{involves}} convoluted cleaning. In this {{paper we propose}} and implement <b>the</b> automation of <b>corpora</b> building for facilitating social media mining and analytics. This automation process incorporates: a) metadata extraction and structuring b) semantic cleaning with tagging and c) learning domain terms/entities. The implementation performs comprehensive cleaning including abbreviation and slang correction, phonetic matching using metaphone algorithm, splitting joined words and identifying/learning entities. It identifies the entities, tags them and creates/updates a knowledgebase (KB) comprising of domain terms. <b>The</b> <b>corpus</b> thus <b>constructed,</b> facilitates multidimensional analysis and summarization. This proposed technique was tested with an experiment in which real world streaming tweets pertaining to Indian politics were collected, structured, cleaned and tagged. The results of the automation experiment can be stated as follows: a) the tweets although primarily in English, contained at times words from the regional languages. The algorithm does not recognize these words and they are construed as domain terms. An accuracy of 85. 55 % was achieved in identifying the correct domain terms and entities. b) The automation required human feedback and intervention which progressively reduced and reached a figure of 18 % with the update and enhancement of the KB. This paper assumes relevance because the implementation automates the entire process of collecting and cleaning the tweets and yields a corpus suitable for multi-faceted analysis...|$|R
30|$|For Spanish, {{we built}} the tonal models with the DB 1 {{database}} and lexical models with the DB 1 and DB 2 databases. The prototypes for <b>the</b> DB 1 <b>corpus</b> were <b>constructed</b> similarly to <b>the</b> German <b>corpus,</b> and prototypes for DB 2 {{were built in}} a similar way as <b>the</b> English <b>corpus.</b> For both cases, we also built the reference A-ME models.|$|R
40|$|As {{the number}} and size of {{computer}} corpora grow, linguistic researchers are increasingly using them to study changes in language over time. Comparing usage {{at one point in}} time with usage at a later or an earlier period seems a stunningly simple and Sausurreanly impeccable method of studying language change. Needless to say the reality is rather different. This paper identifies some of the methodological problems encountered in using computer corpora to describe changes in sexist usages in New Zealand English (NZE) over a twenty-five year period. 1. Which <b>corpora?</b> <b>The</b> ideal situation for comparing corpora as a basis for studying language change would appear to be to use two <b>corpora</b> <b>constructed</b> on parallel principles at two different points in time. Assuming that any variation identified can be reasonably attributed to language change over time, rather than to, say, topic differences or stylistic differences between <b>the</b> <b>corpora,</b> then comparing two similarly <b>constructed</b> <b>corpora</b> seems [...] ...|$|R
40|$|In {{this paper}} we {{introduce}} ukWaC, a large <b>corpus</b> of English <b>constructed</b> by crawling the. uk Internet domain. <b>The</b> <b>corpus</b> {{contains more than}} 2 billion tokens {{and is one of}} the largest freely available linguistic resources for English. The paper describes the tools and methodology used in the construction of <b>the</b> <b>corpus</b> and provides a qualitative evaluation of its contents, carried out through a vocabularybased comparison with the BNC. We conclude by giving practical information about availability and format of <b>the</b> <b>corpus.</b> 1...|$|R
40|$|How do {{the interstate}} {{perceptions}} that drive the security dilemma form? This paper explores {{the role of}} uncertainty in perception formation and conflict in US-China relations over 1969 - 1981 using two new datasets. The first measures internal perceptions: time series valence of the internal discussions and memoranda of US and Chinese policymakers. <b>The</b> text <b>corpora</b> were <b>constructed</b> through digitization of Chinese leader papers and the Foreign Relations of the United States series. Positive, negative, and uncertain valence is measured with dictionary-based semantic analysis. The second dataset measures external actions: diplomatic events in the bilateral relationship, as auto-coded from my new corpus of 1. 3 million New York Times articles on interstate affairs, 1851 - 2011. I find that uncertainty plays a very different role {{in the formation of}} US versus Chinese perceptions. For the United States, uncertainty encourages positive perceptions of China. For China, uncertainty contributes to negative perceptions of the United States...|$|R
40|$|In {{this paper}} I discuss some {{representations}} of oral evaluation {{in the learning}} of English as a Foreign Language (FL) in a Brazilian undergraduate institution for FL teachers. My analysis includes questions and concepts from applied linguistics, discourse analysis and psychoanalysis. In this perspective, subjectivity is constituent of discourse; subject-teachers take contradictory positions during the assessment of their students' learning process. These positions are characterized as gestures of interpretation because they answer demands, in this case, from scientific, technical, juridical and political/transferencial voices. <b>The</b> <b>corpus</b> was <b>constructed</b> with discursive practices of teachers and students {{at the end of}} a 4 year-course of study. In my analysis, I use the concept of Discursive Formations (DF), understood as enunciative regularities within a field of knowledge. I take as themes the elements of traditional oral evaluation, such as fluency and pronunciation, and representations of communicative competence/proficiency. As a result, I show that, in the process of assessment, people take stands within DFs that I have named &# 8220;Including&# 8221; and &# 8220;Excluding&# 8221;, as well as some of its modulations, &# 8220;Unconditional inclusion&# 8221;, and &# 8220;Excluding perfection&# 8221;. The &# 8220;Excluding&# 8221; DF is characterized by the representation of perfection which contradictorily excludes both the students' fault and excellence. The &# 8220;Inclusive&# 8221; DF accepts the student's faults and progress, turning relative their performance. In this DF a progress and a passing grade above an abstract minimum are guaranteed to all or most students...|$|R
40|$|We are {{developing}} a multilingual machine translation system to provide foreign tourists with a multilingual speech translation service in the Winter Olympic Games that {{will be held in}} Korea in 2018. For a knowledge learning to make the multilingual expansibility possible, we needed large bilingual corpus. In Korea {{there were a lot of}} Korean-English bilingual corpus, but Korean-French bilingual corpus and Korean-Spanish bilingual corpus lacked absolutely. Korean-English-French and Korean-English-Spanish triangle <b>corpus</b> were <b>constructed</b> by crowdsourcing translation using the existing large Korean-English corpus. But we found a lot of translation errors from <b>the</b> triangle <b>corpora.</b> This paper aims at filtering of translation errors in large triangle <b>corpus</b> <b>constructed</b> by crowdsourcing translation to reduce the translation loss of triangle corpus with English as a pivot language. Experiment shows that our method improves + 0. 34 BLEU points over the baseline system. ...|$|R
40|$|This paper {{proposes a}} method of {{fertilizing}} a Japanese case frame dictionary to handle complicated expressions: double nominative sentences, non-gapping relation of relative clauses, and case change. Our method {{is divided into two}} stages. In the first stage, we parse a large <b>corpus</b> and <b>construct</b> a Japanese case frame dictionary automatically from the parse results. In the second stage, we apply case analysis to <b>the</b> large <b>corpus</b> utilizing <b>the</b> <b>constructed</b> case frame dictionary, and upgrade the case frame dictionary by incorporating newly acquired information. ...|$|R
40|$|In this paper, {{we present}} our work on {{constructing}} a textual semantic relation corpus by {{making use of}} an existing treebank annotated with discourse relations. We extract adjacent text span pairs and group them into six categories according to the different discourse relations between them. After that, we present the details of our annotation scheme, which includes six textual semantic relations, backward entailment, forward entailment, equality, contradiction, overlapping, and independent. We also discuss some ambiguous examples to show the difficulty of such annotation task, which cannot be easily done by an automatic mapping between discourse relations and semantic relations. We have two annotators {{and each of them}} performs the task twice. The basic statistics on <b>the</b> <b>constructed</b> <b>corpus</b> looks promising: we achieve 81. 17 % of agreement on the six semantic relation annotation with a. 718 kappa score, and it increases to 91. 21 % if we collapse the last two labels with a. 775 kappa score. 1...|$|R
40|$|The {{challenging}} {{issues of}} discourse relation recognition in Chinese are addressed. Due {{to the lack}} of Chinese discourse <b>corpora,</b> we <b>construct</b> a moderate <b>corpus</b> with human-annotated discourse relations. Based on <b>the</b> <b>corpus,</b> a statistical classifier is proposed, and various features are explored in the experi-ments. The experimental results show that our method achieves an accuracy of 88. 28 % and an F-Score of 63. 69 % in four-class clas-sification and achieves an F-Score of 93. 57 % in the best case. ...|$|R
