0|10000|Public
2500|$|The [...] "Principles of Fundamental Justice" [...] require <b>that</b> <b>means</b> <b>used</b> {{to achieve}} <b>a</b> {{societal}} purpose or objective must be reasonably necessary.|$|R
40|$|Abstract: To some extent, <b>using</b> <b>a</b> plane curve to {{approximate}} an offset {{curve of the}} plane Bézier curve is restricted. In this paper, a region approximation idea <b>that</b> <b>means</b> <b>using</b> <b>a</b> “fat curve ” with a width {{to approximate}} the offset curve is proposed, and {{a complete set of}} algorithms to approximate offset curve using disk Bézier curve are given and implemented. In the algorithms, the optimal and uniform approximate curve of the offset curve as the central curve of the Disk Bézier curve is found by using Remez method, and then the upper optimal and uniform approximation principle is proposed to compute the error radius function of the Disk Bézier curve. Thus, the whole Disk Bézier curve can be obtained. In the end of this paper, the approximate effect of the Disk Bézier curve is not only analyzed and assessed, but also some specific examples are provided...|$|R
40|$|Mobile {{computing}} {{can potentially}} {{change the way}} medical, health and information services are delivered. Much of the {{focus has been on}} mobile telephone <b>use</b> particularly from <b>a</b> commercial and marketing perspective. It is often assumed <b>that</b> ‘mobile’ <b>means</b> <b>using</b> <b>a</b> mobile telephone. The growth in mobile devices usage in hospitals, is contributing to better health and medical services delivery. This paper describes two hospital based case studies involving different mobile devices. We argue that understanding the mobile domain in hospitals requires an understanding of both the context and the user, we propose a definition of the mobile hospital work domain...|$|R
40|$|This paper {{describes}} a decentralised approach to social filtering based on trust between agents in a multiagent system. The social filtering {{in the proposed}} approach is built on the interactions between collaborative software agents performing content-based filtering. This <b>means</b> <b>that</b> it <b>uses</b> <b>a</b> mixture of content-based and social filtering and thereby, it takes advantage of both methods...|$|R
5000|$|New York University {{anthropologist}} Susan Anton {{stated that}} even after dating, experts would likely spend many years striving to put these fossils in the proper context {{because there is no}} consensus in paleoanthropology about exactly how such comparisons are used to define the genus Homo. [...] "Some would argue that striding bipedalism is a defining feature, so <b>that</b> being Homo <b>means</b> <b>using</b> <b>a</b> specific way of moving around the environment. Other scholars may look more to cranial characteristics as Homo family features." ...|$|R
50|$|Though RELAX NG's {{ability to}} support {{user-defined}} data types is useful, {{it comes at}} the disadvantage of only having two data types that the user can rely upon. Which, in theory, <b>means</b> <b>that</b> <b>using</b> <b>a</b> RELAX NG schema across multiple validators requires either providing those user-defined data types to that validator or using only the two basic types. In practice however, most RELAX NG processors support the W3C XML Schema set of data types.|$|R
50|$|In {{object-oriented}} programming, forwarding <b>means</b> <b>that</b> <b>using</b> <b>a</b> {{member of}} an object (either a property or a method) results in actually using the corresponding member of a different object: the use is forwarded to another object. Forwarding is <b>used</b> in <b>a</b> number of design patterns, where some members are forwarded to another object, while others are handled by the directly used object. The forwarding object is frequently called a wrapper object, and explicit forwarding members are called wrapper functions.|$|R
5000|$|Non-consent: This {{does not}} {{necessarily}} <b>mean</b> <b>that</b> they <b>use</b> <b>a</b> traditional role-playing game system: it could just be that consequences are enforced based on a [...] "common sense" [...] basis, e.g., if several police officers go to seize a character, even without dice rolls, an administrator might say that the only reasonable outcome is that the character is brought into custody. Or, {{if there is such}} a system, this <b>means</b> <b>that</b> the results of system calculations are final.|$|R
3000|$|... (2) The Choquet {{integral}} is an aggregation operator {{introduced by}} the french mathematician Gustave Choquet [28]. It {{takes into account the}} importance of class of criteria and interaction between criteria. Conversely to the weighted <b>mean</b> <b>that</b> <b>uses</b> <b>a</b> weight on each criteria to aggregate their scores and gives a global score to an entity, the Choquet integral employs a capacity (also known as fuzzy measure) to calculate weights for all groups of criteria. A capacity is defined as follows: [...]...|$|R
5000|$|The {{data type}} time_t, used on {{operating}} {{systems such as}} Unix, is a signed integer {{counting the number of}} seconds {{since the start of the}} Unix epoch (midnight UTC of 1 January 1970), and is often implemented as a 32-bit integer. The latest time that can be represented in this form is 03:14:07 UTC on Tuesday, 19 January 2038 (corresponding to 2,147,483,647 seconds since the start of the epoch). This <b>means</b> <b>that</b> systems <b>using</b> <b>a</b> 32-bit [...] type are susceptible to the Year 2038 problem.|$|R
30|$|The {{results of}} this study show that {{frequency}} use and text messaging are two strong determinants of a phone-related hazard. When it comes to the phone-related accident level, however, the determinant variable is driving experience and use frequency. Cell phone usage while driving could increase the likelihood of both traffic hazards and accidents; however, whether the phone-related accidents happen also depends on the drivers’ driving experience. Moreover, driving experience contributes more than use frequency for the phone-related accidents according to the regression model. <b>That</b> <b>means</b> <b>that</b> <b>using</b> <b>a</b> cell phone is not threatening the safety of all driver groups equally: cell phone usage will result in phone-related accident more easily for novice drivers than experienced ones. Even for novice drivers, about 50  % of them have experienced phone-related hazards, while only 6  % of them have phone-related accidents. This indicates that the safety situation with a cell phone is not extremely serious. The potential phone-related accident risks have been controlled at the acceptable level and have not increased in line with the rapid growth of cell phone usage. Some authors have also suggested that most drivers can manage with their mobile phone while driving (Mikkonen and Backman [28]). However, it does not mean the phone-related accidents should be overlooked, considering that driving the car is the major task, while using cell phones is just a secondary task and may result in additional accident risk. Therefore, traffic safety communities should still enhance the management and education to reduce or prevent the phone-related accidents, especially for the specific risk groups, novice drivers.|$|R
50|$|Known relaxases are {{metal ion}} {{dependent}} tyrosine transesterases. This <b>means</b> <b>that</b> they <b>use</b> <b>a</b> metal ion {{to aid the}} transfer of an ester bond from the DNA phosphodiester backbone to a catalytic tyrosine side chain, resulting in a long-lived covalent phosphotyrosine intermediate that essentially unified the nicked DNA strand and the enzyme as one molecule. Preliminary reports of relaxase inhibition by small molecules that mimic intermediates of this reaction were first reported in 2007. Such inhibition has implications related to preventing the propagation of antibiotic resistance in clinical settings.|$|R
30|$|As noted, the {{majority}} of the students enrolled at UNE are enrolled in online mode, which <b>means</b> <b>that</b> they <b>use</b> <b>a</b> learning management system (LMS) to access their teaching and learning materials. A variety of ICT resources have been provided through the LMS including instructional resources. Many of these provide links to outside resources, usually secure resources where students are required to <b>use</b> <b>a</b> username and password to enter. However, there has only been a handful of units (subjects) that offered the variety of online tools that will be discussed in this article, including a 3 D virtual world for learning.|$|R
50|$|Azura floats on {{the surface}} of the sea and weighs 45 tons (41 tonnes). It has a unique {{floating}} mechanism that can rotate 360 degress. This enables it to extract power from horizontal (surge) as well as vertical (heave) wave motion. It has reserve buoyancy that is very low, allowing it to partially submerge beneath large waves.Azura is a point absorber. This <b>means</b> <b>that</b> it <b>uses</b> <b>a</b> floating surface mechanism to absorb the energy of waves from different directions. This is the most common type of deepwater wave energy generator. The generator is driven with a high-pressure hydraulics system.|$|R
40|$|We give a {{geometric}} {{interpretation of}} the Khovanov complex for virtual links. Geometric interpretation <b>means</b> <b>that</b> we <b>use</b> <b>a</b> cobordism structure like D. Bar-Natan, but we allow non orientable cobordisms. Like D. Bar-Natans geometric complex our construction should work for virtual tangles too. This geometric complex allows, {{in contrast to the}} geometric version of V. Turaev and P. Turner, a direct extension of the classical Khovanov com-plex (h = t = 0) and of the variant of Lee (h = 0, t = 1). Furthermore we give a classification of all unoriented TQFTs which can b...|$|R
5000|$|Best {{practices}} {{require that}} wherever and however certificate status is maintained, {{it must be}} checked whenever one wants {{to rely on a}} certificate. Failing this, a revoked certificate may be incorrectly accepted as valid. This <b>means</b> <b>that</b> to <b>use</b> <b>a</b> PKI effectively, one must have access to current CRLs. This requirement of on-line validation negates one of the original major advantages of PKI over symmetric cryptography protocols, namely that the certificate is [...] "self-authenticating". Symmetric systems such as Kerberos also depend on the existence of on-line services (a key distribution center in the case of Kerberos).|$|R
40|$|It is {{important}} for institutional research to be undertaken into staff perceptions of service delivery on our campuses. However, literature review suggests a dearth of such studies within the Australasian region, particularly regarding support units of universities. This study reports on an investigation into University staff perceptions of importance, satisfaction and frequency of use of services provided by the Resources Group within a University of Technology. The quantitative analysis indicate <b>that</b> <b>mean</b> importance (<b>using</b> <b>a</b> four point scale) of the services varies from 2. 9 for Legal University services to a maximum figure of 3. 6 for Information Technology Services. On the other hand, mean satisfaction rates (again <b>using</b> <b>a</b> four point scale) vary from a minimum of 2. 7 for Facilities and Services to 3. 7 for Legal Services provided on campus. Statistical inferential analysis suggests that staff perception of Resources Group service importance and satisfaction varies with certain demographic variables. These findings {{can be used to}} effect continuous improvements in future services delivery at the University...|$|R
40|$|Many {{software}} development {{organizations have been}} developing corporate software components to benefit from reusing preexisting solutions instead of creating each of their products always from scratch. However, this has typically been done in an unsystematic way, <b>that</b> <b>means</b> without deliberately <b>using</b> <b>a</b> variability mechanism that supports system evolution. As the number of different products grows, it {{becomes more and more}} difficult to manage the variabilities and their interdependencies. This paper presents an industrial case study of applying PuLSE (Product Line Software Engineering), where we extracted the common and variable characteristics of a set of products from several embedded systems components, and improved their variability mechanisms incrementally...|$|R
40|$|Many {{industries}} developing {{complex products}} based on embedded systems rely on architecting {{as a key}} activity. Furthermore, they use product line approaches to find synergies between their products. This <b>means</b> <b>that</b> they <b>use</b> <b>a</b> base platform which is adapted to different products, and {{the architecture of the}} product line thus evolves over time. In previous case studies we have seen that these companies often lack a defined process for the evolutionary architecting of these product lines. The contribution {{of this paper is to}} present such a process, which matches key characteristics of mature architecting practices. It is also discussed how this process compares to observations in industry...|$|R
5000|$|Sights {{can either}} be mounted in neutral or tilted mounts. In a neutral mount (also known as [...] "flat base" [...] or non-tilted mount) the sight will point {{reasonably}} parallel to the barrel, and be close to a zero at 100 meters (about 1 mil low depending on rifle and caliber). After zeroing at 100 meters the sight will thereafter {{always have to be}} adjusted upwards to compensate for bullet drop at longer ranges, and therefore the adjustment below zero will never be <b>used.</b> This <b>means</b> <b>that</b> when <b>using</b> <b>a</b> neutral mount only about half of the scope's total elevation will be usable for shooting at longer ranges: ...|$|R
40|$|In {{this article}} we present a {{panoramic}} depth imaging system. The system is mosaic-based which <b>means</b> <b>that</b> we <b>use</b> <b>a</b> single rotating camera and assemble the captured images in a mosaic. Due to a setoff of the camera's optical center from the rotational center of the system {{we are able to}} capture the motion parallax effect which enables the stereo reconstruction. The camera is rotating on a circular path with the step defined by an angle, equivalent to one column of the captured image. The equation for depth estimation can be easily extracted from system geometry. To find the corresponding points on stereo pair of panoramic images the epipolar geometry needs to be determined...|$|R
5000|$|Answer of Miss Earth 2001: [...] "Well, {{of course}} {{technology}} {{does make a}} lot of problems for us in the environment. But we can do something to make it better. For example in Denmark, {{we do a lot of}} farming and we <b>use</b> <b>a</b> lot of technology to make it easier for us. But <b>that</b> also <b>means</b> <b>that</b> we <b>use</b> <b>a</b> lot of chemicals that spread and go down to a ground water and makes the ground water worse. So what we can do and what we are doing at the moment is to try to make a regulations on what chemicals are allowed to be used and also thereby make controls of it so we can use those chemicals less." [...] - Catharina Svensson, represented Denmark.|$|R
5000|$|As {{opposed to}} editors like UnrealEd, which <b>use</b> <b>a</b> [...] "subtractive" [...] editing style that takes away areas from a filled world space, the Sandbox has an [...] "additive" [...] style (like Quake II). Objects {{are added to}} an overall empty space. The Sandbox's {{concentration}} on potentially huge (in theory, hundreds of square kilometers) terrain, <b>means</b> <b>that</b> it <b>uses</b> <b>an</b> algorithmic form of painting textures and objects onto the landscape. This uses various parameters to define the distribution of textures or types of vegetation. This is intended to save time and make the editing of such large terrains feasible while maintaining the overall [...] "real world" [...] sandbox free roaming style. This is different from some editing styles that often use [...] "fake backdrops" [...] to give the illusion of large terrains.|$|R
40|$|International audienceThis paper {{introduces}} a new algorithm, namely the Equi-Correlation Network (ECON), to perform supervised classification, and regression. ECON is a kernelized LARS-like algorithm, {{by which we}} <b>mean</b> <b>that</b> ECON <b>uses</b> <b>an</b> $l_ 1 $ regularization to produce sparse estimators, ECON efficiently rides the regularization path to obtain the estimator associated to any regularization constant values, and ECON represents the data by way of features induced by a feature function. The originality of ECON is that it automatically tunes {{the parameters of the}} features while riding the regularization path. So, ECON has the unique ability to produce optimally tuned features for each value of the constant of regularization. We illustrate the remarkable experimental performance of ECON on standard benchmark datasets; we also present a novel application of machine learning in the field of computer graphics, namely the approximation of photometric solids...|$|R
40|$|This report synthesizes {{available}} fire history {{climate change}} scientific knowledge to aid managers with fire decisions in tile face of ongoing 21 st Century cIimate change. Fire history {{and climate change}} mange (FHCC} have been ongoing for over 400 million years of Earth history, but increasing human influences during tile Holocene epoch have changed both climate and fire regimes. We describe basic concepts of climate science and explain the causes of accelerating 21 H Century climate change. Fire regimes and ecosystems classification serve to unify ecological and climate factors influencing fire, and are useful for applying fire history and climate manage information to specific ecosystems. Variable and changing patterns of climate-fire interaction occur over different time and space scales that shape use of FHCC knowledge. Ecosystem differences in fire regimes, climate change and available fire history <b>mean</b> <b>that</b> <b>using</b> <b>an</b> ecosystem specific view will be beneficial when applying FHCC knowledge...|$|R
40|$|Abstract. The paper {{proposes a}} new {{stability}} test for two-dimensional (2 D) discrete sys-tems. It is a tabular test; that is, it builds for the tested bivariate polynomial of degree (n 1, n 2) {{a sequence of}} n 2 (or n 1) bivariate polynomials or matrices (the “ 2 D table”) of increasing row and decreasing column sizes. It is an immittance-type test, which <b>means</b> <b>that</b> it <b>uses</b> <b>a</b> three-term recurrence relation to obtain a sequence of matrices with certain symmetry. It differs from some recent immittance tabular tests {{in that it is}} derived from the author’s stability test for real polynomials instead of complex-coefficient polynomials. In comparison with related 2 D stability tests developed before by Karan and Sarisvastava and by Premaratne, it simplifies the number of stability conditions and reduces the overall cost of computation from an exponential to a polynomial order of complexity...|$|R
40|$|Scheduling {{problem is}} one of the Non-deterministic Polynomial (NP) problems. This <b>means</b> <b>that</b> <b>using</b> <b>a</b> normal {{algorithm}} to solve NP problems is so time-consuming a process (it may take months or even years with available equipment), and thus such an algorithm is regarded as an impracticable way of dealing with NP problems. The method of Memetic Algorithm presented in this paper is different from other available algorithms. In this algorithm the problem of a university class Scheduling is solved through applying a new chromosome structure, modifying the normal genetic methods and adding a local search, which is claimed to considerably improve the solution. We included the teacher, class and course information with their maximal constraints in the proposed algorithm, and it produced an optimized scheduling table for a weekly program of the university after creating the initial population of chromosomes and running genetic operators. The results of the study show a high efficiency for the proposed algorithm compared with other algorithms considering maximum Constraints...|$|R
50|$|MiniDiscs use rewritable {{magneto-optical}} storage {{to store the}} data. Unlike the DCC or the analog Compact Cassette, the disc is a random-access medium, making seek time very fast. MiniDiscs can be edited very quickly even on portable machines. Tracks can be split, combined, moved or deleted with ease either on the player or uploaded to PC with Sony's SonicStage V4.3 software and edited there. Transferring data from an MD unit to a non-Windows machine can only be done in real time, preferably via optical I/O, by connecting the audio out port of the MD to an available audio in port of the computer. With {{the release of the}} Hi-MD format, Sony began to release Macintosh compatible software. However, the Mac compatible software is still not compatible with legacy MD formats (SP, LP2, LP4). This <b>means</b> <b>that</b> <b>using</b> <b>an</b> MD recorded on a legacy unit or in a legacy format still requires a Windows machine for non-real time transfers.|$|R
40|$|In this paper, the {{formability}} of {{a single}} layer E-glass non-crimp 3 D orthogonal woven reinforcement (commercialized under trademark 3 WEAVE by 3 Tex Inc.) is experimentally investigated. The study involves the forming process of the 3 D fabric on two complex moulds, namely tetrahedron and double-dome. The tests are assisted by 3 D digital image correlation measurement to have a continuous registration of the fabric local deformation. Moreover, the results of bending tests in warp and weft direction are detailed to enlarge the mechanical properties data set of the 3 D reinforcement, necessary for understanding its deformability capacities in forming processes. The elevated bending stiffness of the 3 D fabric <b>means</b> <b>that</b> <b>use</b> of <b>a</b> blank-holder during forming is not required. The reinforcement has a good drapability and {{it is able to}} form complex shapes without defects (wrinkles and fibre distortions). The collected experimental results represent an important dataset for numerical simulations of any complex shape with the considered 3 D fabric composite reinforcement...|$|R
40|$|This paper {{describes}} a decentralised approach to social filtering based on trust between agents in a multiagent system. The social filtering {{in the proposed}} approach is built on the interactions between collaborative software agents performing content-based filtering. This <b>means</b> <b>that</b> it <b>uses</b> <b>a</b> mixture of content-based and social filtering and thereby, it takes advantage of both methods. Introduction Today's recommender systems using social filtering are mainly centralised, such as Firefly [Firefly 1997], LikeMinds [LikeMinds 1997], etc. What is suggested {{is a system that}} proposes web documents to its users through decentralised social filtering based on trust. The proposed approach consists of a network of users connected by personal agents. Each agent has a model of its user and based, not only on the content of the documents, but also on the trust for other agents, they propose documents to their users and help each other filter documents. The basis of the proposed approach is the c [...] ...|$|R
40|$|One {{opportunity}} to address our world's environmental challenges {{is to change}} our patterns of consumption towards more sustainable ones, e. g. buying used products, renting products, and joining pools for co-consumption. All of these patterns share at least one point of departure: They imply that people use products that other people have used before. In this context, remanufacturing is a particular opportunity. In principle, remanufacturing <b>means</b> <b>that</b> <b>a</b> <b>used</b> product is industrially renovated in order to assure quality. However, remanufacturing is still just a niche, and the established pattern of consumption and production-involving new products-is very dominant. Reflecting this, {{there is a need}} to better understand how to gain acceptance for remanufactured products, and in particular to understand customers' barriers and drivers for consuming used and remanufactured products. Reflecting this background, the tool presented in this paper is aiming to support remanufacturing organisations to get a better understanding about the customers and their possible ways of reasoning when they approach an offer based on a remanufactured product...|$|R
40|$|The {{aim of this}} {{research}} is the definition of the criteria, the sub-criteria, the evaluation procedures, the methods and the formulas to allocate points for a Most Economically Advantageous Tender in order to perform the 2014 / 24 /EU directive that is ensure compliance with the principles of transparency, non-discrimination and equal treatment, with a view to ensuring an objective comparison of the relative value of the tenders. To do that, four criteria categories were identified. The described method was applied for the editing of a tender documentation with the most economically convenient bid for an integrated contract on a definitive project. The project was realized following the BIM methodology, which <b>means</b> <b>that</b> was <b>used</b> <b>an</b> integrated design system. At the end of the evaluation of the offers, completeness indexes and consistency index of the offers were calculated. The proposed method limits litigations because the procedures of evaluation are clear, explicit, and objective. The high levels of congruency found contain binding offers for the companies that, therefore, cannot change anything from what they proposed during the tender...|$|R
40|$|International audienceMining trajectories (or moving object patterns) from spatio-temporal data is {{an active}} {{research}} field. Most of the researches are devoted to extract trajectories that differ in their structure and characteristic in order to capture dif- ferent object behaviors. The first issue is constituted {{from the fact that}} all these methods extract thousand of patterns resulting in a huge amount of redundant knowledge that poses limit in their usefulness. The second issue is supplied from the nature of spatio-temporal database from which different types of patterns could be extracted. This <b>means</b> <b>that</b> <b>using</b> only <b>a</b> single type of patterns is not sufficient to supply an insightful picture of the whole database. Motivating by these issues, we develop a Minimum Description Length (MDL) -based approach that is able to compress spatio-temporal data combin- ing different kinds of moving object patterns. The proposed method results in a rank of the patterns involved in the summarization of the dataset. In order to validate the quality of our approach, we conduct an empirical study on real data to compare the proposed algorithms in terms of effectiveness, running time and compressibility...|$|R
40|$|Abstract. Many people {{studied the}} Minimum Connected Dominating Set (MCDS) problem to {{introduce}} Virtual Backbone (VB) to wireless networks. However, many existing algorithms assume a static wireless network, and when its topology is changed, compute a new CDS all over again. Since wireless networks are highly dynamic due to many reasons, their approaches can be inefficient in practice. Motivated by this observation, we propose Recyclable CDS Algorithm (RCDSA), an efficient VB maintenance algorithm which {{can handle the}} activeness of wireless networks. The RCDSA is built on an approximation algorithm CDS-BD-C 1 by Kim et. al. [1]. When a node is added to or deleted from current graph, RCDSA recycles current CDS {{to get a new}} one. We prove RCDSA’s performance ratio is equal to CDS-BD-C 1 ’s. In simulation, we compare RCDSA with CDS-BD-C 1. Our results show that the average size of CDS by RCDSA is similar with that by CDS-BD-C 1 but RCDSA is at least three times faster than CDS-BD-C 1 due to its simplicity. Furthermore, at any case, a new CDS by RCDSA highly resembles to its old version than the one by CDS-BD-C 1, which <b>means</b> <b>that</b> <b>using</b> RCDSA, <b>a</b> wireless network labors less to maintain its VB when its topology is dynamically changing. ...|$|R
40|$|Abstract. In {{this paper}} {{we present a}} {{panoramic}} depth imaging system. The system is mosaic-based which <b>means</b> <b>that</b> we <b>use</b> <b>a</b> single rotating camera and assemble the captured images in a mosaic. Due to a setoff of the camera’s optical center from the rotational center of the system {{we are able to}} capture the motion parallax effect which enables the stereo reconstruction. The camera is rotating on a circular path with the step defined by an angle equivalent to one column of the captured image. The equation for depth estimation can be easily extracted from system geometry. To find the corresponding points on a stereo pair of panoramic images the epipolar geometry needs to be determined. It can be shown that the epipolar geometry is very simple if we are doing the reconstruction based on a symmetric pair of stereo panoramic images. We get a symmetric pair of stereo panoramic images when we take symmetric columns on the left and on the right side from the captured image center column. Epipolar lines of the symmetrical pair of panoramic images are image rows. We focused mainly on the system analysis. The system performs well in the reconstruction of small indoor spaces...|$|R
40|$|In {{this chapter}} {{we present a}} stereo {{panoramic}} depth imaging system, which builds depth panoramas from multiperspective panoramas while using only one standard camera. The basic system is mosaic-based, which <b>means</b> <b>that</b> we <b>use</b> <b>a</b> single standard rotating camera and assemble the captured images in a multiperspective panoramic image. Due to a setoff of the camera’s optical center from the rotational center of the system, {{we are able to}} capture the motion parallax effect, which enables the stereo reconstruction. The system has been comprehensively analysed. The analyses include the study of influence of different system parameters on the reconstruction accuracy, constraining the search space on the epipolar line, meaning of error in estimation of corresponding point, definition of the maximal reliable depth value, contribution of the vertical reconstruction and influence of using different cameras. They are substantiated with a number of experiments, including experiments addressing the baseline, the repeatability of results in different rooms, by using different cameras, influence of lens distortion presence on the reconstruction accuracy and evaluation of different models for estimation of system parameters. The analyses and the experiments revealed a number of interesting properties of the system. According to the basic system accuracy we definitely can use the system for autonomous robot localization and navigation tasks...|$|R
