2050|10000|Public
5|$|A Woods Hole Oceanographic Institution {{study in}} January 2009 found that emperor {{penguins}} could be {{pushed to the}} brink of extinction by the year 2100 due to global climate change. The study constructed a mathematical model <b>to</b> <b>predict</b> <b>how</b> the loss of sea ice from climate warming would affect a big colony of emperor penguins at Terre Adélie, Antarctica. The study forecasted an 87% decline in the colony's population, from three thousand breeding pairs in 2009 to four hundred breeding pairs in 2100.|$|E
5|$|Huntington's {{disease is}} a {{neurodegenerative}} genetic disorder {{that is associated with}} protein misfolding and aggregation. Excessive repeats of the glutamine amino acid at the N-terminus of the Huntingtin protein cause aggregation, and although the behavior of the repeats is not completely understood, it does lead to the cognitive decline associated with the disease. As with other aggregates, there is difficulty in experimentally determining its structure. Scientists are using Folding@home to study the structure of the Huntingtin protein aggregate and <b>to</b> <b>predict</b> <b>how</b> it forms, assisting with rational drug design methods to stop the aggregate formation. The N17 fragment of the Huntingtin protein accelerates this aggregation, and while there have been several mechanisms proposed, its exact role in this process remains largely unknown. Folding@home has simulated this and other fragments to clarify their roles in the disease. Since 2008, its drug design methods for Alzheimer's disease have been applied to Huntington's.|$|E
5|$|Making these {{predictions}} is not trivial, {{even for}} simple systems. For example, oxaloacetate is formed by malate dehydrogenase within the mitochondrion. Oxaloacetate {{can then be}} consumed by citrate synthase, phosphoenolpyruvate carboxykinase or aspartate aminotransferase, feeding into the citric acid cycle, gluconeogenesis or aspartic acid biosynthesis, respectively. Being able <b>to</b> <b>predict</b> <b>how</b> much oxaloacetate goes into which pathway requires knowledge of the concentration of oxaloacetate {{as well as the}} concentration and kinetics of each of these enzymes. This aim of predicting the behaviour of metabolic pathways reaches its most complex expression in the synthesis of huge amounts of kinetic and gene expression data into mathematical models of entire organisms. Alternatively, one useful simplification of the metabolic modelling problem is to ignore the underlying enzyme kinetics and only rely on information about the reaction network's stoichiometry, a technique called flux balance analysis.|$|E
5000|$|Jobs not {{requiring}} physical presence. One {{character is}} a [...] "Reacter," [...] someone who samples new products and reports her reactions using the Joymaker. The central computer analyzes her reactions {{in the light of}} her known psychological makeup and is able <b>to</b> statistically <b>predict</b> <b>how</b> well the product will sell.|$|R
50|$|Geographically, UTIG's scope {{includes}} the ocean basins, continental margins, Antarctica, and all sites of seismic activity. Chronologically, its scope {{is no less}} vast: from the development of tectonic evolution models that reconstruct continental arrangements {{as much as a}} billion years ago <b>to</b> <b>predicting</b> <b>how</b> future climatic scenarios would impact sea-level changes and thus the habitability of densely populated coastal regions. The Institute's research is highly relevant to natural resource exploration, the assessment of geologic hazards, and the mitigation of environmental damage. The development of new mathematical models, data processing and imaging techniques, and geophysical instrumentation is also an integral part of UTIG's ongoing research and future goals.|$|R
5000|$|The computer-scored DET {{has faced}} criticism. [...] Though {{the test is}} being marketed to {{universities}} as a valid assessment of academic English proficiency , a 2015 exam critique {{published in the journal}} Language Assessment Quarterly found that it was [...] "woefully inadequate" [...] for this purpose. The DET has since added new features like an integrated video interview requiring the test taker to produce speech in response to on-screen questions. Research has also indicated that the DET may outperform the TOEFL when it comes <b>to</b> <b>predicting</b> <b>how</b> the ESL faculty who support students on college campuses will rate the English proficiency and the support needs of incoming international students.|$|R
25|$|Engineers {{typically}} attempt <b>to</b> <b>predict</b> <b>how</b> well {{their designs}} will perform to their specifications prior to full-scale production. They use, among other things: prototypes, scale models, simulations, destructive tests, nondestructive tests, and stress tests. Testing ensures that products will perform as expected.|$|E
25|$|The fate of {{incoming}} pharmaceutical residues in the STP is unpredictable. Some substances {{seem to be}} more or less completely eliminated, while others pass the different steps in the STP unaffected. There is no systematic knowledge at hand <b>to</b> <b>predict</b> <b>how</b> and why this happens.|$|E
25|$|It {{has been}} {{suggested}} that when a side batting second successfully completes the run chase, the D/L method could be used <b>to</b> <b>predict</b> <b>how</b> many runs they would have scored with a full innings (i.e. 50 overs in a One Day International), and use this prediction in the net run rate calculation.|$|E
50|$|In 2003, Michael R. Douglas's {{discovery}} of the string theory landscape, which suggests that string theory has {{a large number of}} inequivalent false vacua, led to much discussion of what string theory might eventually be expected <b>to</b> <b>predict,</b> and <b>how</b> cosmology can be incorporated into the theory.|$|R
50|$|Due to {{complicated}} {{interaction of}} phase and frequency {{it is difficult}} <b>to</b> <b>predict</b> exactly <b>how</b> the reduction from two channels to one will affect {{the sound of a}} particular instrument. Therefore mono sound from a true mono mix is preferable {{to the use of the}} Haeco-CSG stereo to mono process.|$|R
30|$|It {{has been}} {{reported}} that the load for (or to) implant-supported restoration may lead to bone remodeling as bone resorption and/or formation. While many authors supported the process of bone resorption, others elaborated bone apposition and increasing bone density close and remote to implant body (or fixture). This may suggest the role of the implant to reserve alveolar ridge from physiologic/pathologic resorption. The aim of this systematic review was <b>to</b> <b>predict</b> <b>to</b> <b>how</b> extend dental implants can preserve the residual alveolar ridge based on previous clinical investigations.|$|R
25|$|PageRank {{has been}} used to rank spaces or streets <b>to</b> <b>predict</b> <b>how</b> many people (pedestrians or vehicles) come to the {{individual}} spaces or streets. In lexical semantics it {{has been used}} to perform Word Sense Disambiguation, Semantic similarity, and also to automatically rank WordNet synsets according to how strongly they possess a given semantic property, such as positivity or negativity.|$|E
25|$|By 2100, emperor {{penguins}} {{could be}} {{pushed to the}} brink of extinction due to global climate change, according to a Woods Hole Oceanographic Institution study from January 2009. The study applied mathematical models <b>to</b> <b>predict</b> <b>how</b> the loss of sea ice from climate warming would affect an Antarctica colony of emperor penguins, they forecast a decline of 50% {{by the end of the}} century.|$|E
25|$|Very recently, {{there has}} been a rise in {{scientific}} investigations into stereopsis recovery in adults and youths who have had no stereo vision before. While it has now been shown that an adult may gain stereopsis, it is currently not yet possible <b>to</b> <b>predict</b> <b>how</b> likely a stereoblind person is to do so, nor is there general agreement on the best therapeutic procedure. Also the possible implications for the treatment of children with infantile esotropia are still under study.|$|E
50|$|PUFF-PLUME is a {{model used}} <b>to</b> help <b>predict</b> <b>how</b> air {{pollution}} disperses in the atmosphere. It is a Gaussian atmospheric transport chemical/radionuclide dispersion model that includes wet and dry deposition, real-time input of meteorological observations and forecasts, dose estimates from inhalation and gamma shine (i.e., radiation), and puff or continuous plume dispersion modes. It was first developed by the Pacific Northwest National Laboratory (PNNL) in the 1970s.|$|R
30|$|Closely {{related and}} ecologically similar species that overlap in ranges can coexist through {{resource}} partitioning without one pushing {{the others to}} extinction through competition. Understanding resource partitioning among species is essential <b>to</b> <b>predicting</b> <b>how</b> species decline can affect the functioning of communities and ecosystems. In this study, we analyzed niche overlap and resource partitioning of three tree-climbing bird species in disturbed and undisturbed forest sites at La Malinche National Park, Tlaxcala, Mexico. From January to December 2008, resource partitioning between the three species was examined through the frequency of sightings of individuals foraging in different sites in the trees of both forest types. We characterized the pattern of resource utilization by niche breadth and niche overlap. Finally, we tested if these birds divide tree space differentially according to forest type.|$|R
40|$|Understanding the {{scaling of}} {{transmission}} is critical <b>to</b> <b>predicting</b> <b>how</b> infectious diseases will affect populations {{of different sizes}} and densities. The two classic “mean-field” epidemic models—either assuming density-dependent or frequency-dependent transmission—make predictions that are discordant with patterns seen in either within-population dynamics or across-population comparisons. In this paper, we propose {{that the source of}} this inconsistency lies in the greatly simplifying “mean-field” assumption of transmission within a fully-mixed population. Mixing in real populations is more accurately represented by a network of contacts, with interactions and infectious contacts confined to the local social neighborhood. We use network models to show that density-dependent transmission on heterogeneous networks often leads to apparent frequency dependency in the scaling of transmission across populations of different sizes. Network-methodology allows us to reconcile seemingly conflicting patterns of within- and across-population epidemiology...|$|R
25|$|Scientists and historians, for example, {{study the}} {{objective}} world, hoping to elicit {{the truth of}} nature—or perhaps the truth of history. In this way, they hope <b>to</b> <b>predict</b> <b>how</b> the future will unfold in accordance with these laws. In terms of history, by studying the past, the individual can perhaps elicit the laws that determine how events will unfold—in this way the individual can predict the future with more exactness and perhaps take control of events {{that in the past}} appeared to fall outside the control of humans.|$|E
25|$|Communicator {{characteristics}} {{include all}} {{of the features of}} the individual, such as personality, physical appearance, and communication style. These characteristics can lead the partner <b>to</b> <b>predict</b> <b>how</b> the other will communicate in a “dating” situation. The relationship factor involves characteristics that describe {{the relationship between the two}} individuals. These factors include the degree of familiarity, liking, attraction, or similarity. The context feature involves aspects like the environment and the situations the individuals are in such as privacy, formality, and task oriented. These situations either enhance or diminish the interaction on the date and help maintain and structure individual’s goals.|$|E
25|$|Snell's Law {{can be used}} {{to predict}} the {{deflection}} of light rays as they pass through linear media as long as the indexes of refraction and the geometry of the media are known. For example, the propagation of light through a prism results in the light ray being deflected depending on the shape and orientation of the prism. In most materials, the index of refraction varies with the frequency of the light. Taking this into account, Snell's Law can be used <b>to</b> <b>predict</b> <b>how</b> a prism will disperse light into a spectrum. The discovery of this phenomenon when passing light through a prism is famously attributed to Isaac Newton.|$|E
40|$|The {{quality of}} {{semi-finished}} products, {{as they are}} used in thermoforming, highly depends on the raw material used and their processing history. As a consequence, variations in sheet quality are often observed with their influence on processibility and product properties. Currently, there is no standard test for thermoforming sheet materials available, which means {{it is not possible}} <b>to</b> accurately <b>predict</b> <b>how</b> a material will behave in thermoforming. This study deals with the prediction of the thermoformability of sheet materials. A novel test method (Thermoforming Material Characterization TMC) <b>to</b> <b>predict</b> thermoformability is presented...|$|R
40|$|Wakes {{are present}} in many {{engineering}} flows. These flows include internal flows such as mixing chambers and turbomachinery as well as external flows like flow over high-lift or multi-element airfoils. Many times these wakes are exposed to flow conditions such as adverse pressure gradients and streamline curvature that alter the mean flow and turbulent structure of the wake. The ability to understand how pressure gradients and streamline curvature affects {{the structure of the}} wake is essential <b>to</b> <b>predicting</b> <b>how</b> the wake will affect the performance of the application in which it is found. The effects of pressure gradients and curvature of low-speed wakes has been ex-tensively documented. As the transonic flow regime is becoming of more interest as gas speeds in turbomachinery increase this work fills a void in the body of wake knowledge pertaining to curve...|$|R
40|$|This is an {{interesting}} paper. In a nutshell, the authors look at past financial crises to draw out implications for {{what is likely to}} happen today. Overall, the end product is a very useful reference and deserves a place alongside important exercises of this vein, including the seminal work in this area by Reinhart and Rogoff (2009). There are two principal findings relevant to the current events: First, at the heart of any financial crisis is a banking crisis; {{and most of the time}} a banking crisis is associated with a significant contraction in output. Second, the current crisis is largely unique. That is, there is not much regularity in previous financial crises that appears readily applicable <b>to</b> <b>predicting</b> <b>how</b> the current crisis will play out. Both these conclusions can be illustrated by examining a statistical model that the authors use <b>to</b> <b>predict</b> the length of the current recession for an economy like the U. S. L = (6. 03) + (5. 84) · A − (0. 93) · ∆Y −...|$|R
25|$|On its face, {{quantum field}} theory allows {{infinite}} numbers of particles, and leaves {{it up to the}} theory itself <b>to</b> <b>predict</b> <b>how</b> many and with which probabilities or numbers they should exist. When developed further, the theory often contradicts observation, so that its creation and annihilation operators can be empirically tied down. Furthermore, empirical conservation laws like that of mass-energy suggest certain constraints on the mathematical form of the theory, which are mathematically speaking finicky. The latter fact both serves to make quantum field theories difficult to handle, but has also lead to further restrictions on admissible forms of the theory; the complications are mentioned below under the rubrik of renormalization.|$|E
25|$|Zootopia was {{the second}} time Disney used the Hyperion renderer, which they had first used on Big Hero 6. A new fur {{paradigm}} was added to the renderer to facilitate the creation of realistic images of the animals' dense fur. Nitro, a real-time display application developed since the making of Wreck-It Ralph, was used to make the fur more consistent, intact and subtle much more quickly, as opposed to the previous practice of having <b>to</b> <b>predict</b> <b>how</b> the fur would work while making and looking at silhouettes or poses for the character. The tree-and-plant generator Bonsai, first used in Frozen, was used to make numerous variations of trees with very detailed foliage.|$|E
25|$|The {{information}} about the orbit can be used <b>to</b> <b>predict</b> <b>how</b> much energy (and angular momentum) would be radiated {{in the form of}} gravitational waves. As the energy is carried off, the stars should draw closer to each other. This effect is called an inspiral, and it can be observed in the pulsar's signals. The measurements on the Hulse–Taylor system have been carried out over more than 30 years. The change in the orbital period matches the prediction from the gravitational radiation assumed by general relativity to within 0.2 percent. In 1993, Russell Hulse and Joe Taylor were awarded the Nobel Prize in Physics for this work, which was the first indirect evidence for gravitational waves. The lifetime of this binary system, from the present to merger is estimated to be a few hundred million years.|$|E
30|$|If {{this theory}} is valid, the {{assumption}} that the aforementioned coefficients are constant in the porous medium is not correct and it is necessary a full scale of future experiments <b>to</b> <b>predict</b> better understanding <b>how</b> they change with the flow regime.|$|R
40|$|This paper {{describes}} an original experimental procedure {{to extract the}} phase sensitivity of oscillators to noise perturbations. The proposed method relies on measuring {{the width of the}} locking ranges over which the oscillator's response synchronizes with injected small-amplitude signals. It is shown that this sensitivity function can be employed <b>to</b> accurately <b>predict</b> <b>how</b> inner noise sources are transferred to output phase-noise and jitter. The extraction procedure is applied to a relaxation oscillator that exhibits a strongly nonlinear behavior...|$|R
5000|$|The {{most common}} use of TGI Role-Based Assessment is in pre-hire {{screening}} evaluations. RBA’s focus on ‘teaming’ behavior offers {{a different way}} <b>to</b> allegedly <b>predict</b> <b>how</b> an individual will fit with company culture, on a given team, {{and how they are}} likely to respond to specific job requirements. [...] While other pre-hire testing may run the [...] "risk of violating the ADA" [...] (Americans with Disabilities Act), this {{does not appear to be}} an issue with Role-Based Assessment.|$|R
25|$|A new {{scientific}} understanding of collective intelligence defines it as a group's general {{ability to perform}} {{a wide range of}} tasks. Definition, operationalization and statistical methods are similar to the psychometric approach of general individual intelligence. Hereby, an individual's performance on a given set of cognitive tasks is used to measure general cognitive ability indicated by the general intelligence factor g extracted via factor analysis. In the same vein as g serves to display between-individual performance differences on cognitive tasks, collective intelligence research aims to find a parallel intelligence factor for groups c factor' (also called 'collective intelligence factor' (CI)) displaying between-group differences on task performance. The collective intelligence score then is used <b>to</b> <b>predict</b> <b>how</b> this same group will perform on any other similar task in the future. Yet tasks, hereby, refer to mental or intellectual tasks performed by small groups even though the concept is hoped to be transferrable to other performances and any groups or crowds reaching from families to companies and even whole cities. Since individuals' g factor scores are highly correlated with full-scale IQ scores, which are in turn regarded as good estimates of g, this measurement of collective intelligence can also be seen as an intelligence indicator or quotient respectively for a group (Group-IQ) parallel to an individual's intelligence quotient (IQ) even though the score is not a quotient per se.|$|E
500|$|One {{important}} {{variation is}} [...] "group dynamics analysis". Some jury selection {{is concerned with}} the attitudes and bias of individuals. Some trial consultants also try <b>to</b> <b>predict</b> <b>how</b> individuals will form themselves into groups in the jury and which jurors will become leaders and followers in those groups. Consultants also use this tool after jury selection is over.|$|E
500|$|Many {{different}} kinds of analysis are performed on ice cores, including visual layer counting, tests for electrical conductivity and physical properties, and assays for inclusion of gases, particles, radionuclides, and various molecular species. [...] For {{the results of these}} tests to be useful in the reconstruction of palaeoenvironments, {{there has to be a}} way to determine the relationship between depth and age of the ice. [...] The simplest approach is to count layers of ice that correspond to the original annual layers of snow, but this is not always possible. [...] An alternative is to model the ice accumulation and flow <b>to</b> <b>predict</b> <b>how</b> long it takes a given snowfall to reach a particular depth. [...] Another method is to correlate radionuclides or trace atmospheric gases with other timescales such as periodicities in the earth's orbital parameters.|$|E
50|$|When {{the nuclear}} {{industry}} was just getting started in the early 1950s, it was difficult <b>to</b> <b>predict</b> exactly <b>how</b> different kinds of metals and other materials {{would be affected by}} being used in a reactor for prolonged periods of time. MTR was a research reactor jointly designed by Argonne and Oak Ridge National Laboratories that operated until 1970 and provided important data, helping researchers make nuclear power reactors safer and longer lasting.|$|R
40|$|License, which permits {{unrestricted}} use, distribution, {{and reproduction}} in any medium, provided the original work is properly cited. Understanding the scaling of transmission is critical <b>to</b> <b>predicting</b> <b>how</b> infectious diseases will affect populations {{of different sizes}} and densities. The two classic “mean-field ” epidemic models—either assuming density-dependent or frequency-dependent transmission—makepredictions that are discordant with patterns seen in either within-population dynamics or across-population comparisons. In this paper, we propose {{that the source of}} this inconsistency lies in the greatly simplifying “mean-field ” assumption of transmission within a fully-mixed population. Mixing in real populations is more accurately represented by a network of contacts, with interactions and infectious contacts confined to the local social neighborhood. We use network models to show that density-dependent transmission on heterogeneous networks often leads to apparent frequency dependency in the scaling of transmission across populations of different sizes. Network-methodology allows us to reconcile seemingly conflicting patterns of within- and across-population epidemiology. 1...|$|R
40|$|A new {{analysis}} {{approach produces}} visualizations to help development teams identify and prioritize performance issues {{by focusing on}} performance early in the development cycle, evaluating progress, identifying defects, and estimating timelines. // ReAL-WoRLd PeRfoRmAnce is an aspect of software quality that his-torically has been diffi cult to measure. Software developers have devoted enor-mous amounts {{of time and effort}} <b>to</b> effectively <b>predict</b> <b>how</b> well a piece of software will perform under various real-world conditions. It’s especially diffi cult to evaluate performance for applications that rely on human com...|$|R
