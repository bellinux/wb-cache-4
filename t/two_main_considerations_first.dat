2|10000|Public
40|$|Climate change, {{operating}} through related {{physical changes}} (e. g. sea level, ocean temperature) has biological implications (e. g. changes in primary productivity) {{which in turn}} produce direct impacts on human uses of the ocean (e. g., fishing, tourism, ports) and broader induced impacts on human society (e. g., social, economic, community). In fisheries, the {{effects of climate change}} are bound to interact with the effects of fishing, in their cumulative impacts on fish stocks, and on human aspects of the fishery system. It {{is important to recognize that}} the overall approach to, and the specific components of, fisheries management will have major effects on this interaction. Accordingly, this paper explores <b>two</b> <b>main</b> <b>considerations.</b> <b>First,</b> socioeconomic and behavioural aspects of fisheries need to be monitored in the face of climate change, as these will likely have strong management and assessment implications. Second, the need to address the combination of climate change and fishing as forcing factors in fisheries reinforces the necessity for adopting a broad-based precautionaryapproach to management decision making, and for re-designing management systems so that their structure and methods are more robust and adaptive...|$|E
40|$|Póster presentado en el 3 rd European Meeting in Physiological Optics, London, September 7 - 9, 2006. Corneal {{analysis}} is usually done under <b>two</b> <b>main</b> <b>considerations.</b> <b>First</b> one consists of ignoring the second {{surface of the}} element, thus considering the cornea as a single diopter which isolates water from an aqueous medium with an equivalent keratometric index. Second approach consists of considering exact ray tracing and obtaining dot impact maps. Alternatively, differences of optical paths are measured over ray trajectories and thus the transmittance function is calculated. Main problems that may produce such approximations are that the second corneal surface is not taken into account, thus the model has not biological correspondence. From the second approach, ray distribution at the output plane is not uniformly sampled and rearrangement of emerging rays is needed. In this work we analyze the validity of both approximations by comparing those models with a realistic cornea. We propose a solution to overcome resampling at the output plane. We also propose several approximations over the optical path evaluation that may simplify calculation complexity. We will show that minimizing absolute differences in the optical path evaluation {{is not a good}} metrics to obtaining the best fitting corneal model. This work has been partially supported by the Spanish Ministerio de Educación y Ciencia, through the project FIS 2005 - 05053...|$|E
50|$|There are <b>two</b> <b>main</b> thermal <b>considerations.</b> The <b>first</b> is the {{question}} of insulating a floor slab. In older buildings, concrete slabs cast directly on the ground can drain heat from a room. In modern construction, concrete slabs are usually cast above a layer of insulation such as expanded polystyrene, and the slab may contain underfloor heating pipes. However, there are still uses for a slab that is not insulated , typically in outbuildings which are not heated or cooled to room temperature. In those cases, casting the slab directly onto a rocky substrate will maintain the slab at or near the temperature of the substrate throughout the year, and can prevent both freezing and overheating.|$|R
50|$|There are <b>two</b> <b>main</b> <b>considerations</b> for the {{location}} of an explosion: height and surface composition. A nuclear weapon detonated in the air, called an air burst, produces less fallout than a comparable explosion near the ground.|$|R
50|$|With {{advances}} in cancer immunotherapy such as immune checkpoint inhibitors, increased interest {{was given to}} the prospect of oncolytic viruses as immunotherapies. There are <b>two</b> <b>main</b> <b>considerations</b> of the interaction between oncolytic viruses and the immune system.|$|R
30|$|We present {{here the}} <b>two</b> <b>main</b> theorems under <b>consideration.</b>|$|R
40|$|With the {{emergence}} of E-Commerce communication security has become a very important issue. <b>Two</b> <b>main</b> <b>considerations</b> of secure communication systems are authentication, and key distribution. Authentication and key distribution may differ from one system to another due to the system parameters such as bandwidth, and available processing power at the end terminals. This paper focuses on end-to-end authentication and key management strategies in wireless and wired systems. Public and secret key encryption techniques are used to provide authentication and key distribution...|$|R
40|$|The paper {{investigates the}} effects of {{production}} scheduling policies aimed towards improving productive and environmental performances in a job shop system. A green genetic algorithm allows the assessment of multi-objective problems related to sustainability. <b>Two</b> <b>main</b> <b>considerations</b> have emerged from {{the application of the}} algorithm. First, the algorithm is able to achieve a semi-optimal makespan similar to that obtained by the best of other methods but with a significantly lower total energy consumption. Second, the study demonstrated that the worthless energy consumption can be reduced significantly by employing complex energy-efficient machine behaviour policies...|$|R
5000|$|Two of the internationally {{acknowledged}} [...] "Green" [...] certificates for decorative laminates are MAS Certified Green and GREENGUARD. The MAS Certified Green and GREENGUARD marks are {{to certify}} that the products have low chemical emissions. Chemicals tested include VOCs, formaldehyde and other harmful particles. The tests {{are based on}} single occupancy room with outdoor ventilation following the ANSI/ASHRAE Standard 62.1-2007, Ventilation for Acceptable Indoor Air Quality. GREENGUARD especially, has <b>two</b> <b>main</b> <b>consideration,</b> GREENGUARD and GREENGUARD GOLD. The GREENGUARD n GOLD was previously known as the GREENGUARD Children and Schools Certified, signifying its relevance of very low allowable chemical emissions levels {{to ensure the safety}} of young children and school environment.|$|R
40|$|Environmental {{responsibility}} (ER), {{as a major}} {{aspect of}} corporate social responsibility, {{is an important issue}} for organizations in the sport industry as it is for all other organizations. This paper presents three reasons why sport industry firms should embrace environmental responsibility as a management competency. The first is the ethical reason, which consists of <b>two</b> <b>main</b> <b>considerations</b> and applies not just to firms in the sport industry but to all organizations. The second reason pertains specifically to sport organizations and invokes their unique relationship to their customers. The third reason is that embracing ER can lead to economic benefits for the organization. <b>Two</b> <b>main</b> aspects of this advantage –cost savings due to more efficient resource usage and enhanced image – are discussed...|$|R
50|$|When {{deciding}} the linker fusion {{site on the}} protein surface, there are three <b>main</b> <b>considerations.</b> <b>First,</b> the fluorescent protein fragments {{must be able to}} associate with one another when their tethered proteins interact. Structural information and the location of the interaction surface may be useful when determining the fusion site to the linker, although the information is not necessary, as multiple combinations and permutations can be screened. Secondly, the creation of the fusion protein must not significantly alter the localisation, stability, or expression of the proteins to which the fragments are linked as compared to the endogenous wild-type proteins. Finally, the addition of the fluorescent fragment fusion must not affect the biological function of the protein, preferably verified using assays that evaluate all of the proteins' known functions.|$|R
50|$|There are {{at least}} <b>two</b> <b>main</b> <b>considerations</b> when {{performing}} collocations.The first is the sampling pattern of the instrument.Measurements may be dense and regular, such as those from a cross-trackscanning satellite instrument. In this case, some form of interpolationmay be appropriate. On the other hand, the measurements may besparse, such as a one-off field campaign designed for someparticular validation exercise.The second consideration is the instrument footprint, whichcan range from something approaching a point measurementsuch {{as that of a}} radiosonde, or it might be severalkilometers in diameter such as that of a satellite-mounted,microwave radiometer. In the latter case, it is appropriateto take into account the instrument antenna pattern whenmaking comparisons with another instrument having both a smallerfootprint and a denser sampling, that is, several measurementsfrom the one instrument will fit into the footprint of the other.|$|R
40|$|This Forum - Responding to Conflict - {{concentrates}} {{on the issue of}} humanitarian intervention and its legal context. <b>Two</b> <b>main</b> <b>considerations</b> are given attention; the responsibility of nation-states to protect their civilian population from the consequences of natural disasters and armed conflicts, and secondly, the legality and justification of external interventions in the event of nation-state failures to act responsibly. This session will also examine the use of state sovereignty to mask internal humanitarian problems, and awareness of them, among the international community. Speakers: 	Grant Niemann - Senior Lecturer at Flinders University and Chair of the Red Cross International Humanitarian Law Committee in South Australia 	Phoebe Wynn-Pope - has undertaken project and emergency coordination roles respectively with the Australian Red Cross and CARE Australia 	Craig Jurisevic - Trauma Surgeon and former combatant in Kosovo          ...|$|R
40|$|According to Michel Foucault, modernity is {{predicated}} on {{the emergence of an}} instrumental idea of knowledge, which does not affect the constitution of the individual as a subject. This article aims to explore this thesis in the context of British Higher Education through a problematization of widening participation policies, and how they have been increasingly constructed in economic-instrumental terms. This approach suggests <b>two</b> <b>main</b> <b>considerations</b> within the framework of Foucault's argument. First, widening participation initiatives have contributed to reinforce an idea of knowledge as an instrumental set of notions external to the subject rather than a process of transformation of the self. Second, widening participation initiatives have been dominated by a neoliberal approach to the problem of inequality which has turned students into seemingly equal consumers of knowledge. However, it will be argued, this approach contributes to reproduce in different ways the inequality gap between students of different socioeconomic backgrounds...|$|R
40|$|This paper {{deals with}} {{issues related to}} {{management}} of industrial research. The overall research question is how industrial researchers can be managed to increase the company’s benefits. The relevance of this question is put into perspective by <b>two</b> <b>main</b> <b>considerations.</b> On the one hand, it is widely recognized that individual researchers should possess {{a high level of}} autonomy to preserve the ability of research to renew itself. On the other hand, companies need to maintain control over that freedom to develop their research activities in a longterm company perspective. The paper contributes to {{a deeper understanding of the}} management of industrial research by moving the focus from risk management and portfolio planning (decision and control based management) to management in situations marked by high uncertainty and asymmetric distribution of information (management of self-governing systems). More specifically, the evolutionary perspective on individual adaptation is used in combination with evolutionary economics t...|$|R
5000|$|An 1849 {{release of}} The Calcutta Review (a {{periodical}} now {{published by the}} University of Calcutta), stated the following about Gemelli's writings concerning India: [...] "In a previous number of this Review we made an attempt to describe something of the Court and Camp {{of the best and}} wisest prince Muhainmedan India had ever beheld (Aurungzebe, Mogul emperor of Hindustan)... To this we are urged by <b>two</b> <b>main</b> <b>considerations,</b> the character of the age, and the materials at our command.... Sir H. M. Elliot's work has... met with, to a certain extent, adverse criticism, and some doubts have been raised as to the soundness, or the justice, of its conclusions. It is therefore (possible that readers may be willing) to peruse a description of the Government of Aurungzebe, taken not from native historians, but from the accounts of men who saw with the eyes of travelers.|$|R
30|$|Financial {{intermediation}} {{represents the}} fundamental mission of banks to mobilize deposits into credit for economic operators. This paper {{was motivated by}} <b>two</b> <b>main</b> <b>considerations</b> in scholarly and policy circles: (1) the ongoing debate {{about the relationship between}} bank size and the efficiency of financial intermediation, and (2) gaps in the existing literature regarding this subject. Questions about the role of bank size in improving efficiency in the banking sector are reflected in the work of various researchers, including Asongu et al. (2016); Boateng et al. (2018), and Asongu and Odhiambo (2018). Existing research maintains that some big banks might abuse their market power instead of leveraging economies of scale to increase their efficiency in financial intermediation. The mechanisms for enhancing financial intermediation include, inter alia, increasing the quantity of loans, decreasing the price of loans (i.e., interest rates and fees), and reducing information asymmetry between borrowers and lenders (Kusi et al. 2017; Kusi and Opoku Mensah 2018; Tchamyou 2018 a, b).|$|R
40|$|In {{this paper}} some cases of {{distribution}} in Coleoptera Meloidae, possibly referable to the distributional “Mediterranean-southern African disjunct model”, are examined and discussed. At least four genera, two {{belonging to the}} subfamily Meloinae, tribe Mylabrini (Actenodia Laporte de Castelnau, 1840 and Ceroctis Marseul, 1870), and two to the subfamily Nemognathinae, tribe Nemognathini (Sitaris Latreille, 1802 and Stenoria Mulsant, 1857), have ranges which include both the Mediterranean-Saharan area and eastern and southern Africa. Other examples, briefly examined, concern taxa of Meloinae or Nemognathinae which are primarily Mediterranean or Saharan, with isolated subranges also in the eastern Africa. <b>Two</b> <b>main</b> <b>considerations</b> seem evident from this analysis: 1) a distributional disjunct model with only Mediterranean and South African related species does not exist among Meloidae, but at most genera which have the greatest diversity in these two areas and relict species in eastern Africa; 2) the type of disjunct distribution observed in some genera of Meloidae seems referable to convergent biogeographical phenomena more than a late, single vicariance event...|$|R
40|$|Third {{generation}} wireless systems typically employ {{adaptive coding}} and modulation, scheduling, and Hybrid Automatic Repeat reQuest (HARQ) techniques to provide high-speed packet data {{service on the}} downlink. <b>Two</b> <b>main</b> <b>considerations</b> in designing such a system are algorithms for the selection of coding and modulation schemes based on the channel quality of the link and algorithms for {{the selection of the}} user to whom a particular slot is assigned. We propose a systematic approach to optimize the mapping between signal-to-interference-and-noise ratio (SINR) and modulation and coding scheme (MCS) to maximize the throughput by taking into account the type of HARQ scheme employed. We also propose to incorporate frame error rate (FER) and retransmission information {{as a part of the}} scheduling decision. The proposed scheduler ranking methods based on using an effective rate rather than the instantaneous rate provide natural priority to retransmissions over new transmissions, and priority to users with better channel quality. Extensive simulation results comparing performance of the proposed methods to conventional methods are presented...|$|R
40|$|Preliminary and incomplete: not for quotation. This paper {{analyses}} over 100 vintages of National Accounts output data. The {{overwhelming majority}} of this raw data was assembled and provided to us (as unofficial statistics) by Peter Rossiter from the Analytical Services Branch of the Australian Bureau of Statistics, without whose generous assistance this project {{would not have been}} possible. We {{would also like to thank}} David Gruen for helpful comments. The views expressed in this paper are solely those of the authors and should not be attributed to the Reserve Bank of Australia. In this paper we consider the implications of using a theoretical concept like an ‘output gap’- which measures the difference between the economy’s actual level of output relative to ‘potential’- as part of the policy formation process. In particular, we highlight <b>two</b> <b>main</b> <b>considerations</b> in using Australian national accounts data to inform policymakers. Firstly, these data are subject to revision, the size of which can be substantial and which may only occur gradually over many years. Secondly...|$|R
40|$|Stability and {{efficiency}} (i. e. derivative function evaluations per processor) are the <b>two</b> <b>main</b> <b>considerations</b> in deriving good numerical methods for ODE's. The underlying {{challenge is to}} increase the stability region while maintaining or even improving efficiency. To achieve this, some extensions of predictor-corrector based methods, which apply a fixed number of corrector iterations, are considered. This thesis studies two particular members of a family of methods called the Parallel Block Predictor-Corrector Family, which are based on these extensions. These two members are called PBPC/ 2 and PBPC/ 3. They are characterized by iterated corrector evaluations carried out in two adjacent blocks. Stability properties of these methods are analyzed and compared with some existing block-based parallel predictor-corrector methods. Performance of the PBPC/ 2 and PBPC/ 3 methods and these existing block-based parallel predictor-corrector methods is compared using solution formulas which extend over a range of integration orders and which use various number of processors. The results obtained from a stability analysis and from a collection of numerical experiments indicate that the proposed methods provide a potential opportunity to balance stability properties {{and efficiency}} in the parallel computer systems...|$|R
40|$|Image {{restoration}} is {{a dynamic}} field of research. The need for efficient image restoration methods has grown with the massive production of digital images and movies of all kinds. It often happens that in an image acquisition system, an acquired image has less desirable quality than the original image due to various imperfections and/or physical limitations in the image formation and transmission processes. Thus the main objective of image restoration {{is to improve the}} general quality of an image or removing defects from it. The <b>two</b> <b>main</b> <b>considerations</b> in recovery procedures are categorized as blur and noise. In the case of images with presence of both blur and noises, it is impossible to recover a valuable approximation of the image of interest without using some a priori information about its properties. The instability of image restoration is overcome by using a priori information which leads to the concept of image regularization. A lot of regularization methods are developed to cop up with the criteria of estimating high quality image representations. The Maximum A posteriori Probability (MAP) based Bayesian approach provide a systematic and flexible framework for this. This paper presents a survey on image restoration based on various prior models such as tikhonov, TV, wavelet etc in the Bayesian MAP framewor...|$|R
60|$|It became, therefore, an {{important}} matter of state policy that the duke should be married. In fact, the barons and military chieftains who were friendly to him urged this measure upon him, {{on account of}} the great effect which they perceived it would have in settling the minds {{of the people of the}} country and consolidating his power. William accordingly began to look around for a wife. It appeared, however, in the end, that, though policy was the <b>main</b> <b>consideration</b> which <b>first</b> led him to contemplate marriage, love very probably exercised {{an important}} influence in determining his choice of the lady; at all events, the object of his choice was an object worthy of love. She was one of the most beautiful and accomplished princesses in Europe.|$|R
2500|$|When {{deciding}} the linker fusion {{site on the}} protein surface, there are three <b>main</b> <b>considerations.</b> [...] <b>First,</b> the fluorescent protein fragments {{must be able to}} associate with one another when their tethered proteins interact. Structural information and the location of the interaction surface may be useful when determining the fusion site to the linker, although the information is not necessary, as multiple combinations and permutations can be screened. Secondly, the creation of the fusion protein must not significantly alter the localisation, stability, or expression of the proteins to which the fragments are linked as compared to the endogenous wild-type proteins. Finally, the addition of the fluorescent fragment fusion must not affect the biological function of the protein, preferably verified using assays that evaluate all of the proteins' known functions.|$|R
50|$|During {{the late}} 1950s, when the {{decision}} to introduce colour television was first seriously mooted, the then <b>two</b> <b>main</b> systems for <b>consideration</b> were the French SECAM and American NTSC systems, the latter generally considered superior and expected to be adopted. Throughout the 1960s a third competing system, PAL, became available and was eventually adopted by the GPO for use on the 625 line service, {{to be known as}} System I or PAL-I.|$|R
40|$|The Purpose of {{this thesis}} {{is to present}} a {{comprehensive}} survey of Kant’s political theories, and to compare them with earlier thinkers {{to which they are}} related. The choice of this topic is due in the <b>main</b> to <b>two</b> <b>considerations.</b> <b>First,</b> the writer has observed with concern the social and political ills of our Western Civilization - the disintegrating family life, the individual's disillusion with democracy, his selfish, cynical disregard towards his fellow man, his attitude towards the state of getting the most and giving the least, the slow erosion of values and the prevalent contempt of inspiring ideals...|$|R
40|$|Our current {{state-of-the-art}} {{construction design}} {{approach is to}} integrate principles of sustainable design. The philosophy of sustainable building construction relies on designing physical objects, the built environment and services {{to comply with the}} principles of economic, social and ecological sustainability. This requires the architect to design with low-impact environmental materials, energy-efficiency (embodied and operations) and quality and durability (longer-lasting and higherperformance) to create healthy buildings (a good indoor environment) and recycling potential. From the above disciplines of sustainable design, heat, air and moisture processes (HAM) {{play a critical role in}} developing the necessary yardstick for energy-efficiency, durability and healthy buildings. Energy-efficiency and comfort have recently been <b>two</b> <b>main</b> <b>considerations</b> in building designs, while durability has always been a critical parameter. Yet, until recently, no HAM design tools/methods were readily available. Recently, the increasing demand for better-performing calculation methods to assess the moisture behavior of building components prompted an international collaboration between the Oak Ridge National Laboratory (ORNL) in the United States and the Fraunhofer Institute for Building Physics (IBP) in Germany to develop a hygrothermal design tool named WUFI-ORNL/IBP. This hygrothermal design model can assess the response of building envelope systems in terms of heat and moisture loads and can also provide a very useful and fair method for evaluating and optimizing building envelope designs. Today, if architects and engineers do not use the WUFI (Wärme und Feuchte instationär) hygrothermal model for building envelope design, or WUFI-Plus for the whole building de sign, they are assuming unnecessary field failure risks for themselves and their clients...|$|R
40|$|Abstract. This {{research}} {{focuses on}} the virtual environment of place making. In this paper {{we would like to}} emphasize that the place making should be stressed collective views in order to obtain the design application of possibilities. However, in past researches there has been no study that tried to collect the collective views by digital ways. Accordingly, this paper proposes a response thought the Spatial Intention. It could be used to represent the human of body experience. The "moving " and "standing " are appropriate to <b>two</b> <b>main</b> <b>considerations.</b> Both of these could be connected to the action of "focus " and "choice. " these leads to a sequential relationship of place production. The positive significance of the spatial intention lies in the convertibility of physical experience could be implied with a specific understanding. It also could be used to mold the place of knowledge structure. Thereby in order to verify the reliability of the above, we made a social network of virtual environment and used the rapid prototyping method to develop a prototype system. Implementing on the Chinese garden of the actual case, we found that the tag could concentrate as an entire sense in somewhere of place. These tags also could be shared remotely through the social network. Different tags in the sharing mechanism could collage out a place of collective views. This perspective would be used to assist designers to understand the sense of place. It also would be applied to find out the environmental design of possibilities in the future studies. 544 G. M. CHE...|$|R
40|$|Image-guided surgery {{systems are}} often used to provide {{surgeons}} with informational support. Due to several unique advantages such as ease of use, real-time image acquisition, and no ionizing radiation, ultrasound is a common medical imaging modality used in image-guided surgery systems. To perform advanced forms of guidance with ultrasound, such as virtual image overlays or automated robotic actuation, an ultrasound calibration process must be performed. This process recovers the rigid body transformation between a tracked marker attached to the ultrasound transducer and the ultrasound image. A phantom or model with known geometry is also required. In this work, we design and test an ultrasound calibration phantom and software. The <b>two</b> <b>main</b> <b>considerations</b> in this work are utilizing our knowledge of ultrasound physics to design the phantom and delivering an easy to use calibration process to the user. We explore {{the use of a}} three-dimensional printer to create the phantom in its entirety without need for user assembly. We have also developed software to automatically segment the three-dimensional printed rods from the ultrasound image by leveraging knowledge about the shape and scale of the phantom. In this work, we present preliminary results from using this phantom to perform ultrasound calibration. To test the efficacy of our method, we match the projection of the points segmented from the image to the known model and calculate a sum squared difference between each point for several combinations of motion generation and filtering methods. The best performing combination of motion and filtering techniques had an error of 1. 56 mm and a standard deviation of 1. 02 mm...|$|R
40|$|This {{research}} {{focuses on}} the virtual environment of place making. In this paper {{we would like to}} emphasize that the place making should be stressed collective views in order to obtain the design application of possibilities. However, in past researches there has been no study that tried to collect the collective views by digital ways. Accordingly, this paper proposes a response thought the Spatial Intention. It could be used to represent the human of body experience. The “moving” and “standing” are appropriate to <b>two</b> <b>main</b> <b>considerations.</b> Both of these could be connected to the action of “focus” and “choice. ” these leads to a sequential relationship of place production. The positive significance of the spatial intention lies in the convertibility of physical experience could be implied with a specific understanding. It also could be used to mold the place of knowledge structure. Thereby in order to verify the reliability of the above, we made a social network of virtual environment and used the rapid prototyping method to develop a prototype system. Implementing on the Chinese garden of the actual case, we found that the tag could concentrate as an entire sense in somewhere of place. These tags also could be shared remotely through the social network. Different tags in the sharing mechanism could collage out a place of collective views. This perspective would be used to assist designers to understand the sense of place. It also would be applied to find out the environmental design of possibilities in the future studies. ...|$|R
50|$|Taking {{aside the}} {{question}} of legal liability and moral responsibility, the question arises how autonomous vehicles should be programmed to behave in an emergency situation where either passengers or other traffic participants are endangered. A very visual example of the moral dilemma that a software engineer or car manufacturer might face in programming the operating software is described in an ethical thought experiment, the trolley problem: a conductor of a trolley has the choice of staying on the planned track and running over 5 people, or turn the trolley onto a track where it would only kill one person, assuming there is no traffic on it. There are <b>two</b> <b>main</b> <b>considerations</b> {{that need to be}} addressed. First, what moral basis would be used by an autonomous vehicle to make decisions? Second, how could those be translated into software code? Researchers have suggested, in particular, two ethical theories to be applicable to the behavior of autonomous vehicles in cases of emergency: deontology and utilitarianism. Asimov’s three laws of robotics are a typical example of deontological ethics. The theory suggests that an autonomous car needs to follow strict written-out rules that it needs to follow in any situation. Utilitarianism suggests the idea that any decision must be made based on the goal to maximize utility. This needs a definition of utility which could be maximizing the number of people surviving in a crash. Critics suggest that autonomous vehicles should adapt a mix of multiple theories to be able to respond morally right in the instance of a crash.|$|R
40|$|This thesis {{contains}} {{a collection of}} algorithms for working with the twisted groups of Lie type known as Suzuki groups, and small and large Ree groups. The <b>two</b> <b>main</b> problems under <b>consideration</b> are constructive recognition and constructive membership testing. We also consider problems of generating and conjugating Sylow and maximal subgroups. The algorithms are motivated by, and form a part of, the Matrix Group Recognition Project. Obtaining both theoretically and practically efficient algorithms has been a central goal. The algorithms have been developed with, and implemente...|$|R
40|$|Global memory {{models are}} {{evaluated}} by {{using data from}} recognition memory experiments. For recognition, each of the models gives a value of familiarity as the output from matching a test item against memory. The experiments provide ROC (receiver operating characteristic) curves that give information about the standard deviations of familiarity values for old and new test items in the models. The experimental {{results are consistent with}} normal distributions of familiarity (a prediction of the models). However, the results also show that the new-item familiarity standard deviation is about 0. 8 that of the old-item familiarity standard deviation and independent of the strength of the old items (under the assumption of normality). The models are inconsistent with these results because they predict either nearly equal old and new standard deviations or increasing values of old standard deviation with strength. Thus, the data provide the basis for revision of current models or development of new models. In the long tradition of modeling the structures and processes that underlie memory, there have been <b>two</b> <b>main</b> <b>considerations</b> in theory building. The first has been the ability of a theory to cover a wide range of the phenomena under examination. This consideration has become especially important in response to the plethora of simple models that were developed for extremely limited domains 15 or more years ago. The second consideration is the standard criterion in modeling in all disciplines, that is, modeling detailed aspects of data. In memory research, many early models were flawed in {{one or the other of}} these respects and, in particular, most were criticized because they dealt with only a handful of experimental procedures. I...|$|R
40|$|During {{the autumn}} of 2011 two catastrophic, very intense {{rainfall}} events affected two {{different parts of the}} Liguria Region of Italy causing various flash floods. The first occurred in October and the second at the beginning of November. Both the events were characterized by very high rainfall intensities (>  100  mm h − 1) that persisted on a small portion of territory causing local huge rainfall accumulations (>  400  mm  6  h − 1). <b>Two</b> <b>main</b> <b>considerations</b> were made in order to set up this work. The <b>first</b> <b>consideration</b> is that various studies demonstrated that the two events had a similar genesis and similar triggering elements. The second very evident and coarse concern is that <b>two</b> <b>main</b> elements are needed to have a flash flood: a very intense and localized rainfall event and a catchment (or a group of catchments) to be affected. Starting from these assumptions we did the exercise of mixing the two flash flood ingredients by putting the rainfall field of the first event on the main catchment struck by the second event, which has its mouth in the biggest city of the Liguria Region: Genoa. A complete framework was set up to quantitatively carry out a “what if” experiment with the aim of evaluating the possible damages associated with this event. A probabilistic rainfall downscaling model was used to generate possible rainfall scenarios maintaining the main characteristics of the observed rainfall fields while a hydrological model transformed these rainfall scenarios in streamflow scenarios. A subset of streamflow scenarios is then used as input to a 2 -D hydraulic model to estimate the hazard maps, and finally a proper methodology is applied for damage estimation. This leads to the estimation of the potential economic losses and of the risk level for the people that stay in the affected area. The results are interesting, surprising and in a way worrying: a rare but not impossible event (it occurred about 50  km away from Genoa) would have caused huge damages estimated between 120 and EUR  230  million for the affected part of the city of Genoa, Italy, and more than 17   000 potentially affected people...|$|R
40|$|This paper, {{according}} to the basic materials preserved in the Diplomatic Record Office (Tokyo), the Public Record Office (Richmond), and the Hoover Institute Archives (Stanford), deals with the decision making process of the tariff policy of the Chinese Nationalist Government during 1932 - 1934. The results of our analysis are as follows. (1) Intention of policy: Although public opinion formed a striking contrast between the estimate of the 1933 tariff {{and that of the}} 1934 tariff, the two tariffs had very similar intentions of policy. Both of them pursued protecting native industry as well as increasing revenue. There was, however, an important difference between the two. While the 1933 tariff was related to “anti-Japanese pro-British ･ American” course, the 1934 tariff was influenced by “pro-Japanese appeasement” cource. Under such circumstances, the 1934 tariff included reducing rates of some Japanese goods. (2) Makers of policy: The members of the National Tariff Commission in the Ministry of Finance played the most important role in decision making process. Their thoughts about tariff, that were to respect both revenue tariff and protective tariff, were reflected in the 1933 tariff and the 1934 tariff. Secondly, the Ministry of Industry took up the strong position that it was necessary to protect native industry by the tariff barriers. On the other hand, the Ministry of Foreign Affairs, yielding to Japanese demands of reducing rates, had only limited influence. (3) Domestic factors: The <b>two</b> <b>main</b> <b>considerations</b> must be refered to. One was to maintain a greater part of the revenue of the Nationalist Government and another was to obtain support from industrial capitalists who formed indispensable supporters of the Nationalist Government. (4) International factors: Both Japanese and British Governments avoided to giving formal protest to the Nationalist Government. Particulary, latter afraid of attack by Chinese nationalism, tried only informal conversation with the Nationalist Government. A cooperative protest of the powers never came into existence. Those international circumstances gave an opportunity to China to promote her positive tariff policy...|$|R
40|$|Social' or 'class' {{mobility}} is {{in practice}} normally occupational mobility. The occupational dimension, however, has to date received relatively little attention. The thesis explores how occupations and occupational mobility {{have been treated}} in three bodies of work: social mobility studies, sociological theories drawing on Marx, and sociological theories of industrial or post-industrial society. This discussion provides a wider context by means of which to understand occupational mobility, while also suggesting new areas in which mobility analysis can throw light on current debates. Scotland is then taken as a case study. A brief economic history and a more detailed account of occupational transition since the First World War {{lay the groundwork for}} an investigation of mobility per se. Earlier accounts of mobility - particularly that of Glass - are shown to be inadequate. Detailed examination of mobility rates using data from the 19. 75 Scottish Mobility Study involves four <b>main</b> <b>considerations.</b> <b>First,</b> the <b>main</b> patterns are identified, revealing higher rates of intergenerational movement and more 'long-range' mobility than previously expected. The upper middle class in particular displays high levels of inflow and outflow. Second, changing trends in early career mobility over the past 40 years are presented: the trends indicate considerable variation between industrial sectors, and the more recent levelling-off of upward mobility is shown to result largely from the growing dominance of certain service industries in the creation of non-manual employment. -The third area of investigation is the relationship between education and mobility. The generally low level of qualified manpower and the rapid growth of non-manual employment are used to explain the poor association between academic achievement and first occupation until the 1960 s. This relates to the final question of career mobility, which is shown to follow the broad pattern of first job mobility. An exploration of ideas about deskilling and labour markets in terms of mobility rates illustrates how an occupational emphasis establishes wider links to sociological theory than one limited to 'social' mobility. The thesis includes two appendices, one dealing with the occupational class schema used in the study, and the other describing the methodology...|$|R
