189|128|Public
25|$|Tuberous organs, {{the type}} of {{receptive}} organ used for electrocommunication, {{can be divided into}} two types, depending on the way information is encoded: time coders and amplitude coders. There are multiple forms of tuberous organs in each time and amplitude coders, and all weakly electric fish species possess at least one form of the <b>two</b> <b>coders.</b> Time coder fires phase-locked action potential (meaning, the waveform of the action potential is always the same) at a fixed delay time after each outside transient is formed. Therefore, time coders neglect information about waveform and amplitude but focus on frequency of the signal and fire action potentials on a 1:1 basis to the outside transient. Amplitude coders, on the contrary, fire according to the EOD amplitude. While both wave-type and pulse-type fishes have amplitude coders, they fire in different ways: Receptors of wave-type fishes continuously fire at a rate according to their own EOD amplitude; on the other hand, receptors of pulse-type fishes fire burst of spikes to each EOD detected, and the number of spikes in each burst is correlated to the amplitude of the EOD. Tuberous electroreceptors show a V-shaped threshold tuning curve (similar to auditory system), which means that they are tuned to a particular frequency. This particular tuned-in frequency is usually closely matched to their own EOD frequency.|$|E
500|$|As {{with the}} first instalment, Wipeout 2097 was {{developed}} by Liverpudlian developer Psygnosis and the promotional art was designed by Sheffield-based The Designers Republic. The development cycle ran seven months. To cater for the increase in Wipeout players, an easier learning curve was introduced whilst keeping the difficulty at top end for the experienced gamers. The game was originally intended as a tracks add-on for the original Wipeout. No sequel had been planned, but Andy Satterthwaite (who worked on the MS-DOS version of the original) was asked by Psygnosis {{to apply for the}} role [...] "internal producer". He did, and during the interview, asked to do a sequel to Wipeout, but instead ended up developing extra tracks. The add-on was titled Wipeout 2097 because Psygnosis did not want {{to give the impression that}} it was a full sequel. In the United States, it went by the name of Wipeout XL because it was felt that American players would not understand the concept of the game being set a century in the future. The American title was originally to be Wipeout XS (for [...] "Excess"), but it was pointed out that XS could also stand for [...] "extra small". Satterthwaite ended up with a team of <b>two</b> <b>coders</b> (two of who were new), six artists, and Nick Burcombe.|$|E
50|$|Scott’s pi is an {{agreement}} coefficient for nominal data and <b>two</b> <b>coders.</b>|$|E
40|$|This thesis {{deals with}} {{lossless}} image compression. You can nd all {{the process of}} assembling lossless image coder/decoder. There are described many predictors, color models and <b>two</b> entropy <b>coders</b> in this thesis. The results of thesis are compared and disscused with current lossless image format PNG {{at the end of}} thesis...|$|R
30|$|Finally, coding {{was carried}} out by <b>two</b> {{independent}} <b>coders</b> to ensure inter-coder reliability. First, the method was applied by a junior researcher that generated the pilot list of possible codes (39 codes). Second, a post-doc researcher re-coded a fresh version of the entire dataset using the pilot list of codes, resolving issues in concordance with researchers involved.|$|R
5000|$|Hardware-wise, this turbo-code encoder {{consists}} of <b>two</b> identical RSC <b>coders,</b> С1 and C2, {{as depicted in}} the figure, which are connected to each other using a concatenation scheme, called parallel concatenation: ...|$|R
5000|$|Pearson’s intraclass {{correlation}} coefficient rii is an agreement coefficient for interval data, <b>two</b> <b>coders,</b> and very large sample sizes. To obtain it, Pearson's original suggestion was {{to enter the}} observed pairs of values twice into a table, once as c-k and once as k-c, to which the traditional Pearson product-moment correlation coefficient is then applied. [...] By entering pairs of values twice, the resulting table becomes a coincidence matrix {{without reference to the}} <b>two</b> <b>coders,</b> contains n=2N values, and is symmetrical around the diagonal, i.e., the joint linear regression line is forced into a 45° line, and references to coders are eliminated. Hence, Pearson’s {{intraclass correlation}} coefficient is that special case of interval alpha for <b>two</b> <b>coders</b> and large sample sizes, [...] and [...]|$|E
5000|$|Scott’s {{observed}} {{proportion of}} agreement [...] appears in alpha’s numerator, exactly. Scott’s expected proportion of agreement, [...] is asymptotically approximated by [...] when {{the sample size}} n is large, equal when infinite. It follows that Scott’s pi is that special case of alpha in which <b>two</b> <b>coders</b> generate a very large sample of nominal data. For finite sample sizes: [...] Evidently, [...]|$|E
5000|$|A {{coincidence}} matrix cross tabulates the n pairable {{values from}} the canonical {{form of the}} reliability data into a v-by-v square matrix, where v {{is the number of}} values available in a variable. Unlike contingency matrices, familiar in association and correlation statistics, which tabulate pairs of values (Cross tabulation), a coincidence matrix tabulates all pairable values. A coincidence matrix omits references to coders and is symmetrical around its diagonal, which contains all perfect matches, viu = vi'u for <b>two</b> <b>coders</b> i and i' , across all units u. The matrix of observed coincidences contains frequencies: ...|$|E
50|$|Data rake-offs are {{provided}} for potentiometer, synchro, and digital information in allthree coordinates. The azimuth and elevation digital data {{is derived from}} optical-typeanalog-to-digital encoders. <b>Two</b> geared <b>coders</b> with ambiguity resolution are used foreach parameter. The data for each angle is a Gray code 17-bit word in serial form.The overlapping ambiguity bits are removed, and the data is transformed from cyclicGray code to straight binary before recording for transmission to the computer. Therange servo presents a 20-bit straight binary word in serial form after ambiguityresolution and code conversion. The same type optical encoders are used.|$|R
30|$|<b>Two</b> {{independent}} <b>coders</b> (not the authors) {{were trained}} and {{provided with a}} set of coding instructions and the rubric described above. The coders then independently applied the rubric to the text of twelve blog posts. Coders were asked to clearly mark and label sentences in the text that provided evidence of the four different coding categories. Text could be coded as belonging to more than one coding category. Only text that was coded by both coders as fitting into a coding category was included in the results; a measure to ensure reliability in the coding of data.|$|R
40|$|How {{to deliver}} {{clinically}} critical {{information in a}} &# 039;smaller package&# 039;? This paper evaluates a recently proposed perceptually lossless medical image coder (PLMIC) based on the JPEG 2000 coding framework. A performance comparison is made between the PLMIC and <b>two</b> benchmark <b>coders</b> LOCO-I and near lossless LOCO-I at two levels in terms of compression ratio and picture fidelity. Subjective assessment in this instance {{has been carried out}} with 31 medical experts. The results have shown that the PLMIC delivers significantly higher compression ratio performance than that of the state-of-the-art JPEG-LS LOCO-I lossless algorithm without any visible degradation in image quality or clinical information...|$|R
5000|$|... where [...] {{is the sum}} of N {{differences}} between one coder’s rank c and the other coder’s rank k of the same object u. Whereas alpha accounts for tied ranks in terms of their frequencies for all coders, rho averages them in each individual coder's instance. In the absence of ties, 's numerator [...] and 's denominator , where n=2N, which becomes [...] when sample sizes become large. So, Spearman’s rho is that special case of alpha in which <b>two</b> <b>coders</b> rank a very large set of units. Again, [...] and [...]|$|E
5000|$|Robert Weber notes: [...] "To make valid inferences {{from the}} text, it is {{important}} that the classification procedure be reliable in the sense of being consistent: Different people should code the same text in the same way". The validity, inter-coder reliability and intra-coder reliability are subject to intense methodological research efforts over long years.Neuendorf suggests that when human coders are used in content analysis <b>two</b> <b>coders</b> should be used. Reliability of human coding is often measured using a statistical measure of intercoder reliability or [...] "the amount of agreement or correspondence among two or more coders".|$|E
50|$|The next {{release was}} due to be 3.2 and was {{originally}} started in 2003. However, for various reasons work of this version stalled, this resulting in the next release of Ikonboard becoming 3.1.3 and was developed mainly by 2 coders. With the change of ownership around August 2005 the <b>two</b> <b>coders</b> departed leaving development on 3.1.3 at beta 09 stage. When Ikonboard returned online work on 3.1.3 continued, and Ikonboard 3.1.3 was publicly released on January 30, 2006. It contained many new features as well as fixing bugs and a couple security issues. The most significant new additions in this release was Humain Readable Image (HRI) on registration which keeps malicious users from mass registering, and an update centre in the adminCP.|$|E
30|$|The {{test was}} scored {{with a maximum}} of 26 points. In {{accordance}} with the theoretically described cognitive requirements, the nine open-ended items requiring creating one’s own solutions had double weighting and, thus, were awarded 0, 1, or 2 points; the eight forced-choice items requiring only applying and analyzing were awarded 0 or 1 point. To ensure objectivity in the scoring of the open-ended items, a coding manual was developed {{in cooperation with the}} above-mentioned experts. <b>Two</b> trained <b>coders</b> independently scored 62  % of the open-ended responses. Intercoder reliability, as indicated by Krippendorff’s alpha for ordinal data, showed a good average value of 0.88 and ranged between 0.70 and 0.96 for all items.|$|R
40|$|The Facial Action Coding System (FACS) (Ekman & Friesen, 1978) is a {{comprehensive}} and widely used method of objectively describing facial activity. Little is known, however, about inter-observer reliability in coding the occurrence, intensity, and timing of individual FACS action units. The present study evaluated the reliability of these measures. Observational data came from three independent laboratory studies designed to elicit {{a wide range of}} spontaneous expressions of emotion. Emotion challenges included olfactory stimulation, social stress, and cues related to nicotine craving. Facial behavior was video-recorded and independently scored by <b>two</b> FACS-certified <b>coders.</b> Overall, we found good to excellent reliability for the occurrence, intensity, and timing of individual action units and for corresponding measures of more global emotion-specified combinations...|$|R
50|$|The {{data for}} each country is {{analyzed}} {{by at least}} <b>two</b> senior <b>coders</b> and undergraduate assistants. Each nation receives two scores - one from the State Department's report and one from Amnesty International's report. Coders agree with each other regarding a country's score {{at a rate of}} roughly 85%. If coders cannot come to an agreement than a third coder will settle the dispute. Due to the contextual nature of the source data, coding is highly subjective, and coders are instructed to ignore their own personal biases or knowledge when determining a country's score. Coders are also instructed to give countries the benefit of any doubt when scoring. If a coder cannot decided between two numbers, they are instructed to use the lower number when scoring.|$|R
5000|$|An example cited on {{the tech}} news site securityfocus.com by Kevin Poulsen {{illustrates}} how <b>two</b> <b>coders</b> implemented and distributed {{a program that}} disguised itself as activation key generators and cracks for illegal software circulating on peer-to-peer file sharing sites. The duo researched software that was popular on these file sharing sites and tagged their code with their names. As soon as the software was executed, it displayed a large message: “Bad Pirate! So, {{you think you can}} steal from software companies do you? That's called theft, don't worry your secret is safe with me. Go thou sic and sin no more." [...] The software then called back to a central server and logged the file name under which it was executed, amount of time the message was displayed on the downloader’s computer screen and their IP address. The information gathered was then re-posted onto a public website showing the downloader’s IP address and country of origin. The program also had a unique ID embedded into each downloaded copy of it for tracking purposes to keep track of how it traversed the different networks.|$|E
50|$|Coefficients {{measuring}} {{the degree to}} which coders are statistically dependent on each other. When the reliability of coded data is at issue, the individuality of coders can have no place in it. Coders need to be treated as interchangeable. Alpha, Scott’s pi, and Pearson’s original intraclass correlation accomplish this by being definable as a function of coincidences, not only of contingencies. Unlike the more familiar contingency matrices, which tabulate N pairs of values and maintain reference to the <b>two</b> <b>coders,</b> coincidence matrices tabulate the n pairable values used in coding, regardless of who contributed them, in effect treating coders as interchangeable. Cohen’s kappa, by contrast, defines expected agreement in terms of contingencies, as the agreement that would be expected if coders were statistically independent of each other. Cohens conception of chance fails to include disagreements between coders’ individual predilections for particular categories, punishes coders who agree on their use of categories, and rewards those who do not agree with higher kappa-values. This is the cause of other noted oddities of kappa. The statistical independence of coders is only marginally related to the statistical independence of the units coded and the values assigned to them. Cohen’s kappa, by ignoring crucial disagreements, can become deceptively large when the reliability of coding data is to be assessed.|$|E
5000|$|As {{with the}} first instalment, Wipeout 2097 was {{developed}} by Liverpudlian developer Psygnosis and the promotional art was designed by Sheffield-based The Designers Republic. The development cycle ran seven months. To cater for the increase in Wipeout players, an easier learning curve was introduced whilst keeping the difficulty at top end for the experienced gamers. The game was originally intended as a tracks add-on for the original Wipeout. No sequel had been planned, but Andy Satterthwaite (who worked on the MS-DOS version of the original) was asked by Psygnosis {{to apply for the}} role [...] "internal producer". He did, and during the interview, asked to do a sequel to Wipeout, but instead ended up developing extra tracks. The add-on was titled Wipeout 2097 because Psygnosis did not want {{to give the impression that}} it was a full sequel. In the United States, it went by the name of Wipeout XL because it was felt that American players would not understand the concept of the game being set a century in the future. The American title was originally to be Wipeout XS (for [...] "Excess"), but it was pointed out that XS could also stand for [...] "extra small". Satterthwaite ended up with a team of <b>two</b> <b>coders</b> (two of who were new), six artists, and Nick Burcombe.|$|E
40|$|Objective To {{assess the}} impact on the {{reported}} cause-of-death patterns of a verbal autopsy coding strategy based on a review of every death by multiple coders versus a single coder. Methods Deaths in 45 villages (total population 180 162) in southern India were documented during 12 months in 2003 – 2004, and a standard verbal autopsy questionnaire was completed for each death. <b>Two</b> physician <b>coders,</b> each unaware of the other’s decisions, assigned an underlying cause of death in accordance with the causes listed in the chapter headings of the International classification of diseases and related health problems, 10 th revision (ICD- 10). For the three chapter headings that applied to more than 100 of the deaths, agreement for subsets of causes of death within the chapter was also analysed. In the event of discrepancies, a third coder was used to finalize a cause of death. Cohen’s kappa statistic (K) was used to measure levels of agreement between the <b>two</b> physician <b>coders.</b> Findings In total, 1354 deaths were documented, and a verbal autopsy was completed for 1329 (98 %) of them. At the chapter heading level of the ICD- 10, physician coders assigned the same cause to 1255 deaths (94 %) (K = 0. 93; 95 % confidence interval: 0. 92 – 0. 94). The patterns of death derived from the causes assigned by each physician were all very similar to the patterns obtained through the consensus process, with the rank order of the 10 leading causes of death being the same for all three coding methods. Conclusion Duplicate coding of verbal autopsy results has little advantage over a single-coder system for mortality surveillance or for identifying population patterns of death. Resources could be better diverted {{to other parts of the}} mortality surveillance process...|$|R
40|$|Abstract Background Online {{technology}} is a promising resource for conducting clinical research. While the internet may improve a study 2 ̆ 7 s reach, {{as well as the}} efficiency of data collection, it may also introduce a number of challenges for participants and investigators. The objective of this research was to determine the challenges that potential participants faced during the enrollment phase of a randomized controlled intervention trial of Stepping Up to Health, an internet-mediated walking program that utilized a multi-step online enrollment process. Methods We conducted a quantitative content analysis of 623 help tickets logged in a participant management database during the enrollment phase of a clinical trial investigating the effect of an automated internet-mediated walking intervention. Qualitative coding was performed by <b>two</b> trained <b>coders,</b> and 10...|$|R
40|$|Rail {{accidents}} can {{be understood}} in terms of the systemic and individual contributions to their causation. The current study was undertaken to determine whether errors and violations are more often associated with different local and organisational factors that contribute to rail accidents. The Contributing Factors Framework (CFF), a tool developed for the collection and codification of data regarding rail accidents and incidents, was applied to a sample of investigation reports. In addition, a more detailed categorisation of errors was undertaken. Ninety-six investigation reports into Australian accidents and incidents occurring between 1999 and 2008 were analysed. Each report was coded independently by <b>two</b> experienced <b>coders.</b> Task demand factors were significantly more often associated with skill-based errors, knowledge and training deficiencies significantly associated with mistakes, and violations significantly linked to social environmental factors...|$|R
50|$|Tuberous organs, {{the type}} of {{receptive}} organ used for electrocommunication, {{can be divided into}} two types, depending on the way information is encoded: time coders and amplitude coders. There are multiple forms of tuberous organs in each time and amplitude coders, and all weakly electric fish species possess at least one form of the <b>two</b> <b>coders.</b> Time coder fires phase-locked action potential (meaning, the waveform of the action potential is always the same) at a fixed delay time after each outside transient is formed. Therefore, time coders neglect information about waveform and amplitude but focus on frequency of the signal and fire action potentials on a 1:1 basis to the outside transient. Amplitude coders, on the contrary, fire according to the EOD amplitude. While both wave-type and pulse-type fishes have amplitude coders, they fire in different ways: Receptors of wave-type fishes continuously fire at a rate according to their own EOD amplitude; on the other hand, receptors of pulse-type fishes fire burst of spikes to each EOD detected, and the number of spikes in each burst is correlated to the amplitude of the EOD. Tuberous electroreceptors show a V-shaped threshold tuning curve (similar to auditory system), which means that they are tuned to a particular frequency. This particular tuned-in frequency is usually closely matched to their own EOD frequency.|$|E
40|$|Development {{of network}} quality speech coders at 16 kb/s and below {{is an active}} {{research}} area. This thesis focuses {{on the study of}} low-delay Code Excited Linear Predictive (CELP) and tree coders. A 16 kb/s stochastic tree coder based on the (M,L) search algorithm suggested by Iyengar and Kabal and a low-delay CELP coder proposed by AT&T (CCITT 16 kb/s standardization candidate) are examined. The first goal is to compare and study the performance of the <b>two</b> <b>coders.</b> Second objective is to analyze the particular characteristics which make the <b>two</b> <b>coders</b> different from one another. The final goal is the improvement of the performance of the coders, particularly with a view of bringing down the bit rate below 16 kb/s. When compared under similar conditions, the <b>two</b> <b>coders</b> showed comparable performance at 16 kb/s. The analysis of the components and particular characteristics of the tree and CELP coders provide new insight for future coders. Higher performance coder components such as prediction, gain adaptation, and residual signal quantization are needed. Issues in backward adaptive linear prediction analysis for both near and far-sample redundancy removal such as analysis methods, windowing...|$|E
40|$|This chapter {{deals with}} {{practical}} {{solutions for the}} Slepian Wolf (SW) coding problem, which refers {{to the problem of}} lossless compression of correlated sources with coders which do not communicate. Here, we will consider the case of two binary correlated sources X and Y, characterized by their joint distribution. If the <b>two</b> <b>coders</b> communicate, i...|$|E
40|$|The {{previous}} {{literature has}} suggested that the hand movement in co-speech gestures and signs consists of a series of phases with qualitatively different dynamic characteristics. In this paper, we propose a syntagmatic rule system for movement phases that applies to both co-speech gestures and signs. Descriptive criteria for the rule system were developed for the analysis video-recorded continuous production of signs and gesture. It involves segmenting a stream of body movement into phases and identifying different phase types. <b>Two</b> human <b>coders</b> used the criteria to analyze signs and cospeech gestures that are produced in natural discourse. It was found that the criteria yielded good inter-coder reliability. These criteria can be used for the technology of automatic recognition of signs and co-speech gestures in order to segment continuous production and identify the potentially meaningbearing phase...|$|R
40|$|The {{purpose of}} this thesis was to examine how the media {{coverage}} of Confederate symbols in Mississippi has changed over time {{by focusing on the}} following events: the banning of flag sticks at the University of Mississippi athletic events in 1997, the removal of the Mississippi state flag from campus in 2015, the modification of the song “Dixie” at football games in 2009, and the banning of the song from athletic events in 2016. A sample of news articles featured {{on the front page of}} three different Mississippi-based newspapers were reviewed through content analysis by <b>two</b> trained <b>coders</b> for changes in story format, journalistic elements, framing, voice, and tone. The study found that the media coverage of Confederate symbols has changed over time as the media landscape continues to change...|$|R
40|$|For optimal {{progressive}} {{transmission of}} an embedded image code over a noisy channel, we consider an {{unequal error protection}} strategy that minimizes {{the average of the}} expected distortion over a set of intermediate rates. In contrast to previous work, we find a near-optimal solution in real-time. For a binary symmetric channel, <b>two</b> state-of-the-art source <b>coders</b> (SPHIT and JPEG 2000), and a rate-compatible punctured turbo coder as a channel coder, we compare our solution to the strategy that optimizes the end-to-end performance...|$|R
40|$|This paper {{analyses}} two narrowband speech codecs, the 4. 8 kbit/s FS 1016 coder and the 8 kbit/s G 729 coder, using objective psychoacoustic measures. Four {{measures are}} used: loudness, sharpness, roughness and tonality. The results show sharpness and roughness {{as the two}} major contributing factors to the subjective difference between the <b>two</b> <b>coders...</b>|$|E
40|$|Paper {{presented}} to the 7 th Annual Symposium on Graduate Research and Scholarly Projects (GRASP) held at the Marcus Welcome Center, Wichita State University, May 4, 2011. Research completed at the Department of Communication Sciences and Disorders and School of NursingThe {{purpose of this study}} was to assess the reliability of coding mother-neonate interactions. <b>Two</b> <b>coders</b> independently examined videos of two mother-neonate dyads and coded five interactions: mother looking at neonate's face, mother smiling at neonate, mother joggling neonate, mother talking to neonate, and mother touching neonate. High inter-rater reliability was predicted for the first three interactions, which were presumed easy to identify and measure. Reliability analysis of 1, 427 interactions revealed that the <b>two</b> <b>coders</b> agreed less than would be expected by chance alone. This suggests that the process was highly dependent on the individual coders. Reevaluating current coding methods will be essential to develop meaningful standards for reliable coding, which in turn should help us better understand mother-neonate bonding...|$|E
30|$|Transcripts {{from the}} CE {{portion of the}} {{interview}} were organized into descriptions of PBC-related symptoms and symptom impacts by developing a coding framework and coding dictionary. <b>Two</b> <b>coders</b> were used during the study. Inter-rater agreement was evaluated by independent dual coding of 2 of the 20 transcripts and comparing the coding differences and expressing coding consistency by percentage of agreement in the codes assigned.|$|E
30|$|Study 2 {{employs a}} {{quasi-experimental}} design. Teachers {{within the same}} school were assigned to teach with the traditional or Genie 3 curriculum by the school principal, and students were non-randomly assigned to each group for the school year. Both groups were observed once in the fall semester {{and again in the}} spring. The observation procedure was similar to study 1. <b>Two</b> BROMP-certified <b>coders</b> conducted the observations. In this study, observers did not code for Gaming the System, which was all but non-existent in study 1, but they did include an additional behavioral category. Because one of the anticipated benefits of blended learning is that it frees teachers to engage more frequently in targeted interventions, BROMP observers also coded for On Task—Proactive Remediation, as discussed above. These cases were previously coded as On Task—Conversation, so we would expect a comparable reduction in that behavior compared to study 1.|$|R
40|$|International audienceIn VoIP systems, CELP coders, such as G. 729, are {{commonly}} used as they offer good speech quality {{in the absence of}} packet loss. However, the quality degrades in presence of losses. This is due to the existence of long-term predictor in this coder. Otherwise, harmonic coders such as MELP may be a good alternative for VoIP due to their higher resilience to packet loss. In this work, we deal the problem of packetization scheme based on Multiple Description Coding (MDC) applied to the MELP coder. A packet will contain information on <b>two</b> MELP <b>coders</b> operating at 2. 4 and 1. 2 Kbps respectively. The packetization is achieved using 135 bits in 22. 5 ms corresponding to a total rate of 6 kbps. The results show that under typical VoIP operating conditions, the method performs well and outperforms CELP coders when operating at 8 kbps without MDC...|$|R
40|$|Rate {{distortion}} {{functions for}} two-dimensional homogeneous isotropic images are {{compared with the}} performance of 5 source encoders designed for such images. Both unweighted and frequency weighted mean square error distortion measures are considered. The coders considered are differential PCM (DPCM) using six previous samples in the prediction, herein called 6 pel (picutre element) DPCM; simple DPCM using single sample prediction; 6 pel DPCM followed by entropy coding; 8 x 8 discrete cosine transform coder, and 4 x 4 Hadamard transform coder. Other transform coders were studied and found to have about the same performance as the <b>two</b> transform <b>coders</b> above. With the mean square error distortion measure DPCM with entropy coding performed best. The relative performance of the coders changes slightly when the distortion measure is frequency weighted mean square error. The performance of all the coders was separated by only about 4 dB...|$|R
