4|8|Public
50|$|The {{copyright}} for the AGROVOC <b>thesaurus</b> <b>content</b> in English, French, Russian and Spanish {{stays with}} FAO and is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. For any other language, the copyright {{rests with the}} institution responsible for its production.|$|E
50|$|Wordnik, {{a nonprofit}} organization, is an online English {{dictionary}} and language resource that provides dictionary and <b>thesaurus</b> <b>content.</b> Some {{of the content}} is based on print dictionaries such as the Century Dictionary, the American Heritage Dictionary, WordNet, and GCIDE. Wordnik has collected a corpus of billions of words which it uses to display example sentences, allowing it to provide information on a much larger set of words than a typical dictionary. Wordnik uses as many real examples as possible when defining a word.|$|E
30|$|Expanding ‘Polygnosis’ <b>thesaurus</b> <b>content</b> {{by adding}} more facets, as well as, terms in {{existing}} facets.|$|E
50|$|Unlike a {{traditional}} dictionary or <b>thesaurus</b> the <b>content</b> is enlivened by often pungent or politically incorrect observations and asides {{intended to provide}} further comic effect. Those familiar with Ambrose Bierce's Devil's Dictionary might recognise some parallels with Bierce's style though his lacked the overt obscenity. The authors often take delight in lampooning political or media figures of the day, or illustrating terms with fictional dialogue between notionally respectable historical figures. A much-used technique for sexual phrases is to include them in a quoted passage from a non-existent Barbara Cartland novel.|$|R
40|$|The AHRC-funded ‘Mapping Metaphor {{with the}} Historical Thesaurus’ project has traced the {{development}} of metaphor in English from Anglo-Saxon times {{to the present day}} using the unique evidence base of the Historical Thesaurus of the Oxford English Dictionary. The Historical <b>Thesaurus</b> organises the <b>contents</b> of the OED semantically, making it possible to see how vocabulary for any given concept has developed over time. One of the major outputs of the Mapping Metaphor project is the online Metaphor Map, which can be used to investigate metaphor in names and is freely available at: [URL]...|$|R
40|$|This paper {{proposes a}} {{methodology}} to automatically generate sta-tistical language models (SLM) s for {{the recognition of}} utterances in Interactive Voice Response (IVR) systems. The paper aims at creating SLMs for each IVR prompt [1] with minimum amount of human intervention and prior knowledge regarding the expected user utterances at a particular prompt. A combination of prefiller patterns based on spontaneous speech utterances, WordNet [2] and Roget’s <b>thesaurus</b> based <b>content</b> word extraction and, world wide web based statistical validation is used to generate SLMs auto-matically. The created SLM not only reduces the manual labor involved in IVR application development but also focuses on min-imizing the Word Error Rate (WER) and the Semantic Error Rate (SemER) of the ASR transcriptions. We use a WordNet [2] lexi-cal chain based semantic categorizer to classify ASR transcriptions into semantic categories representing each IVR prompt. Index Terms: automatic speech recognition, statistical language model, interactive voice response systems, semantic categorizer...|$|R
40|$|The {{main purpose}} of the Master Thesis – create {{thesaurus}} system oriented to the Lithuanian humanities and social science information systems. Thesaurus scheme is provided {{for the use of}} Lithuanian Academic Libraries Network (LABT) library information systems, the humanities and social science thematic databases (Lithuanian, LiDA, etc.). Thesaurus will be used for the following options: to standardize the humanities and social sciences the use of terms; to standardize the humanities and social sciences keyword introduction of information; to improve the search; to ensure compatibility with global information systems for the humanities and social sciences. System development is done using main technologies: HTML, PHP, AJAX, ORACLE, MARC 21, etc. Review and analysis are done of selected humanitarian and social sciences thesaurus. Model is developed and a prototype of thesaurus system is implemented. The system is tested with HASSET thesaurus data upload. Using thesaurus prototype, adapted for bilingual (English and Lithuanian languages) thesaurus was created, based on HASSET <b>thesaurus</b> <b>content.</b> The thesaurus prototype information retrieval and presentation component is installed in LiDA portal. Thesaurus usage in library information system (ALEPH) survey results presented...|$|E
40|$|Abstract. This paper {{describes}} our {{participation in}} the BioASQ semantic indexing challenge with two hierarchical text categorization systems. Both systems originated from previous research in thesaurus topic assignment applied on small domains from the legal document management field. One of the described systems employs a classical top-down approach based on a collection of local classifiers. The other system builds a Bayesian network induced by the <b>thesaurus</b> structure and <b>contents,</b> taking into account descriptor labels and related terms. We describe the adaptations required {{to deal with a}} large thesaurus like MeSH and a huge document collection and discuss the results obtained in the BioASQ challenge and the limitations of both approaches. ...|$|R
30|$|The {{evaluation}} {{also considered}} aspects of functionality from the user point of view. The general conclusion for practical use of multimedia tools in education was that high usability and simplicity of information access {{should be the}} focus point of any chosen approach in this direction. The ‘Polygnosis’ evaluation suggested that after thorough content review, the platform can successfully deliver rich learning <b>content.</b> To conclude, <b>thesaurus</b> design and <b>content</b> so far cover the main conceptualizations of the domain, since it consists of a sufficient information source for understanding the domain without the need of prior knowledge. However, {{there are many more}} facets and terms {{to be included in the}} thesaurus, such as a ‘Materials’ facet.|$|R
40|$|This {{research}} {{was done with}} the aim of identifying and determining facets in network of correlated relationships of inside and outside field of Islamic Ethic <b>Thesaurus</b> using <b>content</b> analysis method. The research society consisted of 3316 published terms in Islamic Ethic Thesaurus which have correlated relationship together. The method of data collecting was documentary (Library). According to the findings Personality Facet, with 23 lateral and subset facets and 78. 55 percent usage, has the most usage in correlated relationships of Islamic Ethic Thesaurus. The Space Facet without subset facet and 0. 05 percent usage, has the minimum usage in correlated relationships of Islamic Ethic Thesaurus. Thus, {{it can be said that}} ‘Personality’ Facet has the most frequency, and ‘Space’ Facet has the minimum frequency between facets of network of correlated relationships in concepts of inside and outside field of Islamic Ethic Thesaurus. In this study according to the requirements of Islamic Ethic Thesaurus 6 facets were described which include facets of love and friendship, facets of foundation/basis, facets of pests and damages, facets of internal characteristics/sense, facets of signs and symptoms, and facets of ranks and grade...|$|R
40|$|The aim of {{this paper}} is to {{describe}} the main goals and structure of an e-learning course aimed at improving knowledge in the field of subject indexing, within the scope, obviously, of Information Science. The course does not foresee any face-to-face teaching and is designed such that students follow a programme with an increasing degree of difficulty, in order to be able to produce subject access points to information and to build a structured list of indexing terms (<b>thesaurus).</b> The <b>content</b> analysis is thoroughly discussed and put into practice through several assessment modes and the representation of concepts is also extensively discussed by the students in a forum and chat sessions with the instructor. The course presents a detailed programme, following a carefully structured calendar, and envisages a model of teaching and learning based on the progressive acquisition of concepts and knowledge. It requires solid organization and well-defined rules that have acted, until now, as factors that have sustained the students' interest and dedication over the six weeks of the course. This type of course fills a recognized gap in most of the regular courses offered by the universities in Portugal, and has thus attracted the attention mainly of information professionals who work in libraries, archives and other information services...|$|R
40|$|Science {{relies on}} data {{in all its}} {{different}} forms. In molecular biology and bioinformatics in particular large scale data generation has taken centre stage {{in the form of}} high-throughput experiments. In line with this exponential increase of experimental data has been the near exponential growth of scientific publications. Yet where classical data mining techniques are still capable of coping with this deluge in structured data (Chapter 2), access of information found in scientific literature is still limited to search engines allowing searches on the level keywords, titles and abstracts. However, large amounts of knowledge about biological entities and their relations are held within the body of articles. When extracted, this data can be used as evidence for existing knowledge or hypothesis generation making scientific literature a valuable scientific resource. To unlock the information inside the articles requires a dedicated set of techniques and approaches tailored to the unstructured nature of free text. Analogous to the field of data mining for the analysis of structured data, the field of text mining has emerged for unstructured text and a number of applications has been developed in that field. This thesis is about text mining in the field of metabolomics. The work focusses on strategies for accessing large collections of scientific text and on the text mining steps required to extract metabolic reactions and their constituents, enzymes and metabolites, from scientific text. Metabolic reactions are important for our understanding of metabolic processes within cells and that information provides an important link between genotype phenotype. Furthermore information about metabolic reactions stored in databases is far from complete making it an excellent target for our text mining application. In order to access the scientific publications for further analysis they can be used as flat text or loaded into database systems. In Chapter 2 we assessed and discussed the capabilities and performance of XML-type database systems to store and access very large collections of XML-type documents {{in the form of the}} Medline corpus, a collection of more than 20 million of scientific abstracts. XML data formats are common in the field of bioinformatics and are also at the core of most web services. With the increasing amount of data stored in XML comes the need for storing and accessing the data. The database systems were evaluated on a number of aspects broadly ranging from technical requirements to ease-of-use and performance. The performance of the different XML-type database systems was measured Medline abstract collections of increasing size and with a number of different queries. One of the queries assessed the capabilities of each database system to search the full-text of each abstract, which would allow access to the information within the text without further text analysis. The results show that all database systems cope well with the small and medium dataset, but that the full dataset remains a challenge. Also the query possibilities varied greatly across all studied databases. This led us to conclude that the performances and possibilities of the different database types vary greatly, also depending on the type of research question. There is no single system that outperforms the others; instead different circumstances can lead to a different optimal solution. Some of these scenarios are presented in the chapter. Among the conclusions of Chapter 2 is that conventional data mining techniques do not work for the natural language part of a publication beyond simple retrieval queries based on pattern matching. The natural language used in written text is too unstructured for that purpose and requires dedicated text mining approaches, the main research topic of this thesis. Two major tasks of text mining are named entity recognition, the identification of relevant entities in the text, and relation extraction, the identification of relations between those named entities. For both text mining tasks many different techniques and approaches have been developed. For the named entity recognition of enzymes and metabolites we used a dictionary-based approach (Chapter 3) and for metabolic reaction extraction a full grammar approach (Chapter 4). In Chapter 3 we describe the creation of two thesauri, one for enzymes and one for metabolites with the specific goal of allowing named entity identification, the mapping of identified synonyms to a common identifier, for metabolic reaction extraction. In the case of the enzyme thesaurus these identifiers are Enzyme Nomenclature numbers (EC number), in the case of the metabolite thesaurus KEGG metabolite identifiers. These thesauri are applied to the identification of enzymes and metabolites in the text mining approach of Chapter 4. Both were created from existing data sources by a series of automated steps followed by manual curation. Compared to a previously published chemical thesaurus, created entirely with automated steps, our much smaller metabolite thesaurus performed on the same level for F-measure with a slightly higher precision. The enzyme thesaurus produced results equal to our metabolite thesaurus. The compactness of our thesauri permits the manual curation step important in guaranteeing accuracy of the <b>thesaurus</b> <b>contents,</b> whereas creation from existing resources by automated means limits the effort required for creation. We concluded that our thesauri are compact and of high quality, and that this compactness does not greatly impact recall. In Chapter 4 we studied the applicability and performance of a full parsing approach using the two thesauri described in Chapter 3 for the extraction of metabolic reactions from scientific full-text articles. For this we developed a text mining pipeline built around a modified dependency parser from the AGFL grammar lab using a pattern-based approach to extract metabolic reactions from the parsing output. Results of a comparison to a modified rule-based approach by Czarnecki et al. using three previously described metabolic pathways from the EcoCyc database show a slightly lower recall compared to the rule-based approach, but higher precision. We concluded that despite its current recall our full parsing approach to metabolic reaction extraction has high precision and potential to be used to (re-) construct metabolic pathways in an automated setting. Future improvements to the grammar and relation extraction rules should allow reactions to be extracted with even higher specificity. To identify potential improvements to the recall, the effect of a number of text pre-processing steps on the performance was tested in a number of experiments. The one experiment that had the most effect on performance was the conversion of schematic chemical formulas to syntactic complete sentences allowing them to be analysed by the parser. In addition to the improvements to the text mining approach described in Chapter 4 I make suggestions in Chapter 5 for potential improvements and extensions to our full parsing approach for metabolic reaction extraction. Core focus here is the increase of recall by optimising each of the steps required for the final goal of extracting metabolic reactions from the text. Some of the discussed improvements are to increase the coverage of the used thesauri, possibly with specialist thesauri depending on the analysed literature. Another potential target is the grammar, where there is still room to increase parsing success by taking into account the characteristics of biomedical language. On a different level are suggestions to include some form of anaphora resolution and across sentence boundary search to increase the amount of information extracted from literature. In the second part of Chapter 5 I make suggestions as to how to maximise the information gained from the text mining results. One of the first steps should be integration with other biomedical databases to allow integration with existing knowledge about metabolic reactions and other biological entities. Another aspect is some form of ranking or weighting of the results to be able to distinguish between high quality results useful for automated analyses and lower quality results still useful for manual approaches. Furthermore I provide a perspective on the necessity of computational literature analysis in the form of text mining. The main reasoning here is that human annotators cannot keep up with the amount of publications so that some form of automated analysis is unavoidable. Lastly I discuss the role of text mining in bioinformatics and with that also the accessibility of both text mining results and the literature resources necessary to create them. An important requirement for the future of text mining is that the barriers around high-throughput access to literature for text mining applications have to be removed. With regards to accessing text mining results, there is a long way to go for many applications, including ours, before they can be used directly by biologists. A major factor is that these applications rarely feature a suitable user interface and easy to use setup. To conclude, I see the main role of a text mining system like ours mainly in gathering evidence for existing knowledge and giving insights into the nuances of the research landscape of a given topic. When using the results of our reaction extraction system for the identification of ‘new’ reactions it is important to go back to the actual evidence presented for extra validations and to cross-validate the predictions with other resources or experiments. Ideally text mining will be used for generation of hypotheses, in which the researcher uses text mining findings to get ideas on, in our case, new connections between metabolites and enzymes; subsequently the researcher needs to go back to the original texts for further study. In this role text mining is an essential tool on the workbench of the molecular biologist...|$|R

