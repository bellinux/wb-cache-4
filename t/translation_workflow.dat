28|24|Public
50|$|Smartcat is a {{cloud-based}} environment enabling the <b>translation</b> <b>workflow</b> of companies (including translation companies) and individual translators. As such, it falls under several software categories, such as computer-assisted translation, globalization management, and freelance marketplace.|$|E
50|$|Translate.org.za {{released}} various versions and in 2006 Pootle {{was further}} developed {{as part of}} the WordForge project, a project funded by the Open Society Institute and the International Development Research Centre. This added XLIFF file management and infrastructure for <b>translation</b> <b>workflow.</b> Many of these features were added in the 1.0 release.|$|E
5000|$|... openTMS is a free, open source, {{translation}} memory system based entirely on open-source standards (for an explanation/definition of translation memories, see Wikipedia). The system {{relies heavily on}} XLIFF, TMX (Translation Memory eXchange), TBX (Termbase Exchange format) and accompanying standards to ensure a free and unencumbered exchange of translation and localization data. openTMS implements a standard <b>translation</b> <b>workflow,</b> but features a very fine-grained modularity that makes its adaptable to any conceivable workflow design.|$|E
50|$|PhraseApp is {{a web-based}} {{translation}} management system, {{also known as}} Globalization Management System (GMS). Competitors include Transifex and Smartling. It is targeted at open source and commercial software projects and allows the automation of <b>translation</b> <b>workflows</b> through a token-based API.|$|R
40|$|Abstract. The {{localization}} industry currently deploys language <b>translation</b> <b>workflows</b> {{based on}} heterogeneous tool-chains. Standardized tool interchange formats such as XLIFF (XML Localization Interchange File Format) {{have had some}} impact on enabling more agile <b>translation</b> <b>workflows.</b> However the rise of new tools based on machine translation technology and the growing demand for enterprise linked data applications has created new interoperability challenges as workflows need to encompass {{a broader range of}} tools. In this paper we pre-sent an approach of representing mappings between RDF-based representations of multilingual content and meta-data. To represent the mappings, we use a combination of SPARQL Inferencing Notation (SPIN) and meta-data. Our ap-proach allows the mapping representation to be published as Linked Data. In contrast to other frameworks such as R 2 R, the mappings are executed via a standard SPARQL processor. The objective is to provide a more agile approach to <b>translation</b> <b>workflows</b> and greater interoperability between software tools by leveraging the ongoing innovation in the Multilingual Web field. Our use case is a Language Technology retraining workflow where publishing mappings leads to new opportunities for interoperability and end-to-end tool-chain analyt-ics. We present the results from an initial experiment which compared our ap-proach of executing and representing mappings to that of a similar approach-the R 2 R Framework...|$|R
50|$|Traditional <b>translation</b> <b>workflows</b> were {{typically}} lock-step affairs, where the document {{first went to}} A where it was translated, then to B where it was proofread, and maybe to C where a subject matter expert might review it. Questions and answers {{were typically}} handled by the translation manager.. However, by allowing all the participants to share resources and work simultaneously in a single, cloud-based workspace, the lifecycle was shortened and quality increased.|$|R
50|$|SDL Trados Studio has {{integrated}} {{machine translation}} and postediting into its <b>translation</b> <b>workflow.</b> If the appropriate parameter setting is made, SDL Trados Studio will insert a machine translation of a translation unit (TU) if no match {{is found in}} the translation memory. The translator can then post-edit the machine translation for added clarity. SDL Trados currently supports the following MT systems: Language Weaver, SDL BeGlobal, SDL LanguageCloud and Google Translate. SDL Trados Studio also supports the integration of Microsoft Translator and other MT system through its open API and plugin architecture on the SDL OpenExchange.|$|E
5000|$|... memoQ has {{integrated}} {{machine translation}} and postediting into its <b>translation</b> <b>workflow.</b> With {{the selection of}} appropriate conditions and a plug-in for machine translation, machine-generated translation units (TUs) will be inserted if no match is found in an active translation memory. The translator can then post-edit the machine translation {{in the attempt to}} make sense of it. memoQ currently includes plug-ins which support the following MT systems: Omniscien Technologies (formerly Asia Online),Globalese, iTranslate4.eu, KantanMT, Let's MT!, Systran MT, Google Translate, Microsoft Translator and a pseudotranslation engine. Other MT systems can be integrated via the application programming interface (API).|$|E
5000|$|DITA {{provides}} {{support for}} translation via the localization-atts attribute group. Element attributes {{can be set}} to indicate whether {{the content of the}} element should be translated. The language of the element content can be specified, as can the writing direction, the index filtering and some terms that are injected when publishing to the final format. A DITA project can be converted to an XLIFF file and back into its original maps and topics, using the DITA-XLIFF Roundtrip Tool for DITA-OT and computer-assisted translation (CAT) tools, like Swordfish Translation Editor or Fluenta DITA Translation Manager, a tool designed to implement the <b>translation</b> <b>workflow</b> suggested by the article [...] "Using XLIFF to Translate DITA Projects" [...] published by the DITA Adoption TC at OASIS.|$|E
40|$|Machine Translation Quality Estimation is a {{notoriously}} difficult task, which lessens its usefulness in real-world translation environments. Such scenarios {{can be improved}} if quality predictions are accompanied by a measure of uncertainty. However, models in this task are traditionally evaluated {{only in terms of}} point estimate metrics, which do not take prediction uncertainty into account. We investigate probabilistic methods for Quality Estimation that can provide well-calibrated uncertainty estimates and evaluate them in terms of their full posterior predictive distributions. We also show how this posterior information can be useful in an asymmetric risk scenario, which aims to capture typical situations in <b>translation</b> <b>workflows.</b> Comment: Proceedings of CoNLL 201...|$|R
40|$|Bilingual termbanks are {{important}} for many natural language processing (NLP) applications, es-pecially in <b>translation</b> <b>workflows</b> in industrial settings. In this paper, we apply a log-likelihood comparison method to extract monolingual terminology from the source and target sides of a parallel corpus. Then, using a Phrase-Based Statistical Machine Translation model, we create a bilingual terminology with the extracted monolingual term lists. We manually evaluate our novel terminology extraction model on English-to-Spanish and English-to-Hindi data sets, and observe excellent performance for all domains. Furthermore, we report the performance of our monolin-gual terminology extraction model comparing {{with a number of}} the state-of-the-art terminology extraction models on the English-to-Hindi datasets. ...|$|R
40|$|Post-Editing of Machine Translation (MT) {{has become}} a reality in {{professional}} <b>translation</b> <b>workflows.</b> In order to optimize the management of projects that use post-editing and avoid underpayments and mistrust from professional translators, effective tools to assess the quality of Machine Translation (MT) systems need {{to be put in}} place. One field of study that could address this problem is Machine Translation Quality Estimation (MTQE), which aims to determine the quality of MT without an existing reference. Accurate and reliable MTQE can help project managers and translators alike, as it would allow estimating more precisely the cost of post-editing projects in terms of time and adequate fares by discarding those segments that are not worth post-editing (PE) and have to be translated from scratch. In this paper, we report on the results of an impact study which engages professional translators in PE tasks using MTQE. We measured translators? productivity in different scenarios: translating from scratch, post-editing without using MTQE, and post-editing using MTQE. Our results show that QE information, when accurate, improves post-editing efficiency...|$|R
50|$|Based on EN 15038, this {{standard}} transfers the original EN 15038 requirements to the ISO framework. For example, it defines resource types including human resources (such as translators, revisers, reviewers, proofreaders and project managers) {{as well as}} technical and technological resources. Translation steps including translation (including a check of translation by the translator himself or herself), revision by a second person, review (an optional step, designed {{in order to assess}} the suitability of the translation against the agreed purpose, domain, and the recommended corrective measures), proofreading (an optional pre-publication check) and final verification are defined by {{this standard}}. Besides the standard itself, there are six Annexes that help to explain certain aspects of the standard by providing examples or graphics so as to visualize the standard, e.g. Annex A visualizes <b>translation</b> <b>workflow,</b> Annex D lists pre-production tasks.|$|E
40|$|This chapter {{offers a}} {{comparison}} of five formal quality evaluation tools and shows in what ways such tools can {{be integrated into the}} <b>translation</b> <b>workflow.</b> The functional features of the tools are compared, their user-friendliness, their efficiency and their cost. Examples are given that illustrate in what ways the choice of QA tool for a specific translation project is determined by the project settings...|$|E
40|$|The new {{frontier}} of computer assisted translation {{technology is the}} effective integration of statistical MT within the <b>translation</b> <b>workflow.</b> In this respect, the SMT ability of incrementally learning from the translations produced by users plays a central role. A still open problem is the evaluation of SMT systems that evolve over time. In this paper, we propose a new metric for assessing the quality of an adaptive MT component that {{is derived from the}} theory of learning curves: the percentage slope. ...|$|E
40|$|Open {{infrastructures}} {{for publishing}} 	 	Dynamic publishing offers publishers {{one of the}} few routes to audience growth and financial sustainability in an age of digital disruption. 	 Dynamic publishing is a digital workflow where the publication processes are automated (layout, multi-format conversion, distribution, rights clearance, <b>translation</b> <b>workflows</b> and payments) and made available on request for reuse, to give access to new audiences and revenues. Currently, the majority of publishers are excluded from the dynamic publishing arena. The software systems on offer are either too expensive on the high-end of the product range or not ‘fit for purpose’ on the low-end. 	 	The objective of the Hybrid Publishing Consortium (HPC) is to significantly lower the price of professional digital publishing systems through the creation of open infrastructure for publishing based on free software and open standards. 	 The infrastructure would be supported as a variety of industry partners, open source communities and research groups. HPC’s contribution to this network is to connect the scholarly publishing community with the long standing work on open standards in digital publishing by the industry and the open source community...|$|R
40|$|Abstract—Localization, and in {{particular}} translation, is a key aspect of modern end-user software applications. Open source systems have traditionally taken advantage of distributed and volunteer collaboration to carry localization tasks. In this paper, we will analyze the Android source code repository to know how localization and translation is managed: who participates {{in this kind of}} tasks, if the <b>translation</b> <b>workflows,</b> participants and processes follow the same patterns {{as the rest of the}} development, and if the Android project takes benefit from external contributions. Our results show that Android should ease the localization tasks to benefit from external contributions. Steps towards obtaining a specialized team as found in many other free software projects are also encouraged. Keywords-Android; translation; localization; i 18 n; l 10 n; min-ing software repositories; c© 2012 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promo-tional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works...|$|R
40|$|Workflow graphs {{represent}} the main control-flow constructs of industrial process modeling languages such as BPMN, EPC and UML Activity diagrams, whereas free-choice workflow nets {{is a well}} understood class of Petri nets that possesses many efficient analysis techniques. In this paper, we provide new results on the <b>translation</b> between <b>workflow</b> graphs and free-choice workflow nets. We distinguish workflow graphs with and without inclusive Or-logic. For workflow graphs without inclusive logic, we show that workflow graphs and free-choice workflow nets are essentially the same thing. More precisely, each workflow graph and each free-choice workflow net can be brought into an equivalent normal form such that the normal forms are, in some sense, isomorphic. This result {{gives rise to a}} translation from arbitrary free-choice workflow nets to workflow graphs. For workflow graphs with inclusive logic, we provide various techniques to replace inclusive Or-joins by subgraphs without inclusive logic, thus giving rise to <b>translations</b> from <b>workflow</b> graphs to free-choice nets. Additionally, we characterize the applicability of these replacements. Finally, we also display a simple workflow graph with an inclusive Or-join, which, in some sense, cannot be replaced. This shows a limitation of translating inclusive logic into free-choice nets and illustrates also a difficulty of translating inclusive logic into general Petri nets. Keywords: Workflow graphs; Petri nets; Free choice; Inclusive OR-joi...|$|R
40|$|Handling {{terminology}} is {{an important}} matter in a <b>translation</b> <b>workflow.</b> However, current Machine Translation (MT) systems do not yet propose anything proactive upon tools which assist in managing terminological databases. In this work, we investigate several enhancements to analogical learning and test our implementation on translating medical terms. We show that the analogical engine works equally well when translating from and into a morphologically rich language, or when dealing with language pairs written in different scripts. Combining it with a phrasebased statistical engine leads to significant improvements. ...|$|E
40|$|In this presentation, {{we focus}} on {{integrating}} ma-chine translation (MT) into an existing corporate localization and <b>translation</b> <b>workflow.</b> This MT extended workflow includes a customized post-editing sub-workflow together with crowdsourced, incentives based translation evaluation feedback routines that enable automated learning processes. The core of the implementation is a semantic re-pository that comprises the necessary information artifacts and links to language resources to organ-ize, manage and monitor the different human and machine roles, tasks, and the entire lifecylce of the localization and translation supply chain(s). ...|$|E
40|$|Although Machine Translation (MT) {{has been}} {{attracting}} {{more and more}} attention from the translation industry, the quality of current MT systems still requires humans to post-edit translations to ensure their quality. The time necessary to post-edit bad quality trans-lations can be the same or even longer than that of translating without an MT system. It is well known, however, {{that the quality of}} an MT system is generally not homoge-neous across all translated segments. In or-der to make MT more useful to the transla-tion industry, it is therefore crucial to have a mechanism to judge MT quality at the seg-ment level to prevent bad quality translations from being post-edited within the <b>translation</b> <b>workflow.</b> We describe an approach to esti-mate translation post-editing effort at sentence level in terms of Human-targeted Translation Edit Rate (HTER) based on a number of fea-tures reflecting the difficulty of translating the source sentence and discrepancies between the source and translation sentences. HTER is a simple metric and obtaining HTER anno-tated data can be made part of the <b>translation</b> <b>workflow.</b> We show that this approach is more reliable at filtering out bad translations than other simple criteria commonly used in the translation industry, such as sentence length. ...|$|E
40|$|This paper {{describes}} the strategic vision {{for a new}} <b>translation</b> management <b>workflow</b> for the US Government’s National Virtual Transla-tion Center (NVTC). The paper also describes past, current, and planned experiments vali-dating the vision, along with experiment re-sults to-date. The most salient features of the new workflow include the embedding of translation technology at {{the front end of}} the <b>workflow</b> (e. g., <b>translation</b> memory technol-ogy, specialized lexicons, and machine trans-lation), technology-generated “seed translation”, a new human work role called “paralinguist ” to assess the “seed translation” and assign an appropriate translator/post-editor, and new human translation strategies including federated search of online dictionar-ies and collaborative translation. ...|$|R
40|$|The {{increasing}} use of eXtensible Markup Language (XML) {{is bringing}} additional challenges to statistical machine translation (SMT) and computer assisted <b>translation</b> (CAT) <b>workflow</b> integration in the translation industry. This paper analyzes the need to handle XML markup {{as a part of}} the translation material in a technical domain. It explores different ways of handling such markup by applying transducers in pre and post-processing steps. A series of experiments indicates that XML markup needs a specific treatment in certain scenarios. One of the proposed methods not only satisfies the SMT-CAT integration need, but also provides slightly improved translation results on English-to-Spanish and English-to-French translations, compared to having no additional pre or post-processing steps. ...|$|R
40|$|Abstract. This paper {{describes}} use of {{the formal}} modeling language Colored Petri Nets (CPNs) {{in the development of}} a new bank system. As a basis for the paper, we present a requirements model, intheformof a CPN, which describes a new bank work process that must be supported by the new system. This model has been used to specify, validate, and elicit user requirements. The contribution of this paper is to describe two translation steps that go from the requirements CPN to an implementation of the new system. In the first <b>translation</b> step, a <b>workflow</b> model is derived from the requirements model. This model is represented in terms of a so-called Colored Workflow Net (CWN), which is a generalization of the classical workflow nets to CPN. In the second translation step, the CWN is translated into implementation code. The target implementation language is BPEL 4 WS deployed in the context of IBM WebSphere. A semi-automatic <b>translation</b> of the <b>workflow</b> model to BPEL 4 WS is possible beca{{use of the}} structural requirements imposed on CWNs...|$|R
40|$|When {{conducting}} {{market research}} on machine translation, we research {{the volume of}} sales continuously {{in order to determine}} the scale of the machine translation market in Japan. We have officially announced these figures every year. Furthermore, since 2003, we administered questionnaires regarding the Web translation service and MT package software to general users through AAMT homepage. In this paper we will report on the two kinds of execution conditions outlined above and present the results, and introduce to the case study of <b>translation</b> <b>workflow</b> using MT and TM by the Japanese translator 1) ...|$|E
40|$|Following the {{guidelines}} for MT evaluation proposed in the ISLE taxonomy, this paper presents considerations and procedures for evaluating the integration of machine-translated segments into a larger <b>translation</b> <b>workflow</b> with Translation Memory (TM) systems. The scenario here focuses on the software localisation industry, which already uses TM systems and looks to further streamline the overall translation process by integrating Machine Translation (MT). The main agents involved in this evaluation scenario are localisation managers and translators; the primary aspects of evaluation are speed, quality, and user acceptance. Using the penalty feature of Translation Memory systems, the authors also outline a possible method for finding the "right place" for MT produced segments among TM matches with different degrees of fuzziness...|$|E
40|$|Custom machine {{translation}} (MT) engines systematically outperform general-domain MT engines when translating within the relevant custom domain. This paper investigates {{the use of}} the Jensen-Shannon divergence measure for automatically routing new documents within a translation system with multiple MT engines to the appropriate custom MT engine in order to obtain the best translation. Three distinct domains are compared, and the impact of the language, size, and preprocessing of the documents on the Jensen-Shannon score is addressed. Six test datasets are then compared to the three known-domain corpora to predict which of the three custom MT engines they would be routed to at runtime given their Jensen-Shannon scores. The results are promising for incorporating this divergence measure into a <b>translation</b> <b>workflow...</b>|$|E
40|$|This paper 1 {{gives an}} outline of the final results of the TransRouter 2 project. In {{the scope of this}} project a {{decision}} support System for translation managers has been developed, which will support the selection of appropriate routes for translation projects. In this paper emphasis is put on the decision model, which is based an a stepwise refined assessment of <b>translation</b> routes. The <b>workflow</b> of using this system is considered as well. 1...|$|R
40|$|This paper {{presents}} the formal syntax and the operational semantics of Taverna, a workflow management {{system with a}} large user base among the e-Science community. Such formal foundation, which {{has so far been}} lacking, opens the way to the <b>translation</b> between Taverna <b>workflows</b> and other process models. In particular, the ability to automatically compile a simple domain-specific process description into Taverna facilitates its adoption by e-scientists who are not expert workflow developers. We demonstrate this potential through a practical use case. ...|$|R
40|$|Quality {{estimation}} (QE) approaches aim {{to predict}} the quality of an automatically generated output without relying on manually-crafted references. Having access only to the system's input and output, a QE module assigns a score or a label to each (input, output) pair. In this thesis we develop approaches {{to predict the}} quality of outputs of two types of natural language processing (NLP) systems: machine translation (MT) and automatic speech recognition (ASR). The work presented here {{can be divided into}} three parts. The first part presents advances on the standard approaches to MT QE. We describe a general feature extraction framework, several quality indicators dependent and independent from MT systems that generate the translations, and new quality indicators that approximate the cross-lingual mapping between the meaning of source and translated sentences. Such advances result in state-of-the-art performance in two official evaluation campaigns on the MT QE problem. In the second part we show that the standard MT QE approaches suffer from domain drift problems due to the high specificity of labeled data currently available. In the standard MT QE framework, models are trained on data from a specific text type, with translations produced by one MT system and with labels obtained over the work of specific individual translators. Such models present poor performance when {{one or more of these}} conditions change. The ability of a system to adapt and cope with such changes is a facet of the QE problem that so far has been disregarded. To address these issues and deal with the noisy conditions of real-world <b>translation</b> <b>workflows,</b> we propose adaptive approaches to QE that are robust to both the diverse nature of translation jobs and differences between training and test data. In the third part, we propose and define an ASR QE framework. We identify useful quality indicators and show that ASR QE can be performed without having access to the ASR system, by only exploring information of its inputs and outputs. We apply a subset of the same adaptive techniques developed for MT QE and show that the ASR QE setting can also benefit from robust adaptive learning algorithms...|$|R
40|$|Word {{reordering}} is {{a difficult}} task for decoders when the languages involved have {{a significant difference in}} syntax. Phrase-based statistical machine translation (PBSMT), preferred in commercial settings due to its maturity, is particularly prone to errors in long range reordering. Source sentence pre-ordering, as a pre-processing step before PBSMT, proved to be an efficient solution that can be achieved using limited resources. We propose a dependency-based pre-ordering model with parameters optimized using a reordering score to pre-order the source sentence. The source sentence is then translated using an existing phrase-based system. The proposed solution is very simple to implement. It uses a hierarchical phrase-based statistical machine translation system (HPBSMT) for pre-ordering, combined with a PBSMT system for the actual translation. We show that the system can provide alternate translations of less post-editing effort in a <b>translation</b> <b>workflow</b> with German as the source language...|$|E
40|$|Within recent years, corpora {{have gained}} {{considerable}} importance in Translation Studies, {{and a number}} of studies have also appeared which show their value for translator training (e. g. Zanettin et al. 2003). However, results from a recent survey reveal that current practising and trainee translators still have insufficient awareness of corpora and expertise in using them to help in their <b>translation</b> <b>workflow.</b> In addition, while corpus linguistics courses are offered at some universities, no materials for self-learning are available to our knowledge: such materials might not only complement traditional courses, but would also be of special interest for professional translators, who are often under serious time constraints. This paper presents a free eLearning course on “Corpora for Translators” which has been developed by the EU-funded MeLLANGE project in an attempt to fill this gap. It deals with the use of corpora for different translation-related activities (e. g. source text analysis, translation proper, revision, terminology extraction) and is tailored to the needs of professional and trainee translators alike. Within recent years, corpora have gained considerable importance in Translation Studies, {{and a number of}} studies have also appeared which show their value for translator training (e. g. Zanettin et al. 2003). However, results from a recent survey reveal that current practising and trainee translators still have insufficient awareness of corpora and expertise in using them to help in their <b>translation</b> <b>workflow.</b> In addition, while corpus linguistics courses are offered at some universities, no materials for self-learning are available to our knowledge: such materials might not only complement traditional courses, but would also be of special interest for professional translators, who are often under serious time constraints. This paper presents a free eLearning course on “Corpora for Translators” which has been developed by the EU-funded MeLLANGE project in an attempt to fill this gap. It deals with the use of corpora for different translation-related activities (e. g. source text analysis, translation proper, revision, terminology extraction) and is tailored to the needs of professional and trainee translators alike...|$|E
40|$|The {{volume is}} a {{collection}} of papers that deal with the issue of translation quality from a number of perspectives. It addresses the quality of human translation and machine translation, of pragmatic and literary translation, of translations done by students and by professional translators. Quality is not merely looked at from a linguistic point of view, but the wider context of QA in the <b>translation</b> <b>workflow</b> also gets ample attention. The authors take an inductive approach: the papers are based on the analysis of translation data and/or on hands-on experience. The book provides a bird's eye view of the crucial quality issues, the close collaboration between academics and industry professionals safeguarding attention for quality in the 'real world'. For this reason, the methodological stance is likely to inspire the applied researcher. The analyses and descriptions also include best practices for translation trainers, professional translators and project managers...|$|E
40|$|AbstractA core {{technology}} of {{natural language processing}} (NLP) incorporated into many text processing applications {{is a part of}} speech (POS) tagger, a software component that labels words in text with syntactic tags such as noun, verb, adjective, etc. These tags may then be used within more complex tasks such as parsing, question answering, and machine translation (MT). In this paper we describe the phases of our work training and evaluating statistical POS taggers on Arabic texts and their English <b>translations</b> using Kepler <b>workflows.</b> While the original objectives for encapsulating our research code within Kepler workflows were driven by software engineering needs to document and verify the re usability of our software, our research benefitted as well: the ease of rapid retraining and testing enabled our researchers to detect reporting discrepancies, document their source, independently validating the correct results...|$|R
40|$|A central {{problem in}} {{workflow}} concerns optimizing {{the distribution of}} work in a workflow: how should the execution of tasks and the management of tasks be distributed across multiple processing nodes (i. e., computers). In some cases task management and or execution may be at a processing node with limited functionality. For this reason, {{it is useful to}} optimize <b>translations</b> of (sub-) <b>workflow</b> schemas into flowcharts, that can be executed in a restricted environment, e. g., in a scripting language or using a flowchart-based workflow engine. This paper presents a framework for optimizing the physical distribution of workflow schemas, and the mapping of sub-workflow schemas into flowcharts. We provide a general model for representing essentially any distribution of a workflow schema, and for representing a broad variety of execution strategies. The model is based on families of "communicating flowcharts" (CFs). In the framework, a workflow schema is first rewritten as a family of CFs whi [...] ...|$|R
40|$|Translation has {{historically}} been performed by bilinguals equipped with specialised topic knowledge. In the mid 20 th century, textual theory and discourse analysis saw emphasis on a top-down, whole-text approach that {{paved the way for}} modern professional translators as linguistic transfer experts. This professionalisation was further driven by the digital revolution in the 90 s which caused a huge increase in translation demand, and the creation of purpose-designed translation tools—principally translation memory (TM). However, the same technological processes that briefly empowered the professional translator also signalled a return to a bottom-up approach by concentrating on the segment. Twenty years on, <b>translation</b> tools and <b>workflows</b> continue to narrow this focus, even tending towards simple post-editing of machine translated output. As a result, topic-proficient bilinguals are again entering mainstream translation tasks via simplified translation management processes and crowdsourcing approaches. This article explores these recent trends and predicts that, over the next decade, professional translators will find it increasingly difficult to survive as linguistic transfer experts alone...|$|R
