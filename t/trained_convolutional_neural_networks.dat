3|10000|Public
40|$|Abstract?This paper {{studies the}} use of Convolutional Neural Networks to {{automatically}} detect and classify diseases, nutritional deficiencies and damage by herbicides on apple trees from images of their leaves. This task is fundamental to guarantee a high quality of the resulting yields and is currently largely performed by experts in the field, which can severely limit scale and add to costs. By using a novel data set containing labeled examples consisting of 2539 images from 6 known disorders, we show that <b>trained</b> <b>Convolutional</b> <b>Neural</b> <b>Networks</b> are able to match or outperform experts in this task, achieving a 97. 3 % accuracy on a hold-out set. 201...|$|E
40|$|Retinal {{image quality}} {{assessment}} (IQA) algorithms use different hand crafted features without considering {{the important role}} of the human visual system (HVS). We solve the IQA problem using the principles behind the working of the HVS. Unsupervised information from local saliency maps and supervised information from <b>trained</b> <b>convolutional</b> <b>neural</b> <b>networks</b> (CNNs) are combined to make a final decision on image quality. A novel algorithm is proposed that calculates saliency values for every image pixel at multiple scales to capture global and local image information. This extracts generalized image information in an unsupervised manner while CNNs provide a principled approach to feature learning without the need to define hand-crafted features. The individual classification decisions are fused by weighting them according to their confidence scores. Experimental results on real datasets demonstrate the superior performance of our proposed algorithm over competing methods...|$|E
40|$|In Hezaveh et al. 2017 {{we showed}} that deep {{learning}} {{can be used for}} model parameter estimation and <b>trained</b> <b>convolutional</b> <b>neural</b> <b>networks</b> to determine the parameters of strong gravitational lensing systems. Here we demonstrate a method for obtaining the uncertainties of these parameters. We review the framework of variational inference to obtain approximate posteriors of Bayesian neural networks and apply it to a network trained to estimate the parameters of the Singular Isothermal Ellipsoid plus external shear and total flux magnification. We show that the method can capture the uncertainties due to different levels of noise in the input data, as well as training and architecture-related errors made by the network. To evaluate the accuracy of the resulting uncertainties, we calculate the coverage probabilities of marginalized distributions for each lensing parameter. By tuning a single hyperparameter, the dropout rate, we obtain coverage probabilities approximately equal to the confidence levels for which they were calculated, resulting in accurate and precise uncertainty estimates. Our results suggest that neural networks can be a fast alternative to Monte Carlo Markov Chains for parameter uncertainty estimation in many practical applications, allowing more than seven orders of magnitude improvement in speed. Comment: submitted to ApJ...|$|E
30|$|The {{results show}} that the use of a <b>trained</b> <b>convolutional</b> <b>neural</b> <b>network</b> {{classifier}} substantially improves the recognition of agglomerates compared to classification based on particle size.|$|R
40|$|Abstract. <b>Training</b> <b>convolutional</b> <b>neural</b> <b>networks</b> (CNNs) {{on large}} sets of {{high-resolution}} images is too computationally intense {{to be performed}} on commodity CPUs. Such architectures, however, achieve state-of-theart results on low-resolution machine vision tasks such as recognition of handwritten characters. We have adapted the inherent multi-level parallelism of CNNs for Nvidia’s CUDA GPU architecture to accelerate the training by two orders of magnitude. This dramatic speedup permits to apply CNN architectures to pattern recognition tasks on datasets with high-resolution natural images. ...|$|R
50|$|Following the 2005 {{paper that}} {{established}} {{the value of}} GPGPU for machine learning, several publications described more efficient ways to <b>train</b> <b>convolutional</b> <b>neural</b> <b>networks</b> using GPUs. In 2011, they were refined and implemented on a GPU, with impressive results. In 2012, Ciresan et al. significantly improved on the best performance in the literature for multiple image databases, including the MNIST database, the NORB database, the HWDB1.0 dataset (Chinese characters), the CIFAR10 dataset (dataset of 60000 32x32 labeled RGB images), and the ImageNet dataset.|$|R
30|$|CNN has {{provided}} an end-to-end learning {{model in which}} the parameters can be trained by the traditional gradient descent method. The <b>trained</b> <b>convolutional</b> <b>neural</b> <b>network</b> can {{learn the features from}} the image and complete the extraction and classification of image features. As an important branch in the research of <b>neural</b> <b>networks,</b> the <b>convolutional</b> <b>neural</b> <b>network</b> is characterized by the practicability that the features of each layer can be obtained through the convolution kernel of the shared weight by the local region of the upper layer. Thus, this characteristic makes the <b>convolutional</b> <b>neural</b> <b>network</b> more suitable for the learning and expressing the image features than other <b>neural</b> <b>network</b> methods.|$|R
40|$|Inspired by {{the finding}} that {{vanishing}} point (road tangent) guides driver's gaze, in our previous work we showed that vanishing point attracts gaze during free viewing of natural scenes {{as well as in}} visual search (Borji et al., Journal of Vision 2016). We have also introduced improved saliency models using vanishing point detectors (Feng et al., WACV 2016). Here, we aim to predict vanishing points in naturalistic environments by <b>training</b> <b>convolutional</b> <b>neural</b> <b>networks</b> in an end-to-end manner over a large set of road images downloaded from Youtube with vanishing points annotated. Results demonstrate effectiveness of our approach compared to classic approaches of vanishing point detection in the literature...|$|R
40|$|It is {{commonly}} {{agreed that the}} use of relevant invariances as a good statistical bias is important in machine-learning. However, most approaches that explicitly incorporate invariances into a model architecture only make use of very simple transformations, such as translations and rotations. Hence, {{there is a need for}} methods to model and extract richer transformations that capture much higher-level invariances. To that end, we introduce a tool allowing to parametrize the set of filters of a <b>trained</b> <b>convolutional</b> <b>neural</b> <b>network</b> with the latent space of a generative adversarial network. We then show that the method can capture highly non-linear invariances of the data by visualizing their effect in the data space...|$|R
40|$|International audienceThis paper {{addresses}} {{the problem of}} image features selection for pedestrian gender recognition. Hand-crafted features (such as HOG) are compared with learned features which are obtained by <b>training</b> <b>convolutional</b> <b>neural</b> <b>networks.</b> The comparison is performed on the recently created collection of versatile pedestrian datasets which allows us to evaluate the impact of dataset properties {{on the performance of}} features. The study shows that hand-crafted and learned features perform equally well on small-sized homogeneous datasets. However, learned features significantly outperform hand-crafted ones in the case of heterogeneous and unfamiliar (unseen) datasets. Our best model which is based on learned features obtains 79 % average recognition rate on completely unseen datasets. We also show that a relatively small <b>convolutional</b> <b>neural</b> <b>network</b> is able to produce competitive features even with little training data...|$|R
30|$|Gradient descent {{algorithm}} is simple, easy to converge, but easy {{to fall into}} the local optimal solution, and the gradient descent near the saddle point is slow, affecting the training of network model. The saddle point can be avoided in the training process of Newton algorithm, but the method needs to compute Heisen matrix to ensure its non-negative positive definite and a large amount of storage space and to ensure the existence of second-order derivatives of the objective function; otherwise, it is difficult to ensure the convergence of the algorithm [26 – 28]. In order to avoid the difficulties caused by the direct use of the Newton algorithm, the improved BFGS quasi-Newton algorithm [29] is used to <b>train</b> <b>convolutional</b> <b>neural</b> <b>networks.</b>|$|R
40|$|Content-based music {{information}} retrieval tasks {{have traditionally been}} solved using engineered features and shallow processing architectures. In recent years, there has been increasing interest in using feature learning and deep architectures instead, thus reducing the required engineering effort {{and the need for}} prior knowledge. However, this new approach typically still relies on mid-level representations of music audio, e. g. spectrograms, instead of raw audio signals. In this paper, we investigate whether it is possible to apply feature learning directly to raw audio signals. We <b>train</b> <b>convolutional</b> <b>neural</b> <b>networks</b> using both approaches and compare their performance on an automatic tagging task. Although they do not outperform a spectrogram-based approach, the networks are able to autonomously discover frequency decompositions from raw audio, as well as phase-and translation-invariant feature representations...|$|R
40|$|In {{this paper}} we propose to use the Winner Takes All hashing {{technique}} to speed up forward propagation and backward propagation in fully connected layers in <b>convolutional</b> <b>neural</b> <b>networks.</b> The proposed technique reduces significantly the computational complexity, which in turn, allows us to train layers {{with a large number}} of kernels with out the associated time penalty. As a consequence we are able to <b>train</b> <b>convolutional</b> <b>neural</b> <b>network</b> on {{a very large number of}} output classes with only a small increase in the computational cost. To show the effectiveness of the technique we train a new output layer on a pretrained network using both the regular multiplicative approach and our proposed hashing methodology. Our results showed no drop in performance and demonstrate, with our implementation, a 7 fold speed up during the training. Comment: 9 pages, 9 figures, 1 tabl...|$|R
40|$|This paper {{introduces}} {{a new approach}} to automatically quantify the severity of knee OA using X-ray images. Automatically quantifying knee OA severity involves two steps: first, automatically localizing the knee joints; next, classifying the localized knee joint images. We introduce {{a new approach to}} automatically detect the knee joints using a fully <b>convolutional</b> <b>neural</b> <b>network</b> (FCN). We <b>train</b> <b>convolutional</b> <b>neural</b> <b>networks</b> (CNN) from scratch to automatically quantify the knee OA severity optimizing a weighted ratio of two loss functions: categorical cross-entropy and mean-squared loss. This joint training further improves the overall quantification of knee OA severity, with the added benefit of naturally producing simultaneous multi-class classification and regression outputs. Two public datasets are used to evaluate our approach, the Osteoarthritis Initiative (OAI) and the Multicenter Osteoarthritis Study (MOST), with extremely promising results that outperform existing approaches...|$|R
40|$|Many {{structural}} variations (SVs) {{detection methods}} have been proposed due to the popularization of next-generation sequencing (NGS). These SV calling methods use different SV-property-dependent features; however, they all suffer from poor accuracy when running on low coverage sequences. The union of results from these tools achieves fairly high sensitivity but still produces low accuracy on low coverage sequence data. That is, these methods contain many false positives. In this paper, we present CNNdel, an approach for calling deletions from paired-end reads. CNNdel gathers SV candidates reported by multiple tools and then extracts features from aligned BAM files at the positions of candidates. With labeled feature-expressed candidates as a training set, CNNdel <b>trains</b> <b>convolutional</b> <b>neural</b> <b>networks</b> (CNNs) to distinguish true unlabeled candidates from false ones. Results show that CNNdel works well with NGS reads from 26 low coverage genomes of the 1000 Genomes Project. The paper demonstrates that <b>convolutional</b> <b>neural</b> <b>networks</b> can automatically assign the priority of SV features and reduce the false positives efficaciously...|$|R
40|$|Biological {{membranes}} {{are one of}} {{the most}} basic structures and regions of interest in cell biology. In the study of membranes, segment extraction is a well-known and difficult problem because of impeding noise, directional and thickness variability, etc. Recent advances in electron microscopy membrane segmentation are able to cope with such difficulties by <b>training</b> <b>convolutional</b> <b>neural</b> <b>networks.</b> However, because of the massive amount of features that have to be extracted while propagating forward, the practical usability diminishes, even with state-of-the-art GPU's. A significant part of these network features typically contains redundancy through correlation and sparsity. In this work, we propose a pruning method for <b>convolutional</b> <b>neural</b> <b>networks</b> that ensures the training loss increase is minimized. We show that the pruned networks, after retraining, are more efficient in terms of time and memory, without significantly affecting the network accuracy. This way, we manage to obtain real-time membrane segmentation performance, for our specific electron microscopy setup...|$|R
40|$|Generating a novel textual {{description}} of an im-age {{is an interesting}} problem that connects com-puter vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syn-tax of the descriptions. We train a purely bilinear model that learns a metric between an image rep-resentation (generated from a previously <b>trained</b> <b>Convolutional</b> <b>Neural</b> <b>Network)</b> and phrases {{that are used to}} described them. The system is then able to infer phrases from a given image sam-ple. Based on caption syntax statistics, we pro-pose a simple language model that can produce relevant descriptions for a given test image us-ing the phrases inferred. Our approach, which is considerably simpler than state-of-the-art mod-els, achieves comparable results in two popular datasets for the task: Flickr 30 k and the recently proposed Microsoft COCO. 1...|$|R
40|$|This paper {{proposes a}} new {{framework}} for RGB-D-based action recognition that takes advantages of hand-designed features from skeleton data and deeply learned features from depth maps, and exploits effectively both {{the local and}} global temporal information. Specifically, depth and skeleton data are firstly augmented for deep learning and making the recognition insensitive to view variance. Secondly, depth sequences are segmented using the hand-crafted features based on skeleton joints motion histogram to exploit the local temporal information. All training se gments are clustered using an Infinite Gaussian Mixture Model (IGMM) through Bayesian estimation and labelled for <b>training</b> <b>Convolutional</b> <b>Neural</b> <b>Networks</b> (ConvNets) on the depth maps. Thus, a depth sequence can be reliably encoded into a sequence of segment labels. Finally, the sequence of labels is fed into a joint Hidden Markov Model and Support Vector Machine (HMM-SVM) classifier to explore the global temporal information for final recognition...|$|R
40|$|We propose DoReFa-Net, {{a method}} to <b>train</b> <b>convolutional</b> <b>neural</b> <b>networks</b> that have low bitwidth weights and activations using low bitwidth {{parameter}} gradients. In particular, during backward pass, parameter gradients are stochastically quantized to low bitwidth numbers before being propagated to convolutional layers. As convolutions during forward/backward passes can now operate on low bitwidth weights and activations/gradients respectively, DoReFa-Net can use bit convolution kernels to accelerate both training and inference. Moreover, as bit convolutions can be efficiently implemented on CPU, FPGA, ASIC and GPU, DoReFa-Net opens the way to accelerate training of low bitwidth <b>neural</b> <b>network</b> on these hardware. Our experiments on SVHN and ImageNet datasets prove that DoReFa-Net can achieve comparable prediction accuracy as 32 -bit counterparts. For example, a DoReFa-Net derived from AlexNet that has 1 -bit weights, 2 -bit activations, can be trained from scratch using 6 -bit gradients to get 46. 1 % top- 1 accuracy on ImageNet validation set. The DoReFa-Net AlexNet model is released publicly...|$|R
40|$|We {{propose a}} novel visual {{tracking}} algorithm {{based on the}} representations from a discriminatively <b>trained</b> <b>Convolutional</b> <b>Neural</b> <b>Network</b> (CNN). Our algorithm pretrains a CNN using a large set of videos with tracking ground-truths to obtain a generic target representation. Our network is composed of shared layers and multiple branches of domain-specific layers, where domains correspond to individual training sequences and each branch is responsible for binary classification to identify the target in each domain. We train the network with respect to each domain iteratively to obtain generic target representations in the shared layers. When tracking a target in a new sequence, we construct a new network by combining the shared layers in the pretrained CNN with a new binary classification layer, which is updated online. Online tracking is performed by evaluating the candidate windows randomly sampled around the previous target state. The proposed algorithm illustrates outstanding performance compared with state-of-the-art methods in existing tracking benchmarks...|$|R
40|$|Egocentric {{cameras are}} being worn by an {{increasing}} number of users, among them many security forces world-wide. GoPro cameras already penetrated the mass market, and Google Glass may follow soon. As head-worn cam-eras do not capture the face and body of the wearer, it may seem that the anonymity of the wearer can be preserved even when the video is publicly distributed. We show that motion features in egocentric video pro-vide biometric information, and the identity of the user can be determined quite reliably from a few seconds of video. Biometrics are extracted by <b>training</b> <b>Convolutional</b> <b>Neural</b> <b>Network</b> (CNN) architectures on coarse optical flow. Egocentric video biometrics can prevent theft of wear-able cameras by locking the camera when worn by people other than the owner. In video sharing services, this Bio-metric measure can help to locate automatically all videos shot by the same user. An important message in this paper is that people should be aware that sharing egocentric video will compromise their anonymity. 1...|$|R
40|$|We {{study the}} problem of Salient Object Subitizing, i. e. {{predicting}} the existence {{and the number of}} salient objects in an image using holistic cues. This task is inspired by the ability of people to quickly and accurately identify the number of items within the subitizing range (1 - 4). To this end, we present a salient object subitizing image dataset of about 14 K everyday images which are annotated using an online crowdsourcing marketplace. We show that using an end-to-end <b>trained</b> <b>Convolutional</b> <b>Neural</b> <b>Network</b> (CNN) model, we achieve prediction accuracy comparable to human performance in identifying images with zero or one salient object. For images with multiple salient objects, our model also provides significantly better than chance performance without requiring any localization process. Moreover, we propose a method to improve the training of the CNN subitizing model by leveraging synthetic images. In experiments, we demonstrate the accuracy and generalizability of our CNN subitizing model and its applications in salient object detection and image retrieval...|$|R
40|$|We used a large dataset of Sentinel- 1 and TerraSAR-X quicklook images {{downloaded}} {{from the}} internet in order to classify the imagery into different classes of subscenes including: open ocean, land, sea ice and ships using <b>Convolutional</b> <b>Neural</b> <b>Network</b> (CNN) classifiers. We construct a training dataset of subscenes of the images using visual inspection and AIS data. We then focused on the open ocean scenes acquired under different environmental conditions to classify them into different wind speed and sea state categories. We compare the results to wind speed, sea state model results and NOAA buoy measurements. In order to find the subscenes containing ships, icebergs and oil slicks we further utilize the CNN over open ocean and coastal SAR scenes. Statistics on validation is given using categorical cross entropy loss. In addition several high resolution images are used {{in order to test}} the performance of the <b>trained</b> <b>Convolutional</b> <b>Neural</b> <b>Network.</b> This study will help to retrieve such images relevant to maritime investigations of ships, oil and environmental parameters using big data methods...|$|R
40|$|In {{this project}} {{we work on}} {{creating}} a model to classify images for the Pascal VOC Challenge 2012. We use <b>convolutional</b> <b>neural</b> <b>networks</b> <b>trained</b> on a single GPU instance provided by Amazon via their cloud service Amazon Web Services (AWS) to classify images in the Pascal VOC 2012 data set. We <b>train</b> multiple <b>convolutional</b> <b>neural</b> <b>network</b> models and finally settle on the best model which produced a validation accuracy of 85. 6 % and a testing accuracy of 85. 24 %...|$|R
40|$|We {{present a}} filter based {{approach}} for inbetweening. We <b>train</b> a <b>convolutional</b> <b>neural</b> <b>network</b> to generate intermediate frames. This network aim to generate smooth animation of line drawings. Our method can process scanned images directly. Our method {{does not need}} to compute correspondence of lines and topological changes explicitly. We experiment our method with real animation production data. The results show that our method can generate intermediate frames partially. Comment: 10 pages, in Japanes...|$|R
40|$|Adaptive {{stochastic}} gradient {{methods such as}} AdaGrad {{have gained}} popularity in particular for training deep <b>neural</b> <b>networks.</b> The most commonly used and studied variant maintains a diagonal matrix approximation to second order information by accumulating past gradients which are used to tune the step size adaptively. In certain situations the full-matrix variant of AdaGrad is expected to attain better performance, however in high dimensions it is computationally impractical. We present Ada-LR and RadaGrad two computationally efficient approximations to full-matrix AdaGrad based on randomized dimensionality reduction. They are able to capture dependencies between features and achieve similar performance to full-matrix AdaGrad but at a much smaller computational cost. We show that the regret of Ada-LR {{is close to the}} regret of full-matrix AdaGrad which can have an up-to exponentially smaller dependence on the dimension than the diagonal variant. Empirically, we show that Ada-LR and RadaGrad perform similarly to full-matrix AdaGrad. On the task of <b>training</b> <b>convolutional</b> <b>neural</b> <b>networks</b> as well as recurrent <b>neural</b> <b>networks,</b> RadaGrad achieves faster convergence than diagonal AdaGrad. Comment: To appear in Advances in Neural Information Processing Systems 29 (NIPS 2016...|$|R
40|$|Despite {{the recent}} {{advances}} in automatically describing image contents, their applications have been mostly limited to image caption datasets containing natural images (e. g., Flickr 30 k, MSCOCO). In this paper, we present a deep learning model to efficiently detect a disease from an image and annotate its contexts (e. g., location, severity and the affected organs). We employ a publicly available radiology dataset of chest x-rays and their reports, and use its image annotations to mine disease names to <b>train</b> <b>convolutional</b> <b>neural</b> <b>networks</b> (CNNs). In doing so, we adopt various regularization techniques to circumvent the large normal-vs-diseased cases bias. Recurrent <b>neural</b> <b>networks</b> (RNNs) are then trained to describe the contexts of a detected disease, based on the deep CNN features. Moreover, we introduce a novel approach to use the weights of the already trained pair of CNN/RNN on the domain-specific image/text dataset, to infer the joint image/text contexts for composite image labeling. Significantly improved image annotation results are demonstrated using the recurrent neural cascade model by taking the joint image/text contexts into account...|$|R
30|$|Yan et al. [10] {{proposed}} a two-stage MIL method for computer tomography body part recognition. The authors divide the input image into patches and <b>train</b> a <b>Convolutional</b> <b>Neural</b> <b>Network</b> (CNN) on each patch on an IS fashion {{to find the}} discriminative patches. The second stage uses the learned discriminative patches as ground truth and adds a new class to the final layer to represent the non-discriminative patches. The image label corresponds to the label of the most discriminative patch.|$|R
40|$|Discrete Fourier {{transforms}} {{provide a}} significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain {{also provides a}} powerful representation in which to model and <b>train</b> <b>convolutional</b> <b>neural</b> <b>networks</b> (CNNs). We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility {{in the choice of}} pooling output dimensionality. This representation also enables a new form of stochastic regularization by randomized modification of resolution. We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling. Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training...|$|R
40|$|The Bach 10 Separation SMC 2017 dataset {{is derived}} from the Bach 10 dataset, which {{contains}} ten pieces of Bach chorales along the scores. We separate the audio files in the original dataset and in the dataset we synthesized with Sibelius ([URL] using the approaches presented in this paper: Marius Miron, Jordi Janer, Emilia Gomez, "Generating data to <b>train</b> <b>convolutional</b> <b>neural</b> <b>networks</b> for low latency classical music source separation", Sound and Music Computing Conference 2017 The dataset contains the separated audio files along the computed measures which give the quality of separation: SDR, SIR, SAR, computed with BSS Eval 3. 0. For the intellectual rights and the distribution policy of the original dataset check the Bach 10 dataset page: [URL] The files in Bach 10 Separation SMC 2017 dataset are offered free of charge for non-commercial use only. You can not redistribute them nor modify them. This dataset is created by Marius Miron, Music Technology Group - Universitat Pompeu Fabra (Barcelona). This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4. 0 Unported License...|$|R
40|$|The Dice {{score is}} widely used for binary {{segmentation}} due to its robustness to class imbalance. Soft generalisations of the Dice score {{allow it to be}} used as a loss function for <b>training</b> <b>convolutional</b> <b>neural</b> <b>networks</b> (CNN). Although CNNs trained using mean-class Dice score achieve state-of-the-art results on multi-class segmentation, this loss function does neither take advantage of inter-class relationships nor multi-scale information. We argue that an improved loss function should balance misclassifications to favour predictions that are semantically meaningful. This paper investigates these issues in the context of multi-class brain tumour segmentation. Our contribution is threefold. 1) We propose a semantically-informed generalisation of the Dice score for multi-class segmentation based on the Wasserstein distance on the probabilistic label space. 2) We propose a holistic CNN that embeds spatial information at multiple scales with deep supervision. 3) We show that the joint use of holistic CNNs and generalised Wasserstein Dice scores achieves segmentations that are more semantically meaningful for brain tumour segmentation. Comment: Accepted as an oral presentation at the MICCAI 2017 Brain Lesion (BrainLes) Worksho...|$|R
40|$|The {{problem of}} speaker and channel {{adaptation}} in deep <b>neural</b> <b>network</b> (DNN) based {{automatic speech recognition}} (ASR) sys-tems is of substantial interest in advancing the performance of these systems. Recently, the speaker identity vectors (i-vectors) have shown improvements for ASR systems in matched condi-tions. In this paper, we propose {{the application of the}} general factor analysis framework for noisy speech recognition tasks. Several methods for deriving speaker and channel factors are explored including joint factor analysis (JFA) and i-vectors de-rived from DNN posteriors instead of the traditional Universal background model (UBM) approach. We also experiment with the late fusion of i-vector features with bottleneck (BN) fea-tures obtained from a previously <b>trained</b> <b>convolutional</b> <b>neural</b> <b>network</b> (CNN) system. The ASR experiments are performed on the Aspire challenge test data which contains noisy far-field speech while the acoustic models are trained with conversa-tional telephone speech (CTS) data from the Fisher corpus. In these experiments, we show that the factor analysis based meth-ods provide significant improvements in the word error rate (relative improvements of about 11 % compared to the baseline DNN system trained with speaker adapted features) ...|$|R
40|$|Recent {{years have}} {{produced}} great advances in training large, deep <b>neural</b> <b>networks</b> (DNNs), including notable successes in <b>training</b> <b>convolutional</b> <b>neural</b> <b>networks</b> (convnets) to recognize natural images. However, {{our understanding of}} how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e. g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup. Comment: 12 pages. To appear at ICML Deep Learning Workshop 201...|$|R
40|$|We have {{developed}} and <b>trained</b> a <b>convolutional</b> <b>neural</b> <b>network</b> to automatically and simultaneously segment optic disc, fovea and blood vessels. Fundus images were normalised before segmentation {{was performed to}} enforce consistency in background lighting and contrast. For every effective point in the fundus image, our algorithm extracted three channels of input from the neighbourhood of the point and forward the response across the 7 layer network. In average, our segmentation achieved an accuracy of 92. 68 percent on the testing set from Drive database...|$|R
40|$|The {{focus of}} this project has been on <b>training</b> <b>convolutional</b> <b>neural</b> <b>networks</b> for grasp {{detection}} with synthetic data. <b>Convolutional</b> <b>neural</b> <b>networks</b> have had great success {{on a wide variety}} of computer vision tasks, but they require large amounts of labelled training data, which currently is non existent for grasp detection tasks. In this thesis, a novel approach for generating large amounts of synthetic data for grasp detection is proposed. By working solely with depth images, realistic looking data can be generated with 3 D models in a virtual environment. It is proposed to use simulated physics to ensure that the generated depth images captures objects in natural poses. Additionally, the use of heuristics for choosing the best grip vectors for the objects in relation to their environment is proposed, to serve as the labels for the generated depth images. A virtual environment for synthetic depth image generation was created and a <b>convolutional</b> <b>neural</b> <b>network</b> was <b>trained</b> on the generated data. The results show that <b>neural</b> <b>networks</b> can find good grasps from the synthetic depth images for three different types of objects in cluttered scenes. A novel way of creating real world data sets for grasping using a head mounted display and tracked hand controllers is also proposed. The results show that this may enable easy and fast labelling of real data which can be performed without training by non-technical people...|$|R
40|$|Recent {{advances}} in optimization methods used for <b>training</b> <b>convolutional</b> <b>neural</b> <b>networks</b> (CNNs) with kernels, which are normalized according to particular constraints, have shown remarkable success. This work introduces an approach for training CNNs using ensembles of joint spaces of kernels constructed using different constraints. For this purpose, we {{address a problem}} of optimization on ensembles of products of submanifolds (PEMs) of convolution kernels. To this end, we first propose three strategies to construct ensembles of PEMs in CNNs. Next, we expound their geometric properties (metric and curvature properties) in CNNs. We make use of our theoretical results by developing a geometry-aware SGD algorithm (G-SGD) for optimization on ensembles of PEMs to train CNNs. Moreover, we analyze convergence properties of G-SGD considering geometric properties of PEMs. In the experimental analyses, we employ G-SGD to train CNNs on Cifar- 10, Cifar- 100 and Imagenet datasets. The results show that geometric adaptive step size computation methods of G-SGD can improve training loss and convergence properties of CNNs. Moreover, we observe that classification performance of baseline CNNs can be boosted using G-SGD on ensembles of PEMs identified by multiple constraints. Comment: 7 page...|$|R
