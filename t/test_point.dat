485|3538|Public
25|$|South Africa's most capped {{player is}} Victor Matfield with 127 caps. The most-capped back is Percy Montgomery. Montgomery also holds the South African record for Test points with 893, {{which at the}} time of his {{international}} retirement placed him sixth on the all-time list of <b>Test</b> <b>point</b> scorers (he now stands ninth).|$|E
2500|$|The {{distance}} of the <b>test</b> <b>point</b> to the i-th isotherm is given by ...|$|E
2500|$|Examining the CIE 1960 UCS reveals {{this point}} to be closest to 2938K on the Planckian locus, which has a {{co-ordinate}} of (0.2528, 0.3484). The distance of the <b>test</b> <b>point</b> to the locus is under the limit (5.4×10−3), so we can continue the procedure, assured of a meaningful result: ...|$|E
3000|$|... are the ‘n’ <b>test</b> <b>points</b> [33]. The {{number of}} <b>test</b> <b>points</b> ‘n’, in each {{iteration}} step, determines {{the rate of}} convergence of the algorithm.|$|R
40|$|EP 0636895 (A 1) The present {{invention}} {{relates to}} a test grid for an unpopulated printed circuit, comprising <b>test</b> <b>points</b> (T) linked to measurement circuits. Each measurement circuit {{is linked to}} a group (G) of <b>test</b> <b>points,</b> the <b>test</b> <b>points</b> of each group being linked together by resistors (r) of non-zero values...|$|R
40|$|This paper {{presents}} an approach for embedding of test concepts into high-level synthesis. Symbolic <b>test</b> <b>points</b> declared by a test concept specification are mapped on physical registers. This {{is done by}} an assignment of <b>test</b> <b>points</b> to symbolic registers and by a usual register binding tool. The experiments show that this way of integration does not result in larger designs. 1 Introduction The use of built-in self test methods or partial scan paths requires the selection of <b>test</b> <b>points.</b> The selection of <b>test</b> <b>points</b> is a problem which is mostly considered after {{the design of the}} RT-structure. But it can be very difficult or even impossible to find a configuration of <b>test</b> <b>points,</b> which can realize a given test concept. In that cases, where it is impossible to find a configuration of <b>test</b> <b>points,</b> new registers are inserted in the design. These new registers are only used as <b>test</b> <b>points.</b> This paper {{presents an}} approach for embedding of test concepts using the design space. The test concep [...] ...|$|R
2500|$|Whereas the {{original}} {{problem may be}} stated in a finite dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that {{the original}} finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products may be computed easily {{in terms of the}} variables in the original space, by defining them in terms of a kernel function [...] selected to suit the problem. The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant. The vectors defining the hyperplanes can be chosen to be linear combinations with parameters [...] of images of feature vectors [...] that occur in the data base. With this choice of a hyperplane, the points [...] in the feature space that are mapped into the hyperplane are defined by the relation: [...] Note that if [...] becomes small as [...] grows further away from , each term in the sum measures the degree of closeness of the <b>test</b> <b>point</b> [...] to the corresponding data base point [...] In this way, the sum of kernels above can be used to measure the relative nearness of each <b>test</b> <b>point</b> to the data points originating in {{one or the other of}} the sets to be discriminated. Note the fact that the set of points [...] mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets which are not convex at all in the original space.|$|E
5000|$|This {{intuitive}} {{approach can}} be made quantitative by defining the normalized distance between the <b>test</b> <b>point</b> and the set to be [...] By plugging this into the normal distribution we can derive the probability of the <b>test</b> <b>point</b> belonging to the set.|$|E
50|$|The {{drawback}} of {{the above}} approach was that we assumed that the sample points are distributed about the center of mass in a spherical manner. Were the distribution to be decidedly non-spherical, for instance ellipsoidal, then we would expect the probability of the <b>test</b> <b>point</b> belonging to the set to depend {{not only on the}} distance from the center of mass, but also on the direction. In those directions where the ellipsoid has a short axis the <b>test</b> <b>point</b> must be closer, while in those where the axis is long the <b>test</b> <b>point</b> can be further away from the center.|$|E
40|$|This {{research}} {{was designed to}} determine a small set of low-beam <b>test</b> <b>points</b> for recommendation as common <b>test</b> <b>points</b> throughout the world. Our recommendation is a compromise among the following three set of inputs: (1) expert opinion, based on a worldwide survey of 119 experts in lighting and vision, (2) current practice, based on an analysis of candela matrices of 150 production low beams, and (3) scientific evidence concerning visibility and glare under nighttime driving conditions. Expert opinion and scientific evidence did not fully converge on the same <b>test</b> <b>points,</b> with the main difference being {{in the amount of}} light recommended for points at which objects need to be seen. While experts suggested light levels comparable to current production outputs, the recommendations based exclusively on scientific evidence would call for light levels of more than ten times the current levels. Therefore, the <b>test</b> <b>points</b> based exclusively on scientific evidence should be viewed only as ideal <b>test</b> <b>points,</b> but we should aim in the future to explore technologies that would make approximations to these <b>test</b> <b>points</b> feasible...|$|R
5000|$|After {{moving the}} <b>test</b> <b>points,</b> the linear {{equation}} part is repeated, {{getting a new}} polynomial, and Newton's method is used again to move the <b>test</b> <b>points</b> again. This sequence is continued until the result converges to the desired accuracy. The algorithm converges very rapidly.Convergence is quadratic for well-behaved functions—if the <b>test</b> <b>points</b> are within [...] of the correct result, they will be approximately within [...] of the correct result after the next round.|$|R
40|$|Golden section search {{strategies}} (GSSS), dichotomous search strategies (DSS), and Z-score strategies (ZSS) {{are simple}} and robust computerized adaptive testing strategies. GSSS, DSS, and {{one version of}} ZSS are similar in that statistical hypothesis testing occurs at each successive <b>testing</b> <b>point</b> in determining the current ability estimates. After each item is administered, the examinee 2 ̆ 7 s obtained score is compared with the expected score at successive <b>testing</b> <b>points.</b> If the examinee 2 ̆ 7 s obtained score does not exceed a confidence interval of an expected score at a <b>testing</b> <b>point,</b> the examinee 2 ̆ 7 s current ability estimate {{is assumed to be}} equal to that of the <b>testing</b> <b>point.</b> Otherwise, a hypothesis testing will be conducted at the next <b>testing</b> <b>point</b> and the process is continued until the examinee 2 ̆ 7 s current ability estimate is determined and the next item is selected. The three strategies differ in the successive <b>testing</b> <b>points</b> 2 ̆ 7 allocation. Each middle point of successive golden search regions is a <b>testing</b> <b>point</b> in GSSS; each middle point of successive dichotomous search regions is a <b>testing</b> <b>point</b> in DSS; each Z-score estimate evaluated at the previous <b>testing</b> <b>point</b> is the next <b>testing</b> <b>point</b> in ZSS. No hypothesis testing is involved in another version of ZSS in which the current ability estimate is the Z-score estimate evaluated at the previous ability estimate. Results of Monte Carlo studies in three hypothetical item pools and one SAT Verbal item pool showed that GSSS, DSS, and ZSS were computationally efficient, precise throughout the ability continuum, and robust against aberrant responses. Optimal measurement occurs using moderate size confidence intervals. Both versions of ZSS measured well under general conditions. GSSS, DSS, and ZSS provided more accurate and efficient ability estimates than did maximum likelihood estimate strategies (MLES) whenever guessing effect existed. GSSS, DSS, and ZSS were more efficient but not more accurate than MLES whenever guessing was not a factor. For GSSS, DSS, and ZSS, there were no differences in measurement accuracy, but occasional differences in measurement efficiency. GSSS, DSS and ZSS were more robust against guessing and inaccuracy of item parameters and took less time to execute than did MLES...|$|R
50|$|Putting {{this on a}} {{mathematical}} basis, the ellipsoid that best represents the set's probability distribution can be estimated by building the covariance matrix of the samples. The Mahalanobis distance is {{the distance of the}} <b>test</b> <b>point</b> from the center of mass divided by the width of the ellipsoid {{in the direction of the}} <b>test</b> <b>point.</b>|$|E
50|$|FCTs uses {{customer}} specific connectors, {{rather than}} a <b>test</b> <b>point</b> on the PCB.|$|E
5000|$|The {{distance}} of the <b>test</b> <b>point</b> to the i-th isotherm is given by ...|$|E
5000|$|Note {{that the}} error graph does indeed {{take on the}} values [...] at the six <b>test</b> <b>points,</b> {{including}} the end points, but that those points are not extrema. If the four interior <b>test</b> <b>points</b> had been extrema (that is, the function P(x)f(x) had maxima or minima there), the polynomial would be optimal.|$|R
5000|$|Most <b>test</b> <b>points</b> against England in one match(22 points - 1998) ...|$|R
5000|$|... #Caption: Percy Montgomery {{holds the}} South African record for <b>test</b> <b>points.</b>|$|R
5000|$|Given {{a set of}} samples, , we wish to {{estimate}} thedensity, , at a <b>test</b> <b>point,</b> : ...|$|E
50|$|However, we {{also need}} to know if the set is spread out over a large range or a small range, so that we can decide whether a given {{distance}} from the center is noteworthy or not. The simplistic approach is to estimate the standard deviation of the distances of the sample points from the center of mass. If the distance between the <b>test</b> <b>point</b> and the center of mass is less than one standard deviation, then we might conclude that it is highly probable that the <b>test</b> <b>point</b> belongs to the set. The further away it is, the more likely that the <b>test</b> <b>point</b> should not be classified as belonging to the set.|$|E
50|$|In {{order to}} use the Mahalanobis {{distance}} to classify a <b>test</b> <b>point</b> as belonging to one of N classes, one first estimates the covariance matrix of each class, usually based on samples known to belong to each class. Then, given a test sample, one computes the Mahalanobis distance to each class, and classifies the <b>test</b> <b>point</b> as belonging to that class for which the Mahalanobis distance is minimal.|$|E
30|$|The {{salinity}} {{varied from}} 1.5 to 200  g/L, with eight <b>test</b> <b>points.</b>|$|R
30|$|Relative error (RE) {{and root}} {{mean squared error}} (RMSE) were used to {{evaluate}} the error of the approximation models at <b>test</b> <b>points</b> other than those used in building the model. The <b>test</b> <b>points</b> comprises of ten new sample points within the sample space. These points are shown as green doted points augmented in the original design in Fig.  13.|$|R
5000|$|All Black points: 200 points (40 tries) - 200 <b>test</b> <b>points</b> (40 tries) ...|$|R
5000|$|Was the 4th highest {{international}} points scorer {{at time of}} retirement—see List {{of leading}} Rugby union <b>Test</b> <b>point</b> scorers ...|$|E
5000|$|... where ci is {{the class}} of the ith sample.The class of the <b>test</b> <b>point</b> may be {{estimated}} through maximum likelihood.|$|E
50|$|Local linear {{regression}} {{can be applied}} to any-dimensional space, though the question of what is a local neighborhood becomes more complicated. It is common to use k nearest training points to a <b>test</b> <b>point</b> to fit the local {{linear regression}}. This can lead to high variance of the fitted function. To bound the variance, the set of training points should contain the <b>test</b> <b>point</b> in their convex hull (see Gupta et al. reference).|$|E
50|$|There are <b>testing</b> <b>points</b> and clinics, but no {{antiretroviral}} {{therapy is}} reportedly given.|$|R
5000|$|Most <b>test</b> <b>points</b> {{against the}} British Lions in one match (25 points - 2001) ...|$|R
40|$|It {{is proven}} that {{there exists a}} finite set of <b>test</b> <b>points</b> that suffices to {{establish}} the equivalence of two programs s and t if some finite set S of programs can be identified that contains both s and t. It is also proven that the expected number of those <b>test</b> <b>points</b> is bounded by the logarithm of the cardinality of S...|$|R
5000|$|The MDF usually holds {{telephone}} exchange protective devices including heat coils, and {{functions as a}} <b>test</b> <b>point</b> between a line and the exchange equipment.|$|E
5000|$|If the {{distance}} from the <b>test</b> <b>point</b> t to the current node B is greater than the furthest point in Q, ignore B and return Q.|$|E
5000|$|With {{a penalty}} in the 7th minute, Dan Carter surpassed England's Jonny Wilkinson, who sat out the November Tests due to injury, {{as the leading}} <b>Test</b> <b>point</b> scorer in history.|$|E
40|$|In {{supervised}} learning problems, global and local learning algorithms are used. In contrast to global learning algorithms, {{the prediction of}} a local learning algorithm in a <b>testing</b> <b>point</b> is only based on training data which {{are close to the}} <b>testing</b> <b>point.</b> Every global algorithm such as support vector machines (SVM) can be localized in the following way: in every <b>testing</b> <b>point,</b> the (global) learning algorithm is not applied to the whole training data but only to the k nearest neighbors (kNN) of the <b>testing</b> <b>point.</b> In case of support vector machines, the success of such mixtures of SVM and kNN (called SVM-KNN) has been shown in extensive simulation studies and also for real data sets but only little has been known on theoretical properties so far. In the present article, it is shown how a large class of regularized kernel methods (including SVM) can be localized {{in order to get a}} universally consistent learning algorithm...|$|R
25|$|With McCaw's try, {{he becomes}} the first New Zealand forward to score 100 <b>test</b> <b>points.</b>|$|R
25|$|During the match, Dan Carter {{became the}} first player to pass 1400 {{international}} <b>test</b> <b>points.</b>|$|R
