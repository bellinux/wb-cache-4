219|367|Public
2500|$|... {{where the}} <b>true</b> <b>error</b> {{variance}} σ2 {{is replaced by}} an estimate based on the minimised value of the sum of squares objective function S. The denominator, n−m, is the statistical degrees of freedom; see effective degrees of freedom for generalizations.|$|E
5000|$|The {{representativeness}} of {{the sample}} , with respect to [...] and , is defined as:Smaller representativeness is better, since it means that the empirical error of a classifier on the training set is not much lower than its <b>true</b> <b>error.</b>|$|E
5000|$|... {{where the}} <b>true</b> <b>error</b> {{variance}} σ2 {{is replaced by}} an estimate based on the minimised value of the sum of squares objective function S. The denominator, n − m, is the statistical degrees of freedom; see effective degrees of freedom for generalizations.|$|E
40|$|Statically {{analyzing}} requirements specifications {{to assure}} that they possess desirable properties is an important activity in any rigorous software development project. The analysis is performed on an abstraction of the original requirements specification. Abstractions in the model may lead to spurious errors in the analysis output. Spurious errors are conditions that are reported as errors, but information abstracted out of the model precludes the reported conditions from being satisfied. A high ratio of spurious <b>errors</b> to <b>true</b> <b>errors</b> in the analysis output makes it difficult, error-prone, and time consuming to find and correct the <b>true</b> <b>errors.</b> In this paper we describe an iterative and integrative approach for analyzing state-based requirements that capitalizes on the strengths of a symbolic analysis component and a reasoning component while circumventing their weaknesses. The resulting analysis method is fast enough and automated enough to be used {{on a day-to-day basis}} by practicing engineers, and generates analysis reports with a small ratio of spurious <b>errors</b> to <b>true</b> <b>errors.</b> 1...|$|R
3000|$|The overall {{empirical}} convergence {{rates of}} the errors and estimators of the unstabilised computation for adaptive mesh-refinements are better than those for uniform mesh-refinements. This {{is in contrast to}} the stabilised computation, where the <b>true</b> <b>errors</b> [...]...|$|R
40|$|This paper {{considers}} {{the problem of}} fitting an error density to the goodness-of-fit test of the errors in a nonlinear autoregressive stationary time series regression model. The test statistic {{is based on the}} integrated squared error of the nonparametric error density estimate and the null error density. Without knowing the nonlinear autoregressive function, we can show that the test statistic behaves asymptotically the same as the one based on the <b>true</b> <b>errors.</b> Residuals Error density estimation Stationary process...|$|R
50|$|Counter stamped coins have a {{long history}} {{in the early days of}} minted coins. Many {{companies}} used counter stamping as a method to advertise their company. There are thousands of counter stamped coins some of which carry little value while others command values in the thousands. In any case, these coins were changed after leaving the mint, and are not <b>true</b> <b>error</b> coins.|$|E
50|$|The {{basic idea}} of {{bootstrapping}} is that inference about a population from sample data, (sample → population), can be modelled by resampling the sample data and performing inference about a sample from resampled data, (resampled → sample). As {{the population is}} unknown, the <b>true</b> <b>error</b> in a sample statistic against its population value is unknown. In bootstrap-resamples, the 'population' {{is in fact the}} sample, and this is known; hence the quality of inference of the 'true' sample from resampled data, (resampled → sample), is measurable.|$|E
50|$|Burke-Fisher repair {{attempts}} to continue parsing by 'backing up' to 'k' parse tokens before the error point, {{and attempting to}} substitute all possible tokens into all positions from that point {{to the point where}} the error was detected. This procedure is adopted because the point at which an error is detected may not be the point of the actual input error. For instance, the string 'Look up below' might be detected as invalid when 'below' is encountered, but the <b>true</b> <b>error</b> might be that 'up' has been written where 'down' was intended.|$|E
40|$|A {{test for}} serial {{independence}} of regression errors, consistent {{in the direction}} of first order alternatives, is proposed. The test statistic is a function of a Hoeffding-Blum-Kiefer-Rosenblatt type of empirical process, based on residuals. The resultant statistic converges, surprisingly, to the same limiting distribution as the corresponding statistic based on <b>true</b> <b>errors...</b>|$|R
40|$|A {{test for}} serial {{independence}} of regression errors is proposed {{that is consistent}} in the direction ofserial dependence alternatives of first order. The test statistic {{is a function of}} aHoeffding-Blum-Kiefer-Rosenblatt type of empirical process, based on residuals. The resultantstatistic converges, surprisingly, to the same limiting distribution as the corresponding statisticbased on <b>true</b> <b>errors.</b> Empirical process based on residuals; Hoeffding-Blum-Kiefer-Rosenblatt statistic; Serial independence test...|$|R
40|$|Even without feature selection, {{cross-validation}} error estimation {{is problematic}} for small samples {{owing to the}} high variance of the deviation distribution describing {{the difference between the}} estimated and <b>true</b> <b>errors.</b> This paper investigates the increased loss of crossvalidation precision owing to feature selection by comparing deviation distributions and introducing two variation-based measures to quantify the further degradation in performance. 1...|$|R
50|$|Error {{bars are}} {{graphical}} {{representations of the}} variability of data and used on graphs to indicate the error or uncertainty in a reported measurement. They give a general idea of how precise a measurement is, or conversely, how far from the reported value the <b>true</b> (<b>error</b> free) value might be. Error bars often represent one standard deviation of uncertainty, one standard error, or a particular confidence interval (e.g., a 95% interval). These quantities {{are not the same}} and so the measure selected should be stated explicitly in the graph or supporting text.|$|E
50|$|The primary {{advantage}} of utilizing the Schmidt-Kalman filter instead {{of increasing the}} dimensionality of the state space is the reduction in computational complexity. This can enable the use of filtering in real-time systems. Another usage of Schmidt-Kalman is when residual biases are unobservable; that is, {{the effect of the}} bias cannot be separated out from the measurement. In this case, Schmidt-Kalman is a robust way to not try and estimate the value of the bias, but only keep track of the effect of the bias on the <b>true</b> <b>error</b> distribution.|$|E
50|$|There {{is also a}} SAE J1939 standard, {{used for}} CAN data {{transmission}} in heavy road vehicles, which has four logical (boolean) values: False, <b>True,</b> <b>Error</b> Condition, and Not installed (represented by values 0-3). Error Condition means there is a technical problem obstacling data acquisition. The logics for that is for example True and Error Condition=Error Condition. Not installed is used for a feature that {{does not exist in}} this vehicle, and should be disregarded for logical calculation. On CAN, usually fixed data messages are sent containing many signal values each, so a signal representing a not-installed feature will be sent anyway.|$|E
40|$|If natural {{language}} understanding systems are ever {{to cope with the}} full range of English language forms, their designers will have to incorporate a number of features of the spoken vernacular language. This communication discusses such features as non-standard grammatical rules, hesitations and false starts due to self-correction, systematic errors due to mismatches between the grammar and sentence generator, and uncorrected <b>true</b> <b>errors...</b>|$|R
40|$|In this paper, an {{efficient}} algorithm to diagnose design errors in RTL description is proposed. The diagnosis algorithm exploits the hierarchy available in RTL designs to locate design errors. Using data path {{to reduce the}} number of error candidates and ensure that <b>true</b> <b>errors</b> are included in. According to the estimated probability, the most suspected error candidates would be reported first in the display. The advantages of the proposed method are simple and available...|$|R
5000|$|... has a {{t-distribution}} with n − 2 {{degrees of}} freedom if the null hypothesis is <b>true.</b> Thestandard <b>error</b> of the slope coefficient: ...|$|R
50|$|To be {{completely}} accurate, the error term quoted for the reported radiocarbon age should incorporate counting errors {{not only from}} the sample, but also from counting decay events for the reference sample, and for blanks. It should also incorporate errors on every measurement taken as part of the dating method, including, for example, the δ13C term for the sample, or any laboratory conditions being corrected for such as temperature or voltage. These errors should then be mathematically combined to give an overall term for the error in the reported age, but in practice laboratories differ, not only in the terms they choose to include in their error calculations, but also in the way they combine errors. The resulting 1σ estimates have been shown to typically underestimate the <b>true</b> <b>error,</b> and it has even been suggested that doubling the given 1σ error term results in a more accurate value.|$|E
40|$|Bias and {{variance}} for small-sample error estimation {{are typically}} posed {{in terms of}} statistics for the distributions of the true and estimated errors. On the other hand, a salient practical issue asks, given an error estimate, what {{can be said about}} the <b>true</b> <b>error?</b> This question relates to the joint distribution of the true and estimated errors, specifically, the conditional expectation of the <b>true</b> <b>error</b> given the error estimate. A critical issue is that of confidence bounds for the <b>true</b> <b>error</b> given the estimate. We consider the joint distribution of the <b>true</b> <b>error</b> and the estimated error, assuming a random feature-label distribution. From it, we derive the marginal distributions, the conditional expectation of the estimated error given the <b>true</b> <b>error,</b> the conditional expectation of the <b>true</b> <b>error</b> given the estimated error, the conditional variance of the <b>true</b> <b>error</b> given the estimated error, and the 95 % upper con-fidence bound for the <b>true</b> <b>error</b> given the estimated error. Numerous classification and estimation rules are considered across a number of models. Massive simulation is used for continuous models and analytic results are derived for discrete classification. We also consider a breast-cancer study to illustrate how the theory might be applied in practice. Although specific results depend on the classification rule, error-estimation rule, and model, some general trends are seen: (I) if the <b>true</b> <b>error</b> is small (large), then the conditional es-timated error is generally high (low) -biased; (II) the conditional expected <b>true</b> <b>error</b> tends to be larger (smaller) than the estimated error for small (large) estimated errors; and (III) the confidence bounds tend to be well above the estimated error for low error estimates, becoming much less so for large estimates. Key words: Classification error; Estimated error; and Confidence interval...|$|E
40|$|A linearized {{compressible}} viscous Stokes {{system is}} considered. The a posteriori error estimates are defined and {{compared with the}} <b>true</b> <b>error.</b> They are shown to be globally upper and locally lower bounds for the <b>true</b> <b>error</b> of the finite element solution. Some numerical examples are given, showing an efficiency of th...|$|E
40|$|In camera pre-{{calibration}}, {{images of}} a calibration object are commonly {{used to determine the}} internal geometry of a camera. The calibration imaging is often optimized to have the calibration object cover as large image area as possible. This is likely to yield a larger concentration of measured image points {{near the center of the}} image sensor. In this report, the hypothesis is investigated that this non-uniform image point distribution results in a sub-optimal calibration. An area-based reweighting scheme is suggested to improve the calibration. Additionally, the effect of a choice between a 2 D and a 3 D calibration object is investigated. A simulation study was performed where both a standard and area-weighted pre-calibration scheme was used in a parallel and a convergent scene. The estimated uncertainty and <b>true</b> <b>errors</b> were computed and compared to the first order predictions and results of perfect calibrations. The area-based calibration showed no reduction in estimation errors. Furthermore, the 3 D calibration object did not give a noticeable improvement. However, for the standard and area-based calibrations, the <b>true</b> <b>errors</b> surpassed the estimated uncertainties by up to 26 and 58 percent, respectively...|$|R
40|$|In {{this paper}} we {{consider}} the goodness of fit test of the errors of autoregressive models using the kernel estimate of the marginal density function based on residuals. The test statistic {{is based on the}} integrated squared error of the nonparametric density estimate and a smoothed version of the parametric fit of the density. It is shown that the test statistic behaves asymptotically the same as the one based on <b>true</b> <b>errors</b> unless the autoregressive process is unstable. AR(1) process Gaussian test Goodness of fit test Nonparametric density estimate Stationary process Explosive process Unstable process...|$|R
40|$|This Working Paper {{presents}} {{some preliminary}} results {{for a new}} goodness-of- t method for VARMA(p,q) models. Relations between least squares residuals and <b>true</b> <b>errors</b> are re-examined, and a new family of statistics is proposed. A new goodness-of- t process is also suggested, {{that can be seen}} as an extension of a previously proposed technique in Ubierna and Velilla (2007). The limit behavior of this last random object is obtained as a consequence of a collection of asymptotic results that generalize those obtained previously in Hosking (1981) and Ubierna and Velilla (2007) ...|$|R
40|$|We present {{three new}} a posteriori error estimators {{in the energy}} norm for finite element {{solutions}} to elliptic partial differential equations. The estimators are based on solving local Neumann problems in each element. The estimators differ in how they enforce consistency of the Neumann problems. We prove that as the mesh size decreases, under suitable assumptions, two of the estimators approach upper bounds on the norm of the <b>true</b> <b>error,</b> and all three estimators are within multiplicative constants of the norm of the <b>true</b> <b>error.</b> We present numerical results in {{which one of the}} estimators appears to converge to the norm of the <b>true</b> <b>error...</b>|$|E
40|$|Typically the <b>true</b> <b>error</b> of ANN {{prediction}} model is estimated by testing the trained network on new data {{not used in}} model construction. Four well-studied statistical error estimation methods: cross-validation, group cross-validation, jackknife and bootstrap are reviewed and are presented as competing error estimation methodologies {{that could be used}} to evaluate and validate ANN {{prediction model}}s. All four methods utilize the entire sample for the construction of the prediction model and estimate the <b>true</b> <b>error</b> via a resampling methodology...|$|E
30|$|Additionally, a Wilcox {{test was}} {{performed}} to test whether the central tendency of the error percentage distributions was equal to zero. For the ARIMA model, the Wilcox test indicated a 95 % confidence interval of [−[*] 1.91 %, −[*] 1.10 %] with V[*]=[*] 820, 010 and p value ≤[*] 0.0001, providing strong evidence that the ARIMA model had a small negative error percentage bias and that the <b>true</b> <b>error</b> was not 0 %. In contrast, the Wilcox test for the delta algorithm indicated a 95 % confidence interval of [−[*] 0.36 %, 0.20 %] with V[*]=[*] 985, 540 and p value[*]=[*] 0.5624, providing insufficient evidence to reject the null hypothesis that the <b>true</b> <b>error</b> percentage is 0 %. This result implies that the delta algorithm has less error percentage bias and the <b>true</b> <b>error</b> percentage may be closer to 0 %.|$|E
25|$|The {{combination}} of different observations {{as being the}} best estimate of the <b>true</b> value; <b>errors</b> decrease with aggregation rather than increase, perhaps first expressed by Roger Cotes in 1722.|$|R
30|$|The {{use of the}} {{resulting}} local bases in MOR will be dealt with later. Here the approximation properties of the local bases based on the <b>true</b> projection <b>error</b> are investigated.|$|R
2500|$|Note {{that there}} is not {{necessarily}} a strict connection between the true confidence interval, and the <b>true</b> standard <b>error.</b> The <b>true</b> p percent confidence interval is the interval [...] that contains p percent of the distribution, and where (100 [...] p)/2 percent of the distribution lies below a, and (100 [...] p)/2 percent of the distribution lies above b. The <b>true</b> standard <b>error</b> of the statistic is the square root of the true sampling variance of the statistic. These two may not be directly related, although in general, for large distributions that look like normal curves, there is a direct relationship.|$|R
40|$|We {{present a}} new {{approach}} to bounding the <b>true</b> <b>error</b> rate of a continuous valued classifier based upon PAC-Bayes bounds. The method first constructs a distribution over classifiers by determining how sensitive each parameter in the model is to noise. The <b>true</b> <b>error</b> rate of the stochastic classifier found with the sensitivity analysis can then be tightly bounded using a PAC-Bayes bound. In this paper we demonstrate the method on artificial neural networks with results of a  ¢¡¤ £ order of magnitude improvement vs. the best deterministic neural net bounds. ...|$|E
40|$|Producción CientíficaClassification {{rules that}} {{incorporate}} additional information usually present in discrimination problems are receiving certain attention {{during the last}} years as they perform better than the usual rules in poor discrimination problems. Fern´andez et al (2006) proved that these rules have a lower unconditional misclassification probability than the usual Fisher’s rule {{but they did not}} consider the estimation of the conditional error probability when a training sample is given (the so-called <b>true</b> <b>error</b> rate) which is a very interesting parameter in practice. In this paper we consider the problem of estimating the <b>true</b> <b>error</b> rate of these classification rules in the classical topic of discrimination among two normal populations. We prove theoretical results on the apparent error rate of the rules that expose the need of new estimators of their <b>true</b> <b>error</b> rate. Our proposal is to also consider the additional information in the definition of the <b>true</b> <b>error</b> rate estimators. We propose four such new estimators. Two of them are defined incorporating the additional information into the leave-one-out-bootstrap. The other two are the corresponding cross-validation after bootstrap versions. We compare these new estimators with the usual ones in a simulation study and in a cancer trial application, showing the very good behavior, in terms of mean square error, of the leave-one-out bootstrap estimators that incorporate the available additional information. Ministerio de Ciencia e Innovación (Project MTM 2012 - 37129...|$|E
40|$|Model {{selection}} [e. g., 1] {{is considered}} the problem of choosing a hypothesis language which provides an optimal balance between low empirical error and high structural complexity. In this Abstract, we discuss the intuition of a new, very efficient approach to model selection. Our approach is inherently Bayesian [e. g., 2], but instead of using priors on target functions or hypotheses, we talk about priors on error values [...] which leads us to a new mathematical characterization of the expected <b>true</b> <b>error.</b> In the setting of classification learning, a learner is given a sample, drawn according to an unknown distribution of labeled instances, and returns the empirical minimizer (the hypothesis with the least empirical error) which has a certain (unknown) <b>true</b> <b>error.</b> If this process is carried out repeatedly, the <b>true</b> <b>error</b> of the empirical minimizer will vary from run to run as the empirical minimizer depends on the (randomly drawn) sample. This induces a distribution of true errors of empirical minimizers, over the possible samples drawn according to the unknown distribution. If this distribution would be known, one could easily derive the expected <b>true</b> <b>error</b> of the empirical minimizer of a model by integrating over this distribution. This would immediately lead to an optimal model selection algorithm: Enumerate the models, calculate the expected error of each model by integrating over the error distribution, and select the model with the least expected error. PAC theory [3] and the VC framework provide worst-case bounds on the chance of drawing a sample such that the <b>true</b> <b>error</b> of the minimizer exceeds some " [...] "worst-case" meaning that they hold for any distribution of instances and any concept in a given class. By contrast, we focus on how to determine this distributi [...] ...|$|E
40|$|The Palomar Adaptive Optics System {{actively}} corrects {{for changing}} aberrations in light due to atmospheric turbulence. However, the underlying internal static error is unknown and uncorrected by this process. The dedicated wavefront sensor device necessarily lies along {{a different path}} than the science camera, and, therefore, doesn't measure the <b>true</b> <b>errors</b> along the path leading to the final detected imagery. This is a standard problem in adaptive optics (AO) called "non-common path error. " The Autonomous Phase Retrieval Calibration (APRC) software suite performs automated sensing and correction iterations to calibrate the Palomar AO system to levels that were previously unreachable...|$|R
40|$|The {{problems}} faced by the users in the language-checking software are discussed. There are false negatives, where the language-checking software fails to detect <b>true</b> <b>errors,</b> and false positives, where the software detects problems that are not errors. False negatives are troublesome because they might allow users to overlook problems that could be obvious to the human reader. False positives are also troublesome, although this issue has not been studied extensively in a usage context. The level of trust that users attribute to language-checking software may not always commensurate with the software's ability {{to do the job}} without errors...|$|R
50|$|Note {{that there}} is not {{necessarily}} a strict connection between the true confidence interval, and the <b>true</b> standard <b>error.</b> The <b>true</b> p percent confidence interval is the interval b that contains p percent of the distribution, and where (100 &minus; p)/2 percent of the distribution lies below a, and (100 &minus; p)/2 percent of the distribution lies above b. The <b>true</b> standard <b>error</b> of the statistic is the square root of the true sampling variance of the statistic. These two may not be directly related, although in general, for large distributions that look like normal curves, there is a direct relationship.|$|R
