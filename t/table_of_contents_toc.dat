19|10000|Public
5000|$|AMG LASSO is a media {{recognition}} service {{launched by}} the All Media Guide in 2004. The LASSO service automatically recognizes CDs, DVDs, and digital audio files in formats such as MP3, WMA, and others. The service uses CD <b>table</b> <b>of</b> <b>contents</b> (<b>ToC),</b> DVD ToC, and acoustic fingerprint based recognition to recognize media. LASSO is available in versions for PCs and embedded devices.|$|E
5000|$|The {{ability to}} extract the CD-Audio tracks is {{otherwise}} largely dependent on the disc drive used. The first obstacle is the [...] "fake" [...] <b>Table</b> <b>of</b> <b>Contents</b> (<b>ToC),</b> which is intended to mask the audio tracks from CD-ROM drives. However CD-R/RW drives, and similar, can usually access all session data on a disc, and thus can properly read the audio segment.|$|E
5000|$|...cda is {{a common}} {{filename}} extension denoting a small (44 byte) stub file generated by Microsoft Windows for each audio track on a standard [...] "Red Book" [...] CD-DA format audio CD {{as defined by the}} <b>Table</b> <b>of</b> <b>Contents</b> (<b>ToC)</b> (within the lead-in's subcode). These files are shown in the directory for the CD being viewed in the format Track##.cda, where ## is the number of each individual track.|$|E
40|$|Objectives: To explore {{whether the}} {{presence}} <b>of</b> online <b>tables</b> <b>of</b> <b>contents</b> (<b>TOC)</b> {{in an online}} catalog affects circulation (checkouts and inhouse usage). Two major questions were posed: (1) did the presence <b>of</b> online <b>tables</b> <b>of</b> <b>contents</b> for books increase use, and, (2) if it did, what factors might cause the increase...|$|R
40|$|Since the <b>table</b> <b>of</b> <b>content</b> (<b>toc)</b> was {{regularly}} {{too long}} to be visible in its entirety and since the old layout did not allow for vertical scrolling of the toc, we switched over to scrollable panel that slides out from {{the right of the}} screen and is toggled by a short javascript. Because this script needs to be linked from teibp_parameters. xsl this version of the Boilerplate is not backwards compatible...|$|R
50|$|It is a {{database}} of open access journals published in Bangladesh, dealing with {{the full range of}} academic disciplines including both paper based and online only publications. Aim of the project is to make participating peer-reviewed journals' high visibility, high readership and open access over the internet by providing access to <b>tables</b> <b>of</b> <b>contents</b> (<b>TOCs),</b> abstracts and full text. INASP initiated BanglaJOL in June 2007 and officially launched it in September 2007.|$|R
50|$|When first {{released}} in 1982, Compact Discs only contained a <b>Table</b> <b>Of</b> <b>Contents</b> (<b>TOC)</b> {{with the number}} of tracks on the disc and their length in samples.http://s3.amazonaws.com/academia.edu.documents/32801641/Morris_2012_-_Making_Music_Behave.pdf?AWSAccessKeyId=AKIAJ56TQJRTWSMTNPEA&Expires=1477195681&Signature=2TLmhapcR0M5eYsfMQ8FgG2TZa0%3D&response-content-disposition=inline%3B%20filename%3DMaking_music_behave_Metadata_and_the_dig.pdfhttps://books.google.com/books?id=GkIaGZ0HWcMC&pg=PA48&source=gbs_toc_r&cad=4#v=onepage&q&f=false Fourteen years later in 1996, a revision of the CD Red Book standard added CD-Text to carry additional metadata.http://web.ncf.ca/aa571/cdtext.htm But CD-Text was not widely adopted. Shortly thereafter, it became common for personal computers to retrieve metadata from external sources (e.g. CDDB, Gracenote) based on the TOC.|$|E
5000|$|The pregap on a Red Book audio CD is {{the portion}} of the audio track that precedes [...] "index 01" [...] for a given track in the <b>table</b> <b>of</b> <b>contents</b> (<b>TOC).</b> The pregap ("index 00") is {{typically}} two seconds long and usually, but not always, contains silence. Popular uses for having the pregap contain audio are live CDs, track interludes, and hidden songs in the pregap of the first track (detailed below).|$|E
50|$|Webhelp is a chunked html output format in the DocBook xslt stylesheets {{that was}} {{introduced}} in version 1.76.1. The documentation for web help also {{provides an example of}} web help and is part of the DocBook xsl distribution. Its major features include CSS-based page layout without frameset, multilingual full content search, <b>Table</b> <b>of</b> <b>contents</b> (<b>TOC)</b> pane with collapsible TOC tree, Auto-synchronization of content pane and TOC. This web help format was originally implemented by Kasun Gajasinghe and David Cramer as part of the Google Summer of Code 2010 program.|$|E
5000|$|<b>Table</b> <b>of</b> <b>Contents.</b> Insert <b>TOC</b> {{to easily}} {{organize}} notes and navigate through them. TOC is generated automatically and may include a whole notebook's structure or a current section only.|$|R
40|$|Colloque avec actes et comité de lecture. internationale. International audienceA {{labeling}} {{approach for}} automatic recognition <b>of</b> <b>Tables</b> <b>of</b> <b>Contents</b> (<b>ToC)</b> {{is described in}} this paper. A prototype is used for electronic consulting of scientific papers in a digital library system named Calliope. This method operates on a roughly structured ASCII file, produced by OCR. The recognition approach operates by text labeling without using any a priori model. Labeling {{is based on a}} Part of Speech Tagging (PoS) which is initiated by a primary labeling of text component using some specific dictionaries...|$|R
40|$|<b>Table</b> <b>of</b> <b>content</b> (<b>TOC)</b> {{detection}} {{has drawn}} attention now a day because it {{plays an important}} role in digitization of multipage document. Generally book document is multipage document. So it becomes necessary to detect <b>Table</b> <b>of</b> <b>Content</b> page for easy navigation of multipage document and also to make information retrieval faster for desirable data from the multipage document. All the <b>Table</b> <b>of</b> <b>content</b> pages follow the different layout, different way <b>of</b> presenting the <b>contents</b> <b>of</b> the document like chapter, section, subsection etc. This paper introduces a new method to detect <b>Table</b> <b>of</b> <b>content</b> using machine learning technique with different features. With the main aim to detect <b>Table</b> <b>of</b> <b>Content</b> pages is to structure the document according to their contents. Comment: International Journal of Artificial Intelligence and Applications, May- 201...|$|R
50|$|At the {{beginning}} of the disc there is a <b>table</b> <b>of</b> <b>contents</b> (<b>TOC,</b> also known as the System File area of the disc), which stores the start positions of the various tracks, as well as meta information (title, artist) about them and free blocks. Unlike the conventional cassette, a recorded song {{does not need to be}} stored as one piece on the disk, it can be stored in several fragments, similar to a hard drive. Early MiniDisc equipment had a fragment granularity of 4 seconds audio. Fragments smaller than the granularity are not kept track of, which may lead to the usable capacity of a disc actually shrinking. No means of defragmenting the disc is provided in consumer grade equipment.|$|E
5000|$|A normal CD-reader {{will not}} read beyond the first track because, {{according}} to the CD <b>table</b> <b>of</b> <b>contents</b> (<b>TOC),</b> there is no data there. With modified firmware that looks for a second TOC in the high-density region {{it is possible to}} read data from the high-density region even on a normal CD-reader. One can also utilize a [...] "swap-trick" [...] by first letting the CD-reader read the TOC from a normal CD with a large track and then swapping that disc with a GD-ROM in a way that avoids alerting the CD-reader that a new disc has been inserted. It is then possible to read as much data from the high-density region as indicated by the TOC from the first disc.|$|E
5000|$|For example, {{the typical}} Help 1.x build process starts by running MrefBuilder.exe {{to produce an}} XML {{reflection}} file for one or more assemblies. The reflection file is then processed by the XslTransform.exe tool multiple times to apply various XSL transformations that add data such as a [...] "doc model" [...] and optional version information. Next, an XML-based topic manifest is generated and used by the BuildAssembler.exe program, which generates HTML topic files from the reflection data and XML documentation comments. An XML-based <b>table</b> <b>of</b> <b>contents</b> (<b>TOC)</b> file is generated and used by CHMBuilder.exe, along with the HTML files produced by BuildAssembler, to generate HTML Help Workshop project, index and TOC files. Finally, the HTML Help workshop is used to generate a compiled help file (.chm).|$|E
40|$|Consumers {{increasingly}} {{look to the}} Internet {{for health}} information, but available resources are too difficult for the majority to understand. Interactive <b>tables</b> <b>of</b> <b>contents</b> (<b>TOC)</b> can help consumers access health information by providing an easy to understand structure. Using natural language processing and the Unified Medical Language System (UMLS), we have automatically generated TOCs for consumer health information. The TOC are categorized according to consumer-friendly labels for the UMLS semantic types and semantic groups. Categorizing phrases by semantic types is significantly more correct and relevant. Greater correctness and relevance was achieved with documents {{that are difficult to}} read than with those at an easier reading level. Pruning TOCs to use categories that consumers favor further increases relevancy and correctness while reducing structural complexity...|$|R
40|$|In this paper, we {{demonstrate}} a novel landmark photo search and browsing system, Agate, which ranks landmark image search results considering their relevance, diversity and quality. Agate learns from community photos the most interested aspects and related activities of a landmark, and generates adaptively a <b>Table</b> <b>of</b> <b>Content</b> (<b>TOC)</b> as {{a summary of}} the attractions to facilitate user browsing. Image search results are thus re-ranked with the TOC so as to ensure a quick overview of the attractions of the landmarks. A novel non-parametric TOC generation and re-ranking algorithm, MoM-DPM Sets, is proposed as the key technology of Agate. Experimental results based on human evaluation show the effectiveness of our model and user preference for Agate. Categories and Subject Descriptor...|$|R
40|$|While {{extensive}} {{standards have}} been developed for the representation of objects within complex multimedia documents, much less {{attention has been paid to}} the usability of these documents. Key features of electronic text-document browsers are identified and the ways these might be applied to multimedia documents are discussed. Some of these features include <b>Tables</b> <b>of</b> <b>Contents</b> (<b>TOCs),</b> linking, and indexing. Several novel interfaces for digitized multimedia lectures demonstrate how TOCs can provide access to high-level multimedia structures. 1 INTRODUCTION 1. 1 Multimedia Documents Documents may be distinguished from collections of loosely-linked information objects in having a high-level organization. A logical high-level document structure offers many cognitive advantages for the reader as compared to presentation of disjoint information objects. Ideas organized by an author's understanding may be easy for a reader to comprehend. In addition, the author's conceptual structure may be ac [...] ...|$|R
50|$|The data {{structure}} and operation of a MiniDisc {{is similar to that}} of a computer's hard disk drive. The bulk of the disc contains data pertaining to the music itself, and a small section contains the <b>table</b> <b>of</b> <b>contents</b> (<b>TOC),</b> providing the playback device with vital information about the number and location of tracks on the disc. Tracks and discs can be named. Tracks may easily be added, erased, combined and divided, and their preferred order of playback modified. Erased tracks are not actually erased at the time, but are marked so. When a disc becomes full, the recorder can simply slot track data into sections where erased tracks reside. This can lead to some fragmentation but unless many erasures and replacements are performed, the only likely problem is excessive searching, reducing battery life.|$|E
50|$|The lead-in's subcode {{contains}} repeated {{copies of}} the disc's <b>Table</b> <b>Of</b> <b>Contents</b> (<b>TOC),</b> which provides an index of the start positions of the tracks in the program area and lead-out. The track positions are referenced by absolute timecode, relative {{to the start of}} the program area, in MSF format: minutes, seconds, and fractional seconds called frames. Each timecode frame is one seventy-fifth of a second, and corresponds to a block of 98 channel-data frames—ultimately, a block of 588 pairs of left and right audio samples. Timecode contained in the subchannel data allows the reading device to locate the region of the disc that corresponds to the timecode in the TOC. The TOC on discs is analogous to the partition table on hard drives. Nonstandard or corrupted TOC records are abused as a form of CD/DVD copy protection, in e.g. the key2Audio scheme.|$|E
50|$|The {{possible}} {{reason for not}} using a centralized location of information is that tar was originally meant for tapes, which are bad at random access anyway: if the <b>Table</b> <b>Of</b> <b>Contents</b> (<b>TOC)</b> were {{at the start of}} the archive, creating it would mean to first calculate all the positions of all files, which needs doubled work, a big cache, or rewinding the tape after writing everything to write the TOC. On the other hand, if the TOC were at the end-of-file (as is the case with ZIP files, for example), reading the TOC would require that the tape be wound to the end, also taking up time and degrading the tape by excessive wear and tear. Compression further complicates matters; as calculating compressed positions for a TOC at the start would need compression of everything before writing the TOC, a TOC with uncompressed positions is not really useful (since one has to decompress everything anyway to get the right positions) and decompressing a TOC at the end of the file might require decompressing the whole file anyway, too.|$|E
50|$|All of this info {{is stored}} in the <b>TOC</b> (<b>table</b> <b>of</b> <b>contents).</b> The <b>TOC</b> {{is stored in}} a human-readable text-format and can be downloaded, changed with a text editor and re-uploaded to the PJB again. A copy of the TOC is always stored on the unit as well, so errors and damage to the {{original}} TOC can usually be fixed.|$|R
40|$|One of {{the many}} ways to meet the {{challenge}} of offering users a better search experience—and to continue the library 2 ̆ 7 s existence as a “growing organism” (Ranganathan) —in the current information atmosphere is to provide more text to search against, just as Google 2 ̆ 7 s Book Search does. One method for achieving this is to add <b>tables</b> <b>of</b> <b>contents</b> (<b>TOC)</b> and summary notes into a catalog 2 ̆ 7 s bibliographic records for books, thereby offering additional, highly relevant search terms into a library 2 ̆ 7 s database. While adding access points is not a new notion, libraries can start to meet users 2 ̆ 7 expectations by providing them with more information about the books included in library catalogs. This paper explains how the addition <b>of</b> <b>tables</b> <b>of</b> <b>contents</b> (and some summary notes) to a library consortium 2 ̆ 7 s local catalog provided positive results, especially when weighed against the costs of incorporating them into the catalog...|$|R
40|$|A {{labeling}} {{approach to}} automatic recognition <b>of</b> <b>tables</b> <b>of</b> <b>contents</b> (<b>TOC)</b> s is described. A prototype {{is used for}} consulting electronically scientific papers in a digital library system named Calliope. This method operates on an a roughly structured ASCII file, produced with OCR [...] Labeling {{is based on a}} part of speech (POS) tagging. Tagging is initiated by a primary labeling of text component using some specific dictionaries. Significant tags are then grouped in title and author strings and reduced in canonical forms according to contextual rules. Non labeled tokens are integrated in one or another field per either applying contextual correction rules or using a structure model generated from well detected articles. The designed prototype operates with a great satisfaction on different TOC layouts and character recognition qualities. Without manual intervention, 95. 41 % rate of correct segmentation was obtained on 38 journals including 2703 articles and 81. 74 % rate of correct field extraction. 1...|$|R
40|$|Text (Electronic {{database}}) This database provides {{access to}} the <b>table</b> <b>of</b> <b>contents</b> (<b>TOC)</b> of books and pamphlets housed in the Hong Kong Collection, the University of Hong Kong Libraries. It enables users to identify individual papers presented at conferences held in Hong Kong and book chapters of Hong Kong-related publications, regardless of their publishing place and subject areapublished_or_final_versio...|$|E
40|$|The {{purpose of}} this paper is to present the third of three inter-related {{experiments}} investigating the use and usability of e-books in Higher Education based on experiments conducted at the University of Strathclyde. This study has looked in greater detail at user interactions with e-books for reference purposes by focusing on searching and browsing tasks using three search tools: back-of the-book index (BoBI), <b>table</b> <b>of</b> <b>contents</b> (<b>ToC)</b> and full text search (FTS) ...|$|E
40|$|These {{guidelines}} were taken with modifications {{from those of}} the MIT Junior Laboratory. web. mit. edu/ 8. 13 /www/Notebooks. pdf Notebook 1. The notebook should have numbered 8. 5 × 11 pages and a cloth binding. The composition-size notebooks are too small. Don’t get the kind with duplicate pages or spiral bindings. 2. Put your name, address, email address, and phone number {{at the top of the}} first page and label an area below for a <b>table</b> <b>of</b> <b>contents</b> (<b>TOC).</b> This notebook TOC should simply list the experiment name followed by the notebook page where entrie...|$|E
40|$|This study {{describes}} a novel design <b>of</b> Java <b>table</b> browser using XML(jTBX). A standard table-browsing environment {{is provided in}} the form of Java applet which can be opened in any web browser such as Netscape, or Internet Explore with Java 1. 2 plug-in installed. Some manipulation functions to tables are supported for various level of objects in table(table, sub-table, column, row, cell). A hierarchical Java tree provides the <b>table</b> <b>of</b> <b>content</b> (<b>TOC)</b> <b>of</b> available <b>tables.</b> Three tier architecture is used in the jTBX system design. Remote database tier provides raw data from distributed sites. Web server tier generates the response in the standard XML format to the requests from the client side tier (table browser). Metadata for tables are integrated into the XML files (or streams) before being used by client Java applet. Multiple threads are generated for a large table transporting. Headings: Technical Tools and Concept – Java table browser using XML System Structure – Three tier web structure Data transporting method—Multiple Java thread...|$|R
40|$|Article dans revue scientifique avec comité de lecture. A {{labeling}} {{approach for}} automatic recognition <b>of</b> <b>Tables</b> <b>of</b> <b>Contents</b> (<b>ToC)</b> {{is described in}} this paper. A prototype is used for electronic consulting of scientific papers in a digital library system named Calliope. This method operates on a roughly structured ASCII file, produced by OCR. The recognition approach operates by text labeling without using any a priori model. Labeling {{is based on a}} Part of Speech Tagging (PoS) which is initiated by a primary labeling of text component using some specific dictionaries. Significant tags are first grouped in homogeneous classes according to their grammar categories and then reduced in canonical forms corresponding to article fields: "title" and "authors". Non labeled tokens are integrated in one or another field by either applying PoS correction rules or using a structure model generated from well detected articles. The designed prototype operates with a great satisfaction on different ToC layouts and character recognition qualities. Without manual intervention, 96. 3 % rate of correct segmentation was obtained on 38 journals including 2020 articles and 93. 0 % rate of correct field extraction...|$|R
40|$|Nowadays, {{ontology}} {{is common}} in many areas like artificial intelligence, bioinformatics, e-commerce, education and many more. Ontology {{is one of the}} focus areas in the field of Information Retrieval. The purpose of an ontology is to describe a conceptual representation of concepts and their relationships within a particular domain. In other words, ontology provides a common vocabulary for anyone who needs to share information in the domain. There are several ontology domains in various fields including engineering and non-engineering knowledge. However, {{there are only a few}} available ontology for engineering knowledge. Fuzzy logic as engineering knowledge is still not available as ontology domain. In general, fuzzy logic requires step-by-step guidelines and instructions of lab experiments. In this study, we presented domain ontology for Fuzzy Logic Control (FLC) knowledge. We give <b>Table</b> <b>of</b> <b>Content</b> (<b>ToC)</b> with middle strategy based on the Uschold and King method to develop FLC ontology. The proposed framework is developed using Protégé as the ontology tool. The Protégé’s ontology reasoner, known as the Pellet reasoner is then used to validate the presented framework. The presented framework offers better performance based on consistency and classification parameter index. In general, this ontology can provide a platform to anyone who needs to understand FLC knowledge...|$|R
40|$|<b>Table</b> <b>of</b> <b>contents</b> (<b>TOC)</b> {{recognition}} {{has attracted}} {{a great deal of}} attention in recent years. After reviewing the merits and drawbacks of the existing TOC recognition methods, we have observed that book documents are multi-page documents with intrinsic local format consistency. Based on this finding we introduce an automatic TOC analysis method through clustering. This method first detects the decorative elements in TOC pages. Then it learns a layout model used in the TOC pages through clustering. Finally, it generates TOC entries and extracts their hierarchical structure under the guidance of the model. More specifically, broken lines are taken into account in the method. Experimental results show that this method achieves high accuracy and efficiency. In addition, this method has been successfully applied in a commercial E-book production software package. 1...|$|E
40|$|The Bureau of the Census is {{developing}} a Statistical Metadata Content Standard to define the necessary metadata to describe all aspects of survey design, processing, analysis, and data sets. The draft standards document must be easily reviewed by subject matter experts. Our experience has shown that information displayed in the format of a standard is not easily understood by experts outside the standards community. In order to facilitate review and discussion, we chose to display the standard in a format similar to a textbook's <b>Table</b> <b>of</b> <b>Contents</b> (<b>TOC),</b> with subsequent information displayed in an outline format. To further facilitate review and comment, the TOC was published in HTML format on the Bureau's World Wide Web server. Thus a reviewer, using internet browsers such as Mosaic or Netscape, can traverse the document, display context sensitive help, such as definitions, and leave behind comments as he/she scrutinizes the document. In addition to displaying the standard, the TOC pro [...] ...|$|E
40|$|A {{research}} {{survey was}} undertaken {{to study the}} ‘Scientists’ Purpose of Visiting E‐Journal Sites’. The geographic boundary of this research study consists of 16 prominent aerospace organizations of Bangalore. The age‐group {{of this study is}} between 21 ‐ 60 years. The occupation category of this sample population has been broadly classified into, (a) Scientific / R&D, (b) Armed Forces, (c) Teaching, and (d) Manager. The broad areas of specialization of the Aerospace Scientists and Engineers have been classified into (a) Thermal and Fluid Sciences, (b) Avionics, Guidance and Control, (c) Aerospace Structures and Allied Mechanical Sciences, (d) Materials and Metallurgy, (e) Flight Operations and other Allied Disciplines, and (f) General Engineering and Support Sciences. The major conclusions that {{the authors would like to}} highlight in this study are: Analysis of Variance (ANOVA) was applied for testing the significant difference among the 16 mean scores attained from the scientists and engineers of the aerospace organizations for the ‘Purpose of visit to e‐journal sites’. It is observed that all the 16 aerospace organizations show a significant difference (P < 0. 05) in their mean scores viz., ‘Search bibliographic information’, ‘Browse the <b>table</b> <b>of</b> <b>contents</b> (<b>TOC)</b> ’, ‘Browse only Abstracts’, ‘Access the full text of articles’, ‘For gathering research information’, ‘Facilitate teaching’, Keep updated in your subject’, ‘Writing proposals, reports, thesis etc. ’, ‘Prepare for technical presentations’...|$|E
40|$|Institutions and {{researchers}} stand {{to benefit from}} the facilitation of more widespread syndication of, and easier access to, <b>Table</b> <b>of</b> <b>Content</b> (<b>TOC)</b> RSS (Really Simple Syndication [1]) feeds produced for scholarly journals. However, many journal TOC RSS feeds are at present being produced with erroneous, poor or incomplete metadata. This can hamper the usefulness of scholarly current awareness services, and also cause problems for individual subscribers to those feeds. This is exactly what the ticTOCron software toolkit aims to overcome. The ticTOCron toolkit automatically enhances poor, heterogeneous and incomplete metadata found in TOC RSS feeds by making use of a pre-defined "Best Practice" metadata scheme suitable for scholarly journals. In this work we depict the main issues and "bad practices" found in TOC RSS metadata obtained from more than 435 scholarly publishers. Then, we describe software solutions implemented via ticTOCron. Some references are made to the algorithms for generating semantic relations within, between and from the harvested TOCs and to the mechanisms for propagating "metadata associations" from a previously crawled metadata-rich reference set. However, an effort is made to avoid technical jargon and to replace complex technical descriptions with samples and simple comparisons. The original metadata is converted to a canonical format using the "Best Practices metadata set" for scholarly papers proposed by the ticTOCs Project [2]. We also present the results produced by ticTOCron when it was used for enhancing and normalizing TOC RSS feeds collected from more than 12, 000 journals. Finally we propose a sustainable and scalable computational model whereby the automatic solution is complemented and fine-tuned by a cost-effective human cross-validation process...|$|R
40|$|Paper {{submitted}} for the 30 th IATUL Annual Conference, Leuven 1 - 4 June 2009. Institutions and researchers stand {{to benefit from}} the facilitation of more widespread syndication of, and easier access to, <b>Table</b> <b>of</b> <b>Content</b> (<b>TOC)</b> RSS (Really Simple Syndication [1]) feeds produced for scholarly journals. However, many journal TOC RSS feeds are at present being produced with erroneous, poor or incomplete metadata. This can hamper the usefulness of scholarly current awareness services, and also cause problems for individual subscribers to those feeds. This is exactly what the ticTOCron software toolkit aims to overcome. The ticTOCron toolkit automatically enhances poor, heterogeneous and incomplete metadata found in TOC RSS feeds by making use of a pre-defined "Best Practice" metadata scheme suitable for scholarly journals. In this work we depict the main issues and "bad practices" found in TOC RSS metadata obtained from more than 435 scholarly publishers. Then, we describe software solutions implemented via ticTOCron. Some references are made to the algorithms for generating semantic relations within, between and from the harvested TOCs and to the mechanisms for propagating "metadata associations" from a previously crawled metadata-rich reference set. However, an effort is made to avoid technical jargon and to replace complex technical descriptions with samples and simple comparisons. The original metadata is converted to a canonical format using the "Best Practices metadata set" for scholarly papers proposed by the ticTOCs Project [2]. We also present the results produced by ticTOCron when it was used for enhancing and normalizing TOC RSS feeds collected from more than 12, 000 journals. Finally we propose a sustainable and scalable computational model whereby the automatic solution is complemented and fine-tuned by a cost-effective human cross-validation process...|$|R
40|$|AbstractThe {{need for}} rapid access to {{information}} to support critical decisions in public health cannot be disputed; however, development of such systems requires {{an understanding of the}} actual information needs of public health professionals. This paper reports the results of a literature review focused on the information needs of public health professionals. The authors reviewed the public health literature to answer the following questions: (1) What are the information needs of public health professionals? (2) In what ways are those needs being met? (3) What are the barriers to meeting those needs? (4) What {{is the role of the}} Internet in meeting information needs? The review was undertaken in order to develop system requirements to inform the design and development of an interactive digital knowledge management system. The goal of the system is to support the collection, management, and retrieval of public health documents, data, learning objects, and tools. Method:The search method extended beyond traditional information resources, such as bibliographic databases, <b>tables</b> <b>of</b> <b>contents</b> (<b>TOC),</b> and bibliographies, to include information resources public health practitioners routinely use or have need to use—for example, grey literature, government reports, Internet-based publications, and meeting abstracts. Results:Although few formal studies of information needs and information-seeking behaviors of public health professionals have been reported, the literature consistently indicated a critical need for comprehensive, coordinated, and accessible information to meet the needs of the public health workforce. Major barriers to information access include time, resource reliability, trustworthiness/credibility of information, and “information overload”. Conclusions:Utilizing a novel search method that included the diversity of information resources public health practitioners use, has produced a richer and more useful picture of the information needs of the public health workforce than other literature reviews. There is a critical need for public health digital knowledge management systems designed to reflect the diversity of public health activities, to enable human communications, and to provide multiple access points to critical information resources. Public health librarians and other information specialists can serve a significant role in helping public health professionals meet their information needs through the development of evidence-based decision support systems, human-mediated expert searching and training in the use information retrieval systems...|$|R
