699|1050|Public
2500|$|The {{immediate}} predecessors of MP3 were [...] "Optimum Coding in the Frequency Domain" [...] (OCF), and Perceptual <b>Transform</b> <b>Coding</b> (PXFM). These two codecs, {{along with}} block-switching contributions from Thomson-Brandt, were merged into a codec called ASPEC, which was submitted to MPEG, and {{which won the}} quality competition, but that was mistakenly rejected as too complex to implement. The first practical implementation of an audio perceptual coder (OCF) in hardware (Krasner's hardware was too cumbersome and slow for practical use), was an implementation of a psychoacoustic transform coder based on Motorola 56000 DSP chips.|$|E
2500|$|During the {{development}} of the MUSICAM encoding software, Stoll and Dehery's team made a thorough use of a set of high quality audio assessment material selected by a group of audio professionals from the European Broadcasting Union and later used as a reference for the assessment of music compression codecs [...] [...] The subband coding technique was found to be efficient, not only for the perceptual coding of the high quality sound materials but especially for the encoding of critical percussive sound materials (drums, triangle, [...].) due to the specific temporal masking effect of the MUSICAM sub-band filterbank (this advantage being a specific feature of short <b>transform</b> <b>coding</b> techniques).|$|E
5000|$|... #Subtitle level 3: Optivision and its <b>transform</b> <b>coding</b> {{products}} ...|$|E
40|$|In [1], Effros and Chou {{introduce}} a two-stage universal <b>transform</b> <b>code</b> called the weighted universal <b>transform</b> <b>code</b> (WUTC). By replacing JPEG's single, non-optimal <b>transform</b> <b>code</b> with {{a collection of}} optimal <b>transform</b> <b>codes,</b> the WUTC achieves significant performance gains over JPEG. The computational and storage costs of that performance gain are effectively the computation and storage required to operate and store a collection of <b>transform</b> <b>codes</b> rather than a single <b>transform</b> <b>code.</b> We here consider two complexity- and storageconstrained variations of the WUTC. The complexity and storage of the algorithm are controlled by constraining the order of the bases. In the first algorithm, called a fast WUTC (FWUTC), complexity is controlled by controlling the maximum order of each transform. On a sequence of combined text and gray-scale images, the FWUTC achieves performance comparable to the WUTC at 1 / 32 the complexity for rates up to about 0. 10 bits per pixel (bpp), 1 / 16 the complexity for [...] ...|$|R
40|$|In {{traditional}} concolic testing branch {{coverage is}} low. Automated technique {{appears as a}} promising technique to reduce test time and eort. In this project we used a code transformation technique. We take input a simple java program and transform it using various algorithms. We have used four algorithms to <b>transform</b> the given <b>code</b> into a <b>transformed</b> <b>code.</b> We used Quine Mc-cluskey method and Petric methods to achieve the <b>transformed</b> <b>code.</b> After <b>transforming</b> the <b>code,</b> we pass it through a tool called Cobertura which gives the branch coverage of that <b>transformed</b> <b>code.</b> Here we observe {{that the percentage of}} branch coverage using the <b>transformed</b> <b>code</b> is greater than the coverage of original code. Hence, our technique helps to achieve a significant increase in branch coverage in comparison to traditional techniques...|$|R
3000|$|... [...]. The {{possible}} coverage {{types are}} [...] "R" [...] (reaching), [...] "T" [...] (true-evaluation) and [...] "F" [...] (false-evaluation). A dotted line between elements {{from the original}} and the <b>transformed</b> <b>code</b> denotes that the two elements have the same coverage. A dotted arrow from {{an element of the}} original code to an element of the <b>transformed</b> <b>code</b> denotes that a coverage of the original node by the concrete test data implies also the coverage of the element in the <b>transformed</b> <b>code.</b> Structural code coverage is achieved, if all elements of the <b>transformed</b> <b>code</b> have a connection to an element of the original code. Elements of the transformed graph, for which coverage is not preserved, are marked by a surrounding box.|$|R
50|$|Notable codecs {{produced}} by Nellymoser include the Asao Codec used in Adobe Flash, speech codec used in Microsoft's Xbox Live. Speech codecs originated from MIT Lincoln Laboratory and {{were based on}} McAulay and Quatieri's sinusoidal <b>transform</b> <b>coding</b> model. The Asao product family is based on Nellymoser's proprietary Scalable Projective <b>Transform</b> <b>Coding</b> technology (SPTC).|$|E
50|$|ASPEC {{was itself}} based on Multiple {{adaptive}} Spectral audio Coding (MSC) by E. F. Schroeder, Optimum Coding in the Frequency domain (OCF) the doctoral thesis by Karlheinz Brandenburg at the University of Erlangen-Nuremberg, Perceptual <b>Transform</b> <b>Coding</b> (PXFM) by J. D. Johnston at AT&T Bell Labs, and <b>Transform</b> <b>coding</b> of audio signals by Y. Mahieux and J. Petit at Institut f√ºr Rundfunktechnik (IRT/CNET).|$|E
50|$|Extended Adaptive Multi-Rate - Wideband (AMR-WB+) is {{an audio}} codec that extends AMR-WB. It adds support for stereo signals and higher {{sampling}} rates. Another main improvement {{is the use}} of <b>transform</b> <b>coding</b> (transform coded excitation - TCX) additionally to ACELP. This greatly improves the generic audio coding. Automatic switching between <b>transform</b> <b>coding</b> and ACELP provides both good speech and audio quality with moderate bit rates.|$|E
40|$|Effros and Chou (see Proceedings of the IEEE International Conference on Image Processing, Washington, DC, 1995) {{introduce}} a two-stage universal <b>transform</b> <b>code</b> called the weighted universal <b>transform</b> <b>code</b> (WUTC). By replacing JPEG's single, non-optimal <b>transform</b> <b>code</b> with {{a collection of}} optimal <b>transform</b> <b>codes,</b> the WUTC achieves significant performance gains over JPEG. The computational and storage costs of that performance gain are effectively the computation and storage required to operate and store a collection of <b>transform</b> <b>codes</b> rather than a single <b>transform</b> <b>code.</b> We consider two complexity- and storage-constrained variations of the WUTC. The complexity and storage of the algorithm are controlled by constraining the order of the bases. In the first algorithm, called the fast WUTC (FWUTC), complexity is controlled by controlling the maximum order of each transform. On a sequence of combined text and gray-scale images, the FWUTC achieves performance comparable to the WUTC. In the second algorithm, called the jointly optimized fast WUTC (JWUTC), the complexity is controlled by controlling the average order of the transforms. On the same data set and for the same complexity, the performance of the JWUTC always exceeds the performance of the FWUTC. The JWUTC and FWUTC algorithm are interesting both for their complexity and storage savings in data compression and for the insights that they lend into the choice of appropriate fixed- and variable-order bases for image representation...|$|R
40|$|Symbolic {{debugging}} of <b>transformed</b> <b>code</b> requires {{information about}} the impact of applying transformations on statement instances so that the appropriate values can be displayed to a user. We present a technique to automatically identify statement instance correspondences between untransformed and <b>transformed</b> <b>code</b> and generate mappings reflecting these correspondences as code improving transformations are applied. The mappings support classical optimizations as well as loop transformations. Establishing mappings requires analyzing how the position, number, and order of instances of a statement can change in a particular context when transformations are applied. In addition to enabling symbolic debugging of <b>transformed</b> <b>code,</b> these mappings can be used to understand <b>transformed</b> <b>code</b> and to compare values computed in both program versions either manually or automatically. 1. Introduction Compilers apply code improving transformations to achieve high performance for various types of computi [...] ...|$|R
40|$|International audienceThis paper {{describes}} {{the construction of}} second generation bandelet orthogonal bases. The decomposition on a bandelet basis is computed using a wavelet filter bank followed by adaptive geometric orthogonal filters, that require O(N) operations. The resulting geometry is multiscale and calculated with a fast procedure that minimizes a Lagrangian cost at each scale. Image compression with the resulting bandelet <b>transform</b> <b>code</b> gives significantly better results than a wavelet <b>transform</b> <b>code...</b>|$|R
50|$|While at MIT Hinman and Bernstein were {{motivated}} by the video compression work by UC Davis Professor Anil K. Jain (1946-1988) and his colleague Jaswani R. Jain who published an important research paper combining block-based motion compensation and <b>transform</b> <b>coding</b> in December 1981. The result was PictureTel, creating {{one of the first}} real-time systems to implement motion compensation and <b>transform</b> <b>coding</b> in July 1986.|$|E
50|$|VP9 is a {{traditional}} block-based <b>transform</b> <b>coding</b> format. The bitstream format is relatively simple compared to formats that offer similar bitrate efficiency like HEVC.|$|E
50|$|The {{basic process}} of {{digitizing}} an analog signal {{is a kind}} of <b>transform</b> <b>coding</b> that uses sampling in one or more domains as its transform.|$|E
3000|$|... in {{the sense}} that the Voronoi shape of the <b>transformed</b> <b>code</b> is not matched to the {{distortion}} function [...]...|$|R
30|$|In the formula, ICensus(u,[*]v) is the Census <b>transform</b> <b>code</b> of {{the central}} pixel point and ‚äó {{represents}} the bit-bit connection.|$|R
3000|$|... [...]. The {{explicit}} {{solution of}} this minimization problem is unfortunately intractable {{due to the}} inter-dependence of the three <b>transform</b> <b>codes</b> involved.|$|R
50|$|While at University of California, Davis, Jain co-founded Optivision, Inc. in {{the mid-1980s}} with Professor Joseph Goodman from Stanford. Optivision pioneered both JPEG <b>transform</b> <b>coding</b> {{products}} for picture capture systems such as for the California Department of Motor Vehicles, and video compression systems such as videoconferencing that used the block-based motion-compensated <b>transform</b> <b>coding</b> techniques he developed. Jain's work also inspired other video industry entrepreneurs such as Brian Hinman, co-founder of PictureTel, Polycom, and 2Wire. Optivision later, after Jain's untimely death, had {{an initial public offering}} primarily thanks to the Optivision optical switching technology.|$|E
50|$|A {{possible}} bitstream {{representation of}} a macroblock in a video codec which uses motion compensation and <b>transform</b> <b>coding</b> is given below. It {{is similar to the}} format used in H.261.|$|E
50|$|Subsequently, most of {{the video}} {{compression}} standards for two-way communications and video broadcast applications have been based upon motion compensation and <b>transform</b> <b>coding,</b> including those most widely used today such as H.264/MPEG-4 AVC.|$|E
40|$|In [l], Effros and Chou {{introduce}} a two-stage universal <b>transform</b> <b>code</b> called the weighted universal <b>transform</b> <b>code</b> (WUTC). By replacing JPEG‚Äôs single, non-optimal <b>transform</b> <b>code</b> with {{a collection of}} optimal <b>transform</b> <b>codes,</b> the WUTC achieves significant performance gains over JPEG. The computational and storage costs of that performance gain are effectively the computation and storage required to operate and store a collection of <b>transform</b> <b>codes</b> rather than a single <b>transform</b> <b>code.</b> We here consider two complexity- and storageconstrained variations of the WUTC. The complexity and storage of the algorithm are controlled by constraining the order of the bases. In the first algorithm, called a fast WTC (FWUTC), complexity is controlled by controlling the maximum order of each transform. On a sequence of combined text and gray-scale images, the FWUTC achieves performance comparable to the WUTC at 1 / 32 the complexity for rates up to about 0. 10 bits per pixel (bpp), 1 / 16 the complexity for rates up to about 0. 15 bpp, 1 / 8 the complexity for rates up to about 0. 20 bpp, and 1 / 4 the complexity for rates up to about 0. 40 bpp. In the second algorithm, called a jointly optimized fast WUTC (JWUTC), the complexity is controlled by controlling the average order of the transforms. On the same data set and for the same complexity, the performance of the JWUTC always exceeds the performance of the FWUTC. On the data set considered, the performance of the JWUTC is, at each rate, virtually indistinguishable from that of the WUTC at 1 / 8 the complexity. The JWUTC and FWUTC algorithm are interesting both for their complexity and storage savings in data compression and for the insights that they lend into the choice of appropriate fixed- and variable-order bases for image representation. 1068 - 0314 / 97 $ 10. 00 1997 IEEE 2112 12...|$|R
3000|$|... and B 2, so {{that the}} number of bits {{allocated}} to each <b>transform</b> <b>code</b> is fixed and it is only required to optimize the bit allocation among the quantizers within each <b>transform</b> <b>code.</b> For simplicity, we refer to this problem as the constrained bit-allocation problem. In the following, an explicit solution to this problem is derived. Based on the result, we then present a tree-search algorithm to solve the unconstrained problem (8). Under both RD-WZQ and SWC-HRSQ models, the optimal transforms for Gaussian sources are CKLTs. Therefore, we refer to the solution to problem (8) as the SP-DKLT.|$|R
40|$|Abstract‚ÄîThis paper {{describes}} <b>transform</b> coefficient <b>coding</b> in {{the draft}} international standard of High Efficiency Video Coding (HEVC) specification and the driving motivations behind its design. <b>Transform</b> coefficient <b>coding</b> in HEVC encompasses the scanning patterns and coding methods for the last significant coefficient, significance map, coefficient levels, and sign data. Special {{attention is paid to}} the new methods of last significant coefficient coding, multilevel significance maps, high-throughput binarization, and sign data hiding. Experimental results are provided to evaluate the performance of <b>transform</b> coefficient <b>coding</b> in HEVC. Index Terms‚ÄîHigh Efficiency Video Coding (HEVC), high throughput entropy coder, <b>transform</b> coefficient <b>coding,</b> video coding. I...|$|R
50|$|Anil K. Jain was a {{contributor}} to the field of motion video compression. With his colleague Jaswant R. Jain, Anil published the original paper combining block-based motion compensation and <b>transform</b> <b>coding</b> in December 1981.|$|E
50|$|Lapped {{transforms}} substantially {{reduce the}} blocking artifacts that otherwise occur with block <b>transform</b> <b>coding</b> techniques, in particular those using the discrete cosine transform. The best known {{example is the}} modified discrete cosine transform used in the MP3, Vorbis, AAC, and Opus audio codecs.|$|E
50|$|The {{usage of}} video codecs based on vector {{quantization}} has declined significantly {{in favor of}} those based on motion compensated prediction combined with <b>transform</b> <b>coding,</b> e.g. those defined in MPEG standards, as the low decoding complexity of vector quantization has become less relevant.|$|E
40|$|We {{introduce}} a general framework for end-to-end optimization {{of the rate}} [...] distortion performance of nonlinear <b>transform</b> <b>codes</b> assuming scalar quantization. The framework {{can be used to}} optimize any differentiable pair of analysis and synthesis transforms in combination with any differentiable perceptual metric. As an example, we consider a code built from a linear transform followed by a form of multi-dimensional local gain control. Distortion is measured with a state-of-the-art perceptual metric. When optimized over a large database of images, this representation offers substantial improvements in bitrate and perceptual appearance over fixed (DCT) codes, and over linear <b>transform</b> <b>codes</b> optimized for mean squared error. Comment: Accepted as a conference contribution to Picture Coding Symposium 201...|$|R
40|$|This paper {{presents}} a new multiple description method for wavelet <b>transform</b> <b>coded</b> images. This method combines partitioning with concealment to produce descriptions. An optimal concealment method for wavelet <b>transform</b> <b>coded</b> images {{for a given}} partitioning is derived based on the statistical characteristics in wavelet domain. Concealment in LL subband is treated differently {{and is based on}} its smoothness property. An optimal partitioning method is derived corresponding to the concealment. We have designed a coder for wireless image transmission, which emphasizes the need for source and channel coding interaction based on residual redundancy. Optimal channel rate allocation is derived depending upon source significance and channel status information. The method is computationally inexpensive and does not sacrifice compression efficiency. Simulations for wireless image transmission have shown promising results...|$|R
40|$|We present Kovalenko‚Äôs Full-Rank Limit as a tight {{probabilistic}} lower-bound for Error-Performances of Low-Density Parity-Check <b>codes</b> and Luby <b>Transform</b> <b>codes</b> over Binary Erasure Channels. Associated {{with the}} limit, we derive its Full-Rank Overhead as a tight lower-bound for Stable Overheads for successful Maximum-Likelihood Decoding of the codes...|$|R
50|$|Subsequently, most of {{the video}} {{compression}} standards for two-way communications and video broadcast applications were based upon motion compensation and <b>transform</b> <b>coding,</b> including those most widely used today such as MPEG-1, MPEG-2 (used on DVDs) and the most common Internet video H.264/MPEG-4 AVC.|$|E
50|$|Technically speaking, a {{compression}} artifact {{is a particular}} class of data error that is usually the consequence of quantization in lossy data compression. Where <b>transform</b> <b>coding</b> is used, they typically assume the form {{of one of the}} basis functions of the coder's transform space.|$|E
50|$|In signal processing, {{sub-band}} coding (SBC) is {{any form of}} <b>transform</b> <b>coding</b> that breaks a signal {{into a number of}} different frequency bands, typically by using a fast Fourier transform, and encodes each one independently. This decomposition is often the first step in data compression for audio and video signals.|$|E
40|$|We {{present the}} {{parallel}} {{implementation of a}} synthetic aperture radar (SAR) processor algorithm based on a particular two-dimensional Fourier <b>transform</b> <b>code.</b> The computer architecture consists of a toroidal net with transputers on each node. The excellent performance and flexibility of the net is discussed and shown {{in a number of}} graphs...|$|R
50|$|This section {{describes}} how compiler <b>transforms</b> Lisp <b>code</b> to C.|$|R
40|$|With the {{widening}} performance gap between processors and main memory, efficient memory accessing behavior {{is necessary for}} good program performance. Both hand-tuning and compiler optimization techniques are often used to <b>transform</b> <b>codes</b> to improve memory performance. Effective transformations require detailed knowledge about the frequency and causes of cache misses in the code...|$|R
