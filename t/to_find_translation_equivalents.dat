1|10000|Public
40|$|A {{monolingual}} Chinese-to-Chinese SMT {{model as}} well as a global optimization strategy are proposed in this paper to extract equivalent Chinese terms (such as“雷射”lae-ser and“激光”gi-guan for“laser”) which are used in different areas of the various Chinese-speaking communities. Preliminary evaluation shows that the synonymous traditional Chinese (TC) terms for simplified Chinese (SC) terms can be identified with an accuracy of 84 % on a small test set. On the other hand, the traditional-to-simplified Chinese term translation achieves 87 % accuracy. Furthermore, the global optimization strategy generally improves the performance by decreasing search errors. The main idea behind the model is to create “paralelleft/right contexts” out of non-parallel web pages for term alignment. The monolingual SMT model, by its very nature <b>to</b> <b>find</b> <b>translation</b> <b>equivalents</b> can potentially be useful for finding synonym sets (synsets) for any generic monolingual lexicon. The potential for adapting such a model for mining large synsets from non-parallel corpora is therefore expectable...|$|E
40|$|In {{this paper}} {{we present a}} tool that uses {{comparable}} corpora <b>to</b> <b>find</b> appropriate <b>translation</b> <b>equivalents</b> for expressions that are considered by translators as difficult. For a phrase in the source language the tool identifies a range of possible expressions used in similar contexts in target language corpora and presents them to the translator as a list of suggestions. In the paper we discuss the method and present results of human evaluation {{of the performance of}} the tool, which highlight its usefulness when dictionary solutions are lacking. ...|$|R
40|$|In {{this article}} we discuss the {{teaching}} of Finnish and Estonian vocabulary to Finnish and Estonian students learning each other's language as a foreign language, {{and what kinds of}} issues should receive most attention. We classify difficult words which we have faced while teaching, and try to fi nd out why it is so difficult <b>to</b> <b>find</b> a good <b>translation</b> <b>equivalent</b> for them...|$|R
40|$|The paper {{deals with}} {{problems}} of legal translation from Polish into Spanish. It analyses selected terms related to contracts which are regulated in the Polish Civil Code and their possible translations into Spanish. In order <b>to</b> <b>find</b> adequate <b>translation</b> <b>equivalents</b> the author applies {{the method of}} parametrisation of legal terms (along with the method of comparing parallel texts and the skopos theory). The parametrisation of legal terms helps to systematically characterise and compare them and thus to identify differences in the meanings of the source language and target language terms and to choose the best equivalents. It may also facilitate {{the selection of a}} technique of providing <b>translation</b> <b>equivalents</b> for non-equivalent or partially equivalent terms. Parametrisation is understood as determining for each analysed term a set of properties it shows with respect to translationally relevant parameters – one property out of each parameter. A parameter is conceived of as a set of homogeneous properties...|$|R
40|$|This paper {{discusses}} {{the history of}} Latin maxims and expressions in English, Estonian and Polish legal languages {{and the influence of}} Latin on contemporary legal language and its translation. A number of maxims and expressions were examined by comparing their present meanings in those three legal systems. A selection of fifteen of them is presented in the article, namely: ab intestato, corpus iuris civilis, error iuris, fraus est celerem fraudem, ignorantia iuris neminem excusat, impossibilium nulla est obligatio, inter arma silent leges, ipso iure, lex non scripta, pacta sunt servanda, qui tacet – consentire videtur, restitutio in integrum,m sub iudice, and summum ius summa iniuria. The term Actio Pauliana which is used in the bankruptcy law is used here to show how translators may use Latin terms in order <b>to</b> <b>find</b> proper <b>translation</b> <b>equivalents.</b> Finally, the authors point out the differences in pronunciation and spelling of Latin maxims and expressions in English, Estonian and Polish...|$|R
40|$|In {{translation}} practice, typological {{differences between}} languages can pose problems {{in such a}} way that the translator has to compensate a source language structure which does not exist in the target language. It is, however, not always easy <b>to</b> <b>find</b> an adequate <b>translation</b> <b>equivalent</b> for such a lacking structure. The aim of this paper is to show how a multiply annotated and aligned corpus can be used as a translation memory for such typologically driven problems, exploiting the linguistic enrichment of the corpus. It is discussed how an existing translation corpus with high-quality translation and alignment is converted into a database and how this database can be exploited as a large on-line resource displaying various translation options for lexico-grammatical problems. 1...|$|R
25|$|This volume by the author's son is the slimmest and is {{difficult}} <b>to</b> <b>find.</b> Partial <b>translation.</b>|$|R
40|$|In recent years, {{state-of-the-art}} cross-linguistic {{systems have}} been based on parallel corpora. Nevertheless, it is difficult at times <b>to</b> <b>find</b> <b>translations</b> of a certain technical term or named entity even with a very large parallel corpora. In this paper, we present a new method for learning <b>to</b> <b>find</b> <b>translations</b> on the Web for a given term. In our approach, we use a small set of terms and translations to obtain mixed-code snippets returned by a search engine. We then automatically annotate the data with translation tags, automatically generate features to augment the tagged data, and automatically train a conditional random fields model for identifying translations. At runtime, we obtain mixed-code webpages containing the given term and run the model to extract translations as output. Preliminary experiments and evaluation results show our method cleanly combines various features, resulting in a system that outperforms previous works...|$|R
5|$|In {{the scenes}} where the Sherpas were speaking, the show staff {{went to great}} lengths <b>to</b> <b>find</b> <b>translations.</b> Originally, the {{producers}} of the film adaption of the book Into Thin Air were contacted to help. The film producers were shocked at the trouble the Simpsons staff were going to, and replied that they had simply made up translations in the film. The staff then had to consult various experts by telephone.|$|R
40|$|We {{present a}} {{statistical}} word feature, the Word Relation Matrix, {{which can be}} used <b>to</b> <b>find</b> translated pairs of words and terms from non-parallel corpora, across language groups. Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures. Word Relation Matrices are then mapped across the corpora <b>to</b> <b>find</b> <b>translation</b> pairs. Translation accuracies are around 30 % when only the top candidate is counted. Nevertheless, top 20 candidate output give a 50. 9 % average increase in accuracy on human translator performance...|$|R
40|$|We {{show that}} unseen words {{account for a}} large part of the {{translation}} error when moving to new domains. Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al., 2008), we are able <b>to</b> <b>find</b> <b>translations</b> for otherwise OOV terms. We show several approaches to integrating such translations into a phrasebased translation system, yielding consistent improvements in translations quality (between 0. 5 and 1. 5 Bleu points) on four domains and two language pairs. ...|$|R
40|$|ABSURDIST II, an {{extension}} to ABSURDIST, is an algorithm using attributed graph matching <b>to</b> <b>find</b> <b>translations</b> between conceptual systems. It uses {{information about the}} internal structure of systems by itself, or in combination with external information about concept similarities across systems. It supports systems with multiple types of weighted or unweighted, directed or undirected relations between concepts. The algorithm exploits graph sparsity to improve computational efficiency. We present the results of experiments {{with a number of}} conceptual systems, including artificially constructed random graphs with introduced distortions...|$|R
40|$|Corpus {{driven machine}} {{translation}} approaches such as Phrase-Based Statistical Machine Translation and Example-Based Machine Translation {{have been successful}} by using word alignment <b>to</b> <b>find</b> <b>translation</b> fragments for matched source parts in a bilingual training corpus. However, they still cannot properly deal with systematic translation for insertion or deletion words between two distant languages. In this work, we used syntactic chunks as translation units to alleviate this problem, improve alignments and show improvement in BLEU for Korean to English and Chinese to English translation tasks. ...|$|R
5000|$|This metric is {{only used}} <b>to</b> <b>find</b> local <b>{{translation}}</b> in Yadics. This metric with translation transform {{can be solved}} using cross-correlation methods, which are non iterative and can be accelerated using Fast Fourier Transform [...]|$|R
40|$|Machine Translation (MT) for low-resource {{language}} has low-coverage issues due to Out-Of-Vocabulary (OOV) Words. In this research we propose a method using sublexical translation to achieve wide-coverage in Example-Based Machine Translation (EBMT) for English to Bangla language. For sublexical translation we divide the OOV words into sublexical units for getting translation candidates. Previous methods without sublexical <b>translation</b> failed <b>to</b> <b>find</b> <b>translation</b> candidate for many joint words. In this research using WordNet and IPA transliteration algorithm we propose to translate OOV words with explanation. The proposed method {{is better than}} previous OOV words handling. Our proposal improved translation quality by 20 points in human evaluation...|$|R
40|$|We {{present a}} set of {{algorithms}} that enable us to translate natural language sentences by exploiting both a translation memory and a statistical-based translation model. Our results show that an automatically derived translation memory can be used within a statistical framework <b>to</b> often <b>find</b> <b>translations</b> of higher probability than those found using solely a statistical model...|$|R
40|$|Abstract — Camera {{parameters}} estimation is {{an important}} issue in machine vision. This paper proposes a new method <b>to</b> <b>find</b> <b>translation</b> and rotation matrix of camera in sport scene on the basis of vanishing points. Vanishing point (VP) of parallel lines is the image of the point at infinity, which corresponds to the projection of the intersection of parallel lines at infinity. According to projective geometry constraint, camera rotation of the projection matrix is computed directly by two vanishing points and pan and tilt of camera extracted from this matrix. Mathematical proof and Computer simulation are carried out to validate our novel method. Keywords- Camera calibration; pose estimation; Rotation matrix; vanishing points. I...|$|R
40|$|Graduation date: 1971 First, topological vector {{spaces are}} {{examined}} from a partial order structure derived from neighborhood bases of the origin. This structure {{is used to}} produce a minimal vector norm for every Hausdorff locally convex space. Then, topological vector spaces are examined <b>to</b> <b>find</b> <b>translation</b> invariant measures with respect to which functions in the topological dual are integrable. It is shown that every conical measure on a locally convex space E has a unique translationally invariant extension to all of a(E), the Riesz space generated by the real valued continuous affine functions on E. Invariant measures are constructed, characterized, and extended. An integral representation on certain complete weak spaces is found...|$|R
40|$|This paper {{presents}} preliminary {{experiments in}} the use of translation equivalences to disambiguate prepositions or case suffixes. The core of the method is <b>to</b> <b>find</b> <b>translations</b> of the occurrence of the target preposition or case suffix, and assign the intersection of their set of interpretations. Given a table with prepositions and their possible interpretations, the method is fully automatic. We have tested this method on the occurrences of the Basque instrumental case -z in the definitions of a Basque dictionary, looking for the translations in the definitions from 3 Spanish and 3 English dictionaries. The results have been that we are able to disambiguate with 94. 5 % accuracy 2. 3 % of those occurrences (up to 91) ...|$|R
40|$|This paper {{introduces}} {{a method for}} learn-ing <b>to</b> <b>find</b> <b>translation</b> of a given source term on the Web. In the approach, the source term {{is used as a}} query and part of patterns to retrieve and extract transla-tions in Web pages. The method involves using a bilingual term list to learn source-target surface patterns. At runtime, the given term is submitted to a search engine then the candidate translations are ex-tracted from the returned summaries and subsequently ranked based on the surface patterns, occurrence counts, and translit-eration knowledge. We present a proto-type called TermMine that applies the method to translate terms. Evaluation on a set of encyclopedia terms shows that the method significantly outperforms the state-of-the-art online machine translation systems. ...|$|R
40|$|Abstract: Camera {{parameters}} estimation is {{an important}} issue in machine vision. This paper proposes a new method <b>to</b> <b>find</b> <b>translation</b> and rotation matrix of camera in sport scene on the basis of vanishing points for the virtual advertisement insertion. Vanishing point (VP) of parallel lines is the image of the point at infinity, which corresponds to the projection of the intersection of parallel lines at infinity. According to projective geometry constraint, camera rotation of the projection matrix is computed directly by two vanishing points, then pan and tilt of camera extracted from this matrix. Computer simulation and real data experiments are carried out to validate our method. Experimental results have shown that this method can {{meet the needs of the}} virtual advertisement insertion and be put into use in real application scenarios...|$|R
40|$|This work {{presents}} tSEARCH, {{a web-based}} application that provides mechanisms for doing complex searches over {{a collection of}} translation cases evaluated with a large set of diverse measures. tSEARCH uses the evaluation results obtained with the ASIYA toolkit for MT evaluation and it is connected to its on-line GUI, which makes possible a graphical visualization and interactive access to the evaluation results. The search engine offers a flexible query language allowing <b>to</b> <b>find</b> <b>translation</b> examples matching a combination of numerical and structural features associated to the calculation of the quality metrics. Its database design permits a fast response time for all queries supported on realistic-size test beds. In summary, tSEARCH, used with ASIYA, offers developers of MT systems and evaluation metrics {{a powerful tool for}} helping translation and error analysis. ...|$|R
40|$|Translation Studies, in its {{foundational}} terms, {{must assume}} multilingualism but has also long presupposed monoglots: whenever our models separate 'source language' from 'target language,' we present translations as border-markers between monolingual spaces. One way forward is <b>to</b> <b>find</b> <b>translation</b> in all communication, within all linguistic spaces, {{to an extent}} that would dissolve any pretense at monolingualism. An alternative solution, more tied to empirical historiography, is to trace {{the ways in which}} multilingual ideological formations use translation in order to divide linguistic spaces {{at the same time as}} they expand across those same spaces. Using numerous local examples, Pym¿s article examines the use of translators and interpreters to bring about multilingual democracies and considers the ways in which current technologies and practices of translation are reshaping answers to the question...|$|R
40|$|A {{new method}} {{to improve the}} lateral {{resolution}} of differential profile measurement is proposed, together with a related profile recovery algorithm. In this method, the lateral resolution will not be constrained to the distance between two differential probes. It can be {{a fraction of the}} distance and is affected by the scanning resolution. In the corresponding algorithm to recover a profile, d is the probes distance, d r is the scanning resolution, and d r = d/N, where N is the number of groups separated from the difference data set. Integration is applied to every group to build a profile. Finally, spline smoothing regression is used to the profiles <b>to</b> <b>find</b> <b>translations</b> between them <b>to</b> obtain the whole profile. Computer stimulation was used to examine the effectiveness of the proposed recovery algorithm and good accuracy was achieved...|$|R
5000|$|As {{concepts}} of both hustle and flow {{are unique to}} African American culture, {{it turned out to}} be nearly impossible <b>to</b> <b>find</b> proper <b>translations</b> for international release of the film. For example, the Russian translation of the title means [...] "The bustle and the motion". The Italian title is appended with [...] "Il colore della musica" [...] which means [...] "The color of music".|$|R
40|$|Out-of-vocabulary (oov) {{words or}} phrases still remain a {{challenge}} in statistical machine translation especially when {{a limited amount of}} parallel text is available for training or when there is a domain shift from training data to test data. In this paper, we propose a novel approach to finding translations for oov words. We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order <b>to</b> <b>find</b> <b>translations</b> for all the source language phrases. Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations. Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics. ...|$|R
40|$|A {{study by}} Rinsche et al. {{on the size}} of the {{language}} industry in the EU shows that the estimated value of this industry within the EU Member States will be 16. 5 billion EUR by 2015, and that translation alone accounts for more than 50 % of this value. It is therefore important to provide good tools for translators to improve their efficiency and/or reduce their costs. At the Computer and Automation Research Institute of the Hungarian Academy of Sciences we started a one-year research project to build a usable algorithm <b>to</b> <b>find</b> <b>translations</b> and plagiarisms within a large foreign language document collection. The resulting tool is able to search for available translations not only within the local files of a company, but also online on a much larger scale. We incorporated this tool into our online KOPI Plagiarism Search Engine in December 2011 where everybody can try our new algorithm...|$|R
40|$|The {{two-dimensional}} translational containment {{problem is}} <b>to</b> <b>find</b> <b>translations</b> for {{a collection of}} polygons which place them inside a polygonal container without overlapping. The polygons and container may be nonconvex. Our linear-programming-based (LP) translational containment algorithm uses our restrict /evaluate/subdivide paradigm which operates on two-dimensional configuration spaces. The following distance-based subdivision problem arises during the subdivision step: given a polygon U and a point t which is outside U but inside the convex hull of U, find the line L through t which partitions U into two pieces U Γ and U + {{on opposite sides of}} L such that: min(Δ(t; CH(U Γ)); Δ(t; CH(U +))) is maximized, where CH(U) is the convex hull of U and Δ(a; B) is the Euclidean distance from a point a to a point set B. We show that if U is connected, then the distance-based subdivision problem can be solved in O(jU j) time in a real arithmetic model and in [...] ...|$|R
40|$|Sensor data {{presents}} {{the input of}} further simulations and {{plays a major role}} in early warning systems. Processing a high amount of data collected from different sensors requires interoperability at different levels. One of the remaining challenges is to overcome semantic heterogeneity during discovery, retrieval and integration of geodata within the open and distributed environments of current Spatial Data Infrastructures (SDIs). Therefore finding and accessing suitable information is the first step. The second step aims at data integration by transforming the data sources according to the requirements of the data sink In the future sensor networks of the data sink will be services for further information processing like simulation and interpolation The crucial task is not the execution of data transformation but rather <b>to</b> <b>find</b> <b>translation</b> rules in a generic way. This paper introduces an architecture for ontology-based translation of geographic information that solves semantic heterogeneity problems of data integration based on a (real-world) scenario from the area of flood management...|$|R
40|$|This paper {{describes}} a method <b>to</b> <b>find</b> phrase-level <b>translation</b> patterns from parallel corpora by applying dependency structure analysis. We use statistical dependency parsers to determine dependency relations between base phrases in a sentence. Our method is tested {{with a business}} expression corpus containing 10000 English Japanese sentence pairs and achieved approximately 90 % accuracy in extracting bilingual correspondences. The result shows {{that the use of}} dependency relation helps to acquire interesting translation patterns...|$|R
6000|$|Follows on my {{list the}} celebrated Murúj el-Dahab, or [...] "Meads of Gold," [...] by El-Mas'údi, who died in A.H. 346 (= A.D. 957), and whose book extends to A.H. 332 (= A.D. 943). Unable <b>to</b> <b>find</b> the <b>translation</b> of my friend Sprenger, I am {{compelled}} to quote from [...] "Maçoudi. Les Prairies d'Or," [...] texte et traduction par C. Barbier de Meynard et Pavet de Courteille. Société Asiatique, Paris, 1864, vol. iii. pp. 301-305.|$|R
40|$|We develop {{admissible}} A * search heuristics for synchronous parsing with Inversion Transduction Grammar, {{and present}} results both for bitext alignment and for machine translation decoding. We also combine the dynamic programming hook trick with A * search for decoding. These techniques {{make it possible}} <b>to</b> <b>find</b> optimal alignments much more quickly, and make it possible <b>to</b> <b>find</b> optimal <b>translations</b> for the first time. Even {{in the presence of}} pruning, we are able to achieve higher BLEU scores with the same amount of computation. ...|$|R
40|$|We {{present a}} near-quadratic time {{algorithm}} that computes a point inside a simple polygon P having approximately the largest visibility polygon inside P, and a nearlinear time algorithm for finding {{the point that}} will have approximately the largest Voronoi region when added to an n-point set. We apply the same technique <b>to</b> <b>find</b> the <b>translation</b> that approximately maximizes the area of intersection of two polygonal regions in near-quadratic time, and the rigid motion doing so in near-cubic time. ...|$|R
40|$|We {{describe}} our {{systems for}} the SemEval 2014 Task 5 : L 2 writing assistant where a system has <b>to</b> <b>find</b> appropriate <b>translations</b> of L 1 segments in a given L 2 context. We participated in {{three out of four}} possible language pairs (English-Spanish, French-English and Dutch-English) and achieved the best performance for all our submit-ted systems according to word-based ac-curacy. Our models are based on phrase-based machine translation systems and combine topical context information and language model scoring. ...|$|R
40|$|One of the {{important}} factors that affects the performance of Cross Language Information Retrieval(CLIR) {{is the quality of}} translations being employed in CLIR. In order {{to improve the quality of}} translations, it is important to exploit available resources efficiently. Employing different translation resources with different characteristics has many challenges. In this paper, we propose a method for exploiting available translation resources simultaneously. This method employs Learning to Rank(LTR) for exploiting different translation resources. To apply LTR methods for query translation, we define different translation relation based features in addition to context based features. We use the contextual information contained in translation resources for extracting context based features. The proposed method uses LTR to construct a translation ranking model based on defined features. The constructed model is used for ranking translation candidates of query words. To evaluate the proposed method we do English-Persian CLIR, in which we employ the <b>translation</b> ranking model <b>to</b> <b>find</b> <b>translations</b> of English queries and employ the translations to retrieve Persian documents. Experimental results show that our approach significantly outperforms single resource based CLIR methods...|$|R
40|$|A good {{decoding}} {{algorithm is}} {{critical to the success of}} any statistical machine translation system. The decoder's job is <b>to</b> <b>find</b> the <b>translation</b> that is most likely according to set of previously learned parameters (and a formula for combining them). Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions. Unfortunately, examining more of the space leads to unacceptably slow decodings. In thi...|$|R
