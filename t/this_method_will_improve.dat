17|10000|Public
40|$|Should {{a species}} be translocated? Uncertainty {{regarding}} the necessity and feasibility of many translocations complicates answering this question. Here, we review translocation projects, both published and unpublished. Our {{results indicate that}} most projects (1) addressed {{fewer than half of}} the basic criteria established for translocations and (2) were either unjustifiable from a conservation perspective or inadequately designed to guarantee success or preclude negative consequences. We propose a hierarchical decision-making system - an explicit method that integrates existing guidelines, thereby covering a key gap in conservation science - to reduce ambiguity when deciding whether to implement a given translocation project. <b>This</b> <b>method</b> <b>will</b> <b>improve</b> the likelihood of success in translocation projects and contribute to the efficient use of the limited resources available for these conservation efforts. Â© The Ecological Society of America. Peer Reviewe...|$|E
40|$|Assembly {{lines are}} flow-line {{production}} systems which are {{of great importance}} in the industrial production of high quantity standardized commodities and more recently even gained importance in low volume production of customized products. The assembly line involves many famous problems that is unbalancing cycle time among the workstations. Simulation Model (SM) {{is one of the}} most useful modelling tool for the design of many types of systems. It can combine with Genetic Algorithm system (GAs) to obtain the optimum solution in any type of issue area. In this study the SM and GAs will be applied to solve the unbalancing problem at the automobile manufacture system. <b>This</b> <b>method</b> <b>will</b> <b>improve</b> the efficiency of the assembly line through reduces the queuing among the workstation, increase the labor of working time then to increase the ratio of the productions...|$|E
40|$|Elevation {{angles of}} {{returned}} backscatter are calculated at SuperDARN radars using interferometric techniques. These elevation angles allow the altitude of the reflection {{point to be}} estimated, an essential piece of information for many ionospheric studies. The elevation angle calculation requires knowledge of the azimuthal return angle. This directional angle is usually assumed to lie along a narrow beam {{from the front of}} the radar, even though the signals are known to return from both in front of and behind the radar. If the wrong direction of return is assumed, large uncertainties will be introduced through the azimuthal return angle. This paper introduces a means of automatically determining the correct direction of arrival and the propagation mode of backscatter. The application of <b>this</b> <b>method</b> <b>will</b> <b>improve</b> the accuracy of backscatter elevation angle data and aid in the interpretation of both ionospheric and ground backscatter observations. Peer-reviewedPublisher Versio...|$|E
40|$|Discrete Wavelet Transform (DWT) is {{most popular}} {{transformation}} technique adopted for image compression. Image compression {{is done to}} increase speed, reduce the memory and transmission bandwidth for the efficient transfer of image. DWT is more famous because it compresses the image with out any data loss. In this paper a lifting based DWT is proposed to improve the latency and through put. This model is successfully simulated using xilinx software which gives correct functionality. Hope <b>this</b> proposed <b>method</b> <b>will</b> <b>improve</b> the latency and through put...|$|R
40|$|We {{demonstrate}} a rapid and inexpensive approach for the fabrication of high resolution poly(dimethylsiloxane) (PDMS) -based microfluidic devices. The complete {{process of fabrication}} could be performed in several hours (or less) without any specialized equipment other than a consumer-grade wax printer. The channels produced by <b>this</b> <b>method</b> are of high enough quality {{that we are able}} to demonstrate the sizing and separation of DNA fragments using capillary electrophoresis (CE) with no apparent loss of resolution over that found with glass chips fabricated by conventional photolithographic methods. We believe that <b>this</b> <b>method</b> <b>will</b> greatly <b>improve</b> the accessibility of rapid prototyping methods. ...|$|R
40|$|Relative {{individual}} {{information is}} a measurement that scores {{the quality of}} DNA- and RNA-binding sites for biological machines. The development of analytical approaches to increase the power of <b>this</b> scoring <b>method</b> <b>will</b> <b>improve</b> its utility in evaluating the functions of motifs. In this study, the scoring method was applied to potential translation initiation sites in Drosophila to compute Translation Relative Individual Information (TRII) scores. The weight matrix {{at the core of}} the scoring method was optimized based on high-confidence translation initiation sites identified by using a progressive partitioning approach. Comparing the distributions of TRII scores for sites of interest with those for high-confidence translation initiation sites and random sequences provides a new methodology for assessing the quality of translation initiation sites. The optimized weight matrices can also be used to describe the consensus at translation initiation sites, providing a quantitative measure of preferred and avoided nucleotides at each position. </p...|$|R
40|$|A highly {{sensitive}} flow analysis manifold for rapid determination of dissolved reactive phosphate was developed which uses ethanol and UV light to reduce phosphomolybdic acid, {{instead of the}} reactive and short-lived chemical reductants typically employed in molybdenum blue chemistry. This reaction is impractical to perform reproducibly in batch mode, yet is very simple to handle in a flow analysis system and uses a single, very long-lived reagent solution. Interference from common inorganic anions and organic phosphorus species was minimal, and good spike recoveries {{for a range of}} sample matrices were obtained. The proposed flow analysis system is characterised by a limit of detection of 1. 3 μg L(- 1) P, linear range of 5 - 1000 μg L(- 1) P, dynamic range of 5 - 5000 μg L(- 1) P, repeatability of 0. 8 % (1000 μg L(- 1) P, n= 10) and 5. 6 % (10 μg L(- 1) P, n= 10), and sample throughput of 57 h(- 1). It is expected that <b>this</b> <b>method</b> <b>will</b> <b>improve</b> the feasibility of autonomous long-term environmental monitoring of dissolved reactive phosphate using inexpensive apparatus. Restricted Access: Metadata Onl...|$|E
40|$|The risk {{of water}} {{utilities}} {{would include the}} water quality failure and the water quantity failure, from the source to the tap, including the catchment, treatment, distribution and the customer plumbing system. In this paper, we proposed a practical evaluation method based on the analytic hierarchy process (AHP). The hierarchical structure of the water utilities was established {{in terms of the}} fault event analysis from the past failure accidents. The severity of criteria was preset by the experts and the probability of criteria was determined by a modified CUWA-TSM sheet with the consideration of the actual situations of the supply system. The evaluation model was successfully performed by a case study. Although, the method in this paper may not be as good as the framework of WSPs, it has a great advantage compared to WSPs and TSM. The risk management can be applied through specific software packages with a user-friendly interface, which means it is easier to implement. In addition, it can point out the critical control points (CCPs) for the decision-makers. So we believe <b>this</b> <b>method</b> <b>will</b> <b>improve</b> and play a more and more active {{role in the development of}} the risk management in China water works...|$|E
40|$|International audienceAdult-type hematopoietic {{stem and}} {{progenitor}} cells are formed during ontogeny from a specialized subset of endothelium, termed the hemogenic endothelium, via an endothelial-to-hematopoietic transition (EHT) {{that occurs in}} the embryonic aorta and the associated arteries. Despite efforts to generate models, {{little is known about}} the mechanisms that drive endothelial cells to the hemogenic fate and about the subsequent molecular control of the EHT. Here, we have designed a stromal line-free controlled culture system utilizing the embryonic pre-somitic mesoderm to obtain large numbers of endothelial cells that subsequently commit into hemogenic endothelium before undergoing EHT. Monitoring the culture for up to 12 days using key molecular markers reveals stepwise commitment into the blood-forming system that is reminiscent of the cellular and molecular changes occurring during hematopoietic development at the level of the aorta. Long-term single-cell imaging allows tracking of the EHT of newly formed blood cells from the layer of hemogenic endothelial cells. By modifying the culture conditions, it is also possible to modulate the endothelial cell commitment or the EHT or to produce smooth muscle cells at the expense of endothelial cells, demonstrating the versatility of the cell culture system. <b>This</b> <b>method</b> <b>will</b> <b>improve</b> our understanding of the precise cellular changes associated with hemogenic endothelium commitment and EHT and, by unfolding these earliest steps of the hematopoietic program, will pave the way for future ex vivo production of blood cells...|$|E
40|$|The {{generation}} of single-stranded DNA (ssDNA) molecules {{plays a key}} role in the SELEX (Systematic Evolution of Ligands by EXponential enrichment) combinatorial chemistry process and numerous molecular biology techniques and applications, such as DNA sequencing, single-nucleotide polymorphism (SNP) analysis, DNA chips, DNA single-strand conformation polymorphism (SSCP) analysis and many other techniques. The purity and yield of ssDNA can affect the success of each application. This study compares the two ssDNA production methods, the strand separation by streptavidin-coated magnetic beads and alkaline denaturation and the lambda exonuclease digestion, in regard to the purity of generated ssDNA and the efficiency. Here, we demonstrate the considerable benefits of ssDNA production by lambda exonuclease digestion for in vitro selection of DNA aptamers. We believe that the {{generation of}} ssDNA aptamers using <b>this</b> <b>method</b> <b>will</b> greatly <b>improve</b> the success rate of SELEX experiments concerning the recovery of target-specific aptamers...|$|R
40|$|Vascular {{growth factor}}s, {{including}} vascular endothelial growth factor and fibroblast growth factor- 2, bind to heparan sulfate proteoglycans {{in the basement}} membrane. While this binding, storage, and release system provides a critical model for controlled drug release devices, basement membrane—growth factor binding kinetics have not been fully established. We modified endothelial cell—growth factor binding kinetics protocols for the basement membrane. The basement membrane showed low affinity for fibroblast growth factor- 2 (Kd =  185. 8  nM), with a slow off rate (koff =  0. 00338  min− 1). However, results were confounded by growth factor binding to tissue culture polystyrene in a manner strikingly similar to basement membrane. Since substrate binding could not be blocked, a binding kinetics based correction technique was developed to account for polystyrene growth factor binding. <b>This</b> <b>method</b> was validated by conducting binding kinetics experiments on bacteriologic plates that exhibit little growth factor binding. <b>This</b> novel <b>method</b> <b>will</b> <b>improve</b> our understanding of cell and protein interaction with the basement membrane in health and disease. They can also further be applied to develop biomimetic drug delivery systems...|$|R
40|$|Medical {{equipments}} {{related to}} life safety of human, {{it is important}} to detect by a high precise method. Image mosaic which based on Harris corner operator is a commonly used <b>method</b> in <b>this</b> area; Harris operator has low calculation burden, it is simple and stable, so it is more effective comparing with other feature point extracted operators. But in this algorithm, corner points can only be detected in a single-scale, there may be losing information of corner points, causing corner point location offset, extracting false corner points because of noise. In order to solve this question, the acquired images should be processed by dilation and erosion operation firstly, then do image mosaic. Results show that image noise can be eliminated effectively after those morphological processes, as well as the false positive noise generated by image glitch. The success rate of image mosaic and detection accuracy can be greatly improved through the Morphology-Harris operator. Measurement of precision instruments which based on <b>this</b> new <b>method</b> <b>will</b> <b>improve</b> the measurement accuracy, and the research in this area will promote the further development of machine vision technology...|$|R
30|$|In the {{proposed}} method, it becomes difficult {{to adopt the}} exhaustive search for determining γ {{when the number of}} meteorological elements becomes larger. Therefore, we will have to solve this problem by using some alternative schemes. First, {{it would be useful to}} limit the number of the meteorological elements used for estimating each target meteorological element. This approach is similar to the feature selection, i.e., we select the elements that are the most useful for estimation of the target meteorological element. It should be noted that in the experiments described in this paper, we simply selected the three elements that tend to affect each other from the view point of meteorology. Second, we can use some alternative selection algorithms of the best results of γ instead of the exhaustive search. Many optimization methods that can avoid an exhaustive search have been proposed, and a genetic algorithm is one of the most traditional optimization approaches. By using such approaches, we can avoid an exhaustive search and enable the use of more meteorological elements for estimating the target meteorological elements. Some other approaches for learning kernels such as the approach in [18] may also solve this problem. This provides a solution to the problem of the need to tune optimal kernel parameters. This method can be easily extended to a supervised scenario, and it has also been reported that it can outperform the conventional multiple kernel learning approaches. Therefore, the introduction of <b>this</b> <b>method</b> <b>will</b> <b>improve</b> prediction performance.|$|E
40|$|Abstract — Spectrum sensing {{method is}} the {{fundamental}} factor when {{we are working with}} cognitive radio systems. Main aim and fundamental problem of cognitive radio is to identify weather primary users in authorized or licensed spectrum is presented or not. Paper deals with a new scheme of sensing based on the eigenvalues concept. It contain signals of covariance matrix received by the secondary users. In this method we are suggested two algorithms of sensing, one algorithm established by the maximum to minimum eigenvalue ratio. Other algorithm focused on average to minimum eigenvalue ratio. These two are done by using random matrix theories (RMT), and also these RMT are latest and also produce some accurate results. Now we calculate the ratios of distributions and probabilities of detection (Pd) and derive the probabilities of false alarm (Pfa) for the proposed algorithms, and also finding thresholds values for given Pfa. <b>This</b> <b>method</b> <b>will</b> <b>improve</b> the problem of noise uncertainty, and also performance is improved compare to energy detection when highly correlated signal is available. Paper also deals with another method is and also covariance methods. First one is statistical covariance method, it has different noise and received signal, and it is used for finding the primary users presence where there is only noise. These algorithms implemented by use of small number of received signal samples and processed to calculate the sample covariance matrix. By use of sample covariance matrix we are extracted two tes...|$|E
40|$|This paper {{focuses on}} how to {{calculate}} diluted earnings per share (DEPS) when a firm has outstanding employee stock options (ESOs). Three possible methods are described and compared. The first is the current International Accounting Standard 33 – Earnings Per Share (IAS 33) approach {{which is based on}} the intrinsic value of the ESOs. The second method, advocated by Core et al. (2002), is very similar to that of IAS 33 but instead of the intrinsic value uses the fair value of the outstanding options. This paper derives an alternative method which adjusts the earnings for the year by the change in fair value of the outstanding ESOs, with no adjustment to the denominator in the DEPS calculation. The three methods are compared using a simple firm. The earnings adjustment method best describes the change in economic value of the current shareholders, the fair value is more useful in predicting future profits, and the intrinsic value method appear to provide no additional information to that already contained in the other two measures. The earnings adjustment method has a further advantage in that it provides an identical result at a DEPS level to that which would have been obtained if the ESOs were cash-settled and treated as liabilities in terms of IFRS 2. Thus using <b>this</b> <b>method</b> <b>will</b> <b>improve</b> comparability as cash-settled and equity-settled options have a very similar economic effect on current shareholders. ...|$|E
40|$|License, which permits {{unrestricted}} use, distribution, {{and reproduction}} in any medium, provided the original work is properly cited. Relative individual information is a measurement that scores {{the quality of}} DNA- and RNA-binding sites for biological machines. The development of analytical approaches to increase the power of <b>this</b> scoring <b>method</b> <b>will</b> <b>improve</b> its utility in evaluating the functions of motifs. In this study, the scoring method was applied to potential translation initiation sites in Drosophila to compute Translation Relative Individual Information (TRII) scores. The weight matrix {{at the core of}} the scoring method was optimized based on high-confidence translation initiation sites identified by using a progressive partitioning approach. Comparing the distributions of TRII scores for sites of interest with those for high-confidence translation initiation sites and random sequences provides a new methodology for assessing the quality of translation initiation sites. The optimized weight matrices can also be used to describe the consensus at translation initiation sites, providing a quantitative measure of preferred and avoided nucleotides at each position. 1...|$|R
40|$|This paper {{introduces}} {{a method that}} is able to convert the symbols of vectorized maps into polygon attributes. It reduces the required storage space of the database by removing the corresponding polygons from the vectorized data model. This procedure is presented with optimized, linear runtime pattern matching on the raster image source of the map, where the symbols are handled as special textures. <b>This</b> <b>method</b> <b>will</b> be <b>improved</b> by using a raw vector model and the kernel symbols. The kernel of the symbol is the smallest part of the image which is tiling {{the image of the}} symbol. The optimization of the pattern matching is based on the edge filtered map and symbol images, where the points of interest are determined by thresholding the filtered images. These points are evaluated at pattern matching as possible symbol locations. The efficient method provides a rotation independent symbol recognition using the edge filtered map and symbol images, where edges are converted to vectors, then the angle of the possible rotation is calculated from the corresponding vector data...|$|R
40|$|Contamination of the {{conventional}} water resources created several technological problems for industry and domestic water supplies. Apart from that,in order to develop more economical sustainability for researchers,industrial chemical and pharmaceutical processing,so the investigation of hybrid separation processes is proposed. The induction heating concept will be applied because of <b>this</b> technological <b>method</b> <b>will</b> <b>improved</b> the distillation process. Due to this,there {{are two kinds of}} <b>method</b> <b>will</b> carried out, which are the induction heating technique and also the traditional heating technique. For the hybrid technique, the heating mantle will be used as the medium of heating element and also used reservoir water as the inlet of the process. The reservoir water will be used optimally without losing single drop of water (prevent wastage). Then,the temperature will be set on 800 C(inlet) and constantly at 50 Hz. Onward, repeated on 900 C, 1000 C and 1100 C. Similarly for the traditional heating method,equivalent procedure will used but different in heating element technique. Further on,the results of these kinds of <b>methods</b> <b>will</b> be comparing to determine the best results. Therefore,the expected results are;the best quality of distilled water is produced by using induction heating technique and also can reduced the electrical energy consumption...|$|R
40|$|The aim of {{this work}} is to develop {{improved}} methods of identifying the aerodynamic coefficients based on the synthesis of adaptive system, an adaptive algorithm aiming to identify and study the relations to identify and refine the aerodynamic parameters not maneuver lateral motion of the aircraft. The method combines {{the advantages of the}} known methods for the identification of the aerodynamic coefficients, which are presented in the public domain. The main problem {{lies in the fact that}} by traditional flight test aerodynamic coefficients are a set of discrete values. Linear model, which then is used does not describe accurately coefficients in flight, especially when changing parameters in a wide range of flight. The basic idea of the method consists in the following: synthesizing adaptive system that eliminates the inconsistency between the calculated values of the phase coordinates by which to identify and value, taken from the record flight tests. Simulation of changes in time of the phase coordinates is based on the results of other flight tests phase coordinates and mathematical model aircraft. After processing the records modified aircraft movement on the proposed algorithm we obtain the value of aerodynamic coefficients as a function of time for small changes in the parameters of flight and as a function of angle, angular velocity and deviation of steering for changing flight parameters in large range. The method of identification of the aerodynamic coefficients can fairly well determine the aerodynamic coefficients from flight test records: after identification increases the degree of real convergence of transient aircraft and its mathematical model. <b>This</b> <b>method</b> <b>will</b> <b>improve</b> the accuracy of identification of the aircraft motion parameters and therefore improve the quality of control. ??????? ??????????????? ?????????? ??????????? ????????? ?????? ???????????? ?????? ? ??????? ??????? ?? ????????? ???????, ???????? ?? ???????????? ????????? ??????? ??????. ?????????? ?????????? ???????? ????? ?? ???????? ??????? ??????? ????? ??????? ??? ?????? ? ??????? ??????????? ??????? ?????????? ??????? ??????. ? ?????? ?????? ????????? ?????, ??????? ???????? ??????????????????? ??????? ????????????? ?? ?????? ??????? ?????????? ???????. ?? ???????? ???????????? ????????? ??????? ????????????? ???????????????? ?????????????, ??????? ???????????? ? ???????? ??????????. ????? ????????????? ???????????????? ????????????? ????????? ?????????? ?????? ?????????? ???????????????? ???????????? ?? ??????? ?????? ?????????: ????? ?????????? ????????????? ????????????? ??????? ?????????? ?????????? ????????? ????????? ?? ? ??? ?????????????? ??????. ???? ????? ????????? ????????? ???????? ????????????? ?????????? ???????? ??...|$|E
40|$|ABSTRACT Trypanosoma cruzi is the {{etiologic}} agent of Chagas disease, and current methods for its genetic manipulation have been highly inefficient. We report here {{the use of}} the CRISPR (clustered regularly interspaced short palindromic repeats) /Cas 9 (CRISPR-associated gene 9) system for disrupting genes in the parasite by three different strategies. The utility of the method was established by silencing genes encoding the GP 72 protein, which is required for flagellar attachment, and paraflagellar rod proteins 1 and 2 (PFR 1, PFR 2), key components of the parasite flagellum. We used either vectors containing single guide RNA (sgRNA) and Cas 9, separately or together, or one vector containing sgRNA and Cas 9 plus donor DNA for homologous recombi-nation to rapidly generate mutant cell lines in which the PFR 1, PFR 2, andGP 72 genes have been disrupted. We demonstrate that genome editing of these endogenous genes in T. cruzi is successful without detectable toxicity of Cas 9. Our results indicate that PFR 1, PFR 2, and GP 72 contribute to flagellar attachment to the cell body andmotility of the parasites. Therefore, CRISPR/Cas 9 allows efficient gene disruption in an almost genetically intractable parasite and suggest that <b>this</b> <b>method</b> <b>will</b> <b>improve</b> the func-tional analyses of its genome. IMPORTANCE Trypanosoma cruzi is the agent of Chagas disease, which affects millions of people worldwide. Vaccines to prevent this disease are not available, and drug treatments are not completely effective. The study of the biology of this parasite through genetic approaches will make possible the development of new preventive or treatment options. Previous attempts to use the CRISPR/Cas 9 in T. cruzi found a detectable but low frequency of Cas 9 -facilitated homologous recombination and fluorescen...|$|E
40|$|Spectrum sensing {{method is}} the {{fundamental}} factor when {{we are working with}} cognitive radio systems. Main aim and fundamental problem of cognitive radio is to identify weather primary users in authorized or licensed spectrum is presented or not. Paper deals with a new scheme of sensing based on the eigenvalues concept. It contain signals of covariance matrix received by the secondary users. In this method we are suggested two algorithms of sensing, one algorithm established by the maximum to minimum eigenvalue ratio. Other algorithm focused on average to minimum eigenvalue ratio. These two are done by using random matrix theories (RMT), and also these RMT are latest and also produce some accurate results. Now we calculate the ratios of distributions and probabilities of detection (Pd) and derive the probabilities of false alarm (Pfa) for the proposed algorithms, and also finding thresholds values for given Pfa. <b>This</b> <b>method</b> <b>will</b> <b>improve</b> the problem of noise uncertainty, and also performance isimproved compare to energy detection when highly correlated signal is available. Paper also deals with another method is and also covariance methods. First one is statistical covariance method, it has different noise and received signal, and it is used for finding the primary users presence where there is only noise. These algorithms implemented by use of small number of received signal samples and processed to calculate the sample covariance matrix. By use of sample covariance matrix we are extracted two test statistics. Finally we compare these results and concluded that signal presence. These are used in many signal detection applications, and also do not need signal information, also noise power and channel. We did the Simulations based on two ways. First one is randomly generated signals. Other one is done by captured DTV signals taken from ATSV committee, these are broadcasting signals. These methods confirm and verifies the efficiency of the proposed methods...|$|E
30|$|Recent {{research}} documents notify {{that understanding}} emotional expressions {{play an important}} role in the development and maintenance of communal relationships. By accurately interpreting other’s emotions, one can obtain valuable information. All people thus certainly teachers and students use facial expressions to form impressions of another. This paper statistically proved that facial expressions of the students are the most used nonverbal communication mode in the classroom and student’s expressions are significantly correlated to their emotions which can help to recognize their comprehension towards the lecture. Hence this research concludes that Facial Expression plays a vital role in identification of Emotions and Comprehension of the students who are geographically distributed in the virtual classrooms as the classroom communication in virtual classroom is analogous to the communication in real classroom. The effectiveness of <b>this</b> <b>method</b> <b>will</b> be <b>improved</b> by correlating more features from different action units of the face which would improve the recognition rate. The limitation of the method is that if the visualization of the student is not available in the virtual lecture, facial expressions cannot be used a tool by the virtual lecturer to interpret student’s comprehension.|$|R
40|$|AbstractIn this paper, {{large amount}} of {{subsection}} calculations and complex calculation of the traditional algorithm of an electronic anti-fouling system excitation coil current is analyzed, and a method based on a signal superposition theory of a linear system is put forward. What's more, a model system consisting of the electronic anti-fouling system and aqueous solution in the tube is established and the induced current is calculated carrying on reasonable approximates. Then the correctness of the proposed method and rationality of approximate system model are proved using finite element numerical <b>method.</b> <b>This</b> research <b>will</b> <b>improve</b> the adaptability of the electronic anti-fouling system in different water quality and optimize the effect on anti-scaling, and it provides a useful theoretical basis for further research on anti-scaling mechanism of electronic anti-fouling system...|$|R
40|$|Background. Tracer {{production}} for nuclear medicine. Objective. The {{aim of the}} paper is {{to consider the possibility}} of 99 mTc tracer production using low-energy medical cyclotrons installed in Ukraine applying enriched 100 Мо targets. Methods. The cross sections of 100 Mo(p, 2 n) 99 mTc nuclear reactions and reactions, leading to formation of impurities were calculated. The technical aspects of irradiation process were considered. Necessary target thickness and 99 mTc tracer yield for the Eclipse RD (Siemens) and PETtrace (GE) cyclotrons were estimated. Results. Within the framework of proposed concept, 99 mTc tracer yield equals 3. 7  and 35. 5  GBq after 2 h of bombardment for Eclipse RD and PETtrace cyclotrons, respectively. Conclusions. The obtained results showed satisfied 99 mTс tracer yields and feasibility of further development of <b>this</b> <b>method,</b> which <b>will</b> significantly <b>improve</b> the efficiency of cyclotron installations...|$|R
40|$|Trypanosoma cruzi is the {{etiologic}} agent of Chagas disease, and current methods for its genetic manipulation have been highly inefficient. We report here {{the use of}} the CRISPR (clustered regularly interspaced short palindromic repeats) /Cas 9 (CRISPR-associated gene 9) system for disrupting genes in the parasite by three different strategies. The utility of the method was established by silencing genes encoding the GP 72 protein, which is required for flagellar attachment, and paraflagellar rod proteins 1 and 2 (PFR 1, PFR 2), key components of the parasite flagellum. We used either vectors containing single guide RNA (sgRNA) and Cas 9, separately or together, or one vector containing sgRNA and Cas 9 plus donor DNA for homologous recombination to rapidly generate mutant cell lines in which the PFR 1, PFR 2, and GP 72 genes have been disrupted. We demonstrate that genome editing of these endogenous genes in T. cruzi is successful without detectable toxicity of Cas 9. Our results indicate that PFR 1, PFR 2, and GP 72 contribute to flagellar attachment to the cell body and motility of the parasites. Therefore, CRISPR/Cas 9 allows efficient gene disruption in an almost genetically intractable parasite and suggest that <b>this</b> <b>method</b> <b>will</b> <b>improve</b> the functional analyses of its genome. IMPORTANCE Trypanosoma cruzi is the agent of Chagas disease, which affects millions of people worldwide. Vaccines to prevent this disease are not available, and drug treatments are not completely effective. The study of the biology of this parasite through genetic approaches will make possible the development of new preventive or treatment options. Previous attempts to use the CRISPR/Cas 9 in T. cruzi found a detectable but low frequency of Cas 9 -facilitated homologous recombination and fluorescent marker swap between exogenous genes, while Cas 9 was toxic to the cells. In this report, we describe new approaches that generate complete disruption of an endogenous gene without toxicity to the parasites and establish the relevance of several proteins for flagellar attachment and motility...|$|E
40|$|The {{disadvantage}} of the electromagnetic-acoustic (EMA) method receiving ultrasonic waves are low efficiency. The traditional way to enhance its effectiveness is increase the bias field. The {{aim of the}} study was research the way to improve the efficiency of the EMA transformation, using a time-varying bias field. The researches held {{with the help of a}} specially designed installation that allows the magnetization to be performed by a constant and alternating magnetic field (dynamic bias), synchronously with the passage of the received pulse. The object of the study were rods made of different grades of steel with a diameter of 4 – 6 mm, in which the symmetrical zero mode S 0 of the rod wave was excited by the EMA method (in the frequency range of about 40 kHz). A comparative analysis of the amplitudes and form pulses of multiple reflections during static and dynamic reversal of magnetization and with a full cycle of magnetization reversal conducted. The result of the efficiency measurements EMA reception during static and dynamic bias found a significant (up to 5 times) increase in the signal amplitude on the receiving transducer. Taking into account that the main contribution to the excitation mechanism and the reception mechanism made the magnetostrictive effect on low frecuncy, it can assumed that using a dynamic bias field is impacting significant on the effective mobility of magnetic domains (that is changes the dynamic magnetic susceptibility of the material). It is established that it is possible to monitor steel at lower values of the bias field, and, consequently, to reduce the mass dimensions of the magnetic system. Thus, in the course of the researchers found of effect of dynamic bias and effect of dynamic bias increase acoustic pulse amplitude of the signal of the received EMA method. Using <b>this</b> <b>method</b> <b>will</b> <b>improve</b> the quality EMA testing by creating more efficient EMA transducer. Taking into account that the value of the detected effect depends significantly on the steel grade, we can assume its possible application in the methods of express analysis, estimation of structural and stressed states.  </p...|$|E
40|$|Project (M. S., Computer Science) [...] California State University, Sacramento, 2012 The FIEP is a {{mechanism}} that enables firewalls {{to communicate with}} each other???s firewalls and form firewall groups in a network. The information the firewalls communicate with each other would improve their ability to detect any attack and thus protects the network from attack. The FIEP also improves the ability to adapt to changes in the network, informing other firewalls when there is an attack, it also informs about an update in the access control rules in the firewalls in a secure way. Besides this, the FIEP keeps all the firewalls in the group informed about the activity going on in the group such as messaging the entire group about a new firewall joining in or moving away from the group etc. This improves the ability to detect any attack in a more efficient way. In the current scenario, there is no protocol that enables firewalls {{to communicate with each}} other and exchange information. Until recently, not much thought was given to the need for firewalls to talk to each other; firewalled network is isolated from the rest of the network and considered to be secure. But that is not true firewalled network is safe but not totally secure it is prone to distributed attacks. To overcome this drawback, I propose the FIEP, using which firewalls can talk to each other and exchange information. The FIEP is like the Border Gateway Protocol (BGP) which enables routers to exchange routing information and keeps them updated. Similarly, FIEP will enable the firewalls to update firewall rules, form groups and alert the other firewall in the network about attacks, <b>this</b> <b>method</b> <b>will</b> <b>improve</b> the security and increase the robustness of the network. The Goal of this project is to create an initial design of the FIEP which specifies how the firewalls interact with each other and how they can be formed into groups. In version 1 of FIEP, I intend to show the detailed steps involved in communication with other firewalls, for example what type of connection is required, TCP or UDP, how these connections should be established, its requirements and what information will be exchanged e. g. access control rules and establishing a group such as having a lead firewall which will maintain the group information etc. To design FIEP an example network with firewalls in it will be designed first and this example network will be used throughout the project and to finally aid in designing the FIEP. The project will also show current best practices in firewall deployment. The FIEP will be a breakthrough in not just the Network Security domain but will also pave the way for firewall communications. Future and extensive study in this regard can help improve the current problems that the networks face. Computer Scienc...|$|E
40|$|The {{capacity}} of power-distributive oil-transformers is selected {{on the basis}} of their operational mode cost-effectiveness and power-supply reliability of the electrical recipients on condition that the transformers duty should not lead to reduction of their normal life-in-service. Unconscionable and protracted overloads intrinsically decrease reliability and respectively service life of the transformers owing to increased wear of the winding insulation because of the excessive heat. The unaccounted previous loading history and possible changes of the risen overload in the course of its further development can lead to poor accuracy in the admissible overload time estimation. Therefore, that may lead to intolerable excessive heat in the winding insulation or underutilization of the potential overload {{capacity of}} the transformers. Which limits the potentiality of complete demand satisfaction of the electrical consumers. This is exactly why the acting GOST 14209 – 97 strongly recommends the consumers to make their own calculations of the load-carrying capacity based on the real load curves. The authors present a method for ascertainment of the admissible duration of the systematic non-emergency overload of a distributive oil-transformer. The ascertainment method accounts for the overload repetition-factor alteration on the time-interval of its occurrence and respectively the variation in the transformer thermal state. The employment of <b>this</b> <b>method</b> <b>will</b> allow <b>improving</b> accuracy in ascertainment of the admissible duration of the systematic non-emergency overloads and eventually the reliability of the transformer operation and the power-supply system in its entirety. The method realization is intended by means of the transformer-load monitoring as one of the tasks of the electric-energy automated control and accounting system. </p...|$|R
40|$|We {{contribute}} to an improved understanding of Internet traffic characteristics by measuring and analyzing modern Internet backbone data. We start the thesis with an overview of several important considerations for passive Internet traffic collection on large-scale network links. The lessons learned from a successful measurement project on academic Internet backbone links can serve as guidelines to others setting up and performing similar measurements. The data from these measurements are {{the basis for the}} analyses made in this thesis. As a first result we present a detailed characterization of packet headers, which reveals protocol-specific features and provides a systematic survey of packet header anomalies. The packet-level analysis is followed by a characterization on the flow-level, where packets are correlated according to their communication endpoints. We propose a method and accompanying metrics to assess routing symmetry on a flow-level based on passive measurements. <b>This</b> <b>method</b> <b>will</b> help to <b>improve</b> traffic analysis techniques. We used the method on our data, and the results suggest that routing symmetry is uncommon on nonedge Internet links. We then confirm the predominance of TCP as the transport protoco...|$|R
40|$|The major aim of {{tertiary}} structure prediction is to obtain protein models {{with the highest}} possible accuracy. Fold recognition, homology modeling, and de novo prediction methods typically use predicted secondary structures as input, {{and all of these}} methods may significantly benefit from more accurate secondary structure predictions. Although there are many different secondary structure prediction methods available in the literature, their cross-validated prediction accuracy is generally < 80 %. In order to increase the prediction accuracy, we developed a novel hybrid algorithm called Consensus Data Mining (CDM) that combines our two previous successful methods: (1) Fragment Database Mining (FDM), which exploits the Protein Data Bank structures, and (2) GOR V, which is based on information theory, Bayesian statistics, and multiple sequence alignments (MSA). In CDM, the target sequence is dissected into smaller fragments that are compared with fragments obtained from related sequences in the PDB. For fragments with a sequence identity above a certain sequence identity threshold, the FDM method is applied for the prediction. The remainder of the fragments are predicted by GOR V. The results of the CDM are provided {{as a function of the}} upper sequence identities of aligned fragments and the sequence identity threshold. We observe that the value 50 % is the optimum sequence identity threshold, and that the accuracy of the CDM method measured by Q 3 ranges from 67. 5 % to 93. 2 %, depending on the availability of known structural fragments with sufficiently high sequence identity. As the Protein Data Bank grows, it is anticipated that <b>this</b> consensus <b>method</b> <b>will</b> <b>improve</b> because it <b>will</b> rely more upon the structural fragments...|$|R
40|$|Scarcity of land sites {{applicable}} {{for wind}} turbines {{is pushing the}} technology offshore. By going offshore the expenses of nearly all components increases, which has triggered extensive research {{with the aim of}} decreasing component costs, and increasing reliability, as also maintenance costs increase as going offshore. The focus of this thesis is to look at ways to reduce aerodynamic loads which can contribute in lowering structural fatigue loads and thereby component costs for offshore wind turbines. The loading is mainly reported as blade root flapwise bending moment on wind turbines intentionally made for bottom fixed offshore sites. The load reduction is investigated using a different wind turbine configuration with a downwind mounted rotor and further compared with the conventional upwind mounted rotor on a monopile tower. Blades on downwind mounted rotors are exposed to the fluctuating wake behind the towers, known as the tower shadow. The influence from the tower shadow on blade fatigue loads is investigated using three different types of towers; a full height truss type tower, a faring (airfoil shaped) tower and a monopile tower. For reliable wind turbine simulations with downwind mounted rotors, an accurate tower shadow model is essential. Thorough investigations of the tower shadow is presented, including the detailed flow picture of the mean velocity deficit, unsteady and turbulent motions, as well as velocity spectra. The tower shadow is investigated in three different ways; using three dimensional physical model scale experiments, steady tower shadow models and two dimensional computational fluid dynamic (CFD) simulations. By use of the tower shadow from the CFD simulations, an artificial increase in blade fatigue loading is seen as the transversal grid is made coarser. Although this is a ’safe-fail’ design (for the coarser grid), it should be kept in mind as this ’simulated’ safety factor from the coarse grid simulations could increase wind turbine costs. A method for improving the accuracy of the steady tower shadow models (currently the most frequently used model in commercial software) through a preprocessing step, where the results are directly applicable in commercial software for full wind turbine simulations, is presented. <b>This</b> <b>method</b> <b>will</b> <b>improve</b> the reliability of the simulated results. The parameters of the steady tower shadow model and the turbulence intensity are fitted and calibrated with short CFD simulations of the relevant tower geometries. This method accounts for any deviation between the mean velocity deficit obtained from the steady tower shadow model and the CFD simulations, as well as the unsteady motions and turbulence due to the presence of the tower through the calibration of the turbulence intensity (maximum deviation of ± 3 percent with respect to the tower shadow based on the CFD simulations, measured as blade fatigue loading). The response measured as blade fatigue load show an increased loading for the blades on the downwind mounted rotors using the original blades, compared to the conventional upwind mounted rotor on a monopile tower. Introducing softer and lighter blades changed this result, with reductions in blade fatigue loading (compared to the upwind mounted rotor with the original blades) of three, four and five percent for the monopile, truss and fairing towers, respectively. PhD i bygg, anlegg og transportPhD in Civil and Transport Engineerin...|$|E
40|$|Until now, {{there is}} no method {{can be used to}} {{accurately}} assess the particles size distribution as well as textural classes of gypsiferous soils for proper interpretation of physical behavior of these soils, and most laboratory methods involve pretreatment to remove gypsum from the samples. Therefore, the results of the particle size distribution do not reflect the size distribution of the whole soil. This study aimed to develop an alternative method to determine particle size distribution for some gypsiferous soils selected from Al-Ahsa governorate, Saudi Arabia. Five samples from different profiles with different gypsum content were selected to evaluate the modified method. Sand fractions were separated with three disaggregation methods: 1) drying sieving, 2) shaking for 5 hours in a 7 : 3 ethanol: water solution, and 3) sonication for 3 minutes in a 7 : 3 ethanol: water solution. The statistical analysis results revealed that the sonication for 3 minutes in a 7 : 3 ethanol: water solution was the most effective method for separating sand fractions as compared to dry sieving and shaking. Meanwhile, there was slight difference in separating sand fractions between sonication for 3 minutes and shaking for 5 hours. The particle size distribution by the developed method showed increasing in total sand content as compared to standard particle size method. Likewise, comparison of the CEC/clay ratio between the two methods also indicated that the developed method yielded clay contents more consistent with other property data for the same horizons. Consequently, the textural classes obtained from the two methods were different. Therefore; we concluded that the determination of particle size distribution for gypsiferous soils (≤ 40 % gypsum) using <b>this</b> developed <b>method</b> <b>will</b> <b>improve</b> the understanding and ability to proper interpret of physical behavior of these unique soils. We highly recommended using <b>this</b> developed <b>method</b> to separate soil particles from the gypsiferous soils...|$|R
40|$|A unique {{method to}} {{validate}} back scattered ultraviolet (buv) type satellite data that complements the measurements from existing ground networks is proposed. The method involves comparing the zenith sky radiance measurements {{from the ground}} to the nadir radiance measurements taken from space. Since the measurements are compared directly, the proposed method is superior to any other method that involves comparing derived products (for example, ozone), because comparison of derived products involve inversion algorithms which are susceptible to several type of errors. Forward radiative transfer (RT) calculations show that for an aerosol free atmosphere, the ground-based zenith sky radiance measurement and the satellite nadir radiance measurements can be predicted with an accuracy of better than 1 percent. The RT computations also show that for certain values of the solar zenith angles, the radiance comparisons could be better than half a percent. This accuracy is practically independent of ozone amount and aerosols in the atmosphere. Experiences with the Shuttle Solar Backscatter Ultraviolet (SSBUV) program show that {{the accuracy of the}} ground-based zenith sky radiance measuring instrument can be maintained at a level of a few tenth of a percent. This implies that the zenith sky radiance measurements can be used to validate Total Ozone Mapping Spectrometer (TOMS), Solar Backscatter Ultraviolet (SBUV/ 2), and The SCanning Imaging Absorption SpectroMeter for Atmospheric CHartographY (SCIAMACHY) radiance data. Also, <b>this</b> <b>method</b> <b>will</b> help <b>improve</b> the long term precision of the measurements for better trend detection and the accuracy of other BUV products such as tropospheric ozone and aerosols. Finally, in the long term, <b>this</b> <b>method</b> is a good candidate to inter-calibrate and validate long term observations of upcoming operational instruments such as Global Ozone Monitoring Experiment (GOME- 2), Ozone Mapping Instrument (OMI), Ozone Dynamics Ultraviolet Spectrometer (ODUS), and Ozone Mapping and Profiler Suite (OMPS) ...|$|R
40|$|International audienceIn many problems, complex non-Gaussian and/or {{nonlinear}} {{models are}} required to accurately describe a physical system of interest. In such cases, Monte Carlo algorithms are remarkably flexible and extremely powerful approaches to solve such inference problems. However, {{in the presence of}} a high-dimensional and/or multimodal posterior distribution, it is widely documented that standard Monte-Carlo techniques could lead to poor performance. In this paper, the study is focused on a Sequential Monte- Carlo (SMC) sampler framework, a more robust and efficient Monte Carlo algorithm. Although this approach presents many advantages over traditional Monte-Carlo methods, the potential of this emergent technique is however largely underexploited in signal processing. In this work, we aim at proposing some novel strategies that <b>will</b> <b>improve</b> the efficiency and facilitate practical implementation of the SMC sampler specifically for signal processing applications. Firstly, we propose an automatic and adaptive strategy that selects the sequence of distributions within the SMC sampler that minimizes the asymptotic variance of the estimator of the posterior normalization constant. This is critical for performing model selection in modelling applications in Bayesian signal processing. The second original contribution we present improves the global efficiency of the SMC sampler by introducing a novel correction mechanism that allows the use of the particles generated through all the iterations of the algorithm (instead of only particles from the last iteration). This is a significant contribution as it removes the need to discard a large portion of the samples obtained, as is standard in standard SMC <b>methods.</b> <b>This</b> <b>will</b> <b>improve</b> estimation performance in practical settings where computational budget is important to consider...|$|R
40|$|In many problems, complex non-Gaussian and/or {{nonlinear}} {{models are}} required to accurately describe a physical system of interest. In such cases, Monte Carlo algorithms are remarkably flexible and extremely powerful approaches to solve such inference problems. However, {{in the presence of}} a high-dimensional and/or multimodal posterior distribution, it is widely documented that standard Monte-Carlo techniques could lead to poor performance. In this paper, the study is focused on a Sequential Monte-Carlo (SMC) sampler framework, a more robust and efficient Monte Carlo algorithm. Although this approach presents many advantages over traditional Monte-Carlo methods, the potential of this emergent technique is however largely underexploited in signal processing. In this work, we aim at proposing some novel strategies that <b>will</b> <b>improve</b> the efficiency and facilitate practical implementation of the SMC sampler specifically for signal processing applications. Firstly, we propose an automatic and adaptive strategy that selects the sequence of distributions within the SMC sampler that minimizes the asymptotic variance of the estimator of the posterior normalization constant. This is critical for performing model selection in modelling applications in Bayesian signal processing. The second original contribution we present improves the global efficiency of the SMC sampler by introducing a novel correction mechanism that allows the use of the particles generated through all the iterations of the algorithm (instead of only particles from the last iteration). This is a significant contribution as it removes the need to discard a large portion of the samples obtained, as is standard in standard SMC <b>methods.</b> <b>This</b> <b>will</b> <b>improve</b> estimation performance in practical settings where computational budget is important to consider. Comment: arXiv admin note: text overlap with arXiv: 1303. 3123 by other author...|$|R
40|$|Introduction: There {{are many}} {{problems}} with current state-of-the-art protocols for maintenance dosing {{of the oral}} anticoagulant agent warfarin used in clinical practice. The two key challenges include lack of personalized dose adjustment and {{the high cost of}} monitoring the efficacy of the therapy in the form of International Normalized Ratio (INR) measurements. A new dosing algorithm based on the principles of Reinforcement Learning (RL), specifically Q-Learning with functional policy approximation, was created to personalize maintenance dosing of warfarin based on observed INR and to optimize the length of time between INR measurements. <b>This</b> new <b>method</b> <b>will</b> help <b>improve</b> patient’s INR time in therapeutic range (TTR) as well as minimize cost associated with monitoring INR when compared to the current standard of care. Procedure: Using the principles of Reinforcement Learning, an algorithm to control warfarin dosing was created. The algorithm uses 9 different controllers which correspond to 9 different levels of warfarin sensitivity. The algorithm switches between controllers until it selects the controller that most closely resembles the individual patient’s response, and thus the optimal dose change (?Dose) and time between INR measurements (?Time) are personalized for each patient, based on INR observed in the patient. Three simulations were performed using data from 100 artificial patients, generated based on data from real patients, each. The first simulation that was performed was an ideal case scenario (clean simulation where the coefficient of variance (CV) of noise added to the model output = 0) using only the warfarin RL algorithm to prove efficacy. The second simulation was performed using the current standard of care and a CV = 25...|$|R
