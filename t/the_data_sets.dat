4827|10000|Public
25|$|They {{have started}} formulating science plans based on <b>the</b> <b>data</b> <b>sets</b> {{obtained}} from the mission. It is expected {{that in the next}} few months, interesting results about lunar topography, mineral and chemical contents of the Moon and related aspects are expected to be published.|$|E
25|$|A {{recent review}} of over two dozen 18q LOH-survival studies {{concluded}} that there was a significant amount of inconsistency between <b>the</b> <b>data</b> <b>sets.</b> They concluded that loss of 18q remains a marker for poor prognosis, and that DCC status has the potential to define a group of patients who may benefit from specific treatment regimes.|$|E
25|$|EEG {{has several}} strong points {{as a tool}} for {{exploring}} brain activity. EEGs can detect changes over milliseconds, which is excellent considering an action potential takes approximately 0.5–130 milliseconds to propagate across a single neuron, {{depending on the type of}} neuron. Other methods of looking at brain activity, such as PET and fMRI have time resolution between seconds and minutes. EEG measures the brain's electrical activity directly, while other methods record changes in blood flow (e.g., SPECT, fMRI) or metabolic activity (e.g., PET, NIRS), which are indirect markers of brain electrical activity. EEG can be used simultaneously with fMRI so that high-temporal-resolution data can be recorded at the same time as high-spatial-resolution data, however, since the data derived from each occurs over a different time course, <b>the</b> <b>data</b> <b>sets</b> do not necessarily represent exactly the same brain activity. There are technical difficulties associated with combining these two modalities, including the need to remove the MRI gradient artifact present during MRI acquisition and the ballistocardiographic artifact (resulting from the pulsatile motion of blood and tissue) from the EEG. Furthermore, currents can be induced in moving EEG electrode wires due to the magnetic field of the MRI.|$|E
3000|$|... {{means that}} while 95 % of <b>the</b> <b>data</b> <b>set</b> {{does not include}} any outliers, 5 % of <b>the</b> <b>data</b> <b>set</b> {{includes}} outliers.|$|R
5000|$|Entropy [...] is {{a measure}} of the amount of {{uncertainty}} in <b>the</b> (<b>data)</b> <b>set</b> [...] (i.e. entropy characterizes <b>the</b> (<b>data)</b> <b>set</b> [...] ).|$|R
40|$|<b>The</b> <b>data</b> <b>set</b> {{specifications}} for <b>the</b> NASA Aerospace Safety Information System (NASIS) are presented. <b>The</b> <b>data</b> <b>set</b> specifications describe <b>the</b> content, format, and medium of communication of every <b>data</b> <b>set</b> required by <b>the</b> system. All relevant information pertinent {{to a particular}} <b>data</b> <b>set</b> is prepared in a standard form and centralized in a single document. The format for <b>the</b> <b>data</b> <b>set</b> is provided...|$|R
2500|$|This type of {{research}} was replicated by Marxian scholars in many other countries around the world, [...] who often introduced their own technical refinements in <b>the</b> <b>data</b> <b>sets.</b>|$|E
2500|$|A {{statistical}} hypothesis, {{sometimes called}} confirmatory data analysis, is a hypothesis that is testable {{on the basis}} of observing a process that is modeled via a set of random variables. [...] A statistical hypothesis test is a method of statistical inference. Commonly, two statistical data sets are compared, or a data set obtained by sampling is compared against a synthetic data set from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis that proposes no relationship between two data sets. The comparison is deemed statistically significant if the relationship between <b>the</b> <b>data</b> <b>sets</b> would be an unlikely realization of the null hypothesis according to a threshold probabilitythe significance level. Hypothesis tests are used in determining what outcomes of a study would lead to a rejection of the null hypothesis for a pre-specified level of significance. The process of distinguishing between the null hypothesis and the alternative hypothesis is aided by identifying two conceptual types of errors (type 1 & type 2), and by specifying parametric limits on e.g. how much type 1 error will be permitted.|$|E
50|$|Although <b>the</b> <b>data</b> <b>sets</b> were {{constructed}} to preserve customer privacy, the Prize {{has been criticized}} by privacy advocates. In 2007 two researchers from The University of Texas at Austin {{were able to identify}} individual users by matching <b>the</b> <b>data</b> <b>sets</b> with film ratings on the Internet Movie Database.|$|E
30|$|The code {{used for}} the {{analyses}} in this paper is available at [URL] <b>The</b> <b>data</b> <b>set</b> for MovieLens is available at [URL] and <b>the</b> <b>data</b> <b>set</b> for BookCrossing is available at [URL] <b>The</b> <b>data</b> <b>set</b> for IMDb was crawled by the authors in a previous study and cannot be shared for legal reasons.|$|R
40|$|This paper {{illustrates}} {{the use of}} a real-time <b>data</b> <b>set</b> for forecasting. <b>The</b> <b>data</b> <b>set</b> consists of vintages, or snapshots, of <b>the</b> major macroeconomic <b>data</b> available at quarterly intervals in real time. The paper explains the construction of <b>the</b> <b>data</b> <b>set,</b> examines <b>the</b> properties of several of the variables in <b>the</b> <b>data</b> <b>set</b> across vintages, and shows how forecasts can be affected by data revisions. Forecasting...|$|R
30|$|<b>Data</b> <b>set</b> feature {{processing}} scheme includes feature collection, feature {{conversion and}} reservation. The filter feature of <b>data</b> <b>set</b> would be pre-fetched. The classification characteristics of <b>data</b> <b>set</b> {{could be obtained}} based on the classification accuracy of <b>data</b> <b>set.</b> According to <b>the</b> characteristics of <b>the</b> <b>data</b> <b>set,</b> <b>the</b> crowd filter would select the appropriate crowd incentive strategy. Based on the complexity characteristics of <b>the</b> <b>data</b> <b>set</b> classification, <b>the</b> transformation of the subset of <b>the</b> <b>data</b> <b>set</b> would be completed.|$|R
5000|$|... #Subtitle level 2: Limitations and {{criticisms of}} <b>the</b> <b>data</b> <b>sets</b> ...|$|E
5000|$|The diagram shown [...] {{compares the}} results of {{training}} on <b>the</b> <b>data</b> <b>sets</b> ...|$|E
5000|$|The European Data Portal {{provides}} {{access to}} <b>the</b> <b>data</b> <b>sets</b> in different ways, via: ...|$|E
30|$|<b>The</b> {{ontology}} <b>data</b> <b>set</b> {{is linked}} to Link Open Data Cloud (Cyganiak and Jentzsch 2011; HPI 2011). We need to adjust several points for this application such as all the URIs in <b>the</b> <b>data</b> <b>set</b> must be resolved. Linked Open Data Cloud must be in RDF data format such as (RDFa, RDF/XML, Turtle, N-Triples). <b>The</b> <b>data</b> <b>set</b> should contain at least 1000 triples and must be connected via RDF links that are already in <b>the</b> diagram. <b>The</b> <b>data</b> <b>set</b> may be accessed using SPARQL endpoint, RDF crawling, or RDF dump. After <b>the</b> <b>data</b> <b>set</b> meets these criteria, it is added to <b>the</b> <b>Data</b> Hub located at [URL] as shown in “Appendix 4 ”.|$|R
30|$|Hypotheses {{related to}} {{operating}} core (innovation process, cross-functional organisation, {{and implementation of}} tools/technology) and competition-informed pricing The result of H 4 is supported in <b>the</b> full <b>data</b> <b>set</b> (β =  0.170, p <  0.05) and <b>the</b> Malaysian <b>data</b> <b>set</b> (β =  0.255, p <  0.05), while in <b>the</b> Bangladeshi <b>data</b> it was not supported. The result of H 5 was supported in <b>the</b> full <b>data</b> <b>set</b> (β =  0.266, p <  0.05) and <b>the</b> Bangladeshi <b>data</b> <b>set</b> (β =  0.275, p <  0.05), while in <b>the</b> Malaysian <b>data</b> <b>set</b> it was not supported. The result of H 6 was supported in <b>the</b> full <b>data</b> <b>set</b> (β =  0.295, p <  0.01) and <b>the</b> Bangladeshi <b>data</b> <b>set</b> (β =  0.536, p <  0.01), while in <b>the</b> Malaysian <b>data</b> <b>set,</b> H 6 was not supported.|$|R
30|$|<b>The</b> author uses <b>data</b> from <b>the</b> EU-SILC, an {{official}} survey designed {{to describe and}} to explain living conditions in all EU member states. For the analysis, <b>the</b> cross sectional <b>data</b> <b>sets</b> of <b>the</b> waves 2007, 2008 and 2009 are pooled. <b>The</b> <b>data</b> <b>set</b> comprises females who were born between 1960 and 1990. The author limits the analysis to these birth cohorts {{because it can be}} expected that most of these women have finished their education but are not yet retired. Women who are in full-time education are excluded from <b>the</b> <b>data</b> <b>set.</b> <b>The</b> <b>data</b> <b>set</b> is also limited to women who are neither in military and social services nor retired. <b>The</b> <b>data</b> <b>set</b> comprises approximately 205, 000 individuals.|$|R
5000|$|For {{partitioned}} data sets, IEBCOMPR considers <b>the</b> <b>data</b> <b>sets</b> equal if {{the following}} conditions are met: ...|$|E
5000|$|Since {{dimensional}} models only {{gain from}} aggregates on large data sets, at what size of <b>the</b> <b>data</b> <b>sets</b> should one start considering using aggregates? ...|$|E
50|$|Alternate smoothers, {{such as the}} radial function, {{harmonic}} {{function and}} additive function, have been suggested and their performances vary depending on <b>the</b> <b>data</b> <b>sets</b> used.|$|E
40|$|Multidimensional <b>data</b> <b>sets</b> {{often include}} {{categorical}} information. When most columns have categorical information, clustering <b>the</b> <b>data</b> <b>set</b> by similarity of categorical values can reveal interesting patterns in <b>the</b> <b>data</b> <b>set.</b> However, when <b>the</b> <b>data</b> <b>set</b> includes {{only a small}} number (one or two) of categorical columns, the categorical information is probably more useful as a way to partition <b>the</b> <b>data</b> <b>set.</b> For example, researchers might be interested in gene expression data for healthy vs. diseased patients or stock performance for common, preferred, or convertible shares. For these cases, we present a novel way to utilize the categorical information together with clustering algorithms. Instead of incorporating categorical information into the clustering process, we can partition <b>the</b> <b>data</b> <b>set</b> according to categorical information. Clustering is then performed with each subset to generate two or more clustering results, each of which i...|$|R
40|$|This paper {{describes}} a real-time <b>data</b> <b>set</b> for macroeconomists {{that can be}} used for a variety of purposes, including forecast evaluation. <b>The</b> <b>data</b> <b>set</b> consists of quarterly vintages, or snapshots, of <b>the</b> major macroeconomic <b>data</b> available at quarterly intervals in real time. The paper explains the construction of <b>the</b> <b>data</b> <b>set,</b> examines <b>the</b> properties of several of the variables in <b>the</b> <b>data</b> <b>set</b> across vintages, and provides an example showing how data revisions can affect forecasts. Forecasting...|$|R
50|$|To {{locate a}} <b>data</b> <b>set,</b> a program will {{generally}} interrogate a z/OS catalog {{to find the}} volume where <b>the</b> <b>data</b> <b>set</b> resides. Having found the correct volume, the VTOC is searched {{to find out where}} on <b>the</b> disk <b>the</b> <b>data</b> <b>set</b> is stored.|$|R
5000|$|This type of {{research}} was replicated by Marxian scholars in many other countries around the world, [...] who often introduced their own technical refinements in <b>the</b> <b>data</b> <b>sets.</b>|$|E
50|$|Here are graphs of {{the average}} monthly {{temperatures}} and the average monthly rainfall measurements in Songshan, Jilin. Both of <b>the</b> <b>data</b> <b>sets</b> taken for these graphs were taken in the years of 2000 to 2012.|$|E
50|$|In general, {{cladogram}} generation algorithms must {{be implemented}} as computer programs, although some algorithms {{can be performed}} manually when <b>the</b> <b>data</b> <b>sets</b> are trivial (for example, just a few species {{and a couple of}} characteristics).|$|E
5000|$|In particular, {{choosing}} [...] (larger {{than the}} maximum distance in <b>the</b> <b>data</b> <b>set)</b> is possible, but will obviously lead to quadratic complexity, since every neighborhood query will return <b>the</b> full <b>data</b> <b>set.</b> Even when no spatial index is available, this comes at additional cost {{in managing the}} heap. Therefore, [...] should be chosen appropriately for <b>the</b> <b>data</b> <b>set.</b>|$|R
30|$|One may {{conclude}} {{that the results of}} this study, which is similar to findings of Johnson and Wichern [23], indicate <b>the</b> <b>data</b> <b>set</b> with outliers have higher cophenetic correlation values than <b>the</b> <b>data</b> <b>set</b> without outliers.|$|R
40|$|Given only {{information}} {{in the form of}} similarity triplets "Object A is more similar to object B than to object C" about a <b>data</b> <b>set,</b> we propose two ways of defining a kernel function on <b>the</b> <b>data</b> <b>set.</b> While previous approaches construct a low-dimensional Euclidean embedding of <b>the</b> <b>data</b> <b>set</b> that reflects <b>the</b> given similarity triplets, we aim at defining kernel functions that correspond to high-dimensional embeddings. These kernel functions can subsequently be used to apply any kernel method to <b>the</b> <b>data</b> <b>set...</b>|$|R
5000|$|The input for <b>the</b> <b>data</b> <b>sets</b> used in {{the visual}} {{analytics}} process are heterogeneous data sources (i.e., the internet, newspapers, books, scientific experiments, expert systems). From these rich sources, <b>the</b> <b>data</b> <b>sets</b> S = S1, ..., Sm are chosen, whereas each Si , i ∈ (1, ..., m) consists of attributes Ai1, ..., Aik. The goal or output of the process is insight I. Insight is either directly obtained from the set of created visualizations V or through confirmation of hypotheses H as the results of automated analysis methods. This formalization of the visual analytics process is illustrated in the following figure. Arrows represent the transitions from one set to another one.|$|E
50|$|GSEA uses {{complicated}} statistics, so {{it requires}} a computer program to run the calculations. However, because GSEA has become standard practice in the last decade, there are many websites and downloadable programs that will provide <b>the</b> <b>data</b> <b>sets</b> and run the analysis.|$|E
50|$|Gelman et al. in Bayesian Data Analysis (2004) {{consider}} a data set relating to speed-of-light measurements made by Simon Newcomb. <b>The</b> <b>data</b> <b>sets</b> for that book {{can be found}} via the Classic data sets page, and the book's website contains {{more information on the}} data.|$|E
30|$|Use the k-means {{algorithm}} to partition <b>the</b> <b>data</b> <b>set</b> {{and train}} <b>the</b> HWT-SVR model with the weights of Step 5 and <b>the</b> corresponding <b>data</b> <b>set.</b>|$|R
30|$|<b>The</b> <b>data</b> <b>set</b> {{based on}} all plants per plot is useful for showing the {{relevance}} of pruning for commercial practice and <b>the</b> <b>data</b> <b>set</b> based on <b>the</b> plants with slips for understanding the effect of slip pruning per se.|$|R
40|$|This paper {{describes}} a prototype system for registering geologic <b>data</b> <b>sets</b> through ontologies {{to assist in}} integrating and querying heterogeneous geologic <b>data</b> <b>sets.</b> <b>The</b> system consists of three components: an ontology repository, <b>the</b> <b>data</b> <b>set</b> registration, and ontology-aware applications. User-defined ontologies in OWL are saved and used by <b>the</b> system. Each <b>data</b> <b>set</b> must be registered before it becomes available, and the registration semi-automatically generates a mapping from <b>data</b> <b>sets</b> to ontologies. <b>The</b> mapping between <b>data</b> <b>sets</b> and ontologies are used by applications to explore and extract information from <b>the</b> <b>data</b> <b>set...</b>|$|R
