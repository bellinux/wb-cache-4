4|35|Public
40|$|The paper {{describes}} the recent {{results of the}} Italian National PRIN Research Project ROAD-NGN (Optical frequency/wavelength division multiple access technique for next generation networks) regarding innovative technological solutions able to achieve a significant simplification compared to the NG-PON 2 TWDM approach, but increasing both the aggregated <b>transmitted</b> <b>throughput</b> and the data rate available for each user. Downlink and uplink solutions are shown and discusse...|$|E
40|$|M-health {{services}} {{are expected to}} become increas- ingly relevant {{in the management of}} emergency situations, enabling real-time support of remote medical experts. In this context, the transmission of health-related information from an ambulance to a remote hospital is a challenging task, due to the variability and the limitations of the mobile radio link. In particular, the transmission of multiple video streams can improve the efficacy of the tele-consultation service, but requires a large bandwidth to meet the desired quality, not always guaranteed by the mobile network. In this paper we propose a novel cross-layer adaptation strategy for multiple SVC videos delivered over a single LTE channel, which dynamically adjusts the overall <b>transmitted</b> <b>throughput</b> to meet the actual available bandwidth, while being able to provide high quality to diagnostic video sequences and lower (but fair) quality to less critical ambient videos. After having introduced a realistic LTE uplink scenario, including an advanced resource allocation strategy, we show through numerical simulations that the proposed solution is capable to achieve an optimal end-to-end video quality for both the diagnostic and the ambient videos...|$|E
40|$|M-health {{services}} {{are expected to}} become increasingly relevant {{in the management of}} emergency situations by enabling real-time support of remote medical experts. In this context, the transmission of multiple health-related video streams from an ambulance to a remote hospital can improve the efficacy of the teleconsultation service, but requires a large bandwidth to meet the desired quality, not always guaranteed by the mobile network. In order to deliver the multiple streams over a single bandwidth-limited wireless access channel, in this paper we propose a novel optimization framework that enables to classify the available video sources and to automatically select and adapt the best streams to transmit. The camera ranking algorithm jointly works with a cross-layer adaptation strategy for multiple scalable streams to achieve different objectives and/or tradeoffs in terms of number and target quality of the transmitted videos. The final goal of the optimization is to dynamically adjust the overall <b>transmitted</b> <b>throughput</b> to meet the actual available bandwidth, while being able to provide high quality to diagnostic video sequences and lower quality to less critical ambient videos. Numerical simulations considering a realistic emergency scenario with long term evolution advanced (LTE-A) connectivity show that the proposed content/context-aware solution is able to automatically select the best sources of information from a visual point of view and to achieve optimal end-to-end video quality for both the diagnostic and the ambient videos...|$|E
3000|$|... [...]. This should prevent {{secondary}} users from selfishly <b>transmitting</b> with a <b>throughput</b> {{higher than}} required, which {{would reduce the}} achievable throughputs for the other secondary users.|$|R
30|$|In the {{previous}} work, we show {{the improvement in}} the probability of correctly retrieving the <b>transmitted</b> packets (<b>throughput)</b> under the proposed spreading slotted Aloha applied for IEEE 802.15. 6 standard. We conclude that, in general, the proposed scheme achieved a better throughput comparing to the conventional slotted Aloha proposed in the standard. But the delay resulted due to the spreading code usage is always a tradeoff factor against the achieved throughput in the spreading schemes generally.|$|R
40|$|Lightweight IP (lwIP) is an {{open source}} TCP/IP {{networking}} stack for embedded systems. The Xilinx ® Software Development Kit (SDK) provides lwIP software customized to run on various Xilinx embedded systems that can be MicroBlaze ™ or ARM®-based Xilinx Zynq®- 7000 All Programmable (AP) SoC. The information in this application note applies to ARM-based Zynq- 7000 AP SoC systems. This document describes {{how to use the}} lwIP library to add networking capability to an embedded system. In particular, lwIP is utilized to develop these applications: echo server, web server, Trivial File Transfer Protocol (TFTP) server, and receive and <b>transmit</b> <b>throughput</b> tests. You can download the Reference Design Files for this application note from the Xilinx website. For detailed information about the design files, see Reference Design. Hardware and Software Requirements The hardware and software requirements are: • One Xilinx ZC- 702 board for Zynq- 7000 AP SoC-based systems. • Xilinx JTAG for Zynq- 7000 AP SoC-based systems • USB cable for RS 232 UART communication on the board • An ethernet cable connecting the board to a Windows or Linux host • Serial Communications Utility Program, such as HyperTerminal or Teraterm • Xilinx Vivado 2014. 2 for creating hardware design • Xilinx SDK 2014. 2 for running or making modifications to the softwar...|$|R
40|$|The {{constant}} {{evolution of}} the telecommunication technologies is one fundamental aspect that characterizes the modern era. In the context of healthcare and security, different scenarios are characterized {{by the presence of}} multiple sources of information that can support a large number of innovative services. For example, in emergency scenarios, reliable transmission of heterogeneous information (health conditions, ambient and diagnostic videos) can be a valid support for managing the first-aid operations. The presence of multiple sources of information requires a careful communication management, especially in case of limited transmission resource availability. The objective of my Ph. D. activity is to develop new optimization techniques for multimedia communications, considering emergency scenarios characterized by wireless connectivity. Different criteria are defined in order to prioritize the available heterogeneous information before transmission. The proposed solutions are based on the modern concept of content/context awareness: the transmission parameters are optimized taking into account the informative content of the data and the general context in which the information sources are located. To this purpose, novel cross-layer adaptation strategies are proposed for multiple SVC videos delivered over wireless channel. The objective is to optimize the resource allocation dynamically adjusting the overall <b>transmitted</b> <b>throughput</b> to meet the actual available bandwidth. After introducing a realistic camera network, some numerical results obtained with the proposed techniques are showed. In addition, through numerical simulations the benefits are showed, in terms of QoE, introduced by the proposed adaptive aggregation and transmission strategies applied in the context of emergency scenarios. The proposed solution is fully integrated in European research activities, including the FP 7 ICT project CONCERTO. To implement, validate and demonstrate the functionalities of the proposed solutions, extensive transmission simulation campaigns are performed. Hence, the presented solutions are integrated on a common system simulator which is been developed within the CONCERTO project...|$|E
50|$|In a {{point-to-point}} or {{point-to-multipoint communication}} link, where only one terminal is <b>transmitting,</b> the maximum <b>throughput</b> is often equivalent to or {{very near the}} physical data rate (the channel capacity), since the channel utilization can be almost 100% in such a network, except for a small inter-frame gap.|$|R
30|$|Throughput {{deriving}} from broadcast communication for the EP workload also grows {{as the number}} of nodes is augmented. In the single-hop WiNoC, this growth is up to 200 times higher than the injection rate. This behavior is explained by every node receiving every packet sent in the network. All of the routers in a single-hop WiNoC simultaneously receive a broadcast message when it is <b>transmitted.</b> Therefore, <b>throughput</b> is higher than in a multi-hop network for sizes starting from 64 nodes, considerably increasing with the addition of more nodes. For the scenarios simulating networks with 4 – 32 nodes, packet retransmissions make the multi-hop WiNoC have more throughput.|$|R
40|$|A multi-code Code {{division}} {{multiple access}} (CDMA) slotted ALOHA system with a Sub-packet channel load sensing protocol (SCLSP) is proposed to support multi-rate data services. In the SCLSP, each packet occupies two time slots, the channel load of the next time slot can therefore be computed by using the previous channel information. The channel load is used as the access control criterion by users that have packets to <b>transmit.</b> The <b>throughput</b> performance of the proposed system is evaluated and {{compared with that of}} the system using the random access scheme. The SCLSP improves the system performance by greatly enlarging the range of traffic load where good throughput performance is provided. link_to_subscribed_fulltex...|$|R
30|$|The CLC {{periodically}} collects {{the quality}} perceived by each <b>transmitting</b> node. <b>Throughput</b> and loss rates at the application layer of each node {{are used to}} decide based on the demand for transmission capacity and the proper sending rates of nodes to avoid overload. Nodes are assumed to respect their respective allowed sending rate. The sending rates impact the perceived quality and thereby the decisions for next period, whereby the proper load is iteratively found. It {{is defined as a}} centralized unit for the simulation-based evaluation. Later in this paper, we discuss however implementation alternatives such as locating the controller in a gateway node with permanent power and likely to have sufficient processing capabilities and memory.|$|R
40|$|Abstract — In this paper, {{we present}} a new {{energy-efficient}} bandwidth allocation scheme for wireless networks. First of all, we investigate the intrinsic relationship between the energy consumption and transmission rates of mobile terminals, in which transmission rate is determined through channel allocations. Then, we propose two schemes for connection admission control: Victim Selection Algorithm (VSA) and Beneficiary Selection Algorithm (BSA) {{with the intent to}} reduce energy consumption of each terminal. Moreover, we introduce an adjustment algorithm to statistically meet the demands for quality of service (QoS) during the resource allocation. The performance of the proposed schemes is evaluated with respect to energy consumption rate of each successfully <b>transmitted</b> bit, <b>throughput</b> and call blocking probabilities. An extensive analysis and simulation study is conducted for Poisson and self-similar, multi-class traffic. Index Terms — Wireless networks, energy consumption, and connection admission control. I...|$|R
40|$|IEEE 802. 15. 4 {{standard}} {{is widely used}} in wireless personal area networks (WPANs). The devices transmit data during two periods: contention access period (CAP) by accessing the channel using CSMA/CA and contention free period (CFP), which consists of guaranteed time slots (GTS) allocated to individual devices by the personal area network (PAN). However, the use of GTS slot size may lead to severe bandwidth wastage if  the traffic pattern is not fit or {{only a small portion}} of GTS slot is used by allocated device. The proposed scheme devides the GTS slot and then optimizes the GTS slot size by exploiting the value of superframe order (SO) information. The proposed scheme was tested through simulations and the results show that the new GTS allocation scheme perform better than the original IEEE 802. 15. 4 standard in terms of average <b>transmitted</b> packets, <b>throughput,</b> latency and probability of successful packets...|$|R
40|$|Abstract — Battery {{capacity}} of mobile terminals and radio bandwidth are both limited and precious resources in wireless networks. In this paper, {{we present a}} thorough performance study of energy-based admission control scheme {{to make the best}} use of these two resources for effective mobile communications. In order to reduce energy consumption of each terminal, we introduce a Victim Selection Algorithm (VSA) and a Beneficiary Selection Algorithm (BSA) for acquiring bandwidth and releasing bandwidth, respectively. To avoid potential compromise of quality of service (QoS) due to the concern of energy consumption in connection admission control, we further propose an adjustment algorithm to statistically meet the demands for QoS. The performance of the proposed schemes is evaluated with respect to energy consumption rate of each successfully <b>transmitted</b> bit, <b>throughput</b> and call blocking probabilities for a variety of traffic such as Poisson and self-similar, multi-class traffic. Key Words: Wireless networks, Quality of Service, energy consumption, and connection admission control. I...|$|R
40|$|The {{personal}} {{area network}} (PAN) coordinator can assign a guaranteed time slot (GTS) to allocate a particular duration for requested devices in IEEE  802. 15. 4 beacon-enabled mode. The main challenge in the GTS mechanism is how to let the PAN coordinator allocate time slot duration for the devices which request a GTS. If the allocated devices use the GTS partially or the traffic pattern is not suitable, wasted bandwidth will increase, which degrades the performance of the network. In order to overcome the abovementioned problem, this paper proposes the Partitioned GTS Allocation Scheme (PEGAS) for IEEE 802. 15. 4  networks. PEGAS aims to decide the precise moment for the starting time, the end, {{and the length of}} the GTS allocation for requested devices taking into account the values of the superframe order, superframe duration, data packet length, and arrival data packet rate. Our simulation results showed that the proposed mechanism outperforms the IEEE 802. 15. 4 standard in terms of the total number of <b>transmitted</b> packets, <b>throughput,</b> energy efficiency, latency, bandwidth utilization, and contention access period (CAP) length ratio. </p...|$|R
30|$|We present closed-form {{expressions}} of the average link throughput for sensor networks with a slotted ALOHA MAC protocol in Rayleigh fading channels. We compare networks with three regular topologies in terms of <b>throughput,</b> <b>transmit</b> efficiency, and transport capacity. In particular, for square lattice networks, we present a sensitivity analysis of the maximum throughput and the optimum transmit probability {{with respect to the}} signal-to-interference ratio threshold. For random networks with nodes distributed according to a two-dimensional Poisson point process, the average throughput is analytically characterized and numerically evaluated. It turns out that although regular networks have an only slightly higher average link throughput than random networks for the same link distance, regular topologies have a significant benefit when the end-to-end throughput in multihop connections is considered.|$|R
40|$|Abstract. According to SMART 2020, Internet and ICT (Information and {{communication}} technology) could reduce emissions by 15 percent in annual energy costs by 2020. One {{of the main}} methods to reduce the energy costs in communication system {{is to reduce the}} transmitting power of eNBs. In our study, we explore the problem of downlink energy saving for LTE/LTE-A network based on Orthogonal Frequency Division Multiplexing (OFDM). We propose an adaptive eNB transmitting power allocation scheme based on inter-eNB coordination. This power allocation scheme works in distributive way by being formulated as an evolutionary potential game. Numerical results prove that our proposed algorithm notably reduces the overall <b>transmitting</b> power, while <b>throughputs</b> of either overall or edge users are guaranteed at the same time. I...|$|R
40|$|We present closed-form {{expressions}} of the average link throughput for sensor networks with a slotted ALOHA MAC protocol in Rayleigh fading channels. We compare networks with three regular topologies in terms of <b>throughput,</b> <b>transmit</b> efficiency, and transport capacity. In particular, for square lattice networks, we present a sensitivity analysis of the maximum throughput and the optimum transmit probability {{with respect to the}} signal-to-interference ratio threshold. For random networks with nodes distributed according to a two-dimensional Poisson point process, the average throughput is analytically characterized and numerically evaluated. It turns out that although regular networks have an only slightly higher average link throughput than random networks for the same link distance, regular topologies have a significant benefit when the end-to-end throughput in multihop connections is considered. </p...|$|R
40|$|Abstract — In this paper, {{we present}} closed-form ex-pressions {{of the average}} per-node {{throughput}} for sensor networks with a slotted ALOHA MAC protocol in Rayleigh fading channels. We compare networks with three regular topologies in terms of per-node <b>throughput,</b> <b>transmit</b> efficiency, and transport capacity. In particular, for square lattice networks, we present {{an analysis of the}} dependence of the maximum <b>throughput</b> and optimum <b>transmit</b> proba-bility on the signal-to-interference-ratio threshold required for successful reception. For random networks with nodes distributed according to a two-dimensional Poisson point process, the average per-node throughput is analytically characterized and numerically evaluated. It turns out that although regular networks have an only slightly higher per-node throughput than random networks for the same link distance, regular topologies have a significant benefit when the end-to-end throughput in multihop connections is considered. I...|$|R
40|$|The primary {{issue in}} {{downlink}} beamforming for wireless commu-nications {{is how to}} balance the need for high received signal power for each user against the interference produced by the signal at other points in the network. In this paper. we describe several approaches to this, problem: channcl. inversion, regularized channel inversion, vector modulo pre-coding, channel block diagonalization, and coor-dinated transmiVreceivc beamforming. While the basic idea behind these algorithms i s the same, namely the use of channel information at the transmitter to predict and then counteract the interference pro-duced at cach node in the network, each of the algorithms is based on achieving a different performance objcctive. We compare the various gods ofthe above algorithms, and detail their respective ad-vantages and disadvantages in terms of computational complexity, required <b>transmit</b> power. network <b>throughput,</b> and assumed receive...|$|R
40|$|This paper {{presents}} a novel coding approach for both single and multi-hop wireless networks, which exploits the spacial diversity of wireless networks {{to reduce the}} total packets <b>transmitted</b> to improve <b>throughput.</b> For wireless LANs, access points perform XOR coding on outgoing packets according to which packets have been received or overheard. To find the optimal coding solution, we establish the equivalence of the coding problem and the graph coloring problem. For multi-hop networks, the crossing points {{of two or more}} flows perform XOR coding on the received packets before forwarding them to improve the usage of wireless channels. To deal with the case where crossing points are scarce, such as in a high dense mesh network, we implement a route fusion protocol to improve the chance of coding. We evaluate our approach by both simulations and experiments. The results show significant performance gain...|$|R
40|$|Abstract — Unifying and generalizing {{the work}} in [1] and [2], we analyze and {{optimize}} the performance of an adaptive trelliscoded modulation system where receive antenna diversity is implemented by means of maximum ratio combining. As in [1], the analysis {{is done in the}} presence of both estimation and prediction errors. Rayleigh fading on each subchannel is considered, with the estimation and prediction being performed independently on each subchannel. The system optimization process is done {{in such a way that}} the throughput is maximized under a bit error rate (BER) constraint. The numerical example employs a Jakes fading spectrum and shows how the power should be distributed between pilot and data symbols, and how often pilot symbols should be <b>transmitted</b> for maximum <b>throughput</b> under an instantaneous (with respect to the predicted channel) BER constraint. Index Terms — Adaptive coded modulation, PSAM, estimation and prediction error, optimal pilot spacing, optimal power allocation, antenna diversity, and maximum ratio combining. I...|$|R
40|$|With {{the growth}} of {{wireless}} personal communications networks and wireless Local Area Networks (LANs), the need for increased reliability of the radio link has become evident. The use of diversity techniques, such as dual receiving antennas, helps mitigate the effect of multipath fading in both the in-building and land mobile radio environments. A significant issue {{in the design of}} such systems {{is the degree to which}} correlation between the two or more diversity signals can be tolerated. In this paper, we consider the use of diversity techniques in radio systems that are subject to correlation, Rayleigh fading, log-normal shadowing and the radio capture effect. In the presence of two simultaneously <b>transmitting</b> stations, the <b>throughput,</b> conditioned on the local-mean power, is determined exactly for the case of a dual diversity receiving station. The insight gained from the two-station analysis is used to develop an accurate approximation for cases with more than two stations. The degree [...] ...|$|R
40|$|In {{traditional}} {{cognitive radio}} networks, secondary users (SUs) typically access {{the spectrum of}} primary users (PUs) by a two-stage "listen-before-talk" (LBT) protocol, i. e., SUs sense the spectrum holes {{in the first stage}} before transmitting in the second. However, there exist two major problems: 1) transmission time reduction due to sensing, and 2) sensing accuracy impairment due to data transmission. In this paper, we propose a "listen-and-talk" (LAT) protocol with the help of full-duplex (FD) technique that allows SUs to simultaneously sense and access the vacant spectrum. Spectrum utilization performance is carefully analyzed, with the closed-form spectrum waste ratio and collision ratio with the PU provided. Also, regarding the secondary throughput, we report the existence of a tradeoff between the secondary <b>transmit</b> power and <b>throughput.</b> Based on the power-throughput tradeoff, we derive the analytical local optimal transmit power for SUs to achieve both high throughput and satisfying sensing accuracy. Numerical results are given to verify the proposed protocol and the theoretical results...|$|R
40|$|We {{formulate}} {{the resource}} allocation {{problem for the}} uplink of {{code division multiple access}} (CDMA) networks using a game theoretic framework, propose an efficient and distributed algorithm for a joint rate and power allocation, and show that the proposed algorithm converges to the unique Nash equilibrium (NE) of the game. Our choice for the utility function enables each user to adapt its <b>transmit</b> power and <b>throughput</b> to its channel. Due to users' selfish behavior, the output of the game (its NE) may not be a desirable one. To avoid such cases, we use pricing to control each user's behavior, and analytically show that similar to the no-pricing case, our pricing-based algorithm converges to the unique NE of the game, at which, each user achieves its target signal-to-interference-plus-noise ratio (SINR). We also extend our distributed resource allocation scheme to multi-cell environments for base station assignment. Simulation results confirm that our algorithm is computationally efficient and its signalling overhead is low. In particular, we will show that in addition to its ability to attain the required QoS of users, our scheme achieves better fairness in allocating resources and can significantly reduce transmit power as compared to existing schemes...|$|R
40|$|<b>Transmit</b> {{beamforming}} increases <b>throughput</b> {{and transmission}} {{range of a}} wireless communication system. However, the required feedback of channel state information (CSI) consumes radio resources that otherwise {{can be used for}} data transmission. This makes "Feedback or no feedback?" a relevant question to ask. This paper answers this question by proposing intelligent feedback control using a Markov decision process. The feedback controller turns feedback on/off according to the channel state and the criterion of maximum net throughput, namely throughput minus average feedback cost. Assuming channel isotropicity and Markovity, the state of the feedback controller reduces to two channel parameters. This allows the optimal control policy to be efficiently computed using dynamic programming. The optimal control policy is proved to be of the threshold type. Under this policy, feedback is performed whenever a channel parameter indicating the accuracy of transmit CSI is below a threshold, which varies with channel power. The above result holds regardless of whether the controller's state space is discretized or continuous. Simulation shows that feedback control increases net throughput by up to 0. 5 bit/s/Hz without requiring additional bandwidth or antennas. © 2010 IEEE. link_to_subscribed_fulltex...|$|R
30|$|For a given {{conflict}} graph, throughput of a node {{refers to}} the fraction of time that the node <b>transmits,</b> and the <b>throughput</b> region of the conflict graph {{refers to the}} collection of achievable per-node throughputs. In this study, we are mainly interested in {{how much of the}} throughput region can be achieved within the acceptable limits of short-term fairness. We define this subset of the throughput region as short-term fair capacity region. In order to quantify the short-term fair capacity, a short-term fairness horizon threshold has to be determined such that the network is considered short-term unfair when the short-term fairness horizon is beyond this threshold. In a study which is focused on developing a fair MAC protocol[30], the authors observed that it takes 80 – 140 packets per user for the IEEE 802.11 standard to become fair. Considering this result, we select 100 transmissions per node as a threshold for short-term fairness. We also used 50 transmissions per node as another threshold which corresponds to a stricter fairness requirement. However, these choices are not restrictive; the behavior of the capacity region does not significantly change with the selection of the threshold as it will be demonstrated in Section 6.|$|R
30|$|According to the {{exhibited}} analysis {{carried on}} EDC mechanism within the previous Subsection, {{it is evident}} that latency on different RT flows can be minimized by enforcing efficient priority rules on various service types. As a matter of fact, due to traffic variability, guaranteeing low delay while achieving a good level of data rate is realized with a certain level of tradeoff. Therefore, in scenarios where burst RT traffic like the video is involved, it is valuable to allow flows that are able to enhance the system throughput to be scheduled {{as long as they do}} not severely deteriorate the overall delay. In this essence, the LDI mechanism is proposed to grant flows aborted by EDC algorithm a chance of being scheduled on a different basis. It is important to highlight that, the attained flows by LDI mechanism may have relatively high buffer delay values comparing with those selected by EDC. Nevertheless, some of these flows have a good channel quality hence they can be <b>transmitted</b> with high <b>throughput.</b> The significance of LDI is to improve the QoS of RT and NRT flows relaying on the intuition that data rate is a major characteristic for all flow types. Therein, the following remark is availed of as a principle scheduling decision in LDI.|$|R
30|$|On {{the basis}} of the {{simulated}} PU and CR network, we assign frequencies to each of the requesting SUs, using centralized and distributed FA algorithms under investigation. As a benchmark algorithm, we use the centralized and distributed version of Collaborative Max Sum Reward (CSUM) algorithm presented in [13], with a difference of assigning only one frequency per CR. As the performance of graph coloring depends on the conflict graph topology, we test algorithms on a large number of different networks for each setup of the input parameters, in order to have network neutral performance results. Each deployment of the PUs and SUs produces different network topology and a different corresponding conflict graph. All simulations are repeated 500 times with the selected set of input parameters. In each iteration, the new set of PUs and CR Tx-Rx pairs are generated, frequencies are assigned to the PUs, <b>transmit</b> power and <b>throughput</b> are calculated for all SUs, communication and conflict graph is constructed with corresponding interference weighting and categorization. After that, the process of assigning frequencies to all SUs is performed with the proposed algorithms and the experimental performance of the algorithms is compared to the benchmark algorithms. The results are averaged to reduce the influence of the network topology or simulations variance on the network performance and presented results.|$|R
40|$|Abstract—In {{wireless}} networks, throughput optimization is {{an essential}} performance objective that cannot be adequately characterized by a single criterion (such as the minimum <b>transmitted</b> or sum-delivered <b>throughput)</b> and should be specified over all source–destination pairs as a rate region. For a simple and yet fundamental model of tandem networks, a cross-layer optimization framework is formulated to derive the maximum throughput region for saturated multicast traffic. The contents of network flows are specified through network coding (or plain routing) in network layer and the throughput rates are jointly optimized in medium access control layer over fixed set of conflict-free transmission schedules (or optimized over transmission probabilities in random access). If the network model incorporates bursty sources and allows packet queues to empty, {{the objective is to}} specify the stability region as the set of maximum throughput rates that can be sustained with finite packet delay. Dynamic queue management strategies are used to expand the stability region toward the maximum throughput region. Network coding improves throughput rates over plain routing and achieves the largest gains for broadcast communication and intermediate network sizes. Throughput optimization imposes fundamental tradeoffs with transmission and processing energy costs such that the throughput-optimal operation is not necessarily energy efficient. Index Terms—Cross-layer design, energy efficiency, medium access control, network coding, network stability, routing, throughput optimization, wireless networks. I...|$|R
40|$|Channel {{access delay}} in a {{wireless}} adhoc network {{is the major}} source of delay while considering the total end to-end delay. Channel access delays experienced by different relay nodes are different in multi-hop adhoc network scenario. These delays in multi-hop network are analysed in the literature assuming channel access delays are independent and are of same magnitude at all the nodes in the network. In this work, the end to-end delay in a multi-hop adhoc network is analysed taking into account the silent relay nodes. Along with silent relay node effect, Channel access probability (p), transmission radius (r) analogous to <b>transmit</b> power, network <b>throughput</b> and density of nodes arête other factors considered for the end-to-end delay analysis. Effect of network parameters along with silent relay nodes on end-to-end delay is found to be considerably high compared to the previous literature results. Given a bound on end-to-end delay with percentage of silent relay nodes, throughput, node density requirements for a multi-hop adhoc network, optimal ranges of transmission radius and channel access probability can be obtained from the proposed analysis. End-to-end delay increases with silent relay nodes along with transmission radius(r), channel access probability(p), node density and throughput. It is clear from the analysis, that the effect of silent relay nodes on end to-end delay cannot be ignored to maintain certain Quality of service (QoS) metrics for the multi-hop wireless adhoc networ...|$|R
40|$|Abstract — A Wireless Mobile Ad-hoc Network {{consists}} {{of a number of}} mobile nodes that temporally form a dynamic infrastructure less network. To enable communication between nodes that do not have direct radio contact, each node must function as a wireless router and potential forward data traffic of behalf of the other. Detecting malicious nodes (Attacker) in an open ad hoc network in which participating nodes have no previous security associations presents a number of challenges not faced by traditional wired networks. Traffic monitoring in wired networks is usually performed at switches, routers and gateways, but an ad hoc network does not have these types of network elements where the Intrusion Detection System (IDS) can collect and analyze audit data for the entire network. So according to that above definition we conclude MANET is distributed nature and can’t trust to any of the mobile devices because we cannot manage the every time of topology changes on the network. This is very big challenge. So that particular point we create the trust based routing against the black hole attack in AOMDV (Ad-hoc on demand Multipath Distance Vector) routing scheme case. And analyze the behaviour of AOMDV routing time, Black Hole time and AOMDV-IDS time from following parameter like UDP packet loss, Receives, and <b>Transmit,</b> TCP analysis, <b>Throughput,</b> Routing load and packet delivery fraction based. All the simulation done through theNS- 2 Simulator and tested the mobile ad-hoc networ...|$|R
30|$|There {{have been}} a few works {{conducted}} on the performance of dual-hop cooperative networks under the impact of co-channel interference (CCI). The work in [8] focused on the end-to-end performance of multi-user amplify-and-forward (AF) cooperative networks and showed that CCI seriously impairs the relay-destination connection. In the case of average <b>throughput,</b> <b>transmitting</b> directly and over AF relaying systems by adjusting the number of interfering relays and the target signal-to-noise ratio (SNR) generates substantial gains. Regarding AF schemes, the impact of multi-user interference was carefully considered in case interference does not exist, in which according to asymptotic analysis, the diversity gain of the network is limited by interference [9]. The authors in [10] investigated opportunistic relaying under the impact of interference and thermal noise in case channel sensing is operated over slow fading environments. In [11], a dual-hop relay fading network was evaluated in terms of the performance of outage probability of AF and decode-and-forward (DF) relaying schemes in environments where less interference is expected, in which CCIs affect negatively the destination node when the relay node suffers from an additive white Gaussian noise (AWGN). Furthermore, the authors in [12] put forward an interference-aidede EH scheme for DF relaying networks, in which energy is harvested by the relay from the received signal and CCI signals; after that, the received information signal is decoded before being forwarded to the destination. In [13], the performance of multi-antenna two-way relay networks was considered, in which AF and DF relaying strategies wer systemically analysed, and they proposed an antenna selection scheme by optimizing the received SNR.|$|R
40|$|Approved {{for public}} release, {{distribution}} unlimitedRecently developed high speed networks {{are capable of}} transmitting data at rates of 100 Mbps or more. One such network protocol is Fiber Distributed Data Interface (FOOl). This network has a physical. transmissiop rate of 100 Mbps. Analytical and simulation {{studies have shown that}} t:le FOOl protocol should provide actual throughput of HO% to 95 % of this physical rate. Can the end user expect to see this kind of performance? If not, then what kind of throughput can actually be expected and where are the bottle necks? To answer these and other related questions, two areas were studied: First, a perfomance parison between a 40 MHz SPARCstation 10 workstation and a 50 MHz SPARCstation 10 workstation was conducted using the Neal Nelson commercial benchmark tool. Next, a well-known network measurement tool, ttcp, was used to obtain data transfer rates while varying several tunable operating system and network parcll 11 eters. The parameters varied were: Target Token Rotation Time, TCP/IP window size, NFS asynchronous threads, Logical Link buffer siu and Maximum Transfer Unit size. The results from the commercial benchmar~ analyis were used to determine if there are any differences which can affect transfer rates between the two workstations. The results from the conunercial benchmark tool clearly showed that the newer, higher speed processor is faster. The network tool ttcp showed that the TCP/IP window size had the largest impact on throughput performance. Throughput more than doubles from a window size of 4 k to a window size of 20 k. This is followed by having more than one workstation transmitting data simultaneously. Having two workstations <b>transmitting</b> nearly halves <b>throughput</b> This is followed by having a faster processor. A measurement of flle transfers using rep system calls showed that the largest impact on file transfer speed is the overhead of receiving the transferred file. U. S. Army (USA) autho...|$|R
40|$|Deficit Round Robin (DRR) {{is a fair}} packet-based {{scheduling}} discipline {{commonly used}} in wired networks where link capacities do not change with time. However, in wireless networks, especially wireless broadband networks, i. e., IEEE 802. 16 e Mobile WiMAX, {{there are two main}} considerations violate the packet-based service concept for DRR. First, the resources are allocated per Mobile WiMAX frame. To achieve full frame utilization, Mobile WiMAX allows packets to be fragmented. Second, due to a high variation in wireless channel conditions, the link/channel capacity can change over time and location. Therefore, we introduce a Deficit Round Robin with Fragmentation (DRRF) to allocate resources per Mobile WiMAX frame in a fair manner by allowing for varying link capacity and for transmitting fragmented packets. Similar to DRR and Generalized Processor Sharing (GPS), DRRF achieves perfect fairness. DRRF results in a higher throughput than DRR (80 % improvement) while causing less overhead than GPS (8 times less than GPS). In addition, in Mobile WiMAX, the quality of service (QoS) offered by service providers is associated with the price paid. This is similar to a cellular phone system; the users may be required to pay air-time charges. Hence, we have also formalized a Generalized Weighted Fairness (GWF) criterion which equalizes a weighted sum of service time units or slots, called temporal fairness, and <b>transmitted</b> bytes, called <b>throughput</b> fairness, for customers who are located in a poor channel condition or at a further distance versus for those who are near the base stations, or have a good channel condition. We use DRRF to demonstrate the application of GWF. These fairness criteria are used to satisfy basic requirements for resource allocation, especially for non real-time traffic. Therefore, we also extend DRRF to support other QoS requirements, such as minimum reserved traffic rate, maximum sustained traffic rate, and traffic priority. For real-time traffic, i. e., video traffic, we compare the performance of DRRF with deadline enforcement to that of Earliest Deadline First (EDF). The results show that DRRF outperforms EDF (higher achievable throughput under the promised delay latency) and maintains fairness under an overload scenario...|$|R
40|$|In this thesis, we {{investigate}} several relaying strategies for cooperative networks {{with the aim}} of finding techniques to improve the performance of such networks. The objective here is to increase the spectral efficiency while achieving full diversity. Therefore, we focus on two-way relaying and relay assignment since they are both efficient ways in improving the spectral efficiency of cooperative networks. Specifically,we propose efficient relay strategies to cope with the asymmetric data rates in two-way relay channels and address practical issues in relay assignment. In {{the first part of the}} thesis, we consider two decode-and-forward (DF) relaying schemes for two-way relaying channels where the two sources may have different rate requirements. One scheme combines hierarchical zero padding and network coding (HZPNC) at the relay. The novelty of this scheme lies in the way the two signals (that have different lengths) are network-coded at the relay. The other scheme is referred to as opportunistic user selection (OUS) where the user with a better end-to-end channel quality is given priority for transmission. We analyze both schemes where we derive closed form expressions for the end-to-end(E 2 E) bit error rate (BER). Since the two schemes offer a trade-off between performance and throughput, we analyze and compare both schemes in terms of channel access probability and average throughput. We show that HZPNC offers better throughput and fairness for both users, whereas OUS offers better performance. We also compare the performance of HZPNC with existing schemes including the original zero padding, nesting constellation modulation and superposition modulation. We demonstrate through examples the superiority of the proposed HZPNC scheme in terms of performance and/or reduced complexity. In the second part of the thesis, we consider a hybrid relaying scheme for two-way relay channels. As per the proposed scheme, if the E 2 E signal-to-noise ratio (SNR) of both users is above a specified threshold, both sources transmit over orthogonal channels and the relay node uses hierarchical modulation and network coding to relay the combined signals to both sources in the third time slot. Otherwise, the user with the better E 2 E SNR transmits, while the other user remains silent. The advantage of the proposed scheme is that it compromises between throughput and reliability. That is, when both users <b>transmit,</b> the <b>throughput</b> improves. Whereas when the better user transmits, multiuser diversity is achieved. Assuming asymmetric channels, we derive exact closed-form expressions for the E 2 E BER, access probability and throughput for this scheme and compare its performance to that of existing schemes. We also investigate the asymptotic performance of the proposed scheme at high SNRs where we derive the achievable diversity order of both users. We show through analytically and simulation results that the proposed scheme improves 1) the overall system throughput, 2) fairness between the two users, and 3) the transmission reliability. This all comes while achieving diversity two for both users, which is the maximal diversity. In the third part of the thesis, we study relay assignment with limited feedback. In networks with many multiple source-destination pairs, it is normally diffcult for destinations to acquire the channel state information (CSI) of the entire network without feedback. To this end, we design a practical limited feedback strategy in conjunction with two relay assignment schemes, i. e., fullset selection and subset selection, which are based on maximizing the minimum E 2 E SNR among all pairs. In this strategy, each destination acquires its SNR,quantizes it, and feeds it back to the relays. The relays then construct the E 2 E SNR table and select the relay assignment permutation from all possible relay assignment permutations or only a subset of these permutations. We analyze the performance of these schemes over independent Rayleigh fading channels in terms of the worst E 2 E SNR. We derive closed-form expressions for the E 2 E BER and investigate the asymptotic performance at high SNR. We show that relay assignment with quantized CSI can achieve the same first-order diversity as that of the full CSI case, but there is a second-order diversity loss. We also demonstrate that increasing the quantization levels yields performance that is close to that of having full knowledge of the CSI...|$|R
