9|19|Public
5000|$|An {{application}} optimization solution, {{the role}} of distributed top-of-rack (DToR) is to expedite traffic flows between servers across multiple racks, as opposed to typical <b>ToR</b> <b>switch</b> deployments that direct these flows through multiple networking tiers. DToR switches traffic onto dedicated high-speed switch interconnects, avoiding aggregation or Core Switch tiers and reducing server-to-server latency, and improving application performance. DToR is supported on the Virtual Service Platform 7000 Series, and is deployable in two modes: Stack-mode DToR that leverages the FAST protocol from the Stackable Chassis technology to provide scalability up to 8 units, with a virtual backplane capability of 5.12 Tbit/s, and Fabric-mode DToR that leverages the SPB protocol from the Fabric Connect technology to the scale up to 500 units with a bandwidth of up to 280 Tbit/s. Thus, Stack-mode DToR supports up to 256 ports of 10 gigabit Ethernet in a single logical domain, while Fabric-mode DToR supports up to 16,000 ports. [...] DToR {{can be used in}} a standalone mode, in conjunction with the Stackable Chassis technology, and also used at the Server Access method for an implementation of Fabric Connect.|$|E
30|$|Incast {{congestion}} sets {{in early}} when all senders and receiver {{are connected to}} the same <b>TOR</b> <b>switch</b> as compared to the scenario when all senders are not connected to the same <b>TOR</b> <b>switch</b> as reflected in Fig.  7. Incast congestion is more serious for within rack network as compared to out of rack traffic. Figure  7 also shows that incast congestion sets in early in three-tier topology than in fat-tree topology. Similar results are obtained for outcast congestion. Hence incast and outcast concerns are serious in three-tier network topology especially for applications with flows within a <b>TOR</b> <b>switch.</b> In fat-tree topology incast and outcast congestion affects throughput more profoundly when out of rack flows increase.|$|E
30|$|The {{optimization}} problem also includes communication network bandwidth constraints to prevent traffic congestion. We assume that, {{there is no}} communication congestion between the servers located in the same rack because they are connected to their <b>ToR</b> <b>switch</b> with high capacity links. The communications congestion may occur either in the (TORS-CS) links or in PoD links (CS-CS). We assume that a <b>ToR</b> <b>switch</b> will be turned off if none of the servers in that rack are being utilized. Similarly CS in a PoD will be turned off if all the servers connected to its racks are off. We note that an on switch consumes a constant power plus load dependent variable power; the former will {{be referred to as}} static and the latter as dynamic power respectively. We will let PS_ℓ, e^ToRS,PS_ℓ^CS denote static power consumption of a <b>ToR</b> <b>switch</b> on the e’th rack of PoD ℓ, and CS switch in PoD ℓ respectively. Similarly, we will let PD_ℓ, e^ToRS,PD_ℓ^CS denote dynamic power consumption of these switches for per bit transmission rate. We also let PWNIC denote the dynamic power consumption at the network interface card (NIC) of a server for per bit transmission rate.|$|E
30|$|We {{simulate}} the topologies discussed in “Data center network architectures” section. The three-tier DCN topology that we simulate comprises {{of the core}} layer, aggregation layer and edge layer such that 48 -port 1 GigE <b>TOR</b> <b>switches</b> and 128 -port 10 GigE aggregation and core layer switches are used. Total number of TOR servers is 4096. The links between servers and <b>TOR</b> <b>switches</b> are 1 Gbps Ethernet links while all upper links are 10 Gbps Ethernet. The fat-tree DCN topology is simulated such that 48 -port 1 GigE switches and 1 Gbps Ethernet links are used at all layers.|$|R
50|$|The 5500 {{series are}} often used as Top Of Rack (<b>TOR)</b> <b>switches</b> and client access-switches in wiring {{cabinets}} in offices or campus networks. The 5500P series are mainly client access switches connecting VOIP phones and (daisy chained or directly connected) workstations. The -P series are also used to power other devices than phones, such as WiFi Access-points, IP camera's or thin clients.|$|R
50|$|From a {{datacenter}} view, {{the network}} {{starts at the}} rack level, where 19-inch racks are custom-made and contain 40 to 80 servers (20 to 40 1U servers on either side, while new servers are 2U rackmount systems. Each rack has a switch). Servers are connected via a 1 Gbit/s Ethernet link {{to the top of}} rack <b>switch</b> (<b>TOR).</b> <b>TOR</b> <b>switches</b> are then connected to a gigabit cluster switch using multiple gigabit or ten gigabit uplinks. The cluster switches themselves are interconnected and form the datacenter interconnect fabric (most likely using a dragonfly design rather than a classic butterfly or flattened butterfly layout).|$|R
30|$|As {{mentioned}} earlier, LP problem (solvable in polynomial time) {{has less}} complexity compared to ILP problem (NP-hard optimization problem). In the CG solution of our optimization problem, RMP has been formulated as a LP and pricing problems as ILP type. As a result, {{we need to}} determine the optimal ILP solution of the RMP after {{the solution of the}} relaxed LP. Typically, this is done through the branch and bound algorithm [18], which is time consuming, as a result, we propose a heuristic method that satisfies the scheduling time constraint [28, 29]. The proposed method will round up and down the values of the scheduling variables, m_ℓ, e^j_t, of the LP solution [30, 31]. This operation will be carried out after m_ℓ, e^j_t have been sorted according to their priorities. m_ℓ, e^j_ts more likely to be rounded down will be given higher priority. Following this operation, it is possible that all the servers of a rack will become inactive in which case <b>TOR</b> <b>switch</b> serving to that rack will be turned off to save power.|$|E
40|$|This paper reports an FPGA-based {{switch and}} {{interface}} card (SIC) and its application scenario in an all-optical, programmable disaggregated data center network (DCN). Our novel SIC is designed and implemented to replace traditional optical network interface cards, {{plugged into the}} server directly, supporting optical packet switching (OPS) /optical circuit switching (OCS) or time division multiplexing (TDM) /wavelength division multiplexing (WDM) traffic on demand. Placing the SIC in each server/blade, we eliminate electronics {{from the top of}} rack (<b>ToR)</b> <b>switch</b> by pushing all the functionality on each blade while enabling direct intrarack blade-to-blade communication to deliver ultralow chip-to-chip latency. We demonstrate the disaggregated DCN architecture scenarios along with all-optical dimension-programmable N × M spectrum selective Switches (SSS) and an architecture-on-demand (AoD) optical backplane. OPS and OCS complement each other as do TDM and WDM, which can support variable traffic flows. A flat disaggregated DCN architecture is realized by connecting the optical ToR switches directly to either an optical top of cluster switch or the intracluster AoD optical backplane, while clusters are further interconnected to an intercluster AoD for scaling out...|$|E
40|$|Recent trends to pack {{data centers}} with more CPUs per rack {{have led to}} a {{scenario}} in which each individual rack may contain hundreds, or even thousands, of compute nodes us-ing system-on-chip (SoC) architectures. At this increased scale, traditional rack-level star topologies with a top-of-rack (<b>ToR)</b> <b>switch</b> as the hub and servers as the leaves are no longer feasible in terms of monetary cost, physical space, and oversubscription. We propose Theia, an architecture to connect hundreds of SoC nodes within a rack, using inex-pensive, low-latency, hardware elements to group the rack’s servers into subsets which we term SubRacks. We then re-place the traditional per-rack ToR with a low-latency, pas-sive, circuit-style patch panel that interconnects these Sub-Racks. We explore alternatives for the rack-level topology implemented by this patch panel, and we consider approaches for interconnecting racks within a data center. Finally, we in-vestigate options for routing over these new topologies. Our proposal of Theia is unique in that it offers the flexibility of a packet-switched networking over a fixed circuit topology...|$|E
40|$|Virtualization {{has been}} an {{efficient}} method to fully utilize computing resources such as servers. The way of placing virtual machines (VMs) among a large pool of servers greatly affects the performance of data center networks (DCNs). As network resources have become a main bottleneck {{of the performance of}} DCNs, we concentrate on VM placement with Traffic-Aware Balancing to evenly utilize the links in DCNs. In this paper, we first proposed a Virtual Machine Placement Problem with Traffic-Aware Balancing (VMPPTB) and then proved it to be NP-hard and designed a Longest Processing Time Based Placement algorithm (LPTBP algorithm) to solve it. To take advantage of the communication locality, we proposed Locality-Aware Virtual Machine Placement Problem with Traffic-Aware Balancing (LVMPPTB), which is a multiobjective optimization problem of simultaneously minimizing the maximum number of VM partitions of requests and minimizing the maximum bandwidth occupancy on uplinks of Top of Rack (<b>ToR)</b> <b>switches.</b> We also proved it to be NP-hard and designed a heuristic algorithm (Least-Load First Based Placement algorithm, LLBP algorithm) to solve it. Through extensive simulations, the proposed heuristic algorithm is proven to significantly balance the bandwidth occupancy on uplinks of <b>ToR</b> <b>switches,</b> while keeping the number of VM partitions of each request small enough...|$|R
30|$|Table  6 {{presents}} the performance {{characteristics of the}} chosen switches for the communications network. Power consumption parameter values of the switches, PDℓ, e and PSℓ, e, {{are the same as}} given in [36 – 38]. We also assume that dynamic power consumption of a NIC is given by PWNIC[*]=[*] 0.6  microW. <b>ToR</b> <b>switches</b> offer a combination of internal (int) and external (ext) interfaces. The internal interfaces connect to NIC of the blade-servers while the external interfaces connect to Core switches. It is assumed that internal and external interfaces support up to 10 Gbps and 40 Gbps transmission rates respectively.|$|R
40|$|Data center traffic {{characteristics}} {{are not well}} understood. In particular, {{it is not clear}} how the prevalent traffic patterns may impact candidate mechanisms for managing traffic inside the data centers. In this paper, we conduct a measurement study of network-level traffic patterns inside data centers. Based on our empirical insights, we design a traffic generator for creating representative workloads for traffic between <b>TOR</b> <b>switches</b> in a data center. We use this generator to evaluate several traffic engineering techniques and data center network architectures, and analyze their short comings. Our findings highlight the need for fine-grained traffic engineering (TE) mechanisms. We design and implement such an approach using OpenFlow and show how it can significantly improve data center TE. ...|$|R
40|$|The {{potential}} advantages of optics at high link speeds {{have led to}} significant interest in deploying optical switching technology in data-center networks. Initial efforts have focused on hybrid approaches that rely on millisecond-scale circuit switching {{in the core of}} the network, while maintaining the flexibility of electrical packet switching at the edge. Recent demonstrations of microsecond-scale optical circuit switches motivate con-sidering circuit switching for more dynamic traffic such as that generated from a top-of-rack (<b>ToR)</b> <b>switch.</b> Based on these technology trends, we propose a prototype hy-brid ToR, called REACToR, which utilizes a combina-tion of packet switching and circuit switching to appear to end-hosts as a packet-switched ToR. In this paper, we describe a prototype REACToR con-trol plane which synchronizes end host transmissions with end-to-end circuit assignments. This control plane can react to rapid, bursty changes in the traffic from end hosts on a time scale of 100 s of microseconds, sev-eral orders of magnitude faster than previous hybrid ap-proaches. Using the experimental data from a system of eight end hosts, we calibrate a hybrid network simula-tor and use this simulator to predict the performance of larger-scale hybrid networks. ...|$|E
40|$|Scaling the {{capacity}} while maintaining low latency and power consumption {{is a challenge}} for hierarchical data center networks (DCNs) based on electrical switches. In this work we present a novel all-optical flat DCN architecture OPSquare that potentially addresses the scaling issues by employing parallel intra-/inter-cluster switching networks, distributed fast WDM optical cross-connect (OXC) switches, and a novel top-of-rack (<b>ToR)</b> <b>switch</b> architecture. The fast (nanoseconds) WDM OXC switches allow flexible switching capability in both wavelength and time domains and statistical multiplexing. The OPSquare DCN performance targeting Petabit/s capacity has been thoroughly assessed. First the packet loss, latency, throughput, and scalability are numerically investigated under realistic data center traffic model. Results indicate that when scaling the DCN size up to 1024 ToR switches, a packet loss ratio below 10 - 6 and a server end-to-end latency lower than 2 μs can be guaranteed at load of 0. 3 with limited 20 kB buffer. Then, the experimental evaluation of the DCN by employing 4 × 4 OXC prototypes shows multi-path dynamic switching with flow control operation. The case deploying 32 × 32 and 64 × 64 OXC switches connecting 1024 and 4096 ToRs are emulated and limited performance degradation has been observed. The potential of switching higher-order modulation and waveband signals further proves the suitability of OPSquare architecture for Petabit/s and low-latency DCN by using optical switches with moderate radix...|$|E
40|$|The {{potential}} advantages of optics at high link speeds {{have led to}} significant interest in deploying optical switching technology in data-center networks. Initial efforts have focused on hybrid approaches that rely on millisecond-scale circuit switching {{in the core of}} the network, while maintaining the flexibility of electrical packet switching at the edge. Recent demonstrations of microsecond-scale optical circuit switches motivate considering circuit switching for more dynamic traffic such as that generated from a top-of-rack (<b>ToR)</b> <b>switch.</b> Based on these technology trends, this dissertation presents a prototype hybrid ToR design called REACToR. REACToR combines 10 -Gbps packet switching and 100 - Gbps circuit switching, and appears to end-hosts as a 100 -Gbps packet-switched ToR. REACToR synchronizes end host transmissions with end-to-end circuit assignments, and can react to rapid, bursty changes in the traffic from end hosts on a time scale of 100 s of microseconds. To service data center traffic demands effectively, REACToR needs to schedule the heavy bandwidth-hungry flows to the circuit switching network, and the small latency-sensitive flows to the packet switching network. To address this problem, this dissertation also presents a new switch scheduling algorithm called Solstice. Solstice minimizes the frequency of circuit reconfigurations to maximize circuit utilization when reconfiguration delay is not negligible. Evaluations also show that it can schedule data center traffic workloads effectively with practical computational overheads. As a result, when using a REACToR hybrid switch with the Solstice scheduling algorithm, optical circuit switching extends to layers even closer to end hosts. Combined with a lower-provisioned electrical packet switch, the hybrid architecture services data center workloads with almost full bi-sectional bandwidth of high link rates like 100 Gb/s. It provides network performance comparable to a full fat-tree that consists of electrical packet switches, but with much lower cost...|$|E
40|$|As network demand increases, {{data center}} network {{operators}} face {{a number of}} challenges including the need to add capac-ity to the network. Unfortunately, network upgrades can be an expensive proposition, particularly {{at the edge of}} the net-work where most of the network’s cost lies. This paper presents a quantitative study of alternative ways of wiring multiple server links into a data center network. In it, we propose and evaluate Subways, a new approach to wiring servers and Top-of-Rack (<b>ToR)</b> <b>switches</b> that pro-vides an inexpensive incremental upgrade path as well as de-creased network congestion, better load balancing, and im-proved fault tolerance. Our simulation-based results show that Subways significantly improves performance compared to alternative ways of wiring the same number of links and switches together. For example, we show that Subways of-fers up to 3. 1 × better performance on a MapReduce shuffle workload compared to an equivalent capacity network...|$|R
30|$|Admittedly, {{the energy}} {{conservation}} {{in the way}} of powering off devices sacrifices the network fault tolerance, which is an inevitable conflict between them. In order to improve the robustness of the network, we need to add additional number of available backup routes according to the reliability requirements as illustrated in Constraint (3). The selection of backup routes applies the shortest-path routing algorithm other than following the aforementioned multiple route selection rules. This strategy means to reserve as few devices as possible to meet the requirements of fault tolerance. From another perspective, as indicated in [32] the switches are fairly reliable (only 5 % failure rates for <b>ToR</b> <b>switches</b> per year), hence it is not so wise to sacrifice a great deal (network resources, computation time, energy, etc.) for a small probability event. Therefore, the shortest-path routing algorithm is well suited and adequate for the backup routeselection.|$|R
40|$|This paper {{introduces}} Plinko, {{a network}} architecture {{that uses a}} novel forwarding model and routing algorithm to build networks with forwarding paths that, assuming arbitrarily large forwarding tables, are provably resilient against t link failures, ∀t ∈ N. However, in practice, there are clearly limits {{on the size of}} forwarding tables. Nonetheless, when constrained to hardware comparable to modern topof-rack (<b>TOR)</b> <b>switches,</b> Plinko scales with high resilience to networks with up to ten thousand hosts. Thus, as long as t or fewer links have failed, the only reason packets of any flow in a Plinko network will be dropped are congestion, packet corruption, and a partitioning of the network topology, and, even after t + 1 failures, most, if not all, flows may be unaffected. In addition, Plinko is topology independent, supports arbitrary paths for routing, provably bounds stretch, and does not require any additional computation during forwarding. To the best of our knowledge, Plinko is the first network to have all of these properties...|$|R
40|$|Abstract—Optical {{data center}} {{networks}} (DCNs) {{are becoming increasingly}} attractive due to their technological strengths com-pared to traditional electrical networks. However, prior optical DCNs are either hard to scale, vulnerable to single point of failure, or provide limited network bisection bandwidth for many practical DCN workloads. To this end, we present WaveCube, a scalable, fault-tolerant, high-performance optical DCN architecture. To scale, WaveCube removes MEMS 1, a potential bottleneck, from its design. Wave-Cube is fault-tolerant since {{it does not have}} single point of failure and there are multiple node-disjoint parallel paths between any pair of Top-of-Rack (<b>ToR)</b> <b>switches.</b> WaveCube delivers high performance by exploiting multi-pathing and dynamic link bandwidth along the path. Our extensive evaluation results show that WaveCube outperforms previous optical DCNs by up to 400 % and delivers network bisection bandwidth that is 70 %- 85 % of an ideal non-blocking network under both realistic and synthetic traffic patterns. WaveCube’s performance degrades gracefully under failures—it drops 20 % even with 20 % links cut. WaveCube also holds promise in practice—its wiring complexity is orders of magnitude lower than Fattree, BCube and c-Through at large scale, and its power consumption is 35 % of them. I...|$|R
40|$|Current over-provisioned and multi-tier {{data centre}} {{networks}} (DCN) deploy rigid control and management platforms, {{which are not}} able to accommodate the ever-growing workload driven by the increasing demand of high-performance data centre (DC) and cloud applications. In response to this, the EC FP 7 project LIGHTNESS (Low Latency and High Throughput Dynamic Network Infrastructures for High Performance Datacentre Interconnects) is proposing a new flattened optical DCN architecture capable of providing dynamic, programmable, and highly available DCN connectivity services while meeting the requirements of new and emerging DC and cloud applications. LIGHTNESS DCN comprises all-optical switching technologies (Optical Packet Switching (OPS) and Optical Circuit Switching (OCS)) and hybrid Top-of-the-Rack (<b>ToR)</b> <b>switches,</b> controlled and operated by a Software Defined Networking (SDN) based control plane for enhanced programmability of heterogeneous network functions and protocols. Harnessing the power of optics enables DCs to effectively cope with the high-performance applications' demands. The programmability and flexibility provided by the SDN based control plane allow to fully exploit the benefits of the LIGHTNESS multi-technology optical DCN, while provisioning on-demand, dynamic, flexible and highly resilient network services inside DCs. Peer ReviewedPostprint (published version...|$|R
40|$|The {{introduction}} of wireless transmissions into the da-ta center {{has been shown}} to be promising in improving the performance of data center networks (DCN) cost ef-fectively. For high transmission flexibility and perfor-mance, a fundamental challenge is to increase the wire-less availability and enable fully hybrid and seamless transmissions over both wired and wireless DCN com-ponents. Rather than limiting the number of wireless ra-dios by the size of top-of-rack (<b>ToR)</b> <b>switches,</b> we pro-pose a novel DCN architecture, Diamond, which nests the wired DCN with radios equipped on all servers. To harvest the gain allowed by the rich reconfigurable wire-less resources, we propose the low-cost deployment of scalable 3 D Ring Reflection Spaces (RRSs) which are in-terconnected with streamlined wired herringbone to en-able large number of concurrent wireless transmissions through high-performance multi-reflection of radio sig-nals over metal. To increase the number of concurrent wireless transmissions within each RRS, we propose a precise reflection method to reduce the wireless interfer-ence. We build a 60 GHz-based testbed to demonstrate the function and transmission ability of our proposed ar-chitecture. We further perform extensive simulations to show the significant performance gain of Diamond, in supporting up to five times higher server-to-server capac-ity, enabling network-wide load balancing, and ensuring high fault tolerance. ...|$|R
40|$|Optical {{networks}} offering ultra-high {{capacity and}} {{low energy consumption}} per bit are considered as a good option to handle the rapidly growing traffic volume inside data center (DCs). However, most of the optical interconnect architectures proposed for DCs so far are mainly focused on the aggregation/core tiers of the data center networks (DCNs), while relying on the conventional top-of-rack (<b>ToR)</b> electronic packet <b>switches</b> (EPS) in the access tier. A large number of <b>ToR</b> <b>switches</b> in the current DCNs brings serious scalability limitations due to high cost and power consumption. Thus, {{it is important to}} investigate and evaluate new optical interconnects tailored for the access tier of the DCNs. We propose and evaluate a passive optical ToR interconnect (POTORI) architecture for the access tier. The data plane of the POTORI consists mainly of passive components to interconnect the servers within the rack as well as the interfaces toward the aggregation/core tiers. Using the passive components makes it possible to significantly reduce power consumption while achieving high reliability in a cost-efficient way. Meanwhile, our proposed POTORI’s control plane is based on a centralized rack controller, which is responsible for coordinating the communications among the servers in the rack. It can be reconfigured by software-defined networking (SDN) operation. A cycle-based medium access control (MAC) protocol and a dynamic bandwidth allocation (DBA) algorithm are designed for the POTORI to efficiently manage the exchange of control messages and the data transmission inside the rack. Simulation results show that under realistic DC traffic scenarios, the POTORI with the proposed DBA algorithm is able to achieve an average packet delay below 10 μs with the use of fast tunable optical transceivers. Moreover, we further quantify the impact of different network configuration parameters on the average packet delay.   QC 20170503 </p...|$|R
40|$|IEEE International Conference on Image Processing 2013 International audienceIn this paper, {{we propose}} a unified {{formalism}} for video descriptors. This formalism {{is based on}} the descriptors decomposition in three levels: primitive, scattering and projection. With this framework, we are able to rewrite easily all the usual descriptors in the literature such as HOG, HOF, SURF. Then, we propose a new projection method based on approximation with a finite expansion of orthogonal polynomials. Using our framework, we extend all usual descrip- <b>tors</b> by <b>switching</b> the projection step. The experiments are carried out on the well known KTH dataset and on the more challenging Hollywood 2 action classification dataset and show state of the art results...|$|R
5000|$|In 2008-09, {{concerned}} about the rapidly growing demand for cloud computing, the IT industry was contemplating converting the prevalent multi-rooted tree based network topology, of oversubscribed data center networks, to a fat tree topology or to a non-blocking full bisection bandwidth clos topology. The difficulty was that re-hauling these networks with complex aggregate layer network switches, as required in a fat tree network topology, or with thousands of simpler commodity network switches inter-connected with miles of wires as required in a clos network topology, was very expensive. In a study of a production mega data center, published in Oct. 2009, Bahl, Kandula, and Padhye showed that barring a few outliers, traffic demands could be met in existing, slightly-oversubscribed data center networks. Then, to manage the outliers, they augmented the small (i.e. low port density) network switches, {{commonly referred to as}} top-of-the-rack or <b>ToR</b> <b>switches,</b> which connect the server racks {{to the rest of the}} network, with extremely high frequency point-to-point radio frequency links. [...] Notwithstanding concerns about link reliability due to interference, they used these short range, highly directional links advantageously, exploiting frequency_reuse to light-up many links concurrently and interference-free at multi-Gbps rates. The steerable radio beams provided additional inter-rack capacity on an as needed bases thus relieving congestion hot-spots. By building such a network, they became the first researchers to introduce mm-wave (60 GHz) wireless communications in data centers. Three years later, their invention was covered by the New York Times in an article published on Jan. 14, 2012 titled “A wireless road around data traffic jams.” Others have followed up on their seminal work by developing similar networks.|$|R
40|$|Data center {{infrastructure}} {{design has}} recently been receiving significant research interest both from academia and industry, {{in no small part}} due to the growing importance of data centers in supporting and sustaining the rapidly growing web-based applications including search (e. g., Google, Bing), video content hosting and distribution (e. g., You Tube, NetFlix). social networking (e. g., facebook, twitter). and large-scale computations (e. g [...] data mining, bioinformatics, indexing). Today's data centers may contain tens of thousands of computers with significant aggregate bandwidth requirements. For example, the Microsoft Live online services are supported by a Chicago-based data center, {{which is one of the}} largest data centers ever built, spanning more than 700. 000 square feet. and Google has more than I Million servers. As a result, the architecture of the network interconnecting the servers has a significant impact on the agility and reconfigurability of the data center infrastructure to respond to changing application demands and service requirements. Traditionally data center networking was based around top of rack (<b>ToR)</b> <b>switches</b> interconnected through end of rack (EoR) switches, and these in turn are being connected through core switches. This approach, besides being very costly, leads to significant bandwidth oversubscription towards the network core. This prompted several researchers to suggest alternate approaches for scalable cost-effective network infrastructures, based on topologies including Fat-Tree, DCell, BCube, MDCube, and Clos network. In this talk, we detail the trends and challenges in designing massive data centers. We will highlight the research efforts being undertaken by the academic and industrial communities to address these challenges. Finally, we present some of our own solutions by leveraging the key data traffic patterns and web-applications in achieving scalable and cost effective solutions to the design of massive data centers infrastructures...|$|R
40|$|Traffic {{forecasts}} {{predict a}} more than threefold increase in the global datacentre workload in coming years, caused by the increasing adoption of cloud and data-intensive applications. Consequently, {{there has been an}} unprecedented need for ultra-high throughput and minimal latency. Currently deployed hierarchical architectures using electronic packet switching technologies are costly and energy-inefficient. Very high capacity switches are required to satisfy the enormous bandwidth requirements of cloud datacentres and this limits the overall network scalability. With the maturity of photonic components, turning to optical switching in data centres is a viable option to accommodate greater bandwidth and network flexibility while potentially minimising the latency, cost and power consumption. Various DCN architectures have been proposed to date and this thesis includes a comparative analysis of such electronic and optical topologies to judge their suitability based on network performance parameters and cost/energy effectiveness, while identifying the challenges faced by recent DCN infrastructures. An analytical Layer 2 switching model is introduced that can alleviate the simulation scalability problem and evaluate the performance of the underlying DCN architecture. This model is also used to judge the variation in traffic arrival/offloading at the intermediate queueing stages and the findings are used to derive closed form expressions for traffic arrival rates and delay. The results from the simulated network demonstrate the impact of buffering and versubscription and reveal the potential bottlenecks and network design tradeoffs. TCP traffic forms the bulk of current DCN workload and so the designed network is further modified to include TCP flows generated from a realistic traffic generator for assessing the impact of Layer 4 congestion control on the DCN performance with standard TCP and datacentre specific TCP protocols (DCTCP). Optical DCN architectures mostly concentrate on core-tier switching. However, substantial energy saving is possible by introducing optics in the edge tiers. Hence, a new approach to optical switching is introduced using Optical <b>ToR</b> <b>switches</b> which can offer better delay performance than commodity switches of similiar size, while having far less power dissipation. An all-optical topology has been further outlined for the efficient implementation of the optical switch meeting the future scalability demands...|$|R
40|$|Internet-based {{business}} and consumer applications are driving serious increment in traffic to datacenters. Current multi-tier and over provision data center network architectures with significant cost, complexity, and power consumption {{are unable to}} provide adequate throughput and performance according to the increasing traffic growth and Information Technology (IT) demands. The features of recently proposed data center network architectures such as dynamic routing, flexible bandwidth, SDN integration, low latency, high bandwidth, fast allocation, failure recovery, and virtualization are analyzed to associate with recognizing current limitations of data centers. In response to these identified limitations, implementing optical technologies in data center networks brings high performance activities {{to cope with the}} unprecedented demands of cloud computing and emerging web applications in the near future. LIGHTNESS (Low latency and high throughput dynamic network infrastructures for high performance datacenter interconnects) is a European Union (EU) -funded project which introduces a new hybrid optical data center network architecture capable of providing high bandwidth capacity, low end-to-end latency to meet the requirements of new and emerging distributed applications and services. LIGHTNESS data center network includes all-optical switching technologies (Optical Packet Switching (OPS) and Optical Circuit Switching (OCS)) and hybrid Top-of-the-Rack (<b>ToR)</b> <b>switches,</b> which are monitored and controlled by a Software Defined Networking (SDN) based control plane to be flexible for the IT functionalities and programmable for the heterogeneous network. The SDN-based control plane by programmability and flexibility can fully use the efficiency of LIGHTNESS multi-technology optical data center network, while providing on-demand, dynamic, flexible, scalable and highly available network services inside data centers. In addition to describing the LIGHTNESS data center network, this thesis also compares the features of some OPS architectures proposed by Eindhoven University of California (UCDavis), University of Patras, and by the LIGHTNESS project to make a basis for implementing the data plane. Finally, this thesis propose an hybrid datacenter network architecture where a set of racks (i. e. a cluster) are interconnected by means of OPS switches while the clusters are interconnected by means of OCS switches which also provides the Internet access. The performance of the OPS cluster part of the network is evaluated considering both unicast and multicast traffic and different scenarios. The results of the simulation show that lower lost packets can be achieved with the {{reduction in the number of}} nodes within the cluster, load, and multicast. The percentage of the ratio of dropped packets to the transmitted ones due to the conjunction would be increased with the higher amount of network load...|$|R
40|$|Abstract—In this paper, we {{show that}} the problem of {{configuring}} the topology of a data center network to opti-mize data aggregation is NP-hard even when the number of aggregators is 1. Further, the approximation ratio of the algorithm proposed by Wang, Ng, and Shaikh [3] for the case of a single aggregator is (k+ 1) / 2, where k is the degree of <b>ToR</b> (top-of-rack) <b>switches</b> and this algorithm also exhibits an anomalous behavior- increase in the switch degree may result in an increase in the aggregation time. By comparison, if topology configuration is done using the longest processing time (LPT) scheduling rule, the approximation ratio is (4 / 3 - 1 /(3 k)). We show that for every instance of the single aggregator topology configuration problem, the time required to aggregate using the LPT configuration is no more than that using the Wang et al. rule. By coupling the LPT rule with the rule of Wang et al., we achieve a better throughput as promised by LPT {{and at the same time}} reduce the total network traffic. Experimental results show that the LPT rule reduces aggregation time by up to 90 % compared to the Wang et al. rule. The reduction in aggregation time afforded by a known improvement, COMBINE, of LPT relative to Wang et al. is up to 90. 5 %. More interestingly, when either of the LPT rule or COMBINE is augmented with the Wang et al. rule, total network traffic is reduced by up to 90 % relative to using LPT and COMBINE with chains. Keywords-Data Center Networks; Software Defined network-ing; Big Data applications; Map-Reduce task...|$|R

