28|159|Public
50|$|Apache cTAKES: {{clinical}} Text Analysis and Knowledge Extraction System is an open-source {{natural language}} processing system for information extraction from electronic health record clinical free-text. It processes clinical notes, identifying types of clinical named entities — drugs, diseases/disorders, signs/symptoms, anatomical sites and procedures. Each named entity has attributes for the <b>text</b> <b>span,</b> the ontology mapping code, context (family history of, current, unrelated to patient), and negated/not negated.|$|E
5000|$|Exposing {{the textual}} {{content of a}} control is {{accomplished}} {{through the use of}} the TextPattern control pattern, which represents the contents of a text container as a text stream. In turn, [...] TextPattern requires the support of the TextPatternRange class to expose format and style attributes. TextPatternRange supports TextPattern by representing a contiguous <b>text</b> <b>span</b> in a text container with the Start and End endpoints. Multiple or disjoint text spans can be represented by more than one TextPatternRange objects. TextPatternRange supports functionality such as clone, selection, comparison, retrieval and traversal.|$|E
40|$|In this paper, we {{introduce}} YEDDA, {{a lightweight}} but efficient open-source tool for <b>text</b> <b>span</b> annotation. YEDDA provides a systematic solution for <b>text</b> <b>span</b> annotation, ranging from collaborative user annotation to administrator evaluation and analysis. It overcomes the low efficiency of traditional text annotation tools by annotating entities through both command line and shortcut keys, which are configurable with custom labels. YEDDA also gives intelligent recommendations by training a predictive model using the up-to-date annotated text. An administrator client is developed to evaluate annotation quality of multiple annotators and generate detailed comparison report for each annotator pair. YEDDA is developed based on Tkinter and {{is compatible with}} all major operating systems. Comment: In submission LREC 201...|$|E
5000|$|Tagged <b>text</b> <b>spans</b> and {{descriptive}} <b>text</b> for images and symbols ...|$|R
40|$|Abstract: Rhetorical Structure Theory (RST) {{has been}} in {{constant}} research {{for a number of}} years. The different parts of a text are called <b>text</b> <b>spans.</b> RST helps in organizing these <b>text</b> <b>spans.</b> These <b>text</b> <b>spans</b> are connected with each other by Discourse markers. In this paper, we are exploring the possibility of realizing some RST relations (CONTRAST, SEQUENCE and PARALLEL) from multi-nuclear sentences in Bengali Language {{with the help of the}} semantic structure of the sentences and the discourse markers. We present a rule based approach for comprehending the RST relations from the discourse markers in Bengali Language...|$|R
5000|$|The {{exterior}} {{wall of the}} stadium behind home plate was dominated by the following <b>text,</b> <b>spanning</b> most of the stadium's height facing 33rd Street, as a memorial to those killed in the two world wars: ...|$|R
40|$|Named entity tagging {{comprises}} the sub-tasks of identifying a <b>text</b> <b>span</b> and classifying it, but this view ignores {{the relationship between}} the entities and the world. Spatial and temporal entities ground events in space-time, and this relationship is vital for applications such as question answering and event tracking. There is much recent work regarding the temporal dimension [13, 10], but no extensive study of the spatial dimension...|$|E
40|$|Background. In {{the area}} of Geographic Information Systems (GIS), a shared {{discipline}} be-tween informatics and geography, the term geo-parsing is {{used to describe the}} process of iden-tifying names in text, which in computational linguistics is known as named entity recognition and classification (NERC). The term geo-coding is used for the task of mapping from implic-itly geo-referenced datasets (such as structured address records) to explicitly geo-referenced representations (e. g., using latitude and longitude). However, present-day GIS systems provide no automatic geo-coding functionality for unstructured text. In Information Extraction (IE), processing of named entities in text has traditionally been seen as a two-step process comprising a flat <b>text</b> <b>span</b> recognition sub-task and an atomic classifi-cation sub-task; relating the <b>text</b> <b>span</b> to a model of the world has been ignored by evaluations such as MUC or ACE (Chinchor (1998); U. S. NIST (2003)). However, spatial and temporal expressions refer to events in space-time, and the grounding of events is a precondition for accurate reasoning. Thus, automatic grounding can improve many applications such as automatic map drawing (e. g. for choosing a focus) and question answerin...|$|E
40|$|Background. In Information Extraction (IE), {{processing}} of named entities in text {{has traditionally been}} seen as a two-step process comprising a flat <b>text</b> <b>span</b> recognition sub-task and an atomic classification sub-task; relating the <b>text</b> <b>span</b> to a model of the world has been ignored by evaluations such as DARPA/NIST's MUC or ACE. However, spatial and temporal expressions refer to events in spacetime, and the grounding of events is a precondition for accurate reasoning. Thus, automatic grounding can improve many applications such as automatic map drawing (e. g. for choosing a focus) and question answering (e. g., for questions like How far is London from Edinburgh, given a story in which both occur and can be resolved). Whereas temporal grounding has received considerable attention in the recent Past [2, 3], robust spatial grounding has long been neglected. Concentrating on geographic names for populated places, I define the task of automatic Toponym Resolution (TR) as computing the mapping from occurrences of names for places as found in a text to a representation of the extensional semantics of the location referred to (its referent), such as a geographic latitude/longitude footprint. The task of mapping from names to locations is hard due to insufficient and noisy databases, and a large degree of ambiguity: common words need to be distinguished from proper names (geo/non-geo ambiguity), and the mapping between names and locations is ambiguous London can refer to the capital of the UK or t...|$|E
40|$|International audienceThe exchangeability {{assumption}} in topic models like Latent Dirichlet Allocation (LDA) {{often results}} in inferring inconsistent topics for the words of <b>text</b> <b>spans</b> like noun-phrases, which are usually expected to be topically coherent. We propose copulaLDA, that extends LDA by integrating part of the text structure to the model and relaxes the conditional independence assumption between the word-specific latent topics given the per-document topic distributions. To this end, {{we assume that the}} words of <b>text</b> <b>spans</b> like noun-phrases are topically bound and we model this dependence with copulas. We demonstrate empirically the effectiveness of copulaLDA on both intrinsic and extrinsic evaluation tasks on several publicly available corpora...|$|R
50|$|The {{following}} table organises {{the text}} from the letters patent by columns for each rank, with common <b>text</b> <b>spanning</b> multiple columns, depicting {{some of the}} similarities and differences among the proclamations. Gender-specific differences are highlighted in italics.|$|R
40|$|In {{the field}} of multi-document summarization, the Pyramid method has become an {{important}} approach for evaluating machine-generated summaries. The method {{is based on the}} manual annotation of <b>text</b> <b>spans</b> with the same meaning in a set of human model summaries. In this paper, we present an unsupervised, probabilistic topic modeling approach for automatically identifying such semantically similar <b>text</b> <b>spans.</b> Our approach reveals some of the structure of model summaries and identifies topics that are good approximations of the Summary Content Units (SCU) used in the Pyramid method. Our results show that the topic model identifies topic-sentence associations that correspond to the contributors of SCUs, suggesting that the topic modeling approach can generate a viable set of candidate SCUs for facilitating the creation of Pyramids. ...|$|R
40|$|We propose an open-domain {{question}} answering system using Thai Wikipedia as the knowledge base. Two {{types of information}} are used for answering a question: (1) structured information extracted and stored {{in the form of}} Resource Description Framework (RDF), and (2) unstructured texts stored as a search index. For the structured information, SPARQL transformed query is applied to retrieve a short answer from the RDF base. For the unstructured information, keyword-based query is used to retrieve the shortest <b>text</b> <b>span</b> containing the questions’s key terms. From the experimental results, the system which integrates both approaches could achieve an average MRR of 0. 47 based on 215 test questions. ...|$|E
40|$|Sentiment {{analysis}} is increasingly {{viewed as a}} vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or <b>text</b> <b>span,</b> irrespective of the entities mentioned (e. g., laptops) and their aspects (e. g., battery, screen). SemEval- 2014 Task 4 aimed to foster {{research in the field}} of aspect-based sentiment analysis, where the goal is to identify the aspects of given target entities and the sentiment expressed for each aspect. The task provided datasets containing manually annotated reviews of restaurants and laptops, as well as a common evaluation procedure. It attracted 163 submissions from 32 teams. ...|$|E
40|$|We discuss {{factors that}} affect human {{agreement}} on a semantic labeling task in the art history domain, {{based on the results}} of four experiments where we varied the number of labels annotators could assign, the number of annotators, the type and amount of training they received, and the size of the <b>text</b> <b>span</b> being labeled. Using the labelings from one experiment involving seven annotators, we investigate the relation between interannotator agreement and machine learning performance. We construct binary classifiers and vary the training and test data by swapping the labelings from the seven annotators. First, we find performance is often quite good despite lower than recommended interannotator agreement. Second, we find that on average, learning performance for a given functiona...|$|E
40|$|According to many current {{theories}} of coherence relations, a distinction {{can be drawn}} between relations that hold between {{the content of the}} <b>text</b> <b>spans</b> they link (which can be termed semantic relations), and those that hold between the utterances of the <b>text</b> <b>spans</b> themselves or the beliefs which underlie them (which can be termed pragmatic). However, this distinction is not always clearcut. One useful recent proposal by Sweetser eectively results in dividing the class of pragmatic relations in two, distinguishing between epistemic relations (which hold between the beliefs which underlie a speaker's utterances) and speech-act relations (which hold between the utterances themselves). In this paper I begin by reviewing the advantages and shortcomings of both these approaches. I then propose an alternative, intention-based denition of pragmatic relations, which I argue retains the advantages of both approaches while avoiding at least some of their shortcomings. 1 Introduction This pape [...] ...|$|R
40|$|Sentiment {{analysis}} has applications {{in many areas}} and the exploration of its potential has only just begun. We propose Pathos, a framework which performs document sentiment analysis (partly) based on a document’s discourse structure. We hypothesize that by splitting a text into important and less important <b>text</b> <b>spans,</b> and by subsequently making use of this information by weighting the sentiment conveyed by distinct <b>text</b> <b>spans</b> {{in accordance with their}} importance, we can improve the performance of a sentiment classifier. A document’s discourse structure is obtained by applying Rhetorical Structure Theory on sentence level. When controlling for each considered method’s structural bias towards positive classifications, weights optimized by a genetic algorithm yield an improvement in sentiment classification accuracy and macro-level F 1 score on documents of 4. 5 % and 4. 7 %, respectively, in comparison to a baseline not taking into account discourse structure...|$|R
40|$|Abstract. We {{investigate}} {{the use of}} clustering methods for the task of grouping the <b>text</b> <b>spans</b> in a news article that refer to the same event. We provide evidence that {{the order in which}} events are described is structured in a way that can be exploited during clustering. We evaluate our approach on a corpus of news articles describing events that have occurred in the Iraqi War. ...|$|R
40|$|Abstract Background Online {{psychiatric}} {{texts are}} natural language texts expressing depressive problems, published by Internet users via community-based web {{services such as}} web forums, message boards and blogs. Understanding the cause-effect relations embedded in these psychiatric texts can {{provide insight into the}} authors’ problems, thus increasing the effectiveness of online psychiatric services. Methods Previous studies have proposed the use of word pairs extracted from a set of sentence pairs to identify cause-effect relations between sentences. A word pair is made up of two words, with one coming from the cause <b>text</b> <b>span</b> and the other from the effect <b>text</b> <b>span.</b> Analysis of the relationship between these words can be used to capture individual word associations between cause and effect sentences. For instance, (broke up, life) and (boyfriend, meaningless) are two word pairs extracted from the sentence pair: “I broke up with my boyfriend. Life is now meaningless to me”. The major limitation of word pairs is that individual words in sentences usually cannot reflect the exact meaning of the cause and effect events, and thus may produce semantically incomplete word pairs, as the previous examples show. Therefore, this study proposes the use of inter-sentential language patterns such as ≪broke up, boyfriend>, Results Performance was evaluated on a corpus of texts collected from PsychPark ([URL]), a virtual psychiatric clinic maintained by a group of volunteer professionals from the Taiwan Association of Mental Health Informatics. Experimental results show that the use of inter-sentential language patterns outperformed the use of word pairs proposed in previous studies. Conclusions This study demonstrates the acquisition of inter-sentential language patterns for causality detection from online psychiatric texts. Such semantically more complete and precise features can improve causality detection performance. </p...|$|E
40|$|This paper {{proposes to}} tackle open- domain {{question}} answering using Wikipedia as the unique knowledge source: {{the answer to}} any factoid question is a <b>text</b> <b>span</b> in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task. Comment: ACL 2017, 10 page...|$|E
30|$|The writer {{implies that}} such attitudes {{would then be}} {{rendered}} as ‘echoes’, or realised in ‘the poverty of our [future] exchange [s]’. It is this location in a comparative construction which stabilises the invocation of attitude towards a ‘real’ target, the addressees, via strategies of attitude tokens (item (7), Fig.  2) and local signals such as comparison (item (6 d) Fig.  2), without explicitly mentioning those addressees, except in the deictic possessive, your. The addressees remain present only as potential holders of fear, suspicion and doubt, as potential dissemblers of their feelings, and as potential poor interlocutors. The local (i.e. within the <b>text</b> <b>span)</b> targets, however[*]–[*]those boldened in the excerpt above[*]–[*]remain {{at the level of}} implied possessions of the ‘real’ target: your fear, suspicion or doubt (to which the writer expresses positive Inclination about hearing directly), and the poverty of our exchange (to which the writer expresses negative Inclination about hearing their echoes).|$|E
40|$|Provides a concise, {{up-to-date}} {{and highly}} readable {{introduction to the}} subject. It covers deserts and coastlines, as well as periglacial and planetary landforms. The <b>text</b> <b>spans</b> {{the full range of}} aeolian features to include soil erosion and its consequences, continental scale dust storms, sand dunes and loess. It discusses the importance of aeolian processes in the past, and the application of knowledge about aeolian geomorphology in environmental managemen...|$|R
40|$|In this paper, we {{describe}} an annotation scheme for the attribution of abstract objects (propositions, facts, and eventualities) associated with discourse relations and their arguments annotated in the Penn Discourse TreeBank. The scheme aims to capture both {{the source and}} degrees of factuality of the abstract objects through the annotation of <b>text</b> <b>spans</b> signalling the attribution, and of features recording the source, type, scopal polarity, and determinacy of attribution...|$|R
40|$|International audienceIn this paper, {{we address}} the problem of speaker role {{identification}} on a corpus of manually transcribed call center conversations. We first tackle it as a text categorization task. Then, we combine these categorization results with a dialog modeling approach. We achieve 93 % of correct role assignment with the least method. Our method also offers the possibility to extract <b>text</b> <b>spans</b> specific to each role. These strings slightly improve the role identification results and are an interesting element for conversation analysis...|$|R
40|$|We {{introduce}} OCELOT, {{a prototype}} system for automatically generating the "gist" {{of a web}} page by summarizing it. Although most text summarization research to date {{has focused on the}} task of news articles, web pages are quite different in both structure and content. Instead of coherent text with a well-defined discourse structure, they are more often likely to be a chaotic jumble of phrases, links, graphics and formatting commands. Such text provides little foothold for extractive summarization techniques, which attempt to generate a summary of a document by excerpting a contiguous, coherent span of text from it. This paper builds upon recent work in non-extractive summarization, producing the gist of a web page by "translating" it into a more concise representation rather than attempting to extract a <b>text</b> <b>span</b> verbatim. OCELOT uses probabilistic models to guide it in selecting and ordering words into a gist. This paper describes a technique for learning these models automatically from a collection of human-summarized web pages...|$|E
40|$|Most {{studies on}} {{discourse}} markers implicitly assume {{that only one}} marker or discourse relation will occur in a sentence. In reality, more than one relation may hold between text spans and may be cued by multiple discourse markers. We describe here a method for hierarchically organising discourse markers. The hierarchies are intended for use by a generation system to enable the selection and placement {{of more than one}} marker in a single <b>text</b> <b>span.</b> 1 Introduction The majority of studies on discourse markers implicitly assume that only one marker or discourse relation will occur in a sentence or that the presence of multiple markers will not affect the choice and placement of others. However, in reality, more than one relation may hold between text spans which may be cued by multiple markers. The available rules describing the occurrence, choice and placement of a given marker do not account for multiple marker occurrence (Grote et al., 1995; Webber and Joshi, 1998; Power et al., 1999, [...] ...|$|E
40|$|In this paper, {{we present}} our work on {{constructing}} a textual semantic relation corpus by {{making use of}} an existing treebank annotated with discourse relations. We extract adjacent <b>text</b> <b>span</b> pairs and group them into six categories according to the different discourse relations between them. After that, we present the details of our annotation scheme, which includes six textual semantic relations, backward entailment, forward entailment, equality, contradiction, overlapping, and independent. We also discuss some ambiguous examples to show the difficulty of such annotation task, which cannot be easily done by an automatic mapping between discourse relations and semantic relations. We have two annotators {{and each of them}} performs the task twice. The basic statistics on the constructed corpus looks promising: we achieve 81. 17 % of agreement on the six semantic relation annotation with a. 718 kappa score, and it increases to 91. 21 % if we collapse the last two labels with a. 775 kappa score. 1...|$|E
40|$|We {{characterize}} a text-technological {{approach to}} text analysis as {{combination of a}} multi-level representation framework and XML-based document processing techniques. The main advantages of such an approach are the chance to flexibly combine modules for constructing different applications, and the overall robustness resulting from the operational principle of higher-level modules combining the — possibly partial — results of lower-level ones. We illustrate the approach with the specific task of local coherence analysis, i. e. the computation of coherence relations between <b>text</b> <b>spans...</b>|$|R
40|$|The CL-SciSumm 2016 shared task {{introduced}} an interesting problem: given a document D {{and a piece}} of text that cites D, how do we identify the <b>text</b> <b>spans</b> of D being referenced by the piece of text? The shared task provided the first annotated dataset for studying this problem. We present an analysis of our continued work in improving our system's performance on this task. We demonstrate how topic models and word embeddings can be used to surpass the previously best performing system...|$|R
50|$|The {{manuscript}} Arabe 328(ab) is fragmentary. Originally {{it contained}} an estimated 210 to 220 leaves, of which 118 are extant (70 in Paris, 46 in Saint-Petersburg, and one each in Rome and London). The preserved <b>text</b> <b>spans</b> sura 2:275 to 72:2, with lacunae in between. Overall, it contains about 45% of the Quranic text. It {{was produced by}} five scribes, probably working concurrently {{in order to meet}} demand for a fast production. All of the hands use the Hijazi script.|$|R
40|$|With {{the huge}} amount of {{information}} available electronically, there is an increasing demand for automatic text summarization systems. The use of machine learning techniques for this task allows one to adapt summaries to the user needs and to the corpus characteristics. These desirable properties have motivated an increasing amount of work in this field {{over the last few}} years. Most approaches attempt to generate summaries by extracting sentence segments and adopt the supervised learning paradigm which requires to label documents at the <b>text</b> <b>span</b> level. This is a costly process, which puts strong limitations on the applicability of these methods. We investigate here the use of semi-supervised algorithms for summarization. These techniques make use of few labeled data together with a larger amount of unlabeled data. We propose new semi-supervised algorithms for training classification models for text summarization. We analyze their performances on two data sets- the Reuters newswire corpus and the Computation and Language (cmp_lg) collection of TIPSTER SUMMAC. We perform comparisons with a baseline – non learning – system, and a reference trainable summarizer system...|$|E
40|$|Negation words, such as no and not, play a {{fundamental}} role in modifying sentiment of textual expressions. We will {{refer to a}} negation word as the negator and the <b>text</b> <b>span</b> within the scope of the negator as the argument. Commonly used heuristics to estimate the sentiment of negated expressions rely simply on the sentiment of argument (and not on the negator or the argument itself). We use a sentiment treebank to show that these existing heuristics are poor estimators of sentiment. We then modify these heuristics to be dependent on the negators and show that this improves prediction. Next, we evaluate a recently proposed composition model (Socher et al., 2013) that relies on both the negator and the argument. This model learns the syntax and semantics of the negator's argument with a recursive neural network. We show that this approach performs better than those mentioned above. In addition, we explicitly incorporate the prior sentiment of the argument and observe that this information can help reduce fitting errors. Peer reviewed: YesNRC publication: Ye...|$|E
40|$|International audienceWith {{the huge}} amount of {{information}} available electronically, there is an increasing demand for automatic text summarization systems. The use of machine learning techniques for this task allows one to adapt summaries to the user needs and to the corpus characteristics. These desirable properties have motivated an increasing amount of work in this field {{over the last few}} years. Most approaches attempt to generate summaries by extracting sentence segments and adopt the supervised learning paradigm which requires to label documents at the <b>text</b> <b>span</b> level. This is a costly process, which puts strong limitations on the applicability of these methods. We investigate here the use of semi-supervised algorithms for summarization. These techniques make use of few labeled data together with a larger amount of unlabeled data. We propose new semi-supervised algorithms for training classification models for text summarization. We analyze their performances on two data sets - the Reuters news-wire corpus and the Computation and Language (cmp_lg) collection of TIPSTER SUMMAC. We perform comparisons with a baseline - non learning - system, and a reference trainable summarizer system...|$|E
40|$|This paper {{describes}} {{our contribution}} to the SemEval- 2014 Task 9 on sentiment analysis in Twitter. We participated in both strands of the task, viz. classification at message-level (subtask B), and polarity disambiguation of particular <b>text</b> <b>spans</b> within a message (subtask A). Our experi-ments {{with a variety of}} lexical and syntactic fea-tures show that our systems benefit from rich fea-ture sets for sentiment analysis on user-generated content. Our systems ranked ninth among 27 and sixteenth among 50 submissions for task A and B respectively. ...|$|R
5000|$|The Kahun Papyri (KP) (also Petrie Papyri or Lahun Papyri) are a {{collection}} of ancient Egyptian texts discussing administrative, mathematical and medical topics. Its many fragments were discovered by Flinders Petrie in 1889 and are kept at the University College London. This collection of papyri {{is one of the}} largest ever found. Most of the texts are dated to ca. 1825 BC, to the reign of Amenemhat III. In general the collection spans the Middle Kingdom of Egypt. [...] The <b>texts</b> <b>span</b> a variety of topics: ...|$|R
2500|$|Superliner cars got the Phase IV {{striping}} {{located in}} the same location as previously minus the swoop up on the Sightseer Lounge cars. These cars lost their Phase II-style wording above the doors and instead got the word [...] "Superliner" [...] in blue placed above the red & white stripes, slightly overlapping the stripes. The 'Superliner' <b>text</b> <b>spanned</b> most {{of the length of}} the car. The Viewliner cars received a similar treatment but bearing the word [...] "Viewliner". In June 2016, the last Amtrak Locomotive (P32-8BHW 515) was repainted in Phase V colors.|$|R
