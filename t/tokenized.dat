200|158|Public
25|$|PDF {{contains}} <b>tokenized</b> {{and interpreted}} {{results of the}} PostScript source code, for direct correspondence between changes to items in the PDF page description and changes to the resulting page appearance.|$|E
25|$|Often, the PostScript-like PDF code is {{generated}} {{from a source}} PostScript file. The graphics commands that are output by the PostScript code are collected and <b>tokenized.</b> Any files, graphics, or fonts to which the document refers also are collected. Then, everything is compressed to a single file. Therefore, the entire PostScript world (fonts, layout, measurements) remains intact.|$|E
25|$|TeX {{provides}} an unusual macro language; {{the definition of}} a macro not only includes a list of commands but also the syntax of the call. Macros are completely integrated with a full-scale interpreted compile-time language that also guides processing. TeX's macro level of operation is lexical, but it is a built-in facility of TeX, that makes use of syntax interpretation. Comparing with most widely used lexical preprocessors like M4, it differs slightly, as the body of a macro gets <b>tokenized</b> at definition time, that is, it is not completely raw text. Except for a few very special cases, this gives the same behaviour. The TeX macro language has been successfully used to extend TeX to, for instance, LaTeX and ConTeXt.|$|E
30|$|<b>Tokenize</b> the {{extracted}} text.|$|R
5000|$|... #Subtitle level 3: Changing How SQuirreL <b>Tokenizes</b> a Script into Individual Statements ...|$|R
50|$|Following <b>tokenizing</b> is parsing. From there, the {{interpreted}} {{data may}} be loaded into data structures for general use, interpretation, or compiling.|$|R
2500|$|Queries are the {{mechanisms}} that Jet uses to retrieve {{data from the}} database. They can be defined in Microsoft QBE (Query By Example), through the Microsoft Access SQL Window or through Access Basic's Data Access Objects (DAO) language. These are then converted to an SQL SELECT statement. The query is then d [...] this involves parsing the query (involves syntax checking and determining the columns to query in the database table), then converted into an internal Jet query object format, which is then <b>tokenized</b> and organised into a tree like structure. In Jet 3.0 onwards these are then optimised using the Microsoft Rushmore query optimisation technology. The query is then executed and the results passed back to the application or user who requested the data.|$|E
2500|$|The search service {{consists}} of several components, including the Gatherer, the Merger, the Backoff Controller, and the Query Processor, among others. The Gatherer retrieves {{the list of}} URIs {{that need to be}} crawled and invokes proper protocol handler to access the store that hosts the URI, and then the proper property-handler (to extract metadata) and IFilter to extract the document text. Different indices are created during different runs; it is the job of the Merger to periodically merge the indices. While indexing, the indices are generally maintained in-memory and then flushed to disk after a merge to reduce disk I/O. The metadata is stored in property store, which is a database maintained by the ESE database engine. The text is <b>tokenized</b> and the tokens are stored in a custom database built using Inverted Indices. Apart from the indices and property store, another persistent data structure is maintained: the Gather Queue. The Gather Queue maintains a prioritized queue of URIs that needs indexing. The Backoff Controller mentioned above monitors the available system resources, and controls {{the rate at which the}} indexer runs. It has three states: ...|$|E
5000|$|In {{this case}} {{reserved}} words {{are defined as}} part of the lexical grammar, and are each <b>tokenized</b> as a separate type, distinct from identifiers. In conventional notation, the reserved words [...] and [...] for example are <b>tokenized</b> as types [...] and , respectively, while [...] and [...] are both <b>tokenized</b> as type [...]|$|E
30|$|Lexical {{analysis}} In this step, a {{lexical analyzer}} (lexer) <b>tokenizes</b> a job title input using defined regular expressions and matches them against dictionary files.|$|R
5000|$|... may be {{abbreviated}} as [...] as in Microsoft BASIC, but Atari BASIC {{does not}} <b>tokenize</b> it into [...] -ing a program will still show the question mark.|$|R
30|$|We then {{continued}} {{to process the}} textual data. Because textual data is unstructured, the aim of data preparation is to represent the raw text by numeric values. This process contained two steps: <b>tokenizing</b> and counting. In the <b>tokenizing</b> step, we used the CKIP Chinese word segmentation system (Ma and Chen 2003) to handle the text segmentation. In the counting step, term frequency-inverse document frequency (TF-IDF) {{was used as an}} indicator parameter to extract text features. TF-IDF is a measure of how frequent a term is in a document, and how rare a term is in many documents.|$|R
50|$|For {{application}} development, AMOS used {{a proprietary}} BASIC-like language called AlphaBASIC (though several other languages, including Assembler, FORTRAN, Pascal, and COBOL, were available). Older versions interpreted a <b>tokenized</b> executable file. Later versions translate the <b>tokenized</b> executable into x86 code for performance.|$|E
50|$|Lexical preprocessors are the lowest-level of preprocessors as {{they only}} require lexical analysis, that is, they {{operate on the}} source text, prior to any parsing, by {{performing}} simple substitution of <b>tokenized</b> character sequences for other <b>tokenized</b> character sequences, according to user-defined rules. They typically perform macro substitution, textual inclusion of other files, and conditional compilation or inclusion.|$|E
50|$|Like Atari BASIC, {{source code}} in MAC/65 uses line numbers and is <b>tokenized</b> {{as it is}} entered. This allows {{immediate}} reporting of syntax errors, shorter assembly times, and smaller source code sizes (important when both the source and object code are in memory at the same time). Source files can be saved and loaded in either <b>tokenized</b> format or as text files.|$|E
40|$|For {{languages}} whose {{character set}} is {{very large and}} whose orthography does not require spacing between words, such as Japanese, <b>tokenizing</b> and part-of-speech tagging are often the difficult parts of any morphological analysis. For practical systems to tackle this problem, uncontrolled heuristics are primarily used. The use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character cluster- ing based on mutual information into Decision- Tree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both <b>tokenizing</b> and tagging Japanese text...|$|R
50|$|Different tokens might {{carry out}} similar {{information}} (e.g. tokenization and <b>tokenizing).</b> And {{we can avoid}} calculating similar information repeatedly by reducing all tokens to its base form using various stemming and lemmatization dictionaries.|$|R
50|$|Gay Australian Aboriginal {{men have}} {{reported}} {{a lack of}} inclusion and representation in the white-dominated LGBT community. When aboriginal men {{have been included in}} LGBT organizing efforts, it has often been in a <b>tokenizing</b> way.|$|R
5000|$|The document, [...] "a rose is a rose is a rose" [...] can be <b>tokenized</b> as follows: ...|$|E
5000|$|A {{number of}} similar {{techniques}} exist, generally prefixing or suffixing an identifier to indicate different treatment, but the semantics are varied. Strictly speaking, stropping consists of different {{representations of the}} same name (value) in different namespaces, and occurs at the tokenization stage. For example, in ALGOL 60 with matched apostrophe stropping, [...] is <b>tokenized</b> as (Keyword, if), while [...] is <b>tokenized</b> as (Identifier, if) - same value in different token classes.|$|E
5000|$|The {{ability to}} save {{programs}} in three formats (ordinary <b>tokenized</b> binary, ASCII, or a [...] "protected" [...] token format that cannot be LISTed) ...|$|E
5000|$|Text {{processing}} {{support for}} SBCS and UTF-8 encodings, stopwords, indexing of words known not {{to appear in}} the database ("hitless"), stemming, word forms, <b>tokenizing</b> exceptions, and [...] "blended characters" [...] (dual-indexing as both a real character and a word separator).|$|R
40|$|New paint for the UI {{look and}} feel. Reworked Rolling Window Analysis Kmeans Clustering got some love Document Milestones if {{inserted}} beforehand {{can be detected}} on cut <b>Tokenize</b> moved to DataTables Top-Words added Citation information: Kleinman, S., LeBlanc, M. D., Zhang, C. (2015). Lexos. v. 2. 5. [URL]...|$|R
50|$|When Indigenous {{people from}} {{communities}} that are less-accepting of two spirits have sought community among non-Native LGBT communities, however, the tendency for non-Natives to <b>tokenize</b> and appropriate has at times led to rifts rather than unity, with two spirits feeling like they're just another tacked on initial rather than fully included.|$|R
5000|$|... 2010 - Launches Maestro Self-Service Touch Screen Kiosk, fully <b>tokenized</b> {{interface}} {{for payment}} card processing & Maestro PMS Certified for PCI Compliancy, PA-DSS Standards ...|$|E
5000|$|PDF {{contains}} <b>tokenized</b> {{and interpreted}} {{results of the}} PostScript source code, for direct correspondence between changes to items in the PDF page description and changes to the resulting page appearance.|$|E
50|$|Some authors term this a token, using 'token' interchangeably to {{represent}} (a) the string being <b>tokenized,</b> and (b) the token data structure resulting from putting this string through the tokenization process.|$|E
5000|$|Diverse File Formats : In {{order to}} {{correctly}} identify which bytes of a document represent characters, the file format must be correctly handled. Search engines which support multiple file formats {{must be able}} to correctly open and access the document and be able to <b>tokenize</b> the characters of the document.|$|R
40|$|In a 1997 study, Broder et al. [4] {{presented}} a techniquefor estimating {{the degree of}} similarity among pairs of docu- ments. Their technique, which they call shingling, is purelysyntactic; it does not rely on any linguistic knowledge other than the ability to <b>tokenize</b> documents into a list of words. Their technique extracts al...|$|R
40|$|Added {{documentation}} for n-grams, skip n-grams, and regex Added codecov and appveyor Added tidiers for LDA {{objects from}} topicmodels and a vignette on topic modeling Added function to calculate tf-idf of a tidy text dataset and a tf-idf vignette Fixed a bug when tidying by line/sentence/paragraph/regex {{and there are}} multiple non-text columns Fixed a bug when unnesting using n-grams and skip n-grams (entire text was not being collapsed) Added ability to pass a (custom <b>tokenizing)</b> function to token. Also added a collapse argument that makes the choice whether to combine lines before <b>tokenizing</b> explicit. Changed tidy. dictionary to return a tbl_df rather than a data. frame Updated cast_sparse to work with dplyr 0. 5. 0 Deprecated the pair_count function, which has been moved to pairwise_count in the widyr package. This will be removed entirely in a future version...|$|R
5000|$|Atari BASIC uses a token {{structure}} to handle lexical processing for better performance and reduced memory size. The tokenizer converts lines {{using a small}} buffer in memory, and the program is stored as a parse tree. The token output buffer (addressed by a pointer at LOMEM [...] - [...] 80, 8116) is 256 bytes, and any <b>tokenized</b> statement larger than the buffer generates an error (14 [...] - [...] line too long). Indeed, the syntax checking described in the [...] "Program editing" [...] section is {{a side effect of}} converting each line into a <b>tokenized</b> form before it is stored.|$|E
5000|$|The input text {{is first}} <b>tokenized,</b> or broken into words. Typicallyin natural {{language}} processing, contractions such as [...] "'s", [...] "n't", {{and the like}} are considered separate word tokens, as are punctuation marks.|$|E
50|$|Another {{combination}} is with smart card to store locally larger amount of identity data and process information as well. Another is a contactless BLE token that combines secure storage and <b>tokenized</b> release of fingerprint credentials.|$|E
50|$|Many {{graphing}} {{and scientific}} calculators will <b>tokenize</b> the program text, replacing textual programming elements with short numerical tokens. Many graphical calculators work much like computers and use versions of 7-bit, 8-bit or 9-bit ASCII-derived character sets or even UTF-8 and Unicode. Many {{of them have}} a tool similar to the character map on Windows.|$|R
40|$|We {{present an}} {{approach}} to using a morphological analyzer for <b>tokenizing</b> and morphologically tagging (including partof-speech tagging) Arabic words in one process. We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from {{the output of the}} analyzer. We obtain accuracy rates on all tasks in the high nineties. ...|$|R
40|$|It is {{not easy}} to <b>tokenize</b> agglutinative {{languages}} like Japanese and Chinese into words. Many IR systems start with a dictionarybased morphology program like ChaSen [4]. Unfortunately, dictionaries cannot cover all possible words; unknown words such as proper nouns are important for IR. This paper proposes a statistical dictionary-free method for selecting index strings based on recent work on adaptive language modeling...|$|R
