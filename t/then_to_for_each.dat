0|10000|Public
40|$|The present {{tendency}} toward comparatively small families eliminates {{to a great}} extent the possibility of passing clothes on from one member of the family to another. Even in a large family it is not always a desirable thing to do, since a garment selected for one child is not necessarily becoming to another. Far better, <b>then,</b> <b>to</b> plan <b>for</b> <b>each</b> child <b>to</b> wear out his own clothes in so far as possible...|$|R
40|$|I {{examine the}} problem of {{treatment}} choice when a planner observes (i) covariates that describe each member of a population of interest and (ii) the outcomes of an experiment in which subjects randomly drawn from this population are randomly assigned to treatment groups within which all subjects receive the same treatment. Covariate data for the subjects of the experiment are not available. The optimal treatment rule is to divide the population into subpopulations whose members share the same covariate value, and <b>then</b> <b>to</b> choose <b>for</b> <b>each</b> subpopulation a treatment that maximizes its mean outcome. However the planner cannot implement this rule. I draw on my work on nonparametric analysis of treatment response to address the planner's problem. Comment: Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI 1998...|$|R
5000|$|Fountas & Pinnell reading levels (commonly {{referred}} to as [...] "Fountas & Pinnell") are a system of reading levels developed by Irene Fountas and Gay Su Pinnell to support their guided reading method. Reading text is classified according to various parameters, such as word count, number of different words, number of high-frequency words, sentence length, sentence complexity, word repetitions, illustration support, etc. While classification is guided by these parameters, syllable type, an important consideration in beginning reading, is not considered {{as part of the}} leveling system. Small books containing a combination of text and illustrations are <b>then</b> provided <b>to</b> educators <b>for</b> <b>each</b> level.|$|R
40|$|This paper {{reports on}} an ongoing {{development}} of an application that aims at identifying communities of use {{in the context of}} an organized group of people. The described application, called CoWing consists on mining users bookmark files in order to identify communities that share the same information interests. The system is composed of a set of assistant agents, called WINGS, and a central agent that manages the users organization. A Wing agent observes the user behavior in order to learn the user bookmark classification strategy. A hybrid neural case-based reasoning incremental classifier is used for this purpose. Classification knowledge learned by an agent is <b>then</b> used <b>to</b> identify, <b>for</b> <b>each</b> folder in the local bookmark hierarchy, a community of users that share the same interests in the theme of that folder...|$|R
40|$|When {{implementing}} a multilevel security policy for Object-Oriented Databases (OODBs), several aspects {{have to be}} investigated. One of these aspect is the design of multilevel OODBs. In an OODB, data are organized in a complex structure built using different constructs (classes, objects, attributes, links [...] .). Therefore, a first problem is to determine what constructs of the object-oriented model should {{be associated with a}} security level. A second problem is <b>then</b> <b>to</b> define semantics <b>for</b> <b>each</b> assignment of a security level to an object-oriented construct. While assigning the security levels, we have also to be careful with the inference problems which may occur due to the integrity constraints inherent in the object-oriented paradigm. Therefore, a last {{purpose of this paper is}} to define a set of general rules to cope with this problem. 1...|$|R
40|$|In this paper, we {{describe}} experiments conducted on identifying a person using a novel unique correlated corpus of text and audio {{samples of the}} person’s communication in six genres. The text samples include essays, emails, blogs, and chat. Audio samples were collected from individual interviews and group discussions and <b>then</b> transcribed <b>to</b> text. <b>For</b> <b>each</b> genre, samples were collected for six topics. We show that we can identify the communicant with an accuracy of 71 % for six fold cross validation using an average of 22, 000 words per individual across the six genres. For person identification in a particular genre (train on five genres, test on one), an average accuracy of 82 % is achieved. For identification from topics (train on five topics, test on one), an average accuracy of 94 % is achieved. We also report results on identifying a person’s communication in a genre using text genres only as well as audio genres only. ...|$|R
40|$|One of {{the aims}} {{of this paper is}} to present a new {{programming}} paradigm based on the new paradigms intensively used in IT industry. Implementation of these techniques can improve the quality of code through modularization, not only in terms of entities used by a program, but also in terms of states in which they pass. Another aspect followed in this paper takes into account that in the development of software applications, the transition from the design to the source code is a very expensive step in terms of effort and time spent. Diagrams can hide very important details for simplicity of understanding, which can lead to incorrect or incomplete implementations. To improve this process communicative automaton based programming comes with an intermediate step. We will see how it goes after creating modeling diagrams to communicative automata and <b>then</b> <b>to</b> writing code <b>for</b> <b>each</b> of them. We show how the transition from one step to another is much easier and intuitive...|$|R
40|$|In many {{scheduling}} applications, a {{large number}} of jobs are grouped into a comparatively small number of lots made of identical items. It is <b>then</b> sufficient <b>to</b> give, <b>for</b> <b>each</b> lot, the number of jobs it involves plus the description of one single job. The resulting high-multiplicity input format is much more compact than the standard one. As a consequence, in order to be efficient, standard solution methods must be modified. We consider high-multiplicity parallel machine scheduling problems with identical, uniform, and unrelated machines, and two classic objectives: minimum sum of completion times and minimum makespan. For polynomially solvable cases, we provide exact algorithms, while for hard cases we provide approximate, asymptotically exact algorithms. The exact algorithms exploit multiplicities to identify and fix a partial schedule, consisting of most jobs, that is contained in an optimal schedule, and then solve the residual problem optimally. The approximate algorithms use the same approach, but in their case neither it is guaranteed that the fixed partial schedule is contained in an optimal one nor the residual problem is optimally solved. All proposed algorithms are polynomial and easy <b>to</b> implement <b>for</b> practical purposes...|$|R
40|$|AbstractFuture water {{shortages}} require immediate action on development of resources, reduction of demand and higher efficiency in treatment and transmission. Future flood risk management requires immediate action in risk assessment, defence and alleviation systems, forecasting and warning systems and institutional and governance measures. Technology has been revolutionized over {{recent years and}} now, matured with mass production allowing wider uptake of methods and devices. The current situation in the water domain {{is characterized by a}} low level of maturity concerning standardization of ICT solutions and business processes. The massive and rapid spread of communicating devices within the Society and their application to the industrial sectors is not coordinated. The only relevant angle for the development of these technologies (M 2 M) within the water domain has {{to be based on the}} identification of the added value provided in each business process by the introduction of the new solutions. In order to identify which and how ICT solutions can be implemented, it is necessary to look at the water cycle through an approach based on functional domains and business processes. The water uses are associated to business processes and are linked to economical and social values. The final step of the approach is <b>then</b> <b>to</b> identify <b>for</b> <b>each</b> business process how ICT solutions can be implemented and provide added value. This diagnostic has to be shared by professionals and operators in order to ensure a coherent deployment. The motivation of the @qua entity (www. a-qua. org) which gathers major actors from water and ICT domains, is now to ensure this role at the interface with the different actors of the water domain and to elaborate a consistent vision for the future of ICT solutions within the water domain...|$|R
40|$|It is {{a common}} habit among {{practitioners}} to maintain under strong control {{the behavior of a}} bunch of indexes that are known to capture the movements of Eurozone stocks. Baltic Dry Index (BDI), RJ/CRB Commodity Price Index (CRB), Chicago Board Options Exchange Volatility Index (VIX) and Deutsche Bank G 10 Currency Future Harvest Index (DBHI), in fact, are supposed to exhibit a kind of anticipatory behavior with respect to that of Eurozone economy: understanding their dynamics should therefore imply to know in advance how the economic system will behave. The rationale of this chapter is to verify to what extent the use of tools relying on chaos theory and complexity studies (in our case: multiscaling analysis) can be of any help to capture such anticipatory movements. To do this, we performed two separate tasks: we evaluated the Hurst exponent of the aforementioned indexes using a set of techniques, to give robustness to the results; we <b>then</b> moved <b>to</b> compute <b>for</b> <b>each</b> of them the Hölderian function values. The results suggested us the track along which developing a trading system based on the fractal characterization of the Eurostoxx 50 index whose performance will be provided and discussed as well...|$|R
40|$|The space {{conditioning}} sector {{is one of}} the highest energy consumers and of the least efficient {{from the point of view}} of primary-to-end-use matching. In spite of the emergence, in the last decade, of innovative technical solutions based on renewable energy converters, installation of "new" devices without considering the whole picture (building + air conditioning system + energy conversion and supply chain) will hardly remedy the current situation. This study proposes a systemic approach to the joint design-and-analysis of {{space conditioning}} that leads to the identification of the thermodynamically most convenient configurations by integrating thermal building dissipation modeling, thermal consumption simulation and exergy efficiency calculation. The first step is the modeling of thermal characteristics of the building: the obtained temperature maps within the room are <b>then</b> used <b>to</b> compute, <b>for</b> <b>each</b> type of heating element, the actual thermal power required to ensure comfortable indoor temperature. The second step is the simulation of the "external" plant providing the prescribed thermal power, and is carried out by means of a process simulator: such a "global" simulation enables designers and decision makers to compare several feasible different combinations of internal and external systems and to identify the most exergetically convenient pairings. (C) 2013 Elsevier B. V. All rights reserved...|$|R
40|$|In textual analysis, many corpora include texts {{which have}} a chronological order and this {{temporal}} connotation is crucial to understand their structure. Common examples are: addresses delivered by institutional representatives in different years, articles retrieved from newspaper archives, literary works written by an author during his/her active life, essays written by a student in different steps of his/her educational experience, etc. In many applications, the temporal evolution of key topics, words, concepts, etc. are important in order to highlight the distinctive features of the chronological corpus. In a typical bag-of-words approach data are organized in word-type x time-point contingency tables which show the occurrences of each (word-type, time-point) crossing. It is of crucial concern, <b>then,</b> <b>to</b> recognize, <b>for</b> <b>each</b> word-type, the specific sequential pattern {{as well as to}} determine prototype patterns suitable for clustering word-types portraying a similar evolution. A very large number of word-types (vocabulary entries), often characterized by a relatively low number of observed time occurrences (commonly known as large p small n problem), moreover, sparsely spaced over time, poses many challenges from both statistical and computational perspectives. In this context sparsity is represented by a large number of zeros for most of the time interval length; these zero-cells are due to the large number of word-types with low number of corresponding word-tokens (intrinsic feature of textual data) {{as well as to the}} size of time-point subcorpora (the richness of information is highly variable across time in terms of different number of documents and of word-tokens therein). Such discrete data can be thought of as continuous objects represented by functional relationships. In functional data analysis the center of interest is a set of curves, shapes, objects or, more generally, a set of functional observations. The aim of this study is twofold: 1. to identify a specific sequential pattern <b>for</b> <b>each</b> word as a functional object; 2. to partition these curves (word patterns) in clusters, involving the definition of a suitable measure of similarity between curves. We propose a flexible model-based procedure with specific reference to a corpus of end-of-year addresses delivered by the ten Presidents of the Italian Republic in the period 1949 - 2010...|$|R
40|$|This study {{provides}} statistical emulators of crop yields based on global gridded crop model simulations from the Inter-Sectoral Impact Model Intercomparison Project Fast Track project. The ensemble of simulations {{is used to}} build a panel of annual crop yields from five crop models and corresponding monthly summer weather variables {{for over a century}} at the grid cell level globally. This dataset is <b>then</b> used <b>to</b> estimate, <b>for</b> <b>each</b> crop and gridded crop model, the statistical relationship between yields, temperature, precipitation and carbon dioxide. This study considers a new functional form to better capture the non-linear response of yields <b>to</b> weather, especially <b>for</b> extreme temperature and precipitation events. In- and out-of-sample validations show that the statistical emulators are able to closely replicate crop yields projected by crop models and perform well out-of-sample. This study therefore provides a reliable and accessible alternative to global gridded crop yield models. By emulating crop yields for several models using parsimonious equations, the tools provide a computationally efficient method <b>to</b> account <b>for</b> uncertainty in climate change impact assessments. We gratefully acknowledge the financial support for this work from the U. S. Department of Energy, Office of Science under DE-FG 02 - 94 ER 61937, the U. S. Environmental Protection Agency under XA- 83600001 - 1 and XA- 835055101 - 2, and other government, industry, and foundation sponsors of the Joint Program on the Science and Policy of Global Change...|$|R
40|$|In {{the current}} {{economic}} and industrial conditions, with demand ever fluctuating and optimization more important in every field, designing a timetable to define a personnel schedule {{is not an easy}} task. Personnel scheduling is the process of constructing work timetables for its staff. The first part of this process involves determining the number of staff, with particular skills, needed to meet the demand. Individual staff members are allocated to shifts {{in order to meet the}} required staffing levels at different times, and duties are <b>then</b> assigned <b>to</b> individuals <b>for</b> <b>each</b> shift. It is really difficult to find good solutions to these highly constrained and complex problems and even more difficult to determine optimal solutions that minimize costs, meet employee preferences, distribute shifts equitably among employees and satisfy all the workplace constraints. In many organizations, the people involved in developing personnel scheduling need decision support tools to help to provide the right employees at the right time and at the right cost. In general, the unique characteristics of each industry and organization mean that specific mathematical models and algorithms must be developed for personnel scheduling solutions in different areas of application. There is a large number of commercial software packages but sometimes it is not possible to use these in companies with specific features. Accordingly, mathematical programming, optimization and heuristics are suggested to be utilized in order to solve the personnel scheduling problem...|$|R
40|$|Poster (1 page); [URL] audienceAuthorities {{encourage}} people to reduce fat and sugar consumption in public campaigns such as the National Nutritional Health Program in France. French producers are also encouraged to improve the nutritional composition of well-known commercial products by reducing fat and/or sugar contents. The objective of our {{study was to determine}} whether it was possible to do so while maintaining the sensory quality of the reformulated products. The study dealt with the impact of fat and sugar reduction on liking and sensory perception of 6 types of French commercial biscuits and cakes. <b>For</b> <b>each</b> type of product, one example of the French market was studied. Results show that fat- or sugar-reduced biscuits are mainly less appreciated than standard ones when they are perceived as less sweet. Besides, results depend on products. Oqali, the nutritional section of the French food observatory, records the nutritional composition of most food items from the French market in a given set of food categories, including the biscuit one. Thus, <b>for</b> <b>each</b> type of biscuits of our study, plots were produced with the levels of fat and sugar content of most biscuits from the French market. Products from our study were superimposed on these plots in order to define a limit of fat and/or sugar reduction above which liking would be significantly deteriorated. This limit underlines that {{it is not possible to}} reduce fat and sugar content on biscuits endlessly and that product taste should always be taken into account when reformulating. It is <b>then</b> interesting <b>to</b> assess <b>for</b> <b>each</b> biscuit type where the current offer of the market is located relatively to this limit. This information could be used by the food industry to foresee if they could improve the nutritional quality of their products while maintaining their sensory quality...|$|R
40|$|International audienceFor each {{quartile}} {{of income}} per household or per consumption unit, annual time-series have been estimated from panel surveys, with annual waves of observations from 1974 to 2007 : INSEE 2 Households' "Conjoncture" survey from 1974 to 1994; panel "Parc-Auto" 3 Sofres 4 since mid- 80 's. In these data sources, household behaviour is described through; car ownership (percentage of households {{with at least}} one car, of which percentage of multi-car households, average number of cars per adult over 18, which is the minimum age for driving license in France); car use (annual mileage per household or per car). The repeated sample structure of data has been used for improving the accuracy of time-series of variables highly correlated for subsequent years [Cochran, 1977]. In mid- 70 's, car ownership and use were quite low for the poorest income quartile, but the difference has much decreased with all the three higher income groups, which are more homogeneous. Thus, multi-car ownership, which is mainly structured by geographic and demographic determinants, has slowed down -but not reversed- the social diffusion of automobile. As the curves representing car ownership (number of cars per adult) and car use (annual mileage per household) seem to become quite horizontal during the most recent period, logistic curves have been estimated according <b>to</b> time, <b>then</b> <b>to</b> real income. <b>For</b> <b>each</b> quartile of the distribution of households by income per consumption unit, saturation thresholds are estimated, as well as the date or the point of inflection. Follows a discussion on the legitimacy of pooling the data. The relationship between temporal elasticities (<b>for</b> <b>each</b> quartile) and cross-sectional income elasticities, which can be considered as a measurement of inequality at each point in time, will be discussed [Gardes and Madre, 2005]...|$|R
40|$|The {{equilibrium}} {{terrestrial biosphere}} model BIOME 3 simulates vegetation distribution and biogeochemistry, and couples vegetation distribution directly to biogeochemistry. Model inputs consist of latitude, soil texture class, and monthly climate (temperature, precipitation, and sunshine) {{data on a}} 0. 5 degrees grid. Ecophysiological constraints determine which plant functional types (PFTs) may potentially occur. A coupled carbon and water flux model is <b>then</b> used <b>to</b> calculate, <b>for</b> <b>each</b> PFT, the leaf area index (LAI) that maximizes net primary production (NPP), subject to the constraint that NPP must be sufficient to maintain this LAI. Competition between PFTs is simulated by using the optimal NPP of each PFT as an index of competitiveness, with additional rules to approximate the dynamic equilibrium between natural disturbance and succession driven by light competition. Model output consists of a quantitative vegetation state description {{in terms of the}} dominant PFT, secondary PFTs present, and the total LAI and NPP for the ecosystem. Canopy conductance is treated {{as a function of the}} calculated optimal photosynthetic rate and water stress. Regional evapotranspiration is calculated as a function of canopy conductance, equilibrium evapotranspiration rate, and soil moisture using a simple planetary boundary layer parameterization. This scheme results in a two-way coupling of the carbon and water fluxes through canopy conductance, allowing simulation of the response of photosynthesis, stomatal conductance, and leaf area to environmental factors including atmospheric CO 2. Comparison with the mapped distribution of global vegetation shows that the model successfully reproduces the broad-scale patterns in potential natural vegetation distribution. Comparison with NPP measurements, and with an FPAR (fractional absorbed photosynthetically active radiation) climatology based on remotely sensed greenness measurements, provides further checks on the model's internal logic. The model is envisaged as a tool for integrated analysis of the impacts of changes in climate and CO 2 on ecosystem structure and function...|$|R
40|$|The use of {{numerical}} methods, such as {{finite element}} method (FEM), has been widely adopted in solving structural problems with complex geometry under external loads when analytical solutions are unachievable. Basic idea behind FEM is to divide the complex body geometry into smaller and simpler domains, called finite elements, and <b>then</b> <b>to</b> formulate solution <b>for</b> <b>each</b> element instead of seeking a solution for the entire domain. After finding the solutions for all elements they can be combined to obtain a solution for the whole domain. This numerical method is mostly used in engineering, {{but it is also}} useful for studying the biomechanical properties of materials used in medicine and the influence of mechanical forces on the biological systems. Since its introduction in dentistry four decades ago, FEM became powerful tool for the predictions of stress and strain distribution on teeth, dentures, implants and surrounding bone. FEM can indicate aspects of biomaterials and human tissues that can hardly be measured in vivo and can predict the stress distribution in the contact areas which are not accessible, such as areas between the implant and cortical bone, denture and gingiva, or around the apex of the implant in trabecular bone. Aim {{of this paper is to}} present - using results of several successful FEM studies - the usefulness of this method in solving dentistry problems, as well as discussing practical aspects of FEM applications in dentistry. Some of the method limitations, such as impossibility of complete replication of clinical conditions and need for simplified assumptions regarding loads and materials modeling, are also presented. However, the emphasis is on FE modelling of teeth, bone, dentures and implants and their modifications according to the requirements. All presented studies have been carried out in commercial software for FE analysis ANSYS Workbench...|$|R
40|$|The space {{conditioning}} sector {{is at the}} same time one of the highest energy consumers and one of the least efficient from the point of view of primary-to-end-use matching. Both in general and on the average, the gross inadequacy of building structures and insulation combines with an equally gross ill-management of {{space conditioning}} plants. In spite of the emergence in the last decade of innovative technical solutions based on renewable energy converters, installing them without considering the whole air conditioning system will hardly remedy the current situation. The present study proposes a systemic approach to the joint design-and-analysis of space conditioning (building + heating/cooling element + primary energy supply system) that leads to the correct identification of the thermodynamically most convenient configurations. The method consists in a complete and rational integration of thermal building dissipation modelling, thermal consumption simulation and exergy efficiency calculation. In the selection of the "optimal" solution for an air conditioning system different factors should be considered: first the thermal characteristics of the building <b>to</b> be conditioned, <b>then</b> the type of heating element and finally the type of the primary energy conversion system. The first step is the correct modelling of both the geometry and thermal characteristics of the building. For the sake of simplicity, the application discussed here is based on an extremely simplified geometry (a cubic room), on which thermo-fluidodynamic simulations of the effects of heaters elements have been performed via a commercial CFD code (Fluent®). The temperature maps within the room obtained via these CFD simulations are <b>then</b> used <b>to</b> compute, <b>for</b> <b>each</b> type of heating element, the actual thermal power required to meet the environmental comfort standards. The second step is the simulation of the "external" plant needed to provide the prescribed thermal power, and it was carried out by means of a general purpose process simulator (CAMEL-Pro™) : this global simulation enables the designer to compare the performance of all feasible different combinations of internal and external systems and to identify the most exergetically convenient pairings (for a better end-use/primary source matching) ...|$|R
40|$|In {{the context}} of {{classification}} using high-dimensional data such as microarray gene expression data, it is often useful to perform preliminary variable selection. For example, the k-nearest-neighbors classification procedure yields a much higher accuracy when applied on variables with high discriminatory power. Typical (univariate) variable selection methods for binary classification are, e. g., the two-sample t-statistic or the Mann-Whitney test. In small sample settings, the classification error rate is often estimated using cross-validation (CV) or related approaches. The variable selection procedure has <b>then</b> <b>to</b> be applied <b>for</b> <b>each</b> considered training set anew, i. e. <b>for</b> <b>each</b> CV iteration successively. Performing variable selection based on the whole sample before the CV procedure would yield a downwardly biased error rate estimate. CV may {{also be used to}} tune parameters involved in a classification method. For instance, the penalty parameter in penalized regression or the cost in support vector machines are most often selected using CV. This type of CV is usually denoted as "internal CV" in contrast to the "external CV" performed to estimate the error rate, while the term "nested CV" refers to the whole procedure embedding two CV loops. While variable selection and parameter tuning have been widely investigated in {{the context of}} high-dimensional classification, it is still unclear how they should be combined if a classification method involves both variable selection and parameter tuning. For example, the k-nearest-neighbors method usually requires variable selection and involves a tuning parameter: the number k of neighbors. It is well-known that variable selection should be repeated <b>for</b> <b>each</b> external CV iteration. But should we also repeat variable selection <b>for</b> <b>each</b> it internal CV iteration or rather perform tuning based on fixed subset of variables? While the first variant seems more natural, it implies a huge computational expense and its benefit in terms of error rate remains unknown. In this paper, we assess both variants quantitatively using real microarray data sets. We focus on two representative examples: k-nearest-neighbors (with k as tuning parameter) and Partial Least Squares dimension reduction followed by linear discriminant analysis (with the number of components as tuning parameter). We conclude that the more natural but computationally expensive variant with repeated variable selection does not necessarily lead to better accuracy and point out the potential pitfalls of both variants. ...|$|R
40|$|This {{dissertation}} develops {{and tests}} {{a model of}} implementation of related diversification strategies which focuses on design of effective compensation plans for division general managers. Prior authors have found greater use of discretionary and group-performance bonuses in association with greater magnitudes of divisional sharing {{to have a positive}} impact on effectiveness. However, seminal works in the strategy implementation area suggest that, besides magnitude of sharing (which reflects corporate choices about strategy), the relative centralization and directionality of resource sharing flows to and from a focal division (which reflect corporate choices about structure) win also have important compensation design implications. The purpose of the present study was to investigate the role of magnitude, centralization, and directionality of intra-corporate sharing as contingency factors in the design of effective compensation systems for division general managers. To be able to consider the joint implications of these three factors, the dissertation proposed, first, to classify divisional sharing activities into three organizational types: centralized, decentralized-inflow, and decentralized-outflow forms of sharing and, <b>then,</b> <b>to</b> measure magnitude <b>for</b> <b>each</b> of these types. A theoretical model was developed which accounts for the different challenges involved in the control of division managers under each form of sharing. Next, formal results from the theory of contracts literature in economics were integrated into the analysis, and a series of testable hypotheses were formulated about the impact that different combinations of compensation features and resource sharing forms would have on divisional effectiveness. Hypotheses were tested with a sample of 106 divisions of large, related diversified, manufacturing firms. Results were largely supportive of the hypotheses and strongly suggest that effective implementation of different forms of sharing require the use of different compensation solutions at the division manager 2 ̆ 7 s level. Consistent with extant theory of corporate strategy implementation, effective implementation of decentralized-outflow sharing was found to be positively related to the use of discretionary and group-performance bonuses. However, contrary to accepted theory, effective implementation of decentralized-inflow sharing was found to be negatively related to the use of either discretionary or group-performance bonuses. Effective implementation of centralized sharing, in turn, was found to be associated with the use of lower levels of incentive pay, but not with the use of either discretionary or group-performance bonuses. ...|$|R
40|$|Le modèle mathématique décrit dans cet article a pour tâche de choisir les sites sur une rivière où des {{installations}} hydro-électriques seront aménagées, puis de trouver la taille optimale de ces installations. La solution de ce problème dépend naturellement du montant d'argent que la compagnie est prête à investir sur la rivière. Toutefois, ce montant n'est pas connu au départ puisqu'il est lui-même fonction de ce que les installations pourront produire. Il est donc nécessaire de résoudre le problème pour tous les niveaux possibles de production étant donné qu'on ne connaît pas le niveau qui sera choisi. Ce problème est résolu dans cet article par une méthode très efficace qui regroupe l'énumération implicite, la programmation linéaire successive et l'analyse paramétrique. De façon succincte, l'énumération implicite fait le choix des sites qui seront aménagés pour un niveau de production donné. La programmation linéaire successive, quant à elle, se charge de déterminer la taille optimale des installations. Enfin, l'analyse paramétrique montre comment la taille des installations varie avec le niveau de production. L'efficacité de cette méthode vient du fait que l'algorithme d'énumération implicite, qui consomme beaucoup de temps de calcul, est appelé un nombre minimal de fois. The {{purpose of}} the work described in this paper {{was to find a}} method for identifying the development scheme of a valley that will allow a utility to meet its electrical energy need at minimum cost. Suppose, for instance, that the utility wishes to build hydroelectric power plants in a virgin river valley so as to produce D gigawatthours of firm energy each year, and that preliminary surveys of the river show n possible sites. The first step is to select the sites and <b>then</b> <b>to</b> determine, <b>for</b> <b>each,</b> the dam height and hence size of the reservoir, elevation of the water inlet, elevation of the water outlet, and capacity of the power plant. The selection has to be clone in terms of minimizing the investment cost. One of the utility's major difficulties with the above problem is to determine the value of D and hence the amount of money to invest in the valley. The solution depends on the alternatives to such an investment : a nuclear or thermal power plant or development of another valley, for example. In fact, the only completety reliable way to determine the value of D is to consider all the investment possibilities. The question is how do it without overly magnifying the problem ? One way, and incidentally the easiest, is to use a two-step solution : first, determine the optimal development scheme <b>for</b> <b>each</b> river <b>for</b> ail possible values of D, then find the optimal investment policy among the valleys and hence the optimal value of D. In this paper, only the first step is solved. Determination of the optimal development scheme of a valley for ail possible values of D is formulated here as a parametric mixed-integer linear programming problem, which takes the form:minimize cxsubject toax ≽ b + Ɵhx ≽ 0 x-= 0 or 1 where c and x are n vectors, A is an m by n matrix, b is an m vector, h is a change vector of dimension m, and 0 is a scalar that is varied continuously. x is a subvector of x containing those elements that must be 0 or 1. This formulation was chosen despite the nonlinearity of the real problem, in order {{to take advantage of the}} relative computational efficiency of mixed-integer linear programming as offered by IBM's MPSX/ 370 package. The nonlinear functions, like the costs, the production, and the relation between reservoir level and content, were linearized using separable programming in some cases and successive linear approximation in others. Since MPSX/ 370 inhibits the simultaneous use of separable and mixed-integer programming, the author wrote his on branch-and-bound algorithm. The parametric analyses are clone in such a way that the branch-and-bound algorithm, which consumes most of the computer time, is called a minimum number of times...|$|R
40|$|Jalil Belghazi, Ramzi N El Feghali, Thérèse Moussalem, Maya Rejdych, Roland G AsmarThe CardioVascular Institute, Paris, FranceBackground: Four {{electronic}} devices for self-measurement of brachial blood pressure (BP) : the Omron M 1 Plus, the Omron M 6 Comfort, the Spengler KP 7500 D, and the Microlife BP A 100 Plus, were evaluated in four separate studies {{according to the}} International Protocol of the European Society of Hypertension (ESH). Design: The International Validation Protocol is divided into 2 phases: the first phase is performed on 15 selected subjects (45 pairs of BP measurements); if the device passes this phase, 18 supplementary subjects are included (54 pairs of BP measurements) making a total number of 33 subjects (99 pairs of BP measurements) on which the final validation is performed. Methods: The same methodology recommended by the ESH protocol was applied for the 4 studies. In <b>each</b> study and <b>for</b> <b>each</b> subject, 4 BP measurements were performed simultaneously by 2 trained observers using mercury sphygmomanometers alternately with 3 measurements by the tested device. The difference between the BP value given by the device and that obtained by the two observers (mean of the two observers) was calculated <b>for</b> <b>each</b> measure. The 99 pairs of BP differences were classified into 3 categories (≤ 5, ≤ 10, ≤ 15 mmHg). The number of differences in each category was compared with the number required by the International Protocol. An individual analysis was <b>then</b> done <b>to</b> determine <b>for</b> <b>each</b> subject the number of comparisons ≤ 5 mmHg. At least 22 of the 33 subjects should have 2 of their 3 comparisons ≤ 5 mmHg. Results: All 4 tested devices passed the fi rst and {{the second phase of}} the validation process. The average differences between the device and mercury sphygmomanometer readings were – 1. 4 ± 5. 5 and – 0. 4 ± 4. 8 mmHg for SBP and DBP respectively for the Omron M 1 Plus device, – 2. 1 ± 7. 4 and 0. 1 ± 4. 9 mmHg for SBP and DBP respectively for the Omron M 6 Comfort device, – 1. 4 ± 8. 6 and – 0. 1 ± 3. 5 mmHg for SBP and DBP respectively for the Spengler KP 7500 D device, and 1. 6 ± 4. 2 mmHg and 0. 54 ± 2. 8 mmHg for SBP and DBP respectively for the Microlife BP A 100 Plus device. For all devices, readings differing by less than 5, 10, and 15 mmHg for SBP and DBP values fulfill the recommendation criteria of the International Protocol as well as the individual analysis. Conclusions: Omron M 1 Plus (HEM- 4011 C-E), Omron M 6 Comfort (HEM 7000 -E), Spengler KP 7500 D, and Microlife BP A 100 Plus devices fulfilled the validation recommendations of the International Protocol. Keywords: Omron M 1 Plus (HEM- 4011 C-E), Omron M 6 Comfort (HEM- 7000 -E), Spengler KP 7500 D and Microlife BP A 100 Plus, validation, International Protocol, self-blood pressure measuremen...|$|R
40|$|Electrostatic forces {{strongly}} {{influence the}} behavior of granular materials in both dispersed (cloud) systems and semi-packed systems. These forces can cause aggregation or dispersion of particles and are important {{in a variety of}} astrophysical and planetary settings. There are also many industrial and commercial settings where granular matter and electrostatics become partners for both good and bad. This partnership is important for human exploration on Mars where dust adheres to suits, machines, and habitats. Long-range Coulombic (electrostatic) forces, as opposed to contact-induced dipoles and van der Waals attractions, are generally regarded as resulting from net charge. We have proposed that in addition to net charge interactions, randomly distributed charge carriers on grains will result in a dipole moment regardless of any net charge. If grains are unconfined, or fluidized, they will rotate so that the dipole always induces attraction between grains. Aggregates are readily formed, and Coulombic polarity resulting from the dipole produces end-to-end stacking of grains to form filamentary aggregates. This has been demonstrated in USML experiments on Space Shuttle where microgravity facilitated the unmasking of static forces. It has also been demonstrated in a computer model using grains with charge carriers of both sign. Model results very closely resembled micro-g results with actual sand grains. Further computer modeling of the aggregation process has been conducted to improve our understanding of the aggregation process, and to provide a predictive tool for microgravity experiments slated for Space Station. These experiments will attempt to prove the dipole concept as outlined above. We have considerably enhanced the original computer model: refinements to the algorithm have improved the fidelity of grain behavior during grain contact, special {{attention has been paid to}} simulation time steps to enable establishment of a meaningful, quantitative time axis, and calibration of rounding accuracies have been conducted to test cumulative numerical influences in the model. The model has been run for larger grain populations, variable initial cloud densities, and we have introduced random net charging to individual grains, as well as a net charge to the cloud as a whole. The model uses 3 positive and 3 negative charges randomly distributed on each grain, with up to 160 grains contained within various size "boxes" that define the initial number densities in the clouds. Each charge represents localized charged region on a grain, but does not necessarily imply single quantized charge carriers. The Coulomb equations are <b>then</b> allowed <b>to</b> interact <b>for</b> <b>each</b> monopole: dipoles and any higher order charge coupling is a natural product of these "free" interactions over which the modeler exerts no influence. The charges are placed on surfaces of grains at random locations. A series of runs was conducted for neutral grains that had a perfect balance of negative and positive char carriers. Runs were also conducted with grains having additional fractional charges ranging between 0 and 1. By adding fractional charges of one sign, the model created grain populations in which all grains had excess charges the same sign, giving the cloud an overall net charge. This simulates clouds subjected to ionizing radiation (e. protoplanetary debris disk around a protosun), or any other process of charge biasing in a grain population (e. g., volcanic plumes). In another run series, random fractional charges of either sign were added to the grains so th some grains had a slight net positive charge while others had a slight net negative charge. This simulates triboelectrically-charged grain populations in which acquisition of an electron by one surface is at the expense creating a hole elsewhere. This dual sign charging was applied in two ways: in one case the cloud remained neutral by ensuring that all grain excess charges added to zero; in the other case, the cloud was permitted slight net char by not imposing a charge-balance condition. Additional information is contained in the original...|$|R
3000|$|The idea is <b>to,</b> <b>for</b> <b>each</b> cell i, {{go through}} the {{candidate}} numbers of sub-bands, and, <b>for</b> <b>each</b> m between 1 and K, power levels 1,…,L [...]...|$|R
30|$|The kriged {{values were}} <b>then</b> used <b>to</b> produce maps <b>for</b> <b>each</b> {{groundwater}} properties.|$|R
3000|$|Lemma 2.15 [16]. Assume (A 1) is satisfied, {{and there}} is a k 0 > 0 so as <b>to</b> <b>for</b> <b>each</b> k ≥ k 0, there exist ρ [...]...|$|R
50|$|Provide {{clinical}} summaries <b>to</b> patients <b>for</b> <b>each</b> office visit.|$|R
30|$|MTTR = mean time <b>to</b> repair <b>for</b> <b>each</b> of the RVS machine subsystems.|$|R
50|$|Students {{are given}} roles or jobs <b>to</b> {{complete}} <b>for</b> <b>each</b> group meeting.|$|R
5000|$|... #Caption: Players must {{select a}} minigame <b>to</b> {{complete}} <b>for</b> <b>each</b> in-game day.|$|R
50|$|After his collegiate career, Hutson was {{a second}} round draft pick by the Milwaukee Bucks in the 2001 NBA Draft. Hutson <b>then</b> moved <b>to</b> Italy <b>for</b> the 2001-02 season, signed by Basket Napoli. <b>Then</b> moved <b>to</b> Greece <b>for</b> the 2002-03 season, in January 2003, Hutson signed with Peristeri Athens. Signed for the 2003-04 season by Maroussi Athens. Signed for the 2004-05 season by Makedonikos BC. <b>Then</b> moved <b>to</b> Russia <b>for</b> the 2005-06 season, signed by Ural Great Perm. <b>Then</b> back <b>to</b> Greece <b>for</b> the 2006-07 season, signed by Panionios BC. <b>Then</b> moved <b>to</b> Turkey <b>for</b> the 2007-08 season, signed by Efes Pilsen S.K.. <b>Then</b> back <b>to</b> Italy <b>for</b> the 2008-09 season, signed by Virtus Roma.|$|R
3000|$|... <b>for</b> <b>each</b> event. This {{approach}} enables multiple events <b>to</b> occur <b>for</b> <b>each</b> frame {{but only}} keeps the most noticeable ones.|$|R
5000|$|Repeat step 3 <b>to</b> 4 <b>for</b> <b>each</b> k-letter word in {{the query}} sequence.|$|R
3000|$|... <b>to</b> {{estimate}} throughput <b>for</b> <b>each</b> user, {{which can}} <b>then</b> be used <b>to</b> compute total throughput at a grid point.|$|R
30|$|Repeat steps 1 <b>to</b> 5 <b>for</b> <b>each</b> set of features, time-frequency footprint, and SNR.|$|R
