8|10000|Public
40|$|Nowadays, the world’s {{scientific}} {{community has been}} publishing {{an enormous number of}} papers in different scientific fields. In such environment, it is essential to know which databases are equally efficient and objective for literature searches. It seems that two most extensive databases are Web of Science and Scopus. Besides searching the literature, these <b>two</b> <b>databases</b> <b>used</b> to rank journals in terms of their productivity and the total citations received to indicate the journals impact, prestige or influence. This article attempts to provide a comprehensive comparison of these databases to answer frequent questions which researchers ask, such as: How Web of Science and Scopus are different? In which aspects these two databases are similar? Or, if the researcher...|$|E
40|$|Nowadays, the world‘s {{scientific}} {{community has been}} publishing {{an enormous number of}} papers in different scientific fields. In such environment, it is essential to know which databases are equally efficient and objective for literature searches. It seems that two most extensive databases are Web of Science and Scopus. Besides searching the literature, these <b>two</b> <b>databases</b> <b>used</b> to rank journals in terms of their productivity and the total citations received to indicate the journals impact, prestige or influence. This article attempts to provide a comprehensive comparison of these databases to answer frequent questions which researchers ask, such as: How Web of Science and Scopus are different? In which aspects these two databases are similar? Or, if the researchers are forced to choose one of them, which one should they prefer? For answering these questions, these two databases will be compared based on their qualitative and quantitative characteristics...|$|E
40|$|Nowadays, both {{computer}} vision researchers and psychology experts show an increased interest for human facial expression analysis. Despite the {{huge amount of}} research that has been dedicated to this area, {{almost all of them}} concern data recorded in controlled laboratory conditions, which does not always reflect the real world environment in which the human face is partially occluded. Six basic facial expressions are investigated in that case, i. e. when the eyes and eyebrows or the mouth regions are left out. We are interested in finding which part of the face comprised sufficient information with respect to the entire face, in order to correctly classify these six expressions. Each image from the <b>two</b> <b>databases</b> <b>used</b> is convolved with a set of Gabor filters having various orientations and frequencies. The new feature vectors are classified using a maximum correlation classifier and the cosine similarity measure approaches. Overall, the method provides robustness against partial occlusion...|$|E
50|$|This add-on in Ulteo OVD 3 allowed {{setting up}} two {{physical}} Session Managers and databases in a cold-standby cluster. Data was replicated between the <b>two</b> <b>databases</b> <b>using</b> DRBD, and failover was {{handled by the}} Heartbeat cluster manager. High Availability was a Gold module. It is no longer included in the source code for OVD 4, nor available from the Premium repository.|$|R
30|$|<b>Two</b> <b>databases</b> were <b>used</b> {{to analyze}} the {{performance}} of the proposed spectral ratio method for whispered speech detection namely, the CHAINS corpus and a whispered speech corpus collected over the cell phone. In the subsequent sections, the description of the <b>databases</b> <b>used</b> and the experimental results on whisper detection using ROC curves are described. The performance of the proposed whisper detection algorithm is also presented as the whisper diarization error rate.|$|R
30|$|The <b>two</b> {{reference}} <b>databases</b> were <b>used</b> in the imputation {{steps in}} the model described below and constitute the data drivers behind the imputation/copula individual tree model developed in this study.|$|R
30|$|The <b>two</b> <b>databases</b> <b>used</b> for {{identification}} are composed by images captured under controlled environments. The FERET database[34] contains images {{with a lot}} of variations in expression, lighting and aging, divided into five subsets: Fa (gallery set), composed by frontal images of 1, 196 subjects; Fb containing 1, 195 face images with variations in expression; Fc subset, which contains 194 images with variations in lighting; Dup-I with 722 face images taken with an elapsed time with respect to the images in the gallery set; and Dup-II, a subset of Dup I, which contains 234 images in which the elapsed time is at least 1 year. On the other hand, the AR database[35] was created to test face recognition methods in front of various expressions, different illuminations and occlusions. It contains more than 3, 200 face images of 126 people captured on two different sessions. Each person has up to 13 images per session. We randomly selected 100 different subjects (50 males and 50 females) and the neutral expression image of every person in each session was used as gallery {{and the rest of them}} with different expressions, lighting and occlusions were used for testing. Images from both databases were cropped to 114 × 114. Thus, using a coarse block division of 6 × 6, the blocks size will be 19 × 19.|$|E
40|$|This paper {{addresses}} {{the relation between}} international trade and employment in Portugal {{with regard to the}} labour content of trade in intermediates. It considers both the overall level of employment and labour disaggregated by skills (high-skill, medium-skill and low-skill). The assessment makes use of the newly developed internationally linked inputoutput (IO) database named World Input-Output Database (WIOD), complemented with the Socio-Economic Accounts (SEA) for skill-types of labour. The period analysed – 1995 - 2009 - is the longest possible taking into account the <b>two</b> <b>databases</b> <b>used.</b> The amount of labour required to produce imported intermediates (exported intermediates) is taken as a proxy to the job effect of downward (upward) embeddedness of the country into Global Value Chains (GVCs). We conclude that intermediates’ exports are basically intensive in low-skilled labour although presenting a tendency to skill-upgrading during the period analysed, while intermediates’ imports are proportionally much more intensive in skilled labour, predominantly of a medium skill level, an expected result in a country of an intermediate level of development. We also concluded that the estimated net labour content of jobs in trade in intermediates in the final year of the period analysed was globally negative in 51 thousand jobs. Main net losses were observed with Brazil, People’s Republic of China and India, while main net gains were observed with Spain and France. info:eu-repo/semantics/publishedVersio...|$|E
40|$|The {{objective}} {{of this study is}} to develop a patch-based labeling method that cooperates with a label fusion using non-rigid registrations. We present a novel patch-based label fusion method, whose selected patches and their weights are calculated from a combination of similarity measures between patches using intensity-based distances and labeling-based distances, where a previous labeling of the target image is inferred through a label fusion method using non-rigid registrations. These combined similarity measures result in better selection of the patches, and their weights are more robust, which improves the segmentation results compared to other label fusion methods, including the conventional patch-based labeling method. To evaluate the performance and the robustness of the proposed label fusion method, we employ two available databases of T 1 -weighted (T 1 W) magnetic resonance imaging (MRI) of human brains. We compare our approach with other label fusion methods in the automatic hippocampal segmentation from T 1 W-MRI. Our label fusion method yields mean Dice coefficients of 0. 847 and 0. 798 for the <b>two</b> <b>databases</b> <b>used</b> with mean times of approximately 180 and 320 seconds, respectively. The collaboration between the patch-based labeling method and the label fusion using non-rigid registrations is given in the several levels: (a) The pre-selection of the patches in the atlases are improved, (b) The weights of our selected patches are also more robust, (c) our approach imposes geometrical restrictions, such as shape priors, and (d) the work-flow is very efficient. We show that the proposed approach is very competitive with respect to recently reported methods. Comment: 33 pages, 6 figure...|$|E
40|$|WdI和fAOSTAT数据库是研究跨国技术进步、技术扩散、对外投资常用的两个数据库。对这两个数据库的共性指标、使用现状、辅助数据库进行了介绍。以两个序列为例,介绍了WdI和fAOSTAT新版数据库的网络查询界面、检索方法及个性化功能使用技巧。WDI and FAOSTAT are <b>two</b> <b>databases</b> {{frequently}} <b>used</b> {{to study}} cross-country technological progress, technology diffusion, and foreign investment. The paper gives a brief introduction to their common features, status quo of application, and auxiliary <b>databases.</b> Taking <b>two</b> sequences for example, the paper also introduces {{new edition of}} WDI 's and FAOSTAT's databases' inquiry interface, retrieval methods and use skills for personalized functions...|$|R
40|$|Image-based {{approaches}} to sketch recognition typically cast sketch {{recognition as a}} machine learning problem. In systems that adopt image-based recognition, the collected ink is generally fed through a standard three stage pipeline consisting of the feature extraction, learning and classification steps. Although these approaches make regular use of machine learning, existing work falls short of presenting a proper treatment of important issues such as feature extraction, feature selection, feature combination, and classifier fusion. In this paper, we show that all these issues are significant factors, which substantially affect the ultimate performance of a sketch recognition engine. We support our case by experimental results obtained from <b>two</b> <b>databases</b> <b>using</b> representative sets of feature extraction, feature selection, classification, and classifier combination methods. We present {{the pros and cons}} of various choices that can be made while building sketch recognizers and discuss their trade-offs...|$|R
50|$|MacWhinney {{developed}} and directs the CHILDES and TalkBank corpora, <b>two</b> widely <b>used</b> <b>databases</b> for language acquisition research. He manages FluencyBank, a TalkBank project, together with Nan Bernstein Ratner.|$|R
40|$|Incidence of incurable {{disease and}} {{disability}} {{has been increasing}} in the Western world in recent years. Parallel to this increase, survival rates for {{adolescents and young adults}} with life threatening illness, specifically cancer, have not improved relative to younger and older age groups. Palliative care is a total care for patients, regardless the type and status of the illness, and aims to improve quality of life by controlling symptoms and alleviating physical social, psychological and spiritual suffering. The varying level of physical, emotional and psychological maturity makes palliative and supportive care needs of adolescents distinct and challenging.   In this systematic review, the aim was to describe the experiences of palliative and supportive care for adolescents with life threatening illness, from the perspectives of adolescents, family and nurses as providers of palliative and supportive care.   A systematic review of descriptive meta-synthesis was the appropriate method of choice concerning the aims focus on subjective experiences of palliative and supportive care for adolescents. PubMed and CINAHL were the <b>two</b> <b>databases</b> <b>used</b> for data sources of qualitative published articles between the years of 2006 - 2016.   A total of 1066 qualitative published articles were identified, but only 16 articles have met the inclusion criteria. Nine articles were focusing on adolescents’ experiences, whereas five articles were focusing on families’ experiences and the remaining two were on the nurses’ experiences in providing palliative and supportive care to adolescents. A total of eleven themes have emerged. Four themes were from adolescent’s perspective: preserving self or identity, social support, acceptance and feeling positive and care setting. Another four themes were on family’s perspective: social support, information and decision making, care setting and perception of self-image. Whereas the remaining three themes were on nurse’s perspective: importance of family dynamics, reaching out to adolescents, balancing professional and personal involvement.   In conclusion this systematic review meta-synthesis showed that social support, specifically peer’s support had an impact in enhancing coping with the altered body image and ordinary daily social activities of adolescents due to life threatening illnesses and their treatment. Nurses have a key professional role in facilitating the social network of social support to promote quality of life by maintaining the need for normality. ...|$|E
40|$|This paper {{presents}} {{some new}} features for the palmprint based authentication. The Region of interest (ROI) is {{extracted from the}} palmprint image by finding a tangent to the curves between fingers. The perpendicular bisector of this tangent and the tangent itself help demarcate the rectangular area that forms the ROI of the palmprint. Four approaches are presented for the feature extraction. In the first approach the ROI is divided into a suitable number of non-overlapping windows from which fuzzy features are extracted. In the second approach multi-scale wavelet decomposition is applied on the ROI and the detail images are combined to yield a composite image which is partitioned into non-overlapping windows and energy features are extracted. In the third approach sigmoid features are extracted from the ROI and in the fourth approach feature extraction is done using Local Binary Pattern (LBP) based on the directional gradient response. These four sets of features are used for the authentication of users from <b>two</b> <b>databases</b> <b>using</b> Euclidea...|$|R
40|$|Documentation {{service at}} the {{newspaper}} Berria: <b>two</b> <b>databases</b> for <b>use</b> by journalists. The main purpose of the Berria documentation services is to organise and store information, both that created by the newspaper itself and that received from outside, in its different formats, both digital (photos and text) and paper (newspapers, books). As a result, it has databases to manage the photographs and text from Berria, and a traditional library to store information on paper (mostly books, copies and bound paper editions of Berria) ...|$|R
40|$|International audienceThe {{motivation}} {{behind a}} spatio-temporal visual saliency model is to extract salient information from two distinct pathways: static (intensity) and dynamic (motion). Consequently, {{the information from}} these pathways is combined to get the ﬁnal visual saliency map. Since {{the response of the}} pathways is diﬀerent, the step of combination of the maps is important. As a consequence, we study six recent fusion techniques against <b>two</b> video <b>databases</b> <b>using</b> human eye positions from an eye tracker. A criterion is used to evaluate and underline the signiﬁcance of these fusion methods...|$|R
40|$|This paper investigates face {{recognition}} during facial expressions. While face-expressions {{have been treated}} as an adverse factor in standard {{face recognition}} approaches, our research suggests that if a system has a choice {{in the selection of}} faces to use in training and recognition, its best performance would be obtained on faces displaying expressions. Naturally, smiling faces are the most prevalent (among expressive faces) for both training and recognition in dynamic scenarios. We employ a measure of Discrimination Power that is computed from between- class and within-class scatter matrices. <b>Two</b> <b>databases</b> are <b>used</b> to show the performance differences on different sets of faces...|$|R
40|$|As {{our first}} step in {{compiling}} the data on sovereign wealth funds, we created a preliminary sample by combining the profiles of the funds published by J. P. Morgan (Fernandez and Eschweiler 2008) and Preqin (Friedman 2008). In the cases where the <b>two</b> <b>databases</b> <b>use</b> different names for the same sovereign wealth fund, we employ the fund address and related information to eliminate duplicates. We add five funds to the sample that {{were not included in}} these two compilations but are frequently described as sovereign wealth funds in {{at least one of the}} investment datasets noted below. This initial search yields a population of 69 institutions, including some sovereign wealth funds that have been announced but are not yet active. We then merge this initial sample of funds with the available data on characteristics of such funds and on their direct investments (and the investments of subsidiaries in which the fund has at least a 50 percent ownership stake). To extract transactions involving subsidiaries of sovereign wealth fund, we supplement our list of subsidiaries by employing ownership data in the Directory of Corporate Affiliations and Bureau van Dijk’s Orbis. Second, we track the investments of sovereign wealth funds. We sought such information in Dealogic’s M&A Analytics, SDC’s Platinum M&A, and Bureau van Dijk’s Zephyr...|$|R
40|$|Spatially based data {{representing}} soil hydraulic {{properties are}} available from the United States Natural Resources Conservation Service (NRCS) at several spatial resolutions. These <b>databases</b> can be <b>used</b> in physically based rainfall runoff models to predict watershed runoff. The advantages of using either a higher or lower resolution database have yet to be determined. In this study, using data from the Walnut Gulch Experimental Watershed in Southeast Arizona, the hydraulic properties from <b>two</b> NRCS <b>databases</b> are compared as well as their effects on the simulated runoff from the physically based rainfall runoff model KINEROS (Kinematic Runoff and Erosion). The <b>two</b> soils <b>databases</b> <b>used</b> in this study were the State Soil Geographic Data Base (STATSGO), and the Soil Survey Geographic Database (SSURGO) from Cochise County, Arizona. It was determined that the STATSGO did not represent a spatial averaging of the higher resolution SSURGO when the hydraulic parameters of the databases were compared. The difference between runoff predictions produced from the <b>two</b> <b>databases</b> did not increase with increasing watershed size. KINEROS was able to simulate observed runoff on 3 of 4 watersheds <b>using</b> the SSURGO <b>database</b> while it was unable to simulated observed runoff on any watershed <b>using</b> the STATSGO <b>database...</b>|$|R
40|$|It {{has been}} {{suggested}} that the data from bug repositories is not always in sync or complete compared to the logs detail-ing the actions of developers on source code. In this paper, we trace two sources of information relative to software bugs: the change logs of the actions of developers and the issues reported as bugs. The aim is to identify and quantify the discrepancies between the two sources in recording and storing the developer logs relative to bugs. Focussing on the <b>databases</b> produced by <b>two</b> mining soft-ware repository tools, CVSAnalY and Bicho, we use part of the SZZ algorithm to identify bugs and to compare how the ”defects-fixing changes ” are recorded in the <b>two</b> <b>databases.</b> We <b>use</b> a working example to show how to do so. The results indicate that there is a significant amount of information, not in sync when tracing bugs in the <b>two</b> <b>databases.</b> We, therefore, propose an automatic approach to re-align the <b>two</b> <b>databases,</b> so that the collected informa-tion is mirrored and in sync...|$|R
40|$|In {{this paper}} {{we present a}} novel {{approach}} for biometric identification using electroencephalogram (EEG) signals based on features extracted with the Hilbert-Huang Transform (HHT). The instantaneous amplitude and the instantaneous frequency were computed after the HHT, and these were then used to generate the features for classification. The proposed system was evaluated <b>using</b> <b>two</b> publicly available <b>databases</b> in scenarios where only a single electrode is used to provide biometric information. One database (with 122 subjects) has the users viewing a series of pictures while the other one (with 109 subjects) has the users performing motor/imagery tasks. Average identification accuracies of 96 % and 99 % were reached for these <b>two</b> <b>databases</b> respectively <b>using</b> only a single electrode. These compare favourably with previously published results employing {{a variety of other}} features and classification approaches...|$|R
40|$|An {{algorithm}} {{has been}} developed to retrieve snowfall rate (R) over the Continental United States (CONUS) using measurements from NOAA’s Advanced Microwave Sounding Unit (AMSU) or EUMATSAT’s Microwave Humidity Sounder (MHS). The algorithm derives snowfall rate through a linkage between R and Ice Water Path (I) which is retrieved using a two-stream Radiative Transfer Model (RTM). <b>Two</b> <b>databases</b> are <b>used</b> {{in the development of}} the algorithm. One is built to derive an empirical equation that connects I with NEXRAD reflectivity, Z. Then a set of I-R equations is established with the help of existing Z-R relations. A second database is developed to determine the I-R equation to adopt by comparing the retrieved snowfall rate and the hourly snowfall observations from some U. S. weather stations...|$|R
2500|$|... "An {{examination}} {{was conducted}} {{of all the}} original data collection forms, numbering over 1,800 forms, which included review by a translator. The original forms have the appearance of authenticity in variation of handwriting, language and manner of completion. The information contained on the forms was validated against the <b>two</b> numerical <b>databases</b> <b>used</b> in the study analyses. These numerical databases have been available to outside researchers and provided to them upon request since April 2007. Some minor, ordinary errors in transcription were detected, {{but they were not}} of variables that affected the study’s primary mortality analysis or causes of death. The review concluded that the data files used in the study accurately reflect the information collected on the original field surveys." ...|$|R
40|$|Abstract. Retinal fundus imaging {{is widely}} used for eye examinations. The {{acquired}} images provide a unique view on the eye vasculature. The analysis of the vasculature has a high importance especially for detecting cardiovascular diseases. We present a multiscale algorithm for automatic retinal blood vessel segmentation, which is considered as a requirement for the diagnosis of vascular diseases. The algorithm uses a Gaussian resolution hierarchy to decrease computational needs, and allows to use of the same methods to detect vessels of different diameters. The algorithm is tested on <b>two</b> public <b>databases</b> <b>using</b> a common notebook. The algorithm segmented each image in less than 20 s with a competitive accuracy over 93 % in both cases. This proves the applicability for medical applications. ...|$|R
5000|$|... "An {{examination}} {{was conducted}} {{of all the}} original data collection forms, numbering over 1,800 forms, which included review by a translator. The original forms have the appearance of authenticity in variation of handwriting, language and manner of completion. The information contained on the forms was validated against the <b>two</b> numerical <b>databases</b> <b>used</b> in the study analyses. These numerical databases have been available to outside researchers and provided to them upon request since April 2007. Some minor, ordinary errors in transcription were detected, {{but they were not}} of variables that affected the study’s primary mortality analysis or causes of death. The review concluded that the data files used in the study accurately reflect the information collected on the original field surveys." ...|$|R
40|$|AbstractMany organisations have {{separate}} and disperse {{set of data}} for many years. Disperse and separation data have negative impacts for the organizations. Thus, the concept of data warehousing has emerged. From data warehouse concept many different systems and disintegrated data can be modelled and become integrated data. Similarly, in medical data normally these data is not integrated well in the organisation and thus the analysing of the data is difficult. Therefore, this research has the following objectives: 1) to identify the data warehouse design specifications for medical data, 2) to implement a data warehouse <b>using</b> <b>two</b> types of <b>databases</b> <b>using</b> data for cardiovascular disease and 3) to develop applications dashboard for medical data analysis and modelling. In the development, the process of selection and classifying the best features of data for data warehouse are carried out. Cardiovascular disease dataset from the National Heart Institute (IJN) {{is used as a}} data problem. ETL software which is Pentaho, is used to combine all the various databases to create a data warehouse for the data integration process. The original data set is stored in a Microsoft Excel spread sheet and still in its original form without any processing. It <b>uses</b> <b>two</b> types of <b>database</b> <b>using</b> data integration process to create a data warehouse using medical data. Data warehouse development suitability model is validated <b>using</b> <b>two</b> different <b>databases.</b> It adopts the theory of Bill Inmon and lead to application dashboard through ETL software...|$|R
40|$|Abstract. State {{of the art}} 3 D {{simulation}} applications like virtual testbeds {{for space}} robotics, industrial automation or even forest inventory require a highly flexible but still real-time capable data management system. For this, we combine a high-performance internal simulation database with external object-oriented databases into a new real-time capable data management architecture for database-driven 3 D simulation systems. To achieve this, we apply well-known database techniques to a 3 D simulation system’s internal object-oriented data management. Such a simulation database can dynamically adopt completely new data schemata, even at runtime. New simulation applications can then be designed by putting a domain specific schema and the corresponding data into an otherwise ”empty ” simulation database. To seamlessly combine the <b>two</b> <b>databases</b> we <b>use</b> a flexible interface that synchronizes schema and data. ...|$|R
40|$|The {{combination}} of a) our changing understanding of genotypic and phenotypic {{classification of diseases}} and b) the rapid growth and expansion {{of the number of}} entries in <b>two</b> <b>databases</b> targeted toward clinicians resulted in the need to develop a flexible dynamic hierarchical classification system for genetic disorders. The <b>two</b> <b>databases</b> making <b>use</b> of this classification schemas are the GeneClinics (GC) database - www. geneclinics. org and the GeneTests (GT) database - www. genetests. org The GC and GT databases serve respsectively as the users manual and yellow pages of genetic testing. The GeneTests/GeneClinics (GT/GC) classification hierarchy is maintained as a simple set of parent/child relationships in a relational database. The hierarchy is generated in real time in response to a user request. It is not maintained as a set of members with relationships defined by characters that are parsed to determine the structure of the tree. The GT/GC classification hierarchy entries are handled as objects by the data maintenance and search tools and may have a number of attributes and associations that create a rich tool for defining and examining genetic disorder...|$|R
40|$|Objective. We {{sought to}} compile the most updated {{information}} {{on the use of}} marijuana during pregnancy and its effects on the neonate as well as the overall development of a child through all stages. Background. Many states have made marijuana either decriminalized or acceptable for medical use, which may potentially lead to increased marijuana use among pregnant women. It is essential to become aware of the effects that marijuana can have on neonates. Methods. Articles were selected from <b>two</b> <b>databases</b> <b>using</b> keywords. Searches were conducted for articles published between January 1, 2010 and October 1, 2015. Keywords used to search for the articles included: Marijuana, cannabis, pregnancy, gestation, neonatal, stillbirth and low birth weight. Results. Six publications were included in this review article. Each of the studies assessed different neonatal outcomes possibly affected by marijuana use during pregnancy including: birth weight 3 ̆c 2500 grams, very low birth weight 3 ̆c 1500 grams, NICU admission, five minute Apgar score, umbilical artery PH, mean gestational age at delivery, and stillbirths, of which all were shown to be statistically insignificant after adjusting for confounding factors. A significant effect was found regarding the possible negative impact on the immune system, and its effects on quality of life. Conclusion. This review offers a compilation of information for healthcare providers to take into account when discussing marijuana use during pregnancy with their patients. Many of the outcomes showed inconclusive results; further studies are critically needed to fully understand the effects of marijuana in neonates. Grants. No grants to disclose...|$|R
40|$|We {{present a}} study of four North Sámi adpositions {{that can be used}} as both prepositions and postpositions and thus be termed “ambipositions”. We advance three {{hypotheses}} concerning 1) dialectal differences in use of ambipositions in North Sámi, 2) differences between their use as prepositions and postpositions, and 3) a possible typological correlation between the frequency of ambipositions {{and the extent to which}} position is used to differentiate meaning, with North Sámi at the high end of this scale. Our study tests these hypotheses against <b>two</b> <b>databases</b> representing the <b>use</b> of ambipositions in newspapers and in literature...|$|R
40|$|Abstract. In {{order to}} aid domain experts during data integration, several schema {{matching}} techniques have been proposed. Despite the facilities provided by these techniques, mappings between database schemas are still made manually. We propose a methodology for mapping <b>two</b> relational <b>databases</b> that <b>uses</b> ontology matching techniques and {{takes advantage of}} tools like D 2 R Server and AgreementMaker for automating mapping generation and for enabling unified access to information. We present the results obtained by some ontology matching algorithms in this context, demonstrating the feasibility of this approach...|$|R
40|$|Data mining, {{referred}} to as knowledge discovery in databases (KDD), is the nontrivial process of identifying valid, novel and potentially useful patterns in data. Evolutionary algorithms are employed for representing knowledge in rules and causal structures determined by Bayesian networks. <b>Two</b> medical <b>databases</b> are <b>used</b> to learn the rules for representing the patterns of data {{in addition to the}} use of Bayesian networks as causality relationship models among the attributes. Advanced evolutionary algorithms such as generic genetic programming, evolutionary programming and genetic algorithms are used to conduct the learning task...|$|R
30|$|<b>Two</b> <b>databases</b> were <b>used</b> to {{evaluate}} the proposed method namely, the Chinese Academy of Sciences, Institute of Automation (CASIA) gait database: dataset B [57] and the Osaka University, Institute of Scientific and Industrial Research (OU-ISIR) gait database: datasets A and B [58]. The CASIA gait database is good for assessing the view variation effect on gait as it contains {{a large number of}} subjects taken from different viewing angles. The CASIA gait database consists of 124 subjects captured from 11 different angles. The viewing angles range from 0 ° to 180 °, separated by an interval of 18 °. There are ten walking sequences for each subject, with six samples containing subjects walking under normal condition, two samples with subjects walking with coats, and two samples with subjects carrying bags. Therefore, there are altogether 13, 640 (10 [*]×[*] 11 [*]×[*] 124) gait sequences in the database. All the images were cropped and normalized to 120 [*]×[*] 120 pixels.|$|R
40|$|This study {{analyzes}} the utilization {{and retention of}} Naval officers who have received Navy funding for their graduate education. <b>Two</b> <b>databases</b> are <b>used</b> to analyze utilization and retention: the 1993 officer master file and a cohort file of officers who were commissioned in 1980. The 1993 officer master file looks at utilization first by all subspecialties together, second by gender, and finally by designator. The cohort file is used to analyze both utilization and retention. The results indicate that, overall, the Navy receives a relatively good return on its investment. Specifically, the Restricted Line and Staff Corps officer communities have the best utilization rates. The study reveals that Unrestricted Line officers tend to have relatively lower utilization rates than officers in other communities. The cohort data indicate that fully-funded graduate education subspecialists generally have a higher retention rate than their counterparts without fully- funded graduate education. NANAU. S. Navy (U. S. N.) author...|$|R
40|$|Handwritten {{recognition}} {{is a very}} active research domain that led to several works in the literature for the Latin Writing. The current systems tendency is oriented toward the classifiers combination and the integration of multiple information sources. In this paper, we describe two approaches for Arabic handwritten recognition using optimized Multiple classifier system MCS. The first rests on cooperation and selection of feature set in MCS studying the effect of fusion methods on global system performance The second one used Diversity measures and individual accuracy classifier for selecting the best set of classifier; its chooses among the classifier set {{the one with the}} best performance and adds it to the selected classifiers subset. The performance in our approach is calculated using three diversity measures based on correlation between errors. On <b>two</b> <b>database</b> sets <b>using</b> 10 different classifiers, we then test the effect of: the criterion to be optimized (diversity measures), and fusion methods (voting, weighted voting and Behavior Knowledge Space). The experimental results presented are encouraging and open other perspectives in the domain of classifiers selection especially speaking for Arabic Handwritten word recognition...|$|R
40|$|Handwritten {{signatures}} {{are widely}} utilized {{as a form}} of personal recognition. However, they have the unfortunate shortcoming of being easily abused by those who would fake the identification or intent of an individual which might be very harmful. Therefore, the need for an automatic signature recognition system is crucial. In this paper, a signature recognition approach based on a probabilistic neural network (PNN) and wavelet transform average framing entropy (AFE) is proposed. The system was tested with a wavelet packet (WP) entropy denoted as a WP entropy neural network system (WPENN) and with a discrete wavelet transform (DWT) entropy denoted as a DWT entropy neural network system (DWENN). Our investigation was conducted over several wavelet families and different entropy types. Identification tasks, as well as verification tasks, were investigated for a comprehensive signature system study. Several other methods used in the literature were considered for comparison. <b>Two</b> <b>databases</b> were <b>used</b> for algorithm testing. The best recognition rate result was achieved by WPENN whereby the threshold entropy reached 92 %...|$|R
