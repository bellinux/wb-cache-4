329|370|Public
2500|$|A [...] "tagged" [...] PDF (see clause 14.8 in ISO 32000) {{includes}} document {{structure and}} semantics information to enable reliable <b>text</b> <b>extraction</b> and accessibility. Technically speaking, tagged PDF is a stylized {{use of the}} format that builds on the logical structure framework introduced in PDF 1.3. Tagged PDF defines a set of standard structure types and attributes that allow page content (text, graphics, and images) to be extracted and reused for other purposes.|$|E
50|$|A set of {{free tools}} {{designed}} to help WFC translators perform specific advanced functions such as <b>text</b> <b>extraction</b> and alignment.|$|E
50|$|The Recoll {{document}} {{conversion and}} <b>text</b> <b>extraction</b> architecture {{makes it easy}} to write new filters, and many document types are supported.|$|E
30|$|The rest of {{this paper}} is {{organized}} as follows: In Section  2, we introduce the <b>text</b> feature <b>extraction</b> method and its application in detail. Section  2 introduces the deep learning method and its application in <b>text</b> feature <b>extraction</b> and summarizes it in Section  3.|$|R
40|$|A general {{review of}} text {{localization}} methods. <b>Text</b> information <b>extraction</b> {{is a growing}} area of research. Enormous {{work has been done}} to efficiently and robustly extract the text regions from scene text images. Various <b>Text</b> <b>extractions</b> models have been defined comprising of various stages. Amongst them text localization is an important stage and thus researchers mainly focus on this stage. This paper provides a review of various techniques of text localization. The paper also includes a comparison of the performance of these techniques on the parameters such as precision rate and recall rate...|$|R
30|$|The {{application}} of deep learning method in <b>text</b> feature <b>extraction</b> is prospected and summarized.|$|R
5000|$|... iResearch Reporter - Commercial <b>Text</b> <b>Extraction</b> and Text Summarization system, free demo site accepts user-entered query, {{passes it}} on to Google search engine, {{retrieves}} multiple relevant documents, produces categorized, easily readable natural language summary reports covering multiple documents in retrieved set, all extracts linked to original documents on the Web, post-processing, entity extraction, event and relationship extraction, <b>text</b> <b>extraction,</b> extract clustering, linguistic analysis, multi-document, full text, natural language processing, categorization rules, clustering, linguistic analysis, text summary construction tool set.|$|E
5000|$|Populating the Semantic Web, Kristina Lerman, Cenk Gazen, Steven Minton, and Craig A. Knoblock. Proceedings of the AAAI 2004 Workshop on Advances in <b>Text</b> <b>Extraction</b> and Mining, 2004.|$|E
5000|$|<b>Text</b> <b>extraction</b> is {{the process}} of taking the most {{appropriate}} text to describe the graph. This contrasts with a normal search engine where an entire HTML page might be included.|$|E
40|$|Abstract: This paper {{presents}} a <b>text</b> block <b>extraction</b> algorithm that takes as its input {{a set of}} text lines of a given document, and partitions the text lines into a set of text blocks, where each text block {{is associated with a}} set of homogeneous formatting attributes, e. g. text-alignment, indentation. The <b>text</b> block <b>extraction</b> algorithm described in this paper is probability based. We adopt an engineering approach to systematically characterising the text block structures based on a large document image database, and develop statistical methods to extract the text block structures from the image. All the probabilities are estimated from an extensive training set of various kinds of measurements among the text lines, and among the text blocks in the training data set. The off-line probabilities estimated in the training then drive all decisions in the on-line <b>text</b> block <b>extraction.</b> An iterative, relaxation-like method is used to find the partitioning solution that maximizes the joint probability. To evaluate the performance of our <b>text</b> block <b>extraction</b> algorithm, we used a three-fold validation method and developed a quantitative performance measure. The algorithm was evaluated on the UW-III database of some 1600 scanned document image pages. The <b>text</b> block <b>extraction</b> algorithm identifies and segments 91 % of text blocks correctly...|$|R
30|$|Next, several deep {{learning}} methods, applications, improvement methods, and steps used for <b>text</b> feature <b>extraction</b> are introduced.|$|R
50|$|The Project began {{archiving}} Nordic-language {{literature in}} December 1992. As of 2015 it had accomplished digitization to provide graphical facsimiles of old {{works such as}} the Nordisk familjebok, and had accomplished, {{in whole or in}} part, the <b>text</b> <b>extractions</b> and copyediting of these as well as esteemed Latin works and English translations from Nordic authors—e.g., Carl August Hagberg's interpretations of Shakespeare's plays—and sheet music and other texts of cultural interest.|$|R
5000|$|Utilities — Several {{utilities}} {{components are}} implemented, including: <b>Text</b> <b>extraction</b> and merging, RTF to text conversion, encoding conversion, line-break conversion, term extraction, translation comparison, quality check, pseudo-translation, text re-writing, etc.|$|E
50|$|In {{semantics}} and <b>text</b> <b>extraction,</b> {{name resolution}} {{refers to the}} ability of text mining software to determine which actual person, actor, or object a particular use of a name refers to. It can also be referred to as entity resolution.|$|E
50|$|By 2001, technology—image {{scanning}} {{and optical}} character recognition techniques—had improved enough to allow full digitization and <b>text</b> <b>extraction</b> of important target texts, e.g., of both print editions of the Nordisk familjebok (45,000 pages). Project Runeberg is hosted by an academic computer group, Lysator, at Linköping University, in Linköping in southern Sweden.|$|E
30|$|At present, the {{mainstream}} research of <b>text</b> feature <b>extraction</b> aims in the feature selection algorithm and text representation model.|$|R
3000|$|By {{reading a}} large amount of literature, the <b>text</b> feature <b>extraction</b> method and deep {{learning}} method is summarized [...]...|$|R
40|$|Generic layout {{analysis}} [...] process of decomposing document image into homogeneous regions for {{a collection of}} diverse document images [...] has many important applications in document image analysis and understanding such as preprocessing of degraded warped, camera-captured document images, high performance {{layout analysis}} of document images containing complex cursive scripts, and word spotting in historical document images at page level. Many areas in this field like generic <b>text</b> line <b>extraction</b> method are considered as elusive goals so far, still {{beyond the reach of}} the state-of-the-art methods [NJ 07, LSZT 07, KB 06]. This thesis addresses this problem in such a way that it presents generic, domain-independent, <b>text</b> line <b>extraction</b> and <b>text</b> and non-text segmentation methods, and then describes some important applications, that were developed based on these methods. An overview of the key contributions of this thesis is as follows. The first part of this thesis presents a generic <b>text</b> line <b>extraction</b> method using a combination of matched filtering and ridge detection techniques, which are commonly used in computer vision. Unlike the state-of-the-art <b>text</b> line <b>extraction</b> methods in the literature, the generic <b>text</b> line <b>extraction</b> method can be equally and robustly applied to a large variety of document image classes including scanned and camera-captured documents, binary and grayscale documents, typed-text and handwritten documents, historical and contemporary documents, and documents containing different scripts. Different standard datasets are selected for performance evaluation that belong to different categories of document images such as the UW-III [GHHP 97] dataset of scanned documents, the ICDAR 2007 [GAS 07] and the UMD [LZDJ 08] datasets of handwritten documents, the DFKI-I [SB 07] dataset of camera-captured documents, Arabic/Urdu script documents dataset, and German calligraphic (Fraktur) script historical documents dataset. The generic <b>text</b> line <b>extraction</b> method achieves 86...|$|R
50|$|Name/entity {{resolution}} in <b>text</b> <b>extraction</b> and semantics is a notoriously difficult problem, {{in part because}} in many cases there is not sufficient information to make an accurate determination. Numerous partial solutions exist that rely on specific contextual clues found in the data, {{but there is no}} currently known general solution.|$|E
50|$|Most ripping {{programs}} will assist in tagging the encoded files with metadata. The MP3 file format, for example, allows tags with title, artist, album and track number information. Some {{will try to}} identify the disc being ripped by looking up network services like AMG's LASSO, FreeDB, Gracenote's CDDB, GD3 http://www.getdigitaldata.com or MusicBrainz, or attempt <b>text</b> <b>extraction</b> if CD-Text has been stored.|$|E
5000|$|As of October 2010, MyDLP {{includes}} widespread {{data loss}} prevention features [...] such as <b>text</b> <b>extraction</b> from binary formats, incident management queue, source code detection and data identification methods for bank account, {{credit card and}} several national identification numbers. Besided, features like data classification through statistical analysis of trained sentences and native language processor integrated Naive Bayes classifier [...] are claimed to be inspiring.|$|E
3000|$|A {{large amount}} of {{literature}} has been collected to summarize most of {{the application of the}} present <b>text</b> feature <b>extraction</b> method [...]...|$|R
5000|$|... bonzaiboost, a fast (and multi-threaded) C++ {{implementation}} of multi-class/multi-label Adaboost.MH algorithm over small decision tree (bonsai). It offers several <b>text</b> features <b>extraction</b> facilities.|$|R
3000|$|There {{are several}} {{factors that could}} be used to {{implement}} <b>text</b> feature <b>extraction,</b> such as word frequency method, principal component analysis method, TF-IDF method, N-gram method, and emoji method [...]...|$|R
5000|$|Researchers {{have used}} Voyant Tools to analyze texts {{in a wide}} range of {{contexts}} including literature, language teaching, healthcare, and system architecture. Describing approaches to studying the internet using web scraping, Black has noted that [...] "the Voyant Tools project is an excellent source to learn about the kinds of data that humanists can extract from Internet sources because it already supports <b>text</b> <b>extraction</b> from webpages." ...|$|E
5000|$|A [...] "tagged" [...] PDF (ISO 32000-1:2008 14.8) {{includes}} document {{structure and}} semantics information to enable reliable <b>text</b> <b>extraction</b> and accessibility. Technically speaking, tagged PDF is a stylized {{use of the}} format that builds on the logical structure framework introduced in PDF 1.3. Tagged PDF defines a set of standard structure types and attributes that allow page content (text, graphics, and images) to be extracted and reused for other purposes.|$|E
5000|$|... pdftotext is an {{open source}} {{command-line}} utility for converting PDF files to plain text files - i.e. extracting text data from PDF-encapsulated files. It is freely available and included by default with many Linux distributions, and is also available for Windows {{as part of the}} Xpdf Windows port. Such <b>text</b> <b>extraction</b> is complicated as PDF files are internally built on page drawing primitives, meaning the boundaries between words and paragraphs often must be inferred based on their position on the page.|$|E
40|$|In <b>Text</b> Information <b>Extraction</b> (TIE) process, {{the text}} regions are {{localized}} and {{extracted from the}} images. It is an active research problem in computer vision applications. Diversity in text {{is due to the}} differences in size, style, orientation, alignment of text, low image contrast and complex backgrounds. The semantic information provided by an image can be used in different applications such as content based image retrieval, sign board identification etc. <b>Text</b> information <b>extraction</b> comprises of <b>text</b> image classification, text detection, localization, segmentation, enhancement and recognition. This paper contains a quick review on various text localization methods for localizing texts from natural scene images...|$|R
30|$|<b>Text</b> feature <b>extraction</b> that {{extracts}} <b>text</b> {{information is}} an extraction {{to represent a}} text message, it {{is the basis of}} a large number of text processing [3]. The basic unit of the feature is called text features [4]. Selecting a set of features from some effective ways to reduce the dimension of feature space, the purpose of this process is called feature extraction [5]. During feature extraction, uncorrelated or superfluous features will be deleted. As a method of data preprocessing of learning algorithm, feature extraction can better improve the accuracy of learning algorithm and shorten the time. Selection from the document part can reflect the information on the content words, and the calculation of weight is called the <b>text</b> feature <b>extraction</b> [5]. Common methods of <b>text</b> feature <b>extraction</b> include filtration, fusion, mapping, and clustering method. Traditional methods of feature extraction require handcrafted features. To hand-design an effective feature is a lengthy process, and deep learning can be aimed at new applications and quickly acquire new effective characteristic representation from training data.|$|R
40|$|<b>Text</b> line <b>extraction</b> {{is one of}} the {{critical}} steps in document analysis and optical character recognition (OCR) systems. The {{purpose of this study is}} to address the problem of <b>text</b> line <b>extraction</b> of ancient Thai manuscripts written on palm leaves, using an Adaptive Partial Projection (APP) technique by integrating a modified partial projection and smooth histogram with recursion. The proposed approach was compared with a Modified Partial Projection (MPP) looking at vowel analysis and touching components of two consecutive lines. The results from this research suggested that the proposed approach for practical data on palm leaf manuscripts has better performance in solving the line segmentation problem...|$|R
50|$|ABBYY FineReader Engine (also {{known as}} FineReader OCR SDK) is a {{commercial}} software toolkit for developers {{to create new}} applications or enhance the functionality of existing ones with additional capabilities. They include scanning, image preprocessing, field and barcode recognition, <b>text</b> <b>extraction,</b> document conversion and document archiving. Engine-based applications can open, analyze and parse the contents of images, PDF and scanned documents. The results can be exported in various formats, including text-based PDF, Microsoft Office formats, and XML (used for integrating OCR results with other systems).|$|E
5000|$|One of the {{commitments}} of MILE lab {{is the development}} of technology for people with visual impairment to harness knowledge from any available printed material in Indian languages. The lab is working towards reaching this goal. Its work till now included: document mosaicing of coloured, camera captured images <b>text</b> <b>extraction</b> from complex colour images, including camera captured images; document layout analysis; detection of broken and merged characters; OCR technology for Tamil and Kannada; text to speech conversion in Tamil and Kannada; pitch modification using discrete cosine transform in the source domain; automated part of speech tagging; phrase prediction and prosody modeling.|$|E
5000|$|Media Cloud is an {{open-source}} {{content analysis}} tool {{that aims to}} map news media coverage of current events. It [...] "performs five basic functions -- media definition, crawling, <b>text</b> <b>extraction,</b> word vectoring, and analysis." [...] Media cloud [...] "tracks hundreds of newspapers and thousands of Web sites and blogs, and archives the information in a searchable form. The database ... enables researchers to search for key people, places and events — from Michael Jackson to the Iranian elections — and find out precisely when, where and how frequently they are covered." [...] Media Cloud {{was developed by the}} Berkman Center for Internet & Society at Harvard University and launched in March 2009. It's distributed under the GNU GPL 3+.|$|E
50|$|Project Runeberg (Swedish, Projekt Runeberg) is {{a digital}} {{cultural}} archive initiative that publishes free electronic versions of books significant {{to the culture}} {{and history of the}} Nordic countries. Patterned after Project Gutenberg, it was founded by Lars Aronsson and colleagues at Linköping University and began archiving Nordic-language literature in December 1992. As of 2015 it had accomplished digitization to provide graphical facsimiles of old works such as the Nordisk familjebok, and had accomplished, in whole or in part, the <b>text</b> <b>extractions</b> and copyediting of these as well as esteemed Latin works and English translations from Nordic authors, and sheet music and other texts of cultural interest.|$|R
30|$|<b>Text</b> feature <b>extraction</b> plays {{a crucial}} role in text classification, {{directly}} influencing the accuracy of text classification [3, 10]. It is based on VSM (vector space model, VSM), in which a text is viewed as a dot in N-dimensional space. Datum of each dimension of the dot represents one (digitized) feature of the text. And the text features usually use a keyword set. It means that {{on the basis of a}} group of predefined keywords, we compute weights of the words in the text by certain methods and then form a digital vector, which is the feature vector of the text [10]. Existing <b>text</b> feature <b>extraction</b> methods include filtration, fusion, mapping, and clustering method, which are briefly outlined below.|$|R
40|$|There are {{numerous}} stylish documents {{which do not}} have the traditional text layouts where printed text regions are not parallel to each other. Such complex layouts make <b>text</b> line <b>extraction</b> challenging due to multi-orientation of paragraphs. This paper introduces a system for the <b>text</b> line <b>extraction</b> from the complex layout documents. Proposed method is based on the concept of dilation and histogram profiling. The text regions are extracted using dilation and food fill based approach, then paragraph orientation is determined and individual text lines are extracted. The accuracy of extracted text lines are evaluated using the new proposed concept that is also based on the histogram profiling. The results of proposed approach on the complex layouts are promising...|$|R
