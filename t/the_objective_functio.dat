12|10000|Public
40|$|The paper {{presents}} {{a comparison of}} performance Static Var Compensator (SVC) and Thyristor Controlled Series Compensator (TCSC) with objective function to minimize the transmission loss, improve the voltage and monitoring the cost of installation. Simulation performed on standard IEEE 30 -Bus RTS and indicated that PSO a feasible to achieve <b>the</b> <b>objective</b> <b>functio...</b>|$|E
40|$|Abstract In {{this paper}} we propose an {{interior}} point method for solving the dual form of minmax type problems The dual variables are updated {{by means of a}} scaling supergradient method The boundary of the dual feasible region is avoided by the use of a logarithmic barrier function A major dierence with other interior point methods is the nonsmoothness of <b>the</b> <b>objective</b> <b>functio...</b>|$|E
40|$|The paper {{presents}} {{a comparison of}} Computational Intelligence techniques are Evolutionary Programming Swarm Optimization (EPSO), Particle Swarm Optimization (PSO), Evolutionary Programming (EP) to optimal placement and sizing of Static Var Compensator. The technique has been implemented to minimize the transmission loss and improve the voltage profile of the system. Simulation performed on standard IEEE 118 -Bus RTS and indicated that EPSO a feasible to achieve <b>the</b> <b>objective</b> <b>functio...</b>|$|E
40|$|This paper {{deals with}} a new {{variable}} metric algorithm for stochastic optimization problems. The essence of this is as follows: there exist two stochastic quasigradient algorithms working simultaneously - {{the first in the}} main space, the second with respect to the matrices that modify the space variables. Almost sure convergence of the algorithm is proved for the case of the convex (possibly nonsmooth) <b>objective</b> <b>functio...</b>|$|R
40|$|We {{combine a}} refined version of {{two-point}} step-size adaptation with the covariance matrix adaptation evolution strategy (CMA-ES). Additionally, we suggest polished formulae for the learning {{rate of the}} covariance matrix and the recombination weights. In contrast to cumulative step-size adaptation or to the 1 / 5 -th success rule, the refined two-point adaptation (TPA) does not rely on any internal model of optimality. In contrast to conventional self-adaptation, the TPA will achieve a better target step-size in particular with large populations. The disadvantage of TPA is that it relies on two additional <b>objective</b> <b>functio...</b>|$|R
40|$|This paper {{presents}} an adaptive weighted sum method for multiobjective optimization problems. The authors developed the bi-objective adaptive weighted sum method, which determines uniformly-spaced Pareto optimal solutions, finds solutions on non-convex regions, and neglects non-Pareto optimal solutions. However, the method could solve only problems with two objective functions. In this work, the bi-objective adaptive weighted sum method is extended to problems {{with more than}} two <b>objective</b> functions. In <b>the</b> first phase, the usual weighted sum method is performed to approximate Pareto surfaces quickly, and a mesh of Pareto front patches is identified. Each Pareto front patch is then refined by imposing additional equality constraints that connect the pseudo nadir point and the expected Pareto optimal solutions on a piecewise planar surface in <b>the</b> <b>objective</b> space. It is demonstrated that the method produces a well-distributed Pareto front mesh for effective visualization and finds solutions in non-convex regions. Two numerical examples and a simple structural optimization problem are solved as case studies. J = objective function vector x = design vector p = vector of fixed parameters g = inequality constraint vector h = equality constraint vector m = number of objectives α = i th weighting factor i J i = normalized <b>objective</b> <b>functio...</b>|$|R
40|$|International audienceIn this study, a multi {{objective}} programming {{model for}} nurse scheduling is developed. In this paper, {{we present a}} goal programming (GP) model that accommodates both hard and soft constraints for a monthly planning horizon. The hard constraints should be adhered to strictly, whereas the soft constraints can be violated when necessary. The relative importance values of the soft constraints have been computed by the analytical hierarchy process (AHP), which are used as coefficients of the deviations from the soft constraints in <b>the</b> <b>objective</b> <b>functio...</b>|$|E
40|$|We {{consider}} the objective {{function of a}} simple integer recourse problem with fixed technology matrix. Using properties of the expected value function, we prove a relation between the convex hull of this function and the expected value function of a continuous simple recourse program. We present an algorithm to compute the convex hull of the expected value function in case of discrete right-hand side random variables. Allowing for restrictions on the first stage decision variables, this result is then extended to the convex hull of <b>the</b> <b>objective</b> <b>functio...</b>|$|E
40|$|We {{give new}} {{convergence}} {{results for the}} block Gauss–Seidel method for problems where the feasible set is the Cartesian product of m closed convex sets, {{under the assumption that}} the sequence generated by the method has limit points. We show that the method is globally convergent for m= 2 and that for m> 2 convergence can be established both when the objective function f is componentwise strictly quasiconvex with respect to m− 2 components and when f is pseudoconvex. Finally, we consider a proximal point modification of the method and we state convergence results without any convexity assumption on <b>the</b> <b>objective</b> <b>functio...</b>|$|E
40|$|After {{decades of}} success, {{research}} on evolutionary algorithms aims at developing a sound {{theory that describes}} and predict the behavior of these algorithms. One research topic of interest is {{the analysis of the}} role of crossover and recombination in genetic algorithms, especially since various papers come to different conclusions. The goals of this paper are to revisit some well-known concepts and to discuss some new aspects that might be helpful for further clarification. 1 Introduction Genetic algorithms [6, 10, 12], evolution strategies [14, 17], and evolutionary programming [9, 8] are the three main streams of a class of stochastic optimization procedures that are often referred to as evolutionary algorithms. All evolutionary algorithms are heuristic population-based search procedures that incorporate random variation and selection. Typically, such population-based search procedures generate offspring in each generation. A fitness value (defined by a fitness or <b>objective</b> <b>functio</b> [...] ...|$|R
40|$|Congestion {{control in}} the current Internet is {{accomplished}} mainly by TCP/IP. To understand the macroscopic network behavior that results from TCP/IP and similar endto-end protocols, one main analytic technique {{is to show that}} the the protocol maximizes some global <b>objective</b> function of <b>the</b> network traffic. Here we analyze a particular end-to-end, MIMD (multiplicative-increase, multiplicative-decrease) protocol. We show that if all users of the network use the protocol, and all connections last for at least logarithmically many rounds, then the total weighted throughput (value of all packets received) is near the maximum possible. Our analysis includes round-trip-times, and (in contrast to most previous analyses) gives explicit convergence rates, allows connections to start and stop, and allows capacities to change. 1. Congestion control and optimization Congestion {{control in the}} current Internet is accomplished mainly by TCP/IP — 90 % of Internet traffic is TCP-based [41]. Meanwhile the design and analysis of TCP and other end-to-end congestion-control protocols are only partially understood and are becoming the subject of increasing attention [25, 28]. One main analytic technique is to interpret the protocol as solving some underlying combinatorial optimization problem on the network — to show that the protocol causes the traffic distribution, over time, to optimize some global <b>objective</b> <b>functio...</b>|$|R
40|$|We {{consider}} {{parallel computing}} of the Householder QR decomposition on SMP machines. This decomposition {{is one of}} the basic tools in matrix computations and is used in various problems such as the least square problem and the singular value decomposition of a rectangular matrix. Since this algorithm consists almost entirely of BLAS routines such as matrix-vector multiplications, the simplest way of parallelization is to parallelize each BLAS routine. Moreover, using the blocking technique [1], we can use matrix-matrix multiplications, which can be efficiently parallelized. On the other hand, the TSQR algorithm has been proposed in 2007 [2]. Thanks to this algorithm, coarse-grain parallelization of the QR decomposition for a Tall Skinny matrix has become possible. Additionally, using the blocking technique, we can apply this algorithm to the QR decomposition of not necessarily tall skinny matrices [3]. For efficient parallel computing, we need to consider two points at the same time; how to combine the BLAS-level parallelism and the TSQR algorithm, and how to partition a matrix into blocks. In our poster, we aim for automatic determination of these two things depending on the target machine {{and the size of the}} target matrix. In our approach, we first identify parameters to optimize. Next we define an <b>objective</b> <b>functio...</b>|$|R
40|$|The paper {{proposes a}} method for {{constructing}} a sparse estimator for the inverse covariance (concentration) matrix in high-dimensional settings. The estimator uses a penalized normal likelihood approach and forces sparsity by using a lasso-type penalty. We establish a rate of con-vergence in the Frobenius norm as both data dimension p and sample size n are allowed to grow, and show that the rate depends explicitly on how sparse the true concentration matrix is. We also show that a correlation-based version of the method exhibits better rates in the operator norm. The estimator is required to be positive definite, but we avoid having to use semi-definite programming by re-parameterizing <b>the</b> <b>objective</b> <b>functio...</b>|$|E
40|$|The problem o f {{estimating}} seabed geoacoustic parameters by inverting measured ocean acoustic fields {{has received}} considerable attention in recent years. Matched-field inversion (MFI) {{is based on}} searching for the set of geoacoustic model parameters m that minimizes an objective function quantifying the misfit between measured and modelled acoustic fields. A number of approaches have been applied to this challenging nonlinear optimization problem. In particular, adaptive simplex simulated annealing (ASSA) [1], a hybrid optimization algorithm that combines local (gradient-based) downhill simplex moves within a fast simulated annealing global search, has proved highly effective for MFI. In a Bayesian formulation o f MFI, <b>the</b> <b>objective</b> <b>functio...</b>|$|E
40|$|Abstract: The paper {{proposes a}} method for {{constructing}} a sparse estimator for the inverse covariance (concentration) matrix in high-dimensional settings. The estimator uses a penalized normal likelihood approach and forces sparsity by using a lasso-type penalty. We establish a rate of convergence in the Frobenius norm as both data dimension p and sample size n are allowed to grow, and show that the rate depends explicitly on how sparse the true concentration matrix is. We also show that a correlationbased version of the method exhibits better rates in the operator norm. The estimator is required to be positive definite, but we avoid having to use semi-definite programming by re-parameterizing <b>the</b> <b>objective</b> <b>functio...</b>|$|E
40|$|In {{the past}} few years co-clustering has emerged as an {{important}} data mining tool for two way data analysis. Co-clustering is more advantageous over traditional one dimensional clustering in many ways such as, ability to find highly correlated sub-groups of rows and columns. However, one of the overlooked benefits of co-clustering is that, {{it can be used to}} extract meaningful knowledge for various other knowledge extraction purposes. For example, building predictive models with high dimensional data and heterogeneous population is a non-trivial task. Co-clusters extracted from such data, which shows similar pattern in both the dimension, can be used for a more accurate predictive model building. Several applications such as finding patient-disease cohorts in health care analysis, finding user-genre groups in recommendation systems and community detection problems can benefit from co-clustering technique that utilizes the predictive power of the data to generate co-clusters for improved data analysis. In this paper, we present the novel idea of Predictive Overlapping Co-Clustering (POCC) as an optimization problem for a more effective and improved predictive analysis. Our algorithm generates optimal co-clusters by maximizing predictive power of the co-clusters subject to the constraints on the number of row and column clusters. In this paper precision, recall and f-measure have been used as evaluation measures of the resulting co-clusters. Results of our algorithm has been compared with two other well-known techniques - K-means and Spectral co-clustering, over four real data set namely, Leukemia, Internet-Ads, Ovarian cancer and MovieLens data set. The results demonstrate the effectiveness and utility of our algorithm POCC in practice. Comment: This paper has been withdrawn by the authors due to a crucial sign error in <b>objective</b> <b>functio...</b>|$|R
50|$|<b>The</b> <b>objective</b> of {{economic}} policies {{is to achieve}} efficiency. <b>The</b> <b>objective</b> of social policies is to achieve equity. <b>The</b> <b>objective</b> of environmental policies is to achieve sustainability (often involving implementing Green infrastructure. <b>The</b> <b>objective</b> of institutional governance is to achieve equilibrium (‘Ethical Equilibrium’) among the three areas.|$|R
30|$|Stage VI(a): {{validate}} <b>objective</b> <b>The</b> model’s <b>objective</b> {{is often}} based on business objectives defined by domain experts. In some cases <b>the</b> <b>objective</b> may be identified from empirical data but this still requires validation by experts. Importantly, the BN’s outcomes should be reviewed in light of <b>the</b> <b>objective,</b> {{to ensure that the}} model is representing <b>the</b> <b>objective</b> adequately and providing outputs that are informative about <b>the</b> <b>objective.</b>|$|R
40|$|This paper mainly {{deals with}} the {{analysis}} of IPP in clustering. Clustering is exemplified by the unsupervised learning of patterns and clusters that may exist in a given database and is {{a useful tool for}} Knowledge Discovery in Database (KDD). A mathematical programming formulation of this problem is proposed that is theoretically justifiable and computationally implementable in a finite number of steps. The clustering algorithm applies the hierarchical clustering methodology [1] where points or clusters with the shortest distance are merged into a cluster until the desired number of clusters is achieved. Numerical examples are given for the above algorithmic approach. The Mathematical programming model subject to some constraints is a broad discipline that has been applied for various theoretical and applied problems. In this paper the described various integer programming model for clustering is implemented. The fundamental Non Linear programming problem, consists of minimizing <b>the</b> <b>objective</b> <b>functio...</b>|$|E
40|$|The {{interaction}} between {{private and public}} objectives may {{be a source of}} conflict in case the farmers decide to produce energy, many factors must be taken into account: i) induced instability of the agricultural commodity market and consequent risk due to wider price fluctuations; ii) domestic political decisions concerning the prices of crops and incentives; iii) environmental conservation and natural resources policy; iv) efficiency of agrifood and agro-energy chain and management of information; v) other factors related to the social condition or status of farmers as security, quality of life. (Lansink O., 2006). These considerations suggest the policy makers {{to take into account the}} opportunity to formulate strategies that best conciliate these conflicting interests by searching a compromise. (Dodgson et al. 2000; Voogd 1982), Weeler and Russel (1977) Different solutions have been tempted by introducing several goals in the farm decision making: gross margin, minimum seasonal cash exposure, stable employment, ecological impact, quality of natural resource conservation. For the farm planning problem, the decision maker, according with his own preferences, will set up a ranking of some preferred attributes to be included in <b>the</b> <b>objective</b> <b>functio...</b>|$|E
40|$|Due to {{uncertainties}} in {{data and}} in forward modelling, the inherent limitations in data coverage and the non-linearity {{of the governing}} equation, earthquake source imaging {{is a problem with}} multiple solutions. The multiplicity of solutions can be conveniently expressed using a Bayesian approach, which allow to state inferences on model parameters in terms of probability density functions. The estimation of the posterior state of information, expressing the combination of the a priori knowledge on model parameters with the information contained in the data, is achieved in two steps. First, we explore the model space using an evolutionary algorithm to identify good data fitting regions. Secondly, using a neighbourhood algorithm and considering the entire ensemble of models found during the search stage, we compute a geometric approximation of the true posterior that is used to generate a second ensemble of models from which Bayesian inference can be performed. We apply this methodology to infer kinematic parameters of a synthetic fault rupture through fitting of strong motion data. We show how multiple rupture models are able to reproduce the observed waveforms within the same level of fit, suggesting therefore that the solution of the inversion cannot be expressed in terms of a single model but rather as a set of models which show certain statistical properties. For all model parameters we compute the posterior marginal distribution. We show how for some parameters the posterior do not follow a Gaussian distribution rendering the usual characterization in terms of mean value and standard deviation not correct. We compare the posterior marginal distributions with the ‘raw' marginal distributions computed from the ensemble of models generated by the evolutionary algorithm. We show how they are systematically different proving therefore that the search algorithm we adopt cannot be directly used to estimate uncertainties. We also analyse the stability of our inferences comparing the posterior marginals computed by different independent ensembles. The solutions provided by independent explorations are similar but not identical because each ensemble searches the model space differently resulting in different reconstructed posteriors. Our study illustrates how uncertainty estimates derive from the topology of the objective function, and how accurate and reliable resolution analysis is limited by the intrinsic difficulty of mapping the ‘true' structure of <b>the</b> <b>objective</b> <b>functio...</b>|$|E
30|$|Term 6 {{represents}} <b>the</b> <b>objective</b> function {{which includes}} <b>the</b> <b>objective</b> functions of DCs of the chains. Constraints 61 and 62 are calculating <b>the</b> <b>objective</b> function of <b>the</b> plants of the chains.|$|R
40|$|This paper scrutinises <b>the</b> <b>objectives</b> {{emphasized}} by <b>the</b> syllabus and assessment {{on one hand}} and compares these to <b>the</b> <b>objectives</b> tested in <b>the</b> public examination conducted at the end of four year secondary education. As certification is decided by the performance in the public examination, it is assumed that <b>the</b> <b>objectives</b> tested in <b>the</b> examination match <b>the</b> <b>objectives</b> of <b>the</b> syllabus an...|$|R
5000|$|To {{minimize}} <b>the</b> <b>objective</b> function (i.e. {{solve the}} inverse problem) we compute the gradient of <b>the</b> <b>objective</b> function using <b>the</b> same rationale {{as we would}} to minimize a function of only one variable. The gradient of <b>the</b> <b>objective</b> function is: ...|$|R
25|$|Since the {{entering}} variable will, in general, increase from 0 {{to a positive}} number, the value of <b>the</b> <b>objective</b> function will decrease if the derivative of <b>the</b> <b>objective</b> function {{with respect to this}} variable is negative. Equivalently, the value of <b>the</b> <b>objective</b> function is decreased if the pivot column is selected so that the corresponding entry in <b>the</b> <b>objective</b> row of <b>the</b> tableau is positive.|$|R
50|$|The {{envelope}} theorem is {{a result}} about the differentiability properties of <b>the</b> <b>objective</b> function of a parameterized optimization problem. As we change parameters of <b>the</b> <b>objective,</b> <b>the</b> envelope theorem shows that, in a certain sense, changes in the optimizer of <b>the</b> <b>objective</b> do {{not contribute to the}} change in <b>the</b> <b>objective</b> function. <b>The</b> envelope theorem is an important tool for comparative statics of optimization models.|$|R
50|$|Since the {{entering}} variable will, in general, increase from 0 {{to a positive}} number, the value of <b>the</b> <b>objective</b> function will decrease if the derivative of <b>the</b> <b>objective</b> function {{with respect to this}} variable is negative. Equivalently, the value of <b>the</b> <b>objective</b> function is decreased if the pivot column is selected so that the corresponding entry in <b>the</b> <b>objective</b> row of <b>the</b> tableau is positive.|$|R
40|$|Abstract: In the {{optimization}} problem which only measurements of <b>the</b> <b>objective</b> function are available, {{it is difficult}} or impossible to directly obtain the gradient of <b>the</b> <b>objective</b> function. Although <b>the</b> second order simultaneous perturbation stochastic approximation (2 SPSA) algorithm solves this problem successfully by efficient gradient approximation that relies on measurements of <b>the</b> <b>objective</b> function, <b>the</b> accuracy of the algorithm depends on the matrix conditioning of <b>the</b> <b>objective</b> function Hessian. In order to eliminate the influence caused by <b>the</b> <b>objective</b> function Hessian, this paper uses nonlinear conjugate gradient method to decide the search direction of <b>the</b> <b>objective</b> function. By synthesizing different nonlinear conjugate gradient methods, it ensures each search direction to b...|$|R
30|$|Term 113 {{represents}} <b>the</b> <b>objective</b> function {{which includes}} <b>the</b> <b>objective</b> functions of <b>the</b> DCs for SC 1, SC 2 and SC 3. Constraints 114 – 115 are calculating <b>the</b> <b>objective</b> function of <b>the</b> plants for SC 1, SC 2 and SC 3 correspondingly.|$|R
30|$|Term 54 {{represents}} <b>the</b> <b>objective</b> function {{which includes}} <b>the</b> <b>objective</b> functions of DC of SC 1 and plant of SC 2. Constraints 55 and 56 are calculating <b>the</b> <b>objective</b> function of <b>the</b> plant of SC 1 and DC of SC 2 correspondingly.|$|R
50|$|<b>The</b> <b>objective</b> of <b>the</b> {{system is}} to perform some {{specified}} function. <b>The</b> <b>objective</b> of organizational control is {{to see that the}} specified function is achieved. <b>The</b> <b>objective</b> of operational control is to ensure that variations in daily output are maintained within prescribed limits.|$|R
30|$|Term 125 {{represents}} <b>the</b> <b>objective</b> function {{which includes}} <b>the</b> <b>objective</b> functions of <b>the</b> DC, plant for SC 1, SC 2 and SC 3. Constraints 126 – 128 are calculating <b>the</b> <b>objective</b> function of <b>the</b> plant, DC of SC 1, SC 2 and SC 3 correspondingly.|$|R
50|$|Identification of SWOTs is {{important}} because they can inform later steps in planning to achieve <b>the</b> <b>objective.</b> First, decision-makers should consider whether <b>the</b> <b>objective</b> is attainable, given the SWOTs. If <b>the</b> <b>objective</b> is not attainable, they must select a different <b>objective</b> and repeat <b>the</b> process.|$|R
30|$|Step 4 : Estimating <b>the</b> <b>objective</b> {{function}} value. Based on {{the experimental}} data, <b>the</b> <b>objective</b> function value was calculated.|$|R
3000|$|In FTTC-TMBF, <b>the</b> <b>objectives</b> are {{throughput}} maximization, fairness, balancing and robustness against failures; therefore, <b>the</b> <b>objective</b> function is: [...]...|$|R
3000|$|<b>The</b> <b>objective</b> function, f (...), is a {{feedback}} {{control of the}} [...] "Quality metric" [...] component. <b>The</b> <b>objective</b> function is used both for candidate block searching and the quality metric. In this study, <b>the</b> <b>objective</b> function was <b>the</b> standard deviation of the wavelet coefficients. The dual task of <b>the</b> <b>objective</b> function is to locate the high-energy areas from the reference image and measure the corresponding areas' energy from the test images. The proposed reference-image framework is modular, and other quality attributes can be calculated by changing <b>the</b> <b>objective</b> function. <b>The</b> next subsections describe the components' functions in more detail.|$|R
