1253|4733|Public
2500|$|... where [...] is a <b>training</b> <b>sample</b> with {{target value}} [...] The inner product plus {{intercept}} [...] is the prediction for that sample, and [...] {{is a free}} parameter {{that serves as a}} threshold: all predictions have to be within an [...] range of the true predictions. Slack variables are usually added into the above to allow for errors and to allow approximation in the case the above problem is infeasible.|$|E
5000|$|... or equivalently, to {{maximize}} the expected log probability of a <b>training</b> <b>sample</b> [...] selected randomly from : ...|$|E
5000|$|In machine learning, it {{is desired}} {{to have a}} {{training}} set that represents the true distribution of samples. This can be quantified using the notion of representativeness. Denote by P the probability distribution from which the samples are drawn. Denote by [...] the set of hypotheses (potential classifiers) and denote by [...] the corresponding set of error functions, i.e, for every , there is a function , that maps each <b>training</b> <b>sample</b> (features,label) to the error of the classifier [...] on that sample (for example, if we do binary classification and the error function is the simple 0-1 loss, then [...] is a function that returns 0 for each <b>training</b> <b>sample</b> on which [...] is correct and 1 for each <b>training</b> <b>sample</b> on which [...] is wrong). Define: ...|$|E
40|$|Abstract—Theoretical {{analyses}} on generalization {{error of}} a model space for kernel regressors with respect to <b>training</b> <b>samples</b> are given in this paper. In general, the distance between an unknown true function and a model space tends to be small with a larger set of <b>training</b> <b>samples.</b> However, it is not clarified that a larger set of <b>training</b> <b>samples</b> achieves a smaller difference at each point of the unknown true function and the orthogonal projection of it onto the model space, compared with a smaller set of <b>training</b> <b>samples.</b> In this paper, we show that the upper bound of the squared difference at each point of these two functions with a larger set of <b>training</b> <b>samples</b> is not larger than that with a smaller set of <b>training</b> <b>samples.</b> We als...|$|R
40|$|Recognition is a {{fundamental}} computer vision problem, in which <b>training</b> <b>samples</b> are used to learn models, that then assign labels to test samples. The utilization of <b>training</b> <b>samples</b> is of vital importance to visual recognition, which can be addressed by increasing the capability of the description methods and the model learning methods. Two visual recognition tasks namely object detection and action recognition and are considered in this thesis. Active learning utilizes selected subsets of the training dataset as <b>training</b> <b>samples.</b> Active learning methods select the most informative <b>training</b> <b>samples</b> in each iteration, and therefore require fewer <b>training</b> <b>samples</b> to attain comparable performance to passive learning methods. In this thesis, an active learning method for object detection that exploits the distribution of <b>training</b> <b>samples</b> is presented. Experiments show that the proposed method outperforms a passive learning method and a simple margin active learning method. Weakly supervised learning facilitates learning on <b>training</b> <b>samples</b> with weak labels. In this thesis, a weakly supervised object detection method is proposed to utilize <b>training</b> <b>samples</b> with probabilistic labels. Base detectors are used to create object proposals from <b>training</b> <b>samples</b> with weak labels. Then the object proposals are assigned estimated probabilistic labels. A Generalized Hough Transform based object detector is extended to utilize the object proposals with probabilistic labels as <b>training</b> <b>samples.</b> The proposed method is shown to outperform both a comparison method that assigns strong labels to object proposals, and a weakly supervised deformable part-based models method. The proposed method also attains comparable performance to supervised learning methods. Increasing the capability of the description method can improve the utilization of <b>training</b> <b>samples.</b> In this thesis, temporal pyramid histograms are proposed {{to address the problem}} of missing temporal information in the classical bag of features description method used in action recognition. Experiments show that the proposed description method outperforms the classical bag of features method in action recognition...|$|R
3000|$|... {{denotes the}} number of <b>training</b> <b>samples</b> for SVM in the {{advanced}} recognition (After the primary recognition, the <b>training</b> <b>samples</b> for SVM is reduced). In general, [...]...|$|R
5000|$|Take a <b>training</b> <b>sample</b> , {{compute the}} probabilities {{of the hidden}} units and sample a hidden {{activation}} vector [...] from this probability distribution.|$|E
50|$|Instantaneously trained neural {{networks}} are feedforward artificial {{neural networks}} {{that create a}} new hidden neuron node for each novel <b>training</b> <b>sample.</b> The weights to this hidden neuron separate out not only this <b>training</b> <b>sample</b> but others that are near it, thus providing generalization. This training {{can be done in}} a variety of ways and the most popular network in this family is called the CC4 network where the separation is done using the nearest hyperplane that can be written down instantaneously. These networks use unary coding for an effective representation of the data sets.|$|E
5000|$|The {{conformal}} approach [...] {{that uses}} conformal mapping to interpolate each <b>training</b> <b>sample</b> between grid nodes {{in a continuous}} surface. A one-to-one smooth mapping is possible in this approach.|$|E
40|$|Central {{to several}} {{objective}} approaches to Bayesian model selection {{is the use}} of <b>training</b> <b>samples</b> (subsets of the data), so as to allow utilization of improper objective priors. The most common prescription for choosing <b>training</b> <b>samples</b> is to choose them to be as small as possible, subject to yielding proper posteriors; these are called minimal <b>training</b> <b>samples...</b>|$|R
3000|$|... are {{the number}} of <b>training</b> <b>samples</b> for the SVM and {{the number of}} <b>training</b> <b>samples</b> for the SVM in the {{advanced}} recognition of the hybrid recognition, respectively.|$|R
40|$|Abstract—Statistical {{learning}} {{methods for}} human detection require {{large quantities of}} <b>training</b> <b>samples</b> and thus suffer from high sample collection costs. Their detection performance is also liable to be lower when the <b>training</b> <b>samples</b> are collected in a different environment {{than the one in}} which the detection system must operate. In this paper we propose a generative learning method that uses the automatic generation of <b>training</b> <b>samples</b> from 3 D models together with an advanced MILBoost learning algorithm. In this study, we use a three-dimensional human model to automatically generate positive samples for learning specialized to specific scenes. Negative <b>training</b> <b>samples</b> are collected by random automatic extraction from video stream, but some of these samples may be collected with incorrect labeling. When a classifier is trained by statistical learning using incorrectly labeled <b>training</b> <b>samples,</b> detection performance is impaired. Therefore, in this study an improved version of MILBoost is used to perform generative learning which is immune to the adverse effects of incorrectly labeled <b>samples</b> among the <b>training</b> <b>samples.</b> In evaluation, we found that a classifier <b>trained</b> using <b>training</b> <b>samples</b> generated from a 3 D human model was capable of better detection performance than a classifier <b>trained</b> using <b>training</b> <b>samples</b> extracted by hand. The proposed method can also mitigate the degradation of detection performance when there are image of people mixed in with the negative samples used for learning. I...|$|R
5000|$|GRNN {{represents}} an improved technique in the neural networks {{based on the}} non-parametric regression. The idea is that every <b>training</b> <b>sample</b> will represents a mean to a radial basis neuron.|$|E
50|$|Learning for Life (LFL) {{coordinates}} the Law Enforcement Explorer {{program at}} the national level. LFL provides resources such as advisor <b>training,</b> <b>sample</b> policies, and insurance. LFL also hosts a biannual conference and competition, the National Law Enforcement Explorer Conference.|$|E
50|$|The {{other way}} is to think of {{neuronal}} weights as pointers to the input space. They form a discrete approximation {{of the distribution of}} training samples. More neurons point to regions with high <b>training</b> <b>sample</b> concentration and fewer where the samples are scarce.|$|E
30|$|For {{the author}} {{identification}} problem, we have split the available posts for a user into five separate <b>training</b> <b>samples.</b> This {{has proved to}} work quite well, {{but the number of}} <b>training</b> <b>samples</b> per user has been quite arbitrarily selected. The optimal value of <b>training</b> <b>samples</b> is probably dependent upon the number of potential authors as well as how much posts we have available for each user, but finding such an optimal value has been outside the scope of this article. However, as a rule of thumb, the more posts we have for a certain user, the more high-quality <b>training</b> <b>samples</b> we can create.|$|R
40|$|Central {{to several}} {{objective}} approaches to Bayesian model selection {{is the use}} of <b>training</b> <b>samples</b> (subsets of the data), so as to allow utilization of improper objective priors. The most common prescription for choosing <b>training</b> <b>samples</b> is to choose them to be as small as possible, subject to yielding proper posteriors; these are called minimal <b>training</b> <b>samples.</b> When data can vary widely in terms of either information content or impact on the improper priors, use of minimal <b>training</b> <b>samples</b> can be inadequate. Important examples include certain cases of discrete data, the presence of censored observations, and certain situations involving linear models and explanatory variables. Such situations require more sophisticated methods of choosing <b>training</b> <b>samples.</b> A variety of such methods are developed in this paper, and successfully applied in challenging situations. 1. Introduction. Trainin...|$|R
40|$|In {{real-world}} applications, {{the image}} of faces varies with illumination, facial expression, and poses. It seems that more <b>training</b> <b>samples</b> are able to reveal possible images of the faces. Though minimum squared error classification (MSEC) is a widely used method, its applications on face recognition usually suffer from the problem of {{a limited number of}} <b>training</b> <b>samples.</b> In this paper, we improve MSEC by using the mirror faces as virtual <b>training</b> <b>samples.</b> We obtained the mirror faces generated from original <b>training</b> <b>samples</b> and put these two kinds of samples into a new set. The face recognition experiments show that our method does obtain high accuracy performance in classification...|$|R
5000|$|Theorem: Let [...] be a nonempty set and [...] a positive-definite real-valued kernel on [...] with {{corresponding}} reproducing kernel Hilbert space [...] Given a <b>training</b> <b>sample</b> , {{a strictly}} monotonically increasing real-valued function , and an arbitrary empirical risk function , then for any [...] satisfying ...|$|E
5000|$|Theorem: Let [...] be a nonempty set, [...] a positive-definite real-valued kernel on [...] with {{corresponding}} reproducing kernel Hilbert space , and let [...] be a differentiable regularization function. Then given a <b>training</b> <b>sample</b> [...] and {{an arbitrary}} empirical risk function , a minimizer ...|$|E
50|$|The Kak neural network, {{which was}} first {{proposed}} by Subhash Kak, is an instantaneously trained neural network {{that creates a}} new hidden neuron for each <b>training</b> <b>sample,</b> achieving instantaneous training for binary data and also for real data if some small additional processing is allowed. These networks, therefore, model short-term biological memory.|$|E
30|$|In Table 4, the VN digit has {{the highest}} {{difference}} correction rate in 20 % of <b>training</b> <b>samples</b> and 100 % of <b>training</b> <b>samples.</b> At the step when 20 % samples {{were added to the}} model, the VN digits reached 0.27 and at 100 % <b>training</b> <b>samples</b> step, 0.97. The second highest difference correction rate was on the ISOLET database and the lowest difference was on the JVPD database. Table 4 shows that, on most of the database, when <b>training</b> <b>samples</b> were added, the classification accuracy increased. However, in steps 2 and 3 on the JVPD database, the accuracy was not increased when training data were added, but it even decreased in step 4.|$|R
30|$|For {{the second}} experiment, {{the aim was}} to {{ascertain}} how efficient in terms of number of <b>training</b> <b>samples</b> the learning process was, with multi-modal representations, in comparison with each individual modality. The reasoning is that it may be considered “unfair” to compare a vision-only system which used 60 <b>training</b> <b>samples</b> against a visuo-tactile system that used 120 (60 visual and 60 touch). Instead, the total number of <b>training</b> <b>samples</b> was set to a fixed value and the accuracy for uni-modal and multi-modal was computed. For example, when the number of <b>training</b> <b>samples</b> was set to 40, tactile-only and visual-only recognition was performed using 40 <b>training</b> <b>samples,</b> but multi-modal recognition was performed using 20 visual and 20 tactile, or 35 visual and 5 tactile, or any other combination. This is different to all previous work encountered, where, when it comes to sensor fusion, all data from both modalities are typically used (such as in the first experiment).|$|R
40|$|As a {{supervised}} dimensionality reduction technique, linear {{discriminant analysis}} has a serious overfitting problem {{when the number of}} <b>training</b> <b>samples</b> per class is small. The main reason is that the between-and within-class scatter matrices computed from the limited number of <b>training</b> <b>samples</b> deviate greatly from the underlying ones. To overcome the problem without increasing the number of <b>training</b> <b>samples,</b> we propose making use {{of the structure of the}} given training data to regularize the between- and within-class scatter matrices by between-and within-cluster scatter matrices, respectively, and simultaneously. The within-and between-cluster matrices are computed from unsupervised clustered data. The within-cluster scatter matrix contributes to encoding the possible variations in intraclasses and the between-cluster scatter matrix is useful for separating extra classes. The contributions are inversely proportional to the number of <b>training</b> <b>samples</b> per class. The advantages of the proposed method become more remarkable as the number of <b>training</b> <b>samples</b> per class decreases. Experimental results on the AR and Feret face databases demonstrate the effectiveness of the proposed method...|$|R
5000|$|... by {{computing}} the embeddings of the prediction step via the kernel sum rule and the embedding of the conditioning step via kernel Bayes' rule. Assuming a <b>training</b> <b>sample</b> [...] is given, one can in practice estimate [...] and filtering with kernel embeddings is thus implemented recursively {{using the following}} updates for the weights ...|$|E
5000|$|A {{model of}} an unknown {{probability}} distribution p, may be proposed based on a <b>training</b> <b>sample</b> that was drawn from p. Given a proposed probability model q, one may evaluate q by asking how well it predicts a separate test sample x1, x2, ..., xN also drawn from p. The perplexity of the model q is defined as ...|$|E
5000|$|Out-of-bag (OOB) error, {{also called}} out-of-bag estimate, {{is a method}} of {{measuring}} the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating to sub-sample data samples used for training. OOB is the mean prediction error on each <b>training</b> <b>sample</b> , using only the trees {{that did not have}} [...] in their bootstrap sample.|$|E
3000|$|... [...]. In our approach, <b>training</b> <b>samples</b> are {{clustered}} {{in the primary}} recognition, and only the rough samples are used to train the SVM in the advanced recognition. More specifically, there are 70 <b>training</b> <b>samples</b> for the SVM in the advanced recognition, i.e., [...]...|$|R
3000|$|... [...]. Let us {{suppose that}} not all nodes have {{access to all the}} <b>samples,</b> so the <b>training</b> <b>samples</b> {{accessible}} from node j is the subset S_n^j. Let us also denote the set of the indices of the <b>training</b> <b>samples</b> in S [...]...|$|R
30|$|We {{used all}} the <b>{{training}}</b> <b>samples</b> in the AFEW training set and collected <b>training</b> <b>samples</b> from the CK+ and MMI DBs, {{according to the}} person-independent tenfold cross-validation rule. To reduce the training process time, we <b>trained</b> the <b>samples</b> from the three datasets together. All of the <b>training</b> <b>samples</b> were normalized to 100 × 100 pixel facial patches and processed by histogram equalization, and no color information was used. To enhance the generalization performance of boosting learning, we used some transformations in the <b>training</b> <b>samples</b> (mirror reflection, rotate the images, etc.), and finally increased the original number of samples {{by a factor of}} 64. Normalization was not performed on any of the testing sample sequences. In the training stages, we adopted the training data of the current processing expression as positive sample data, and data from other expressions as negative data.|$|R
5000|$|The Kak neural network, {{also called}} the CC4 network [...] is an instantaneously trained neural network that creates a new [...] "hidden neuron" [...] for each <b>training</b> <b>sample,</b> {{achieving}} immediate training for binary data. The training algorithm for binary data creates links to the new hidden node that simply reflects the binary values in the training vector. Hence, no computation is involved.|$|E
5000|$|The {{available}} sample {{units with}} known attributes and known performances {{is referred to}} as the [...] "training sample". The units in other samples, with known attributes but unknown performances, are referred to as [...] "out of training sample" [...] units. The out of sample units do not necessarily bear a chronological relation to the <b>training</b> <b>sample</b> units. For example, the <b>training</b> <b>sample</b> may consist of literary attributes of writings by Victorian authors, with known attribution, and the out-of sample unit may be newly found writing with unknown authorship; a predictive model may aid in attributing a work to a known author. Another example is given by analysis of blood splatter in simulated crime scenes in which the out of sample unit is the actual blood splatter pattern from a crime scene. The out of sample unit may be from the same time as the training units, from a previous time, or from a future time.|$|E
50|$|The {{best choice}} of k {{depends upon the}} data; generally, larger values of k reduce the effect of noise on the classification, but make {{boundaries}} between classes less distinct. A good k can be selected by various heuristic techniques (see hyperparameter optimization). The special case where the class is predicted to be the class of the closest <b>training</b> <b>sample</b> (i.e. when k = 1) is called the nearest neighbor algorithm.|$|E
40|$|An {{algorithm}} is developed to automatically screen the outliers from massive <b>training</b> <b>samples</b> for Global Land Survey - Imperviousness Mapping Project (GLS-IMP). GLS-IMP {{is to produce}} a global 30 m spatial resolution impervious cover data set for years 2000 and 2010 based on the Landsat Global Land Survey (GLS) data set. This unprecedented high resolution impervious cover data set is not only significant to the urbanization studies but also desired by the global carbon, hydrology, and energy balance researches. A supervised classification method, regression tree, is applied in this project. A set of accurate <b>training</b> <b>samples</b> {{is the key to}} the supervised classifications. Here we developed the global scale <b>training</b> <b>samples</b> from 1 m or so resolution fine resolution satellite data (Quickbird and Worldview 2), and then aggregate the fine resolution impervious cover map to 30 m resolution. In order to improve the classification accuracy, the <b>training</b> <b>samples</b> should be screened before used to train the regression tree. It is impossible to manually screen 30 m resolution <b>training</b> <b>samples</b> collected globally. For example, in Europe only, there are 174 training sites. The size of the sites ranges from 4. 5 km by 4. 5 km to 8. 1 km by 3. 6 km. The amount <b>training</b> <b>samples</b> are over six millions. Therefore, we develop this automated statistic based algorithm to screen the <b>training</b> <b>samples</b> in two levels: site and scene level. At the site level, all the <b>training</b> <b>samples</b> are divided to 10 groups according to the percentage of the impervious surface within a sample pixel. The samples following in each 10 % forms one group. For each group, both univariate and multivariate outliers are detected and removed. Then the screen process escalates to the scene level. A similar screen process but with a looser threshold is applied on the scene level considering the possible variance due to the site difference. We do not perform the screen process across the scenes because the scenes might vary due to the phenology, solar-view geometry, and atmospheric condition etc. factors but not actual landcover difference. Finally, we will compare the classification results from screened and unscreened <b>training</b> <b>samples</b> to assess the improvement achieved by cleaning up the <b>training</b> <b>samples.</b> Keywords...|$|R
30|$|The QDF {{performed}} better when the <b>training</b> <b>samples</b> were correlated {{than when they}} were under uncorrelated normal distribution. The QDF {{performed better}} resulting in the reduction in misclassification error rates as group centroid separator increases with non increasing sample size under correlated <b>training</b> <b>samples.</b>|$|R
3000|$|... [...]. Similarly, when 60 {{samples are}} used as <b>training</b> <b>samples,</b> all of these samples are used to train the {{classical}} SVM, while there are 36 <b>training</b> <b>samples</b> for the SVM in the advanced recognition of the hybrid recognition, i.e., m = 60 and m [...]...|$|R
