5|10|Public
50|$|Rede Globo {{was founded}} in 1965, {{one year after the}} 1964 Brazilian coup d'état, and it {{consolidated}} as the biggest television network of Brazil during the 1970s. In this period, the military government implemented a policy of modernization of the telecommunications infrastructure of the country. In 1965 it created Embratel {{at the same time that}} Brazil was joining Intelsat. In 1968, the Ministry of Communications was created, and in the same year, the first FM radio stations appeared and the AERP (Assessoria Especial de Relações Públicas) was also created, which reinforced the need to propagate patriotic ideas and nationalism. In 1969, Brazil integrated with the global system of satellite communications. The intention of the military government was to oppose the cultural dominance characteristic of the left at the time. One of its weapons would have been television, with the regime turning a blind eye to the partnership between Roberto Marinho and the multinational Time-Life Television, which was technically prohibited by law. This contributed to the <b>technological</b> <b>jump</b> of Rede Globo.|$|E
40|$|We {{demonstrate}} spin-accumulation signals {{controlled by}} the gate voltage in a metal-oxide-semiconductor field effect transistor structure with a Si channel and a CoFe/$n^{+}$-Si contact at room temperature. Under {{the application of a}} back-gate voltage, we clearly observe the three-terminal Hanle-effect signal, i. e., spin-accumulation signal. The magnitude of the spin-accumulation signals can be reduced with increasing the gate voltage. We consider that the gate controlled spin signals are attributed to the change in the carrier density in the Si channel beneath the CoFe/$n^{+}$-Si contact. This study is not only a <b>technological</b> <b>jump</b> for Si-based spintronic applications with gate structures but also reliable evidence for the spin injection into the semiconducting Si channel at room temperature. Comment: 4 pages, 3 figure...|$|E
40|$|Generation, manipulation, and {{detection}} of a pure spin current, i. e., {{the flow of}} spin angular momentum without a charge current, are prospective approaches for realizing next-generation spintronic devices with ultra low electric power consumptions. Conventional ferromagnetic electrodes such as Co and NiFe have so far been utilized as a spin injector for generating the pure spin currents in nonmagnetic channels. However, the generation efficiency of the pure spin currents is extremely low at room temperature, giving rise to a serious obstacle for device applications. Here, we demonstrate the generation of giant pure spin currents at room temperature in lateral spin valve devices with a highly ordered Heusler-compound Co$_ 2 $FeSi spin injector. The generation efficiency of the pure spin currents for the Co$_ 2 $FeSi spin injectors reaches approximately one hundred times as large as that for NiFe ones, indicating that Heusler-compound spin injectors enable us to materialize a high-performance lateral spin device. The present study is a <b>technological</b> <b>jump</b> in spintronics and indicates the great potential of ferromagnetic Heusler compounds with half metallicity for generating pure spin currents. Comment: 16 pages, 3 figure...|$|E
2500|$|In 2012, the {{university}} completed {{construction of a}} Sustainable Learning Center (SLC) building. According to {{the university}}, the building sets a [...] "new global standard for green buildings" [...] by incorporating local building materials and being completely energy self-sufficient, even during its construction. It was {{designed to meet the}} Living Building Challenge requirements and become one of three buildings in the US to meet that standard as well as LEED Platinum certification. It houses both research and classroom activities and allows students to [...] "interactively monitor performance and energy efficiency". University officials hope this building proves that the county has the expertise for <b>technological</b> <b>jumps</b> of this kind.|$|R
40|$|Abstract: The {{union of}} discoveries in various {{different}} areas results in great <b>technological</b> <b>jumps,</b> as for example, {{the union of}} the Radiology with the Computer Science that is {{the base of the}} technology of the diagnosis by image. With the incorporation of the computerized tomography scan with the three dimensional processing (3 D) of the image, it was possible to have a virtual visualization of the body. Considering the great technological advances in medicine and engineering, this work aimed to present a Methodology for translating information from computerized tomography to CAD/CAM system, in order to design and to manufacture the model of human prosthesis. The integration of different areas such as Engineering assisting Medicine, Computer Science in Engineering and Physics in Engineering is other very important side of this work...|$|R
40|$|AbstractThe {{concept of}} {{evolution}} trends {{comes from the}} idea that all technical systems follow the same patterns of evolution, thus {{making it possible to}} anticipate the <b>technological</b> <b>jumps</b> enabling an inventive problem to be solved. The problem nowadays is that evolution horizons have turned to the need for more ecological products. Hence, the question to be answered is that of how evolution lines are affected by this objective change. In order to analyse this question, the authors selected the LiDS-wheel tool, which was developed for classifying and managing the different strategies {{that can be used in}} eco-design. The aim of the present work is to compare the evolution trends of TRIZ with the eco-design strategies presented by LiDS in order to analyse the effects on the environmental parameters when the evolution trends advance in a design. The paper discusses the usefulness of the current evolution lines used for eco-design...|$|R
40|$|International audienceUnderstanding the {{mechanisms}} {{that control the}} filtration of complex colloidal suspensions is a major challenge {{in the development of}} membrane-based processes in industry. One of the main limiting factors for the development of these processes lies in the formation of a polarization layer and/or deposit near the separating membrane. Elucidating the structure of the first layer of deposited colloids during the first steps of filtration, constitutes a considerable <b>technological</b> <b>jump</b> in the understanding of the physical mechanisms implied in the formation of these deposits. It is also important to identify the effects of the physicochemical properties, the rheological behavior and the hydrodynamic fields on the organisation of the colloids during filtration. The focus of this work is the in-situ characterization of the induced structures and flow properties of the polarization layers of colloidal suspensions when simultaneously subjected to a transmembrane pressure and tangential flow over the membrane. To fulfill this challenge a new tangential ultrafiltration SAXS cell has been developed at the "Laboratoire de Rhéologie" Grenoble, France. This cell allowed combining small angle x-ray scattering (SAXS) at the European Synchrotron Radiation Facility (ID 02 beamline) with membrane separation processes. Systems studied are anisotropic colloidal aqueous dispersions composed of sepiolite fibers 1 micrometer long and 0. 01 micrometers in diameter, a natural non-swelling fibrous clay. The initial concentration of the filtered suspensions was in the semi-dilute domain for which the suspensions exhibit a shear thinning rheological behavior. During filtration, a transmembrane pressure was applied and successive tangential flows were imposed. The permeation flux was continuously measured and simultaneously the x-ray beam (40 x 250 micrometer) crossed the lateral dimension of the cell to probe the structure of the deposit with time at different heights above the membrane. As already described in previous studies concerning frontal filtration mode [1 - 2], a calibration curve relating the absolute scattering intensity to the particles concentration has allowed to deduce the concentration profiles in the deposits at distances to the membrane above 300 micrometers. During these experiments the high level of concentration reached in the deposit up to 50 times the initial concentration, and the highly anisotropic structure formed has been identified as one of the main mechanisms controlling the filtration flux decrease. The results prove the possibility to obtain pertinent structural information in the vicinity of membrane surfaces during ultrafiltration. It offers essential experimental data necessary for improvement in theoretical and numerical modeling of the filtration process. [1] F. Pignon, A. Alemdar, A. Magnin, and T. Narayanan, Langmuir, 19, 8638 (2003). [2] F. Pignon, G. Belina, T. Narayanan, X. Paubel, A. Magnin and G. Gésan-Guiziou, The Journal of Chemical Physics, 121 (16), 8138 (2004) ...|$|E
40|$|Over {{the centuries}} the {{equipment}} {{used by the}} process industry went through little changes: it have been perfected but it have never been substantially changed. Indeed the type of chemical reactor currently used is the stirred tank, that works {{in the same way}} of a similar one built in 1800; logically, materials, control systems or safety systems changed, but the basic engineering remained the same. In recent years, a new equipment was proposed: it performs the same functions as the existing one, occupying less space, requiring less power and operating in a safer way. The changes required in a plant to achieve the above mentioned objectives are called Process Intensification; it can be described as the following: "Any chemical engineering development that leads to a substantially smaller, cleaner, safer and more energy efficient technology is process intensification". From the Process Identification point of view, it is possible to mention the development of new equipment, such as the Spinning Disk Reactors and Heat Exchange (HEX) Reactors, characterized by a remarkable <b>technological</b> <b>jump</b> with respect to the existing equipment: designers began to use physical phenomena previously neglected, such as the centrifugal force in the spinning disk reactor, or to combine into one equipment more unit operations, such as Reverse-Flow Reactors, Reactive Distillation [...] . These recent developments certainly provide more compact and cleaner plants, but there are more uncertainties about their capability to produce an actual increase of the safety. The use of more complex equipment, in example with moving parts or with more intense sources of energy, can even bring to safety problems not detected in traditional plants, also modifying the reliability of the system. Under the definition of Process Intensification it is possible to indicate different kinds of improvements to the plants: in order to analyze the effects of these improvements on safety and reliability, we made an assessment of the reliability in a traditional plant, and in an intensified plant, comparing their results. The process analyzed is related to a plant for the VOC (Volatile Organic Compound) abatement in a stream of inert gas. The traditional system is based on a fixed bed reactor; the intensified plant uses a Reverse-Flow Reactors. The selected plants were firstly subjected to a traditional safety analysis, using an operability analysis and then operating the extraction and quantification of the fault trees. During the analysis, we realized that the traditional methods (HAZOP and FT) worked well if applied to conventional systems, which arrive to a steady-state, but they were less suitable for modern plants, that work in a transitional regime. After the traditional safety analysis, we proceeded with a Integrated Dynamic Decision Analysis, that allows to evaluate more in detail the behavior for not stationary plants, in case of failure. From the application of the methodology to the specific case some general conclusions have been draw...|$|E
40|$|Abstract: Central banks {{should not}} be in the {{business}} of trying to prick asset price bubbles. Bubbles generally arise out of some combination of irrational exuberance, <b>technological</b> <b>jumps,</b> and financial deregulation (with more of the second in equity price bubbles and more of the third in real estate booms). Accordingly, the connection between monetary conditions and the rise of bubbles is rather tenuous, and anything short of inducing a recession by tightening credit conditions prohibitively is unlikely to stem their rise. Even if a central bank were willing to take that one-in-three or less shot at cutting off a bubble, the cost-benefit analysis hardly justifies such preemptive action. The macroeconomic harm from a bubble bursting is generally a function of the financial system’s structure and stability—in modern economies with satisfactory bank supervision, the transmission of a negative shock from an asset price bust is relatively limited, as was seen in the United States in 2002. However, where financial fragility does exist, as in Japan in the 1990 s, the costs of inducing a recession go up significantly, so the relative disadvantages of monetary preemption over letting the bubble run its course mount. In the end, there is no monetary substitute for financial stability, and no market substitute for monetary ease during severe credit crunch. These two realities imply that the central bank should not take asset prices directly into account in monetary policymaking but should be anything but laissez-faire in responding to sharp movements in inflation and output...|$|R
40|$|Central banks {{should not}} be in the {{business}} of trying to prick asset price bubbles. Bubbles generally arise out of some combination of irrational exuberance, <b>technological</b> <b>jumps,</b> and financial deregulation (with more of the second in equity price bubbles and more of the third in real estate booms). Accordingly, the connection between monetary conditions and the rise of bubbles is rather tenuous, and anything short of inducing a recession by tightening credit conditions prohibitively is unlikely to stem their rise. Even if a central bank were willing to take that one-in-three or less shot at cutting off a bubble, the cost-benefit analysis hardly justifies such preemptive action. The macroeconomic harm from a bubble bursting is generally a function of the financial system’s structure and stability—in modern economies with satisfactory bank supervision, the transmission of a negative shock from an asset price bust is relatively limited, as was seen in the United States in 2002. However, where financial fragility does exist, as in Japan in the 1990 s, the costs of inducing a recession go up significantly, so the relative disadvantages of monetary preemption over letting the bubble run its course mount. In the end, there is no monetary substitute for financial stability, and no market substitute for monetary ease during severe credit crunch. These two realities imply that the central bank should not take asset prices directly into account in monetary policymaking but should be anything but laissez-faire in responding to sharp movements in inflation and output, even if asset price swings are their source. bubbles, asset prices, monetary policy, central banks...|$|R
5000|$|A {{closely related}} concept {{is that of}} ‘tunneling through’ the Environmental Kuznets Curve (EKC). [...] The concept proposes that {{developing}} countries could learn from the experiences of industrialized nations, and restructure growth and development to address potentially irreversible environmental damages from an early stage and thereby ‘tunnel’ through any prospective EKC. Environmental quality thereby {{does not have to}} get worse before it gets better and crossing safe limits or environmental thresholds can be avoided. Although in principle the concepts of leapfrogging (focused on <b>jumping</b> <b>technological</b> generations) and tunnelling through (focused on pollution) are distinct, in practice they tend to be conflated.|$|R
40|$|Climate change causes an {{increased}} frequency of unfavourable environmental scenarios with abiotic and biotic stresses, requiring {{the development of}} novel adapted varieties. Phenotyping is the major limitation for selecting genotypes in this context. Phenome (www. phenome-fppn. fr) develops a versatile, high-throughput infrastructure and a suite of methods allowing characterisation of hundreds of genotypes of different species under environmental scenarios of climate changes (e. g. drought, high CO 2 and high temperatures). The infrastructure consists of (1) two platforms in controlled conditions (capacity of 1700 plants each) for in-depth analysis of leaf or root system architectures and growths under ranges of water deficits, CO 2 concentration and temperature; (2) two field platforms with semi-controlled environments, in particular large rainout-shelters and one free-air carbon enrichment (FACE) system (capacity 800 individual plots each); (3) three field platforms with higher throughputs (capacity 2000 individual plots each) equipped with soil and climate sensors. All platforms can cope with throughputs of 200 - 300 genotypes with the necessary number of repetitions and manipulate and/or control environmental conditions in order to impose well-characterised scenarios. Platforms are equipped with a consistent set of 3 D functional imaging techniques, namely detailed imaging of roots and shoots in controlled conditions, canopy imaging with an autonomous 'phenomobile' that captures functional and 3 D images of each plot, and drones that image hundreds of plots jointly. Two supporting platforms centralise metabolomic and structural measurements associated with phenotyping experiments. Platforms are accessible to public and private partners via the project website. Applications with <b>technological</b> <b>jumps</b> are developed at infrastructure level, with partnerships with French SMEs. They (1) improve our capacity to measure plant traits at different resolutions in field and platforms (eg root and shoot architectures, light interception, transpiration rate) and environmental conditions (novel sensors); (2) organise phenotypic data originating from different platforms, {{so that they can}} be saved and analysed for a long period by a wide scientific community; (3) handle very large datasets with applications on data cleaning via artificial intelligence and analyses of time-related data; an interface with plant and crop models is developed. These methods and techniques are widely transferred towards the phenotyping community, academic and industrial. Phenome has already resulted in the development of SMEs aimed at phenotyping and/or precision agriculture (including one spin off, several patents and new activities of already existing SMEs. Networking and training activities are developed towards seed companies, SMEs and the extension system. Phenome is part of an Infrastructure (I 3) European project (EPPN) and is participating to an initiative for a preparatory phase for a European ESFRI infrastructure. It participates to the French national roadmap of Infrastructures with a widened partnership (CEA, CNRS, INRA) ...|$|R
40|$|Today's {{demand for}} space-borne Synthetic Aperture Radar (SAR) data {{has grown to}} the point where {{significant}} commercial funding of advanced space-borne radar system development has been being made available. The current generation of commercial space-based SAR imaging satellites, such as RADARSAT- 2, Sentinel- 1, TerraSAR-X/TanDEM-X (PAZ), COSMO-SkyMED and ALOS- 2, operate at a single frequency (L-, Cand X-band) and are based on active phased array antenna technology that offers beam agility and adds polarization diversity. Consequently, these modern satellites are equipped with more than one receive channel (i. e., AD-converter) that can also be utilized to record measurements from multiple apertures in along-track direction. This is the principal prerequisite for a ground moving target indication (GMTI) 1 capability. While space-based SAR GMTI offers many advantages like global ground coverage and access to strategic regions, it also faces several obstacles such as high satellite velocity, Earth rotation and oftentimes small target reflection energy caused by the enormous distances of more than 1, 000 km among others. This book chapter presents the state-of-the-art of space-based SAR-GMTI science and technology with focus on recent advances and the latest direction of research and development (R&D) activities. Owing to an exponential cost <b>jump,</b> <b>technological</b> advances of space-based radars especially with regard to increased power, increased aperture sizes and additional receiver channels have only been somewhat incremental in the last decades. Spacecraft with more than two parallel receive paths are only expected to materialize two generations down the line. Hence, current R&D put emphasis on innovative new concepts trying to circumvent these technological limitations thereby often pushing the resources on existing SAR payloads to their limits. 2 Virtually all of these concepts are accompanied by cutting-edge but complex and resource-hungry signal-processing algorithms that only recently became feasible based on the fast-paced evolution in computing power over the last decade. Many of the presented proof-of-concept studies are considered building blocks of future operational space-based SAR capabilities, for instance, the synergy between high-resolution-wide-swath (HRWS) imaging and motion indication and estimation. This chapter attempts to provide a comprehensive, in-depth overview of the theory and the radar signalprocessing techniques required for space-based SAR-GMTI corroborated by real multichannel data from RADARSAT- 2...|$|R

