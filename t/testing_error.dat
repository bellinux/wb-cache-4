109|2383|Public
50|$|Robust machine {{learning}} typically {{refers to the}} robustness of {{machine learning}} algorithms. For a machine learning algorithm to be considered robust, either the <b>testing</b> <b>error</b> has {{to be consistent with}} the training error, or the performance is stable after adding some noise to the dataset.|$|E
5000|$|Five {{items are}} taken from the {{continuous}} process based on sound statistical sampling. These are processed in a batch and tested twice each. This includes replication of initial samples but {{does not allow for}} batch-to-batch variation in processing. The repeated tests on each provide some measure and control of <b>testing</b> <b>error.</b>|$|E
5000|$|Five {{items are}} taken from the {{continuous}} process based on sound statistical sampling. These are processed in five different batches and tested twice each. This plan includes proper replication of initial samples and also includes batch-to-batch variation. The repeated tests on each provide some measure and control of <b>testing</b> <b>error.</b>|$|E
40|$|In {{this paper}} we present the {{architecture}} of an intelligent <b>test</b> <b>error</b> detection agent that is able to independently supervise the test process. By means of rationally applied bin and cause specific retests it should detect and correct the majority of <b>test</b> <b>errors</b> with minimal additional test effort. To achieve this, the agent utilizes <b>test</b> <b>error</b> models learned from historical example data to rate single wafer runs. The resulting run specific <b>test</b> <b>error</b> hypotheses are sequentially combined with information gained from regular and ordered retests in order to infer and update a global <b>test</b> <b>error</b> hypothesis. Based on this global hypothesis the agent decides if a <b>test</b> <b>error</b> exists, what its most probable cause is and which bins are affected. Consequently, {{it is able to}} initiate proper retests to check the inferred hypothesis and if necessary correct the affected test runs. The paper includes a description of the general architecture and discussions about possible <b>test</b> <b>error</b> models, the inference approach to generate the <b>test</b> <b>error</b> hypotheses from the given information and a possible set of rules to act upon the inferred hypothesis...|$|R
40|$|In model {{selection}} {{procedures in}} supervised learning, a model is usually chosen {{so that the}} expected <b>test</b> <b>error</b> over all possible test input points is minimized. On the other hand, when the test input points (without output values) are available in advance, it is more effetive to choose a model so that the <b>test</b> <b>error</b> only at the test input points at hand is minimized. In this paper, we follow this idea and derive an estimator of the <b>test</b> <b>error</b> at the given test input points for linear regression. Our estimator is {{proved to be an}} unbiased estimator of the <b>test</b> <b>error</b> at the given test input points under certain conditions. Through the simulations with artificial and standard benchmark data sets, we show that the proposed method is successfully applied in <b>test</b> <b>error</b> estimation and is compared favorably to the standard cross-validation and an empirical Bayesian method in ridge parameter selection...|$|R
40|$|The {{estimated}} <b>test</b> <b>error</b> of {{a learned}} classifier {{is the most}} commonly reported measure of classifier performance. However, constructing a high quality point estimator of the <b>test</b> <b>error</b> {{has proved to be}} very difficult. Furthermore, common interval estimators (e. g. confidence intervals) are based on the point estimator of the <b>test</b> <b>error</b> and thus inherit all the difficulties associated with the point estimation problem. As a result, these confidence intervals do not reliably deliver nominal coverage. In contrast we directly construct the confidence interval by use of smooth data-dependent upper and lower bounds on the <b>test</b> <b>error.</b> We prove that for linear classifiers, the proposed confidence interval automatically adapts to the non-smoothness of the <b>test</b> <b>error,</b> is consistent under fixed and local alternatives, and does not require that the Bayes classifier be linear. Moreover, the method provides nominal coverage on a suite of test problems using a range of classification algorithms and sample sizes. Keywords...|$|R
50|$|In {{software}} <b>testing,</b> <b>error</b> guessing is a {{test method}} in which test cases used to find bugs in programs are established based on experience in prior testing. The scope of test cases usually rely on the software tester involved, who uses past experience and intuition to determine what situations commonly cause software failure, or may cause errors to appear. Typical errors include divide by zero, null pointers, or invalid parameters.|$|E
50|$|In {{software}} <b>testing,</b> <b>error</b> guessing is a {{test method}} in which test cases used to find bugs in programs are established based on experience in prior testing. The scope of test cases usually rely on the software tester involved, who uses past experience and intuition to determine what situations commonly cause software failure, or may cause errors to appear. Typical errors include divide by zero, null pointers, or invalid parameters. Error guessing has no explicit rules for testing; test cases can be designed depending on the situation, either drawing from functional documents or when an unexpected/undocumented error is found while testing operations.|$|E
40|$|In this paper, Short Term Load Forecasting (STLF) can {{be applied}} using Generalized Neuron Model (GNM) for under sum square error {{gradient}} function for different learning rates, with various training epochs and constant leaning rate, by having 30, 000 training epochs. The simulation results were the root mean square <b>testing</b> <b>error,</b> maximum <b>testing</b> <b>error,</b> minimum <b>testing</b> <b>error</b> were predicted...|$|E
5000|$|The VC {{dimension}} {{can predict}} a probabilistic {{upper bound on}} the <b>test</b> <b>error</b> of a classification model. Vapnik proved that the probability of the <b>test</b> <b>error</b> distancing from an upper bound (on data that is drawn i.i.d. from the same distribution as the training set) is given by: ...|$|R
40|$|Bergstrom {{showed that}} a {{necessary}} condition for a Pareto optimum with non-paternalistic altruism is classification as a selfish Pareto optimum. This paper shows that Bergstrom’s result does not generalize to the benefit-cost analysis of generic changes in public goods. There may exist good projects that will be rejected by a selfish-benefit cost test, a selfish <b>test</b> <b>error.</b> Selfish <b>test</b> <b>error</b> is linked to preference interdependence between public goods and income distribution, the same condition Musgrave identified as problematic for optimal public goods provision without altruism. Transferable selfish utilities provide freedom from selfish <b>test</b> <b>error...</b>|$|R
30|$|For the OSD(I)and {{the list}} of the <b>test</b> <b>error</b> {{patterns}} (10), the bit errors in the MRIPs can be considered as conditionally independent. Similarly, for the POSD(I 1,I 2)and {{the list of}} the <b>test</b> <b>error</b> patterns (12), the bit errors in the two segments can be considered to be conditionally independent.|$|R
40|$|This study {{reports the}} results from three {{artificial}} neural network models. Levenberg-Marquardt (LM), Generalized Regression Neural Networks (GRNN) and Learning Vector Quantization (LVQ) are applied to eight classification problems. Ten-fold cross validation is used to demonstrate the error rate of networks. The experiments show that the generalized regression neural networks outperform the other classifiers, where the average training performance is 0. 0436, the <b>testing</b> <b>error</b> rate is 0. 137 and the classification rate is 0. 80. On the contrary, by using Levenberg-Marquardt, the average training performance is 0. 092, the <b>testing</b> <b>error</b> rate is 0. 169 and the classification rate is 0. 59 and by using learning vector quantization the average training performance is 0. 078, the <b>testing</b> <b>error</b> rate is 0. 363 and the classification rate is 0. 64...|$|E
3000|$|All {{classifiers}} will {{be implemented}} using the ensemble[26] with Fisher linear discriminant as the base learner. The security is quantified using the ensemble’s ‘out-of-bag’ (OOB) error EOOB, which is an unbiased estimate of the minimal total <b>testing</b> <b>error</b> under equal priors, [...]...|$|E
40|$|Genetic testing {{technology}} has brought new hope—and reproductive options—to would-be parents. But when a <b>testing</b> <b>error</b> occurs or {{the results are}} misread or miscommunicated, children may be born with severe, lifelong disabilities. Attorneys can help families of disabled children get the compensation they need to secure their future. ...|$|E
40|$|Abstract—As in today’s {{semiconductor}} industries test {{costs can}} make up to 50 {{percent of the total}} production costs, an efficient <b>test</b> <b>error</b> detection becomes more and more important. In this paper, we present a new machine learning approach to <b>test</b> <b>error</b> detection that should provide a faster recognition of test system faults as well as an improved <b>test</b> <b>error</b> recall. The key idea is to learn a classifier ensemble, detecting typical <b>test</b> <b>error</b> patterns in wafer test results immediately after finishing these <b>tests.</b> Since <b>test</b> <b>error</b> detection has not yet been discussed in the machine learning community, we define central problem-relevant terms and provide an analysis of important domain properties. Finally, we present comparative studies reflecting the failure detection performance of three individual classifiers and three ensemble methods based upon them. As base classifiers we chose a decision tree learner, a support vector machine and a Bayesian network, while the compared ensemble methods were simple and weighted majority vote as well as stacking. For the evaluation, we used cross validation and a specially designed practical simulation. By implementing our approach in a semiconductor test department for the observation of two products, we proofed its practical applicability...|$|R
5000|$|Scores on 3 Administrative Scales used to {{identify}} <b>test</b> <b>errors</b> or unusual profiles.|$|R
40|$|International audienceThe {{high cost}} of testing certain analog, mixed-signal, and RF {{circuits}} has driven {{in the recent years}} the development of alternative low-cost tests to replace the most costly or even all standard specification tests. However, {{there is a lack of}} solutions for evaluating the parametric <b>test</b> <b>error,</b> that is, the <b>test</b> <b>error</b> for circuits with process variations, resulting from this replacement. For this reason, test engineers are often reluctant to adopt alternative tests since it is not guaranteed that test cost reduction is not achieved at the expense of sacrificing test quality. In this paper, we present a technique to estimate the parametric <b>test</b> <b>error</b> fast and reliably with parts per million accuracy. The technique is based on extreme value theory and statistical blockade. Relying on a small number of targeted simulations, it is capable of providing accurate estimates of parametric <b>test</b> <b>error</b> in the general scenario where a set of alternative tests replaces all or a subset of standard specification tests...|$|R
30|$|It can be {{seen that}} slight {{differences}} exist in the values between experimental and calculated results. Considering the existence of <b>testing</b> <b>error,</b> it can be suggested that the calculated result is in reasonable agreement with the experimental one. Therefore, present model is reliable to analysis the thermal stress–strain process of rigid restraint TSCB.|$|E
40|$|Abstract—A Novel fuzzy {{neural network}} {{combining}} with support vector learning mechanism called support-vector-based {{fuzzy neural network}}s (SVBFNN) is proposed. The SVBFNN combine the capability of minimizing the empirical risk (training error) and expected risk (<b>testing</b> <b>error)</b> of support vector learning in high dimensional data spaces and the efficient human-like reasoning of FNN...|$|E
40|$|We {{consider}} binary classification {{problems with}} positive definite kernels and square loss, {{and study the}} convergence rates of stochastic gradient methods. We show that while the excess testing loss (squared loss) converges slowly to zero {{as the number of}} observations (and thus iterations) goes to infinity, the <b>testing</b> <b>error</b> (classification error) converges exponentially fast if low-noise conditions are assumed...|$|E
40|$|We examine {{methods to}} {{estimate}} the average and variance of <b>test</b> <b>error</b> rates over a set of classifiers. We begin {{with the process of}} drawing a classifier at random for each example. Given validation data, the average <b>test</b> <b>error</b> rate can be estimated as if validating a single classifier. Given the test example inputs, the variance can be computed exactly. Next, we consider the process of drawing a classifier at random and using it on all examples. Once again, the expected <b>test</b> <b>error</b> rate can be validated as if validating a single classifier. However, the variance must be estimated by validating all classifers, which yields loose or uncertain bounds. Key words machine learning, Vapnik-Chervonenkis, validation. 1 Introduction The average and variance of the <b>test</b> <b>error</b> rate over a set of classifiers are indicators of the potential performance ability of the classifiers as an ensemble, in which their decisions are combined through some form of fusion (Bishop, 1995; Jacobs et. al., 1991; Jo [...] ...|$|R
30|$|Finally, we {{note that}} it is {{straightforward}} to develop the skipping criteria for efficient search {{of the list of}} the <b>test</b> <b>error</b> patterns in the OSD-based decoding schemes. For instance, one can consider the Hamming distances for one or more segments of the MRIPs between the received hard decisions (before the decoding) and the temporary decisions obtained using the <b>test</b> <b>error</b> patterns from the list. If any or all of the Hamming distances are above given thresholds, the <b>test</b> <b>error</b> pattern can be discarded without re-encoding and calculating its Euclidean distance. For the Q= 2 segments OSD being considered, our empirical results indicate that the thresholds of the number of bit errors in the first and the second segments should be ⌈ 0.35 dmin⌉and dmin, respectively.|$|R
30|$|This measure {{creates a}} test set as {{proposed}} by L 3 and returns the <b>test</b> <b>error</b> of the 1 NN classifier.|$|R
40|$|Albeit the {{application}} of Adaptive Neuro-Fuzzy Inference System (ANFIS) in many real world problems, it is not used in the entrepreneurship studies. The aim {{of this paper is}} to develop an ANFIS model to forecasting corporate entrepreneurship (CE) success at industrial organizations. A conceptual model consisting critical success factors (motivations) and critical failure factors (barriers) of CE success is proposed. 11 experts (academic and industrial) validated the CE success and failure factors with the help of ANFIS rules. 464 MBA graduates who are working at industrial organizations have participated in this research. For the sake of ANFIS, data was divided into two groups (training and checking). Findings reveal that ANFIS <b>testing</b> <b>error</b> with training data is 0. 057103 and ANFIS <b>testing</b> <b>error</b> with checking data is 0. 03342. So that, the developed fuzzy inference system has the best applicability and predictability for CE success...|$|E
40|$|Extensive use of {{automation}} in {{the clinical}} laboratory creates the potential for systematic errors that affect {{a large number of}} patient results before the error is discovered. When a large-scale <b>testing</b> <b>error</b> is found, the approaches recommended for responding to individual medical mishaps are often inadequate. This report uses 2 case studies to illustrate some of the unique challenges facing laboratory managers confronted with a large-scale <b>testing</b> <b>error.</b> We identify 9 distinct constituencies that may be impacted by large-scale testing errors, each of which requires laboratory management’s thoughtful and timely attention. In October 2008, Quest Diagnostics initiated what may have been the largest recall of clinical test results, due to a systematic laboratory error that potentially affected more than 300, 000 vitamin D values. 1 According to a company spokesperson, 7 % of vitamin D tests performed by Quest from early 2007 to mid 2008 were impacted by incorrec...|$|E
40|$|Abstract—This {{article is}} about {{steganalysis}} {{based on data}} mining and aggregated hypothesis testing. The article contains the description of several example steganalytic methods which work on separate fields. Further is presented a system which aggregates {{them in order to}} gain a better effectiveness of steganalysis process. Finally example tests are presented. Keywords—Steganography, steganalysis, data mining, hypothesis <b>testing,</b> <b>error</b> estimation. I...|$|E
30|$|Recall {{that the}} <b>test</b> <b>error</b> {{patterns}} are uniquely specified by bits within the MRIPs, whereas the bits outside the MRIPs are obtained using the parity check matrix of the code. In order {{to generate a}} list of the <b>test</b> <b>error</b> patterns independently of the particular generator matrix (i.e., independently of the particular code) as well as independently of the particular received sequence, we consider only the bit errors within the MRIPs. Hence, we assume that, for all <b>test</b> <b>error</b> patterns, the bit errors outside the MRIPs affect the value of the metric in (5) equally. More importantly, {{in order to reduce the}} list decoding complexity while improving the BER performance, we consider partitioning of the MRIPs into disjoint segments. This decoding strategy employing segments of the MRIPs is investigated next.|$|R
40|$|Abstract. Inertial {{parameters}} of the motor assembly include its mass, CM (center of mass) position, moment of inertia and product of inertia. Taking one vehicle drive motor as the research object, its mass and CM position are measured by using weight method and moment balance method respectively. Its moment of inertia and product of inertia are measured by using three-wire pendulum. On the basis of analyzing the <b>test</b> <b>error,</b> this paper proposed specific measures to reduce the <b>test</b> <b>error...</b>|$|R
40|$|When {{performing}} {{supervised learning}} with the model selected using validation error from sample splitting and cross validation, {{the minimum value}} of the validation error can be biased downward. We propose two simple methods that use the errors produced in the validating step to estimate the <b>test</b> <b>error</b> after model selection, and {{we focus on the}} situations where we select the model by minimizing the validation error and the randomized validation error. Our methods do not require model refitting, and the additional computational cost is negligible. In the setting of sample splitting, we show that, the proposed <b>test</b> <b>error</b> estimates have biases of size o(1 /√(n)) under suitable assumptions. We also propose to use the bootstrap to construct confidence intervals for the <b>test</b> <b>error</b> based on this result. We apply our proposed methods to a number of simulations and examine their performance...|$|R
40|$|Abstract: Systematic <b>testing,</b> <b>error</b> {{analysis}} and observations of {{both students and}} professionals performing typical activities, like translating or searching dictionaries, for instance, have enabled us to identify a number of shortcomings among our students which we think can be rectified by introducing relatively simple, computer based teaching techniques. The paper {{presents the results of}} studies done so far, as well as teaching applications developed {{as a result of these}} studies. Key Words: translation; error analysis; vocabulary; grammar; teachin...|$|E
40|$|Understanding and {{preventing}} overfitting {{is a very}} important issue in artificial neural network design, implementation, and application. Weigend (1994) reports that the presence and absence of overfitting in neural networks depends on how the <b>testing</b> <b>error</b> is measured, and that there is no overfitting in terms of the classification error (symbolic-level errors). In this paper, we show that, in terms of the classification error, overfitting does occur for certain representation used to encode the discrete attributes. We desig...|$|E
40|$|The work {{deals with}} the {{application}} of Support Vector Machines (SVM) for environmental and pollution spatial data analysis and modeling. The main {{attention is paid to}} classification of spatially distributed data with SVM and comparison with probabilistic mapping using nonparametric geostatistical model (indicator kriging). SVMs with RBF kernels were used. It is shown that optimal bandwidth of kernel can be chosen by minimizing <b>testing</b> <b>error.</b> Real data on sediments pollution in the Geneva lake are used...|$|E
3000|$|... [...]. Finally, the <b>testing</b> <b>errors</b> {{caused by}} {{rotation}} inaccuracy can be compensated by the solutions of γ_ 0 l^k, γ̃_ 0 l^k [...] and φ [...]...|$|R
40|$|Abstract: Support Vector Machines (SVMs) and Adaptive Boosting (AdaBoost) are two {{successful}} classification methods. They {{are essentially}} the same as they both try to maximize the minimal margin on a training set. In this work, we present an even platform to compare these two learning algorithms in terms of their <b>test</b> <b>error,</b> margin distribution and generalization power. Two basic models of polynomials and decision stumps are used to evaluate eight real-world binary datasets. We concluded that the generalization power of AdaBoost with linear SVMs as base learners and that of SVMs with decision stumps as kernels outperform other scenarios. Although the training error of AdaBoost approaches to zero with the increase number of weak learners, its <b>test</b> <b>error</b> starts to rise at a certain step. For both SVMs and AdaBoost, the cumulative margin distribution is indicative of the <b>test</b> <b>error.</b> Key words: SVM, AdaBoost, margin I...|$|R
30|$|Consider all (or an {{appropriate}} subset of) <b>test</b> <b>error</b> patterns (TEPs). By definition, a TEP, noted by e, is a binary vector of length k and Hamming weight w≤i.|$|R
