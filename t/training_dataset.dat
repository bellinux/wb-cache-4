1762|1148|Public
25|$|In the {{training}} phase, evaluation task {{participants were asked}} to use a <b>training</b> <b>dataset</b> to induce the sense inventories for a set of polysemous words. The <b>training</b> <b>dataset</b> consisting of a set of polysemous nouns/verbs and the sentence instances that they occurred in. No other resources were allowed other than morphological and syntactic Natural Language Processing components, such as morpohological analyzers, Part-Of-Speech taggers and syntactic parsers.|$|E
25|$|The KH-99 {{algorithm}} by Knudsen and Hein {{lays the}} basis of the Pfold approach to predicting RNA secondary structure. In this approach the parameterization requires evolutionary history information derived from an alignment tree in addition to probabilities of columns and mutations. The grammar probabilities are observed from a <b>training</b> <b>dataset.</b>|$|E
25|$|PCFGs extend {{context-free}} grammars {{similar to}} how hidden Markov models extend regular grammars. Each production is assigned a probability. The {{probability of a}} derivation (parse) {{is the product of}} the probabilities of the productions used in that derivation. These probabilities can be viewed as parameters of the model, and for large problems it is convenient to learn these parameters via machine learning. A probabilistic grammar's validity is constrained by context of its <b>training</b> <b>dataset.</b>|$|E
30|$|Lastly, we {{evaluate}} {{the performance of}} PreCount with different sizes of <b>training</b> <b>datasets.</b> For this evaluation, we varied the sizes of <b>training</b> <b>datasets</b> from 30 to 120 days. This evaluation is especially crucial in-order to investigate how the PreCount algorithm will perform during periods of early deployment where limited <b>training</b> <b>datasets</b> are available.|$|R
30|$|We {{extracted}} {{the text}} of the <b>train</b> <b>dataset</b> in part of title, abstract, and content. Lexitron dictionary [12] is adopted for use in this step. The <b>train</b> <b>dataset</b> articles are specified tags manually.|$|R
30|$|We {{evaluate}} different {{machine learning}} classifiers {{on both the}} filtered and unfiltered <b>training</b> <b>datasets.</b>|$|R
2500|$|Each {{structure}} in the grammar is assigned production probabilities devised from the structures of the <b>training</b> <b>dataset.</b> These prior probabilities give weight to predictions accuracy. The number of times each rule is used depends on the observations from the <b>training</b> <b>dataset</b> for that particular grammar feature. These probabilities are written in parenthesis in the grammar formalism and each rule will have a total of 100%. For instance: ...|$|E
2500|$|Many common {{approaches}} to sparse dictionary learning {{rely on the}} fact that the whole input data [...] (or at least a large enough <b>training</b> <b>dataset)</b> is available for the algorithm. However, this might not be the case in the real-world scenario as the size of the input data might be too big to fit it into memory. The other case where this assumption can not be made is when the input data comes in a form of a stream. Such cases lie in the field of study of online learning which essentially suggests iteratively updating the model upon the new data points [...] becoming available.|$|E
50|$|At this point, {{the data}} is split into two equal but {{mutually}} exclusive elements, a test and a <b>training</b> <b>dataset.</b> The <b>training</b> <b>dataset</b> {{will be used to}} let rules evolve which match it closely. The test dataset will then either confirm or deny these rules.|$|E
40|$|International audienceModel-based image {{segmentation}} {{has been extensively}} used in medical imaging to learn both shape and appearance of anatomical structures from <b>training</b> <b>datasets.</b> The more <b>training</b> <b>datasets</b> are used, the more accurate is the segmented model as we account {{for more information about}} its variability. However, <b>training</b> <b>datasets</b> of large size with a proper sampling of the population may not always be available. In this paper, we compare the performance of statistical models in the context of lower limb bones segmentation using MR images when {{only a small number of}} datasets is available for training. For shape, both PCA-based priors and shape memory strategies are tested. For appearance, methods based on intensity proﬁles are tested, namely mean intensity proﬁles, multivariate Gaussian distributions of pro- ﬁles and multimodal proﬁles from EM clustering. Segmentation results show that local and simple methods perform the best when a small number of datasets is available for training. Conversely, statistical methods feature the best segmentation results when the number of <b>training</b> <b>datasets</b> is increased...|$|R
3000|$|... [...]. Then {{each of the}} <b>training</b> <b>datasets</b> {{is again}} split {{randomly}} into two almost equal folds. For instance, S [...]...|$|R
3000|$|We {{create a}} dataset with 130 {{different}} couples of input/output data. We split the dataset in two, creating a <b>train</b> <b>dataset</b> with 80 [...]...|$|R
50|$|In the {{training}} phase, evaluation task {{participants were asked}} to use a <b>training</b> <b>dataset</b> to induce the sense inventories for a set of polysemous words. The <b>training</b> <b>dataset</b> consisting of a set of polysemous nouns/verbs and the sentence instances that they occurred in. No other resources were allowed other than morphological and syntactic Natural Language Processing components, such as morpohological analyzers, Part-Of-Speech taggers and syntactic parsers.|$|E
5000|$|Each {{structure}} in the grammar is assigned production probabilities devised from the structures of the <b>training</b> <b>dataset.</b> These prior probabilities give weight to predictions accuracy. The number of times each rule is used depends on the observations from the <b>training</b> <b>dataset</b> for that particular grammar feature. These probabilities are written in parenthesis in the grammar formalism and each rule will have a total of 100%. For instance: ...|$|E
50|$|Rather {{than relying}} on a single template, {{multiple}} templates can be used. The idea is to represent an image as a deformed version {{of one of the}} templates. For example, there could be one template for a healthy population and one template for a diseased population. However, in many applications, {{it is not clear how}} many templates are needed. A simple albeit computationally expensive way to deal with this is to have every image in a <b>training</b> <b>dataset</b> be a template image and thus every new image encountered is compared against every image in the <b>training</b> <b>dataset.</b> A more recent approach automatically finds the number of templates needed.|$|E
30|$|With the {{proposed}} auto-tagging methodology, the classification process supports retrieving specific information but tagging {{is focused on}} how to choose appropriate tags for the article. In this paper, classification is high because the test dataset (140 articles) is comprised of the <b>train</b> <b>dataset</b> (70 articles) that are used for classification. However, tag selection is not affected by such <b>train</b> <b>dataset.</b> Because tagging is implemented by semantic analysis of the article’s content by which TF-IDF weight and ontology weight are focused.|$|R
30|$|Supervised {{learning}} uses labeled <b>training</b> <b>datasets</b> {{to create}} models. There are various methods for labeling datasets known as ground truth (cf., Section 2.4). This learning technique is employed to “learn” to identify patterns or {{behaviors in the}} “known” <b>training</b> <b>datasets.</b> Typically, this approach is used to solve classification and regression problems that pertain to predicting discrete or continuous valued outcomes, respectively. On the other hand, {{it is possible to}} employ semi-supervised ML techniques in the face of partial knowledge. That is, having incomplete labels for training data or missing labels. Unsupervised learning uses unlabeled <b>training</b> <b>datasets</b> to create models that can discriminate between patterns in the data. This approach is most suited for clustering problems. For instance, outliers detection and density estimation problems in networking, can pertain to grouping different instances of attacks based on their similarities.|$|R
3000|$|... 2 [*]=[*]. 64, [...]. 34, and [...]. 30). Cross-validation {{results are}} again quite good, with {{sufficiently}} similar results for <b>training</b> <b>datasets</b> (mean adjusted R [...]...|$|R
50|$|The {{selection}} environment {{consists of}} the set of training records, which are also called fitness cases. These fitness cases could be a set of observations or measurements concerning some problem, and they form {{what is called the}} <b>training</b> <b>dataset.</b>|$|E
5000|$|For {{each model}} m in the bucket: Do c times: (where 'c' is some constant) Randomly divide the <b>training</b> <b>dataset</b> into two datasets: A, and B. Train m with A Test m with B Select {{the model that}} obtains the highest average score ...|$|E
50|$|The KH-99 {{algorithm}} by Knudsen and Hein {{lays the}} basis of the Pfold approach to predicting RNA secondary structure. In this approach the parameterization requires evolutionary history information derived from an alignment tree in addition to probabilities of columns and mutations. The grammar probabilities are observed from a <b>training</b> <b>dataset.</b>|$|E
3000|$|... where M is {{the number}} of <b>training</b> <b>datasets.</b> When the {{calculated}} value of the MSE is less or equal to the predefined desired MSE (MSE [...]...|$|R
30|$|Caltech 101 [28] image dataset has 9144 {{images and}} 102 categories. We removed the {{background}} category, {{given that the}} background images have no common feature {{in terms of their}} appearance. Every category has 40 ∼ 80 images with the size of roughly 300 × 200 pixels. We divided the <b>dataset</b> into a <b>train</b> <b>dataset</b> and a test <b>dataset.</b> The <b>train</b> <b>dataset</b> is composed of the top 80 % images of every category sorted by filename, and the test dataset is composed {{by the rest of the}} images.|$|R
40|$|International audienceWe {{propose a}} hybrid model {{combining}} a generative model and a discriminative model for signal labelling and classification tasks, aiming at taking {{the best from}} each world. The idea is to focus the learning of the discriminative model on most likely state sequences as output by the generative model. This allows {{taking advantage of the}} usual increased accuracy of generative models on small <b>training</b> <b>datasets</b> and of discriminative models on large <b>training</b> <b>datasets.</b> We instantiate this framework with Hidden Markov Models and Hidden Conditional Random Fields. We validate our model on financial time series and on handwriting data...|$|R
50|$|The {{disadvantages}} with lazy learning {{include the}} large space requirement {{to store the}} entire <b>training</b> <b>dataset.</b> Particularly noisy training data increases the case base unnecessarily, because no abstraction is made during the training phase. Another disadvantage is that lazy learning methods are usually slower to evaluate, though this is coupled with a faster training phase.|$|E
50|$|For example, in October 2006, Netflix {{offered a}} $1 million prize for a 10% {{improvement}} in its recommendation system. Netflix also released a <b>training</b> <b>dataset</b> for the competing developers to train their systems. While releasing this dataset, they provided a disclaimer: To protect customer privacy, all personal information identifying individual customers {{has been removed}} and all customer ids sic {{have been replaced by}} randomly assigned ids sic.|$|E
50|$|Medical {{images can}} vary {{significantly}} across individuals due to people having organs of different shapes and sizes. Therefore, representing medical images {{to account for}} this variability is crucial. A popular approach to represent medical images is {{through the use of}} one or more atlases. Here, an atlas refers to a specific model for a population of images with parameters that are learned from a <b>training</b> <b>dataset.</b>|$|E
30|$|These {{acquired}} vibration {{signals are}} {{used to test the}} DBN-based fault diagnosis system. These vibration signals are divided into <b>training</b> <b>datasets</b> and testing datasets separately, and both datasets are randomized before being used in the DBN model.|$|R
40|$|Members of {{the remote}} sensing community, {{especially}} those working in atmospheric-profile retrieval science, often expend considerable energy and {{resources in the}} construction of <b>training</b> <b>datasets</b> for two of its primary components: (1) fast transmittance models; (2) regression retrieval. While the latter often provides a first guess for some physical retrieval technique, it can also serve as the “answer. ” The Experiment An experiment was designed to employ the IAPP retrieval package and the “flyover ” (direct-readout) ATOVS passes received at Madison, Wisconsin, to assess the influence of different <b>training</b> <b>datasets</b> on retrieval accuracy. At the same time, the effect of advances in the underlying line-by-line calculations on the performance of fast transmittance models, along with changes in the <b>training</b> <b>datasets,</b> could be evaluated. In the area of radiative transfer, the CIMSS- 32 (31 profiles plus the Standard Atmosphere) and UMBC- 49 (48 profiles plus Standard Atmosphere) <b>training</b> <b>datasets</b> have been employed to construct coefficients for the fast models. For regression retrieval/first-guess development, the NOAA- 88 b and SEEBORv 3 datasets have been used. Retrievals have been produced from all available NOAA- 16 flyover passes from 10 February 2005 onward, using the following "combinations of ingredients": A. existing "operational " version (IAPPv 2. 01) : CIMSS- 32 fast transmittance, NOAA 88 b first guess (Old/Old); B. UMBC- 49 fast transmittance and NOAA 88 b first guess (New/Old); C. UMBC- 49 fast transmittance and SEEBORv 3 first guess (New/New). For all three fast-model/regression situations, the retrievals were also produced with the aid of an NWP first guess, obtained from NCEP’s GFS (formerly AVN) model...|$|R
40|$|The {{purpose of}} this study was to {{investigate}} the effect of a noise injection method on the “overfitting” problem of artificial neural networks (ANNs) in two-class classification tasks. The authors compared ANNs trained with noise injection to ANNs trained with two other methods for avoiding overfitting: weight decay and early stopping. They also evaluated an automatic algorithm for selecting the magnitude of the noise injection. They performed simulation studies of an exclusive-or classification task with <b>training</b> <b>datasets</b> of 50, 100, and 200 cases (half normal and half abnormal) and an independent testing dataset of 2000 cases. They also compared the methods using a breast ultrasound dataset of 1126 cases. For simulated <b>training</b> <b>datasets</b> of 50 cases, the area under the receiver operating characteristic curve (AUC) was greater (by 0. 03) when training with noise injection than when training without any regularization, and the improvement was greater than those from weight decay and early stopping (both of 0. 02). For <b>training</b> <b>datasets</b> of 100 cases, noise injection and weight decay yielded similar increases in the AUC (0. 02), whereas early stopping produced a smaller increase (0. 01). For <b>training</b> <b>datasets</b> of 200 cases, the increases in the AUC were negligibly small for all methods (0. 005). For the ultrasound dataset, noise injection had a greater average AUC than ANNs trained without regularization and a slightly greater average AUC than ANNs trained with weight decay. These results indicate that training ANNs with noise injection can reduce overfitting to a greater degree than early stopping and to a similar degree as weight decay...|$|R
50|$|The {{process of}} {{decision}} tree induction with gene expression programming starts, as usual, {{with an initial}} population of randomly created chromosomes. Then the chromosomes are expressed as decision trees and their fitness evaluated against a <b>training</b> <b>dataset.</b> According to fitness they are then selected to reproduce with modification. The genetic operators {{are exactly the same}} that are used in a conventional unigenic system, for example, mutation, inversion, transposition, and recombination.|$|E
5000|$|Evolutionary {{algorithms}} work {{by trying}} to emulate natural evolution. First, a random series of [...] "rules" [...] are set on the <b>training</b> <b>dataset,</b> which try to generalize the data into formulas. The rules are checked, {{and the ones that}} fit the data best are kept, the rules that do not fit the data are discarded. The rules that were kept are then mutated, and multiplied to create new rules.|$|E
50|$|LCS will cycle {{through these}} steps {{repeatedly}} for some user defined {{number of training}} iterations, or until some user defined termination criteria have been met. For online learning, LCS will obtain a completely new training instance each iteration from the environment. For offline learning, LCS will iterate through a finite <b>training</b> <b>dataset.</b> Once it reaches the last instance in the dataset, it {{will go back to}} the first instance and cycle through the dataset again.|$|E
30|$|To create <b>training</b> <b>datasets,</b> we {{randomly}} selected {{images from the}} available datasets. Specifically, the images on pp. 1975 - 1984 and pp. 4326 - 4335 (see Table  3) constituted the training sets, and other images {{were included in the}} test sets.|$|R
30|$|In all experiments, the k-nearest {{neighborhood}} (KNN) classifier with Euclidean {{distance and}} k[*]=[*] 5 is utilized for classification after feature selection. To avoid {{the influence of}} the classifier, the <b>training</b> <b>datasets</b> of the classifier for all experiments are kept the same.|$|R
40|$|International audienceDeep {{convolutional}} {{neural networks}} have recently proven extremely effective for difficult face recognition problems in uncontrolled settings. To train such networks, very large training sets are needed {{with millions of}} labeled images. For some applications, such as near-infrared (NIR) face recognition, such large <b>training</b> <b>datasets</b> are not publicly available and difficult to collect. In this work, we propose a method to generate very large <b>training</b> <b>datasets</b> of synthetic images by compositing real face images in a given dataset. We show that this method enables to learn models from as few as 10, 000 training images, which perform on par with models trained from 500, 000 images. Using our approach we also obtain state-of-the-art results on the CASIA NIR-VIS 2. 0 heterogeneous face recognition dataset...|$|R
