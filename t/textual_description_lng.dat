0|1227|Public
30|$|The {{approaches}} {{based on}} hearing comprise (i) <b>textual</b> <b>description,</b> {{in which a}} <b>textual</b> <b>description</b> of the graphical content was provided to learners prior to the lecture—the <b>textual</b> <b>description</b> was synthesized by screen readers; (ii) verbal description, in which the graphical representation content was read aloud by the educator during the lecture; and (iii) assistant support, in which the graphical representation content was read aloud by an assistant during the lecture.|$|R
40|$|The <b>textual</b> <b>description</b> {{of video}} {{sequences}} exploits conceptual {{knowledge about the}} behavior of depicted agents. An explicit representation of such behavioral knowledge facilitates not only the <b>textual</b> <b>description</b> of video evaluation results, but {{can also be used}} for the inverse task of generating synthetic image sequences from <b>textual</b> <b>descriptions</b> of dynamic scenes. Moreover, it is shown here that the behavioral knowledge representation within a cognitive vision system can be exploited even for prediction of movements of visible agents, thereby improving the overall performance of a cognitive vision system...|$|R
40|$|Documenting {{business}} processes using process models is {{common practice in}} many organizations. However, not all process information is best captured in process models. Hence, many organizations complement these models with <b>textual</b> <b>descriptions</b> that specify additional details. The problem with this supplementary use of <b>textual</b> <b>descriptions</b> is that existing techniques for automatically searching process repositories are limited to process models. They are not capable of taking the information from <b>textual</b> <b>descriptions</b> into account and, therefore, provide incomplete search results. In this paper, we address this problem and propose a technique {{that is capable of}} searching textual as well as model-based process descriptions. It automatically extracts process information from both descriptions types and stores it in a unified data format. An evaluation with a large Austrian bank demonstrates that the additional consideration of <b>textual</b> <b>descriptions</b> allows us to identify more relevant processes from a repository...|$|R
40|$|The {{algorithmic}} {{generation of}} <b>textual</b> <b>descriptions</b> of real world image sequences requires conceptual knowledge. The algorithmic generation of synthetic image sequences from <b>textual</b> <b>descriptions</b> requires conceptual knowledge, too. An explicit representation formalism for behavioral knowledge based on formal logic is presented {{which can be}} utilized in both tasks [...] Understanding and Creation of video sequences...|$|R
5000|$|DESCRIPTION: A <b>textual</b> <b>description</b> of the {{functioning}} of the command or function.|$|R
40|$|This thesis {{investigates the}} task of {{learning}} visual object category recognition from <b>textual</b> <b>descriptions.</b> The work contributes primarily to the recognition of fine-grained object categories, such as animal and plant species, where {{it may be difficult}} to collect many images for training. but where <b>textual</b> <b>descriptions</b> are readily available, for example from online nature guides. The idea of using <b>textual</b> <b>descriptions</b> for fine-grained object category recognition is explored in three separate but related tasks. The first is {{the task of}} learning recognition of object categories solely from textual descriptions; no category-specific training images are used. Our proposed framework comprises three components: (i) natural language processing to build object category models from textual descriptions; (ii) visual processing to extract visual attributes from test images; (Hi) generative model connecting textual terms and visual attributes from images. As an 'upper-bound' we also evaluate how well humans perform in a similar task. The proposed method was evaluated on a butterfly dataset as an example, performing substantially better than chance, and interestingly comparable to the performance of non-native English speakers. The second task is an extension to the first. Here we focus on the problem of learning models for attribute terms (e. g. "orange bands"), from a set of training classes disjoint from the test classes. Attribute models are learnt independently for each attribute term in a weakly supervised fashion from <b>textual</b> <b>descriptions,</b> and are used in conjunction with <b>textual</b> <b>descriptions</b> of the test classes to build probabilistic models for object category recognition. A modest accuracy was achieved with our method when evaluated on a butterfly dataset, although performance was substantially improved with some human supervision to combine similar attribute terms. The third task explores how <b>textual</b> <b>descriptions</b> can be used to automatically harvest training images for each object category. Starting with just the category name, a <b>textual</b> <b>description</b> and no example images, web pages are gathered from search engines, and images filtered based on how similar their surrounding texts are to the given <b>textual</b> <b>description.</b> The idea is that images in close proximity to texts that are similar to the <b>textual</b> <b>description</b> are more likely to be example images of the desired category. The proposed method is demonstrated for a set of butterfly categories. where images were successfully re-ranked based on their corresponding text blocks alone, with many categories achieving higher precision than their baselines at early stages of recall. The proposed approaches of exploiting <b>textual</b> <b>descriptions,</b> although still in their infancy, shows potential for visual object recognition tasks, effectively reducing the amount of human supervision required for annotating images. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
5000|$|Description : Zero or more. ML_STRING. A non-formatted <b>textual</b> <b>description</b> of the event.|$|R
5000|$|Exhibitor {{profiles}} (normally a <b>textual</b> <b>description</b> of each exhibitor plus their contact details) ...|$|R
5000|$|Area {{of concern}} view: <b>Textual</b> <b>description</b> of a {{phenomenon}} {{represented in the}} role model.|$|R
40|$|<b>Textual</b> <b>descriptions</b> {{of design}} {{patterns}} are ambiguous and {{may lead to}} conflicting interpretations. Since patterns are meant for communication and education, a correct and complete understanding is a prerequisite to their successful usage. Formal specification of design patterns is meant to complement existing <b>textual</b> <b>descriptions.</b> Formal specification allows a rigorous reasoning of design patterns and facilitates tool support for their usage...|$|R
50|$|Julia's job was {{to explore}} a virtual world {{consisting}} of pages of <b>textual</b> <b>descriptions,</b> with links between them, and to construct an internal map of that world and answer questions about it (including path information such as the shortest route from one room to another, and matching information, such as which rooms contained {{a certain kind of}} object or <b>textual</b> <b>description).</b>|$|R
40|$|Abstract—Concern {{localization}} {{refers to}} the process of locating code units that match a particular <b>textual</b> <b>description.</b> It takes as input textual documents such as bug reports and feature requests and outputs a list of candidate code units that need to be changed to address the bug reports or feature requests. Many information retrieval (IR) based concern localization techniques have been proposed in the literature. These techniques typically represent code units and <b>textual</b> <b>descriptions</b> as a bag of tokens at one level of abstraction, e. g., each token is a word, or each token is a topic. In this work, we propose multi-abstraction concern localization. A code unit and a <b>textual</b> <b>description</b> is represented at multiple abstraction levels. Similarity of a <b>textual</b> <b>description</b> and a code unit, is now made by considering all these abstraction levels. We have evaluated our solution on AspectJ bug reports and feature requests from the iBugs benchmark dataset. Th...|$|R
5000|$|High level {{graphical}} and <b>textual</b> <b>description</b> of {{operational concept}} (high level organizations, missions, geographic configuration, connectivity, etc.).|$|R
5000|$|Validating UML models: quality {{engineers}} {{restore a}} <b>textual</b> <b>description</b> of a domain, original and restored descriptions are compared.|$|R
40|$|Concern {{localization}} {{refers to}} the process of locating code units that match a particular <b>textual</b> <b>description.</b> It takes as input textual documents such as bug reports and feature requests and outputs a list of candidate code units that need to be changed to address the bug reports or feature requests. Many information retrieval (IR) based concern localization techniques have been proposed in the literature. These techniques typically represent code units and <b>textual</b> <b>descriptions</b> as a bag of tokens at one level of abstraction, e. g., each token is a word, or each token is a topic. In this work, we propose multi-abstraction concern localization. A code unit and a <b>textual</b> <b>description</b> is represented at multiple abstraction levels. Similarity of a <b>textual</b> <b>description</b> and a code unit, is now made by considering all these abstraction levels. We have evaluated our solution on AspectJ bug reports and feature requests from the iBugs benchmark dataset. The experiment shows that our proposed approach outperforms a baseline approach, in terms of Mean Average Precision, by up to 19. 36 %...|$|R
40|$|Abstract—Developers often receive many feature requests. To {{implement}} these features, developers can leverage various methods {{from third}} party libraries. In this work, we propose an automated approach that takes as input a <b>textual</b> <b>description</b> of a feature request. It then recommends methods in library APIs that developers {{can use to}} implement the feature. Our recommendation approach learns from records of other changes made to software systems, and compares the <b>textual</b> <b>description</b> of the requested feature with the <b>textual</b> <b>descriptions</b> of various API methods. We have evaluated our approach on more than 500 feature requests of Axis 2 /Java, CXF, Hadoop Common, HBase, and Struts 2. Our experiments show that our approach is able to recommend the right methods from 10 libraries with an averag...|$|R
50|$|The {{interface}} {{is available}} in many languages, and the <b>textual</b> <b>description</b> of each item may have multiple versions in different languages.|$|R
30|$|Items of {{this type}} again confront examinees with a brief <b>textual</b> <b>description</b> of a {{business}} transaction, identical to the items-type described above.|$|R
50|$|The National Center for Biomedical Ontology (www.bioontology.org) {{develops}} {{tools for}} automated annotation of database records {{based on the}} <b>textual</b> <b>descriptions</b> of those records.|$|R
5000|$|High-Level Operational Concept Graphic (OV-1) : High level {{graphical}} and <b>textual</b> <b>description</b> of {{operational concept}} (high level organizations, missions, geographic configuration, connectivity, etc.).|$|R
5000|$|ROLAP {{tools are}} better at {{handling}} non-aggregatable facts (e.g., <b>textual</b> <b>descriptions).</b> MOLAP tools tend to suffer from slow performance when querying these elements.|$|R
30|$|The {{acronyms}} {{were derived}} from the original Spanish names. Therefore, the <b>textual</b> <b>descriptions</b> {{do not reflect the}} acronyms. We also provide the English acronyms in parentheses.|$|R
40|$|Concern {{localization}} {{refers to}} the process of locating code units that match a particular <b>textual</b> <b>description.</b> It takes as input textual documents such as bug reports and feature requests and outputs a list of candidate code units that are relevant to the bug reports or feature requests. Many information retrieval (IR) based concern localization techniques have been proposed in the literature. These techniques typically represent code units and <b>textual</b> <b>descriptions</b> as a bag of tokens at one level of abstraction, e. g., each token is a word, or each token is a topic. In this work, we propose a multi-abstraction concern localization technique named MULAB. MULAB represents a code unit and a <b>textual</b> <b>description</b> at multiple abstraction levels. Similarity of a <b>textual</b> <b>description</b> and a code unit is now made by considering all these abstraction levels. We combine a vector space model and multiple topic models to compute the similarity and apply a genetic algorithm to infer semi-optimal topic model configurations. We have evaluated our solution on 136 concerns from 8 open source Java software systems. The experimental results show that MULAB outperforms the state-of-art baseline PR, which is proposed by Scanniello et al. in terms of effectiveness and rank...|$|R
30|$|Descriptive metadata. This {{includes}} the project {{name and a}} <b>textual</b> <b>description</b> of the project, {{which can be used}} to enable projects to be retrieved by free text searches.|$|R
40|$|The {{algorithmic}} {{generation of}} <b>textual</b> <b>descriptions</b> of image sequences requires conceptual knowledge. In our case, a stationary camera recorded image sequences of road tra#c scenes. The necessary conceptual knowledge {{has been provided}} {{in the form of}} a so-called Situation Graph Tree (SGT). Other endeavors such as the generation of a synthetic image sequence from a <b>textual</b> <b>description</b> or the transformation of machine vision results for use in a driver assistance system could profit from the exploitation of the same conceptual knowledge, but more in a planning (pre-scriptive) rather than a de-scriptive context...|$|R
3000|$|Projects - 10, 104 tuples with 52 {{attributes}} describing project {{information about}} program references, activities, <b>textual</b> <b>description</b> of project scope and objectives, details about partners and so on; [...]...|$|R
5000|$|Validating a bug fix: {{given an}} {{original}} and modified source code, quality engineers restore a <b>textual</b> <b>description</b> of the bug that was fixed, original and restored descriptions are compared.|$|R
40|$|We {{present an}} {{in-depth}} {{analysis of the}} Xilinx bitstream format. The information gathered in this paper allows bitstream compilation and decompilation. While not actually compromising current bitstream security, the easiness of the decompilation process should raise awareness about bitstream security issues. Available documentation from Xilinx and some custom assumptions about the bitstream format are presented and analyzed, so as to first gather a database mapping bitstream data to its related netlist elements, thanks to a suitable algorithm applied to a well-chosen bitstream. This database is then used as input to an efficient program which can compile a bitstream from a low-level <b>textual</b> <b>description</b> or conversely decompile a bitstream to the same <b>textual</b> <b>description</b> for any subsequent input. The whole process of database gathering and the decompilation of the bitstream format for a particular chip runs at about the speed of bitgen compilation. The sole process of compiling/decompiling a bitstream from/to its associated <b>textual</b> <b>description</b> runs two orders of magnitude faster...|$|R
40|$|International audienceDevelopers often receive many feature requests. To {{implement}} these features, developers can leverage various methods {{from third}} party libraries. In this work, we propose an automated approach that takes as input a <b>textual</b> <b>description</b> of a feature request. It then recommends methods in library APIs that developers {{can use to}} implement the feature. Our recommendation approach learns from records of other changes made to software systems, and compares the <b>textual</b> <b>description</b> of the requested feature with the <b>textual</b> <b>descriptions</b> of various API methods. We have evaluated our approach on more than 500 feature requests of Axis 2 /Java, CXF, Hadoop Common, HBase, and Struts 2. Our experiments show that our approach is able to recommend the right methods from 10 libraries with an average recall-rate@ 5 of 0. 690 and recall-rate@ 10 of 0. 779 respectively. We also show that the state-of-the-art approach by Chan et al., that recommends API methods based on precise text phrases, is unable to handle feature requests...|$|R
40|$|In this paper, {{we present}} a {{metamodel}} for <b>textual</b> use case <b>descriptions,</b> structurally conforming to the UML, to specify the behavior of use cases in a flow-oriented manner. While being primarily targeted at supporting requirements engineers in creating consistent use case models, the metamodel defines a textual representation of use case behavior that is easily understandable for readers, who are unaware of the underlying metamodel. Hence, the known benefits of natural language use case descriptions are preserved. Being formalized, consistency between UML-based use case representations and their <b>textual</b> <b>descriptions</b> can be automatically ensured. With NaUTiluS {{we present a}}n extensible, Eclipse-based toolkit, which offers integrated UML use case modeling support, as well as editing capabilities for their <b>textual</b> <b>descriptions...</b>|$|R
50|$|Video {{papers are}} a recent {{addition}} to practice of scientific publications. They most often combine an online video demonstration {{of a new}} technique or protocol combined with a rigorous <b>textual</b> <b>description.</b>|$|R
5000|$|Validating {{model changes}} {{for a new}} requirement: given an {{original}} and changed versions of a model, quality engineers restore the <b>textual</b> <b>description</b> of the requirement, original and restored descriptions are compared.|$|R
5000|$|Mundus subterraneus, quo universae denique naturae divitiae is a {{scientific}} textbook written by Athanasius Kircher, {{and published in}} 1665. The work depicts Earth's geography through <b>textual</b> <b>description,</b> as well as lavish illustrations.|$|R
40|$|Ontology {{development}} is a non-trivial task requiring expertise in the chosen ontological language. We propose a method for making the content of ontologies more transparent by presenting, {{through the use of}} natural language generation, naturalistic descriptions of ontology classes as textual paragraphs. The method has been implemented in a proof-of- concept system, OntoVerbal, that automatically generates paragraph-sized <b>textual</b> <b>descriptions</b> of ontological classes expressed in OWL. OntoVerbal has been applied to ontologies that can be loaded into Protégé and been evaluated with SNOMED CT, showing that it provides coherent, well-structured and accurate <b>textual</b> <b>descriptions</b> of ontology classes...|$|R
40|$|AbstractThis paper proposes {{ontology}} based {{conceptual framework}} for storage and retrieval of Digitized Museum Artifacts. The proposed framework uses ontology structure for automatic image annotation. It supports semantic retrieval by combining ontological concepts, visual and textual features automatically extracted from images and their <b>textual</b> <b>descriptions.</b> The Ontology-driven analysis module automatically generates annotation for domain objects. This paper also reports a new dataset designed for its evaluation. The dataset consists of images displayed in various galleries of Allahabad museum along with their <b>textual</b> <b>description.</b> We have collected 1200 images and extracted their visual and textual features {{for the purpose of}} retrieval...|$|R
50|$|This {{could be}} a {{specification}} published by creators of Foo Protocol. Conversation flows, transaction interchanges, and states are not defined in ASN.1, but are left to other notations and <b>textual</b> <b>description</b> of the protocol.|$|R
