28|30|Public
40|$|The {{validity}} of the Kinesthetic Aftereffect (KAE) {{as a measure of}} personality has been criticized because of KAE’s poor test-retest reliability. However. systematic bias effects render KAE retest sessions invalid and make test-retest reliability an inappropriate measure of KAE’s <b>true</b> <b>reliability.</b> Internal consistency calculations, a better estimate of KAE’s <b>true</b> <b>reliability,</b> have been flawed in the past. New analyses of internal consistency data from 10 samples using 2 different KAE procedures are presented. Internal consistency of KAE scores was found to be high (median. 89 for 5 samples with Petrie’s procedure and median. 59 for 5 samples with Weintraub’s procedure). Some increment in reliability can apparently be obtained by extending the number of trials in the Weintraub procedure...|$|E
40|$|This study {{examined}} a historical mixture model {{approach to the}} evaluation of ratings made in “gold standard” and two-rater 2 × 2 contingency tables. Peirce's i and the derived i average were discussed {{in relation to a}} widely used index of reliability in the behavioral sciences, Cohen's κ. Sample size, population base rate of occurrence, the true “science of the method”, and guessing rates were manipulated across simulations. In “gold standard” situations, Peirce's i tended to recover the <b>true</b> <b>reliability</b> of ratings as well as better than κ. In two-rater situations, iave tended to recover the <b>true</b> <b>reliability</b> as well as better than κ in most situations. The empirical utility and potential theoretical benefits of mixture model methods in estimating reliability are discussed, as are the associations between the i statistics and other modern mixture model approaches...|$|E
40|$|We {{consider}} the following question: "Is there a confidence upper limit with given minimum coverage properties which is better than all other confidence upper limits with the given minimum coverage properties?". We prove that for discrete data this question is answered in the negative when certain readily checked and commonly satisfied assumptions hold <b>true.</b> <b>Reliability</b> Confidence upper limit Discrete data...|$|E
30|$|In [13], Modiano {{proposed}} a <b>true</b> hop-by-hop <b>reliability</b> mechanism, which has better energy usage than standard Simple Positive-ACK protocol. This protocol, called Packet Length Optimization (PLO), adapts {{the length of}} transmitted packets {{in accordance with the}} underlying channel conditions; large packets are transmitted during good channel conditions and vice versa.|$|R
40|$|This article {{discusses}} {{basic concepts}} in Classical Test Theory and Item Response Theory. In {{the context of}} Classical Test Theory the concepts of observed and <b>true</b> score, <b>reliability</b> of observed scores, and item indices are discussed. Some common rules of thumb to interpret numeric values of these indices are also presented in line with some caveats. Some problems with Classical Test Theory are also summarized. In the context of Item Response Theory, basically, the logistic models are discussed highlighting {{the importance of the}} concept to be measured. Practical guidelines in taking the decision to accept the IRT model are discussed...|$|R
40|$|With {{all major}} European primes {{starting}} {{to work on}} programs 3 ̆e 100 satellites in 2015 the time for Space grade Commercial Off The Shelf components has come. For many years there have been discussions on costs savings by offering COTS components and especially the cubesat community knows several vendors that offer space systems in a COTS fashion. None of these component however qualify for the extreme requirements posed by the current constellations under design as {{most of them have}} to operate above 1000 km and are preferred to have a 10 to 15 year lifetime. This leads to several megarads of total dose en 15 years at 1400 km will require the ability to withstand some 65. 000 thermal cycles. In order to fulfil these requirements full space grade <b>true</b> high <b>reliability</b> components will be needed where on the other hand the financial constraints are very strong. This is bound to lead to a completely new generation of real high reliability Space grade Commercial Off The Shelf (SCOTS) components. Without knowing these constellations would come, Lens R 2 ̆ 6 D has been focussing on recurring production of <b>true</b> high <b>reliability</b> sensors and now has several sensors in final stage of optimisation which will be offered in a SCOTS approach. The presentation will focus on the mega constellation market, the issues faced when developing SCOTS components and current state of development of our BiSon series of sunsensors...|$|R
40|$|Summary. It {{is first}} {{pointed out that}} most often used {{reliability}} coefficient α and one-factor model based reliability ρ are seriously biased when unique factors are covariated. In the case, the α {{is no longer a}} lower bound of the <b>true</b> <b>reliability.</b> Use of Bollen’s formula (Bollen 1980) on reliability is highly recommended. A web-based program termed “STERA ” is developed which can make stepwise reliability anal-ysis very easily with the help of factor analysis and structural equation modeling...|$|E
40|$|Cronbach's {{coefficient}} alpha {{is the most}} widely used estimator of the reliability of tests and scales. However, it has been criticized as being a lower bound and hence underestimating <b>true</b> <b>reliability.</b> A popular alternative to {{coefficient alpha}} is composite reliability, which is usually calculated in conjunction with structural equation modeling. A quantitative analysis of 2, 524 pairs of coefficient alpha and composite reliability values derived from empirical investigations revealed that although the average composite reliability value (. 86) exceeded the average corresponding coefficient alpha value (. 84), the difference was relatively inconsequential for practical applications such as meta-analysis. clos...|$|E
40|$|The article {{addresses}} {{the issue of}} intercoder reliability in meta-analyses. The current practice of reporting a single, mean intercoder agreement score in meta-analytic research leads to systematic bias and overestimates the <b>true</b> <b>reliability.</b> An alternative approach is recommended in which average intercoder agreement scores or other reliability statistics are calculated within clusters of coded variables. These clusters form a hierarchy in which the correctness of coding decisions at a given level of the hierarchy is contingent on decisions made at higher levels. Two separate studies of intercoder agreement in meta-analysis are presented to assess {{the validity of the}} model...|$|E
40|$|This paper {{examined}} {{how well the}} Standardized alpha coefficient (Standardized-alpha) estimates the <b>true</b> scale <b>reliability</b> under different test conditions. Six test conditions investigated were: strictly parallel, essentially parallel, strictly tau-equivalent, essentially tau-equivalent, strictly congeneric, and essentially congeneric conditions. The performance of the Standardized-alpha was compared {{with the performance of}} the Coefficient-alpha. The results revealed that the Standardized-alpha and the Coefficient-alpha estimated equally well under the parallel conditions. Under the tau-equivalent conditions, the Standardized-alpha displayed non-negligible amount of errors, while the amount of errors was very small for the Coefficient-alpha. Under congeneric conditions, both Standardized-alpha and Coefficient-alpha displayed non-negligible amount of errors. However, the amount of overall error was always larger for Standardized-alpha...|$|R
40|$|Reliability {{modeling}} {{must take}} into account two different types of phenomena, including the fault-occurrence behavior and the fault/error-handling behavior of a system. The effectiveness of the fault/error-handling behavior can be captured by instantaneous coverage probabilities. This paper has the objective to show that the assumption of instantaneous coverage leads to conservative predictions of system reliability for systems characterized by relatively long interevent times for fault occurrences and relatively short interevent times for fault/error-handling actions. The importance of this result is related {{to the fact that it}} can now be shown that model predictions based on instantaneous coverage are lower bounds on the <b>true</b> system <b>reliability.</b> Attention is given to a semi-Markov reliability model, instantaneous coverage approximations, the proof of conservative prediction, and the computation of coverage probabilities...|$|R
40|$|Nowadays {{crowdsourcing}} {{is widely}} used in supervised machine learning to facilitate the collection of ratings for unlabelled training sets. In order to get good quality results it is worth rejecting results from noisy/unreliable raters, {{as soon as they}} are discovered. Many techniques for filtering unreliable raters rely on the presentation of training instances to the raters identified as most accurate to date. Early in the process, the <b>true</b> rater <b>reliabilities</b> are not known and unreliable raters may be used as a result. This paper explores improving the quality of ratings for train- ing instances by performing re-rating. The re-rating relies on the detection of such in- stances and the acquisition of additional ratings for them when the rating process is over. We compare different approaches to re-rating and compare the improvements in labeling accuracy and the labeling costs of these approaches...|$|R
40|$|Approved {{for public}} release; {{distribution}} unlimited. The min:'. nial cut lover "bound for k-out-of-n systems is computed and compared wi^-h the <b>true</b> <b>reliability</b> of these systems. The {{size of the}} system, n, :. s increased; and selected degrees of system complexity, k/n, are studied. The resulting graphs cf system reliability versus component reliability indicate that both size and complexity cause a deterioration of the approximation, but "they also indicate {{that there is a}} limit to this deterioration. The minimal cut lower bound is then examined, theoretically, as the size of the system increases to infinity; and the limits of deterioration are obtained. [URL] United States Nav...|$|E
40|$|Cronbach’s α {{is widely}} used in social science {{research}} to estimate {{the internal consistency of}} reliability of a measurement scale. However, when items are not strictly parallel, the Cronbach’s α coefficient provides a lower-bound estimate of <b>true</b> <b>reliability,</b> and this estimate may be further biased downward when items are dichotomous. The estimation of standardized Cronbach’s α for a scale with dichotomous items can be improved by using the upper bound of coefficient ϕ. SAS and SPSS macros have been developed in this article to obtain standardized Cronbach’s α via this method. The simulation analysis showed that Cronbach’s α from upper-bound ϕ might be appropriate for estimating the real reliability when standardized Cronbach’s α is problematic...|$|E
40|$|A {{hierarchical}} scheme with {{cells and}} modules {{is crucial for}} managing design complexity during a large integrated circuit design. We present a methodology for thermal aware cell-based electromigration analysis suitable for integrating electromigration reliability analysis into a conventional IC design flow. A block or cell is characterized for reliability while it is characterized for power and timing. Reusing cell characterization data significantly reduces computational load while analyzing a full-chip layout. During full-chip analysis, we compute a layout-level temperature profile from cell power dissipations using a Fast Fourier Transform based algorithm. The described full-chip reliability assessment methodology has been implemented in an interconnect reliability CAD tool. We have exercised the tool to demonstrate performance-reliability tradeoff and the significance of thermalaware reliability analysis for <b>true</b> <b>reliability</b> aware design. Categories and Subject Descriptor...|$|E
40|$|Approved {{for public}} release, {{distribution}} unlimitedThis thesis is a comparative accuracy study of several discrete methods for lower confidence limits on series system reliability. Computer simulations {{were used to}} compare the accuracy of the procedures. Five hundred replications were used in all simulations. Accuracy of each procedure was determined by computing appropriate percentile points of the distributions of the lower confidence limits. A randomization technique was used to improve the performance of one of the procedures. The systems simulated had reliabilities ranging from 0. 720 to 0. 950. They were composed of five, ten, thirteen, and fifteen components, and had component sample sizes of fifteen, thirty, fifty, and larger in the case of unequal sample sizes. Based on the simulation results the accuracy of the procedures were compared by common comparison with the <b>true</b> system <b>reliabilities</b> which were known in advance prior to the component tests. Lieutenant Colonel, Indonesian Nav...|$|R
40|$|Culpepper and Aguinis (2011) {{highlighted}} {{the benefit of}} using the errors-in-variables (EIV) method to control for measurement error and obtain unbiased regression estimates. The current study investigated the EIV method and compared it to change scores and analysis of covariance (ANCOVA) in a two group pretest-posttest design. Results indicated that the EIV method’s estimates were unbiased under many conditions, but the EIV method consistently demonstrated lower power than the change score method. An additional risk with using the EIV method is that one must enter the covariate reliability into the EIV model, and results highlighted that estimates are biased if a researcher chooses a value that differs from the <b>true</b> covariate <b>reliability.</b> Obtaining unbiased results also depended on sample size. Our conclusion {{is that there is}} no additional benefit to using the EIV method over change score or ANCOVA methods for comparing the amount of change in pretest-posttest designs...|$|R
50|$|No {{arbitrarily}} reliable {{data transfer}} without end-to-end acknowledgment and re-transmission mechanisms: The ARPANET {{was designed to}} provide reliable data transport between any two end points of the network much like a simple I/O channel between a computer and a nearby peripheral device. In order to remedy any potential failures of packet transmission normal ARPANET messages were handed from one node to the next node with a positive acknowledgment and retransmission scheme; after a successful handover they were then discarded, no source-to-destination re-transmission in case of packet loss was catered for. However, in spite of significant efforts, perfect reliability as envisaged in the initial ARPANET specification {{turned out to be}} impossible to providea reality that became increasingly obvious once the ARPANET grew well beyond its initial four node topology. The ARPANET thus provided a strong case for the inherent limits of network based hop-by-hop reliability mechanisms in pursuit of <b>true</b> end-to-end <b>reliability.</b>|$|R
40|$|We {{propose a}} market-based {{information}} aggregation mechanism {{to manage the}} supply side uncertainty in the supply chain. In our analytical model, a simple supply chain consists {{of a group of}} retailers who order a homogeneous product from two suppliers. The two suppliers differ in their ability to fulfill orders – one always delivers orders and the other fulfills orders probabilistically. We model the supply chain decisions as a Stackelberg game where the supplier who has uncertain reliability decides a wholesale price before the retailers who independently receive signals about the supplier’s reliability determine their sourcing strategies. We then propose an information market to trade binary contracts with payoffs contingent on the supplier’s <b>true</b> <b>reliability.</b> Using a simple uniform demand distribution, we demonstrate that the market-based information aggregation mechanism improves the overall supply chain efficiency...|$|E
40|$|The latent class {{reliability}} coefficient (LCRC) is improved {{by using the}} divisive latent class model instead of the unrestricted latent class model. This results in the divisive latent class {{reliability coefficient}} (DLCRC), which unlike LCRC avoids making subjective decisions about the best solution and thus avoids judgment error. A computational study using large numbers of items shows that DLCRC also is faster than LCRC and fast enough for practical purposes. Speed and objectivity render DLCRC superior to LCRC. A decisive feature of DLCRC is that it aims at closely approximating the multivariate distribution of item scores, which might render the method suited when test data are multidimensional. A simulation study focusing on multidimensionality shows that DLCRC in general has little bias relative to the <b>true</b> <b>reliability</b> and is relatively accurate compared to LCRC and classical lower bound methods coefficients α and λ 2 and the greatest lower bound...|$|E
40|$|A select {{overview}} {{is provided}} of ongoing research {{focusing on the}} development and verification of integrated structural analysis and optimal design capabilities for advanced aerospace propulsion and power systems. Subjects discussed include the following: (1) Composites - analytical models (composite mechanics), integrated computational methods, and characterization of composite structural response and durability for resin-, metal-, and ceramic-matrix systems; (2) Advanced inelastic analysis - algorithm/numerical methods for more accurate and efficient analysis; (3) Constitutive modeling - theoretical formulation and characterization of thermoviscoplastic material behavior; (4) Computational simulation - engine structures from components to assembly, and up to an entire engine system subjected to simulated test-stand and mission load histories; (5) Probabilistic structural analysis - quantification {{of the effects of}} uncertainty in geometry, material, loads, and boundary conditions on structural response for <b>true</b> <b>reliability</b> assessment; and (6) Interdisciplinary optimization - incorporation of mathematical optimization and multidisciplinary analyses to provide streamlined, autonomous optimal design systems...|$|E
40|$|The {{underlying}} {{structure of}} {{why and how}} consumers value reliability of electric service is explored, together with the technological options and cost characteristics {{for the provision of}} reliability and the conditions under which market mechanisms can be used to match these values and costs efficiently. This analysis shows that the level of reliability of electricity provided through a network is a public good within a neighborhood, and unless planned demand reductions by customers have the identical negative value as an unexpected service interruption, market mechanisms will not reveal the <b>true</b> value of <b>reliability.</b> A publi...|$|R
40|$|The {{purpose of}} the present study was to develop a scale to measure sense of {{fulfillment}} in the context of family life. When junior high school students feel comfortable at home by spending time with their family, there are two pronounced factors that contribute to their state of mind. One of these is receiving recognition of their ideas and behavior by their family (spending time with family / feeling accepted). The other is having heart-to-heart communication with their parents (sharing their <b>true</b> feelings). The <b>reliability</b> and validity of this scale is demonstrated in the study...|$|R
30|$|To cater for {{the battery}} {{constraints}} of wireless devices, {{it is important}} to provide reliable communication without significant energy depletion. Contemporary wireless standards (e.g., 802.15. 4 [2], 802.11 [3], and 802.16 [4] standards) support a positive-ACK based retransmission scheme to provide reliable communication. This scheme, referred to as Simple Positive-ACK throughout the paper, has not been designed for energy efficiency. While there have been efforts to improve the energy efficiency of transmission reliability on wireless networks [5 – 17], most of the proposed protocols introduce a significant level of resource complexity to replace Simple Positive-ACK. Moreover, most of these protocols are not <b>true</b> hop-by-hop <b>reliability</b> protocols, although it has been acknowledged widely that hop-by-hop reliability is the key to overall network reliability [7 – 11]. Some of these protocols are designed for a particular communication model of a specific technology and hence cannot be classified as generic wireless ad hoc reliability protocol [9, 17].|$|R
40|$|Assuming {{that some}} extreme sample values have been censored or discarded, a {{reliability}} analysis of several two-parameter models, as the exponential, Pareto and power-function laws, is presented. Explicit expressions for the distribution, density and {{moments of the}} natural generalized pivot of the <b>true</b> <b>reliability</b> are deduced. Its asymptotic normality is also shown. Reliability point estimates are derived by selecting summary features of the pivotal quantity. Reliability confidence limits, which are shown to provide exact coverage probabilities, are readily found by solving simple nonlinear equations. Quite accurate approximate limits are given in closed forms. In addition, a procedure for determining confidence intervals of shortest length is proposed. Reliability tests, which are carried out using generalized p-values, satisfy the conventional repeated-sampling property. Minimum sample sizes and decision rules of optimal reliability demonstration plans, which accept good (bad) products with a certain high (low) probability, are obtained via iterative methods. A numerical example is included for illustrative purposes. ...|$|E
30|$|Version 1 of the {{database}} (K 08) aimed to rank numerically {{the reliability of}} paleointensity data based upon (1) the experimental method; (2) a specimen’s material; (3) the type of specimen (e.g., cylinder or chip); and (4) the number of specimens measured. The ranking {{was based on a}} comparison of archeointensity results obtained using various materials and methods (Donadini et al. 2007). The ranks of the four groups were equally weighted and this subjective choice may not reflect the <b>true</b> <b>reliability</b> of the data. Ranking could be based only upon the data available. In a number of cases, there were insufficient data to rank the data and this may exclude sites that may accurately record the paleomagnetic field or produce sample groups too small to provide statistically meaningful comparisons. The ranking did not consider the selection of specimen level data, i.e., the use of a range of statistics to assess the quality of the paleointensity experiment (see, e.g., Paterson et al. (2014)), which may influence the site mean data.|$|E
40|$|Today's {{psychological}} measurement depends {{almost exclusively}} on the "standardized test. " A certain amount of non-standardization, however, exists {{in the administration of}} any standardized test, with the amount unknown for any given test score. Time limits on tests pose a bigger problem since another variable is introduced, pressure. Test taking motivation must also be considered. The test could be too easy or too difficult, thus boring or frustrating the individual. Reliability is also a difficulty, since there is no <b>true</b> <b>reliability</b> computed for an individual. Proper application of computer technology permits a solution to many of the problems raised by standardized tests. The tests would be individualized, with items of known difficulty grouped or stratified by level of difficulty. The testing situation could be tailored to fit an individual's preferences and/or abilities and disabilities. Administrative fluctuations and test taking motivation could be eliminated. Individualized item sequence would tailor the test to the individual, as far as difficulty is concerned. Through the item sequence, reliability would,become more accurate, as the computer could more exactly pinpoint levels of difficulty. (KJ...|$|E
40|$|Providing excess {{electrical}} generation {{capacity for}} reliability purposes has an economic cost; {{it is also}} <b>true</b> that higher <b>reliability</b> adds {{to the value of}} electric service. After some point, however, the additional benefits do not warrant the additional cost. In this paper we examine the considerations that should determine sensible reliability levels for electric generation systems. We construct a cost/benefit argument which suggests [...] subject to various provisos that we make [...] that the present " 1 -day-in- 10 -year" loss of load probability target reliability planning criterion may be uneconomically high and that these targets might reasonably be reduced to at least a " 5 -day-in- 10 -year" level. ...|$|R
40|$|Reliability indices for mastery tests depend {{not only}} on true-score {{variance}} but also on mean and cutoff scores. This dependence was examined {{in the case of}} three decision-theoretic indices: (1) the coefficient of agreement; (2) kappa; and (3) the proportion of correct decisions. The binomial error model was assumed, with a two-parameter beta distribution for <b>true</b> scores. The <b>reliability</b> indices were computed at five values of the mean, four values of KR- 21, and four cutoff scores. Results show that the dependence of kappa on mean and cutoff scores is opposite to that of the proportion of correct decisions, which is linearly related to average threshold loss. Moreover, kappa can be very small when most examinees are classified correctly. Thus, objections against the classical reliability coefficient apply even more strongly to kappa...|$|R
40|$|Petroleum {{exploration}} and development are capital intensive and smart economic decisions {{that need to be}} made to profitably extract oil and gas from the reservoirs. Accurate quantification of uncertainty in production forecasts will help in assessing risk and making good economic decisions. This study investigates the effect of combining dynamic data with the uncertainty in static data to see the effect on estimates of uncertainty in production forecasting. Fifty permeability realizations were generated for a reservoir in west Texas from available petrophysical data. We quantified the uncertainty in the production forecasts using a likelihood weighting method and an automatic history matching technique combined with linear uncertainty analysis. The results were compared with the uncertainty predicted using only static data. We also investigated approaches for best selecting a smaller number of models from a larger set of realizations to be history matched for quantification of uncertainty. We found that incorporating dynamic data in a reservoir model will result in lower estimates of uncertainty than considering only static data. However, incorporation of dynamic data does not guarantee that the forecasted ranges will encompass the <b>true</b> value. <b>Reliability</b> of the forecasted ranges depends on the method employed. When sampling multiple realizations of static data for history matching to quantify uncertainty, a sampling over the entire range of realization likelihoods shows larger confidence intervals and is more likely to encompass the true value for predicted fluid recoveries, as compared to selecting the best models...|$|R
40|$|This paper {{considers}} {{the problem of}} choosing between an existing component whose reliability is well established and a new component that has an unknown reliability. In some scenarios, the designer may have some initial beliefs about the new component’s reliability. The designer may also {{have the opportunity to}} obtain more information and to update these beliefs. Then, based on these updated beliefs, the designer must make a decision between the two components. This paper examines the statistical approaches for updating reliability assessments and the decision policy that the designer uses. We consider four statistical approaches for modeling the uncertainty about the new component and updating assessments of its reliability: a classical approach, a precise Bayesian approach, a robust Bayesian approach, and an imprecise probability approach. The paper investigates the impact of different approaches on the decision between the components and compares them. In particular, given that the test results are random, the paper {{considers the}} likelihood of making a correct decision with each statistical approach under different scenarios of available information and <b>true</b> <b>reliability.</b> In this way, the emphasis is on practical comparisons of the policies rather than on philosophical arguments...|$|E
30|$|Reliability {{is defined}} in two {{different}} ways: 1) as the ratio between the population variance of the true scores and the variance of the observed scores or, 2) as the correlations between tests and retests taken {{under the assumption that}} the underlying true scores have not changed and that the tests and retests are conditionally independent, given the true scores. True scores are by definition unobservable and for this reason {{it is not possible to}} set up experiments providing data that can be used to estimate reliability. Instead, classical test theory uses Cronbach’s Alpha which is known to provide a lower bound of the <b>true</b> <b>reliability.</b> In IRT where models describing the association between the latent variable and the true scores on one hand and item responses on the other hand are known, it is easy to set up Monte Carlo experiments where test-retest reliability can be estimated directly by using 10, 000 simulated samples for the estimation. Harmon & Mesbah [47] describe such methods. We used these methods not only to estimate a simulated test-retest correlation, but also to estimate the correlation between true and observed scores since this correlation provides a better measure of the performance of the measurement instrument.|$|E
40|$|Predicting the {{reliability}} of software systems based on a component-based approach is inherently difficult, in particular due to failure dependencies between software compo-nents. One possible way to assess and include dependency aspects in software reliability models is to find upper bounds for probabilities that software components fail simulta-neously and then include these into {{the reliability}} models. In earlier research, {{it has been shown}} that including partial dependency information may give substantial improvements in predicting {{the reliability of}} compound software compared to assuming independence between all software components. Furthermore, {{it has been shown that}} including de-pendencies between pairs of data-parallel components may give predictions close to the system’s <b>true</b> <b>reliability.</b> In this paper, a Bayesian hypothesis testing approach for find-ing upper bounds for probabilities that pairs of software components fail simultaneously is described. This approach consists of two main steps: 1) establishing prior probability distributions for probabilities that pairs of software components fail simultaneously and 2) updating these prior probability distributions by performing statistical testing. In this paper, the focus is on the first step in the Bayesian hypothesis testing approach, and two possible procedures for establishing a prior probability distribution for the probability that a pair of software components fails simultaneously are proposed...|$|E
40|$|In {{recent years}} {{non-parametric}} density estimation {{has been extensively}} employed in several fields as a powerful descriptive tool, which is far more informative and robust than histograms. Moreover, the increased computation power of modern computers has made non-parametric density estimation a relatively "cheap" computation, helping to easily detect unexpected aspects of the distribution such as bimodality. However, it is also often neglected that non-parametric methods can only provide {{an estimate of the}} <b>true</b> density, whose <b>reliability</b> depends on various factors, such as the number of data available and the bandwidth. We will focus here on kernel density estimation and discuss the problem of computing bootstrap confidence intervals and test statistics for point-wise density estimation using Stata. Construction of confidence intervals and test of hypothesis about the true density are carried out using an asymptotically pivotal studentized statistic after computing a suitable estimator for its variance. The issue of asymptotic biased correction is also discussed and tackled. ...|$|R
40|$|The {{power of}} {{significance}} tests based on difference scores is indirectly {{influenced by the}} reliability of the measures from which differences are obtained. Reliability depends on the relative magnitude of true score and error score variance, but statistical power {{is a function of the}} absolute magnitude of these components. Explicit power calculations reaffirm the paradox put forward by Overall & Woodward (1975, 1976) -that significance tests of differences can be powerful even if {{the reliability of the}} difference scores is 0. This anomaly arises because power is a function of observed score variance but is not a function of <b>reliability</b> unless either <b>true</b> score variance or error score variance is constant. Provided that sample size, significance level, directionality, and the alternative hypothesis associated with a significance test remain the same, power always increases when population variance decreases, independently of reliability. Index terms: difference scores, error of measurement, power, significance tests, t test, test <b>reliability,</b> <b>true</b> scores...|$|R
40|$|Investigations {{regarding}} the failure behavior of heavy wire bonds show {{a significant decrease}} of cycles in strength with increasing wire diameter. The article explains the reasons theoretically. Based on the theory, possibilities to increase reliability are discussed. The innovation about this is the shaping of a chute on the bonding surface. This has to happen {{in a way that}} the current ampacity is not reduced. A laser will therefore cut across the bonding surface crosswise to the course of the wire. Previously the particular depth of cut is determined by simulation. The simulations prove the theoretically predicted enhancement of the <b>reliability</b> <b>true.</b> In addition, the method can be used as "non-destructive" in-situ measurement for quality assurance, since the crack growth can be derived from the thermography directly in the notches. The notches work as a magnifier for the thermography camera and eliminate the black coloring as a precondition in typical thermography. The research work is carried out within the project RoBE partially which is funded by BmBF; the manufacturing method and the geometric shapes have been patented...|$|R
