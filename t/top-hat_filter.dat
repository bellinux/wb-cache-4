36|35|Public
50|$|Exact non-digital {{implementations}} {{are only}} theoretically possible. Top-hat filters {{can be constructed}} by chaining theoretical low-band and high-band filters. In practice, an approximate <b>top-hat</b> <b>filter</b> can be constructed in analogue hardware using approximate low-band and high-band filters.|$|E
50|$|In real-space {{the filter}} {{performs}} nearest-neighbour filtering, incorporating components from neighbouring y-function values. However, despite their ease of implementation their practical use is limited as the real-space {{representation of a}} <b>top-hat</b> <b>filter</b> is the sinc function, which has the often undesirable effect of incorporating non-local frequencies.|$|E
50|$|The name <b>Top-hat</b> <b>filter</b> {{refers to}} several real-space or Fourier space {{filtering}} techniques (not {{to be confused}} with the top-hat transform). The name top-hat originates from the shape of the filter, which is a rectangle function, when viewed in the domain in which the filter is constructed.|$|E
50|$|In Fourier space, a top hat filter {{selects a}} band of signal of desired {{frequency}} by the specification of a lower and upper bounding frequencies. <b>Top-hat</b> <b>filters</b> are particularly easy to implement digitally.|$|R
50|$|Other {{forms of}} {{discrete}} wavelet transform include the non- or undecimated wavelet transform (where downsampling is omitted), the Newland transform (where an orthonormal basis of wavelets is formed from appropriately constructed <b>top-hat</b> <b>filters</b> in frequency space). Wavelet packet transforms are {{also related to}} the discrete wavelet transform. Complex wavelet transform is another form.|$|R
40|$|Progress {{variable}} approaches {{permit the}} efficient large eddy simulation (LES) of complex industrial combustion systems, where assumed shape filtered density functions (ß-FDFs) {{are widely used}} to account for subgrid scale effects. In this study a new modelling approach for the LES of partially premixed combustion is introduced, {{which is based on}} <b>top-hat</b> <b>filtered</b> premixed flamelet-generated manifolds (TH-PFGM) which are consistent with the LES methodology. Due to the <b>top-hat</b> <b>filtering</b> the resulting lookup tables require fewer dimensions than conventional ß-integrated tables, permitting a low-storage representation. In the present paper TH-PFGM is applied to a lifted swirl flame in a model gas turbine combustor. The paper presents the underlying TH-PFGM modelling theory, its extension to any number of dimensions, and simulation results from the LES of the model combustor. Results show that TH-PFGM accurately captures the flame lift off dynamics governed by a low frequency penetration of the flame into the fuel supply, which leads to fluid expansion and in turn flame lift off. The statistical data for flow and species concentration fields from LES are in good accordance with the experimental evidence, as well as results from a comparable LES study...|$|R
40|$|A {{digital filter}} called the <b>top-hat</b> <b>filter</b> is very useful for {{processing}} EELS spectra: strong background suppression almost removes the steep slope of AE~ 2 3 in spectra; in filtered spectra, tiny core-edges of microelements {{can be more}} easily detected and identified than in original ones; and by the change of the filter width as {{the needs of the}} case demand, an adequate filtered spec-trum can be obtained (expanding the filter width enables us to emphasize the existence of a core-edge and minimizing the filter width allows preservation of the fine structure of a core-edge). Key words = digital processing: <b>top-hat</b> <b>filter</b> (THF) : electron energy loss spec-troscopy (EELS) : qualitative analysis: core-edg...|$|E
40|$|Abstract—The paper {{presents}} a design method for high-speed image processing {{system based on}} FPGA structure. A temporal/spatial fusion filtering algorithm is proposed for detecting infrared small target. The algorithm is composed of <b>Top-hat</b> <b>filter,</b> three frames difference filter (TFDF), or operation, closing operation and adaptive threshold. Pipeline is taken to improve the processing speed, which is determined by <b>Top-hat</b> <b>filter</b> that cost most processing time. Then parallel streaming pipeline architecture of Top-hat is proposed. Some Dual-port RAM and FIFO is taken to realize the parallel structure, which can realize real-time processing. Multi-Core Shared-Memory is proposed to tackle the problem of high speed parallel storage. The experiments show that infrared small target can be detected real-time. Keywords-Infrared small target detecting; Pipeline architecture; FPGA; Temporal/spatial fusion filtering; Top-hat. I...|$|E
40|$|Abstract: The novel {{method of}} {{improving}} the quality metric of protein microarray image {{presented in this paper}} reduces impulse noise by using an adaptive median filter that employs the switching scheme based on local statistics characters; and achieves the impulse detection by using the difference between the standard deviation of the pixels within the filter window and the current pixel of concern. It also uses a <b>top-hat</b> <b>filter</b> to correct the background variation. In order to decrease time consumption, the <b>top-hat</b> <b>filter</b> core is cross structure. The experimental results showed that, for a protein microarray image contaminated by impulse noise and with slow background variation, the new method can significantly increase the signal-to-noise ratio, correct the trends in the background, and enhance the flatness of the background and the consistency of the signal intensity...|$|E
40|$|We {{propose a}} new filter, a smooth-$k$ space filter, {{to use in}} the Press-Schechter {{approach}} to model the dark matter halo mass function which overcomes shortcomings of other filters. We test this against the mass function measured in N-body simulations. We find that the commonly used sharp-$k$ filter fails to reproduce the behaviour of the halo mass function at low masses measured from simulations of models with a sharp truncation in the linear power spectrum. We show that the predictions with our new filter agree with the simulation results over a wider range of halo masses for both damped and undamped power spectra than is the case with the sharp-$k$ and real-space <b>top-hat</b> <b>filters.</b> Comment: 15 pages, 7 figures, LaTeX. Prepared for submission to JCA...|$|R
40|$|AbstractIn this paper, {{we propose}} an {{improved}} keypoint detection algorithm of object-based recognition for non-uniform illumination, called IKDSIFT, which is implemented using the SIFT approach, morphological operations, <b>Top-Hat</b> <b>filtering</b> and various techniques in pre-processing procedures. The number of keypoint rate of data sets was compared. Data sets consist of three hundred 150 x 150 images and thirty 851 x 566 images with different uniform and non-uniform illumination. The experimental {{results show that}} the number of keypoint detection is reciprocal to peak selection thresholds. The best algorithm is the proposed IKDSIFT, followed by the SIFT. The ASIFT performs the worst. Additionally, the SIFT and ASIFT can detect some peak selection thresholds while the IKDSIFT can detect all ranges of the peak and obtains the best result comparing to other ones. Hence, the proposed algorithm looks promising to be used for recognizing under non-uniform illumination...|$|R
40|$|Abstract. I {{investigate}} {{the effects of}} source clustering on the weak lensing statistics, more particularly on the statistical properties of the local convergence, κ, at large angular scales. The Perturbation Theory approach shows that the variance is not affected by source clustering at leading order but higher order moments such as {{the third and fourth}} moments can be. I compute the magnitude of these effects in case of an Einstein-de Sitter Universe for the angular <b>top-hat</b> <b>filtered</b> convergence. In these calculations the so-called Broadhurst and multiple lens coupling effects are neglected. The source clustering effect is found to be particularly important when the redshift distribution is broad enough so that remote background sources can be significantly lensed by closer concentrations of galaxy sources. The source clustering effects are shown to remain negligible, for both the skewness and the kurtosis, when the dispersion of the redshift of the sources is less than abou...|$|R
40|$|In present study, {{we discuss}} results of {{applicability}} of discrete filters for large eddy simulation (LES) method of forced compressible magnetohydrodynamic (MHD) turbulent flows with the scale-similarity model. Influences {{and effects of}} discrete filter shapes on the scale-similarity model are examined in physical space using a finite-difference numerical schemes. We restrict ourselves to the Gaussian filter and the <b>top-hat</b> <b>filter.</b> Representations of this subgrid-scale model which correspond to various 3 - and 5 -point approximations of both Gaussian and top-hat filters for different values of parameter ϵ (the ratio of the mesh size to the cut-off lengthscale of the filter) are investigated. Discrete filters produce more discrepancies for magnetic field. It is shown that the Gaussian filter is {{more sensitive to the}} parameter ϵ than the <b>top-hat</b> <b>filter</b> in compressible forced MHD turbulence. The 3 -point filters at ϵ= 2 and ϵ= 3 give the least accurate results and the 5 -point Gaussian filter shows the best results at ϵ= 2. Comment: 9 pages, 4 figure...|$|E
40|$|VirtualShave {{is a novel}} tool {{to remove}} hair from digital dermatoscopic images. First, {{individual}} hairs are identified using a <b>top-hat</b> <b>filter</b> followed by morphological postprocessing. Then, they are replaced through PDE-based inpainting with {{an estimate of the}} underlying occluded skin. VirtualShave's performance is comparable to that of a human operator removing hair manually, and the resulting images are almost indistinguishable from those of hair-free skin...|$|E
40|$|Approximate {{higher order}} {{polynomial}} inversion of the <b>top-hat</b> <b>filter</b> is developed {{with which the}} turbulent stress tensor in Large-Eddy Simulation can be consistently represented using the filtered field. Generalized (mixed) similarity models are proposed which improved the agreement with the kinetic energy transfer to small scales. These similarity models are analyzed for random periodic signals and the ensemble averaged spectra of the turbulent stress tensor and the corresponding models are compared...|$|E
40|$|I {{investigate}} {{the effects of}} source clustering on the weak lensing statistics, more particularly on the statistical properties of the local convergence, kappa, at large angular scales. The Perturbation Theory approach shows that the variance is not affected by source clustering at leading order but higher order moments such as {{the third and fourth}} moments can be. I compute the magnitude of these effects in case of an Einstein-de Sitter Universe for the angular <b>top-hat</b> <b>filtered</b> convergence. In these calculations the so-called Broadhurst and multiple lens coupling effects are neglected. The source clustering effect is found to be particularly important when the redshift distribution is broad enough so that remote background sources can be significantly lensed by closer concentrations of galaxy sources. The source clustering effects are shown to remain negligible, for both the skewness and the kurtosis, when the dispersion of the redshift of the sources is less than about 0. 15. Comment: 10 pages, 2 figures, accepted for publication in Astronomy and Astrophysics, revised versions with minor change...|$|R
40|$|Abstract — This paper {{presents}} {{a continuation of}} the study on a mathematical morphology based on left-continuous conjunctive uninorms given in [1]. Experimental results are displayed using the morphological Top-Hat transformation, used to highlight certain components of the image, and on the reduction and elimination of noise using alternate filters that are generated with the operators of opening and closing associated with these morphological operators Keywords — Mathematical morphology, <b>Top-Hat,</b> alternate <b>filters,</b> uninorms, representable uninorms, idempotent uninorm. ...|$|R
40|$|ABSTRACT [...] The {{analysis}} of Her- 2 /neu status {{is an effective}} indicator for the diagnosis of several types of breast carcinomas. Conventional evaluation is a difficult task since it involves manual counting of dots in multiple fluorescent in situ hybridization (FISH) images. In this paper we present a multistage algorithm for the automated evaluation of Her- 2 /neu status by the {{analysis of}} FISH images from breast carcinomas. The algorithm focuses on the detection of FISH spots and on the cell nuclei segmentation in order to perform overall case classification as positive or negative. Spots detection includes mainly a <b>top-hat</b> <b>filtering</b> stage, a binary thresholding, a 3 D template matching and a grey level contrast evaluation. Nuclei segmentation consists of a non-linear blue channel correction step, a global thresholding by Otsu algorithm, a grey level hole classification by a geometric rule and of the marked watershed transform using local h-dome maxima as markers. By the measurement of the FISH signals ratio per cell nucleus we perform the classification of cases. The performances of the algorithm were evaluated with receiver operating characteristic (ROC) analysis...|$|R
40|$|We {{develop a}} linear {{algorithm}} for extracting extragalactic point {{sources for the}} Compact Source Catalogue of the upcoming PLANCK mission. This algorithm {{is based on a}} simple <b>top-hat</b> <b>filter</b> in the harmonic domain with an adaptive filtering range which does not require a priori knowledge of the CMB power spectrum and the experiment parameters such as the beam size and shape nor pixel noise level. Comment: 8 pages, minor changes and typo corrections, accepted for publication in MNRA...|$|E
40|$|International audienceThis paper {{address the}} problem of {{synchronization}} in the context of audio data hiding. For real time transmissions purposes, the data decoding process has to deal with synchronization issues. This paper proposes an new synchronization scheme that optimizes performances of the systems which are based on spread spectrum synchronization by the use of mathematical morphologic tools that present good performance for peak detection. A brief theoretical presentation of the <b>Top-Hat</b> <b>filter</b> is recalled and the enhanced system is derived from the analysis of the advantages and disadvantages. The final scheme provides a robust synchronization system and is compared with the classical solutions...|$|E
40|$|The {{manifestation of}} point {{sources in the}} {{upcoming}} PLANCK maps is a direct reflection of {{the properties of the}} pixelized antenna beam shape for each frequency, which is related to the scan strategy, pointing accuracy, noise properties and map-making algorithm. In this paper we firstly compare analytically the three filters for the PLANCK point source extraction, namely, the adaptive <b>top-hat</b> <b>filter</b> (ATHF), the theoretically-optimal filter (TOF), and the so-called pseudo-filter based on wavelet technique. Our analyses are {{based on the premise that}} the experiment parameters the TOF and the pseudo-filter assume and require are already known: the CMB and noise power spectrum and a circular Gaussian beam shape and size, whereas the ATHF does not need any a priori knowledge. The analyses show that the TOF is optimal of them all in terms of the gain after the parameter inputs. On the other hand, the wavelet technique is only asymptotic to the TOF. We simulate the PLANCK HFI 100 GHz channel with elliptical beam in rotation to test the efficiency of the TOF and the ATHF. The uncertainties on the angular power spectrum will hamper the efficiency of the TOF. To tackle the real situations for the PLANCK point source extraction, most importantly, the elliptical beam shape with slow precession and change of the ellipticity ratio due to possible mirror degradation effect, the adaptive <b>top-hat</b> <b>filter</b> is computationally efficient and well suited to the construction of the PLANCK Early Release Compact Source Catalogue (ERCSC) ...|$|E
40|$|Accepted Received in {{original}} form We {{are engaged in}} a programme of imaging with the STIS and NICMOS (NIC 2) instruments aboard the Hubble Space Telescope (HST), to search for the galaxy counterparts of 18 high-redshift z> 1. 75 damped Lyα absorption lines and 5 Lyman-limit systems seen in the spectra of 16 target quasars. This paper presents the results of the imaging campaign with the NIC 2 camera. We describe the steps followed in reducing the data and combining in mosaics, and the methods used for subtracting the image of the quasar in each field, and for constructing error frames that include the systematic errors associated with the psf subtraction. To identify candidate counterparts, that are either compact or diffuse, we convolved the image and variance frames with circular <b>top-hat</b> <b>filters</b> of diameter 0. 45 and 0. 90 arcsec respectively, to create frames of summed S/N within the aperture. For each target quasar we provide catalogues listing positions and aperture magnitudes of all sources within a square of side 7. 5 ′′ centred on the quasar, detected at S/N> 6. We find a total of 41 candidates o...|$|R
40|$|Higher order cumulants {{of point}} processes, such as skew and kurtosis, require {{significant}} computational effort to calculate. The traditional counts-in-cells method implicitly requires {{a large amount}} of computation since, for each sampling sphere, a count of particles is necessary. Although alternative methods based on tree algorithms can reduce execution time considerably, such methods still suffer from shot noise when measuring moments on low amplitude signals. We present a novel method for calculating higher order moments that is based upon first <b>top-hat</b> <b>filtering</b> the point process data on to a grid. After correcting for the smoothing process, we are able to sample this grid using an interpolation technique to calculate the statistics of interest. The filtering technique also suppresses noise and allows us to calculate skew and kurtosis when the point process is highly homogeneous. The algorithm can be implemented efficiently in a shared memory parallel environment provided a data-local random sampling technique is used. The local sampling technique allows us to obtain close to optimal speed-up for the sampling process on the Alphaserver GS 320 NUMA architecture. Comment: Non-specialist paper, 9 pages, 5 figures, accepted for publication in Int. J. of High Perf. Comp. & Networkin...|$|R
40|$|In {{this paper}} we propose an {{approach}} to detect microcalciﬁcations in digital mammograms using the dual-tree complex wavelet trans-form(DT-CWT). The approach follows four basic strategies, namely, image denoising, band suppression, morphological transformation and inverse complex wavelet transform. Recently, the DT-CWT has shown a good performance in applications that involve image processing due to more data phase information, shift invariance, and directionality than other wavelet transforms. The procedure of image denoising is carried out with a thresholding algorithm that computes recursively the optimal threshold at each level of wavelet decomposition. In order to maximise the detection a morphological conversion is then proposed and applied to the high frequencies subbands of the wavelet transformation. This procedure is applied {{to a set of}} digital mammograms from the mammography image analysis society (MIAS) database. Experimental results show that the proposed denoising algorithm and morphological transformation in combination with the DT-CWT procedure performs better than the stationary and discrete wavelet transforms and the <b>top-hat</b> <b>ﬁltering.</b> The approach reported in this paper seems to be meaningful to aid in the results on mammogram interpretation and to get an earlier and opportune diagnostic for breast cancer...|$|R
40|$|Abstract—This paper {{proposes a}} fast method for {{detecting}} symbolic road markings (SRMs) and stop-lines. The proposed method efficiently restricts the search area {{based on the}} lane detection results and finds SRMs and stop-lines in a cost-effective manner. The SRM detector generates multiple SRM candidates using a <b>top-hat</b> <b>filter</b> and projection histogram and classifies their types using a histogram of oriented gradient (HOG) feature and total error rate (TER) -based classifier. The stop-line detector creates stop-line candidates via random sample consensus (RANSAC) -based parallel line pair estimation and verifies them using the HOG feature and TER-based classifier. The proposed method achieves reasonable detection rates and extremely low false positive rates along with a fast computing time. I...|$|E
40|$|International audienceThe {{aim of this}} {{research}} is proposing a 3 -D similarity enhancement technique useful for improving the segmentation of cardiac structures in Multi-Slice Computerized Tomography (MSCT) volumes. The similarity enhancement is obtained by subtracting the intensity of the current voxel and the gray levels of their adjacent voxels in two volumes resulting after preprocessing. Such volumes are: a. - a volume obtained after applying a Gaussian distribution and a morphological <b>top-hat</b> <b>filter</b> to the input and b. - a smoothed volume generated by processing the input with an average filter. Then, the similarity volume is used as input to a region growing algorithm. This algorithm is applied to extract the shape of cardiac structures, such as left and right ventricles, in MSCT volumes. Qualitative and quantitative results show the good performance of the proposed approach for discrimination of cardiac cavities...|$|E
40|$|We {{compared}} Hall's peak to continuum ratio method with a peak ratio {{method in}} order to quantify light elements (C, N, and O) in organic specimens {{as a model for}} biological thin sections. X-ray spectra were recorded by an energy dispersive X-ray spectrometer equipped with an ultra thin window detector. Spectra were processed by means of a <b>top-hat</b> <b>filter</b> adapted to peak full-width half maximum. The peak intensities were measured by multiple least square fitting to reference spectra. For most elements of biological interest, theoretical and experimental k-factors were determined. Absorption correction was found to be important for quantitation of carbon, nitrogen, and oxygen. Boron was efficiently detected; however, quantitative analysis was not possible. We conclude from our experiments that the peak ratio method is more suitable for quantitation of elemental concentrations in biological thin sections than the peak to continuum method...|$|E
40|$|ABSTRACT [...] The {{evaluation}} of fluorescent {{in situ hybridization}} images (FISH) {{is one of the}} most widely used methods to determine Her- 2 /neu status of breast samples, a valuable prognostic indicator. Conventional evaluation is a difficult task since it involves manual counting of dots in multiple images. In this paper we present a multistage algorithm for the automated classification of FISH images from breast carcinomas. The algorithm focuses not only on the detection of FISH dots but also on overall case classification. The algorithm includes two combined stages for nuclei and dot detection respectively. The dot detection consists of a <b>top-hat</b> <b>filtering</b> stage followed by 3 D template matching to separate real signals from noise. Nuclei segmentation includes a non-linearity correction step, global thresholding and a geometric rule to distinguish between holes within a nucleus and holes between nuclei. Finally, the marked watershed transform is used to segment cell nuclei with markers detected as local h-dome maxima. Combining the two stages allows the measurement of FISH signals ratio per cell nucleus and the collective classification of cases as positive or negative. The system was evaluated with receiver operating characteristic (ROC) analysis and the results were encouraging for the further development of this method. I...|$|R
40|$|Subgrid-scale {{models for}} Large Eddy Simulation (LES) {{in both the}} velocity-pressure and the vorticity-velocity {{formulations}} were evaluated and compared in a priori tests using spectral Direct Numerical Simulation (DNS) databases of isotropic turbulence: 128 (exp 3) DNS of forced turbulence (Re(sub(lambda)) = 95. 8) filtered, using the sharp cutoff filter, to both 32 (exp 3) and 16 (exp 3) synthetic LES fields; 512 (exp 3) DNS of decaying turbulence (Re(sub(Lambda)) = 63. 5) filtered to both 64 (exp 3) and 32 (exp 3) LES fields. Gaussian and <b>top-hat</b> <b>filters</b> were also used with the 128 (exp 3) database. Different LES models were evaluated for each formulation: eddy-viscosity models, hyper eddy-viscosity models, mixed models, and scale-similarity models. Correlations between exact versus modeled subgrid-scale quantities were measured at three levels: tensor (traceless), vector (solenoidal 'force'), and scalar (dissipation) levels, and for both cases of uniform and variable coefficient(s). Different choices for the 1 /T scaling appearing in the eddy-viscosity were also evaluated. It {{was found that the}} models for the vorticity-velocity formulation produce higher correlations with the filtered DNS data than their counterpart in the velocity-pressure formulation. It was also found that the hyper eddy-viscosity model performs better than the eddy viscosity model, in both formulations. info:eu-repo/semantics/publishe...|$|R
40|$|International audienceSmall {{temperature}} anisotropies in {{the cosmic}} microwave background (CMB) can be sourced by density perturbations via the late-time integrated Sachs–Wolfe (ISW) effect. Large voids and superclusters are excellent environments to make a localized measurement of this tiny imprint. In some cases excess signals have been reported. We probed these claims with an independent data set, using the first year data of the Dark Energy Survey (DES) in a different footprint, and using a different superstructure finding strategy. We identified 52 large voids and 102 superclusters at redshifts 0. 2 < z < 0. 65. We used the Jubilee simulation to a priori evaluate the optimal ISW measurement configuration for our compensated <b>top-hat</b> <b>filtering</b> technique, and then performed a stacking measurement of the CMB temperature field based on the DES data. For optimal configurations, we detected a cumulative cold imprint of voids with ΔT_f ≈ − 5. 0 ± 3. 7 [*]μK and a hot imprint of superclusters ΔT_f ≈ 5. 1 ± 3. 2 [*]μK; this is ∼ 1. 2 σ higher than the expected |ΔT_f| ≈ 0. 6 [*]μK imprint of such superstructures in Λ cold dark matter (ΛCDM). If we instead use an a posteriori selected filter size (R/R_v = 0. 6), we can find a temperature decrement as large as ΔT_f ≈ − 9. 8 ± 4. 7 [*]μK for voids, which is ∼ 2 σ above ΛCDM expectations and is comparable to previous measurements made using Sloan Digital Sky Survey superstructure data...|$|R
40|$|We propose new identities for dynamic subgrid {{modeling}} in large-eddy simulation {{involving an}} explicit filter and its inverse. Exact defiltering {{of a class}} of numerical realizations of the <b>top-hat</b> <b>filter</b> is developed. The approach is applied to large-eddy simulation of the temporal mixing layer. Smagorinsky’s model is adopted as base model {{and the results are}} compared to the standard dynamic eddy-viscosity model as well as to filtered DNS (direct numerical simulation) results. The difference between the results of the two models for the present application is found to be quite small. This is explained by performing a sensitivity analysis with respect to the dynamic coefficient, which hints towards a “self-restoring” response underlying the observed robustness of the physical predictions. Using DNS data the validity of the assumption that the model coefficients are independent of filter width is tested and found to favor the inverse modeling procedure. The computational effort of the dynamic inverse model is 15 % smaller than of the standard dynamic eddy-viscosity model...|$|E
40|$|We {{present results}} of the {{investigations}} of the statistical properties of a joint density and velocity divergence probability distribution function (PDF) in the mildly nonlinear regime. For that purpose we use both perturbation theory results (complete as possible up to third order terms), obtained analytically for a Gaussian filter and extended for a <b>top-hat</b> <b>filter,</b> and numerical simulations. In particular we derive the quantitative and qualitative predictions for constrained averages and constrained dispersions – which describe the nonlinearities and the stochasticity properties beyond the linear regime – and compare them against numerical simulations. We find overall a good agreement with scaling relations for the Ω-dependence of these quantities satisfactory reproduced. Finally, guided by our analytical and numerical results, we construct a robust phenomenological description of the joint PDF in a closed analytic form. The good agreement of our formula with results of N-body simulations {{for a number of}} cosmological parameters provides a sound validation of the presented approach. Our results provide a basis for a potentially powerful tool with which it is possible to analyze galaxy survey data in order to test the gravitational instability paradigm beyond the linear regime and put useful constraints on cosmological parameters...|$|E
40|$|Although the {{hydrophobicity}} {{is usually}} an arduous parameter {{to be determined}} in the field, it has been pointed out as a good option to monitor aging of polymeric outdoor insulators. Concerning this purpose, digital image processing of photos taken from wet insulators has been the main technique nowadays. However, important challenges on this technique still remain to be overcome, such as; images from non-controlled illumination conditions can interfere on analyses and no existence of standard surfaces with different levels of hydrophobicity. In this paper, the photo image samples were digitally filtered to reduce the illumination influence, and hydrophobic surface samples were prepared from wetting silicon surfaces with solution of water-alcohol. Furthermore norevious studies triying to quantify and relate these properties in a mathematical function were found, {{that could be used}} in the field by the electrical companies. Based on such considerations, high quality images of countless hydrophobic surfaces were obtained and three different image processing methodologies, the fractal dimension and two Haralick textures descriptors, entropy and homogeneity, associated with several digital filters, were compared. The entropy parameter Haralick's descriptors filtered with the White <b>Top-Hat</b> <b>filter</b> presented the best result to classify the hydrophobicity...|$|E
40|$|Context. Future {{large scale}} cosmological surveys will provide huge data sets whose {{analysis}} requires efficient data compression. In particular, {{the calculation of}} accurate covariances is extremely challenging with increasing number of statistics used. Aims. The aim of the present work is to introduce a formalism for achieving efficient data compression, based on a local expansion of statistical measures around a fiducial cosmological model. We specifically apply and test this approach for the case of cosmic shear statistics. In addition, we study how well band powers {{can be obtained from}} measuring shear correlation functions over a finite interval of separations. Methods. We demonstrate the performance of our approach, using a Fisher analysis on cosmic shear tomography described in terms of E-/B-mode separating statistics (COSEBIs). Results. We show that our data compression is highly effective in extracting essentially the full cosmological information from a strongly reduced number of observables. Specifically, the number of statistics needed decreases by at least one order of magnitude relative to the COSEBIs, which already compress the data substantially compared to the shear two-point correlation functions. The efficiency appears to be affected only slightly if a highly inaccurate covariance is used for defining the compressed statistics, showing the robustness of the method. Furthermore, we show the strong limitations on the possibility to construct <b>top-hat</b> <b>filters</b> in Fourier space, for which the real-space analog has a finite support, yielding strong bounds on the accuracy of band power estimates...|$|R
40|$|We {{experimentally}} {{demonstrate a}} simple method {{to measure the}} biphoton joint spectrum by mapping the spectral information onto the temporal domain using a dispersive medium. Various <b>top-hat</b> spectral <b>filters</b> are used to limit the spectral (and hence, temporal) extent of the broadband downconversion photons measured. The sharp edges of the spectral filters are utilized as spectral markers for dispersion characterization of the dispersive medium. This method allows dispersion characterization and joint spectral measurement to be completed simultaneously. The joint spectrum (which extends beyond 100 nm, centered about 1. 5 micron) of the type-II downconverted photon pairs generated from a poled optical fiber is obtained with this method. Comment: 6 pages, 4 figure...|$|R
40|$|Future {{large scale}} cosmological surveys will provide huge data sets whose {{analysis}} requires efficient data compression. Calculating accurate covariances is extremely challenging with {{increasing number of}} statistics used. Here we introduce a formalism for achieving efficient data compression, based on a local expansion of statistical measures around a fiducial cosmological model. We specifically apply and test this approach for the case of cosmic shear statistics. We demonstrate the performance of our approach, using a Fisher analysis on cosmic shear tomography {{described in terms of}} E-/B-mode separating statistics (COSEBIs). We show that our data compression is highly effective in extracting essentially the full cosmological information from a strongly reduced number of observables. Specifically, the number of statistics needed decreases by at least one order of magnitude relative to the COSEBIs, which already compress the data substantially compared to the shear two-point correlation functions. The efficiency appears to be affected only slightly if a highly inaccurate covariance is used for defining the compressed statistics, showing the robustness of the method. We conclude that an efficient data compression is achievable and that the number of compressed statistics depends on the number of model parameters. In addition, we study how well band powers can be obtained from measuring shear correlation functions over a finite interval of separations. We show the strong limitations on the possibility to construct <b>top-hat</b> <b>filters</b> in Fourier space, for which the real-space analog has a finite support, yielding strong bounds on the accuracy of band power estimates. The error on an estimated band-power is larger for a narrower filter and a smaller angular range which for relevant cases can be as large as 10 %. Comment: 10 pages, 6 figure...|$|R
