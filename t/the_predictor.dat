4052|10000|Public
25|$|Principal {{component}} regression (PCR) is {{used when}} the number of predictor variables is large, or when strong correlations exist among <b>the</b> <b>predictor</b> variables. This two-stage procedure first reduces <b>the</b> <b>predictor</b> variables using principal component analysis then uses the reduced variables in an OLS regression fit. While it often works well in practice, there is no general theoretical reason that the most informative linear function of <b>the</b> <b>predictor</b> variables should lie among the dominant principal components of the multivariate distribution of <b>the</b> <b>predictor</b> variables. The partial least squares regression is the extension of the PCR method which does not suffer from the mentioned deficiency.|$|E
25|$|Under the {{condition}} that the errors are uncorrelated with <b>the</b> <b>predictor</b> variables, LLSQ yields unbiased estimates, but even under that condition NLLSQ estimates are generally biased.|$|E
25|$|With the Bedford Attachment, the Army {{now had a}} {{complete}} gun laying system for the first time. As all three axes could be read continually, the predictors could be fed information directly from the radar with no optical inputs needed. Likewise, the guns themselves were either automatically driven from <b>the</b> <b>predictor,</b> or only required the layers to follow mechanical pointers to match <b>the</b> <b>predictor</b> output, a concept known as laying needle on needle. Even the fuse settings were automatically set from the range values coming from the radar. The entire gunnery problem was now highly automated end-to-end.|$|E
30|$|This {{is another}} binary {{classification}} algorithm which separates the classes {{based on the}} linear combination of <b>the</b> <b>predictors</b> [47]. LDA model assumes that each of <b>the</b> <b>predictors</b> has <b>the</b> same variance. It calculates the mean and variance of <b>the</b> <b>predictors</b> for each class based on this assumption.|$|R
40|$|We {{provide a}} remedy for two {{concerns}} that have dogged the use of prin-cipal components in regression: (i) principal components are computed from <b>the</b> <b>predictors</b> alone and do not make apparent use of the response, and (ii) principal components are not invariant or equivariant under full rank linear transformation of <b>the</b> <b>predictors.</b> <b>The</b> development begins with principal fitted components (Cook, 2007) and uses normal models for the inverse regression of <b>the</b> <b>predictors</b> on <b>the</b> response to gain reductive information for the forward re-gression of interest. This approach includes methodology for testing hypotheses {{about the number of}} components and about conditional independencies among <b>the</b> <b>predictors...</b>|$|R
40|$|Abstract. We {{provide a}} remedy for two {{concerns}} that have dogged the use of principal components in regression: (i) principal components are computed from <b>the</b> <b>predictors</b> alone and do not make apparent use of the response, and (ii) principal components are not invariant or equivariant under full rank linear transformation of <b>the</b> <b>predictors.</b> <b>The</b> development begins with principal fitted components [Cook, R. D. (2007). Fisher lecture: Dimension reduction in regression (with discussion). Statist. Sci. 22 1 – 26] and uses normal models for the inverse regression of <b>the</b> <b>predictors</b> on <b>the</b> response to gain reductive information for the forward regression of interest. This approach includes methodology for testing hypotheses {{about the number of}} components and about conditional independencies among <b>the</b> <b>predictors.</b> Key words and phrases: Central subspace, dimension reduction, inverse regression, principal components. 1...|$|R
25|$|The arrangement, or {{probability}} distribution of <b>the</b> <b>predictor</b> variables x {{has a major}} influence on the precision of estimates of β. Sampling and design of experiments are highly developed subfields of statistics that provide guidance for collecting data in such a way to achieve a precise estimate of β.|$|E
25|$|This was the {{economic}} and social situation in which <b>the</b> <b>predictor</b> of the coming disaster, a typhoid (Infected Water) epidemic, emerged. Many thousands died in populated urban centres, most significantly Ypres. In 1318 a pestilence of unknown origin, sometimes identified as anthrax, targeted the animals of Europe, notably sheep and cattle, further reducing the food supply and income of the peasantry.|$|E
25|$|To make a range measurement, the {{operator}} {{would turn the}} potentiometer dial {{in an effort to}} get the leading edge of the target blip to line up with a vertical line on the CRT. The range was not read off the CRT, but the dial. The dial also turned a magslip, or selsyn as it is more commonly known today. The output of the magslip was used to directly turn the controls on <b>the</b> <b>predictor,</b> allowing the radar to continually update the range measurement.|$|E
40|$|AbstractThe central {{mean and}} central subspaces of {{generalized}} multiple index model {{are the main}} inference targets of sufficient dimension reduction in regression. In this article, we propose an integral transform (ITM) method for estimating these two subspaces. Applying the ITM method, estimates are derived, separately, for two scenarios: (i) No distributional assumptions are imposed on <b>the</b> <b>predictors,</b> and (ii) <b>the</b> <b>predictors</b> are assumed to follow an elliptically contoured distribution. These estimates are shown to be asymptotically normal with the usual root-n convergence rate. The ITM method is different from other existing methods in that it avoids estimation of the unknown link function between the response and <b>the</b> <b>predictors</b> {{and it does not}} rely on distributional assumptions of <b>the</b> <b>predictors</b> under scenario (i) mentioned above...|$|R
40|$|Objective: A key step in informing {{mental health}} {{resource}} allocation {{is to identify}} <b>the</b> <b>predictors</b> of service utilisation and costs. This project aims to identify <b>the</b> <b>predictors</b> of mental health-related acute service utilisation and treatment costs in the year following an acute public psychiatric hospital admission...|$|R
40|$|We {{show that}} under a {{linearity}} condition {{on the distribution}} of <b>the</b> <b>predictors,</b> <b>the</b> coefficient in single-index regression can be estimated with the same efficiency as in the case when the link function is known. Thus, the linearity condition seems to substitute for knowing the exact conditional distribution of the response given the linear combinations of <b>the</b> <b>predictors.</b> Comment: 10 page...|$|R
500|$|Reiss {{concluded}} that Sexual Preference [...] "has value for suggesting directions and the likely worth of ideas", but that given its shortcomings {{there was no}} way in which its authors could definitively resolve the issues they explored, despite their claim to [...] "once and for all" [...] discredit some theoretical ideas about homosexuality. Reiss wrote that Bell et al. asked questions that were [...] "vague" [...] and [...] "open-ended", had an [...] "arbitrary and rigid conception" [...] of what could be done with their data and lacked [...] "theoretical development" [...] in its handling, and deliberately minimized the importance of <b>the</b> <b>predictor</b> variables they used to test psychoanalytic and other theories. He found their conclusion that sexual orientation has a biological basis unconvincing. DeLamater wrote that Sexual Preference benefited from Bell et al.′s [...] "eclectic theoretical basis", which drew from the psychodynamic model, social learning theory, sociological models that emphasize the importance of peer relationships, and labeling theory. However, while he accepted Bell et al.′s claim that their study was methodologically superior to prior work on homosexuals, he still found it problematic for many reasons and hesitated to endorse its conclusions. In his view, the path analysis involved [...] "arbitrary classification and sequencing of variables".|$|E
2500|$|Weak exogeneity. [...] This {{essentially}} {{means that}} <b>the</b> <b>predictor</b> variables x {{can be treated}} as fixed values, rather than random variables. [...] This means, for example, that <b>the</b> <b>predictor</b> variables {{are assumed to be}} error-free—that is, not contaminated with measurement errors. Although this assumption is not realistic in many settings, dropping it leads to significantly more difficult errors-in-variables models.|$|E
2500|$|Using only <b>the</b> <b>predictor</b> {{formula and}} the {{corrector}} for the velocities one obtains a direct or explicit method {{which is a}} variant of the Verlet integration method: ...|$|E
30|$|In Factor 1 (Dangerous Violations), we {{can account}} for 25.5  % of the {{variance}} using <b>the</b> <b>predictors</b> under scrutiny. <b>The</b> <b>predictors</b> suggest that <b>the</b> rules are intentionally violated especially by young men who travel many kilometres per year, are experienced drivers, have not completed university-level education, live in big cities, and are entrepreneurs.|$|R
40|$|The central {{mean and}} central subspaces of {{generalized}} multiple index model {{are the main}} inference targets of sufficient dimension reduction in regression. In this article, we propose an integral transform (ITM) method for estimating these two subspaces. Applying the ITM method, estimates are derived, separately, for two scenarios: (i) No distributional assumptions are imposed on <b>the</b> <b>predictors,</b> and (ii) <b>the</b> <b>predictors</b> are assumed to follow an elliptically contoured distribution. These estimates are shown to be asymptotically normal with the usual root-n convergence rate. The ITM method is different from other existing methods in that it avoids estimation of the unknown link function between the response and <b>the</b> <b>predictors</b> {{and it does not}} rely on distributional assumptions of <b>the</b> <b>predictors</b> under scenario (i) mentioned above. Average derivative estimate Generalized multiple index model Integral transform Kernel density estimation Sufficient dimension reduction...|$|R
30|$|Example  3.1 {{indicates}} that <b>the</b> unbiased <b>predictors</b> of δ are also <b>the</b> unbiased <b>predictors</b> of y_ 0 and Ey_ 0, and <b>the</b> unbiased <b>predictors</b> of y_ 0 and Ey_ 0 are also <b>the</b> unbiased <b>predictors</b> of δ since Eδ=Ey_ 0 =E(Ey_ 0)=X_ 0 β. However, admissibility of those predictors for each studied variables are different.|$|R
2500|$|The {{meaning of}} the {{expression}} [...] "held fixed" [...] may depend on how the values of <b>the</b> <b>predictor</b> variables arise. If the experimenter directly sets the values of <b>the</b> <b>predictor</b> variables {{according to a study}} design, the comparisons of interest may literally correspond to comparisons among units whose predictor variables have been [...] "held fixed" [...] by the experimenter. Alternatively, the expression [...] "held fixed" [...] can refer to a selection that takes place in the context of data analysis. In this case, we [...] "hold a variable fixed" [...] by restricting our attention to the subsets of the data that happen to have a common value for the given predictor variable. This is the only interpretation of [...] "held fixed" [...] {{that can be used in}} an observational study.|$|E
2500|$|Errors-in-variables models (or [...] "measurement error models") {{extend the}} {{traditional}} linear regression model to allow <b>the</b> <b>predictor</b> variables X {{to be observed}} with error. This error causes standard estimators of β to become biased. Generally, the form of bias is an attenuation, meaning that the effects are biased toward zero.|$|E
2500|$|Linearity. [...] This {{means that}} {{the mean of the}} {{response}} variable is a linear combination of the parameters (regression coefficients) and <b>the</b> <b>predictor</b> variables. [...] Note that this assumption is much less restrictive than it may at first seem. [...] Because <b>the</b> <b>predictor</b> variables are treated as fixed values (see above), linearity is really only a restriction on the parameters. [...] <b>The</b> <b>predictor</b> variables themselves can be arbitrarily transformed, and in fact multiple copies of the same underlying predictor variable can be added, each one transformed differently. [...] This trick is used, for example, in polynomial regression, which uses linear regression to fit the response variable as an arbitrary polynomial function (up to a given rank) of a predictor variable. This makes linear regression an extremely powerful inference method. [...] In fact, models such as polynomial regression are often [...] "too powerful", in that they tend to overfit the data. [...] As a result, some kind of regularization must typically be used to prevent unreasonable solutions coming out of the estimation process. [...] Common examples are ridge regression and lasso regression. [...] Bayesian linear regression can also be used, which by its nature is more or less immune to the problem of overfitting. (In fact, ridge regression and lasso regression can both be viewed as [...] special cases of Bayesian linear regression, with particular types of prior distributions placed on the regression coefficients.) ...|$|E
40|$|AbstractWe {{consider}} informative dimension reduction for regression {{problems with}} random <b>predictors.</b> Based on <b>the</b> conditional specification of the model, we develop a methodology for replacing <b>the</b> <b>predictors</b> {{with a smaller}} number of functions of <b>the</b> <b>predictors.</b> We apply <b>the</b> method to the case where the inverse conditional model is in the linear exponential family. For such an inverse model and the usual Normal forward regression model it is shown that, for any number of <b>predictors,</b> <b>the</b> sufficient summary has dimension two or less. In addition, we develop a test of dimensionality. The relationship of our method with the existing dimension reduction theory based on the marginal distribution of <b>the</b> <b>predictors</b> is discussed...|$|R
40|$|In {{this thesis}} {{heterogeneity}} in inflation forecasts is analysed. First, the Survey of Professional Forecasters and the Michigan survey are analysed with {{focus on the}} mean, median and individual forecasters. The forecasters are analysed with respect to forecast bias and forecast accuracy. The performance of the individual forecasters are analysed and compared {{to the performance of}} the mean and median forecasters. The mean and median forecasters are found to be less biased and more accurate than the individual forecasters. Second, a simple dynamic stochastic general equilibrium model describing an economy with heterogeneous consumers is analysed. The consumers in the model use two predictors and switch between <b>the</b> <b>predictors</b> based on their past performance. <b>The</b> <b>predictors</b> in <b>the</b> model are also analysed with focus on forecast bias and forecast accuracy. <b>The</b> mean <b>predictor</b> is found to be both less biased and more accurate than <b>the</b> two <b>predictors</b> used by <b>the</b> consumers in the model. However, the results are more mixed for <b>the</b> median <b>predictor</b> which is found to be less biased, but less accurate than <b>the</b> <b>predictors</b> used by <b>the</b> consumers...|$|R
30|$|<b>The</b> <b>predictors</b> {{of malaria}} formed a logical {{clinical}} {{combination of the}} above attributes.|$|R
2500|$|The restyled Packard {{line for}} 1955 showed Teague's keen eye for detail {{and his ability}} to produce {{significant}} changes based on limited budgets. However, the company was not doing well following the purchase of struggling Studebaker Corporation in 1954. The last Teague design for Packard was the Executive, introduced mid-1956 and derived from the Clipper Custom, launched just as sales of the luxury Packard line collapsed. Teague also designed the last Packard show car, <b>the</b> <b>Predictor,</b> plus a new Packard and Clipper lineup for 1957 that would have followed the general lines of <b>the</b> <b>Predictor.</b> The design was stillborn when the Detroit Packard operations were shut down completely in mid-1956. Lacking funds for all-new models, the Studebaker-Packard Corporation had to make use of existing and economical Studebaker designs. Working with little time and money, the stopgap 1957 Studebaker-based Packard models became known as [...] "Packardbakers." [...] The 1957 Packard Clipper, popularly derided as [...] "a Studebaker wearing Packard makeup", was designed largely by Teague, and was intended as a temporary stopgap to keep the brand going until the company's fortunes improved and a [...] "real" [...] Packard model could again be made.|$|E
2500|$|Standard linear {{regression}} models with standard estimation techniques make {{a number of}} assumptions about <b>the</b> <b>predictor</b> variables, the response variables and their relationship. [...] Numerous extensions have been developed that allow each of these assumptions to be relaxed (i.e. reduced to a weaker form), {{and in some cases}} eliminated entirely. [...] Some methods are general enough that they can relax multiple assumptions at once, and in other cases this can ns. [...] Generally these extensions make the estimation procedure more complex and time-consuming, and may also require more data in order to produce an equally precise model.|$|E
2500|$|The {{notion of}} a [...] "unique effect" [...] is {{appealing}} when studying a complex system where multiple interrelated components influence the response variable. In some cases, it can literally be interpreted as the causal effect of an intervention that {{is linked to the}} value of a predictor variable. However, {{it has been argued that}} in many cases multiple regression analysis fails to clarify the relationships between <b>the</b> <b>predictor</b> variables and the response variable when the predictors are correlated with each other and are not assigned following a study design. A commonality analysis may be helpful in disentangling the shared and unique impacts of correlated independent variables.|$|E
40|$|Dimension {{reduction}} {{is central to}} an analysis of data with many predictors. Sufficient dimension reduction aims to identify the smallest possible number of linear combinations of <b>the</b> <b>predictors,</b> called <b>the</b> sufficient <b>predictors,</b> that retain all of the information in <b>the</b> <b>predictors</b> about <b>the</b> response distribution. In this paper we propose a Bayesian solution for sufficient dimension reduction. We directly model the response density in terms of <b>the</b> sufficient <b>predictors</b> using a finite mixture model. This approach is computationally efficient and offers a unified framework to handle categorical predictors, missing predictors, and Bayesian variable selection. We illustrate the method using both a simulation study and an analysis of an HIV data set...|$|R
40|$|We {{consider}} informative dimension reduction for regression {{problems with}} random <b>predictors.</b> Based on <b>the</b> conditional specification of the model, we develop a methodology for replacing <b>the</b> <b>predictors</b> {{with a smaller}} number of functions of <b>the</b> <b>predictors.</b> We apply <b>the</b> method to the case where the inverse conditional model is in the linear exponential family. For such an inverse model and the usual Normal forward regression model it is shown that, for any number of <b>predictors,</b> <b>the</b> sufficient summary has dimension two or less. In addition, we develop a test of dimensionality. The relationship of our method with the existing dimension reduction theory based on the marginal distribution of <b>the</b> <b>predictors</b> is discussed. Conditional density ratios Dimension reduction Regression graphics Sufficient summary...|$|R
30|$|We report <b>the</b> <b>predictors</b> of mid-term {{survival}} {{in patients who}} survive the ICU phase.|$|R
2500|$|British {{night air}} defences {{were in a}} poor state. Few {{anti-aircraft}} guns had fire-control systems, and the underpowered searchlights were usually ineffective against aircraft at altitudes above [...] In July 1940, only 1,200 heavy and 549 light guns were deployed {{in the whole of}} Britain. Of the [...] "heavies", some 200 were of the obsolescent [...] type; the remainder were the effective [...] and [...] guns, with a theoretical [...] "ceiling"' of over [...] but a practical limit of [...] because <b>the</b> <b>predictor</b> in use could not accept greater heights. The light guns, about half of which were of the excellent Bofors 40 mm, dealt with aircraft only up to [...] Although the use of the guns improved civilian morale, with the knowledge the German bomber crews were facing the barrage, it is now believed that the anti-aircraft guns achieved little and in fact the falling shell fragments caused more British casualties on the ground.|$|E
2500|$|Lack {{of perfect}} {{multicollinearity}} in the predictors. [...] For standard least squares estimation methods, the design matrix X must have full column rank p; otherwise, {{we have a}} condition known as perfect multicollinearity in <b>the</b> <b>predictor</b> variables. [...] This can be triggered by having two or more perfectly correlated predictor variables (e.g. if the same predictor variable is mistakenly given twice, either without transforming one of the copies or by transforming one of the copies linearly). It can also happen if there is too little data available compared {{to the number of}} parameters to be estimated (e.g. fewer data points than regression coefficients). In the case of perfect multicollinearity, the parameter vector β will be non-identifiable—it has no unique solution. [...] At most {{we will be able to}} identify some of the parameters, i.e. narrow down its value to some linear subspace of Rp. See partial least squares regression. [...] Methods for fitting linear models with multicollinearity have been developed; some require additional assumptions such as [...] "effect sparsity"—that a large fraction of the effects are exactly zero. Note that the more computationally expensive iterated algorithms for parameter estimation, such as those used in generalized linear models, do not suffer from this problem.|$|E
2500|$|Jürgen Schmidhuber's formal {{theory of}} {{creativity}} postulates that creativity, curiosity, and interestingness are by-products {{of a simple}} computational principle for measuring and optimizing learning progress. Consider an agent able to manipulate its environment and thus its own sensory inputs. The agent can use a black box optimization method such as reinforcement learning to learn (through informed trial and error) sequences of actions that maximize the expected sum of its future reward signals. There are extrinsic reward signals for achieving externally given goals, such as finding food when hungry. But Schmidhuber's objective function to be maximized also includes an additional, intrinsic term to model [...] "wow-effects." [...] This non-standard term motivates purely creative behavior of the agent even {{when there are no}} external goals. A wow-effect is formally defined as follows. As the agent is creating and predicting and encoding the continually growing history of actions and sensory inputs, it keeps improving <b>the</b> <b>predictor</b> or encoder, which can be implemented as an artificial neural network or some other machine learning device that can exploit regularities in the data to improve its performance over time. The improvements can be measured precisely, by computing the difference in computational costs (storage size, number of required synapses, errors, time) needed to encode new observations before and after learning. This difference depends on the encoder's present subjective knowledge, which changes over time, but the theory formally takes this into account. The cost difference measures the strength of the present [...] "wow-effect" [...] due to sudden improvements in data compression or computational speed. It becomes an intrinsic reward signal for the action selector. The objective function thus motivates the action optimizer to create action sequences causing more wow-effects. Irregular, random data (or noise) do not permit any wow-effects or learning progress, and thus are [...] "boring" [...] by nature (providing no reward). Already known and predictable regularities also are boring. Temporarily interesting are only the initially unknown, novel, regular patterns in both actions and observations. This motivates the agent to perform continual, open-ended, active, creative exploration.|$|E
30|$|Among <b>the</b> pre-selected <b>predictors,</b> {{a further}} {{analysis}} was performed to identify those with statistically significant (or with a tendency to significance) capability to stratify between positive (malignant) and negative (benign) cases. The dichotomization was performed {{by means of an}} iterative process aiming to identify <b>the</b> <b>predictor’s</b> threshold minimizing <b>the</b> p-value of the Fisher’s test. <b>The</b> final <b>predictors</b> were selected if p <  0.08.|$|R
30|$|Equations 4 and 5 {{constitute}} {{a theory of}} how the response is related to <b>the</b> <b>predictors</b> in Pr(Y, X). But any relationships between the response and <b>the</b> <b>predictors</b> are “merely” associations. There is no causal overlay. Equation 4 is not a causal model. Nor is it a representation of how the data were generated — {{we already have a}} model for that.|$|R
40|$|Background: Previous {{studies have}} shown that qi-gong, a form of mind-body {{movement}} therapy, may be beneficial for people with type 2 diabetes; however, no controlled studies have been conducted to examine <b>the</b> <b>predictors</b> and mediators of qi-gong effects on indicators of diabetes control. This study examined the effects of qi-gong on diabetes control and identified <b>the</b> <b>predictors</b> and mediators of these effects...|$|R
