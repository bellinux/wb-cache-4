2|36|Public
50|$|After {{a two-week}} break Novak entered the Wimbledon Championships, {{where he played}} much better than in the {{previous}} tournaments on clay. He reached the final without dropping a set in the first five matches and won a <b>tight</b> <b>meeting</b> with del Potro, the meeting lasting almost five hours and being the longest semifinal in Wimbledon history. However, two days later Djokovic was not able to threaten Murray despite leading {{in the second and third}} set and having saved three championship points. He lost in straight sets, for the first time since 2010.|$|E
40|$|Abstract—In double {{patterning}} lithography (DPL), overlay errors between two patterning {{steps of the}} same layer translate into CD variability. Since CD uniformity budget is very <b>tight,</b> <b>meeting</b> requirement of overlay control {{is one of the}} biggest challenges for deploying DPL. In this paper, we electrically evaluate overlay errors for back-end-of-line DPL with the goal of studying relative effects of different overlay sources and interactions of overlay control with design parameters. Exper-imental results show the following: 1) the expected electrical impact of overlay in a path is not significant (< 6 % worst-case RC variation) and should be the basis for determining overlay budget requirement; 2) the worst-case electrical impact of overlay in a single line remains a serious concern (up to 16. 6 % RC and up to 50 mV increase of peak crosstalk noise); 3) translational overlay error has the largest electrical impact compared to other overlay sources; and 4) overlay in y direction (x for horizontal metallization) has negligible electrical impact and, therefore, preferred routing direction should be taken into account for overlay sampling and alignment strategies. Design methods for reducing overlay electrical impact in wires are then identified. Finally, we explore positive/negative process options from an electrical perspective and conclude that positive process is preferred. Index Terms—Alignment strategy, congestion, design for man-ufacturability, {{double patterning}}, layout decomposition, neg-ative process, overlay, positive process, wire spreading, wire widening. I...|$|E
40|$|Prior {{research}} suggests that goal setting and an emphasis en <b>meeting</b> <b>tight</b> budget targets may influence the extent of subordinates 2 ̆ 7 performance and slack creation. This study hypothesizes that other accounting controls may moderate these relationships. Specifically, it hypothesizes that: (i) budgetary performance is increased and (ii) budgetary slack creation is decreased when an emphasis on setting and <b>meeting</b> <b>tight</b> budget targets is complemented with a high extent of cost control. The results support a significant two-way interaction between Emphasis on setting and <b>meeting</b> <b>tight</b> budget targets and Cost control affecting budgetary performance. A significant two-way interaction between Emphasis en setting and <b>meeting</b> <b>tight</b> budget targets and Cost control affecting the propensity to create slack was also found for production managers. Marketing managers 2 ̆ 7 propensity to create slack {{was found to be}} associated only with Emphasis en setting and <b>meeting</b> <b>tight</b> budget targets...|$|R
50|$|On April 20, 2012, it was {{reported}} that Paul had been attending the team's <b>tight</b> end <b>meetings</b> and would be switching from wide receiver to tight end. During the offseason some of the Redskins' staff, including coach Mike Shanahan and teammate Darrel Young, compared him to former tight end, Shannon Sharpe. In the Week 12 win against the Dallas Cowboys on Thanksgiving, Paul scored his first career touchdown.|$|R
5000|$|Crowdsourcing alone may {{not give}} the best {{feedback}} on applications. A diverse testing approach that pools both crowdsource testing and a dedicated testing team may be favorable. [...] "Having this diversity of staffing allows you to scale your resources {{up and down in}} a fluid manner, <b>meeting</b> <b>tight</b> deadlines during peak periods of development and testing, while controlling costs during slow periods." ...|$|R
5000|$|The Utility Clothing Scheme was a {{rationing}} scheme {{introduced in}} the United Kingdom by the British government during World War II. In response to the shortage of clothing materials and labour due {{to the requirements of}} the war effort, the Board of Trade sponsored the creation of ranges of [...] "utility clothing" [...] <b>meeting</b> <b>tight</b> specifications regarding the amount of material and labour allowed to be used in their construction. Utility clothing, and later utility furniture, was marked with the CC41 mark.|$|R
40|$|Recently we {{have shown}} how hot-spots during test can be avoided without {{unnecessarily}} increas-ing the testing time by using a thermal-safe test scheduling approach [15]. In this work, we inves-tigate the impact of scan shift frequency scaling on the thermal-safe test scheduling performance and propose an algorithm which embeds shift frequency scaling into the test scheduling process. Experimental results show that this approach offers shorter overall testing times and significantly improved ability of <b>meeting</b> <b>tight</b> thermal constraints when compared to existing thermal-safe test scheduling approach based on a fixed scan shift frequency. ...|$|R
30|$|F{{or large}} organisations (companies, governments or large {{research}} projects) access to appropriate levels of computational resources is easily within their reach. However, for smaller organisations this {{can often be}} beyond their means – especially if the organisation is not expecting to make significant use of the resources. Traditionally these organisations have relied on access to shared resources managed by others or making do with the resources available – which may preclude them from <b>meeting</b> <b>tight</b> deadlines or require them to make compromises {{in order to achieve}} these deadlines. These compromises may be through reduced complexity models (simpler or less realistic) or the processing of smaller data sets than desired.|$|R
50|$|Not {{only did}} the design team have to {{overcome}} the climatic conditions of Australia, but the building also had to be simultaneously energy and environmentally efficient, respectful of the site, and inspirational for its users, whilst <b>meeting</b> <b>tight</b> budget constraints. Thus, a solution was achieved by utilising a heat stop cellular polycarbonate sheeting to the roof and walls where {{only a small portion}} of (solar) heat energy was transferred. Detailed modelling proved that Danpalon™, a translucent, insulating UV-resistant material, could be used over a steel frame to incorporate central ventilation for fresh air cooling. Energy usage is thus low and mechanical ventilation is not required.|$|R
40|$|Ceramic sponge {{precision}} machining is a challenging issue for conventional technologies. Micro Abrasive Waterjet (µAWJ) cutting technology {{proves to be}} effective in ceramic sponge fine features machining. A specific sponge filling procedure was developed to preserve the jet shape and to prevent material collapse. The typical AWJ defects were reduced by selecting the proper feedrate for the Ø 0. 3 mm jet configuration developed at Waterjet Laboratory of Politecnico di Milano. After cutting and filling agent removal, complex near-net-shape parts were obtained, <b>meeting</b> <b>tight</b> tolerances and exploiting the µAWJ technology valuable flexibility. Some complex shaped case study parts with convex and concave corners as well as holes and thin walls are presented...|$|R
40|$|Abstract—Cloud {{computing}} {{has drawn}} increasing {{attention from the}} scientific computing community due to its ease of use, elasticity, and relatively low cost. Because a high-performance computing (HPC) application is usually resource demanding, without careful planning, it can incur a high monetary expense even in Cloud. We design a tool called CAP 3 (Cloud Auto-Provisioning framework for Parallel Processing) to help a user minimize the expense of running an HPC application in Cloud, while meeting the user-specified job deadline. Given an HPC application, CAP 3 automatically profiles the application, builds a model to predict its performance, and infers a proper cluster size that can finish the job within its deadline while minimizing the total cost. To further reduce the cost, CAP 3 intelligently chooses the Cloud’s reliable on-demand instances or low-cost spot instances, {{depending on whether the}} remaining time is <b>tight</b> in <b>meeting</b> the application’s deadline. Experiments on Amazon EC 2 show that the execution strategy given by CAP 3 is costeffective, by choosing a proper cluster size and a proper instance type (on-demand or spot). Index Terms—Cloud computing; provisioning; virtual cluster; parallel scientific application; spot instance I...|$|R
50|$|Adam West and Burt Ward go {{in search}} of the Batmobile after it is stolen, and along the way find {{themselves}} forced to consult their memories to decipher clues left by the criminals, evidently people they had known from filming the Batman television series. Memories include affairs, one of which led to Burt being chased by a knife-bearing lover, medicine Burt was forced to take so his genitals would not be so visible in his <b>tights,</b> and Adam <b>meeting</b> the actress who played Batgirl (Yvonne Craig) and accidentally placing a hand on her breast during filming. As it turns out, the villains who stole the Batmobile were the actors who once played the Riddler (Frank Gorshin) and Catwoman (Julie Newmar).|$|R
40|$|The {{development}} of reliable software is a challenging task, {{especially in a}} business environment that forces developers to focus on <b>meeting</b> <b>tight</b> deadlines instead of producing quality software. Researchers and practitioners are exploring various approaches for addressing this problem, such as autonomic computing and conscientious autopoietic software. These approaches describe software systems {{that are capable of}} managing and preserving themselves. In this paper, we propose a new, concrete self-managing software architecture based on the biological concept of commensalistic symbiosis and the notion of autopoietic software. We present a detailed description of our architecture, and a working prototype of a minimal commensalistic system. In addition, we specify a new programming language, examine usage scenarios and discuss implementation issues for realizing a working commensalistic system on a larger scale...|$|R
40|$|The roadmap for {{high-performance}} computing {{it is currently}} switching to multi-core architectures. Industry has shifted to the multi-core paradigm as single-core processors are reaching the power consumption wall. The solution is to put multiple and simple processors in the same chip fabric becoming chip multiprocessors (CMPs). An efficient interconnect layer for CMP archi-tectures is needed to connect all the cores. Networks-on-chip (NoCs) are the key components of these architectures, {{and they have to}} deal with the com-munication scalability challenge while <b>meeting</b> <b>tight</b> power, area and latency design constraints. 2 D mesh topologies are usually preferred by designers of general purpose NoCs. However, manufacturing faults may break their regularity. Moreover, resource management frameworks may require the segmentation of the net-work into irregular regions under virtualization or power-awareness scenarios. Under these conditions, efficient routing becomes a challenge. Although o...|$|R
40|$|AIDA [...] a novel {{elaboration}} on Michael O. Rabin's IDA [21] [...] is {{a communication}} protocol that uses redundancy to achieve both timeliness and reliability. In AIDA redundancy {{is used to}} tackle several crucial problems. In particular, redundancy is used to tolerate failures, to {{increase the likelihood of}} <b>meeting</b> <b>tight</b> time-constraints, and to ration (based on task priorities) the limited bandwidth in the system. AIDA is a probabilistic protocol {{in the sense that it}} does not guarantee the fulfillment of hard time constraints. Instead, it guarantees a lower bound on the probability of fulfilling such constraints. Such a bound could be lowered so as to satisfy any level of confidence in the timeliness and reliability of the system. In this paper we present AIDA and contrast it with traditional communication scheduling techniques used in conjunction with time-critical applications in general, and distributed multimedia systems in particular. The suitability of AIDA-based bandwidth allocatio [...] ...|$|R
40|$|This paper {{presents}} the results of a mission concept study for an autonomous micro-scale surface lander also referred to as PANIC - the Pico Autonomous Near-Earth Asteroid In Situ Characterizer. The lander is based on the shape of a regular tetrahedron with an edge length of 35 cm, has a total mass of approximately 12 kg and utilizes hopping as a locomotion mechanism in microgravity. PANIC houses four scientific instruments in its proposed baseline configuration which enable the in situ characterization of an asteroid. It is carried by an interplanetary probe to its target and released to the surface after rendezvous. Detailed estimates of all critical subsystem parameters were derived to demonstrate the feasibility of this concept. The study illustrates that a small, simple landing element is a viable alternative to complex traditional lander concepts, adding a significant science return to any near-Earth asteroid (NEA) mission while <b>meeting</b> <b>tight</b> mass budget constraints. Comment: 16 pages, 5 figure...|$|R
40|$|In this paper, performance, {{efficiency}} and emission experimental {{results are presented}} from a prototype 434 cm 3, highly turbocharged (TC), two cylinder engine with brake power limited to approximately 60 kW. These results are compared to current small engines found in today’s automobile marketplace. A normally aspirated (NA) 1. 25 liter, four cylinder, modern production engine with similar brake power output is used for comparison. Results illustrate the potential for downsized engines to significantly reduce fuel consumption while still maintaining engine performance. This has advantages in reducing vehicle running costs together with <b>meeting</b> <b>tighter</b> carbon dioxide (CO 2) emission standards. Experimental results highlight the performance potential of smaller engines with intake boosting. This is demonstrated with the test engine achieving 25 bar {{brake mean effective pressure}} (BMEP). Results are presented across varying parameter domains, including engine speed, compression ratio (CR), manifold absolute pressure (MAP) and lambda (λ). Engine operating limits are also outlined, with spark knock highlighted as the major limitation in extending the operating limits for this downsized engine. 16 - 20 SeptemberOpen Acces...|$|R
40|$|The {{demands for}} an {{actuator}} to deploy, position and shape large spaced-based structures form a {{unique set of}} design criteria. In many applications it is desirable to hold displacements or forces between two points to within specified requirements (the regulation problem) and to periodically to change position (the tracking problem). Furthermore, the interest generally lies in satisfying the dynamic performance requirements while expending minimal power, while <b>meeting</b> <b>tight</b> tolerances and while experiencing little wear and fatigue. The space-based actuator must also be able to withstand a variety of operational conditions such as impacts, low temperatures and thermal changes {{over an extended period of}} time. In large space structure design, actuators are distributed throughout the structure for control and direct actuation and must be designed to provide redundant operation in case of individual actuator failure. Current actuator systems for large-scale structures have utilized conventional actuators and motors, along with elaborate linkages or mechanisms to shape, position, and deploy as well as provide structural protection. The developed design uses unique characteristics of permanent magnets to creat...|$|R
40|$|The {{high-performance}} computing domain is enriching with {{the inclusion of}} Networks-on-chip (NoCs) as {{a key component of}} many-core (CMPs or MPSoCs) architectures. NoCs face the communication scalability challenge while <b>meeting</b> <b>tight</b> power, area and latency constraints. Designers must address new challenges that were not present before. Defective components, the enhancement of application-level parallelism or power-aware techniques may break topology regularity, thus, efficient routing becomes a challenge. In this paper, uLBDR (Universal Logic-Based Distributed Routing) is proposed as an efficient logic-based mechanism that adapts to any irregular topology derived from 2 D meshes, being an alternative to the use of routing tables (either at routers or at end-nodes). uLBDR requires a small set of configuration bits, thus being more practical than large routing tables implemented in memories. Several implementations of uLBDR are presented highlighting the trade-off between routing cost and coverage. The alternatives span from the previously proposed LBDR approach (with 30 % of coverage) to the uLBDR mechanism achieving full coverage. This comes with a small performance cost, thus exhibiting the trade-off between fault tolerance and performance...|$|R
30|$|The {{complexity}} of today's embedded electronic systems {{as well as}} their demanding performance and reliability requirements are such that their design can no longer be tackled with ad hoc techniques while still <b>meeting</b> <b>tight</b> time to-market constraints. In this paper, we present a system level design approach for electronic circuits, utilizing the platform-based design (PBD) paradigm as the natural framework for mixed-domain design formalization. In PBD, a meet-in-the-middle approach allows systematic exploration of the design space through a series of top-down mapping of system constraints onto component feasibility models in a platform library, which is based on bottom-up characterizations. In this framework, new designs can be assembled from the precharacterized library components, giving the highest priority to design reuse, correct assembly, and efficient design flow from specifications to implementation. We apply concepts from design centering to enforce robustness to modeling errors as well as process, voltage, and temperature variations, which are currently plaguing embedded system design in deep-submicron technologies. The effectiveness of our methodology is finally shown on the design of a pipeline A/D converter and two receiver front-ends for UMTS and UWB communications.|$|R
40|$|In {{data center}} applications, {{predictability}} in service time and controlled latency, especially tail latency, {{are essential for}} building performant applications. This {{is especially true for}} applications or services built by accessing data across thousands of servers to generate a user response. Current practice has been to run such services at low utilization to rein in latency outliers, which decreases efficiency and limits the number of service invocations developers can issue while still <b>meeting</b> <b>tight</b> latency budgets. In this paper, we analyze three data center applications, Memcached, OpenFlow, and Web search, to measure the effect of 1) kernel socket handling, NIC interaction, and the network stack, 2) application locks contested in the kernel, and 3) application-layer queueing due to requests being stalled behind straggler threads on tail latency. We propose Chronos, a framework to deliver predictable, low latency in data center applications. Chronos uses a combination of existing and new techniques to achieve this end, for example by supporting Memcached at 200, 000 requests per second per server at mean latency of 10 µs with a 99 th percentile latency of only 30 µs, a factor of 20 lower than baseline Memcached...|$|R
40|$|The {{complexity}} of today&# 39;s embedded electronic systems {{as well as}} their demanding performance and reliability requirements are such that their design can no longer be tackled with ad hoc techniques while still <b>meeting</b> <b>tight</b> time to-market constraints. In this paper, we present a system level design approach for electronic circuits, utilizing the platform-based design (PBD) paradigm as the natural framework for mixed-domain design formalization. In PBD, a meet-in-the-middle approach allows systematic exploration of the design space through a series of top-down mapping of system constraints onto component feasibility models in a platform library, which is based on bottom-up characterizations. In this framework, new designs can be assembled from the precharacterized library components, giving the highest priority to design reuse, correct assembly, and efficient design flow from specifications to implementation. We apply concepts from design centering to enforce robustness to modeling errors as well as process, voltage, and temperature variations, which are currently plaguing embedded system design in deep-submicron technologies. The effectiveness of our methodology is finally shown on the design of a pipeline A/D converter and two receiver front-ends for UMTS and UWB communications...|$|R
40|$|Ultraprecision, {{single point}} diamond turning (SPDT) {{is a tool}} based {{machining}} technology that allows the ability to produce high quality surface finishes {{on the order of}} nanometers while <b>meeting</b> <b>tight</b> form tolerances on the order of micrometers. It is generally agreed that surface finish in SPDT is primarily affected by four factors: Tool edge quality, relative vibration between the tool and workpiece, material properties and microstructure, and tool geometry (nose radius and machining parameters) machining. To the author’s knowledge, no work has been done to combine all the factors to study their effect on surface generation in SPDT. This is important given that the factors are highly interdependent. Two diamond tools with nose radius of 12 mm were used; however, one of them was chemically honed. Results suggest that the honed tool provides a much better surface finish with a significantly reduced amount of running-in stage tool wear. The cutting edge radius of the diamond tools was measured using a novel 3 D confocal laser microscope to analyze the chemical honing process ii and to measure tool wear. The presence of built-up edge (BUE) is more prominent on the honed tool earlier in its life which results in unpredictable surface roughness to appear sooner than o...|$|R
40|$|Abstract—The {{high-performance}} computing domain is enrich-ing with {{the inclusion of}} networks-on-chip (NoCs) as {{a key component of}} many-core (CMPs or MPSoCs) architectures. NoCs face the communication scalability challenge while <b>meeting</b> <b>tight</b> power, area, and latency constraints. Designers must address new challenges that were not present before. Defective components, the enhancement of application-level parallelism, or power-aware techniques may break topology regularity, thus, efficient routing becomes a challenge. This paper presents universal logic-based distributed routing (uLBDR), an efficient logic-based mechanism that adapts to any irregular topology derived from 2 -D meshes, instead of using routing tables. uLBDR requires a small set of configuration bits, thus being more practical than large routing tables implemented in memories. Several implementations of uLBDR are presented highlighting the tradeoff between routing cost and coverage. The alternatives span from the previously proposed LBDR approach (with 30 % of coverage) to the uLBDR mechanism achieving full coverage. This comes with a small performance cost, thus exhibiting the tradeoff between fault tolerance and performance. Power consumption, area, and delay estimates are also provided highlighting the efficiency of the mechanism. To do this, different router models (one for CMPs and one for MPSoCs) have been designed as a proof concept. Index Terms—Fault-tolerance, logic design, networks-on-chip, routing. I...|$|R
500|$|The Canadians reorganised and on 14 August they {{launched}} Operation Tractable; {{three days later}} Falaise fell. The Allied noose was relentlessly closing around von Kluge's force, and it fell to the 1st Polish Armoured Division to draw it <b>tight.</b> In a <b>meeting</b> with his divisional commanders on 19 August, Simonds emphasised the importance of quickly closing the Falaise Pocket to General Stanisław Maczek. Assigned responsibility for the Moissy–Chambois–Coudehard area, Maczek's 1st Polish Armoured Division had split into three battlegroups—each of an armoured regiment and an infantry battalion—and were sweeping the countryside north of Chambois. However, facing stiff German resistance and with Koszutski's battlegroup having [...] "gone astray" [...] and needing to be rescued, the division had not yet taken Chambois, Coudehard, or the Mont Ormel ridge. Galvanised by Simonds, Maczek was determined to get his men onto their objectives as soon as possible. The 10th Dragoons (10th Polish Motorised infantry Battalion) and 10th Polish Mounted Rifle Regiment (the division's armoured reconnaissance regiment) drove hard on Chambois, the capture of which would effect a link-up with the United States 90th Infantry Division who were simultaneously attacking the town from the south. Having taken Trun and Champeaux the 4th Canadian Armoured Division was able to assist, and by the evening of 19 August the town was in Allied hands.|$|R
30|$|Interviews {{conducted}} in four sectors—automotive industry, pharmaceuticals, telecom equipment manufacturing and software development 18 —have confirmed that companies and public R&D units (HEIs and PROs) {{are driven by}} fundamentally different incentives and goals {{to be involved in}} R&D and innovation activities. Hence, there are inherent hindrances to B-A collaboration. In brief, companies are interested in a relatively wide array of R&D activities (from day-to-day problem solving to long-term strategic research, some of which may require producing advanced scientific and technological knowledge, or even path-breaking new theoretical results), but those should lead to business results (e.g. enhanced productivity, larger market shares, entry to new markets, increased profits). Projects are regularly monitored and assessed, and when necessary, a given project could be substantially reshaped (e.g. {{in terms of the number}} of participants, R&D methods applied, budget), or even stopped. Thus, <b>tight</b> project management (<b>meeting</b> deadlines and ‘respecting’ budget constraints) and keeping commercially sensible information secret are of vital importance. In contrast, researchers working for universities and PROs are not simply interested, but even forced to disclose their results as quickly and as widely as possible, given the evaluation criteria applied in the academic world. Further, they are usually less accustomed to tight project management, but noticeable changes have occurred in recent years, due to tighter control exercised by both the domestic and foreign funding agencies.|$|R
40|$|Chip multiprocessors (CMPs) {{are gaining}} {{momentum}} in the high-performance computing domain. Networks-on-chip (NoCs) are key components of CMP architectures, in {{that they have to}} deal with the communication scalability challenge while <b>meeting</b> <b>tight</b> power, area and latency constraints. 2 D mesh topologies are usually preferred by designers of general purpose NoCs. However, manufacturing faults may break their regularity. Moreover, resource management frameworks may require the segmentation of the network into irregular regions. Under these conditions, efficient routing becomes a challenge. Although the use of routing tables at switches is flexible, it does not scale in terms of latency and area due to its memory requirements. Logic-based distributed routing (LBDR) is proposed as a new routing method that removes the need for routing tables at all. LBDR enables the implementation of many routing algorithms on most of the practical topologies we may find in the near future in a multi-core system. From an initial topology and routing algorithm, a set of three bits per switch/output port is computed. Evaluation results show that, by uysing a small logic, LBDR mimics the performance of routing algorithms when implemented with routing tables, both in regular and irregular topologies. LBDR implementation in a real NoC switch is also explored, proving its smooth integration in the architecture and its negligible hardware and performance overhead...|$|R
40|$|Copyright © 2010 Pierluigi Nuzzo et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. The complexity of today’s embedded electronic systems {{as well as their}} demanding performance and reliability requirements are such that their design can no longer be tackled with ad hoc techniques while still <b>meeting</b> <b>tight</b> time to-market constraints. In this paper, we present a system level design approach for electronic circuits, utilizing the platform-based design (PBD) paradigm as the natural framework for mixed-domain design formalization. In PBD, a meet-in-the-middle approach allows systematic exploration of the design space through a series of top-down mapping of system constraints onto component feasibility models in a platform library, which is based on bottom-up characterizations. In this framework, new designs can be assembled from the precharacterized library components, giving the highest priority to design reuse, correct assembly, and efficient design flow from specifications to implementation. We apply concepts from design centering to enforce robustness to modeling errors as well as process, voltage, and temperature variations, which are currently plaguing embedded system design in deep-submicron technologies. The effectiveness of our methodology is finally shown on the design of a pipeline A/D converter and two receiver front-ends for UMTS and UWB communications. 1...|$|R
40|$|Current simulation-sampling {{techniques}} construct accurate {{model state}} for each measurement by continuously warming large microarchitectural structures (e. g., caches and the branch predictor) while functionally simulating {{the billions of}} instructions between measurements. This approach, called functional warming, is the main performance bottleneck of simulation sampling and requires hours of runtime while the detailed simulation of the sample requires only minutes. Existing simulators can avoid functional simulation by jumping directly to particular instruction stream locations with architectural state checkpoints. To replace functional warming, these checkpoints must additionally provide microarchitectural model state that is accurate and reusable across experiments while <b>meeting</b> <b>tight</b> storage constraints. In this paper, we present a simulation-sampling framework that replaces functional warming with live-points without sacrificing accuracy. A live-point stores the bare minimum of functionallywarmed state for accurate simulation of a limited execution window while placing minimal restrictions on microarchitectural configuration. Live-points can be processed in random rather than program order, allowing simulation results and their statistical confidence to be reported while simulations are in progress. Our framework matches the accuracy of prior simulation-sampling techniques (i. e., ± 3 % error with 99. 7 % confidence), while estimating the performance of an 8 -way out-of-order superscalar processor running SPEC CPU 2000 in 91 seconds per benchmark, on average, using a 12 GB live-point library. 1...|$|R
5000|$|The Canadians reorganised and on 14 August they {{launched}} Operation Tractable; {{three days later}} Falaise fell. The Allied noose was relentlessly closing around von Kluge's force, and it fell to the 1st Polish Armoured Division to draw it <b>tight.</b> In a <b>meeting</b> with his divisional commanders on 19 August, Simonds emphasised the importance of quickly closing the Falaise Pocket to General Stanisław Maczek. Assigned responsibility for the Moissy-Chambois-Coudehard area, Maczek's 1st Polish Armoured Division had split into three battlegroups—each of an armoured regiment and an infantry battalion—and were sweeping the countryside north of Chambois. However, facing stiff German resistance and with Koszutski's battlegroup having [...] "gone astray" [...] and needing to be rescued, the division had not yet taken Chambois, Coudehard, or the Mont Ormel ridge. Galvanised by Simonds, Maczek was determined to get his men onto their objectives as soon as possible. The 10th Dragoons (10th Polish Motorised infantry Battalion) and 10th Polish Mounted Rifle Regiment (the division's armoured reconnaissance regiment) drove hard on Chambois, the capture of which would effect a link-up with the United States 90th Infantry Division who were simultaneously attacking the town from the south. Having taken Trun and Champeaux the 4th Canadian Armoured Division was able to assist, and by the evening of 19 August the town was in Allied hands.|$|R
40|$|The diverse {{approaches}} to translation {{quality in the}} industry can be grouped in two broad camps: top-down and bottom-up. The author has recently published a decade-long study of the language services (Quality in Professional Translation, Bloomsbury, 2013). Research for the study covered translation providers from individual freelance translators working at home, to large-scale institutions including the European Union Directorate-General for Translation, commercial translation companies and divisions, and not-for-profit translation groups. Within the two broad ‘top-down’ and ‘bottom-up’ camps, a range of further sub-models was identified and catalogued (e. g. ‘minimalist’ or ‘experience-dependent’). The shared distinctive features of each sub-group were described, with a particular focus on their use of technologies. These different approaches have significant implications for, first, the integration of industry standards on quality, and, second, the efficient harnessing of technology throughout the translation workflow. This contribution explains the range of industry {{approaches to}} translation quality then asks how these map on to successful integration of standards, and features of the leading tools which are designed to support or enhance quality. Are standards and technologies inevitably experienced as an imposition by translators and others involved in the translation process? Significantly, no straightforward link was found between a ‘top-down’ or ‘bottom-up’ approach to assessing or improving translation quality and effective use of tools or standards. Instead, positive practice was identified {{across a range of}} approaches. The discussion outlines some painless ways these developments are being channelled to improve quality, or more frequently, to maintain it while <b>meeting</b> <b>tighter</b> deadlines. Some models existed beyond, or were partially integrated in, ‘professional’ translation (e. g. pro bono translators, and volunteer Open Source localizers). What lessons can we learn from enthusiasts in such communities, who sometimes adopt or create approaches voluntarily...|$|R
40|$|A {{letter report}} {{issued by the}} General Accounting Office with an {{abstract}} that begins "Nonresponse follow-up [...] in which Census Bureau enumerators go door-to-door to count individuals who have not mailed back their questionnaires [...] was the most costly and labor intensive of all 2000 Census operations. According to Bureau data, labor, mileage, and administrative costs totaled $ 1. 4 billion, or 22 percent of the $ 6. 5 billion allocated for the 2000 Census. Several practices were critical to the Bureau's timely competition of nonresponse follow-up. The Bureau (1) had an aggressive outreach and promotion campaign, simplified questionnaire, and other efforts to boost the mail response rate and thus reduce the Bureau's nonresponse follow-up workload; (2) used a flexible human capital strategy that enabled it to meet its national recruiting and hiring goals and position enumerators where they were most needed; (3) called on local census offices to identify local enumeration challenges, such as locked apartment buildings and gated communities, and to develop action plans to address them; and (4) applied ambitious interim "stretch" goals that encouraged local census offices to finish 80 percent of their nonresponse follow-up workload within the first four weeks and be completely finished {{by the end of}} the eighth week, as opposed to the ten-week time frame specified in the Bureau's master schedule. Although these initiatives were key to <b>meeting</b> <b>tight</b> time frames for nonresponse follow-ups, the Bureau's experience in implementing them highlights challenges for the next census in 2010. First, maintaining the response rate is becoming increasingly expensive. Second, public participation in the census remains problematic. Third, the address lists used for nonresponse follow-up did not always contain the latest available information because the Bureau found it was infeasible to remove many late-responding households. Fourth, the Bureau's stretch goals appeared to produce mixed results. Finally, there are questions about how reinterview procedures aimed at detecting enumerator fraud and other quality problems were implemented. ...|$|R
40|$|Video {{delivery}} is anticipated to become {{among the most}} popular services in networking. The rapid advances in communication technologies, combined with the increasing efficiency of video compression techniques have paved the way for innovative and exciting video communication applications. However, the acceptance of video communication services can suffer severely from variations and deterioration of the video quality. In the absence of Quality-of-Service aware networks, the communicating applications cannot control the service parameters offered to them by the underlying protocol layers. By consequence, the applications may experience varying packet delivery delays and often also packet loss. This results in severe degradation in the video quality as perceived by the user. In this dissertation, we develop a framework for prioritized video delivery. We introduce models for the estimation of distortion resulting from loss of video packets. Based on these models, we propose a video packet prioritization mechanism, which reflects accurately the importance of the video data carried in one packet for the video quality at the end user. Using this prioritization scheme, we design error control and avoidance techniques, which achieve near-to-optimal video quality. Given the trend towards wireless and mobile access to video services, we advocate the deployment of proxy caches in proximity to end users. Proxy caches should adapt the video stream to the needs of each video receiver, hence, ameliorating the perceived video quality, while still <b>meeting</b> <b>tight</b> delay constraints and low device capabilities. We introduce the idea of proxy caching for adaptive retransmission and show how appropriate retransmission schemes, based on size-distortion optimization, leads to significant quality improvements, even under strong delay and buffer constraints. We then construct a forward error correction code, which protects the different video packets at different levels based on their distortion-based priority. We show that our code achieves near-optimal quality at low code rates and end-to-end delays. Finally, we develop a rate estimation and shaping algorithm, which drops low priority video packets to meet rate constraints...|$|R
40|$|The next {{generation}} of space observatories will use larger mirrors while <b>meeting</b> <b>tighter</b> optical performance requirements than current space telescopes. The spacecraft designs must satisfy the drive for low-mass, low-cost systems, and be robust to uncertainty since design validation {{will be based on}} analysis instead of pre-launch tests. Analytical techniques will be required to identify which technologies or structural architectures are most appropriate to meet conflicting system requirements, but traditionally, model-based dynamic analysis would only take place after a single point design is chosen. The challenges facing future space telescopes require a new approach to conceptual design, and motivate the creation of design tools to identify superior, robust designs earlier in the design lifecycle using model-based analysis methods. A conceptual design methodology is proposed, in which both nominal performance as well as robustness to uncertainty are evaluated across multiple design realizations. A modeling environment is created so that for any set of design variables, such as mirror architecture or dimensions of the spacecraft, a finite element model is automatically generate and analyzed. (cont.) A frequency-based dynamic analysis is performed for each design realization using integrated disturbance-to-performance models that include control systems and vibration isolators. Next, the uncertainty in early stages of design is considered and Design of Experiments tools such as the analysis of variance are used to identify critical uncertainty parameters. Lastly, parametric uncertainties are propagated through the model to bound the outputs. Aspects of this methodology are applied to several telescopes in order to demonstrate the practicality of this approach in real-life design studies. Critical uncertainty parameter identification and uncertainty analysis tools are applied to the Terrestrial Planet Finder interferometer. A parameterized model is prepared and a trade-space analysis performed for the ground-based Thirty Meter Telescope. Finally, the methodology as a whole is applied to a new space telescope design employing lightweight mirrors and a segmented aperture. An exploration of the design space is followed by uncertainty evaluation of the optimal designs. Over 1200 unique design realizations are evaluated, and the architecture families that provide the best performance and robustness to uncertainty are identified. by Scott Alan Uebelhart. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, Dept. of Aeronautics and Astronautics, 2006. Includes bibliographical references (p. 261 - 272) ...|$|R
40|$|Phytophagous (plant-feeding) {{insects are}} {{extremely}} species-rich and typically display <b>tight</b> host associations (<b>meeting</b> and mating {{on or near}} their host plant) with one or {{a small number of}} hosts. This specialized lifestyle can promote diversification through assortative mating, ultimately leading to genetically differentiated host races (host associated differentiation; HAD). It has been shown that HAD can cascade up to the parasitic wasps (parasitoids) that utilize the phytophagous insects as hosts. Cascading HAD occurs when there is genetic differentiation among parasitoids as a result of differential host plant use by their host insects. Thus, host switching can promote parasitoid diversification as well. Here, I present three studies designed to help understand aspects of parasitoid shifts to novel hosts and environments. All of the studies in this dissertation utilize the Rhagoletis complex of flies and their associated parasitoids. Specifically, I address i) the role of subtle trait variation and environmental context in predicting successful parasitoid host shifts; ii) whether parasitoid host discrimination (a trait that can influence host shifts) is an innate or learned behavior; and iii) whether contemporary patterns of host shifts among parasitoids are echoed by historical host shifts in cophylogenetic analyses of host and parasitoid genera? Towards my first aim, I present a phenomenological model developed to predict successful host shifts by parasitoids. The simulations of the model explore how environmentally mediated traits can affect successful parasitoid colonization of a new host. For my second aim, I hypothesize that behaviors impacting parasitoid host plant preferences host shifts will be genetically based rather than a learned behavior. Shifting to a new host plant has been shown to cause reproductive isolation in phytophagous insects because of strong fidelity with their host plant. Parasitoids, however, have no direct contact with the host plant as they develop entirely within the host insect. The differences in life history traits could result parasitoid host shifts being driven by random changes in host preferences. I present preliminary results suggesting that parasitoids preferentially respond to their ancestral host plant’s olfactory cues, suggesting that host preferences have a genetic basis. Finally, I present a cophylogenetic analysis of Rhagoletis hosts and their parasitoids. I find that cospeciation is the most common coevolutionary event, although there is evidence of recent host shifting that contributes to current parasitoid species diversity. The results of these studies can help us understand how host shifts can act as a potential mechanism driving diversification in parasitoids...|$|R
40|$|Graybel (a fictitious name {{used for}} privacy reasons) {{is a large}} {{developer}} of pharmaceuticals. Graybel's Antibody Protein Engineering Group (APEG) is responsible for early stage drug development of biotherapeutic molecules. Part of this responsibility is delivering high quality molecules while <b>meeting</b> <b>tight</b> deadlines. Across the industry there is constant pressure to decrease timelines, {{while at the same}} time the complexity of molecules is increasing. In order to meet this challenge, APEG must be highly adaptable. Unfortunately, unanticipated biology, long project lead times, unpredictable workflows and inadequate workflow tracking systems make it difficult to precisely determine what causes delays. This uncertainty, combined with the inability to quickly pilot changes to process or methodology, makes each potential change both risky and costly. The goal of this project was to provide APEG with two things: the knowledge needed to build a robust workflow tracking system and simulations that would assist in finding root causes of issues and allow for low-cost piloting of potential solutions. Combined, a workflow tracking database and decision tool would greatly reduce the risk associated with implementing changes, allowing APEG to adapt to meet increasingly difficult industry standards. Multiple avenues were used to collect the data needed on APEG's workflow. The primary source of data is interviews, with both management and experienced bench workers. These interviews provided data on workflow paths and estimates for workflow stage durations that could not be found elsewhere. In addition, they provided a way for APEG members to be involved in the project. Additional data was gathered from rudimentary systems that are used to track workflow within some functional groups. This data was then used to create detailed process maps, and simulations. Once validated, simulation results were analyzed and experimented with to determine current bottlenecks, potential future issues and possible fixes for these problems. In addition, a new metric was introduced for quantitatively evaluating the difficulty of a project called the Technology Readiness Level (TRL). Essential project decisions were identified, and recommendations made to track those issues. Bottlenecks were identified through queue analysis. Potential changes to fix these and other issues were piloted to determine effect. Future states, both with and without these changes, were simulated to determine potential problems. From this, causes of current and potential future delay were identified and recommendations developed. Recommendations included staffing changes, cross training, real-life piloting and developing a deeper understanding of certain processes. by Katherine A. D. Davis. Thesis: M. B. A., Massachusetts Institute of Technology, Sloan School of Management, 2016. Thesis: S. M., Massachusetts Institute of Technology, Department of Mechanical Engineering, 2016. Cataloged from PDF version of thesis. Includes bibliographical references (pages 90 - 91) ...|$|R
