2599|1281|Public
5|$|David A. Wheeler's paper How to Prevent {{the next}} Heartbleed {{analyzes}} why Heartbleed wasn't discovered earlier, and suggests several techniques {{which could have}} led to a faster identification, as well as techniques which could have reduced its impact. According to Wheeler, the most efficient technique which could have prevented Heartbleed is an atypical <b>test</b> <b>suite</b> thoroughly performing what he calls negative testing, i.e. testing that invalid inputs cause failures rather than successes. Wheeler highlights that a single general-purpose <b>test</b> <b>suite</b> could serve as a base for all TLS implementations.|$|E
5|$|COBOL 2002 {{suffered}} from poor support: no compilers completely supported the standard. Micro Focus {{found that it}} was {{due to a lack of}} user demand for the new features and due to the abolition of the NIST <b>test</b> <b>suite,</b> which had been used to test compiler conformance. The standardization process was also found to be slow and under-resourced.|$|E
5|$|Many alpha, beta, and release-candidates {{are also}} {{released}} as previews and for testing before final releases. Although {{there is a}} rough schedule for each release, this is often pushed back, if the code is not ready. The development team monitors {{the state of the}} code by running the large unit <b>test</b> <b>suite</b> during development, and using the BuildBot continuous integration system.|$|E
40|$|Model-based {{software}} development is gaining interest in domains such as avionics, space, and automotives. The model {{serves as the}} central artifact for the development efforts (such as, code generation), therefore, {{it is crucial that}} the model be extensively validated. Automatic generation of interaction <b>test</b> <b>suites</b> is a candidate for partial automation of this model validation task. Interaction testing is a combinatorial approach that systematically tests all t-way combinations of inputs for a system. In this paper, we report how well interaction <b>test</b> <b>suites</b> (2 -way through 5 -way interaction <b>test</b> <b>suites)</b> structurally cover a model of the modelogic of a flight guidance system. We conducted experiments to (1) compare the coverage achieved with interaction <b>test</b> <b>suites</b> to that of randomly generated tests and (2) determine if interaction <b>test</b> <b>suites</b> improve the coverage of black-box <b>test</b> <b>suites</b> derived from system requirements. The experiments show that the interaction <b>test</b> <b>suites</b> provide little benefit over the randomly generated tests and do not improve coverage of the requirements-based tests. These findings raise questions on the application of interaction testing in this domain. 1...|$|R
40|$|Abstract. Today, <b>test</b> <b>suites</b> {{of several}} ten {{thousand}} {{lines of code}} are specified using the Testing and Test Control Notation (TTCN- 3). Experience shows that the resulting <b>test</b> <b>suites</b> suffer from quality problems with respect to internal quality aspects like usability, maintainability, or reusability. Therefore, a quality assessment of TTCN- 3 <b>test</b> <b>suites</b> is desirable. A powerful approach to detect quality problems in source code is the identification of code smells. Code smells are patterns of inappropriate language usage that is error-prone or may lead to quality problems. This paper presents a quality assessment approach for TTCN- 3 <b>test</b> <b>suites</b> {{which is based on}} TTCN- 3 code smells: To this aim, various TTCN- 3 code smells have been identified and collected in a catalogue; the detection of instances of TTCN- 3 code smells in <b>test</b> <b>suites</b> has been automated by a tool. The applicability of this approach is demonstrated by providing results from the quality assessment of several standardised TTCN- 3 <b>test</b> <b>suites.</b> ...|$|R
40|$|AbstractIn {{this paper}} we {{describe}} {{an experiment in}} automatic generation of <b>test</b> <b>suites</b> for protocol <b>testing.</b> We report the results gained with generation of <b>test</b> <b>suites</b> based on advanced verification techniques applied to a real industrial protocol. In this experiment, several tools have been used: the commercial tool GEODE (VERILOG) {{was used for the}} generation of finite state graph models from SDL specifications, the tool Aldebaran of the CADP toolbox for the minimization of transition systems, and a prototype named TGV (for Test Generation using Verification techniques) for the generation of <b>test</b> <b>suites</b> which has been developed in the CADP toolbox. TGV is based on verification techniques such as synchronous product and on-the-fly verification. These tools have been applied to an industrial protocol, the DREX protocol. The comparison of produced <b>test</b> <b>suites</b> with hand written <b>test</b> <b>suites</b> proves the relevance of the used techniques...|$|R
25|$|Run through <b>test</b> <b>suite</b> to {{validate}} system, adjust details as required.|$|E
25|$|There is a {{comprehensive}} set of example documents in OpenDocument format available. The whole <b>test</b> <b>suite</b> is available under the Creative Commons Attribution 2.5 license.|$|E
25|$|In 2011, on the {{official}} CSS 2.1 <b>test</b> <b>suite</b> by standardization organization W3C, WebKit, the Chrome rendering engine, passes 89.75% (89.38% out of 99.59% covered) CSS 2.1 tests.|$|E
40|$|Algorithmic {{construction}} of software interaction <b>test</b> <b>suites</b> has focussed on pairwise coverage; less {{is known about}} the efficient {{construction of}} <b>test</b> <b>suites</b> for t-way interactions with t ≥ 3. This work extends an efficient density-based algorithm for pairwise coverage to generate t-way interaction <b>test</b> <b>suites,</b> and show that it guarantees a logarithmic upper bound {{on the size of the}} <b>test</b> <b>suites</b> as a function of the number of factors. To complement this theoretical guarantee, an implementation is outlined and some practical improvements are made. Computational comparisons with other published methods are reported. Many of the results improve upon those in the literature. However, limitations on the ability of one-test-at-a-time algorithms are also identified...|$|R
40|$|In {{this report}} we {{describe}} {{an experiment in}} automatic generation of <b>test</b> <b>suites</b> for protocol <b>testing.</b> We report the results gained with generation of <b>test</b> <b>suites</b> based on advanced verification techniques applied to a real industrial protocol. In this experiment, several tools have been used : the commercial tool GEODE (VERILOG) {{was used for the}} generation of finite state graph models from SDL specifications, the tool Aldebaran of the CADP toolbox for the minimization of transition systems, and a prototype named TGV (for Test Generation using Verification techniques) for the generation of <b>test</b> <b>suites</b> which has been developed in the CADP toolbox. TGV is based on verification techniques such as synchronous product and on-the-fly verification. These tools have been applied to an industrial protocol, the DREX protocol. The comparison of produced <b>test</b> <b>suites</b> with hand written <b>test</b> <b>suites</b> proves the relevance of the used techniques. Key-words: protocol, black box testing, test generation, [...] ...|$|R
3000|$|The six {{complete}} <b>test</b> <b>suites</b> were prioritized using {{each test}} prioritization, and the cumulative {{effectiveness of these}} <b>test</b> <b>suites</b> was measured in twenty-one parts. Afterwards, the cumulative effectiveness was {{used to calculate the}} APFD of each scenario. The APFD value was calculated using Eq. 1, F [...]...|$|R
25|$|IE8 {{passes the}} Acid2 test, but fails the Acid3 test {{with a score}} of 20/100. During its development, Microsoft {{developed}} over 7,000 tests for CSS level 2 compliance, which were submitted to the W3C for inclusion in their <b>test</b> <b>suite.</b>|$|E
25|$|In 2016, Google {{introduced}} {{the ability to}} run Android apps on supported Chrome OS devices, {{with access to the}} entire Google Play Store. The previous Native Client-based solution was dropped in favor of a container containing Android's frameworks and dependencies (initially based on Android 6.0), which allows Android apps to have direct access to the Chrome OS platform, and allow the OS to interact with Android contracts such as sharing. Engineering director Zelidrag Hornung explained that ARC had been scrapped due to its limitations, including its incompatibility with the Android Native Development Toolkit (NDK), and that it was unable to pass Google's own compatibility <b>test</b> <b>suite.</b>|$|E
2500|$|Donald Knuth has {{indicated}} {{several times that}} the source code of TeX has been placed into the [...] "public domain", and he strongly encourages modifications or experimentations with this source code. In particular, since Knuth highly values the reproducibility of the output of all versions of TeX, any changed version must not be called TeX, or anything confusingly similar. [...] To enforce this rule, any implementation of the system must pass a <b>test</b> <b>suite</b> called the TRIP test before being allowed to be called TeX. [...] The question of license is somewhat confused by the statements included {{at the beginning of}} the TeX source code, which indicate that [...] "all rights are reserved. [...] Copying of this file is authorized only if (...) you make absolutely no changes to your copy". [...] This restriction should be interpreted as a prohibition to change the source code as long as the file is called tex.web. [...] This interpretation is confirmed later in the source code when the TRIP test is mentioned ("If this program is changed, the resulting system should not be called 'TeX'"). The American Mathematical Society tried in the early 1980s to claim a trademark for TeX. [...] This was rejected because at the time [...] "TEX" [...] (all caps) was registered by Honeywell for the [...] "Text EXecutive" [...] text processing system.|$|E
40|$|A {{number of}} {{structural}} coverage criteria {{have been proposed}} to measure the adequacy of testing efforts. In the avionics and other critical systems domains, <b>test</b> <b>suites</b> satisfying structural coverage criteria are mandated by standards. With the advent of powerful automated test generation tools, {{it is tempting to}} simply generate test inputs to satisfy these structural coverage criteria. However, while techniques to produce coverage-providing tests are well established, the effectiveness of such approaches in terms of fault detection ability has not been adequately studied. In this work, we evaluate the effectiveness of <b>test</b> <b>suites</b> generated to satisfy four coverage criteria through counterexample-based test generation and a random generation approach-where tests are randomly generated until coverage is achieved-contrasted against purely random <b>test</b> <b>suites</b> of equal size. Our results yield three key conclusions. First, coverage criteria satisfaction alone can be a poor indication of fault finding effectiveness, with inconsistent results between the seven case examples (and random <b>test</b> <b>suites</b> of equal size often providing similar-or even higher-levels of fault finding). Second, the use of structural coverage as a supplement-rather than a target-for test generation can have a positive impact, with random <b>test</b> <b>suites</b> reduced to a coverage-providing subset detecting up to 13. 5 percent more faults than <b>test</b> <b>suites</b> generated specifically to achieve coverage. Finally, Observable MC/DC, a criterion designed to account for program structure and the selection of the test oracle, can-in part-address the failings of traditional structural coverage criteria, allowing for the generation of <b>test</b> <b>suites</b> achieving higher levels of fault detection than random <b>test</b> <b>suites</b> of equal size. These observations point to risks inherent in the increase in test automation in critical systems, and the need for more research in how coverage criteria, test generation approaches, the test oracle use-, and system structure jointly influence test effectiveness...|$|R
40|$|Abstract—A {{number of}} {{structural}} coverage criteria {{have been proposed}} to measure the adequacy of testing efforts. In the avionics and other critical systems domains, <b>test</b> <b>suites</b> satisfying structural coverage criteria are mandated by standards. With the advent of powerful automated test generation tools, {{it is tempting to}} simply generate test inputs to satisfy these structural coverage criteria. However, while techniques to produce coverage-providing tests are well established, the effectiveness of such approaches in terms of fault detection ability has not been adequately studied. In this work, we evaluate the effectiveness of <b>test</b> <b>suites</b> generated to satisfy four coverage criteria through counterexample-based test generation and a random generation approach—where tests are randomly generated until coverage is achieved—contrasted against purely random <b>test</b> <b>suites</b> of equal size. Our results yield three key conclusions. First, coverage criteria satisfaction alone can be a poor indication of fault finding effectiveness, with inconsistent results between the seven case examples (and random <b>test</b> <b>suites</b> of equal size often providing similar—or even higher—levels of fault finding). Second, the use of structural coverage as a supplement—rather than a target—for test generation can have a positive impact, with random <b>test</b> <b>suites</b> reduced to a coverage-providing subset detecting up to 13. 5 % more faults than <b>test</b> <b>suites</b> generated specifically to achieve coverage. Finally, Observable MC/DC, a criterion designed to account for program structure and the selection of the test oracle, can—in part—address the failings of traditional structural coverage criteria, allowing for the generation of <b>test</b> <b>suites</b> achieving higher levels of fault detection than random <b>test</b> <b>suites</b> of equal size. These observations point to risks inherent in the increase in test automation in critical systems, and the need for more research in how coverage criteria, test generation approaches, the test oracle used, and system structure jointly influence test effectiveness. Index Terms—Software Testing, System Testing...|$|R
40|$|Software {{engineers}} frequently update COTS components {{integrated in}} component-based systems, and can often chose among many candidates produced by different vendors. This paper tackles both {{the problem of}} quickly identifying components that are syntactically compatible with the interface specifications, but badly integrate in target systems, {{and the problem of}} automatically generating regression <b>test</b> <b>suites.</b> The technique proposed in this paper to automatically generate compatibility and prioritized <b>test</b> <b>suites</b> is based on behavioral models that represent component interactions, and are automatically generated while executing the original <b>test</b> <b>suites</b> on previous versions of target systems. ...|$|R
50|$|Rational Quality Manager has the {{following}} test artifacts: Test Plan, <b>Test</b> <b>Suite,</b> Test Case, Test Script, Test Data, <b>Test</b> <b>Suite</b> execution Record, Test Case Execution Record, <b>Test</b> <b>Suite</b> Results, Test Case Results.|$|E
50|$|The Ada Conformity Assessment <b>Test</b> <b>Suite</b> (ACATS) is the <b>test</b> <b>suite</b> {{used for}} Ada {{processor}} conformity testing.|$|E
5000|$|POSIX Conformance Testing: A <b>test</b> <b>suite</b> for POSIX {{accompanies the}} standard: VSX-PCTS or the VSX POSIX Conformance <b>Test</b> <b>Suite.</b>|$|E
40|$|AbstractAn {{abstract}} coalgebraic {{approach to}} well-structured relations on processes is presented, based on notions of <b>tests</b> and <b>test</b> <b>suites.</b> Preorders and equivalences on processes are modelled as coalgebras for behaviour endofunctors lifted to {{a category of}} <b>test</b> <b>suites.</b> The general framework is specialized {{to the case of}} finitely branching labelled transition systems. It turns out that most equivalences from the so-called van Glabbeek spectrum can be described by well-structured <b>test</b> <b>suites.</b> As an immediate application, coinductive proof principles are described for these equivalences, in particular for trace equivalence...|$|R
40|$|In model-based testing, {{particularly}} of event-driven programs, {{the problem of}} balancing reliability with cost challenges testers. This study provides empirical data to help testers decide how best to achieve this balance. We empirically evaluate the size and fault-detection ability of <b>test</b> <b>suites</b> that satisfy various adequacy criteria, some specific to event-driven software. Our results, based on one subject program, show that <b>test</b> <b>suites</b> that exercise interactions between event handlers detect substantially more faults, but are substantially larger, than <b>test</b> <b>suites</b> that only guarantee coverage of statements or individual event handlers. ...|$|R
40|$|In {{this paper}} we develop a general {{framework}} to reason about testing. The difficulty of testing is assessed {{in terms of}} the amount of tests that must be applied to determine whether the system is correct or not. Based on this criterion, five testability classes are presented and related. We also explore conditions that enable and disable finite testability, and their relation to testing hypotheses is studied. We measure how far incomplete <b>test</b> <b>suites</b> are from being complete, which allows us to compare and select better incomplete <b>test</b> <b>suites.</b> The complexity of finding that measure, as well as the complexity of finding minimum complete <b>test</b> <b>suites,</b> is identified. Furthermore, we address the reduction of testing problems to each other, that is, we study how the problem of finding <b>test</b> <b>suites</b> to <b>test</b> systems of some kind can be reduced to the problem of finding <b>test</b> <b>suites</b> for another kind of systems. This enables to export testing methods. In order to illustrate how general notions are applied to specific cases, many typical examples from the formal testing techniques domain are presented...|$|R
50|$|The chief {{advantage}} of this approach to semantic description {{is that it is}} easy to determine whether a language implementation passes a <b>test</b> <b>suite.</b> The user can simply execute all the programs in the <b>test</b> <b>suite,</b> and compare the outputs to the desired outputs. However, when used by itself, the <b>test</b> <b>suite</b> approach has major drawbacks as well. For example, users want to run their own programs, which {{are not part of the}} test suite; indeed, a language implementation that could only run the programs in its <b>test</b> <b>suite</b> would be largely useless. But a <b>test</b> <b>suite</b> does not, by itself, describe how the language implementation should behave on any program not in the test suite; determining that behavior requires some extrapolation on the implementor's part, and different implementors may disagree. In addition, it is difficult to use a <b>test</b> <b>suite</b> to test behavior that is intended or allowed to be nondeterministic.|$|E
50|$|Phoronix uses Phoronix <b>Test</b> <b>Suite</b> {{to compare}} {{performance}} of different operating systems - Linux distributions, OpenSolaris and FreeBSD. Critics of Phoronix <b>Test</b> <b>Suite</b> argue {{that some of}} the third party tests are unfair towards some platforms.|$|E
50|$|To measure what {{percentage}} of code has been exercised by a <b>test</b> <b>suite,</b> one or more coverage criteria are used. Coverage criteria is usually defined as a rule or requirement, which <b>test</b> <b>suite</b> needs to satisfy.|$|E
40|$|With {{time-delay}} systems arising, time-delay {{system testing}} has attracted much attention. Additionally, evaluating {{the cost and}} effectiveness is required {{to make a good}} test strategy in practice. In this paper, we take time-delay and other five factors (state number, input number, output number, completeness degree, and accessibility degree) into account and present a timer embedded FSM (TEFSM) model to design a comparative strategy for assessing the coverage criteria and <b>test</b> <b>suites</b> generation methods for time-delay systems. We explore the impact on the average length of <b>test</b> <b>suites,</b> in which the <b>test</b> <b>suites</b> generation methods, coverage criteria, and TEFSM model parameters are involved...|$|R
40|$|Abstract. Two {{tools are}} {{presented}} which support test case management for accessibility <b>test</b> <b>suites.</b> Creating <b>test</b> <b>suites</b> for the Web Content Ac-cessibility Guidelines 2. 0 is one major {{objective of the}} EU-funded project BenToWeb 1. Parsifal is a desktop application which easily allows editing test description les. Test description les compose an XML layer con-taining descriptive information about the particular test cases. Amfortas is a web application which allows controlled evaluation of the <b>test</b> <b>suites</b> by users. Controlled in that sense means, that Amfortas not only stores the evaluation results, but also {{is aware of the}} physical and technical condition of the evaluator. ...|$|R
5000|$|Search abilities, saving {{time when}} <b>test</b> <b>suites</b> have {{thousands}} of tests ...|$|R
50|$|Unlike Retest all, this {{technique}} runs {{a part of}} the <b>test</b> <b>suite</b> (owing to the cost of retest all) if the cost of selecting the part of the <b>test</b> <b>suite</b> is less than the Retest all technique.|$|E
5000|$|Phoronix <b>Test</b> <b>Suite</b> (PTS) is a free, {{open-source}} benchmark {{software for}} Linux and other operating systems which is developed by Phoronix Media with cooperation from an undisclosed number of {{hardware and software}} vendors.The Phoronix <b>Test</b> <b>Suite</b> has been endorsed by sites such as Linux.com, LinuxPlanet and has been called [...] "the best benchmarking platform" [...] by Softpedia. The Phoronix <b>Test</b> <b>Suite</b> is also used by Tom's Hardware, ASELabs and other review sites.|$|E
50|$|Google has {{handed the}} tests from Sputnik <b>test</b> <b>suite</b> to Ecma International for {{inclusion}} in its ECMAScript 262 <b>test</b> <b>suite.</b> Some Sputnik tests however {{have been found to}} have issues and do not conform to ECMAScript 5th edition specification.|$|E
40|$|Testing is an {{important}} activity in engineering of industrial software. For such software, testing is usually performed manually by handcrafting <b>test</b> <b>suites</b> based on specific design techniques and domain-specific experience. To support developers in testing, different approaches for producing good <b>test</b> <b>suites</b> have been proposed. In {{the last couple of}} years combinatorial testing has been explored with the goal of automatically combining the input values of the software based on a certain strategy. Pairwise testing is a combinatorial technique used to generate <b>test</b> <b>suites</b> by varying the values of each pair of input parameters to a system until all possible combinations of those parameters are created. There is some evidence suggesting that these kinds of techniques are efficient and relatively good at detecting software faults. Unfortunately, there is little experimental evidence on the comparison of these combinatorial testing techniques with, what is perceived as, rigorous manually handcrafted testing. In this study we compare pairwise <b>test</b> <b>suites</b> with <b>test</b> <b>suites</b> created manually by engineers for 45 industrial programs. The <b>test</b> <b>suites</b> were evaluated in terms of fault detection, code coverage and number of tests. The results of this study show that pairwise testing, while useful for achieving high code coverage and fault detection for the majority of the programs, is almost as effective in terms of fault detection as manual testing. The results also suggest that pairwise testing is just as good as manual testing at fault detection for 64 % of the programs...|$|R
40|$|The {{comprehensive}} test of modern communication systems leads to large and complex <b>test</b> <b>suites</b> which {{have to be}} maintained throughout the system life-cycle. Experience with those written in the standardised Testing and Test Control Notation (TTCN- 3) {{has shown that the}} maintenance of <b>test</b> <b>suites</b> is a non-trivial task and its burden can be reduced with appropriate tool support. To this aim, we have developed the TRex tool, published as open-source tool under the Eclipse Public License, which supports the assessment and automatic restructuring of TTCN- 3 <b>test</b> <b>suites</b> by providing suitable metrics and refactorings. This paper presents TRex, its functionality, and its implementation...|$|R
5000|$|The AAF Software Development Kit (SDK), which {{includes}} developer utilities and validation <b>test</b> <b>suites</b> ...|$|R
