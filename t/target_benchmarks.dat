13|201|Public
40|$|Mikko H. LipastiElectrical and Computer Engineering, University of Wisconsin-Madison mikko@engr. wisc. edu ABSTRACT Understanding and {{exploiting}} the dynamic behavior of programs {{is key to}} improving performance beyond what is possible using static techniques. We present an implementation of a phase tracking algorithm and show {{that it can be}} successfully applied to existing SPEC 2000 benchmarks, many of which show a consistent phase behavior. We then show the application of phase information to optimize branch prediction. Our results indicate a reduction of up to 44. 35 % in branch predicition misses, using simpler predictors based on per phase information. We also get an improvement of up to 4. 70 % in IPC for our <b>target</b> <b>benchmarks...</b>|$|E
40|$|Understanding and {{exploiting}} the dynamic behavior of programs {{is key to}} improving performance beyond what is possible using static techniques. We present an implementation of a phase tracking algorithm and show {{that it can be}} successfully applied to existing SPEC 2000 benchmarks, many of which show a consistent phase behavior. We then show the application of phase information to optimize branch prediction and cache utilization. Our results indicate a reduction of up to 44. 35 % in branch predicition misses, using simpler predictors based on per phase information. We also get an improvement of up to 4. 70 % in IPC for our <b>target</b> <b>benchmarks.</b> We also applied phase information to dynamically resize L 1 caches and show that comparable IPC can be achieved with a lower average cache size of 49 K as compared to the default size of 128 K...|$|E
40|$|This paper {{presents}} the novel idea of multi-placement struc-tures, for a fast and optimized placement instantiation in ana-log circuit synthesis. These structures {{need to be}} generated only once for a specific circuit topology. When used in synthesis, these pre-generated structures instantiate various layout floor-plans for various sizes and parameters of a circuit. Unlike pro-cedural layout generators, they enable fast placement of circuits while keeping {{the quality of the}} placements at a high level during a synthesis process. The fast placement is a result of high speed instantiation resulting from the efficiency of the multi-placement structure. The good quality of placements derive from the ex-tensive and intelligent search process that is used to build the multi-placement structure. The <b>target</b> <b>benchmarks</b> of these struc-tures are analog circuits in the vicinity of 25 modules. An al-gorithm for the generation of such multi-placement structures is presented. Experimental results show placement execution times with an average of a few milliseconds making them usable dur-ing layout-aware synthesis for optimized placements. 1...|$|E
50|$|This formula can in turn be used {{to compute}} the implied {{correlation}} of the <b>target</b> with the <b>benchmark</b> as follows. If the <b>target</b> and <b>benchmark</b> volatilities can be accurately estimated using volatilities implied by the prices of options on the securities, then the option prices on the alpha index would imply {{the correlation between the}} <b>target</b> and the <b>benchmark.</b> This gives researchers a tool to calculate forward looking implied estimate of the correlation between target and market and the beta of the target security.|$|R
30|$|The {{external}} scheduler initially performs {{an assessment}} of the type and load of the OU pipelines. It then determines the capabilities of the available sites in processing the pipelines by retrieving older <b>targeted</b> <b>benchmarks</b> or completing new on the fly. The OU pipelines are then submitted to the sites according to FPLT and job failures are handled by resubmission. The pseudocode of the external scheduler is presented in Algorithmic Box 1.|$|R
40|$|We {{propose a}} uniform {{approach}} {{for the design}} and analysis of prior-free competitive auctions and online auctions. Our philosophy is to view the benchmark function as a variable parameter of the model and study a broad class of functions instead of a individual <b>target</b> <b>benchmark.</b> We consider a multitude of well-studied auction settings, and improve upon a few previous results. (1) Multi-unit auctions. Given a β-competitive unlimited supply auction, the best previously known multi-unit auction is 2 β-competitive. We design a (1 +β) -competitive auction reducing the ratio from 4. 84 to 3. 24. These results carry over to matroid and position auctions. (2) General downward-closed environments. We design a 6. 5 -competitive auction improving upon the ratio of 7. 5. Our auction is noticeably simpler than the previous best one. (3) Unlimited supply online auctions. Our analysis yields an auction with a competitive ratio of 4. 12, which significantly narrows the margin of [4, 4. 84] previously known for this problem. A particularly important tool in our analysis is a simple decomposition lemma, which allows us to bound the competitive ratio against a sum of benchmark functions. We use this lemma in a "divide and conquer" fashion by dividing the <b>target</b> <b>benchmark</b> into the sum of simpler functions...|$|R
40|$|Flexible docking {{and scoring}} using the Internal Coordinate Mechanics {{software}} (ICM) was benchmarked for ligand binding mode prediction against the 85 co-crystal {{structures in the}} modified Astex data set. The ICM virtual ligand screening was tested against the 40 DUD <b>target</b> <b>benchmarks</b> and 11 -target WOMBAT sets. The self-docking accuracy was evaluated for the top 1 and top 3 scoring poses at each ligand binding site with near native conformations below 2 Å RMSD found in 91 % and 95 % of the predictions, respectively. The virtual ligand screening using single rigid pocket conformations provided the median area under the ROC curves equal to 69. 4 with 22. 0 % true positives recovered at 2 % false positive rate. Significant improvements up to ROC AUC = 82. 2 and ROC(2 %) = 45. 2 were achieved following our best practices for flexible pocket refinement and out-of-pocket binding rescore. The virtual screening can be further improved by considering multiple conformations of the target...|$|E
40|$|Submitted {{on behalf}} of EDAA ([URL] audienceThis paper {{presents}} the novel idea of multi-placement structures, for a fast and optimized placement instantiation in analog circuit synthesis. These structures need to be generated only once for a specific circuit topology. When used in synthesis, these pre-generated structures instantiate various layout floorplans for various sizes and parameters of a circuit. Unlike procedural layout generators, they enable fast placement of circuits while keeping {{the quality of the}} placements at a high level during a synthesis process. The fast placement is a result of high speed instantiation resulting from the efficiency of the multi-placement structure. The good quality of placements derive from the extensive and intelligent search process that is used to build the multi-placement structure. The <b>target</b> <b>benchmarks</b> of these structures are analog circuits in the vicinity of 25 modules. An algorithm for the generation of such multi-placement structures is presented. Experimental results show placement execution times with an average of a few milliseconds making them usable during layout-aware synthesis for optimized placements...|$|E
40|$|In this paper, a {{low-power}} {{approach to}} the design of embedded VLIW processor architectures is proposed. To solve the most part of data hazards in the pipeline, processors use forwarding (or bypassing) hardware to provide the required operands from the inter-stage pipeline registers directly to the inputs of the function units. The operands are then stored in the Register File during the write-back pipeline stage. In this paper, we propose a power optimization technique based on the exploitation of the forwarding paths in the processor to avoid the power cost of writing/reading short-lived variables to/from the Register File. In application-specific embedded systems, experimental evidence has shown that a significant number of variables are short-lived, that is their liveness (from first definition to last use) spans only few instructions. Values of short-lived variables can be accessed directly through the forwarding registers, avoiding write-back. An application example of our solution to a VLIW embedded core, when accessing the Register File, has shown a power saving up to 35 % with respect to the unoptimized approach on the given set of <b>target</b> <b>benchmarks.</b> The performance overhead is equal to one-gate delay to be added on the processor critical-path...|$|E
40|$|This paper {{provides}} a framework to identify and achieve a benchmark portfolio structure for government debt based upon the trade-off between expected debt-service-cost and risk. Using actual Korean government debt data, we empirically derive a medium-term efficient frontier conditional upon the existing portfolio structure. In addition, a <b>target</b> <b>benchmark</b> portfolio is identified from the efficient frontier by employing a penalty function with cost-at-risk and duration gap as two penalty factors. The target portfolio identified above also implies an optimal borrowing policy as to the maturity mix of the government bond issuance. [H 6, G 1, F 3]...|$|R
50|$|The post-trade process {{involves}} first {{recording the}} data from previous trading periods, including trade timing, arrival price, average execution price, and relevant details about market movement. These data are then measured and compared to several benchmarks, such as the volume-weighted average price, time-weighted average price, participation-weighted average price, or {{a variety of other}} measures. Implementation shortfall is a commonly <b>targeted</b> <b>benchmark,</b> which is the sum of all explicit and implicit costs. Sometimes, an opportunity cost of not transacting is factored in. After measurement, costs must be attributed to their underlying causes. Finally, this analysis is used to evaluate performance and monitor future transactions.|$|R
3000|$|As {{mentioned}} before, {{the potential}} {{output of the}} economy cannot be measured directly, consequently there is no observable <b>target</b> or <b>benchmark</b> for comparison. This {{makes it difficult to}} evaluate alternative specifications. 7 [...]...|$|R
40|$|Proposes a {{low-power}} {{approach to}} the design of embedded very long instruction word (VLIW) processor architectures based on the forwarding (or bypassing) hardware, which provides operands from interstage pipeline registers directly to the inputs of the function units. The power optimization technique exploits the forwarding paths to avoid the power cost of writing/reading short-lived variables to/from the register file (RF). Such optimization is justified by the fact that, in application-specific embedded systems, a significant number of variables are short-lived, that is, their liveness (from first definition to last use) spans only few instructions. Values of short-lived variables can thus be accessed directly through the forwarding registers, avoiding writeback to the RF by the producer instruction and successive read from the RF by the consumer instruction. The decision concerning the enabling of the RF writeback phase is taken at compile time by the compiler static scheduling algorithm. This approach implies a minimal overhead on the complexity of the processor control logic and, thus, no critical path increase. The application of the proposed solution to a VLIW embedded core has shown an average RF power saving of 7. 8 % with respect to the unoptimized approach on the given set of <b>target</b> <b>benchmarks...</b>|$|E
40|$|Abstract—In this paper, {{we propose}} a {{low-power}} {{approach to the}} design of embedded very long instruction word (VLIW) pro-cessor architectures based on the forwarding (or bypassing) hard-ware, which provides operands from interstage pipeline registers directly to the inputs of the function units. The power optimiza-tion technique exploits the forwarding paths to avoid the power cost of writing/reading short-lived variables to/from the register file (RF). Such optimization is justified by the fact that, in appli-cation-specific embedded systems, a significant number of vari-ables are short-lived, that is, their liveness (from first definition to last use) spans only few instructions. Values of short-lived vari-ables can thus be accessed directly through the forwarding regis-ters, avoiding writeback to the RF by the producer instruction and successive read from the RF by the consumer instruction. The de-cision concerning the enabling of the RF writeback phase is taken at compile time by the compiler static scheduling algorithm. This approach implies a minimal overhead on the complexity of the pro-cessor control logic and, thus, no critical path increase. The ap-plication of the proposed solution to a VLIW embedded core has shown an average RF power saving of 7. 8 % with respect to the un-optimized approach on the given set of <b>target</b> <b>benchmarks</b> (patent pending owned by ST Microelectronics). Index Terms—Low-power design, microprocessors, VLSI design...|$|E
40|$|Abstract Understanding and {{exploiting}} the dynamic behavior ofprograms {{is key to}} improving performance beyond what is possible using static techniques. We present an imple-mentation of a phase tracking algorithm and show {{that it can be}} successfully applied to existing SPEC 2000 bench-marks, many of which show a consistent phase behavior. We then show the application of phase information tooptimize branch prediction and cache utilization. Our results indicate a reduction of up to 44. 35 % in branchpredicition misses, using simpler predictors based on per phase information. We also get an improvement of up to 4. 70 % in IPC for our <b>target</b> <b>benchmarks.</b> 1 Introduction Previous research has shown that understanding and ex-ploiting the dynamic behavior of programs is key to improving performance beyond what is possible using statictechniques. A program's control flow is one such aspect of its dynamic behavior that has been extensively used tooptimize program execution, typically by identifying the various phases in which a program executes. A phase is defined as an interval of executionduring which a given measured program metric is relatively stable. Phases can both be tracked (to detect what phase the ex-ecution is in) and predicted (to determine when another phase is about to be entered). The phase behavior seenin any program metric is directly a function of the wa...|$|E
40|$|Performance of {{investment}} managers is predominantly evaluated against <b>targeted</b> <b>benchmarks,</b> such as stock, bond or commodity indices. However, most professional databases do not retain timeseries {{for companies that}} disappeared, {{and do not necessarily}} track the change of constitution in these benchmarks. Consequently, standard tests of performance suffer from the “look-ahead benchmark bias,” where a given strategy is naively back-tested against the assets constituting the benchmark of reference {{at the end of the}} testing period (i. e. now), rather than at the very beginning of that period. We report that the “look-ahead benchmark bias” can exhibit a surprisingly large amplitude for portfolios of common stocks (up to 8...|$|R
40|$|Portfolio theory {{must address}} {{the fact that in}} reality, {{portfolio}} managers are evaluated relative to a benchmark, and therefore adopt risk management practices to account for the benchmark performance. We capture this risk management consideration by allowing a prespecified shortfall from a target benchmark-linked return, consistent with growing interest in such practice. In a dynamic setting, we demonstrate how a risk averse portfolio manager optimally under- or overperforms a <b>target</b> <b>benchmark</b> under different economic conditions, depending on his attitude towards risk and choice of the benchmark. Investors can therefore achieve their desired gain/loss characteristics for funds under management through an appropriate combined choice of the benchmark and money manager. ...|$|R
50|$|Investors might view Alpha Indexes as {{a measure}} of {{numerical}} difference between the returns of the <b>target</b> and the <b>benchmark</b> component. It {{is important to note that}} the percentage increase in the index over a period, say a day, is only approximately equal to the difference between the returns of the <b>target</b> and its <b>benchmark.</b> The extent to which this approximation fails is called “slippage”.|$|R
40|$|Collider, {{direct and}} {{indirect}} searches for dark matter have typically little or no sensitivity to weakly interacting massive particles (WIMPs) with masses above a few TeV. This rather unexplored regime can however be probed through the search for distinctive gamma-ray spectral features produced by the annihilation of WIMPs at very high energies. Here we present a dedicated search for gamma-ray boxes [...] sharp spectral features that cannot be mimicked by astrophysical sources [...] with the upcoming Cherenkov Telescope Array (CTA). Using realistic projections for the instrument performance and detailed background modelling, a profile likelihood analysis is implemented to derive the expected upper limits and sensitivity reach after 100 h of observations towards a 2 ^∘× 2 ^∘ region around the Galactic centre. Our results show that CTA {{will be able to}} probe gamma-ray boxes down to annihilation cross sections of 10 ^- 27 - 10 ^- 26 cm^ 3 /s up to tens of TeV. We also identify a number of concrete particle physics models providing thermal dark matter candidates that can be used as <b>target</b> <b>benchmarks</b> in future search campaigns. This constitutes a golden opportunity for CTA to either discover or rule out multi-TeV thermal dark matter in a corner of parameter space where all other experimental efforts are basically insensitive. Comment: 29 pages, 8 figures, 1 table, matches published version and includes erratu...|$|E
40|$|In the nineteen-eighties, {{synthetic}} workloads such as Whetstone and Dhrystone {{fell out}} of favor as benchmarks of computer performance because they became unrepresentative {{of the performance of}} continuously-evolving applications. Hand-coded synthetic benchmarks take work to develop and maintain, are language feature specific, and are subject to compiler optimizations that eliminate code meant to make a significant contribution to performance. We present an automatic benchmark synthesis method that addresses these problems. The method automatically creates C-code that, when compiled and executed, is representative of the features of a target application but executes in a fraction of the original runtime. Our benchmark synthesis technique takes an actual executable, performs control flow analysis and workload characterization, and generates a representative synthetic benchmark. The representative sequences of instructions are instantiated as in-line assembly-language instructions in the synthetic benchmark. We synthesize versions of the SPEC 95 and STREAM benchmarks with both perfect branching and a simple branching model. We find that benchmarks can be synthesized to an average IPC within 3. 9 % of the average IPC of the <b>target</b> <b>benchmarks</b> with remarkably similar instruction mix, cache access characteristics, RUU occupancies, and dependency characteristics. In addition, the change in IPC for a synthetic benchmark due to a design change is found to be proportional to the change in IPC for the original application. The synthesized versions of the SPEC 95 benchmarks execute in 0. 1 % of the original execution time. 1...|$|E
40|$|In this thesis, I would {{examine the}} {{discourse}} of Japan?s English language education reform for primary and secondary schools through the close reading of the Ministry of Education, Culture, Science and Technology?s (MEXT) ??Action Plan: Cultivating ?Japanese with English Abilities,?? released in 2003. This document marked a critical touchstone of Japan?s drastic move for English curriculum change by suggesting the shift of national attitude from hesitancy to willingness {{in the name of}} change for the needs of language improvement. ?Action Plan? served as a master plan for the MEXT by providing the attainment goals, key tasks, and benchmarks that would see fit to achieve in the next five years. It raised the public awareness and stirred up the public debate, for containing challenging proposals such as implementation of standardized English exams (TOEIC and TOEFL) for student assessment and teaching qualification, innovative teaching practices to high schools (i. e., Super English Language-High school[SEL-Hi]), and English as foreign language activities to primary schools. Specifically, first, I would discuss how Japan?s cultural ambivalence toward English language since the late 19 th century sets up the contexts for nation?s historical struggle in upgrading the curriculum that draws the problems reflecting on the MEXT?s recent education policy proposal. Then, I would examine Action Plan?s attainment goals setting and key agendas highlighted as the MEXT?s main strategy, and analyze its critical issues and problems affecting the needs for both students and teachers. The issues include the mismatched targets, ill-defined goals setting, and benchmarks for academic achievements and project proposals aimed for teacher training and quality instruction (i. e., JET program, and Assistant Language Teachers [ALTs]). Finally, I would provide the implications for Action Plan?s impact on educational practice by assessing student?s learning achievement and <b>target</b> <b>benchmarks</b> set for students and teachers in the five years after its release. At the end of conclusion, I would offer the list of recommendations for effective administrative policy that could provide better teaching and learning practice in Japanese schools...|$|E
40|$|To {{meet the}} demands of modern architectures, {{optimizing}} compilers must incorporate an ever larger number of increasingly complex transformation algorithms. Since code transformations may often degrade performance or interfere with subsequent transformations, compilers employ predictive heuristics to guide optimizations by predicting their effects a priori. Unfortunately, the unpredictability of optimization interaction and the irregularity of today's wide-issue machines severely limit the accuracy of these heuristics. As a result, compiler writers may temper high variance optimizations with overly conservative heuristics or may exclude these optimizations entirely. While this process results in a compiler capable of generating good average code quality across the <b>target</b> <b>benchmark</b> set, it is at the cost of missed optimization opportunities in individual code segments...|$|R
40|$|Stepwise <b>benchmark</b> <b>target</b> {{selection}} in {{data envelopment analysis}} (DEA) is a realistic and effective method by which inefficient decision-making units (DMUs) can choose benchmarks in a stepwise manner. We propose, {{for the construction of}} a benchmarking network (i. e., a network structure consisting of an alternative sequence of <b>benchmark</b> <b>targets),</b> an approach that integrates the cross-efficiency DEA, K-means clustering and context-dependent DEA methods to minimize resource improvement pattern inconsistency in the selection of the intermediate <b>benchmark</b> <b>targets</b> (IBTs) of an inefficient DMU. The specific advantages and overall effectiveness of the proposed method were demonstrated by application to a case study of 34 actual container terminal ports and the successful determination of the stepwise benchmarking path of an inefficient DMU...|$|R
40|$|Magister Commercii - MComSince {{the listing}} of the Satrix 40 in November 2000, Exchange Traded Fund (ETFs) have grown to become an {{investment}} vehicle of choice amongst retail and institutional investors of the Johannesburg Securities Exchange (JSE). Albeit gaining such an enormous traction, investors' remains curious about ETFs ability to successfully replicate the movements of their <b>target</b> <b>benchmark</b> indices and also their capability to yield arbitrage profit opportunity through mispricing. In addition to that, investors are also interested to know whether ETFs as an index tracking investment vehicle are resilient in variously cycles of the economy. Motivated by this gap {{in the body of}} knowledge, this research undertakes to evaluate the tracking ability and pricing efficiency of 19 ETFs listed on the JSE over various cycles of the economy. According to Faulkner, Loewald and Makrelov (2013) South African economy experienced the effect of the 2008 global financial crisis between 1 September 2008 and 30 June 2009. For that reason, the examination period of this research is segmented into four main categories namely: full examination period which spans from the launch date of each of the ETF under review until 30 September 2015, pre-crisis period that is between the launch date and 29 August 2008, crisis-period dated 1 September 2008 and 30 June 2009 and the post-crisis or the recovery phase being 1 July 2009 through 30 September 2015. The tracking ability results across all the sub-periods suggested that, on average, ETFs yields daily returns which closely resemble that of their <b>target</b> <b>benchmark</b> indices but with relatively high level of volatility. With regard to the tracking error as another tracking ability measurement, it was discovered that the ETFs under review were inadequately replicating the movements of their <b>target</b> <b>benchmark</b> indices irrespective of the economic cycle. In tandem with the evidence documented by Mateus and Rahmani (2014) from the London Stock Exchange (LSE), tracking errors were substantially high during the 2008 global financial crisis as opposed to the prior and the post crisis period. Across all the examination periods, sizeable amount of tracking error was found to be associated to the ETFs which mimics the international broad-market access underlying indices. Amongst other things, the diversity of these indices as well as the trading hours overlap between the JSE and their host market were found to be the key attributing factors. On the contrary, ETFs which replicates most liquid <b>target</b> <b>benchmark</b> indices such as the FTSE/JSE Top 40 index appeared to have lower tracking error on relative basis. In this regard, the liquidity of the FTSE/JSE Top 40 index proved to be the main attribute. Apart from the diversity or the liquidity of indices, the length of the examination period also had a significant influence towards the magnitude of tracking errors. In this instance, shorter examination period were found to be characterised by noise or volatility in the market which makes it difficult for the ETFs providers to promptly rebalance their portfolios and align them to their <b>target</b> <b>benchmark</b> indices. Over and above these factors, this research discovered that tracking errors across all the sub-periods were largely driven by management fees and daily volatility of the ETFs market prices, more especially during the crisis period. On the one hand, trading volume and the effect of dividends distribution had a negative influence towards the magnitude of tracking errors. On the question of how efficient these 19 ETFs are, the empirical findings revealed that significant deviation between the ETFs closing price and the Net Asset Value (NAV) does exist either being a discount or premium. In line with the prior work on the JSE by Charteris (2013), ETFs which mimics local based indices were found to be trading mostly on a discount to the NAV whilst the opposite was true {{in the case of the}} international broad-market access ETFs. At the same token, international broad-market access ETFs portrayed sizeable amount of premiums across all the cycles of the economy. In line with the analysis of tracking errors, such enormous premiums were mainly driven by lack of synchronicity in the trading hours between the JSE and host market wherein these ETFs <b>target</b> <b>benchmark</b> indices are listed. Empirical literature suggests that ETFs that exhibit discount and premium which fails to persist for more than one trading day are deemed to be efficiently priced since there is limited opportunity to arbitrage. On that note, this research found that mispricing of ETFs which mimics most liquid indices such as the domestic broad-market access and sectorial indices disappears within a period of one trading day. For that reason, majority of these ETFs were considered to be efficiently priced against their NAV. Contrarily, discounts and premiums exhibited by ETFs which mostly replicate style based and the international broad-market access indices appeared to be persistent even to the fifth trading day. From the attribution point of view, the complexity of these ETFs underlying indices as well as the trading hours overlap between the JSE and the host market of these indices were found to be the main drivers of such level of mispricing. In addition to that, attribution analysis through linear regression proved that transaction cost (bid-ask spread), daily volatility of the ETFs market prices as well as the impact of trading volume had a positive influence towards the existence of discounts and premiums observed across all sub-periods...|$|R
40|$|The goal of {{this project}} was to {{evaluate}} a hospital acquired pressure ulcer (HAPU) prevention program implemented in a New York City hospital in 2012. The program objective was to encourage collaboration of team members to prevent HAPUs {{in order to reduce}} prevalence rates to national <b>target</b> <b>benchmarks.</b> The program provided caregivers information about epidemiology, etiology, prevention, and treatment of HAPUs, and presented HAPU prevention as a collaborative process in order to foster greater awareness in each participant 2 ̆ 7 s role as an agent of change by emphasizing the importance of communication between members of the health care team. This project evaluated that program by exploring changes in the incidence of HAPUs following implementation of the HAPU prevention program. This study was retrospective in nature and used a backdated analysis of archival data collected as a separate-sample, pretestâ??posttest, and quasi-experimental design to assess the relationship of the frequency of HAPUs to the implementation of a skin safety program. The data collected was between July 2012 and December 2013 from 2 medical/surgical units in a metropolitan hospital in New York City. Data were analyzed using descriptive statistics and t tests for independent samples. Incidence of HAPUs fell on both units, with t tests demonstrating statistically significant differences and large effect sizes on both units, suggesting clinical and practical significance of the findings. While this project does not establish improved HAPU incidence as a direct consequence of the skin health education program, findings of the project provide insight for hospital leaders in their efforts to reduce HAPU rates. Results of the project suggest a HAPU prevention program emphasizing development of knowledge and skills as well as the promotion of collaboration between health care team members may be effective in reducing HAPU incidence rates. This project also provides a low cost educational option to reduce healthcare disparities and promote positive social change. Further research in similar contexts is recommended for future study...|$|E
40|$|International audienceCurrent {{processors}} {{have gone}} through multiple internal opti- mization to speed-up the average execution time e. g. pipelines, branch prediction. Besides, internal communication mechanisms and shared resources like caches or buses have a sig- nificant impact on Worst-Case Execution Times (WCETs). Having an accurate estimate of a WCET is now a challenge. Probabilistic approaches provide {{a viable alternative to}} single WCET estimation. They consider WCET as a probabilistic distribution associated to uncertainty or risk. In this paper, we present synthetic benchmarks and associated analysis for several LEON 3 configurations on FPGA <b>targets.</b> <b>Benchmarking</b> exposes key parameters to execution time variability allowing for accurate probabilistic modeling of system dynamics. We analyze the impact of architecture- level configurations on average and worst-case behaviors...|$|R
40|$|In {{an article}} {{prepared}} for New Directions for Philanthropic Fundraising, Richard Steinberg and Mark Wilhelm summarize {{the type of}} data collected in COPPS and provide at least four areas of fundraising practice that can be improved with knowledge gained about donors from COPPS: <b>targeting,</b> predicting, <b>benchmarking,</b> and persuasion...|$|R
50|$|Key {{performance}} indicators (KPI) are essential {{parts of a}} measurable objective, which {{is made up of}} a direction, KPIs, <b>benchmark,</b> <b>target,</b> and time frame.|$|R
40|$|This paper {{presents}} {{our research}} progress for improving write performance of ZABBIX with a NoSQL database for storing histories whose write frequency is increased {{as the number}} of monitoring <b>targets.</b> The <b>benchmark</b> results shows that write performance with our proposed method is increased by approximately three times compared to that with the original. ...|$|R
40|$|By {{their very}} nature, target date funds are complex investments. This {{complexity}} makes them especially challenging to benchmark, with each fund containing unique attributes and features that ideally {{should be addressed}} individually based on the unique goals and situation of the potential investor. In our first article related to this topic, “Selecting a <b>Target</b> Date <b>Benchmark,</b> ” we assume...|$|R
40|$|We use best {{practice}} benchmarking rationales {{to propose a}} dynamic research design that accounts for the endogenous components of across-firms heterogeneous routines to study changes in performance and their link to organizational knowledge investments. We thus contribute to the operationalization of management theoretical frameworks based on resources and routines. The research design employs frontier measures that provide industry-level benchmarking in organizational settings, and proposes some new indicators for firm-level strategic benchmarking. A profit-oriented analysis of the U. S. technology industry during 2000 - 2011 illustrates the usefulness of our design. Findings reveal that industry revival following economic distress comes along with wider gaps between best and worst performers. Second stage analyses show that increasing intangibles stocks is positively associated with fixed <b>target</b> <b>benchmarking,</b> while enhancing R&D spending is linked to local frontier progress. The discussion develops managerial interpretations of the benchmarking measures that are suitable for control mechanisms and reward systems...|$|R
30|$|However, for our analysis, {{the basic}} form of system of {{equations}} of both the demand-driven and supply-driven models have to be modified to make the gross outputs of ‘food grains’ as exogenous in our system. The output figures in both Leontief and Ghosh model are treated as endogenous to the system whereas the demand figures are always considered exogenous. To consider food grains demand and supply fixed, we consider output of food grain as exogenous to the system. We develop a modified I-O framework for analysing resource mobilization issues to sustain long-term development goal in an economy. The {{system of equations}} has been modified accordingly to incorporate exogenous output figure of food grains. We are able to estimate the price impact due the implementation of NFSA using Ghosh model. Our study would primarily remain focused on measuring impact on {{rest of the economy}} so that the <b>target</b> (<b>benchmark)</b> production of food grain as per scenarios could be achieved. The detailed structure of the methodology is given below.|$|R
40|$|The {{systematic}} {{evaluation of}} program analyses {{as well as}} software-engineering tools requires benchmark suites that are representative of real-world projects in the domains for which the tools or analyses are designed. Such benchmarks currently only exist for a few research areas and even where they exist, they are often not effectively maintained, due to the required manual effort. This makes evaluating new analyses and tools on software that relies on current technologies often impossible. We describe ABM, a methodology to semi-automatically mine software repositories to extract up-to-date and representative sets of applications belonging to specific domains. The proposed methodology facilitates the creation of such collections and {{makes it easier to}} release updated versions of a benchmark suite. Resulting from an instantiation of the methodology, we present a collection of current real-world Java business web applications. The collection and methodology serve {{as a starting point for}} creating current, <b>targeted</b> <b>benchmark</b> suites, and thus helps to better evaluate current program-analysis and software-engineering tools...|$|R
40|$|International audienceIn this chapter, {{we propose}} methods for {{correcting}} gate-level designs by identifying appropriate logic functions for internal gates. We introduce programmable circuits, such as look up table (LUT) and multiplexer (MUX) to the circuits under debugging, {{in order to}} formulate the correction processes mathematically. There are two steps in the proposed methods. The first one is to identify sets of gates and their appropriate inputs whose functions are to be modified. The second one is to actually identify logic functions for the correction by solving QBF (Quantified Boolean Formula) problems with repeated application of SAT solvers. There {{are a number of}} bugs which cannot be corrected unless the inputs of the gates to be modified are changed from the original ones, and the selection of such additional inputs is a key for effective debugging. We show a couple of methods by which appropriate inputs to the gates can be efficiently identified. Experimental results for each such a method as well as their combinations <b>targeting</b> <b>benchmark</b> circuits as well as industrial ones are shown...|$|R
40|$|BACKGROUND: The manifestations {{associated}} with non-survival after multiple trauma may vary importantly between countries and institutions. The {{aim of the}} present study was to assess the quality of performance by comparing actual mortality rates to the literature. METHODS: The study involved evaluation of a prospective consecutive multiple trauma cohort (injury severity score, ISS 0. 001 each) functioned as major independent prognostic parameters of both 24 h and 30 -day mortality. Various TRISS versions hardly differed in their precision (area under the curve [AUC] 0. 83 - 0. 84), but they did differ considerably in their level of requirement, with the TRISS using newer National Trauma Data Bank coefficients (NTDB-TRISS) offering the highest <b>target</b> <b>benchmark</b> (predicted mortality 13 %, Z value - 5. 7) in the prediction of 30 -day mortality. CONCLUSIONS: Because of the current lack of a single, internationally accepted scoring system for the prediction of mortality after multiple trauma, the comparison of outcomes between medical centers remains unreliable. To achieve effective quality control, a practical benchmarking model, such as the TRISS-NTDB, should be used worldwide...|$|R
40|$|Active {{portfolio}} management {{is concerned with}} objectives related to the outperformance of the return of a <b>target</b> <b>benchmark</b> portfolio. In this paper, we consider a dynamic active {{portfolio management}} problem where the objective {{is related to the}} tradeoff between the achievement of performance goals and the risk of a shortfall. Specifically, we consider an objective that relates the probability of achieving a given performance objective to {{the time it takes to}} achieve the objective. This allows a new direct quantitative analysis of the risk/return tradeoff, with risk defined directly in terms of probability of shortfall relative to the benchmark, and return defined in terms of the expected time to reach investment goals relative to the benchmark. The resulting optimal policy is a state-dependent policy that provides new insights. As a special case, our analysis includes the case where the investor wants to minimize the expected time until a given performance goal is reached subject to a constraint on the shortfall probability. portfolio theory, benchmarking, active portfolio management, stochastic control...|$|R
