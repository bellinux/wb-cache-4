4234|10000|Public
5|$|MacDougall stated his {{experiment}} {{would have}} to be repeated many times before any conclusion could be obtained. The experiment is widely regarded as flawed and unscientific due to <b>the</b> <b>small</b> <b>sample</b> size, the methods used, as well as the fact only one of the six subjects met the hypothesis. The case has been cited as an example of selective reporting. Despite its rejection within the scientific community, MacDougall's experiment popularized the concept that the soul has weight, and specifically that it weighs 21 grams.|$|E
25|$|Edgar Jones and Simon Wessely {{have argued}} that <b>the</b> <b>small</b> <b>sample</b> size and the single {{location}} studied limit {{the validity of the}} validation study.|$|E
25|$|A study {{performed}} by Maclean and colleagues introduced pure vaporized salvinorin-A to four subjects demonstrated a {{relative lack of}} dysphoric effects and increased presence of dose-related positive effects that contrasts with previous research with kappa agonists such as enadoline, as well as Salvia Divinorum itself. However, this {{could be due to}} <b>the</b> <b>small</b> <b>sample</b> size, the relative psychedelic experience of the subjects, or the route of administration.|$|E
30|$|The one {{exception}} (1.5 times) has much shorter texts and <b>smaller</b> <b>samples.</b> Hence <b>the</b> lack of contrast {{may be due}} to <b>the</b> <b>small</b> <b>samples,</b> and thus it can be ignored.|$|R
5000|$|The {{second example}} is for unequal variances ( [...] , [...] ) and unequal sample sizes ( [...] , [...] ). <b>The</b> <b>smaller</b> <b>sample</b> has <b>the</b> larger variance: ...|$|R
30|$|The {{algorithms}} {{proposed in}} this paper consist of four modules: (1) the higher layer feature extraction module based on the SAE model in <b>the</b> target-domain <b>small</b> <b>sample</b> target object, (2) the transfer learning SAE local feature extracting module in the cross-domain big data image database, (3) the global feature extracting module in <b>the</b> target-domain <b>small</b> <b>sample</b> target object images based on the CNN model, and (4) the classification module in <b>the</b> target-domain <b>small</b> <b>sample</b> target object image.|$|R
25|$|However, {{there is}} not enough {{evidence}} to say whether or not women with twin pregnancies should be given oral betamimetics {{to reduce the risk of}} preterm birth. In some studies betamimetics have reduced the rate of preterm labour in twin pregnancies however the studies are too small to draw any solid conclusions. Likewise, putting a stitch in the neck of the womb (a cervical suture) to prevent premature birth has not been shown to work in women carrying more than one baby due to <b>the</b> <b>small</b> <b>sample</b> sizes in the studies.|$|E
25|$|In {{statistic}}s, the Durbin–Watson statistic is a {{test statistic}} used to detect the presence of autocorrelation (a relationship between values separated from each other by a given time lag) in the residuals (prediction errors) from a regression analysis. It is named after James Durbin and Geoffrey Watson. <b>The</b> <b>small</b> <b>sample</b> distribution of this ratio was derived by John von Neumann (von Neumann, 1941). Durbin and Watson (1950, 1951) applied this statistic to the residuals from least squares regressions, and developed bounds tests for the null hypothesis that the errors are serially uncorrelated against the alternative that they follow a first order autoregressive process. Later, John Denis Sargan and Alok Bhargava developed several von Neumann–Durbin–Watson type test statistics for the null hypothesis that the errors on a regression model follow a process with a unit root against the alternative hypothesis that the errors follow a stationary first order autoregression (Sargan and Bhargava, 1983). Note that the distribution of this test statistic {{does not depend on}} the estimated regression coefficients and the variance of the errors.|$|E
25|$|In {{the first}} {{experiments}} with laser heating, temperature {{came from a}} calibration of laser power made with known melting points of various materials. When using the pulsed ruby laser this was unreliable due to the short pulse. YAG lasers quickly become the standard, heating for relatively long duration, and allowing observation of the sample throughout the heating process. It was with the first use of YAG lasers that Bassett used an optical pyrometer to measure temperatures {{in the range of}} 1000°C to 1600°C. The first temperature measurements had a standard deviation of 30°C from the brightness temperature, but due to <b>the</b> <b>small</b> <b>sample</b> size was estimated to be 50°C with the possibility that the true temperature of the sample being was 200°C higher than that of the brightness measurement. Spectrometry of the incandescent light became the next method of temperature measurement used in Bassett's group. The energy of the emitted radiation could be compared to known black body radiation spectra to derive a temperature. Calibration of these systems is done with published melting points or melting points as measured by resistive heating.|$|E
40|$|International audienceIn this paper, {{we study}} the {{stability}} of sampled-data systems. A combined continuous-time and discrete-time approach is proposed, by adopting a switched system technique. Depending on the sampling period value, we treat {{the system as a}} switched system consisting of two subsystems. The two subsystems correspond to <b>the</b> <b>small</b> <b>sampling</b> case and <b>the</b> large sampling case, respectively. First, <b>the</b> <b>small</b> <b>sampling</b> case is studied by using the Razumikhin technique in the continuous-time framework. A condition is given to guarantee that the Lyapunov function decreases at the <b>sampling</b> instants, for <b>the</b> <b>small</b> <b>sampling</b> case. This Razumikhin-based result can be combined with the existing discrete-time methods so that the whole stability interval (<b>small</b> and large <b>sampling</b> parts) can be verified. This criterion is necessary and sufficient to guarantee the quadratic stability so that the maximum quadratic stability interval can be obtained. This combined continuous-time and discrete-time approach has two advantages over the existing approaches: 1) it can lead to a larger stability interval than those derived by the conventional continuous-time and discrete-time approaches; 2) it may reduce the computational complexity...|$|R
40|$|Classic maximum entropy {{quantile}} function method (CMEQFM) {{based on}} the probability weighted moments (PWMs) can accurately estimate the quantile function of random variable on <b>small</b> <b>samples,</b> but inaccurately on <b>the</b> very <b>small</b> <b>samples.</b> To overcome this weakness, least square maximum entropy quantile function method (LSMEQFM) and that with constraint condition (LSMEQFMCC) are proposed. To improve the confidence level of quantile function estimation, scatter factor method is combined with maximum entropy method to estimate the confidence interval of quantile function. From the comparisons of these methods about two common probability distributions and one engineering application, it is showed that CMEQFM can estimate the quantile function accurately on <b>the</b> <b>small</b> <b>samples</b> but inaccurately on <b>the</b> very <b>small</b> <b>samples</b> (10 samples); LSMEQFM and LSMEQFMCC can be successfully applied to <b>the</b> very <b>small</b> samples; with consideration of the constraint condition on quantile function, LSMEQFMCC is more stable and computationally accurate than LSMEQFM; scatter factor confidence interval estimation method based on LSMEQFM or LSMEQFMCC has good estimation accuracy on the confidence interval of quantile function, and that based on LSMEQFMCC is the most stable and accurate method on <b>the</b> very <b>small</b> <b>samples</b> (10 samples) ...|$|R
40|$|At present, {{infrared}} microspectrophotometry can {{be defined}} as the study of infrared absorption intensities for <b>samples</b> <b>smaller</b> than <b>the</b> mechanical slit of the spectrometer. This definition arises from the fact that <b>the</b> <b>smallest</b> <b>sample</b> that can be studied by a spectrophotometer without auxiliary optic...|$|R
25|$|In {{the wake}} of the lockout which took away much of the 2011 NBA offseason and {{shortened}} the league's schedule considerably, the Warriors made a push to acquire restricted free agent DeAndre Jordan, who would have automatically taken over as the starting center for Golden State. Biedriņš might have been a logical amnesty candidate under the provisions of the NBA's new collective bargaining agreement, but the Warriors instead chose to amnesty Charlie Bell while retaining Biedriņš. The Warriors' push to acquire Jordan fell through when the Los Angeles Clippers matched the Warriors' offer sheet to Jordan, so they instead signed free agent center Kwame Brown. Biedriņš won the starting job to begin the year before being sidelined for three games with an ankle injury. When Brown was lost for the season with a torn pectoralis major muscle in the third of those games, Biedriņš once again took over as starter, where he remained until his continued ineffectiveness prompted first-year coach Mark Jackson to insert Ekpe Udoh in as starter. A March 13 trade which shipped Udoh, Brown and Monta Ellis to the Milwaukee Bucks for Stephen Jackson and the out-for-the-season Andrew Bogut thrust Biedriņš back into the starting lineup, where he remained until Jackson decided to give rookie Jeremy Tyler a chance to prove himself at the beginning of April. For the season, Biedriņš averaged 15.7 minutes (lowest since his second year in the league), 1.7 points, 3.7 rebounds, 0.3 assists (all career lows), 1.0 blocks and 0.5 steals while shooting 60.9% from the floor and 11.1% from the free throw line (admittedly with <b>the</b> <b>small</b> <b>sample</b> size of one make in nine free throw attempts).|$|E
500|$|Two small Akodon {{collected}} in 1993 in Tucumán Province, northwestern Argentina, {{were given the}} name Akodon diminutus in 1994, but that name is a nomen nudum and therefore not available for use under the International Code of Zoological Nomenclature. In 1999, Mónica Díaz and others described these animals more fully as a new species, Akodon aliquantulus, which they considered closely related to A.puer caenosus. The specific name means [...] "how little" [...] or [...] "how few" [...] in Latin and refers to {{the small size of}} the species and <b>the</b> <b>small</b> <b>sample</b> Díaz and colleagues could use. In the 2005 third edition of Mammal Species of the World, Guy Musser and Michael Carleton termed the differentiation between A.aliquantulus and A.lutescens (=puer) [...] "unimpressive" [...] and recommended further taxonomic research. Common names proposed for A.aliquantulus include [...] "Diminutive Akodont" [...] and [...] "Tucumán Grass Mouse".|$|E
500|$|MacDougall's {{experiment}} {{has been}} the subject of considerable skepticism, and he has been accused of both flawed methods and outright fraud in obtaining his results. Noting that only one of the six patients measured supported the hypothesis, Karl Kruszelnicki has stated the experiment is a case of selective reporting, as MacDougall ignored the majority of the results. Kruszelnicki also criticized <b>the</b> <b>small</b> <b>sample</b> size, and questioned how MacDougall was able to determine the exact moment when a person had died considering the technology available in 1907. In 2008 physicist Robert L. Park wrote that MacDougall's experiments [...] "are not regarded today as having any scientific merit", and psychologist Bruce Hood wrote that [...] "because the weight loss was not reliable or replicable, his findings were unscientific". Professor Richard Wiseman said that within the scientific community, the experiment is confined to a [...] "large pile of scientific curiosities labelled 'almost certainly not true'".|$|E
30|$|We run the IPWRA {{procedure}} separately {{by country}} and gender for any outcome we consider. This strategy yields less precise estimates, because of <b>the</b> <b>smaller</b> <b>sample</b> size, but {{a better fit}} and more reliable estimates of the probability of treatment.|$|R
40|$|The cendif module {{is part of}} the somersd package, and calculates {{confidence}} intervals for the Hodges–Lehmann median difference between values of a variable in two subpopulations. The traditional Lehmann formula, unlike the formula used by cendif, assumes that the two subpopulation distributions are different only in location, and that the subpopulations are therefore equally variable. The cendif formula therefore contrasts with the Lehmann formula as the unequal-variance t-test contrasts with the equal-variance t-test. In a simulation study, designed to test cendif to destruction, the performance of cendif was compared to that of the Lehmann formula, using coverage probabilities and median confidence interval width ratios. The simulations involved sampling from pairs of Normal or Cauchy distributions, with subsample sizes ranging from 5 to 40, and between-subpopulation variability scale ratios ranging from 1 to 4. If the sample numbers were equal, then both methods gave coverage probabilities close to the advertized confidence level. However, if the sample numbers were unequal, then the Lehmann coverage probabilities were over-conservative if <b>the</b> <b>smaller</b> <b>sample</b> was from <b>the</b> less variable population, and over-liberal if <b>the</b> <b>smaller</b> <b>sample</b> was from <b>the</b> more variable population. The cendif coverage probability was usually closer to the advertized level, if <b>the</b> <b>smaller</b> <b>sample</b> was not very <b>small.</b> However, if <b>the</b> <b>sample</b> sizes were 5 and 40, and the two populations were equally variable, then the Lehmann coverage probability was close to its advertised level, while the cendif coverage probability was over-liberal. The cendif confidence interval, in its present form, is therefore robust both to non-Normality and to unequal variablity, but may be less robust to the possibility that <b>the</b> <b>smaller</b> <b>sample</b> size is very small. Possibilities for improvement are discussed. ...|$|R
40|$|Because it is {{difficult}} and complex to determine the probability distribution of small samples，it is improper to use traditional probability theory to process parameter estimation for <b>small</b> <b>samples.</b> Bayes Bootstrap method is always used in the project. Although，the Bayes Bootstrap method has its own limitation，In this article an improvement {{is given to the}} Bayes Bootstrap method，This method extended the amount of samples by numerical simulation without changing the circumstances in a <b>small</b> <b>sample</b> of <b>the</b> original sample. And the new method can give the accurate interval estimation for <b>the</b> <b>small</b> <b>samples.</b> Finally，by using <b>the</b> Monte Carlo simulation to model simulation to <b>the</b> specific <b>small</b> <b>sample</b> problems. <b>The</b> effectiveness and practicability of the Improved-Bootstrap method was proved...|$|R
2500|$|... a = 2: N'a = [...] 1/Simpson's index (without <b>the</b> <b>small</b> <b>sample</b> correction) ...|$|E
2500|$|HPLC is {{distinguished}} from traditional ("low pressure") liquid chromatography because operational pressures are significantly higher (50–350 bar), while ordinary liquid chromatography typically {{relies on the}} force of gravity to pass the mobile phase through the column. Due to <b>the</b> <b>small</b> <b>sample</b> amount separated in analytical HPLC, typical column dimensions are 2.1–4.6mm diameter, and 30–250mm length. [...] Also HPLC columns are made with smaller sorbent particles (2–50 μm in average particle size). [...] This gives HPLC superior resolving power (the ability to distinguish between compounds) when separating mixtures, which makes it a popular chromatographic technique.|$|E
2500|$|The {{contrast}} between the genetic profiles of the Hokkaido Jōmon skeletons and the modern Ainu illustrates another uncertainty in source models derived from modern DNA samples:However, probably due to <b>the</b> <b>small</b> <b>sample</b> size or close consanguinity {{among the members of}} the site, the frequencies of the haplogroups in Funadomari skeletons were quite different from any modern populations, including Hokkaido Ainu, who have been regarded as the direct descendant of the Hokkaido Jomon people. [...] The descendants of source populations with the closest relationship to the genetic profile from the time when differentiation occurred are not obvious. Source population models can be expected to become more robust as more results are compiled, the heritage of modern proxy candidates becomes better understood, and fossil DNA in the regions of interest is found and considered.|$|E
25|$|The {{error in}} this {{approximation}} decays quadratically (as 1/N2), {{and it is}} suited for all but <b>the</b> <b>smallest</b> <b>samples</b> or highest precision: for n = 3 the bias is equal to 1.3%, and for n = 9 the bias is already less than 0.1%.|$|R
3000|$|... (r), {{including}} the two samples {{in the same}} order. If samples are not balanced, we say {{that the size of}} <b>the</b> <b>smallest</b> <b>sample</b> is n {{and the size of the}} largest is m. The sum of the ranks of the two samples is [...]...|$|R
30|$|E sample with 345 and 18 values, respectively, in each case. As will {{be shown}} in the following, the large samples are used for the {{elaboration}} of our statistical model and, by means of <b>the</b> <b>small</b> <b>samples,</b> <b>the</b> suitability of our approach is demonstrated.|$|R
2500|$|ABA-based {{techniques}} {{are often used}} to change behaviors associated with autism, {{so much so that}} ABA itself is often mistakenly considered to be synonymous with therapy for autism. ABA for autism may be limited by diagnostic severity and IQ. The most influential and widely cited review of the literature regarding efficacy of treatments for autism is the National Research Council's book Educating Children with Autism (2001) which concluded that ABA was the best research supported and most effective treatment for the main characteristics of autism. Some critics claimed that the NRC's report was an inside job by behavior analysts [...] but there were no board certified behavior analysts on the panel (which did include physicians, speech pathologists, educators, psychologists, and others). Other criticisms raised include <b>the</b> <b>small</b> <b>sample</b> sizes used in the published research to date. Medications have not been proven to correct the core deficits of ASDs and are not the primary treatment. [...] ABA is the primary treatment according to the American Academy of Pediatrics. Recent reviews of the efficacy of ABA-based techniques in autism include: ...|$|E
2500|$|Another {{section of}} {{scholars}} {{believe that the}} Satavahanas originated in western Deccan (present-day Maharashtra). All of the four extant inscriptions from the early Satavahana period (c. 1st century BCE) {{have been found in}} and around this region. The oldest known Satavahana inscription was found at [...] of the Pandavleni Caves in Nashik district, and was issued during the reign of Kanha (100-70 BCE). An inscription found at Naneghat was issued by Nayanika (or Naganika), the widow of Satakarni I; another inscription found at Naneghat has been dated to the same period on paleographic basis. A slightly later inscription dated to the reign of Satakarni II has been found at Sanchi in Madhya Pradesh, located to the north of Maharashtra. The majority of the other Satavahana inscriptions have also been found in western Deccan. On the other hand, the epigraphic evidence from eastern Deccan does not mention the Satavahanas before the 4th century CE. At Nevasa, a seal and coins attributed to Kanha have been discovered. Coins attributed to Satakarni I have also been discovered at Nashik, Nevasa and Pauni in Maharashtra (besides places in eastern Deccan and present-day Madhya Pradesh). Based on these evidences, some historians argue that the Satavahanas initially came to power in the area around their capital Pratishthana (modern Paithan, Maharashtra) and then expanded their territory to eastern Deccan. Carla Sinopoli cautions that the inference about the western Deccan origin of the Satavahanas is [...] "tentative at best" [...] given <b>the</b> <b>small</b> <b>sample</b> of early inscriptions.|$|E
5000|$|... a = 2: Na = 1/Simpson's index (without <b>the</b> <b>small</b> <b>sample</b> correction) ...|$|E
50|$|For unequal {{variance}}s, Student's t-test gave a low p-value when <b>the</b> <b>smaller</b> <b>sample</b> had {{a larger}} variance (Example 2) {{and a high}} p-value when the larger sample had a larger variance (Example 3). For unequal variances, Welch's t-test gave p-values close to simulated p-values.|$|R
50|$|The {{error in}} this {{approximation}} decays quadratically (as 1/N2), {{and it is}} suited for all but <b>the</b> <b>smallest</b> <b>samples</b> or highest precision: for n = 3 the bias is equal to 1.3%, and for n = 9 the bias is already less than 0.1%.|$|R
40|$|AbstractThe gas-atomized Al-Si {{alloy powder}} with {{different}} particle sizes {{was subjected to}} isothermal annealing for understanding the effect of solidification rate on precipitation and growth Si crystals. The results show that Si precipitates grew more quickly in <b>the</b> <b>small</b> <b>samples</b> with a large solidification rate due to high interfacial energy. Moreover, these Si crystals {{had a tendency to}} form a quasi-spherical shape after annealing at a low temperature or for a short holding time. Coarsening of the Si precipitates during annealing was examined using a LSW equation. Thermal stability of the rapidly solidified alloy was significantly influenced by its original microstructure as a result of high solidification rate. Furthermore, more serious clustering of Si-Si phase was also observed in <b>the</b> <b>small</b> <b>samples,</b> attributed to <b>the</b> rapid coarsening of the Si phases...|$|R
50|$|Edgar Jones and Simon Wessely {{have argued}} that <b>the</b> <b>small</b> <b>sample</b> size and the single {{location}} studied limit {{the validity of the}} validation study.|$|E
5000|$|Hendry, D.F. and R.W. Harrison (1974). [...] "Monte Carlo {{methodology}} and <b>the</b> <b>small</b> <b>sample</b> behaviour of ordinary and two-stage least squares." [...] Journal of Econometrics, 2, 151-174.|$|E
50|$|Although some {{positive}} trends {{were observed in}} the exploratory exposure response analyses, given <b>the</b> <b>small</b> <b>sample</b> size, caution {{should be used in}} the interpretation of the exposure-response analyses.|$|E
50|$|With such a <b>small</b> <b>sample</b> size, if {{one wants}} at least 95% confidence, one {{is reduced to}} saying that the median is between the minimum and the maximum of the 6 {{observations}} with probability 31/32 or approximately 97%. Size 6 is, in fact, <b>the</b> <b>smallest</b> <b>sample</b> size such that the interval determined by the minimum and the maximum {{is at least a}} 95% confidence interval for the population median.|$|R
3000|$|Step 2. Summing {{the ranks}} of <b>the</b> <b>smaller</b> <b>sample</b> (U). This amount is already {{the result of the}} test. In fact, the more the sum of {{the ranks of}} <b>the</b> <b>smaller</b> <b>sample</b> differs from <b>the</b> {{expected}} one, the more the probability that the two samples are random assortments of ranks loses consistency. The verification in the table will tell us whether to keep or reject the null hypothesis beyond the different critical thresholds of significance. For each level of significance, two extreme values are inscribed in the table: a very small and a very large one. The significance is reached if U is <b>smaller</b> than <b>the</b> <b>smallest</b> value or higher than the highest value in the table. If the samples are large enough, it is possible to do without the table and exploit the fact that U tends to be normally distributed around the expected value of the null hypothesis: [...]...|$|R
30|$|The {{limitations}} {{of the current study}} are as follows: First, we did not include non-CCRCC, such as papillary RCC and homophobic RCC, because their different cellular pathological types may influence diffusion. Second, we did not evaluate the renal tumors < 15  mm, therefore, <b>the</b> <b>smaller</b> <b>sample</b> size might have affected the results.|$|R
