586|636|Public
2500|$|In 2008, Philippe Hanrigou (then at ThoughtWorks) made [...] "Selenium Grid", which {{provides}} a hub allowing the running of multiple Selenium tests concurrently on any number of local or remote systems, thus minimizing <b>test</b> <b>execution</b> time. Grid offered, as open source, a similar capability to the internal/private Google cloud for Selenium RC. Pat Lightbody had already made a private cloud for [...] "HostedQA" [...] which {{he went on to}} sell to Gomez, Inc.|$|E
5000|$|Multilingual tests (change the {{language}} during <b>test</b> <b>execution)</b> ...|$|E
5000|$|Automatic <b>test</b> <b>execution</b> and {{evaluation}} (assessment) in a Test campaign ...|$|E
30|$|Finally, we {{performed}} a cost analysis, to compare the expenses for monitoring <b>tests</b> <b>execution</b> and heparin infusion between the study groups.|$|R
30|$|Since in TDD {{the tests}} are {{executed}} frequently, throughout the development process the <b>test</b> <b>executions</b> should achieve good code coverage. This {{is reinforced by the}} fact that the tests that have failed previously are always executed again, enabling the debugging for a specific test scenario.|$|R
30|$|After {{years of}} development, fuzzing {{has become more}} fine-grained, {{flexible}} and smarter than ever. Feedback-driven fuzzing provides an efficient way of guided testing, traditional and new techniques play roles of sensors to gain various information during the <b>testing</b> <b>execution</b> and make the fuzzing guided accurately.|$|R
5000|$|Test {{planning}} through {{test set}} configuration and <b>test</b> <b>execution</b> configuration ...|$|E
50|$|The {{difference}} between the concept of <b>test</b> <b>execution</b> engine and operation system is that the <b>test</b> <b>execution</b> engine monitors, presents and stores the status, results, time stamp, length and other information for every Test Step of a Test Sequence, but typically an operation system does not perform such profiling of a software execution.|$|E
5000|$|An {{advanced}} <b>test</b> <b>execution</b> engine {{may have}} additional functions, such as: ...|$|E
40|$|We {{present a}} method to enhance fault {{localization}} for software systems based on a frequent pattern mining algorithm. Our method {{is based on a}} large set of test cases for a given set of programs in which faults can be detected. The <b>test</b> <b>executions</b> are recorded as function call trees. Based on test oracles the tests can be classified into successful and failing tests. A frequent pattern mining algorithm is used to identify frequent subtrees in successful and failing <b>test</b> <b>executions.</b> This information is used to rank functions according to their likelihood of containing a fault. The ranking suggests an order in which to examine the functions during fault analysis. We validate our approach experimentally using a subset of Siemens benchmark programs. ...|$|R
40|$|During {{software}} development and maintenance stages, programmers have to frequently debug the software. One {{of the most}} difficult and complex tasks in the debugging activity is software fault localization. A commonly-used method to fix software fault is computing suspiciousness of program elements according to failed <b>test</b> <b>executions</b> and passed <b>test</b> <b>executions.</b> However, this technique does not give full consideration to dependences between program elements, thus its capacity for efficient fault localization is limited. Our research intends to introduce program slicing technique and statistical method which extracts dependencies between program elements and refines execution history, then builds program slicing spectra to rank suspicious elements by a statistical metric. We expect that our method will contribute directly to the improvement of the effectiveness and the accuracy of software fault localization and reduce the {{software development}} and maintenance effort and cost. © 2012 IEEE. IEEE Computer Society; ACM; University of Zurich (UZH), Department of Informatics; Technical Council on Software Engineering (TCSE); Special Interest Group on Software Engineering (SIGSOFT); SI-SEDuring software development and maintenance stages, programmers have to frequently debug the software. One {{of the most difficult}} and complex tasks in the debugging activity is software fault localization. A commonly-used method to fix software fault is computing suspiciousness of program elements according to failed <b>test</b> <b>executions</b> and passed <b>test</b> <b>executions.</b> However, this technique does not give full consideration to dependences between program elements, thus its capacity for efficient fault localization is limited. Our research intends to introduce program slicing technique and statistical method which extracts dependencies between program elements and refines execution history, then builds program slicing spectra to rank suspicious elements by a statistical metric. We expect that our method will contribute directly to the improvement of the effectiveness and the accuracy of software fault localization and reduce the software development and maintenance effort and cost. © 2012 IEEE...|$|R
5000|$|Manageability - {{effective}} <b>test</b> design, <b>execution,</b> and traceability ...|$|R
5000|$|Cross-browser <b>test</b> <b>execution</b> - Internet Explorer, Firefox, Chrome and Safari (web browser) ...|$|E
50|$|Test {{specification}} is selected, {{loaded and}} {{executed by the}} <b>test</b> <b>execution</b> engine similarly, as application software is selected, loaded and executed by operation systems. The <b>test</b> <b>execution</b> engine should not operate on the tested object directly, but though plug-in modules similarly as an application software accesses devices through drivers which are installed on the operation system.|$|E
5000|$|Grid {{benchmarking}} {{and comparison}} of <b>test</b> <b>execution</b> days reduces analysis and review effort.|$|E
40|$|We use {{output from}} dynamic {{analysis}} to assist theorem-proving of safety properties of distributed algorithms. The algorithms {{are written in}} the IOA language, {{which is based on}} the mathematical I/O automaton model. Daikon, a dynamic invariant discovery tool, generalizes from <b>test</b> <b>executions,</b> producing assertions about the observed behavior of the algorithm...|$|R
30|$|Inside uuid, the Optimizer {{has found}} two cases (fromURN and fromBytes functions) {{where there was}} a problem {{regarding}} the absence of coverage in the tests, that is, at no time these functions are exercised during the unit <b>tests</b> <b>execution.</b> So, it was possible to remove them and still succeed in running the tests.|$|R
5000|$|The keyword-driven testing {{methodology}} divides <b>test</b> process <b>execution</b> into several stages: ...|$|R
5000|$|<b>Test</b> <b>execution</b> {{for single}} {{programs}} is possible {{from within the}} editors. Newer versions of ABAP Unit (>= SAP_BASIS 7.02) offer an integration with coverage metrics and a report to schedule automatic <b>test</b> <b>execution</b> with mail notification. ABAP Unit offers no feature to define test suites programmatically. Mass runs for entire packages can be executed with the Code Inspector integration only.|$|E
5000|$|... a {{consistency}} oracle {{that compares}} {{the results of}} one <b>test</b> <b>execution</b> to another for similarity ...|$|E
5000|$|TCI: TTCN-3 Control Interfaces is the {{interface}} {{to control the}} <b>test</b> <b>execution.</b> It is divided in: ...|$|E
5000|$|MIN {{external}} interface - {{may be used}} {{to start}} <b>test</b> case <b>execution</b> from external system ...|$|R
30|$|These results {{detailed}} in Section 1 {{demonstrate that the}} optimized cut height approach produces more accurate clusters than dynamic cut height selection, provided that suitable ground truth data is available to find the recommended heights. Furthermore, we also note that the optimized cut height approach performs more consistently, selecting the same top performing input attributes during training and <b>testing</b> <b>executions</b> on both data populations.|$|R
40|$|The goal of {{software}} testing analysis is to validate that an implementation satis es its specifications. Many errors in software {{are caused by}} generalizable aws in the source code. Property-based testing assures that a given program is free of the specified generic aws. Property-based testing uses property specifications and a dataflow analysis of the program to guide evaluation of <b>test</b> <b>executions</b> for correctness and completeness...|$|R
50|$|Exploratory {{testing means}} {{simultaneous}} test design and <b>test</b> <b>execution</b> {{with an emphasis}} on learning. Scripted testing means that learning and test design happen prior to <b>test</b> <b>execution,</b> and quite often the learning has to be done again during <b>test</b> <b>execution.</b> Exploratory testing is very common, but in most writing and training about testing it is barely mentioned and generally misunderstood. Some writers consider it a primary and essential practice. Structured exploratory testing is a compromise when the testers are familiar with the software. A vague test plan, known as a test charter, is written up, describing what functionalities need to be tested but not how, allowing the individual testers to choose the method and steps of testing.|$|E
50|$|A <b>test</b> <b>execution</b> {{engine is}} a type of {{software}} used to test software, hardware or complete systems.|$|E
5000|$|Oracle Test Manager for test process management, {{including}} {{test requirements}} management, test management, <b>test</b> <b>execution</b> and defect tracking.|$|E
40|$|Abstract: In {{software}} development and maintenance stages, programmers need to frequently debug the software. Software fault localization {{is one of}} the most exclusive, tedious and time intense activities in program debugging. A common approach to fix software error is computing suspiciousness of program elements according to failed <b>test</b> <b>executions</b> and passed <b>test</b> <b>executions.</b> However, this technique does not give full consideration to dependences between program elements and therefore it reduce the ability for efficient fault localization. Developers must identify statements involved in failures and select suspicious statements that may contain faults. Our paper presents a new technique that identify statements involved in failure –those executed by failed test cases through narrowing the search domain using Slicing Technique (Control and Data dependence slice) by slicing the program and making it more effective with the CCDD (Coupling Control and Data Dependency) approach in Value Replacement. The proposed approach is more efficient and is more accurate in locating statements that directly effect the faulty statements. This approach can also be applied to many other research areas...|$|R
50|$|The {{verdict is}} the result of a <b>test</b> case <b>execution.</b> It has 5 {{possible}} values: none, pass, inconc, fail, error.|$|R
5000|$|Iftach Ian Amit (Hebrew: יפתח איאן עמית) is an Israeli Hacker/computer {{security}} {{researcher and}} practitioner. He {{is one of}} the co-founders of the Tel Aviv DEF CON Group DC9723, the Penetration <b>Testing</b> <b>Execution</b> Standard, and presented at hacker conventions such as DEF CON, Black Hat, BlueHat, RSA Conference. He has been named SC Magazine's top experts and featured at Narratively's cover piece on Attack of the Superhackers and is frequently quoted and interviewed ...|$|R
50|$|Rational Quality Manager {{includes}} an integrated <b>test</b> <b>execution</b> environment for running tests developed within the product {{as well as}} running tests created in other manual, functional, performance, and security testing tools. Options for <b>test</b> <b>execution</b> include running a test case directly, grouping test cases into test suites for parallel or sequential execution, or creating test case and test-suite execution records to map test environment information directly to the test cases and test suites.|$|E
5000|$|A <b>test</b> <b>execution</b> engine by {{executing}} a test specification, it may perform {{different types of}} operations on the product, such as: ...|$|E
5000|$|Test {{automation}} {{management systems}} leverage automation effort towards efficient and continuous processes of delivering <b>test</b> <b>execution</b> and new working tests by: ...|$|E
3000|$|..., {{we use a}} {{modified}} DBF-based schedulability test as presented in [7]. In this <b>test,</b> the <b>execution</b> pattern of each migrating task τ [...]...|$|R
40|$|Automated program repair is {{a problem}} of finding a {{transformation}} (called a patch) of a given incorrect program that eliminates the observable failures. It has important applications such as providing debugging aids, automatically grading assignments and patching security vulnerabilities. A common challenge faced by all existing repair techniques is scalability to large patch spaces, since there are many candidate patches that these techniques explicitly or implicitly consider. The correctness criterion for program repair is often given as a suite of tests, since a formal specification of the intended program behavior may not be available. Current repair techniques do not scale due to the large number of <b>test</b> <b>executions</b> performed by the underlying search algorithms. We address this problem by introducing a methodology of patch generation based on a test-equivalence relation (if two programs are "test-equivalent" for a given test, they produce indistinguishable results on this test). We propose two test-equivalence relations based on runtime values and dependencies respectively and present an algorithm that performs on-the-fly partitioning of patches into test-equivalence classes. Our experiments on real-world programs reveal that the proposed methodology drastically reduces the number of <b>test</b> <b>executions</b> and therefore provides an order of magnitude efficiency improvement over existing repair techniques, without sacrificing patch quality...|$|R
40|$|Software {{testing is}} an {{essential}} component {{in the development of}} quality software. It is important for students to have a solid introduction to this in their academic career. Since students are not often excited to write code, they certainly will not be interested in testing their code. Students will often only submit <b>test</b> <b>executions</b> for a small handful of test cases and rarely will they regression test the entire test suite. The repetition necessary for proper testing is an obvious contributing factor to this problem. Enter the test harness. Automation of a test suite is an effective way to allow for quick and repeatable testing of a program. Though, it requires effort to build the test harness, {{it can be used to}} test the program throughout the development and maintenance process. Allowance for quick regression testing in every <b>test</b> harness <b>execution</b> is an added bonus. 1...|$|R
