3|9233|Public
25|$|For <b>time-to-event</b> <b>outcome</b> <b>data</b> {{that may}} be censored, {{survival}} analysis (e.g., Kaplan–Meier estimators and Cox proportional hazards models for time to coronary heart disease after receipt of hormone replacement therapy in menopause) is appropriate.|$|E
40|$|Background: Joint {{longitudinal}} and time-to-event {{data models}} have been established in a single study case as beneficial compared to separate longitudinal or time-to-event analyses {{in a range of}} cases, including data with study dropout, time-to-event models with longitudinal covariates measured with error, or cases when the relationship between longitudinal and time-to-event outcomes is of interest. However the methodology available for multi-study cases such as meta-analyses is limited. Aims: To investigate different approaches of modelling of multi-study joint longitudinal and <b>time-to-event</b> <b>outcome</b> <b>data.</b> Methods: Several methods are examined to account for between study heterogeneity, including as one stage methods that can include random effects at the study level, stratification of baseline hazard by study and use of fixed study indicator terms and their interactions with treatment assignment, or approaches for two stage pooling of joint model fits. These methods are applied to a real data example and further investigated in a simulation study. Software have been developed in R to allow these methods to be easily applied in future investigations, which will be available in a package alongside joineR collaboration. Results: The results from the real data example and simulation study will be presented at conference...|$|E
40|$|This is a {{compilation}} of current and past work on targeted maximum likelihood estimation. It features the original targeted maximum likelihood learning paper as well as chapters on super (machine) learning using cross validation, randomized controlled trials, realistic individualized treatment rules in observational studies, biomarker discovery, case-control studies, and <b>time-to-event</b> <b>outcomes</b> with censored <b>data,</b> among others. We hope this collection is helpful to the interested reader and stimulates additional research in this important area...|$|R
40|$|In a {{meta-analysis}} of randomized controlled trials with <b>time-to-event</b> <b>outcomes,</b> an aggregate <b>data</b> approach {{may be required}} for some or all included studies. Variation in the reporting of survival analyses in journals suggests that no single method for extracting the log(hazard ratio) estimate will suffice. Methods are described which improve upon a previously proposed method for estimating the log(HR) from survival curves. These methods extend to life-tables. In the situation where the treatment effect varies over time and the trials in the meta-analysis have different lengths of follow-up, heterogeneity may be evident. In order to assess whether the hazard ratio changes with time, several tests are proposed and compared. A cohort study comparing life expectancy {{of males and females}} with cerebral palsy and a systematic review of five trials comparing two anti-epileptic drugs, carbamazepine and sodium valproate, are used for illustration. Copyright (C) 2002 John Wiley Sons, Ltd...|$|R
40|$|The {{method of}} {{instrumental}} variable (IV) analysis {{has been widely}} used in economics, epidemiology, and other fields to estimate the causal effects of intermediate covariates on outcomes, {{in the presence of}} unobserved confounders and/or measurement errors in covariates. Consistent estimation of the effect has been developed when the outcome is continuous, while methods for binary outcome produce inconsistent estimation. In this dissertation, we examine two IV methods in the literature for binary outcome and show the bias in parameter estimate by a simulation study. The identifiability problem of IV analysis with binary outcome is discussed. Moreover, IV methods for <b>time-to-event</b> <b>outcome</b> with censored <b>data</b> remain underdeveloped. We propose two Bayesian approaches for IV analysis with censored <b>time-to-event</b> <b>outcome</b> by using a two-stage linear model: One is a parametric Bayesian model with normal and non-normal elliptically contoured error distributions, and the other is a semiparametric Bayesian model with Dirichlet process mixtures for the random errors, in order to relax the parametric assumptions and address heterogeneous clustering problems. Markov Chain Monte Carlo sampling methods are developed for both parametric and semiparametric Bayesian models to estimate the endogenous parameter. Performance of our methods is examined by simulation studies. Both methods largely reduce bias in estimation and greatly improve coverage probability of the endogenous parameter, compared to the regular method where the unobserved confounders and/or measurement errors are ignored. We illustrate our methods on the Women's Health Initiative Observational Study and the Atherosclerosis Risk in Communities Study...|$|R
40|$|Abstract Background The number {{needed to}} treat (NNT) is a {{well-known}} effect measure for reporting the results of clinical trials. In the case of <b>time-to-event</b> <b>outcomes,</b> the calculation of NNTs is more difficult than {{in the case of}} binary data. The frequency of using NNTs to report results of randomised controlled trials (RCT) investigating <b>time-to-event</b> <b>outcomes</b> and the adequacy of the applied calculation methods are unknown. Methods We searched in PubMed for RCTs with parallel group design and individual randomisation, published in four frequently cited journals between 2003 and 2005. We evaluated the type of outcome, the frequency of reporting NNTs with corresponding confidence intervals, and assessed the adequacy of the methods used to calculate NNTs in the case of <b>time-to-event</b> <b>outcomes.</b> Results The search resulted in 734 eligible RCTs. Of these, 373 RCTs investigated <b>time-to-event</b> <b>outcomes</b> and 361 analyzed binary data. In total, 62 articles reported NNTs (34 articles with <b>time-to-event</b> <b>outcomes,</b> 28 articles with binary outcomes). Of the 34 articles reporting NNTs derived from <b>time-to-event</b> <b>outcomes,</b> only 17 applied an appropriate calculation method. Of the 62 articles reporting NNTs, only 21 articles presented corresponding confidence intervals. Conclusion The NNT is used as effect measure to present the results from RCTs with binary and <b>time-to-event</b> <b>outcomes</b> in the current medical literature. In the case of time-to-event data incorrect methods were frequently applied. Confidence intervals for NNTs were given in one third of the NNT reporting articles only. In summary, there is much room for improvement in the application of NNTs to present results of RCTs, especially where the outcome is time to an event. </p...|$|R
40|$|A {{common goal}} of {{longitudinal}} studies is to relate {{a set of}} repeated observations to a time-to-event endpoint. One {{example of such a}} design is in the area of a late-life depression research where repeated measurement of cognitive and functional outcomes can contribute to one's ability to predict whether or not an individual will have a major depressive episode over a period of time. This research proposes a novel model for the relationship between multivariate longitudinal measurements and a <b>time-to-event</b> <b>outcome.</b> The goal of this model is to improve prediction for the <b>time-to-event</b> <b>outcome</b> by considering all longitudinal measurements simultaneously. In this dissertation, we investigate a joint modeling approach for mixed types of multivariate longitudinal <b>outcomes</b> and a <b>time-to-event</b> <b>outcome</b> using a Bayesian paradigm. For the longitudinal model of continuous and binary outcomes, we formulate multivariate generalized linear mixed models with two types of random effects structures: shared random effects and correlated random effects. For the joint model, the longitudinal <b>outcomes</b> and the <b>time-to-event</b> <b>outcome</b> are assumed to be independent conditional on available covariates and the shared parameters, which are associated with the random effects of the longitudinal outcome processes. A Bayesian method using Markov chain Monte Carlo (MCMC) computed in OpenBUGS is implemented for parameter estimation. We illustrate the prediction of future event probabilities within a fixed time interval for patients based on our joint model, utilizing baseline data, post-baseline longitudinal measurements, and the <b>time-to-event</b> <b>outcome.</b> Prediction of event or mortality probabilities allows one to intervene clinically when appropriate. Hence, such methods provide a useful public health tool at both the individual and the population levels. The proposed joint model is applied to data sets on the maintenance therapies in a late-life depression study and the mortality in idiopathic pulmonary fibrosis. The performance of the method is also evaluated in extensive simulation studies...|$|R
40|$|BACKGROUND: Joint {{modelling}} of longitudinal and time-to-event data {{is often}} preferred over separate longitudinal or time-to-event analyses {{as it can}} account for study dropout, error in longitudinally measured covariates, and correlation between longitudinal and <b>time-to-event</b> <b>outcomes.</b> The joint modelling literature focuses mainly on the analysis of single studies with no methods currently available for the meta-analysis of joint model estimates from multiple studies. METHODS: We propose a 2 -stage method for meta-analysis of joint model estimates. These methods are applied to the INDANA dataset to combine joint model estimates of systolic blood pressure with time to death, time to myocardial infarction, and time to stroke. Results are compared to meta-analyses of separate longitudinal or time-to-event models. A simulation study is conducted to contrast separate versus joint analyses over a range of scenarios. RESULTS: Using the real dataset, similar results were obtained by using the separate and joint analyses. However, the simulation study indicated a benefit of use of joint rather than separate methods in a meta-analytic setting where association exists between the longitudinal and <b>time-to-event</b> <b>outcomes.</b> CONCLUSIONS: Where evidence of association between longitudinal and <b>time-to-event</b> <b>outcomes</b> exists, results from joint models over standalone analyses should be pooled in 2 -stage meta-analyses...|$|R
40|$|In 2005, Barthel, Royston, and Babiker {{presented}} a menu-driven Stata program under the generic name of ART (assessment of resources for trials) to calculate {{sample size and}} power for complex clinical trial designs with a time-to- event or binary outcome. In this article, we describe a Stata tool called ARTPEP, which is intended to project the power and events of a trial with a <b>time-to-event</b> <b>outcome</b> into the future given patient accrual figures so far and assumptions about event rates and other defining parameters. ARTPEP {{has been designed to}} work closely with the ART program and has an associated dialog box. We illustrate the use of ARTPEP with data from a phase III trial in esophageal cancer. artpep, artbin, artsurv, artmenu, randomized controlled trial, <b>time-to-event</b> <b>outcome,</b> power, number of events, projection, ARTPEP, ART...|$|R
40|$|This report {{examines}} the operating characteristics of adaptively randomized trials relative to equally randomized trials {{in regard to}} power and bias. We also examine {{the number of patients}} in the trial assigned to the superior treatment. The effects of prior selection, sample size, and patient prognostic factors are investigated for both binary and <b>time-to-event</b> <b>outcomes.</b> Content...|$|R
40|$|Cluster {{randomized}} trials (CRTs) involve the random assignment of intact social units rather than independent subjects to intervention groups. <b>Time-to-event</b> <b>outcomes</b> often are endpoints in CRTs where the intracluster correlation coefficient (ICC) {{serves as a}} descriptive parameter to assess the similarity among outcomes in a cluster. However, estimating the ICC in CRTs with <b>time-to-event</b> <b>outcomes</b> is a challenge due {{to the presence of}} censored observations. The ICC is estimated for two CRTs using the censoring indicators and observed outcomes. A simulation study explores the effect of administrative censoring on estimating the ICC. Results show that the ICC estimators derived from censoring indicators and observed outcomes are negatively biased for positively correlated outcomes. Analytic work further supports these results. Censoring indicators may be preferred to estimate the ICC under moderate frequency of administrative censoring while the observed outcomes may be preferred under minimal frequency of administrative censoring...|$|R
40|$|Background: This study {{aimed to}} {{synthesize}} available {{evidence on the}} efficacy of dihydroartemisinin-piperaquine (DHP) in treating uncomplicated Plasmodium vivax malaria in people living in endemic countries. Methodology and Principal Findings: This is a meta-analysis of randomized controlled trials (RCT). We searched relevant studies in electronic databases up to May 2013. RCTs comparing efficacy of (DHP) with other artemisinin-based combination therapy (ACT), non-ACT or placebo were selected. The primary endpoint was efficacy expressed as PCR-corrected parasitological failure. Efficacy was pooled by hazard ratio (HR) and 95 % CI, if studies reported <b>time-to-event</b> <b>outcomes</b> by the Kaplan-Meier method or data available for calculation of HR Nine RCTs with 14 datasets {{were included in the}} quantitative analysis. Overall, most of the studies were of high quality. Only a few studies compared with the same antimalarial drugs and reported the outcomes of the same follow-up duration, which created some difficulties in pooling of <b>outcome</b> <b>data.</b> We found the superiority of DHP over chloroquine (CQ) (at da...|$|R
40|$|Joint {{modelling}} {{has emerged}} to be a potential tool to analyse <b>data</b> with a <b>time-to-event</b> <b>outcome</b> and longitudinal measurements collected over {{a series of}} time points. Joint modelling involves the simultaneous modelling of the two components, namely the time-to-event component and the longitudinal component. The main challenges of joint modelling are the mathematical and computational complexity. Recent advances in joint modelling have seen the emergence of several software packages which have implemented some of the computational requirements to run joint models. These packages have {{opened the door for}} more routine use of joint modelling. Through simulations and real data based on transition to psychosis research, we compared joint model analysis of <b>time-to-event</b> <b>outcome</b> with the conventional Cox regression analysis. We also compared a number of packages for fitting joint models. Our results suggest that joint modelling do have advantages over conventional analysis despite its potential complexity. Our results also suggest that the results of analyses may depend on how the methodology is implemented...|$|R
40|$|The ROC {{curve and}} the {{corresponding}} AUC are popular tools {{for the evaluation of}} diagnostic tests. They have been recently extended to assess prognostic markers and predictive models. However, due to the many particularities of <b>time-to-event</b> <b>outcomes,</b> various definitions and estimators have been proposed in the literature. This review article aims at presenting the ones that accommodate to right-censoring, which is common when evaluating such prognostic markers...|$|R
40|$|Joint {{models are}} {{statistical}} tools for estimating {{the association between}} <b>time-to-event</b> and longitudinal <b>outcomes.</b> One challenge to the application of joint models is its computational complexity. Common estimation methods for joint models include a two-stage method, Bayesian and maximum-likelihood methods. In this work, we consider joint models of a <b>time-to-event</b> <b>outcome</b> and multiple longitudinal processes and develop a maximum-likelihood estimation method using the expectation–maximization algorithm. We assess {{the performance of the}} proposed method via simulations and apply the methodology to a data set to determine the association between longitudinal systolic and diastolic blood pressure measures and time to coronary artery disease...|$|R
40|$|The {{performance}} of different propensity score methods for estimating marginal hazard ratios Peter C. Austina,b,c*† Propensity score methods {{are increasingly being}} used to reduce or minimize the effects of confounding when estimating the effects of treatments, exposures, or interventions when using observational or non-randomized data. Under the assumption of no unmeasured confounders, previous {{research has shown that}} propensity score methods allow for unbiased estimation of linear treatment effects (e. g., differences in means or proportions). However, in biomedical research, <b>time-to-event</b> <b>outcomes</b> occur frequently. There is a paucity of research into the {{performance of}} different propensity score methods for estimating the effect of treatment on <b>time-to-event</b> <b>outcomes.</b> Furthermore, propensity score methods allow for the estimation of marginal or population-average treatment effects. We conducted an extensive series of Monte Carlo simulations to examine the performance of propensity score matching (1 : 1 greedy nearest-neighbor matching within propensity score calipers), stratifi-cation on the propensity score, inverse probability of treatment weighting (IPTW) using the propensity score, and covariate adjustment using the propensity score to estimate marginal hazard ratios. We found that both propensity score matching and IPTW using the propensity score allow for the estimation of marginal hazard ratios with minimal bias. Of these two approaches, IPTW using the propensity score resulted in estimates with lower mean squared error when estimating the effect of treatment in the treated. Stratification on the propensity score and covariate adjustment using the propensity score result in biased estimation of both marginal and con-ditional hazard ratios. Applied researchers are encouraged to use propensity score matching and IPTW using the propensity score when estimating the relative effect of treatment on <b>time-to-event</b> <b>outcomes.</b> Copyright...|$|R
40|$|Amulti-armmulti-stage {{clinical}} trial design for binary outcomes with application to tuberculosis Daniel J Bratton*, Patrick PJ Phillips and Mahesh KB Parmar Background: Randomised controlled trials {{are becoming increasingly}} costly and time-consuming. In 2011, Royston and colleagues proposed a particular class of multi-arm multi-stage (MAMS) designs intended {{to speed up the}} evaluation of new treatments in phase II and III {{clinical trial}}s. Their design, which controls the type I error rate and power for each pairwise comparison, discontinues randomisation to poorly performing arms at interim analyses if they fail to show a pre-specified level of benefit over the control arm. Arms in which randomisation is continued to the final stage of the trial are compared against the control on a definitive <b>time-to-event</b> <b>outcome</b> measure. To increase efficiency, interim comparisons can be made on an intermediate <b>time-to-event</b> <b>outcome</b> which is on the causal pathway to the definitive outcome. Methods: We adapt Royston’s MAMS design to binary outcomes observed {{at the end of a}} fixed follow-up period and analysed using an absolute difference in proportions. We apply the design to tuberculosis (TB), an area where man...|$|R
40|$|As an {{epidemiological}} parameter, {{the population}} attributable fraction {{is an important}} measure to quantify the public health attributable risk of an exposure to morbidity and mortality. In this article, we extend this parameter to the attributable fraction function in survival analysis of <b>time-to-event</b> <b>outcomes,</b> and further establish its estimation and inference procedures based on the widely used proportional hazards models. Numerical examples and simulations studies are presented to validate and demonstrate the proposed methods...|$|R
40|$|We {{consider}} a conceptual {{correspondence between the}} missing data setting, and joint modeling of longitudinal and <b>time-to-event</b> <b>outcomes.</b> Based on this, we formulate an extended shared random effects joint model. Based on this, we provide a characterization of missing at random, which {{is in line with}} that in the missing data setting. The ideas are illustrated using data from a study on liver cirrhosis, contrasting the new framework with conventional joint models. status: publishe...|$|R
40|$|Abstract Background In {{systematic}} {{reviews and}} meta-analyses, <b>time-to-event</b> <b>outcomes</b> are most appropriately analysed using hazard ratios (HRs). In {{the absence of}} individual patient data (IPD), methods are available to obtain HRs and/or associated statistics by carefully manipulating published or other summary data. Awareness and adoption of these methods is somewhat limited, perhaps because they are published in the statistical literature using statistical notation. Methods This paper aims to 'translate' the methods for estimating a HR and associated statistics from published time-to-event-analyses into less statistical and more practical guidance and provide a corresponding, easy-to-use calculations spreadsheet, to facilitate the computational aspects. Results A wider audience {{should be able to}} understand published time-to-event data in individual trial reports and use it more appropriately in meta-analysis. When faced with particular circumstances, readers can refer to the relevant sections of the paper. The spreadsheet can be used to assist them in carrying out the calculations. Conclusion The methods cannot circumvent the potential biases associated with relying on published data for systematic reviews and meta-analysis. However, this practical guide should improve the quality of the analysis and subsequent interpretation of systematic reviews and meta-analyses that include <b>time-to-event</b> <b>outcomes.</b> </p...|$|R
40|$|Abstract Background Observational post-marketing {{assessment}} studies often involve {{evaluating the}} effect of a rare treatment on a <b>time-to-event</b> <b>outcome,</b> through the estimation of a marginal hazard ratio. Propensity score (PS) methods are the most used methods to estimate marginal effect of an exposure in observational studies. However there is paucity of data concerning their performance in a context of low prevalence of exposure. Methods We conducted an extensive series of Monte Carlo simulations to examine the performance of the two preferred PS methods, known as PS-matching and PS-weighting to estimate marginal hazard ratios, through various scenarios. Results We found that both PS-weighting and PS-matching could be biased when estimating the marginal effect of rare exposure. The less biased results were obtained with estimators of average treatment effect in the treated population (ATT), in comparison with estimators of average treatment effect in the overall population (ATE). Among ATT estimators, PS-weighting using ATT weights outperformed PS-matching. These results are illustrated using a real observational study. Conclusions When clinical objectives are focused on the treated population, applied researchers are encouraged to estimate ATT with PS-weighting for studying the relative effect of a rare treatment on <b>time-to-event</b> <b>outcomes...</b>|$|R
30|$|The study {{data were}} meta-analysed where possible. The {{meta-analysis}} of <b>time-to-event</b> <b>outcomes</b> in Review Manager 5.3 (Nordic Cochrane Centre 2014) uses ‘O-E’ and ‘V’ statistics or hazard ratios (HR) for each trial. If {{these were not}} reported in a given trial we calculated them from the available statistics, if possible, using the methods described in Tierney (Tierney et al. 2007). Heterogeneity in meta-analyses was assessed using the I 2 statistic. If the I 2 value was > 50  % we did not pool the effect estimates but used the range of effects from the individual studies instead. <b>Time-to-event</b> <b>outcomes,</b> entered as ‘O–E and Variance’ outcomes, were statistically synthesised using a fixed-effect model and arranged so that HRs >  1 favoured the ALND group and HRs <  1 favoured the comparison group. Dichotomous outcomes were summarised as risk ratios (RR) and analysed using a fixed-effects model according to the Mantel–Haenszel method and arranged so that RRs <  1 favoured the ALND group and RRs >  1 favoured the comparison group. All analyses were conducted in Review Manager 5.3 (Nordic Cochrane Centre 2014). We included only the data available in trial reports or through contact with the trial authors. No data imputation was attempted.|$|R
40|$|The data {{presented}} in this article are related to the research article entitled “Measuring differential treatment benefit across marker specific subgroups: the choice of outcome scale” (Satagopan and Iasonos, 2015) [1]. These data were digitally reconstructed from figures published in Larkin et al. (2015) [2]. This article describes the steps to digitally reconstruct patient-level <b>data</b> on <b>time-to-event</b> <b>outcome</b> and treatment and biomarker groups using published Kaplan-Meier survival curves. The reconstructed data set and the corresponding computer programs are made publicly available to enable further statistical methodology research...|$|R
40|$|In {{longitudinal}} studies {{it is often}} of interest to investigate how a marker that is repeatedly measured in time {{is associated with a}} time to an event of interest, e. g., prostate cancer studies where longitudinal PSA level measurements are collected in conjunction with the time-to-recurrence. Joint Models for Longitudinal and Time-to-Event Data: With Applications in R provides a full treatment of random effects joint models for longitudinal and <b>time-to-event</b> <b>outcomes</b> that can be utilized to analyze such data. The content is primarily explanatory, focusing on applications of joint modeling, bu...|$|R
40|$|The joineR package {{implements}} {{methods for}} analysing data from longitudinal {{studies in which}} the response from each subject consists of a time-sequence of repeated measurements and a possibly censored time-toevent outcome. The modelling framework for the repeated measurements is the linear model with random effects and/or correlated error structure. The model for the <b>time-to-event</b> <b>outcome</b> is a Cox proportional hazards model with log-Gaussian frailty. Stochastic dependence is captured by allowing the Gaussian random effects of the linear model to be correlated with the frailty term of the Cox proportional hazards model...|$|R
40|$|Indiana University-Purdue University Indianapolis (IUPUI) Epidemiologic and {{clinical}} studies routinely collect longitudinal measures of multiple outcomes. These longitudinal outcomes {{can be used}} to establish the temporal order of relevant biological processes and their association with the onset of clinical symptoms. In {{the first part of this}} thesis, we proposed to use bivariate change point models for two longitudinal outcomes with a focus on estimating the correlation between the two change points. We adopted a Bayesian approach for parameter estimation and inference. In the second part, we considered the situation when <b>time-to-event</b> <b>outcome</b> is also collected along with multiple longitudinal biomarkers measured until the occurrence of the event or censoring. Joint models for longitudinal and time-to-event data {{can be used to}} estimate the association between the characteristics of the longitudinal measures over time and survival time. We developed a maximum-likelihood method to joint model multiple longitudinal biomarkers and a <b>time-to-event</b> <b>outcome.</b> In addition, we focused on predicting conditional survival probabilities and evaluating the predictive accuracy of multiple longitudinal biomarkers in the joint modeling framework. We assessed the performance of the proposed methods in simulation studies and applied the new methods to data sets from two cohort studies. National Institutes of Health (NIH) Grants R 01 AG 019181, R 24 MH 080827, P 30 AG 10133, R 01 AG 09956...|$|R
40|$|We {{aimed to}} examine {{the extent to which}} {{inaccurate}} assumptions for nuisance parameters used to calculate sample size can affect the power of a randomized controlled trial (RCT). In a simulation study, we separately considered an RCT with continuous, dichotomous or <b>time-to-event</b> <b>outcomes,</b> with associated nuisance parameters of standard deviation, success rate in the control group and survival rate in the control group at some time point, respectively. For each type of outcome, we calculated a required sample size N for a hypothesized treatment effect, an assumed nuisance parameter and a nominal power of 80 %. We then assumed a nuisance parameter associated with a relative error at the design stage. For each type of outcome, we randomly drew 10, 000 relative errors of the associated nuisance parameter (from empirical distributions derived from a previously published review). Then, retro-fitting the sample size formula, we derived, for the pre-calculated sample size N, the real power of the RCT, taking into account the relative error for the nuisance parameter. In total, 23 %, 0 % and 18 % of RCTs with continuous, binary and <b>time-to-event</b> <b>outcomes,</b> respectively, were underpowered (i. e., the real power was 90 %). Even with proper calculation of sample size, a substantial number of trials are underpowered or overpowered because of imprecise knowledge of nuisance parameters. Such findings raise questions about how sample size for RCTs should be determined...|$|R
40|$|We {{propose a}} joint {{model for a}} <b>time-to-event</b> <b>outcome</b> and a {{quantile}} of a continuous response repeatedly measured over time. The quantile and survival processes are associated via shared latent and manifest variables. Our joint model provides a flexible approach to handle informative drop-out in quantile regression. A general Monte Carlo Expectation Maximization strategy based on importance sampling is proposed, which is directly applicable under any distributional assumption for the longitudinal outcome and random effects, and parametric and non-parametric assumptions for the baseline hazard. Model properties are illustrated through a simulation study and an application to an original data set about dilated cardiomyopathies...|$|R
40|$|Conducting a {{clinical}} trial at multiple study centres raises {{the issue of whether}} and how to adjust for centre heterogeneity in the statistical analysis. In this paper, weaddress this issue for multicentre clinical trials with a <b>time-to-event</b> <b>outcome.</b> Based on simulations, we show that the current practice of ignoring centre heterogeneity can be seriously misleading, and we illustrate the performances of the frailty modelling approach over competing methods. A special attention is paid to the problem of misspecification of the frailty distribution. The appendix provides sample codes in R and in SAS to perform the analyses in this paper...|$|R
40|$|Response-adaptive {{randomization}} {{designs are}} becoming increasingly popular in clinical trial practice. In this paper, we present RARtool, a user interface software developed in MATLAB for designing response-adaptive randomized comparative clinical trials with censored <b>time-to-event</b> <b>outcomes.</b> The RARtool software can compute different types of optimal treatment allocation designs, and it can simulate response-adaptive randomization procedures targeting selected optimal allocations. Through simulations, an investigator can assess design characteristics {{under a variety of}} experimental scenarios and select the best procedure for practical implementation. We illustrate the utility of our RARtool software by redesigning a survival trial from the literature...|$|R
40|$|Evaluating percentiles of {{survival}} was proposed {{as a possible}} method to analyze <b>time-to-event</b> <b>outcomes.</b> This approach sets the cumulative risk of the event of interest to a specific proportion and evaluates the time by which this proportion is attainedIn this context, exposure-outcome associations can be {{expressed in terms of}} differences in survival percentiles, expressing the difference in survival time by which different subgroups of the study population experience the same proportion of events, or in terms of percentile ratios, expressing the strength of the exposure in accelerating the time to the event. Additive models for conditional survival percentiles have been introduced, and their use to estimate multivariable-adjusted percentile differences, and additive interaction on the metric of time has been described. On the other hand, the percentile ratio has never been fully described, neither statistical methods have been presented for its models-based estimation. To bridge this gap, we provide a detailed presentation of the percentile ratio as a relative measure to assess exposure-outcome associations in the context of time-to-event analysis, discussing its interpretation and advantages. We then introduce multiplicative statistical models for conditional survival percentiles, and present their use in estimating percentile ratios and multiplicative interactions in the metric of time. The introduction of multiplicative models for survival percentiles allows researchers to apply this approach in a large variety of context where multivariable adjustment is required, enriching the potentials of the percentile approach as a flexible and valuable tool to evaluate <b>time-to-event</b> <b>outcomes</b> in medical research...|$|R
40|$|A common {{objective}} in longitudinal studies is the joint modelling of a longitudinal response with a <b>time-to-event</b> <b>outcome.</b> Random effects are typically {{used in the}} joint modelling framework to explain the interrelationships between these two processes. However, estimation {{in the presence of}} random effects involves intractable integrals requiring numerical integration. We propose a new computational approach for fitting such models that is based on the Laplace method for integrals that makes the consideration of high dimensional random-effects structures feasible. Contrary to the standard Laplace approximation, our method requires much fewer repeated measurements per individual to produce reliable results. Copyright (c) 2009 Royal Statistical Society. ...|$|R
40|$|We {{present a}} general Bayesian {{framework}} for cost-effectiveness analysis (CEA) from clinical trial data. This framework allows for very flexible modelling of both cost and efficacy related trial data. A common CEA technique is established for this wide class of models through linking mean efficacy and mean {{cost to the}} parameters of any given model. Examples are given in which efficacy may be measured as a continuous, binary, ordinal or <b>time-to-event</b> <b>outcome,</b> and in which costs are modelled as distributed normally, log-normally, as a mixture or non-parametrically. A case study is presented, illustrating the methodology and illuminating the role of prior information. Copyright © 2001 John Wiley & Sons, Ltd. ...|$|R
40|$|BACKGROUND: Several {{studies have}} {{demonstrated}} the association between the time to hypertension event and multiple baseline measurements for adults, yet other survival cardiovascular disease (CVD) outcomes such as high cholesterol and heart attack have been somewhat less considered. The Fels Longitudinal Study (FLS) provides {{us an opportunity to}} connect adult blood pressure (BP) at certain ages to the time to first CVD outcomes. The availability of long-term serial BP measurements from FLS also potentially allows us to evaluate if the trend of the measured BP biomarkers over time predicts survival outcomes in adulthood through statistical modeling. METHODS: When the reference standard is right-censored <b>time-to-event</b> (survival) <b>outcome,</b> the C index or concordance C, is commonly used as a summary measure of discrimination between a survival outcome that is possibly right censored and a predictive-score variable, say, a measured biomarker or a composite-score output from a statistical model that combines multiple biomarkers. When we have subjects longitudinally followed up, it is of primary interest to assess if some baseline measurements predict the <b>time-to-event</b> <b>outcome.</b> Specifically, in this study, systolic blood pressure, diastolic blood pressure, as well as their variation over time, are considered predictive biomarkers, and we assess their predictive ability for certain <b>time-to-event</b> <b>outcomes</b> in terms of the C index. RESULTS: There are a few summary C index differences that are statistically significant in predicting and discriminating certain CVD metric at certain age stage, though some of these differences are altered in the presence of medicine treatment and lifestyle characteristics. The variation of systolic BP measures over time has a significantly different predicting ability comparing with systolic BP measures at certain given time point, for predicting certain survival outcome such as high cholesterol level. CONCLUSIONS: Adult systolic and diastolic BP measurements may have significantly different ability in predicting time to first CVD events. The fluctuation of BP measurements over time may have better association than BP measurement at a single baseline time point, with the time to first CVD events...|$|R
40|$|Background: Patient {{and health}} system {{determinants}} of outcomes following pancreatic cancer resection, particularly {{the relative importance}} of hospital and surgeon volume, are unclear. Our objective was to identify patient, tumour and health service factors related to mortality and survival amongst a cohort of patients who underwent completed resection for pancreatic cancer. Methods: Eligible patients were diagnosed with pancreatic adenocarcinoma between July 2009 and June 2011 and had a completed resection performed in Queensland or New South Wales, Australia, with either tumour-free (R 0) or microscopically involved margins (R 1) (n = 270). Associations were examined using logistic regression (for binary outcomes) and Cox proportional hazards or stratified Cox models (for <b>time-to-event</b> <b>outcomes).</b> Results: Patients treated by surgeons who performe...|$|R
40|$|We develop {{analysis}} methods for clinical trials with <b>time-to-event</b> <b>outcomes</b> which correct for treatment changes during follow-up, yet {{are based on}} comparisons of randomized groups and not of selected groups. A causal model relating observed event times to event times {{that would have been}} observed under other treatment scenarios is fitted using the semi-parametric approach of Robins and Tsiatis (avoiding assumptions about the relationship between treatment changes and prognosis). The methods are applied to the Concorde trial of immediate versus deferred zidovudine, to investigate how the results would have differed if no participant randomized to deferred zidovudine had started treatment before reaching ARC or AIDS. We consider issues relating to model choice, non-constant treatment effects and censoring...|$|R
