303|2211|Public
2500|$|<b>Test</b> <b>specifications</b> and {{parameter}} {{requirements are}} {{specified in the}} generic specification ...|$|E
2500|$|As {{with any}} use of {{mathematical}} models, {{it is important}} to assess the fit of the data to the model. [...] If item misfit with any model is diagnosed as due to poor item quality, for example confusing distractors in a multiple-choice test, then the items may be removed from that test form and rewritten or replaced in future test forms. If, however, a large number of misfitting items occur with no apparent reason for the misfit, the construct validity of the test will need to be reconsidered and the <b>test</b> <b>specifications</b> may need to be rewritten. Thus, misfit provides invaluable diagnostic tools for test developers, allowing the hypotheses upon which <b>test</b> <b>specifications</b> are based to be empirically tested against data.|$|E
50|$|In 2014, AUTOSAR Acceptance Tests were {{introduced}} to minimize test effort and test costs. Acceptance <b>Test</b> <b>Specifications</b> are system <b>test</b> <b>specifications</b> with interfaces to the application and the bus. The specification of standard acceptance tests contribute to these objectives.|$|E
40|$|This Functional Design Criteria is {{designed}} to summarize and give guidance during the development of design, manufacturing and <b>testing</b> <b>specification</b> documents. As the overview document bounding parameters are specified with detailed acceptance criteria to be developed in the more detailed and separate design, manufacturing and <b>testing</b> <b>specification</b> documents...|$|R
50|$|The <b>test</b> <b>specification</b> {{should be}} stored in the test {{repository}} in a text format (such as source code).Test data is sometimes generated by some test data generator tool.Test data can {{be stored in}} binary or text files.Test data should also be stored in the test repository together with the <b>test</b> <b>specification.</b>|$|R
5000|$|ETSI TS 102 230 - Physical, {{electrical}} and logical <b>test</b> <b>specification</b> ...|$|R
5000|$|To {{ensure that}} actual {{implementations}} of such standardised features are interoperable, the standardisation bodies also create so called <b>test</b> <b>specifications.</b> These document detail exact procedures {{on how to}} test that an implementation under test acts according to conformance requirements. Important <b>test</b> <b>specifications</b> for the (U)SIM interface are: ...|$|E
5000|$|<b>Test</b> <b>specifications</b> and {{parameter}} {{requirements are}} {{specified in the}} generic specification ...|$|E
50|$|<b>Test</b> <b>specifications</b> are {{sometimes}} defined as stage 4, as they follow stage 3.|$|E
5000|$|... 3GPP TS 31.121 - Universal Subscriber Identity Module (USIM) {{application}} <b>test</b> <b>specification</b> ...|$|R
50|$|The tests {{demonstrate}} compliance {{with either the}} WITS PSA Test Specification, or the vendors own <b>test</b> <b>specification.</b>|$|R
50|$|QUnit's {{assertion}} methods {{follow the}} CommonJS unit <b>testing</b> <b>specification,</b> which itself was influenced {{to some degree}} by QUnit.|$|R
5000|$|Test {{classes are}} also called test {{objectives}} , test templates [...] and <b>test</b> <b>specifications.</b>|$|E
5000|$|Twist allows <b>test</b> <b>specifications</b> to {{be written}} in English or any UTF-8 {{supported}} language.|$|E
50|$|MTP Level 2 {{is tested}} using the {{protocol}} tester and <b>test</b> <b>specifications</b> described inQ.755,Q.755.1,Q.780 andQ.781.|$|E
50|$|The <b>test</b> <b>{{specification}}</b> is software. Test specification {{is sometimes}} referred to as test sequence, which consists of test steps.|$|R
5000|$|A test {{execution}} engine by executing a <b>test</b> <b>specification,</b> it may perform {{different types of}} operations on the product, such as: ...|$|R
30|$|An {{alternative}} <b>test</b> <b>specification</b> includes quarter indicators {{instead of}} Quarter. The results of my tests are not sensitive to this specification choice.|$|R
5000|$|<b>Test</b> <b>specifications</b> for the {{compliance}} testing of future IEEE 1000BASE-T1 (IEEE802.3bp) Physical Interface (PHY) devices ...|$|E
5000|$|Complete the {{ecosystem}} further with requirement and <b>test</b> <b>specifications</b> for harnesses, switches, ECUs, and additional functionalities.|$|E
5000|$|TC3 also defines 1000BASE-T1 {{magnetics}} {{characteristics and}} CMC limit lines for differential and mixed mode parameters, resulting in CMC performance and <b>test</b> <b>specifications.</b>|$|E
40|$|Recently, {{international}} standard bodies approved the third {{evolution of the}} TTCN <b>test</b> <b>specification</b> language (TTCN- 3) as a requirement to modernise and widen its application beyond pure OSI conformance testing. Even though TTCN- 3 makes the description of complex distributed test behaviour much easier {{there is still a}} requirement from the user community to provide a visualisation means for <b>test</b> <b>specification,</b> development and tracing. Message Sequence Charts (MSC) appeared to be a particularly attractive candidate as a graphical means for visualising TTCN- 3 test cases...|$|R
50|$|The test {{execution}} engine does not carry {{any information about}} the tested product. Only the <b>test</b> <b>specification</b> and the <b>test</b> data carries information about the tested product.|$|R
50|$|The Open Mobile Alliance {{adopted in}} 2008 a {{strategy}} of using TTCN-3 for translating some of the test cases in an enabler <b>test</b> <b>specification</b> into an executable representation.|$|R
5000|$|To {{ensure that}} a test or {{selection}} procedure remains predictive, employers should keep abreast of changes in their job requirements and should update the <b>test</b> <b>specifications</b> or selection procedures accordingly.|$|E
50|$|The exam is knowledge- and experience-based. Candidates {{are given}} {{three hours to}} answer 175 multiple-choice questions. The <b>test</b> <b>specifications</b> {{are based on the}} PHR/SPHR Body of Knowledge, which is {{composed}} of six functional areas.|$|E
5000|$|In March 2016 the SIG {{introduced}} its twelfth technical committee (TC12), aimed to create <b>test</b> <b>specifications</b> for the compliance testing of future IEEE 1000BASE-T1 (IEEE802.3bp) Physical Interface (PHY) devices. The planned specifications {{will cover the}} following areas: ...|$|E
30|$|The {{test cases}} are {{basically}} defined {{according to the}} conformance test of the communication protocol, for example, Flexray physical layer conformance <b>test</b> <b>specification</b> [15] for the Flexray communication system.|$|R
50|$|The A2DP <b>test</b> <b>specification</b> (V1.0) {{contains}} a reference {{implementation of the}} encoder and decoder for the SBC codec. A Linux implementation is available at BlueZ - The Linux Bluetooth Stack.|$|R
50|$|The M-3DI Standard {{will also}} feature a {{comprehensive}} quality control and <b>testing</b> <b>specification</b> to apply during {{the production of}} 3D LC Shutter Glasses; This specification {{is yet to be}} finalised and published.|$|R
50|$|As {{with any}} use of {{mathematical}} models, {{it is important}} to assess the fit of the data to the model. If item misfit with any model is diagnosed as due to poor item quality, for example confusing distractors in a multiple-choice test, then the items may be removed from that test form and rewritten or replaced in future test forms. If, however, a large number of misfitting items occur with no apparent reason for the misfit, the construct validity of the test will need to be reconsidered and the <b>test</b> <b>specifications</b> may need to be rewritten. Thus, misfit provides invaluable diagnostic tools for test developers, allowing the hypotheses upon which <b>test</b> <b>specifications</b> are based to be empirically tested against data.|$|E
50|$|This {{standard}} covers keyword-driven testing. Keyword-driven {{testing is}} {{an approach to}} specifying software tests (normally automated) that is already widely used in the software testing industry. This standard is intended for users who want to create keyword-driven <b>test</b> <b>specifications,</b> create corresponding frameworks, or build test automation based on keywords.|$|E
5000|$|To assure {{interoperability}} and functionality of {{the security}} mechanisms listed above, ICAO and German Federal Office for Information Security (BSI) have specified several test cases. These <b>test</b> <b>specifications</b> are updated with every new protocol and are covering details starting from the paper used and ending in the chip that is included.|$|E
50|$|TTCN-3 <b>test</b> <b>specification</b> {{language}} {{is used for}} the purposes of specifying conformance tests for WiMAX implementations. The WiMAX test suite is being developed by a Specialist Task Force at ETSI (STF 252).|$|R
40|$|Abstract: In this paper, {{we present}} a novel {{framework}} TESTAF to support automatic generation and execution of test cases using object-oriented formal specifications. We use IFAD VDM++ as the specification language, but the ideas presented can be applied equally well to other object-oriented formal notations. The TESTAF framework requires a VDM++ specification for a class, a corresponding implementation in C++, and a <b>test</b> <b>specification,</b> to generate and execute test cases, and evaluate the results. The <b>test</b> <b>specification</b> defines valid <b>test</b> sequences in an intermediate specification language based on regular expressions. The framework uses the formal specification of the class, and the <b>test</b> <b>specification</b> to generate empty test shells, which are then filled in with the test data to create concrete test cases. The test data for a method are generated from the input space defined by the method pre condition and the class invariant. The TESTAF applies boundary value analysis strategy to generate the test data. A test driver then executes the implementation with the test data, and uses a conjunction of method post condition and the class invariant as a test oracle to evaluate the results, while reporting failed test cases to the user. Key Words: automated <b>testing,</b> formal <b>specification,</b> object-oriented software Category: D. 2. ...|$|R
40|$|Testing of on-chip RF and {{microwave}} circuits {{has always been}} a challenge to test engineers and has been more so in the recent past due to the high signal frequencies involved and the dense levels of circuit integration. In this paper, we propose to embed low-cost sensors into RF signal paths for the purpose of built-in test. The sensor characteristics are chosen {{in such a way that}} the sensor outputs, which are low frequency or DC signals, are tightly correlated with the target <b>test</b> <b>specification</b> values of the RF device-under-test. Hence, instead of testing the devices specifically for complex performance metrics (this is difficult for embedded circuits), the outputs of the sensors are used to accurately estimate the target <b>test</b> <b>specification</b> values when the device-under-test is stimulated with sinusoidal stimulus. This significantly impacts the cost of manufacturing test and allows testing to be performed using low-cost external testers. Using this method, the target <b>test</b> <b>specification</b> values can be estimated with an accuracy of ± 5 % of their actual value. 1...|$|R
