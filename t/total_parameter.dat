24|1034|Public
50|$|Subspace Gaussian Mixture Model (SGMM) is an {{acoustic}} modeling approach {{in which all}} phonetic states share a common Gaussian Mixture Model structure, and the means and mixture weights vary in a subspace of the <b>total</b> <b>parameter</b> space.|$|E
5000|$|The term [...] "corner case" [...] {{comes about}} by {{physical}} analogy with [...] "edge case" [...] {{as an extension}} of the [...] "flight envelope" [...] metaphor to a set of testing conditions whose boundaries are determined by the 2n combinations of extreme (minimum and maximum) values for the number n of variables being tested, i.e., the <b>total</b> <b>parameter</b> space for those variables. Where an edge case involves pushing one variable to a minimum or maximum, putting users at the [...] "edge" [...] of the configuration space, a corner case involves doing so with multiple variables, which would put users at a [...] "corner" [...] of a multidimensional configuration space.|$|E
40|$|We {{describe}} an acoustic modeling approach {{in which all}} phonetic states share a common Gaussian Mixture Model structure, and the means and mixture weights vary in a subspace of the <b>total</b> <b>parameter</b> space. We call this a Subspace Gaussian Mixture Model (SGMM). Globally shared parameters define the subspace. This style of acous-tic model allows for a much more compact representation and gives better results than a conventional modeling approach, particularl...|$|E
30|$|<b>Total</b> <b>parameters</b> considered =  3 (Average {{standstill}} distance (ax), additive part {{of safety}} distance (bx_add), multiplicative part of safety distance, bx_mult).|$|R
30|$|Considering {{that each}} input/output is {{described}} by three membership functions (Fig. 5), the number of 4 (input/output variables) × 3 (membership functions) × 3 (<b>parameters)</b> = 36 <b>total</b> <b>parameters</b> have to be configured. Moreover, the tuning of these parameters has to be performed {{in the direction of}} a high quality and a robust-to-attacking conditions watermarked image.|$|R
50|$|This has <b>total</b> 18 <b>parameters</b> {{which is}} sub grouped in four major to {{calculate}} the indexes.|$|R
40|$|Both {{statistics}} and quantum theory deal with prediction using probability. We {{will show that}} there can be established a connection between these two areas. This will at the same time suggest a new, less formalistic way of looking upon basic quantum theory. A <b>total</b> <b>parameter</b> space Φ, equipped with a group G of transformations, gives the mental image of some quantum system, {{in such a way that}} only certain components, functions of the <b>total</b> <b>parameter</b> ϕ can be estimated. Choose an experiment/ question a, and get from this a parameter space Λ^a, perhaps after some model reduction compatible with the group structure. The essentially statistical construction of this paper leads under natural assumptions to the basic axioms of quantum mechanics, and thus implies a new statistical interpretation of this traditionally very formal theory. The probabilities are introduced via Born's formula, and this formula is proved from general, reasonable assumptions, essentially symmetry assumptions. The theory is illustrated by a simple macroscopic example, and by the example of a spin 1 / 2 particle. As a last example we show a connection to inference between related macroscropic experiments under symmetry. Comment: The paper has been withdrawn because it is outdate...|$|E
40|$|From {{various points}} of view {{it is argued that}} one may find {{phenomena}} similar to the quantum effects also in macroscopic cases. This forces one to give up as a general requirement the assumption of realism as formulated by Gill and others. For any potential set of experiments on a limited set of units, we find it useful to introduce for these units the concept of a <b>total</b> <b>parameter,</b> a set of parameters which is so large that a joint value is meaningless. Comment: The paper has been withdrawn because it is outdate...|$|E
40|$|A {{workshop}} held in 2005 {{defined a}} large number of parameters of interest for users of lunar simulants. The need for formal requirements and standards in the manufacture and use of simulants necessitates certain features of measurements. They must be definable, measureable, useful, and primary rather than derived. There are also certain features that must be avoided. Analysis of the <b>total</b> <b>parameter</b> list led to the realization that almost all of the parameters could be tightly constrained, though not predicted, if only four properties were measured: Particle composition, particle size distribution, particle shape distribution, and bulk density. These four are collectively referred to as figures of merit (FoMs). An evaluation of how each of the parameters identified in 2005 is controlled by the four FoMs is given...|$|E
40|$|The paper {{presents}} {{basic information}} on the preparation of environmental samples for trace analysis. Sample preparation is of utmost importance {{for the quality of}} analytical results and their usefulness for decision making in the area of environmental protection and management. The type and number of oper-ations required for a given procedure is determined by the available information about the sample and the goal of the analytical determination (speciation analysis, determination of <b>total</b> <b>parameters</b> or elemental composition, etc. ...|$|R
40|$|AbstractPurpose: At this study, it {{was aimed}} to compare health {{promoting}} behaviors of Turkish (TS) and Foreigner (FS) students of university. Methods: At this study, 64 FS and 70 TS were participated voluntarily, {{mean age of}} FS and TS were 23, 11 and 20, 87 respectively. Volunteers were performed demographic questionnaire and Healthy Lifestyle Behaviors Scale II (HLBS). 34 Foreigner participants were performed HLBS II inventory by post and 30 participants were performed by e-mail. Independent-t test was performed for comparing groups. The level of significance was set at 0. 05. Results: Meaningful difference was found at the age parameter (p 0. 05). When HLBS and its aspects were compared according to TS and FS; statistically meaningful difference was found at the healthy responsibility, spiritual growth, interpersonal relations, stress management and HLPL <b>total</b> <b>parameters</b> (p 0. 05). When HLBS and its aspects were compared according to female TS and FS, statistically meaningful difference was found at healthy responsibility, spiritual growth, interpersonal relations, stress management and HLPL <b>total</b> <b>parameters</b> (p 0. 05). Conclusion: It was so clear that University students needed to be informed about Healthy Lifestyle Behaviors We thought that health, guidance and counseling unites of both campus and other environments had to work more actively with students in co-operatio...|$|R
5000|$|... where i {{indicates}} a phonon mode. The <b>total</b> Grüneisen <b>parameter</b> {{is the sum}} of all γis. It {{is a measure of the}} anharmonicity of the system and closely related to the thermal expansion.|$|R
40|$|Although {{research}} {{has previously been}} done on multilingual speech recognition, {{it has been found}} to be very difficult to improve over separately trained systems. The usual approach has been to use some kind of "universal phone set" that covers multiple languages. We report experiments on a different approach to multilingual speech recognition, in which the phone sets are entirely distinct but the model has parameters not tied to specific states that are shared across languages. We use a model called a "Subspace Gaussian Mixture Model" where states' distributions are Gaussian Mixture Models with a common structure, constrained to lie in a subspace of the <b>total</b> <b>parameter</b> space. The parameters that define this subspace can be shared across languages. We obtain substantial WER improvements with this approach, especially with very small amounts of in-language training data...|$|E
40|$|The heat {{transfer}} coefficients between radiant surfaces and room {{are influenced by}} several parameters: surfaces temperature distributions, internal gains, air movements. The aim {{of this paper is}} to evaluate the {{heat transfer}} coefficients between radiant ceiling and room in typical conditions of occupancy of an office or residential building. Internal gains were therefore simulated using heated cylinders and heat losses using cooled surfaces. Evaluations were developed by means of experimental tests in an environmental chamber. Heat transfer coefficient may be expressed separately for radiation and convection or as one <b>total</b> <b>parameter,</b> but this choice may lead to different considerations about thermal performance of the system. In order to perform correct evaluations, it is therefore extremely important to use the proper reference temperature. The obtained values confirm tendencies found in the literature, indicating limitations and possibilities of radiant ceiling systems improvemen...|$|E
40|$|Biophysical {{modeling}} {{studies have}} suggested that neurons with active dendrites can be viewed as linear units augmented by product terms which arise from interactions between synaptic inputs within the same dendritic subregions. However, the degree to which local nonlinear synaptic interactions augment the memory capacity of a neuron is not known in a quantitative sense. To approach this question, we have studied the family of subsampled quadratic classiers, i. e. linear classiers augmented by the best k terms from the set of K = (d 2 + d) = 2 second-order product terms available in d dimensions. We developed an expression for the <b>total</b> <b>parameter</b> entropy of an SQ classier, whose form shows that the capacity of an SQ classier does not reside solely in its conventional weight values|i. e. the explicit memory used to store constant, linear, and higher-order coeÆcients. Rather, we identify a second type of parameter exibility which jointly contributes to an SQ classier's capacity: the [...] ...|$|E
40|$|The {{differential}} scattering parameter {{has been}} measured for 0. 04 -micron tungsten particles in hydrogen and nitrogen at temperatures to 1080 K. The differential scattering parameter {{has also been}} measured for 0. 1 micron tungsten, three types of carbon particles, and fly ash in nitrogen at temperatures to 1000 K. The 0. 04 micron tungsten shows a temperature dependent <b>total</b> scattering <b>parameter</b> varying from around 4000 sq cm per g at room temperature to 7000 sq cm per g at 1088 K. The temperatures over which data were obtained are not high enough to confirm the temperature dependence of the <b>total</b> scattering <b>parameter</b> of tungsten...|$|R
30|$|Last but not least, a {{total of}} around 9 % of {{parameters}} were disrupted in a meta-analysis (Table 2). This is {{twice as much as}} what could be obtained by chance only (generally considered as 5 %). Surprisingly, 43.5 % of significant different parameters were concentrated in male kidneys for all commercialized GMOs, even if only around 25 % of the <b>total</b> <b>parameters</b> measured were kidney-related. If the differences had been distributed by chance in the organs, not significantly more than 25 % differences would have been found in the kidney. Even if our own counter analysis is removed from the calculation, showing numerous kidney dysfunctions [2], around 32 % of disturbances are still noticed in kidneys.|$|R
3000|$|... where K is the <b>total</b> {{number of}} <b>parameters.</b> In this paper, {{we assume that}} γ belongs to a bounded subset [...]...|$|R
40|$|Maximal expiratory volume-time and flow-volume (MEVT and MEFV) curves were {{constructed}} from the measurements of young male nonsmoking, mild and moderate asthmatic patients (mean age, 29. 7 yrs.). Eleven {{parameters of the}} pulmonary function tests, two MEVT, six MEFV, and three mean time constant (MTC) parameters, were calculated from the curves. These parameters were used in 15 analyses through the all possible selection procedure (APAP) discriminating between mild and moderate asthmatics. The probability of misclassification was computed {{with each of the}} eleven parameters, and all eleven probabilities thus obtained were compared with each other. This procedure showed us that the probability of misclassification ranged from 30. 83 % to 45. 40 % and that the most useful parameter was MTC 50 - 25. The probability of misclassification computed using all eleven parameters (<b>total</b> <b>parameter</b> group) was 15. 90 %. The discriminant analysis indicated that the flow-volume patterns varied according the severity of bronchial asthma, thus, the flow-volume curve was considered to be important in analyzing the severity of bronchial asthma. </p...|$|E
40|$|Abstract: In this paper, {{we examine}} the optimal {{quantization}} of signals for system identification. We deal with memoryless quantization for the output signals and derive the optimal quantization schemes. The objective functions are the errors of least squares parameter estimation subject to a constraint {{on the number of}} subsections of the quantized signals or the expectation of the optimal code length for either high or low resolution. In the high-resolution case, the optimal quantizer is found by solving Euler–Lagrange’s equations and the solutions are simple functions of the probability densities of the regressor vector. In order to clarify the minute structure of the quantization, the optimal quantizer in the low resolution case is found by solving recursively a minimization of a one-dimensional rational function. The solution has the property that it is coarse near the origin of its input and becomes dense away from the origin in the usual situation. Finally the required quantity of data to decrease the <b>total</b> <b>parameter</b> estimation error, caused by quantization and noise, is discussed...|$|E
40|$|This paper {{addresses}} {{the assertion that}} X-linked and haplodiploid genetic systems are inherently limited {{with respect to the}} potential for selectively maintained genetic polymorphisms. Using a variation of Haldane and Jayakar's (1964) parameterization of selection on an X-linked locus, analytical expressions are derived for the proportion of the <b>total</b> <b>parameter</b> space (P) in which stable diallelic polymorphism is attained. P {{is a function of the}} ratio of selection coefficients (r) associated with homozygous and hemizygous genotypes, and the intensity of selection (s). Analytical expressions for the opportunity for polymorphism at an autosomal locus (Pa) are also derived for comparison to the X-linked case. P and Pa are maximal and equal if the ratios of selection coefficients are - 1 and selection is intense. Otherwise, P is slightly less than Pa, but the difference between autosomal and sex-linked loci is less than the range of values of P obtained over the range of r. Several arguments are presented suggesting that polymorphism arising from differential selection in the sexes (r < 0) is probabilistically and biologically feasible...|$|E
5000|$|Effective stress (σ') {{acting on}} a soil is {{calculated}} from two <b>parameters,</b> <b>total</b> stress (σ) and pore water pressure (u) according to: ...|$|R
50|$|TopoR {{simultaneously}} optimizes several alternative {{variants of}} the layout. Variants {{with the worst}} <b>parameters</b> (<b>total</b> wire length and number of vias) will be removed.|$|R
40|$|We {{describe}} a neural network learning algorithm that implements differential learning in a generalized backpropagation framework. The algorithm regulates model complexity during the learning procedure, generating the best low-complexity approximation to the Bayes-optimal classifier {{allowed by the}} training sample. It learns to recognize handwritten digits of the AT&T DB 1 database. Learning is done with little human intervention. The algorithm generates a simple neural network classifier from the benchmark partitioning of the database; the classifier has 650 <b>total</b> <b>parameters</b> and exhibits a test sample error rate of 1. 3 %. 1 INTRODUCTION Recent advances in machine learning theory {{make it possible to}} generate pattern classifiers that are consistently robust estimates of the Bayes-optimal (i. e., minimum probability-of-error) classifier. Moreover, these advances guarantee good approximations to the Bayes-optimal classifier from models with the minimum functional complexity (e. g., the fewest p [...] ...|$|R
40|$|The {{important}} {{condition for}} materials compression are their viscoelastic properties. Two methods {{are used in}} this work for determination viscoelastic parameters of calcium hydrogephosphate dihydrate - the Creep test and the Stress relaxation test. Both methods provide us with elastic and plastic constants. During the Creep test the compaction pressure is kept constant for 180 s and a decrease of tablet height is measured. This tablet decreasing allows us to calculate viscoelastic parameters. The most important result is the factor of total plasticity FPC, which is equal to 0, 89441. The greater this value is the better compaction properties the material has. A constant height of tablet is held for 180 s in the Stress relaxation test and a decrease of pressure within the tablet is recorded. This pressure decrease is caused by three different processes. The third process is most important for the tablet plasticity and is expressed by the <b>total</b> <b>parameter</b> of third process PSR 3 C = 328, 86916 MPas. Again, the greater this value is the better compaction properties the material has...|$|E
40|$|A common {{analytical}} {{problem in}} neuroscience is {{the interpretation of}} neural activity with respect to sensory input or behavioral output. This is typically achieved by regressing measured neural activity against known stimuli or behavioral variables to produce a "tuning function" for each neuron. Unfortunately, because this approach handles neurons individually, it cannot take advantage of simultaneous measurements from spatially adjacent neurons that often have similar tuning properties. On the other hand, sharing information between adjacent neurons can errantly degrade estimates of tuning functions across space if there are sharp discontinuities in tuning between nearby neurons. In this paper, we develop a computationally efficient block Gibbs sampler that effectively pools information between neurons to de-noise tuning function estimates while simultaneously preserving sharp discontinuities that might exist {{in the organization of}} tuning across space. This method is fully Bayesian and its computational cost per iteration scales sub-quadratically with <b>total</b> <b>parameter</b> dimensionality. We demonstrate the robustness and scalability of this approach by applying it to both real and synthetic datasets. In particular, an application to data from the spinal cord illustrates that the proposed methods can dramatically decrease the experimental time required to accurately estimate tuning functions...|$|E
40|$|The {{continued}} {{scaling of}} CMOS technologies introduces new difficulties to statistical circuit analysis and invalidates {{many of the}} methodologies developed earlier. The analysis of device parameter distributions reveals multiple sources of parameter correlations, some of which exhibit mutually opposing trends. We found that applying principal component analysis (PCA) to such heterogeneous statistical data may lead to confounding of data and result in underestimation of the <b>total</b> <b>parameter</b> variance. This imposes considerable constraints {{on the use of}} several methods of statistical circuit analysis based on PCA. Also, the highly nonlinear relationships between the device parameters become more pronounced and cannot be approximated as linear even in the differential range. As a result, the response surface models based on the linear expansion of the performance variable around the nominal point of the device model parameters may lead to significant prediction errors. To address these difficulties, we propose a conceptually simple and accurate approach of direct sampling that treats the extracted SPICE parameter sets and their physical locations as an inseparable set and thus bypasses the dangerous stage of statistical inferences. We illustrate the methodology by applying it to the statistical analysis of a production CMOS process...|$|E
40|$|Abstract: An {{example was}} given in the {{textbook}} All of Statistics (Wasserman, 2004, pages 186 - 188) for arguing that, in the problems with a great many parameters Bayesian inferences are weak, because they rely heavily on the likelihood function that captures information of {{only a tiny fraction}} of the <b>total</b> <b>parameters.</b> Alternatively he suggested non-Bayesian Horwitz-Thompson estimator, which cannot be obtained from a likelihood-based approaches, including Bayesian approaches. He argued that Horwitz-Thompson estimator is good since it is unbiased and consistent. In this paper, I compared the mean square errors of Horwitz-Thompson estimator with a Bayes estimator at a wide range of parameter configurations. I also simulated these two estimators to visualize them directly. From these comparisons, I conclude that the simple Bayes estimator works better than Horwitz-Thompson estimator for most parameter configurations. Hence Bayesian inferences are not weak for this example...|$|R
40|$|Deep Neural nets (NNs) with {{millions}} of parameters {{are at the heart}} of many state-of-the-art computer vision systems today. However, recent works have shown that much smaller models can achieve similar levels of performance. In this work, we address the problem of pruning parameters in a trained NN model. Instead of removing individual weights one at a time as done in previous works, we remove one neuron at a time. We show how similar neurons are redundant, and propose a systematic way to remove them. Our experiments in pruning the densely connected layers show that we can remove upto 85 % of the <b>total</b> <b>parameters</b> in an MNIST-trained network, and about 35 % for AlexNet without significantly affecting performance. Our method can be applied on top of most networks with a fully connected layer to give a smaller network. Comment: BMVC 201...|$|R
50|$|Thus, for {{amorphous}} materials, a <b>total</b> of five <b>parameters</b> {{are sufficient}} to fully describe {{the dependence of}} both n and k on photon energy, E.|$|R
40|$|We {{describe}} a general method of studying prevalent properties of diffeomorphisms of a compact manifold M, where by prevalent we mean true for Lebesgue almost every parameter ε in a generic finite-parameter family {fε} of diffeomorphisms on M. Usually a dynamical property P can be formulated {{in terms of}} properties Pn of trajectories of finite length n. Let P be such a dynamical property that can be expressed {{only in terms of}} periodic trajectories. The first idea of the method is to discretize M and split the set of all possible periodic trajectories of length n for the entire family {fε} into a finite number of approximating periodic pseudotrajectories. Then for each such pseudotrajectory, we estimate the measure of parameters for which it fails Pn. This bounds the <b>total</b> <b>parameter</b> measure for which Pn fails by a finite sum over the periodic pseudotrajectories of length n. Application of Newton Interpolation Polynomials to estimate the measure of parameters that fail Pn for a given periodic pseudotrajectory of length n is the second idea. We outline application of these ideas to two quite different problems: • Growth of number of periodic points for prevalent diffeomorphisms (Kaloshin-Hunt). • Palis’ conjecture on finititude of number of “localized” sinks for prevalent surface dif-feomorphisms (Gorodetski-Kaloshin) ...|$|E
40|$|We {{consider}} {{the viability of}} the minimal supersymmetric standard model with a light (m˜t 1 < 45 GeV) stop. In order for its relic abundance to be cosmologically significant, the photino as dark matter must be quite close in mass to the stop, (m˜t 1 − m˜γ) ≃ 3 − 7 GeV. However, as we show, the photino despite its low mass is virtually undetectable by either direct or indirect means. We also discuss the implications of these masses on the top quark branching ratios. Despite its name, the minimal supersymmetric standard model (MSSM) contains {{a very large number of}} unknown mass parameters. Given some theoretical assumptions, and the available accelerator constraints on some of these masses, cosmology becomes a useful tool in further constraining this parameter space when the lightest supersymmetric particle (LSP) is stable [1]. As the LSP is a potential dark matter candidate, regions in parameter space can be excluded when the relic density of the LSP is excessive. Though there are large portions of the <b>total</b> <b>parameter</b> space in which the LSP in fact comprises much of the dark matter necessary to obtain closure density (or the flat rotation curves of spiral galaxies) [2, 3], direct or indirect detection of the LSP is most favorable when it is relatively light and is a mix o...|$|E
40|$|Probabilistic {{approach}} to analysis of synergism in mathematical models of biochemical networks is introduced. It {{is based on}} system analysis concept when information {{on the importance of}} a parameter of a complex biochemical model is evaluated as part of joint interaction with a complete set of model parameters. For example, this approach accounts for uncertainties in the estimates of enzyme activities and kinetic parameters involved in kinetic modelling of the networks and/or concentration of metabolite or cofactors involved in the interaction of a pathway with perturbations on a cellular level. The parameters are considered as random variables with assumed corresponding probability distribution functions, and total effects of their variability on the network fluxes are evaluated. A numerical measure of synergism of an individual parameter with respect to interaction with model parameters is defined as the difference between the ensemble expected value of conditional variance for the complementary parameters and the variance of conditional expected value of the particular parameter relative to the <b>total</b> <b>parameter</b> ensemble dispersion. In order to demonstrate the concept, the proposed method is applied to two simple cases and to a complex model. The first case is the analysis of synergism between activator and substrate in uni-uni type I mechanism. In the second example, synergism between enzymes involved in a flux through a serial pathway is evaluated. As an example of a complex system, synergism between glycogenolytic flux in a skeletal muscle and involved cellular level cofactors is analyzed...|$|E
5000|$|Ctot : <b>total</b> CpG {{influence}} <b>parameter,</b> {{is defined}} as the sum of coupling factors for any given probe, which provides a measure of local CpG density ...|$|R
5000|$|By {{overloading}} the constructor, {{one could}} pass the tip and <b>total</b> as <b>parameters</b> at creation. This shows the overloaded constructor with two parameters. This overloaded constructor {{is placed in}} the class as well as the original constructor we used before. Which one gets used depends on the number of arguments provided when the new Bill object is created (none, or two): ...|$|R
40|$|Oxidative stress {{derived from}} {{perinatal}} asphyxia {{appears to be}} closely linked to neonatal brain damage and lipid peroxidation biomarkers have shown to provide predictive power of oxidative stress related pathologies in situations of hypoxia and reoxygenation in the newborn. The objective of this work was to develop and validate of a comprehensive liquid chromatography tandem mass spectrometry approach for the quantitative profiling of 28 isoprostanoids in newborn plasma samples covering {{a broad range of}} lipid peroxidation product classes. The method was developed taking into account the specific requirements for its use in neonatology (i. e. limited sample volumes, straightforward sample processing and high analytical throughput). The method was validated following stringent FDA guidelines and was then applied to the analysis of 150 plasma samples collected from newborns. Information obtained from the quantitative analysis of isoprostanoids was critically compared to that provided by a previously developed approach aiming at the semi-quantitative detection of <b>total</b> <b>parameters</b> of fatty acid derived lipid peroxidation biomarkers...|$|R
