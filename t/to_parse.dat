1728|2173|Public
5|$|Many music {{theorists have}} used Parsifal to explore {{difficulties}} {{in analyzing the}} chromaticism of late 19th century music. Theorists such as David Lewin and Richard Cohn have explored the importance of certain pitches and harmonic progressions both in structuring and symbolizing the work. The unusual harmonic progressions in the leitmotifs which structure the piece, {{as well as the}} heavy chromaticism of act 2, make it a difficult work <b>to</b> <b>parse</b> musically.|$|E
5|$|Watson's basic working {{principle}} is <b>to</b> <b>parse</b> keywords in a clue while searching for related terms as responses. This gives Watson some {{advantages and disadvantages}} compared with human Jeopardy! players. Watson has deficiencies in understanding the contexts of the clues. As a result, human players usually generate responses faster than Watson, especially to short clues. Watson's programming prevents it from using the popular tactic of buzzing before it is sure of its response. Watson has consistently better reaction time on the buzzer once it has generated a response, and is immune to human players' psychological tactics, such as jumping between categories on every clue.|$|E
25|$|On {{the other}} hand, the editor often {{displays}} code during its creating, {{while it is}} incomplete or incorrect, and the strict parsers (like ones used in compiles) would fail <b>to</b> <b>parse</b> the code most of the time.|$|E
50|$|Scannerless <b>parsing</b> refers <b>to</b> <b>parsing</b> {{the input}} character-stream directly, without a {{distinct}} lexer.|$|R
5000|$|... {{opening tag}} and closing tag that {{surrounds}} what is <b>to</b> be <b>parsed.</b> ie. [...] (<b>parses</b> <b>to</b> '007') ...|$|R
50|$|Editing {{can be done}} {{not only}} on the text of a {{previous}} command, but also on variable substitutions. Operators range from simple string search/replace <b>to</b> <b>parsing</b> a pathname <b>to</b> extract a specific segment.|$|R
25|$|The {{ability to}} {{appropriately}} generalize to whole classes of yet unseen words, {{coupled with the}} abilities <b>to</b> <b>parse</b> continuous speech and keep track of word-ordering regularities, may be the critical skills necessary to develop proficiency with and knowledge of syntax and grammar.|$|E
25|$|WebCrawler {{was used}} to build the first {{publicly}} available full-text index of {{a subset of the}} Web. It was based on lib-WWW to download pages, and another program <b>to</b> <b>parse</b> and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query.|$|E
25|$|The non-persistent (or reflected) cross-site {{scripting}} vulnerability {{is by far}} {{the most}} basic type of web vulnerability. These holes show up when the data provided by a web client, most commonly in HTTP query parameters (e.g. HTML form submission), is used immediately by server-side scripts <b>to</b> <b>parse</b> and display a page of results for and to that user, without properly sanitizing the request.|$|E
5000|$|In natural {{language}} processing, deterministic <b>parsing</b> refers <b>to</b> <b>parsing</b> algorithms {{that do not}} back up. LR-parsers are an example. (This meaning of the words [...] "deterministic" [...] and [...] "non-deterministic" [...] differs from that used to describe nondeterministic algorithms.) ...|$|R
40|$|Many {{traditional}} TAG parsers handle ambiguity {{by considering}} {{all of the}} possible choices as they unfold during parsing. In contrast, D-theory parsers cope with ambiguity by using underspecified descriptions of trees. This chapter introduces a novel approach <b>to</b> <b>parsing</b> TAG, namely one that explores how D-theoretic notions may be applied <b>to</b> TAG <b>parsing.</b> Combining the D-theoretic approach <b>to</b> TAG <b>parsing</b> as we do here raises new issues and problems. D-theoretic underspecification {{is used as a}} novel approach in the context of TAG parsing for delaying attachment decisions. Conversely, the use of TAG reveals the need for additional types of underspecification that have not been considered so far in the D-theoretic framework. These include combining sets of trees into their underspecified equivalents as well as underspecifying combinations of trees. Herein we examine various issues that arise in this new approach <b>to</b> TAG <b>parsing</b> and present solutions to some of the problems. We also describe [...] ...|$|R
40|$|We {{describe}} two shared task {{systems and}} associated experiments. The German to English system used reordering rules applied <b>to</b> <b>parses</b> and morphological splitting and stemming. The English to German system used an additional translation step which recreated compound words and generated morphological inflection...|$|R
25|$|In February 2017, an {{implementation}} error {{caused by}} a single mistyped character in code used <b>to</b> <b>parse</b> HTML created a buffer overflow error on Cloudflare servers. Similar in its effects to the Heartbleed bug discovered in 2014, this overflow error, widely known as Cloudbleed, allowed unauthorized third parties to read data {{in the memory of}} programs running on the servers—data that should otherwise have been protected by TLS.|$|E
25|$|In the 1920s, {{changes in}} {{lifestyle}} and serious epidemics like tuberculosis made {{the government of}} Canada interested in tracking the Inuit of Canada's Arctic. Traditionally Inuit names reflect what is important in Inuit culture: environment, landscape, seascape, family, animals, birds, spirits. However these traditional names were difficult for non-Inuit <b>to</b> <b>parse.</b> Also, the agglutinative nature of Inuit language meant that names seemed long and were difficult for southern bureaucrats and missionaries to pronounce.|$|E
25|$|Linguistic {{competence}} {{is treated as}} more comprehensive term for lexicalists, such as Jackendoff and Pustejovsky, within the generative school of thought. They assume a modular lexicon, a set of lexical entries containing semantic, syntactic and phonological information deemed necessary <b>to</b> <b>parse</b> a sentence. In the generative lexicalist view this information is intimately tied up with linguistic competence. Nevertheless, their models are still {{in line with the}} mainstream generative research in adhering to strong innateness, modularity and autonomy of syntax.|$|E
5000|$|Skeletons in the Parser: Using Shallow <b>Parsing</b> <b>to</b> Improve Deep <b>Parsing,</b> M. Swift, J. Allen, and D. Gildea.|$|R
50|$|Terence John Parr (b. Los Angeles, 1964) is a {{professor}} of computer science at the University of San Francisco. He {{is best known for his}} ANTLR parser generator and contributions <b>to</b> <b>parsing</b> theory. He also developed the StringTemplate engine for Java and other programming languages.|$|R
40|$|In {{this paper}} we study various reasons and {{mechanisms}} for combining Supertagging with Lexicalized Tree-Adjoining Grammar (LTAG) parsing. Because {{of the highly}} lexicalized nature of the LTAG formalism, we experimentally show that notions other than sentence length play a factor in observed parse times. In particular, syntactic lexical ambiguity and sentence complexity (both are terms we define in this paper) are the dominant factors that affect parsing efficiency. We show how a Supertagger {{can be used to}} drastically reduce the syntactic lexical ambiguity for a given input and can be used in combination with an LTAG parser <b>to</b> radically improve <b>parsing</b> efficiency. We then turn our attention <b>to</b> from <b>parsing</b> efficiency <b>to</b> <b>parsing</b> accuracy and provide a method by which we can effectively combine the output of a Supertagger and a statistical LTAG parser using a co-training algorithm for bootstrapping new labeled data. This combination method can be used to incorporate new labeled data from raw text <b>to</b> improve <b>parsing</b> accuracy...|$|R
25|$|Researchers {{have shown}} that this problem is intimately linked with the ability <b>to</b> <b>parse</b> language, and that those words {{that are easy to}} segment due to their high {{transitional}} probabilities are also easier to map to an appropriate referent. This serves as further evidence of the developmental progression of language acquisition, with children requiring an understanding of the sound distributions of natural languages to form phonetic categories, parse words based on these categories, and then use these parses to map them to objects as labels.|$|E
25|$|If {{a string}} in the {{language}} of the grammar has more than one parsing tree, then the grammar is said to be an ambiguous grammar. Such grammars are usually hard <b>to</b> <b>parse</b> because the parser cannot always decide which grammar rule it has to apply. Usually, ambiguity is a feature of the grammar, not the language, and an unambiguous grammar can be found that generates the same context-free language. However, there are certain languages that can only be generated by ambiguous grammars; such languages are called inherently ambiguous languages.|$|E
25|$|An example: Suppose {{there is}} a library {{function}} whose purpose is <b>to</b> <b>parse</b> a single syslog file entry. What should this function do if the entry is malformed? There is no one right answer, because the same library could be deployed in programs for many different purposes. In an interactive log-file browser, {{the right thing to}} do might be to return the entry unparsed, so the user can see it—but in an automated log-summarizing program, {{the right thing to do}} might be to supply null values for the unreadable fields, but abort with an error, if too many entries have been malformed.|$|E
40|$|This is a {{continuation}} of paper [5] presented at CS&P’ 2012 and of its improved version [6]. The subject is conversion of grammars from Extended Backus-Naur Form <b>to</b> <b>Parsing</b> Expression Grammars. Parsing Expression Grammar (PEG), as introduced by Ford [1, 2], is essentially a recursive-descent parser with limite...|$|R
40|$|This paper {{suggests}} {{two ways}} of improving transition-based, non-projective dependency parsing. First, we add a transition to an existing non-projective parsing algorithm, so it can perform either projective or non-projective parsing as needed. Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a significant improvement <b>to</b> <b>parsing</b> accuracy, showing near state-of-theart performance with respect <b>to</b> other <b>parsing</b> approaches evaluated on the same data set. ...|$|R
40|$|The paper {{describes}} {{the development of}} an example-based parser for Chinese. After reviewing related research, we explain our approach <b>to</b> <b>parsing</b> and generalization. Experiments are performed in order <b>to</b> evaluate dierent <b>parsing</b> models with respect to recall, precision, the f-score and the medium size of a parsed chunk. The main concern is to nd an optimal strategy for the introduction of generalizations into the parsing examples, as well as to establish the eect of a fuzzy parsing strategy...|$|R
25|$|For example, {{there are}} many {{families}} of graphs that are close enough analogues of formal languages {{that the concept of}} a calculus is quite easily and naturally extended to them. Indeed, many species of graphs arise as parse graphs in the syntactic analysis of the corresponding families of text structures. The exigencies of practical computation on formal languages frequently demand that text strings be converted into pointer structure renditions of parse graphs, simply as a matter of checking whether strings are well-formed formulas or not. Once this is done, {{there are many}} advantages to be gained from developing the graphical analogue of the calculus on strings. The mapping from strings <b>to</b> <b>parse</b> graphs is called parsing and the inverse mapping from parse graphs to strings is achieved by an operation that is called traversing the graph.|$|E
25|$|Leavitt {{continued}} {{to publish the}} eponymous almanac after he moved to Meredith. He had already founded a newspaper in 1811, while still living in Gilmanton, which he called The New Hampshire Register, and which he continued publishing for several years (181117). The Register became well-known thanks to Leavitt's habit of printing brief synopses of historical events. On the side, between farming, teaching school, and publishing his newspaper, Leavitt wrote and published at least eight textbooks on mathematics, grammar, astronomy, geography and music, including an edition of Nicolas Pike's widely distributed A New and Complete System of Arithmetic in 1826, {{as well as the}} staple textbooks of teachers across the nation The Scholar's Review and Teacher's Daily Assistant. The scores of textbooks produced by Leavitt, a dizzying array of titles down the years, including Complete Directions for Parsing the English Language; Or, The Rules of Grammar Made Easy: Being a New Grammatical Essay, Designed as a Supplement to Lindley Murray's Grammar, for the Use of Students as Soon as They Begin <b>to</b> <b>Parse,</b> make one wonder where Leavitt found the time.|$|E
500|$|... parses and executes {{successfully}} in some environments (e.g. SQLite or PostgreSQL) which unify a NULL boolean with Unknown but fails <b>to</b> <b>parse</b> in others (e.g. in SQL Server Compact). MySQL behaves similarly to PostgreSQL {{in this regard}} (with the minor exception that MySQL regards TRUE and FALSE as {{no different from the}} ordinary integers 1 and 0). PostgreSQL additionally implements a IS UNKNOWN predicate, which can be used to test whether a three-value logical outcome is Unknown, although this is merely syntactic sugar.|$|E
5000|$|<b>To</b> {{correctly}} <b>parse</b> without lookahead, {{there are}} three solutions: ...|$|R
40|$|We {{propose to}} model {{multiword}} expressions as dependency subgraphs, and realize this {{idea in the}} grammar formalism of Extensible Dependency Grammar (XDG). We extend XDG to lexicalize dependency subgraphs, and show how to compile them into simple lexical entries, amenable <b>to</b> <b>parsing</b> and generation with the existing XDG constraint solver...|$|R
40|$|We {{discuss the}} concept of {{robustness}} with respect <b>to</b> <b>parsing</b> a context-free language. Our approach {{is based on the}} notions of fuzzy language, (generalized) fuzzy context-free grammar and parser / recognizer for fuzzy languages. As concrete examples we consider a robust version of Cocke-Younger-Kasami's algorithm and a robust kind of recursive descent recognizer...|$|R
500|$|Other {{programs}} that undertake <b>to</b> <b>parse</b> Perl, such as source-code analyzers and auto-indenters, {{have to contend}} not only with ambiguous syntactic constructs {{but also with the}} undecidability of Perl parsing in the general case. Adam Kennedy's PPI project focused on parsing Perl code as a document (retaining its integrity as a document), instead of parsing Perl as executable code (that not even Perl itself can always do). It was Kennedy who first conjectured that [...] "parsing Perl suffers from the 'halting problem'", which was later proved.|$|E
500|$|Writing for The New York Times Magazine, Jay Caspian Kang {{outlined}} {{his position}} on the fairness and skill-based aspects of DFS, explaining that [...] "D.F.S., the game itself, is not inherently crooked. Most of the benefits praised by its enthusiasts — the ease of play, the camaraderie among fans, the challenge of solving what amounts to a math puzzle — are real. It does take skill <b>to</b> <b>parse</b> game film, diligently follow the news and interpret the thousands of bits of sports information that are generated each night. If a problem gambler at the poker rooms I frequent in New York City were to hire a programmer and flood the D.F.S. market with his lineups, he would almost certainly hemorrhage money." ...|$|E
500|$|Nat Hentoff (The Reporter) {{struggled}} {{to disagree with}} Goodman's claim that schools provided little room for [...] "spontaneity" [...] and free spiritedness. However, he felt that Goodman inadequately explained how primary schools could be improved in content and staffing. Hentoff said that the book's key flaw was its position in a [...] "political vacuum", offering no means for society to acknowledge Goodman's expressed unviability of their schooling model. Donald Barr (New York Herald Tribune Book Week) wrote that Goodman seemed like [...] "an itinerant peddler of sedition" [...] who spoke of virtuous [...] "dissonance". Barr considered Goodman [...] "extraordinarily sensitive to children and adolescents" [...] and complimented his [...] "brilliant authenticity" [...] when describing how children learn [...] "defiance and embarrassment". However, Barr found Goodman's [...] "purblind resentment of all authority" [...] to obstruct his points and to leave his readers skeptical. Children, Barr wrote, are lost if they cannot find the limits they serve to test, and [...] "partisan" [...] Goodman was unable <b>to</b> <b>parse</b> the wickedness of continually [...] "yielding, ... tolerating, understanding" [...] children who must feel resistance against their transgressions to develop the respect they seek.|$|E
40|$|International audienceExisting {{approaches}} <b>to</b> <b>parsing</b> {{images of}} objects featuring complex, non-hierarchical structure rely on exploration {{of a large}} search space combining {{the structure of the}} object and positions of its parts. The latter task requires randomized or greedy algorithms that do not produce repeatable results or strongly depend on the initial solution. To address the problem we propose to model and optimize the structure of the object and position of its parts separately. We encode the possible object structures in a graph grammar. Then, for a given structure, the positions of the parts are inferred using standard MAP-MRF techniques. This way we limit the application of the less reliable greedy or randomized optimization algorithm to structure inference. We apply our method <b>to</b> <b>parsing</b> images of building facades. The results of our experiments compare favorably to the state of the art...|$|R
40|$|Previous {{studies in}} {{data-driven}} dependency parsing {{have shown that}} tree transformations can improve parsing accuracy for specific parsers and data sets. We investigate to what extent this can be generalized across languages/treebanks and parsers, focusing on pseudo-projective parsing, {{as a way of}} capturing non-projective dependencies, and transformations used <b>to</b> facilitate <b>parsing</b> of coordinate structures and verb groups. The results indicate that the beneficial effect of pseudo-projective parsing is independent of parsing strategy but sensitive to language or treebank specific properties. By contrast, the construction specific transformations appear to be more sensitive <b>to</b> <b>parsing</b> strategy but have a constant positive effect over several languages...|$|R
40|$|As for {{semantic}} role labeling (SRL) task, when {{it comes}} <b>to</b> utilizing <b>parsing</b> information, both traditional methods and recent recurrent neural network (RNN) based methods use the feature engineering way. In this paper, we propose Syntax Aware Long Short Time Memory(SA-LSTM). The structure of SA-LSTM modifies according <b>to</b> dependency <b>parsing</b> information in order <b>to</b> model <b>parsing</b> information directly in an architecture engineering way instead of feature engineering way. We experimentally demonstrate that SA-LSTM gains more improvement from the model architecture. Furthermore, SA-LSTM outperforms the state-of-the-art on CPB 1. 0 significantly according to Student t-test ($p< 0. 05 $) ...|$|R
