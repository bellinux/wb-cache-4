0|10000|Public
40|$|We {{revisit the}} problem of {{extending}} quadrature formulas for general weight functions, and provide a generalization of Patterson's <b>method</b> for <b>the</b> constant weight function. <b>The</b> <b>method</b> {{can be used to}} compute a nested sequence of quadrature formulas for integration with respect to any continuous probability measure on the real line with finite moments. The advantages of <b>the</b> <b>method</b> <b>include</b> <b>that</b> it works directly with the moments of the underlying distribution, and that for distributions with rational moments the existence of the formulas can be verified by exact rational arithmetic. Comment: Minor update in 2016 : fixing a typo in the Mathematica cod...|$|R
40|$|Identification of {{body parts}} is an {{important}} first step for many tasks such as action recogni-tion in automatic surveillance systems. In this paper, we present a body part segmentation system for image and video analysis. The proposed system utilises Hidden Markov Models and modified shape context features for statistical modeling of the human body shape. In our solution, we also demonstrate how a general and robust solution can be developed with the synthetically generated training data. The sequences of synthetically generated images are generated using three dimensional rendering and motion capture information. After the training phase, the model is used to segment silhouette images into four body parts; arms, legs, body and head. In experiments, the system is successfully used in body part segmentation, unusual activity detection in surveillance applications and arm swing detection in gait analysis. The advantages of <b>the</b> <b>method</b> <b>include</b> <b>that</b> <b>the</b> same model can be employed without any modifications of parameters after initial training...|$|R
40|$|This article {{describes}} the results of estimation of future managers’ informational competency level performed based on <b>the</b> developed <b>method</b> <b>that</b> <b>includes</b> diagnostics of maturity of informational thesaurus and three main informational competency components. It is proposed to carry out development of informational competency through implementation of integrative cours...|$|R
40|$|In {{this work}} we present the full-wave {{analysis}} of the Expanded Very Large Array (EVLA) radio telescope. <b>The</b> analysis <b>methods</b> <b>include</b> <b>that</b> offered by <b>the</b> commercial electromagnetic simulation software package, FEKO, using solvers such as the multi-level fast multipole method (MLFMM) and iterative MLFMM / large element physical optics (LE-PO) techniques. The aforementioned solvers exhibit different memory and runtime performances, as well as varying accuracy levels with which parameters such as the polarisation and gain of the radio telescope can be calculated. The purpose of this work is to review these methods in more detail, when applied to a large structure such as the EVLA...|$|R
40|$|AbstractThis report {{analyzes}} the long time stability of four methods for non-iterative, sub-physics, uncoupling for the evolutionary Stokes–Darcy problem. <b>The</b> four <b>methods</b> uncouple each timestep into separate Stokes and Darcy solves using ideas from splitting methods. Three methods uncouple sequentially while {{one is a}} parallel uncoupling method. We prove long time stability of four splitting based partitioned methods under timestep restrictions depending on the problem parameters. <b>The</b> <b>methods</b> <b>include</b> those <b>that</b> are stable uniformly in S 0, the storativity coefficient, for moderate kmin, the minimum hydraulic conductivity, uniformly in kmin for moderate S 0 and with no coupling between the timestep and the spacial meshwidth...|$|R
50|$|Disadvantages of <b>the</b> <b>method</b> <b>include</b> <b>the</b> fact <b>that</b> the TCP session {{has to be}} {{directed}} through the accelerator; this means that if routing changes, so that the accelerator {{is no longer in}} the path, the connection will be broken. It also destroys the end-to-end property of the TCP ack mechanism; when the ACK is received by the sender, the packet has been stored by the accelerator, not delivered to the receiver.|$|R
40|$|Abstract. The DIRECT {{algorithm}} is a deterministic sampling method for bound constrained Lipschitz continuous optimization. We prove a subsequential convergence result for the DIRECT algorithm that quantifies {{some of the}} convergence observations in the literature. Our results apply to several variations on <b>the</b> original <b>method,</b> <b>including</b> one <b>that</b> will handle general constraints. We use techniques from nonsmooth analysis, and our framework is based on recent results for the MADS sampling algorithms...|$|R
40|$|The signed loop {{approach}} is a beautiful way to rigorously study the two-dimensional Ising model with no external field. In this paper, we explore the foundations of <b>the</b> <b>method,</b> <b>including</b> details <b>that</b> have so far been neglected or overlooked in the literature. We demonstrate how <b>the</b> <b>method</b> {{can be applied to}} the Ising model on the square lattice to derive explicit formal expressions for the free energy density and two-point functions in terms of sums over loops, valid all the way up to the self-dual point. As a corollary, it follows that the self-dual point is critical both for the behaviour of the free energy density, and for the decay of the two-point functions. © 2013 Springer Science+Business Media New York...|$|R
40|$|This paper {{outlines}} and qualitatively {{compares the}} implementations of seven different methods for solving Poisson’s equation on <b>the</b> disk. <b>The</b> <b>methods</b> <b>include</b> two classical finite elements, a cotan formula-based discrete differential geometry approach and four isogeometric constructions. The comparison reveals numerical convergence rates and, particularly for isogeometric constructions based on Catmull–Clark elements, {{the need to}} carefully choose quadrature formulas. <b>The</b> seven <b>methods</b> <b>include</b> two <b>that</b> are new to isogeometric analysis. Both new methods yield O(h 3) convergence in the L 2 norm, also when points are included where n 6 ≠ 4 pieces meet. One construction {{is based on a}} polar, singular parameterization; the other is a G 1 tensor-product construction...|$|R
40|$|We {{introduce}} {{and study}} methods for inferring {{and learning from}} correspondences among neurons. The approach enables alignment of data from distinct multiunit studies of nervous systems. We show that <b>the</b> <b>methods</b> for inferring correspondences combine data effectively from cross-animal studies to make joint inferences about behavioral decision making that are not possible with the data from a single animal. We focus on data collection, machine learning, and prediction in the representative and long-studied invertebrate nervous system of the European medicinal leech. Acknowledging the computational intractability of the general problem of identifying correspondences among neurons, we introduce efficient computational procedures for matching neurons across animals. <b>The</b> <b>methods</b> <b>include</b> techniques <b>that</b> adjust for missing cells or additional cells in the different data sets that may reflect biological or experimental variation. <b>The</b> <b>methods</b> highlight <b>the</b> value harnessing inference and learning in new kinds of computational microscopes for multiunit neurobiological studies...|$|R
30|$|Identification of {{patterns}} in data (e.g., streamflow) {{serves as a}} fundamental approach towards modeling and prediction of the underlying systems. Numerous methods {{have been developed for}} identification {{of patterns}} in data (in space, time, and space–time) and possible connections between the components involved. Such methods can be categorized in different ways depending on their concepts and use of data, such as linear and nonlinear, deterministic and stochastic, parametric and non-parametric, supervised and unsupervised, and their combinations. <b>The</b> <b>methods</b> <b>include</b> those <b>that</b> are based on correlation, trend, spectrum, data distribution, data reconstruction, dimension, scaling, regression, clustering, and classification, among others. They have been extensively applied to identify patterns in hydrologic data around the world; see, for example, Labat et al. (2011), Sivakumar and Singh (2012), Özger et al. (2013), Tongal and Berndtsson (2014), and Xu et al. (2015) for some recent studies, and Salas et al. (1995) and Sivakumar and Berndtsson (2010) for compilations.|$|R
40|$|Differential algebra {{approaches}} to structural identifiability {{analysis of a}} dynamic system model in many instances heavily depend upon Ritt's pseudodivision at an early step in analysis. The pseudodivision algorithm is used to find the characteristic set, of which a subset, the input-output equations, is used for identifiability analysis. A simpler algorithm is proposed for this step, using Gröbner Bases, along with a proof of <b>the</b> <b>method</b> <b>that</b> <b>includes</b> a reduced upper bound on derivative requirements. Efficacy of the new algorithm is illustrated with two biosystem model examples. Comment: 17 page...|$|R
50|$|Biosensors {{are used}} {{for a range of}} {{different}} biological measurements, such as real time study of ligand- receptor kinetics or the rapid detection of biomarkers or pathogens. The cartridge is made of a newly developed OSTE polymer which has an excess of thiol functional groups, allowing it to covalently bond to the gold surface of the sensor. <b>The</b> fabrication <b>method</b> of <b>the</b> OSTE cartridges is based on replica molding with rapid UV-curing and is therefore applicable to micro structured components and scalable up to high volume fabrication. The process addresses the need for a simple and straightforward production of robust, low cost and disposable sensor chips and eliminates many of the limitations of previous methods where PDMS molds were used. The benefits of <b>the</b> novel <b>method</b> <b>include</b> <b>that</b> it allows for one step integration at low temperature in a dry environment, is uncomplicated and rapid, and eliminates the need for clamping, the necessity of manual assembly, the use of elastic and biocompatible cartridge materials, or surface activation/coating. OSTE packaging for biosensing has been demonstrated for QCM, and photonic ring resonator sensors.|$|R
40|$|Source term {{estimation}} for atmospheric dispersion {{deals with}} {{estimation of the}} emission strength and location of an emitting source using all available information, including site description, meteorological data, concentration observations and prior information. In this paper, Bayesian methods for source term estimation are evaluated using Prairie Grass field observations. <b>The</b> <b>methods</b> <b>include</b> those <b>that</b> require <b>the</b> specification of the likelihood function and those which are likelihood free, also known as approximate Bayesian computation (ABC) <b>methods.</b> <b>The</b> performances of five different likelihood functions in the former and six different distance measures {{in the latter case}} are compared for each component of the source parameter vector based on Nemenyi test over all the 68 data sets available in the Prairie Grass field experiment. Several likelihood functions and distance measures are introduced to source term estimation for the first time. Also, ABC method is improved in many aspects. Results show that discrepancy measures which refer to likelihood functions and distance measures collectively have significant influence on source estimation. There is no single winning algorithm, but these methods can be used collectively to provide more robust estimates...|$|R
30|$|The {{accuracy}} of the quantification of activity concentration depends {{on a number of}} factors in addition to the segmentation of the image. The foundation for image-based activity quantification in nuclear medicine is the tomographic reconstruction and <b>the</b> correction <b>methods</b> <b>included</b> in <b>that</b> algorithm. If proper compensation for the major physical effects, in particular attenuation and scatter in the patient, is not performed, there is less hope of a reliable quantification. Accordingly, in terms of activity concentration, the numeric results achieved in this study are not necessarily generalizable between reconstruction programs. On the other hand, in terms of the segmentation itself and the resulting estimated volumes, <b>the</b> <b>method</b> used for, e.g., scatter compensation may be of less importance.|$|R
40|$|Microlensing light curves are {{typically}} computed either by ray-shooting maps or by contour integration via Green's theorem. We present an improved version of <b>the</b> second <b>method</b> <b>that</b> <b>includes</b> a parabolic correction in Green's line integral. In addition, we present an accurate analytical {{estimate of the}} residual errors, which allows the implementation of an optimal strategy for the contour sampling. Finally, we give a prescription for dealing with limb-darkened sources reaching arbitrary accuracy. These optimizations lead to a substantial speed-up of contour integration codes along with a full mastery of the errors. Comment: 34 pages, 11 figure...|$|R
50|$|In 1931 he {{published}} {{a paper on}} {{what is now called}} the Krylov subspace and Krylov subspace <b>methods.</b> <b>The</b> paper deals with eigenvalue problems, namely, with computation of the characteristic polynomial coefficients of a given matrix. Krylov was concerned with efficient computations and, as a computational scientist, he counts the work as a number of separate numerical multiplications; something not very typical for a 1931 mathematical paper. Krylov begins with a careful comparison of <b>the</b> existing <b>methods</b> <b>that</b> <b>include</b> <b>the</b> worst-case-scenario estimate of the computational work in <b>the</b> Jacobi <b>method.</b> Later, he presents his own method which is superior to <b>the</b> known <b>methods</b> of that time and is still widely used.|$|R
40|$|In {{this paper}} is {{presented}} a method for neural activity estimation over the brain that take into account, for {{the solution of the}} inverse problem, a dynamic model for the neural activity in a realistic head model calculated with bounded elements method, according to a physiologically based model that describes the real interaction between neurons. The solution of the inverse problem is calculated using high performance computing. This analysis is performed for simulated EEG signals for SNR of 25 dB, 15 dB and 5 dB. The obtained results show the robustness of <b>the</b> estimation <b>method</b> <b>that</b> <b>includes</b> <b>the</b> dynamic model in comparison with the static model for several levels of noise...|$|R
40|$|A {{method is}} {{presented}} for producing analytical results {{applicable to the}} standard two-party deterministic dense coding protocol, wherein communication of K perfectly distinguishable messages is attainable {{with the aid of}} K selected local unitary operations on one qudit from a pair of entangled qudits of equal dimension d in a pure state. <b>The</b> <b>method</b> utilizes <b>the</b> properties of a (d^ 2) x(d^ 2) unitary matrix whose initial columns represent message states of the system used for communication, augmented by sufficiently many additional orthonormal column vectors so that the resulting matrix is unitary. Using the unitarity properties of this augmented message-matrix, we produce simple proofs of previously established results including (i) an upper bound on the value of the square of the largest Schmidt coefficient, given by d/K, and (ii) the impossibility of finding a pure state that can enable transmission of K=d^ 2 - 1 messages but not d^ 2. Additional results obtained using <b>the</b> <b>method</b> <b>include</b> proofs <b>that</b> when K=d+ 1 the upper bound on the square of the largest Schmidt coefficient (i) always reduces to at least (1 / 2) [1 +sqrt{(d- 2) /(d+ 2) }], and (ii) reduces to (d- 1) /d in the special case that the identity and shift operators are two of the selected local unitaries. Comment: 16 page...|$|R
40|$|Public {{transport}} {{passengers in}} Malang commonly called public transportation or public transit is on {{of the means}} of transport that is often used by people to perform daily activities - day, but public transport passengers in the city of Malang is now often seen to cause problems and distrupt traffic. The {{purpose of this study was}} to determine the level of service of public transport passengers GA pathway. <b>The</b> <b>method</b> used is <b>the</b> empirical <b>methods</b> <b>that</b> <b>include</b> surveys of static and dynamic survey. The results of the performance study of public transit passenger load factor pathway GA earned an average of 189, 15 %,the average frequency of 62 kend / h, average time headway of 1, 0 minutes, average travel speed of 16, 44 km/h...|$|R
40|$|Correlations for calculating diffuse solar {{radiation}} can be classified into models with global {{solar radiation}} (H-based method) and without it (Non-H <b>method).</b> <b>The</b> objective {{of the present study}} is to compare the performance of H-based and Non-H <b>methods</b> for calculating <b>the</b> {{diffuse solar radiation}} in regions without solar radiation measurements. The comparison is carried out at eight meteorological stations in China focusing on the monthly average daily diffuse solar radiation. Based on statistical error tests, the results show <b>that</b> <b>the</b> Non-H <b>method</b> <b>that</b> <b>includes</b> other readily available meteorological elements gives better estimates. Therefore, it can be concluded that <b>the</b> Non-H <b>method</b> is more appropriate than the H-based one for calculating the diffuse solar radiation in regions without solar radiation measurements. (c) 2012 Elsevier Ltd. All rights reserved...|$|R
40|$|In {{order to}} {{determine}} which technique would provide adequate tissue for histological examination when the 'Tru-Cut' needle is used for liver biopsy, the livers of cadavers were biopsied by a single operator, under direct vision, using a 'Tru-Cut' needle. The modified breast biopsy technique either preceded or followed one of two alternative methods, {{one of which was}} recommended by the manufacturer, in a random manner. Thereafter, the biopsies were repeated using the alternative sequence. The mean length of the liver biopsy specimens that were obtained using the modified breast biopsy technique was 16. 3 mm (range: 8 - 20 mm). The corresponding figures for <b>the</b> manufacturerer's <b>method</b> were 7. 7 mm (range: 2 - 14 mm) and for the third technique were 2. 7 mm (range: 0. 5 - 8 mm). Three of the 40 specimens (7. 5 %) obtained using the latter technique fragmented when placed in formalin; this did not occur in any of the 40 specimens taken using the modified breast biopsy technique. This investigation indicates that the modified breast biopsy technique should be used when 'Tru-Cut' needle biopsy of the liver is performed. This provides specimens which are adequate for histological diagnosis. In addition, the safety of liver biopsy, which is compromised by poor technique, is improved. <b>The</b> alternative <b>methods,</b> <b>including</b> <b>that</b> recommended by <b>the</b> manufacturer, must be avoided...|$|R
40|$|This thesis {{deals with}} Žilka's <b>method</b> - <b>the</b> use of {{recorder}} in musicotherapy. It {{consists of the}} basic facts {{about the influence of}} sound and healthy breathing on human's mind and body. It describes life and activities of Czech flute player and music teacher Václav Žilka, who brought the recorder to light in the 2 nd half of 20 th century. Introductory part of the thesis focuses on historical background. The practical part mentions <b>the</b> Žilka's <b>method</b> <b>that</b> <b>includes</b> <b>the</b> analysis of flute practice book Veselé pískání - zdravé dýchání, practical comments, breath exercises and musico-educational projects for children. The example of breath exercise and chronology of Žilka's life is an important appendices. Key words: Recorder, musicotherapy, children, respiratory diseases...|$|R
40|$|Numerical experiments, constituting of 90 {{simulations}} in FDS, {{were used}} in this paper for two purposes. Firstly an evaluation of previously derived correlations for ceiling jet excess temperatures and velocities was performed. Secondly the numerical experiments were used to demonstrate how computer simulations {{could be used as}} a complement to actual fire experiments in fire science research. The evaluation indicates that the existing correlations will give a good estimate of the average temperature in a ceiling jet. However, it seams like the correlations will not give a good estimate of the maximum excess temperature or velocity. Consequently, a new correlation to estimate the maximum temperature was developed. The novelty of this research is primarily <b>the</b> <b>method</b> <b>that</b> <b>includes</b> an outline of how numerical experiments can be used in fire science research...|$|R
40|$|From data {{generated}} using 1 H NMR titrations, different methodologies {{to calculate}} binding constants are compared. <b>The</b> ‘local’ analysis <b>method</b> that uses {{only a single}} isotherm (only one H-bond donor) is compared against <b>the</b> ‘global’ <b>method</b> (<b>that</b> <b>includes</b> many or all H-bond donors). The results indicate that for simple systems both methods are suitable, however, the global approach consistently provides a K a value with uncertainties up to 30 % smaller. For more complex binding, <b>the</b> global analysis <b>method</b> gives much more robust results than <b>the</b> local <b>methods.</b> This study also highlights the need to explore several different modes when data do not fit well to a simple 1 : 1 complexation model and illustrates the need for better methods to estimate uncertainties in supramolecular binding experiments. <br /...|$|R
40|$|The {{concept of}} Disaster Resilience has {{received}} considerable attention {{in recent years}} and it is increasingly used as an approach for measuring response of communities to natural disasters. Recently a framework named PEOPLES has been developed by MCEER to measure performance of communities to natural disasters. <b>The</b> <b>method</b> <b>includes</b> seven dimensions <b>that</b> <b>include</b> both technical and socio-economic aspects. All resilience dimensions and their respective indices to measure community performances are obviously interdependent. As first step, the physical dimension has been implemented in software and indices have been proposed to measure performance of buildings and lifelines. This paper tries to focus on developing methodologies to consider interdependencies between buildings (e. g. hospitals, strategic buildings, etc) and lifelines (road networks, etc.). An approach considering network interdependencies have been developed which is based on the time series analysis of the restoration curves of the different infrastructures. The case study of 2011 Tohoku Earthquake has been presented to illustrate the implementations issu...|$|R
40|$|A {{method of}} {{controlling}} {{a height of}} an electron beam gun and wire feeder during an electron freeform fabrication process includes utilizing a camera to generate {{an image of the}} molten pool of material. The image generated by the camera is utilized to determine a measured height of the electron beam gun relative {{to the surface of the}} molten pool. <b>The</b> <b>method</b> further <b>includes</b> ensuring <b>that</b> <b>the</b> measured height is within the range of acceptable heights of the electron beam gun relative to the surface of the molten pool. The present invention also provides for measuring a height of a solid metal deposit formed upon cooling of a molten pool. The height of a single point can be measured, or a plurality of points can be measured to provide 2 D or 3 D surface height measurements...|$|R
40|$|Abstract. In this paper, {{we propose}} the {{canonical}} correlation kernel (CCK), that seamlessly integrates {{the advantages of}} lower dimensional representation of videos with a discriminative classifier like SVM. In the process of defining the kernel, we learn a low-dimensional (linear as well as nonlinear) representation of the video data, which is originally represented as a tensor. We densely compute features at single (or two) frame level, and avoid any explicit tracking. Tensor representation provides the holistic view of the video data, which is {{the starting point of}} computing the CCK. Our kernel is defined in terms of the principal angles between the lower dimensional representations of the tensor, and captures the similarity of two videos in an efficient manner. We test our approach on four public data sets and demonstrate consistent superior results over the state of <b>the</b> art <b>methods,</b> <b>including</b> those <b>that</b> use canonical correlations. ...|$|R
40|$|The subcontractor’s {{selection}} {{problem is}} currently {{treated as a}} supply chain problem with a prequalification procedure to balance the main objectives of the client: cost, quality, and time. Unfortunately, most of the selection processes are analysed under the same methodology without considering that variations in project, type of activity, and other attributes should affect <b>the</b> chosen <b>method.</b> To provide a novel form of treating subcontractor’s selection, we proposed an additive sorting method to categorize activities to be outsourced in civil construction based on ROR-UTADIS method, which is a modification of <b>the</b> UTADIS <b>method</b> <b>that</b> <b>includes</b> new forms of supplying preference information. It was applied {{in the construction of}} a brewery in Brazil. It was perceived that <b>the</b> <b>method</b> is applicable and intuitive for decision makers, even though {{there are quite a few}} points to be taken, analysed to avoid misclassification...|$|R
40|$|Necessary and {{sufficient}} conditions are given {{so that the}} Sobolev-type partial differential equations generate a contraction semigroup. It is shown that any nonlinear contraction from L/sup 1 /(R) to itself that preserves the integral and commutes with translations satisfies maximum and minimum principles. This lemma {{is applied to the}} solution operator S/sub t/ to give necessary {{and sufficient}} conditions that S/t/ satisfy a maximum principle, despite the dispersive nature. Sufficient conditions are given so that the solutions converge, as nu and beta tend to zero, to the entropy solution of the conservation law. A larger class of monotone finite-difference schemes for the numerical solution of the conservation law motivated by finite-difference discretizations of the Sobolev equations, is introduced, and convergence results are proved for methods in this class. <b>The</b> <b>methods</b> analyzed <b>include</b> some <b>that</b> were previously used to approximate the solution of a linear waterflood problem in petroleum engineering...|$|R
40|$|We {{develop a}} simple {{systematic}} method, valid for all strengths of disorder, to obtain analytically the full distribution of conductances P(g) for a quasi one dimensional wire within {{the model of}} non-interacting fermions. <b>The</b> <b>method</b> {{has been used in}} [1 - 3] to predict sharp features in P(g) near g= 1 and the existence of non-analyticity in the conductance distribution in the insulating and crossover regimes, as well as to show how P(g) changes from Gaussian to log-normal behavior as the disorder strength is increased. Here we provide many details of <b>the</b> <b>method,</b> <b>including</b> intermediate results <b>that</b> offer much insight into the nature of the solutions. In addition, we show within the same framework that while for metals P(g) is a Gaussian around g >> 1, there exists a log-normal tail for g << 1, consistent with earlier field theory calculations. We also obtain several other results that compare very well with available exact results in the metallic and insulating regimes. Comment: 9 figures, 50 pages (figures included). To appear in Annals of Physic...|$|R
40|$|International audienceThis paper {{presents}} {{a method for}} monitoring mental state of small isolated crews during long-term missions (such as space mission, polar expeditions, submarine crews, meteorological stations, and etc). It combines the records of negotiation game with monitoring of the nonverbal behavior of the players. We analyze the records of negotiation game {{that has taken place}} between the crew members who were placed in isolated environment for 105 days during the Mars- 500 experiment. The outcomes of the analysis, differently from the previously made conclusions, show that there was not a significant deviation of the rational choice of the players. We propose an extension of <b>the</b> <b>method</b> <b>that</b> <b>includes</b> monitoring of <b>the</b> nonverbal behavior of the players next to recording the game records. <b>The</b> <b>method</b> is focused on those aspects of psychological and sociological states that are crucial for the performance of the crew. In particular, we focus on measuring of emotional stress, initial signs of conflicts, trust, and ability to collaborate...|$|R
40|$|Facing the {{increasing}} {{amount of information}} available on the World Wide Web, intelligent techniques for content-based information filtering gain more and more importance. Conventional approaches using keyword- or text-based retrieval methods have been developed that perform reasonably well. However, these approaches have problems with ambiguous and imprecise information. The semantic web that aims at supplementing information sources with a formal specification of its meaning using ontologies can potentially help to overcome this problem. At the moment, however, the semantic web still su#ers from its own problems in terms of heterogeneous ontologies {{and the need to}} relate them to each other. In this paper, we argue that we can overcome this problem by using shared vocabularies, a standardized language for encoding ontology that supports basic terminological reasoning (in this case DAML+OIL) and techniques from approximate reasoning. We introduce the approach on an informal level using didactic example and give a formal characterization of <b>the</b> <b>method</b> <b>that</b> <b>include</b> correctness proofs for the problem of information filtering...|$|R
40|$|Framework for user {{modeling}} is represented that {{is useful for}} both supervised and unsupervised machine learning techniques which will {{reduce the cost of}} development that is typically related to the knowledge-based approaches of machine learning for supervised approaches and {{user modeling}} that is basically required for the handling of the label-data. Experimental data is used for Research in bioinformatics. Vast amounts of experimental data populate the Current biological databases. Bioinformatics uses the machine learning concepts and has attained a lot of success in this research field. We focus on semi-surprised framework which incorporates labeled and unlabeled data in the general-purpose learner. Some of transfer graph, learning algorithms and <b>the</b> standard <b>methods</b> <b>that</b> <b>include</b> support vector machines and as a special case the regularized least squares can be obtained. We can use properties of reproducing the kernel Hilbert space to prove the new. Represented theorems provide the theoretical base for algorithms...|$|R
40|$|The coupled-cluster and Brueckner {{electron}} correlation {{methods are}} {{used to determine the}} equilibrium structures, dipole moments, harmonic vibrational frequencies, and IR intensities of NH 3, FON, Be 3, BeC 2, and BeO 2. The singles and doubles coupled-cluster and Brueckner doubles methods are employed, and <b>the</b> corresponding <b>methods</b> <b>that</b> <b>include</b> a perturbational estimate of connected triple excitations are also investigated. The T sub 1 diagnostic is found to provide a good indication of the magnitude of the difference between the results obtained with the coupled-cluster and Brueckner methods. For NH 3, the T sub 1 diagnostic is small and so the differences between results obtained from coupled-cluster and Brueckner theories are quite small. It is found for all of the molecules under consideration that inclusion of the contribution from connected triple excitations {{is more important than the}} differences between the Brueckner and coupled-cluster correlation methods...|$|R
40|$|The {{identification}} of non-linear stochastic spatio-temporal dynamical systems given by stochastic partial differential equations {{is of great}} significance to engineering practice, since it can always provide useful insight into the mechanism and physical characteristics of the underlying dynamics. In this study, based on <b>the</b> difference <b>method</b> for stochastic partial differential equations, a novel state-space model named multi-input-multi-output extended partially linear model for stochastic spatio-temporal dynamical system is proposed. A new Reproducing Kernel Hilbert Space-based algorithm named extended partially linear least square ridge regression is thus particularly developed for the {{identification of}} the extended partially linear model. Compared with existing identification methods available for spatio-temporal dynamics, the advantages of <b>the</b> proposed identification <b>method</b> <b>include</b> <b>that</b> (i) it can make full use of the partially linear structural information of physical models, (ii) it can achieve more accurate estimation results for system non-linear dynamics and (iii) the resulting estimated model parameters have clear physical meaning or properties closely related to the underlying dynamical system. Moreover, the proposed extended partially linear model also provide a convenient state-space model for system analysis and design (e. g. controller or filter design) of the class of non-linear stochastic partial differential dynamical systems. Department of Mechanical Engineerin...|$|R
