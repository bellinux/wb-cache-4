108|48|Public
5000|$|... the omitted {{variable}} {{must be a}} {{determinant of}} the dependent variable (i.e., its <b>true</b> <b>regression</b> coefficient is not zero); and ...|$|E
40|$|We {{investigate}} two empirical Bayes {{methods and}} a hierarchical Bayes method for adapting {{the scale of}} a Gaussian process prior in a nonparametric regression model. We show that all methods lead to a posterior contraction rate that adapts to the smoothness of the <b>true</b> <b>regression</b> function. Furthermore, we show that the corresponding credible sets cover the <b>true</b> <b>regression</b> function whenever this function satisfies a certain extrapolation condition. This condition depends on the specific method, but is implied by a condition of self-similarity. The latter condition is shown {{to be satisfied with}} probability one under the prior distribution...|$|E
40|$|Our {{goal is to}} {{accurately}} estimate the error in any prediction of a regression model. We propose a probabilistic regression framework for basis function regression models, which includes widely used kernel methods such as support vector machines and nonlinear ridge regression. The framework outputs a point specific estimate of {{the probability that the}} <b>true</b> <b>regression</b> surface lies between two user specified values, denoted by y 1 and y 2. More formally, given any y 2 > y 1, we estimate the Pr(y 1 f(x)), where y is a <b>true</b> <b>regression</b> surface, x is the input, and f(x) is the basis function model. Thus the framework encompasses the less general standard error bar approach used in regression...|$|E
40|$|Analysis of 65 {{children}} over a 4 -year period on tests of operational and causal thinking offers support for Piagetls notion of stage progression. In kindergarten and grade one, {{the majority of}} children in this longitudinal study were between preoperational and the achievement stage of operational thought. By grade tvo, the majority had attained the terminal stage on seven of nine tests given. Dy grade three, the children achieved terminal stages on all tests. Over a 3 -year period only eight <b>true</b> <b>regressions</b> occurred. This number constituted {{less than one percent}} of the total possible regressions. Both the Piaget test scores and the Wechsler intelligence scale for Children measures were slightly higher for the regressing children. In thi? study, "regressing children', were not less intelligent than nonregressinq children. However, the numbers were too small to warrant any conclusions. (WY...|$|R
40|$|The main {{contributions}} {{of this paper}} can be summarized as follows. First, we compare the Partial Least Squares (PLS) and the Principal Component Analysis (PCA), under fairly general conditions. (In particular, {{the existence of a}} <b>true</b> linear <b>regression</b> is not assumed.) We prove that PLS and PCA are equivalent, to within a first-order approximation, hence providing a theoretical explanation for empirical findings reported by other researchers. Secondly, we assume the existence of a <b>true</b> linear <b>regression</b> equation and obtain an asymptotic formula for the mean-square error (MSE) of the PLS parameter estimator. We use the derived MSE formula in an analytical example to compare the performance achieved by PLS with that of the ordinary LS (OLS). Keywords: biased regression, partial least squares regression, principal component analysis, first-order analysis, mean square error study. This work has been supported by Swedish Research Council for Engineering Sciences under contract 93 [...] 00669. [...] ...|$|R
40|$|This paper {{examines}} the classical {{theory of the}} relationship between the money supply, inflation, and output. The purpose of the paper is to determine empirically if the quantity theory of money holds <b>true.</b> Using <b>regression</b> analysis, one can observes if the theory is accurate. Taking data over time and from three separate countries, I used the ordinary least squares method to determine the correctness of the quantity theory of money. I used a large amount of other statistically methods to determine the preciseness of the theory...|$|R
30|$|A {{simulation}} in R using known {{regression coefficients}} shows the technique accurately estimates the <b>true</b> <b>regression</b> coefficients when {{the data are}} contaminated by outliers, with performance {{comparable to that of}} Aeberhard et al. (2014). The technique also accurately identifies the sample observations subject to contamination.|$|E
40|$|Minimax L_ 2 {{risks for}} high-dimensional nonparametric {{regression}} are derived under two sparsity assumptions: (1) the <b>true</b> <b>regression</b> surface is a sparse function that depends only on d=O(n) important predictors among {{a list of}} p predictors, with p=o(n); (2) the <b>true</b> <b>regression</b> surface depends on O(n) predictors but is an additive function where each additive component is sparse but may contain two or more interacting predictors and may have a smoothness level different from other components. For either modeling assumption, a practicable extension of the widely used Bayesian Gaussian process regression method is shown to adaptively attain the optimal minimax rate (up to n terms) asymptotically as both n,p→∞ with p=o(n). Comment: Published at [URL] in the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|In {{this article}} we study the {{asymptotic}} predictive optimality of a model selection criterion based on the cross-validatory predictive density, already available in the literature. For a dependent variable and associated explanatory variables, we consider a class of linear models as approximations to the <b>true</b> <b>regression</b> function. One selects a model among these using the criterion under study and predicts a future replicate of the dependent variable by an optimal predictor under the chosen model. We show that for squared error prediction loss, this scheme of prediction performs asymptotically {{as well as an}} oracle, where the oracle here refers to a model selection rule which minimizes this loss if the <b>true</b> <b>regression</b> were known. Comment: Published in at [URL] the IMS Collections ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|We propose {{inference}} {{tools for}} least angle regression and the lasso, from the joint distribution of suitably normalized spacings of the LARS algorithm. From this we extend {{the results of}} the asymptotic null distribution of the “covariance test ” of Lockhart et al. (2013). But we go much further, deriving exact finite sample results for a new asymptotically equivalent procedure called the “spacing test”. This provides exact conditional tests at any step of the LAR algorithm as well as “selection intervals ” for the appropriate <b>true</b> underlying <b>regression</b> parameter. Remarkably, these tests and intervals account correctly for the adaptive selection done by LARS...|$|R
40|$|We {{report on}} a boy with {{ambiguous}} genitalia (hypospadias and cryptorchidism), a 46,XX/ 46,XY karyotype, complete Wolffian differentiation and no Mullerian derivatives, a left ovotestis, and no gonadal tissue on the right side. <b>True</b> hermaphroditism and <b>regression</b> of testicular tissue in this case indicate that chimerism may not only disturb gonadal differentiation but also arrest its development...|$|R
40|$|Many machine {{learning}} tools for regression {{are based on}} recursive partitioning of the covariate space into smaller regions, where the regression function can be estimated locally. Among these, regression trees and their ensembles have demonstrated impressive empirical performance. In this work, we {{shed light on the}} machinery behind Bayesian variants of these methods. In particular, we study Bayesian regression histograms, such as Bayesian dyadic trees, in the simple regression case with just one predictor. We focus on the reconstruction of regression surfaces that are piecewise constant, where the number of jumps is unknown. We show that with suitably designed priors, posterior distributions concentrate around the <b>true</b> step <b>regression</b> function at a near-minimax rate. These results do not require the knowledge of the true number of steps, nor the width of the true partitioning cells. Thus, Bayesian dyadic regression trees are fully adaptive and can recover the <b>true</b> piecewise <b>regression</b> function nearly as well as if we knew the exact number and location of jumps. Our results constitute the first step towards understanding why Bayesian trees and their ensembles have worked so well in practice. As an aside, we discuss prior distributions on balanced interval partitions and how they relate to an old problem in geometric probability. Namely, we relate the probability of covering the circumference of a circle with random arcs whose endpoints are confined to a grid, a new variant of the original problem...|$|R
40|$|AbstractFor a {{well-known}} class of nonparametric regression function estimators of nearest neighbor type the uniform measure of {{deviation from the}} estimators to the <b>true</b> <b>regression</b> function is studied. Under weak regularity conditions it is shown that the estimators are uniformly consistent with probability one and the corresponding rate of convergence is near-optimal...|$|E
40|$|When {{employing}} model selection {{methods such as}} the Lasso and the Adaptive Lasso, it {{is typical}} to estimate the smoothing parameter by m-fold crossvalidation, e. g., m = 10. In problems where the <b>true</b> <b>regression</b> function is sparse and the signals large, such crossvalidation typically works well, with the Adaptive Lasso being an oracle method. However, in regression modeling of genomic studies involving Single Nucleotide Polymorphisms (SNP), the <b>true</b> <b>regression</b> functions, while thought to be sparse, do not have large signals. We demonstrate empirically that in such problems, the number of selected variables using the Adaptive Lasso with 10 -fold crossvalidation is a random variable that has considerable and surprising variation. Similar remarks apply to the Lasso. Our study strongly questions the suitability of performing only a single run of m-fold crossvalidation with any oracle method, {{and not just the}} Adaptive Lasso...|$|E
40|$|In this paper, {{we propose}} a new semiparametric {{regression}} estimator {{by using a}} hybrid technique of a parametric approach and a nonparametric penalized spline method. The overall shape of the <b>true</b> <b>regression</b> function is captured by the parametric part, while its residual is consistently estimated by the nonparametric part. Asymptotic theory for the proposed semiparametric estimator is developed, showing that its behavior {{is dependent on the}} asymptotics for the nonparametric penalized spline estimator {{as well as on the}} discrepancy between the <b>true</b> <b>regression</b> function and the parametric part. As a naturally associated application of asymptotics, some criteria for the selection of parametric models are addressed. Numerical experiments show that the proposed estimator performs better than the existing kernel-based semiparametric estimator and the fully nonparametric estimator, and that the proposed criteria work well for choosing a reasonable parametric model. Comment: 20 pages, 3 figure...|$|E
40|$|Expressions {{are derived}} for {{generalized}} ridge (GR), ordinary ridge (OR) and shrunken least squares (SLS) predictors that are optimal for predicting the response {{at a single}} or at multiple future observations. As {{in the case of}} biased estimation, these predictors depend on the <b>true</b> (population) <b>regression</b> coefficient values and the true variance of the underlying linear regression model. Hence, we propose operational predictors where the unknown parameters in the biased predictors are estimated from the data. Using the Mean Squared Error of Prediction (MSEP) criterion, we compare the proposed predictors with the OLS predictor. Several traditional biased predictors, including the predictors based on th...|$|R
40|$|Two new models, the {{covariance}} and {{regression slope}} models, are proposed for assessing validity generalization. The new models are less restrictive {{in that they}} require only one hypothetical distribution (distribution of range restriction for the covariance model and distribution of predictor reliability for the regression slope model) for their implementation, {{in contrast to the}} correlation model which requires hypothetical distributions for criterion reliability, predictor reliability, and range restriction. The new models, however, are somewhat limited in their applicability since they both assume common metrics for predictors and criteria across validation studies. Several simulation (monte carlo) studies showed the new models to be quite accurate in estimating the mean and variance of population <b>true</b> covariances and <b>regression</b> slopes. The results also showed that the accuracy of the covariance, regression slope, and correlation models is affected by the degree to which hypothetical distributions of artifacts match their <b>true</b> distributions; the <b>regression</b> slope model appears to be slightly more robust than the other two models...|$|R
40|$|Ford and Silvey [(1980) A {{sequentially}} constructed {{design for}} estimations a nonlinear parametric function. Biometrika 67, 381 - 388] conjectured that the usual asymptotic confidence interval arising from a sequential design {{based on a}} nonlinear model has the same asymptotic properties as it would if it had arisen from a fixed design, and simulation results support this conjecture favorably. We give a simple martingale proof that their conjecture is <b>true.</b> Martingales Nonlinear <b>regression</b> models Sequential design...|$|R
30|$|Where Se {{refers to}} the local {{standard}} error of the k th parameter estimate, taking in account the variation in the data (Cheng et al. 2011). Pseudo t-values follow approximately a standard normal distribution if the <b>true</b> <b>regression</b> parameter is zero and mapping them is useful for identifying spatial variations in relationships between explanatory covariates and the outcome covariate.|$|E
40|$|For a {{well-known}} class of nonparametric regression function estimators of nearest neighbor type the uniform measure of {{deviation from the}} estimators to the <b>true</b> <b>regression</b> function is studied. Under weak regularity conditions it is shown that the estimators are uniformly consistent with probability one and the corresponding rate of convergence is near-optimal. Strong consistency regression function near neighbour rules...|$|E
40|$|Whenever the {{specification}} of a dynamic regression relationship is in doubt, we should think of adopting a rational transfer-function model with separate parameters in the systematic part and the disturbance part. Some {{of the models}} which are commonly used in applied econometrics can give rise to very misleading estimates when the {{two parts of the}} <b>true</b> <b>regression</b> relationship have different dynamic properties. ...|$|E
50|$|This {{problem is}} {{especially}} <b>true</b> for total <b>regression</b> reparenting. Clients in therapy are totally immersed {{in an environment}} that promotes regression into the child ego state. However, during this time at which the client may spend months or years, the client is left {{at the mercy of the}} therapist and the institution. There is no higher authority to monitor the actions of the therapist as well as the well being of the client. The client is left to trust his or her therapist throughout therapy.|$|R
40|$|We {{prove that}} {{boosting}} with the squared error loss, L 2 Boosting, is consistent for very high-dimensional linear models, where {{the number of}} predictor variables is allowed to grow essentially as fast as O(exp(sample size)), assuming that the <b>true</b> underlying <b>regression</b> function is sparse {{in terms of the}} ℓ 1 -norm of the regression coefficients. In the language of signal processing, this means consistency for de-noising using a strongly overcomplete dictionary if the underlying signal is sparse in terms of the ℓ 1 -norm. We also propose here an AICbased method for tuning, namely for choosing the number of boosting iterations. This makes L 2 Boosting computationally attractive since it is not required to run the algorithm multiple times for cross-validation as commonly used so far. We demonstrate L 2 Boosting for simulated data, in particular where the predictor dimension is large in comparison to sample size, and for a difficult tumor-classification problem with gene expression microarray data...|$|R
40|$|We {{propose a}} method for {{constructing}} p-values for general hypotheses in a high-dimensional linear model. The hypotheses can be local for testing a single regression parameter {{or they may be}} more global involving several up to all parameters. Furthermore, when considering many hypotheses, we show how to adjust for multiple testing taking dependence among the p-values into account. Our technique is based on Ridge estimation with an additional correction term due to a substantial projection bias in high dimensions. We prove strong error control for our p-values and provide sufficient conditions for detection: for the former, we do not make any assumption {{on the size of the}} <b>true</b> underlying <b>regression</b> coefficients while regarding the latter, our procedure might not be optimal in terms of power. We demonstrate the method in simulated examples and a real data application. Comment: Published in at [URL] the Bernoulli ([URL] by the International Statistical Institute/Bernoulli Society ([URL]...|$|R
40|$|Many nonparametric {{regression}} estimators (smoothers) {{have been}} proposed that provide a more flexible method for estimating the <b>true</b> <b>regression</b> line compared to {{using some of the}} more obvious parametric models. A basic goal when using any smoother is computing a confidence band for the <b>true</b> <b>regression</b> line. Let M(Y|X) be some conditional measure of location associated with the random variable Y, given X and let x be some specific value of the covariate. When using the LOWESS estimator, an extant method that assumes homoscedasticity can be used to compute a confidence interval for M(Y|X = x). A trivial way of computing a confidence band is to compute confidence intervals for K covariate values, each having probability coverage 1 − α. But an obvious concern is that the simultaneous probability coverage can be substantially smaller than 1 − α. A method is suggested for dealing with this issue that allows heteroscedasticity and simultaneously performs better than the Bonferroni method or the Studentized maximum modulus distribution...|$|E
40|$|Nonparametric {{regression}} is {{a standard}} tool to uncover statistical relationships between pairs of random variables. Unfortunately, implementation of fully nonparametric smoothers are negatively impacted by the curse of dimensionality, which is why until now, these tool have been had limited success in image analysis. Recent advances in nonparametric smoothing [1] have shown that a simple iterative bias correction scheme can adapt to the underlying smoothness of the <b>true</b> <b>regression</b> function, {{and as a result}} can partially mitigate the curse of dimensionality. Practically, one can get good multivariate smoothers in dimensions of up to 20 to 50 dimensions (when the <b>true</b> <b>regression</b> is smooth). This makes it now possible to explore casting the problem of image denoising and image inpainting as a nonparametric smoothing problem. The first part of the talk will discuss the theory underlying the iterative bias reduction smoothing approach (as implemented in R [2]). The second half present some recent results on applying that method to image data and comparison with existing procedures...|$|E
40|$|In data anaysis {{concerning}} {{the investigation of}} the relationship between a dependent variable Y and an independent variable X, one may wish to determine whether this relationship is monotone or not. This determination may be of interest in itself, or it may form part of a (nonparametric) regression analysis which relies on monotonicity of the <b>true</b> <b>regression</b> function. In this paper we generalize the test of positive correlation by proposing a test statistic for monotonicity based on fitting a parametric model, say a higher order polynomial, to the data with and without the monotonicity constraint. The statistic has an asymptotic chi-bar-squared distribution under the null hypothesis that the <b>true</b> <b>regression</b> function is on the boundary of the space of monotone functions. Based on the theoretical results, an algorithm is developed for testing the significance of the statistic, and it is shown to perform well in several null and non-null settings. Extensions to fitting regression splines [...] ...|$|E
40|$|Classification in {{bioinformatics}} often {{suffers from}} small samples {{in conjunction with}} large numbers of features, which makes error estimation problematic. When a sample is small, there is insufficient data to split the sample and the same data are used for both classifier design and error estimation. Error estimation can suffer from high variance, bias, or both. The problem of choosing a suitable error estimator is exacerbated by the fact that estimation performance depends on the rule used to design the classifier, the feature-label distribution to which the classifier is to be applied, and the sample size. This paper reviews the performance of training-sample error estimators with respect to several criteria: estimation accuracy, variance, bias, correlation with the <b>true</b> error, <b>regression</b> on the <b>true</b> error, and accuracy in ranking feature sets. A number of error estimators are considered: resubstitution, leave-one-out cross-validation, 10 -fold cross-validation, bolstered resubstitution, semi-bolstered resubstitution,. 632 bootstrap,. 632 + bootstrap, and optimal bootstrap...|$|R
40|$|Three linear equating {{methods for}} the common item non-equivalent populations design, a design often used in practice, are {{compared}} by an analytical method. The models include: (1) Tucker's equally reliable method; (2) Levine's equally reliable methods; and (3) the congeneric method recently introduced by Woodrull (1986). Tucker's method makes assumptions about observed score regressions and is based oa a linear regression model. The Levine and congeneric methods make assumptions about <b>true</b> score <b>regressions</b> and are based on linear structural models. The analysis is graphically illustrated using data from actual test administrations. If groups differ greatly as shown by their performance in the anchor and if application of Tucker's equating method is not tenable, the disattenuated correlation between Y and V should be computed. If this disattenuated correlation is significantly less than unity, the Levine method should not be used, and the congeneric method becomes an appealing alternative. (SLD) Reproductions supplied by EDRS are the best {{that can be made}} from the original document...|$|R
40|$|Incorrect {{versions}} of Figures 5 and 6 containing nor-malization errors were accidentally published by Borcherdt (2002). They {{should be replaced}} with the figures shown here. The text and tabulated regression values published in Borch-erdt (2002) are correct and refer to the figures shown here. Figure 5. Borcherdt—Average spectral ratio for the short-period band for site class D (a) and site class C (b). The empirical data as derived using the hypocentral distance norm and the Abrahamson and Silva (A&S) spectral norm at 0. 3 sec are plotted on each figure. Corresponding regression curves, 95 % confidence limits for the observed values, and the 80 % confidence limits for the ordinate to the <b>true</b> population <b>regression</b> line are shown. 374 Erratum Figure 6. Borcherdt—Average spectral ratio for the mid-period band for site classes D and C. The empirical data as derived using the hypocentral distance norm and the Abra-hamson and Silva (A&S) spectral norm at 1. 0 sec are plotted on each figure. Corresponding regression curves, 95 % confidence limits for the observed values, and the 80 % confidenc...|$|R
40|$|We {{consider}} the kernel partial least squares algorithm for non-parametric regression with stationary dependent data. Probabilistic convergence {{rates of the}} kernel partial least squares estimator to the <b>true</b> <b>regression</b> function are established under a source and an effective dimensionality condition. It is shown both theoretically and in simulations that long range dependence results in slower convergence rates. A protein dynamics example shows high predictive power of kernel partial least squares...|$|E
40|$|Regularization is an {{essential}} element of virtually all kernel methods for nonparametric regression problems. A critical factor in the effectiveness of a given kernel method is the type of regularization that is employed. This article compares and contrasts members from a general class of regularization techniques, which notably includes ridge regression and principal component regression. We derive an explicit finite-sample risk bound for regularization-based estimators that simultaneously accounts for (i) the structure of the ambient function space, (ii) the regularity of the <b>true</b> <b>regression</b> function, and (iii) the adaptability (or qualification) of the regularization. A simple consequence of this upper bound is that the risk of the regularization-based estimators matches the minimax rate in a variety of settings. The general bound also illustrates how some regularization techniques are more adaptable than others to favorable regularity properties that the <b>true</b> <b>regression</b> function may possess. This, in particular, demonstrates a striking difference between kernel ridge regression and kernel principal component regression. Our theoretical results are supported by numerical experiments. Comment: 19 pages, 4 figure...|$|E
40|$|Computer {{techniques}} for scatter-plot smoothing are valuable exploratory analysis tools. However, one must exercise caution when making inferences based on {{features of the}} smoothed data. Here, the size and number of observed modes are studied: the asymptotic distributions are determined under the assumption of constancy of the <b>true</b> <b>regression</b> function, and sufficient conditions for consistency of the estimated number of modes and their locations are given in the general case. Mode bump hunting Poisson approximation...|$|E
40|$|Ankara : The Department of Economics, Bilkent University, 2010. Thesis (Master's) [...] Bilkent University, 2010. Includes bibliographical {{references}} leaves 72 - 74. This thesis {{provides a}} simulation based study on Kalman Filter estimation of time varying parameter models when nonstationary series {{are included in}} regression equation. In this study, we have performed several simulations in order to present the outcomes and ramifications of Kalman Filter estimation applied to time varying regression models {{in the presence of}} random walk series. As a consequence of these simulations, we demonstrate that Kalman Filter estimation cannot prevent the emergence of spurious regression in time varying parameter models. Furthermore, so as to detect the presence of spurious regression, we also propose a new method, which suggests penalizing Kalman Filter recursions with endogenously generated series. These series, which are created endogenously by utilizing Cochrane’s variance ratio statistic, are replaced by state evolution parameter Tt in transition equation of time varying parameter model. Consequently, Penalized Kalman Filter performs well in distinguishing nonsense relation from a <b>true</b> cointegrating <b>regression.</b> Eroğlu, Burak AlparslanM. S...|$|R
30|$|Compared {{to average}} years of {{schooling}} and lowest {{years of schooling}}, the highest years of schooling has a more stable effect on parents’ life quality at old age because there are more variables that are significant and at a more significant level. If we construct a comparison within the significant variables, {{we can see that}} the standardized partial coefficient is relatively larger. As an example, this is <b>true</b> for the <b>regression</b> of self-assessed health condition, and the partial regression coefficient on the highest years of schooling is 0.077, on the lowest years of schooling is 0.055, and on the average years of schooling is 0.068.|$|R
40|$|One of {{the biggest}} {{challenges}} in nonparametric regression is the curse of dimensionality. Additive models are known to overcome this problem by estimating only the individual additive effects of each covariate. However, if the model is misspecified, the accuracy of the estimator compared to the fully nonparametric one is unknown. In this work the efficiency of completely nonparametric regression estimators such as the Loess is compared to the estimators that assume additivity in several situations, including additive and non-additive regression scenarios. The comparison is done by computing the oracle mean square error of the estimators with regards to the <b>true</b> nonparametric <b>regression</b> function. Then, a backward elimination selection procedure based on the Akaike Information Criteria is proposed, which is computed from either the additive or the nonparametric model. Simulations show that if the additive model is misspecified, the percentage of time it fails to select important variables can be higher than that of the fully nonparametric approach. A dimension reduction step is included when nonparametric estimator cannot be computed due to the curse of dimensionality. Finally, the Boston housing dataset is analyzed using the proposed backward elimination procedure and the selected variables are identified...|$|R
