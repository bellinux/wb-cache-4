10|10000|Public
50|$|Sometimes {{the optical}} module is {{replaced}} by an electrical interface module that implements either an active or passive electrical connection to the outside world. This is used when the link is short, particularly when connecting to a <b>top</b> <b>of</b> <b>rack</b> switch.|$|E
50|$|The 5500 {{series are}} often used as <b>Top</b> <b>Of</b> <b>Rack</b> (TOR) {{switches}} and client access-switches in wiring cabinets in offices or campus networks. The 5500P series are mainly client access switches connecting VOIP phones and (daisy chained or directly connected) workstations. The -P series are also used to power other devices than phones, such as WiFi Access-points, IP camera's or thin clients.|$|E
50|$|From a {{datacenter}} view, {{the network}} {{starts at the}} rack level, where 19-inch racks are custom-made and contain 40 to 80 servers (20 to 40 1U servers on either side, while new servers are 2U rackmount systems. Each rack has a switch). Servers are connected via a 1 Gbit/s Ethernet link to the <b>top</b> <b>of</b> <b>rack</b> switch (TOR). TOR switches are then connected to a gigabit cluster switch using multiple gigabit or ten gigabit uplinks. The cluster switches themselves are interconnected and form the datacenter interconnect fabric (most likely using a dragonfly design rather than a classic butterfly or flattened butterfly layout).|$|E
5000|$|... #Caption: TXE3 <b>racks</b> see <b>top</b> <b>of</b> the <b>racks</b> for description. The MCU {{is vastly}} {{different}} from the TXE4 and includes the cyclic store ...|$|R
5000|$|... #Caption: <b>Top</b> end <b>of</b> the <b>rack</b> {{railway section}} in Paranapiacaba ...|$|R
5000|$|... #Caption: At a Hillsong United {{show at the}} Gigantinho {{sports arena}} in Brazil, a Dolby Lake Processor shows its four {{circular}} control displays at the <b>top</b> <b>of</b> other <b>racked</b> gear.|$|R
5000|$|The PowerConnect 7024 and 7048 were {{introduced}} April 1, 2011. They {{had the same}} ports as the 6224/6248, QoS features for iSCSI, and incorporate 802.3az Energy Efficient Ethernet. The 7000-series offer {{the same type of}} ports: both the 1G on the front as optional 10G and stacking modules on the rear-side. Unlike the 6200 series, firmware for the 7000 series does support new functionality via version 4.x and 5.x like their 10G brothers in the 8024 and 8100 series.A variant with reversible air flow is available for <b>top</b> <b>of</b> <b>rack</b> data center applications. The 6000 series remained on the market. The PCT7000 series also offer an out-of-band management ethernet interface. You can configure the switch to allow both in-band as out-of-band management. By default the oob interface allows management per webgui (http) and telnet, but also https and ssh can be enabled on both in-band as out-of-band management. The PCT7000 series can be stacked with other PCT7000's but also with the PCM6348 blade switch ...|$|E
5000|$|The Nexus 4000 series {{consists}} {{of only the}} model 4001: a blade-switch module for IBM BladeCenter that has all 10 Gbit Fibre Channel over Ethernet or FCoE interfaces. This blade-switch had 14 server-facing downlinks running on 1Gbit/s or 10 Gbit/s and six uplinks using 10Gbit/s SFP+ modules. For out-of-band management three ethernet-interfaces are available: one external 10/100/1000 bit/s copper interface, one internal management interface for the AMM or Advanced Management Module and one in-band interface using the VLAN interface option. And this blade-switch also has a serial console cable for {{direct access to the}} CLIAt present only switches for the IBM blade systems are available. When the Nexus 4000 series were announced in 2009 it was expected that there would be Nexus 4001 series for IBM and Dell (and not HP) but in February 2010 it became clear that Cisco canceled the Nexus 4001d for the Dell M1000eFor the HP blade system Cisco released a Fabric Extender, which compares with the Nexus 2000 <b>top</b> <b>of</b> <b>rack</b> devices, but then in a blade-form factor. The FEX that was developed for the Dell blade system, which was due to be released in the summer of 2010 was dropped {{at the same time as}} the Nexus 4001d in February of that year ...|$|E
40|$|This paper reports an FPGA-based {{switch and}} {{interface}} card (SIC) and its application scenario in an all-optical, programmable disaggregated data center network (DCN). Our novel SIC is designed and implemented to replace traditional optical network interface cards, {{plugged into the}} server directly, supporting optical packet switching (OPS) /optical circuit switching (OCS) or time division multiplexing (TDM) /wavelength division multiplexing (WDM) traffic on demand. Placing the SIC in each server/blade, we eliminate electronics from the <b>top</b> <b>of</b> <b>rack</b> (ToR) switch by pushing all the functionality on each blade while enabling direct intrarack blade-to-blade communication to deliver ultralow chip-to-chip latency. We demonstrate the disaggregated DCN architecture scenarios along with all-optical dimension-programmable N × M spectrum selective Switches (SSS) and an architecture-on-demand (AoD) optical backplane. OPS and OCS complement each other as do TDM and WDM, which can support variable traffic flows. A flat disaggregated DCN architecture is realized by connecting the optical ToR switches directly to either an optical top of cluster switch or the intracluster AoD optical backplane, while clusters are further interconnected to an intercluster AoD for scaling out...|$|E
50|$|Separate I/O drawers, {{placed at}} the <b>top</b> <b>of</b> a <b>rack</b> or in a {{separate}} rack, are air cooled and contain 8 compute cards and 8 PCIe expansion slots for Infiniband or 10 Gigabit Ethernet networking.|$|R
5000|$|Nylon toys: {{put it in}} the <b>top</b> <b>rack</b> <b>of</b> the dish-washer or wash {{with soap}} and water.|$|R
40|$|To {{address the}} sustainability, scalability, and {{reliability}} problems that data centers are currently facing, we propose three passive optical interconnect (POI) architectures on <b>top</b> <b>of</b> the <b>rack.</b> The evaluation {{results show that}} all three architectures offer high reliability performance (connection availability for intra-rack interconnections higher than 99. 999 %) in a cost-efficient way. QC 20160525 </p...|$|R
40|$|Several recent {{publications}} {{confirm that}} faults {{are common in}} high-performance computing systems. Therefore, further attention to the faults experienced by such computing sys-tems is warranted. In this paper, we present a study of DRAM and SRAM faults in large high-performance com-puting systems. Our goal is to understand {{the factors that influence}} faults in production settings. We examine the impact of aging on DRAM, finding a marked shift from permanent to transient faults {{in the first two years}} of DRAM lifetime. We examine the impact of DRAM vendor, finding that fault rates vary by more than 4 x among vendors. We examine the physical location of faults in a DRAM device and in a data center; contrary to prior studies, we find no correlations with either. Finally, we study the impact of altitude and rack placement on SRAM faults, finding that, as expected, altitude has a substantial impact on SRAM faults, and that <b>top</b> <b>of</b> <b>rack</b> placement correlates with 20 % higher fault rate. 1 Sandia is a multiprogram laboratory operated by Sandia Corporation, a Lockheed Martin Company, for the United States Department of Energy under Contract DE-AC 04 - 94 AL 85000. This document’s Sandia identifier is 2013...|$|E
40|$|Virtualization {{has been}} an {{efficient}} method to fully utilize computing resources such as servers. The way of placing virtual machines (VMs) among a large pool of servers greatly affects the performance of data center networks (DCNs). As network resources have become a main bottleneck {{of the performance of}} DCNs, we concentrate on VM placement with Traffic-Aware Balancing to evenly utilize the links in DCNs. In this paper, we first proposed a Virtual Machine Placement Problem with Traffic-Aware Balancing (VMPPTB) and then proved it to be NP-hard and designed a Longest Processing Time Based Placement algorithm (LPTBP algorithm) to solve it. To take advantage of the communication locality, we proposed Locality-Aware Virtual Machine Placement Problem with Traffic-Aware Balancing (LVMPPTB), which is a multiobjective optimization problem of simultaneously minimizing the maximum number of VM partitions of requests and minimizing the maximum bandwidth occupancy on uplinks of <b>Top</b> <b>of</b> <b>Rack</b> (ToR) switches. We also proved it to be NP-hard and designed a heuristic algorithm (Least-Load First Based Placement algorithm, LLBP algorithm) to solve it. Through extensive simulations, the proposed heuristic algorithm is proven to significantly balance the bandwidth occupancy on uplinks of ToR switches, while keeping the number of VM partitions of each request small enough...|$|E
40|$|During {{the past}} decade, machine {{learning}} has become extremely popular {{and can be}} found in many aspects of our every day life. Nowayadays with explosion of data while rapid growth of computation capacity, Distributed Deep Neural Networks (DDNNs) which can improve their performance linearly with more computation resources, have become hot and trending. However, there has not been an in depth study of the performance of these systems, and how well they scale. In this paper we analyze CNTK, one of the most commonly used DDNNs, by first building a performance model and then evaluating the system two settings: a small cluster with all nodes in a single rack connected to a <b>top</b> <b>of</b> <b>rack</b> switch, and in large scale using Blue Waters with arbitary placement of nodes. Our main focus was the scalability of the system with respect to adding more nodes. Based on our results, this system has an excessive initialization overhead because of poor I/O utilization which dominates the whole execution time. Because of this, the system does not scale beyond a few nodes (4 in Blue Waters). Additionally, due to a single server-multiple worker design the server becomes a bottleneck after 16 nodes limiting the scalability of the CNTK...|$|E
5000|$|There {{are several}} other types <b>of</b> less common <b>rack</b> types {{that are also}} used, based on a [...] "template" [...] to hold the billiard balls tightly together. Most {{commonly}} it is a thin plastic sheet with diamond-shaped cut-outs that hold the balls that {{is placed on the}} table with the balls set on <b>top</b> <b>of</b> the <b>rack.</b> The rack is used to set up the [...] "break" [...] and removed before the [...] "break shot" [...] occurs.|$|R
50|$|Panniers {{are usually}} built to attach to a rear rack or front rack already fitted to the bicycle. Removable panniers hook onto the <b>top</b> edge <b>of</b> the <b>rack</b> {{and are often}} {{held in place by}} a latch or elastic mechanism.|$|R
5000|$|Just inside Le Cellier steakhouse {{there is}} the counter where you would check in for your meal. Behind that counter is a small wine storage room. In the very <b>top</b> center <b>of</b> the <b>rack</b> are three bottles of wine, a large one with two small ones on <b>top</b> <b>of</b> it forming a {{familiar}} shape.|$|R
40|$|Data center {{infrastructure}} {{design has}} recently been receiving significant research interest both from academia and industry, {{in no small part}} due to the growing importance of data centers in supporting and sustaining the rapidly growing web-based applications including search (e. g., Google, Bing), video content hosting and distribution (e. g., You Tube, NetFlix). social networking (e. g., facebook, twitter). and large-scale computations (e. g [...] data mining, bioinformatics, indexing). Today's data centers may contain tens of thousands of computers with significant aggregate bandwidth requirements. For example, the Microsoft Live online services are supported by a Chicago-based data center, {{which is one of the}} largest data centers ever built, spanning more than 700. 000 square feet. and Google has more than I Million servers. As a result, the architecture of the network interconnecting the servers has a significant impact on the agility and reconfigurability of the data center infrastructure to respond to changing application demands and service requirements. Traditionally data center networking was based around <b>top</b> <b>of</b> <b>rack</b> (ToR) switches interconnected through end of rack (EoR) switches, and these in turn are being connected through core switches. This approach, besides being very costly, leads to significant bandwidth oversubscription towards the network core. This prompted several researchers to suggest alternate approaches for scalable cost-effective network infrastructures, based on topologies including Fat-Tree, DCell, BCube, MDCube, and Clos network. In this talk, we detail the trends and challenges in designing massive data centers. We will highlight the research efforts being undertaken by the academic and industrial communities to address these challenges. Finally, we present some of our own solutions by leveraging the key data traffic patterns and web-applications in achieving scalable and cost effective solutions to the design of massive data centers infrastructures...|$|E
5000|$|... #Caption: BBC {{engineers}} equalising audio landlines circa 1959. The {{boxes with}} two large black dials towards the <b>top</b> <b>of</b> the equipment <b>racks</b> are adjustable Zobel equalisers. They are used both for temporary outside broadcast lines and for checking the engineer's calculations prior to building permanent units ...|$|R
40|$|Split {{view of the}} Lander Cloak House in New Castle, Indiana. The top image {{shows the}} {{interior}} of the store. Open umbrellas are displayed on <b>top</b> <b>of</b> cabinets holding <b>racks</b> <b>of</b> coats. Some clothing is also displayed on forms. The bottom image is an exterior view of the shop's entrance. The postcard message announces a January sale being held at the store...|$|R
50|$|Main {{intermediate}} {{station on the}} line at site of the locomotive workshops {{as well as the}} <b>top</b> end <b>of</b> the <b>rack</b> rail. Trains must reverse a short distance before continuing their climb to Udhagamandalam. It is normal for the locomotive to be changed here with diesel traction, being normal for all trains to Udhagamandalam.|$|R
2500|$|... "The Time Ball {{consists}} of a cast iron cylinder, piston, rack and pinion lifting mechanism, safety lock, electric motor and ball. The cylinder is 30cm diameter and 3 m long. Approximately 7 litres of soapy water fills {{the bottom of the}} cylinder to act as a buffer for the piston. The piston is a 10cm thick block of rubber with a bleeder valve to adjust the rate <b>of</b> descent. The <b>rack</b> extends from the piston to the Time Ball and is 7.5 meters long. A pinion engages into the rack by a slide gear, which enables the electric motor to raise the rack. A large hand wheel is also attached with a pawl gear to prevent back-slipping. The Time Ball is attached to the <b>top</b> <b>of</b> the <b>rack</b> and is raised 2.7 metres.|$|R
50|$|The Rack 'n Roll {{field is}} {{dominated}} by 'The Rack', a large metal contraption with three levels of hanging metal bars, with each level having 8 arms evenly spaced in an octagonal manner. Each arm (known as a 'spider leg') has space for two game pieces. Any more pieces placed on a spider leg beyond the first two are ignored for scoring purposes. At {{the beginning of the}} match, the rack is arbitrarily translated or rotated within three feet of the center of the field in order to give some randomness and to encourage autonomous modes that do not depend on dead-reckoning. At the <b>top</b> <b>of</b> the <b>Rack</b> are four green-colored lights above the 1, 3, 5, and 7 legs to aid in autonomous-mode tracking.|$|R
50|$|Blanking {{plates and}} other {{fittings}} around the edge, top, floor, or the rack direct air intake {{so that only}} air from the cold aisle reaches equipment intakes and prevent leakage of exhaust air into the intake area. Fans on the top or rear doors of the cabinet ensure a negative pressure for exhaust air coming out of equipment. Effective airflow management prevents hot spots, which are especially common in the <b>top</b> spaces <b>of</b> a <b>rack,</b> and allows the temperature of cold aisles to be raised.|$|R
50|$|Each ISPR {{provides}} 1.571 m3 (55.5 ft3) {{of internal}} volume being about 2 m (79.3 in) high, 1.05 m (41.3 in) wide, and 85.9 cm (33.8 in) deep. The rack weighs 104 kg (230 lb) and can accommodate an additional 700 kg (1540 lb) of payload equipment. The rack has internal mounting provisions to allow attachment of secondary structure. The ISPRs will be {{outfitted with a}} thin center post to accommodate sub-rack-sized payloads, such as the 483 mm (19-inch rack) Spacelab Standard Interface Rack (SIR) Drawer or the Space Shuttle Middeck Locker. Utility pass-through ports are located on each side to allow cables to be run between Racks. Module attachment points are provided at the <b>top</b> <b>of</b> the <b>rack</b> and via pivot points at the bottom. The pivot points support installation and maintenance. Tracks on the exterior front posts allow mounting of payload equipment and laptop computers. Additional adapters on the ISPRs are provided for ground handling.|$|R
40|$|Abstract. The power {{trend of}} using server systems in data center is {{continuously}} increasing. Cooling system consumed 38 % of total energy usage {{which is a}} significant contribution in the energy consumption. As a result, the efficient energy usage in data center is concerned. Normally a raised-floor is widely used in data center cooling system which delivers cool air through perforated tiles to a front <b>of</b> server <b>racks.</b> However it is usually found that this cool air cannot effectively remove a heat dissipation at the <b>top</b> <b>of</b> server <b>racks.</b> Therefore, the environmental condition in data center must be designed strictly to avoid disruption that caused by overheat. This paper gives some guidelines {{to help in the}} better design. Commercial Computational Fluid Dynamics (CFD) program was used to analyze the air flow from raised-floor air conditioning system. A tetrahedral of 1. 8 million meshes with k-ε turbulence model were used to obtain the air flow velocity and temperature distributions in the room. The model was validated by comparing simulation results with actual measurements. As a result, dimensionless parameters in the form of supply heat index(SHI), for understanding the optimization of relative airflow distribution to the heat load <b>of</b> server <b>rack</b> was found. It shows that these parameters provide an effective tool to the improvement of energy efficiency in raised floor data center...|$|R
5000|$|A new {{approach}} to the inter-rack cabling was taken. A false ceiling was built above the <b>top</b> <b>of</b> the <b>racks,</b> creating a cable loft. The cables were just pushed through holes in the cable loft and taken {{to where they were}} going by the shortest route. The holes were sealed after all the cables had been put in place, as a fire precaution. The result was a complete mess of a cable loft, but all cables were labelled; it was quicker and easier than the normal way of lacing all the cables. The exchange was housed in a prototype K-type single-story building having a special provision reinforced ceiling for the overhead cabling already mentioned. The construction included thermal insulation panels and double-glazing to minimise heat loss, and heating was by under-floor electrical heaters operated on off-peak supplies. Ventilation arrangements were by eight ventilating units, each handling 600 cu. ft. per min, and a series of [...] "hit-and-miss"-type louvers above the windows {{on each side of the}} building provided outlets for heated air.|$|R
5000|$|Heavy {{equipment}} or equipment which is commonly accessed for servicing, for which attaching or detaching at all four corners simultaneously would pose a problem, {{is often not}} mounted directly onto the rack but instead is mounted via rails (or slides). A pair of rails is mounted directly onto the rack, and the equipment then slides into the rack along the rails, which support it. When in place, the equipment may also then be bolted to the rack. The rails may {{also be able to}} fully support the equipment in a position where it has been slid clear of the rack; this is useful for inspection or maintenance of equipment which will then be slid back into the rack. [...] Some rack slides even include a tilt mechanism allowing easy access to the <b>top</b> or bottom <b>of</b> <b>rack</b> mounted equipment when it is fully extended from the rack.|$|R
5000|$|The Japanese Government Railways {{opened the}} Takasaki to Yokokawa section in 1885, the Naoetsu to Sekiyama section the {{following}} year, and the Sekiyama - Nagano - Karuizawa section in 1888. In order to surmount the 552 metre altitude difference between Yokokawa and Karuizawa (which are 10 km apart), it then constructed an Abt rack section through the Usui Pass, {{which opened in}} 1893, and was double-tracked for 1 km from Karuizawa to the <b>top</b> <b>of</b> the <b>rack</b> section. A horse-drawn tramway operated between Yokokawa and Karuizawa until the rack section opened. The Hokuetsu Railway opened the Naoetsu to Nagaoka section in 1897, extending the line to Niigata in 1904. Both companies were nationalised in 1907. In 1909 , the Imperial Japanese Railway authories invited bids for the electrification of the route. A German company was selected to provide the engines and General Electric supplied the turbines at the power station. In 1912, the rack section was electrified using third rail at 600 V DC, this being the first use of this method in Japan. The electrification allowed {{for the use of}} faster and longer trains which reduced journey times and also pollution from the steam engines.|$|R
40|$|A rack for {{laboratory}} {{bottles and}} jars for chemicals and medicines {{has been designed}} to provide the maximum strength and security to the glassware {{in the event of a}} significant earthquake. The rack preferably is rectangular and may be made of a variety of chemically resistant materials including polypropylene, polycarbonate, and stainless steel. It comprises a first plurality of parallel vertical walls, and a second plurality of parallel vertical walls, perpendicular to the first. These intersecting vertical walls comprise a self-supporting structure without a bottom which sits on four legs. The <b>top</b> surface <b>of</b> the <b>rack</b> is formed by the <b>top</b> edges <b>of</b> all the vertical walls, which are not parallel but are skewed in three dimensions. These top edges form a grid matrix having a number of intersections of the vertical walls which define a number of rectangular compartments having varying widths and lengths and varying heights...|$|R
30|$|In [51], {{the authors}} {{introduced}} a power efficient resource allocation algorithm for jobs in cloud computing data centers. The developed approach was {{also based on}} GA. Resource allocation was performed to optimize job completion time and data center power consumption. It considered a static scheduling of independent jobs on homogeneous single-core resources. The proposed algorithm, called Non-dominated Sorting Genetic Algorithm II (NSGA-II), was applied to cloud environments to explore space solutions and efficiently search for the optimal solution. The data center was modeled as three-tier fat-tree layers architecture: access, aggregation, and core layers. The access layer provided connection to servers which were arranged into racks with each rack being served by a single <b>Top</b> <b>of</b> the <b>rack</b> switch. Two fitness functions were used here: task completion time and data center power consumption. When {{the execution of the}} algorithm was completed and optimal Pareto solutions were obtained, it became possible to fine tune the trade-off between power consumption and execution time. Then, by using a procedure called ranking, the population of solutions were sorted heuristically into different non-domination levels. This procedure was repeated for every solution creating different groups or non-domination; an integer value called rank was assigned to each non-domination level. When applying selection and sorting, NSGA-II was able to deal with constraints. The solution with less constraint violation had a better rank.|$|R
30|$|Deliver {{the sample}} tube from the A series <b>of</b> <b>racks</b> to the B series <b>of</b> <b>racks.</b>|$|R
5000|$|... #Caption: Foxer decoy float {{resting on}} the <b>top</b> <b>of</b> the depth charge <b>racks</b> <b>of</b> HMS Hind (U39) ...|$|R
40|$|In {{the course}} of {{experiments}} on the transmission of horse-sickness and blue-tongue of sheep [cf. preceding paper], it was found necessary to evolve methods for rearing mosquitos, feeding them on experimental animals and keeping them alive in South Africa, where adverse climatic conditions, particularly the low humidity, {{proved to be the}} most important obstacle. As previous experience on this subject was of little real value, the methods had to be worked out from the beginning, and a somewhat detailed account of observations and results is given as a guide for future workers. The paper is divided into four sections, the first of which deals with the catching of adult and larval mosquitos. The second gives the methods used for keeping mosquitos alive in the laboratory and includes a description <b>of</b> a <b>rack</b> holding two tiers of cages in which a high humidity was maintained by running water on to the metal <b>top</b> <b>of</b> the <b>rack,</b> allowing it to soak the hessian with which the rack was covered and. draining the excess away by means of gutters round the base. Methods of feeding mosquitos on horses are described in the third section, which also shows the arrangements for attaching cages to the horses by means of elastic bands attached to a girth or by inserting them in holes in a specially constructed metal, saddle. Details are given of a special insect-proof stable in which the horse is prevented from lying down and a high humidity is maintained by means of walls of hessian kept wet in a manner similar to that used for the cages. When feeding mosquitos on sheep, the subject of the last section, the cages were held in position by tapes tied to locks of wool [cf. R. A. E. B 22 171], and sufficient moisture was supplied by covering the <b>tops</b> <b>of</b> the cages with damp cotton-wool held in position by the same tapes. The articles have been scanned in colour with a HP Scanjet 5590; 600 dpi. Adobe Acrobat XI Pro was used to OCR the text and also for the merging and conversion to the final presentation PDF-format. mn 2015 mn 201...|$|R
40|$|In this paper, {{we define}} the {{pullback}} crossed modules {{in the category}} <b>of</b> <b>racks</b> which mainly based on a pullback diagram <b>of</b> <b>rack</b> morphisms with extra crossed module data on some of its arrows. Furthermore we prove that the conjugation functor, which is defined between the category of crossed modules of groups and <b>of</b> <b>racks,</b> preserves the pullback crossed modules. Comment: Preliminary versio...|$|R
