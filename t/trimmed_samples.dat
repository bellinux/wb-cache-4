7|52|Public
40|$|Burr {{model is}} {{especially}} {{suitable for the}} life-testing of the products that age with time. <b>Trimmed</b> <b>samples</b> are widely utilized in several areas of statistical practice, especially when some sample values at either or both extremes might have been adulterated. In this article, the problem of estimating the parameter of Burr distribution type II based on <b>trimmed</b> <b>samples</b> under informative and uninformative has been addressed. The problem discussed using Bayesian approach to estimate the shape parameter of Burr type II distribution. Elicitation of hyperparameter through prior predictive approach has also been discussed. Posterior predictive distributions along with posterior predictive intervals and credible intervals have also been derived under different priors. A comparison has been made using the Monte Carlo simulation. A real life data example has also been discussed...|$|E
40|$|The data {{available}} for estimating welfare indicators are often inconveniently incomplete data: {{they may be}} censored or truncated. Furthermore, for robustness reasons, researchers sometimes use <b>trimmed</b> <b>samples.</b> By using the statistical tool known as the Influence Function we derive distribution-free asymptotic variances for wide classes of welfare indicators {{not only in the}} complete data case, but also in the important cases where the data have been trimmed, censored or truncated...|$|E
40|$|Compressional-wave velocity, wet-bulk density, and {{porosity}} {{were measured}} on sediments and rocks recovered from Deep Sea Drilling Project Holes 515 B and 516 F. Wet-bulk densities were measured by both gravimetric and GRAPE methods. Velocities were measured on <b>trimmed</b> <b>samples</b> with the Hamilton frame velocimeter. The shipboard measurement techniques {{are discussed in}} the explanatory notes chapter (Coulbourn, this volume) and are described in detail by Boyce (1976 a). Only the shipboard measurements are reported here...|$|E
40|$|In this reply, we {{comment on}} Reed and Sidek’s {{replication}} of Nijkamp and Poot. We note that Nijkamp and Poot’s conclusions are robust to, first, <b>trimming</b> their <b>sample</b> of primary studies to improve comparability across studies and, second, greatly expanding {{the number of}} observations for meta-analysis by including multiple estimates {{from each of the}} <b>trimmed</b> <b>sample</b> of primary studies. We argue that future meta-analyses of this empirical literature would benefit from a narrower focus on specific fisca...|$|R
40|$|Mounted {{illustrations}} are Persian embroidery <b>trim</b> <b>samples</b> using {{gold and}} silver. On spine: Perse. No. t. -p. Label mounted on inside front cover: Maison Sedille, Eugène Unsworth & Cie successeurs. Mary Ann Beinecke Decorative Art Collection. Sterling and Francine Clark Art Institute Library. Mode of access: Internet...|$|R
40|$|Causal {{inference}} with observational studies often {{relies on}} the assumptions of unconfoundedness and overlap of covariate distributions in different treatment groups. The overlap assumption is violated when some units have propensity scores close to 0 or 1, and therefore both practical and theoretical researchers suggest dropping units with extreme estimated propensity scores. However, existing trimming methods ignore the uncertainty in this design stage and restrict inference only to the <b>trimmed</b> <b>sample,</b> due to the non-smoothness of the trimming. We propose a smooth weighting, which approximates the existing <b>sample</b> <b>trimming</b> but has better asymptotic properties. An advantage of the new smoothly weighted estimator is its asymptotic linearity, which ensures that the bootstrap {{can be used to}} make inference for the target population, incorporating uncertainty arising from both the design and analysis stages. We also extend the theory to the average treatment effect on the treated, suggesting <b>trimming</b> <b>samples</b> with estimated propensity scores close to 1. Comment: 21 pages, 1 figures and 3 table...|$|R
40|$|We {{say that}} two probabilities are similar at level α {{if they are}} {{contaminated}} versions (up to an α fraction) of the same common probability. We show how this model is related to minimal distances between sets of trimmed probabilities. Empirical versions turn out to present an overfitting effect {{in the sense that}} trimming beyond the similarity level results in <b>trimmed</b> <b>samples</b> that are closer than expected to each other. We show how this can be combined with a bootstrap approach to assess similarity from two data samples. Comment: Published in at [URL] the Bernoulli ([URL] by the International Statistical Institute/Bernoulli Society ([URL]...|$|E
40|$|This paper {{examines}} robust estimators of {{core inflation}} for Belgian historical CPI data, and for euro area Harmonised Indices of Consumer Prices. Evidence of fat tails in the cross-sections of price changes {{is provided by}} traditional measures, {{as well as by}} a robust measure of the tail weights that is not vulnerable to the masking phenomenon. Trimmed means are considered in the first instance. We introduce a new estimator where the optimal trimming percentage is the lowest percentage for which the hypothesis of normality of the <b>trimmed</b> <b>samples</b> cannot be rejected {{on the basis of the}} Jarque-Bera statistic. Two variants are considered one with a constant and one with a time-varying optimal trimming percentage. The latter has a higher breakdown point. Symmetric and asymmetric trimming are considered as well. Another robust estimator, the one-step Huber-type skipped mean, which is less vulnerable to the masking phenomenon, is also examined. It is shown that the robust estimators outperform the traditional core inflation measures found in the literature. However, as traditional measures, they lag rather than lead observed inflation. This was particularly so in the 70 s and the 80 s when the oil price shocks had substantial second-round effects on Belgian inflation. Inflation, Core inflation, Relative prices, Robust estimators of central tendency...|$|E
40|$|Background: This paper {{contributes}} to the evidence-base on prices and alcohol use by presenting meta-analytic summaries of price and income elasticities for alcohol beverages. The analysis improves on previous meta-analyses by correcting for outliers and publication bias. Methods: Adjusting for outliers is important to avoid assigning too much weight to studies with very small standard errors or large effect sizes. <b>Trimmed</b> <b>samples</b> are used for this purpose. Correcting for publication bias is important to avoid giving too much weight to studies that reflect selection by investigators or others involved with publication processes. Cumulative meta-analysis is proposed as a method to avoid or reduce publication bias, resulting in more robust estimates. The literature search obtained 182 primary studies for aggregate alcohol consumption, which exceeds the database used in previous reviews and meta-analyses. Results: For individual beverages, corrected price elasticities are smaller (less elastic) by 28 - 29 percent compared with consensus averages frequently used for alcohol beverages. The average price and income elasticities are: beer, - 0. 30 and 0. 50; wine, - 0. 45 and 1. 00; and spirits, - 0. 55 and 1. 00. For total alcohol, the price elasticity is - 0. 50 and the income elasticity is 0. 60. Conclusions: These new results imply that attempts to reduce alcohol consumption through price or tax increases will be less effective or more costly than previously claimed...|$|E
50|$|In 1987, Sequential was finalizing {{development}} of a 16-bit sampler called the Prophet 3000, but {{went out of business}} and was acquired by Yamaha. Yamaha liquidated the few completed Prophet 3000 samplers at a substantial discount. Like most of the Sequential line, this sampler contained features that were far ahead of its time, such as automatic pitch detection, key-mapping, a remote control interface, and facilities for easily looping and <b>trimming</b> <b>sampled</b> sounds. Many of these technologies were later included in Yamaha's A-series samplers.|$|R
3000|$|... 8 I <b>trim</b> the <b>sample</b> {{to exclude}} governorates/provinces {{with a public}} {{employment}} rate that {{is larger than the}} 99 th percentile to avoid the undue influence of small province with very high public employment rates.|$|R
40|$|This paper {{evaluates the}} impact of the 2008 Rapid Improvement Programme that aimed at {{promoting}} normal birth and reducing caesarean section rates in the English NHS. Using Hospital Episode Statistics maternity records for the period 2001 - 2013 a panel data analysis was performed to determine whether the implementation of the programme reduced caesarean sections rates in participating hospitals. The results obtained using either the unadjusted sample of hospitals or a <b>trimmed</b> <b>sample</b> determined by a propensity score matching approach indicate that {{the impact of}} the programme was small. More specifically there were 2. 3 to 3. 4 fewer caesarean deliveries in participating hospitals, on average, during the post-programme period offering a limited scope for cost reduction. This result mainly comes from the {{reduction in the number of}} emergency caesareans as no significant effect was uncovered for planned caesarean deliveries...|$|R
40|$|Graduation date: 1989 In {{the problem}} of testing the median using a random sample from a certain distribution, and if no other {{parametric}} family is suggested, the t-test {{is known to be}} the optimal procedure when this distribution is normal. If the sample appears to be non-normal, one has the choice either to consider a non-parametric approach or to try to correct for non-normality before applying the t-test. In this thesis we investigate the effect of applying certain power transformations as an action to correct for non-normality before applying the t-test. Also we investigate the effect of applying a power transformation then trimming a certain proportion from the data on each tail as a double action to correct for non-normality. This problem is first considered by Doksum and Wong (1983), who apply the Box-Cox power transformations to positive, right-skewed data when testing for the equality of distributions of two independent samples. In the present work we provide results for the one-sample case using two alternatives to the Box-Cox power family which are applicable to all data sets. Whenever it can be assumed that the data is a random sample from a symmetric distribution with heavy tails, it is shown that the John-Draper family of modtlus power transformations, with the transformation parameter being positive and smaller than 1, is appropriate to correct for non-normality and the t-test based on the transformed data is asymptotically more efficient and has better power properties than the t-test based on the data in its original scale. -When the data is thought to have a skewed distribution and can assume negative as well as positive values, a new family of transformations, referred to as the two-domain family, is introduced. It is shown that the t-test based on the data after applying this new transformation is also asymptotically more efficient and has better power properties than the t-test in the original scale. A simulation study shows that trimming a certain proportion on each tail of the data transformed by one of the above two transformations then applying the t-test to the <b>trimmed</b> <b>samples</b> yields a considerable gain in power compared to the t-test in the original scale...|$|E
40|$|We {{consider}} a simple adaptive {{test for the}} homogeneity of scales due to Hall and Padmanabhan (1997) {{which is based on}} the ratio of adaptively <b>trimmed</b> <b>sample</b> variances. We develop a modification of this test that has the form of a combined bootstrap test and that is obtained within the nonparametric combination of dependent tests framework (Pesarin, 2001). We considered other approaches for the combination of tests concluding that the Liptak method is the most suitable one for the problem at hand. We compare the modified test with the original one in terms of robustness of significance level and power in a simulation study which considers distributions that range from symmetric to skewed and from light to heavy tailed ones. We show that the modified test is more powerful than the original one under heavier than normal tailed and very skewed distributions. A practical application to detect difference in scale of energy intake of lactating and non pregnant and not lactating women is presented...|$|R
30|$|We {{argue that}} other {{government}} policies are unlikely to drive our results. Two major policies that affect elderly Singaporeans in the age group we study, are the Pioneer Generation Package (PGP) and eligibility for CPF payouts. All individuals aged 65 or above in 2014 are eligible for the Pioneer Generation Package (PGP) which provides healthcare subsidies that vary only by age cohort, not socioeconomic status. As the age distribution of our <b>trimmed</b> <b>sample</b> is similar across the treatment and control groups, this policy is unlikely to affect our analyses. We also note that PGP was already in place {{before the start of}} the SSS. The CPF payout eligibility age is also unlikely to affect our analyses, as the eligibility age also varies only by age cohort and not socioeconomic status. Furthermore, there is no change in CPF eligibility status within the period of our study for both our main and supplementary samples. All individuals in our main sample are already age-eligible for CPF payouts at the start of our study period; all individuals in our supplementary analyses will not be old enough to reach CPF payout eligibility age by the end of the period studied.|$|R
40|$|The photoproduction of the p and the w was studied {{using data}} {{taken by the}} NA 1 {{experiment}} at the CERN SPS. The beam {{was that of a}} tagged photon with energy between 70 - 225 GeV incident on an active target, which consisted of a monolithic germanium block and strips of silicon detector. The decay products were detected by the forward FRAMM spectrometer. The p and the w events were identified by their decays into and channels respectively. Using clean samples of events and taking into account their respective branching ratios and simulated geometrical acceptances the ratio was measured to be 9. 64 +/- 0. 54. The interaction-point distribution of a <b>trimmed</b> <b>sample</b> of p events resulted in the ratio of the interaction rates in the germanium and the silicon parts of the target, leading to the value of a, which describes the A-dependence of the nuclear cross-section by [equation] where A is the nuclear mass number. The measured [alpha] values for the overall and the coherent event samples, respectively, are [alpha = 1. 45 +- 00. 5 and [alpha]Coh = 1. 44 +- 0. 06 <p...|$|R
40|$|The {{presence}} of leverage points in regression {{can have a}} dramatic impact on the finite sample efficiencies (versus least squares under Gaussian errors) of high breakdown estimators. In general, as the x-value of a leverage point becomes more extreme, the finite sample efficiencies decrease and larger sample sizes are required for asymptotic results to apply. However, the least median of squares (LMS) and least trimmed squares (LTS) estimators exhibit higher finite sample than asymptotic efficiencies. On the other hand, the finite sample efficiency of the Schweppe-type one-step generalized M estimator (S 1 S), starting from LTS, which minimizes {{the sum of the}} h smallest squared residuals, converges from below to its asymptotic value as the sample size increases. We suggest that the finite sample variance itself be replaced by a <b>trimmed</b> <b>sample</b> variance which accounts for the long tail behavior encountered in finite samples. An improvement on the S 1 S estimator, which consists of never downweighting any point with one of the h smallest squared LTS residuals, performs quite well with respect to either measure of efficiency, while retaining a high breakdown point. Asymptotic efficiency finite sample efficiency breakdown point exact fit point least median of squares least trimmed squares one-step estimator...|$|R
30|$|The {{samples of}} the lung tissue from SPA-treated animals were excised from the dorsal-caudal regions of both lungs and fixed in 10 % {{buffered}} formalin for 48  h before <b>trimming.</b> Similar <b>samples</b> were retrieved from control animals. Fixed tissues were processed (Tissue-Tek VIP 5 vacuum infiltration tissue processor, Sakura Finetek USA, Torrance, CA, USA), embedded in paraffin, sectioned (thickness 4  μm), and stained with hematoxylin-eosin. For each animal, ten randomly selected fields were assessed (original magnification[*]×[*] 400) for alveolar fibrin deposition, alveolar inflammatory cell infiltration, and interstitial and intra-alveolar edema [31].|$|R
40|$|This study {{concerns}} {{a comparison of}} the ECHAM 3 /T 42 simulated series of daily extreme temperatures and series observed in southern Moravia (a part of the Czech Republic). ECHAM climate model was developed from the ECMWF model (the former part of its name EC) and parametrizations were created at the Max Planck Institute in Hamburg (the latter part of the abbreviation HAM). Simulated (1 CO 2) times series of daily variables have rarely been validated against the real datasets. In this paper, attention is focused on autocorrelation coef®cients whose estimates are computed by the jackknife method and differences in the estimates between the simulations and observations are examined. It is shown that for the average simulated series (4 gridpoints) the jackknife autocorrelation coef®cients are substantially larger in all seasons than those computed for the average series in Moravia (5 stations). The daily extreme temperature variability is underestimated in the simulations, the persistence of the simulated series being much higher. In order to gain an additional insight into this ®nding trimmed means and <b>trimmed</b> <b>sample</b> variances are computed. An examination of frequencies of day-to-day changes (absolute values) calculated from the observations and simulations shows that small day-to-day temperature changes are clearly preferred in the model at the expense of larger changes which are recorded in Moravia. It is obvious that the largest changes observed are not captured in the simulations. 1...|$|R
40|$|From 1997 to 1999, the {{prevalence}} of Salmonella was assessed at different stages through the pork, poultry, and beef meat production chains. Different dilutions of the initial sample suspension were analyzed to provide a semiquantitative evaluation of Salmonella contamination and to determine the most representative dilution necessary to detect a reduction in prevalence. An average of 300 samples {{for each type of}} meat were analyzed. According to Fisher's exact test, the dilution to be used to detect a reduction in prevalence was chosen based on an initial prevalence of 20 to 26 %. Based on this introductory study, a new sampling plan representative of the nationwide Belgian meat production process was used from 2000 through to 2003. This study confirmed the consistently high rate and level of contamination of poultry meat: broiler and layer carcasses were the most contaminated samples followed by broiler fillets and poultry meat preparations. A constant and significant decrease in Salmonella prevalence was observed for pork carcasses, trimmings, and minced meat and for beef minced meat. Less than 3 % of beef carcasses and <b>trimming</b> <b>samples</b> were positive for Salmonella. The Belgian plan, as utilized from 2000 to 2003, was suitable for monitoring of zoonoses because the sampling plan was representative of nationwide production processes, covered all periods of the year, and was executed by trained samplers and the analyses were carried out by recognized laboratories using an identical analytical method. Peer reviewe...|$|R
40|$|Two {{different}} ways of <b>trimming</b> the <b>sample</b> path of a stochastic process in D[0, 1]: global ("trim as you go") trimming and record time ("lookback") trimming are analysed to find conditions for the corresponding operators to be continuous {{with respect to the}} (strong) J 1 -topology. A key condition is {{that there should be no}} ties among the largest ordered jumps of the limit process. As an application of the theory, via the continuous mapping theorem we prove limit theorems for trimmed Levy processes, using the functional convergence of the underlying process to a stable process. The results are applied to a reinsurance ruin time problem...|$|R
40|$|Abstract. Since the {{publication}} of Guerre, Perrigne, and Vuong [2000, GPV], their two-step nonparametric estimation method has become the gold standard in empirical auctions work. One drawback of the method stems from a weakness of kernel-smoothed density estimators known as boundary effects: such estimators are inconsistent at the endpoints of the support and exhibit excessive (downward) bias within a neighborhood. Because of this problem, two-stage nonparametric estimators of first-price auctions require <b>sample</b> <b>trimming,</b> or discarding first-stage private value estimates based on bids near the sample extrema. Data loss can significantly reduce inferential power in auctions, where sample sizes are typically small to begin with and convergence rates of nonparametric estimators are relatively slow. In finite samples, this leads to poor performance on {{a substantial portion of}} the valuation support adjacent to the boundaries. It also negatively affects mean squared error on the interior of the support by making optimal bandwidth selection problematic. We propose a modification to the GPV estimator which incorporates a recent boundary-corrected kernel density estimator and allows the researcher to avoid <b>sample</b> <b>trimming</b> and the associated complications. We demonstrate a substantial improvement in finite-sample performance of the estimator through a Monte Carlo study. We then show how boundary correction easily applies to the various extensions of GPV and use it to analyze a dataset involving asymmetric affiliated bidders in oil lease auctions. We document how estimates and inferences change under the improved estimator. In particular, <b>sample</b> <b>trimming</b> masks a substantial degree of bidder asymmetry present in the data, as well as resulting inefficiencies in allocations of oil leases...|$|R
40|$|We {{consider}} {{a combination of}} heavily <b>trimmed</b> sums and <b>sample</b> quantiles which arises when examining properties of clustering criteria and prove limit theorems. The object of interest, which we call the Empirical Cross-over Function, is an L-statistic whose weights do not comply with the requisite regularity conditions for usage of ex- isting limit results. The law of large numbers, CLT and a functional CLT are proven. Comment: Submitte...|$|R
40|$|The {{influence}} of varying laser trim {{patterns on the}} electrical performance of a novel CuAlMo thin film resistor material were investigated. The benefits and limitations of various trim geometries were considered before two patterns, the ‘L’ cut and serpentine cut, were selected to laser <b>trim</b> resistor <b>samples</b> to target values of 1 to 10 Ω, using previously optimized laser conditions. The effect of increasing trim gain and varying trim pattern on the stability and standard deviation of the films were then systematically investigated. A two stage trimming process was utilized to reduce resistance drift figures to < 0. 1 % following storage for 168 hours at 125 °C in air and also allowed much tighter resistance tolerances of < ± 0. 1 % to be achieved...|$|R
40|$|Purpose – The {{purpose of}} this paper is to examine the day-of-the-week effect for three primary money market {{instruments}} in Canada. The sample period is 1980 - 2009. Design/methodology/approach – The authors use three approaches. First, a parametric t-test is employed to determine if a particular day-of-the-week mean return is significantly different from zero, using both a full <b>sample</b> and a <b>trimmed</b> <b>sample.</b> Next, the Wilcoxon signed ranked test is utilized to assess whether the median weekday return is different from zero for each day. Lastly, a binary regression model is used to test if Monday's mean return is different from other days. Findings – The traditional Monday effect is prevalent in the 1980 s for corporate paper and treasury bills (TB), but not for bankers acceptances (BA). In the 1990 s, the Monday effect disappears completely. However, in the 2000 s the Monday effect reappears, but is positive (it reverses) for both corporate paper and BA. The authors also find strong support for Wednesday being a high return day, which concurs with related money market studies. Research limitations/implications – While the results are statistically significant, the economic significance is dubious. This study helps market participants in that it shows that they need to allow for distinct day-of-the-week patterns when using yield spreads. Practical implications – One practical implication for practitioners is to time purchases of Canadian money market securities for Monday when returns are low (relying on the results of the full sample period). Issuers should time sales for non-Mondays when returns are higher and yields are lower. Originality/value – This study is original in that it is the first one to analyze day-of-the-week effects in the Canadian money market. The authors compare the results to studies that focus on the US market. Canada, Day-of-the-week effect, Financial investment, Financial markets, Returns...|$|R
50|$|The current Sheeley House {{began taking}} shape after the {{property}} was purchased by John B. Paul in 1884, who surfaced the building with local red brick. Extensive renovations began in 1981, restoring the original interior and exterior of the building from old photographs, duplicating new <b>trim</b> from existing <b>samples,</b> and a new cedar shingle roof installed. The metal ceiling and French tile floor in the bar are original. A new bar was built duplicating the bar in the Paul House Saloon picture.|$|R
30|$|FIB-SEM is a {{technique}} to generate high resolution three-dimensional images of biological samples in micrometer scale (Kizilyaprak et al., 2014). Samples are prepared by a similar method to transmission electron microscopy, typically by fixing the sample with aldehyde, staining with heavy metals such as osmium and uranium then embedding in an epoxy resin. The surface of the block of resin-embedded sample is imaged by detection of back-scattered electrons. Following imaging, the focused ion beam is used to trim a thin section (typically less than 30  nm) {{from the face of}} the block. After the section is <b>trimmed,</b> the <b>sample</b> block is raised back to the focal plane and imaged again. This sequence of <b>sample</b> imaging, section <b>trimming</b> and block raising can acquire many thousands of images with perfect alignment in an automated fashion and yield a 3 D volume data of specimen.|$|R
40|$|Estimation {{of average}} {{treatment}} effects under unconfounded or ignorable treatment assignment is often hampered {{by lack of}} overlap in the covariate distributions between treatment groups. This lack of overlap can lead to imprecise estimates, and can make commonly used estimators sensitive to the choice of specification. In such cases researchers have often used ad hoc methods for <b>trimming</b> the <b>sample.</b> We develop a systematic approach to addressing lack of overlap. We characterize optimal subsamples for which the average treatment effect can be estimated most precisely. Under some conditions, the optimal selection rules depend solely on the propensity score. For {{a wide range of}} distributions, a good approximation to the optimal rule is provided by the simple rule of thumb to discard all units with estimated propensity scores outside the range [0. 1, 0. 9]. Copyright 2009, Oxford University Press. ...|$|R
40|$|Professional Doctorate - Doctor of Business Administration (DBA) In {{response}} {{to a couple of}} remarkable revelations of corporate fraud and the increasing level of domination of mainland companies on the Hong Kong stock market that has occurred over the past two decades, this study examines whether boardroom characteristics, financial reporting credibility and corporate ownership structure have an effect on corporate fraud in Hong Kong. In addition, it explores how these factors affect the severity of fraud, and examines possible distinctive features of fraud committed by Chinese companies compared to their local counterparts. The data are obtained from enforcement reports from various regulatory bodies in Hong Kong, together with court reports from local news clippings. This study uses a <b>trimmed</b> <b>sample</b> of 116 event observations comprising 69 suspicious fraud observations and 8 defendant fraud observations. A sample of no-fraud companies with comparable characteristics is used as a control group. Data for the reporting period from 2002 to 2011 are chosen for empirical analyses. Probit models are used to examine factors contributing to the likelihood and severity of corporate fraud. The empirical results indicate that board size, board tenure, multiple board appointments across audit committee members, as well as management turnover, are crucial in explaining corporate fraud and fraud severity. In particular, the first three explanatory variables are negatively related to fraud likelihood and fraud severity while the fourth is positively related. The negative association between outside directors and corporate fraud has been widely documented in prior studies. Surprisingly, this study shows that the proportion of outside directors, as well as corporate ownership structure and financial performance indicators, have no significant effect on corporate wrongdoings in Hong Kong. This study has practical implications regarding the design of appropriate corporate governance mechanisms for the listed entities. One implication is that most listed entities barely meet the required number of independent non-executive directors who must be selected by substantial shareholders. As a result of this, it is a concern that the directors’ independence may be impaired and their effectiveness in curbing management may be lowered...|$|R
5000|$|Each module {{is capable}} of storing a sample up to 26 seconds. The {{combined}} total maximum sampling time with eight modules is three minutes and thirty seconds. Sample rates range from 10 kHz to 60 kHz. High fidelity is achieved at 35 kHz and above, resulting in full bandwidth sounds of over seven seconds per voice module; one minute for all eight modules. In addition to recording individual samples, the modules may be [...] "chained" [...] to form one recording almost seven minutes long. Editing commands allow you to <b>trim</b> and move <b>samples.</b>|$|R
30|$|We use a difference-in-differences (DiD) {{strategy}} {{to identify the}} causal effects of the SSS. The sample for our main analysis {{is made up of}} individuals aged 65 and above in 2016.2 The treated group consists of those who reported receiving an SSS payout at least once in 2016, while the control group consists of those who did not. Our treatment definition allows us to interpret the policy shock as exogenous, since eligibility of payouts for 2016 was based on 2015 administrative data. Individuals were thus unable to select into treatment after the SSS was announced in March 2016. To tackle the possibility that our treatment and control groups may be sufficiently different to invalidate the DiD parallel trends assumption, we <b>trim</b> our <b>sample,</b> using individuals’ treatment propensity, to construct more similar treatment and control groups. We also verify that our results are robust to a battery of checks that include the use of different control groups, different re-weighting techniques, and the addition of group-specific time fixed effects (details in Section 5).|$|R
30|$|For the {{compression}} test, the MFC aerogel materials were <b>trimmed</b> into cylindrical <b>samples</b> using curved tweezers (Dumont, Switzerland) with {{a diameter of}} 30  mm and a height of 30  mm. The compression tests were performed with a micro-mechanical testing machine (Instron 5848, USA), with a sensor attached to a 5  N load cell operated {{at the rate of}} 10  mm/min. The modulus of the aerogels was derived from the initial linear region of the stress–strain curves generated from the experiment. At least five samples were measured. In addition, the energy absorption was calculated by integrating the area under the stress–strain curve from 0 to 70  % of the strain.|$|R
40|$|Master of ScienceDepartment of StatisticsPaul NelsonTrimmed {{means are}} robust estimators of {{location}} for distributions having heavy tails. Theory and simulation indicate that little efficiency is lost under normality when using appropriately trimmed means {{and that their}} use with data from distributions with heavy tails can result in improved performance. This report uses the principle of equivariance applied to <b>trimmed</b> means <b>sampled</b> from a Cauchy distribution to form a discrepancy function of the data and parameters whose distribution is free of the unknown median and scale parameter. Quantiles of this discrepancy function are estimated via asymptotic normality and simulation and used to construct confidence intervals for the median of a Cauchy distribution. A nonparametric approach based {{on the distribution of}} order statistics is also used to construct confidence intervals. The performance of these intervals in terms of coverage rate and average length is investigated via simulation when the data are actually sampled from a Cauchy distribution and when sampling is from normal and logistic distributions. The intervals based on simulation estimation of the quantiles of the discrepancy function are shown to perform well across a range of <b>sample</b> sizes and <b>trimming</b> proportions when the data are actually sampled from a Cauchy distribution and to be relatively robust when sampling is from the normal and logistic distributions...|$|R
40|$|ABSTRACT: This paper {{presents}} the cyclic load-controlled tests {{to investigate the}} liquefaction potential of non-plastic soil samples. The 71 mm diameter <b>samples</b> were <b>trimmed</b> from undisturbed-frozen <b>samples</b> and tested using the MTS servo controlled dynamic testing system. The tests were conducted at the effective confining pressures (σ’ 3) of 107 kPa (1 tsf) and 321 kPa (3 tsf) with the frequency of 1 Hz. The {{results show that the}} almost all samples reach a zero effective strength condition (σ’ ≈ 0) at the cyclic strains between 0. 5 % and 4 %. The relation between number of cycles, to generate 1 % cyclic strain, and the stress ratio is linear. The tendency of pore pressure buildup at 1 % cyclic strain decreases with the increase of stress ratio...|$|R
40|$|To {{investigate}} {{the influence of}} anisotropy of the clay on the undrained shear stren-gth, consolidated undrained triaxial shear tests were performed on two kinds of cylindri-cal <b>samples</b> <b>trimmed</b> vertically and horizontally from the artificially sedimented kaolin clay. The sedimented kaolin clay {{was made by the}} following procedure: kaolin clay wasmixed with 0. OlN NasSiO 8 solution and sedimented from a dilute suspesion whose claycontent was about 10 weight %. Orientation of clay particles was measured by usingX-ray diffraction and it was found that particles in the sedimented clay orientatedparallel to the horizontal bedding plane. For normally consolidated samples, the undrained shear strength of vertically cutsamples was almost equal to that of horizontally cut ones, and for overconsolidatedsamples, the former was greater than the latter. It is concluded that anisotropy with respect to undrained strength of anisotropic clayis mainly attributed to the character of tendency of volume change due to shear...|$|R
40|$|The L-statistics form an {{important}} class of estimators in nonparametric statistics. Its members include <b>trimmed</b> means and <b>sample</b> quantiles and functions thereof. This article {{is devoted to}} theory and applications of L-statistics for repeated measurements data, wherein the measurements on the same subject are dependent and the measurements from different subjects are independent. This article has three main goals: (a) Show that the L-statistics are asymptotically normal for repeated measurements data. (b) Present three statistical applications of this result, namely, location estimation using trimmed means, quantile estimation and construction of tolerance intervals. (c) Obtain a Bahadur representation for sample quantiles. These results are generalizations of similar results for independently and identically distributed data. The practical usefulness of these results is illustrated by analyzing a real data set involving measurement of systolic blood pressure. The properties of the proposed point and interval estimators are examined via simulation...|$|R
