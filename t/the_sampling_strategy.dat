449|10000|Public
2500|$|The {{relative}} {{risk of death}} due to the 2003 invasion and occupation was estimated by comparing mortality in the 17.8 months after the invasion with the 14.6 months preceding it. The authors stated, [...] "Making conservative assumptions, we think that about 100,000 excess deaths, or more have happened since the 2003 invasion of Iraq." [...] Among such [...] "conservative assumptions" [...] is the exclusion of data from Fallujah in many of its findings. Since interpreting {{the results of the}} study would be complicated by the inclusion of an outlier cluster in Fallujah, where heavy fighting caused far more casualties than elsewhere in Iraq, the study focused mainly on the results that excluded the Fallujah cluster. [...] While the authors argued that the Fallujah cluster's inclusion could be justified as a normal part of <b>the</b> <b>sampling</b> <b>strategy</b> (the authors noted that other [...] "hotspots" [...] like Najaf had not ended up being surveyed), and the authors presented two sets of results in some cases (one set including the Fallujah data and one not), the article, and most press coverage of the article, stresses the data that excluded the Fallujah cluster.|$|E
5000|$|There {{are many}} {{variants}} {{on the basic}} PRM method, some quite sophisticated, that vary <b>the</b> <b>sampling</b> <b>strategy</b> and connection strategy to achieve faster performance. See e.g. [...] for a discussion.|$|E
5000|$|Studies can be {{designed}} to observe intra-host or inter-host interactions. Bacterial phylodynamic studies usually focus on inter-host interactions with samples from many different hosts in a specific geographical location or several different geographical locations. [...] The {{most important part of}} a study design is how to organize <b>the</b> <b>sampling</b> <b>strategy.</b> [...] For example, the number of sampled time points, the sampling interval, and the number of sequences per time point are crucial to phylodynamic analysis. [...] Sampling bias causes problems when looking at a diverse taxological samples. [...] For example, sampling from a limited geographical location may impact effective population size.|$|E
30|$|Select 24 plots {{using one}} of <b>the</b> <b>sampling</b> <b>strategies</b> under consideration.|$|R
40|$|Over {{the last}} three years the Centre for Soil Treatment has sampled 2570 soil {{stockpiles}} for the assessment of soil quality. For all stockpiles <b>the</b> same <b>sampling</b> <b>strategy</b> was used. 100 increments of approximately 180 grams were taken and collected into two batch samples. Appropriate sample pre-treatment and analyses resulted in two sets of analytical results per stockpile. These paired results were statistically interpreted in order to define the heterogeneity of the original soil stockpiles. By estimating this original heterogeneity, the effectiveness of <b>the</b> used <b>sampling</b> <b>strategy</b> could be determined. It is concluded that <b>the</b> chosen <b>sampling</b> <b>strategy</b> is adequate for the majority of soil stockpiles...|$|R
40|$|This is an {{introduction}} to main choices to address in different steps of a research that makes use of qualitative interviews. The various chapters address theories and concepts, types of interviews, the interview scheme, <b>the</b> <b>sampling</b> <b>strategies,</b> entering <b>the</b> field, the analysis of interviews and {{the writing of the}} research results...|$|R
50|$|There is some {{argument}} over <b>the</b> <b>sampling</b> <b>strategy</b> {{to be employed}} in trial trenching, especially in evaluating sites that are intended for development. Issues such as the effectiveness of certain trench layouts or {{the percentage of the}} site to be dug (normally around 5% at present) are widely discussed. Whether an effective picture of past human activity on a site can be truly estimated through this methods is widely debated. Development can destroy buried archaeology forever and a reliable evaluation methodology is very important. Whilst it is difficult to quantify the number of false negative results there have certainly been examples of evaluations suggesting a relatively limited amount of past activity which has had to be upwardly revised during the excavation.|$|E
5000|$|The {{relative}} {{risk of death}} due to the 2003 invasion and occupation was estimated by comparing mortality in the 17.8 months after the invasion with the 14.6 months preceding it. The authors stated, [...] "Making conservative assumptions, we think that about 100,000 excess deaths, or more have happened since the 2003 invasion of Iraq." [...] Among such [...] "conservative assumptions" [...] is the exclusion of data from Fallujah in many of its findings. Since interpreting {{the results of the}} study would be complicated by the inclusion of an outlier cluster in Fallujah, where heavy fighting caused far more casualties than elsewhere in Iraq, the study focused mainly on the results that excluded the Fallujah cluster. While the authors argued that the Fallujah cluster's inclusion could be justified as a normal part of <b>the</b> <b>sampling</b> <b>strategy</b> (the authors noted that other [...] "hotspots" [...] like Najaf had not ended up being surveyed), and the authors presented two sets of results in some cases (one set including the Fallujah data and one not), the article, and most press coverage of the article, stresses the data that excluded the Fallujah cluster.|$|E
40|$|No {{aspect of}} the {{research}} plan is more critical for assuring the usefulness of a study than <b>the</b> <b>sampling</b> <b>strategy.</b> It will determine if {{the results of the}} study can be applied as evidence and contributes to the trustworthiness of the results. <b>The</b> <b>sampling</b> <b>strategy</b> is a critical part of research design. An appropriate sampling plan is vital for drawing the right conclusions from a study. Good sampling is critical for the confident application of the study findings to other people, settings, or time periods...|$|E
40|$|This paper studies <b>the</b> <b>sampling</b> <b>strategies</b> for <b>the</b> Expert Network (EexNet), a {{statistical}} learning system used for patient record classification at the Mayo Clinic. The {{goal is to}} achieve high accuracy classification at an affordable computational cost in very large applications. The learning curves of ExpNet were observed {{with respect to the}} choice of trainin...|$|R
40|$|We {{explore the}} {{possibilities}} of obtaining compression in video through modified <b>sampling</b> <b>strategies</b> using multichannel imaging systems. The redundancies in video streams are exploited through compressive sampling schemes to achieve low power and low complexity video sensors. <b>The</b> <b>sampling</b> <b>strategies</b> {{as well as the}} associated reconstruction algorithms are discussed. These compressive sampling schemes could be implemented in the focal plane readout hardware resulting in drastic reduction in data bandwidth and computational complexity...|$|R
50|$|<b>The</b> {{adaptive}} <b>sampling</b> <b>strategy</b> dramatically reduces <b>the</b> {{rendering time}} for high-quality rendering - the higher quality and/or size of data-set, the more significant advantage over <b>the</b> regular/even <b>sampling</b> <b>strategy.</b> However, adaptive ray casting upon a projection plane and adaptive sampling along each individual ray do not map {{well to the}} SIMD architecture of modern GPU. Multi-core CPUs, however, are a perfect fit for this technique, making them suitable for interactive ultra-high quality volumetric rendering.|$|R
40|$|This paper {{describes}} a new methodology for sampling counting stations for the Annual Traffic Census (ATC) in Hong Kong. The ATC aims to provide {{data for the}} estimation of annual average daily traffic at road links, and annual vehicle kilometrage at different segments of the road network. The required precision levels in the estimation of these key parameters are used {{as a basis for}} the formulation of <b>the</b> <b>sampling</b> <b>strategy.</b> Cluster analyses of counting stations are also employed to enhance the efficiency of <b>the</b> <b>sampling</b> <b>strategy.</b> Department of Civil and Environmental Engineerin...|$|E
40|$|AbstractRecently more {{research}} works {{are focused on}} multiobjective evolutionary algorithm (MOEA) duo to its ability of global and local search for solving multiobjective optimization problem (MOOP) and ability to provide more practical solutions to decision maker; however, most of existing MOEAs cannot achieve satisfactory results in both quality and computational speed. This paper proposes a hybrid sampling strategy-based multiobjective evolutionary algorithm (HSS-MOEA) {{to deal with such}} problem. HSS-MOEA tactfully combines <b>the</b> <b>sampling</b> <b>strategy</b> of vector evaluated genetic algorithm (VEGA) and <b>the</b> <b>sampling</b> <b>strategy</b> according to a new Pareto dominating and dominated relationship-based fitness function (PDDR-FF). <b>The</b> <b>sampling</b> <b>strategy</b> of VEGA prefers the edge area of the Pareto front and PDDR-FF-based sampling strategy has the tendency converging toward the central area of the Pareto front. The hybrid sampling strategies preserve both the convergence rate and the distribution performance. Numerical comparisons show that HSS-MOEA could get the better convergence performance, slightly better or equivalent distribution performance, and obviously better efficiency than existing MOEAs...|$|E
30|$|To {{minimize}} counting errors, each {{water sample}} was analyzed {{at least in}} two different volumes. <b>The</b> <b>sampling</b> <b>strategy</b> for the microbiological investigations {{was the same as}} described for the hydrochemical sampling.|$|E
40|$|Abstract—Cluster {{filtering}} {{is a kind}} of test selection technique, which saves human {{efforts for}} result inspection by reducing test size and finding maximum failures. Cluster <b>sampling</b> <b>strategies</b> {{play a key role in}} the cluster filtering technique. A good <b>sampling</b> <b>strategy</b> can greatly improve the failure detection capability. In this paper, we propose a new cluster <b>sampling</b> <b>strategy</b> called execution-spectra-based <b>sampling</b> (ESBS). Different from <b>the</b> existing <b>sampling</b> <b>strategies,</b> ESBS iteratively selects test cases from each cluster. In each iteration process, ESBS selects the test case that has the maximum suspiciousness to be a failed test. For each test, its suspiciousness is computed based on the execution spectra information of previous passed and failed test cases selected from the same cluster. <b>The</b> new <b>sampling</b> <b>strategy</b> ESBS is evaluated experimentally and the results show that it is more effective than <b>the</b> existing <b>sampling</b> <b>strategies</b> in most cases. Keywords-test selection; cluster filtering; cluster sampling; execution spectra; I...|$|R
40|$|This {{paper is}} devoted {{to the study of the}} {{behavior}} of the use of double sampling for dealing with nonresponses, when ranked set <b>sample</b> is used. <b>The</b> characteristics of <b>the</b> <b>sampling</b> <b>strategies</b> are derived. <b>The</b> structure of the errors generated the need of studying of the optimality of the strategies by performing a set Monte Carlo experiments...|$|R
40|$|Although {{sampling}} is {{a crucial}} component of research methodology, it has received little attention in intervention research with populations at risk for HIV infection. We review the challenges involved in sampling these populations for evaluating behavioral and social interventions. We assess the four strategies used for street and network sampling that {{have been reported in}} the HIV-intervention research literature and used because traditional probability sampling was not possible. <b>The</b> <b>sampling</b> <b>strategies</b> are: 1) targeted, 2) stratified, (3) time-space, and (4) respondent-driven. Although each has strengths and limitations in terms of its ability to produce valid results that enhance generalizability, the choice of a particular strategy depends on the goal of the study, characteristics of the target population, and the availability of resources and time for collecting and analyzing sampling-related data. Continued efforts are needed to improve <b>the</b> <b>sampling</b> <b>strategies</b> used in evaluation studies of HIV risk-reduction interventions...|$|R
30|$|<b>The</b> <b>sampling</b> <b>strategy</b> {{included}} weighting {{the sample}} size by population {{and the number of}} the types of drug outlets in the province or district. Drug outlets to be sampled were selected randomly.|$|E
40|$|It {{has been}} {{established}} that active learning is effective for learning complex, subjective query concepts for image retrieval. However, active learning has been applied in a concept independent way, (i. e., the kernel-parameters and <b>the</b> <b>sampling</b> <b>strategy</b> are identically chosen) for learning query concepts of differing complexity. In this work, we first characterize a concept’s complexity using three measures: hitrate, isolation and diversity. We then propose a multimodal learning approach that uses images ’ semantic labels to guide a concept-dependent, active-learning process. Based on the complexity of a concept, we make intelligent adjustments to <b>the</b> <b>sampling</b> <b>strategy</b> and the sampling pool from which images are to be selected and labeled, to improve concept learnability. Our empirical study on a 300 K-image dataset shows that concept-dependent learning is highly effective for image-retrieval accuracy...|$|E
40|$|This paper {{presents}} a kernel density estimation method {{by means of}} real-coded crossovers. Estimation of density algorithms (EDAs) are evolutionary optimization techniques, which determine <b>the</b> <b>sampling</b> <b>strategy</b> {{by means of a}} parametric probabilistic density function estimated from the population. Real-coded Genetic Algorithm (RCGA) does not explicitly estimate any probabilistic distribution, however, the probabilistic model of the population is implicitly estimated by crossovers and <b>the</b> <b>sampling</b> <b>strategy</b> is determined by this implicit probabilistic model. Based on this understanding, we propose a novel density estimation algorithm by using crossovers as nonparametric kernels and apply this kernel density estimation to the Gaussian Mixture modeling. We show that the proposed method is superior in the robustness of the computation and in the accuracy of the estimation by the comparison of conventional EM estimation...|$|E
40|$|Mobile sensing {{has been}} {{recently}} proposed for sampling spatial fields, where mobile sensors record the field along various paths for reconstruction. Classical and contemporary sampling typically assumes that <b>the</b> <b>sampling</b> locations are approximately known. This work explores multiple <b>sampling</b> <b>strategies</b> along random paths to sample and reconstruct a two dimensional bandlimited field. Extensive simulations are carried out, with insights from sensing matrices and their properties, to evaluate <b>the</b> <b>sampling</b> <b>strategies.</b> Their performance {{is measured by}} evaluating the stability of field reconstruction from field <b>samples.</b> <b>The</b> effect of location unawareness on some <b>sampling</b> <b>strategies</b> is also evaluated by simulations. Comment: 6 pages, 2 figures; submitted to ICASSP 201...|$|R
40|$|Sequential Monte Carlo {{methods are}} {{powerful}} algorithms to sample from sequences of complex probability distributions. They are mainly {{based on a}} combination of importance sampling and resampling techniques. The efficiency of these methods depends crucially on <b>the</b> <b>sampling</b> <b>strategies</b> adopted. In this paper, we present an extended importance sampling framework which allows more freedom than standard techniques to impute random samples. This makes it possible to develop efficient and original <b>sampling</b> <b>strategies.</b> Applications to optimal filtering problems illustrate this approach...|$|R
40|$|The {{computational}} {{efficiency of}} 2. 5 -D seismic wave modelling {{in the frequency}} domain depends largely on <b>the</b> wavenumber <b>sampling</b> <b>strategy</b> used. This involves determining the wavenumber range {{and the number of}} <b>the</b> <b>sampling</b> points, and overcoming the singularities in the wavenumber spectrum when taking the inverse Fourier transform to yield the frequency-domain wave solution. In this paper, we employ our newly developed Gaussian quadrature grid numerical modelling method and extensively investigate <b>the</b> wavenumber <b>sampling</b> <b>strategies</b> for 2. 5 -D frequency-domain seismic wave modelling in heterogeneous, anisotropic media. We show analytically and numerically that the various components of the Green's function tensor wavenumber-domain solutions have symmetric or antisymmetric properties and other characteristics, all of which can be fully used to construct effective and efficient <b>sampling</b> <b>strategies</b> for <b>the</b> inverse Fourier transform. We demonstrate two sampling schemes-called irregular and regular <b>sampling</b> <b>strategies</b> for <b>the</b> 2. 5 -D frequency-domain seismic wave modelling technique. The numerical results, which involve calibrations against analytic solutions, comparison of <b>the</b> different wavenumber <b>sampling</b> <b>strategies</b> and validation by means of 3 -D numerical solutions, show that <b>the</b> two <b>sampling</b> <b>strategies</b> are both suitable for efficiently computing the 3 -D frequency-domain wavefield in 2 -D heterogeneous, anisotropic media. These strategies depend on the given frequency, elastic model parameters and maximum wavelength and the offset distance from the sourc...|$|R
40|$|To {{study the}} {{statistical}} behavior of clouds for different climate regimes, the {{spatial and temporal}} stability of VIS-IR bidimensional histograms is tested. Also, the effect of data sampling and averaging on the histogram shapes is considered; in particular <b>the</b> <b>sampling</b> <b>strategy</b> used by the International Satellite Cloud Climatology Project is tested...|$|E
3000|$|... 7 We use the Turkish Household Labor Force {{survey in}} our analysis. <b>The</b> <b>sampling</b> <b>strategy</b> of this survey {{is based on}} the “addressed-based {{population}} registration system,” which targets the non-institutional civilian population only. Thus, refugees are not included into the sample. See Section 3 for further details on the data set and restrictions.|$|E
40|$|AbstractThe {{numerical}} and computational aspects {{underlying the}} approximation of moment independent sensitivity measures are discussed. Sampling plans based on column substitution and column permutations are evaluated and compared for both analytical test cases and a practical application. The influence of <b>the</b> <b>sampling</b> <b>strategy</b> (simple random, latin hypercube or quasi-random sequences) is investigated...|$|E
40|$|This paper {{proposes a}} general class of estimators for {{estimating}} the median in double <b>sampling.</b> <b>The</b> position estimator, stratification estimator and regression type estimator attain the minimum variance {{of the general}} class of estimators. The optimum values of the first-phase and second-phase sample sizes are also obtained for the fixed cost and the fixed variance cases. An empirical study examines the performance of <b>the</b> double <b>sampling</b> <b>strategies</b> for median estimation. Finally, {{an extension of the}} methods of Chen Qin (1993) and Kuk Mak(1994) is considered for <b>the</b> double <b>sampling</b> <b>strategy.</b> References...|$|R
40|$|Australian Cynodon {{germplasm}} has {{not been}} comprehensively exploited for bermudagrass improvement. In this paper we will describe ‘EcoTurf’ a four year (2007 - 2011) project to develop water and nutrient use efficient bermudagrasses from Australian biodiversity. We describe <b>the</b> <b>sampling</b> <b>strategies</b> of Australian biodiversity, the physiological and molecular tools used to characterise the collected germplasm of over 1000 ecotypes and detail some of the important outcomes...|$|R
30|$|In this article, we {{will see}} that a much more {{efficient}} approach {{can be used by}} substituting <b>the</b> Metropolis-Hastings <b>sampling</b> <b>strategy</b> by a line search approach inspired in <b>the</b> slice <b>sampling</b> technique [15].|$|R
40|$|FIGURE 1. (A) Bathymetric {{map of the}} Darwin MV {{with the}} core {{locations}} on the MV indicated (adapted from Vanneste et al. 2011) (B) Schematic representation of <b>the</b> <b>sampling</b> <b>strategy.</b> PUC: push core, sampled for pore-water geochemistry and meiofaunal community analyses; MC: megacorer, sampled for meiofaunal community analyses (Pape et al. 2011) ...|$|E
30|$|Since quite recently, {{large-scale}} wall-to-wall remote-sensing {{systems are}} available for this purpose (see e.g. the CLASlite tool by Asner et al. 2009). However, {{the issue of a}} rigorous statistical assessment of the accuracy of <b>the</b> <b>sampling</b> <b>strategy</b> adopted to produce estimates is, in essence, still undervalued. As a corollary, in the REDD context, estimation must be assessed to have pre-fixed requisites of statistical accuracy (UN-REDD 2011). On this issue, {{it is worth noting that}} an objective estimation of accuracy is possible only in a design-based approach, where no assumptions are made about the population under study, in such a way that accuracy stems from <b>the</b> <b>sampling</b> <b>strategy</b> actually adopted to carry out estimates. Thus, accuracy is real, not assumed or modelled as in model-based approaches, where accuracy crucially depends on the model which is presumed to generate the population under study.|$|E
40|$|Abstract. Imitation {{learning}} {{is an effective}} strategy to reinforcement learning, which avoids the delayed reward problem by learning from mentor-demonstrated trajectories. A limitation for imitation {{learning is}} that collecting sufficient qualified demonstrations is quite expensive. In this work, we study how an agent can automatically improve its perfor-mance from a weak policy, by automatically acquiring more demonstra-tions for learning. We propose the LEWE framework to sample tasks for the weak policy to execute, and then learn from the successful trajecto-ries to achieve an improvement. As <b>the</b> <b>sampling</b> <b>strategy</b> {{is the key to}} the efficiency of LEWE, we further propose to incorporate active learning for <b>the</b> <b>sampling</b> <b>strategy</b> for LEWE. Experiments in a spatial positioning task show that LEWE with active learning can effectively and efficiently improve the weak policy and achieves a better performance than the comparing sampling approaches...|$|E
40|$|<b>The</b> Experience <b>Sampling</b> Method (ESM) {{has been}} widely used to collect {{longitudinal}} survey data from participants; in this domain, smartphone sensors are now used to augment <b>the</b> context-awareness of <b>sampling</b> <b>strategies.</b> In this paper, we study the effect of ESM design choices on the inferences {{that can be made}} from participants sensor data, and on the variance in survey responses that can be collected. In particular, we answer the question: are the behavioural inferences that a researcher makes with a trigger-defined subsample of sensor data biased by <b>the</b> <b>sampling</b> <b>strategy’s</b> design? We demonstrate that different single-sensor <b>sampling</b> <b>strategies</b> will result in what we refer to as contextual dissonance: a disagreement in how much different behaviours are represented in the aggregated sensor data. These results are not only relevant to researchers who use the ESM, but call for future work into strategies that may alleviate the biases that we measure. ACM Classification Keyword...|$|R
40|$|Approximation models (also {{known as}} metamodels) {{have been widely}} used in {{engineering}} design to facilitate analysis and optimization of complex systems that involve computationally expensive simulation programs. The accuracy of metamodels is directly related to <b>the</b> <b>sampling</b> <b>strategies</b> used. Our goal in this paper is to investigate the general applicability of sequential sampling for creating global metamodels. Various sequential sampling approaches are reviewed and new approaches are proposed. The performances of these approaches are investigated against that of the onestage approach using a set of test problems with a variety of features. The potential usages of sequential <b>sampling</b> <b>strategies</b> are also discussed...|$|R
40|$|The {{focus of}} this paper is on how to select a small sample of {{examples}} for labeling that can help us to evaluate many different classification models unknown at <b>the</b> time of <b>sampling.</b> We are particularly interested in studying <b>the</b> <b>sampling</b> <b>strategies</b> for problems in which the prevalence of the two classes is highly biased toward one of the classes. The evaluation measures of interest we want to estimate as accurately as possible are those obtained from the contingency table. We provide a careful theoretical analysis on sensitivity, specificity, and precision and show how <b>sampling</b> <b>strategies</b> should be adapted to the rate of skewness in data in order to effectively compute the three aforementioned evaluation measures. ...|$|R
