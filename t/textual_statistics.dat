18|3|Public
40|$|Qualitative methods {{allow us}} to work on stories, while {{quantitative}} approaches often cannot. But can we work on a story with quali-quantitative instruments? Can we find a story in a corpus just {{by the use of}} diagrams, without reading any of its individual documents? We will analyze one series of events that occurred in France in 2012 : the accident that took place at the Fessenheim reactor and the subsequent political decision to close the plant. The data have been gathered from the OT-Media database. We will make use of <b>textual</b> <b>statistics</b> and narrative models to understand the constitution and evolution of this series of events. Our aim is to show how the use of <b>textual</b> <b>statistics</b> permits to realize narrative analyses on multimedia corpora...|$|E
40|$|Literary {{research}} tool {{can provide}} concordance {{and many other}} <b>textual</b> <b>statistics</b> relating to authorship or sequence of composition. Mechanical text manipulation provides wide variety of text formats and conventions (such as upper case). This program is written in FORTRAN H for use on IBM- 360 computer...|$|E
40|$|Abstract In {{this paper}} we {{describe}} the approaches adopted to gener-ate the runs submitted to ImageCLEFPhoto 2009 with an aim to pro-mote document diversity in rankings. Four of our runs are text based approaches that employ <b>textual</b> <b>statistics</b> extracted from the captions of images, i. e. MMR [1] as {{a state of the}} art method for result diversification, two approaches that combine relevance information and clustering tech-niques, and an instantiation of Quantum Probability Ranking Principle. The fifth run exploits visual features of the provided images to re-rank the initial results by means of Factor Analysis. The results reveal that our methods based on only text captions consistently improve the per-formance of the respective baseline, while the approach that combines visual features with <b>textual</b> <b>statistics</b> shows lower levels of improvements. ...|$|E
40|$|Object {{recognition}} and localization are important tasks in computer vision. The {{focus of this}} work is the incorporation of contextual information in order to im-prove object {{recognition and}} localization. For instance, it is natural to expect not to see an elephant {{to appear in the}} middle of an ocean. We consider a sim-ple approach to encapsulate such common sense knowledge using co-occurrence statistics from web documents. By merely counting the number of times nouns (such as elephants, sharks, oceans, etc.) co-occur in web documents, we obtain a good estimate of expected co-occurrences in visual data. We then cast the problem of combining <b>textual</b> co-occurrence <b>statistics</b> with the predictions of image-based classifiers as an optimization problem. The resulting optimization problem serves as a surrogate for our inference procedure. Albeit the simplicity of the resulting optimization problem, it is effective in improving both recognition and localiza-tion accuracy. Concretely, we observe significant improvements in recognition and localization rates for both ImageNet Detection 2012 and Sun 2012 datasets. ...|$|R
50|$|Each chapter {{introduces}} the published, excavated, artifactual, and archival sources from earliest {{times to the}} late twentieth century and not only examines the context in which they were produced, preserved, and received, but also suggests and comments on the best secondary scholarship in Chinese, Japanese, and Western languages. The present fourth edition (green-cover) updates, expands, and corrects the third edition. Some 9,800 primary, secondary, reference works, journals, and databases are introduced {{in the course of the}} discussion (compared to 8,800 in the third edition; 4,000 in the yellow-cover edition of 2000; and 2,900 in the blue-cover edition of 1998). In addition to the greatly extended scope of the New Manual, new features are introductions to hundreds of digital resources (including 225 of the latest databases) and a wide selection of up-to-date secondary works (including citations of over 1,500 journal articles and book chapters). Interspersed throughout are short essays introducing the ancillary disciplines required for Chinese historical studies—archeology, astronomy, chronology and calendrics, codicology, diplomatics, epigraphy, genealogy, historical geography, historical linguistics, numismatics, onomastics, paleography, prosopography, sigillography, <b>statistics,</b> <b>textual</b> criticism, topology, and special branches of study such as oracle-bone script, bamboo and silk books, Dunhuang, Qingshuijiang, and Huizhou documents or the Ming-Qing archives. Other short essays address the uses of history and how to avoid errors in thought and analysis. There are chapters on translation into and out of Chinese. Throughout the New Manual lists of basic terms give standard translations.|$|R
40|$|Background {{national}} and international trends have identified concerns over the ability of health and social care workforces in {{meeting the needs of}} service users. Attention has increasingly been drawn to problems of recruiting and retaining professionals within higher education; however data in relation to the midwifery profession is scant. Aim to examine the perceptions and experiences of midwifery educators, in south-west England, about the challenges facing them sustaining the education workforce of the future. Design a mixed methodology approach was adopted involving heads of midwifery education and midwife educators. Methodology midwifery participants were recruited from three higher education institutions in south west England. Data collection comprised of self-administered questionnaires plus individual qualitative interviews with heads of midwifery education (n= 3), and tape recorded focus groups with midwife academics (n= 19). Numerical data were analysed using descriptive <b>statistics.</b> <b>Textual</b> data were analysed for themes that represented the experiences and perspectives of participants. Ethics approval was granted by one University Ethics committee. Findings demographic data suggests that within south-west England, there is a clear ageing population and few in possession of a doctorate within midwifery. The six identified sub-themes represented in the data describe challenges and tensions that midwifery academics experienced in their efforts to attract new recruits and retain those in post in a highly changing educational environment which demands more from a contracting workforce. Conclusion and implications for practice there remain some serious challenges facing midwifery educators in sustaining the future education workforce, which if unresolved may jeopardise standards of education and quality of care women receive. Active succession planning and more radical approaches that embrace flexible careers will enable educational workforce to be sustained and by a clinically credible and scholarly orientated midwifery workforce...|$|R
40|$|Ce qui compte ‖ (‗what counts‘), {{which should}} also be read as ―ceux qui comptent‖ (‗those who count‘), thus {{referring}} to statistical lexicographers, is the second volume of a compendium of papers by Etienne Brunet, a Professor at the University of Nice who spent {{the last third of}} the twentieth century developing methods and tools in lexicometry. For those who have never heard of Etienne Brunet, who essentially published in French, he could be depicted as the most prominent scholar of the French school of (computational) <b>textual</b> <b>statistics,</b> following the seminal role played in this respect by Charles Muller (1977; at a time when computers were not so common in the scholarly landscape). With a background in literary studies, he discovered the possible usage of computers for identifying the stylistic characteristics of a text {{in the course of his}} doctoral work on Jean Giroudoux and thereafter spent all his academic life exploring further the application of statistical methods to the understanding of lexical phenomena in texts. Etienne Brunet entered the still to be explored field of <b>textual</b> <b>statistics</b> with...|$|E
40|$|In this paper, after reconstructing some {{essential}} phases in {{the evolution}} of automatic analysis of texts, the steps of an ideal strategy for the statistical analysis of textual data are defined. The characteristics of lexical and textual analysis are described, as well as some techniques of information extraction, that employ resources which are endogenous and exogenous with respect to the texts to be examined. In order to show the potential of <b>textual</b> <b>statistics</b> and of the most recent Text Mining applications, some relevant case studies concerning statistical survey and document analysis are illustrated...|$|E
40|$|In {{this paper}} we {{describe}} the approaches adopted to generate the five runs submitted to ImageClefPhoto 2009 by the University of Glasgow. The aim of our methods is to exploit document diversity in the rankings. All our runs used text statistics extracted from the captions associated to each image in the collection, except one run which combines the <b>textual</b> <b>statistics</b> with visual features extracted from the provided images. The results suggest that our methods based on text captions significantly improve {{the performance of the}} respective baselines, while the approach that combines visual features with text statistics shows lower levels of improvements...|$|E
40|$|Through the {{probabilistic}} reasoning {{is possible to}} take rational decisions {{even if there is}} limited information. The bayesian network is a probabilistic model that allows representing the uncertain knowledge in an given domain (of knowledge) in a graphical way. This method is not much used in planning yet, while it could be useful to analyse the uncertain variables that condition decisions in this field, e. g. the evaluation of the landscape elements to be protected. What are the elements, and the relationship, that represent the alpine landscape in the collective imagination? We have tried to find them using bayesian networks and <b>textual</b> <b>statistics.</b> ...|$|E
40|$|In <b>textual</b> <b>statistics,</b> as {{in natural}} {{language}} processing and corpus linguistics, the study of sequences of contiguous words that occur together more often than by chance is a major topic of interest. In the case of pairs of words, Fisher’s exact test is becoming the reference index to identify them. The objective {{of this study is}} to propose a generalization of this index to the analysis of trigrams and longer sequences using a Monte-Carlo procedure. The results of an initial evaluation suggest that this approach could complement other indices, but also that it has a major drawback: a large number of trigrams get a maximum score of collocation...|$|E
40|$|A {{system is}} {{proposed}} that combines textual and visual statistics {{in a single}} index vector for content-based search of a WWW image database. <b>Textual</b> <b>statistics</b> are captured in vector form using latent semantic indexing (LSI) based on text in the containing HTML document. Visual statistics are captured in vector form using color and orientation histograms. By using an integrated approach, it becomes possible {{to take advantage of}} possible statistical couplings between the content of the document (latent semantic con-tent) and the contents of images (visual statistics). The combined approach allows improved performance in con-ducting content-based search. Search performance experi-ments are reported for a database containing 100, 000 im-ages collected from the WWW. ...|$|E
40|$|This work uses {{statistical}} {{and linguistic}} procedures to analyse a corpus of 57 traditional End of Year addresses by nine {{presidents of the}} Italian Republic. These addresses are compared and contrasted {{in order to identify}} the discoursal distinctive features and the similarities and differences among them. The proposed methodology is an attempt to link traditional qualitative methods with modern <b>textual</b> <b>statistics.</b> Results show that the hypergeometric, chi 2 and bootstrap tests are effective in identifying distinctive words among presidents ('between' perspective) and among addresses delivered by the same president ('within' perspective) and that individual characteristic features and personal traits are more important than other factors in the End of Year Addresses topics...|$|E
40|$|Here a {{collection}} of 1169 abstracts, which corresponds to articles that the Journal of Marketing Research has published from 2005 to 2014, are analysed under a novel approach. We apply several statistical methods, such as Principal Components Analysis and Correspondence Analysis to identify the way Marketing vocabulary is evolving. Similarly those articles that introduce new vocabulary are identified and the preferred words by authors are also detected. In order to provide an easy-to-understand explanation, we provide our results graphically. A word-cloud with the most frequent words is given first. Secondly abstracts-words are represented on the factorial plane. Finally one representation of word-years allows us to detect changes on the vocabulary through the passing of time. Comment: Marketing, <b>Textual</b> <b>Statistics,</b> Vocabulary evolving, Influential articles, Correspondence analysi...|$|E
40|$|In this paper, {{we present}} a simple but {{efficient}} approach for the automatic mood classification of microblogging messages from Plurk platform. In contrast with Twitter, Plurk {{has become the most}} popular microblogging service in Taiwan and other countries 1; however, no previous research has been done for the emotion and mood recognition, nor the Chinese affective terms or corpus available. Following the line of mashup programming, we thus construct a dynamic plurk corpus by pipelining Plurk APIs, Yahoo! Chinese segmentation APIs, etc to preprocess and annotate the corpus data. Based on the corpus, we conduct experiments by way of combining <b>textual</b> <b>statistics</b> and emoticons data, and our method yield the results with high performance. This work can be further extended to combine with affective ontology designed with emotion theory of appraisal...|$|E
40|$|International audienceRhetorical {{strategy}} is relevant {{in the law}} domain, where language is a vital instrument. <b>Textual</b> <b>statistics</b> have much to offer for uncovering such a strategy. We propose a methodology that starts from a non-structured text; first, the breakpoints are automatically detected and lexically homogeneous parts are identified; then, {{the shape of the}} text through the trajectory of these parts and their hierarchical structure are uncovered; finally, the argument flow is tracked along. Several methods are combined. Chronological clustering of multidimensional count series detects the breakpoints; the shape of the text is revealed by applying correspondence analysis to the parts×words table while the progression of the argument is described by labelled time-constrained hierarchical clustering. This methodology is illustrated on a rhetoric forensic application, concretely a closing speech delivered by a prosecutor at Barcelona Criminal Court. This approach could also be useful in politics, communication and professional writing...|$|E
40|$|Some WWW image engines {{allow the}} user to form a query in terms of text {{keywords}}. To build the image index, keywords are extracted heuristically from HTML documents containing each image, and/or from the image URL and file headers. Unfortunately, text-based image engines have merely retro-fitted standard SQL database query methods, {{and it is difficult}} to include images cues within such a framework. On the other hand, visual statistics (e. g., color histograms) are often insufficient for helping users find desired images in a vast WWW index. By truly unifying textual and visual statistics, one would expect to get better results than either used separately. In this paper, we propose an approach that allows the combination of visual statistics with <b>textual</b> <b>statistics</b> in the vector space representation commonly used in query by image content systems. Text statistics are captured in vector form using latent semantic indexing (LSI). The LSI index for an HTML document is then associated with each of the images contained therein. Visual statistics (e. g., color, orientedness) are also computed for each image. The LSI and visual statistic vectors are then combined into a single index vector that can be used for content-based search of the resulting image database. By using an integrated approach, we are able to take advantage of possible statistical couplings between the topic of the document (latent semantic content) and the contents of images (visual statistics). This allows improved performance in conducting content-based search. This approach has been implemented in a WWW image search engine prototype. National Science Foundation (IIS- 9624168, CDA- 9623865, CDA- 9529403, ACI- 9619019...|$|E
40|$|The present work {{sets out}} to {{investigate}} the stylistic profiles of two modern Chinese versions of Cervantes’s Don Quijote (I) : by Yang Jiang (1978), the first direct translation from Castilian to Chinese, and by Liu Jingsheng (1995), {{which is one of}} the most commercially successful versions of the Castilian literary classic. This thesis focuses on a detailed linguistic analysis carried out with the help of the latest textual analytical tools, natural language processing applications and statistical packages. The type of linguistic phenomenon singled out for study is four-character expressions (FCEXs), which are a very typical category of Chinese phraseology. The work opens with the creation of a descriptive framework for the annotation of linguistic data extracted from the parallel corpus of Don Quijote. Subsequently, the classified and extracted data are put through several statistical tests. The results of these tests prove to be very revealing regarding the different use of FCEXs in the two Chinese translations. The computational modelling of the linguistic data would seem to indicate that among other findings, while Liu’s use of archaic idioms has followed the general patterns of the original and also of Yang’s work in the first half of Don Quijote I, noticeable variations begin to emerge in the second half of Liu’s more recent version. Such an idiosyncratic use of archaisms by Liu, which may be defined as style shifting or style variation, is then analyzed in quantitative terms through the application of the proposed context-motivated theory (CMT). The results of applying the CMT-derived statistical models show that the detected stylistic variation may well point to the internal consistency of the translator in rendering the second half of Part I of the novel, which reflects his freer, more creative and experimental style of translation. Through the introduction and testing of quantitative research methods adapted from corpus linguistics and <b>textual</b> <b>statistics,</b> this thesis has made a major contribution to methodological innovation in the study of style within the context of corpus-based translation studies. Imperial Users onl...|$|E
40|$|Stylistic {{variation}} in text is not incidental but {{an integral part}} of the intended and understood communication: the content and form of the message cannot be divorced. Discussing style, however is difficult, without descending into specifics or idiosyncracies. Mostly, readers report stylistic differences in terms of genres. Genres, while vague and undefined, are wellestablished and talked about: very early on, readers learn to distinguish genres. Differences readers are aware of are mostly based on utility- not on textual characteristics per se. By <b>textual</b> <b>statistics,</b> it is easy enough to establish that there are observable differences between genres we find in collections of textual material. However, using the textual, lexical, and other linguistic features we find to cluster the collection without anchoring information in usage will risk finding statistically stable categories of data without explanatory power or utility. This chapter argues for more informed target metrics for the statistical processing of text collections. Much as operationalized relevance proved a useful goal to strive for in information retrieval, research in textual stylistics, whether application oriented or philologically inclined, needs goals formulated in terms of pertinence, relevance, and utility — notions that agree with reader experience of text. This brief paper gives an example of statistical stylistic experimentation and argues for more informed measures of variation and choice and more informed measures of readership analysis to be able to posit dimensions of textual variation usefully. Variation in text Texts are much more than what they are about. Authors make choices when they write a text: they decide how to organize the material they have planned to introduce; they make choices between synonyms and syntactic constructions; they choose an intended audience for the text. Authors will make these choices in various ways and for various reasons: based on personal preferences, on their view of the reader, and on what they know and like about other similar texts. These choices are governed by a range of constraints...|$|E
40|$|Traditionally, {{humanities}} scholars {{carrying out}} research {{on a specific}} or on multiple literary work(s) {{are interested in the}} analysis of related texts or text passages. But the digital age has opened possibilities for scholars to enhance their traditional workflows. Enabled by digitization projects, humanities scholars can nowadays reach a large number of digitized texts through web portals such as Google Books or Internet Archive. Digital editions exist also for ancient texts; notable examples are PHI Latin Texts and the Perseus Digital Library. This shift from reading a single book “on paper” to the possibility of browsing many digital texts is one of the origins and principal pillars of the digital humanities domain, which helps developing solutions to handle vast amounts of cultural heritage data – text being the main data type. In contrast to the traditional methods, the digital humanities allow to pose new research questions on cultural heritage datasets. Some of these questions can be answered with existent algorithms and tools provided by the computer science domain, but for other humanities questions scholars need to formulate new methods in collaboration with computer scientists. Developed in the late 1980 s, the digital humanities primarily focused on designing standards to represent cultural heritage data such as the Text Encoding Initiative (TEI) for texts, and to aggregate, digitize and deliver data. In the last years, visualization techniques have gained more and more importance when it comes to analyzing data. For example, Saito introduced her 2010 digital humanities conference paper with: “In recent years, people have tended to be overwhelmed by a vast amount of information in various contexts. Therefore, arguments about ’Information Visualization’ as a method to make information easy to comprehend are more than understandable. ” A major impulse for this trend was given by Franco Moretti. In 2005, he published the book “Graphs, Maps, Trees”, in which he proposes so-called distant reading approaches for textual data that steer the traditional way of approaching literature towards a completely new direction. Instead of reading texts in the traditional way – so-called close reading –, he invites to count, to graph and to map them. In other words, to visualize them. This dissertation presents novel close and distant reading visualization techniques for hitherto unsolved problems. Appropriate visualization techniques have been applied to support basic tasks, e. g., visualizing geospatial metadata to analyze the geographical distribution of cultural heritage data items or using tag clouds to illustrate <b>textual</b> <b>statistics</b> of a historical corpus. In contrast, this dissertation focuses on developing information visualization and visual analytics methods that support investigating research questions that require the comparative analysis of various digital humanities datasets. We first {{take a look at the}} state-of-the-art of existing close and distant reading visualizations that have been developed to support humanities scholars working with literary texts. We thereby provide a taxonomy of visualization methods applied to show various aspects of the underlying digital humanities data. We point out open challenges and we present our visualizations designed to support humanities scholars in comparatively analyzing historical datasets. In short, we present (1) GeoTemCo for the comparative visualization of geospatial-temporal data, (2) the two tag cloud designs TagPies and TagSpheres that comparatively visualize faceted textual summaries, (3) TextReuseGrid and TextReuseBrowser to explore re-used text passages among the texts of a corpus, (4) TRAViz for the visualization of textual variation between multiple text editions, and (5) the visual analytics system MusikerProfiling to detect similar musicians to a given musician of interest. Finally, we summarize our and the collaboration experiences of other visualization researchers to emphasize the ingredients required for a successful project in the digital humanities, and we take a look at future challenges in that research field...|$|E

