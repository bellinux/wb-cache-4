7|24|Public
50|$|Sim4 is a {{nucleotide}} sequence alignment program akin to BLAST but specifically tailored to DNA to cDNA/EST (Expressed Sequence <b>Tag)</b> <b>alignment</b> (as opposed to DNA-DNA or protein-protein alignment). It {{was written by}} Florea et al.|$|E
5000|$|Tag cloud visual {{taxonomy}} {{is determined}} {{by a number of}} attributes: tag ordering rule (e.g. alphabetically, by importance,by context, randomly, ordered for visual quality), shape of the entire cloud (e.g. rectangular, circle, given map borders), shape of tag bounds (rectangle, or character body), tag rotation (none, free, limited), vertical <b>tag</b> <b>alignment</b> (sticking to typographical baselines, free).A tag cloud on the web must address problems of modeling and controlling aesthetics, constructing a two-dimensional layout of tags, and all these must be done in short time on volatile browser platform.Tags clouds to be used on the web must be in HTML, not graphics, to make them robot-readable,they must be constructed on the client side using the fonts available in the browser,and they must fit in a rectangular box [...]|$|E
40|$|Recent {{progress}} in massively parallel sequencing platforms has allowed for genome-wide measurements of DNA-associated proteins {{using a combination}} of chromatin immunoprecipitation and sequencing (ChIP-seq). While a variety of methods exist for analysis of the established microarray alternative (ChIP-chip), few approaches have been described for processing ChIP-seq data. To fill this gap, we propose an analysis pipeline specifically designed to detect protein binding positions with high accuracy. Using three separate datasets, we illustrate new methods for improving <b>tag</b> <b>alignment</b> and correcting for background signals. We also compare sensitivity and spatial precision of several novel and previously described binding detection algorithms. Finally, we analyze the relationship between the depth of sequencing and characteristics of the detected binding positions, and provide a method for estimating the sequencing depth necessary for a desired coverage of protein binding sites. A combination of chromatin immunoprecipitation and microarray hybridization (ChIP-chip) has been used extensively to determine chromosome binding patterns of DNA-associated proteins 1. Several recent studies have demonstrated that newly developed high-throughpu...|$|E
40|$|The {{availability}} of complete, annotated genomic sequence information in model organisms {{is a rich}} resource that can be extended to understudied orphan crops through comparative genomic approaches. We report here a software tool (CISprimerTOOL) for the identification of conserved intron scanning regions using expressed sequence <b>tag</b> <b>alignments</b> to a completely sequenced model crop genome. The method used is based on earlier studies reporting the assessment of conserved intron scanning primers (called CISP) within relatively conserved exons located near exon-intron boundaries from onion, banana, sorghum and pearl millet alignments with rice. The tool is freely available to academic users at www. icrisat. org/gt-bt/ CISPTool. htm...|$|R
40|$|Incorporating {{linguistic}} knowledge into word alignment {{is becoming increasingly}} important for current approaches in statistical machine translation research. To improve automatic word alignment and ultimately machine translation quality, an annotation framework is jointly proposed by LDC (Linguistic Data Consortium) and IBM. The framework enriches word alignment corpora to capture contextual, syntactic and language-specific features by introducing linguistic <b>tags</b> to the <b>alignment</b> annotation. Two annotation schemes constitute the framework: <b>alignment</b> and <b>tagging.</b> The <b>alignment</b> scheme aims to identify minimum translation units and translation relations by using minimum-match and attachment annotation approaches. A set of word <b>tags</b> and <b>alignment</b> link <b>tags</b> are designed in the tagging scheme to describe these translation units and relations. The framework produces a solid ground-level alignment base upon which larger translation unit alignment can be automatically induced. To test the soundness of this work, evaluation is performed on a pilot annotation, resulting in inter- and intra- annotator agreement of above 90 %. To date LDC has produced manual word <b>alignment</b> and <b>tagging</b> on 32, 823 Chinese-English sentences following this framework. 1...|$|R
40|$|We {{describe}} a method (implemented in a program, GAZE) for assembling arbitrary evidence for individual gene components (features) into predictions of complete gene structures. Our system is generic in {{that both the}} features themselves, and the model of gene structure against which potential assemblies are validated and scored, are external to the system and supplied by the user. GAZE uses a dynamic programming algorithm to obtain the highest scoring gene structure according to the model and posterior probabilities that each input feature {{is part of a}} gene. A novel pruning strategy ensures that the algorithm has a run-time effectively linear in sequence length. To demonstrate the flexibility of our system in the incorporation of additional evidence into the gene prediction process, we show how {{it can be used to}} both represent nonstandard gene structures (in the form of trans-spliced genes in Caenorhabditis elegans), and make use of similarity information (in the form of Expressed Sequence <b>Tag</b> <b>alignments),</b> while requiring no change to the underlying software. GAZE is available at [URL]...|$|R
40|$|In this paper, {{we propose}} a novel Weakly-Supervised D-ual Clustering (WSDC) {{approach}} for image semantic seg-mentation with image-level labels, i. e., collaboratively per-forming image segmentation and <b>tag</b> <b>alignment</b> with those regions. The proposed approach is motivated from the ob-servation that superpixels belonging to an object class usu-ally exist across multiple images and hence can be gath-ered via {{the idea of}} clustering. In WSDC, spectral clus-tering is adopted to cluster the superpixels obtained from a set of over-segmented images. At the same time, a lin-ear transformation between features and labels {{as a kind of}} discriminative clustering is learned to select the discrimi-native features among different classes. The both clustering outputs should be consistent as much as possible. Besides, weakly-supervised constraints from image-level labels are imposed to restrict the labeling of superpixels. Finally, the non-convex and non-smooth objective function are efficient-ly optimized using an iterative CCCP procedure. Exten-sive experiments conducted on MSRC and LabelMe dataset-s demonstrate the encouraging performance of our method in comparison with some state-of-the-arts. 1...|$|E
40|$|Natural {{language}} {{understanding is}} to specify a computational model that maps sentences to their semantic mean representation. In this paper, we propose a novel framework {{to train the}} statistical models without using expensive fully annotated data. In particular, the input of our framework {{is a set of}} sentences labeled with abstract semantic annotations. These annotations encode the underlying embedded semantic structural relations without explicit word/semantic <b>tag</b> <b>alignment.</b> The proposed framework can automatically induce derivation rules that map sentences to their semantic meaning representations. The learning framework is applied on two statistical models, the conditional random fields (CRFs) and the hidden Markov support vector machines (HM-SVMs). Our experimental results on the DARPA communicator data show that both CRFs and HM-SVMs outperform the baseline approach, previously proposed hidden vector state (HVS) model which is also trained on abstract semantic annotations. In addition, the proposed framework shows superior performance than two other baseline approaches, a hybrid framework combining HVS and HM-SVMs and discriminative training of HVS, with a relative error reduction rate of about 25 % and 15 % being achieved in F-measure...|$|E
40|$|During 1998 {{the primary}} focus of the Genome Sequence DataBase (GSDB; [URL]) located at the National Center for Genome Resources (NCGR) has been to improve data quality, improve data collections, and provide new methods and tools to access and analyze data. Data quality has been {{improved}} by extensive curation of certain data fields necessary for maintaining data collections and for using certain tools. Data quality has also been increased by improvements to the suite of programs that import data from the International Nucleotide Sequence Database Collaboration (IC). The Sequence <b>Tag</b> <b>Alignment</b> and Consensus Knowledgebase (STACK), a database of human expressed gene sequences developed by the South African National Bioinformatics Institute (SANBI), became available within the last year, allowing public access to this valuable resource of expressed sequences. Data access was improved by the addition of the Sequence Viewer, a platform-independent graphical viewer for GSDB sequence data. This tool has also been integrated with other searching and data retrieval tools. A BLAST homology search service was also made available, allowing researchers to search all of the data, including the unique data, that are available from GSDB. These improvements are designed to make GSDB more accessible to users, extend the rich searching capability already present in GSDB, and to facilitate the transition to an integrated system containing many different types of biological data...|$|E
40|$|Translated bi-texts contain {{complementary}} language cues, {{and previous}} work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions {{between the two}} languages. However, most previous approaches to bilingual <b>tagging</b> assume word <b>alignments</b> are given as fixed input, which can cause cascading errors. We observe that NER label information {{can be used to}} correct alignment mistakes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two unidirectional alignment models. We introduce additional cross-lingual edge factors that encourage agreements between <b>tagging</b> and <b>alignment</b> decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines. ...|$|R
40|$|As {{vertebrate}} genome sequences {{near completion}} and research refocuses to their analysis, {{the issue of}} effective genome annotation display becomes critical. A mature web tool for rapid and reliable display of any requested portion of the genome at any scale, together with several dozen aligned annotation tracks, is provided at [URL] This browser displays assembly contigs and gaps, mRNA and expressed sequence <b>tag</b> <b>alignments,</b> multiple gene predictions, cross-species homologies, single nucleotide polymorphisms, sequence-tagged sites, radiation hybrid data, transposon repeats, and more as a stack of coregistered tracks. Text and sequence-based searches provide quick and precise access to any region of specific interest. Secondary links from individual features lead to sequence details and supplementary off-site databases. One-half of the annotation tracks are computed at the University of California, Santa Cruz from publicly available sequence data; collaborators worldwide provide the rest. Users can stably add their own custom tracks to the browser for educational or research purposes. The conceptual and technical framework of the browser, its underlying MYSQL database, and overall use are described. The web site currently serves over 50, 000 pages per day to over 3000 different users...|$|R
40|$|Type IVa pili (T 4 aP) are {{ubiquitous}} microbial appendages {{used for}} adherence, twitching motility, DNA uptake, and electron transfer. Many of these functions depend on dynamic assembly and disassembly of the pilus by a megadalton-sized, cell envelope-spanning protein complex {{located at the}} poles of rod-shaped bacteria. How the T 4 aP assembly complex becomes integrated into the cell envelope {{in the absence of}} dedicated peptidoglycan (PG) hydrolases is unknown. After ruling out the potential involvement of housekeeping PG hydrolases in the installation of the T 4 aP machinery in Pseudomonas aeruginosa, we discovered that key components of inner (PilMNOP) and outer (PilQ) membrane subcomplexes are recruited to future sites of cell division. Midcell recruitment of a fluorescently <b>tagged</b> <b>alignment</b> subcomplex component, mCherry-PilO, depended on PilQ secretin monomers—specifically, their N-terminal PG-binding AMIN domains. PilP, which connects PilO to PilQ, was required for recruitment, while PilM, which is structurally similar to divisome component FtsA, was not. Recruitment preceded secretin oligomerization in the outer membrane, as loss of the PilQ pilotin PilF had no effect on localization. These results were confirmed in cells chemically blocked for cell division prior to outer membrane invagination. The hub protein FimV and a component of the polar organelle coordinator complex—PocA—were independently required for midcell recruitment of PilO and PilQ. Together, these data suggest an integrated, energy-efficient strategy for the targeting and preinstallation—rather than retrofitting—of the T 4 aP system into nascent poles, without the need for dedicated PG-remodeling enzymes...|$|R
40|$|In {{order to}} {{provided}} a novel maximised {{approach to the}} generation of accurate, comprehensive, consensus sequences of the expressed human genome, we have developed and produced a system for a novel-representation, broad gene coverage, consensus database of expressed human gene fragments (ESTs). To perform clustering of ESTs, we have developed and employed D 2 -cluster, an algorithm based on the d 2 -search algorithm (Hide et al. 1994) speci cally for EST clustering. D 2 -cluster does not require alignment in order to perform clustering (Burke, Davison and Hide, in prep). We have incorporated d 2 -cluster into a portable and novel system to perform clustering, alignment and automated error analysis of publicly available expressed sequence tags (STACK PACK). The system includes a statistically robust algorithm that can detect and compensate for error within an aligned cluster of ESTs. We have manufactured a database of partial human consensus sequences from 552 013 ESTs from dbEST 040896 and TIGR. The database is termed Sequence <b>Tag</b> <b>Alignment</b> and Consensus Knowledgebase (STACK). STACK 1. 0 contains 18 divisions based on tissue annotation identifying 204 431 unique sequences and generating 76 131 consensi which represent 321 134 ESTs. The consensus sequences have an average length of 497 bases, a 39 % increase over the 357 base average length of the input data set. Clone Ids are used to join 92 759 unique sequences and 48 858 consensi into 61 632 linked sequences, averaging 900 bases each. The distribution of clusters compares favourably with UniGene, re ecting the di erence in methodology of clustering and the higher input number of sequences into STACK. SANIGENE high accuracy database is also generated, consisting of sequences which agree inatleast two ESTs. STACK is a distributable, core information resource upon which a comprehensive knowledgebase can be built. ...|$|E
40|$|Completed genome {{sequences}} provide templates for {{the design}} of genome analysis tools in orphan species lacking sequence information. To demonstrate this principle, we designed 384 PCR primer pairs to conserved exonic regions flanking introns, using Sorghum/Pennisetum expressed sequence <b>tag</b> <b>alignments</b> to the Oryza genome. Conserved-intron scanning primers (CISPs) amplified single-copy loci at 37 % to 80 % success rates in taxa that sample much of the approximately 50 -million years of Poaceae divergence. While the conserved nature of exons fostered cross-taxon amplification, the lesser evolutionary constraints on introns enhanced single-nucleotide polymorphism detection. For example, in eight rice (Oryza sativa) genotypes, polymorphism averaged 12. 1 per kb in introns but only 3. 6 per kb in exons. Curiously, among 124 CISPs evaluated across Oryza, Sorghum, Pennisetum, Cynodon, Eragrostis, Zea, Triticum, and Hordeum, 23 (18. 5 %) seemed to be subject to rigid intron size constraints that were independent of per-nucleotide DNA sequence variation. Furthermore, we identified 487 conserved-noncoding sequence motifs in 129 CISP loci. A large CISP set (6, 062 primer pairs, amplifying introns from 1, 676 genes) designed using an automated pipeline showed generally higher abundance in recombinogenic than in nonrecombinogenic regions of the rice genome, thus providing relatively even distribution along genetic maps. CISPs are an effective means to explore poorly characterized genomes for both DNA polymorphism and noncoding sequence conservation on a genome-wide or candidate gene basis, and also provide anchor points for comparative genomics across a diverse range of species...|$|R
30|$|Text {{analysis}} in TMM 13, {{was based on}} part-of-speech tagging and is summarized next: (a) extraction of the movie transcript from the English subtitle file, (b) part-of-speech <b>tagging,</b> (c) audio-text <b>alignment,</b> (d) assignment of a text saliency value { 0.2, 0.5, 0.7, 1 } to each word, and finally (e) text saliency computation and assignment of a text saliency value to each frame.|$|R
40|$|We have {{developed}} a rice (Oryza sativa) genome annotation database (Osa 1) that provides structural and functional annotation for this emerging model species. Using the sequence of O. sativa subsp. japonica cv Nipponbare from the International Rice Genome Sequencing Project, pseudomolecules, or virtual contigs, of the 12 rice chromosomes were constructed. Our most recent release, version 3, represents our third build of the pseudomolecules and is composed of 98 % finished sequence. Genes were identified using a series of computational methods developed for Arabidopsis (Arabidopsis thaliana) that were modified for use with the rice genome. In release 3 of our annotation, we identified 57, 915 genes, of which 14, 196 are related to transposable elements. Of these 43, 719 nontransposable element-related genes, 18, 545 (42. 4 %) were annotated with a putative function, 5, 777 (13. 2 %) were annotated as encoding an expressed protein with no known function, and the remaining 19, 397 (44. 4 %) were annotated as encoding a hypothetical protein. Multiple splice forms (5, 873) were detected for 2, 538 genes, {{resulting in a total}} of 61, 250 gene models in the rice genome. We incorporated experimental evidence into 18, 252 gene models {{to improve the quality of}} the structural annotation. A series of functional data types has been annotated for the rice genome that includes alignment with genetic markers, assignment of gene ontologies, identification of flanking sequence <b>tags,</b> <b>alignment</b> with homologs from related species, and syntenic mapping with other cereal species. All structural and functional annotation data are available through interactive search and display windows as well as through download of flat files. To integrate the data with other genome projects, the annotation data are available through a Distributed Annotation System and a Genome Browser. All data can be obtained through the project Web pages at [URL]...|$|R
3000|$|To the {{learning}} technologist, {{a mandate to}} “teach thinking skills” immediately brings to mind such functions as presenting problems, teaching reasoning and problem solving strategies, managing inquiry projects, scripting argumentation, <b>tagging</b> items for <b>alignment</b> with curriculum goals, and so forth. By contrast, a mandate to help students develop into “thinkers of significant thoughts” may not immediately bring to mind anything in particular beyond what is already available in an information-rich classroom. Further reflection, however, suggests a number of definite challenges for technology designed for helping learners develop into thinkers: [...]...|$|R
40|$|This chapter {{presents}} some of {{the basic}} language engineering preprocessing steps (tokenization, part-of-speech tagging, lemmatization, and sentence and word <b>alignment).</b> <b>Tagging</b> is among the most important processing steps and its accuracy significantly influences any further processing. Therefore, tagset design, validation and correction of training data and the various techniques for improving the tagging quality are discussed in detail. Since sentence and word alignment are prerequisite operations for exploiting parallel corpora for a multitude of purposes such as machine translation, bilingual lexicography, import annotation etc., these issues are also explored in detail...|$|R
40|$|Many {{models in}} NLP involve latent variables, such as unknown parses, <b>tags,</b> or <b>alignments.</b> Finding the optimal model {{parameters}} is then usually a difficult nonconvex optimization problem. The usual practice is {{to settle for}} local optimization methods such as EM or gradient ascent. We explore how one might instead search for a global optimum in parameter space, using branch-and-bound. Our method would eventually find the global maximum (up to a user-specified ɛ) if run for long enough, but at any point can return a suboptimal solution together with an upper bound on the global maximum. As an illustrative case, we study a generative model for dependency parsing. We search for the maximum-likelihood model parameters and corpus parse, subject to posterior constraints. We show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints. We use the Reformulation Linearization Technique to produce convex relaxations during branch-and-bound. Although these techniques do not yet provide a practical solution to our instance of this NP-hard problem, they sometimes find better solutions than Viterbi EM with random restarts, in the same time. ...|$|R
40|$|The large {{fraction}} of repetitive DNA in many plant genomes has complicated {{all aspects of}} DNA sequencing and assembly, and thus techniques that enrich for genes and low-copy sequences have been employed to isolate gene space. Methyl-sensitive restriction enzymes, with six base pair recognition sites, were evaluated on genomic DNA of the bread wheat ‘Chinese Spring’ as {{a different approach to}} enrich for genes. I, I, I, and II were used to digest wheat genomic DNA and fragments ranging from 400 bp to 2. 0 kb were cloned and unidirectionally sequenced. All four enzymes provided some level of enrichment for gene space; however, II and I reduced the number of clones with repeat elements to just 16. 2 and 19. 1 %, respectively. II and I were also effective in enrichment in corn and tobacco. Corn libraries made with II and I had 58. 7 and 71. 2 %, respectively, of the clones with significant expressed sequence <b>tag</b> (EST) <b>alignments,</b> while tobacco libraries made with the same enzymes had 51. 7 and 65. 3 %, respectively. With the development of ultra-throughput sequencing technologies, this technique provides an opportunity to rapidly and efficiently obtain sequencing from gene-rich regions...|$|R
40|$|Labeling of {{sentence}} boundaries is {{a necessary}} prerequisite for many natural language processing tasks, including part-of-speech <b>tagging</b> and sentence <b>alignment.</b> Endof -sentence punctuation marks are ambiguous; to disambiguate them most systems use brittle, special-purpose regular expression grammars and exception rules. As an alternative, we have developed an efficient, trainable algorithm that uses a lexicon with part-of-speech probabilities and a feed-forward neural network. After training for less than one minute, the method correctly labels over 98. 5 % of sentence boundaries in a corpus of over 27, 000 sentence-boundary marks. We show the method to be efficient and easily adaptable to different text genres, including single-case texts. 1 Introduction Labeling of sentence boundaries {{is a necessary}} prerequisite for many natural language processing (NLP) tasks, including part-of-speech tagging (Church, 1988),(Cutting et al., 1991), and sentence alignment (Gale and Church, 1993), (Kay a [...] ...|$|R
40|$|Phaeosphaeria nodorum (anamorph: Stagonospora nodorum) is {{the causal}} agent of Stagonospora nodorum blotch (SNB, syn. glume blotch) in wheat. P. nodorum is {{estimated}} to cause up to 31 % wheat yield loss worldwide. Within Australia it is the primary pathogen of wheat and {{is estimated to}} cause losses of $ 108 million per annum. The genome assembly of P. nodorum was sequenced in 2005 {{and was the first}} species in the class Dothideomycetes, a significant fungal taxon containing several major phytopathogens, to be publically released. The P. nodorum genome database has since evolved from basic sequence data into a powerful resource for studying the SNB host-pathogen interaction and advancing the understanding of fungal genome structure. The genes of P. nodorum have been annotated to a high level of accuracy and now serve as a model dataset for comparative purposes. P. nodorum gene annotations have been refined by a combination of several techniques including manual curation, orthology with related species, expressed sequence <b>tag</b> (EST) <b>alignment,</b> and proteogenomics. Analysis of the repetitive DNA in the P. nodorum genome lead to the development of software for the analysis of repeat-induced point mutation (RIP), a fungal-specific genome defence mechanism, which was a major improvement upon previous methods. Comparative genomics between P. nodorum and related species has highlighted a novel pattern of genome sequence conservation between filamentous fungi called ‘mesosynteny’ and has lead to the development of novel 'genome finishing' strategies...|$|R
40|$|This paper {{presents}} {{an overview of}} the University of Washington statistical machine translation system developed for the 2006 TCSTAR evaluation campaign. We use a statistical phrase-based system with multiple decoding passes and a log-linear probability model. Our main focus was on exploring the possibility of using morpho-syntactic knowledge (lemmas and part-of-speech <b>tags)</b> for word <b>alignment,</b> language modeling, processing out-of-vocabulary words, and reordering. Use of these knowledge sources led to substantial improvements for translation from English into Spanish and minor improvements for the opposite translation direction. In addition, we investigated hidden-event n-gram models for postprocessing of machine translation output. 1. Overview The UW machine translation system for parliamentary proceedings is a statistical phrase-based system. Building on earlier experiences with a smaller Spanish to English translation task (Kirchhoff and Yang, 2005), the system was retrained from scratch for the 2006 TC-STAR EPPS tasks of Spanish-English and English-Spanish translation of verbatim and final text edition (FTE) data. The system uses th...|$|R
40|$|The {{segmentation}} {{of a text}} into {{sentences is}} a necessary prerequisite for many natural language processing tasks, including part-of-speech <b>tagging</b> and sentence <b>alignment.</b> This is a non-trivial task, however, since end-ofsentence punctuation marks are ambiguous. A period, for example, can denote a decimal point, an abbreviation, {{the end of a}} sentence, or even an abbreviation {{at the end of a}} sentence. To disambiguate punctuation marks most systems use brittle, special-purpose regular expression grammars and exception rules. Such approaches are usually limited to the text genre for which they were developed and cannot be easily adapted to new text types. They can also not be easily adapted to other natural languages. As an alternative, I present an efficient, trainable algorithm that can be easily adapted to new text genres and some range of natural languages. The algorithm uses a lexicon with part-of-speech probabilities and a feedforward neural network for rapid training. The method des [...] ...|$|R
40|$|The eggshell is {{a highly}} ordered {{structure}} resulting from the deposition of calcium carbonate and an organic matrix from the acellular uterine fluid. Characterization of the individual matrix components is necessary to determine their influence upon calcite crystal shape, size, and orientation during eggshell calcification. We have purified and sequenced a novel 32 -kDa protein, ovocalyxin- 32 (OCX- 32), which is present at high levels in the uterine fluid during the terminal phase of eggshell formation, and is localized predominantly in the outer eggshell. Database searches identified expressed sequence <b>tags</b> (ESTs) whose <b>alignment</b> yielded the complete cDNA. OCX- 32 protein possesses limited identity (32 %) to two unrelated proteins: latexin, a carboxypeptidase inhibitor expressed in rat cerebral cortex and mast cells, and to a skin protein that is encoded by a retinoic acid receptor-responsive gene, TIG 1. The timing of OCX- 32 secretion into the uterine fluid suggests that it {{may play a role}} in the termination of mineral deposition...|$|R
40|$|Because the new Proton {{platform}} from Life Technologies produced {{markedly different}} data {{from those of}} the Illumina platform, the conventional Illumina data analysis pipeline could not be used directly. We developed an optimized SNP calling method using TMAP and GATK (OTG-snpcaller). This method combined our own optimized processes, Remove Duplicates According to AS <b>Tag</b> (RDAST) and <b>Alignment</b> Optimize Structure (AOS), together with TMAP and GATK, to call SNPs from Proton data. We sequenced four sets of exomes captured by Agilent SureSelect and NimbleGen SeqCap EZ Kit, using Life Technology's Ion Proton sequencer. Then we applied OTG-snpcaller and compared our results with the results from Torrent Variants Caller. The results indicated that OTG-snpcaller can reduce both false positive and false negative rates. Moreover, we compared our results with Illumina results generated by GATK best practices, and we found that the results of these two platforms were comparable. The good performance in variant calling using GATK best practices can be primarily attributed to the high quality of the Illumina sequences...|$|R
40|$|In this paper, {{we present}} a hybrid {{approach}} to align single words, compound words and idiomatic expressions from bilingual parallel corpora. The objective is to develop, improve and maintain automatically translation lexicons. This approach combines linguistic and statistical information {{in order to improve}} word alignment results. The linguistic improvements taken into account refer to the use of an existing bilingual lexicon, named entities recognition, grammatical tags matching and detection of syntactic dependency relations between words. Statistical information refer to the number of occurrences of repeated words, their positions in the parallel corpus and their lengths in terms of number of characters. Single-word alignment uses an existing bilingual lexicon, named entities and cognates detection and grammatical <b>tags</b> matching. Compound-word <b>alignment</b> consists in establishing correspondences between the compound words of the source sentence and the compound words of the target sentences. A syntactic analysis is applied on the source and target sentences in order to extract dependency relations between words and to recognize compound words. Idiomatic expressions alignment starts with a monolingual term extractio...|$|R
40|$|This article {{describes}} a computational method to construct gene models by using evidence generated from a diverse set of sources, including those {{typical of a}} genome annotation pipeline. The program, called Combiner, takes as input a genomic sequence and the locations of gene predictions from ab initio gene finders, protein sequence <b>alignments,</b> expressed sequence <b>tag</b> and cDNA <b>alignments,</b> splice site predictions, and other evidence. Three different algorithms for combining evidence in the Combiner were implemented and tested on 1783 confirmed genes in Arabidopsis thaliana. Our results show that combining gene prediction evidence consistently outperforms even the best individual gene finder and, in some cases, can produce dramatic improvements in sensitivity and specificity. Computational identification of complete gene models in eukaryote genomes remains a challenging task (Zhang 2002). In the Arabidopsis genome project (The Arabidopsis Genome Initiative 2000), human experts integrated the output of different gene prediction programs with sequence homology data from searches of protein and transcript databases to construct the published gene models. Difficulties in creating accurate annotation arise {{for a variety of}} reasons. Sometimes the evidence for a gene is weak...|$|R
40|$|Recently, {{with the}} machine {{learning}} trend, most of the machine translation systems on over the world use two syntax tree sets of two relevant languages to learn syntactic tree transfer rules. However, for the English-Vietnamese language pair, this approach is impossible because until now {{we have not had}} a Vietnamese syntactic tree set which is correspondent to English one. Building of a very large correspondent Vietnamese syntactic tree set (thousands of trees) requires so much work and take the investment of specialists in linguistics. To take advantage from our available English-Vietnamese Corpus (EVC) which was <b>tagged</b> in word <b>alignment,</b> we choose the SITG (Stochastic Inversion Transduction Grammar) model to construct English-Vietnamese syntactic tree sets automatically. This model is used to parse two languages {{at the same time and}} then carry out the syntactic tree transfer. This English-Vietnamese bilingual syntactic tree set is the basic training data to carry out transferring automatically from English syntactic trees to Vietnamese ones by machine learning models. We tested the syntax analysis by comparing over 10, 000 sentences in the amount of 500, 000 sentences of our English-Vietnamese bilingual corpus and first stage got encouraging result (analyzed about 80 %) [5]. We have made use the TBL algorithm (Transformation Based Learning) to carry out automatic transformations from English syntactic trees to Vietnamese ones based on that parallel syntactic tree transfer set[6]. Keywords: SITG, EVC, syntactic tree transfer, bilingual corpus, TBL. 1...|$|R
40|$|Figure 65 - DNA ID tags of {{specimens}} {{that are}} over 100 years old. ID tag # 2 is shown as an example. The <b>tag</b> region sequence <b>alignment</b> {{of the three}} species: Hermeuptychia sosybius, Hermeuptychia hermybius, and Hermeuptychia intricata {{is shown in the}} middle and positions at which sequences differ are highlighted in cyan and boxed. Each of the three species differs from the other two by at least 2 nucleotides, and Hermeuptychia sosybius is different from Hermeuptychia intricata by 4 nucleotides. Forward and reverse primer regions are shaded. DNA of the tag was amplified and sequenced in both forward and reverse directions from two over- 100 -years-old specimens from the same locality (SC: Clarendon Co.). Forward and reverse sequences traces for the first specimen are shown above the reference sequences and the two traces for the second specimen are shown below. It is clear from the traces that the specimen above (13385 G 10, Fig. 36) is Hermeuptychia sosybius, (A, T, T, & C at these 4 positions, no contamination seen) and the one below (13385 G 08, Fig. 33) is Hermeuptychia intricata (G, C, C, & T at these 4 positions and equally unambiguous traces). Nucleotides that identify each specimen are indicated in large letters on yellow background and arrows point to the trace peaks revealing these nucleotides. This strategy was applied to identify 12 very old specimens of three species in a random order and yielded unambiguous identifications for 11 of them. One sample appeared to be contaminated, and the traces showed the presence of several nucleotides in many positions. All 11 DNA-based identifications agreed with genitalic identifications...|$|R
40|$|International audienceIn this paper, {{we present}} a hybrid {{approach}} to align single words, compound words and idiomatic expressions from bilingual parallel corpora. The objective is to develop, improve and maintain automatically translation lexicons. This approach combines linguistic and statistical information {{in order to improve}} word alignment results. The linguistic improvements taken into account refer to the use of an existing bilingual lexicon, named entities recognition, grammatical tags matching and detection of syntactic dependency relations between words. Statistical information refer to the number of occurrences of repeated words, their positions in the parallel corpus and their lengths in terms of number of characters. Single-word alignment uses an existing bilingual lexicon, named entities and cognates detection and grammatical <b>tags</b> matching. Compound-word <b>alignment</b> consists in establishing correspondences between the compound words of the source sentence and the compound words of the target sentences. A syntactic analysis is applied on the source and target sentences in order to extract dependency relations between words and to recognize compound words. Idiomatic expressions alignment starts with a monolingual term extraction for each of the source and target languages, which provides a list of sequences of repeated words and a list of potential translations. These sequences are represented with vectors which indicate their numbers of occurrences and the numbers of segments in which they appear. Then, the translation relation between the source and target expressions are evaluated with a distance metric. The single and compound word aligners have been evaluated on a subset of 1103 sentences in English and French of the JOC (Official Journal of the European Community) corpus. The obtained results showed that these aligners generate a translation lexicon with 90 % of precision for single words and 84 % of precision for compound words. We evaluated the idiomatic expressions aligner on a subset of the Canadian Parliament Hansard corpus and we obtained a precision of 81 %...|$|R

