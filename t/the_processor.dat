8508|10000|Public
5|$|Intel {{announced}} the official name of <b>the</b> <b>processor,</b> Itanium, on October 4, 1999.|$|E
5|$|From {{the advent}} of very-large-scale {{integration}} (VLSI) computer-chip fabrication technology in the 1970s until about 1986, speed-up in computer architecture was driven by doubling computer word size—the amount of information <b>the</b> <b>processor</b> can manipulate per cycle. Increasing the word size reduces the number of instructions <b>the</b> <b>processor</b> must execute to perform an operation on variables whose sizes are greater than {{the length of the}} word. For example, where an 8-bit processor must add two 16-bit integers, <b>the</b> <b>processor</b> must first add the 8 lower-order bits from each integer using the standard addition instruction, then add the 8 higher-order bits using an add-with-carry instruction and the carry bit from the lower order addition; thus, an 8-bit processor requires two instructions to complete a single operation, where a 16-bit processor would be able to complete the operation with a single instruction.|$|E
5|$|Godbout {{also sold}} {{components}} to Processor Technology for their 4K Static RAM board and serial / parallel interface board. Lee Felsenstein designed an Altair compatible video board that provided 16 lines of 64 {{upper and lower}} case characters on {{a black and white}} television. This $160 board became very popular and led to <b>the</b> <b>Processor</b> Technology Sol-20 Computer in 1976.|$|E
50|$|There is an inter {{processor}} link {{through which}} <b>the</b> <b>processors</b> exchange information needed for mutual coordination and verifying the 'state of health’ of the other. If {{the exchange of}} information fails, one of <b>the</b> <b>processors</b> which detect <b>the</b> same takes over the entire load including the calls that are already set up by <b>the</b> failing <b>processor.</b> However, <b>the</b> calls that were being established by <b>the</b> failing <b>processor</b> are usually lost. Sharing of resources calls for an exclusion mechanism so that both <b>the</b> <b>processors</b> do not seek the same resource at the same time. The mechanism may be implemented in software or hardware or both. Figure shows a hardware exclusion device which, when set by one of <b>the</b> <b>processors,</b> prohibits access to a particular resource by <b>the</b> other <b>processor</b> until it is reset by <b>the</b> first <b>processor.</b>|$|R
40|$|In {{this paper}} we {{consider}} the problem of reassigning tasks that were running on a failed processor of a multiprocessor system to <b>the</b> remaining fault-free <b>processors.</b> <b>The</b> new assignment of the tasks to <b>the</b> fault-free <b>processors</b> should 1) preserve the capacity constraints of <b>the</b> <b>processors,</b> 2) balance <b>the</b> load on <b>the</b> <b>processors,</b> 3) assign tasks to <b>processors</b> for which <b>the...</b>|$|R
40|$|In a {{symmetric}} multiprocessor (SMP), every processor {{has its own}} cache, and all <b>the</b> <b>processors</b> {{and memory}} modules {{are connected to the}} central interconnect, which is usually a shared bus. As <b>the</b> <b>processors</b> become faster, <b>the</b> central interconnect architecture of traditional SMP’s impedes performance because it cannot keep up with <b>the</b> <b>processors</b> capabilities. As SMP’s have hardware-enforced cache coherence, the snoop bandwidth require...|$|R
5|$|However, power {{consumption}} P by a chip {{is given by}} the equation P = C × V 2 × F, where C is the capacitance being switched per clock cycle (proportional {{to the number of}} transistors whose inputs change), V is voltage, and F is <b>the</b> <b>processor</b> frequency (cycles per second). Increases in frequency increase the amount of power used in a processor. Increasing processor {{power consumption}} led ultimately to Intel's May 8, 2004 cancellation of its Tejas and Jayhawk processors, which is generally cited as the end of frequency scaling as the dominant computer architecture paradigm.|$|E
5|$|In 1989, HP {{determined}} that Reduced Instruction Set Computing (RISC) architectures were approaching a processing limit at one instruction per cycle. HP researchers investigated a new architecture, later named Explicitly Parallel Instruction Computing (EPIC), that allows <b>the</b> <b>processor</b> to execute multiple instructions in each clock cycle. EPIC implements {{a form of}} very long instruction word (VLIW) architecture, in which a single instruction word contains multiple instructions. With EPIC, the compiler determines in advance which instructions can be executed at the same time, so the microprocessor simply executes the instructions and does not need elaborate mechanisms to determine which instructions to execute in parallel.|$|E
5|$|The Itanium bus {{interfaces}} to {{the rest}} of the system via a chipset. Enterprise server manufacturers differentiate their systems by designing and developing chipsets that interface <b>the</b> <b>processor</b> to memory, interconnections, and peripheral controllers. The chipset is the heart of the system-level architecture for each system design. Development of a chipset costs tens of millions of dollars and represents a major commitment to the use of the Itanium. IBM created a chipset in 2003, and Intel in 2002, but neither of them developed chipsets to support newer technologies such as DDR2 or PCI Express.|$|E
50|$|In load-sharing operation, an {{incoming}} call is assigned randomly or in a predetermined order {{to one of}} <b>the</b> <b>processors</b> which then handles the call right through completion. Thus, both <b>the</b> <b>processors</b> are active simultaneously and share the load and the resources dynamically. Both <b>the</b> <b>processors</b> {{have access to the}} entire exchange environment which is sensed as well as controlled by these <b>processors.</b> Since <b>the</b> calls are handled independently by <b>the</b> <b>processors,</b> they have separate memories for storing temporary call data. Although programs and semi permanent data can be shared, they are kept in separate memories for redundancy purposes.|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThis thesis developed <b>the</b> ballistics <b>processor</b> of a multiple processor airborne tactical system. <b>The</b> multiple <b>processor</b> system {{consisted of three}} INTEL- 8080 microcomputers: <b>the</b> executive <b>processor,</b> <b>the</b> navigational <b>processor</b> and <b>the</b> ballistics <b>processor.</b> <b>The</b> ballistics <b>processor</b> utilized a general second order Runge-Kutta method of integration of the equations of motion of unguided air-to-surface weapons. <b>The</b> ballistics <b>processor</b> computed sufficiently accurate and timely solutions to enable <b>the</b> executive <b>processor</b> to extrapolate an accurate release point for the weapon. The algorithm permitted complete flexibility in release conditions and allowed a complete arsenal of air-to-surface weapons currently carried on the A 7 -E aircraft. The cost of <b>the</b> ballistics <b>processor</b> using current "off the shelf" components is $ 1635 and the entire tactical system was estimated to cost $ 3891. [URL] United States Nav...|$|R
5000|$|Extended Direct Control {{allowing}} <b>the</b> <b>processors</b> in a duplex configuration {{to present}} an external interrupt to <b>the</b> other <b>processor</b> ...|$|R
5|$|The device uses a 65nm process, {{includes}} {{two to four}} cores, up to 24MB on-die caches, Hyper-Threading technology and integrated memory controllers. It implements double-device data correction, which helps to fix memory errors. Tukwila also implements Intel QuickPath Interconnect (QPI) to replace the Itanium bus-based architecture. It has a peak interprocessor bandwidth of 96GB/s and a peak memory bandwidth of 34GB/s. With QuickPath, <b>the</b> <b>processor</b> has integrated memory controllers and interfaces the memory directly, using QPI interfaces to directly connect to other processors and I/O hubs. QuickPath is also used on Intel processors using the Nehalem microarchitecture, making it probable that Tukwila and Nehalem {{will be able to}} use the same chipsets.|$|E
5|$|Because {{microcomputers}} in Japan {{were not}} powerful enough {{at the time}} to perform the complex tasks involved in designing and programming Space Invaders, Nishikado had to design his own custom hardware and development tools for the game. He created the arcade board using new microprocessors from the United States. The game uses an Intel 8080 central processing unit, features raster graphics on a CRT monitor and monaural sound hosted by a combination of analog circuitry and a Texas Instruments SN76477 sound chip. Despite the specially developed hardware, Nishikado was unable to program the game as he wanted—the Control Program board was not powerful enough to display the graphics in color or move the enemies faster—and he considered the development of the hardware {{the most difficult part of}} the whole process. While programming the game, Nishikado discovered that <b>the</b> <b>processor</b> was able to render the alien graphics faster the fewer were on screen. Rather than design the game to compensate for the speed increase, he decided to keep it as a challenging gameplay mechanism.|$|E
5|$|Computer systems {{make use}} of caches—small and fast {{memories}} located close to <b>the</b> <b>processor</b> which store temporary copies of memory values (nearby in both the physical and logical sense). Parallel computer systems have difficulties with caches that may store the same value {{in more than one}} location, with the possibility of incorrect program execution. These computers require a cache coherency system, which keeps track of cached values and strategically purges them, thus ensuring correct program execution. Bus snooping {{is one of the most}} common methods for keeping track of which values are being accessed (and thus should be purged). Designing large, high-performance cache coherence systems is a very difficult problem in computer architecture. As a result, shared memory computer architectures do not scale as well as distributed memory systems do.|$|E
40|$|Abstract Consider a {{two-stage}} manufacturing system {{composed of}} a batch processor and its upstream feeder <b>processor.</b> Jobs exit <b>the</b> feeder <b>processor</b> and join a queue in front of <b>the</b> batch <b>processor,</b> where they wait to be processed. <b>The</b> batch <b>processor</b> has a finite capacity Q, and the processing time is independent {{of the number of}} jobs loaded into <b>the</b> batch <b>processor.</b> In certain manufacturing systems (including semiconductor wafer fabrication), a processing time window exists from the time the job exits <b>the</b> feeder <b>processor</b> till <b>the</b> time it enters <b>the</b> batch <b>processor.</b> If <b>the</b> batch <b>processor</b> has not started processing a job within the job’s processing time window, the job cannot proceed without undergoing rework or validation by process engineers. We generalize this scenario by assigning a reward R for each successfully processed job by <b>the</b> feeder <b>processor,</b> and a cost C for each job that exceeds its processing time window without being processed by <b>the</b> batch <b>processor.</b> We examine a problem where <b>the</b> feeder <b>processor</b> has a deterministic processing time and <b>the</b> batch <b>processor</b> has stochastic processing time, and determine that the optimal control policy at <b>the</b> feeder <b>processor</b> is insensitive to whether <b>the</b> batch <b>processor</b> is under no-idling or full-batch policy...|$|R
5000|$|... the 64100A {{mainframe}} bus is used {{to control}} <b>the</b> emulation <b>processor</b> and to communicate between <b>the</b> mainframe <b>processor</b> and <b>the</b> emulation <b>processor.</b>|$|R
50|$|When an {{operating}} system is booted on a symmetric multiprocessing (SMP) machine, only one <b>processor,</b> <b>the</b> boot-strap <b>processor,</b> will be active. After {{the operating system}} has configured itself, it will instruct <b>the</b> other <b>processors</b> to jump {{to a piece of}} trampoline code that will initialize <b>the</b> <b>processors</b> and wait for the operating system to start scheduling threads on them.|$|R
25|$|These early {{architectures}} introduced {{parallel processing}} at <b>the</b> <b>processor</b> level, with innovations such as vector processing, in which <b>the</b> <b>processor</b> can perform several operations during one clock cycle, {{rather than having}} to wait for successive cycles.|$|E
25|$|<b>The</b> <b>processor</b> {{analyzes}} the markup and passes structured information to an application. The specification places requirements on what an XML processor must do and not do, but the application is outside its scope. <b>The</b> <b>processor</b> (as the specification calls it) {{is often referred}} to colloquially as an XML parser.|$|E
25|$|M (bits 0–4) is <b>the</b> <b>processor</b> mode bits.|$|E
5000|$|The {{work of a}} {{computation}} {{executed by}} [...] <b>processors</b> is <b>the</b> total number of primitive operations that <b>the</b> <b>processors</b> perform. Ignoring communication overhead from synchronizing <b>the</b> <b>processors,</b> this {{is equal to the}} time used to run the computation on a single processor, denoted [...]|$|R
40|$|Increasing link speeds {{have placed}} {{enormous}} {{burden on the}} processing requirements and <b>the</b> <b>processors</b> are expected {{to carry out a}} variety of tasks. Network Processors (NP) [1] [2] is the blanket name given to <b>the</b> <b>processors,</b> which are traded for flexibility and performance. Network Processors are offered by a number of vendors; to take the main burden of processing requirement of network related operations from <b>the</b> conventional <b>processors.</b> <b>The</b> Network <b>Processors</b> cover a spectrum of design tradeoff, that span in between the custom ASIC and <b>the</b> general-purpose <b>processors.</b> IXP 1200 (Intel’s network processor) is one among them. This paper focuses on deriving the analytical bounds on the optimum number of threads in IXP 1200 at 1 Gbps wire speed. 1...|$|R
5000|$|Blocking case: <b>The</b> <b>processors</b> {{assigned}} to <b>the</b> interrupted jobs are blocked and cannot execute other jobs in their queue until the jobs from <b>the</b> damaged <b>processors</b> are cleared.|$|R
25|$|An {{early and}} {{important}} application of CSP was its use for specification and verification of {{elements of the}} INMOS T9000 Transputer, a complex superscalar pipelined processor designed to support large-scale multiprocessing. CSP was employed in verifying the correctness of both <b>the</b> <b>processor</b> pipeline, and the Virtual Channel Processor which managed off-chip communications for <b>the</b> <b>processor.</b>|$|E
25|$|Using a call gate. A call gate is {{a special}} address stored by the kernel in a list in kernel memory at a {{location}} known to <b>the</b> <b>processor.</b> When <b>the</b> <b>processor</b> detects a call to that address, it instead redirects to the target location without causing an access violation. This requires hardware support, but the hardware for it is quite common.|$|E
25|$|In ARM-based machines, {{peripheral}} {{devices are}} usually attached to <b>the</b> <b>processor</b> by mapping their physical registers into ARM memory space, into the coprocessor space, or by connecting to another device (a bus) {{that in turn}} attaches to <b>the</b> <b>processor.</b> Coprocessor accesses have lower latency, so some peripherals—for example, an XScale interrupt controller—are accessible in both ways: through memory and through coprocessors.|$|E
25|$|Being {{within an}} hour's drive of Silicon Valley, Kildall {{heard about the}} first commercially {{available}} microprocessor, the Intel 4004. He bought one of <b>the</b> <b>processors</b> and began writing experimental programs for it. To learn more about <b>the</b> <b>processors,</b> he worked at Intel as a consultant on his days off.|$|R
40|$|We {{present an}} {{implementation}} of a finite-difference approximation for the solution of partial differential equations on transputer networks. The grid structure associated with the finite-difference approximation is exploited by using geometric partitioning of the data among <b>the</b> <b>processors.</b> This provides a very low degree of communication between <b>the</b> <b>processors...</b>|$|R
50|$|The mobile {{version of}} <b>the</b> Allendale <b>processor,</b> <b>the</b> Merom-2M, was also {{introduced}} in 2007, featuring 1MB of L2 cache but only 533 MT/s FSB with <b>the</b> T23xx <b>processors.</b> <b>The</b> bus clock was subsequently raised to 667 MT/s with <b>the</b> T3xxx Pentium <b>processors</b> {{that are made}} from the same dies.|$|R
25|$|IRQ mode: A {{privileged mode}} that is entered {{whenever}} <b>the</b> <b>processor</b> accepts an interrupt.|$|E
25|$|Elimination of <b>the</b> <b>processor</b> dynamic {{throttling}} policies used in Windows XP and Windows Server 2003.|$|E
25|$|FIQ mode: A {{privileged mode}} that is entered {{whenever}} <b>the</b> <b>processor</b> accepts a Fast interrupt request.|$|E
40|$|This chapter {{presents}} {{the design and}} analysis of variable-precision, interval arithmetic <b>processors.</b> <b>The</b> <b>processors</b> give <b>the</b> user the ability to specify the precision of the computation, determine {{the accuracy of the}} results, and recompute inaccurate results with higher precision. <b>The</b> <b>processors</b> support a wide variety of arithmetic operations on variable-precision floating point numbers and intervals. Efficient hardware algorithms and specially designed functional units increase the speed, accuracy, and reliability of numerical computations. Area and delay estimates indicate that <b>the</b> <b>processors</b> can be implemented with areas and cycle times that are comparable to conventional IEEE double-precision floating point coprocessors. Execution time estimates indicate that <b>the</b> <b>processors</b> are two to three orders of magnitude faster than a conventional software package for variable-precision, interval arithmetic. 1. 1 INTRODUCTION Floating point arithmetic provides a high-speed method for perform [...] ...|$|R
40|$|Abstract. The paper {{presents}} some hardware {{solutions for}} the bit-byte CPU of a PLC, which are oriented for maximum optimisation of data exchange between <b>the</b> CPU <b>processors.</b> <b>The</b> optimisation intends to maximum utilisation of the possibilities given by the two-processor architecture of the CPUs. The key point is preserving high speed of instruction processing by <b>the</b> bit <b>processor,</b> and high functionality of <b>the</b> byte <b>processor.</b> <b>The</b> optimal structure should enable <b>the</b> <b>processors</b> to work concurrently for {{as much of the}} tome as possible, and minimise the situations, when one processor has to wait for the other...|$|R
40|$|A {{processor}} farm is a {{distributed system}} {{that consists of}} a unique master processor together {{with a number of}} identical slave <b>processors.</b> <b>The</b> master <b>processor</b> interacts with some environment that generates tasks to be solved by <b>the</b> slave <b>processors.</b> <b>The</b> <b>processors</b> are connected via some communication network that takes care of the communication of the tasks between the master and the slaves. We show ho...|$|R
