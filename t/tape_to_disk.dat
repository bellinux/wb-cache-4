7|10000|Public
50|$|More than 500 Gold & Platinum {{recordings}} were mastered at Artisan. Its engineering {{staff was}} known for meticulous attention to detail in the <b>tape</b> <b>to</b> <b>disk</b> transfer. Later {{was known for}} recording and mixing many Hits and great records.Artisan's engineering staff included Bob MacLeod, John Golden, Kevin Gray, Jo Hansch, Jon Lowry, and Aaron Connor.|$|E
50|$|There {{are several}} reasons that IBM {{provided}} a system generation process rather than simply providing a mechanism to restore the system from <b>tape</b> <b>to</b> <b>disk.</b> System/360 did not have self-identifying I/O devices, and the customer could request installation of I/O devices at arbitrary addresses. As a result, IBM had to provide a mechanism for the cutomer to define the I/O configuration to OS/360. Also, OS/360 supported several different options; IBM needed {{a way for the}} customer to select the code appropriate for the options needed at a particular installation.|$|E
40|$|International Telemetering Conference Proceedings / October 21, 2002 / Town & Country Hotel and Conference Center, San Diego, CaliforniaThe role of {{recorders}} in telemetry applications {{has undergone}} many changes throughout the years. We’ve seen the evolution from multi-track <b>tape</b> <b>to</b> <b>disk</b> to solid state technologies, both for airborne and ground based equipment. Data acquisition and collection system design has changed {{as well and}} a recent trend in airborne is to merge acquisition and recording. On the ground, increased decentralization of data collection and processing has generated the requirement to provide backup storage to protect against communication circuit outages. This paper explores the trend to adopt network based data acquisition, collection, and distribution systems for telemetry applications and the impact on recording techniques and equipment. It shows that in this emerging approach the recorder returns to its root mission of attempting to provide the fastest, largest capacity for {{the least amount of}} investment. In a network based architecture the recorder need only accept and reproduce data operating independently from the acquisition process...|$|E
50|$|The {{software}} {{was written in}} a proprietary version of the programming language JOVIAL termed JSS JOVIAL. The system was updated over time <b>to</b> change <b>tape</b> drives <b>to</b> <b>disk</b> cartridges and single-line printers to multi-line printers. The memory in the H5118ME was expanded at least twice to the system maximum of 512,000 18-bit words.|$|R
50|$|However, {{if one has}} {{a working}} system, it is far easier to dump the ROM data <b>to</b> <b>tape,</b> <b>disk,</b> etc. and {{transfer}} the data file to one's target machine.|$|R
40|$|Video-on-Demand (VOD) servers are {{becoming}} feasible. These servers have voluminous data to store and manage. A tape-based tertiary storage system {{seems to be}} a reasonable solution to lowering the cost of storage and management of this continuous data. In this paper we address the issues of decomposing and placing continuous data blocks on tapes, and the scheduling of multiple requests for materializing objects from <b>tapes</b> <b>to</b> <b>disks.</b> We first study different policies for continuous object decomposition and blocks placement on tapes under different characteristics of the tertiary storage drives. Afterwards, we propose a scheduling algorithm for object materialization. 1 Introduction Most multimedia data is currently stored on magnetic hard disks, yet the proliferation of such applications is generating massive amounts of data such that a storage subsystem based solely on hard disks will be too expensive to support and manage. The video-on-demand (VOD) server is one particular multimedia [...] ...|$|R
40|$|The Collider Detector at Fermilab (CDF) {{experiment}} {{records and}} analyses proton-antiproton interactions at a center-of-mass energy of 2 TeV. Run II of the Fermilab Tevatron started in April of this year. The {{duration of the}} run {{is expected to be}} over two years. One of the main data handling strategies of CDF for Run II is to hide all tape access from the user and to facilitate sharing of data and thus disk space. A disk inventory manager was designed and developed over the past years {{to keep track of the}} data on disk, to coordinate user access to the data, and to stage data back from <b>tape</b> <b>to</b> <b>disk</b> as needed. The CDF Run II disk inventory manager consists of a server process, a user and administrator command line interfaces, and a library with the routines of the client API. Data are managed in filesets which are groups of one or more files. The system keeps track of user access to the filesets and attempts to keep frequently accessed data on disk. Data that are not on disk are automatically staged back from tape as needed. For CDF the main staging method is based on the mt-tools package as tapes are written according to the ANSI standard...|$|E
40|$|The central Andes of South America {{extend from}} {{approximately}} 14 deg. S to 28 deg. S as an unbroken chain of mountains and volcanoes over 2000 km long. It {{is here that}} the Nazca plate dives under the South American plate at angles varying from 10 deg to 30 deg. Very {{little is known about}} the volcanoes comprising this classic, subduction-type plate margin. A catalogue of the volcanoes in the central Andes is being prepared by Dr. P. W. Francis and Dr. C. A. Wood at the NASA Lunar and Planetary Institute. At present, more than 800 volcanoes of Cenozoic age have been recognized in the chain, with an estimated 75 - 80 major, active Quarternary volcanoes. Approximately one hundred 1536 x 1536 pixel color composite Optronics positives were produced from six full LANDSAT Thermatic Mapper scenes and three partial TM scenes. These positives cover a large portion of the central Andes. The positives were produced from LANDSAT data using the VAX imaging package, LIPS. The scenes were first transferred from magnetic <b>tape</b> <b>to</b> <b>disk.</b> The LIPS package was then used to select volcanically interesting areas which were then electronically enhanced. Finally, the selected areas were transferred back to tape and printed on the Optronics equipment. The pictures are color composites using LANDSAT TM bands 7, 4, and 2 in the red, green, and blue filters, respectively...|$|E
40|$|One {{of the big}} {{challenges}} in Grid computing is storage management and access. Several solutions exist to store data in a persistent way. In this work we describe our contribution within the Worldwide LHC Computing Grid project. Substantial samples of data produced by the High Energy Physics detectors at CERN are shipped for initial processing to specific large computing centers worldwide. Such centers are normally able to provide persistent storage for tens of Petabytes of data mostly on tapes. Special physics applications are used to refine and filter the data after spooling the required files from <b>tape</b> <b>to</b> <b>disk.</b> At smaller geographically dispersed centers, physicists perform the analysis of such data stored on disk-only caches. In this thesis we analyze the application requirements such as uniform storage management, quality of storage, POSIX-like file access, performance, etc. Furthermore, security, policy enforcement, monitoring, and accounting {{need to be addressed}} carefully in a Grid environment. We then make a survey of the multitude of storage products deployed in the WLCG infrastructure, both hardware and software. We outline the specific features, functionalities and diverse interfaces offered to users. Among the other storage services, we describe StoRM, a storage resource manager that we have designed and developed to provide an answer to specific user request for a fast and efficient Grid interface to available paral lel file systems. We propose a model for the Storage Resource Management protocol for uniform storage management and access in the Grid. The black box testing methodology has been applied in order to verify the completeness of the specifications and validate the existent implementations. We finally describe and report on the results obtained...|$|E
40|$|Video-on-Demand (VOD) servers are {{becoming}} feasible. These servers have voluminous data to store and manage. If only disk-based secondary storage systems {{are used to}} store and manage this huge amount of data the system cost would be extensively high. A tape-based tertiary storage system {{seems to be a}} reasonable solution to lowering the cost of storage and management of this continuous data. However, the usage of a tertiary storage system to store large continuous data introduces several issues. These are mainly the replacement policy on disks, the decomposition and the placement of continuous data chunks on tapes, and the scheduling of multiple requests for materializing objects from <b>tapes</b> <b>to</b> <b>disks.</b> In this paper we address these issues and we propose solutions based on some heuristics we experimented in a simulator. We first extend a replacement policy that has been proposed for a single user environment to a multi-user one with several servicing streams. We then study different polic [...] ...|$|R
50|$|The {{application}} of IT {{to the tree}} components of the management control loop evolved over time as new technologies were developed. Recording of operational transactions {{was one of the}} first needs to be automated through the use of 80 column punch cards. As electronics progressed, the records were moved, first <b>to</b> magnetic <b>tape,</b> then <b>to</b> <b>disk.</b> Software technology progressed as well and gave rise to database management systems that centralized the access and control of the data.|$|R
5000|$|Further, it is {{becoming}} more and more common for people to talk about [...] "filming" [...] with a camcorder even though no [...] "film" [...] is involved. Similarly, the term [...] "taping" [...] is often used (for lack of a better term) though no tape (or film) is involved, where live video is recorded directly <b>to</b> video <b>tape,</b> a direct <b>to</b> <b>disk</b> recording using a hard disk recorder, or a tapeless camcorder using flash media.|$|R
40|$|Objectivity federated {{databases}} {{may contain}} many terabytes {{of data and}} span thousands of files. In such an environment, it is often easy for a user to pose a query that may return an iterator over millions of objects, requiring opening thousands of databases. This presentation describes several technologies developed for such settings: (1) a query estimator, which tells the user how many objects satisfy the query, and how many databases will be touched, prior to opening all of those files; (2) an order-optimized iterator, which behaves like an ordinary iterator except that elements are returned in an order optimized for efficient access, presorted by the database (and container) in which they reside; (3) a parallel implementation of the order-optimized iterator, allowing any number of processes in a parallel or distributed system to iterate over disjoint subcollections of terms satisfying the query, partitioned by the database or container in which the items reside. These technologies {{have been developed for}} scientific experiments that will require handling thousands of terabytes of data annually, but they are intended to be applicable in other massive data settings as well. In such environments, significant amounts of data will reside on tertiary storage, accessible via Objectivity`s recently-announced HPSS (High Performance Storage System) interface. When deployed in large-scale physics settings later in 1998, the query estimator will further inform the user of the number of tape mounts required to satisfy the query, and provide rough time estimates for data delivery. The order-optimized iterator will be connected to a cache manager that will prefetch from <b>tape</b> <b>to</b> <b>disk</b> the files needed by the query (known from the query estimation step), and will decide which items to deliver to the user next according to the order in which data become available in the disk cache...|$|E
50|$|The later VT100 {{terminal}} {{implemented the}} more sophisticated ANSI escape sequences standard (now ECMA-48) for functions such as controlling cursor movement, character set, and display enhancements. The Hewlett Packard HP 2640 series had perhaps the most elaborate escape sequences for block and character modes, programming keys and their soft labels, graphics vectors, and even saving data <b>to</b> <b>tape</b> or <b>disk</b> files.|$|R
5000|$|In the July 1983 {{issue of}} Compute!'s Gazette the Exatron Stringy Floppy for the Commodore VIC-20 and the 64 was reviewed. Calling the {{peripheral}} [...] "a viable alternative" [...] <b>to</b> <b>tape</b> or <b>disk,</b> the magazine noted that [...] "under ideal conditions, a Stringy Floppy can outperform a VIC-1540/1541 disk drive". Texas Instruments licensed the Stringy Floppy as the Waferdrive for its TI 99/2 and CC-40 computers.|$|R
40|$|Angiographic {{experiments}} {{were performed on}} isolated canine left ventricle preparations using donor dog to supply blood to the coronary circulation via a rotary pump to control coronary flow. The angiographic record was transferred from video <b>tape</b> <b>to</b> video <b>disk</b> for detailed uninterrupted sequential analysis at a frequency of 60 fields/sec. It is shown {{that the use of}} a biplane X-ray technique and a metabolically supported isolated canine left ventricle preparation provides an angiographically ideal means of measuring the mechanical dynamics of the myocardium while the intact left ventricular myocardial structure and electrical activation pattern retain most of the in situ ventricular characteristics. In particular, biplane X-ray angiography of the left ventricle can provide estimates of total ventricular function such as ejection fraction, stroke volume, and myocardial mass correct to within 15 % under the angiographically ideal conditions of the preparation...|$|R
5000|$|Lasso Logic was {{the first}} company to provide a disk-based {{continuous}} data protection appliance, called Lasso CDP, which continually backed up servers, applications, PCs, and laptops to itself in real time. This allowed users who lost files {{to go back to}} various useful points in time. More recent backups were saved at more frequent intervals, but as backup versions receded in time, some of those versions were dropped by way of a complex algorithm which preserved theoretically more useful versions. Thus a user could always find a version to return to from the inception of backup, from minutes ago to years ago. Before Lasso Logic, the only backup options were single point in time (SPIT) snapshot software that allowed IT administrators to back up <b>to</b> <b>tape</b> or <b>to</b> <b>disk,</b> usually at a resolution of once a day. The value proposition of the company was [...] "set it and forget it" [...] backup, thus eliminating the labor burden of tape-based backup.|$|R
5000|$|A {{business}} {{may choose}} to rationalize the physical media {{to take advantage of}} more efficient storage technologies. This will result in having to move physical blocks of data from one <b>tape</b> or <b>disk</b> <b>to</b> another, often using virtualization techniques. The data format and content itself will not usually be changed in the process and can normally be achieved with minimal or no impact to the layers above.Hence called Storage migration ...|$|R
40|$|The Phase 2 HPCx {{system will}} have 100 Tb of storage space, of which around 70 Tb {{comprises}} offline tape storage rather than disk. This represents {{a doubling of}} the storage available on Phase 1. Users who create {{a large amount of}} data can benefit significantly from archiving data <b>to</b> <b>tape</b> <b>to</b> free up <b>disk</b> space. This guide gives a quick overview of the tape storage archive system on HPCx and how to use it. If you have any comments or questions then please feel free to send them in to the HPCx helpdesk. i Content...|$|R
5000|$|The Tower of Hanoi {{rotation}} {{method is}} more complex. It {{is based on}} the mathematics of the Tower of Hanoi puzzle, using a recursive method to optimize the back-up cycle. Every <b>tape</b> corresponds <b>to</b> a <b>disk</b> in the puzzle, and every <b>disk</b> movement <b>to</b> a different peg corresponds with a backup <b>to</b> that <b>tape.</b> So the first tape is used every other day (1, 3, 5, 7, 9, ...), the second tape is used every fourth day (2, 6, 10, ...), the third tape is used every eighth day (4, 12, 20, ...).|$|R
50|$|In {{spite of}} these disadvantages, there are several use cases where LTFS-formatted <b>tape</b> is {{superior}} <b>to</b> <b>disk</b> and other data storage technologies. While LTO seek times can range from 10 to 100 seconds, the streaming data transfer rate can match or exceed disk data transfer rates. Additionally, LTO cartridges are easily transportable and hold far more data than any other removable data storage format. The ability to copy a large file or a large selection of files (up to 1.5 TB for LTO-5 or 2.5 TB for LTO-6) <b>to</b> an LTFS-formatted <b>tape,</b> allows easy exchange of data to a collaborator or saving of an archival copy.|$|R
50|$|The DCC Studio {{application}}, however, was {{a useful}} application {{that makes it}} possible to copy audio from <b>tape</b> <b>to</b> hard <b>disk</b> and vice versa, regardless of the SCMS status of the tape. This makes it possible to circumvent SCMS with DCC Studio. The program also allows users to manipulate the PASC audio files that were recorded <b>to</b> hard <b>disk</b> in various ways: they can change equalization settings, cut/copy and paste track fragments, and place and move audio markers and name those audio markers from the PC keyboard. It is possible to record a mix tape by selecting the desired tracks from a list, and moving the tracks around in a playlist. Then the user can click on the record button to copy the entire playlist back <b>to</b> DCC <b>tape,</b> while simultaneously recording markers (such as reverse and end-of-tape) and track titles. It is not necessary to record the track titles and tape markers separately (as you would do with a stationary recorder), and thanks to the use of a PC keyboard, it is possible to use characters in song titles that are not available when using a stationary machine's remote control.|$|R
5000|$|The game {{originally}} shipped on cassette, {{and required}} the users to {{type in the}} scenario and save it to a separate data tape before playing. The pre-rolled missions were outlined in a separate [...] "Battle Manual", which also included short stories introducing the game world and the individual missions. This process was greatly improved on the diskette versions, which had the games saved out as data files that could be loaded up by name. Users could also create their own scenarios using the separate [...] "" [...] program, saving them <b>to</b> <b>tape</b> or <b>disk.</b>|$|R
50|$|Almost universally, home {{computers}} had a BASIC interpreter {{combined with a}} line editor in permanent read-only memory which one could use to type in BASIC programs and execute them immediately or save them <b>to</b> <b>tape</b> or <b>disk.</b> In direct mode, the BASIC interpreter was also used as the user interface, and given tasks such as loading, saving, managing, and running files. One exception was the Jupiter Ace, which had a Forth interpreter instead of BASIC. A built-in programming language {{was seen as a}} requirement for any computer of the era, and was the main feature setting {{home computers}} apart from video game consoles.|$|R
5000|$|Plug-In {{software}} cartridges so {{the computer}} user could immediately begin using the computer at power-on. The user {{would not have}} to load a program from <b>tape</b> or <b>disk</b> <b>to</b> start operating the computer. Exidy would provide three program cartridges under license—Microsoft 8K BASIC, Word Processor Cartridge (which was the “Killer App“ for PCs at the time), and an Assembler Cartridge (for programmers to write their own custom software for proprietary applications). Blank cartridges were provided for custom applications and the most popular application was customer generated foreign language character sets, which made the Exidy Sorcerer the most popular international PC [...]|$|R
50|$|Keypunches {{and punched}} cards were still {{commonly}} used for both data and program entry through the 1970s but were rapidly made obsolete {{by changes in}} the entry paradigm and by the availability of inexpensive CRT computer terminals. Eliminating the step of transferring punched cards <b>to</b> <b>tape</b> or <b>disk</b> (with {{the added benefit of}} saving the cost of the cards themselves) allowed for improved checking and correction during the entry process. The development of video display terminals, interactive timeshared systems and, later, personal computers allowed those who originated the data or program to enter it directly instead of writing it on forms to be entered by keypunch operators.|$|R
40|$|In massive scale video {{streaming}} server, {{it is necessary}} to update the contents of the disk storage in regular fashion, e. g. daily, or weekly. This is particularly because the prohibitively huge amount of data maintained in the massive scale server cannot be stored solely on the secondary storage. Operation of retrieving the data blocks from the disk for streaming purpose bears tight timing constraints. Each video stream is assigned a certain amount of main memory buffer to synchronize the asynchronous disk retrieval operation and synchronous playback operation. The disk write operation for content update may harm the QoS of the on-ging stream. Given that the temporal unavailability of the streaming service is not acceptable, we investigate the impact of writing the data blocks from the <b>tape</b> drive <b>to</b> the <b>disk</b> over on-going video playbacks. We propose to dedicate additional amount of memory buffer to individual stream to mitigate the interference of non-real-time disk write oper [...] ...|$|R
50|$|George 2 {{added the}} concept of spooling. Jobs and input data were read in from cards or paper <b>tape</b> <b>to</b> an input well on disk or tape. The jobs were then run, writing output <b>to</b> <b>disk</b> or <b>tape</b> spool files, which were then written to the output peripherals. The input/processing/output stages were run in parallel, {{increasing}} machine utilisation. On larger machines {{it was possible to}} run multiple jobs simultaneously.|$|R
50|$|IBM Spectrum Archive {{enables you}} to {{automatically}} move infrequently accessed data from <b>disk</b> <b>to</b> <b>tape</b> <b>to</b> lower costs while retaining ease of use and {{without the need for}} proprietary tape applications.|$|R
5000|$|Staging : Sometimes backup {{jobs are}} copied <b>to</b> a staging <b>disk</b> before being copied <b>to</b> <b>tape.</b> This process is {{sometimes}} referred to as D2D2T, an acronym for <b>Disk</b> <b>to</b> <b>Disk</b> <b>to</b> <b>Tape.</b> This can be useful if there is a problem matching the speed of the final destination device with the source device as is frequently faced in network-based backup systems. It can also serve as a centralized location for applying other data manipulation techniques.|$|R
40|$|As {{processor}} {{cycle times}} decrease, memory system performance becomes ever more critical to overall performance. Continually changing technology and workloads create {{a moving target}} for computer architects {{in their effort to}} design cost-effective memory systems. Meeting the demands of ever changing workloads and technology requires the following: Efficient techniques for evaluating memory system performance, Tuning programs to better use the memory system, and New memory system designs. This thesis makes contributions in each of these areas. Hardware and software developers rely on simulation to evaluate new ideas. In this thesis, I present a new interface for writing memory system simulators—the active memory abstraction—designed specifically for simulators that process memory references as the application executes and avoids storing them <b>to</b> <b>tape</b> or <b>disk.</b> Active memory allows simulators to optimize for the common case, e. g., cache hits, achieving simulation times only 2 - 6 times slower than the original un-instrumented application. The efficiency of the active memory abstraction can be used by software designers t...|$|R
40|$|Brookhaven National Laboratory (BNL) Experiment 787 's second {{generation}} Unix-based data aquisition system {{is comprised of}} several independent programs, each of which controls a specific aspect of the experiment. These programs include packages for reading events from the hardware systems, analyzing and reducing the data, distributing the results to various data consumers, and logging the data <b>to</b> <b>tape</b> or <b>disk.</b> Most of these can be run in stand-alone mode, for ease of development and testing. There are also a number of daemon processes for writing special data records to the data streams, and several monitor programs for evaluating and controlling {{the progress of the}} whole. Coordination of these processes is achieved through a combination of pipes, signals, shared memory, and FIFOs, overseen by the user through a Motif graphical user interface. The system runs on a Silicon Graphics 4 D/ 320, interfaced to a Fastbus system through the BNL Fastbus/VME interface (BBFC), and runs under Irix an [...] ...|$|R
40|$|This paper {{describes}} how federated and object/relational database systems can exploit cost-effective active storage hierarchies. By active storage hierarchy we mean a database system that uses all storage media (i. e. optical, <b>tape,</b> and <b>disk)</b> <b>to</b> store and retrieve data {{and not just}} disk. A detailed discussion of the Atomic Data Store data warehouse concept {{can be found in}} [CB 99]. These also describe a commercial relational database product, StorHouse/Relational Manager (RM), that executes SQL queries directly against data stored in a complete storage hierarchy. This paper focuses on applications that can use, and may even require the use of, emerging federated and object/relational database technologies. Our analysis is based on two products now in development. We will refer to these as StorHouse/Fed (a federated database system that includes StorHouse/RM) and StorHouse/ORM (an Object-Relational database system). We conclude by describing candidate applications (with an emphasis on the federal sector) that can exploit the combination of costeffective active storage hierarchy with federated and/or object/relational database technology...|$|R
50|$|The video source {{could be}} from: a telecine, a video tape {{recorder}} (VTR), a motion picture film scanner, virtual telecine or a Direct <b>to</b> <b>Disk</b> Recording (DDR) or the older system called a film chain. A high end broadcast color suite may use a Da Vinci Systems or Pandora Int.'s color corrector. If a VTR is the source for the video the room is often called a <b>tape</b> <b>to</b> <b>tape</b> suite. Many suites are designed to operate as a telecine suite or a <b>tape</b> <b>to</b> <b>tape</b> suite by changing {{the configuration of the}} suite. The operator of the suite is usually called a Colorist. If a telecine is the source this is called a Film <b>to</b> <b>Tape</b> operation. A color suite may use one video standard or be able to change configuration to a number of standards like: high-definition video, NTSC, or PAL or a DI workflow. Color suites are sometime placed in digital cinema movie theaters with a video projector for color correction to that display format.|$|R
50|$|Linear Tape File System (LTFS) allows files {{stored on}} {{magnetic}} <b>tape</b> <b>to</b> be accessed {{in a similar}} fashion <b>to</b> those on <b>disk</b> or removable flash drives. It requires both a specific format of data on the tape media and software to provide a file system interface to the data.|$|R
40|$|This paper {{describes}} how database systems can use and exploit a cost-effective active storage hierarchy. By active storage hierarchy we mean a database system that uses all storage media (i. e. optical, <b>tape,</b> and <b>disk)</b> <b>to</b> store and retrieve data {{and not just}} disk. We describe and emphasize the active part, whereby all storage types are used to store raw data that is converted to strategic business information. We describe an evolution to the Data Warehouse concept, called Atomic Data Store, whereby atomic data is stored in the database system. Atomic data is defined as storing all the historic data values and executing queries against the historic queries. We also describe a Data Warehouse information collection, flow and central data store Hub-and-Spoke architecture, used to feed data into Data Marts. We also describe a commercial product; StorHouse/Relational Manager (RM). RM is a commercial relational database system that executes SQL queries directly against data stored on the storage hierarchy (i. e. tape, optical, disk). We conclude with {{a brief overview of}} a real world AT&T Call Detail Warehouse (CDW) case study...|$|R
