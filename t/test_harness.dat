130|73|Public
50|$|A <b>test</b> <b>harness</b> {{may be part}} of {{a project}} deliverable. It is kept {{separate}} from the application source code and may be reused on multiple projects. A <b>test</b> <b>harness</b> simulates application functionality; it has no knowledge of test suites, test cases or test reports. Those things are provided by a testing framework and associated automated testing tools.|$|E
5000|$|Test harness: The software, tools, {{samples of}} data input and output, and {{configurations}} are all referred to collectively as a <b>test</b> <b>harness.</b>|$|E
50|$|A <b>test</b> <b>harness</b> {{should allow}} {{specific}} tests to run (this helps in optimizing), orchestrate a runtime environment, {{and provide a}} capability to analyse results.|$|E
5000|$|An {{automated}} <b>testing</b> <b>harness</b> {{to connect}} to all the leading testing tools ...|$|R
5000|$|Lightweight {{software}} test automation is {{the process}} of creating and using relatively short and simple computer programs, called lightweight <b>test</b> <b>harnesses,</b> designed to <b>test</b> a software system. Lightweight <b>test</b> automation <b>harnesses</b> are not tied to a particular programming language but are most often implemented with the Java, Perl, Visual Basic [...]NET, and C# programming languages. Lightweight <b>test</b> automation <b>harnesses</b> are generally four pages of source code or less, and are generally written in four hours or less. Lightweight test automation is often associated with Agile software development methodology.|$|R
5000|$|The test specification. This {{specifies}} all {{the tests}} {{that are to}} be performed and what results are to be checked for. Often tests are run within automated <b>test</b> <b>harnesses</b> and the <b>tests</b> specified either within code files or script files.|$|R
50|$|The <b>test</b> <b>harness</b> will {{generally}} be specific to a development environment such as Java. However, interoperability test harnesses {{have been developed}} for use in more complex systems.|$|E
5000|$|Industry analysts {{report that}} service {{virtualization}} is {{best suited for}} [...] "IT shops with significant experience with 'skipping' integration testing due to 'dependent software', and with a reasonably sophisticated <b>test</b> <b>harness.</b>|$|E
5000|$|Tape is a tap-producing <b>test</b> <b>harness</b> for node and browsers {{requiring}} few APIs It can be {{used for}} unit and integration testing. Tests are code. So, you can run tests as modules with node ...|$|E
50|$|Ideally, {{each test}} case is {{independent}} from the others. Substitutes such as method stubs, mock objects, fakes, and <b>test</b> <b>harnesses</b> {{can be used}} to assist testing a module in isolation. Unit tests are typically written and run by software developers to ensure that code meets its design and behaves as intended.|$|R
40|$|TutorialInternational audienceThe {{second part}} of the {{presentation}} will focus on Evolutionary Fuzzing, in which the fuzzing process is driven by a Genetic Algorithm (GA). GA is itself guided by an heuristic that is fault and <b>test</b> <b>harnessing</b> dependent: the fitness function. It will list examples of such functions, some recent advances and directions for future work in Evolutionary Fuzzing...|$|R
50|$|SQLite uses {{automated}} {{regression testing}} prior to each release. Over 2 million tests are run {{as part of}} a release's verification. Starting with the August 10, 2009 release of SQLite 3.6.17, SQLite releases have 100% branch test coverage, one of the components of code coverage. The <b>tests</b> and <b>test</b> <b>harnesses</b> are partially public domain and partially proprietary.|$|R
50|$|For example, when {{attempting}} to build an application that needs to interface with an application on a mainframe computer, but no mainframe is available during development, a <b>test</b> <b>harness</b> may be built {{to use as a}} substitute.|$|E
50|$|Test {{harnesses}} {{allow for}} the automation of tests. They can call functions with supplied parameters and print out and compare the results to the desired value. The <b>test</b> <b>harness</b> is a hook to the developed code, which can be tested using an automation framework.|$|E
5000|$|TAP {{was created}} for {{the first version of}} the Perl {{programming}} language (released in 1987), as part of the Perl's core <b>test</b> <b>harness</b> (...) [...] The [...] module was written by Tim Bunce and Andreas König to allow Perl module authors to take advantage of TAP.|$|E
5000|$|The PLI (now VPI) enables Verilog to {{cooperate}} with other programs written in the C language such as <b>test</b> <b>harnesses,</b> instruction set simulators of a microcontroller, debuggers, and so on. For example, it provides the C functions [...] and [...] which are used to write and read the argument of the current Verilog task or function, respectively.|$|R
40|$|Abstract—Software {{drivers are}} usually {{developed}} after hardware devices become available. This dependency can induce a long product cycle. Although co-simulation and co-verification techniques have been utilized {{to facilitate the}} driver development, Hardware/Software (HW/SW) interface models, as the <b>test</b> <b>harnesses,</b> are often challenging to specify. Such interface models should have formal semantics, be efficient for testing, and cover all HW/SW behaviors described by HW/SW interface protocols. We present an approach to formalizing HW/SW interface specifications, where we propose a semantic model, relative atomicity, to capture the concurrency model in HW/SW interfaces; demonstrate our approach via a realistic example; elaborate on how we have utilized this approach in device/driver development process; and discuss criteria for evaluating our formal specifications. We have detected fifteen issues in four English specifications. Furthermore, our formal specifications are readily useful as the <b>test</b> <b>harnesses</b> for co-verification, which has discovered twelve real bugs in five industrial driver programs. I...|$|R
40|$|International audienceThe Next Generation LearnLib (NGLL) is a {{framework}} for model-based construction of dedicated learning solutions {{on the basis of}} extensible component libraries, which comprise various methods and tools to deal with realistic systems including <b>test</b> <b>harnesses,</b> reset mechanisms and abstraction/refinement techniques. Its construction style allows application experts to control, adapt, and evaluate complex learning processes with minimal programming expertise...|$|R
5000|$|... #Caption: An {{example of}} a model-based testing {{workflow}} (offline test case generation). IXIT refers to implementation extra information and refers to information needed to convert an abstract test suite into an executable one. Typically, IXIT contains information on the <b>test</b> <b>harness,</b> data mappings and SUT configuration.|$|E
50|$|In {{software}} testing, a <b>test</b> <b>harness</b> or {{automated test}} framework {{is a collection}} of software and test data configured to test a program unit by running it under varying conditions and monitoring its behavior and outputs. It has two main parts: the test execution engine and the test script repository.|$|E
50|$|Interface {{engines are}} built on top of Interface Environment. Interface engine {{consists}} of a parser and a test runner. The parser is present to parse the object files coming from the object repository into the test specific scripting language. The test runner executes the test scripts using a <b>test</b> <b>harness.</b>|$|E
40|$|A {{primary goal}} of {{generative}} programming and model-driven development is to {{raise the level of}} abstraction at which designers and developers interact with the software systems they are building. During initial development, the benefits of abstraction are clear. However, during testing and maintenance, increased distance from the implementation can be a disadvantage. We view test cases and <b>test</b> <b>harnesses</b> as an essential bridge between the high-level specifications and the implementation. As such, the generation of test cases for fully generated components and <b>test</b> <b>harnesses</b> for partially generated components is of fundamental importance to model-driven systems. In this paper we present our experience with test-case and test-harness generation in a model-driven, component-based distributed system. We describe our development tool, MODEST, and motivate our decision to invest the extra e#ort needed to generate test code. Additionally, we describe our approach to test-case and test-harness generation and provide a quantification of the relative e#ort of generating test artifacts versus application artifacts...|$|R
40|$|We {{will use}} a {{combination}} of two tools to test Java programs: – junit is a unit testing tool – jcoverage is a statement/branch coverage tool • Both tools are available for free from the WWW. • jcoverage has its own testing tool (jtestrun) but we will use junit as a <b>testing</b> <b>harness.</b> – junit is included in the jcoverage distribution. Software Engineering (Unit Testing Tools) Downloading the Softwar...|$|R
40|$|A {{primary goal}} of {{generative}} programming and model-driven development is to {{raise the level of}} abstraction at which designers and developers interact with the software systems they are building. During initial development, the benefits of abstraction are clear. However, during testing and maintenance, increased distance from the implementation can be a disadvantage. We view test cases and <b>test</b> <b>harnesses</b> as an essential bridge between the high-level specifications and the implementation...|$|R
50|$|Associated {{with the}} framework, but not {{strictly}} part of it, {{is the concept}} of FuseDocs which is a semi-formalized form of documentation written in XML that specifies the inputs and outputs of each fuse file. There are third-party tools available which can use FuseDocs to do things like generate <b>test</b> <b>harness</b> code.|$|E
50|$|An {{alternative}} {{definition of}} a <b>test</b> <b>harness</b> is software constructed to facilitate integration testing. Where test stubs are typically components of the application under development and are replaced by working components as the application is developed (top-down integration testing), test harnesses are external to the application being tested and simulate services or functionality not available in a test environment.|$|E
5000|$|SubUnit is a {{streaming}} {{protocol for}} test results. It allows communication between unit tests and a <b>test</b> <b>harness.</b> Originally developed for unit testing in 2005 by Robert Collins. Subunit comes with command line filters to process a SubUnit stream and language bindings for Python, C, C++ and Shell. Bindings {{are easy to}} write for other languages.|$|E
40|$|Multithreaded {{software}} is typically built with specialized concurrent objects like atomic integers, queues, and maps. These objects' methods {{are designed to}} behave according to certain consistency criteria like atomicity, despite being optimized to avoid blocking and exploit parallelism, e. g., by using atomic machine instructions like compare and exchange (cmpxchg). Exposing atomicity violations is important since they generally lead to elusive bugs {{that are difficult to}} identify, reproduce, and ultimately repair. In this work we expose atomicity violations in concurrent object implementations from the most widely-used software development kit: The Java Development Kit (JDK). We witness atomicity violations via simple <b>test</b> <b>harnesses</b> containing few concurrent method invocations. While stress testing is effective at exposing violations given catalytic <b>test</b> <b>harnesses</b> and lightweight means of falsifying atomicity, divining effectual catalysts can be difficult, and atomicity checks are generally cumbersome. We overcome these problems by automating test-harness search, and establishing atomicity via membership in precomputed sets of acceptable return-value outcomes. Our approach enables testing millions of executions of each harness each second (per processor core). This scale is important since atomicity violations are observed in very few executions (tens to hundreds out of millions) of very few harnesses (one out of hundreds to thousands). Our implementation is open source and publicly available...|$|R
40|$|Abstract — In {{any large}} {{software}} organization, the street cred of source code is of principle concern. Often-times {{we find that}} a given code base can talk a mean game, but when time comes to “throw down, ” that code base {{is nowhere to be}} seen. While intuitively we as software developers come to gain a sense of which code bases are trust-worthy (or, “will ball ‘till they fall”), some systematic method for measuring and reporting this is necessary. In this paper we present the GUnit <b>testing</b> <b>harness</b> (pronounced, “Gee Unit”), which does just that...|$|R
40|$|This {{technical}} report {{consists of the}} two papers discussing testing technology. INFUSE: Integration Testing with Crowd Control describes the test management facilities provided by the lNFUSE change management system. lNFUSE partially automates the construction of <b>test</b> <b>harnesses</b> and regression <b>test</b> suites at each level of the integration hierarchy from components available from lower levels. Adequate Testing and Object-Oriented Programming applies the axions of adequate testing to object-oriented programming languages and examines their implications. Contrary to our original expectations, we discover that in the general case classes must be retested in every context of reuse...|$|R
5000|$|The Test Anything Protocol (TAP) is a {{protocol}} to allow communication between unit tests and a <b>test</b> <b>harness.</b> It allows individual tests (TAP producers) to communicate test {{results to the}} testing harness in a language-agnostic way. Originally developed for unit testing of the Perl interpreter in 1987, producers and parsers are now available for many development platforms.|$|E
50|$|This validates {{that the}} <b>test</b> <b>harness</b> is working correctly, {{shows that the}} new test does not pass without {{requiring}} new code because the required behavior already exists, and it rules {{out the possibility that}} the new test is flawed and will always pass. The new test should fail for the expected reason. This step increases the developer's confidence in the new test.|$|E
5000|$|Several testing {{frameworks}} are available, as is {{software that}} generates test stubs based on existing source code and testing requirements. Stubs and Drivers {{are two types}} of <b>test</b> <b>harness.</b> Test harnesses are the collection of software and test data which is configured so that one can test a program unit by simulating different set of conditions, while monitoring the behavior and outputs.|$|E
40|$|Abstract. A {{primary goal}} of {{generative}} programming and model-driven development is to {{raise the level of}} abstraction at which designers and developers interact with the software systems they are building. During initial development, the benefits of abstraction are clear. However, during testing and maintenance, increased distance from the implementation can be a disadvantage. We view test cases and <b>test</b> <b>harnesses</b> as an essential bridge between the high-level specifications and the implementation. As such, the generation of test cases for fully generated components and <b>test</b> <b>harnesses</b> for partially generated components is of fundamental importance to model-driven systems. In this paper we present our experience with test-case and test-harness generation for a family of model-driven, component-based distributed systems. We describe our development tool, MODEST, and motivate our decision to invest the extra effort needed to generate test code. We present our approach to test-case and test-harness generation and describe the benefits to developers and maintainers of generated systems. Furthermore, we quantify the relative cost of generating test code versus application code and find that the artifact templates for producing test code are simpler than those used for application code. Given the described benefits to developers and maintainers and the relatively low cost of test-code development, we argue that test-code generation should be a fundamental feature of model-driven development efforts. ...|$|R
40|$|The {{following}} {{aspects of}} a research program concerned with tracking gray whales were documented: (1) design, fabrication and testing of a girdle-type harness and associated gear (release mechanism, tracking transmitter, xenon flasher), (2) design, fabrication and testing of instrumentation packs (subminiature recorder, sensor, electronics), (3) field preparations for the January-February 1974 expedition off Mexico, (4) travel arrangements, (5) preliminary field report (capture and handling of juvenile whales, instrumentation and housing <b>tests,</b> <b>harness</b> abrasion and chafing, respiration measurements, sea tracking, distribution, number, and behavior of whales at Lopez Mateos), (6) review, data reduction, and analysis of results...|$|R
40|$|Despite {{considerable}} {{commercial exploitation}} of fault tolerance systems, significant and difficult research problems remain {{in such areas}} as fault detection and correction. A research project is described which constructs a distributed computing test bed for loosely coupled computers. The project is constructing a tool kit to support research into distributed control algorithms, including a distributed Ada compiler, distributed debugger, <b>test</b> <b>harnesses,</b> and environment monitors. The Ada compiler is being written in Ada and will implement distributed computing at the subsystem level. The design goal is to provide a variety of control mechanics for distributed programming while retaining total transparency at the code level...|$|R
