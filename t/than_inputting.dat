6|2479|Public
5000|$|In Exec 8, work is {{organized}} into jobs, called [...] "runs," [...] which are scheduled {{based on their}} priority and need for lockable resources such as Uniservo tape drives or Fastrand drum files. The control language syntax uses the [...] "@" [...] symbol (which Univac called [...] "the master space") as the control statement recognition symbol. It was immediately followed by the command or program name, then a comma and any option switches. After a space character, {{the remainder of the}} statement differed for particular commands. A command to compile a FORTRAN program would look like [...] "@FOR,options sourcefile, objectfile". Input data for an application could be read from a file (generally card images), or immediately follow the @ command in the run stream. All lines until the sentinel command [...] "@END" [...] were assumed to be input data, so forgetting to insert it led to the compiler interpreting subsequent commands as program data. For this reason, it was preferable to process data in files rather <b>than</b> <b>inputting</b> it in the run stream.|$|E
40|$|Abstract. Currently, {{inputting}} mathematical formulas into {{a document}} using a PC requires more effort by users <b>than</b> <b>inputting</b> normal text. This fact inhibits the spreading of mathematical formulas as internet contents. We propose {{a method for}} predicting user’s inputs of mathematical formulas using an N-gram model: a popular probabilistic language model in natural language processing. Mathematical formulas are usually presented in hierarchical structure. There-fore, our method incorporates hierarchical information of mathematical formu-las to create a prediction model. We try to achieve high prediction accuracy of inputting characters for mathematical formulas...|$|E
40|$|Part 1 : Long and Short PapersInternational audienceCurrently, {{inputting}} mathematical formulas into {{a document}} using a PC requires more effort by users <b>than</b> <b>inputting</b> normal text. This fact inhibits the spreading of mathematical formulas as internet contents. We propose {{a method for}} predicting user’s inputs of mathematical formulas using an N-gram model: a popular probabilistic language model in natural language processing. Mathematical formulas are usually presented in hierarchical structure. Therefore, our method incorporates hierarchical information of mathematical formulas to create a prediction model. We try to achieve high prediction accuracy of inputting characters for mathematical formulas...|$|E
5000|$|... #Caption: Output (mechanical) {{energy is}} always lower <b>than</b> <b>input</b> energy ...|$|R
5000|$|... 2001: The Lutec [...] "Free Energy Generator" [...] - {{a device}} {{claiming}} to produce 30 times more output <b>than</b> <b>input,</b> but actually outputs 33% of input power.|$|R
50|$|Undervoltage-lockout {{should be}} used for {{switched-mode}} power supplies when the output impedance of an electrical network higher <b>than</b> <b>input</b> impedance of regulator. It's prevent oscillations and possible malfunction of regulator.|$|R
30|$|Considering these {{discussion}} on pedestrian detection and deep motion features, {{there is room}} to improve pedestrian detection by using deeply learned motion feature. Rather <b>than</b> <b>inputting</b> optical flow stacks into the temporal stream, we use SDt [7], as it can factor out non-informative motion. This is crucial, as many videos captured using car-mounted cameras contain ego-motions from the car, and removing such motions to extract important flow is necessary. In this paper, we discuss the strength of detection by fusing spatial channel features and stabilized motion features both learned by deep neural nets. To {{the best of our}} knowledge, this paper proposes the first detection method that uses motion with ConvNets.|$|E
40|$|Abstract — Several {{experimental}} studies revealed that expert {{systems have been}} successfully applied in real world domains such as medical diagnoses, traffic control, and many others. However, {{one of the major}} drawbacks of classic expert systems is their reliance on human domain experts which require time, care, experience and accuracy. This shortcoming also may result in building knowledge bases that may contain inconsistent rules or contradicting rules. To treat the abovementioned we intend to propose and develop automated methods based on data mining called Associative Classification (AC) that can be easily integrated into an expert system to produce the knowledge base according to hidden correlations in the input database. The methodology employed in the proposed expert system is based on learning the rules from the database rather <b>than</b> <b>inputting</b> the rules by the knowledge engineer from the domain expert and therefore, care and accuracy as well as processing time are improved. The proposed automated expert system contains a novel learning method based on AC mining that has been evaluated on Islamic textual data according to several evaluation measures including recall, precision and classification accuracy. Furthermore, five different classification approaches: Decision trees (C 4. 5, KNN, SVM, MCAR and NB) and the proposed automated expert system have been tested on the Islamic data set to determine the suitable method in classifying Arabic texts...|$|E
40|$|Abstract — Traditional {{methods of}} AC/AC {{converters}} have general drawbacks: output voltage is lower <b>than</b> <b>input</b> voltage, the input side THD is poor and output voltage frequency is lower <b>than</b> <b>input</b> voltage frequency by using voltage regulation method and cycloconverters. We introduce the novel approach- DC-modulated AC/AC converters in this paper, which successfully overcomes the drawbacks. Simulation and experimental {{results of the}} DC-modulated AC/AC converter are the evidences to verify our design. These methods will be very widely used in industrial applications. Keywords-component; DC-modulated AC/AC convertes, total harmonic distortion (THD), power factor (PF), power transfer efficiency (η),. I...|$|R
30|$|Further, stress {{recognition}} systems {{provided with}} TSHDTP as input produced significantly better stress recognition measures <b>than</b> <b>inputs</b> with TSHDTP replaced by TSLBP-TOP (p[*]<[*] 0.01). This suggests that stress patterns were better captured by TSHDTP features than TSLBP-TOP features.|$|R
5000|$|Dendritic APs are {{initiated}} {{more effectively}} by synchronous spatially clustered <b>inputs</b> <b>than</b> equivalent disperse <b>inputs.</b>|$|R
50|$|By {{imposing}} sparsity on {{the hidden}} units during training (whilst having {{a larger number}} of hidden units <b>than</b> <b>inputs),</b> an autoencoder can learn useful structures in the input data. This allows sparse representations of inputs. These are useful in pretraining for classification tasks.|$|R
50|$|Aquacultured {{shellfish}} include various oyster, mussel, and clam species. These bivalves are filter and/or deposit feeders, which rely on ambient {{primary production}} rather <b>than</b> <b>inputs</b> of fish or other feed. As such, shellfish aquaculture is generally perceived as benign or even beneficial.|$|R
50|$|Calculations made by Solow {{claimed that}} {{economic}} growth was mainly driven by technological progress (productivity growth) rather <b>than</b> <b>inputs</b> {{of capital and}} labor. However recent economic research has invalidated that theory, since Solow did not properly consider changes in both investment and labor inputs.|$|R
50|$|This {{shows that}} if the output gear GB has more teeth <b>than</b> the <b>input</b> gear GA, then the gear train amplifies the input torque. And, if the output gear has fewer teeth <b>than</b> the <b>input</b> gear, then the gear train reduces the input torque.|$|R
50|$|If {{the output}} gear of {{a gear train}} rotates more slowly <b>than</b> the <b>input</b> gear, then the gear train is called a speed reducer. In this case, because the output gear must have more teeth <b>than</b> the <b>input</b> gear, the speed reducer amplifies the input torque.|$|R
5000|$|When {{labels are}} more {{expensive}} to gather <b>than</b> <b>input</b> examples, semi-supervised learning can be useful. Regularizers {{have been designed to}} guide learning algorithms to learn models that respect the structure of unsupervised training samples. If a symmetric weight matrix [...] is given, a regularizer can be defined: ...|$|R
40|$|High-efficiency dc/dc {{converter}} {{has been}} developed that provides commonly used voltages of plus or minus 12 Volts from an unregulated dc source of from 14 to 40 Volts. Unique features of converter are its high efficiency at low power level and ability to provide output either larger or smaller <b>than</b> <b>input</b> voltage...|$|R
50|$|This {{theory was}} {{contrary}} to popular management-belief at the time. Where Bill Reddin maintained that managerial effectiveness is {{defined in terms of}} output rather <b>than</b> <b>input,</b> meaning what they achieve rather than what they do, his colleagues in behavioralist studies and human psychology held that there were indeed ideal styles of management behavior.|$|R
30|$|An {{important}} {{result in}} Table  1 and Fig.  10 {{is that the}} dynamic response of the nailed plywood–timber joints is relatively more sensitive to <b>input</b> frequency <b>than</b> <b>input</b> acceleration. Another important {{result is that the}} most severe frequency that causes the bending fatigue failure of nails does not coincide with the most severe frequency that causes a typical failure in static loading, nail-head-pull-through.|$|R
5000|$|... must {{be greater}} than m (otherwise the output will not be smaller <b>than</b> the <b>input)</b> ...|$|R
50|$|A {{converter}} that outputs {{a voltage}} higher <b>than</b> the <b>input</b> voltage (such as a boost converter).|$|R
50|$|A {{converter}} where {{output voltage}} is lower <b>than</b> the <b>input</b> voltage (such as a buck converter).|$|R
5000|$|If {{the feature}} space [...] has lower {{dimensionality}} <b>than</b> the <b>input</b> space , then the feature vector [...] {{can be regarded}} as a compressed representation of the input [...] If the hidden layers are larger <b>than</b> the <b>input</b> layer, an autoencoder can potentially learn the identity function and become useless. However, experimental results have shown that autoencoders might still learn useful features in these cases.|$|R
50|$|Current inputs {{are less}} {{sensitive}} to electrical noise (e.g. from welders or electric motor starts) <b>than</b> voltage <b>inputs.</b>|$|R
50|$|Isochronous burst {{transmission}} is used when the information-bearer channel rate is higher <b>than</b> the <b>input</b> data signaling rate.|$|R
5000|$|... #Caption: Simplified {{chain of}} ownership. In reality, a {{transaction}} {{can have more}} <b>than</b> one <b>input</b> and more <b>than</b> one output.|$|R
30|$|The second {{empirical}} technique, transition-sensitive power models, {{is based}} on <b>input</b> transitions rather <b>than</b> <b>input</b> statistics. The method, proposed by Mehta et al. [13], assumes that a power model is provided for each functional unit—a table containing the power consumed for each input transition. Closely related input transitions and power patterns can be concentrated in clusters, thereby reducing {{the size of the}} table. Other researchers have also proposed similar macro-model-based power estimation approaches [14, 15].|$|R
5000|$|... #Caption: Simplified {{chain of}} {{ownership}} (SDC). In reality, a transaction can have more <b>than</b> one <b>input</b> and more <b>than</b> one output.|$|R
5000|$|Distributors {{allowed an}} output pulse to be wired to more <b>than</b> one <b>input</b> without {{creating}} a back circuit between the inputs.|$|R
50|$|The method {{begins by}} {{describing}} a program's inputs {{in terms of}} the four fundamental component types. It then goes on to describe the program's outputs in the same way. Each input and output is modelled as a separate Data Structure Diagram (DSD). To make JSP work for compute-intensive applications, such as digital signal processing (DSP) it is also necessary to draw algorithm structure diagrams, which focus on internal data structures rather <b>than</b> <b>input</b> and output ones.|$|R
5000|$|In {{a normal}} voltage divider, the {{resistance}} of each branch is less than {{the resistance of}} the whole, so the output voltage is less <b>than</b> the <b>input.</b> Here, due to the negative resistance, the total AC resistance [...] {{is less than the}} resistance of the diode alone [...] so the AC output voltage [...] is greater <b>than</b> the <b>input</b> [...] The voltage gain [...] is greater than one, and increases without limit as [...] approaches [...]|$|R
30|$|An autoencoder, firstly {{introduced}} in Rumelhart et al. [72], is a feedforward network that {{can learn a}} compressed, distributed representation of data, usually {{with the goal of}} dimensionality reduction or manifold learning. An autoencoder usually has one hidden layer between input and output layer. Hidden layer usually has a more compact representation <b>than</b> <b>input</b> and output layers, i.e., hidden layer has fewer units <b>than</b> <b>input</b> or output layer. Input and output layer usually has the same setting, which allows an autoencoder to be trained unsupervised with same data fed in at the input and to be compared with what is at the output layer. The training process is the same as traditional neural network with backpropagation; the only difference lying in the error is computed by comparing the output to the data itself [2]. Mitchell et al. [73], showed a nice illustration of autoencoder. He built a three-layer structure (eight unit for input and output layer and three unit for the hidden layer in between), then he fed the one-hot vector representation into the input and output layer, the hidden layer turned out to approximating the data with inputs’ binary representation [2].|$|R
5000|$|Behave in a boost or buck {{operation}} only. Thus the obtainable {{output voltage}} range is limited, either smaller or greater <b>than</b> the <b>input</b> voltage.|$|R
40|$|A {{definition}} of moral hazard in multiple peril crop insurance is proposed {{that focuses on}} expected indemnities rather <b>than</b> <b>input</b> use. Five years of production and insurance data for a panel of Kansas wheat farms is used to empirically test {{for this type of}} moral hazard. Results suggest that moral hazard affects multiple peril crop insurance indemnities in poor production years but that no significant moral hazard occurs in years when growing conditions are favorable. Copyright 1997, Oxford University Press. ...|$|R
50|$|Georgia State College of Law {{is ranked}} as the 56th best law school by U.S. News & World Report, an {{improvement}} from 64th in 2014. The College of Law also boasts a strong Health Law program (ranked 3rd nationally) {{and a large}} part-time program (ranked 14th nationally). The school is ranked 33rd by Above The Law using metrics that focus more on student outcomes rather <b>than</b> <b>inputs.</b> Additionally, GSU has been named by Princeton Review as a top national law school.|$|R
