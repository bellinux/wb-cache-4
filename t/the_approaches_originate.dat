0|10000|Public
40|$|There {{is a new}} <b>approach</b> to <b>the</b> derivative, an {{algebraic}} one, {{different from}} using infinitesimals or limits. <b>The</b> <b>approach</b> <b>originated</b> in research on mathematics education and has been developed to the stage {{that it can be}} tested there. For mathematics research there is scope for further development of foundations. The paper shortly reviews <b>the</b> <b>approach</b> and <b>the</b> literature about it...|$|R
50|$|<b>The</b> Paideia <b>approach</b> <b>originated</b> in 1969 with {{psychologist}} Dr. Francis Mechner. His oversaw {{his first}} school, the Paideia School in Armonk, New York, from 1969 to 1974. The Queens Paideia School opened with approximately 10 {{students for the}} 2009-10 school year.|$|R
40|$|We {{describe}} generating {{functions for}} several important families of classical symmetric functions and shifted Schur functions. <b>The</b> <b>approach</b> is <b>originated</b> from vertex operator realization of symmetric functions {{and offers a}} unified method to treat various families of symmetric functions and their shifted analogues...|$|R
40|$|The {{realization}} of new distributed and heterogeneous software applications {{is a challenge}} that software engineers have to face. Logic programming and multi-agent systems can play a very effective role in the rapid prototyping of new software products. This paper proposes a general <b>approach</b> to <b>the</b> prototyping of complex and distributed applications modelled as multi-agent systems and outlines the autonomous research experiences of different research groups from which <b>the</b> <b>approach</b> <b>originates.</b> All <b>the</b> experiences have logic programming as the common foundation and deal with {{different aspects of the}} problem: integration of heterogeneous data and reasoning systems, animation of formal specifications and the development of agent-based software. The final goal is joining these diverse experiences into a unique open framewor...|$|R
50|$|Pediatric neuropsychological {{assessments}} {{evolved from}} {{knowledge and experience}} from the assessment of adults with brain damage. Consequently, early tests were not specifically designed with children in mind and were often normed on small samples of children. The development of the NEPSY was revolutionary as it was specifically designed {{for the purpose of}} testing children. The NEPSY is grounded in developmental and neuropsychological theory and practice. <b>The</b> diagnostic <b>approach</b> <b>originated</b> in <b>the</b> Lurian <b>approach</b> to assessment (Luria, 1973, 1980).|$|R
40|$|We {{present a}} tensor-structured method to {{calculate}} the Møller-Plesset (MP 2) cor-rection to the Hartree-Fock energy with reduced computational consumptions. <b>The</b> <b>approach</b> <b>originates</b> from <b>the</b> 3 D grid-based low-rank factorization of the two-electron integrals performed by the purely algebraic optimization. The computational scheme benefits from fast multilinear algebra implemented on the separable representations of the molecular orbital transformed two-electron integrals, the doubles amplitude ten-sors and other fours order data-arrays involved. The separation rank estimates are discussed. The so-called quantized approximation of the long skeleton vectors com-prising the tensor factorizations of the main entities allows to reduce the storage costs. The detailed description of tensor algorithms for evaluation of the MP 2 energy correc-tion is presented. The efficiency of these algorithms is illustrated {{in the framework of}} Hartree-Fock calculations for compact molecules, including alanine and glycine amino acids...|$|R
40|$|Blendshapes”, {{a simple}} linear model of facial expression, is <b>the</b> {{prevalent}} <b>approach</b> to realistic facial animation. It has driven animated characters in Hollywood films, {{and is a}} standard feature of commercial animation packages. <b>The</b> blendshape <b>approach</b> <b>originated</b> in industry, and became a subject of academic research relatively recently. This course describes the published {{state of the art}} in this area, covering both literature from the graphics research community, and developments published in industry forums. We show that, despite the simplicity of <b>the</b> blendshape <b>approach,</b> there remain open problems associated with this fundamental technique...|$|R
50|$|Structuralist {{economics}} is {{an approach to}} economics that {{emphasizes the importance of}} taking into account structural features (typically) when undertaking economic analysis. <b>The</b> <b>approach</b> <b>originated</b> with <b>the</b> work of the Economic Commission for Latin America (ECLA or CEPAL) and is primarily associated with its director Raúl Prebisch and Brazilian economist Celso Furtado. Prebisch began with arguments that economic inequality and distorted development was an inherent structual feature of the global system exchange. As such, early structuralist models emphasised both internal and external disequilibria arising from the productive structure and its interactions with the dependent relationship developing countries had with the developed world. Prebisch himself helped provide the rationale for the idea of Import substitution industrialization, {{in the wake of the}} Great Depression and World War II. The alleged declining terms of trade of the developing countries, the Singer-Prebisch hypothesis, played a key role in this.|$|R
40|$|A simple {{dynamic model}} {{proposed}} for flexible links is briefly reviewed and experimental control {{results are presented}} for different flexible systems. A simple dynamic model is useful for rapid prototyping of manipulators and their control systems, for possible application to manipulator design decisions, and for real time computation as might be applied in model based or feedforward control. Such a model is proposed, with the further advantage that clear physical arguments and explanations {{can be associated with}} its simplifying features and with its resulting analytical properties. The model is mathematically equivalent to Rayleigh's method. Taking the example of planar bending, <b>the</b> <b>approach</b> <b>originates</b> in its choice of two amplitude variables, typically chosen as the link end rotations referenced to the chord (or the tangent) motion of the link. This particular choice is key in establishing the advantageous features of the model, and it was used to support the series of experiments reported...|$|R
40|$|Context of {{scholarly}} results improves discovery, sharing and re-use in library and research communities. OpenAIREplus facilitates access to European research. Its {{mission is to}} interlink research publications, data, contributors and grants. Introducing pilots for the Social & Life Sciences we show how disciplinary services {{can be used to}} enhanced publications in cross-disciplinary environments. <b>The</b> <b>approach</b> followed <b>originate</b> from text-mining and interlinks innovative repository services and initiatives like DataCite. ...|$|R
40|$|The use {{of large}} N approximations to treat {{strongly}} interacting quantum systems been very extensive {{in the last}} decade. <b>The</b> <b>approach</b> <b>originated</b> in elementary particles theory, but has found many applications in condensed matter physics. Initially, the large N expansion was developed for the Kondo and Anderson models of magnetic impurities in metals. Soon thereafter it was extended to the Kondo and Anderson lattice models for mixed valence and heavy fermions phenomena in rare earth compounds [1, 2]. In these notes we shall formulate and apply <b>the</b> large N <b>approach</b> to <b>the</b> quantum Heisenberg model [3 – 6]. This method provides an additional avenue to the static and dynamical correlations of quantum magnets. The mean field theories derived below can describe both ordered and disordered phases, at zero and at finite temperatures, and they complement <b>the</b> semiclassical <b>approaches.</b> Generally speaking, <b>the</b> parameter N labels an internal SU(N) symmetry at each lattice site (i. e., the number of “flavors ” a Schwinger boson or a constrained fermion can have). In most cases, the large N approximation ha...|$|R
5000|$|This {{approach}} is usually housed under a common debate {{in understanding the}} Holocaust, known as the functionalism versus intentionalism debate. Functionalists represent {{the argument that the}} decision to kill the Jews developed over time with a concept called [...] "cumulative radicalization" [...] (Hans Mommsen). Intentionalists, on the other hand, believe that the Final Solution was intended to occur all along and use antisemitism to prove this point. In the functionalism versus intentionalism debate, <b>the</b> bottom-up <b>approach</b> <b>originated</b> under <b>the</b> functionalist perspective. Götz Aly, specifically, has argued the case for <b>the</b> bottom-up <b>approach</b> from <b>the</b> functionalist view.|$|R
40|$|A novel {{anisotropic}} estimator for {{image restoration}} is presented. <b>The</b> proposed <b>approach</b> <b>originates</b> from <b>the</b> geometric {{idea of a}} starshaped estimation neighborhood topology. In this perspective, an optimal adaptation is achieved by selecting in a pointwise fashion the ideal starshaped neighborhood for the estimation point. In practice, this neighborhood is approximated by a sectorial structure composed by conical sectors of adaptive size. Special varying-scale kernels, supported on these sectors, are exploited {{in order to bring}} the original geometrical problem to a practical multiscale optimization. It is proposed to use this [...] ...|$|R
40|$|Agile {{methods have}} been {{suggested}} {{as a means to}} meet the challenges in large sys­tems’ development, in particular large-scale development of software. However, with in­creasing scale, limitations of agile methods begin to materialize. To this end, we outline an integration driven development (IDD) approach that combines plan-driven, incremental develop­ment with agile methods. The planning is based on a visualization of dependencies between needed system changes – deltas – called the anatomy. <b>The</b> IDD <b>approach</b> <b>originated</b> in <b>the</b> Ericsson telecom practice more than a decade ago, and is now used regularly in large system develop­ment projects...|$|R
40|$|Abstract. This is {{the first}} of a series of papers {{dedicated}} to a coordinate-free approach to several classic geometries such as hyperbolic (real, complex, quaternionic), elliptic (spherical, Fubini-Study), and lorentzian ones. These geometries carry a certain simple structure that is in some sense stronger than the riemannian one. Their basic geometrical objects have linear nature. Such objects provide natural compactifications of commonly studied geometries. The usual riemannian concepts are easily derivable from the strong structure and thus gain their coordinate-free form. Many examples show how this view can help when dealing with explicit classic geometries and illustrate fruitful features of <b>the</b> <b>approach.</b> In this paper, only projective aspects of classic geometries are studied. The methods were first tested in [AGG] and [AGu] where they were successfully applied to constructing complex hyperbolic manifolds and to solving problems in complex hyperbolic geometry. 1. Classic geometries: introduction, definition, examples, and motivation 1. 1. Introduction. This series of papers constitutes an attempt to systematically develop a coordinate-free view on several classic geometries. <b>The</b> <b>approach</b> <b>originates</b> from [AGG] where, in order to simplify formulae, we expressed several complex hyperbolic geometry concepts in a more invariant and convenient form...|$|R
40|$|In {{this paper}} we propose a {{flexible}} approach that supports heterogeneous requirements on systems for the semantic annotation of web content. The flexibility of <b>the</b> <b>approach</b> <b>originates</b> from a model based on the definition of abstract events, which captures at the logical level the main interactions occurring in a system for combined management of ontologies and web content. Application-specific semantics is then provided operationally as an assignment of handlers to these events. While the abstract events are rather coarse-grained to reduce prior commitment, preconditions on the handlers express application-specific distinctions based on contextual information associated with each specific event. Although the possibility to define completely new handlers guarantees the generality of our approach, we foster convention over configuration by providing a set of default handlers, which can be customized by filling their extension points. The use of customizable handlers, whether or not the default ones, reduces the development effort and guarantees consistent user experience despite evolving requirements. A comprehensive framework for semantic annotation of web content has been realized and will be hereafter introduced...|$|R
40|$|This {{paper is}} a {{continuation}} of <b>the</b> <b>approach</b> <b>originated</b> by Ramchandani [14]; the firing times are thus associated with transitions of a net, and tokens are removed from the transitions' input places at the beginning of firings. It is shown that the behavior of two very different classes of timed Petri nets, D [...] timed and M [...] timed nets, can be described within one, uniform formalism with only a few Note: This version of the paper is derived from the original text by using a different text processing system. A few minor corrections have been made during the reformatting. class-dependent details. In D [...] timed Petri nets [7, 14, 15, 19] the firing times are deterministic (or constant), i. e., there is a positive (rational) number assigned to each transition of a net which determines the "duration" of its firings. In M [...] timed Petri nets (or stochastic Petri nets) [2, 11, 21] the firing times are exponentially distributed random variables, and the corresponding firing rates are assigned to transitions of a net...|$|R
40|$|We have {{developed}} a new method for determining the corotation radii of density waves in disk galaxies, which makes use of the calculated radial distribution of an azimuthal phase shift between the potential and density wave patterns. <b>The</b> <b>approach</b> <b>originated</b> from improved theoretical understandings {{of the relation between}} the morphology and kinematics of galaxies, and on the dynamical interaction between density waves and the basic-state disk stars which results in the secular evolution of disk galaxies. In this paper, we present the rationales behind the method, and the first application of it to several representative barred and grand-design spiral galaxies, using near-infrared images to trace the mass distributions, as well as to calculate the potential distributions used in the phase shift calculations. We compare our results with those from other existing methods for locating the corotations, and show that the new method both confirms the previously-established trends of bar-length dependence on galaxy morphological types, as well as provides new insights into the possible extent of bars in disk galaxies. The method also facilitates the estimatio...|$|R
50|$|An Influence diagram(ID) is {{essentially}} a graphical representation of the probabilistic interdependence between Performance Shaping Factors (PSFs), factors which pose a likelihood of influencing {{the success or failure}} of the performance of a task. <b>The</b> <b>approach</b> <b>originates</b> from <b>the</b> field of decision analysis and uses expert judgement in its formulations. It is dependent upon the principal of human reliability and results from the combination of factors such as organisational and individual factors, which in turn combine to provide an overall influence. There exists a chain of influences in which each successive level affects the next. The role of the ID is to depict these influences and the nature of the interrelationships in a more comprehensible format. In this way, the diagram may be used to represent the shared beliefs of a group of experts on the outcome of a particular action and the factors that may or may not influence that outcome. For each of the identified influences quantitative values are calculated, which are then used to derive final Human Error Probability (HEP) estimates.|$|R
40|$|The paper starts {{by looking}} at the {{competing}} paradigms in pursuing nanotechnology, namely <b>the</b> top–down <b>approach</b> <b>originating</b> from microelectronics, and <b>the</b> bottom–up <b>approach</b> inspired by biological sciences. This almost dogmatic dichotomy uncovers the challenges posed by interdisciplinary and transdisciplinary knowledge transfer. Several possible knowledge mechanisms are proposed using the analogy with the classical transfer phenomena theory, namely "diffusional", "convective", "turbulent", "radiative" and "interphasic" transfer mechanisms. Furthermore, possible modes of knowledge production are outlined. These possible mechanisms for knowledge transfer and production are then analyzed in the context of micro-electro-mechanical systems and nanotechnology development. This analysis is extended to formulate a framework for facilitating knowledge transfer and production in nanotechnology...|$|R
5000|$|The gongfu tea {{ceremony}} or kung fu tea ceremony ( [...] or [...] ), {{is a kind}} of Chinese tea ceremony, {{involving the}} ritualized preparation and presentation of tea. It is probably based on <b>the</b> tea preparation <b>approaches</b> <b>originated</b> in Fujian and the Chaoshan area of eastern Guangdong. The term literally means [...] "making tea with skill". [...] Today, <b>the</b> <b>approach</b> is used popularly by teashops carrying tea of Chinese origins, and by tea connoisseurs as a way to maximize the taste of a tea selection, especially a finer one.|$|R
40|$|Every {{algorithm}} {{which can}} be executed on a computer can at least in principle be realized in hardware, i. e. by a discrete physical system. The problem is that up to {{now there is no}} programming language by which physical systems can constructively be described. Such tool, however, is essential for the compact description and automatic production of complex systems. This paper introduces a programming language, called Akton-Algebra, which provides the foundation for the complete description of discrete physical systems. <b>The</b> <b>approach</b> <b>originates</b> from <b>the</b> finding that every discrete physical system reduces to a spatiotemporal topological network of nodes, if the functional and metric properties are deleted. A next finding is that there exists a homeomorphism between the topological network and a sequence of symbols representing a program by which the original nodal network can be reconstructed. Providing Akton-Algebra with functionality turns it into a flow-controlled general data processing language, which by introducing clock control and addressing can be further transformed into a classical programming language. Providing Akton-Algebra with metrics, i. e. the shape and size of the components, turns it into a novel hardware system construction language. </p...|$|R
40|$|In this {{manuscript}} we review {{new ideas and}} first results on application of <b>the</b> Graphical Models <b>approach,</b> <b>originated</b> from Statistical Physics, Information Theory, Computer Science and Machine Learning, to optimization problems of network flow type with additional constraints related to the physics of the flow. We illustrate the general concepts {{on a number of}} enabling examples from power system and natural gas transmission (continental scale) and distribution (district scale) systems. Comment: 28 pages, 1 figur...|$|R
40|$|International audienceIn this paper, we further develop <b>the</b> <b>approach,</b> <b>originating</b> in [13], to " computation-friendly " {{hypothesis}} testing via Convex Programming. Most {{of the existing}} results on {{hypothesis testing}} aim to quantify in a closed analytic form separation between sets of distributions allowing for reliable decision in precisely stated observation models. In contrast to this descriptive (and highly instructive) traditional framework, <b>the</b> <b>approach</b> we promote here can be qualified as operational – the testing routines and their risks are yielded by an efficient computation. All we know in advance is that, under favorable circumstances, specified in [13], the risk of such test, whether high or low, is provably near-optimal under the circumstances. As a compensation {{for the lack of}} " explanatory power, " this approach is applicable to a much wider family of observation schemes and hypotheses to be tested than those where " closed form descriptive analysis " is possible. In the present paper our primary emphasis is on computation: we make a step further in extending the principal tool developed in [13] – testing routines based on affine detectors – to a large variety of testing problems. The price of this development is the loss of blanket near-optimality of the proposed procedures (though it is still preserved in the observation schemes studied in [13], which now become particular cases of the general setting considered here) ...|$|R
40|$|In this paper, we further develop <b>the</b> <b>approach,</b> <b>originating</b> in [14 (arXiv: 1311. 6765), 20 (arXiv: 1604. 02576) ], to "computation-friendly" {{hypothesis}} testing and statistical estimation via Convex Programming. Specifically, {{we focus on}} estimating a linear or quadratic form of an unknown "signal," known {{to belong to a}} given convex compact set, via noisy indirect observations of the signal. Most of the existing theoretical results on the subject deal with precisely stated statistical models and aim at designing statistical inferences and quantifying their performance in a closed analytic form. In contrast to this descriptive (and highly instructive) traditional framework, <b>the</b> <b>approach</b> we promote here can be qualified as operational [...] the estimation routines and their risks are yielded by an efficient computation. All we know in advance is that under favorable circumstances to be specified below, the risk of the resulting estimate, whether high or low, is provably near-optimal under the circumstances. As a compensation for the lack of "explanatory power," this approach is applicable to a much wider family of observation schemes than those where "closed form descriptive analysis" is possible. The paper is a follow-up to our paper [20 (arXiv: 1604. 02576) ] dealing with {{hypothesis testing}}, in what follows, we apply the machinery developed in this reference to estimating linear and quadratic forms...|$|R
40|$|Discounted {{cash flow}} methods for making R 2 ̆ 6 amp;D {{investment}} decisions cannot properly capture the option value in R 2 ̆ 6 amp;D. Since market and technology uncertainties change {{expectations about the}} viability of many new products, the value of projects is frequently adjusted during the R 2 ̆ 6 amp;D stages. Capturing the adjustment in expectations has an option value that may significantly differ from the Net Present Value of R 2 ̆ 6 amp;D projects. However, there are no historic time series for estimating {{the uncertainty of the}} value of R 2 ̆ 6 amp;D projects. As a result, the standard Black and Scholes model for financial option valuation needs to be adjusted. The aim {{of this paper is to}} report the application of a particular option pricing model for setting the budget of R 2 ̆ 6 amp;D projects. The option value of the model captures jumps or business shifts in market or technology conditions. <b>The</b> <b>approach</b> <b>originates</b> from applying current insight into the valuation of R 2 ̆ 6 amp;D projects to the field of multimedia research at Philips Corporate Research. This way, the gap between real option theory and R 2 ̆ 6 amp;D practice is further diminished...|$|R
40|$|In this paper, we further develop <b>the</b> <b>approach,</b> <b>originating</b> in [GJN], to "computation-friendly" {{hypothesis}} testing via Convex Programming. Most {{of the existing}} results on {{hypothesis testing}} aim to quantify in a closed analytic form separation between sets of distributions allowing for reliable decision in precisely stated observation models. In contrast to this descriptive (and highly instructive) traditional framework, <b>the</b> <b>approach</b> we promote here can be qualified as operational [...] the testing routines and their risks are yielded by an efficient computation. All we know in advance is that, under favorable circumstances, specified in [GJN], the risk of such test, whether high or low, is provably near-optimal under the circumstances. As a compensation {{for the lack of}} "explanatory power," this approach is applicable to a much wider family of observation schemes and hypotheses to be tested than those where "closed form descriptive analysis" is possible. In the present paper our primary emphasis is on computation: we make a step further in extending the principal tool developed in the cited paper [...] testing routines based on affine detectors [...] to a large variety of testing problems. The price of this development is the loss of blanket near-optimality of the proposed procedures (though it is still preserved in the observation schemes studied in [GJN], which now become particular cases of the general setting considered here). [GJN]: Goldenshluger, A., Juditsky, A., Nemirovski, A. "Hypothesis testing by convex optimization," Electronic Journal of Statistics 9 (2), 201...|$|R
40|$|Recent ideas {{based on}} the {{properties}} of assemblies of frictionless particles in mechanical equilibrium provide a perspective of amorphous systems different from that offered by <b>the</b> traditional <b>approach</b> <b>originating</b> in liquid theory. The relation, if any, between these two points of view, and {{the relevance of the}} former to the glass phase, has been difficult to ascertain. In this paper we introduce a model for which both theories apply strictly: it exhibits on the one hand an ideal glass transition and on the other `jamming' features (fragility, soft modes) virtually identical to that of real systems. This allows us to disentangle the different contents and domains of applicability of the two physical phenomena. Comment: 4 pages, 6 figures Modified content, new figur...|$|R
40|$|Higher-order {{theories}} of consciousness argue that conscious awareness crucially depends on higher-order mental representations that represent oneself {{as being in}} particular mental states. These theories have featured prominently in recent debates on conscious awareness. We provide new leverage on these debates by reviewing the empirical {{evidence in support of}} the higher-order view. We focus on evidence that distinguishes the higher-order view from its alternatives, such as the first-order, global workspace and recurrent visual processing theories. We defend the higher-order view against several major criticisms, such as prefrontal activity reflects attention but not awareness, and prefrontal lesion does not abolish awareness. Although <b>the</b> higher-order <b>approach</b> <b>originated</b> in philosophical discussions, we show that it is testable and has received substantial empirical support. Â© 2011 Elsevier Ltd. Link_to_subscribed_fulltex...|$|R
40|$|This {{contribution}} {{describes a}} computational homogenization <b>approach</b> to model <b>the</b> multi-physics processes in Li-ion batteries in a multi-scale view. <b>The</b> adopted <b>approach</b> <b>originates</b> from <b>the</b> fundamental balance laws (of mass, momentum, charge) at both scales and the multi scale analysis roots itself on an energy-based weak {{formulation of the}} balance laws, which allows to extend the Hill–Mandel energy averaging theorem to the problem at hand. Electroneutrality assumption has been taken into account. Maxwell’s equations are considered in a quasi-static sense in a rigorous setting. Time dependent scale transitions are formulated, {{as required by the}} length/time scales involved in Li-ion batteries processes, while scale separation in time is argued. Constitutive assumptions, computational procedures and simulations will be collected in a companion paper...|$|R
40|$|This article {{considers}} {{the development of}} reduced chemistry models for argon plasmas using Principal Component Analysis (PCA) based methods. Starting from an electronic specific Collisional-Radiative model, a reduction of the variable set (i. e. mass fractions and temperatures) is proposed by projecting the full set on a reduced basis made up of its principal components. Thus, the flow governing equations are only solved for the principal components. <b>The</b> proposed <b>approach</b> <b>originates</b> from <b>the</b> combustion community, where Manifold Generated Principal Component Analysis (MG-PCA) has been developed as a successful reduction technique. Applications consider ionizing shock waves in argon. The results obtained show {{that the use of}} the MG-PCA technique enables for a substantial reduction of the computational time. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
40|$|The {{twenty-first century}} has brought lots of {{challenges}} {{for people in}} all spheres, including education. In the new context, traditional approaches often seem ineffective and therefore new tools and methods have to be applied. An alternative approach that might be useful in the given context is design thinking – <b>the</b> <b>approach</b> that <b>originated</b> in architecture, design and art, and nowadays is applied in many fields. It is a human-centered problem-solving approach {{that may be used}} in the teaching/learning process to develop twenty-first century skills and enhance creativity and innovation. This paper introduces readers to the origin of design thinking, its attributes and processes as well as its application in pedagogy...|$|R
40|$|This paper {{takes on}} a {{narrative}} and quantitative <b>approach</b> to examine <b>the</b> dynamic effects of oil-price shocks to the U. S. economy. Based on market information collected from various oil-industry trade journals, we separate different kinds of oilprice shocks, and construct measures of exogenous oil shocks that are free of endogeneity and anticipatory problems. Estimation results indicate that oil shocks have had substantial and statistically significant impacts on the U. S. economy {{during the past two}} and a half decades. By contrast, traditional VAR identification strategies lead to a much weaker and insignificant real effect for the same period. Further investigation suggests that this discrepancy is possibly {{due to a lack of}} identification on <b>the</b> VAR <b>approach,</b> <b>originating</b> from mixing <b>the</b> exogenous oil-supply shocks with endogenous oil-price movements driven by changes in oil demand...|$|R
40|$|The {{gravitational}} anomalies in two dimensions, {{specifically the}} Einstein anomaly and the Weyl anomaly, are fully determined {{by means of}} dispersion relations. In this <b>approach</b> <b>the</b> anomalies <b>originate</b> from the peculiar infrared feature of the imaginary part of the relevant formfactor which approaches a δ-function singularity at zero momentum squared when m → 0. Comment: 10 page...|$|R
40|$|DissertationThe sensory {{integration}} <b>approach</b> <b>originates</b> from physical (anatomical and physiological) evidence whilst <b>the</b> play therapy <b>approach</b> <b>originates</b> from psychological evidence. Apart from play therapy, the researcher has also attended various courses in {{sensory integration}} therapy. Although {{both of these}} approaches are used as intervention methods with children who display behavioural, emotional and social difficulties, the researcher considered whether {{it was important for}} a play therapist to be aware of sensory integration therapy. The researcher then started this study in order to investigate the incidence of sensory integration dysfunction in children who receive play therapy. After completing the study, the researcher is of opinion that it is indeed necessary for play therapists to be aware of sensory integration theory in order to provide holistic play therapy intervention and to ensure positive therapy outcomes. Social WorkM. Diac. (Play Therapy...|$|R
40|$|A {{nonlinear}} {{modification of}} the Schrödinger equation is proposed in which the Lagrangian density for the Schrödinger equation is supplemented by terms polynomial in ∆m ln Ψ∗ Ψ multiplied by Ψ∗Ψ, thus introducing nonlinearity through the phase S rather than the amplitude R of the wave function Ψ = ReiS. Despite an infinite, in principle, number of free parameters in a fully developed implementation of the modification, it possesses a simple minimal version that may deserve a further study. This particular extension necessarily compromises the separability of noninteracting systems in <b>the</b> <b>approach</b> <b>originated</b> by Bia̷lynicki-Birula and Mycielski, {{referred to as the}} weak separability, but preserves all other physically relevant properties of the Schrödinger equation. It offers the simplest way to modify the Bohm quantum potential so as to allow a phase contribution to it, preserves known stationary states of quantum mechanical systems, and introduces a term that under some circumstances can dominate the first relativistic correction to the Schrödinger equation. The place of this modification in a broader spectrum of various other nonlinear modifications of the Schrödinger equation is discussed in some detail and it is found that the Lagrangian for its simplest variant can be developed into the Lagrangian of a restricted version of the Doebner-Goldin modification of this equation. It is also pointed out that a large class of particular models generated by this modification contradict the thesis that the homogeneity of the Schrödinger equation entails its weak separability. Electronic address...|$|R
