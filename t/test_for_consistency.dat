56|10000|Public
30|$|A {{pre-test}} of {{the questionnaire}} was administered to 20 respondents randomly selected from the mobile telephone companies, the Cronbach alpha was used to <b>test</b> <b>for</b> <b>consistency,</b> and the table below (Table 2) shows {{the results of the}} Cronbach alpha test.|$|E
40|$|This paper {{develops}} a non-parametric <b>test</b> <b>for</b> <b>consistency</b> of players' {{behavior in a}} series of games with the Quantal Response Equilibrium (QRE). The test exploits a characterization of the equilibrium choice probabilities in any structural QRE as the gradient of a convex function, which thus satisfies the cyclic monotonicity inequalities. Our testing procedure utilizes recent econometric results for moment inequality models. We assess our test using lab experimental data from a series of generalized matching pennies games. We reject the QRE hypothesis in the pooled data, but it cannot be rejected in the individual data for over half of the subjects...|$|E
40|$|Results are {{presented}} from an October 17, 1988, {{flight of the}} NASA ER- 2 stratospheric research aircraft. The flight sampled a nearly constant air mass at 20 km altitude, near 39 deg N latitude, from before sunrise until near noon. The instrumentation on board simultaneously measured NO, ClO, O 3, temperature, and pressure. The measurements are combined with modeled photodissociation coefficients and known reaction kinetics to infer abundances of other important species, {{and the results are}} compared to previous estimates as a <b>test</b> <b>for</b> <b>consistency</b> in the understanding of the photochemical processes governing the species distributions...|$|E
30|$|Data were <b>tested</b> <b>for</b> <b>consistency</b> with {{a normal}} {{distribution}} using the Ryan-Joiner test in Minitab v 16 (Minitab Inc., USA). Parametric or non-parametric methods were used as appropriate, {{as well as the}} method of Bland and Altman [33] and descriptive statistics.|$|R
30|$|The {{quality of}} the data was {{assessed}} by an independent clinical research assistant through data monitoring online. Systematic <b>tests</b> <b>for</b> <b>consistency</b> of the data were performed. Five per cent of the CRFs were randomly analysed. If discrepancies were > 5  % in a centre, all data registered for that centre were verified.|$|R
40|$|Two {{aggregation}} {{schemes for}} food demand systems are <b>tested</b> <b>for</b> <b>consistency</b> with the Generalized Composite Commodity Theorem (GCCT). One scheme {{is based on}} the standard CES classification of food expenditures. The second scheme {{is based on the}} Food Guide Pyramid. Evidence is found that both schemes are consistent with the GCCT. Demand and Price Analysis,...|$|R
40|$|Revealed {{preference}} theory o¤ers {{a criterion}} for decision-making quality: if decisions are high quality then {{there exists a}} utility function the choices maximize. We conduct a large-scale experiment to <b>test</b> <b>for</b> <b>consistency</b> with utility maximization. Consistency scores vary markedly within and across socioeconomic groups. In particular, consistency is strongly related to wealth: a standard deviation increase in consistency is associated with 15 - 19 percent more household wealth. This association is quantitatively robust to conditioning on correlates of unobserved constraints, preferences, and beliefs. Consistency with utility maximization under laboratory conditions thus captures decision-making ability that applies across domains and in-‡uences important real-world outcomes...|$|E
40|$|Revealed {{preference}} theory {{offers a}} criterion for decision-making quality: if decisions are high quality then {{there exists a}} utility function that the choices maximize. We conduct a large-scale field experiment that enables us to <b>test</b> <b>for</b> <b>consistency</b> with utility maximization. We find that high-income and high-education subjects display greater levels of consistency than low-income and low-education subjects, men are more consistent than women, and young subjects are more consistent than older subjects. We also find that consistency with utility maximization is strongly related to wealth: a standard deviation increase in standard consistency scores is associated with 15 - 19 percent more wealth...|$|E
30|$|Machine {{learning}} for detecting DoS {{attacks in the}} cloud was used by several researchers. This work benefits from many advanced machine learning and and artificial intelligence techniques to predict the status of VMs (i.e, malicious or normal). Lonea et al. [13] uses the normal traffic pattern received from the Virtual Bridge (VB) of the VM to validate for consistency against behavioral patterns of attacks. They use a network intrusion detection system, that analyzes the normal traffic flow obtained from the VB, to check and <b>test</b> <b>for</b> <b>consistency</b> against the attack behavioral patterns. Thus, if abnormal traffic has been detected, the anomaly information will be reported and an alarm generated.|$|E
40|$|Future {{redundant}} inequalities in a CLP(R) {{program are}} those that are guaranteed to be subsumed after no more than one subsequent procedure call, usually {{in the context of a}} recursive procedure. Inequalities must generally be <b>tested</b> <b>for</b> <b>consistency</b> with the collected constraint set and then added to it. However, future redundant inequalities need only be <b>tested</b> <b>for</b> <b>consistency,</b> thus resulting in dramatic savings in execution speed and space usage. We generalize the notion of future redundancy in a number of ways and thus broaden its applicability. We define the notion of partial future redundancy, which relaxes the subsumption requirement for procedure calls matching base cases. Furthermore, we argue that the latter class is a more natural class of programs to write. We also introduce the notion of return future redundancy, and describe optimizations in the form of source-to-source transformations taking advantage of these properties. Finally, we make some observations about [...] ...|$|R
40|$|A {{number of}} Constraint logic Programming systems, {{including}} CLP(R) and Prolog III, decide simultaneous linear inequalities {{as part of}} the fundamental operational step of constraint solving. While this can contribute tremendously to the usefulness of the systems, it is computationally quite expensive. Non-ground inequalities must generally be <b>tested</b> <b>for</b> <b>consistency</b> with the collected constraint set and then added to it, increasing its size, and thus making the next such test more expensive. Future redundant inequalities in a program are those that are guaranteed to be subsumed after no more than one subsequent procedure call, usually {{in the context of a}} recursive procedure. It has been noted that such inequalities need only be <b>tested</b> <b>for</b> <b>consistency</b> with the current constraint set, thus resulting in dramatic savings in execution speed and space usage. In this paper we generalize the notion of future redundancy in a number of ways and thus broaden its applicability. Thus we show how to d [...] ...|$|R
40|$|The nonparametric {{approach}} to consumer-demand analysis-based on revealed-preference axioms-is reviewed. Particular {{attention is paid}} to questions of size and power of <b>tests</b> <b>for</b> <b>consistency</b> of data with the existence of a stable, well-behaved utility function that could have generated the data. An application to Australian meat demand is used to show how these notions can be quantified and how prior information about elasticities, following Sakong and Hayes, may be used to increase the power of the approach. ...|$|R
40|$|The Nested Multinomial Logit (NMNL) {{model is}} used {{extensively}} in modeling consumer choices among discrete alternatives {{when the number}} of alternatives is large. Unfortunately, applied researchers often find that estimated NMNL models fail to meet the Daly-ZacharyMcFadden (DZM) sufficient conditions for consistency with stochastic utility maximization. Borsch-Supan (1990) provides a relaxed set of conditions to <b>test</b> <b>for</b> <b>consistency.</b> While these conditions are increasingly cited, they are seldom tested. This paper corrects and extends BorschSupan 2 ̆ 7 s Theorem 2, providing simple necessary conditions on first, second, and third derivatives of choice probabilities and a graph oft he bounds they place on dissimilarity parameters...|$|E
40|$|Abstract Background The {{repeated}} {{measures in the}} Framingham Heart Study in the Genetic Analysis Workshop 13 data set allow us to <b>test</b> <b>for</b> <b>consistency</b> of linkage results within a study across time. We compared regression-based linkage to variance components linkage across time for six quantitative traits in the real data. Results The variance components approach found 11 significant linkages, the regression-based approach found 4. There was only one region that overlapped. Consistency between exams generally decreased as the time interval between exams increased. The regression-based approach showed higher consistency in linkage results across exams. Conclusion The low consistency between exams and between methods may help explain the lack of replication between studies in this field. </p...|$|E
40|$|In {{order to}} {{determine}} the physical properties of the hottest and most luminous stars, and understand how these properties change as a function of metallicity, we have analyzed HST/UV and high S/N optical spectra of an additional 20 Magellanic Cloud stars, doubling the sample presented in the first paper in this series. Our analysis uses NLTE line-blanketed models that include spherical extension and the hydrodynamics of the stellar wind. In addition, our dataset includes FUSE observations of OVI and HST near-UV He I and He II lines to <b>test</b> <b>for</b> <b>consistency</b> of our derived stellar properties for a few stars. The results from the 1 Based on observations made with the NASA/ESA Hubble Space Telescope, obtained at the Spac...|$|E
40|$|The {{restriction}} of exogeneity of certain variables in structural VAR models is rarely <b>tested</b> <b>for</b> <b>consistency</b> {{with the actual}} data. The reason is obvious: such a test requires estimates of the structural parameters. This paper proposes a solution for models that assume long-run or contemporaneous recursive structures in identification. We show that in such cases, the exogeneity restriction can be assessed statistically using the well-known Granger non-causality test which is conveniently performed in the reduced-form VAR model. Two empirical examples are offered to demonstrate the usefulness of this result. ...|$|R
40|$|The {{handed down}} {{latitudinal}} data ascribed to Eratosthenes and Hipparchus are composed and each <b>tested</b> <b>for</b> <b>consistency</b> {{by means of}} adjustment theory. For detected inconsistencies new explanations are given concerning {{the origin of the}} data. Several inconsistent data can be ascribed to Strabo. Differences in Hipparchus' data can often be explained by the different types and precision of the data. Gross errors in Eratosthenes' data are explained by their origination from the lengths of sea routes. From Eratosthenes' data concerning Thule a numerical value for Eratosthenes' obliquity of the ecliptic is deduced. Comment: 31 pages, 6 figure...|$|R
30|$|Organized skepticism: all ideas must {{be tested}} and {{are subject to}} rigorous, {{structured}} community (peer review) scrutiny. Basically, this is the <b>test</b> <b>for</b> logical <b>consistency</b> and reliability. It is not, however, in Merton’s intentions, an invitation for total relativism.|$|R
40|$|Dark {{energy in}} General Relativity is {{typically}} non-interacting with other matter. However, {{it is possible}} that the dark energy interacts with the dark matter, and in this case, the dark matter can violate the universality of free fall (the weak equivalence principle). We show that some forms of the dark sector interaction do not violate weak equivalence. For those interactions that do violate weak equivalence, there are no available laboratory experiments to probe this violation for dark matter. But cosmology provides a test for violations of the equivalence principle between dark matter and baryons [...] via a <b>test</b> <b>for</b> <b>consistency</b> of the observed galaxy velocities with the Euler equation. Comment: 7 pages, 3 figures, references added, published in JCA...|$|E
40|$|Revealed {{preference}} theory {{offers a}} criterion for decision-making quality: if decisions are high quality then {{there exists a}} utility function the choices maximize. We conduct a large-scale experiment to <b>test</b> <b>for</b> <b>consistency</b> with utility maximization. Consistency scores vary markedly within and across socioeconomic groups. In particular, consistency is strongly related to wealth: a standard deviation increase in consistency is associated with 15 - 19 percent more household wealth. This association is quantitatively robust to conditioning on correlates of unobserved constraints, preferences, and beliefs. Consistency with utility maximization under laboratory conditions thus captures decision-making ability that applies across domains and influences important real-world outcomes. We thank Douglas Gale and Raymond Fisman for detailed comments and suggestions...|$|E
40|$|BACKGROUND. The {{repeated}} {{measures in the}} Framingham Heart Study in the Genetic Analysis Workshop 13 data set allow us to <b>test</b> <b>for</b> <b>consistency</b> of linkage results within a study across time. We compared regression-based linkage to variance components linkage across time for six quantitative traits in the real data. RESULTS. The variance components approach found 11 significant linkages, the regression-based approach found 4. There was only one region that overlapped. Consistency between exams generally decreased as the time interval between exams increased. The regression-based approach showed higher consistency in linkage results across exams. CONCLUSION. The low consistency between exams and between methods may help explain the lack of replication between studies in this field. Framingham Heart Study; National Heart Lung and Blood Institute (N 01 -HC- 25195...|$|E
40|$|This {{paper is}} devoted to {{revealed}} preference theory and its applications to <b>testing</b> economic data <b>for</b> <b>consistency</b> with utility maximization hypothesis, construction of index numbers, and forecasting. The quantitative measures of inconsistency of economic data with utility maximization behavior are also discussed. The structure of the paper is based on comparison between the two tests of revealed preference theory - generalized axiom of revealed preference (GARP) and homothetic axiom of revealed prefernce (HARP). We do this comparison both theoretically and empirically. In particular we assess empirically the power of these <b>tests</b> <b>for</b> <b>consistency</b> with maximization behavior {{and the size of}} forecasting sets based on them. For the forecasting problem we show that when using HARP there is an effective way of building the forecasting set since this set is given by the solution of the system of linear inequalities. The paper also touches upon the question of testing a set of Engel curves rather than finite set of observations <b>for</b> <b>consistency</b> with utility maximization behavior and shows that this question has effective solution when we require the rationalizing utility function to be positively homogeneous...|$|R
40|$|This thesis uses nonparametric {{revealed}} preference {{methods to}} derive new <b>tests</b> <b>for</b> <b>consistency</b> with models of consumer behaviour, {{and discuss the}} implications for welfare analysis. Chapter 1 demonstrates how to conduct revealed preference analysis when prices, and hence budget constraints, are only partially observed. This chapter extends the revealed preference results of Crawford and Polisson (2015), derived for the static case, to dynamic settings, allowing for storability of goods. Necessary and sufficient conditions <b>for</b> <b>consistency</b> with intertemporal models are derived, which do not require the researcher to distinguish between corner solutions and unavailability of the good, or to impute prices. Chapter 2 discusses the validity of using reported happiness measures as proxies of utility or social welfare, by <b>testing</b> <b>for</b> <b>consistency</b> between revealed and reported preference orderings in Japanese household survey data. Although the expenditure behaviour of most households is consistent with standard models of utility maximisation, it is generally inconsistent with the preference ordering given by their reported happiness. This inconsistency is likely due to reporting error in the happiness measure, and suggests that happiness and utility are empirically distinct and noninterchangeable. Chapter 3 investigates the effect of price inattention on inflation misperceptions and cost-of-living indices, by developing a behavioural model in which consumers only notice price changes above a certain threshold. A data application, using supermarket scanner data, demonstrates that this model generates plausible results; in particular, consumers have more accurate perceptions of inflation during periods of high or volatile inflation, but may substantially misperceive inflation when it is low. These results {{have important implications for}} conducting welfare analysis when consumers are not fully attentive to price changes. </p...|$|R
30|$|Our {{framework}} {{supports the}} integration of data from different sources so that analysts can understand conflicts and run ‘what-if’ scenarios for counterinsurgency scenarios. It also provides methodological support for scholars of insurgency in two ways. First, it allows for military theories, individually or synthetically, to be <b>tested</b> <b>for</b> <b>consistency</b> by exploring the implications of their suppositions. Second, it allows for intriguing empirical phenomena to be encountered and explored as the disjuncture between the actual world and the world contained within the model. We expect {{that the use of}} our framework for these different endeavours will further drive its evolution, both through changes in software and refinement of its mathematical structure.|$|R
40|$|Marschak (1953) {{suggested}} that applied research should begin by determining the minimal set of assumptions and data {{needed to make}} a prediction of interest. Standard identification analyses correspond poorly to this search, as they have either stringent data (local average treatment effects/IV and non-parametric) or structure (parametric) requirements. Yet, in this spirit, much empirical (Chetty, 2009) and some theoretical work has asked which effects of policy interventions may be forecast from the local observable levels and derivatives of the structure. I formalize this inquiry as the firstorder identification problem and illustrate it with the case of multi-product production. Under perfect competition or monopoly, but not standard oligopoly, Slutsky conditions for firm optimization provide strategies for identifying, for example, the effects of price controls. They also <b>test</b> <b>for</b> <b>consistency</b> of conjectural variations...|$|E
40|$|Noxious {{facilities}} and impacts are intuitively expected to affect residential property values (PV). This research analyzes {{the results of}} 69 PV studies in 10 categories of noxious facilities to <b>test</b> <b>for</b> <b>consistency</b> of significant, negative PV impacts. Waste facilities {{were one of the}} categories. Consistent PV impacts occur with airports and roads and where there are direct effects on air quality and visibility impacts, as well as landslide and flood zones as a result of observable, specified and quantified physical impacts. Property values decrease by between 2 and 22. 5 %. In contrast, PV impacts are inconsistent near nuclear plants, waste facilities, urban developments, electrical power plants and transmission lines. In order to clarify the reasons for these counterintuitive results, the facility characteristics and impacts perceived by prospective buyers must be quantified and correlated with PV changes...|$|E
40|$|Abstract. The weak {{instance}} {{model is}} a framework to consider the relations in a database as a whole, regardless of the way attributes are grouped in the individual relations. Queries and updates can be performed involving any set of attributes. The management of updates {{is based on a}} lattice structure on the set of legal states, and inconsistencies and ambiguities can arise. In the general case, the <b>test</b> <b>for</b> <b>consistency</b> and determinism may involve the whole database. In this paper it is shown how, for the highly signi cant class of independent schemes, updates can be handled e ciently, considering only the relevant portion of the database. Key words. Relational databases, weak instance model, lattice on database states, update operations, chase procedure, optimization AMS subject classi cations. 68 P 15, 68 Q 25, 05 A 0...|$|E
40|$|Small world network {{models have}} been {{effective}} in capturing the variable behaviour of reported case data of the SARS coronavirus outbreak in Hong Kong during 2003. Simulations of these models have previously been realized using informed "guesses" of the proposed model parameters and <b>tested</b> <b>for</b> <b>consistency</b> with the reported data by surrogate analysis. In this paper we attempt to provide statistically rigorous parameter distributions using Approximate Bayesian Computation sampling methods. We find that such sampling schemes are a useful framework for fitting parameters of stochastic small world network models where simulation {{of the system is}} straightforward but expressing a likelihood is cumbersome. Department of Applied MathematicsDepartment of Electronic and Information Engineerin...|$|R
40|$|The {{thermodynamics}} of solid-liquid equilibrium {{is applied}} to binary systems including a semicrystalline polymer and a solvent. Group-contribution activity coefficient models are used for describing the nonideal behavior of the liquid phase. The solid polymer properties are obtained via group-contribution methods and <b>tested</b> <b>for</b> <b>consistency.</b> Methods <b>for</b> the simultaneous solution of the solid-liquid and liquid-liquid equilibrium are presented. The full phase envelope of polymer solutions can be predicted, {{and the use of}} the empirical solubility parameter approach is avoided. The effects of polymer semicrystallinity and molecular weight on the solid-liquid equilibria are displayed, and a comparison of our predictions with experimental data is performed...|$|R
40|$|The {{current value}} of Vud is {{determined}} from superallowed beta-decay experiments. Other methods, briefly summarised here, {{have to overcome}} specific experimental hurdles before they are competitive. However, the nuclear results do depend on a nuclear-structure calculation of isospin-symmetry breaking, which is often a cause of some concern. We show here, by adopting the Conserved Vector Current (CVC) hypothesis, that these theoretical corrections can be <b>tested</b> <b>for</b> <b>consistency.</b> In this <b>test,</b> calculations based on the shell model with Saxon-Woods radial functions perform the best. Comment: Proceedings of CKM 2010, the 6 th International Workshop on the CKM Unitarity Triangle, University of Warwick, UK, 6 - 10 September 201...|$|R
40|$|This paper {{develops}} a formal <b>test</b> <b>for</b> <b>consistency</b> of players' {{behavior in a}} series of games with the quantal response equilibrium (QRE). The test exploits a characterization of the equilibrium choice probabilities in a QRE as the gradient of a convex function, which thus satisfies the cyclic monotonicity inequalities. Our testing procedure utilizes recent econometric results for moment inequality models. We assess the performance of the test using both Monte Carlo simulation and lab experimental data from a series of generalized matching pennies games. Our experimental findings are consistent with the literature: the joint hypothesis of QRE, risk neutrality and player role homogeneity is rejected in the pooled data, but cannot be rejected in the individual data for over half of the subjects. By considering subsets of cycle monotonicity inequalities, our approach also highlights the nature of QRE consistency violations...|$|E
40|$|The Census Bureau has {{proposed}} dropping the long census form from the 2010 census. Detailed data formerly {{collected on the}} long form would be collected in a very large ongoing survey: the American Community Survey (ACS). Full implementation of the ACS in three million households was scheduled to begin in 2003, {{but at this point}} may be postponed. The first national-level pilot test of the ACS was conducted with 700, 000 households over the calendar year of 2000. This ACS pilot test was called the Census 2000 Supplementary Survey, or C 2 SS. If the ACS is to replace the census long form, estimates for demographic characteristics that it collects should be consistent with estimates from the census. To <b>test</b> <b>for</b> <b>consistency,</b> staff compared estimates for demographic variables from the C 2 SS and Census 2000. Unexpectedly large differences were found betwee...|$|E
40|$|We {{estimate}} {{the degree to}} which the baryon density, Ω_b, can be determined from the galaxy power spectrum measured from large scale galaxy redshift surveys, and in particular, the Sloan Digital Sky Survey. A high baryon density will cause wiggles to appear in the power spectrum, which should be observable at the current epoch. We assume linear theory on scales ≥ 20 h^- 1 Mpc and do not include the effects of redshift distortions, evolution, or biasing. With an optimum estimate of P(k) to k∼ 2 π/(20 h^- 1 Mpc), the 1 σ uncertainties in Ω_b are roughly 0. 07 and 0. 016 in flat and open (Ω_ 0 = 0. 3) cosmological models, respectively. This result suggests that it should be possible to <b>test</b> <b>for</b> <b>consistency</b> with big bang nucleosynthesis estimates of Ω_b if we live in an open universe. Comment: 23 Pages, 10 Postscript figure...|$|E
40|$|In {{professional}} literature on statistics, a constantly growing interest is observed in non-parametric methods. Commonly these methods {{are based on}} counting, rank or position statistics and on a number or length of series. In this paper, the least popular tests, namely tests based {{on a number of}} empty cells, are presented. David-Hellwig test and a two-sample consistency test are considered. Empirical power of the tests is presented in comparison to classic tests: Kolmogorov and Shapio-Wilk <b>test</b> <b>for</b> <b>testing</b> normality of a distribution and t-Student’s and Wilcoxon <b>tests</b> <b>for</b> <b>testing</b> <b>consistency</b> of two distributions...|$|R
40|$|In {{the present}} study of the {{microwave}} and hard X-ray characteristics of 13 solar flares emitting microwave fluxes greater than 500 solar flux units, simultaneous 3 - 35 GHz and hard X-ray observations were conducted in the 30 - 500 keV energy range. An analysis is conducted {{to determine whether the}} same distribution of energetic electrons can explain both emissions; <b>tests</b> <b>for</b> any correlations between them yield results suggesting that optically thick microwave emission, near the peak frequency, originates in the same electron population that produces the hard X-rays. A single temperature model and a multitemperature model were <b>tested</b> <b>for</b> <b>consistency</b> with the coincident X-ray and microwave spectra at microwave burst maximum; neither model, however, attempts to explain the high frequency component of the microwave spectrum...|$|R
40|$|Timestamps {{stored on}} digital media play an {{important}} role in digital investigations. Unfortunately, timestamps may be manipulated, and also refer to a clock that can be erroneous, failing or maladjusted. This reduces the evidentiary value of timestamps. This paper takes the approach that historical adjustments to a clock can be hypothesized in a clock hypothesis. Clock hypotheses can then be <b>tested</b> <b>for</b> <b>consistency</b> with stored timestamps. A formalism for the definition and testing of a clock hypothesis is developed, and <b>test</b> methods <b>for</b> clock hypothesis <b>consistency</b> are demonstrated. With the number of timestamps found in typical digital investigations, the methods presented in this paper can justify clock hypotheses without having to rely on timestamps from external sources. This increases the evidentiary value of timestamps, even when the originating clock has been erroneous, failing or maladjusted...|$|R
