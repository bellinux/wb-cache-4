11|10|Public
30|$|Finally, {{we propose}} that the present {{methodology}} represents the seed of a potential tool for systematically comparing connectivity estimators. The {{reason for this is}} twofold: (a) the framework provides a standardized approach whereby comparisons can be made systematically and (b) may be used {{even in the absence of}} formally rigorous statistical criteria, i.e. even if only ad hoc decision rules are available and is therefore not restricted to methods with <b>theoretically</b> <b>well-established</b> detection criteria. We have future plans to include bootstrap-based connectivity detection schemes under the same standardized framework for comparison purposes.|$|E
40|$|This paper {{studies the}} {{relationship}} between union power, unemployment and wages. We find many <b>theoretically</b> <b>well-established</b> links suggesting that unemployment negatively affects bargaining power. As an empirical strategy we use Austrian data from 1966 until 2015 {{on the development of}} collectively bargained minimum wages. Such data directly reflects the result of a bargaining process between unions and employer organizations, and enables to show the link between bargaining power and unemployment. Moreover, it allows for further discussion on the potential influence of other macroeconomic and institutional variables, such as trade openness and union density...|$|E
40|$|In {{order to}} solve the problem of {{scheduling}} delay-constrain heterogeneous data traffic in wireless industrial network ， we propose the Traffic Aware Multiple Slotframes scheduling algorithm. It includes two main procedures: Compute available slot set and Allocate communication resources to each node. And then we give the detail description of the algorithm used in star topology and tree topology. We adopt the <b>theoretically</b> <b>well-established</b> graph theory methods of matching and coloring {{to solve the}} problem of communication resources allocation in tree topology. The experimental results show that the proposed method can achieve highly reliability and energy efficiency over other strategies that follows the new trend of developing green networking and support the emerging IoT application...|$|E
40|$|Small ion {{crystals}} in a Paul trap are stable even in {{the absence}} of laser cooling. Based on this <b>theoretically</b> and experimentally <b>well-established</b> fact we propose the following diagnostic criterion for establishing the presence of a crystallized beam: Absence of heating following the shut-down of all cooling devices. The validity of the criterion is checked with the help of detailed numerical simulations. Comment: REVTeX, 11 pages, 4 figures; submitted to PR...|$|R
40|$|Squeezed {{correlations}} of particle-antiparticle pairs were predicted to exist if the hadron masses were modified {{in the hot}} and dense medium formed in high energy heavy ion collisions. Although <b>well-established</b> <b>theoretically,</b> {{they have not yet}} been observed experimentally. We suggest here a clear method to search for such signal, by analyzing the squeezed correlation functions in terms of measurable quantities. We illustrate this suggestion for simulated phi-phi pairs at RHIC energies. Comment: Revised extended text, one plot added, one figure was moved to another page in the paper (total of 3 figures, Fig. (2) having parts a, b and c); added acknowledgmen...|$|R
40|$|A novel type of {{correlation}} involving particle-antiparticle pairs {{was found}} out in the 1990 ’s. Currently known as squeezed or Back-to-Back Correlations (BBC), they should be present if the hadronic masses are modified in the hot and dense medium formed in high energy heavy ion collisions. Although <b>well-established</b> <b>theoretically,</b> such hadronic correlations {{have not yet been}} observed experimentally. In this phenomenological study we suggest a promising way to search for the BBC signal, by looking into the squeezed correlation function of and K+K− pairs at RHIC energies, as function of the pair average momentum, K 12 =(k 1 + k 2) / 2. The effects of in-medium mass-shift on the identical particle correlations (Hanbury-Brown & Twiss effect) are also discussed...|$|R
40|$|AbstractTransient {{pulse method}} (TPM) {{originally}} introduced by Brace et al. is a <b>theoretically</b> <b>well-established</b> and widely used method. The advantages {{and limitations of}} this method for rock permeability measurement are discussed. The modified storage-adjustable TPM can develop the advantages and bypass the limitations so as to measure rock permeability in a large-scale range. The approximate solution is the preferred mathematic method because of the ease of data deduction. However, the systematic error associated with the approximate solution is involved with the limited situations of parameters β and γ. The design criteria are presented for optimizing the experimental configuration for a desired measuring precision and duration as well as ease of data deduction. Therefore, well application of the advanced storage-adjustable TPM can be prospected...|$|E
40|$|This work {{presents}} {{a new approach}} to the multiresolution modeling of polygonal meshes. This approach is based on the <b>theoretically</b> <b>well-established</b> fractal image compression techniques. A polygonal mesh is represented as a fractal using an iterated function system (IFS). In this way, a level of detail can be obtained over a region of the mesh by successively iterating the IFS. The main advantage is that it becomes possible to recover new levels of detail that were not present in the original mesh, so that the quality is not lost as the observer approaches the mesh. Another characteristic is that the same representation can be used over textures, and in this case the algorithm is directly implemented over the GPU. The visualization time obtained allows this new approach to be used in real-time interactive computer graphic applications...|$|E
40|$|In {{studies of}} {{interfaces}} with dynamic chemical composition, bulk and interfacial quantities are often coupled via surface conservation laws of excess surface quantities. While {{this approach is}} easily justified for microscopically sharp interfaces, its applicability {{in the context of}} microscopically diffuse interfaces is less <b>theoretically</b> <b>well-established.</b> Furthermore, surface conservation laws (and interfacial models in general) are often derived phenomenologically rather than systematically. In this article, we first provide a mathematically rigorous justification for surface conservation laws at diffuse interfaces based on an asymptotic analysis of transport processes in the boundary layer and derive general formulae for the surface and normal fluxes that appear in surface conservation laws. Next, we use nonequilibrium thermodynamics to formulate surface conservation laws in terms of chemical potentials and provide a method for systematically deriving the structure of the interfacial layer. Finally, we derive surface conservation laws for a few examples from diffusive and electrochemical transport...|$|E
40|$|Among {{the various}} {{frameworks}} for the description and modelling of reactive systems, process algebra [2] plays a prominent role. It {{has proved to}} be suitable, e. g., at the level of requirement specification, at the level of design specification, and also for the formal analysis of systems. In this context, the process algebra CSP [10, 22, 11] provides a <b>well-established,</b> <b>theoretically</b> thoroughly studied, and in industry applied formalism for the modelling and verification of concurrent systems. Its applications range from train control systems, see e. g. [5, 9], over software for the international space station [3, 4] to the verification of security protocols, see e. g. [23]. In 2005, the CSP community celebrated its birthday with the workshop ”Communicating Sequential Processes, the first 25 years ” [1]. From its beginning on, CSP came along with simulatio...|$|R
40|$|The hadronic {{correlation}} among particle-antiparticle pairs was {{highlighted in}} the late 1990 ’s, culminating with the demonstration that it should exist if the masses of the hadrons were modified in the hot and dense medium formed in high energy heavy ion collisions. They were called Back-to-Back Correlations (BBC) of particle-antiparticle pairs, also known as squeezed correlations. However, {{even though they are}} <b>well-established</b> <b>theoretically,</b> such hadronic correlations have not yet been experimentally discovered. Expecting to compel the experimentalists to search for this effect, we suggest here a clear way to look for the BBC signal, by constructing the squeezed correlation function of φφ and K + K − pairs at RHIC energies, plotted in terms of the average momentum of the pair, K 12 = 1 2 (k 1 + k 2), inspired by procedures adopted in Hanbury-Brown & Twiss (HBT) correlations...|$|R
40|$|Squeezed {{correlations}} of particle antiparticle pairs, also called Back-to- Back Correlations, are predicted to appear if the hadron masses are modified {{in the hot}} and dense hadronic medium formed in high energy nucleus-nucleus collisions. Although <b>well-established</b> <b>theoretically,</b> the squeezed-particle correlations {{have not yet been}} searched for experimentally in high energy hadronic or heavy ion collisions, clearly requiring optimized forms to experimentally search for this effect. Within a nonrelativistic treatment developed earlier we show that one promising way to search for the BBC signal is to look into the squeezed correlation function of pairs of ’s at RHIC energies, plotted in terms of the averagemomentum of the pair, K 12 = 1 2 (k 1 + k 2). This variable’s modulus, 2 |K 12 |, is the non-relativistic limit of the variable Qbbc, introduced herewith. The squeezing effects on the HBT correlation function are also discussed...|$|R
40|$|Whilst {{the child}} quantity-quality (QQ) model is <b>theoretically</b> <b>well-established,</b> the {{empirical}} literature offers only partial support. Motivated by the limited causal empirical evidence in both historic and contemporary societies, {{this study examines}} the relationship connecting fertility and child quality for individual families in England and Wales {{at the start of}} the 20 th century. Using data from the 1911 census returns, I estimate whether reductions in family size reduce the probability of leaving school. To account for the endogenous nature of fertility decisions, I use the sex composition of the first two births in families with at least two children as an instrumental variable (IV) for family size. Overall, I find evidence in support of a child QQ effect, as children in the 13 - 15 age cohort born into smaller families were more likely remain in school. Whilst the IV results are very similar to the non-IV ones, one drawback is that the IV estimates are quite imprecise...|$|E
40|$|The {{evolution}} of large-scale magnetic fields in disk galaxies is investigated numerically. The gasdynamical simulations in a disk perturbed by spiral or bar potential {{are incorporated into}} the kinematic calculations of induction equations to elucidate the effects of non-axisymmetric disk structure on magnetic fields. The effects of interstellar turbulence are given as the turbulent diffusion of magnetic fields. The usually adopted dynamo mechanism of alpha-effect is not considered in our computations, {{because it is not}} obvious about the actual existence of the effect in a galaxy. Our principal concern is to clear how observationally and <b>theoretically</b> <b>well-established</b> gas flow affects the magnetic-field structure and evolution, without putting a lot of artificial parameters in the model. We have found that the density-wave streaming motion of gas has a significant influence on the distribution of magnetic fields: the lines of force are well aligned with spiral arms due to the compressional and additional shearing flow of gas in these regions. Comment: 14 pages, latex documentstyle {l-aa...|$|E
40|$|We {{address the}} {{relationship}} between educational attainment and radical right voting (i. e., voting for the PVV) in the Netherlands. We tested whether lower educated people are overrepresented among the electorate of the PVV - as often found in earlier research - and considered underlying explanations for this relationship. Using data derived from the Religion in Dutch Society (SOCON, 2011 / 2012) survey, {{we were able to}} empirically test a set of innovative mediators (e. g., interethnic contact, euroscepticism, associational involvement and social trust) simultaneously next to <b>theoretically</b> <b>well-established</b> mediators (e. g., perceived ethnic threat, nationalistic attitudes and authoritarianism). Our results indicated that lower educated {{people are more likely to}} cast their vote for the PVV than higher educated people, due to their level of perceived ethnic threat, anti-Muslim attitudes and authoritarianism. Using bootstrapping, only ethnic threat perceptions turned out to significantly mediate {{the relationship between}} educational attainment and radical right voting, ruling out many other explanations. Our findings underline the importance of precluding spurious influences when addressing radical right voting and show that radical right parties' emphasis on the economic and cultural threats that immigrants would pose for Western societies seems to bear fruit in terms of mobilizing lower educated people, at least among the Dutch electorate...|$|E
40|$|Squeezed {{correlations}} of particle-antiparticle pairs, also called Back-to-Back Correlations, are predicted to appear if the hadron masses are modified {{in the hot}} and dense hadronic medium formed in high energy nucleus-nucleus collisions. Although <b>well-established</b> <b>theoretically,</b> the squeezed-particle correlations {{have not yet been}} searched for experimentally in high energy hadronic or heavy ion collisions, clearly requiring optimized forms to experimentally search for this effect. Within a non-relativistic treatment developed earlier we show that one promising way to search for the BBC signal is to look into the squeezed correlation function of pairs of phi-mesons at RHIC energies, plotted in terms of the average momentum of the pair, K 12 =(k 1 +k 2) / 2. This variable's modulus, 2 |K 12 |, is the non-relativistic limit of the variable Q_bbc, introduced herewith. The squeezing effects on the HBT correlation function are also discussed. Comment: 4 pages, 2 figures. To be published in the Proceedings of Quark Matter 2008, Jaipur, Indi...|$|R
40|$|The hot {{and dense}} medium formed in high energy heavy ion {{collisions}} may modify some hadronic properties. In particular, if hadron masses are shifted in-medium, it was demonstrated {{that this could}} lead to back-to-back squeezed correlations (BBC) of particle-antiparticle pairs. Although <b>well-established</b> <b>theoretically,</b> the squeezed correlations {{have not yet been}} discovered experimentally. A method has been suggested for the empirical search of this effect, which was previously illustrated for phi-phi pairs. We apply here the formalism and the suggested method to the case of K^+ K^- pairs, since they may be easier to identify experimentally. The time distribution of the emission process plays {{a crucial role in the}} survival of the BBC's. We analyze the cases where the emission is supposed to occur suddenly or via a Lorentzian distribution, and compare with the case of a Levy distribution in time. Effects of squeezing on the correlation function of identical particles are also analyzed. Comment: 9 pages and 6 figures (figures 2 to 6 contain 4 plots each). Paragraph added to text, figures 2 to 6 revised for improving visualizatio...|$|R
40|$|This {{thesis is}} a {{collection}} of experimental and theoretical studies on social norms and cooperation. The first two chapters focus on effects of social norms on people's behaviour in strategic situations. The Golden Rule is first studied in an ultimatum bargaining experiment. The results show that while most people follow the Golden Rule in the ultimatum game situation, experience and feedback of playing the opposite role has an important effect on golden-rule behaviour. Then the link between people's expectation of social norms and their own behaviour is studied in an experiment of a trust game. Only about half of the subjects show a consistent behaviour according to their own expected norm. Moreover, experience and feedback has asymmetric effects on the behaviour of trustors and trustees. In next two chapters, the way people cooperate and how to sustain a more efficient cooperative result are studied by using both theoretical and experimental methods. I first experimentally explore the mechanisms that make people more willing to cooperate and increase the overall welfare in a public goods game. Then I <b>theoretically</b> study a <b>well-established</b> cooperative solution for the bankruptcy problem and design a non-cooperative game that gives the solution as the unique sub-game perfect equilibrium outcome. ...|$|R
40|$|The thesis {{concerns}} a numerical {{implementation of the}} Parquet summation of diagrams within Green's functions theory applied to calculations of nuclear systems. The main motivation has been to investigate whether {{it is possible to}} develop this approach to a level comparable in accuracy and reliability to other ab initio nuclear structure methods. The Green's functions approach is <b>theoretically</b> <b>well-established</b> in many-body theory, but to our knowledge, no actual application to nuclear systemshas been previously published. It has a number of desirable properties, foremostthe gently scaling with system size compared to direct diagonalization and the closeness to experimentally accessible quantities. The main drawback is the numerical instabilities due to the pole structure of the one-particle propagator, leading to convergence difficulties. This issue {{is one of the main}} focal points of the work presented in this thesis, and strategies to improve the convergence properties are described and investigated. We have applied the method both to a simple model which can be solved by exact diagonalization and to the more realistic 4 He system. The results shows that our implementation is close to the exact solution in the simple model as long as the interaction strengths are small. As the number of particles increases, convergence is increasingly hard to obtain. In the 4 He case, we obtain results in the vicinity of the results from comparable approaches. The numerical in-stabilities in the current implementation still prevents the desired accuracy and stability necessary to achieve the current benchmark standards...|$|E
40|$|Geographical {{epidemiology}} encapsulates {{those problems}} that aim to understand space and/or space time {{trends in the}} disease(s) of interest, a goal clearly important in both public health and economic contexts. The increasing availability of point-location, coordinate data for studies in geographical epidemiology calls for a greater scrutiny of statistical point process theory with respect to these applications. Though the statistical analysis of planar point patterns is now <b>theoretically</b> <b>well-established,</b> there remain many aspects which warrant further research, especially from a practical perspective. The need becomes even greater {{when we consider the}} relative youth of methods for the analysis of spatiotemporal observations, where non-trivial variation and even dependence throughout the space-time continuum can exist. This work aims to address these issues by careful review, theoretical refinement, empirical testing, and real-world analyses of certain statistical tools used in point process problems in geographical epidemiology. We scrutinise the kernel-smoothed density-ratio estimator of the so called relative risk function, a particularly flexible approach given the anticipated spatial heterogeneity of the observations over a given geographical region. This discussion introduces the adaptive (i. e. variable bandwidth) risk function, as well as novel asymptotic methods for computation of tolerance contours designed to identify sub-regions of statistically significant fluctuations in risk. More sophisticated statistical methodology is warranted in certain situations, where it may be assumed that both ‘global’ heterogeneity and ‘local’ correlation drives the space and/or space-time disease dispersion. A comprehensive review of the stochastic log-Gaussian Cox process, in both purely spatial and spatiotemporal contexts, is conducted. A suite of novel numerical experiments investigate the performance of convenient, yet ad hoc, minimum contrast parameter estimation techniques for the dependence structure of the latent Gaussian process. The computer code arising from the review and refinement of the above methodologies was instrumental in the release of two separate software packages. These are available in the R environment, and also showcased here. A number of additional collaborations with applied researchers around the world serve to further highlight the contributions made throughout the course of this research project and the importance of sound statistical methods in geographical epidemiology...|$|E
40|$|Interactions between {{surface and}} {{groundwater}} systems are <b>well-established</b> <b>theoretically</b> and observationally. While numerical models that solve both surface and subsurface flow equations {{in a single}} framework (matrix) are increasingly being applied, computational limitations have restricted their use to local and regional studies. Regional or watershed-scale simulations have been effective tools for understanding hydrologic processes; however, {{there are still many}} questions, such as the adaptation of water resources to anthropogenic stressors and climate variability, that can only be answered across large spatial extents at high resolution. In response to this grand challenge in hydrology, we present the results of a parallel, integrated hydrologic model simulating surface and subsurface flow at high spatial resolution (1 km) over much of continental North America (~ 6. 3 M km 2). These simulations provide integrated predictions of hydrologic states and fluxes, namely, water table depth and streamflow, at very large scale and high resolution. The physics-based modeling approach used here requires limited parameterizations and relies only on more fundamental inputs such as topography, hydrogeologic properties and climate forcing. Results are compared to observations and provide mechanistic insight into hydrologic process interaction. This study demonstrates both the feasibility of continental-scale integrated models and their utility for improving our understanding of large-scale hydrologic systems; the combination of high resolution and large spatial extent facilitates analysis of scaling relationships using model outputs...|$|R

