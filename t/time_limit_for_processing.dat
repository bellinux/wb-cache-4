2|10000|Public
40|$|The audit {{examined}} {{the extent to}} which the 11 Victorian public sector (VPS) departments and Victoria Police meet the requirements of the Freedom of Information Act 1982. Freedom of Information (FOI) is a cornerstone of a thriving democracy. Since FOI legislation was introduced 30 years ago, Victoria has gone from being at the forefront of FOI law and administration to one of the least progressive jurisdictions in Australia. Over time, apathy and resistance to scrutiny have adversely affected the operation of the Act, restricting the amount of information being released. As a result, agencies are not meeting the object of the Act, which is ‘to extend as far as possible the right of the community to access information’. The public’s right to timely, comprehensive and accurate information is consequently being frustrated. The VPS’s systemic failure to support this right is a failure to deliver Parliament’s intent. The prevailing culture and lack of transparent processes allow principal officers—secretaries and chief executive officers of agencies—to avoid fulfilling their responsibilities. Principal officers are not being held to account for their agency’s underperformance and non-compliance. Agencies are routinely disregarding the 45 - day statutory <b>time</b> <b>limit</b> <b>for</b> <b>processing</b> requests and the five-day ministerial noting period, and there are serious flaws in record keeping practices and FOI searches in the Department of Human Services and Victoria Police. The Department of Justice has not satisfactorily fulfilled its role as lead agency for FOI. More effective leadership is required to promote an appropriate culture, improve transparency of government information and adequately inform Parliament and the community about FOI. Image: alles-schlumpf / flick...|$|E
40|$|The {{free speech}} {{campaign}} by {{a coalition of}} major media organisations has put freedom of information on the electoral agenda. Kevin Rudd has promised improved access to government documents if Labor wins office and the Coalition has indicated that it too will have “something to say” about the issue before polling day. The proof will be in the post election pudding but ABC Radio National’s Peter Mares lives in hope, since his recent experience of FOI suggests there is plenty of scope for reform. IN MID-SEPTEMBER I interviewed communications minister Senator Helen Coonan on ABC Radio National about the alarming statistics used in the government’s $ 22 million NetAlert information campaign. Statistics like this one: “over half of 11 -to- 15 year olds surveyed who chat online are contacted by strangers. ” I had seen the statistic on a billboard at a tram stop. As a parent I was concerned. As I journalist I was suspicious. As both parent and journalist I wanted to know more. I went to the NetAlert website and downloaded the booklet “Protecting Australian families on line” (subsequently sent to every household in Australia). On page 8, I found a statistic similar to the one I had seen at the tram stop: “In a recent study, 40 per cent of children who chat online said they had been contacted by someone they didn’t know. ” This statistic was supported by a footnote referring to a survey: “A Snapshot of Online Behaviour and Attitudes of Children,” Wallis Consulting Group, July 2007. Assuming this to be published research, I searched for the document online. After failing to locate it, I rang the Wallis Consulting Group to ask for a copy, only to be told that the research had been commissioned by the government and could not be made public without official permission. I rang the NetAlert 1800 information line, but the man who answered my call had no idea what I was talking about. I turned to the Australian Communications Media Authority, which manages the NetAlert infoline and was directed from there to the Department of Communications, Information Technology and the Arts (DCITA). A helpful public affairs official emailed me a four page summary of the research that had been made public by the minister. But it did not contain the information I sought. In fact it raised more questions than answers because the data in the summary did not appear to match either of the two statistics quoted above. I went back to DCITA and asked again for the full report. I was referred to the minister’s office. Her media minder told me the document could not be made public because it contained sensitive personal information about individual children who been stalked or otherwise abused online. This was stretching credibility. The research summary released by the minister clearly identifies the study by the Wallis Consulting Group as a survey. The NetAlert booklet sent to households does include case studies (with names changed) but these are all drawn from “the NetAlert help line or the Australian Federal Police” - not from the Wallis Consulting Group survey report (which is cited separately). I then interviewed the minister on The National Interest on ABC Radio National and asked her for the data to back up the alarming NetAlert statistics. In a robust exchange, Senator Coonan promised to consider releasing more of the research, given the issues I had raised. When I followed up with her office a few days later the response was a frosty “we’ll get back to you. ” Anticipating (correctly as it turned out) that no one would “get back to me,” I took the additional step of lodging a freedom of information request. For speed and convenience I tried to make my request by phone or email but this proved impossible. Every FOI application attracts a $ 30 application fee that must be paid in advance. The only way to pay is with a cheque sent in the mail, because the department lacks credit card facilities or any mechanism for electronic payment. I posted off my $ 30 cheque, still optimistic that I might get access to the research prior to the federal election since, under the Act, the department must respond to an FOI request within 30 days. After all, the document in question was clearly identified in a government brochure sent to every household in Australia, so it could not be too difficult to locate in departmental files. I reasoned that the material was unlikely to be exempt for reasons of commercially sensitivity or cabinet confidentiality, and surely it {{was not the kind of}} material that would “cause damage to relations between the Commonwealth and a State. ” On this basis, I assumed access would be straightforward. I was wrong. Three weeks after submitting my request I received a response. I was informed that under Sections 27 and 15 (6) of the FOI Act the <b>time</b> <b>limit</b> <b>for</b> <b>processing</b> my application had been extended (doubled) from 30 days to 60 days. This was because my request necessitated consultation with a third party “regarding release of documents concerning their business affairs” (presumably the Wallis Consulting Group). I was also told that the cost of my FOI request would be $ 138. 72 cents. This was in addition to the $ 30 application fee and comprised $ 4 worth of photocopying, $ 15. 25 for search and retrieval time (1. 02 hours at $ 15 per hour) and $ 119. 46 for “decision making time” (5. 97 hours at $ 20 per hour). Just who was making decisions and about what, I’m not quite sure. I was told that if I intended to pursue my application, then I would need to provide a deposit of 25 per cent of these costs ($ 34. 68). Again, this could only be paid with a cheque sent in the mail. And until the money arrived, my FOI application was on hold. In other words, each day of delay extended the statutory time limit for the department to respond to my request. I sent off a cheque immediately. Unfortunately the mail must have been running slow that week, because my payment took six days to wend its way from Melbourne to the correct desk in Canberra. And so in a subsequent letter, I was informed by that “the new date for notifying a decision” regarding access to the documents I had requested was the 29 th of November, five days after the federal election. I was also informed that I my request for a refund of the original $ 30 application fee had been turned down. I had requested the fee be remitted on the basis that the research underpinning a $ 22 million public information campaign was clearly a matter of “public interest” (as allowed under the Act). Unfortunately I had failed to say what I would actually do with the research if and when I received it. I had assumed - naively - that my intentions were obvious. I had stated that I was making the request in my professional capacity as a journalist and thought it self-evident that I planned to report on the information revealed to me. However, this was not apparent to the “responsible decision maker” (a departmental assistant secretary). She rejected my application to remit the application fee on the basis that I had “not indicated whether the information, if released, would actually come to the attention of the public. ” It was “not inconceivable,” she argued, that the “particular information if released, may not actually be brought to the public’s attention because, for example the information may not be considered to be particularly newsworthy. ” There is a catch 22 here. To qualify for a waiver of the $ 30 FOI application fee, I must demonstrate that release of the documents is “in the general public interest or in the interest of a substantial section of the public. ” The proof rests on an undertaking that I will report on the contents of the documents after they are released to me. Yet since I do not know in advance what the documents contain, I cannot plausibly guarantee that they will prove “sufficiently newsworthy” to warrant editorial space. The kicker is that the longer my FOI application takes, the less newsworthy the documents are likely to become. I could have asked for an “internal review” of the decision not to remit my $ 30, but that would have attracted another $ 40 application fee. And frankly, life is too short. • Peter Mares presents The National Interest every Sunday at noon on ABC Radio National. This article first appeared in the Age...|$|E
40|$|Online {{education}} poses big challenges, {{especially in}} distance education, where students must adapt to self-directed learning, {{teachers need to}} adopt {{a different way of}} delivering educational contents and developing knowledge, and course coordinators have to cope with high student numbers and strict <b>time</b> <b>limits</b> <b>for</b> <b>processing</b> all the information originated in the learning space. Social learning analytics provides tools and methods for extracting information that is useful for improving the learning process. This case study shows how instructors and course coordinators can use the tool Gephi to generate relevant information that would otherwise be difficult to gain. Analysis of empirical data from a cross-curricular course with 656 students proves the usefulness of Gephi for social learning analytics studies and demonstrates how the tool can provide relevant indicators of student activity and engagement. The study also discusses the potential of social learning analytics for improving online instruction via learning data visualization...|$|R
5000|$|According to {{the latest}} Doing Business Report, Greece is among the 10 economies {{of the world that}} showed the largest {{improvement}} of business climate in 2011/12. It ranks 78 in the Ease of doing business index in 2012, a big step forward compared to the previous year when it ranked 100, a bigger leap in improving its regulatory environment than in any of the previous six years. The authors of the report note that the reasons for Greece's good performance were the implementation of regulatory reforms in the following three areas: 1) It [...] "reduced the time required to obtain a construction permit by introducing strict <b>time</b> <b>limits</b> <b>for</b> <b>processing</b> permit applications at the municipality". 2) It [...] "strengthened investor protections by requiring greater immediate and annual disclosure of material related-party transactions", and 3) it [...] "enhanced its insolvency process by abolishing the conciliation procedure and introducing a new rehabilitation proceeding".|$|R
5000|$|Extensive {{changes were}} also {{made to the}} {{possibilities}} for the legal remedy of the failure to observe various <b>time</b> <b>limits</b> within the European patent system. Most particularly, the failure to pay various fees (filing fee, designation fee, search fee and examination fee) were, until the coming into force of the EPC 2000, excluded from the standard legal remedies of further processing [...] and re-establishment of rights. Under the EPC 1973, the failure to observe the <b>time</b> <b>limits</b> <b>for</b> paying these fees could only be remedied by strict grace periods. These grace periods have now {{been replaced by the}} application of further processing [...] and, if the further processing <b>time</b> <b>limit</b> is missed, re-establishment of rights, which although not directly applicable to the <b>time</b> <b>limit</b> missed originally, is applicable to the failure to observe the <b>time</b> <b>limit</b> <b>for</b> further <b>processing.</b>|$|R
40|$|Abstract. Existing work on multiway spatial joins {{focuses on}} the {{retrieval}} of all exact solutions with no <b>time</b> <b>limit</b> <b>for</b> query <b>processing.</b> Depending on the query and data properties, however, exhaustive processing of multiway spatial joins can be prohibitively expensive due to the exponential nature of the problem. Furthermore, if there do not exist any exact solutions, {{the result will be}} empty even though there may exist solutions that match the query very closely. These shortcomings motivate the current work, which aims at the retrieval of the best possible (exact or approximate) solutions within a time threshold, since fast retrieval of approximate matches {{is the only way to}} deal with the ever increasing amounts of multimedia information in several real time systems. We propose various techniques that combine local and evolutionary search with underlying indexes to prune the search space. In addition to their usefulness as standalone methods <b>for</b> approximate query <b>processing,</b> the techniques can be combined with systematic search to enhance performance when the goal is retrieval of the best solutions. 1...|$|R
40|$|Abstract The British {{took over}} Tanganyika from the Germans in 1919 after the First World War. In facilitating colonial {{economic}} policies the British Colonial Government enacted Land Ordinance Cap 113 of 1923 and Land Acquisition Ordinance Cap 118 of 1926. These laws facilitated {{the acquisition of}} native lands and considerably changed the way expropriation was handled leaving behind permanent marks on the later practice. The colonial practice exposed the inner most economic intents of the British government. Use of legal phrases like for public purpose embedded in the ordinance had multiple legal interpretations and loose definition befitting the colonial economic cravings of the time. Although major provisions of the colonial acquisition laws {{are reflected in the}} later laws evidence suggests that a few elements of colonial expropriation practices have also sneaked in as silent laws of expropriation but others have not. Quite a few practices had been deliberately discarded or inadvertently forgotten for lack of a political will to purify and emulate them or due to lack of good record keeping. Using historical data and archival records from the Tanzania National Archives this study explored colonial expropriations mainly by focusing on the principles laws practices and procedures used. The main objective {{of the study was to}} identify good practices used during the said era the intended and unintended consequences of these unreported practices especially those which need to be emulated by the current laws and practices. The study concluded by shedding light on bad practices which are being exercised to date uncritically but also acknowledging good colonial expropriation practices which existed then but could be emulated. First the study insists that public purpose clause in expropriation must be affected with good and fair intentions and a mechanism to check this be set. Secondly PAPs involvement in land acquisition and compensation negotiations should be codified into laws. Thirdly adequacy of compensation should be improved by legalising payment of solatium on top of the basic compensation amounts. Fourthly the practice ought to institute financial ability to develop a plot as a basic prerequisite for obtaining an alternative plot during expropriation. Lastly there is a need to institutionalise statutory <b>time</b> <b>limits</b> <b>for</b> <b>processing</b> land acquisition and compensation claims and the time needed to remove PAPs from lands so acquired...|$|R
40|$|In many {{intelligence}} agencies, {{the processing}} of data into usable information ready for analysis poses a significant bottleneck. Typically, much more data is available than what can be processed in the <b>limited</b> <b>time</b> available <b>for</b> <b>processing.</b> We formulate the problem faced by an intelligence collection unit, when processing incoming raw information for delivery to intelligence analysts, as an exploration-exploitation problem: the processor has to choose between exploring for new sources of relevant information and exploiting known sources. To address the exploration-exploitation problem, we develop a mathematical model of the processor's knowledge and examine algorithms that allow the processor to maximize the discovery of relevant data given a <b>time</b> <b>limit.</b> We derive insights {{on the performance of}} different algorithms using a simulated case study. Israel Defense Forces autho...|$|R
40|$|Inexact {{retrieval}} of multiway spatial joins {{refers to the}} cases where (i) there is <b>limited</b> <b>time</b> <b>for</b> query <b>processing</b> and {{the goal is to}} retrieve the best possible solutions within this limit (ii) there is unlimited time and the goal is to retrieve a single exact solution, if such a solution exists, or the best approximate one otherwise. The first case is motivated by the high cost of join processing in realtime systems involving large amounts of multimedia data, while the second one is motivated by applications that require "negative" examples. This papers proposes several algorithms <b>for</b> query <b>processing</b> under theses conditions. For the limited-time case we develop some non-deterministic search heuristics that can quickly retrieve good solutions. However, these heuristics are not guaranteed to find the best solutions, even without a <b>time</b> <b>limit.</b> Therefore, <b>for</b> the unlimited-time case we propose systematic search algorithms tailored specifically for the efficient {{retrieval of}} a [...] ...|$|R
40|$|The {{purpose of}} the paper is to {{investigate}} the effects of unconscious versus conscious ways of making decisions in a dynamic decision-making task. An experimental setting is used to study this question; three experimental groups are distinguished: immediate decision-making (only <b>limited</b> <b>time</b> <b>for</b> cognitive <b>processing),</b> considered decisionmaking (time <b>for</b> conscious <b>processing),</b> and distracted decision-making (time <b>for</b> unconscious <b>processing).</b> As experimental stimulus, a simulator based on the Kaibab Plateau model is employed. Although more than 100 subjects have been tested so far, group differences are not significant for most data examined. Implications comprise {{the improvement of the}} experiment in order to derive at more substantial results. The value of the paper {{lies in the fact that}} it connects to a recent discussion in psychology and transfers it into a domain in the core interest of the system dynamics community: decision-making in situations with dynamic complexity...|$|R
40|$|This paper {{deals with}} multiway spatial joins when (i) there is <b>limited</b> <b>time</b> <b>for</b> query <b>processing</b> {{and the goal}} is to {{retrieve}} the best possible solutions within this limit (ii) there is unlimited time and {{the goal is to}} retrieve a single exact solution, if such a solution exists, or the best approximate one otherwise. The first case is motivated by the high cost of join processing in real-time systems involving large amounts of multimedia data, while the second one is motivated by applications that require `negative' examples. We propose several search algorithms <b>for</b> query <b>processing</b> under theses conditions. For the limited-time case we develop some non-deterministic search heuristics that can quickly retrieve good solutions. However, these heuristics are not guaranteed to find the best solutions, even without a <b>time</b> <b>limit.</b> Therefore, <b>for</b> the unlimited-time case we describe systematic search algorithms tailored specifically for the efficient retrieval of a single solution. Both types of algorithms are integrated with R-trees in order to prune the search space. Our proposal is evaluated with extensive experimental comparison...|$|R
40|$|A general-purpose, {{real-time}} Process Manager (PM) {{designed to}} control the execution of “AI ” processes in {{many different kinds of}} computer game is presented. It is viewed as a first step towards standardising AI execution for games, which is a prerequisite for AI hardware acceleration. The PM supports “egocentric ” AI processing, which is an approach that favours believability over accuracy of simulation when constructing a virtual world for entertainment purposes. The PM can maintain, {{from the point of view}} of a human player, the “ego”, a constant frame-rate at the expense of AI processing under variable AI loads. A game that uses the Process Manager will appear to run smoothly even if the time required to fully process currently active objects exceeds the time available to update and draw a single frame. The PM user can specify a global upper <b>time</b> <b>limit</b> <b>for</b> AI <b>processing,</b> and the PM will attempt to ensure that this limit is never exceeded. A fuzzy upper limit and graceful degradation of AI performance replaces a sharp cut-off point for loss of frame rate. In consequence, the PM reduces the need for manual optimisation of AI code and thereby facilitates the implementation of more populated and interaction-rich virtual worlds. The PM also supports (a) simulated, multi-threaded AI process execution, (b) specification of the run-time frequency of AI processes, (c) suspendable and sliceable “behavioursets”, (d) automatic process interleaving and activation delay to minimise per-frame CPU load, and (e) user-specified conditions that define the relative “importance ” of AI processes that is used to determine which processes to postpone under high loads. The design of the PM, its useful properties, and a prototype implementation on Sony Computer Entertainment’s PlayStation 2 hardware is described. Results from testing the Process Manager in a simple problem domain are analysed...|$|R
40|$|Configuration {{similarity}} is {{a special}} form of content-based image retrieval that considers relative, object locations. It {{can be used as}} a standalone method, or to complement retrieval based on visual or semantic features. The corresponding queries ask for sets of objects, that. satisfy some spatio-temporal constraints, e. g., "find all,triplets of objects (nu(1), nu(2), nu(3)), such that, nu(1) is northeast of nu(2), which is inside nu(3). " Exhaustive processing (i. e., retrieval of the best solutions) of configuration similarity queries, in general, has exponential complexity and fast search for sub-optimal solutions is the only way. to deal with the vast amounts of multimedia information in several real-time applications. In this paper we first discuss the utilization of nonsystematic search heuristics, based on genetic algorithms, simulated annealing and hill climbing approaches. An extensive experimentation. with real and synthetic datasets. reveals that hill climbing techniques are the best for the current problem; therefore, as a subsequent step we study the search space, and, develop improved variations of hill climbing that take advantage, of the. special structure. of the problem to enhance speed. The, proposed heuristic methods significantly outperform systematic search when them is only <b>limited</b> <b>time</b> <b>for</b> query <b>processing...</b>|$|R
40|$|In the {{theoretical}} {{part of the}} diploma the attention paid to the preparation of polymer nanoparticles and the properties of polymeric nanoparticles. Work also focuses on various form of colloidal nanocarriers. The focus of the diploma lies in the experiment. The main aim of the thesis was to prepare detailed study of influence of a very low concentrations of anionic sodium laurylsulfate and cationic cetyltrimethylamonium bromide on size and zetapotential of nanoparticles prepared by emulsionsolvent - distribution method from terpolymer of DL-lactid acid, glycolic acid and tripenterthritol. It was shown that even at 0, 05 % concentration of the emulsifier biodegradable polyester nanoparticles can be prepared {{and that they are}} stable in the mother dispersive liquor for 48 hours, which is the period considerably exceeding the <b>time</b> <b>limit</b> neceséry <b>for</b> further <b>processing.</b> In the tested concentration range the simple parameter dependence of size of nanoparticles on the concentration of emulsifier was not proven. At the lower concentrations of emulsifier the dispersion of nanoparticles diminished the size in time, at a the higher concentration of 0, 20 % the optimum time dispersion in the time interval between 40 seconds and 60 seconds was found. In case of a long-term storage of nanoparticles there are [...] ...|$|R
40|$|Abstract—Configuration {{similarity}} is {{a special}} form of content-based image retrieval that considers relative object locations. It {{can be used as}} a standalone method, or to complement retrieval based on visual or semantic features. The corresponding queries ask for sets of objects that satisfy some spatio-temporal constraints, e. g., “find all triplets of objects (I, P, Q), such that I is northeast of P, which is inside Q. ” Exhaustive processing (i. e., retrieval of the best solutions) of configuration similarity queries, in general, has exponential complexity and fast search for sub-optimal solutions is the only way to deal with the vast amounts of multimedia information in several real-time applications. In this paper we first discuss the utilization of nonsystematic search heuristics, based on genetic algorithms, simulated annealing and hill climbing approaches. An extensive experimentation with real and synthetic datasets reveals that hill climbing techniques are the best for the current problem; therefore, as a subsequent step we study the search space, and develop improved variations of hill climbing that take advantage of the special structure of the problem to enhance speed. The proposed heuristic methods significantly outperform systematic search when there is only <b>limited</b> <b>time</b> <b>for</b> query <b>processing.</b> Index Terms—Content-based retrieval, local search algorithms, spatial similarity. I...|$|R
40|$|The LHC, at design capacity, has a bunch-crossing rate of 40 MHz {{whereas the}} ATLAS {{experiment}} has an average recording {{rate of about}} 1 kHz. To reduce the rate of events, but maintain high selection efficiency for rare events such as physics signals beyond the Standard Model, a two-level trigger system is used. Events are selected based on physics signatures such as presence of energetic leptons, photons, jets or large missing energy. Despite the <b>limited</b> <b>time</b> available <b>for</b> <b>processing</b> collision events the trigger system is able to exploit topological information, as well as using multi-variate methods. In total, the ATLAS trigger systems consists of thousands of different individual triggers. The ATLAS trigger menu specifies which triggers are used during data taking and how much rate a given trigger is allocated. This menu reflects not only the physics goals of the collaboration but also takes into consideration the instantaneous luminosity of the LHC and the design limits of the ATLAS detector and offline <b>processing</b> farm. <b>For</b> 2017 data taking, the trigger selections and menus have been improved to handle expected higher luminosities of up to 2. 0 x 10 ^{ 34 }cm^{- 2 }s^{- 1 } and to ensure robustness {{in the presence of}} multiple interactions per bunch crossing. We describe the criteria for designing the ATLAS trigger menu used for the LHC Run 2 period. Furthermore, we discuss how the trigger menu is deployed for data taking, through different phases: validation before deployment, decisions on prescale values for different triggers (ahead of running, or live in case of sudden rate changes), and monitoring during data taking itself...|$|R
40|$|<b>Time</b> <b>Limit</b> <b>for</b> Tax Assessment The thesis {{deals with}} the <b>time</b> <b>limit</b> <b>for</b> tax {{assessment}} as it is regulated especially in the Tax Procedure Code (Act No. 280 / 2009 Coll.). The <b>time</b> <b>limit</b> <b>for</b> tax assessment represents {{a period of time}} during which it is possible to assess the tax. This <b>time</b> <b>limit</b> thus creates a time frame of the whole tax administration because tax decision is its aim, and in the case in which tax cannot be assessed {{there is no reason to}} proceed with any tax administration. Its main purpose consists in the establishment of certainty providing that after the lapse of the period of time, rights and duties relating to substantive tax law cannot be changed. The thesis is composed of two basic chapters. Chapter One is introductory and describes the phenomenon of <b>time</b> <b>limits.</b> It is subdivided into three parts. Part One focuses on the concept and categories of <b>time</b> <b>limits.</b> Part Two explains general aspects of the <b>time</b> <b>limit</b> <b>for</b> tax assessment. Part Three {{deals with the}} calculation of <b>time</b> <b>limits.</b> Chapter Two focuses on legal regulation of the <b>time</b> <b>limit</b> <b>for</b> tax assessment. Chapter Two is divided in ten parts. Part One defines the beginning of the <b>time</b> <b>limit</b> <b>for</b> tax assessment. Part Two looks at the extension of the <b>time</b> <b>limit</b> <b>for</b> tax assessment. Part Three deals with the interruption of time [...] ...|$|R
40|$|Registered {{delegations}} {{can make}} presentations to the Board. <b>Time</b> <b>limits</b> <b>for</b> individual delegations will be established to allow all registered delegations to present within the <b>time</b> <b>limit</b> <b>for</b> this item. This agenda item has a <b>time</b> <b>limit</b> of 20 minutes including questions; extension {{is at the}} discretion of the Board...|$|R
50|$|Exceeding <b>time</b> <b>limits</b> <b>for</b> {{obstacles}} is penalized.|$|R
50|$|The former <b>time</b> <b>limit</b> <b>for</b> {{paying the}} further search fees {{was a period}} the EPO {{specified}} as between two weeks and six weeks. Under new Rule 64 EPC, in force as of April 1, 2010, the <b>time</b> <b>limit</b> <b>for</b> the further search fees is now two months.|$|R
5000|$|The second {{important}} rule is <b>time</b> <b>limits.</b> The standard <b>time</b> <b>limits</b> <b>for</b> an NPDA debate are: ...|$|R
5000|$|... #Subtitle level 2: <b>Time</b> <b>limit</b> <b>for</b> {{filing a}} divisional {{application}} ...|$|R
50|$|Pilichowski, Czeslaw. No <b>Time</b> <b>Limit</b> <b>for</b> these Crimes! Warsaw: Interpress, 1980.|$|R
5000|$|... #Subtitle level 2: <b>Time</b> <b>limits</b> <b>for</b> disputing a debt or {{requesting}} validation ...|$|R
5000|$|There is no <b>time</b> <b>limit</b> <b>for</b> each {{panel to}} {{complete}} the recruitment.|$|R
50|$|The tournament had no weight classes, or weight limits. Each match had no rounds, but a 15-minute <b>time</b> <b>limit</b> {{was imposed}} <b>for</b> the quarterfinal and an 18-minute <b>time</b> <b>limit</b> <b>for</b> semi-final round matches in the tournament.|$|R
25|$|ANSWER did not {{honor the}} {{agreed-upon}} <b>time</b> <b>limits</b> <b>for</b> its {{sections of the}} pre-march Rally...|$|R
5000|$|Trial of Persons Ceasing to be Subject to Service Law and <b>Time</b> <b>Limits</b> <b>for</b> Trials ...|$|R
5000|$|... {{introduce}} tighter <b>time</b> <b>limits</b> <b>for</b> {{responding to}} requests {{and for them}} to be enforced more effectively ...|$|R
50|$|Parties are {{expected}} to comply with strictly enforced <b>time</b> <b>limits</b> when applying <b>for</b> a review or appeal. The <b>time</b> <b>limit</b> <b>for</b> a Review application is within 14 days of the judgment being issued, with a discretion to extend the <b>time</b> <b>limit</b> on a just and equitable basis.|$|R
50|$|The race {{begins at}} 5:00 A.M. The overall <b>time</b> <b>limit</b> <b>for</b> {{the race is}} 15 hours, 30 minutes.|$|R
5000|$|There are <b>time</b> <b>limits</b> <b>for</b> noting appeals, but reviews have to {{be brought}} only within a {{reasonable}} time.|$|R
50|$|Florida {{state law}} chapter 3646, {{approved}} February 16, 1885, extended the <b>time</b> <b>limit</b> <b>for</b> completion to January 1, 1888.|$|R
50|$|The <b>time</b> <b>limit</b> <b>for</b> a team {{to advance}} the ball over the center line is reduced to six seconds.|$|R
50|$|The <b>time</b> <b>limits</b> <b>for</b> play tended not {{to follow}} the FIDE format, but more closely {{resemble}} classical <b>time</b> <b>limits,</b> to give some assistance to players in the endgame phase.|$|R
50|$|The Indian version {{adopted the}} clock format from the US version {{and is the}} fourth country's version to do (the others being Japan and the UK; {{although}} the US version abandoned the clock for the 2010-11 season when the show's 2010 format changes were made), although the <b>time</b> <b>limits</b> were shorter than in the UK version. There was a 30-second <b>time</b> <b>limit</b> <b>for</b> questions 1 and 2, and a 45-second <b>time</b> <b>limit</b> <b>for</b> questions 3 to 7, but questions 8 to 13 were not timed.|$|R
