7137|10000|Public
25|$|Hence these equations, {{which might}} be called {{functions}} elsewhere are in analytic geometry characterized as parametric equations and <b>the</b> <b>independent</b> <b>variables</b> are considered as parameters.|$|E
25|$|Confidence limits can {{be found}} if the {{probability}} distribution of the parameters is known, or an asymptotic approximation is made, or assumed. Likewise statistical tests on the residuals can be made if the probability distribution of the residuals is known or assumed. The probability distribution of any linear combination of the dependent variables can be derived if the probability distribution of experimental errors is known or assumed. Inference is particularly straightforward if the errors are assumed to follow a normal distribution, which implies that the parameter estimates and residuals will also be normally distributed conditional on the values of <b>the</b> <b>independent</b> <b>variables.</b>|$|E
25|$|Different {{choices for}} the frame of {{reference}} and expansion parameters are possible in Stokes-like approaches to the non-linear wave problem. In 1880, Stokes himself inverted the dependent and independent variables, by taking the velocity potential and stream function as <b>the</b> <b>independent</b> <b>variables,</b> and the coordinates (x,z) as the dependent variables, with x and z being the horizontal and vertical coordinates respectively. This has the advantage that the free surface, in a frame of reference in which the wave is steady (i.e. moving with the phase velocity), corresponds with a line on which the stream function is a constant. Then the free surface location is known beforehand, and not an unknown part of the solution. The disadvantage is that the radius of convergence of the rephrased series expansion reduces.|$|E
50|$|Regress the {{mediator}} on <b>the</b> <b>independent</b> <b>variable</b> {{to confirm that}} <b>the</b> <b>independent</b> <b>variable</b> is {{a significant predictor of}} {{the mediator}}. If the mediator is not associated with <b>the</b> <b>independent</b> <b>variable,</b> then it couldn’t possibly mediate anything.|$|R
50|$|Regress the {{dependent}} <b>variable</b> on <b>the</b> <b>independent</b> <b>variable</b> {{to confirm that}} <b>the</b> <b>independent</b> <b>variable</b> is {{a significant predictor of}} {{the dependent}} variable.|$|R
50|$|Usually {{the test}} {{result is the}} {{dependent}} variable, the measured response based on the particular conditions of the test or the level of <b>the</b> <b>independent</b> <b>variable.</b> Some tests, however, may involve changing <b>the</b> <b>independent</b> <b>variable</b> to determine <b>the</b> level at which a certain response occurs: in this case, the test result is <b>the</b> <b>independent</b> <b>variable.</b>|$|R
500|$|Derivatives may be {{generalized}} to functions of several real variables. In this generalization, the derivative is reinterpreted as a linear transformation whose graph is (after an appropriate translation) the best linear approximation to the graph of the original function. The Jacobian matrix is the matrix that represents this linear transformation {{with respect to the}} basis given by the choice of independent and dependent variables. [...] It can be calculated in terms of the partial derivatives with respect to <b>the</b> <b>independent</b> <b>variables.</b> [...] For a real-valued function of several variables, the Jacobian matrix reduces to the gradient vector.|$|E
2500|$|... <b>the</b> <b>in{{dependent}}</b> <b>variables</b> are a {{cause of}} the changes in the dependent variable; ...|$|E
2500|$|There are m {{observations}} in y and n parameters in β with m>n. X is a m×n matrix whose elements are either constants or functions of <b>the</b> <b>independent</b> <b>variables,</b> x. The weight matrix W is, ideally, the inverse of the variance-covariance matrix [...] of the observations y. <b>The</b> <b>independent</b> <b>variables</b> {{are assumed to}} be error-free. The parameter estimates are found by setting the gradient equations to zero, which results in the normal equations ...|$|E
5000|$|Irreversibility: In some {{withdrawal}} designs, once {{a change}} in <b>the</b> <b>independent</b> <b>variable</b> occurs, <b>the</b> dependent variable is affected. This cannot be undone by simply removing <b>the</b> <b>independent</b> <b>variable.</b>|$|R
40|$|Establishing a {{functional}} relationship between <b>the</b> <b>independent</b> and <b>the</b> dependent <b>variable</b> {{is the primary}} focus of applied behavior analysis. Accurate and reliable description and observation of both <b>the</b> <b>independent</b> and dependent <b>variables</b> are necessary to achieve this goal. Although considerable attention has been focused on ensuring the integrity of the dependent variable in the operant literature, similar effort has not been directed at ensuring the integrity of <b>the</b> <b>independent</b> <b>variable.</b> Inaccurate descriptions of the application of <b>the</b> <b>independent</b> <b>variable</b> may threaten <b>the</b> reliability and validity of operant research data. A survey of articles in the Journal of Applied Behavior Analysis demonstrated that the majority of articles published do not use any assessment of the actual occurrence of <b>the</b> <b>independent</b> <b>variable</b> and a sizable minority do not provide operational definitions of <b>the</b> <b>independent</b> <b>variable.</b> <b>The</b> feasibility and utility of ensuring the integrity of <b>the</b> <b>independent</b> <b>variable</b> is described...|$|R
5000|$|... (1) Temporal precedence. For example, if <b>the</b> <b>in{{dependent}}</b> <b>variable</b> precedes <b>the</b> {{dependent variable}} in time, this would provide evidence suggesting a directional, and potentially causal, link from <b>the</b> <b>independent</b> <b>variable</b> to <b>the</b> dependent variable.|$|R
2500|$|... {{involving}} the partial derivative of y with respect tox1. [...] The {{sum of the}} partial differentials with respect to all of <b>the</b> <b>independent</b> <b>variables</b> is the total differential ...|$|E
2500|$|In statistics, the {{coefficient}} of determination, denoted R2 or r2 and pronounced [...] "R squared", is {{the proportion of the}} variance in the dependent variable that is predictable from <b>the</b> <b>independent</b> <b>variable(s).</b>|$|E
2500|$|Higher order {{differentials}} {{in several}} variables also become more complicated when <b>the</b> <b>independent</b> <b>variables</b> are themselves allowed {{to depend on}} other variables. [...] For instance, for a function f of x and y which are allowed to depend on auxiliary variables, one has ...|$|E
50|$|As {{mentioned}} above, Sobel’s test {{is performed}} {{to determine if}} the relationship between <b>the</b> <b>independent</b> <b>variable</b> and dependent variable has been significantly reduced after inclusion of the mediator variable. In other words, this test assesses whether a mediation effect is significant. It examines the relationship between <b>the</b> <b>independent</b> <b>variable</b> and <b>the</b> dependent variable compared to the relationship between <b>the</b> <b>independent</b> <b>variable</b> and dependent variable including the mediation factor.|$|R
50|$|However, {{when time}} is <b>the</b> <b>independent</b> <b>{{variable}},</b> {{and values of}} some other variable are plotted {{as a function of}} time, normally <b>the</b> <b>independent</b> <b>variable</b> time is plotted horizontally, as in the line graph to the right.|$|R
50|$|Mathematically {{it is not}} {{necessary}} to assign physical dimensions to the signal or to <b>the</b> <b>independent</b> <b>variable.</b> In <b>the</b> following discussion the meaning of x(t) will remain unspecified, but <b>the</b> <b>independent</b> <b>variable</b> will be assumed to be that of time.|$|R
2500|$|... "12. Algorithmic theories... In {{setting up}} a {{complete}} algorithmic theory, {{what we do is}} to describe a procedure, performable for each set of values of <b>the</b> <b>independent</b> <b>variables,</b> which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, [...] "yes" [...] or [...] "no," [...] to the question, [...] "is the predicate value true?"" [...] (Kleene 1943:273) ...|$|E
2500|$|... {{which are}} shown in figure1. [...] The {{difference}} between the various models lies in which of these variables are regarded as <b>the</b> <b>independent</b> <b>variables.</b> [...] These current and voltage variables are most useful at low-to-moderate frequencies. At high frequencies (e.g., microwave frequencies), the use of power and energy variables is more appropriate, and the two-port currentvoltage approach is replaced by an approach based upon scattering parameters.|$|E
2500|$|The Gauss–Markov theorem. In {{a linear}} {{model in which}} the errors have {{expectation}} zero conditional on <b>the</b> <b>independent</b> <b>variables,</b> are uncorrelated and have equal variances, the best linear unbiased estimator of any linear combination of the observations, is its least-squares estimator. [...] "Best" [...] means that the least squares estimators of the parameters have minimum variance. The assumption of equal variance is valid when the errors all {{belong to the same}} distribution.|$|E
5000|$|... #Caption: In single {{variable}} calculus, {{a function}} is typically graphed with the horizontal axis representing <b>the</b> <b>independent</b> <b>variable</b> and <b>the</b> vertical axis representing {{the dependent variable}}. In this function, y is the dependent variable and x is <b>the</b> <b>independent</b> <b>variable.</b>|$|R
5000|$|... where [...] In this model, the short-run (same-period) {{effect of}} a unit change in <b>the</b> <b>independent</b> <b>variable</b> is <b>the</b> value of b, while the long-run (cumulative) {{effect of a}} {{sustained}} unit change in <b>the</b> <b>independent</b> <b>variable</b> can {{be shown to be}} ...|$|R
50|$|Yet other graphs {{may have}} one curve for which <b>the</b> <b>independent</b> <b>variable</b> is plotted {{horizontally}} and another curve for which <b>the</b> <b>independent</b> <b>variable</b> is plotted vertically. For example, in the IS-LM graph shown here, the IS curve shows {{the amount of}} the dependent variable spending (Y) as a function of <b>the</b> <b>independent</b> <b>variable</b> <b>the</b> interest rate (i), while the LM curve shows the value of the dependent variable, the interest rate, that equilibrates the money market as a function of <b>the</b> <b>independent</b> <b>variable</b> income (which equals expenditure on an economy-wide basis in equilibrium). Since the two different markets (the goods market and the money market) take as given different <b>independent</b> <b>variables</b> and determine by their functioning different dependent variables, necessarily one curve has its <b>independent</b> <b>variable</b> plotted horizontally and the other vertically.|$|R
2500|$|However, if {{the errors}} are not {{normally}} distributed, a {{central limit theorem}} often nonetheless implies that the parameter estimates will be approximately normally distributed {{so long as the}} sample is reasonably large. [...] For this reason, given the important property that the error mean is independent of <b>the</b> <b>independent</b> <b>variables,</b> the distribution of the error term is not an important issue in regression analysis. [...] Specifically, it is not typically important whether the error term follows a normal distribution.|$|E
2500|$|The {{expressions}} given {{above are}} based on the implicit assumption that the errors are uncorrelated {{with each other and with}} <b>the</b> <b>independent</b> <b>variables</b> and have equal variance. The Gauss–Markov theorem shows that, when this is so, [...] is a best linear unbiased estimator (BLUE). If, however, the measurements are uncorrelated but have different uncertainties, a modified approach might be adopted. Aitken showed that when a weighted sum of squared residuals is minimized, [...] is [...] the BLUE if each weight is equal to the reciprocal of the variance of the measurement ...|$|E
2500|$|... {{is called}} the regressand, {{endogenous}} variable, response variable, measured variable, criterion variable, or dependent variable [...] (see dependent and independent variables). The decision as to which variable in a data set is modeled {{as the dependent variable}} and which are modeled as <b>the</b> <b>independent</b> <b>variables</b> may be based on a presumption that the value of one of the variables is caused by, or directly influenced by the other variables. Alternatively, there may be an operational reason to model one of the variables in terms of the others, in which case there need be no presumption of causality.|$|E
30|$|A Baron and Kenny {{mediation}} {{analysis was}} conducted to assess if microfinancing mediated the relationship between access to finance {{and the growth of}} microenterprises. Three regressions were conducted to determine whether the data supported a mediating relationship. For mediation to be supported, the following four conditions must be met: (1) <b>the</b> <b>independent</b> <b>variable</b> must be related to the dependent <b>variable,</b> (2) <b>the</b> <b>independent</b> <b>variable</b> must be related to the mediator variable, (3) the mediator must be related to the dependent variable while in the presence of <b>the</b> <b>independent</b> <b>variable,</b> and (4) <b>the</b> <b>independent</b> <b>variable</b> should no longer be a significant predictor of the dependent variable {{in the presence of the}} mediator variable (Baron & Kenny, 1986). In this analysis, <b>the</b> <b>independent</b> <b>variable</b> is access to finance, the mediator is microfinancing, and the dependent variable is AVEGROW. The linear regression of access to finance on the growth of microenterprises computed in Table  4 will not be necessary when the mediation analysis takes place (Kenny & Judd, 2014).|$|R
30|$|These {{three sets}} of the {{variables}} – PCT patents as the dependent <b>variable,</b> POLCON as <b>the</b> <b>independent</b> <b>variable</b> capturing <b>the</b> institutional dimension of decentralization, and Hofstede’s measures as <b>the</b> <b>independent</b> <b>variable</b> capturing <b>the</b> cultural dimension of decentralization – constitute the variables of major interest in this study.|$|R
5000|$|If <b>the</b> <b>independent</b> <b>variable</b> z is {{omitted from}} the regression, then the {{estimated}} values of the response parameters of <b>the</b> other <b>independent</b> <b>variables</b> will be given by the usual least squares calculation, ...|$|R
2500|$|Quantitative {{genetics}} {{focuses on}} genetic variance due to genetic interactions. Any two locus interactions {{at a particular}} gene frequency can be decomposed into eight independent genetic effects using a weighted regression. In this regression, the observed two locus genetic effects are treated as dependent variables and the [...] "pure" [...] genetic effects are used as <b>the</b> <b>independent</b> <b>variables.</b> [...] Because the regression is weighted, the partitioning among the variance components will change {{as a function of}} gene frequency. [...] By analogy it is possible to expand this system to three or more loci, or to cytonuclear interactions ...|$|E
2500|$|Regression for fitting a [...] "true relationship". In {{standard}} regression analysis, {{that leads}} to fitting by least squares, there is an implicit assumption that errors in the independent variable are zero or strictly controlled {{so as to be}} negligible. When errors in the independent variable are non-negligible, models of measurement error can be used; such methods can lead to parameter estimates, hypothesis testing and confidence intervals that take into account the presence of observation errors in <b>the</b> <b>independent</b> <b>variables.</b> An alternative approach is to fit a model by total least squares; this can be viewed as taking a pragmatic approach to balancing the effects of the different sources of error in formulating an objective function for use in model-fitting.|$|E
2500|$|When {{performing}} statistical analysis, {{independent variables}} that affect a particular dependent variable {{are said to}} be orthogonal if they are uncorrelated, since the covariance forms an inner product. In this case the same results are obtained for the effect of any of <b>the</b> <b>independent</b> <b>variables</b> upon the dependent variable, regardless of whether one models the effects of the variables [...] individually with simple regression or simultaneously with multiple regression. If correlation is present, the factors are not orthogonal and different results are obtained by the two methods. This usage arises from the fact that if centered by subtracting the expected value (the mean), uncorrelated variables are orthogonal in the geometric sense discussed above, both as observed data (i.e., vectors) and as random variables (i.e., density functions).|$|E
3000|$|... {{function}} in (9) {{does not have}} an expression with finite number of coefficients, it is difficult to get an exact expression for (10). There are several approximations proposed in [23 – 26]. However, all these approximations are suitable for a specific range of <b>the</b> <b>independent</b> <b>variable.</b> For example, when <b>the</b> <b>independent</b> <b>variable</b> [...]...|$|R
5000|$|... "A {{threat to}} {{external}} validity is {{an explanation of}} how you might be wrong in making a generalization." [...] Generally, generalizability is limited when the cause (i.e. <b>the</b> <b>independent</b> <b>variable)</b> depends on other factors; therefore, all threats to external validity interact with <b>the</b> <b>independent</b> <b>variable</b> - a so-called background factor x treatment interaction.|$|R
50|$|From these models, the {{mediation}} effect is calculated as (τ - τ’). This represents {{the change in}} the magnitude of the effect that <b>the</b> <b>independent</b> <b>variable</b> has on <b>the</b> dependent variable after controlling for the mediator. From examination of these equations it can be determined that (αβ) = (τ - τ’). The α term represents the magnitude of the relationship between <b>the</b> <b>independent</b> <b>variable</b> and <b>the</b> mediatior. The β term represents the magnitude {{of the relationship between the}} mediator and dependent variable after controlling for the effect of <b>the</b> <b>independent</b> <b>variable.</b> Therefore (αβ) represents the product of these two terms. In essence this is the amount of variance in the dependent variable that is accounted for by <b>the</b> <b>independent</b> <b>variable</b> through <b>the</b> mechanism of the mediator. This is the indirect effect, and the (αβ) term has been termed the product of coefficients.|$|R
