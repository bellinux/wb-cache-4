6|10000|Public
30|$|No {{matter what}} the {{parameter}} values are used <b>the</b> <b>processing</b> <b>has</b> <b>to</b> be stopped at same point of time. When to stop {{is a question that}} is rarely raised in the discussion of the ACO. The frequency of changes of the BSF route lessens as the number of iterations increases. This means that the computational effort needed to execute subsequent iterations is less and less profitable. This is clearly visible on the Fig. 2 which illustrates the typical performance of an ACO.|$|E
40|$|The {{main goal}} of the thesis is to {{complete}} theoretical and algorithmic background for processing of prior information about the system {{in the process of}} adaptive control design. <b>The</b> <b>processing</b> <b>has</b> <b>to</b> respect that the expertise of the information provider is out of the adopted Bayesian frame work and that there is no supervisor available who would check internal consistency of the processed knowledge. The use of fictious data allows to merge different prior information items without taking into account their possible mutual influence. The developed and refined ideas have to be implemented into DESIGNER software so that {{it will be possible to}} demonstrate positive contributions of the built-in prior information on the control achieved by the tuned controllers. The demonstration should be done on a range of simulated controlled systems, on a pilot plant and on hybrid experiment combining real data and non-linear realistic SIMULINK model of the corresponding plant. Available from STL, Prague, CZ / NTK - National Technical LibrarySIGLECZCzech Republi...|$|E
40|$|Metaphors {{are useful}} {{because they are}} efficient: they {{transfer}} a complex of meaning in a few words. Information systems are social constructs. Therefore, metaphors seem to be especially useful for explaining the space of possible meaning complexes or designs of information systems. Three information system metaphors and the associated meaning complexes are explained: the mill, the cell, and the mind. An information system as a mill {{is characterized by the}} efficient processing of large quantities of information. <b>The</b> <b>processing</b> <b>has</b> <b>to</b> be done using fixed, that is, invariant, rules en patterns that may be very complex. An information system as a cell is characterized by its fluent and adequate interaction with people. The information system consists of objects that take care of preserving their own integrity and that react on events. The cell metaphor is characterized by interaction and integrity. The information system as a mind appears as an intelligent assistant embodying that mind. An information system as a mind is characterized by capabilities like knowledge use, autonomy and learning. These three metaphors can be combined, and are combined, in real-life organizations...|$|E
30|$|Before {{turning to}} <b>the</b> <b>processing</b> stage all <b>the</b> threads of <b>the</b> <b>processing</b> block <b>have</b> <b>to</b> wait in {{a barrier to}} ensure that all of them have loaded its {{corresponding}} data. After the calculation step may be a second stage of synchronization of the block threads before writing to the GPU global memory.|$|R
50|$|OLAP {{data was}} handled as a core data type of HL, with {{specific}} syntax to accommodate multidimensional data concepts, and complete programmatic freedom {{to explore and}} utilise the data. This made it {{very different from the}} industry trend of query-based OLAP and SQL engines. On the upside, it allowed amazing flexibility in the applications to which it could be applied. On the downside, it mean that 3-tier configurations were never successfully implemented since <b>the</b> <b>processing</b> <b>had</b> <b>to</b> be close to the data itself. This hindered large-scale deployment to many clients, and the use of OLAP data from other vendors. In reality, its own data access times were probably some of the fastest around—at the individual cell level; they <b>had</b> <b>to</b> be in order to be practical. However, when fetching back bulk data for non-cooperating clients, or data from other vendors, the queries could not be optimised as a whole. Its own data access used a machine-wide shared memory cache.|$|R
40|$|Image {{processing}} {{is widely}} used in many applications, including medical imaging, industrial manufacturing, and security systems. Often {{the size of the}} image is very large, <b>the</b> <b>processing</b> time <b>has</b> <b>to</b> be very small and usually real-time constraints <b>have</b> <b>to</b> be met. Therefore, during the last decades there has been an increasing interest in the development and the use of parallel algorithms in image processing...|$|R
40|$|Business data is {{frequently}} exchanged between heterogeneous information systems using standard EDI formats like EDIFACT, X 12 {{and in the}} future also XML/EDI. For inhouse use, data represented in these formats must be converted to inhouse data formats by message converters. With the growing usage of EDI, high volumes of data, like financial transactions, must be converted and processed efficiently and reliably. Considering the complex hierarchical structure of EDI messages, message conversion is a complex data transformation problem. For efficient processing, the different conversion steps have to be executed in an optimized manner exploiting the available, typically distributed, processing architecture. In addition, <b>the</b> <b>processing</b> <b>has</b> <b>to</b> take into account different optimization goals, like maximizing throughput, minimizing delays and keeping deadlines. We approach message conversion as a data management problem. First we show that the nested relational data model is adequate for representing the message structure and the transformation operations. Based on this we develop a cost model. This serves as input to an optimization strategy for the conversion processing that uses efficient scheduling strategies. We present a novel scheduling strategy and give simulation results that show that they outperform alternative strategies for typical workloads of EDI converters...|$|E
40|$|The {{permanent}} scatterer technique invented at POLIMI has meanwhile {{developed into}} a remarkable operational method. It facilitates innovative data products such as urban subsidence maps or atmospheric delay measurements and permits new geophysical applications. The accuracy and validity of this techniques has been demonstrated in several projects at DLR. Due to the outstanding availability of data, time series of this technique have mainly been produced from data of the compatible satellite sensors ERS- 1 and ERS- 2. These time series can even span a continuous time range of about twelve years. This long-term observation enables the monitoring of displacements with millimetre accuracy and even facilitates the detection of seasonal periodic effects. The sensor ERS- 1 made its last acquisition in 2000. The similarly constructed successor ERS- 2 still monitors the Earth's surface even after nine years of operation. But recent acquisitions are unfortunately not suited for general interferometric applications. The reason is a heavily varying Doppler centroid frequency due to failures of gyros. The ERS- 2 successor ENVISAT/ASAR is able to pursue the unique continuity in the monitoring of urban areas. But it operates with a slightly different radar frequency compared to the ERS sensors. Consequently the interferometric principle becomes more complicated and <b>the</b> <b>processing</b> <b>has</b> <b>to</b> be modified. We will present the required changes for the permanent scatterer cross interferometry on the developed scientific permanent scatterer system. 1...|$|E
40|$|Nowadays, we {{are confronted}} with a virtual domain based on {{information}} technology artifacts and organize ourselves in virtual organizations. The limitations {{to the development of}} virtual organizations are those of the human imagination. Multiple ideas about what virtual organizations should be or should do are possible, and can be studied based on the metaphor concept. Metaphors are useful because they are efficient: they transfer a complex of meaning in a few words. Information systems are social constructs. Therefore, metaphors seem to be especially useful for explaining the space of possible meaning complexes or designs of information systems. Three information system metaphors and the associated meaning complexes are explained: the mill, the cell, and the mind. An information system as a mill is characterized by the efficient processing of large quantities of information. <b>The</b> <b>processing</b> <b>has</b> <b>to</b> be done using fixed, that is, invariant, rules en patterns that may be very complex. An information system as a cell is characterized by its fluent and adequate interaction with people. The information system consists of objects that take care of preserving their own integrity and that react on events. The cell metaphor is characterized by interaction and integrity. The information system as a mind appears as an intelligent assistant embodying that mind. An information system as a mind is characterized by capabilities like knowledge use, autonomy and learning. These three metaphors can be combined, and are combined, in real-life organizations...|$|E
50|$|Until {{the early}} 1960s, {{high-quality}} slate {{was still being}} mined and processed in the Schielenbach to the village’s southwest. The mining rights were leased mostly to Breitenthal citizens by the municipality. Many Breitenthalers earned extra income - often as a secondary job - doing this, even though by today’s standards the working conditions were very unfavourable. Local geological peculiarities meant that the slate could only be mined underground, not quarried at the surface: since the slate in the Breitenthal area stood steeply upright, and because {{only a small part}} of it was technically exploitable, mining <b>had</b> <b>to</b> proceed right into the mountain. (Around the Schielenbach, many old boreholes can still be found; they all belong to the same mining complex.) Any meaningful <b>processing</b> of <b>the</b> material thus mined could then only be done as long as it was moist from groundwater. This in turn meant that <b>the</b> <b>processing</b> <b>had</b> <b>to</b> be done on site, and this is still witnessed today by the great waste heaps in the Schielenbach former mining area.|$|R
40|$|The {{objective}} of this contribution is to characterize alternatives of information systems used for managing, processing and evaluation of information related to company vehicles. Especially we focus on logging, transferring and processing of on-road vehicle movement information in inland and international transportation. This segment of company information system <b>has</b> <b>to</b> monitor the car movement – actively or passively – according to demand {{of the company and}} after <b>the</b> <b>processing</b> it <b>has</b> <b>to</b> evaluate and give the complex monitoring of a situation of all the company vehicles to the controller...|$|R
40|$|Listening to {{connected}} {{speech is}} a task that humans perform effortlessly each day. This is surprising given the short time that <b>the</b> <b>processing</b> system <b>has</b> <b>to</b> deal with different types of information. Segmental phonemes and suprasegmental phonological information (prosody or pitch) as well as syntactic and semantic information must be accessed and coordinated within milliseconds. With respect to syntactic and semantic processes, two alternative views have been proposed in psycholinguistic comprehension models. One view, which is characterized by serial or syntax-first models, holds that syntax is processed autonomously prior to semantic information [1, 2]. A second view, represented by interactive o...|$|R
40|$|For {{design and}} {{development}} engineers, difficulties arise ensuring that existing manufacturing equipment <b>has</b> the potential <b>to</b> handle both large product variation {{and complexity of}} process. <b>The</b> food <b>processing</b> industry maintains <b>the</b> highest number of product variations, {{some of which are}} short lived or seasonal, a factor with which <b>the</b> <b>processing</b> equipment <b>has</b> <b>to</b> cope. This paper presents the idea of “constraint modelling”, and identifies it employment in the investigation of the capabilities and optimized performance limits of such equipment. The paper also introduces the concept of multi-instance modelling and its benefits. The approach being employed is illustrated by a number of industrial case study examples, taken from <b>the</b> food <b>processing</b> industry...|$|R
50|$|The {{carbon dioxide}} {{injection}} plant {{is located on}} Barrow Island, a Class A nature reserve. Therefore, the development of <b>the</b> <b>processing</b> plant <b>had</b> <b>to</b> undergo very strict environmental regulations. The Joint Venture established expert panels to protect the biodiversity of the island and surrounding marine environment. To ensure that the Dupuy Formation was suitable for underground injection of reservoir carbon dioxide, the Gorgon Joint Venture undertook additional drilling, well tests, and seismic surveys prior to making the final investment decision. An on-going monitoring program, including observation wells and seismic surveys, will assist in managing {{the performance of the}} injected carbon dioxide in the Dupuy Formation.|$|R
30|$|In {{order for}} wooden {{materials}} {{to be processed}} in high quality, {{it is necessary to}} use planing machinery and cutters properly. To do so, one needs to be informed about the wooden materials being processed, wood processing techniques, existence of sawdust, cutting device geometry, engine revolution number, and the type and expiry date of the cutter used [4, 5]. <b>The</b> <b>processing</b> parameters <b>have</b> <b>to</b> be optimal in order to minimize the cost and use of production materials while not decreasing the product quality. Regarding the product quality and particularly the cost, the hardening which is seen during the planing process and the cutting power were seen <b>to</b> both <b>have</b> a significant effect [6].|$|R
40|$|An {{experimental}} radar at 94 GHz {{has been}} developed, which {{is capable of}} delivering an instantaneous bandwidth of 4 GHz. The radar uses an FM-CW waveform and is fully solid state. The data undergo an ISAR imaging process to derive high resolution scattering centre distributions for targets measured in tower/turnlable configuration. <b>The</b> high resolution <b>processing</b> <b>has</b> <b>to</b> rely on an appropriate polar reformatting procedure, which is not necessary for ISAR imaging at millimetre waves with resolutions worse than 10 cm...|$|R
40|$|To operate {{technology}} intelligence efficiently {{access to}} up-to-date relevant and sufficiently complete information is essential. Indeed {{availability of information}} is higher than ever by reason of digitalization. However it also causes the problem of information overload. The available mass of data <b>has</b> <b>to</b> be searched assorted and assessed to identify the actual needed information. In addition <b>the</b> entire information <b>processing</b> <b>has</b> <b>to</b> be continued permanently or to be repeated for each new object of investigation otherwise {{the validity of the}} results is not given any more...|$|R
40|$|All {{digital copying}} aims to {{reproduce}} an original image as faithfully as possible under certain constraints. In <b>the</b> past, image <b>processing</b> <b>had</b> <b>to</b> {{be implemented in}} hardware for performance reasons. Here, a 100 % software solution is outlined. In order to find such a solution an appropriate methodology based on <b>the</b> array <b>processing</b> language J is used. Although J is ideal for prototyping such designs, its wider application is seriously hindered {{by the lack of}} awareness of array processing languages amongst engineers, and by the lack of available education in this language and methodology...|$|R
50|$|To {{ensure that}} the domain {{actually}} modelled is usable for analyzing and further <b>processing,</b> <b>the</b> language <b>has</b> <b>to</b> ensure {{that it is possible}} to reason in an automatic way. To achieve this it <b>has</b> <b>to</b> include formal syntax and semantics. Another advantage by formalizing is the ability to discover errors in an early stage. It is not always that the language best fitted for the technical actors is the same as for the social actors.|$|R
50|$|Steven Spielberg {{gave his}} backing to the {{restoration}} effort and recommended that Stanley Kubrick {{be informed of}} the project. Kubrick, who had disowned the film, <b>had</b> nothing <b>to</b> do with the physical restoration of the film, though he gave his approval to the effort; and the producers wanted his final approval of their work. Universal's negative was unusable {{because it had been}} cut twice and the colors were badly faded. Kubrick's print of the film, which was donated to the Museum of Modern Art, could not be used for the restoration because it was considered archival. The original studio black-and-white separation prints, used as a backup in 1960, were used, though <b>the</b> <b>processing</b> lab <b>had</b> <b>to</b> develop a new lens capable of printing the Technirama frame without losing fidelity. The restoration cost about $1 million.|$|R
50|$|An event {{channel is}} a {{mechanism}} whereby {{the information from}} an event generator is transferred to the event engine or sink.This could be a TCP/IP connection or any type of input file (flat, XML format, e-mail, etc.). Several event channels can be opened at the same time. Usually, because <b>the</b> event <b>processing</b> engine <b>has</b> <b>to</b> process them in near real time, the event channels will be read asynchronously. The events are stored in a queue, waiting to be processed later by <b>the</b> event <b>processing</b> engine.|$|R
40|$|Artificial {{olfaction}} system (the so-called electronic nose) is a {{very promising}} tool to monitor the malodour in the field. Usual measurement techniques of odour use human olfaction or conventional analytical techniques. The first category represents the real odour perception but is not applicable to measure continuously bad odours in the field. The second class of techniques gives the mixture composition but not the global information representative of the odour perception. The e-nose <b>has</b> the potentialities <b>to</b> combine "the odour perception" and the "monitoring in the field". However {{to be able to}} reach that goal, <b>the</b> signal <b>processing</b> <b>has</b> <b>to</b> be adapted to work in complex environment. The research group in Arlon has more than ten years experience in the measure of environmental malodours in the field. The paper presents the minimal requirements that the group considers as essential for artificial olfaction system to become successful for this application. Peer reviewe...|$|R
40|$|To meet NASA Space Transportation System goals <b>the</b> Shuttle <b>Processing</b> Contractors <b>have</b> <b>to</b> reduce Space Transportation System ground {{processing}} time and ground processing costs. These objectives {{must be met}} without compromising safety of flight or safety during assembly, test, and service operations. Ground processing requirements are analyzed to determine critical serial flow paths and costly labor-intensive tasks. Processing improvements are realized by improvements in processing methodology, by application of computer-aided technology, and by modernization of KSC facilities. Ongoing improvement efforts are outlined and progress-to-date is described...|$|R
40|$|Active {{control of}} {{vibration}} allows to reduce annoying vibrations in mechanical structures significantly. However, {{during the development}} process of such systems, the overall system comprising the excitation, the mechanical structures, the actuators, the sensors, and <b>the</b> signal <b>processing</b> subsystem <b>has</b> <b>to</b> be considered from the beginning. Within {{the scope of this}} contribution a possible model-based development procedure for electro mechanical systems with distinct interactions is presented. The development of a resilient active engine mount serves as an example. The presented mount features a novel topology that involves reduced requirements concerning the power amplification...|$|R
40|$|We {{present a}} novel {{framework}} for encoding latency analysis of arbitrary multiview video coding prediction structures. This framework avoids {{the need to}} consider an specific encoder architecture for encoding latency analysis by assuming an unlimited <b>processing</b> capacity on <b>the</b> multiview encoder. Under this assumption, only {{the influence of the}} prediction structure and <b>the</b> <b>processing</b> times <b>have</b> <b>to</b> be considered, and the encoding latency is solved systematically by means of a graph model. The results obtained with this model are valid for a multiview encoder with sufficient processing capacity and serve as a lower bound otherwise. Furthermore, with the objective of low latency encoder design with low penalty on rate-distortion performance, the graph model allows us to identify the prediction relationships that add higher encoding latency to the encoder. Experimental results for JMVM prediction structures illustrate how low latency prediction structures with a low rate-distortion penalty can be derived in a systematic manner using the new model...|$|R
40|$|Digital signal {{processing}} is being employed {{more and more}} in modern ultrasound scanners. This has made it possible to do dynamic receive focusing for each sample and implement other advanced imaging methods. <b>The</b> <b>processing,</b> however, <b>has</b> <b>to</b> be very fast and cost-effective at the same time. Dedicated chips are used in order to do real time processing. This often makes it difficult to implement radically different imaging strategies on one platform and makes the scanners less accessible for research purposes. Here flexibility is the prime concern, and the storage of data from all transducer elements over 5 to 10 seconds is needed to perform clinical evaluation of synthetic and 3 D imaging. This paper describes a real-time system specifically designed for research purposes. The purpose of the system is to make it possible to acquire multi-channel data in real-time from clinical multi-element ultrasound transducers, and to enable real-time or near realtime <b>processing</b> of <b>the</b> acquired data. T [...] ...|$|R
30|$|Due to such concerns, {{information}} {{protection is}} related to the value that Big Data can bring to Internet credit services. 100 Credit expends a great deal of effort to protect its data. From a network security perspective, it follows Chinese government regulations to obtain security certifications from the Ministry of Public Security. It also follows the international standard of ISO 27000. 100 Credit’s data is stored in servers that are disconnected from the Internet and cannot be physically accessed by employees. <b>The</b> data <b>processing</b> program <b>has</b> <b>to</b> be executed on a computer terminal that does not have data storage capabilities. Thus, employees cannot export the company’s data from the servers.|$|R
40|$|AbstractThe {{steps in}} the {{maturation}} of the precursor of the large subunit (pre-HycE) of hydrogenase 3 from Escherichia coli taking place after incorporation of both iron and nickel were investigated. Pre-HycE could be matured and processed {{in the absence of}} the small subunit but association with the cytoplasmic membrane required heterodimer formation between the two subunits. Pre-HycE formed a complex with the chaperone-like protein HypC {{in the absence of the}} small subunit and, in this complex, also incorporated nickel. For <b>the</b> C-terminal <b>processing,</b> HypC <b>had</b> <b>to</b> leave <b>the</b> complex since only a HypC-free, nickel-containing form of pre-HycE was a substrate for the maturation endopeptidase...|$|R
40|$|In {{this paper}} we study the {{scheduling}} {{of a given}} set of jobs on several identical parallel machines tended by a common server. Each job must be processed {{on one of the}} machines. Prior <b>to</b> <b>processing</b> <b>the</b> server <b>has</b> <b>to</b> set up the relevant machine. The objective is to schedule the jobs so as to minimize the total weighted job completion times. We provide an approximation algorithm to tackle this intractable problem and analyze the worst-case performance of the algorithm for the general, as well as a special, case of the problem. Department of Logistics and Maritime Studie...|$|R
40|$|The {{rolling circle}} {{replication}} of small circular plant pathogenic RNAs requires a processing step to convert multimeric intermediates to monomers {{which are then}} circularized. Eleven such RNAs are known so far, two are viroids, one is viroid-like and the remainder are satellite RNAs dependent on a helper virus for replication. <b>The</b> <b>processing</b> step is RNA-catalysed in all cases, at least in vitro. All plus forms of these RNAs self-cleave via the hammerhead structure whereas only eight of the minus RNAs self-cleave, five via the hammerhead structure and three via the hairpin structure. There are about 20 other viroids where <b>the</b> <b>processing</b> mechanim <b>has</b> yet <b>to</b> be determined but they are likely candidates for {{a new type of}} self-cleavag...|$|R
40|$|Because {{electromagnetic}} waves are strongly attenuated in the ocean, researchers <b>have</b> turned <b>to</b> {{the use of}} acoustics for such problems as communications and detection. Acoustic propagation in the ocean is dominated by interactions with the surface and seafloor, leading to complicated arrivals structures. The computing power needed to model this environment makes the use of adaptive processes such as time-reversal an attractive alternative. Further, the use of self-adaptive processes, which use the data itself <b>to</b> influence <b>the</b> <b>processing,</b> <b>have</b> proven <b>to</b> be both robust and powerful. This thesis concerns itself with four such processes. The first derives the canonical minimum mean-squared error linear equalizer (MMSE-LE) as a self-adaptive process related to iterative time reversal. The next chapter analyzes the feasibility of using a horizontally-aligned synthetic aperture to perform time reversal communications. Also derived in this thesis are bounds on the capacity of a strongly dispersive channel, such as {{those found in the}} ocean, with practical constellation-based constraints on the transmitted signal. A final self-adaptive process is described in an attempt to classify a target as resonant or non-resonant based on the measured back-scattered fiel...|$|R
40|$|Phenolic {{compounds}} have antioxidant {{properties and}} activate endogenous detoxification defense systems able to scavenge the reactive species of oxygen. The external layers of wheat caryopsis, largely constituting by-products of the milling industry such as bran and various middlings, contain relevant amounts of phenolic compounds. The {{aim of the}} research <b>has</b> been <b>to</b> evaluate the effect of supplementation with wheat bran aqueous extracts, obtained by ultrasoundassisted technologies, on the sensory properties and antioxidant activity of dry pasta. The HPLC-DAD characterization of the extract evidenced the presence of ferulic and p-coumaric acids. The supplemented pasta showed significantly higher antioxidant activity and phenolic content than the control, coupled to good overall sensory judgment. In addition, two different pasta drying diagrams were adopted, and the comparison of the corresponding end-products allowed it to be pointed out that <b>the</b> <b>processing</b> technology <b>has</b> <b>to</b> be carefully set up to prevent possible detrimental effects on the antioxidant activity. The proposed utilization of bran might add value to a milling by-product that, otherwise, is mostly employed in animal feeding...|$|R
40|$|The {{author has}} {{identified}} the following significant results. Analysis of <b>the</b> <b>processing</b> results <b>has</b> led <b>to</b> <b>the</b> following conclusions: (1) LANDSAT imagery was a reliable resource for the stratification of level 2 forest features (softwood, hardwood, tundra, and water). These features {{can be classified}} with an accuracy of 72. 4 percent + or - 5. 9 percent at the 90 percent confidence level. (2) Training fields selected for signature development from only 10 percent of the area did not adequately and efficiently cover the class variability for the entire area. (3) Derived regression transformations were ineffective in recovering the loss of level 1 forest proportions and level 2 softwood and hardwood proportions...|$|R
50|$|Conversely, the MIMO multiple-access-channel or MIMO MAC {{represents}} a MIMO uplink {{case in the}} multiple sender to single receiver wireless network. Examples of advanced receive processing for MIMO MAC are joint interference cancellation and SDMA-based uplink user scheduling. For advanced receive <b>processing,</b> <b>the</b> receiver <b>has</b> <b>to</b> know the channel state information at the receiver (CSIR). Knowing CSIR is generally easier than knowing CSIT. However, knowing CSIR costs a lot of uplink resources to transmit dedicated pilots from each user to the AP. MIMO MAC systems outperforms point-to-point MIMO systems especially {{when the number of}} receiver antennas at an AP is larger than the number of transmit antennas at each user.|$|R
40|$|What can the {{statistical}} structure of natural images {{teach us about}} the human brain? Even though the visual cortex {{is one of the}} most studied parts of the brain, surprisingly little is known about how exactly images are processed to leave us with a coherent percept of the world around us, so we can recognize a friend or drive on a crowded street without any eﬀort. By constructing probabilistic models of natural images, the goal of this thesis is to understand the structure of the stimulus that is the raison d etre for the visual system. Following the hypothesis that <b>the</b> optimal <b>processing</b> <b>has</b> <b>to</b> be matched to the structure of that stimulus, we attempt to derive computational principles, features that the visual system should compute, and properties that cells in the visual system should have. Starting from machine learning techniques such as principal component analysis and independent component analysis we construct a variety of sta- tistical models to discover structure in natural images that can be linked to receptive ﬁeld properties of neurons in primary visual cortex such as simple and complex cells. We show that by representing images with phase invariant, complex cell-like units, a better statistical description of the vi- sual environment is obtained than with linear simple cell units, and that complex cell pooling can be learned by estimating both layers of a two-layer model of natural images. We investigate how a simpliﬁed model of <b>the</b> <b>processing</b> in <b>the</b> retina, where adaptation and contrast normalization take place, is connected to the nat- ural stimulus statistics. Analyzing the eﬀect that retinal gain control has on later cortical processing, we propose a novel method to perform gain control in a data-driven way. Finally we show how models like those pre- sented here can be extended to capture whole visual scenes rather than just small image patches. By using a Markov random ﬁeld approach we can model images of arbitrary size, while still being able to estimate the model parameters from the data. What can {{the statistical}} structure of natural images teach us about the human brain? Even though the visual cortex {{is one of the most}} studied parts of the brain, surprisingly little is known about how exactly images are processed to leave us with a coherent percept of the world around us, so we can recognize a friend or drive on a crowded street without any eﬀort. By constructing probabilistic models of natural images, the goal of this thesis is to understand the structure of the stimulus that is the raison d etre for the visual system. Following the hypothesis that <b>the</b> optimal <b>processing</b> <b>has</b> <b>to</b> be matched to the structure of that stimulus, we attempt to derive computational principles, features that the visual system should compute, and properties that cells in the visual system should have...|$|R
40|$|There {{are some}} applications, where the polar, {{cylindrical}} or spherical co-ordinate {{systems can be}} used for finding a solution of given problem. Especially some technical problems [Lio 97 a], like sonar and radar applications, where the distance from an object is measured under known angles, flow computation, radiation, medical imaging and ultrasound imaging etc. could benefit from their use. It is a usual practice that all the data from those applications are transformed to the cartesian orthogonal co-ordinate system, where all data are processed. The data are than displayed directly or transform back to the original co-ordinate system. Nevertheless {{it is well known that}} representation of a point in E 2 is different drom a line representation and therefore <b>the</b> <b>processing</b> pipeline <b>have</b> <b>to</b> respect this fact. The polar, cylindrical and spherical co-ordinate systems offer some possibilities how to handle graphical information in an unambiguous way and also make an effective <b>processing.</b> On <b>the</b> other hand, it is necessary to say that in usual practical graphical applications the direct use of non-linear co-ordinates can be quite complicated can hopefully lead to new understanding of some fundamental algorithms and developing of new more effective methods...|$|R
