9|11|Public
40|$|Tele-presence {{is a vast}} {{terminology}} incorporating techniques, {{methods and}} ways to make someone present at a location virtually to achieve a specific goal. <b>Tele-presence</b> <b>systems</b> can take many forms ranging from a simple software to a complex artificial intelligence based robots. In this paper, we develop a simple and cost effective tele-presence and multi-purpose robot controlled by both Wi-Fi and through web. Furthermore, the developed system supports live video streaming from any location {{as well as the}} video and audio transmission. The robot is tested for patient monitoring and a complete system is designed for this purpose...|$|E
40|$|In this contribution, we embed {{full-duplex}} multichannel {{communication interfaces}} for <b>tele-presence</b> <b>systems</b> into a general PSfrag frame- replacements work. On the reproduction side, we consider {{a wide range}} of multichannel acoustic rendering techniques including traditional stereophony, ’ 5. 1 ’ systems, and wave field synthesis using loudspeaker arrays for sound immersion. On the recording side, microphone arrays are discussed for capturing clean desired signals with spatial information. Based on this general framework, real-time implementations of such full-duplex multichannel communication systems are then described. We combine wave field synthesis with multichannel acoustic echo cancellation and adaptive beamforming and discuss a real-time implementation on standard desktop and laptop PCs...|$|E
40|$|Abstract. We {{present a}} novel perception-driven {{approach}} to low-cost <b>tele-presence</b> <b>systems,</b> to support immersive experience in continuity be-tween projected video and conferencing room. We use geometry and spec-tral correction to impart for perceptual continuity {{to the whole}} scene. The geometric correction comes from a learning-based approach to iden-tifying horizontal and vertical surfaces. Our method redraws the pro-jected video to match its vanishing point {{with that of the}} conference room in which it is projected. We quantify intuitive concepts such as the depth-of-field using a Gabor filter analysis of overall images of the con-ference room. We equalise spectral features across the projected video and the conference room, for spectral continuity between the two...|$|E
40|$|Abstract. We present work in {{progress}} on a tele-immersion system for telerehabilitation using real-time stereo vision and virtual environments. Stereo reconstruction is used to capture user’s 3 D avatar in real time and project it into a shared virtual environment, enabling a patient and therapist to interact remotely. Captured data {{can also be used}} to analyze the movement and provide feedback to the patient as we present in a preliminary study of stepping-in-place task. Such <b>tele-presence</b> <b>system</b> could in the future allow patients to interact remotely with remote physical therapist and virtual environment while objectively tracking their performance...|$|R
40|$|The first moments at a disater scene are chaotic. The {{command center}} {{initially}} operates with little knowledge of hazards, geography and casualties, building up {{knowledge of the}} event slowly as information trickles in by voice radio channels. RealityFlythrough is a <b>tele-presence</b> <b>system</b> that stitches together live video feeds in real-time, using the principle of visual closure, to give command center personnel the illusion {{of being able to}} explore the scene interactively by moving smoothly between the video feeds. Using RealityFlythrough, medical, fire, law enforcement, hazardous materials, and engineering experts may be able to achieve situational awareness earlier, and better manage scarce resources. The RealityFlythrough system i...|$|R
40|$|This paper {{presents}} the architecture {{and implementation of}} a <b>tele-presence</b> wheelchair <b>system</b> based on <b>tele-presence</b> robot, intelligent wheelchair, and touch screen technologies. The <b>tele-presence</b> wheelchair <b>system</b> consists of a commercial electric wheelchair, an add-on tele-presence interaction module, and a touchable live video image based user interface (called TIUI). The tele-presence interaction module is used to provide video-chatting for an elderly or disabled person with the family members or caregivers, and also captures the live video of an environment for tele-operation and semi-autonomous navigation. The user interface developed in our lab allows an operator to access the system anywhere and directly touch the live video image of the wheelchair to push it as if he/she {{did it in the}} presence. This paper also discusses the evaluation of the user experience...|$|R
40|$|Current {{interactive}} <b>tele-presence</b> <b>systems</b> {{are designed}} and optimized for one {{particular type of}} cyber-physical activity such as conversation, video chat, or gaming. However, with the emerging new 3 D tele-immersive (TI) systems, such as our own TI system, called TEEVE (TEle-immersion for EVErybody), we observe that the same TI system platform is being used for very different activities. In this paper, we classify the TI activities {{with respect to their}} physical characteristics, qualitatively analyze the cyber side of TI activities, and argue that one needs to consider very different performance profiles of the same TI system platform in order to achieve high quality of experience (QoE) for different cyber-physical TI activities. 1...|$|E
40|$|Recently, the {{omnidirectional}} image sensors {{have been}} applied to <b>tele-presence</b> <b>systems,</b> because the sensor can capture images with large field of views at video rate. On the other hand, head mount display (HMD) has been generally used as a personal display for virtual reality applications such as a tele-presence. However, almost all HMDs have a problem that {{the field of view}} (FOV), about 60 degree horizontally, of its presented image was terribly narrower than that of human. The problem makes reality and immersion lower in these applications. In this paper, we propose highimmersive visualization system that can display 180 degrees horizontal view by using a new catadioptrical HMD and an omnidirectional image sensor. The HMD consists of ellipsoidal and hyperboloidal curved mirrors, and can display 180 degrees horizontal view...|$|E
40|$|We {{begin by}} {{reviewing}} current spatial approaches to CSCW (mediaspaces, spatial video conferencing, collaborative virtual environments and telepresence applications) and classifying them along the proposed dimensions of transportation, artificiality and spatiality. This classification {{leads us to}} identify new shared space applications; so called mixed realities. We present {{an example of a}} mixed reality called the Internet Foyer, an application which provides a unified entry point into an organisation's physical and electronic environments and which supports awareness and chance encounters between the occupants of physical and synthetic space. INTRODUCTION Interest in spatial approaches to CSCW has grown over recent years. Specific examples of the spatial approach include media spaces [2], spatially oriented video conferencing [12, 13, 19], collaborative virtual environments [1, 24] and <b>tele-presence</b> <b>systems</b> [14]. There has recently been some debate as to the relationship between these v [...] ...|$|E
40|$|This {{paper will}} discuss MDraw, {{a tool that}} {{supports}} cooperative work in the Tele-presence environment. The Tele-Presence project lies within a Swedish research program called MultiG, wich does research on distributed multimedia applications and high-speed networks. In the <b>Tele-Presence</b> <b>system,</b> several users can simultaneously reside in a virtual world where they can see graphical representations of each other. MDraw is a simple drawing tool, shaped like a whiteboard, that can be placed in any virtual world {{where it can be}} seen and accessed by all users present. The whiteboard supports the creation and manipulation of simple objects, like lines and ellipses, and also offers a video function that gives users the ability to display a live video picture on the board. Drawing is accomplished by using a wand, which is a six-dimensional interaction device [...] ...|$|R
40|$|Eye gaze is an {{important}} conversational resource that until now could only be supported across a distance if people were rooted to the spot. We introduce EyeCVE, the world’s first <b>tele-presence</b> <b>system</b> that allows people in different physical locations to not only see what each other are doing but follow each other’s eyes, even when walking about. Projected into each space are avatar representations of remote participants, that reproduce not only body, head and hand movements, but also those of the eyes. Spatial and temporal alignment of remote spaces allows the focus of gaze as well as activity and gesture {{to be used as}} a resource for non-verbal communication. The temporal challenge met was to reproduce eye movements quick enough and often enough to interpret their focus during a multiway interaction, along with communicating other verbal and non-verbal language. The spatial challenge met was to maintain communicational eye gaze while allowing free movement of participants within a virtually shared common frame of reference. This paper reports on the technical and especially temporal characteristics of the system...|$|R
40|$|P eople who {{experience}} an immersive VR systemusually report {{feeling as if}} they were really in the displayed virtual situation, and can often be observed behaving in accordance with that feeling, even though they know that they’re not actually there. Researchers refer to this feeling as “presence ” in virtual environ-ments, yet the term has come to have many uses and meanings, all of which evolved from the notion of <b>tele-presence</b> in teleoperator <b>systems.</b> Currently, many applications, such those that use VR in psychotherapy, rely on presence for their success. Many empirical stud-ies have attempted to determine the impact of various technological factors—such as field-of-view, frame rate, stereo, and head tracking—on how much presence VR participants reportedly experience. Typically, researchers assess this using questionnaires. However...|$|R
40|$|Terms such as "Virtual Campus" or "Virtual Faculty" are {{currently}} being {{used to refer to}} the development of educational activities using telecommunication resources that provide powerful mechanisms for distance learning. Traditional universities follow a "synchronous" teaching model, where instructor and students join together in the same space (the classroom) {{and at the same time}} (that given by the timetable). In order to translate this scenario to a distance learning environment, <b>tele-presence</b> <b>systems</b> are required; examples of those are videoconferencing systems, and many classes of groupware. The implementation of such systems is of special interest for traditional universities, because it is a way of broadening their area of action by means of removing the distance barrier. 1. Introduction to the virtual faculty or virtual campus. Terms such as Virtual Faculty or Virtual Campus {{are currently}} being used to refer to the development of educational activity using telecommun [...] ...|$|E
40|$|Tele-presence {{refers to}} {{technologies}} enabling the remote {{presence of an}} observer or operator of robotics machines - {{through the use of}} monitoring and display devices. This involves the facilitation of 3 D space perception on the basis of 2 D pictures, a problem which is of interest to engineers, and psychologists who study space and picture perception. From a functional perspective the issue requires the specification of the necessary characteristics of a tele-presence system for effective task performance by a human observer. Since the central problem in picture perception is the conflict between the 3 D re-presentation of the scene and the 2 D surface of the picture, one possibility for <b>tele-presence</b> <b>systems</b> to reduce this conflict consists in the use of a camera which is slaved to observer movement. Thus the video picture is yoked to the head movement of the observer: changes in the video picture viewed by the observer emulate the changes that would have occurred in the visual field if the observer was viewing the scene directly. The explanation for reduced cue conflict and improved depth perception in pictures lies in the availability of motion parallax information. The main aim of this research was to see whether tele-presence which provides motion parallax information on a video picture improves depth perception compared to static tele-presence. While theoretical claims concerning the usefulness of motion parallax have a long history, the empirical findings are more equivocal. The basic design compared depth perception of a moving observer with that of a stationary observer. Two initial experiments showed that the movement condition leads to more accurate depth perception than the stationary condition, both under tele-presence and direct viewing conditions. Experiments 4 to 7 showed that active observation leads only to non- significantly better accuracy than passive observation. Interrupting the natural link between action and perception by reversing the picture tends to reduce the difference between the movement and the stationary condition. However, combining the analysis of the active, passive and reverse picture conditions did not lead to significant differences. A further experiment using an adjustment task supported the finding that reverse viewing does not reduce accuracy. In general the differences between the movement and the static condition while significant were not very strong which suggested that other sources of information such as visual angle information may have specified depth to a considerable extent. Simulation of fully remote tele-presence was expected to provide stronger differences. However, the differences were small and explainable in terms of short term learning processes resulting in perceptual fixity, i. e. an inability {{to take advantage of the}} information available. It was concluded that motion parallax is probably only a weak cue to depth under practical circumstances, and that learning effects in <b>tele-presence</b> <b>systems</b> require further attention. Future attention should be directed at learning processes and at the complexity of the stimulus displays. The study of learning processes may help to understand the consistent finding of large individual differences in using motion parallax information. And the study of more complex stimulus displays would enable a more adequate assessment of the ecological emphasis on the role of motion parallax...|$|E
40|$|Real-world {{radiance}} values {{can range}} over eight {{orders of magnitude}} from starlight to direct sunlight but few digital cameras capture more than three orders in a single Low Dynamic Range (LDR) image. We approach this problem using established High Dynamic Range (HDR) techniques in which multiple images are captured with different exposure times so that all portions of the scene are correctly exposed at least once. These images are then combined to create an HDR image capturing {{the full range of}} the scene. HDR capture introduces new challenges; movement in the scene creates faded copies of moving objects, referred to as ghosts. Many techniques have been introduced to handle ghosting, but typically they either address specific types of ghosting, or are computationally very expensive. We address ghosting by first detecting moving objects, then reducing their contribution to the final composite on a frame-by-frame basis. The detection of motion is addressed by performing change detection on exposure-normalized images. Additional special cases are developed based on a priori knowledge of the changing exposures; for example, if exposure is increasing every shot, then any decrease in intensity in the LDR images is a strong indicator of motion. Recent Superpixel over-segmentation techniques are used to refine the detection. We also propose a novel solution for areas that see motion throughout the capture, such as foliage blowing in the wind. Such areas are detected as always moving, and are replaced with information from a single input image, and the replacement of corrupted regions can be tailored to the scenario. We present our approach {{in the context of a}} panoramic tele-presence system. <b>Tele-presence</b> <b>systems</b> allow a user to experience a remote environment, aiming to create a realistic sense of "being there" and such a system should therefore provide a high quality visual rendition of the environment. Furthermore, panoramas, by virtue of capturing a greater proportion of a real-world scene, are often exposed to a greater dynamic range than standard photographs. Both facets of this system therefore stand to benefit from HDR imaging techniques. We demonstrate the success of our approach on multiple challenging ghosting scenarios, and compare our results with state-of-the-art methods previously proposed. We also demonstrate computational savings over these methods...|$|E
40|$|A {{portable}} comprehensive navigational {{system has}} been developed that both robotic and human explorers can use to determine their location, attitude, and heading anywhere on the lunar surface independent of external infrastructure (needs no Lunar satellite network, line of sight to the Sun or Earth, etc.). The system combines robust processing power with an extensive topographical database to create a real-time atlas (GIS Geospatial Information System) that is able to autonomously control and monitor both single unmanned rovers and fleets of rovers, as well as science payload stations. The system includes provisions for teleoperation and <b>tele-presence.</b> The <b>system</b> accepts (but does not require) inputs {{from a wide range}} of sensors. A means was needed to establish a location when the search is taken deep in a crater (looking for water ice) and out of view of Earth or any other references. A star camera can be employed to determine the user's attitude in menial space and stellar map in body space. A local nadir reference (e. g., an accelerometer that orients the nadir vector in body space) can be used in conjunction with a digital ephemeris and gravity model of the Moon to isolate the latitude, longitude, and azimuth of the user on the surface. That information can be used in conjunction with a Lunar GIS and advanced navigation planning algorithms to aid astronauts (or other assets) to navigate on the Lunar surface...|$|R
40|$|Abstract. Stroke is {{a leading}} cause of {{disability}} in particular affecting older people. Although the causes of stroke are well known and it is possible to reduce these risks, {{there is a need to}} improve rehabilitation techniques. Early studies in the literature suggest that early intensive therapies can enhance patient‟s recovery. According to physiotherapy literature, attention and motivation are key factors for motor relearning following stroke. Machine mediated therapy offers a great potential to improve the outcome of patients engaged on rehabilitation for upper limb motor impairment. Haptic Interfaces are a particular group of Robots that are attractive due to their ability to safely interact with humans. They can enhance traditional therapy tools, provide therapy “on demand ” and can present accurate objective measurements of patient‟s progression. Our recent studies suggest the use of <b>tele-presence</b> and VR-based <b>systems</b> can potentially motivate patients to exercise for longer periods of time. The creation of human-like trajectories is essential for retraining upper limb movements of patients that have lost manipulation functions following stroke. By coupling models for human arm movement with haptic interfaces and VR technology it is possible to create a new class of robot mediated neuro rehabilitation tools. This paper provides an overview on different approaches to robot mediated therapy and describes a system based on haptics and virtual reality visualisation techniques, where particular emphasis is given to different control strategies for interaction derived from minimum jerk theory and the aid of virtual and mixed reality based exercises...|$|R
40|$|Haptic {{collaborative}} {{tasks are}} actions performed jointly by several partners, involving {{direct or indirect}} physical contact among them. A typical example of such tasks are collaborative manipulation tasks, where the partners apply forces on a same object to impose it a desired motion or {{bring it to a}} target location. Human beings learn naturally how to perform such tasks with other human partners, but implementing such behaviors on a robotic platform is challenging. When jointly manipulating an object, the partners no longer act independently, and must negotiate a common plan to perform the task. To avoid conflicts among the partners' intentions, the leader-follower model defines a task leader, who imposes a task plan to the other partners, while the latter act as follower and follow at best the intentions of the leader. This model has often been used in physical Human-Robot Interaction (pHRI). Because robotic systems have limited cognitive capabilities in comparison to human beings, a follower role has generally been assigned to robotic systems to cooperate with human operators. Recently, thanks to the increasing computational power embedded into the robots, more and more initiative has been given to robotic assistants. In some recent works, robots were sometimes even given the possibility to lead human operators. In the context of physical tasks, where the partners are in direct or indirect contact through an object and exchange mechanical energy, we believe that the haptic channel is a favored and fast way for the partners to exchange information about their intentions. Therefore, this thesis will focus on the kinesthetic aspects of collaborative tasks. The long-term aim of the project is to endow humanoid robots with the necessary haptic skills to perform collaborative tasks with a human operator as a partner rather than as a helper. The work presented here proposes solutions towards this direction. In its first part, our contribution is to extend the leader-follower model to continuous, time-varying role distributions among the partners in the context of haptic dyadic collaborative tasks. This model describes the behavior of each partner of the dyad as a variable weighting between the two extreme leader and follower behaviors. Our goal is to abstract the concept of role distribution from the implementation of the underlying controllers, and to describe the behavior of dyads using two independent functions that will shape the behavior of each partner in term of leadership. We exemplify the use of our model in a virtual reality scenario where a human operator manipulates an object in cooperation with a virtual robotic system. We also explore possible strategies to exploit it. The problem we adress is to define how the weighting between both behaviors can be adjusted automatically on a robotic system, depending on various criteria such as constraints of the robot or knowledge from human-human haptic interaction. Simulations and experiments conducted on a humanoid robot are presented to illustrate the proposed solutions. The results show that the extended leader-follower model can be applied to realize collaborative tasks with a human operator while avoiding self-collision. The model also encompasses the specialization phenomenon recently highlighted in human-human collaborative haptic tasks. We then propose to use a programming by demonstration method to teach collaborative skills to a robotic system. This method uses a probabilistic framework to encode the characteristics of the task and reproduce it autonomously. This framework is based on Gaussian Mixture Models and Gaussian Mixture Regression and has been successfully applied to various stand-alone tasks. We remind the main components of this framework and present its application to collaborative lifting tasks between a humanoid robot and a human operator. Our first contribution is the design of the experimental setup, based on a teleoperation system whith kinesthetic feedback which allows the human teacher to demonstrate the task while taking into account the constraints and sensor data of the robotic system. The main contribution, however, is the use of this methodology to attempt to assess the validity of our extended leader-follower model, by highlighting smooth switching behaviors on human partners during collaborative lifting tasks. The experimental data aquired during reproductions of the task is analyzed within this perspective. The second part of this thesis focuses on the control of humanoid robots in the context of pHRI. We examine several paradigms of interaction: interaction between two remote human partners through a <b>tele-presence</b> <b>system,</b> direct interaction between an autonomous humanoid robot and a human operator, and collaborative transportation tasks between a human operator and a humanoid robot. Behind these different paradigms of interaction lies one common problem: the generation of whole-body motion and gait in response to external forces that arise from the haptic interaction with a human operator. This thesis does not aim at tackling the problem of gait generation at the mechanical and control level. We will rather use state-of-the-art algorithms which do not consider external disturbances, and show to what extent they can be used to generate complex and intuitive collaborative behaviors. Our contributions in this part are thus to integrate impedance control and gait generation within an existing control architecture in a generic and flexible way in order to (i) use the resulting controller in various contexts, (ii) demonstrate how the basic principles of impedance control can be implemented on a complex platform biped humanoid robot while exploiting all the capabilities of such platforms and (iii), highlight the limitations of the passivity-based approaches often used in pHRI, and thereby justify further research in the field of pHRI. The work presented in this part has been integrated within a complex demonstrator where the robot walks in a teleoperated manner and performs autonomously a collaborative transportation task with a human operator. (résumé en anglais uniquement...|$|R
40|$|Movement {{disorders}} (MD) {{include a}} group of neurological disorders that involve neuromotor systems. MD can result in several abnormalities ranging from an inability to move, to severe constant and excessive movements. Strokes are {{a leading cause of}} disability affecting largely the older people worldwide. Traditional treatments rely on the use of physiotherapy that is partially based on theories and also heavily reliant on the therapists training and past experience. The lack of evidence to prove that one treatment is more effective than any other makes the rehabilitation of stroke patients a difficult task. UL motor re-learning and recovery levels tend to improve with intensive physiotherapy delivery. The need for conclusive evidence supporting one method over the other and the need to stimulate the stroke patient clearly suggest that traditional methods lack high motivational content, as well as objective standardised analytical methods for evaluating a patient's performance and assessment of therapy effectiveness. Despite all the advances in machine mediated therapies, there is still a need to improve therapy tools. This chapter describes a new approach to robot assisted neuro-rehabilitation for upper limb rehabilitation. Gentle/S introduces a new approach on the integration of appropriate haptic technologies to high quality virtual environments, so as to deliver challenging and meaningful therapies to people with upper limb impairment in consequence of a stroke. The described approach can enhance traditional therapy tools, provide therapy "on demand" and can present accurate objective measurements of a patient's progression. Our recent studies suggest the use of <b>tele-presence</b> and VR-based <b>systems</b> can potentially motivate patients to exercise for longer periods of time. Two identical prototypes have undergone extended clinical trials in the UK and Ireland with a cohort of 30 stroke subjects. From the lessons learnt with the Gentle/S approach, it is clear also that high quality therapy devices of this nature have a role in future delivery of stroke rehabilitation, and machine mediated therapies should be available to patient and his/her clinical team from initial hospital admission, through to long term placement in the patient's home following hospital discharge...|$|R

