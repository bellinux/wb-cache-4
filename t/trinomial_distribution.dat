19|10|Public
2500|$|In {{the case}} of a {{discrete}} variable, the sampling distribution of the median for small-samples can be investigated as follows. [...] We take the sample size to be an odd number [...] [...] If a given value [...] is to be the median of the sample then two conditions must be satisfied. [...] The first is that at most [...] observations can have a value of [...] or less. [...] The second is that at most [...] observations can have a value of [...] or more. [...] Let [...] be the number of observations which have a value of [...] or less and let [...] be the number of observations which have a value of [...] or more. [...] Then [...] and [...] both have a minimum value of 0 and a maximum of [...] [...] If an observation has a value below , it is not relevant how far below [...] it is and conversely, if an observation has a value above , it is not relevant how far above [...] it is. [...] We can therefore represent the observations as following a <b>trinomial</b> <b>distribution</b> with probabilities [...] , [...] and [...] [...] The probability that the median [...] will have a value [...] is then given by ...|$|E
5000|$|Hardy-Weinberg {{principle}} (it is a <b>trinomial</b> <b>distribution</b> with probabilities [...] ) ...|$|E
5000|$|Multinomial {{distribution}} (Hardy-Weinberg is a <b>trinomial</b> <b>distribution</b> with probabilities [...] ) ...|$|E
40|$|We {{propose a}} robust model for {{discovering}} differentially expressed genes which directly incorporates biological significance, i. e., effect dimension. Using the so-called c-fold rule, we transform the expressions into a nominal observed random variable with three categories: below a fixed lower threshold, above a fixed upper threshold or within the two thresholds. Gene expression data is then {{transformed into a}} nominal variable with three levels possibly originated by three different distributions corresponding to under expressed, not differential, and over expressed genes. This leads to a statistical model for a 3 -component mixture of <b>trinomial</b> <b>distributions</b> with suitable constraints on the parameter space. In order to obtain the MLE estimates, we show how to implement a constrained EM algorithm with a latent label for the corresponding component of each gene. Different strategies for a statistically significant gene discovery are discussed and compared. We illustrate the method on a little simulation study and a real dataset on multiple sclerosis. ...|$|R
40|$|Likelihood ratio {{tests are}} widely used to test {{statistical}} hypotheses about parametric families of probability distributions. If interest is restricted to a subfamily of distributions, then it is natural to inquire if the restricted LRT is superior to the unrestricted LRT. Marden's general LRT conjecture posits that any restriction placed on the alternative hypothesis will increase power. The only published counterexample to this conjecture is rather technical and involves a restriction that maintains the dimension of the alternative. We formulate the dimension-restricted LRT conjecture, which posits that any restriction that replaces a parametric family with a subfamily of lower dimension will increase power. Under standard regularity conditions, we then demonstrate that the restricted LRT is asymptotically {{more powerful than the}} unrestricted LRT for local alternatives. Remarkably, however, even the dimension-restricted LRT conjecture fails in the case of finite samples. Our counterexamples involve subfamilies of multinomial distributions. In particular, our study of the Hardy-Weinberg subfamily of <b>trinomial</b> <b>distributions</b> provides a simple and elegant demonstration that restrictions may not increase power. Comment: 21 pages, 1 figur...|$|R
40|$|The price {{time series}} of the Italian {{government}} bonds (BTP) futures is studied by means of scaling concepts originally developed for random walks in statistical physics. The series of overnight price di erences is mapped onto a one-dimensional random walk: the bond walk. The analysis of the root mean square uctuation function and of the auto-correlation function indicates the absence of both short- and long-range correlations in the bond walk. A simple Monte Carlo simulation of a random walk with <b>trinomial</b> probability <b>distribution</b> is able to reproduce the mai...|$|R
5000|$|The {{numbers of}} the Tetrahedron {{can also be found}} in the <b>Trinomial</b> <b>Distribution.</b> This is a {{discrete}} probability distribution used to determine the chance some combination of events occurs given three possible outcomes−the number of ways the events could occur is multiplied by the probabilities that they would occur. The formula for the <b>Trinomial</b> <b>Distribution</b> is: ...|$|E
5000|$|Each {{number in}} any layer is a {{coefficient}} of the <b>Trinomial</b> <b>Distribution</b> and the trinomial expansion. This non-linear arrangement {{makes it easier}} to: ...|$|E
5000|$|In mathematics, Pascal's pyramid is a {{three-dimensional}} {{arrangement of the}} trinomial numbers, which are the coefficients of the trinomial expansion and the <b>trinomial</b> <b>distribution.</b> Pascal's Pyramid is the three-dimensional analog of the two-dimensional Pascal's triangle, which contains the binomial numbers and relates to the binomial expansion and the binomial distribution. The binomial and trinomial numbers, coefficients, expansions, and distributions are subsets of the multinomial constructs with the same names.|$|E
30|$|In our paper, we {{consider}} a ratio involving {{two or more}} random variables that jointly have a multinomial distribution. This situation is similar to relative risk or risk ratio which is {{the ratio of the}} probability of an event occurring (for example, developing a disease or being injured) in an exposed group to the probability of the event occurring in a comparison, non-exposed group. However, while the probabilities in the risk ratio are independent (in the sense that they describe two independent events in two independent groups), in our case, the probabilities are tied together through the covariance between multinomial categories. These ratios serve as a common framework for opinion polls, statistical quality control, and consumer preference studies. Confidence intervals for the odds ratio, which can be easily calculated, if the standard deviation is known, are especially important for applications. Nelson (1972) presented estimates, confidence intervals, and hypothesis tests for the odds ratio in <b>trinomial</b> <b>distributions.</b> Piegorsch and Richwine (2001) examined some types of confidence intervals in the context of analysis of genetic mutant spectra. Quesenberry and Hurst (1964) and Goodman (1965) explored methods for obtaining a set of simultaneous confidence intervals for the probabilities of a multinomial distribution. A comparison of performance of various confidence intervals also appeared in Alghamdi (2015); Aho and Bowyer (2015). To the best of our knowledge, however, there has been no analytical treatment of the ratio of multinomial proportions including derivations for formulae for the mean and variance of such a ratio.|$|R
40|$|We {{analyze the}} time series of {{overnight}} returns for the bund and btp futures exchanged at LIFFE (London). The overnight returns of both assets are mapped onto a one-dimensional symbolic-dynamics random walk: The `bond walk'. During the considered period (October 1991 - January 1994) the bund-future market opened {{earlier than the}} btp-future one. The crosscorrelations between the two bond walks, as well as estimates of the conditional probability, {{show that they are}} not independent; however each walk can be modeled by means of a <b>trinomial</b> probability <b>distribution.</b> Monte Carlo simulations confirm {{that it is necessary to}} take into account the bivariate dependence in order to properly reproduce the statistical properties of the real-world data. Various investment strategies have been devised to exploit the `prior' information obtained by the aforementioned analysis. ...|$|R
40|$|The price {{time series}} of the Italian {{government}} bonds (BTP) futures is studied by means of scaling concepts originally developed for random walks in statistical physics. The series of overnight price differences is mapped onto a one-dimensional random walk: the bond walk. The analysis of the root mean square fluctuation function and of the auto-correlation function indicates the absence of both short- and long-range correlations in the bond walk. A simple Monte Carlo simulation of a random walk with <b>trinomial</b> probability <b>distribution</b> is able to reproduce the main features of the bond walk. PACS numbers: 05. 40. +j, 05. 90. +m Typeset using REVT E X 1 I. INTRODUCTION The relationship between economics and the physical sciences has a long and interesting history. Outstanding economists of the past explicitly inspired their work {{to the principles of}} Newtonian physics and statistical mechanics, attracted by the success of these theories. It is, for instance, the case of Vilfredo P [...] ...|$|R
5000|$|In {{the case}} of a {{discrete}} variable, the sampling distribution of the median for small-samples can be investigated as follows. We take the sample size to be an odd number [...] If a given value [...] is to be the median of the sample then two conditions must be satisfied. The first is that at most [...] observations can have a value of [...] or less. The second is that at most [...] observations can have a value of [...] or more. Let [...] be the number of observations which have a value of [...] or less and let [...] be the number of observations which have a value of [...] or more. Then [...] and [...] both have a minimum value of 0 and a maximum of [...] If an observation has a value below , it is not relevant how far below [...] it is and conversely, if an observation has a value above , it is not relevant how far above [...] it is. We can therefore represent the observations as following a <b>trinomial</b> <b>distribution</b> with probabilities [...] , [...] and [...] The probability that the median [...] will have a value [...] is then given by ...|$|E
40|$|Probabilistic {{models have}} been {{frequently}} applied in professional sports to quantify the importance of winning a match. However, existing models fail {{to account for the}} possibility of a draw outcome, a frequent occurrence in multi-result sports like football. In this paper, we calculate match importance using a <b>trinomial</b> <b>distribution</b> model that accounts for the possibility of a drawn result. Using German Bundesliga football, we demonstrate through case studies that the importance of a match can be evaluated with respect to win and draw results separately...|$|E
40|$|To bring {{correlation}} between binomial random variables {{is an important}} statistical problem {{with a lot of}} theoretical and practical applications. In this paper we provide a new formulation of bivariate binomial distribution in the sense that marginally each of the two random variables has a binomial distribution and they have some non-zero correlation in the joint distribution. A 2 x 2 contingency table is the immediate application of the proposed model. Bivarite binomial distribution 2 x 2 contingency table <b>Trinomial</b> <b>distribution</b> Quasi-binomial distribution Maximum likelihood estimate Odds ratio...|$|E
40|$|Ballistic {{particles}} {{interacting with}} irregular surfaces {{are representative of}} many physical problems in the Knudsen diffusion regime. In this paper, the collisions of ballistic particles interacting with an irregular surface modeled by a quadratic Koch curve, are studied numerically. The q moments of the source spatial distribution of collision numbers μ(x) are characterized by a sequence of "collision exponent" τ(q). The measure μ(x) {{is found to be}} multifractal even when a random micro-roughness (or random re-emission) of the surface exists. The dimensions f(α), obtained by a Legendre transformation from τ(q), consist of two parabolas corresponding to a trinomial multifractal. This is demonstrated for a particular case by obtaining an exact f(α) for a multiplicative <b>trinomial</b> mass <b>distribution.</b> The <b>trinomial</b> nature of the multifractality is related to the type of surface macro-irregularity considered here and is independent of the micro-roughness of the surface which however influence the values of α_min and α_max. The information dimension D_I increases significantly with the micro-roughness of the surface. Interestingly, in contrast with this point of view, the surface seems to work uniformly. This correspond to an absence of screening effects in Knudsen diffusion. Comment: 10 pages, 9 figure...|$|R
40|$|Many modern liquid {{scintillation}} counters use two coincident photomultiplier (PM) tubes to reduce background noise. These counters detect most charged particle emitting nuclides with an efficiency near 1; however, they are noticeably less efficient for detecting lower energy emissions from nuclides such as tritium, because each decay produces so few photons that any coincident detection is less probable. For quenched samples, the photon production is further reduced, yielding a corresponding contraction {{of the observed}} energy spectra. The probability (or efficiency) for detecting a charged particle increases from 0 to 1 as its photon production (or its energy) increases from 0 to higher values. A model for this probability was developed and tested against experimental measurements. A <b>trinomial</b> probability <b>distribution</b> modeled the efficiency {{as a function of}} observed spectral energy. The three probabilities of the trinomial include two for photon detection by the two PM tubes and one for nondetection. For a TRI. CARB 2000 CNLL {{liquid scintillation}} analyzer, reasonable efficiency models resulted when using the trinomial formalism as a basis for empiri-cally selecting the efficiencies. Using measurements of low-energy Auger and conversion electrons of quenched 57 Co standards as an experimental guide, four efficiency curves were modeled and tested. Scintillation spectra of 3 H and 14 C quenched standards were convoluted with the efficiency curves to test each model. The dpm predicted by the best model agreed with that known for the standards, provided that cpm/dpm> 0. 25. The method may be extended down to cpm/dpm 0. 1 by incorporating a "bootstrap " correction technique...|$|R
40|$|The {{study of}} {{proportions}} {{is a common}} topic in many fields of study. The standard beta distribution or the inflated beta distribution may be a reasonable choice to fit a proportion in most situations. However, they do not fit well variables that do not assume values in the open interval (0, c), 0 < c < 1. For these variables, the authors introduce the truncated inflated beta distribution (TBEINF). This proposed distribution {{is a mixture of}} the beta distribution bounded in the open interval (c, 1) and the <b>trinomial</b> <b>distribution.</b> The authors present the moments of the distribution, its scoring vector, and Fisher information matrix, and discuss estimation of its parameters. The properties of the suggested estimators are studied using Monte Carlo simulation. In addition, the authors present an application of the TBEINF distribution for unemployment insurance data. CAPESCNP...|$|E
40|$|A superintegrable, {{discrete}} {{model of the}} quantum isotropic oscillator in two-dimensions is introduced. The system is defined on the regular, infinite-dimensional N×N lattice. It is governed by a Hamiltonian expressed as a seven-point difference operator involving three parameters. The exact solutions of the model are given {{in terms of the}} two-variable Meixner polynomials orthogonal with respect to the negative <b>trinomial</b> <b>distribution.</b> The constants of motion of the system are constructed using the raising and lowering operators for these polynomials. They are shown to generate an su(2) invariance algebra. The two-variable Meixner polynomials are seen to support irreducible representations of this algebra. In the continuum limit, where the lattice constant tends to zero, the standard isotropic quantum oscillator in two dimensions is recovered. The limit process from the two-variable Meixner polynomials to a product of two Hermite polynomials is carried out by involving the bivariate Charlier polynomials. Comment: Minor modifications, 14 pages, 4 figure...|$|E
40|$|In 1971, Schelling {{introduced}} {{a model in}} which families move if they have too many neighbors of the opposite type. In this paper we will consider a metapopulation version of the model in which a city is divided into N neighborhoods {{each of which has}} L houses. There are ρNL red families and ρNL blue families. Families are happy if there are ≤ ρcL families of the opposite type in their neighborhood, and unhappy otherwise. Each family moves to each vacant house at rates that depend on their happiness at their current location and that of their destination. Let Tri(pR, pB) be a <b>trinomial</b> <b>distribution</b> with probability pR and pB of red and blue, and probability 1 − pR − pB of empty. Suppose first that ρc> 0. 25963. In this case, if neighborhoods are large then there are critical values ρb ρb a new segregated equilibrium (1 / 2) Tri(ρ 1, ρ 2) + (1 / 2) Tri(ρ 2, ρ 1) appears with ρ 1 > ρc> ρ 2. When ρb ρd the segregated state is the unique stationary distribution. When ρc < 0. 25963, Tri(ρ, ρ) may be th...|$|E
40|$|The {{objective}} {{of this study is}} to identify can waste into three types based on the images by using a probability approach of <b>trinomial</b> <b>distribution</b> in term regression. Predictor variables considered are the color intensity of red, green, and blue of the images taken at the top, down, and side pose successively. From an independence test between each of the predictor variable and can waste type noted that only the color intensity of red which the image taken at top pose that does not correspond to the can waste types. Based on the Nagelkerke value is found that the variance of the predictor variable data in identifying the can waste type is able to explain the variance of the types of 59. 1 percent. The final model show that the significant predictor variables are the colors intensity of green and blue which the image taken at the top pose, the color intensity of red which the image taken at down pose, and the color intensity of red, green and blue which the image taken at side pose successively. The model can identify cans waste into three types based on the images correctly by 73. 13 %...|$|E
40|$|Screening of grouped {{urine sample}} was {{suggested}} during the Second World War {{as a method}} for reducing the cost of detecting syphilis in U. S. soldiers. Grouping {{has been used in}} epidemiological studies for screening of human immunodeficiency virus HIV/AIDS antibody to help curb the spread of the virus in recent studies. It reduces the cost of testing and more importantly it offers a feasible way to lower the misclassifications associated with labeling samples when imperfect tests are used. Furthermore, misclassifications can be reduced by employing a re-testing design in a group testing procedure. This study has developed a computational statistical model for classifying a large sample of interest based on a proposed design of group testing with re-testing. This model permits computation of moments on the number of tests and misclassification arising in this design. Simulated data from a multinomial distribution (specifically a <b>trinomial</b> <b>distribution)</b> has been used to illustrate these computations. From our study, it has been established that re-testing reduces misclassifications significantly and more so, it is stable at high rates of probability of incidences as compared to Dorfman procedure although re-testing comes with a cost i. e. {{increase in the number of}} tests. Re-testing considered reduces the sensitivity of the testing scheme but at the same time it improves the specificity...|$|E
40|$|Multilocus {{sequence}} data provide {{far greater}} power to resolve species limits than the single locus data typically used for broad surveys of clades. However, current statistical methods {{based on a}} multispecies coalescent framework are computationally demanding, {{because of the number}} of possible delimitations that must be compared and time-consuming likelihood calculations. New methods are therefore needed to open up the power of multilocus approaches to larger systematic surveys. Here, we present a rapid and scalable method that introduces two new innovations. First, the method reduces the complexity of likelihood calculations by decomposing the tree into rooted triplets. The distribution of topologies for a triplet across multiple loci has a uniform <b>trinomial</b> <b>distribution</b> when the 3 individuals belong to the same species, but a skewed distribution if they belong to separate species with a form that is specified by the multispecies coalescent. A Bayesian model comparison framework was developed and the best delimitation found by comparing the product of posterior probabilities of all triplets. The second innovation is a new dynamic programming algorithm for finding the optimum delimitation from all those compatible with a guide tree by successively analyzing subtrees defined by each node. This algorithm removes the need for heuristic searches used by current methods, and guarantees that the best solution is found and potentially could be used in other systematic applications. We assessed the performance of the method with simulated, published and newly generated data. Analyses of simulated data demonstrate that the combined method has favourable statistical properties and scalability with increasing sample sizes. Analyses of empirical data from both eukaryotes and prokaryotes demonstrate its potential for delimiting species in real cases...|$|E
40|$|Three {{generalised}} distributions are {{studied in}} this thesis from different aspects. The Hurwitz-Lerch zeta distribution (HLZD) that generalises the logarithmic distribution and a class of distributions that follows the power law is considered. To investigate the effects of parameters on the stochastic properties of the HLZD, stochastic orders between members in this large family are established. A relationship between the tail behaviours of the HLZD and that of a class of generalised logarithmic distribution is highlighted. The HLZD has shown good flexibilities in empirical modelling. A robust probability generating function based estimation method using Hellinger-type divergence is implemented in data-fitting {{and the results are}} compared with various other generalisations of logarithmic distribution. An augmented probability generating function is constructed to overcome the difficulties of this estimation procedure when some data are grouped. The Poisson-stopped sum of the Hurwitz-Lerch zeta distribution (Poisson-HLZD) is then proposed as a new generalisation of the negative binomial distribution. Several methods have been used in deriving the probability mass function for this new distribution to show the connections among different approaches from mathematics, statistics and actuarial science. Basic statistical measures and probabilistic properties of the Poisson-HLZD are examined and the usefulness of the model is demonstrated through examples of data-fitting on some real life datasets. Finally, the inverse <b>trinomial</b> <b>distribution</b> (ITD) is reviewed. Both Poisson-HLZD and ITD are proved to have mixed Poisson formulation, which extend the applications of the models for various phenomena. The associated mixing distribution for the ITD is obtained as an infinite Laguerre series and the result is compared to some numerical inversions of Laplace transform...|$|E
40|$|Assessing match {{importance}} in professional football (soccer) is beneficial {{for a number}} of reasons, primarily in predictive modelling for match outcome and attendance. Previously, modelling of match importance has focused on some specific end-of-season aim, such as winning the league championship or avoiding relegation. Calculation has typically involved the use of retrospective measures or complex computer simulation procedures. These methods have drawbacks, where retrospective measures cannot be implemented into a live season; and complex computer simulation procedures require a great deal of runtime. In this paper, we explore the effect of simplifying the calculation of match importance using a probabilistic method by relaxing match outcome independence assumptions at varying degrees of severity in an attempt to counteract these drawbacks. The probabilistic measure builds off of previous research by defining match importance as the difference between success probabilities conditional on the result of a match. Probabilities follow a <b>trinomial</b> <b>distribution</b> to account for the possibility of each team winning, losing or drawing a match; and for simplicity remain constant throughout a season and independent of the teams playing. A complete season simulation model provided the basis for comparison with the simplified model, where analysis compared the relative size of the bias of outcome probabilities and the characteristics of importance distributions for teams aiming to finish in the top position {{at the end of the}} season. Results indicated that the complexity of the complete simulation procedure is not required as the distribution of importance remains similar to that of the simpler probabilistic measure. Results also suggested that team strength should be incorporated into match importance calculations because the assumption of constant match outcome probabilities between teams underestimates the variation in round by round cumulative season points totals...|$|E
40|$|PhDThis thesis {{consists}} of two parts. The purpose of {{the first part of}} the research is to obtain Bayesian sample size determination (SSD) using loss or utility function with a linear cost function. A number of researchers have studied the Bayesian SSD problem. One group has considered utility (loss) functions and cost functions in the SSD problem and others not. Among the former most of the SSD problems are based on a symmetrical squared error (SE) loss function. On the other hand, in a situation when underestimation is more serious than overestimation or vice-versa, then an asymmetric loss function should be used. For such a loss function how many observations do we need to take to estimate the parameter under study? We consider different types of asymmetric loss functions and a linear cost function for sample size determination. For the purposes of comparison, firstly we discuss the SSD for a symmetric squared error loss function. Then we consider the SSD under different types of asymmetric loss functions found in the literature. We also introduce a new bounded asymmetric loss function and obtain SSD under this loss function. In addition, to estimate a parameter following a particular model, we present some theoretical results for the optimum SSD problem under a particular choice of loss function. We also develop computer programs to obtain the optimum SSD where the analytic results are not possible. In the two parameter exponential family it is difficult to estimate the parameters when both are unknown. The aim of the second part is to obtain an optimum decision for the two parameter exponential family under the two parameter conjugate utility function. In this case we discuss Lindleyâ 8 ̆ 09 ̆ 9 s (1976) optimum decision for one 6 parameter exponential family under the conjugate utility function for the one parameter exponential family and then extend the results to the two parameter exponential family. We propose a two parameter conjugate utility function and then lay out the approximation procedure to make decisions on the two parameters. We also offer a few examples, normal distribution, <b>trinomial</b> <b>distribution</b> and inverse Gaussian distribution and provide the optimum decisions on both parameters of these distributions under the two parameter conjugate utility function...|$|E

