3798|10000|Public
25|$|Regression for {{prediction}}. Here a {{model is}} fitted {{to provide a}} prediction rule for application {{in a similar situation}} to which the data used for fitting apply. Here <b>the</b> <b>dependent</b> <b>variables</b> corresponding to such future application would be subject to the same types of observation error as those in the data used for fitting. It is therefore logically consistent to use the least-squares prediction rule for such data.|$|E
25|$|Repeated-measures {{experiments}} are those which take place through intervention on multiple occasions. In {{research on the}} effectiveness of psychotherapy, experimenters often compare a given treatment with placebo treatments, or compare different treatments against each other. Treatment type is the independent variable. <b>The</b> <b>dependent</b> <b>variables</b> are outcomes, ideally assessed in several ways by different professionals. Using crossover design, researchers can further increase the strength of their results by testing both of two treatments on two groups of subjects.|$|E
25|$|Confidence limits can {{be found}} if the {{probability}} distribution of the parameters is known, or an asymptotic approximation is made, or assumed. Likewise statistical tests on the residuals can be made if the probability distribution of the residuals is known or assumed. The probability distribution of any linear combination of <b>the</b> <b>dependent</b> <b>variables</b> can be derived if the probability distribution of experimental errors is known or assumed. Inference is particularly straightforward if the errors are assumed to follow a normal distribution, which implies that the parameter estimates and residuals will also be normally distributed conditional on {{the values of the}} independent variables.|$|E
50|$|Manipulations are not {{intended}} to verify that the manipulated factor caused variation in <b>the</b> <b>dependent</b> <b>variable.</b> This is verified by random assignment, manipulation before measurement of <b>the</b> <b>dependent</b> <b>variable,</b> and statistical tests of effect of the manipulated <b>variable</b> on <b>the</b> <b>dependent</b> <b>variable.</b> Thus, a failed manipulation check does not refute {{the hypothesis that the}} manipulation caused variation in <b>the</b> <b>dependent</b> <b>variable.</b>|$|R
50|$|Regress <b>the</b> <b>dependent</b> <b>variable</b> on <b>the</b> {{independent}} variable {{to confirm that}} the {{independent variable}} is a significant predictor of <b>the</b> <b>dependent</b> <b>variable.</b>|$|R
40|$|Bayesian tobit {{regression}} estimates {{a linear}} regression {{model with a}} censored <b>dependent</b> <b>variable</b> using a Gibbs sampler. <b>The</b> <b>dependent</b> <b>variable</b> may be censored from below and/or from above. For other linear regression models with fully observed <b>dependent</b> <b>variables,</b> see Bayesian regression (Section??), maximum likelihood normal regression (Section??), or least squares (Section??). Syntax> z. out x. out s. out <- sim(z. out, x = x. out) Inputs zelig() accepts the following arguments to specify how <b>the</b> <b>dependent</b> <b>variable</b> is censored. ˆ below: point at which <b>the</b> <b>dependent</b> <b>variable</b> is censored from below. If <b>the</b> <b>dependent</b> <b>variable</b> is only censored from above, set below =-Inf. The default value is 0. ˆ above: point at which <b>the</b> <b>dependent</b> <b>variable</b> is censored from above. If <b>the</b> <b>dependent</b> <b>variable</b> is only censored from below, set above = Inf. The default value is Inf. Additional Inputs Use the following arguments to monitor the convergence of the Markov chain...|$|R
25|$|Johnson’s first {{published}} {{review of the}} research on cooperation and competition appeared in 1970 in his book, The Social Psychology of Education. This was followed by a more comprehensive review, with his brother Roger, published in the Review of Educational Research in 1974 and the editing of a special issue of the Journal of Research and Development in Education in 1978. In 1981 and 1983 he pioneered the use of meta-analysis in publishing reviews of the impact of cooperative, competitive, and individualistic on achievement/productivity and on interpersonal attraction. In 1989 he and his brother published a book, Cooperation and Competition: Theory and Research, which contained a series of meta-analyses (currently being revised with updates)on many of <b>the</b> <b>dependent</b> <b>variables</b> relevant to Social Interdependence Theory.|$|E
25|$|Different {{choices for}} the frame of {{reference}} and expansion parameters are possible in Stokes-like approaches to the non-linear wave problem. In 1880, Stokes himself inverted the dependent and independent variables, by taking the velocity potential and stream function as the independent variables, and the coordinates (x,z) as <b>the</b> <b>dependent</b> <b>variables,</b> with x and z being the horizontal and vertical coordinates respectively. This has the advantage that the free surface, in a frame of reference in which the wave is steady (i.e. moving with the phase velocity), corresponds with a line on which the stream function is a constant. Then the free surface location is known beforehand, and not an unknown part of the solution. The disadvantage is that the radius of convergence of the rephrased series expansion reduces.|$|E
2500|$|As was {{mentioned}} before, the estimator [...] is linear in y, {{meaning that it}} represents a linear combination of <b>the</b> <b>dependent</b> <b>variables</b> yis. The weights in this linear combination are functions of the regressors X, and generally are unequal. The observations with high weights are called influential {{because they have a}} more pronounced effect on the value of the estimator.|$|E
50|$|In {{traditional}} data studies, extracting {{the cause}} for <b>the</b> <b>dependent</b> <b>variable</b> to change {{may prove to be}} difficult. Experimentalists have the ability to create certain tasks that elicit <b>the</b> <b>dependent</b> <b>variable.</b>|$|R
40|$|The logit {{regression}} model is generally {{used as a}} method for estimating relationships in which <b>the</b> <b>dependent</b> <b>variable</b> is binary in nature, though it is also useful for estimation when <b>the</b> <b>dependent</b> <b>variable</b> is continuous but bounded on the unit intervals. Logit parameter estimates in this case are obtained by ordinary least squares regression on a simple transformation on <b>the</b> <b>dependent</b> <b>variable.</b> In such applications, however, measurement error in <b>the</b> <b>dependent</b> <b>variable,</b> rather than being relatively benign as in ordinary linear regressions, {{is a source of}} heteroscedasticity, calling into question the efficiency of the OLS estimator in such cases. ...|$|R
3000|$|... where <b>the</b> <b>dependent</b> <b>variable</b> of (1) is labor {{productivity}} (or {{value added}} per employee) of firm “i” in sector “j” in year “t”, while <b>the</b> <b>dependent</b> <b>variable</b> of (2) is average firm wage.|$|R
5000|$|Do {{changes in}} the {{independent}} variable(s) have significant effects on <b>the</b> <b>dependent</b> <b>variables?</b> ...|$|E
5000|$|... {{where the}} action, , is a {{functional}} of <b>the</b> <b>dependent</b> <b>variables</b> [...] with their derivatives and s itself ...|$|E
5000|$|... is {{invariant}} under brief infinitesimal {{variations in}} <b>the</b> <b>dependent</b> <b>variables.</b> In other words, they satisfy the Euler-Lagrange equations ...|$|E
5000|$|The {{measurement}} of <b>the</b> <b>dependent</b> <b>variable</b> was originally conceptualized as the public's perceived issue [...] "salience", but subsequent studies have conceptualized <b>the</b> <b>dependent</b> <b>variable</b> as awareness, attention, or concern, leading to differing outcomes.|$|R
5000|$|... (1) Temporal precedence. For example, if the {{independent}} <b>variable</b> precedes <b>the</b> <b>dependent</b> <b>variable</b> in time, this would provide evidence suggesting a directional, and potentially causal, link from {{the independent}} <b>variable</b> to <b>the</b> <b>dependent</b> <b>variable.</b>|$|R
40|$|Many {{statistical}} tests rely on {{the assumption}} that the residuals of a model are normally distributed. Rank-based inverse normal transformation (INT) of <b>the</b> <b>dependent</b> <b>variable</b> is one of the most popular approaches to satisfy the normality assumption. Studies regularly adjust for covariates and then normalize the residuals. This study investigated the effect of regressing covariates against <b>the</b> <b>dependent</b> <b>variable</b> and then applying rank-based INT to the residuals. The correlation between <b>the</b> <b>dependent</b> <b>variable</b> and covariates at each stage of processing was assessed. An alternative approach was tested of applying rank-based INT to <b>the</b> <b>dependent</b> <b>variable</b> before regressing covariates was tested. Analyses based on both simulated and real data examples demonstrated that applying rank-based INT to <b>the</b> <b>dependent</b> <b>variable</b> residuals after regressing out covariates re-introduces a linear correlation between <b>the</b> <b>dependent</b> <b>variable</b> and covariates in almost all situations. This will increase type- 1 errors and reduce power. Our proposed alternative approach, where rank-based INT was applied prior to controlling for covariate effects, gave residuals that were normally distributed and linearly uncorrelated with covariates. This approach is therefore recommended...|$|R
5000|$|... {{where the}} {{residuals}} {{are defined as}} {{the differences between the}} values of <b>the</b> <b>dependent</b> <b>variables</b> (observations) and the model values ...|$|E
50|$|<b>The</b> <b>dependent</b> <b>variables</b> are the {{intercepts}} and {{the slopes}} {{for the independent}} variables at Level 1 in the groups of Level 2.|$|E
5000|$|Consider the {{simplest}} case, {{a system with}} one independent variable, time. Suppose <b>the</b> <b>dependent</b> <b>variables</b> q are such that the action integral ...|$|E
5000|$|In {{mathematical}} modeling, <b>the</b> <b>dependent</b> <b>variable</b> is {{studied to}} see if {{and how much it}} varies as the independent variables vary. In the simple stochastic linear model [...] the term [...] is the i th value of <b>the</b> <b>dependent</b> <b>variable</b> and [...] is i th value of the independent variable. The term [...] is known as the [...] "error" [...] and contains the variability of <b>the</b> <b>dependent</b> <b>variable</b> not explained by the independent variable.|$|R
50|$|In the Arellano-Bond method, first {{difference}} {{of the regression}} equation are taken to eliminate the fixed effects. Then, deeper lags of <b>the</b> <b>dependent</b> <b>variable</b> are used as instruments for differenced lags of <b>the</b> <b>dependent</b> <b>variable</b> (which are endogenous).|$|R
50|$|Regress <b>the</b> <b>dependent</b> <b>variable</b> on both <b>the</b> {{mediator}} {{and independent}} variable {{to confirm that}} the mediator is a significant predictor of <b>the</b> <b>dependent</b> <b>variable,</b> and <b>the</b> previously significant independent variable in Step #1 is now greatly reduced, if not nonsignificant.|$|R
50|$|MPC {{uses the}} current plant measurements, the current dynamic {{state of the}} process, the MPC models, and the process {{variable}} targets and limits to calculate future changes in <b>the</b> <b>dependent</b> <b>variables.</b> These changes are calculated to hold <b>the</b> <b>dependent</b> <b>variables</b> close to target while honoring constraints on both independent and dependent variables. The MPC typically sends out only the first change in each independent variable to be implemented, and repeats the calculation when the next change is required.|$|E
5000|$|Using {{the reduced}} row echelon form, {{determine}} {{which of the}} variables [...] x1, x2, ..., xn are free. Write equations for <b>the</b> <b>dependent</b> <b>variables</b> {{in terms of the}} free variables.|$|E
5000|$|... {{where the}} {{components}} of the vector [...] are <b>the</b> (<b>dependent)</b> <b>variables</b> describing a parcel of air (such as velocity, pressure, temperature etc.) and the function [...] represents source and/or sink terms.|$|E
5000|$|... #Caption: In single {{variable}} calculus, {{a function}} is typically graphed with the horizontal axis representing {{the independent variable}} and the vertical axis representing <b>the</b> <b>dependent</b> <b>variable.</b> In this function, y is <b>the</b> <b>dependent</b> <b>variable</b> and x is the independent variable.|$|R
50|$|In {{regression}} analysis, {{the distinction}} between errors and residuals is subtle and important, and leads {{to the concept of}} studentized residuals. Given an unobservable function that relates the independent <b>variable</b> to <b>the</b> <b>dependent</b> <b>variable</b> - say, a line - the deviations of <b>the</b> <b>dependent</b> <b>variable</b> observations from this function are the unobservable errors. If one runs a regression on some data, then the deviations of <b>the</b> <b>dependent</b> <b>variable</b> observations from <b>the</b> fitted function are the residuals.|$|R
50|$|In {{statistical}} modeling, {{regression analysis}} is a statistical process for estimating {{the relationships among}} variables. It includes many techniques for modeling and analyzing several variables, when {{the focus is on}} the relationship between a <b>dependent</b> <b>variable</b> and one or more independent variables (or 'predictors'). More specifically, regression analysis helps one understand how the typical value of <b>the</b> <b>dependent</b> <b>variable</b> (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed. Most commonly, regression analysis estimates the conditional expectation of <b>the</b> <b>dependent</b> <b>variable</b> given <b>the</b> independent variables - that is, the average value of <b>the</b> <b>dependent</b> <b>variable</b> when <b>the</b> independent variables are fixed. Less commonly, the focus is on a quantile, or other location parameter of the conditional distribution of <b>the</b> <b>dependent</b> <b>variable</b> given <b>the</b> independent variables. In all cases, the estimation target is a function of the independent variables called the regression function. In regression analysis, it is also of interest to characterize the variation of <b>the</b> <b>dependent</b> <b>variable</b> around <b>the</b> regression function which can be described by a probability distribution. A related but distinct approach is necessary condition analysis (NCA), which estimates the maximum (rather than average) value of <b>the</b> <b>dependent</b> <b>variable</b> for a given value of the independent variable (ceiling line rather than central line) in order to identify what value of the independent variable is necessary but not sufficient for a given value of <b>the</b> <b>dependent</b> <b>variable.</b>|$|R
50|$|In {{mathematical}} modeling, statistical {{modeling and}} experimental sciences, there are dependent and independent variables. The models or experiments investigate how the former {{depend on the}} latter. <b>The</b> <b>dependent</b> <b>variables</b> represent the output or outcome whose variation is being studied. The independent variables represent inputs or causes, i.e., potential reasons for variation or, in the experimental setting, the variable controlled by the experimenter. Models and experiments test or determine the effects that the independent variables have on <b>the</b> <b>dependent</b> <b>variables.</b> Sometimes, independent variables may be included for other reasons, such as for their potential confounding effect, without a wish to test their effect directly.|$|E
50|$|A reverse {{process to}} regression, where {{instead of a}} future {{dependent}} variable being predicted from known explanatory variables, a known observation of <b>the</b> <b>dependent</b> <b>variables</b> is used to predict a corresponding explanatory variable.|$|E
50|$|Quantities such as {{regression}} coefficients, are statistical parameters in {{the above}} sense, since they index the family of conditional probability distributions that describe how <b>the</b> <b>dependent</b> <b>variables</b> {{are related to the}} independent variables.|$|E
40|$|We {{show that}} for any semilinear partial {{differential}} equation of order m, the infinitesimals {{of the independent}} variables depend only on the independent variables and, if m> 1 and the equation is also linear in its derivatives of order m − 1 of <b>the</b> <b>dependent</b> <b>variable,</b> then <b>the</b> infinitesimal of <b>the</b> <b>dependent</b> <b>variable</b> is at most linear on <b>the</b> <b>dependent</b> <b>variable.</b> Many examples of important partial differential equations in Analysis, Geometry and Mathematical- Physics are given in order to enlighten the main result...|$|R
50|$|When using multinomial {{logistic}} regression, {{one category}} of <b>the</b> <b>dependent</b> <b>variable</b> is {{chosen as the}} reference category. Separate odds ratios are determined for all independent variables for each category of <b>the</b> <b>dependent</b> <b>variable</b> with <b>the</b> exception of the reference category, which is omitted from the analysis. The exponential beta coefficient represents {{the change in the}} odds of <b>the</b> <b>dependent</b> <b>variable</b> being in a particular category vis-a-vis the reference category, associated with a one unit change of the corresponding independent variable.|$|R
40|$|AbstractRather general {{results are}} {{obtained}} {{for determining the}} nature of infinitesimal generators which can be admitted by a given differential equation. Simple criteria are given {{to determine whether or}} not (1) the infinitesimals of independent variables can depend only on independent variables and (2) the infinitesimal of <b>the</b> <b>dependent</b> <b>variable</b> can depend at most linearly on <b>the</b> <b>dependent</b> <b>variable.</b> Many examples are given. A trivial consequence of this paper is that an admitted Lie group, for any linear partial differential equation of at least second order, must have infinitesimals of independent variables depending only on independent variables and the infinitesimal of <b>the</b> <b>dependent</b> <b>variable</b> depends at most linearly on <b>the</b> <b>dependent</b> <b>variable.</b> This latter result previously has only been proved for a second order linear PDE...|$|R
