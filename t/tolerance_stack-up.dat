20|3|Public
5000|$|Dimensional <b>tolerance</b> <b>stack-up</b> {{analysis}} using product and manufacturing information (PMI) on CAD models ...|$|E
50|$|In {{geometric}} dimensioning and tolerancing, {{a projected}} tolerance zone is defined {{to predict the}} final dimensions and locations of features on a component or assembly subject to <b>tolerance</b> <b>stack-up.</b>|$|E
30|$|The {{tolerance}} {{chain in}} the tolerance chart depicts the sequel of machining operations and their working dimensions. The <b>tolerance</b> <b>stack-up</b> and selection of reference surfaces for the subsequent machining operations is inferred from the diagram. Subsequently, the tolerances over the dimensions {{and the stock}} removal on the machining operation are also derived.|$|E
40|$|A {{semantic}} tolerance modeling scheme {{based on}} generalized intervals was recently proposed to allow for embedding more tolerancing intents in specifications {{with a combination of}} numerical intervals and logical quantifiers. By differentiating a priori and a posteriori tolerances, the logic relationships among variables can be interpreted, which is useful to verify completeness and soundness of numerical estimations in tolerance analysis. In this paper, we present a semantic tolerance analysis approach to estimate size and geometric <b>tolerance</b> <b>stack-ups</b> based on closed loops of interval vectors. An interpretable linear system solver is constructed to ensure interpretability of numerical results. A direct linearization method for nonlinear systems is also developed. This new approach enhances traditional numerical analysis methods by preserving logical information during computation such that more semantics can be derived from variation estimations. 1...|$|R
40|$|Abstract: 2 ̆ 2 We {{introduce}} a methodology for concurrent design that considers {{the allocation of}} tolerances and manufacturing processes for minimum cost. Cost is approximated as a hyperbolic function over <b>tolerance,</b> and worst-case <b>stack-up</b> <b>tolerance</b> is assumed. Two simulated annealing techniques are introduced to solve the optimization problem. The first assumes independent, unordered, manufacturing processes and usesa Monte-Carlo simulation; the second assumes well known individual process cost functions which can be manipulated to create a single continuous function of cost versus tolerance with discontinuous derivatives solved with a continuous simulated annealing algorithm. An example utilizing a system of friction wheels over the manufacturing processes of turning, grinding, and saw cutting bar stock demonstrates excellent results. 2 ̆...|$|R
40|$|In {{the digital}} mock-up, parts and {{assemblies}} {{are used in}} ideal configurations. In fact, tolerances are represented as annotation on the CAD model and <b>tolerances</b> <b>stack-up</b> is not considered during the optimization of mechanical system assemblability and F. E Analysis. Then, the improving of the numerical model requires consideration of tolerances in the CAD model. An approach to incorporate dimensional and geometrical tolerance in CAD model is developed. Indeed, models, that allow obtaining assemblies with defects, are realized. In a bottom-up design process (component-to-assembly), the proposed model founds components with defects allowed by tolerances. Components with defects are deduced from the combination of two model sets which are obtained by using two sub-algorithms. A first sub-algorithm incorporates dimensional tolerances in CAD model. This model founds relationships between component dimensions (driving and driven dimensions) and performs a three dimension tolerancing chain by a technique based on connected graphs. The second sub-algorithm tacks into account geometrical tolerances in CAD model. Tolerance zone is discretized by parameters deduced from deviation torsor of toleranced element. This discretization allows obtaining possible realistic configuration of the toleranced element. Then, face movements are realized to obtain components with defects. Thus, assemblies with defects {{can be obtained by}} performing various combinations between components with defects. Nevertheless, the assembly regeneration, with realistic components, requires redefining assembly mates which are initially assigned to nominal assembly. Then, a new approach to defining realistic assembly mates in the case of rigid assembly is presented...|$|R
40|$|Recent {{legislative}} and social pressures have driven manufacturers to consider effective part reuse and material recycling {{at the end}} of product life at the design stage. One of the key considerations is to use joints that can disengage with minimum labor, part damage, and material contamination. This paper extends our previous work on the design of high-stiffness reversible locator-snap system that can disengage non-destructively with localized heat [1, 2], to include 1) modeling for <b>tolerance</b> <b>stack-up</b> and 2) lock-and-key concept to ensure that snaps only disengage when the right procedure is followed. The design problem is posed as an optimization problem to find the locations, numbers, and orientations of locators and snaps, and the number, locations and sizes of heating areas, which realize the release of snaps with minimum heat, compliance, and <b>tolerance</b> <b>stack-up.</b> The motion and structural requirements are considered constraints. Screw Theory is utilized to pre-calculate a set of feasible types and orientations of locators and snaps that are examined during optimization. The optimization problem is solved using Multi Objective Genetic Algorithm (MOGA) coupled with structural and thermal FEA. The method is applied on two case studies. The Pareto-optimal solutions present alternative designs with different trade-offs between the design objectives while meeting all the constraints...|$|E
40|$|Every {{manufacturing}} process {{leaves on the}} surface a signature, i. e., a systematic pattern that characterizes all the features machined with that process. The present work investigates the effects of considering the manufacturing signature in solving a <b>tolerance</b> <b>stack-up</b> function. A new variational model was developed that allows {{to deal with the}} form tolerance. It was used to solve a case study involving three parts with or without considering the correlation among the points of the same surface due to the manufactur-ing signature. A sensitivity analysis was developed by considering different values of the applied geometrical tolerances. [DOI: 10. 1115 / 1. 4028937]...|$|E
40|$|Fixtures are tools {{used to hold}} {{parts in}} {{specific}} positions and orientations so that certain manufacturing steps {{can be carried out}} within required accuracies. Despite the importance of fixtures in the production of expensive devices at Sandia National Laboratories, there is little in-house expertise in mathematical design issues associated with fixtures. As a result, fixtures typically do not work as intended when they are first manufactured. Thus, an inefficient and expensive trial-and-error approach must be utilized. This design methodology adversely impacts important mission duties of Sandia National Laboratories, such as the production of neutron generators. The work performed under the support of this LDRD project took steps toward providing mechanical designers with software tools based on rigorous analytical techniques for dealing with fixture stability and <b>tolerance</b> <b>stack-up...</b>|$|E
40|$|The {{purpose of}} this {{research}} was to develop a prototype liquid hydrogen boll-off recovery system. Perform analyses to finalize recovery system cycle, design detail components, fabricate hardware, and conduct sub-component, component, and system level tests leading to the delivery of a prototype system. The design point and off-design analyses identified cycle improvements to increase the robustness of the system by adding a by-pass heat exchanger. Based on the design, analysis, and testing conducted, the recovery system will liquefy 31 % of the gaseous boil off from a liquid hydrogen storage tank. All components, including a high speed, miniature turbocompressor, were designed and manufacturing drawings were created. All hardware was fabricated and tests were conducted in air, helium, and hydrogen. Testing validated the design, except for the turbocompressor. A rotor-to-stator clearance issue was discovered {{as a result of a}} concentricity <b>tolerance</b> <b>stack-up...</b>|$|E
40|$|International audienceThe {{safety and}} {{performance}} requirements for mechanisms are such that the necessary accuracy of part geometry is difficult to reach using classical manufacturing processes. This paper proposes a manufacturing <b>tolerance</b> <b>stack-up</b> method based on the analysis line method. This technique enables both the analysis and the synthesis of ISO manufacturing specifications through a new approach which relies on production specifications, adjustment specifications and their analysis to stack up the 3 D resultant. The originality of the method resides in the 3 D calculation for location requirements, which takes into account angular effects and probing operations on numerical-control machine-tools in order to define a local Work Coordinate System (WCS). For achieving tolerance analysis, deviations are modelled using Small-Displacement Torsor. This tolerance analysis method enables one to determine explicit three-dimensional linear relations between manufacturing tolerances and functional requirements. These relations {{can be used as}} constraints for tolerance optimization...|$|E
40|$|AbstractThis paper {{presents}} a methodology {{for the least}} cost tolerance allocation of systems with time-variant deviations, such as deformation, thermal expansion, mobility of parts and wear. By means of the approach, the product developer can identify the system's optimal tolerance design {{against the backdrop of}} the diverging requirements: wide tolerances to reduce costs vs. narrow tolerances to ensure functionality. Therefore, Particle Swarm Optimization – a global statistical optimization technique – is applied to the time-variant tolerance-optimization problem to determine the tolerance range as well as a certain mean shift for each non-ideal dimension. The practical use of the methodology is illustrated for a modified <b>tolerance</b> <b>stack-up</b> problem. We provide a comprehensive walkthrough of the methodology's application to the stack-up problem. Therefore, the essential mathematical background, the required functional dependencies and the considered parameters are detailed in a step-by-step procedure. So, we aim to establish the application of the statistical tolerance-cost-optimization in academia and industry and to motivate students and young graduates to apply this methodology by themselves...|$|E
40|$|AbstractTolerances of {{components}} can accumulate {{to result in}} quality variations when these components are assembled. One {{way to reduce the}} assembled variation is to tighten tolerance specifications of each component that, however, increases product cost. This paper investigates the effect of grouping for components with uniform and normal distributions by the developed “grouped random assembly” method. It is a method that first sorts and divides components into several groups and then assemble each group with corresponding group in order to reducing assembly tolerance. Distribution of resultant dimension based on the “grouped random assembly” approach is then analyzed. The results showed that, without changing components’ tolerance specifications, assembly tolerance depends on the number of grouping. <b>Tolerance</b> <b>stack-up</b> can be dramatically reduced with a suitable grouping strategy. The merit of this research is to develop a theoretical foundation for the grouped random assembly method. The results lead to a design for assembly strategy that assembly tolerance can be effectively reduced without tightening components’ tolerances...|$|E
40|$|Abstract. Computer aided process {{planning}} is the bridge between CAD and CAM. Setup planning {{is the major}} key to transform design concept into manufacturing domain, which is mainly experience based activity in modern manufacturing industry. Setup planning is a complicated non-linear task constrained by many factors such as tool approach direction, geometric feature relationship, fixturing constraint, tolerance requirement and manufacturing practice. Setup planning identifies which features must be machined in each setup and determines locating datum for each setup. This paper focuses {{on the development of}} a formalized procedure for automatic generation of feasible setups. For preventing of <b>tolerance</b> <b>stack-up</b> tried to use datum face as reference plane in fixture design. So, this paper presents a new method for setup planning with accurate respect to datum faces in design and machining. For the proposed work the authors have introduced two concepts namely, “inferiority face ” and “control face”. A rule-based procedure in several steps is used for solving the problem. The system is developed in Visual Basic on a Solid Works platform. Trial runs with industrial parts indicate that the system is applicable for industrial use...|$|E
40|$|In {{fiscal year}} 1993, tests of an o-ring/tetraseal {{retainer}} designed {{to replace a}} gasket-type seal used in PUREX-type process jumper connectors encouraged the design of an improved seal block. This new seal block combines several parts into one unitized component called an integral seal block. This report summarizes development and leak testing of the new integral seal block. The integral seal block uses a standard o-ring nested in a groove to accomplish leak tightness. This seal block eliminates the need to machine acme threads into the lower skirt casting and seal retainers, eliminates <b>tolerance</b> <b>stack-up,</b> reduces parts inventory, and eliminates an unnecessary leak path in the jumper connector assembly. This report also includes test data on various types of o-ring materials subjected to heat and pressure. Materials tested included Viton, Kalrez, and fluorosilicone, with some incidental data on teflon coated silicone o-rings. Test experience clearly demonstrates the need to test each seal material for temperature and pressure in its intended application. Some materials advertised as being {open_quotes}better{close_quotes} at higher temperatures did not perform up to expectations. Inspection of the fluorosilicone and Kalrez seals after thermal testing indicates that {{they are much more}} susceptible to heat softening than Viton...|$|E
40|$|Abstract-Canister-launch vehicle(LV) {{interface}} size is optimised using {{selective assembly}} method. Any over or under {{size of the}} interface is a mission critical. LV and canister are made of nos of sections and their respective positions are fixed in a assembly. Due to tolerance stack up in the assembly shape of the canister and LV varies from a perfect cylinder. Optimum size of the interface is found out for the assemblies where it satisfies the functionality without any interference or extra clearance for canister-LV assembly. Various sets of sections are available for LV and canister in production line and they are assembled selectively based on their effect on final assembly deviation. Assembly deviations are accepted such that the range of deviation for LV or for canister is minimum. One particular size of the interface suits to all the canister-LV combination without affecting the mission requirement. This paper explain how optimum inter face size is decided for sets of canister-LV assemblies based on tolerance stack up model. Selective assembly method and genetic algorithm are used to find out suitable canister-LV combinations. The method of finding out optimum interface size helps to avoid nos of assembly-disassembly trials and related testing to find out suitable canister-LV pair and leaves remote chances of getting some assemblies which {{does not meet the}} functional requirement. Keywords- Canister-LV interface, selective assembly, <b>tolerance</b> <b>stack-up,</b> genetic algorithm...|$|E
30|$|Xue and Ji (2001) {{proposed}} a methodology {{for dealing with}} angular features in tolerance charting. Ji and Xue (2002) obtained the mean working dimensions from the reverse chain matrix containing reverse tolerance chains. Huang et al. (2005) devised a procedure for determining the process tolerances directly from multiple correlated critical tolerances in an assembly. Process-oriented tolerancing was focused upon, by considering all the variations arising due to tool wear, measurement device fluctuations, <b>tolerance</b> <b>stack-up</b> propagation (Ding et al. 2005). A process optimization model was introduced which considers process means and process tolerances simultaneously, with sequential operation adjustment to reduce process variability, and with part compensation to offset process shifting (Jeang et al. 2007). Peng et al. (2008) derived quality loss function of interrelated critical-to-quality dimensions. Through this quality loss function, the design-tolerances of the component are determined for achieving an improved product as well as process quality. The tolerance chart balancing was mathematically modeled for minimizing the manufacturing cost and quality loss (Jeang 2011). Concurrent tolerancing was identified as an optimization problem and a feasible solution for systematically distributing the process tolerances within the design constraints was proposed (Sivakumar et al. 2012). Contreras (2013) proposed simplification of tolerance chains through a surface position tolerance (SPT) method for tolerance chart balancing. Chen et al. (2013) optimized the process parameters for the plastic injection molding. An improvement in the process potential capability index (C p) and process performance capability index (C pk) was registered through process capability improvement studies on thrust face thickness characteristic of connecting rod (Sharma and Rao 2013).|$|E
40|$|Tolerance {{analysis}} of an assembly {{is an important}} issue for mechanical design. Among various tolerance analysis methods, statistical analysis is the most commonly employed method. However, the conventional statistical tolerance method is often based on the normal distribution. It fails to predict the resultant tolerance of an assembly of parts with non-normal distributions. In this paper, a novel method based on statistical moments is proposed. Tolerance distributions of parts are first transferred into statistical moments that are then used for computing <b>tolerance</b> <b>stack-up.</b> The computed moments, particularly the variance, the skewness and the kurtosis, are then mapped back to probability distributions in order to calculate the resultant tolerance of the assembly. The proposed method can be used to analyse the resultant tolerance specification for non-normal distributions with different skewness and kurtosis. Simulated results showed that tail coefficients of different distributions with the same kurtosis are close to each other for normalised probabilities between 3 and 3. That is, the tail coefficients of a statistical distribution can be predicted by the coefficients of skewness and kurtosis. Two examples are illustrated in the paper to demonstrate the proposed method. The predicted resultant tolerances of the two examples are only 0. 5 % and 1. 5 % differences compared with that by the Monte Carlo simulation for 1, 000, 000 samples. The proposed method is much faster in computation with higher accuracy than conventional statistical tolerance methods. The merit of the proposed method is that the computation is fast and comparatively accurate for both symmetrical and unsymmetrical distributions, particularly when the required probability is between 2 and 3...|$|E
40|$|Driven by {{the moral}} sense of obligation, {{legislative}} and social pressures, manufacturers now consider effective part reuse and material recycling {{at the end}} of product life at the design stage. It is a key consideration to use joints that can disengage with minimum labor, part damage, and material contamination. This paper extends our previous work on the design of high-stiffness reversible locator-snap system that can disengage nondestructively with localized heat (Shalaby and Saitou, 2006, Optimal Heat-Reversible Snap Joints for Frame-Panel Assembly in Aluminum Space Frame Automotive Bodies," Proceedings of the LCE 2006 : The 13 th CIRP International Conference on Life Cycle Engineering, Leuven, Belgium, May 31 -Jun. 2, pp. 411 - 416; Shalaby and Saitou, 2008, "Design for Disassembly With High-Stiffness, Heat-Reversible Locator-Snap Systems," ASME J. Mech. Des., 130 (12), p. 121701) to include (1) modeling for <b>tolerance</b> <b>stack-up</b> and (2) lock-and-key concept to ensure that snaps only disengage when the right procedure is followed. The design problem is posed as an optimization problem to find the locations, numbers, and orientations of locators and snaps, and the locations and sizes of heating areas, to release the snaps with minimum heat, compliance, and tolerance stackup. The motion and structural requirements are considered constraints. Screw theory is employed to precalculate the set of feasible types and orientations of locators and snaps that are examined during optimization. Multi-objective genetic algorithm coupled with structural and thermal finite element analysis is used to solve the optimization problem. The method is applied on two case studies. The Pareto-optimal solutions present alternative designs with different trade-offs between the design objectives...|$|E
40|$|The {{premise of}} {{traditional}} methods of reliability predictions, such as MIL-HDBK- 217, is that the failure rate of a system is primarily determined by the components comprising the system. Historically, {{a significant number of}} failures also stem from non-component causes such as design deficiencies, manufacturing defects, inadequate requirements, induced failures, etc., that have not been explicitly addressed in prediction methods. The data in Figure 1 contains the nominal percentage of failures attributable to each of the eight identified predominant failure causes based on data collected by the Reliability Analysis Center. The definitions of these failure causes are as follows: Parts- Failure resulting from a part (e. g. microcircuit, transistor, resistor, connector, etc.) failing to perform its intended function. Design- Failure resulting from an inadequate design. Examples are <b>tolerance</b> <b>stack-up,</b> unanticipated logic conditions, a non-robust design for given environmental stresses, etc. Manufacturing- Failure resulting from anomalies in the manufacturing process, (i. e. faulty solder joints, bent connector pins, etc.). System Management- Failure to interpret system requirements, or failure to provide the resources required to design and build a reliable system. Wearout- Failure resulting from wearout related failure mechanisms. Examples of components exhibiting wearout related failure mechanisms are electrolytic capacitors, solder joints, tubes (TWTs), and switch and relay contacts. No Defect- Perceived failure that cannot be reproduced upon further testing. These failures {{may or may not be}} actual failures; however they are removals and therefore count toward the logistic failure rate. Induced- Failure resulting from an externally applied stress. Examples are electrical overstress and maintenance induced failures. Software- Failure of a system to perform its intended function due to the manifestation of a software fault. In 1994, the U. S. Military Specifications and Standards Reform initiative decreed the adoption of performance based specifications for acquiring and modifying weapon systems. This led to the cancellation of many military specifications and standards. This, coupled with the fact the Air Force has re-directed th...|$|E
40|$|Assembly plays a {{vital role}} in the quality of a final product and has a great impact on the {{manufacturing}} cost. The mechanical assemblies consist of parts that inevitably have variations from their ideal dimensions. These variations propagate and accumulate as parts are assembled together. Excessive amount of variations in an assembly may cause improper functionality of the product being assembled. Improving assembly quality and reducing the assembly time and cost are the main objectives of this thesis. The quality of an assembly is determined in terms of variations in critical assembly dimensions, also known as Key Characteristics (KCs). Key Characteristics are designated to indicate where excess variation will affect product quality and what product features and tolerances require special attention. In order to improve assembly quality and reduce assembly time and cost, it is necessary to: (1) model non-ideal parts based on tolerances defined in design standards or current industrial practice of component inspection, (2) model assemblies and their associated assembly processes to analyse <b>tolerance</b> <b>stack-up</b> in the assembly, (3) develop probabilistic model to predict assembly variation after product assembly, and (4) implement control strategies for minimising assembly variation propagations to find optimum configuration of the assembly. Two assembly models have been developed, a linear model and a fully non-linear model for calculating assembly variation propagations. The assembly models presented in this thesis also allows for inclusion of geometric feature variation of each assembly component. Methods of incorporating geometric feature variations into an assembly variation model are described and analysis techniques are explained. The assembly variation model and the geometric variation models have been developed for 20 and 3 D assemblies. Modelling techniques for incorporating Modelling and Controlling Variation Propagation in Mechanical Assemblies process and measurement noise are also developed and described for the non- linear assembly model and results are given to demonstrate the calculation of assembly variations while considering part, process and measurement errors. Two assembly case studies originating in sub-assemblies of aero-engines have been studied: Case Study 1, representing the rotating part (rotor) of an aero-engine, and Case Study 2, representing non-rotating part (stator) of an aero-engine. A probabilistic method based on the linear model is presented as a general analytical method for analysis of 3 D mechanical assemblies. Probability density functions are derived for assembly position errors to analyse a general mechanical assembly, and separate probability functions are derived for the Key Characteristics (KCs) for assembly in Case Studies 1 and 2. The derived probability functions are validated by using the Monte Carlo simulation method based on the exact (fully non-linear) model. Results showed that the proposed probabilistic method of estimating tolerance accumulation in mechanical assemblies is very efficient and accurate when compared to the Monte Carlo simulation method, particularly if large variations at the tails of the distributions are considered. Separate control strategies have been implemented for each case study. Four methods are proposed to minimise assembly variations for Case Study 1, and one error minimisation method is suggested for assemblies of Case Study 2. Based on the developed methods to optimise assembly quality, the two case studies were investigated, and {{it was found that the}} proposed optimisation methods can significantly improve assembly quality. The developed optimisation methods do not require any special tooling (such as fixtures) and can easily be implemented in practice. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|Modern {{manufacturing}} processes are strongly affected by part and process variations. Understanding the final {{shape of the}} assembly is a crucial task to be achieved during the design stage to reduce cost and production time. Typically, when parts are put together variations propagate part-to-part. This stack-up effect is strictly related to part deviations, assembly sequence, assembly constraints, part flexibility. All these fac-tors combine into a non-linear way. Technical literature provides valid methodologies to analyze <b>tolerance</b> <b>stack-up</b> problems. In this contest, assemblies are usually classified into two main categories: rigid assemblies and compliant assemblies. In the first case, parts are assumed ideal-rigid and the additional variation due to elastic or plastic defor-mation is not accounted. When the flexibility of parts is not negligible, as into manufac-turing processes involving sheet-metal parts, its effect has to be accounted into numeri-cal simulations. This dissertation provides {{a contribution to the}} modeling and the simu-lation both of rigid and compliant assemblies. With respect to rigid assembly, a general methodology, called SVA-TOL (Sta-tistical Variation Analysis for Tolerancing), is proposed. The methodology is based into two main steps. Tolerance specifications are modeled in the first step following Interna-tional Standard rules. Variational features are so generated. Then, by using these varia-tional features, assembly constraints are modeled as combination of elementary geome-try entities: points, lines and planes. Two assembly solvers are illustrated: the sequential solver and the least squares solver. The sequential solver allows to analyze assembly constraints taking into account the assembly sequence. Specific mathematical tools, such as Screw Theory and Graph Theory, are here adopted to automatically calculate the list of degrees of freedom al-lowed for a specific part being assembled. The least squares solver, instead, permits to analyze all assembly constraint si-multaneously by best fitting all mating conditions. Case studies have pointed out the field of applicability of proposed assembly solvers. With respect to compliant assembly, a general frame-work, called SVA-FEA (Statistical Variation Analysis & Finite Element Analysis), is presented. SVA-FEA al-lows to statistically simulate single- and multi-station assembly processes under linear assumptions. A Global Sensitivity Matrix is introduced to link input part or process de-viations and output assembly deviations. In this way, no Monte Carlo simulation is needed. The whole assembly process is based on the Place, Clamp, Fasten and Release (PCFR) cycle. Parts are positioned on fixturing frames, where are clamped and fastened. Then, they are released reaching their final assembly configuration. To numerically cal-culate the sensitivity matrix two FEA runs, performed on the nominal geometry, are re-quired. The first one calculates the fixturing and fastening forces, by applying the method of influence coefficients. These forces are then applied into the second FEA run to simulate the final elastic spring-back. SVA-FEA methodology has been implemented into a friendly MatLAB®’s GUI, allowing to define the whole assembly process, with its variability, and to visualize the final assembly deviations, in terms on mean and standard deviations. Two significant case studies have highlighted how SVA-FEA al-lows to simulate both single- and multi-station assembly processes. Results have been compared with ones coming from commercial CAT software, showing a good numeri-cal correlation. SVA-FEA assumes that input statistical deviations are independent among them. This means that no covariance effect is accounted. To overcome this weakness, a new non-linear methodology is also proposed. The non-linear methodology, to do variational analysis of compliant assembly, uses a Monte Carlo approach to statistically generate input free shape geometry. The methodology can be described as follows. For each Monte Carlo step, input geometry is generated according to an automatic morphing mesh procedure. Then, the PCFR cycle is simulated. The geometry is so updated for each phase of the PCFR cycle. To reach more realistic results, contact pairs, defined as surface-to-surface type, may be introduced into the numerical simulation. The morphing mesh approach allows to generated free shape parts starting from few control points, defined on the nominal geometry, and setting the tolerance error value. Finally, critical remarks are outlined, highlighting future directions of research in the field of tolerance analysis of rigid and compliant assembly processes...|$|E
40|$|Knowledge of {{the mass}} of the {{manipulated}} load (i. e. payload) in off-highway machines is useful information {{for a variety of reasons}} ranging from knowledge of machine stability to ensuring compliance with transportion regulations. This knowledge is difficult to ascertain however. This dissertation concerns itself with delineating the motivations for, and difficulties in development of a dynamic payload weighing algorithm. The dissertation will describe how the new type of dynamic payload weighing algorithm was developed and progressively overcame some of these difficulties. The payload mass estimate is dependent upon many different variables within the off-highway vehicle. These variables include static variability such as machining tolerances of the revolute joints in the linkage, mass of the linkage members, etc as well as dynamic variability such as whole-machine accelerations, hydraulic cylinder friction, pin joint friction, etc. Some initial effort was undertaken to understand the static variables in this problem first by studying the effects of machining tolerances on the working linkage kinematics in a four-wheel-drive loader. This effort showed that if the linkage members were machined within the tolerances prescribed by the design of the linkage components, the <b>tolerance</b> <b>stack-up</b> of the machining variability had very little impact on overall linkage kinematics. Once some of the static dependent variables were understood in greater detail significant effort was undertaken to understand and compensate for the dynamic dependent variables of the estimation problem. The first algorithm took a simple approach of using the kinematic linkage model coupled with hydraulic cylinder pressure information to calculate a payload estimate directly. This algorithm did not account for many of the aforementioned dynamic variables (joint friction, machine acceleration, etc) but was computationally expedient. This work however produced payload estimates with error far greater than the 1 % full scale value being targeted. Since this initial simplistic effort met with failure, a second algorithm was needed. The second algorithm was developed upon the information known about the limitations of the first algorithm. A suitable method of compensating for the non-linear dependent dynamic variables was needed. To address this dilemma, an artificial neural network approach was taken for the second algorithm. The second algorithm’s construction was to utilise an artificial neural network to capture the kinematic linkage characteristics and all other dynamic dependent variable behaviour and estimate the payload information based upon the linkage position and hydraulic cylinder pressures. This algorithm was trained using emperically collected data and then subjected to actual use in the field. This experiment showed that that the dynamic complexity of the estimation problem was too large for a small (and computationally feasible) artificial neural network to characterize such that the error estimate was less than the 1 % full scale requirement. A third algorithm was required due to the failures of the first two. The third algorithm was constructed to ii take advantage of the kinematic model developed and utilise the artificial neural network’s ability to perform nonlinear mapping. As such, the third algorithm developed uses the kinematic model output as an input to the artificial neural network. This change from the second algorithm keeps the network from having to characterize the linkage kinematics and only forces the network to compensate for the dependent dynamic variables excluded by the kinematic linkage model. This algorithm showed significant improvement over the previous two but still did not meet the required 1 % full scale requirement. The promise shown by this algorithm however was convincing enough that further effort was spent in trying to refine it to improve the accuracy. The fourth algorithm developed proceeded with improving the third algorithm. This was accomplished by adding additional inputs to the artificial neural network that allowed the network to better compensate for the variables present in the problem. This effort produced an algorithm that, when subjected to actual field use, produced results very near the 1 % full scale accuracy requirement. This algorithm could be improved upon slightly with better input data filtering and possibly adding additional network inputs. The final algorithm produced results very near the desired accuracy. This algorithm was also novel in that for this estimation, the artificial neural network was not used soley as the means to characterize the problem for estimation purposes. Instead, much of the responsibility for the mathematical characterization of the problem was placed upon a kinematic linkage model that then fed it’s own payload estimate into the neural network where the estimate was further refined during network training with calibration data and additional inputs. This method of nonlinear state estimation (i. e. utilising a neural network to compensate for nonlinear effects in conjunction with a first principles model) has not been seen previously in the literature...|$|E

