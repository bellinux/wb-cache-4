562|857|Public
5000|$|<b>Throughput</b> <b>rate</b> is an {{obsolete}} term in {{the terminology}} of automated chemical analysis. It may mean either: ...|$|E
50|$|Limitation {{for modern}} digital {{computing}} systems are processing speed, input-output <b>throughput</b> <b>rate,</b> {{the number of}} input-output devices, and software compatibility with upgrade parts.|$|E
50|$|Interconnects {{up to four}} T640 chassis into {{a single}} routing entity. It has 32 slots and a {{sustainable}} <b>throughput</b> <b>rate</b> of 2.5 Tbit/s (up to 3 billion pps).|$|E
40|$|Student <b>throughput</b> <b>rates</b> {{have become}} a much debated topic, {{not only in the}} South African {{academic}} environment but {{in many parts of the}} world. Of particular concern are Master's and PhD degrees: how many years should students take to complete them? Universities are taking completion times very seriously, as they should, and faculties are being urged to improve their average <b>throughput</b> <b>rates</b> based on statistics that are generated annually. [URL]...|$|R
40|$|As part {{of their}} most recent {{strategic}} plan, the South African Department of Science and Technology have proposed that the training of PhD students should be the ‘driver’ to propel South Africa into a knowledge economy. The intention here is to drive significant {{economic growth in the}} country. I support this vision wholeheartedly. However, the length of time it takes for students to get degrees has become a strong point of contention, with higher education institutions seriously challenged to increase their <b>throughput</b> <b>rates.</b> As a deputy dean in a science faculty I am regularly engaged in discussions regarding <b>throughput</b> <b>rates</b> at many levels within the university at which I am employed. An important first step in this discussion is to realise that initiatives to improve <b>throughput</b> <b>rates</b> for bachelor’s degrees will be different to those needed for postgraduate degrees...|$|R
5000|$|... “Steam {{infusion}} {{has allowed}} us to nearly double our previous <b>throughput</b> <b>rates,</b> furthermore our meat sauce quality and consistency has improved dramatically.” Mark Carnaghan, Factory Manager, Greencore ...|$|R
50|$|Mechanical cells use a large mixer and {{diffuser}} mechanism at {{the bottom}} of the mixing tank to introduce air and provide mixing action. Flotation columns use air spargers to introduce air {{at the bottom}} of a tall column while introducing slurry above. The countercurrent motion of the slurry flowing down and the air flowing up provides mixing action. Mechanical cells generally have a higher <b>throughput</b> <b>rate,</b> but produce material that is of lower quality, while flotation columns generally have a low <b>throughput</b> <b>rate</b> but produce higher quality material.|$|E
50|$|Melt blowing is also capable {{to produce}} drug-loaded fibers for {{controlled}} drug delivery. The high <b>throughput</b> <b>rate</b> (extrusion feeding), solvent-free operation accompanied {{with the increased}} surface area of the product make melt blowing a promising new formulation technique.|$|E
5000|$|Also {{despite the}} {{vertical}} peeler’s low cost due to compact design, the throughput of vertical centrifuge is restricted {{compared to other}} peeler or centrifuge systems. With consideration of low batch <b>throughput</b> <b>rate,</b> overall performance in comparison to other processes, (comparison) ...|$|E
40|$|RDBMS {{databases}} typically increase {{read operation}} throughput and decrease write operations throughput when more machines are involved. NoSQL databases increase at both <b>throughput</b> <b>rates</b> when more machines are involved. However, NoSQL databases are typically build {{to run with}} tens, hundreds or even thousands of machines while {{little is known about}} their performance on a smaller scale. This thesis describes the measured maximum <b>throughput</b> <b>rates</b> of read and write operations at RDBMS and NoSQL databases with one to four machines. These measurements show a potential for future databases. Software EngineeringComputer ScienceElectrical Engineering, Mathematics and Computer Scienc...|$|R
40|$|The {{study was}} necessitated {{by the fact}} that the <b>throughput</b> <b>rates</b> of {{accounting}} students studying at distance learning institutions in South Africa are disturbingly low when compared to students studying at residential universities. Bearing in mind the magnitude of the University of South Africa’s (Unisa’s) market share of accounting students in South Africa, it was pivotal to comprehend the unique challenges related to retention and throughput of these students. This thesis reports on a case study of the use of mobile phones in an Accounting module by applying the theories of didactic conversation and transactional distance to understand the retention and <b>throughput</b> <b>rates</b> of the Accounting students in an open distance learning (ODL) environment. Considering the landscape of accounting education in South Africa, the specific challenges faced by accounting students at Unisa and the recent scholarly discourse on retention and throughput of distance education students, this study contributes to the limited theoretical understanding of students’ retention and <b>throughput</b> <b>rates</b> in an Accounting module at Unisa. This theoretical understanding has been obtained through combining the transactional distance theory of Moore (1973) and the didactic conversation of Holmberg (1982) in a single conceptual framework. By applying this framework, this thesis makes an original contribution to the deepening understanding of the retention and <b>throughput</b> <b>rates</b> of accounting students in an ODL environment. The study has shown that retention and <b>throughput</b> <b>rates</b> can be improved through the lessening of the transactional distance between facilitator and student and by improving the quality and extent of the two-way didactic conversation in the learning process. To this effect, the study provided empirical evidence of the successful use of various complementing technology interventions, suitable for accounting students with time constraints, to enhance the learning process. Management AccountingD. Com...|$|R
50|$|There {{are several}} {{considerations}} {{that need to}} be made when designing any chemical process unit. Design conceptualization and considerations can begin once product purities, yields, and <b>throughput</b> <b>rates</b> are all defined.|$|R
5000|$|... {{very high}} power intensities - IsaMills operate at power intensities up to 350 kilowatts {{per cubic meter}} ("kW/m3"). For comparison, the power {{intensity}} of a ball mill is about 20 kW/m3. This high power intensity allows the IsaMill to produce fine particles at a high <b>throughput</b> <b>rate.</b> The high power intensity of the IsaMill comes from its high stirring speed of about 20 meters per second ("m/s").|$|E
50|$|In February 2007, 3SP Ltd {{conducted}} performance benchmarking of the SSL-Explorer solution using a {{test bed}} platform of three systems using different specifications of hardware. The benchmarking {{was conducted with}} the assumption that a minimum 256 kbit/s data <b>throughput</b> <b>rate</b> would be a realistic value to place upon a responsive VPN tunnel for use such as remote desktop access. The BEA jRockit JRE was used in all tests on both Microsoft Windows and Linux systems.|$|E
50|$|Another major {{field that}} is {{currently}} being looked into is the further development of in-bed heat exchanger used with circulating fluidized bed technology. With this design, the bed materials fill the in-bed heat exchanger through the open top of the circulating fluidized bed furnace, which enables the control of materials through the in-bed heat exchanger. By being {{able to control the}} materials <b>throughput</b> <b>rate,</b> better control of heat absorption as well as bed temperature in the furnace is achievable. With further development in this field, {{we will be able to}} fully utilize the energy required to drive the furnace with minimum energy wastage.|$|E
40|$|The study {{confirmed}} that most chairs of departments (CODs) in the College of Economic and Management Sciences (CEMS) regard Master’s and doctoral (M and D) <b>throughput</b> <b>rates</b> as poor. A proposed checklist for assessing M and D research proposals is advanced. The {{aim of the}} research was to establish if a proposed checklist may contribute to improved <b>throughput</b> <b>rates.</b> Inconsistent assessment criteria are used to assess the research proposals of M and D students. Structured interviews with CODs revealed that most departments are using criteria based on previous experience to assess M and D research proposals. These criteria have not been documented. The proposed checklist is regarded by CODs as being potentially valuable for assessing M and D proposals, but they are less optimistic that it could lead to improved <b>throughput</b> <b>rates.</b> The proposed checklist may nevertheless assist prospective M and D students to plan their research {{in a way that will}} avoid critical problems. Academic departments should consider testing the proposed checklist for its potential contribution to improve M and D throughput. Finance, Risk Management and Bankin...|$|R
40|$|Low <b>throughput</b> <b>rates</b> {{at schools}} and {{universities}} across South Africa are cause for great concern because of the resultant financial burden on the state, the increase in unemployment and inadequate delivery of much-needed highly skilled professionals. The advent of the fourth economic wave accompanied as it is by fundamental changes in the workplace globally calls for a judicious response from theorists, practitioners, researchers and learners. This article surveys the extent and possible causes of the low <b>throughput</b> <b>rates</b> in higher education and draws {{on the results of}} recent research projects to design intervention guidelines aimed at facilitating access to and success in higher education. A combined quantitative+qualitative approach is recommended. [URL]...|$|R
50|$|During {{an active}} survey, the {{wireless}} adapter {{is associated with}} one or several access points to measure round-trip time, <b>throughput</b> <b>rates,</b> packet loss, and retransmissions. Active surveys are used to troubleshoot wifi networks or to verify performance post-deployment.|$|R
50|$|Juniper's TX Matrix Plus is {{the central}} {{switching}} and routing element that can interconnect up to 16 T1600 chassis into a single routing entity with 128 slots and a sustainable <b>throughput</b> <b>rate</b> of up to 25 Tbit/s (30.7 billion pps). With TX Matrix Plus, operators can build systems containing up to 16 line card chassis {{for a total of}} up to 1024 10-Gigabit Ethernet ports or 256 40 Gbit/s ports. Using the virtualization capabilities of JCS1200, this available resource can be partitioned into aggregation or edge routing, or into the support of virtual service networks for advanced partitioned services such as video, mobile, and all corporate traffic.|$|E
5000|$|Determining {{how much}} data from each stream {{should be in}} each {{interleaved}} segment (the size of the interleave) is complicated, yet an important requirement. Improper interleaving will result in buffer underflows or overflows, as the receiver gets more of one stream than it can store (e.g. audio), before it gets enough data to decode the other simultaneous stream (e.g. video). The MPEG Video Buffering Verifier (VBV) assists in determining if a multiplexed PS can be decoded by a device with a specified data <b>throughput</b> <b>rate</b> and buffer size. [...] This offers feedback to the muxer and the encoder, {{so that they can}} change the mux size or adjust bitrates as needed for compliance.|$|E
50|$|M40 was {{the first}} product by Juniper Networks, which was {{released}} in 1998. The M40 {{was the first}} of its kind capable of scaling to meet the internet standards, which can move 40 million packets per second with a <b>throughput</b> <b>rate</b> in excess of 20 Gbit/s full-duplex. With the initial offering of m40, Juniper came up with the Internet Processor I. The proprietary ASIC was the fundamental core of Juniper's Packet Forwarding Engine (PFE). The PFE consisted of a shared memory, a single forwarding table, and a one-write, one-read architecture.The entire PFE was capable of forwarding at 40 Mpps, a capacity more than 100 times faster than that of any other available router architectures at that time. The M40 is one of the first routers on this scale, about 10 times faster than Cisco's 12000.|$|E
50|$|The optical {{stretcher}} {{has since}} been developed into a versatile biophysical tool used by many groups worldwide for contact-free, marker-free measurements of whole-cell rheology.Using automated setups, high <b>throughput</b> <b>rates</b> of more than 100 cells/hour have been achieved, allowing for statistical analysis of the data.|$|R
40|$|The {{bandwidth}} efficiency of coded modulation coupled with high coding gains over equivalent uncoded schemes has rendered the concept attractive for application in many practical communication systems. In this paper, {{we examine the}} performance by simulation, of TCM, MTCM and BCM on IS 1 channels. Simulations were performed assuming the receiver operates on the received signal using decision feed-back equalization {{in conjunction with the}} Viterbi decoder. Results indicate that coded modulation schemes may be viable in systems that require low <b>throughput</b> <b>rates.</b> Coded schemes experience performance losses, however, in sys-tems that require higher <b>throughput</b> <b>rates</b> mainly because the equalization process cannot adequately reduce the dis-tortion induced by the channel, and the Viterbi decoder is unable U, operate on the distorted signal. 1...|$|R
50|$|Infineta {{announced}} its Data Mobility Switch in June 2011. The DMS {{was the first}} WAN optimization technology to work at <b>throughput</b> <b>rates</b> of 10 Gbit/s. Infineta designed the product in FPGA hardware around a multi-Gigabit switch fabric to minimize latency.The DMS used compression similar to data deduplication.|$|R
50|$|Windows Vista also {{implements}} I/O scheduling as prioritized I/O. Disk I/O requests in Windows Vista {{are assigned}} priorities; {{a higher priority}} request is given preferential treatment, over a request that has a lower priority, during {{the execution of the}} request. Windows Vista defines five priority classes - Very Low, Low, Normal, High and Critical. By default I/O requests are assigned Normal priority. Windows Vista also allows reservation of bandwidth on a per-application basis during disk access; this aims to guarantee the required <b>throughput</b> <b>rate</b> to the application when it accesses the disk. Both these features are used by Windows Media Player with respect to media playback. In addition, the following applications and components use prioritized I/O: Disk Defragmenter, SuperFetch, Windows Defender, Windows Search, and applications that run at startup.|$|E
50|$|The 74 kb {{mupirocin}} {{gene cluster}} contains six multi-domain enzymes and twenty-six other peptides (Table 1). Four large multi-domain type I polyketide synthase (PKS) proteins are encoded, {{as well as}} several single function enzymes with sequence similarity to type II PKSs. Therefore, it is believed that mupirocin is constructed by a mixed type I and type II PKS system. The mupirocin cluster exhibits an atypical acyltransferase (AT) organization, in that there are only two AT domains, and both are found on the same protein, MmpC. These AT domains are the only domains present on MmpC, while the other three type I PKS proteins contain no AT domains. The mupirocin pathway also contains several tandem acyl carrier protein doublets or triplets. This may be an adaptation to increase the <b>throughput</b> <b>rate</b> or to bind multiple substrates simultaneously.|$|E
50|$|In {{practice}} {{it is not}} possible to obtain the quoted theoretical maximum <b>throughput</b> <b>rate</b> for machines in a placement system. It is necessary to derate the theoretical numbers to obtain realistic values, due to unexpected downtime, board load and unload time and machine configuration. Other factors include PWB size, component mix, and the requirement for more complex vision recognition for fine-pitch components. There are many techniques of derating. Global derating considers system-wide stops, slow-downs and set ups as well as machine factors. To calculate the amount of global or system derating, one should take the average of the number of total components placed per hour in a long period (i.e. an entire product shift). Regularly scheduled stops should be included when determining the level of global derating the system requires.Rigorous derating, which considers each piece of equipment in service for a particular product individually, must be conducted by specific machine model for the line balancing. Rigorous derating values are necessary for full optimization of the process.|$|E
40|$|The {{operational}} requirements {{and development of}} a system designed to meet LACIE needs for data to be available at given stations simultaneously, to measure <b>throughput</b> <b>rates,</b> and perform efficiency analyses are described. The final automated status and tracking system (ASATS) is defined and problems encountered during its evolutionary process are discussed...|$|R
40|$|Abstract—In {{wireless}} networks, throughput optimization is {{an essential}} performance objective that cannot be adequately characterized by a single criterion (such as the minimum transmitted or sum-delivered throughput) and should be specified over all source–destination pairs as a rate region. For a simple and yet fundamental model of tandem networks, a cross-layer optimization framework is formulated to derive the maximum throughput region for saturated multicast traffic. The contents of network flows are specified through network coding (or plain routing) in network layer and the <b>throughput</b> <b>rates</b> are jointly optimized in medium access control layer over fixed set of conflict-free transmission schedules (or optimized over transmission probabilities in random access). If the network model incorporates bursty sources and allows packet queues to empty, {{the objective is to}} specify the stability region as the set of maximum <b>throughput</b> <b>rates</b> that can be sustained with finite packet delay. Dynamic queue management strategies are used to expand the stability region toward the maximum throughput region. Network coding improves <b>throughput</b> <b>rates</b> over plain routing and achieves the largest gains for broadcast communication and intermediate network sizes. Throughput optimization imposes fundamental tradeoffs with transmission and processing energy costs such that the throughput-optimal operation is not necessarily energy efficient. Index Terms—Cross-layer design, energy efficiency, medium access control, network coding, network stability, routing, throughput optimization, wireless networks. I...|$|R
30|$|While an MPSoC {{implementation}} of the QAM modulator is beneficial in terms of throughput, there are overheads associated with the on-chip network. As such, the MPSoC-based modulator was compared to a straightforward implementation featuring multiple QAM modulators, {{in an effort to}} identify the conditions that favor the MPSoC implementation. Comparison was carried out under variable incoming rates, system configurations and fault conditions, and simulation results showed on average double <b>throughput</b> <b>rates</b> during normal operation and ~ 25 % less throughput degradation at the presence of faulty components, at the cost of approximately 35 % more area, obtained from an FPGA implementation and synthesis results. The hardware overheads, which stem from the NoC and the resource allocation algorithm, are well within the typical values for NoC-based systems [11, 12] and are adequately balanced by the high <b>throughput</b> <b>rates</b> obtained.|$|R
50|$|Another {{significant}} change {{that aims to}} improve network throughput is the automatic resizing of TCP Receive window. The receive window (RWIN) specifies how much data a host is prepared to receive, and is limited by, among other things, the available buffer space. In other words, it {{is a measure of}} how much data the remote transmitter can send before requiring an acknowledgement for the outstanding data. When the receive window is too small, the remote transmitter will frequently find that it has hit the limit of how much outstanding data it can transmit, even though there is enough bandwidth available to transmit more data. This leads to incomplete link utilization. So using a larger RWIN size boosts throughput in such situations; an auto-adjusting RWIN tries to keep the <b>throughput</b> <b>rate</b> as high as is permissible by the bandwidth of the link. Receive window auto tuning functionality continually monitors the bandwidth and the latency of TCP connections individually and optimize the receive window for each connection. The window size is increased in high-bandwidth (~5 Mbit/s+) or high-latency (>10ms) situations.|$|E
5000|$|Growth cost: Begging incurs {{a growth}} cost in canary chicks. [...] When the brood is very hungry, begging becomes {{a form of}} {{scramble}} competition whereby offspring jostle to be closest to the feeding adult. When the brood has recently been fed, adults instead actively choose offspring to feed because begging reliably signals nestling hunger. Experimental manipulations of begging behaviour and food reward show that excessive begging retards growth, both immediately and in the longer term, {{and the impact of}} the manipulation is greatest in chicks with the highest potential daily mass gain. Furthermore, the greater the difference in begging intensity between siblings during the experiment, the greater the difference between them in the mass lost as a result of metabolic expenditure. This growth cost of begging can be interpreted as a fitness cost, because daily mass gain is strongly correlated with the likelihood of survival to independence. Begging may incur an additional indirect growth cost through its actions on digestive efficiency. Chicks that are forced to beg excessively produce a greater number of fecal sacs, although not more fecal waste in total, than their less exercised siblings. The faster rate of fecal sac production may indicate an increased digesta <b>throughput</b> <b>rate,</b> which is known to reduce digestive efficiency.|$|E
5000|$|With {{commonly}} deployed ADSL over POTS (Annex A), {{the band}} from 26.075 kHz to 137.825 kHz {{is used for}} upstream communication, while 138-1104 kHz is used for downstream communication. Under the usual DMT scheme, each of these is further divided into smaller frequency channels of 4.3125 kHz. These frequency channels are sometimes termed bins. During initial training to optimize transmission quality and speed, the ADSL modem tests each of the bins to determine the signal-to-noise ratio at each bin's frequency. Distance from the telephone exchange, cable characteristics, interference from AM radio stations, and local interference and electrical noise at the modem's location can adversely affect the signal-to-noise ratio at particular frequencies. Bins for frequencies exhibiting a reduced signal-to-noise ratio will be used at a lower <b>throughput</b> <b>rate</b> or not at all; this reduces the maximum link capacity but allows the modem to maintain an adequate connection. The DSL modem will make a plan on how to exploit each of the bins, sometimes termed [...] "bits per bin" [...] allocation. Those bins that have a good signal-to-noise ratio (SNR) will be chosen to transmit signals chosen from {{a greater number of}} possible encoded values (this range of possibilities equating to more bits of data sent) in each main clock cycle. The number of possibilities must not be so large that the receiver might incorrectly decode which one was intended in the presence of noise. Noisy bins may only be required to carry as few as two bits, a choice from only one of four possible patterns, or only one bit per bin in the case of ADSL2+, and very noisy bins are not used at all. If the pattern of noise versus frequencies heard in the bins changes, the DSL modem can alter the bits-per-bin allocations, in a process called [...] "bitswap", where bins that have become more noisy are only required to carry fewer bits and other channels will be chosen to be given a higher burden.|$|E
40|$|Assembly {{automation}} is {{well established}} as hard automation in the manufacturing industry with proven economic advantages in mass production. Modern industries are adopting mass customization as a new strategy to handle the increasing diversity in customer demands. The main challenge is to produce an increasing variety of products with mass production efficiency. Conventional assembly tooling is not flexible enough to handle the product variations. Also the existing flexible assembly tooling usually operates at lower <b>throughput</b> <b>rates.</b> Hence new type of assembly tooling is needed to impart flexibility, while retaining the capability of high <b>throughput</b> <b>rates.</b> This research is aimed to redesign the existing part-feeders and feed tracks into modular, parametric tooling. An approach for developing the modular, parametric part-feeders is presented. Using this approach, some parametric nonvibratory feeders are designed and developed. Conceptual designs for modular, parametric gravity feed tracks are also presented...|$|R
40|$|In this paper, {{the linear}} algebra {{techniques}} used in image restoration are outlined, and updating algorithms on a systolic array model are presented for solving the resulting linear systems of equations. C. R. Categories: G 1. 3 Keywords: Matrix updating, Systolic arrays, Image restoration 1 Introduction Image processing involves various operations {{which can be}} carried out on the image data. These operations include preprocessing, image enhancement, feature detection, image compression and image restoration. We restrict ourselves to image restoration. Image restoration involves linear algebra techniques which restore a degraded image to something close to the original. The most important feature of image processing requirements are high <b>throughput</b> <b>rates</b> and large amounts of data which should be processed in real-time. In general, most real-time image processing <b>throughput</b> <b>rates</b> are higher than the performance of current parallel architectures. Therefore, image processing applications ha [...] ...|$|R
40|$|The {{performance}} of a single-chip shared-memory multiprocessor system for encryption/decryption of large quantities of information is examined. The multiprocessor uses an asynchronous bus system to increase scalability and to reduce design effort. The target application is the Advanced Encryption Standard (AES) with 128 bit key. Chip area requirements and encryption/decryption <b>throughput</b> <b>rates</b> are examined. 1...|$|R
