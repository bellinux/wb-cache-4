6|10000|Public
40|$|Differentiation {{matrices}} {{associated with}} radial basis function (RBF) collocation methods often have eigenvalues with positive real parts of significant magnitude. This prevents {{the use of}} the methods for time-dependent problems, particulary if explicit time integration schemes are employed. In <b>this</b> <b>work,</b> <b>accuracy</b> and eigenvalue stability of sym-metric and asymmetric RBF collocation methods are numerically ex-plored for some model hyperbolic initial boundary value problems in one and two dimensions...|$|E
40|$|International audienceUnions {{of balls}} {{are widely used}} shape representations. Given a shape, {{computing}} a union of balls that is both accurate in some sense and of small cardinality is thus a challenging problem. In <b>this</b> <b>work,</b> <b>accuracy</b> is ensured by imposing that the union of balls, called covering, {{is included in the}} shape and covers a parameterized core set (namely the erosion) of the shape. For a family of simple shapes, we propose a polynomial-time greedy algorithm that computes a covering of minimum cardi-nality for a given shape...|$|E
40|$|Lately, Ensemble Empirical Mode Decomposition (EEMD) {{techniques}} receive {{growing interest}} in biomedical data analysis. Event-Related Modes (ERMs) represent features extracted by an EEMD from electroencephalographic (EEG) recordings. We present a new approach for source localization of EEG data based on combining ERMs with inverse models. As the first step, 64 channel EEG recordings are pooled according to six brain areas and decomposed, by applying an EEMD, into their underlying ERMs. Then, based upon the problem at hand, the most closely related ERM, in terms of frequency and amplitude, is combined with inverse modeling techniques for source localization. More specifically, the standardized low resolution brain electromagnetic tomography (sLORETA) procedure is employed in <b>this</b> <b>work.</b> <b>Accuracy</b> and robustness of {{the results indicate that}} this approach deems highly promising in source localization techniques for EEG data...|$|E
30|$|In <b>this</b> <b>work,</b> the <b>accuracy</b> of {{approximate}} solution, {{when taking}} larger n and smaller σ, {{is expected that}} more accurate the approximate results.|$|R
30|$|Tadakamadla [35] {{deployed}} ZigBee {{network for}} monitoring {{the presence and}} movements of vehicles and humans into an indoor environment. It uses the RSSI to determine the position of tagged entities; the randomness of RSSI and the dependency on the user’s body and orientation cause the main error contribution. In <b>this</b> <b>work</b> an <b>accuracy</b> of 3 m and 35 % precision were obtained.|$|R
40|$|Abstract. To {{analyze the}} effect of the {{manufacture}} and assembly errors on the accuracy reliability of power servo tool holder, the accuracy reliability sensitivity is investigated in <b>this</b> <b>work.</b> The <b>accuracy</b> reliability model of power servo tool holder is introduced firstly. The sensitivity analysis theory of the reliability perturbation method is given. Then, the method is applied to the reliability sensitivity of the power servo tool holder of which the type is SFW 25 and the importance order of the manufacture and assembly errors for the accuracy reliability is obtained. It offers the guide for improving the accuracy reliability of the considered power servo tool holder...|$|R
40|$|Copper is a {{ductile metal}} with {{excellent}} electrical conductivity, and finds extensive {{use as an}} electrical conductor, heat conductor, as a building material, and {{as a component of}} various alloys. In <b>this</b> <b>work</b> <b>accuracy</b> of methods for quantitative determination (gravimetric and titrimetric methods of analysis) of copper(II) ion was studied. Gravimetric methods do not require a calibration or standardization step (as all other analytical procedures except coulometry do) because the results are calculated directly from the experimental data and molar masses. Thus, when only one or two samples are to be analyzed, a gravimetric procedure may be the method of choice because it involves less time and effort than a procedure that requires preparation of standards and calibration. In this work in gravimetric analysis the concentration of copper(II) ion is established through the measurement of a mass of CuSCN and CuO. Titrimetric methods is a process in which a standard reagent is added to a solution of an analyze until the reaction between the analyze and reagent is judged to be complete. In this work in titrimetric analysis the concentration of copper(II) ion is established through the measurement of a volume of different standard reagents: Km, Na 2 S 2 O 3 and AgNO 3. Results were discussed individually and mutually with the aspect of exactility, reproductivity and rapidity. Relative error was calculated for all methods...|$|E
40|$|Optimization of {{structures}} {{is always a}} fantasizing area for many researchers from past few decades. Many efforts have been taken in reducing the errors from the optimization process especially after advancement in computer faclities. The resulting structures are more efficient, economical and reliable. In traditional optimization techniques, {{the most important factor}} affecting on optimization process is the search direction which is the derivative of change in respose of structure due to change in design variables. This derivative is called as sensitivity derivative. Accuracy of sensitivity analysis is very much dependent on the method of structural analysis, technique of sensitivity calculation, computational efficiency etc. In <b>this</b> <b>work,</b> <b>accuracy</b> of sensitivity derivatives in elastic and plastic analyses are investigated on the basis of small strain theory. Combined with the Finite element method, which provides an excellent tool for the analysis of complex structures, the different techniques used for sensitivity calculations are finite difference method, semianalytical method and analytical method. Detail discussion of formulation and implementation of these methods are presented. Comparative study shows the relative error, cost of computation and efficiency of the above methods. From the results obtained, it can be stated that the finite difference method is the simplest technique that does not require access to the finite elemen analysis code and hence requires less efforts. However, this method is inefficient and less accurate. Analytical method is the most accurate method but its formulation and implementation is difficult as compared to other two metods. Semianalytical method is found to be a compromise of the two that results in more accurate solutions than from the finite diference method and is easy to implement as compared to the analytical method. The comparisons provide useful information for design engineers to decide a suitable method in the calculation of sensitivity erivatives for different structural optimization problems...|$|E
40|$|Improving and {{predicting}} {{the accuracy of}} positioning estimates derived from the global positioning system (GPS) {{continues to be a}} problem of great interest. Dependable and accurate positioning is especially important for navigation applications such as the landing of commercial aircraft. This subject gives rise to many interesting and challenging mathematical problems. This dissertation investigates two such problems. The first problem involves the study of the relationship between positioning accuracy and satellite geometry configurations relative to a user 2 ̆ 7 s position. In <b>this</b> <b>work,</b> <b>accuracy</b> is measured by so-called dilution of precision (DOP) terms. The DOP terms arise from the linear regression model used to estimate user position from GPS observables, and are directly related to user position errors. An analysis of the statistical properties explaining the behavior of the DOP terms is presented. The most accurate satellite geometries and worst configurations are given for some cases. The second problem involves finding methods for detecting and repairing cycle-slips in range delay data between a satellite and a receiver. The distance between a satellite and a receiver can be estimated by measuring the difference in the carrier frequency phase shift experienced between the satellite and receiver oscillators. Cycle-slips are discontinuities in the integer number of complete cycles in these data, and are caused by interruptions or degradations in the signal such as low signal to noise ratio, software failures, or physical obstruction of the signals. These slips propagate to errors in user positioning. Cycle-slip detection and repair are crucial to maintaining accurate positioning. Linear regression models and sequential hypothesis testing are used to model, detect, and repair cycle-slips. The effectiveness of these methods is studied using data obtained from ground-station receivers...|$|E
40|$|In <b>this</b> <b>work</b> the <b>accuracy</b> of {{wake vortex}} {{predictions}} of DLR's Wake Encounter Avoidance and Advisory System (WEAA) is analyzed {{by means of}} data gathered from a flight test campaign in April 2014. The system is based on airborne data exchange between aircraft and allows pilots to avoid potentially dangerous wake vortex encounters. As the vortex evolution is strongly controlled by atmospheric parameters, the acquisition of meteorological data is crucial for WEAA. The accuracy of the wake vortex predictions is investigated, employing first the current Automatic Dependent Surveillance (ADS-B) standard, which is then extended by additional meteorological data transmitted via telemetry, or data from numerical weather predictions...|$|R
40|$|In <b>this</b> <b>work,</b> an <b>accuracy</b> {{analysis}} of the estimation {{of the time of}} flight (ToF) for time domain reflectometry (TDR) signals is carried out. For this purpose, three different criteria (referred to as ‘maximum derivative’, ‘zero derivative’, and ‘tangents crossing’) were comparatively applied for the evaluation of the ToF of a TDR signal propagating along a set of RG- 58 coaxial cables (with different known length and with known electric parameters). Successively, as a further experimental test, the same criteria were applied on bi-wire cables with unknown electric parameters. Results show that, among the tested criteria, the ‘zero derivative’ criterion provided the best accuracy in the estimation of the ToF...|$|R
40|$|Wear {{testing of}} {{prosthetic}} joint components is fundamental for understanding wear mechanisms {{and improving the}} quality of manufactured orthopaedic prostheses. To this end, accurate measuring solutions are needed for quantifying wear volumes and characterizing the geometry of worn bearing surfaces. In <b>this</b> <b>work,</b> capability and <b>accuracy</b> of industrial computed tomography in wear assessment of prosthetic components are investigates. Advantages and limitations are determined in comparison with measurements by coordinate measuring machines...|$|R
40|$|Because clouds play an {{important}} role in the earth’s radiation energy budget, improving the cloud models in general circulation models (GCMs) is a necessary step for climate studies. In partially cloudy skies, the geometric and optical properties of individual cloud elements need to be considered. In general, GCMs ignore the geometry of ice clouds and water clouds. They also ignore the optical properties of water clouds, modeling them as blackbodies. In <b>this</b> <b>work,</b> the <b>accuracy</b> of cloud approximations will be examined in the 8 -mm to 12 -mm window region. Calculations for water clouds and ice clouds of two geometries in two different atmospheric conditions show that the black cloud assumption is good for opaque water clouds and the GCM treatment of ice clouds can be accurate...|$|R
30|$|Note that PC {{measurement}} can {{be employed}} to investigate both the quantum confined Stark effect (QCSE) and field-dependent absorption changes of the active region [30]. However, extraction of the absorption spectra is more relevant for EAM employing the surface-normal structure where 100 % quantum efficiency is normally assumed. Therefore, only the former is presented in <b>this</b> <b>work</b> since the <b>accuracy</b> of the absorption spectra for WG structure {{will depend on the}} knowledge of the coupling coefficient and intrinsic propagation loss.|$|R
40|$|Abstract – Previous {{researchers}} have reported success in predicting ethnicity and in predicting gender from {{features of the}} iris texture. This paper {{is the first to}} consider both problems using similar experimental approaches. Contributions of <b>this</b> <b>work</b> include greater <b>accuracy</b> than previous <b>work</b> on predicting ethnicity from iris texture, empirical evidence that suggests that gender prediction is harder than ethnicity prediction, and empirical evidence that ethnicity prediction is more difficult for females than for males. Keywords- iris biometric; soft biometric; texture analysis; gender prediction; ethnicity prediction. I...|$|R
40|$|The {{purpose of}} this project is to {{evaluate}} {{the possibility of using}} mobile GIS in mapping land use. Also included is a technical description of the equipment for field mapping (Juno SC and GeoExplorer 2008 GeoXH) and software for collection, editing and updating data in the field (ArcPad and TerraSync). The field research of the current state of land use was carried out as a part of <b>this</b> <b>work.</b> The positional <b>accuracy</b> and availability of the GPS signal were compared with concrete examples of mapped landscape objects. Key words: mobile GIS, GPS, mapping land us...|$|R
40|$|Abstract—The {{accuracy}} {{evaluation of}} image feature detectors is done using the repeatability criterion. Therefore, a wellknown data set consisting of image sequences and homography matrices is processed. This data serves as ground truth mapping {{information for the}} evaluation and is used in many computer vision papers. An accuracy validation of the benchmarks has not been done so far and is provided in <b>this</b> <b>work.</b> The <b>accuracy</b> is limited and evaluations of feature detectors may result in erroneous conclusions. Using a differential evolution approach for the optimization of a new, feature-independent cost function, {{the accuracy of the}} ground truth homographies is increased. The results are validated using comparisons between the repeatability rates before and after the proposed optimization. The new homographies provide better repeatability results for each detector. The repeatability rate is increased by up to 20 %. I...|$|R
40|$|Photoplethysmography (PPG) is a noninvasive optical technique, {{which can}} also be used to derive {{important}} parameters other than arterial oxygen saturation (SpO 2). In <b>this</b> <b>work,</b> the <b>accuracy</b> of the technique on detecting changes in blood perfusion during different levels of vascular occlusions has been explored. A dual-wavelength, reflectance PPG probe was applied on the left forearm of 10 healthy volunteers and raw PPG signals were acquired by a research PPG processing system. The raw PPG signals were separated into pulsatile AC and continuous DC PPG components. The signals were used to estimate SpO 2 and changes in concentration of oxygenated, deoxygenated, and total haemoglobin. Different levels of occlusions, from 20 mmHg to total occlusion were induced by a pressure-cuff on the left arm. The system was able to indicate all the occlusions. In particular, the haemoglobin concentration changes estimated from PPG were in high agreement with Near Infrared Spectroscopy measurements...|$|R
40|$|This {{contribution}} {{deals with}} the problem of automatic phoneme segmentation using HMMs. Automatization of speech segmentation task is important for applications, where large amount of data is needed to process, so manual segmentation is out of the question. In this paper we focus on automatic segmentation of recordings, which will be used for triphone synthesis unit database creation. For speech synthesis, the speech unit quality is a crucial aspect, so the maximal accuracy in segmentation is needed here. In <b>this</b> <b>work,</b> different kinds of HMMs with various parameters have been trained and their usefulness for automatic segmentation is discussed. At the end of <b>this</b> <b>work,</b> some segmentation <b>accuracy</b> tests of all models are presented...|$|R
50|$|Each of the museum’s {{exhibitions}} has as a basis, some substantial {{study or}} group of studies published by the National Academies. Expert scientists distill <b>this</b> <b>work,</b> ensuring scientific <b>accuracy.</b> The scientific committees and museum staff consult additional science experts on specific aspects of each exhibition. Finally, the exhibits are reviewed by focus groups from the general public to assure that they not only are scientifically correct, but also user-friendly, enjoyable and effective in conveying the information correctly. Exhibit topics reflect current scientific issues and complex topics that help people to use science to solve problems in their communities.|$|R
40|$|Accurate {{delivery}} of external beam radiation therapy relies on localization {{of the treatment}} target. For <b>this</b> <b>work,</b> the <b>accuracy</b> of an electromagnetic localization system (ELS) was first verified. Next, prostate deformations and rotations occurring throughout therapy were analyzed. Motion studies were also conducted to investigate {{the use of the}} ELS during intensity modulated arc therapy (IMAT) for prostate cancer. Lastly, appropriate margins were determined and new plans utilizing smaller margins were tested for two patients who exhibited large transponder displacements. Electromagnetic alignments were accurate to within 1 mm as compared to x-ray imaging. All patients fell within the default geometric residual limit (2 mm) and most fell within the default rotation limit (10 °). The ELS appeared to be suitable for use during IMAT with a 5 mm margin. A 3 mm margin was tested and was adequate when the main displacements were translational shifts; however, it was not adequate when large rotational displacements occurred. Thesi...|$|R
40|$|In {{the factory}} of the future, {{most of the}} {{operations}} will be done by autonomous robots that need visual feedback to move around the working space avoiding obstacles, to work collaboratively with humans, to identify and locate the working parts, to complete the information provided by other sensors to improve their positioning accuracy, etc. Different vision techniques, such as photogrammetry, stereo vision, structured light, time of flight and laser triangulation, among others, are widely used for inspection and quality control processes in the industry and now for robot guidance. Choosing which type of vision system to use is highly dependent on the parts {{that need to be}} located or measured. Thus, in this paper a comparative review of different machine vision techniques for robot guidance is presented. <b>This</b> <b>work</b> analyzes <b>accuracy,</b> range and weight of the sensors, safety, processing time and environmental influences. Researchers and developers can take it as a background information for their future works...|$|R
40|$|The {{thin film}} {{deposition}} {{property and the}} process difference during the wafer size migration from 12 ″ (300 mm) to 18 ″ (450 mm) in the Chemical Vapor Deposition (CVD) equipment is improved and reduced, respectively, when the chamber hardware is designed {{with the help of}} 3 D full chamber modeling and 3 D experimental visual technique developed in <b>this</b> <b>work.</b> The <b>accuracy</b> of 3 D chamber simulation model is demonstrated with the experimental visual technique measurement. With the CVD chamber hardware design of placing the inlet position and optimizing the distance between the susceptor edge and the reactor wall, the better thin film deposition property and the larger process compatibility during the wafer size migration from 12 ″ (300 mm) to 18 ″ (450 mm) for the industry cost reduction can be achieved. Non-dimensional Nusselt parameter is also found to be the effective indicator to monitor the thin film deposition property...|$|R
30|$|A WUSN testbed {{architecture}} is presented and aspects such as physical layout and software are discussed. The use {{of paper and}} plastic pipes are considered in detail, explaining the advantages of these devices {{in the process of}} burying and unburying sensor nodes. The influences of the antenna orientation and the soil moisture are highlighted. The importance of the qualification tests and procedures to identify the transitional region in a WUSN are discussed. Finally, results from experiments with the WUSN testbed are provided. The analysis of the results exemplifies the relation between the application of the guidelines proposed in <b>this</b> <b>work</b> with the <b>accuracy</b> of the results.|$|R
40|$|<b>This</b> <b>work</b> {{assesses the}} <b>accuracy</b> of {{specific}} numerical models {{in predicting the}} cure kinetics of a commercially available isotropic conductive adhesive material. A series of Differential Scanning Calorimetry (DSC) analyses have been performed on the materials to determine fundamental cure data. Cure models have been fitted to these experimental data using both the traditional and Particle Swarm Optimization (PSO) fitting methods. The traditional model fitting approach indicates a significant variation in the activation energy during the cure process. The particle swarm optimization fitting method is able to provide coefficient sets for all cure models assessed. Results obtained with these models are in relatively good agreement with experimental data...|$|R
40|$|<b>This</b> <b>work</b> {{deals with}} <b>accuracy</b> of silicon detectors, {{especially}} pixel detector DEPFET. Prediction of particle's in-detector position measurement errors was determined from simulated data and dependence of these errors on several parameters was analysed-some of the parameters {{relating to the}} detector (e. g. pixel size) and another relating to the particle (e. g. its energy). Basics about principle of silicon detectors, their applications and accuracy, experiment Belle and Belle II in KEK (The High Energy Accelerator Research Organization) and also statistic method as ANOVA, regression trees, division by probability (using entropy) are summarized here. Finally, original processing of simulated data (using regression trees) and resulting error predictions are presented...|$|R
30|$|Jiang et al. (2014) use {{logistic}} regression to predict performance using {{a mixture of}} a student’s achievement in the first assignment and social interaction within the MOOC community. <b>This</b> <b>work</b> achieved an <b>accuracy</b> of 92  % in predicting whether a student achieved a distinction or normal certificate and achieved 80  % accuracy in predicting whether someone achieved a normal certificate or did not complete (Jiang et al. 2014). In other works, Romero et al. (2013) have developed a data mining tool for Moodle that compares the performance of data mining techniques, including statistical methods, decision trees, rule and fuzzy rule induction methods, and neural networks, to predict a student’s final mark. <b>This</b> <b>work</b> used data from quizzes, assignments, and forums and achieved a very moderate accuracy of 65  %.|$|R
40|$|Mechanical {{deformability}} {{of cells}} {{is an important}} property for their function and development, {{as well as a}} useful marker of cell state. The classical technique of micropipette aspiration allows single-cell studies and we provide here a method to measure the two basic mechanical parameters, elastic modulus and Poisson’s ratio. The proposed method, developed from finite-element analysis of micropipette aspiration experiments, may be implemented in future technologies for the automated measurement of mechanical properties of cells, based on the micropipette aspiration technique or on the cell transit through flow constrictions. We applied this method to measure the elastic parameters of lymphocytes, in which the mechanical properties depend on their activation state. Additionally, we discuss in <b>this</b> <b>work</b> the <b>accuracy</b> of previous models to estimate the elastic modulus of cells, in particular the analytical model by Theret et al., widely used in the field. We show the necessity of using an improved model, taking into account the finite size of the cells, to obtain new insights that may remain hidden otherwise...|$|R
40|$|BC (Black Carbon), {{which can}} be found in the atmosphere, is {{characterized}} by a large value of the imaginary part of the complex refractive index and, therefore, might have an impact on the global warming effect. To study the interaction of BC with light often computer simulations are used. One of the methods, which are capable of performing light scattering simulations by any shape, is DDA (Discrete Dipole Approximation). In <b>this</b> <b>work</b> its <b>accuracy</b> was estimated in respect to BC structures using the latest stable version of the ADDA (vr. 1. 2) algorithm. As the reference algorithm the GMM (Generalized Multiparticle Mie-Solution) code was used. The study shows that the number of volume elements (dipoles) is the main parameter that defines the quality of results. However, they can be improved by a proper polarizability expression. The most accurate, and least time consuming, simulations were observed for IGT_SO. When an aggregate consists of particles composed of ca. 750 volume elements (dipoles), the averaged relative extinction error should not exceed ca. 4. 5 %...|$|R
40|$|Because {{of their}} {{sensitivity}} and {{high level of}} discrimination, short tandem repeat (STR) maker systems are currently the method of choice in routine forensic casework and data banking, usually in multiplexes up to 15 – 17 loci. Constraints related to sample amount and quality, frequently encountered in forensic casework, will not allow to change this picture in the near future, notwithstanding the technological developments. In this study, we present a free online calculator named PopAffiliator ([URL] for individual population affiliation in the three main population groups, Eurasian, East Asian and sub-Saharan African, based on genotype profiles for the common set of STRs used in forensics. This calculator performs affiliation based on a model constructed using machine learning techniques. The model was constructed using a data set of approximately fifteen thousand individuals collected for <b>this</b> <b>work.</b> The <b>accuracy</b> of individual population affiliation is approximately 86 %, showing that the common set of STRs routinely used in forensics provide {{a considerable amount of}} information for population assignment, in addition to being excellent for individual identification...|$|R
40|$|Alzheimer's Diasese (AD) {{diagnosis}} can {{be carried}} out by analysing functional or structural changes in the brain. Functional changes associated to neurological disorders can be figured out by positron emission tomography (PET) as it allows to study the activation of certain areas of the brain during specific task development. On the other hand, neurological disorders can also be discovered by analysing structural changes in the brain which are usually assessed by Magnetic Resonance Imaging (MRI). In fact, computer-aided diagnosis tools (CAD) that have been recently devised for the diagnosis of neurological disorders use functional or structural data. However, functional and structural data can be fused out in order to improve the accuracy and to diminish the false positive rate in CAD tools. In this paper we present a method for the diagnosis of AD which fuses multimodal image (PET and MRI) data by combining Sparse Representation Classifiers (SRC). The method presented in <b>this</b> <b>work</b> shows <b>accuracy</b> values up to 95 % and clearly outperforms the classification outcomes obtained using single-modality images. Universidad de Málaga. Campus de Excelencia Internacional Andalucía Tech...|$|R
40|$|Driver fatigue {{has become}} an {{important}} factor to traffic accidents worldwide, and effective detection of driver fatigue has major significance for public health. The purpose method employs entropy measures for feature extraction from a single electroencephalogram (EEG) channel. Four types of entropies measures, sample entropy (SE), fuzzy entropy (FE), approximate entropy (AE), and spectral entropy (PE), were deployed {{for the analysis of}} original EEG signal and compared by ten state-of-the-art classifiers. Results indicate that optimal performance of single channel is achieved using a combination of channel CP 4, feature FE, and classifier Random Forest (RF). The highest accuracy can be up to 96. 6 %, which has been able {{to meet the needs of}} real applications. The best combination of channel + features + classifier is subject-specific. In <b>this</b> <b>work,</b> the <b>accuracy</b> of FE as the feature is far greater than the Acc of other features. The accuracy using classifier RF is the best, while that of classifier SVM with linear kernel is the worst. The impact of channel selection on the Acc is larger. The performance of various channels is very different...|$|R
40|$|The {{general rate}} model {{provides}} a reliable platform to predict elution bands in both linear and non-linear chromatography provided the required equilibrium functions and the coefficients quantifying the mass transfer {{in and around}} the particles are available. If further the variation of the equilibrium functions with changes in the mobile phase composition is known, this model is also able to predict gradient elution chromatography. Significant disadvantages of the model are the need to specify three kinetic coefficients and the amount of computing time required for the numerical solution of the underlying equations. Thus, several simplified models have been suggested lumping mass transfer resistances together. In <b>this</b> <b>work</b> the <b>accuracy</b> of predicting chromatographic bands based on the numerical solution of two lumped models has been analyzed. Elution profiles calculated by (a) the transport-dispersive and (b) the equilibrium-dispersive models were compared between each other and with the solution of the more detailed general rate model. In the analysis performed both linear and non-linear chromatography was considered under isocratic and gradient conditions. copyright 2003 Elsevier B. V. All rights reserved. [accessed 2013 November 28 th...|$|R
40|$|A frequency-domain {{approach}} to efficiently simulate and minimize the crosstalk between high speed interconnects is proposed in this paper. Several methods for modeling coupled microstrip transmission lines are discussed. Several possible simulation strategies are also considered. A straightforward yet rigorous frequency domain approach is followed. This {{approach can be}} used for linearly and non-linearly terminated microstrip coupled lines, since it exploits the Harmonic Balance technique. A typical example of microstrip interconnects is simulated and the results are compared with those obtained in previous work by other authors, using time-domain methods. The simulation method proposed in <b>this</b> <b>work</b> yields good <b>accuracy.</b> A crosstalk minimization problem is formulated and resolved following the method proposed...|$|R
40|$|We present spectroscopic {{measurements}} of seven vibrational levels v= 29 - 35 of the A(1 ^ 1 Σ_u^+) excited state of Li_ 2 molecules by the photoassociation of a degenerate Fermi gas of ^ 6 Li atoms. The absolute uncertainty of our measurements is ± 0. 00002 cm^- 1 (± 600 kHz) {{and we use}} these new data to further refine an analytic potential for <b>this</b> state. <b>This</b> <b>work</b> provides high <b>accuracy</b> photo-association resonance locations essential for the eventual high resolution mapping of the X(1 ^ 1 Σ_g^+) state enabling further improvements to the s-wave scattering length determination of Li and enabling the eventual creation of ultra-cold ground state ^ 6 Li_ 2 molecules...|$|R
40|$|Analytical {{modelling}} {{and experimental}} measurement can {{are used to}} evaluate the performance of a network. Models provide insight and measurement provides realism. For software defined networks (SDN) it is unknown how well the existing queueing models represent the performance of a real SDN network. This leads to uncertainty between what can be predicted and the actual behaviour of a software defined network. <b>This</b> <b>work</b> investigates the <b>accuracy</b> of software defined network queueing models. This is done through comparing the performance results of analytical models to experimental performance results. The outcome of this is an understanding of how reliable the existing queueing models are and areas where the queueing models can be improved...|$|R
