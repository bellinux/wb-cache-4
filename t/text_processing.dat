1604|448|Public
5|$|Python {{has been}} used in {{artificial}} intelligence tasks. As a scripting language with modular architecture, simple syntax and rich <b>text</b> <b>processing</b> tools, Python is often used for natural language processing tasks.|$|E
5|$|The Perl {{languages}} borrow features {{from other}} programming languages including C, shell script (sh), AWK, and sed. They provide powerful <b>text</b> <b>processing</b> facilities without the arbitrary data-length limits of many contemporary Unix commandline tools, facilitating easy manipulation of text files. Perl 5 gained widespread {{popularity in the}} late 1990s as a CGI scripting language, {{in part due to}} its then unsurpassed regular expression and string parsing abilities.|$|E
25|$|The default <b>text</b> <b>processing</b> {{applications}} in Windows 10 (WordPad) and Mac OS 10.9 (TextEdit) support OpenDocument Text.|$|E
40|$|This paper {{presents}} {{an approach to}} partial parse selection for robust deep processing. The work {{is based on a}} bottom-up chart parser for HPSG parsing. Following the definition of partial parses in (Kasper et al., 1999), different partial parse selection methods are presented and evaluated on the basis of multiple metrics, from both the syntactic and semantic viewpoints. The application of the partial parsing in spontaneous speech <b>texts</b> <b>processing</b> shows promising competence of the method. ...|$|R
50|$|Within {{the root}} element, apart from {{any number of}} {{attributes}} and other elements, {{there may also be}} more optional <b>text,</b> comments, <b>processing</b> instructions and whitespace.|$|R
40|$|Extraction {{of events}} and {{understanding}} related temporal expression among them is a major challenge in natural language <b>processing.</b> In longer <b>texts,</b> <b>processing</b> on sentence-by-sentence or expression-by-expression basis often fails, {{in part due to}} the disregard for the consistency of the processed data. We present an ensemble method, which reconciles the output of multiple classifiers for temporal expressions, subject to consistency constraints across the whole text. The use of integer programming to enforce the consistency constraints globally improves upon the best published results from the TempEval- 3 Challenge considerably...|$|R
25|$|Unicode {{supports}} all {{of these}} types of writing systems through its numerous scripts. Unicode also adds further properties to characters to help differentiate the various characters and the ways they behave within Unicode <b>text</b> <b>processing</b> algorithms.|$|E
25|$|Perl is a high-level, general-purpose, interpreted, dynamic {{programming}} language. It was originally invented by Larry Wall, a linguist {{working as a}} systems administrator for NASA, in 1987, as a general purpose Unix scripting language to make report processing easier. Perl is also used for <b>text</b> <b>processing,</b> system administration, web application development, bioinformatics, network programming, applications that require database access, graphics programming etc.|$|E
25|$|In {{order to}} resolve issues brought by Han unification, a Unicode Technical Standard known as Unicode Ideographic Variation Database {{have been created}} to resolve the problem of specifying {{specific}} glyph in plain text environment.. By registering glyph collections into Ideographic Variation Database (IVD), {{it is possible to}} use Ideographic Variation Selectors to form Ideographic Variation Sequence (IVS) to specify or restrict the apporipate glyph in <b>text</b> <b>processing</b> in Unicode environment.|$|E
5000|$|Larry Heck {{was named}} Fellow of the Institute of Electrical and Electronics Engineers (IEEE) in 2016 for {{leadership}} in application of machine learning to spoken and <b>text</b> language <b>processing.</b>|$|R
5000|$|Integrating Applications of <b>Text</b> and Speech <b>Processing</b> (natural {{language}} understanding, question-answering strategies, assistive technologies) ...|$|R
30|$|The parsing {{application}} also formatted the log <b>text</b> for <b>processing</b> by LIWC. The participants’ input was extracted, removing {{all other}} information, including Freudbot responses and timestamps. Special delimiters were {{inserted in the}} the text to identify where interventions had taken place.|$|R
25|$|The SVD is also applied {{extensively}} to {{the study}} of linear inverse problems, and is useful in the analysis of regularization methods such as that of Tikhonov. It is widely used in statistics where it is related to principal component analysis and to Correspondence analysis, and in signal processing and pattern recognition. It is also used in output-only modal analysis, where the non-scaled mode shapes can be determined from the singular vectors. Yet another usage is latent semantic indexing in natural language <b>text</b> <b>processing.</b>|$|E
25|$|A common {{feature of}} Unix-like systems, Linux {{includes}} traditional specific-purpose programming languages targeted at scripting, <b>text</b> <b>processing</b> and system configuration and management in general. Linux distributions support shell scripts, awk, sed and make. Many programs {{also have an}} embedded programming language to support configuring or programming themselves. For example, regular expressions are supported in programs like grep and locate, the traditional Unix MTA Sendmail contains its own Turing complete scripting system, and the advanced text editor GNU Emacs is built around a general purpose Lisp interpreter.|$|E
25|$|In the 1960s, Thompson {{also began}} work on regular expressions. Thompson had {{developed}} the CTSS version of the editor QED, which included regular expressions for searching text. QED and Thompson's later editor ed (the standard text editor on Unix) contributed greatly to the eventual popularity of regular expressions, and regular expressions became pervasive in Unix <b>text</b> <b>processing</b> programs. Almost all programs that work with regular expressions today use some variant of Thompson's notation. He also invented Thompson's construction algorithm used for converting regular expression into nondeterministic finite automaton {{in order to make}} expression matching faster.|$|E
40|$|Haodi Feng City University of Hong Kong fenghaodi@hotmail. com Kang Chen and Technology TsingHua University, Beijing, PRC Chunyu Kit Department of Chinese, Translation and Linguistics Xiaotie Deng City University of Hong Kong Abstract Chinese {{texts are}} di#erent from English texts {{in that they}} have no spaces to mark the {{boundaries}} of words. This makes the segmentation a special issue in Chinese <b>texts</b> <b>processing.</b> Since the amount of Chinese texts grows rapidly, especially due to the fast increase of the Internet, the number of Chinese words is also increasing fast. Those segmentation methods that depend on an existing dictionary thus have an obvious defect when they are used to segment texts which may contain words unknown to the dictionary...|$|R
40|$|The novel {{algorithm}} {{proposed in}} this thesis will improve the non-negative matrix factorization. It will help factorizing and categorizing large data matrices. This algorithm will have application in facial recognition, document and <b>text</b> clustering. <b>Processing</b> high dimensional data will be easier using this algorithm...|$|R
40|$|The Internet {{continues}} to grow at a phenomenal rate {{and the amount of}} information on the web is overwhelming. It provides us {{a great deal of information}} resource. Due to its wide distribution, its openness and high dynamics, the resources on the web are greatly scattered and they have no unified management and structure. This greatly reduces the efficiency in using web information. Web text feature extraction is considered as the main problem in text mining. We use Vector Space Model (VSM) as the description of web text and present a novel feature extraction algorithm which is based on the improved particle swarm optimization with reverse thinking particles (PSORTP). This algorithm will greatly improve the efficiency of web <b>texts</b> <b>processing.</b> Key words...|$|R
25|$|The first {{well-known}} public {{presentation of}} markup languages in computer <b>text</b> <b>processing</b> {{was made by}} William W. Tunnicliffe {{at a conference in}} 1967, although he preferred to call it generic coding. It {{can be seen as a}} response to the emergence of programs such as RUNOFF that each used their own control notations, often specific to the target typesetting device. In the 1970s, Tunnicliffe led the development of a standard called GenCode for the publishing industry and later was the first chair of the International Organization for Standardization committee that created SGML, the first standard descriptive markup language. Book designer Stanley Rice published speculation along similar lines in 1970. Brian Reid, in his 1980 dissertation at Carnegie Mellon University, developed the theory and a working implementation of descriptive markup in actual use.|$|E
2500|$|Alexander Gelbukh and Grigori Sidorov (2001) [...] Proc. CICLing-2001, Conference on Intelligent <b>Text</b> <b>Processing</b> and Computational Linguistics, February 18–24, 2001, Mexico City. Lecture Notes in Computer Science N 2004, , , Springer-Verlag: 332–335.|$|E
2500|$|Development of {{the office}} suit Notis started in January 1978 out of the {{existing}} expertise in typesetting {{and a desire to}} sell computers to the public sector. [...] The word processing section was based on a program which had been developed by an employee while working at CERN and this was combined with systems for incorporating tables with figures such as budgets. The system also features search and e-mail. At first the QED text editor was used, but this was later replaced with TED, developed by Kvam Data. Notis was installed on all systems from 1980 and quickly became popular among customers. Because ND's screens were not optimized for <b>text</b> <b>processing,</b> from 1982 Tandberg Data delivered tailor-made keyboards and keyboards. Because <b>text</b> <b>processing</b> required high performance and extra peripherals, ND made high margins on the implementation. ND bought the typesetting system Comtec in 1981, which was combined with Nortext and resulted in large increases in the sale of the systems.|$|E
50|$|The ACM Symposium on Document Engineering is {{a yearly}} conference of {{computer}} scientists interested in <b>text</b> or document <b>processing.</b>|$|R
40|$|A {{cyclical}} process {{theory of}} macrostructure is presented which addresses the LTM representations of macropropositions, micropropositions and their relationship. The process model is simulated {{by a computer}} program ca'lled a Simulation of <b>Text</b> On-line <b>Processing</b> (STOP), which enables theory specific predictions to be derived for any text...|$|R
50|$|The IBM 5520 Administrative System was a <b>text</b> {{and data}} <b>processing</b> system, {{announced}} by IBM General Systems Division (GSD) in 1979.|$|R
2500|$|This is because, as was {{realized}} in the 1960s, the concept of [...] "macro processing" [...] is independent {{of the concept of}} [...] "assembly", the former being in modern terms more word processing, <b>text</b> <b>processing,</b> than generating object code. The concept of macro processing appeared, and appears, in the C programming language, which supports [...] "preprocessor instructions" [...] to set variables, and make conditional tests on their values. Note that unlike certain previous macro processors inside assemblers, the C preprocessor is not Turing-complete because it lacks the ability to either loop or [...] "go to", the latter allowing programs to loop.|$|E
2500|$|In {{computers}} and telecommunication systems, writing systems {{are generally not}} codified as such, but graphemes and other grapheme-like units that are required for <b>text</b> <b>processing</b> are represented by [...] "characters" [...] that typically manifest in encoded form. There are many , such as ISO/IEC 8859-1 (a character repertoire and encoding scheme oriented toward the Latin script), CJK (Chinese, Japanese, Korean) and bi-directional text. Today, many such standards are re-defined in a collective standard, the ISO/IEC 10646 [...] "Universal Character Set", and a parallel, closely related expanded work, The Unicode Standard. Both are generally encompassed by the term Unicode. In Unicode, each character, in every language's writing system, is (simplifying slightly) given a unique identification number, known as its code point. Computer operating systems use code points to look up characters in the font file, so the characters can be displayed on the page or screen.|$|E
2500|$|Donald Knuth has {{indicated}} {{several times that}} the source code of TeX has been placed into the [...] "public domain", and he strongly encourages modifications or experimentations with this source code. In particular, since Knuth highly values the reproducibility of the output of all versions of TeX, any changed version must not be called TeX, or anything confusingly similar. [...] To enforce this rule, any implementation of the system must pass a test suite called the TRIP test before being allowed to be called TeX. [...] The question of license is somewhat confused by the statements included {{at the beginning of}} the TeX source code, which indicate that [...] "all rights are reserved. [...] Copying of this file is authorized only if (...) you make absolutely no changes to your copy". [...] This restriction should be interpreted as a prohibition to change the source code as long as the file is called tex.web. [...] This interpretation is confirmed later in the source code when the TRIP test is mentioned ("If this program is changed, the resulting system should not be called 'TeX'"). The American Mathematical Society tried in the early 1980s to claim a trademark for TeX. [...] This was rejected because at the time [...] "TEX" [...] (all caps) was registered by Honeywell for the [...] "Text EXecutive" [...] <b>text</b> <b>processing</b> system.|$|E
40|$|Abstract. This {{paper is}} a {{contribution}} to <b>text</b> semantics <b>processing</b> and its application to advanced question-answering where {{a significant portion of}} a well-formed text is required as a response. We focus on procedural texts of various domains, and show how titles, instructions, instructional compounds and arguments can be extracted...|$|R
50|$|TCS Innovation Lab, Delhi: Software Architectures, Software as a Service, natural {{language}} <b>processing,</b> <b>text,</b> data and process analytics, multimedia applications and graphics.|$|R
30|$|The phonemic Polish {{language}} corpus contains words {{written with}} the use of phonemic notation, obtained on the basis of automatic grapheme-to-phoneme conversion of an orthographic <b>text.</b> Automatic <b>processing</b> of a natural language, very often requires the implementation of automatic grapheme-to-phoneme conversion. Grapheme-to-phoneme conversion determines phonemic transcriptions directly from orthographic representations [23].|$|R
5000|$|The <b>text</b> <b>processing</b> of {{a regular}} {{expression}} is a virtual editing machine, having a primitive programming language that has named registers (identifiers), and named positions in the sequence of characters comprising the text. Using these the [...] "text processor" [...] can, for example, mark a region of text, and then move it. The <b>text</b> <b>processing</b> of a utility is a filter program, or filter. These two mechanisms comprise <b>text</b> <b>processing.</b>|$|E
50|$|In {{this way}} markup such as font and color {{are not really}} a {{distinguishing}} factor, because the character sequences that affect font and color are simply standard characters inserted automatically by a background <b>text</b> <b>processing</b> mode, made to work transparently by compliant text editors, yet becoming otherwise visible as <b>text</b> <b>processing</b> commands when that mode is not in effect. So <b>text</b> <b>processing</b> is defined most basically (but not entirely) around the visual characters (or graphemes) rather than the standard, yet invisible characters.|$|E
50|$|<b>Text</b> <b>processing</b> is, unlike an algorithm, is a {{manually}} administered {{sequence of}} simpler macros {{that are the}} pattern-action expressions and filtering mechanisms. In either case the programmer's intention is impressed indirectly upon a given set of textual characters {{in the act of}} <b>text</b> <b>processing.</b> The results of a <b>text</b> <b>processing</b> step are sometimes only hopeful, and the attempted mechanism is often subject to multiple drafts through visual feedback, until the regular expression or markup language details, or until the utility options, are fully mastered.|$|E
5000|$|Text {{encoding}} uses a {{markup language}} to tag {{the structure and}} other features of a <b>text</b> to facilitate <b>processing</b> by computers. (See also Text Encoding Initiative.) ...|$|R
40|$|International audienceAs {{a general}} rule, data {{analytics}} are now mandatory for companies. Scanned document analysis brings additional challenges introduced by paper damages and scanning quality. In an industrial context, this work {{focuses on the}} automatic understanding of sale receipts which enable access to essential and accurate consumption statistics. Given an image acquired with a smart-phone, the proposed work mainly focuses on the first steps of the full tool chain which aims at providing essential information such as the store brand, purchased products and related prices with the highest possible confidence. To get this high confidence level, even if scanning is not perfectly controlled, we propose a double check processing tool-chain using Deep Convolutional Neural Networks (DCNNs) {{on one hand and}} more classical image and <b>text</b> <b>processings</b> on another hand. The originality of this work relates in this double check processing and in the joint use of DCNNs for different applications and text analysis...|$|R
40|$|The {{research}} {{we present}} {{in this paper}} focuses on the automatic management of the knowledge about experience goods and their features, starting from real texts generated online by internet users. The details about an experiment conducted on a dataset of product reviews, on which we tested a set of rule based and statistical solutions, will be described in the paper. The main goals are the review classification, the extraction of relevant product features and their systematization into product-driven ontologies. Feature extraction is performed through a rule based strategy grounded on SentIta, an Italian collection of subjective lexical resources. Features and Reviews are classified thanks to a distributional semantic algorithm. In the end, we face {{the problem of the}} extracted knowledge organization by integrating the subjective information produced by the internet users within a product-driven ontology. The NLP tool exploited in the work is LG-Starship, a hybrid framework for the on Italian <b>texts</b> <b>processing</b> based on the Lexicon-Grammar theory...|$|R
