15|279|Public
50|$|The {{simplest}} {{and most}} objective form of content analysis considers unambiguous {{characteristics of the}} text such as word frequencies, the page area taken by a newspaper column, or the duration of a radio or television program. Analysis of simple word frequencies is limited because {{the meaning of a}} word depends on surrounding <b>text.</b> <b>Keyword</b> In Context routines address this by placing words in their textual context. This helps resolve ambiguities such as those introduced by synonyms and homonyms.|$|E
50|$|Concept search {{techniques}} were developed because of limitations imposed by classical Boolean keyword search technologies {{when dealing with}} large, unstructured digital collections of <b>text.</b> <b>Keyword</b> searches often return results that include many non-relevant items (false positives) or that exclude too many relevant items (false negatives) because {{of the effects of}} synonymy and polysemy. Synonymy means that one of two or more words in the same language have the same meaning, and polysemy means that many individual words have more than one meaning.|$|E
40|$|Abstract—We {{propose a}} new segmentation-free method for keyword {{spotting}} in handwritten documents based on Heat Kernel Signature (HKS). After keypoints are located by the keypoint detector for SIFT on the document pages and the query image, HKS descriptors are extracted {{from a local}} patch centered at each keypoint. In order to locate the positions where the query image appears in document pages, we present a searching method which tries to locate a local zone which contains enough matching keypoints corresponding to the query image. Our method does not need any pre-processing steps. Keyword Spotting is a method used to spot query words in a large collection of documents. Unlike Optical Character Recognition (OCR), which tries to translate every character into ASCII <b>text,</b> <b>keyword</b> spotting always returns a rankin...|$|E
40|$|Scientific <b>texts</b> domain <b>keyword</b> {{is one of}} {{the basic}} {{elements}} of the text high-level semantics acquisition, domain ontology building and the knowledge representation in semantic grid, knowledge grid and escience environment. It is also the indispensable foundation and prerequisite work of Web scientific texts automatic classification, clustering and personalized services. TFIDF based TDDF formula is proposed to extract scientific <b>texts</b> domain <b>keyword.</b> The experiments proved that TDDF formula extracting <b>texts</b> domain <b>keyword</b> is superior to the classic TFIDF formula does. Above discussions and achievements can provide certain support not only for the establishment of semantic grid, knowledge grid and escience environment, but also for the Web knowledge acquisition, representation and text information retrieval and so on...|$|R
30|$|Obtain the <b>text</b> for <b>keyword</b> extraction; in our implementation, we {{used the}} title and the keyword list of the paper.|$|R
40|$|Abstract. Two common {{approaches}} to retrieving images from a collection are retrieval by <b>text</b> <b>keywords,</b> and retrieval by visual content. However, {{it is widely}} recognised {{that it is impossible}} for keywords alone to fully describe visual content. In this paper we present our approach of combining evidence from a contentoriented XML retrieval system and a content-based image retrieval system using a linear evidence combination approach as part of the INEX 2005 Multimedia track. ...|$|R
40|$|Abstract—Cloud {{computing}} {{is becoming}} increasingly prevalent in recent years. It introduces {{an efficient way to}} achieve manage-ment flexibility and economic savings for distributed applications. To take advantage of computing and storage resources offered by cloud service providers, data owners must outsource their data onto public cloud servers which are not within their trusted domains. Therefore, the data security and privacy become a big concern. To prevent information disclosure, sensitive data has to be encrypted before uploading onto the cloud servers. This makes plain <b>text</b> <b>keyword</b> queries impossible. As the total amount of data stored in public clouds accumulates exponentially, it is very challenging to support efficient keyword based queries and rank the matching results on encrypted data. Most current works only consider single keyword queries without appropriate ranking schemes...|$|E
40|$|Abstract — In {{recent days}} cloud {{computing}} is an emerging technology {{in terms of}} economy, as data owners are motivated to move their valuable data systems from local machines to commercial public cloud. But the security and sensitivity of the data should be preserved before outsourcing to public cloud. The data should be encrypted before outsourcing it in to the cloud, which needs to overcome the traditional way of data retrieval methods based on plain <b>text</b> <b>keyword</b> search. While dealing with large number of data users and files in cloud environment, {{it is necessary to}} consider efficient searching mechanism for better retrieval of data from cloud. Multi keyword query is an efficient data retrieval technique. In this proposed work we are using secure multi keyword search on encrypted cloud data and preserve the strict privacy policies. Coordinate matching and inner product similarity is used t...|$|E
40|$|Abstract: Email {{communication}} {{today is}} a way of working and communicating for most businesses and public in general. Being able to efficiently receive and send emails therefore becomes a must. Spam email detection and removal then becomes a vital process for the successful email communications, security and convenience. This paper describes a novel way of analysing and filtering incoming emails based on the <b>text</b> (<b>keyword)</b> salient features identified within. The method presented has promising results {{and at the same time}} significantly better performance than other statistical and probabilistic methods and at the same time offers a mechanism that can automatically adapt to new (unseen) email trends. The salient features of emails are selected automatically based on functions combining word frequency and other discriminating matrices, and then encoded into appropriate numerical vector models. The method is compared against the state-of-the-art Multinomial Naïve Bayes, Support Vector Machines and Boosted Decision Tress classifiers for identifyin...|$|E
5000|$|General Course Descriptive Data, including: course identifiers, {{language}} of content (English, Spanish, etc.), subject area (Maths, Reading, etc.), descriptive <b>text,</b> descriptive <b>keywords</b> ...|$|R
5000|$|<b>Text</b> {{retrieval}} tools: <b>Keyword</b> Retrieval, Query-by-Example, Cluster Extraction.|$|R
40|$|Abstract—In {{classical}} {{image classification}} approaches, lowlevel features have been used. But the high dimensionality of feature spaces poses {{a challenge in}} terms of feature selection and distance measurement during the clustering process. In this paper, we propose an approach to generate visual keyword and combine both visual and <b>text</b> <b>keywords</b> of the image to form a multimodal vector for image classification. This multimodality helps in extracting the image to image, text to text and text to image relations. A visual keyword is derived using vector quantization of image tiles. We arrange the visua...|$|R
40|$|Theoretical thesis. Bibliography: pages [71]- 77. 1. Introduction [...] 2. Literature review [...] 3. Data and {{preparation}} [...] 4. Application of ectraction techniques to BioASQ dataset [...] 5. Evaluation of five extraction processes [...] 6. Results and discussionThis research project explored {{the use of}} automatic keyword and keyphrase extraction techniques {{as a means to}} generate answers to biomedical questions. Keywords and keyphrases provide an essential way to present the topic of a given document and can help readers access core information in <b>text.</b> <b>Keyword</b> and keyphrase extraction techniques are typically used in information retrieval tasks. The purpose of this project is to select the suitability of these techniques in order to extract key concepts in training dataset of BioASQ shared task, as a first step towards achieving query-based abstractive summarisation. The outputs are measured by F 1 -metric to distinguish the performance of each technique. Mode of access: World wide web 1 online resource (xvi, 77 pages) diagrams, graphs, table...|$|E
40|$|Abstract. Filtering tweets {{relevant}} to a given entity is an important task for online reputation management systems. This contributes to a reliable analysis of opinions and trends regarding a given entity. In this paper we describe our participation at the Filtering Task of RepLab 2013. The goal of the competition is to classify a tweet as relevant or not {{relevant to}} a given entity. To address this task we studied a large set of features that can be generated to describe the relationship between an entity and a tweet. We explored dierent learning algorithms as well as, dierent types of features: <b>text,</b> <b>keyword</b> similarity scores between enti-ties metadata and tweets, Freebase entity graph and Wikipedia. The test set of the competition comprises more than 90000 tweets of 61 entities of four distinct categories: automotive, banking, universities and music. Results show that our approach is able to achieve a Reliability of 0. 72 and a Sensitivity of 0. 45 on the test set, corresponding to an F-measure of 0. 48 and an Accuracy of 0. 908...|$|E
40|$|Abstract — While {{structured}} P 2 P systems (such as DHTs) {{are often}} {{regarded as an}} improvement over unstructured P 2 P systems (such as super-peer networks) in terms of routing efficiency, {{it is not clear}} which architecture is better for full text search. This paper provides a quantitative comparison of full <b>text</b> <b>keyword</b> search in structured and unstructured P 2 P systems. We examine three techniques (and optimizations to those techniques) proposed in the literature: using a DHT along with inverted lists and Bloom filters; using a super-peer network; and using a random walk over an unstructured network. We use real Web documents and user queries to measure the cost for both document publishing and query processing, in terms of bandwidth and response time. Our results show that all three techniques use roughly the same bandwidth to process queries (with the super-peer technique having a slight edge). The structured network provides the best response time (30 percent better than a super-peer network), but has a high cost of document publishing, using six times as much bandwidth as the super-peer system. The random walk technique requires no publishing, but has a very long response time unless multiple random walks operate in parallel. I...|$|E
40|$|Abstract-Automatically {{assigning}} relevant <b>text</b> <b>keywords</b> {{to image}} {{is an important}} problem. Many algorithms have been proposed {{in the past decade}} and achieved good performance. Efforts have focused upon many other fields but properties of features have not been well investigated. In most cases, a group of features is selected in advance but important feature properties are not well used to feature selection. In this paper the performance of different features are compared, different combinations of features and a number of classification methods applied on the image annotation task, which gives insight into the features properties are also discussed...|$|R
50|$|Digital encyclopedias {{also offer}} greater search {{abilities}} than printed versions. While the printed versions rely on indexes {{to assist in}} searching for topics, computer accessible versions allow searching through article <b>text</b> for <b>keywords</b> or phrases.|$|R
40|$|Nowadays, web scale image {{search engines}} (e. g. Google Image Search, Microsoft Live Image Search) rely almost purely on {{surrounding}} text features. Users type keywords {{in hope of}} finding {{a certain type of}} images. The search engine returns thousands of images ranked by the <b>text</b> <b>keywords</b> extracted from the surrounding text. However, many of returned images are noisy, disorganized, or irrelevant. Even Google and Microsoft have no Visual Information for searching of images. Using visual information to re rank and improve text based image search results is the idea. This improves the precision of the text based image search ranking by incorporating the informatio...|$|R
40|$|Cryptography and {{steganography}} are the {{two significant}} techniques used in secrecy of communications and in safe message transfer. In this study CLSM – Couple Layered Security Model is suggested which has a hybrid structure enhancing information security using features of cryptography and steganography. In CLSM system; the information which has been initially cryptographically encrypted is steganographically embedded in an image at the next step. The information is encrypted {{by means of a}} <b>Text</b> <b>Keyword</b> consisting of maximum 16 digits determined by the user in cryptography method. Similarly, the encrypted information is processed, during the embedding stage, using a 16 digit pin (I-PIN) which is determined again by the user. The carrier images utilized in the study have been determined as 24 bit/pixel colour. Utilization of images in. jpeg,. tiff,. pnp format has also been provided. The performance of the CLSM method has been evaluated according to the objective quality measurement criteria of PSNR-dB (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index). In the study, 12 different sized information between 1000 and 609, 129 bits were embedded into images. Between 34. 14 and 65. 8 dB PSNR values and between 0. 989 and 0. 999 SSIM values were obtained. CLSM showed better results compared to Pixel Value Differencing (PVD) method, Simulated Annealing (SA) Algorithm and Mix column transform based on irreducible polynomial mathematics methods...|$|E
40|$|The article {{describes}} the status of our ongoing R&D at the U. S. National Library of Medicine (NLM) towards {{the development of an}} advanced multimedia database biomedical information system that supports content-based image retrieval (CBIR). NLM maintains a collection of 17, 000 digitized spinal X-rays along with text survey data from the Second National Health and Nutritional Examination Survey (NHANES II). These data serve as a rich data source for epidemiologists and researchers of osteoarthritis and musculoskeletal diseases. It is currently possible to access these through <b>text</b> <b>keyword</b> queries using our Web-based Medical Information Retrieval System (WebMIRS). CBIR methods developed specifically for biomedical images could offer direct visual searching of these images by means of example image or user sketch. We are building a system which supports hybrid queries that have text and image-content components. R&D goals include developing algorithms for robust image segmentation for localizing and identifying relevant anatomy, labeling the segmented anatomy based on its pathology, developing suitable indexing and similarity matching methods for images and image features, and associating the survey text information for query and retrieval along with the image data. Some highlights of the system developed in MATLAB and Java are: use of a networked or local centralized database for text and image data; flexibility to incorporate new research work; provides a means to control access to system components under development; and use of XML for structured reporting. The article details the design, features, and algorithms in this third revision of this prototype system, CBIR 3...|$|E
30|$|With {{the radical}} {{expansion}} of the digitization in the living world, it has become imperative to find a method to browse and search images efficiently from immense database. In general, three types of approaches for image retrieval are, text-based, content-based and semantic based. In recent times, web-based search engines such as, Google, Yahoo, etc., are being used extensively to search for images based on <b>text</b> <b>keyword</b> searching. Here, any image needs to be indexed properly before retrieving by text-based approach. Such an approach is highly tiresome and also unrealistic to handle by human annotation. Hence, more efficient search mechanism called “content based image retrieval” (CBIR) is required. Image retrieval has become a thrust area {{in the field of}} medicine, amusement and science etc.. The search in content based approach is made by analyzing the actual content of the image rather using metadata such as, keywords, tags or descriptions associated with an image. Hence, system can filter images based on their content would provide better indexing and return more accurate results. The effectiveness of a CBIR approach is greatly depends on feature extraction, which is its prominent step. The CBIR employs visual content of an image such as color, texture, shape and faces etc., to index the image database. Hence these features can be further classified as general (texture, color and shape) and domain specific (fingerprints, human faces) features. In this paper, we mainly focused on low-level features; the feature extraction method used in this paper is an effective way of integrating low-level features into whole. Widespread literature survey on CBIR is accessible in [1 – 4].|$|E
40|$|Image {{annotation}} is {{a method}} for representing an image with a suitable keyword closer to its semantic concept. Automatically assigning relevant <b>text</b> <b>keywords</b> to image is an important problem. Many algorithms and combination of different features have been proposed {{in the past and}} achieved good performance. Efforts have focused upon many other fields and some predefined set of features in the area of Automatic image annotation. But properties of features and their complementing combinations have not been well investigated. In this paper the performance of different feature combinations are compared, and find out the one which outperforms the other combinations by applying the Fuzzy K-nearest neighbor algorithm as the classification method...|$|R
5000|$|Image meta search - {{search of}} images based on {{associated}} metadata such as <b>keywords,</b> <b>text,</b> etc.|$|R
40|$|A {{method for}} saving storage space for text strings, such as {{compiler}} diagnostic messages, is described. The method relies on hand-selection {{of a set}} of text-strings which are common to one or more messages. These phrases are then stored only once. The storage technique gives rise to a mathematical optimization problem: determine how each message should use the available phrases to minimize its storage requirement. This problem is non-trivial when phrases which overlap exist. However, we present a dynamic programming algorithm which solves the problem in time which grows linearly with the number of characters in the <b>text.</b> <b>Keywords</b> and Phrases: diagnosic messages, error messages, common phrases, minimum space, text storage, optimization, dynamic programming...|$|R
30|$|Reference [18] has {{proposed}} that DF (document frequency) {{is the most}} simple method than others, but is inefficient on making use of the words with the lowest rising frequency well; Reference [19] has pointed that IG (information gain) can reduce the dimension of vector space model by setting the threshold, {{but the problem is}} that it is too hard to set the appropriate threshold; Reference [20] has thought that the method MI can make the words with the lowest rising frequency get more points than by other methods, because it is good at doing these words. In reference [21], a survey on intelligent techniques for feature selection and classification techniques used of intrusion detection has been presented and discussed. In addition, a new feature selection algorithm called intelligent rule based on attribute selection algorithm and a novel classification algorithm named intelligent rule-based enhanced multi-class support vector machine have been proposed. In reference [22], to address low efficiency and poor accuracy of keyword extraction of traditional TF-IDF (term frequency-inverse document frequency) algorithm, a <b>text</b> <b>keyword</b> extraction method based on word frequency statistics is put forward. Experimental results show that TF-IDF algorithm based on word frequency statistics not only overmatches traditional TF-IDF algorithm in precision ratio, recall ratio, and F 1 index in keyword extraction, but also enables to reduce the run time of keyword extraction efficiently. In reference [23], a feature extraction algorithm based on average word frequency of feature words within and outside the class is presented. This algorithm can improve the classification efficiently. In reference [24], a modified text feature extraction algorithm is proposed. The experimental results suggest that this algorithm is able to describe text features more accurately and better be applied to text features processing, Web text data mining, and other fields of Chinese information processing. In reference [25], a method, which targets the feature of short texts and is able to automatically recognize feature words of short texts, is brought forward. According to experimental results, compared with traditional feature extraction methods, this method is more suitable for the classification of short texts. In reference [26], this paper presented an ensemble-based multi-filter feature selection method that combines the output of one third split of ranked important features of information gain, gain ratio, chi-squared, and ReliefF. The resulting output of the EMFFS is determined by combining the output of each filter method.|$|E
40|$|Advances {{in remote}} sensing and {{geographic}} information system (GIS) technology {{have led to a}} dramatic increase in the amount of available geographic information - from satellite imagery and spatial databases to interactive maps. However, access to this information remains limited, particularly in developing countries. To help put this information in the hands of those who need it, the UN Food and Agriculture Organisation (FAO) has developed GeoNetwork, a spatial information management system that provides access via the Internet {{to a wide range of}} geo-referenced data from a variety of sources. GeoNetwork is designed to improve access to the FAO´s spatial and agrometeorological databases for decision-makers in agriculture, forestry and fisheries. Because GeoNetwork uses data descriptions (metadata) that conform to international standards, it allows for sharing of data among UN agencies, NGOs and research institutions worldwide. GeoNetwork is already in use in several countries. In Mozambique, for example, 12 government and international agencies working on agriculture, food security and humanitarian issues have been using GeoNetwork since September 2003 to share information and avoid duplication of work. The World Food Program (WFP) has also implemented the system in its regional offices in Senegal, South Africa and Uganda. A multi-layered view GeoNetwork allows users to search for data using a variety of search criteria (e. g. free <b>text,</b> <b>keyword,</b> country). All data formats are compatible with most professional GIS applications and freeware GIS tools. GeoNetwork has an inbuilt map viewer, called InterMap, which allows users to overlay map layers from multiple servers housed at development institutions worldwide to create a customized thematic composite map on their own computer. Each layer typically illustrates one or more variables, such as biophysical features (e. g. vegetation density, soil quality or rivers), climate (e. g. rainfall) or infrastructure (e. g. human settlements, roads or reservoirs). By overlaying various map layers, InterMap can illustrate the spatial relationships between a series of variables. It can suggest, for example, the extent to which a poor transport infrastructure is keeping a region in poverty, despite a rich agricultural endowment. GeoNetwork is free, open source software - a particular plus for users in developing countries, who can use, modify and redistribute the system source code and do not need to rely on foreign suppliers or costly proprietary software. See a target=_new href=[URL] FAO breaks new ground in geographic data management, FAO Press Release, 21 July 2004. To learn more about GeoNetwork, visit a target=_new href=[URL] www. fao. org/ [...] . /. The program can be downloaded free from a target=_new href=[URL] www. scourgeforge. net/ [...] . /Advances in remote sensing and {{geographic information system}} (GIS) technology have led to a dramatic increase in the amount of available geographic information - from satellite imagery and spatial databases to interactive maps...|$|E
40|$|One of {{the main}} goals of {{semantic}} search is to retrieve and connect information related to queries, offering users rich structured information about a topic instead {{of a set of}} documents relevant to the topic. Previous work reports that searching for information about individual entities such as persons, places and organisations is {{the most common form of}} Web search. Since the Semantic Web was first proposed, the amount of structured data on the Web has increased dramatically. This is particularly the case for what is known as Linked Data, information that has been published using Semantic Web standards such as RDF and OWL. Such structured data opens up new possibilities for improving entity search on the Web, integrating facts from independent sources, and presenting users with contextually-rich information about entities. This research focuses on entity search of Linked Data in terms of three different forms of search: structured queries, where users can use the SPARQL query language for manipulating data sources; exploratory search, where users can browse from one entity to another; and focused search, where users can input an entity query as a free <b>text</b> <b>keyword</b> search. We undertake a comparative study between two distinct information architectures for structured querying to manipulate Linked Data over the Web. Specifically, we evaluate some {{of the main}} operators in SPARQL using several datasets of Linked Data. We introduce a framework of five criteria to evaluate 15 current state-of-the-art semantic tools available for exploratory search of Linked Data, in order to establish how well these browsers make available the benefits of Linked Data and entity search for human users. We also use the criteria to determine the browsers that are best suited to entity exploration. Further, we propose a new model, the Attribute Importance Model, for entity-aggregated search, with the purpose of improving user experience when finding information about entities. The model develops three techniques: (1) presenting entity type-based query suggestions; (2) clustering aggregated attributes; and (3) ranking attributes based on their importance to a given query. Together these constitute a model for developing more informative views and enhancing users’ understanding of entity descriptions on the Web. We then use our model to provide an interactive approach, with the Information Visualisation toolkit InfoVis, that enables users to visualise entity clusters generated by our Attribute Importance Model. Thus this thesis addresses two challenges of searching Linked Data. The first challenge concerns the specific issue of information resolution during the search: the reduction of query ambiguity and redundant results that contain irrelevant descriptions when searching for information about an entity. The second challenge concerns the more general problem of technical complexity, and addresses to the limited adoption of Linked Data that we ascribe to the lack of understanding of Semantic Web technologies and data structures among general users. These technologies pose new design problems for human interaction such as overloading data, navigation styles, and browsing mechanisms. The Attribute Importance Model addresses both these challenges...|$|E
40|$|Abstract — In {{this paper}} {{the idea of}} mind map mining is presented. We propose that {{information}} retrieved from mind maps could improve academic search engines. The basic idea is that from a mind map’s <b>text,</b> <b>keywords</b> can be retrieved to describe research articles referenced by the mind map. So far, we have not conducted any research on mind map mining. Therefore this paper should only {{be seen as an}} early research in progress paper, outlining the ideas and aiming to stimulate a discussion. We start the discussion in this paper by presenting some challenges that mind map mining is likely to face. Index Terms — academic search engines, mind mapping, mind maps, search engines, text minin...|$|R
40|$|Abstract An {{information}} retrieval system employs a similarity heuristic {{to estimate the}} probability that documents and queries match each other. The heuristic is usually formulated {{in the context of}} a collection, so that the relationship between each document and the collection that contains it affects the scoring used to provide the ranked set of answers in response to a query. In this paper we continue our study of documentcentric similarity measures, but seek to eliminate the reliance on collection statistics in setting the documentrelated components of the measure. There is a direct implementation benefit of being able to do this – it means that impact-sorted inverted indexes can be built with just a single parse of the source <b>text.</b> <b>Keywords</b> Information Retrieval. ...|$|R
40|$|Abstract. This paper {{describes}} an approach for {{the integration of}} linguistic information in passage retrieval in an open-source question answering system for Dutch. Annotation produced by the wide-coverage dependency parser Alpino is stored in multiple index layers to be matched with natural language question that have been analyzed by the same parser. We present a genetic algorithm to select features {{to be included in}} retrieval queries and for optimizing keyword weights. The system is trained on questions annotated with their answers from the competition on Dutch question answering within the Cross-Language Evaluation Forum (CLEF). The optimization yielded a significant improvement of about 19 % in mean reciprocal rank scores on unseen evaluation data compared to the base-line using traditional information retrieval with plain <b>text</b> <b>keywords.</b> ...|$|R
40|$|Currently most {{existing}} image {{search engines}} suchas Google Images index web images majorly using <b>text</b> <b>keywords</b> {{extracted from the}} context, which may return large amount of junk information. We propose a novel clustering based filtering method to filter those junk images. Firstly we apply K-way min-max cut to cluster images returned by Google into multiple clusters based on the mixture of feature kernels, with kernel weights being determined automatically instead of hard fix. Secondly we select the best cluster in a robust way, and rank all the rest clusters according to their similarity with the best one. Finally those low-rank clusters can be filtered out as junk clusters. In experiments we obtain very comparative filtering performance to the current state-of-the-art, and improve Google Images search results significantly...|$|R
40|$|This paper {{describes}} an approach for {{the integration of}} linguistic information in passage retrieval in an open-source question answering system for Dutch. Annotation produced by the wide-coverage dependency parser Alpino is stored in multiple index layers to be matched with natural language question that have been analyzed by the same parser. We present a genetic algorithm to select features {{to be included in}} retrieval queries and for optimizing keyword weights. The system is trained on questions annotated with their answers from the competition on Dutch question answering within the Cross-Language Evaluation Forum (CLEF). The optimization yielded a significant improvement of about 19 % in mean reciprocal rank scores on unseen evaluation data compared to the base-line using traditional information retrieval with plain <b>text</b> <b>keywords...</b>|$|R
40|$|Abstract. A unified medical image {{retrieval}} framework integrating {{visual and}} <b>text</b> <b>keywords</b> using a novel multi-modal query expansion (QE) is presented. For the content-based image search, visual keywords are modeled using {{support vector machine}} (SVM) -based classification of local color and texture patches from image regions. For the text-based search, keywords from the associated annotations are extracted and indexed. The correlations between the keywords in both the visual and text feature spaces are analyzed for QE by considering local feedback information. The QE approach can propagate user perceived semantics from one modality to another and improve retrieval effectiveness when combined in multi-modal search. An evaluation of the method on imageCLEFmed’ 08 dataset and topics results in a mean average precision (MAP) score of 0. 15 over comparable searches without QE or using only single modality. ...|$|R
40|$|In this paper, {{we propose}} a novel {{framework}} for text-to-image generation based on association between text and image modalities. As an association model, we use hierarchical hypergraphs which consist of two layers including heterogeneous hypergraphs. While the first layer {{is composed of}} two hypergraphs: a text hypergraph and an image hypergraph, a hypergraph in the second layer associates two modalities by merging two hypergraphs in the first layer. In our model, hypergraphs are learned by self-organizing method based on random sampling. With multimodal association represented in the learned model, an intermediate image is generated by cross-modal inference when <b>text</b> <b>keywords</b> are given as a query. We use Korean magazine articles as a text-image data for experiments and we illustrate generated intermediate images and retrieved images similar to the intermediates as experimental results. ...|$|R
40|$|Current web video {{search results}} rely {{exclusively}} on <b>text</b> <b>keywords</b> or user-supplied tags. A search on typical popular video often returns many duplicate and near-duplicate videos {{in the top}} results. This paper outlines ways to cluster and filter out the nearduplicate video using a hierarchical approach. Initial triage is performed using fast signatures derived from color histograms. Only when a video cannot be clearly classified as novel or nearduplicate using global signatures, we apply a more expensive local feature based near-duplicate detection which provides very accurate duplicate analysis through more costly computation. The results of 24 queries in a data set of 12, 790 videos retrieved from Google, Yahoo! and YouTube show that this hierarchical approach can dramatically reduce redundant video displayed to the user in the top result set, at relatively small computational cost...|$|R
40|$|The goal of {{this work}} is {{analysis}} of the terms and translation of Shivastotras from the work Shivastotravali. It's author is Utapaleda, the founder of philosophical system called Pratyabhijñā, {{which is one of}} the most important works in the school of Kashmiri Shaivism. The translation is focused on the difficult, mainly philosophical and religious terms, which appear in the text. The text of Shivastotravali is special because of it's religious character, thanks to which it also belongs to bhakti literature. Because of this fact, my work contains except for chapter concerning philosophical tradition also the chapter which deals with bhakti, which is the best characterized by total devotion to God. The result of this work is terminological dictionary containing difficult words, which we can find in the <b>text.</b> <b>Keywords</b> Utpaladeva Kashmiri Shaivism Pratyabhijna Bhakt...|$|R
