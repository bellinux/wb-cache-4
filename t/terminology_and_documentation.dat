8|10000|Public
40|$|Physician {{reimbursement}} and {{the coding}} {{to support it}} are critically important to the sustained health of any physician 2 ̆ 7 s practice. This article reviews {{the recent history of}} physician reimbursement from the government and third-party payers and physician coding to support reimbursement. Explanations of <b>terminology</b> <b>and</b> <b>documentation</b> requirements are included...|$|E
40|$|To documentalists fits {{to develop}} and use {{documentary}} languages for indexing and information recovery, to guarantee terminological unit in the bibliographical processing products. We should look for in Terminology, the conceptual and theorical basis in documentary languages construction of specialized areas of the human knowledge, considering that Documentation has not theoretical basis to construct specialized lexicons. The documentalist who wants to develop documentary languages has to unite <b>Terminology</b> <b>and</b> <b>Documentation.</b> The Terminology will supply the methodology for search and use of terms and the Documentation the conceptual basis for the documentary language structuring...|$|E
40|$|This {{informative}} {{article entitled}} "Management and Translation : A Marriage of Convenience" describes how, {{in the day}} to day operations of the Translation Bureau as well as its future-oriented activities, management and translation have, over the years, formed an indissociable bond. Five fields - contracting services, <b>terminology</b> <b>and</b> <b>documentation,</b> recruitment and training, office automation and computer translation - are examined in some depth. Those wishing to obtain an English version of this article or more information should contact the author at the Planning, Management and Technology Branch, Translation Bureau, Secretary of State Department, Ottawa, Ontario, K 1 A OM 5...|$|E
5000|$|... gives {{terms and}} {{definitions}} related to nanomanufacturing processes {{in the field of}} nanotechnologies. It forms one part of multi-part <b>terminology</b> <b>and</b> definitions <b>documentation</b> covering the different aspects of nanotechnologies.|$|R
40|$|This paper {{examines}} how the changes currently {{taking place in}} <b>terminology</b> processing <b>and</b> <b>documentation</b> {{are related to the}} mul-tilingual needs of translation, and also how progress in natural language processing in general, <b>and</b> <b>terminology</b> processing in par-ticular, can {{contribute to the development of}} reliable, up-to-date terminology support tools for translators. The paper also describes some recent experiences in the automatic identification of termino-logical units from corpora. The paper concludes by identifying some specific areas in terminology software development which can benefit from the expertise of translators and other language professionals...|$|R
40|$|Scope: The {{objective}} was to perform an inventory and critical evaluation of folate data in selected European and international databases. The ultimate aim was to establish guidelines for compiling standardized folate databases for international nutritional studies. Methods and results: An ad hoc questionnaire was prepared to critically compare and evaluate folate data completeness, quantification, <b>terminologies,</b> <b>and</b> <b>documentation</b> of 18 European and international databases, and national fortification regulations. Selected countries participated in the European Prospective Investigation into Nutrition and Cancer (EPIC) project and European Food Information Resource Network (EuroFIR). Folate completeness was generally high. ‘Total folate’ {{was the most common}} <b>terminology</b> <b>and</b> microbiological assay was the most frequently reported quantification method. There is a lack of comparability within and between databases {{due to a lack of}} value documentation, the use of generic or non-appropriate terminologies, folate value conversions, and/or lack of identification of synthetic folic acid. Conclusion: Full value <b>documentation</b> <b>and</b> the use of EuroFIR component identifiers and/or INFOODS tagnames for total folate (‘FOL’) and synthetic folic acid (‘FOLAC’), with the additional use of individual folates, will increase comparability between databases. For now, the standardized microbiological assay for total folate and HPLC for synthetic folic acid are the recommended quantification methods...|$|R
40|$|A {{survey of}} the nature and phylogenetic {{distribution}} of nematode vulval appendages revealed 3 major classes based on composition, position, and orientation that included membranes, flaps, and epiptygmata. Minor classes included cuticular inflations, protruding vulvar appendages of extruded gonadal tissues, vulval ridges, and peri-vulval pits. Vulval membranes were found in Mermithida, Triplonchida, Chromadorida, Rhabditidae, Panagrolaimidae, Tylenchida, and Trichostrongylidae. Vulval flaps were found in Desmodoroidea, Mermithida, Oxyuroidea, Tylenchida, Rhabditida, and Trichostrongyloidea. Epiptygmata were present within Aphelenchida, Tylenchida, Rhabditida, including the diverged Steinernematidae, and Enoplida. Within the Rhabditida, vulval ridges occurred in Cervidellus, peri-vulval pits in Strongyloides, cuticular inflations in Trichostrongylidae, and vulval cuticular sacs in Myolaimus and Deleyia. Vulval membranes have been confused with persistent copulatory sacs deposited by males, and some putative appendages may be artifactual. Vulval appendages occurred almost exclusively in commensal or parasitic nematode taxa. Appendages were discussed based on their relative taxonomic reliability, ecological associations, and distribution in the context of recent 18 S ribosomal DNA molecular phylogenetic trees for the nematodes. Characters were found to be distributed across subsets of terminal and phylogenetically distant taxa, demonstrating considerable homoplasy. Accurate definitions, <b>terminology,</b> <b>and</b> <b>documentation</b> of the taxonomic distribution of vulval appendages are important in evaluations of hypotheses for either parallelism and developmental constraint or convergence and adaptation...|$|E
40|$|Agenda for {{collaboration}} or an agency agenda? Professionals’ experiences of colla­boration {{according to a}} coordinated individual plan (CIP) An increasing number {{of children and adolescents}} develop complex needs that require simultaneous action by different professionals. Several reports state that efforts for these children and adolescents have become increasingly specialized and fragmented. Since 2010, there are statutory requirements {{for collaboration}} according to a coordinated individual plan (SIP) between health care and social services. Pre-school and school can after regional agreement be involved in the co-ordination as equal partner. Collaboration in line with CIP is expected to offset the fragmentation for benefit of the service users’ ability to monitor and comprehend interventions. The aim was to investigate professionals’ experiences of CIP. The study consists of qualitative analysis of 12 focus group interviews with a total of 71 staff with different professions in health care, education and social services about their experiences of CIP. The results indicate that the participants act according to their core mission: nurturing, teaching and investigation. Two main categories with four sub-categories each appeared in the analysis. The main category, hindering factors, contains the categories: different mandates and requirements, requirements for presence initiative, questioning and censure, and timelines and prioritization. The main category of facilitating factors contains the categories: similar interpretation of common agreement, mutual respect and shared learning, common <b>terminology</b> <b>and</b> <b>documentation,</b> and willingness to collaborate. The analysis indicate that CIP was perceived as alternating between, on the one hand, a pro-active and service-focused tool, and on the other hand, a competing and compelling professional instrument...|$|E
40|$|There is {{increasing}} attention in Australia and internationally on advance care planning (ACP), {{a process which}} assists competent people {{to make decisions about}} their healthcare for a possible future time when they may no longer be competent. ACP can include the use of a written document and/or use of a substitute decision-maker to make healthcare decisions at a time of future incompetence. ACP is much more prevalent in the US than in Australia or other English-speaking countries. Australia is a federation of states and territories, which all make their own health law, resulting in different legislative provision, documentation and terminology. Despite research demonstrating strong support for ACP, it is not well understood, nor is it well utilised. Even in public hospitals, incorrect <b>terminology</b> <b>and</b> <b>documentation</b> put health professionals at risk. A recent position statement by the Australian Medical Association (AMA) recommends uniform national legislation for legally enforceable advance directives. Unfortunately, the AMA statement also proposes allowing doctors to ignore an advance healthcare directive if the doctor believes that it is 2 ̆ 7 inconsistent with good medical practice or advances in medical science, thereby preserving doctors 2 ̆ 7 clinical judgement and discretion 2 ̆ 7. It is a fundamental pillar of the doctor/patient relationship that it is the patient who ultimately holds the power to consent to, or refuse, treatment. Competent patients must always have the last say over what happens to their bodies. For those who fear loss of capacity, ACP can provide reassurance and take away some of the fear around the end stage of life...|$|E
5000|$|The {{database}} {{is maintained}} by <b>Terminology</b> <b>and</b> Reference Section, <b>Documentation</b> Division, Department of General Assembly and Conference Management, with {{its headquarters in}} New York City. The database {{has been put on}} the Internet in order to facilitate the understanding of the work of the UN by the public who {{do not have access to}} the intranet of the UN Secretariat.|$|R
40|$|Translation Environment Tools (TEnTs) {{became popular}} in the early 1990 s as a partial {{solution}} for coping with ever-increasing translation demands and {{the decreasing number of}} translators available. TEnTs allow the creation of repositories of legacy translations (translation memories) <b>and</b> <b>terminology</b> (integrated termbases) used to identify repetition in new source texts and provide alternate translations, thereby reducing the need to translate the same information twice. While awareness of the important role of <b>terminology</b> in translation <b>and</b> <b>documentation</b> management has been on the rise, little research is available on best practices for building and using integrated termbases. The present research is a first step toward filling this gap and provides a set of guidelines on how best to optimize the design and use of integrated termbases. Based on existing translation technology <b>and</b> <b>terminology</b> management literature, as well as our own experience, we propose that traditional <b>terminology</b> <b>and</b> terminography principles designed for stand-alone termbases should be adapted when an integrated termbase is created in order to take into account its unique characteristics: active term recognition...|$|R
40|$|An Integrated Management System (IMS) {{is based}} on the {{combination}} of separate Management Systems (MSs) with the aim of planning, realising, controlling, auditing, and systematically improving arrange of company performances to enhance efficiency and competitiveness. Although an IMS is a highly relevant management approach - capable of generating significant benefits, including less bureaucracy and paperwork, combined audits, fewer costs and more efficient management, and use of resources - to date there is no international formalised standard for the integration process. Within the EMAF project a priority was to design a methodology for IMS implementation - the backbone of POEMS - encompassing the Quality Management System (QMS) and Environmental Management System (EMS), addressed to the agri-food sector. A preliminary step in this context was a literature review of theoretical and empirical studies, focused on different perspectives, strategies, methodologies, and degree of integration. A model of IMS, specific to the agri-food sector, was defined: compatibility, similar <b>terminology,</b> procedures <b>and</b> <b>documentation,</b> but above all, the common structure of MSs based on the Plan-Do-Check-Act (PDCA) cycle, represented the methodological pillars of the proposed integrated approach. Finally, the application of the model was tested in a pilot company, operating in the tornato processing industry...|$|R
40|$|The {{development}} of the Space Launch System (SLS) launch vehicle requires cross discipline teams with extensive knowledge of launch vehicle subsystems, information theory, and autonomous algorithms dealing with all operations from pre-launch through on orbit operations. The characteristics of these systems must be matched with the autonomous algorithm monitoring and mitigation capabilities for accurate control and response to abnormal conditions throughout all vehicle mission flight phases, including precipitating safing actions and crew aborts. This presents a large complex systems engineering challenge being addressed in part {{by focusing on the}} specific subsystems handling of off-nominal mission and fault tolerance. Using traditional model based system and software engineering design principles from the Unified Modeling Language (UML), the Mission and Fault Management (M&FM) algorithms are crafted and vetted in specialized Integrated Development Teams composed of multiple development disciplines. NASA also has formed an M&FM team for addressing fault management early in the development lifecycle. This team has developed a dedicated Vehicle Management End-to-End Testbed (VMET) that integrates specific M&FM algorithms, specialized nominal and off-nominal test cases, and vendor-supplied physics-based launch vehicle subsystem models. The flexibility of VMET enables thorough testing of the M&FM algorithms by providing configurable suites of both nominal and off-nominal test cases to validate the algorithms utilizing actual subsystem models. The intent is to validate the algorithms and substantiate them with performance baselines for each of the vehicle subsystems in an independent platform exterior to flight software test processes. In any software development process there is inherent risk in the interpretation and implementation of concepts into software through requirements and test processes. Risk reduction is addressed by working with other organizations such as S&MA, Structures and Environments, GNC, Orion, the Crew Office, Flight Operations, and Ground Operations by assessing performance of the M&FM algorithms in terms of their ability to reduce Loss of Mission and Loss of Crew probabilities. In addition, through state machine and diagnostic modeling, analysis efforts investigate a broader suite of failure effects and detection and responses that can be tested in VMET and confirm that responses do not create additional risks or cause undesired states through interactive dynamic effects with other algorithms and systems. VMET further contributes to risk reduction by prototyping and exercising the M&FM algorithms early in their implementation and without any inherent hindrances such as meeting FSW processor scheduling constraints due to their target platform - ARINC 653 partitioned OS, resource limitations, and other factors related to integration with other subsystems not directly involved with M&FM. The plan for VMET encompasses testing the original M&FM algorithms coded in the same C++ language and state machine architectural concepts as that used by Flight Software. This enables the {{development of}} performance standards and test cases to characterize the M&FM algorithms and sets a benchmark from which to measure the effectiveness of M&FM algorithms performance in the FSW development and test processes. This paper is outlined in a systematic fashion analogous to a lifecycle process flow for engineering development of algorithms into software and testing. Section I describes the NASA SLS M&FM context, presenting the current infrastructure, leading principles, methods, and participants. Section II defines the testing philosophy of the M&FM algorithms as related to VMET followed by section III, which presents the modeling methods of the algorithms to be tested and validated in VMET. Its details are then further presented in section IV followed by Section V presenting integration, test status, and state analysis. Finally, section VI addresses the summary and forward directions followed by the appendices presenting relevant information on <b>terminology</b> <b>and</b> <b>documentation...</b>|$|E
40|$|The {{engineering}} {{development of}} the National Aeronautics and Space Administration's (NASA) new Space Launch System (SLS) requires cross discipline teams with extensive knowledge of launch vehicle subsystems, information theory, and autonomous algorithms dealing with all operations from pre-launch through on orbit operations. The nominal and off-nominal characteristics of SLS's elements and subsystems must be understood and matched with the autonomous algorithm monitoring and mitigation capabilities for accurate control and response to abnormal conditions throughout all vehicle mission flight phases, including precipitating safing actions and crew aborts. This presents a large and complex systems engineering challenge, which is being addressed in part {{by focusing on the}} specific subsystems involved in the handling of off-nominal mission and fault tolerance with response management. Using traditional model-based system and software engineering design principles from the Unified Modeling Language (UML) and Systems Modeling Language (SysML), the Mission and Fault Management (M&FM) algorithms for the vehicle are crafted and vetted in Integrated Development Teams (IDTs) composed of multiple development disciplines such as Systems Engineering (SE), Flight Software (FSW), Safety and Mission Assurance (S&MA) and the major subsystems and vehicle elements such as Main Propulsion Systems (MPS), boosters, avionics, Guidance, Navigation, and Control (GNC), Thrust Vector Control (TVC), and liquid engines. These model-based algorithms and their development lifecycle from inception through FSW certification are an important focus of SLS's development effort to further ensure reliable detection and response to off-nominal vehicle states during all phases of vehicle operation from pre-launch through end of flight. To test and validate these M&FM algorithms a dedicated test-bed was developed for full Vehicle Management End-to-End Testing (VMET). For addressing fault management (FM) early in the development lifecycle for the SLS program, NASA formed the M&FM team as part of the Integrated Systems Health Management and Automation Branch under the Spacecraft Vehicle Systems Department at the Marshall Space Flight Center (MSFC). To support the {{development of the}} FM algorithms, the VMET developed by the M&FM team provides the ability to integrate the algorithms, perform test cases, and integrate vendor-supplied physics-based launch vehicle (LV) subsystem models. Additionally, the team has developed processes for implementing and validating the M&FM algorithms for concept validation and risk reduction. The flexibility of the VMET capabilities enables thorough testing of the M&FM algorithms by providing configurable suites of both nominal and off-nominal test cases to validate the developed algorithms utilizing actual subsystem models such as MPS, GNC, and others. One of the principal functions of VMET is to validate the M&FM algorithms and substantiate them with performance baselines for each of the target vehicle subsystems in an independent platform exterior to the flight software test and validation processes. In any software development process there is inherent risk in the interpretation and implementation of concepts from requirements and test cases into flight software compounded with potential human errors throughout the development and regression testing lifecycle. Risk reduction is addressed by the M&FM group but in particular by the Analysis Team working with other organizations such as S&MA, Structures and Environments, GNC, Orion, Crew Office, Flight Operations, and Ground Operations by assessing performance of the M&FM algorithms in terms of their ability to reduce Loss of Mission (LOM) and Loss of Crew (LOC) probabilities. In addition, through state machine and diagnostic modeling, analysis efforts investigate a broader suite of failure effects and associated detection and responses to be tested in VMET to ensure reliable failure detection, and confirm responses do not create additional risks or cause undesired states through interactive dynamic effects with other algorithms and systems. VMET further contributes to risk reduction by prototyping and exercising the M&FM algorithms early in their implementation and without any inherent hindrances such as meeting FSW processor scheduling constraints due to their target platform - the ARINC 6535 -partitioned Operating System, resource limitations, and other factors related to integration with other subsystems not directly involved with M&FM such as telemetry packing and processing. The baseline plan for use of VMET encompasses testing the original M&FM algorithms coded in the same C++ language and state machine architectural concepts as that used by FSW. This enables the development of performance standards and test cases to characterize the M&FM algorithms and sets a benchmark from which to measure their effectiveness and performance in the exterior FSW development and test processes. This paper is outlined in a systematic fashion analogous to a lifecycle process flow for engineering development of algorithms into software and testing. Section I describes the NASA SLS M&FM context, presenting the current infrastructure, leading principles, methods, and participants. Section II defines the testing philosophy of the M&FM algorithms as related to VMET followed by section III, which presents the modeling methods of the algorithms to be tested and validated in VMET. Its details are then further presented in section IV followed by Section V presenting integration, test status, and state analysis. Finally, section VI addresses the summary and forward directions followed by the appendices presenting relevant information on <b>terminology</b> <b>and</b> <b>documentation...</b>|$|E
40|$|The <b>terminology</b> <b>and</b> {{notation}} {{used in the}} SIL {{source program}} for SNOBOL 4 developed {{over a period of}} years. As the internal structure of the implementation evolved, much of the original <b>terminology</b> became obsolete <b>and</b> inappropriate. Since existing source material <b>and</b> <b>documentation</b> was in use by implementors, changes in notation <b>and</b> <b>terminology</b> were not made...|$|R
40|$|The {{complexity}} of modern engineered systems motivates {{the requirement for}} timely access to technical <b>and</b> operational <b>documentation</b> (Boy 1991, 1992). Documents are both the most valuable and the most expensive knowledge resource in engineering organizations (Carter 1992). Product and product-related documents may be intended for use by thousands of people over a life cycle of many years (Nelson and Schuler 1995; Malcolm, Poltrock, and Schuler 1991). Designers, engineers, operators, maintenance technicians, suppliers, and subcontractors often require {{access to the same}} documents, but for different purposes and with different perspectives <b>and</b> <b>terminology.</b> Because <b>documentation</b> specialists cannot anticipate all the circumstances and questions that may arise, they try to organize and index text, graphic, and multimedia in a context-free manner. People, however, resist reading manuals that describe system features in a task-neutral way (Rettig 1991) Instead they use information retrieval strategies that are context-dependent (Mathé and Chen 1994; Boy and Mathé 1993; Boy 1991). For example...|$|R
40|$|The {{application}} of chemical-specific toxicokinetic or toxicodynamic data to address interspecies differences and human {{variability in the}} quantification of hazard has potential to reduce uncertainty and better characterize variability compared {{with the use of}} traditional default or categorically-based uncertainty factors. The present review summarizes the state-of-the-science since the introduction of the World Health Organization/International Programme on Chemical Safety (WHO/IPCS) guidance on chemical-specific adjustment factors (CSAF) in 2005 and the availability of recent applicable guidance including the WHO/IPCS guidance on physiologically-based pharmacokinetic (PBPK) modeling in 2010 as well as the U. S. EPA guidance on data-derived extrapolation factors in 2014. A summary of lessons learned from an analysis of more than 100 case studies from global regulators or published literature illustrates the utility and evolution of CSAF in regulatory decisions. Challenges in CSAF development related to the adequacy of, or confidence in, the supporting data, including verification or validation of PBPK models. The analysis also identified issues related to adequacy of CSAF documentation, such as inconsistent <b>terminology</b> <b>and</b> often limited and/or inconsistent reporting, of both supporting data and/or risk assessment context. Based on this analysis, recommendations for standardized <b>terminology,</b> <b>documentation</b> <b>and</b> relevant interdisciplinary research and engagement are included to facilitate the continuing evolution of CSAF development and guidance...|$|R
40|$|ENGLISH] This paper {{presents}} {{the elaboration of}} the ontology based system of documental information about forced displacement in Colombia. The research had as its main purpose to build an ontology that allows to represent and retrieve the reference information and {{the full text of}} documents on the subject. It presents a brief description of ontology and its application in information management and retrieval. The building of the ontology included three moments, which required the adoption of diverse methodologies of <b>terminology,</b> <b>documentation,</b> <b>and</b> computer engineering. The methodology of terminology was used to build the domain tree and the thesaurus and to structure the hierarchical relationships. The methodologies proposed by Noy and Macguiness and by the Universidad Politécnica de Madrid to structure ontologies were used to create the ontology and to determine the application of diverse computer's languages and tools. Finally, the paper describes the system of documental information, and the user's interface...|$|R
40|$|This article {{explores the}} role of {{languages}} of special purposes (LSP) <b>and</b> <b>terminology</b> in professional communication <b>and</b> <b>documentation</b> in businesses. Different communication models (Hoffmann 1985; Cassany 2004; Roelke 2010 [1999]) are discussed {{in order to show}} how LSP <b>and</b> <b>terminology</b> contribute to efficient, precise and appropriate communication in a professional setting. The second part of the article demonstrates that terminology plays a pivotal role in business documentation. Finally, the reciprocal relationship between communication <b>and</b> <b>documentation</b> is also discussed...|$|R
5000|$|Indexed to the MEDCIN ® {{terminology}} {{through a}} contextual hierarchy {{to the full}} array of medical <b>terminology</b> standards <b>and</b> concepts with intelligent prompting (IP). The indexing allows for the presentation <b>and</b> <b>documentation</b> of relevant clinical symptoms, history, physical findings, and diagnoses to the CCC nursing terminology from the Current Procedural <b>Terminology</b> (CPT)®, Diagnostic <b>and</b> Statistical Manual of Mental Disorders (DSM-IV), ICD, LOINC®, RxNORM, SNOMED CT® and others for virtually any clinical condition.|$|R
40|$|The primary {{objective}} {{of this paper is}} to describe how a computer-based corpus of texts can be of assistance to the terminology community, particularly for terminologists working in lesser-used living languages. Introduction The explosive growth of scientific <b>and</b> technical <b>documentation,</b> mainly due to progress in science and technology but also in some small way helped along by the development of word processors and world-wide computer-based communications systems, has been `good' for lesser-used living languages. The authors of this plethora of scientific <b>and</b> technical <b>documentation</b> express themselves not just in the so-called majority languages, such as English, German, French, Japanese, but also in lesser-used living languages like Catalan, Dutch, Norwegian, and Welsh, for instance. These authors are generally consumers of existing <b>terminology</b> <b>and</b> in certain domains also act as revisers/adaptors of existing <b>terminology</b> <b>and</b> as producers of new terminology. Given the smaller number [...] ...|$|R
40|$|Translation Environment Tools (TEnTs) {{became popular}} in the early 1990 s as a partial {{solution}} for coping with ever-increasing translation demands and {{the decreasing number of}} translators available. TEnTs allow the creation of repositories of legacy translations (translation memories) <b>and</b> <b>terminology</b> (integrated termbases) used to identify repetition in new source texts and provide alternate translations, thereby reducing the need to translate the same information twice. While awareness of the important role of <b>terminology</b> in translation <b>and</b> <b>documentation</b> management has been on the rise, little research is available on best practices for building and using integrated termbases. The present research is a first step toward filling this gap and provides a set of guidelines on how best to optimize the design and use of integrated termbases. Based on existing translation technology <b>and</b> <b>terminology</b> management literature, as well as our own experience, we propose that traditional <b>terminology</b> <b>and</b> terminography principles designed for stand-alone termbases should be adapted when an integrated termbase is created in order to take into account its unique characteristics: active term recognition, d one-click insertion of equivalents into the target text and document pretranslation. The proposed modifications to traditional principles cover a wide range of issues, including using record structures with fewer fields, adopting the TBX-Basic’s record structure, classifying records by project or client, creating records based on equivalent pairs rather concepts in cases where synonyms exist, recording non-term units and multiple forms of a unit, and using translated documents as sources. The overarching hypothesis and its associated concrete strategies were evaluated first against a survey of current practices in terminology management within TEnTs and later through a second survey that tested user acceptance of the strategies. The result is a set of guidelines that describe best practices relating to design, content selection and information recording within integrated termbases that will be used for translation purposes. These guidelines will serve as a point of reference for new users of TEnTs, as an academic resource for translation technology educators, as a map of challenges in terminology management within TEnTs that translation software developers seek to resolve and, finally, as a springboard for further research on the optimization of integrated termbases for translation...|$|R
40|$|Abstract Information {{transfer}} {{plays an}} important role in the design process and can be very easily presented on UML (Unified Modelling Language) diagrams. Author believes that <b>terminology</b> <b>and</b> notation of visual modelling with UML can be used as common language for design of the mechatronic systems <b>and</b> as <b>documentation</b> tool on every design phase. Index terms UML, mechatronics, flexible arm I...|$|R
40|$|Tectonics {{plays an}} {{important}} role in the genesis and subsequent mlnlng development of the Kwaggashoek East ore body. Lithological key units control the effectiveness of the ore forming processes, affecting the in situ ore reserve, The Kwaggashoek East deposit is the product of primary and secondary processes. A genetic model focussed on the source, migration and deposition of iron suggests a possible original source of iron as the product of very dilute hydrothermal input into deep ocean waters, with subsequent migration through structural conduits. Supergene processes account for the upgrading of the ore and the phosphorus redistribution. A good correlation between samples in a preliminary geostatistical study reflects the effectiveness of this process in the high grade ore zone. A broad overview of the economic issues which affect the commercialization of iron, indicates a balanced supply-demand situation for the five next years. The reserve estimation procedure requires accurate scientific <b>terminology</b> <b>and</b> appropriate methodology. <b>Documentation</b> is essential <b>and</b> should be detailed enough to allow for future reassessment. The results of three estimation methods in Kwaggashoek East differ by less than 5 %. The accuracy of the final results depends more on geological interpretation and assumptions than on the method applied. Although optimization of grade and tonnage in the Kwaggashoek East deposit seems to be met with the actual cut-off grade used in the Thabazimbi mine district, the grade-quality concept introduced in this thesis indicates a decrease in the estimated reserves for the deposi...|$|R
5000|$|<b>Terminology</b> <b>and</b> Knowledge Engineering. Proceedings of the 5th International Congress on <b>Terminology</b> <b>and</b> Knowledge Engineering TKE'99. TermNet, Wien 1999, [...]|$|R
40|$|The {{purpose of}} this thesis is to analyse how a {{translator}} should proceed with translating a medical text. The focus in this thesis is specifically on medical <b>terminology</b> <b>and</b> phraseology, how translation problems regarding these aspects of a medical text can be solved, and how the English <b>terminology</b> <b>and</b> phraseology relate to the Dutch <b>terminology</b> <b>and</b> phraseology. This is not only done by researching medical texts <b>and</b> medical <b>terminology</b> <b>and</b> phraseology in general; this thesis is also accompanied by an annotated translation of the article “Embryonal Brain Tumors” by Tiffany Sin Yu Chan, Xin Wang, Tara Spence, Michael Taylor and Annie Huang, of which the terminology, phraseology {{as well as other}} translational problems are analysed. This thesis concludes that each type of medical text requires a different approach and that consistency is an important factor when translating medical <b>terminology</b> <b>and</b> phraseology. It also shows that, although terminology finds its roots in Latin <b>and</b> Greek, both <b>terminology</b> <b>and</b> phraseology can be best acquired by osmosis...|$|R
30|$|Conclusions: Substantial {{variation}} {{exists in}} the <b>terminology</b> <b>and</b> definitional criteria for cohorts of patients receiving mechanical ventilation. Standardization of <b>terminology</b> <b>and</b> definitional criteria is required for study data to be maximally informative.|$|R
50|$|Mahadevan {{suggests}} that Gaudapada adopted Buddhist <b>terminology</b> <b>and</b> borrowed its doctrines to his Vedantic goals, much like early Buddhism adopted Upanishadic <b>terminology</b> <b>and</b> borrowed its doctrines to Buddhist goals; both used pre-existing concepts and ideas to convey new meanings.|$|R
40|$|<b>Terminology</b> <b>and</b> {{multilingualism}} {{have been}} one of the main focuses of the Athena Project. Linked Heritage as a legacy of this project also deals with <b>terminology</b> <b>and</b> bring theory to practice applying the recommendations given in the Athena Project. Linked Heritage as a direct follow-up of these recommendations on <b>terminology</b> <b>and</b> multilingualism is currently working on the development of a Terminology Management Platform (TMP). This platform will allow any cultural institution to register, SKOSify <b>and</b> manage its <b>terminology</b> in a collaborative way. This Terminology Management Platform will provide a network of multilingual <b>and</b> cross-domain <b>terminologies.</b> <b>Terminology</b> <b>and</b> multilingualism {{have been one}} of the main focuses of the Athena Project. Linked Heritage as a legacy of this project also deals with <b>terminology</b> <b>and</b> bring theory to practice applying the recommendations given in the Athena Project. Linked Heritage as a direct follow-up of these recommendations on <b>terminology</b> <b>and</b> multilingualism is currently working on the development of a Terminology Management Platform (TMP). This platform will allow any cultural institution to register, SKOSify <b>and</b> manage its <b>terminology</b> in a collaborative way. This Terminology Management Platform will provide a network of multilingual <b>and</b> cross-domain <b>terminologies.</b> </p...|$|R
40|$|The paper {{explores the}} {{concepts}} of information <b>and</b> <b>documentation</b> support, the basic features of information <b>and</b> <b>documentation</b> support of the Office of the Lviv regional council, presented promising areas of improvement information <b>and</b> <b>documentation</b> support of work Office of the Lviv regional council...|$|R
50|$|Need to {{introduce}} <b>terminology</b> <b>and</b> the setup...|$|R
30|$|We {{begin by}} {{introducing}} some <b>terminology</b> <b>and</b> lemmas.|$|R
5000|$|... “Civilization and the Problem of <b>Terminology</b> <b>and</b> Performance”.|$|R
5000|$|Carbon nanotube, {{includes}} general nanotube <b>terminology</b> <b>and</b> diagrams.|$|R
5000|$|... #Subtitle level 2: Molecular <b>terminology</b> <b>and</b> naming {{conventions}} ...|$|R
5000|$|... #Subtitle level 2: <b>Terminology</b> <b>and</b> its {{historical}} development ...|$|R
