2|10000|Public
40|$|There are countless {{collections}} of historical documents in archives and libraries that contain plenty of valuable information for historians and researchers. The extraction {{of this information}} has become a central task among the Document Analysis researches and practitioners. There is an increasing interest to digital preserve and provide access to these kind of documents. But only the digitalization {{is not enough for}} the researchers. The extraction and/or indexation of information of this documents has had an increased interest among researchers. In many cases, and in particular in historical manuscripts, the full transcription of these documents is extremely difficult due the inherent deficiencies: poor physical preservation, different writing styles, obsolete languages, etc. Word spotting has become a popular an efficient alternative to full transcription. It inherently involves a high level of degradation in the images. <b>The</b> <b>search</b> <b>of</b> <b>words</b> is holistically formulated as a visual search of a given query shape in a larger image, instead of recognising the input text and searching the query word with an ascii string comparison. But the performance of classical word spotting approaches depend on the degradation level of the images being unacceptable in many cases. In this thesis we have proposed a novel paradigm called contextual word spotting method that uses the contextual/semantic information to achieve acceptable results whereas classical word spotting does not reach. The contextual word spotting framework proposed in this thesis is a segmentation-based word spotting approach, so an efficient word segmentation is needed. Historical handwritten documents present some common difficulties that can increase the difficulties the extraction of the words. We have proposed a line segmentation approach that formulates the problem as finding the central part path in the area between two consecutive lines. This is solved as a graph traversal problem. A path finding algorithm is used to find the optimal path in a graph, previously computed, between the text lines. Once the text lines are extracted, words are localized inside the text lines using a word segmentation technique from the state of the art. Classical word spotting approaches can be improved using the contextual information of the documents. We have introduced a new framework, oriented to handwritten documents that present a highly structure, to extract information making use of context. The framework is an efficient tool for semi-automatic transcription that uses the contextual information to achieve better results than classical word spotting approaches. The contextual information is automatically discovered by recognizing repetitive structures and categorizing all the words according to semantic classes. The most frequent words in each semantic cluster are extracted and the same text is used to transcribe all them. The experimental results achieved in this thesis outperform classical word spotting approaches demonstrating the suitability of the proposed ensemble architecture for spotting words in historical handwritten documents using contextual information...|$|E
40|$|Advisor/s: Josep Lladós, Alicia Fornés. Date and {{location}} of PhD thesis defense: 14 November 2014, Autonomous University of BarcelonaThere are countless collections of historical documents in archives and libraries that contain plenty of valuable information for historians and researchers. The extraction of this information has become a central task among the Document Analysis researches and practitioners. There is an increasing interest to digital preserve and provide access to these kind of documents. But only the digitalization {{is not enough for}} the researchers. The extraction and/or indexation of information of this documents has had an increased interest among researchers. In many cases, and in particular in historical manuscripts, the full transcription of these documents is extremely difficult due the inherent deficiencies: poor physical preservation, different writing styles, obsolete languages, etc. Word spotting has become a popular an efficient alternative to full transcription. It inherently involves a high level of degradation in the images. <b>The</b> <b>search</b> <b>of</b> <b>words</b> is holistically formulated as a visual search of a given query shape in a larger image, instead of recognising the input text and searching the query word with an ascii string comparison. But the performance of classical word spotting approaches depend on the degradation level of the images being unacceptable in many cases. In this thesis we have proposed a novel paradigm called contextual word spotting method that uses the contextual/semantic information to achieve acceptable results whereas classical word spotting does not reach. The contextual word spotting framework proposed in this thesis is a segmentation-based word spotting approach, so an efficient word segmentation is needed. Historical handwritten documents present some common difficulties that can increase the difficulties the extraction of the words. We have proposed a line segmentation approach that formulates the problem as finding the central part path in the area between two consecutive lines. This is solved as a graph traversal problem. A path finding algorithm is used to find the optimal path in a graph, previously computed, between the text lines. Once the text lines are extracted, words are localized inside the text lines using a word segmentation technique from the state of the art. Classical word spotting approaches can be improved using the contextual information of the documents. We have introduced a new framework, oriented to handwritten documents that present a highly structure, to extract information making use of context. The framework is an efficient tool for semi-automatic transcription that uses the contextual information to achieve better results than classical word spotting approaches. The contextual information is automatically discovered by recognizing repetitive structures and categorizing all the words according to semantic classes. The most frequent words in each semantic cluster are extracted and the same text is used to transcribe all them. The experimental results achieved in this thesis outperform classical word spotting approaches demonstrating the suitability of the proposed ensemble architecture for spotting words in historical handwritten documents using contextual information...|$|E
40|$|Abstract: Machine {{translation}} (MT) {{is always}} a challenging job. It is really difficult {{to build up a}} complete machine translation system for natural languages. Machine translation includes natural language understanding and generation. The proposed system represents a new solution for building a MT system for English to Bangla translation, by modifying the rule-based transfer approach of MT system. In machine translation <b>the</b> <b>searching</b> <b>of</b> <b>word</b> from <b>the</b> lexicon is a compulsory task, here this searching stage is utilized efficiently by proposing an intelligent integer based lexicon system, consists of a number of separate lexicons and an algorithm is also developed for <b>searching</b> words from <b>the</b> lexicon in order to accomplish the basic steps of machine translation...|$|R
40|$|This work {{consists}} of a study {{on one of the}} most important and prominent aspects of Jos e Angel Valente's poetry: <b>the</b> <b>search</b> <b>of</b> <b>the</b> <b>word,</b> done by the author through his work. Therefore, this study seeks to show the different stages that we can find in the work of the writer, always referred to this search...|$|R
40|$|Abstract:- This paper {{presents}} a technique for tagging in {{natural language processing}} that can enhance the speed and accuracy of the part-of-speech tagging in the statistical parsing by using pipelining concept for fast <b>searching</b> and indexing. <b>The</b> running time of a parser depends upon <b>the</b> <b>searching</b> <b>of</b> respective <b>words</b> in <b>the</b> word-bank and their respective tags to match with the parse trees stored in the Parse Tree database...|$|R
40|$|OLISSIPO {{extracts}} {{lists of}} basic vocabulary from any Latin text and displays {{them together with}} linguistic and extra-linguistic information stored in a database. It contains other functionalities, such as statistical analysis, <b>search</b> <b>of</b> <b>words</b> in <b>the</b> text, displaying <b>of</b> <b>the</b> context, <b>search</b> <b>of</b> <b>words</b> in <b>the</b> database (which {{can be used as}} a dictionary). The LECTIO project has also been carried out by the Centro de Estudos Clássicos of the University of Lisbon and the Istituto di Linguistica Computazionale of CNR in Pisa. Designed to be a follow-up of OLISSIPO, it aims at producing a prototype for analysis of Latin texts with more functionalities. The final result will be an open tool to be used both in teaching/learning and in scientific research...|$|R
60|$|Second. Or, {{which may}} {{and will be}} great help to thee if thou shalt be sincere therein, namely, to betake thyself to <b>the</b> <b>search</b> <b>of</b> <b>the</b> <b>Word,</b> {{especially}} where thou readest of the conversion of men, and try if thy conversion be like, or has a good resemblance or oneness with theirs. But in this have a care that thou dost not compare thyself with those good folk of whose conversion thou readest not, or of the breaking of whose heart {{there is no mention}} made in Scripture; for all that are recorded in the Scripture for saints have not their conversion, as to the manner or nature of it, recorded in the Scripture.|$|R
40|$|This {{research}} advances computational {{cognitive modeling}} <b>of</b> visual <b>search,</b> and <b>the</b> synergistic relationship between cognitive modeling and eye tracking. The paper presents cognitive {{models of the}} perceptual, cognitive, and motor processing involved in <b>the</b> visual <b>search</b> <b>of</b> <b>words</b> in structured layouts that vary in density. The layouts are allsparse, all-dense, or mixed. A principled approach is taken to account for eye movement data, specifically {{the mean number of}} fixations per trial and mean fixation durations. A random search strategy without replacement is used as a base model. The best-fitting model assumes that people examine two to three items per fixation regardless of the density. A new implementation of the EPIC cognitive architecture is used to build the models in this study. Modeling adjustments necessary to account for the data are discussed...|$|R
5000|$|Active age of {{a domain}} should not be {{confused}} with the date of registration on a domain's WHOIS record, but instead refers to the time when Google first indexed pages on the domain. Keyword competitiveness refers to <b>the</b> <b>search</b> frequency <b>of</b> a <b>word</b> on Google search, with observation suggesting that <b>the</b> higher <b>the</b> <b>search</b> frequency <b>of</b> a <b>word,</b> <b>the</b> increasing likelihood that the sandbox filter effect will come into play. While the presence of the Google Sandbox has been long debated, Google has made no direct disclosure on the matter. However, as the sandbox effect almost certainly refers to a set of filters in play for anti-spam purposes, it is unlikely Google would ever provide details on the matter. In one instance though, Google's John Mueller, [...] has mentioned that [...] "it can take a bit <b>of</b> time for <b>search</b> engines to catch up with your content, and to learn to treat it appropriately. It's one thing to have a fantastic website, but search engines generally need a bit more to be able to confirm that, and to rank your site - your content - appropriately". This could be understood as the cause for the sandbox effect.|$|R
40|$|The Integral Dictionary is a {{comprehensive}} lexical network for French, English, German, Italian, and Spanish. It is based on componential semantics and lexical functions. The network structure superimposes two graphs. A first graph consists of a hierarchy of concepts divided into classes and themes where the words form the terminal nodes of the graph. A second graph links the words together using lexical functions derived from the Meaning-Text theory. We first introduce the lexical network whose database for French words is. comparable in its size to that of WordNet. We then describe two semantic distances to evaluate the proximity <b>of</b> two. <b>words</b> in the graph and to find their distinctive semantic components. Finally, we give examples of applications we developed with it: <b>the</b> <b>search</b> <b>of</b> a <b>word</b> from a definition and the extraction of the semantic features of a text...|$|R
40|$|International audienceA finite {{language}} X over an alphabet S {{is complete}} if any word in S^* {{is a factor}} <b>of</b> a <b>word</b> in X^*. A word {{which is not a}} factor of X^* is said uncompletable. Among them, some are minimal as all their proper factors belong to Fact(X^*). The problem is to find bounds on the length of the shortest minimal uncompletable words depending on k, the maximal length <b>of</b> <b>words</b> in X. Though Restivo's conjecture stating an upper bound in 2 k^ 2 was already contradicted twice, the problem of the existence of a quadratic upper bound is still open. Our approach is original and synergic. We start by characterizing minimal uncompletable words. An efficient in practice algorithm is given that speeds up <b>the</b> <b>search</b> <b>of</b> such <b>words.</b> Finally, a genetic algorithm using a SAT-solver allows us to obtain new results for the first values of k...|$|R
2500|$|Typically, diff {{is used to}} {{show the}} changes between two {{versions}} of the same file. Modern implementations also support binary files. The output is called a [...] "diff", or a patch, since the output can be applied with the Unix program patch. [...] The output of similar file comparison utilities are also called a [...] "diff"; like the use <b>of</b> the <b>word</b> [...] "grep" [...] for describing <b>the</b> act <b>of</b> <b>searching,</b> <b>the</b> <b>word</b> diff became a generic term for calculating data difference and the results thereof.|$|R
40|$|Word {{alignment}} plays {{a critical}} role in statistical machine translation (SMT) and cross-language information retrieval. Until now, most existing methods get the word alignment within the whole range of the sentence length. The alignment quality is unsatisfactory. In this paper, we propose a novel approach to word alignment based on multi-grain model (WAMG). We split a parallel sentence pair into blocks in different grain and get the word alignments within each corresponding block. Our approach is able to restrict <b>the</b> <b>search</b> space <b>of</b> <b>word</b> alignment in the relatively accurate local range and reduce the mapping error. The experiments have shown that our approach outperforms the traditional word alignment algorithm relatively by about 12 % in AER and improves the performance of Chinese-to-English nglish translation system relatively by about 2. 8 % in BLEU. Index Terms — Statistical machine translation, nslation, word alignment, multi-grain, sub-sentence, block...|$|R
40|$|Simplification of raster {{images is}} a {{relatively}} mature subject in the image analysis and recognition field, but it is yet quite far from being perfect. The purpose of our approach is to provide contrast and edge enhancement, followed by a thresholding process that yields a binary version of the original image. Then, we apply a noise reduction algorithm followed by a thinning step to produce a simplified image which can be analyzed with less consuming time and decreasing <b>the</b> cost <b>of</b> <b>searching.</b> Key <b>words...</b>|$|R
40|$|Abstract. In {{this paper}} we {{describe}} {{a method of}} effective handling of linguistic data by means of covering and inhibiting patterns – patterns that “compete” each other. A methodology of developing such patterns is outlined. Applications {{in the areas of}} morphology, hyphenation and part-of-speech tagging are shown. This pattern-driven approach to language engineering allows the combination of linguist expertise with the data learned from corpora – layering <b>of</b> knowledge. <b>Searching</b> for information in pattern database (dictionary problem) is blindingly fast – linear with respect to <b>the</b> length <b>of</b> <b>searching</b> <b>word</b> as with other finite-state approaches. ...|$|R
40|$|We {{present a}} ring theoretic {{approach}} to Cerny's conjecture via the Wedderburn-Artin theory. We first introduce the radical {{ideal of a}} synchronizing automaton, and then the natural notion of semisimple synchronizing automata. This is a rather broad class since it contains simple synchronizing automata like those in Cerny's series. Semisimplicity gives also the advantage of "factorizing" the problem of finding a synchronizing word into the sub-problems <b>of</b> finding "short" <b>words</b> that are zeros into the projection of the simple components in the Wedderburn-Artin decomposition. In the general case this last problem is related to <b>the</b> <b>search</b> <b>of</b> radical <b>words</b> <b>of</b> length at most (n- 1) (2) where n {{is the number of}} states of the automaton. We show that the solution of this "Radical Conjecture" would give an upper bound 2 (n- 1) (2) for the shortest reset word in a strongly connected synchronizing automaton. Finally, we use this approach to prove the Radical Conjecture in some particular cases and Cerny's conjecture for the class of strongly semisirnple synchronizing automata. 'These are automata whose sets <b>of</b> synchronizing <b>words</b> are cyclic ideals, or equivalently, ideal regular languages that are closed under taking roots...|$|R
40|$|ABSTRACTThe study aims {{to review}} {{articles}} that relate organizational life cycle to management controls, published in {{national and international}} journals. International journals were selected from <b>the</b> <b>search</b> <b>of</b> <b>the</b> <b>word</b> “accounting” {{in the title of}} the journals in SIBi database of University of São Paulo, having found 67 journals. To select the articles the words “life cycle” were <b>searched</b> in <b>the</b> title, abstract and keywords, in which 39 articles were identified. For national publications, <b>the</b> <b>search</b> focused on <b>the</b> journals of Administration/ Accounting/Tourism, classified in Qualis CAPES, published between 2005 and 2009. The selection criteria for the articles was the presence of the terms “life cycle” and “management control” in the title, abstract and keywords, in which 18 articles were identified. The reading of the abstracts of these articles was done so as to assess its suitability for the purpose of the study, resulting in seven international and eight national articles. A descriptive research was carried out through content analysis. The analysis revealed an emerging and reduced scientific production of 15 articles relating organizational life cycle to management controls, {{but at the same time}} signals the opportunity to study in the face of what was presented...|$|R
40|$|This review {{has been}} done after careful {{research}} of articles published in indian journal <b>of</b> psychiatry with <b>the</b> <b>search</b> <b>words</b> <b>of</b> manic depressive psychosis and bipolar mood disorder. Many articles in the following areas are included: 1) Etiology: genetic studies: 2) Etiology – neuro psychological impairment: 3) Adult bipolar disorder 4) Epidemological 5) Clinical picture – phenomenology: 6) Course of bipolar mood disorder: 7) Juvenile onset bipolar affective disorder 8) Secondary mania: 9) Clinical variables and mood disorders: 10) Disability: 11) Comorbidity: 12) Treatment: biological 13) Recent evidence: 14) Pharmacological evidence in special population. Though {{there seems to be}} significant contribution, there are still lot of areas which need careful intervention. The findings in various studies from the indian point of view are reviewed...|$|R
40|$|In {{order to}} make the Scriptures {{relevant}} for each generation, the church must continually <b>search</b> <b>the</b> <b>Word</b> <b>of</b> God diligently in order to apply its teaching to the contemporary issues for each culture and in each generation. However, relevant biblical theology must be continually checked and corrected In the modern quest for a relevant African Christian Theology many have become overly optimistic about the moral nature of man and his religion, ignoring the teachings of Scripture. In this first article of a two-part series, Pastor Alfred Muli examines Romans 1. · 18 - 25 which serves as a lodestar in the evaluation of African Traditional Religion and the shaping of African Christian Theology...|$|R
40|$|Abstract. Using recent {{results on}} the {{occurrence}} times {{of a string of}} symbols in a stochastic process with mixing properties, we present a new method for <b>the</b> <b>search</b> <b>of</b> rare <b>words</b> in biological sequences modelled by a Markov chain. We obtain a bound on the error between the distribution of the number of occurrences <b>of</b> a <b>word</b> in a sequence and its Poisson approximation. A global bound is already given by a Chen-Stein method. Our approach, the ψ-mixing method, gives local bounds. Since we only need the error in the tails of distribution, the global uniform bound of Chen-Stein is too large and it is a better way to consider local bounds. It {{is the first time that}} local bounds are devised for Poisson approximation. We search for two thresholds on the number of occurrences from which we can regard a studied word as an over-represented or an under-represented one. A biological role is suggested for these over- or under-represented words. Our method gives such thresholds for a panel <b>of</b> <b>words</b> much broader than the Chen-Stein method which cannot give any result in a great number of cases where our method works. Comparing the methods, we observe a better accuracy for the ψ-mixing method for the bound of the tails of distribution. Our method can obviously be used in domains other than biology. We also present the software PANOW (available a...|$|R
40|$|Using recent {{results on}} the {{occurrence}} times {{of a string of}} symbols in a stochastic process with mixing properties, we present a new method for <b>the</b> <b>search</b> <b>of</b> rare <b>words</b> in biological sequences generally modelled by a Markov chain. We obtain a bound on the error between the distribution of the number of occurrences <b>of</b> a <b>word</b> in a sequence (under a Markov model) and its Poisson approximation. A global bound is already given by a Chen-Stein method. Our approach, the psi-mixing method, gives local bounds. Since we only need the error in the tails of distribution, the global uniform bound of Chen-Stein is too large and it is a better way to consider local bounds. We search for two thresholds on the number of occurrences from which we can regard the studied word as an over-represented or an under-represented one. A biological role is suggested for these over- or under-represented words. Our method gives such thresholds for a panel <b>of</b> <b>words</b> much broader than the Chen-Stein method. Comparing the methods, we observe a better accuracy for the psi-mixing method for the bound of the tails of distribution. We also present the software PANOW (available at [URL] dedicated to the computation of the error term and the thresholds for a studied word. Comment: 29 pages, 0 figure...|$|R
40|$|Center for Learning & Teaching, M 234 x 2945 This is an {{annotated}} {{list of all}} {{the journals}} in the "Center for Learning and Teaching Library, " a collection of materials on teaching and learning at the college level. The collection is on reserve in the Center for Learning and Teaching located in M 236. " To search this list, you can use <b>the</b> Find or <b>Search</b> function <b>of</b> your <b>word</b> processor in either Word or WordPerfect press edit and find. Enter the text you want to search for. Three hints for more productive searches. 1. <b>Search</b> for roots <b>of</b> <b>words.</b> For example, {{if you are interested in}} classroom assessment techniques, search for assess and you will find anything with "assess, " "assesses, " "assessed, " "assessing, " "assessment, " etc., in the title or description. You can use wildcard characters in a search as well, but you can't type them from the keyboard; you must select Codes from the Match menu in the Find and Replace dialog box. In general, it's easier just to search for roots. 2. Search for synonyms. In addition to searching for assess, search for grad to find "grade, " "grades, " and "grading," evaluat to find "evaluate, " "evaluating, " "evaluation, " etc. <b>The</b> <b>search</b> function <b>of</b> <b>word</b> processors is not very powerful so you can't do an "or " search. You will have to do a separate search for each synonym. 3. Search for disciplines as well as teaching topics. There are many articles, for example, on teaching specific subjects, such as writing...|$|R
40|$|Movement {{information}} on objects in videos {{can be used}} to characterize the content of a scene. This paper proposes a novel shape-based movement-searching algorithm that can effectively search human movements in video streams. Movement information is represented a sequence of boundary shapes of human subjects extracted from input video frames. Information on individual shapes in the sequence is converted into a 1 D shape feature by using a shape descriptor. Human movements can be identified in a sequence of shape features extracted from the video, as in <b>the</b> case <b>of</b> <b>searching</b> for <b>words</b> in a long block of text. A comparison of the performance of the proposed algorithm with that of other methods shows that the proposed method can effectively describe human movement information and be useful for movement retrieval applications...|$|R
5000|$|In {{the first}} place, prophetic {{theology}} {{will have to}} be biblical: [...] "Our KAIROS impels us to return to the Bible, and to <b>search</b> <b>the</b> <b>Word</b> <b>of</b> God for a message that is relevant to what we are experiencing in South Africa today". It does not [...] "pretend to be comprehensive and complete;" [...] it is consciously devised for this situation, and therefore needs to take seriously the need to read the [...] "signs of the times" [...] (...) [...] It is always a call to action, a call for [...] "repentance, conversion and change". This will involve confrontation, taking a stand, and persecution. It is, nevertheless, fundamentally a [...] "message of hope." [...] It is spiritual: [...] "Infused with a spirit of fearlessness ... courage ... love ... understanding ... joy and hope".|$|R
40|$|Abstract. An {{approach}} with <b>the</b> capability <b>of</b> <b>searching</b> a <b>word</b> portion in document images is proposed in this paper, {{to facilitate the}} detection and location of the user-specified query words. A feature string is synthesized according to the character sequence in the user-specified word, and each word image extracted from documents are represented by a feature string. Then, an inexact string matching technology is utilized to measure the similarity between the two feature strings, based on which we can estimate how the document word image is relevant to the user-specified word and decide whether its portion {{is the same as}} the user-specified word. Experimental results on real document images show that it is a promising approach, which is capable of detecting and locating the document words that entirely match or partially match with the user-specified word. ...|$|R
40|$|Technological {{advances}} {{in the fields of}} medicine and allied sciences had given much needed momentum into the field of molecular biology and regenerative medicine. They indeed provided a boost to innovate new yields for both hard tissue and soft tissue regeneration in dentistry. One among them is the use of platelet concentrates (platelet rich plasma [PRP], platelet rich fibrin [PRF]). Autologous concentrate of blood platelets with a suspension of growth factors offers an enhanced healing of hard and soft tissues. It is an auxiliary benefit for an operator to be aware of platelet concentrates and its healing properties for delivering unsurpassed oral health care to patients. The current article outlines the principles, objectives and clinical insight to the regenerative potential of platelet concentrates in various fields <b>of</b> dentistry. <b>The</b> <b>search</b> <b>words</b> <b>of</b> <b>the</b> PubMed data base were PRF and other permutations of keywords such as "PRP dentistry", PRF dentistry, PRF regenerative dentistry...|$|R
40|$|In {{this paper}} we {{describe}} {{a method of}} effective handling of linguistic data by means of covering and inhibiting patterns [...] patterns that "compete" each other. A methodology of developing such patterns is outlined. Applications {{in the areas of}} morphology, hyphenation and part-of-speech tagging are shown. This pattern-driven approach to language engineering allows the combination of linguist expertise with the data learned from corpora [...] layering <b>of</b> knowledge. <b>Searching</b> for information in pattern database (dictionary problem) is blindingly fast [...] linear with respect to <b>the</b> length <b>of</b> <b>searching</b> <b>word</b> as with other finite-state approaches. 1 Introduction There is a need to store empirical language data in almost all areas on natural language engineering (LE). Finite-state methods [21, 13, 16, 17, 10] have found their revival in the last decade. The theory of finite-state automata (FSA) and transducers (FST) is a well developed part of theoretical computer science (for an overview, [...] ...|$|R
40|$|International audienceIn {{the actual}} {{interconnected}} world, {{the speed of}} broadcasting of information leads the formation of opinions towards more and more immediacy. Big social networks, by allowing distribution, and therefore broadcasting of information in a almost instantaneous way, also speed up the formation of opinions concerning actuality. Then, these networks are great observatories of opinions and e-reputation. In this e-reputation monitoring task, {{it is easy to}} get a set of information (web pages, blog pages, tweets, [...] .) containing a chosen word or a set <b>of</b> <b>words</b> (a company name, a domain of interest, [...] .), and then we can easily <b>search</b> for <b>the</b> most used words. But a harder, but more interesting task, is to track the set <b>of</b> jointly used <b>words</b> in this dataset, because this latter contains the more shared advice about <b>the</b> initial <b>searched</b> set <b>of</b> <b>words.</b> Precisely, <b>the</b> exhaustive discovering of the shared properties of a collection of objects is the main task of the Galois lattices used in the Formal Concept Analysis. In this...|$|R
40|$|This {{project is}} {{concerned}} with <b>the</b> development <b>of</b> a <b>search</b> algorithm for a large archival database. The Port Elizabeth Genealogical Information System (PEGIS) contains a database consisting of almost 600000 individuals. <b>The</b> standard <b>search</b> algorithms are no longer sufficient to locate individuals in the database. A new algorithm was required that allows <b>searches</b> on any <b>of</b> <b>the</b> <b>words</b> or dates in the database, {{as well as a}} means to specify where in the desired record a word should occur. A ranking function of retrieved records was also required. A literature study on the field of Information Retrieval and on algorithms designed specifically for the PEGIS was done. These algorithms were adapted and hybridized to yield a search algorithm that allows for the boolean formulation of queries and the specification <b>of</b> <b>the</b> structure <b>of</b> <b>search</b> <b>words</b> in <b>the</b> desired records. The algorithm ranks retrieved records in assumed relevance to the user. The new algorithms were evaluated with regards to retrieval speed and accuracy and were found to be very effective...|$|R
40|$|The {{purpose of}} this study was to {{investigate}} the effects of paper and e-dictionaries on Iranian intermediate learners' reading comprehension. To this end, 90 female English Foreign language learners were randomly selected and assigned into 2 experimental groups (e-dictionaries and paper dictionaries groups) and 1 control group. All the groups took a pretest using no dictionaries. After 2 weeks of treatment design for the experimental groups, all the 3 groups took part in the posttest. The experimental groups did their task with their relevant dictionaries, whereas the control group did their task without using any kind of dictionary. Data were analyzed through analysis of covariance (ANCOVA) and paired samples t test. Results showed that the participants’ reading comprehension improved from the pretest to the posttest in both experimental groups. Results also indicated that the learners in the e-dictionaries group outperformed those in the paper dictionaries group. The outcome of study reveals that e-dictionaries could improve students’ reading comprehension by motivating them, shortening <b>the</b> time <b>of</b> <b>searching</b> <b>words</b> and reading a text, and increasing the number of look ups...|$|R
40|$|The {{purpose of}} this thesis is to explore whether the drop in voter turnout between the 2009 and 2014 European Parliament elections, {{as well as the}} general apathy toward the {{politics}} of the European Union, in the Czech Republic was contributed to by the mass media. Specifically, this thesis will examine the two most read daily newspapers of both 2009 and 2014, Blesk and Mladá Fronta Dnes, and analyze its articles with <b>the</b> use <b>of</b> <b>search</b> <b>words</b> in order to assess whether any change occurred in the way the European Union, its policies and institutions, were presented immediately prior to the 2009 and 2014 European Parliament elections. This will be achieved through the use of content analysis, both quantitative and qualitative, while evaluating the articles in comparative fashion. The thesis will argue that, given the essential role of the media as intermediary between the world of politics and the electorate, the portrayal, or lack thereof, of the European Union is a factor in explaining the electoral dynamics of the country. Key Words: voter turnout; European Parliament elections; mass media; electoral apathy Powered by TCPDF (www. tcpdf. org...|$|R
40|$|The {{article was}} a review study. This is an {{approach}} {{which allows the}} researcher to understand the phenomenon in depth (Karamustafaoglu, 2009). The researchers have adopted the literature review approach to investigate the problem in depth. The structures of literature have been analyzed through the handbooks mostly {{in the field of}} teacher education on literacy terms. This was a review study about exploring barriers to literacy from parents and teachers’ perspective. This was linked to the current literature in understanding about the phenomenon which was investigated. This study, therefore, relied on different kinds of literature such as published articles, theses and, books. Majority of them were obtained from research databases such as Web of Science and Google Scholar. Namamba and Rao (2017), and Manzar-Abbas and Lu (2013) followed the same method of review in conducting their review studies. In order to obtain relevant literature, search words such as “barriers to literacy” and “parents and teachers’ perspective” were used. In addition, books on teacher education were also consulted. All the obtained literature was summarized according to <b>the</b> <b>search</b> <b>words</b> <b>of</b> <b>the</b> study. All the obtained literature was cited properly in text and in the list. This allowed the researchers to extract and incorporate key information about exploring barriers to literacy from parents and teachers’ perspective into this article. The studies drawn in this review article reflect the learning about barriers to literacy from parents and teachers’ perspective in diverse directions...|$|R
60|$|There is {{yet another}} thing {{that the spirit of}} Antichrist is {{immediately}} concerned in; and that is, the antichristian names of the men that worship the beast: the names, I mean, that the Antichrist hath baptized them into: for those names are breathed upon them by the very spirit of Antichrist; and are such as are absolutely names of blasphemy, or such as do closely border thereupon; some such as Elihu durst not for his life give unto men, only he calls them 'flattering titles' (Job 32:21,22). Now therefore, of the danger (though not of the names themselves) you read sufficiently in the scripture; and perhaps the Holy Ghost has contented himself with giving of items that are general, that men might, as to them, be the more cautious of what names they give one to another (Rev 17:5); but this is clear, they are worn by men of spiritual employ: but since they are but mentioned, and are not distinctly nominated, how should we know which are they, and which not? Verily, by <b>searching</b> <b>the</b> <b>word</b> <b>of</b> God, and by seeing by that what names we are allowed to give unto men, with reference to their offices, dignities, and places: for God has a quarrel with the names, {{as well as with the}} persons that wear them; and when his Son shall down with Antichrist, he will slay seven thousand names of men, as well as the persons of the worshippers of the beast (Rev 11:13).|$|R
40|$|Abstract. In {{the actual}} {{interconnected}} world, {{the speed of}} broadcasting of information leads the formation of opinions towards more and more immediacy. Big social networks, by allowing distribution, and therefore broadcasting of information in a almost instantaneous way, also speed up the formation of opinions concerning actuality. Then, these networks are great observatories of opinions and e-reputation. In this e-reputation monitoring task, {{it is easy to}} get a set of information (web pages, blog pages, tweets, [...] .) containing a chosen word or a set <b>of</b> <b>words</b> (a company name, a domain of interest, [...] .), and then we can easily <b>search</b> for <b>the</b> most used words. But a harder, but more interesting task, is to track the set <b>of</b> jointly used <b>words</b> in this dataset, because this latter contains the more shared advice about <b>the</b> initial <b>searched</b> set <b>of</b> <b>words.</b> Precisely, <b>the</b> exhaustive discovering of the shared properties of a collection of objects is the main task of the Galois lattices used in the Formal Concept Analysis. In this article we state clearly the characteristics, advantages and constraints of one of the more successful online social networks: Twitter. Then we detail the difficult task of tracking, on Twitter, the most forwarded information about a chosen subject. We also explain how the characteristics of Galois lattices permit to solve elegantly and efficiently this problem. But, retrieving the most used corpus <b>of</b> <b>words</b> is not enough, we have to show the results in an informative and readable manner, which is not easy when the result is a Galois Lattice. Then we propose a visualisation called topigraphic network of tags, which represent a tag cloud in a network of concepts with a topographic allegory, which permits to visualise the more important concepts found about a given search on Twitter. ...|$|R
6000|$|... "Lysbeth," [...] he said, [...] "I {{will tell}} {{to you what}} I would not tell to any other living creature, not being {{one of my own}} brotherhood, for whether you accept me or reject me, I know well that I am as safe in {{speaking}} to you as when upon my knees I speak to the God I serve. I am what you call a heretic. I am a member of that true faith to which I hope to draw you, but which if you do not wish it I should never press upon you. It is chiefly because I am what I am that for so long I have hung back from speaking to you, since I did not know whether it would be right--things being thus--to ask you to mix your lot with mine, or whether I ought to marry you, if you would marry me, keeping this secret from you. Only the other night I sought counsel of--well, never mind of whom--and we prayed together, and together <b>searched</b> <b>the</b> <b>Word</b> <b>of</b> God. And there, Lysbeth, by some wonderful mercy, I found my prayer answered and my doubts solved, for the great St. Paul had foreseen this case, as in that Book all cases are foreseen, and I read how the unbelieving wife may be sanctified by the husband, and the unbelieving husband by the wife. Then everything grew clear to me, and I determined to speak. And now, dear, I have spoken, and it is for you to answer." ...|$|R
30|$|The STD system {{consists}} of four different stages: in the first stage, a phone recognition is conducted to output phone lattices based on two different speech recognizers: (1) a standard triphone context-dependent hidden Markov model (HMM) speech recognizer with mixtures of diagonal covariance Gaussians as observation density functions in the states and (2) a biphone context-dependent HMM speech recognizer where the observation probabilities are obtained from a multilayer perceptron (MLP). In the second stage, a STD subsystem hypothesizes detections from each speech recognizer. The 1 -best output of each phonetic recognizer is used as source text for an edit distance search. In doing so, each putative detection could be any substring which has a phonetic edit distance with <b>the</b> <b>searched</b> <b>word</b> <b>of</b> less than 50 % of its length. Next, we take all the detections found from the different phonetic recognizers and merge them. For overlapped detections, the best detection (i.e., {{the one with the}} minimum edit distance) remains. In the third stage, two different confidence measures based on minimum edit distance and lattice information are used as confidence scores for each putative detection. The former is computed from standard substitution, insertion, and deletion errors in the 1 -best phone sequence given by each speech recognizer, and normalized by the length <b>of</b> the <b>word.</b> The latter is computed as follows: (1) we determined each lattice by using HLRescore from HTK [58] so that a smaller and more useful graph is used next; (2) we run the lattice-tool from the SRILM toolkit [49] to obtain the corresponding acoustic mesh graph; (3) the confidence calculated in the acoustic mesh graph is used in a modified edit distance algorithm where, instead of all costs equal to 1, we simply sum the confidence of the matching phones with <b>the</b> <b>searched</b> <b>word.</b> Then, <b>the</b> score <b>of</b> a putative detection is the sum of the confidences through the acoustic mesh <b>of</b> <b>the</b> <b>searched</b> <b>word</b> between <b>the</b> time limits where the detection resides. This score is also normalized by the length <b>of</b> the <b>word.</b> The fourth stage makes use of the Bosaris toolkiti to fuse both scores obtained in the previous stage to compute the final confidence for each detection. A full system description can be found in [59].|$|R
