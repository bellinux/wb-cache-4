10000|10000|Public
5|$|Assuming {{genetic drift}} {{is the only}} {{evolutionary}} force acting on an allele, {{at any given time}} <b>the</b> <b>probability</b> that an allele will eventually become fixed in the population is simply its frequency in the population at that time. For example, if the frequency p for allele A is 75% and the frequency q for allele B is 25%, then given unlimited time <b>the</b> <b>probability</b> A will ultimately become fixed in the population is 75% and <b>the</b> <b>probability</b> that B will become fixed is 25%.|$|E
5|$|The {{inner product}} between two state vectors {{is a complex}} number known as a {{probability}} amplitude. During an ideal measurement of a quantum mechanical system, <b>the</b> <b>probability</b> that a system collapses from a given initial state to a particular eigenstate is given by {{the square of the}} absolute value of <b>the</b> <b>probability</b> amplitudes between the initial and final states. The possible results of a measurement are the eigenvalues of the operator—which explains the choice of self-adjoint operators, for all the eigenvalues must be real. <b>The</b> <b>probability</b> distribution of an observable in a given state can be found by computing the spectral decomposition of the corresponding operator.|$|E
5|$|A {{number of}} scoring systems exist {{to help with}} diagnosis; however, their use is {{controversial}} due to insufficient accuracy. The modified Centor criteria are a set of five criteria; the total score indicates <b>the</b> <b>probability</b> of a streptococcal infection.|$|E
40|$|BNL's efforts {{focused on}} the {{following}} specific items; <b>the</b> <b>probabilities</b> of failure for perfect steam generator tubes, <b>the</b> <b>probabilities</b> of failure for steam generator tubes containing long axisymmetrically thinned sections, and <b>the</b> <b>probabilities</b> of failure for steam generator tubes containing finite length (relatively short) axisymmetric wastages. (auth...|$|R
5000|$|Finally <b>the</b> <b>probabilities</b> of <b>the</b> {{events may}} be {{identified}} with <b>the</b> <b>probabilities</b> of <b>the</b> condition which the outcomes in the event satisfy, ...|$|R
30|$|<b>The</b> joint <b>probability</b> P(A, B) {{of events}} A and B is P(A ∩ B)=P(B | A)×P(A), and <b>the</b> {{conditional}} <b>probability</b> is <b>the</b> normalized joint <b>probability.</b> <b>The</b> generative methodology aims at modeling <b>the</b> joint <b>probability</b> P(A, B) by predicting <b>the</b> conditional <b>probability.</b> On <b>the</b> other hand, in the discriminative methodology a function is learned for <b>the</b> conditional <b>probability.</b>|$|R
5|$|An {{adaptive}} trait is {{an aspect}} of the developmental pattern of the organism which enables or enhances <b>the</b> <b>probability</b> of that organism surviving and reproducing.|$|E
5|$|David, C. L. 1973: An {{objective}} of estimating <b>the</b> <b>probability</b> of severe thunderstorms. Preprint Eight conference of Severe Local Storms. Denver, Colorado, American Meteorological Society, 223–225.|$|E
5|$|Following a {{decompression}} schedule {{does not}} completely protect against DCS. The algorithms used {{are designed to}} reduce <b>the</b> <b>probability</b> of DCS to a very low level, but do not reduce it to zero.|$|E
5000|$|If <b>the</b> prior <b>probabilities</b> are all <b>the</b> same <b>the</b> <b>probabilities</b> are, ...|$|R
5000|$|The {{process can}} be {{analyzed}} using <b>the</b> method of <b>probability</b> generating function. Let p0, p1, p2, ... be <b>the</b> <b>probabilities</b> of producing 0, 1, 2, ... offspring by each individual in each generation. Let dm be <b>the</b> extinction <b>probability</b> by <b>the</b> mth generation. Obviously, d0= 0. Since <b>the</b> <b>probabilities</b> for all paths that lead to 0 by the m-th generation must be added up, <b>the</b> extinction <b>probability</b> is nondecreasing in generations. That is, ...|$|R
50|$|Many {{people would}} feel more unsure about taking Gamble A in which <b>the</b> <b>probabilities</b> are unknown, rather than Gamble B, in which <b>the</b> <b>probabilities</b> are easily {{seen to be}} one half for each outcome.|$|R
5|$|For comparison, Euclid's {{original}} subtraction-based algorithm {{can be much}} slower. A single integer {{division is}} equivalent to the quotient q number of subtractions. If the ratio of a and b is very large, the quotient is large and many subtractions will be required. On the other hand, {{it has been shown that}} the quotients are very likely to be small integers. <b>The</b> <b>probability</b> of a given quotient q is approximately ln|u/(u−1)| where u=(q+1)2. For illustration, <b>the</b> <b>probability</b> of a quotient of 1, 2, 3, or 4 is roughly 41.5%, 17.0%, 9.3%, and 5.9%, respectively. Since the operation of subtraction is faster than division, particularly for large numbers, the subtraction-based Euclid's algorithm is competitive with the division-based version. This is exploited in the binary version of Euclid's algorithm.|$|E
5|$|It is {{estimated}} that flows at the river mouth by 1995 had declined to only 27% of natural outflows. <b>The</b> <b>probability</b> of the bottom end of the Murray experiencing drought-like flows had increased from 5% under natural conditions to 60% by 1995.|$|E
5|$|Risk is a {{combination}} of hazard, vulnerability and likelihood of occurrence, which can be <b>the</b> <b>probability</b> of a specific undesirable consequence of a hazard, or the combined probability of undesirable consequences of all the hazards of an activity.|$|E
50|$|<b>The</b> <b>probabilities</b> of <b>the</b> various ranking {{combinations}} {{are described}} below. All these probabilities are described for 52-card teen patti, without the two Joker cards. In Joker versions, <b>the</b> <b>probabilities</b> change widely, most importantly for pairs.|$|R
40|$|This {{paper offers}} a {{procedure}} for specifying probabilities {{for students to}} select answers on a multiple-choice test that, unlike previous procedures, satisfies {{all three of the}} following structural consistency conditions: (1) for any student, the sum over questions of <b>the</b> <b>probabilities</b> that <b>the</b> student will use the correct answers is the student's score on the test; (2) for any student, the sum over possible answers of <b>the</b> <b>probabilities</b> of using <b>the</b> answers is 1. 0; and (3) for any answer to any question, the sum over students of <b>the</b> <b>probabilities</b> of using that answer is {{the number of students who}} used that answer. When applied to an exam, these fully consistent <b>probabilities</b> had <b>the</b> same power to identify cheaters as <b>the</b> <b>probabilities</b> proposed by Wesolowsky, and noticeably better power than <b>the</b> <b>probabilities</b> suggested by Frary et al. ...|$|R
3000|$|Equation (11) {{states that}} the {{reliability}} of the WSN is the summation of all <b>the</b> <b>probabilities</b> of <b>the</b> WSN states that have a structure function value of unity (i.e., <b>the</b> <b>probabilities</b> of all <b>the</b> paths of the WSN S). Depending on the set of failed components in the state π, <b>the</b> individual <b>probabilities</b> Prob(x [...]...|$|R
5|$|The {{birth of}} wolf pups {{so close to}} the state border raised <b>the</b> <b>probability</b> of a future {{long-term}} wolf population in California. In June 2014, the California Fish and Game Commission voted 3–1 to protect those wolves under the state Endangered Species Act.|$|E
25|$|<b>The</b> <b>probability</b> that A will occur, {{given that}} B has occurred, is <b>the</b> <b>probability</b> of A and B {{occurring}} divided by <b>the</b> <b>probability</b> ofB.|$|E
25|$|For events not independent, <b>the</b> <b>probability</b> {{of event}} B {{following}} event A (or event A causing B) is <b>the</b> <b>probability</b> of A multiplied by <b>the</b> <b>probability</b> that A and B both occur.|$|E
30|$|Entropy {{is large}} for {{the images that}} are texturally not uniform, when all <b>the</b> <b>probabilities</b> in <b>the</b> l(d,θ) vector are close to each other. <b>The</b> more <b>the</b> <b>probabilities</b> in vector are diversified, the smaller is the entropy.|$|R
50|$|Most {{analyses}} of <b>the</b> <b>probabilities</b> of either <b>the</b> coin method or yarrow-stalk method agree on <b>the</b> <b>probabilities</b> for each method. The coin method varies {{significantly from the}} yarrow-stalk method, in that the former gives <b>the</b> same <b>probability</b> to both <b>the</b> moving lines and to both the static lines, {{which is not the}} case in the yarrow-stalk method.|$|R
50|$|Informally, this is {{the largest}} {{possible}} difference between <b>the</b> <b>probabilities</b> that <b>the</b> two <b>probability</b> distributions can assign to the same event.|$|R
25|$|Continuous {{probability}} distributions can {{be described}} in several ways. <b>The</b> <b>probability</b> density function describes the infinitesimal probability of any given value, and <b>the</b> <b>probability</b> that the outcome lies in a given interval can be computed by integrating <b>the</b> <b>probability</b> density function over that interval. On the other hand, the cumulative distribution function describes <b>the</b> <b>probability</b> that the random variable is no larger than a given value; <b>the</b> <b>probability</b> that the outcome lies in a given interval can be computed by taking the difference between the values of the cumulative distribution function at the endpoints of the interval. The cumulative distribution function is the antiderivative of <b>the</b> <b>probability</b> density function provided that the latter function exists.|$|E
25|$|For {{independent}} events, <b>the</b> <b>probability</b> of {{the occurrence}} of all is <b>the</b> <b>probability</b> of each multiplied together.|$|E
25|$|The {{standard}} {{approach is to}} test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. <b>The</b> <b>probability</b> of type I error is therefore <b>the</b> <b>probability</b> that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and <b>the</b> <b>probability</b> of type II error is <b>the</b> <b>probability</b> that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is <b>the</b> <b>probability</b> that it correctly rejects the null hypothesis when the null hypothesis is false.|$|E
3000|$|... {{decide to}} {{transmit}} or not, {{based on the}} final combined sensing decision of the coalition head. Therefore, <b>the</b> <b>probabilities</b> of detection and false alarm of a coalition head are also <b>the</b> <b>probabilities</b> of detection and false alarm of each CR [...]...|$|R
5000|$|Berkson's paradox arises because <b>the</b> {{conditional}} <b>probability</b> of '''' given [...] {{within the}} three-cell subset equals <b>the</b> conditional <b>probability</b> in <b>the</b> overall population, but <b>the</b> unconditional <b>probability</b> within <b>the</b> subset is inflated relative to <b>the</b> unconditional <b>probability</b> in <b>the</b> overall population, hence, within the subset, {{the presence of}} [...] decreases <b>the</b> conditional <b>probability</b> of '''' (back to its overall unconditional probability): ...|$|R
50|$|The {{adaptive}} encoding uses <b>the</b> <b>probabilities</b> from <b>the</b> previous {{sample in}} sound encoding, {{from the left}} and upper pixel in image encoding, and additionally from the previous frame in video encoding. In the wavelet transformation, <b>the</b> <b>probabilities</b> are also passed through the hierarchy.|$|R
25|$|So, <b>the</b> <b>probability</b> of {{the entire}} sample space is 1, and <b>the</b> <b>probability</b> of the null event is 0.|$|E
25|$|For every node u that is {{selected}} to S, <b>the</b> <b>probability</b> that u {{will be removed}} from S is at most 1/2. PROOF: This probability is at most <b>the</b> <b>probability</b> that a higher-neighbour of u {{is selected}} to S. For each higher-neighbour v, <b>the</b> <b>probability</b> that it is selected is at most 1/2d(v), which is at most 1/2d(u) (since d(v)≤d(u)). By union bound, <b>the</b> <b>probability</b> that no higher-neighbour is selected is at most d(u)/2d(u) = 1/2.|$|E
25|$|Hence, <b>the</b> <b>probability</b> {{of failure}} is at most 2k (compare this with <b>the</b> <b>probability</b> of failure for the Miller-Rabin primality test, {{which is at}} most 4k).|$|E
50|$|Second, analysts {{must take}} <b>the</b> initial <b>probability</b> of each event into account. <b>The</b> <b>probabilities</b> of events {{must be taken}} in {{isolation}} from one another.|$|R
40|$|Generically <b>the</b> <b>probabilities</b> of {{observational}} results {{depend upon}} both the quantum {{state and the}} rules for extracting <b>the</b> <b>probabilities</b> from it. It is often argued that inflation may make our observations independent of the quantum state. In a framework in which one considers {{the state and the}} rules as logically separate, it is shown how it is possible that <b>the</b> <b>probabilities</b> are indeed independent of the state, but the rules for achieving this seem somewhat implausible. Alberta-Thy- 13 - 09, arXiv: 0907. 475...|$|R
5000|$|If all {{assumptions}} used in deriving {{a confidence}} interval are met, <b>the</b> nominal coverage <b>probability</b> will equal <b>the</b> coverage <b>probability</b> (termed [...] "true" [...] or [...] "actual" [...] coverage probability for emphasis). If any assumptions are not met, <b>the</b> actual coverage <b>probability</b> could either {{be less than}} or greater than <b>the</b> nominal coverage <b>probability.</b> When <b>the</b> actual coverage <b>probability</b> is greater than <b>the</b> nominal coverage <b>probability,</b> <b>the</b> interval is termed [...] "conservative", if it is less than <b>the</b> nominal coverage <b>probability,</b> <b>the</b> interval is termed [...] "anti-conservative", or [...] "permissive." ...|$|R
