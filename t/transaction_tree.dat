7|10|Public
50|$|The Dynamic {{two-phase}} commit (Dynamic two-phase commitment, D2PC) protocol is {{a variant}} of Tree 2PC with no predetermined coordinator. It subsumes several optimizations that have been proposed earlier. Agreement messages (Yes votes) start to propagate from all the leaves, each leaf when completing its tasks {{on behalf of the}} transaction (becoming ready). An intermediate (non leaf) node sends when ready an agreement message to the last (single) neighboring node from which agreement message has not yet been received. The coordinator is determined dynamically by racing agreement messages over the <b>transaction</b> <b>tree,</b> at the place where they collide. They collide either at a <b>transaction</b> <b>tree</b> node, to be the coordinator, or on a tree edge. In the latter case one of the two edge's nodes is elected as a coordinator (any node). D2PC is time optimal (among all the instances of a specific <b>transaction</b> <b>tree,</b> and any specific Tree 2PC protocol implementation; all instances have the same tree; each instance has a different node as coordinator): By choosing an optimal coordinator D2PC commits both the coordinator and each cohort in minimum possible time, allowing the earliest possible release of locked resources in each transaction participant (tree node).|$|E
40|$|In {{this paper}} we {{consider}} real-time concurrency {{control for the}} nested transaction model. We analyze problems that have pure optimistic and pessimistic approaches. As the solution we propose a hybrid concurrency control algorithm which acts as an optimistic for transactions from different transaction trees and as pessimistic inside a single <b>transaction</b> <b>tree...</b>|$|E
40|$|The nested {{transaction}} {{model was}} introduced {{to satisfy the requirements}} of advanced database applications. Moreover, it is currently the basic transaction model for new databases like workflow systems, mobile databases, and objectrelational databases. Though there are several performance evaluation studies of different concurrency control mechanisms in nested transactions, effects of transaction parameters on the overall system performance have not received any attention. In this paper, we study the effects of transactions characteristics on system performance. We developed a detailed simulation model and conducted several experiments to measure the impact of transactions characteristics on the performance. First, the effect of the number of leaves on the performance of nested transactions is investigated under different shaping parameters. Also, effects of the depth of the <b>transaction</b> <b>tree</b> on the system performance are investigated...|$|E
50|$|A <b>transaction</b> program {{scheduling}} <b>tree</b> {{allows the}} client to establish relative usage for groups of transaction programs. Concurrency limits avoid one type of work dominating the system {{to the exclusion of}} other work and avoid creating an over commitment of resources. Up to 4094 nodes may be created in the tree.|$|R
40|$|The {{instructions}} of high utility item sets {{is maintained}} in a tree-based data structure named utility pattern tree UPTree so that candidate item sets might be generated efficiently exclusively with two scans of database. Within this work time consuming on each database scan is exponentially increasing {{just like the}} size of the database increases. To beat this drawback, we are going to present a Two-Phase algorithm to efficiently prune through wide range of candidates and precisely obtain the complete range of high utility item sets. High Two phase utility mining algorithm is matched, intended for finding item sets that contribute high utility. Within the first phase of this very algorithm all utility items are collected and then in the other phase Filtering non-utility frequent candidates is likewise efficient because we only have to design a hash based P tree from candidates and push all <b>transactions</b> the <b>tree</b> to compute subsets. Consequently, both time and space complexity are both viewed as fully determined when using the complexity of a given frequent itemsets mining method used...|$|R
40|$|We hedge European and Barrier {{options in}} a {{discrete}} time and discrete space setting by uwing stochastic optimization {{to minimize the}} mean downside hedge error under <b>transaction</b> costs. Scenario <b>trees</b> are generated using a method which ensures the absence of arbitrage and which matches the mean and variance of the underlying asset price in the sampled scenarios to those of a given distribution. The stochastic optimization based strategy is benchmarked to the method of delta hedging for the case where the underlying asset price following a discretized geometric Brownian motion and implemented for the case where the underlying asset prices is driven by a discretized Variance Gamms proces. ...|$|R
40|$|In this paper, {{we present}} a {{mechanism}} to specify and to validate consistency constraints in object oriented databases. Constraints are specified using pre and post-conditions associated with an exception handling mechanism. During transaction run-time, we treat exceptions corresponding to errors (in this case, we use immediate exceptions which are processed immediately) or presumption of errors (in this case, we use deferred exceptions which are processed {{at the end of}} the transaction), in order to insure validation. To refine our mechanism, we enlarge it to nested transactions. Deferred exceptions can be processed at each node of the <b>transaction</b> <b>tree.</b> Using a predefined exception, we propagate an abort of a sub-transaction to its parent transaction. Thus, the parent transaction can choose among different policies for processing the sub-transaction abort. 1 Introduction For the programming point of view, writing applications in an object-oriented databases framework is still a delicat [...] ...|$|E
40|$|In this paper, we formalize {{and prove}} the {{correctness}} of a concurrency control algorithm {{for an open}} and safe nested transaction using I/O automaton model. The model uses {{the notion of a}} recovery point subtransaction in the nested <b>transaction</b> <b>tree.</b> Our nested transaction model uses a prewrite operation before an actual write operation to increase the concurrency. It is termed “open and safe ” as prewrites allow early reads (before database writes on disk) without cascading aborts. In our model we have also modeled the buffer management operations as nested transactions, and the concurrency control algorithm controls their executions. Non-access subtransactions, objects and the scheduler are modeled as I/O automata with the help of some pre and post conditions. These conditions capture the operational semantics and behavior of each automaton during the execution of transactions. While modeling we also take into account log activities, which occur during the execution of transactions. We also briefly sketch the recovery algorithm. The correctness proof shows that the concurrency control algorithm for our model is serially correct. Our proof makes use of assertional reasoning and provides many interesting invariant, thus gives a better understanding of our transaction mode...|$|E
40|$|Scientific studies, new {{findings}} and applications exist {{only when they}} arepublished. Unpublished all works are not useful. Preparation of any scientificpaper could be as hard and complicated as its related topic. Period fromcollecting data about paper to sending it to proper journal contains many difficultand complex stages. In this period, paper researcher may be disorganised andtaken care of different subjects which {{are not related to}} paper. “When we muststop collecting sources”, “What we must do if results of the analysis are notsatisfied”, “Which titles must be taken care of during writing the paper” likequestions make researchers confused. “Step by step” type behaviour would beuseful to protect researchers from this mess during preparation of the paper. So,a shortcut to help researchers can be demonstrated. Writing a paper is like alabyrinth. A researcher can be lost easily if that person doesn’t have any mapwhich shows a general view of the labyrinth. This process is examined with TheTheory of Constraints, because a lot of constraints exists in the process ofpreparing a paper. In the Theory of Constraints, Future Reality Tree (FRT) isused to structure a paper writing process map. Then, this map is transformed astep by step process with <b>Transaction</b> <b>Tree</b> (TT) ...|$|E
40|$|Abstract: A {{generalized}} correlated {{random walk}} {{is a process}} of partial sums Xk = kP j= 1 Yj such that (X;Y) forms a Markov chain. For a sequence (Xn) of such processes where each Y nj takes only two values, we prove weak convergence to a di®usion process whose generator is explicitly described in terms of the limiting behaviour of the transition probabilities for the Y n. Applications include asymptotics for option replication under transaction costs and an approximation for a given di®usion by regular recombining binomial trees. Key words: correlated random walk, di®usion limit, weak convergence, mathematical ¯-nance, large investor, <b>transaction</b> costs, binomial <b>tree...</b>|$|R
40|$|Abstract- Advanced {{database}} application areas, such {{as computer}} aided design, office automation, digital libraries, data-mining, hypertext and multimedia systems, need to handle complex data structures with set-valued attributes. These information systems contain implicit data {{that will be}} necessary to extract and exploit, by using data mining techniques. To exploit the data from these systems, the choice of appropriate storage structures becomes essential. In this paper, we propose a new compact structure to represent a transactions database, called a signatures tree, to speed up the signature file scanning. The construction of this tree requires only one single access to the <b>transactions</b> database. This <b>tree</b> will be used later to compute maximum support, extract frequent itemsets and generate association rules...|$|R
40|$|For many retail organisations, data {{collection}} is a routine activity {{and there is}} widespread recognition {{of the potential for}} analysis of past transaction data {{to improve the quality of}} business decisions. The use of several data mining tools will be explored and their outputs compared and contrasted. Experiments have been designed and carried out, including a detailed analysis of results. Decision trees will be found to produce overly complex rules with too much emphasis on negative items. Contrary to Bay and Pazzani's assertion, association rules can be used to find differences between groups. Keywords: data mining, <b>transaction</b> data, decision <b>trees,</b> association rules, contrast sets ii Table of contents Abstract [...] . i Table of contents [...] ii List of tables [...] iv List of figures [...] . v Abbreviations [...] vi Acknowledgements [...] vii Chapter 1...|$|R
40|$|Abstract. Agent {{technology}} is a promising solution for coping with the ever-growing complexity of modern information systems involving highly distributed and error-prone information sources. Unfortunately, current prototypes often lack the robustness necessary for real-world deployment. We propose transaction mechanisms as a possible remedy. For this, we divide Multiagent Systems into two layers: (1) a planning layer, whose actions are idempotent, do not cause any real-world side-effects and are therefore easily recoverable, and (2) an execution layer, {{which is responsible for}} all real-world actions and runs under transactional protection to achieve robustness. The execution layer consists of special Execution Agents, one for each agent in the planning layer. A Planning Agent develops its local plan and transfers it to its Execution Agent {{in the form of a}} <b>transaction</b> <b>tree</b> that models the execution dependencies between agent actions. The Execution Agent executes this tree and provides recovery mechanisms to deal with possible failures according to a predefined contingency behavior within the agent plan. In this paper, we focus on the social aspects of agent planning and illustrate how to map cooperatively developed shared plans with their underlying cooperation patterns to a set of coordinated transaction trees to be executed by the Execution Agents. 1...|$|E
40|$|In {{this paper}} we propose an {{indexing}} structure for bitemporal databases. Such structure is based on two trees, one indexing valid time and another indexing <b>transaction</b> time. The <b>trees</b> share pointers to the actual data records, which are thus not duplicated. Bitemporal queries are processed by dividing the query in two parts, a valid time part and a transaction time part. Each tree is searched according to these partial queries, {{and the answer is}} determined by the correct composition of the partial answers. We show how simple coordination of the tree searching, along with a simple assumption on the temporal data, improves query processing performance. The proposed structure also allows querying either time dimension, separately from the other one. 1 Introduction Temporal databases (TDBs) have been the object of quite some research regarding many of its aspects [17] and much has been published in the field [8, 10, 16, 18]. It has been recognized that two dimensions of time need to be su [...] ...|$|R
40|$|For most manufacturers, {{success or}} failure is {{determined}} by how effectively and efficiently their products are sold through their marketing channel members, so the management of marketing channels {{plays an important role}} in market competition. Most existing work studies the problem of marketing channel management in a qualitative way. Recently, with the increase of amount of sales data, how to enhance the marketing channel quantitatively is significant. As the marketing channel can be viewed as a tree, in this paper, a new marketing channel management strategy based on frequent subtree mining is proposed. The proposed method is illustrated under the real-world sales data in ERDOS group. Firstly, the <b>tree</b> <b>transaction</b> is formed monthly. For each monthly transaction, only those channel members that pass the basic sales plan will be included. Secondly, we use the TreeMiner algorithm to discover embedded frequent subtrees. Finally, different management strategies are used for different kinds of discovered patterns. We show that our method can correspond to the seven decision areas in traditional marketing channel management...|$|R
40|$|In recent years, It {{has been}} shown that {{outsourcing}} the most expensive task in data mining process to a service provider (e. g., cloud) brings several benefits to the data owner such as cost relief and less commitment to storage and computational resources. This introduces Data mining as Service paradigm. An Outsourcing data to the server facing a critical problem of verification, whether the server returned true and complete computation result to the client. The correctness integrity of mining results may be corrupted if the service provider is with random fault or malicious. In the case of frequent item set mining Apriori, Eclat, FP-growth algorithms are used for a vast database which contains a large number of <b>transactions.</b> Merkel hash <b>tree,</b> Bilinear pairing, Artificial Item set Planting (AIP) techniques are used for result verification of outsourced computation. The work introduces a new system for result verification of outsourced frequent item set mining computation. In proposed system tree comparison is the main component used for the verification. Client formed tree from received result of outsourced computation by calculating support value for the frequent item set, and another tree is created for the small part of the outsourced computation. By comparing these two trees verification of the outsourced computation is done...|$|R
40|$|Data mining is an Artificial Intelligence (AI) {{powered tool}} that can {{discover}} useful information within a database. Efficient algorithms to find frequent patterns {{are important in}} data mining research. Frequent pattern mining required in many business applications such as market analysis, production control, Science exploration and group decision support systems etc. Several effective data structures, such as two-dimensional arrays, trees and tries have been proposed to collect candidate and frequent item-sets. It seems as the tree structure is most extractive for storing item-sets. The outstanding tree has been proposed so far is called FP-tree (Frequent Pattern) which is a prefix tree structure. Some advancement with the tree structure is called the CATS <b>tree</b> (Compressed Arranged <b>Transaction</b> Sequences). CATS <b>Tree</b> extends the idea of FP-Tree to improve storage compression and allow frequent pattern mining without generation of candidate item-sets. Ant Colony Optimization (ACO) is the emerging field of artificial intelligence. ACO has shown a tremendous performance to solve many real time problems like Travelling Salesman Problem, Packet routing in Network and so on. Here in this paper {{the representation of the}} database is shown as a Graph to solve the frequent pattern mining problem, which is a prime requirement to solve any problem by ACO...|$|R

