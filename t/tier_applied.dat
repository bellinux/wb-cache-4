0|30|Public
30|$|The herbal {{medicinal}} product Bronchipret® TP film-coated tablets contains a fixed combination of thyme and primula dry extracts (BRO) and has long and successfully {{been used for}} the treatment of acute bronchitis. However, the underlying pharmacological mechanisms of action have not been determined so far. We report a <b>tiered</b> approach <b>applying</b> in vivo and in vitro studies to investigate the pharmacodynamic activity and underlying mechanisms of action, and to identify possible active ingredients contributing to the product’s pharmacodynamic activity.|$|R
5000|$|In Series 2, Robert Billington accidentally {{dropped his}} <b>tiered</b> showstopper while <b>applying</b> {{finishing}} touches to the cake. Both judges and presenters {{came to his}} aid to salvage the bottom tier of his cake. He was able to present the incomplete showstopper as a single-tiered cake.|$|R
40|$|CERN {{controls}} {{software is}} often developed on Java foundation. Some systems {{carry out a}} combination of data, network and processor intensive tasks within strict time limits. Hence, there is a demand for high performing, quasi real time solutions. Extensive prototyping of the new CERN monitoring and alarm software required us to address such expectations. The system must handle dozens of thousands of data samples every second, along its three <b>tiers,</b> <b>applying</b> complex computations throughout. To accomplish the goal, a deep understanding of multithreading, memory management and interprocess communication was required. There are unexpected traps hidden behind an excessive use of 64 bit memory or severe impact on the processing flow of modern garbage collectors. Tuning JVM configuration significantly affects {{the execution of the}} code. Even more important is the amount of threads and the data structures used between them. Accurately dividing work into independent tasks might boost system performance. Thorough profiling with dedicated tools helped understand the bottlenecks and choose algorithmically optimal solutions. Different virtual machines were tested, in a variety of setups and garbage collection options. The overall work provided for discovering actual hard limits of the whole setup. We present this process of architecting a challenging system in view of the characteristics and limitations of the contemporary Java runtime environment...|$|R
3000|$|... [...]. Specifically, at each {{decision}} {{stage for}} each application in each interval, one resource {{of the appropriate}} flavour is allocated to the bottleneck <b>tier.</b> We <b>apply</b> the utilization law [13] to identify the bottleneck tier. WAM is then invoked to compute Eq. (6) and hence Eq. (7). This process is then repeated for all applications. The resource allocation plan which gives the least SV value is selected and used to obtain the resource allocation plan in the next stage. As shown in Table 2, the number of performance model invocations required by RAP-IE is reduced to A*T.|$|R
40|$|The {{effluent}} {{guidelines and}} standards for the feedlot category were first published in 1974 (1). The feedlot category includes beef, dairy, swine, poultry, horses, and sheep. The current regulations {{are based on a}} three <b>tier</b> structure and <b>apply</b> to all operations with ≥ 1, 000 animal units (AUs) and some with ≥ 300 AUs, if the...|$|R
50|$|This {{period was}} {{followed}} by a dark era in which Benfica was internally overshadowed by Ovarense, Portugal Telecom and FC Porto. Even though this period {{is considered to be a}} dark one, Benfica did manage to reach the LPB final once as well as winning Super Cups and finishing runners-up in the national cup and in the league cup. Also noteworthy is an away win against Real Madrid, in the 1996-97 EuroCup, the same season the Spaniards won the competition. The team finally decided to withdraw from the top <b>tier</b> and <b>applied</b> for the second league, the Proliga, which was the highest division run by the Federação Portuguesa de Basquetebol. However the first division was folded and the LPB was again being overviewed by the national federation. This allowed the team to make a comeback to the main league.|$|R
40|$|This article {{analyzes}} {{the impact of}} multilateral market access liberalization of the Doha Round agricultural negotiations. At the center are the effects of variations in the magnitudes of tariff cuts, different tariff cutting formulas, the tariff capping as well as different numbers and width of tariff bands. The simulations are conducted with an extended version of the GTAP model and the GTAP database (6. 0) including bound and applied rates and a module to cut tariffs at the 6 -digit tariff line level. The results reveal that the heights of the tariff cuts {{and the kind of}} <b>tiered</b> formula <b>applied</b> are most important for the outcome of the Doha Round, while the width of tariff bands and the tariff capping only have a moderate influence on the countries' trade pattern and welfare. In contrast, the number of tariff bands is not important for the results. ...|$|R
50|$|As of {{the start}} of the 2009 season clubs wishing to {{participate}} in the Super League competition have to gain a licence granted by the Rugby Football League removing promotion to and relegation from the top <b>tier.</b> Featherstone did <b>apply</b> for the first round of licences though with the popular view this would not be successful and {{could be used as a}} learning experience for future bids. In July 2008 the RFL made its decision selection all current Super League teams plus two from the National Leagues, Salford and Celtic Crusaders.|$|R
40|$|This {{paper is}} devoted to {{statistical}} analysis of spatial competition and cooperation between European airports. We propose a new multi-tier modification of spatial models, which allow estimating of spatial influence varying with the distance. Competition and cooperation effects don't diminish steadily with moving from a given airport, their structure is more complex. The suggested model {{is based on a}} set of distance tiers, with different possible effects inside each <b>tier.</b> We <b>apply</b> the proposed modification to the standard spatial stochastic frontier model and use it to estimation of competition and cooperation effects for European airport and airport's efficiency levels. We identify three tiers of spatial influence with different completion-cooperation ratio in each one. In the first, closest to an airport, tier we note significant advantage of cooperation effects over competition ones. In the second, more distant, tier we discover the opposite situation – significant advantage of completion effects. The last tier's airports doesn't influence significantly. In this paper we also consider some other possible applications of the proposed spatial multi-tier model. ...|$|R
40|$|A two modules Decision Support System (DSS-ERAMANIA) was {{developed}} {{in order to support}} the site-specific Ecological Risk Assessment (ERA) for contaminated sites. Within the first module, the TRIAD and the Weight of Evidence approaches were used to develop a site-specific Ecological Risk Assessment framework including three tires of investigation. Selected ecological observations and ecotoxicological tests were compared according to Multi Criteria Decision Analysis (MCDA) methods and expert judgment, and the obtained ranking was used to identify a suitable set of tests, at each investigation <b>tier,</b> to be <b>applied</b> to the examined case study. A simplified application of the proposed methodology, implemented in the Module 1 of the DSS-ERAMANIA, is described and discussed...|$|R
40|$|Al-mostansaria {{university}} college of sciences Multi-tier web server systems {{are used in}} many importantcontexts and their security is {{a major cause of}} concern. Such systems can exploit strategies. In this paper, a model was present based onthree-tier architecture (Client tier, Server tier and Database <b>tier)</b> and <b>applying</b> multilevel security on it. The database server tier consists of the DBMS or the database management system and the database and we built it off-line to reduce unauthorized access to sensitive data. The Client tier, which is usually a web browser, processes and displays HTML resources, issues HTML requests and processes the responses. These web browsers are HTTP clients that interact with the Web servers using standard protocols. The Middle or application server tier consists most of the application logic. Inputs receives from the clients and interacts with the database but only the results sent to application server then to client. This achieved by using multilevel of security to protect database, using Authorization, Password Encryption. The process of authorization done by allowing the access to proposed system pages depending on authorized level; Password encrypted using bcrypt with fallbacks on sha- 256 / 512 with key stretching to protect it from cracking by any types of attack. Client-to-Application Server Protocol (CAP) uses the RC 4 A algorithm to provide data confidentiality to secure transmitted information from application server to client...|$|R
30|$|For a plant risk assessment, the {{predicted}} environmental concentrations (PEC) in soil calculated {{according to the}} CVMP guideline [2] is compared to {{the predicted}} no effect concentration (PNEC) obtained from the seedling emergence and seedling growth test according to OECD 208 after applying an assessment factor (AF) of 100. In tier B, the NOEC or EC 10 is multiplied with an AF of 10. If a risk for plants is identified, the EMA reflection paper [4] recommends conducting a higher <b>tier</b> assessment by <b>applying</b> a statistical extrapolation technique, the so-called species sensitivity distribution (SSD). The extended plant test would add the option to conduct a modified test in case a risk for plants has been identified and the substance {{has been shown to}} form non-extractable residues in manure or transformation products ≥ 10  % of the applied amount.|$|R
40|$|Most coreference {{resolution}} models determine if two mentions are coreferent using a single function over {{a set of}} constraints or features. This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones. To overcome this problem, we propose a simple coreference architecture based on a sieve that <b>applies</b> <b>tiers</b> of deterministic coreference models {{one at a time}} from highest to lowest precision. Each tier builds on the previous tier’s entity cluster output. Further, our model propagates global information by sharing attributes (e. g., gender and number) across mentions in the same cluster. This cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time. The framework is highly modular: new coreference modules can be plugged in without any change to the other modules. In spite of its simplicity, our approach outperforms many state-of-the-art supervised and unsupervised models on several standard corpora. This suggests that sievebased approaches could be applied to other NLP tasks. ...|$|R
40|$|In the {{presented}} work two {{case studies}} using highly reactive Isothiazol- 3 -one biocides and Ionic liquids are presented. It was {{the aim of}} these studies to get a deeper knowledge on the hazard potential of the selected substances by applying a mode of action based testing strategy and a flexible toxicological and ecotoxicological test battery. Summarising the results obtained from both case studies, it could be shown that the <b>applied</b> <b>tiered</b> and mode of action based testing strategy in combination with a flexible test battery was a valuable tool to identify and screen modes of toxic action as well as structure-activity relationships of reactive and non-reactive chemicals. The identified hazard potentials of isothiazol- 3 -one biocides and ionic liquids reduced the uncertainty of their toxicological and ecotoxicological impacts and thus the obtained results {{can lead to a}} refined multidimensional risk assessment of these substances. Additionally, the uncovered structure-activity relationships can be used to design more inherently safer and hence sustainable biocides and ionic liquids. The used T-SAR based approach could be identified as a promising candidate for the above cited 2 ̆ 2 intelligent testing strategies 2 ̆ 2 in the hazard assessment of environmentally relevant chemicals...|$|R
40|$|International audienceDesign {{of power}} {{delivery}} network (PDN) is a constrained optimization problem. An ideal PDN must limit voltage drop {{that results from}} switching circuits' transients, satisfy current density constraints that arise from electromigration limits, yet use only minimal metal resources so that design density targets can be met. It should also provide an efficient thermal conduit to address heat flux. Furthermore, an ideal PDN should be a regular structure to facilitate design productivity and manufacturability, yet be resilient to address varying power demands across its distribution area. In 3 -D ICs, these problems are further constrained {{by the need to}} minimize through-silicon via (TSV) area and bridge power lines of different dimensions across tiers, while addressing varying power demands in lateral and vertical directions. In this paper, we propose an unconventional power grid optimization solution that allows us to resize each <b>tier</b> individually by <b>applying</b> tier-specific constraints and yet be optimal in a multitier network, where each tier is locally resized while globally constrained. Tier-specific constraints are derived from electrical and thermal targets of 3 -D PDNs. Two resizing algorithms are presented that optimize 3 -D PDNs standalone or 3 -D PDNs together with TSVs. We demonstrate these solutions on a three-tier setup where significant area savings can be achieved...|$|R
40|$|Poisson Point Process (PPP) {{has been}} widely adopted as an {{efficient}} model for the spatial distribution of base stations (BSs) in cellular networks. However, real BSs deployment are rarely completely random, due to environmental impact on actual site planning. Particularly, for multi-tier heterogeneous cellular networks, operators have to place different BSs according to local coverage and capacity requirement, and the diversity of BSs' functions may result in different spatial patterns on each networking tier. In this paper, we consider a two-tier scenario that consists of macrocell and microcell BSs in cellular networks. By analyzing these two <b>tiers</b> separately and <b>applying</b> both classical statistics and network performance as evaluation metrics, we obtain accurate spatial model of BSs deployment for each tier. Basically, we verify the inaccuracy of using PPP in BS locations modeling for either macrocells or microcells. Specifically, we find that the first tier with macrocell BSs is dispersed and can be precisely modelled by Strauss point process, while Matern cluster process captures the second tier's aggregation nature very well. These statistical models coincide with the inherent properties of macrocell and microcell BSs respectively, thus providing a new perspective in understanding the relationship between spatial structure and operational functions of BSs...|$|R
40|$|Purpose: To {{describe}} and validate bespoke software designed to extract morphometric data from ciliary muscle Visante Anterior Segment Optical Coherence Tomography (AS-OCT) images. Method: Initially, {{to ensure the}} software was capable of appropriately <b>applying</b> <b>tiered</b> refractive index corrections and accurately measuring orthogonal and oblique parameters, 5 sets of custom-made rigid gas-permeable lenses aligned to simulate the sclera and ciliary muscle were imaged by the Visante AS-OCT and were analysed by the software. Human temporal ciliary muscle data from 50 participants extracted via the internal Visante AS-OCT caliper method and the software were compared. The repeatability of the software was also investigated by imaging the temporal ciliary muscle of 10 participants on 2 occasions. Results: The mean difference between the software and the absolute thickness measurements of the rigid gas-permeable lenses were not statistically significantly different from 0 (t = - 1. 458, p = 0. 151). Good correspondence was observed between human ciliary muscle measurements obtained by the software and the internal Visante AS-OCT calipers (maximum thickness t = - 0. 864, p = 0. 392, total length t = 0. 860, p = 0. 394). The software extracted highly repeatable ciliary muscle measurements (variability ≤ 6 % of mean value). Conclusion: The bespoke software is capable of extracting accurate and repeatable ciliary muscle measurements and is suitable for analysing large data sets...|$|R
40|$|Derivatives pricing {{models have}} been widely applied in the {{financial}} industry for building software systems for pricing derivative instruments. However, {{most of the research}} work on financial derivatives is concentrated on computational models and formulas. There is little guidance for quantitative developers on how to apply these models successfully in order to build robust, efficient and extensible software applications. The present paper proposes an innovative design of a web-based application for real-time financial derivatives pricing, which is entirely based on design patterns, both generic and web-based application specific. Presentation tier, business tier and integration <b>tier</b> patterns are <b>applied.</b> Financial derivatives, underlying instruments and portfolios are modelled. Some of the principal models for evaluating derivatives (Black-Scholes, binomial trees, Monte Carlo simulation) are incorporated. Arbitrage opportunities and portfolio rebalancing requirements are detected in real time {{with the help of a}} notification mechanism. The novelty in this paper is that the latest trends in software engineering, such as the development of web-based applications, the adoption of multi-tiered architectures and the use of design patterns, are combined with financial engineering concepts to produce design elements for software applications for derivatives pricing. Although our design best applies to the popular J 2 EE technology, its flexibility allows many of the principles presented to be adopted by web-based applications implemented with alternative technologies...|$|R
50|$|NOx control {{requirements}} apply worldwide to any installed {{marine diesel}} engine over 130 kW of output power {{other than the}} engines used solely for emergency purposes not in respect of the marine vessel’s tonnage where the engine is installed. However, there are different levels of regulations {{that are based on}} the ship’s date of construction. Those levels are broken down into 3 <b>Tiers.</b> Tier I <b>applies</b> to the ships built after January 1st of 2000. It states that for engines below 130 rpm must have the total weighted cycle emission limit (g/kWh) of 17, engines that are between 130 and 1999 rpm must have no more than 12.1 (g/kWh), engines above 2000 rpm must have the limit of 9.8 (g/kWh). Tier II has the following requirements: 14.4 (g/kWh) for engines less than 13 rpm, 9.7 (g/kWh) for engines 130 - 1999 rpm, and for engines over 2000 rpm 7.7 (g/kWh) is the limit. Tier II limits apply to the ships constructed after January 1st of 2011.Tier III controls only apply in the specific areas where the NOx emission are more seriously controlled (NECAs) and apply to the ships constructed after January 1st of 2016. For engines under 130 rpm the limit is 3.4 (g/Kwh), engines between 130-1999 rpm the limit us 2.4 (g/kWh), engines above 2000 rpm must have the total weighted cycle emission limit of 2.0 (g/kWh).|$|R
30|$|Bioassays play {{a central}} role in effect-directed {{analysis}} (EDA), and their selection and application have to consider rather specific aspects of this approach. Meanwhile, bioassays with zebrafish, an established model organism in different research areas, are increasingly being utilized in EDA. Aiming to contribute for the optimal application of zebrafish bioassays in EDA, this review provides a critical overview of previous EDA investigations that applied zebrafish bioassays, discusses the potential contribution of such methods for EDA and proposes strategies to improve future studies. Over the last 10  years, zebrafish bioassays have guided EDA of natural products and environmental samples. The great majority of studies performed bioassays with embryos and early larvae, which allowed small-scale and low-volume experimental setups, minimized sample use and reduced workload. Biotesting strategies applied zebrafish bioassays as either the only method guiding EDA or instead integrated into multiple bioassay approaches. Furthermore, <b>tiered</b> biotesting <b>applied</b> zebrafish methods in both screening phase as well as for further investigations. For dosing, most of the studies performed solvent exchange of extracts and fractions to dimethyl sulfoxide (DMSO) as carrier. However, high DMSO concentrations were required for the testing of complex matrix extracts, indicating that future studies might benefit from the evaluation of alternative carrier solvents or passive dosing. Surprisingly, only a few studies reported the evaluation of process blanks, indicating a need to improve and standardize methods for blank preparation and biotesting. Regarding evaluated endpoints, while acute toxicity brought limited information, the assessment of specific endpoints was of strong value for bioactivity identification. Therefore, the bioassay specificity and sensitivity to identify the investigated bioactivity are important criteria in EDA. Additionally, it might be necessary to characterize the most adequate exposure windows and assessment setups for bioactivity identification. Finally, a great advantage of zebrafish bioassays in EDA of environmental samples is the availability of mechanism- and endpoint-specific methods for the identification of important classes of contaminants. The evaluation of mechanism-specific endpoints in EDA is considered to be a promising strategy to facilitate the integration of EDA into weight-of-evidence approaches, ultimately contributing for the identification of environmental contaminants causing bioassay and ecological effects.|$|R
40|$|This {{doctoral}} dissertation takes a buy side perspective to third-party logistics (3 PL) providers’ service <b>tiering</b> by <b>applying</b> a linear serial dyadic view to transactions. It takes its {{point of departure}} {{not only from the}} unalterable focus on the dyad levels as units of analysis and how to manage them, but also the characteristics both creating and determining purposeful conditions for a longer duration. A conceptual framework is proposed and evaluated on its ability to capture logistics service buyers’ perceptions of service tiering. The problem discussed is in the theoretical context of logistics and reflects value appropriation, power dependencies, visibility in linear serial dyads, a movement towards the more market governed modes of transactions (i. e. service tiering) and buyers’ risk perception of broader utilisation of the logistics services market. Service tiering, in a supply chain setting, with the lack of multilateral agreements between supply chain members, is new. The deductive research approach applied, in which theoretically based propositions are empirically tested with quantitative and qualitative data, provides new insight into (contractual) transactions in 3 PL. The study findings imply that the understanding of power dependencies and supply chain dynamics in a 3 PL context is still in its infancy. The issues found include separation of service responsibilities, supply chain visibility, price-making behaviour and supply chain strategies under changing circumstances or influence of non-immediate supply chain actors. Understanding (or failing to understand) these issues may mean remarkable implications for the industry. Thus, the contingencies may trigger more open-book policies, larger liability scope of 3 PL service providers or insourcing of critical logistics activities from the first-tier buyer core business and customer service perspectives. In addition, a sufficient understanding of the issues surrounding service tiering enables proactive responses to devise appropriate supply chain strategies. The author concludes that qualitative research designs, facilitating data collection on multiple supply chain actors, may capture and increase understanding of the impact of broader supply chain strategies. This would enable pattern-matching through an examination of two or more sides of exchange transactions to measure relational symmetries across linear serial dyads. Indeed, the performance of the firm depends not only on how efficiently it cooperates with its partners, but also on how well exchange partners cooperate with an organisation’s own business...|$|R
40|$|Modern {{language}} documentation {{depends on}} suitable software infrastructure. ELAN {{is a well-known}} tool developed at The Language Archive / MPI-PL which allows multi-tier, multi-speaker, time-linked annotation of audio and video recordings, in particular in a field work and language documentation setting. In {{the past two years}} ELAN has been under constant development. Here we will give an overview of the major recent enhancements to ELAN and ongoing work. These changes combined provide for a better and much faster process for the field linguist. Below we address five aspects, each consisting of multiple new features. We will discuss briefly their impact on typical workflows. First, there are modes that help you perform specialized tasks more efficiently. These are a) the segmentation mode, b) the transcription mode, and c) the interlinearization mode. With a focused user interface for each task, the segmentation and the transcription modes together provide very efficient means for the initial steps of a typical workflow. The interlinearization mode, which is still in an early phase of development, is optimized for the next steps of (morphological) parsing, glossing and tagging. It does so by providing an interface to a new program: Lexan. Lexan is an extensible system for "annotyzers" (annotation-suggestion modules). These can be used to perform many complex and simple tasks: from tier copying via word segmentation and interlinearization to machine learning. Second, the interoperability with FLEx (FieldWork Language Explorer) has been improved. An export function for the FLEx file format now complements the, updated, import function. Third, extensive support for performing operations on multiple files have been added. These include a) file-format conversion (including Toolbox and Praat), and b) creation of similarly structured EAF files for a selection of media files. Fourth, facilities have been added to create new tiers with annotations on the basis of existing <b>tiers</b> while <b>applying</b> logical operations. E. g. if the annotation occurs in both tier A and tier B, then copy it combined to tier C. The concept of creating new tiers on the basis of existing ones is currently further explored in Lexan (mentioned above). However these features provide for a straightforward interface to basic, but extremely helpful operations. Fifth, preliminary interaction with relevant web services (online audio-video and text processors that create annotations) has been implemented. In short, in the past years several crucial features have been added that make ELAN better and faster to use in many aspects...|$|R
40|$|For human risk {{assessments}} extrapolation of experimental {{data to the}} real human exposure situation is required. In this context the REACH guidance document has proposed default extrapolation factors for inter-species differences, {{as well as for}} differences in duration of exposure. Already several groups have analysed interspecies and time-extrapolation factors, but predominately based on limited datasets (Kalberlah et al., 2002; Vermeire et al., 2001; Pieters et al., 1998). The Fraunhofer Institute ITEM has developed the database RepDose on subacute to chronic toxicity studies in rodents. The high quality and amount of data in RepDose allows to reevaluate the current extrapolation factors for rats and mice and for time-extrapolation. A <b>tiered</b> approach is <b>applied</b> to evaluate both assessment factors distinguishing oral and inhalation exposure. Based on allometric considerations, for toxicokinetic properties a factor of 7 is proposed by the guidance to extrapolate doses as mg/kg bw from mice to humans and a factor of 4 for rats. Thus an extrapolation factor of 1. 75 would account for differences between rats and mice. Further according to the guidance a factor of 2. 5 is applied to account for toxicodynamic differences between species. The data in RepDose show, that on average the differences between both species are smaller than the standard extrapolation factor. Assessment factors for time-extrapolation are derived for subchronic to chronic, subacute to chronic and subacute to subchronic exposure. For the chemicals covered in RepDose, in general lower time-extrapolation factors are derived compared to those proposed by the REACH guidance document. The impact of local effects and certain physical-chemical properties like accumulation is described...|$|R
40|$|In {{striking}} {{the ban on}} same-sex marriage in Obergefell v. Hodges, the Supreme Court avoided tiers of scrutiny, thus declining to apply rational basis in a non-deferential manner {{as it had in}} other cases involving sexual orientation. In oral argument in Fisher v. University of Texas, the Court signaled its growing discomfort with the Grutter v. Bollinger strict scrutiny doctrine, which affords a level of institutional deference in tension with narrow tailoring and least restrictive means. And although the Court claims to apply intermediate scrutiny in gender-based equal protection cases, the cases devolve de facto applications of strict scrutiny or rational basis, based on whether the Court claims a real-sex difference or an overbroad gender-based generalization. The tiers-of-scrutiny doctrine has evolved from two to three formal tiers, yet a closer reading suggests five <b>applied</b> <b>tiers.</b> As a basis for prediction, the tiers are inverted, producing the following counterintuitive sequence: strict scrutiny, rational basis plus, intermediate scrutiny, strict scrutiny lite, and rational basis. The result has been doctrinal confusion, a lack of predictability, and calls by leading scholars and jurists for abandonment or fundamental reform. This Article offers a theoretical account for why tiers of scrutiny should not be jettisoned and why the existing scheme, as applied to race, sexual orientation, and gender, has produced anomalous — perhaps even disingenuous — applications. It further demonstrates how a modest re-conception operating within the general framework of existing tiers can greatly simplify applications, avoid the most critical anomalies, and thereby improve doctrinal predictability and coherence...|$|R
40|$|Food {{safety control}} {{is a matter for}} concern for all parts of the food supply chain, {{including}} governments that develop food safety policy, food industries that must control potential hazards, and consumers who need to keep to the intended use of the food. In the future, food safety policy may be set using the framework of risk analysis, part of which is the development of (inter) national microbiological risk assessment (MRA) studies. MRA studies increase our understanding of the impact of risk management interventions and of the relationships among subsequent parts of food supply chains with regard to the safety of the food when it reaches the consumer. Application of aspects of MRA in the development of new food concepts has potential benefits for the food industry. A <b>tiered</b> approach to <b>applying</b> MRA can best realize these benefits. The tiered MRA approach involves calculation of microbial fate for a product and process design on the basis of experimental data (e. g., monitoring data on prevalence) and predictive microbiological models. Calculations on new product formulations and novel processing technologies provide improved understanding of microbial fate beyond currently known boundaries, which enables identification of new opportunities in process design. The outcome of the tiered approach focuses on developing benchmarks of potential consumer exposure to hazards associated with new products by comparison with exposure associated with products that are already on the market and have a safe history of use. The tiered prototype is a tool to be used by experienced microbiologists as a basis for advice to product developers and can help to make safety assurance for new food concepts transparent to food inspection services...|$|R
40|$|The aim of {{this study}} was to {{determine}} prospectively the frequency of pathogenic chromosomal microdeletions and microduplications in a large group of referred patients with developmental delay (DD), intellectual disability (ID) or autism spectrum disorders (ASD) within a genetic diagnostic service. First <b>tier</b> testing was <b>applied</b> using a standardised oligo-array comparative genomic hybridization (CGH) platform, replacing conventional cytogenetic testing that would have been used in the past. Copy number variants (CNVs) found to be responsible for the clinical condition on the request form could all be subdivided into three groups: well established pathogenic microdeletion/microduplication/aneuploidy syndromes, predicted pathogenic CNVs as interpreted by the laboratory, and recently established pathogenic disease susceptibility CNVs. Totalled from these three groups, with CNVs of uncertain significance excluded, detection rates were: DD (13. 0 %), ID (15. 6 %), ASD (2. 3 %), ASD with DD (8. 2 %), ASD with ID (12. 7 %) and unexplained epilepsy with DD, ID and ASD (10. 9 %). The greater diagnostic sensitivity arising from routine application of array CGH, compared with previously used conventional cytogenetics, outweighs the interpretative issues for the reporting laboratory and referring clinician arising from detection of CNVs of uncertain significance. Precise determination of any previously hidden molecular defect responsible for the patient's condition is translated to improved genetic counselling. Jillian Nicholl, Wendy Waters, John C. Mulley, Shanna Suwalski, Sue Brown, Yvonne Hull, Christopher Barnett, Eric Haan, Elizabeth M. Thompson, Jan Liebelt, Lesley McGregor, Michael G. Harbord, John Entwistle, Chris Munt, Dierdre White, Anthony Chitti, David Baulderstone, David Ketteridge, Array Referral Consortium, Kathryn Friend, Sharon M. Bain and Sui Y...|$|R
40|$|Prevention {{of toxic}} {{pollutant}} discharge and remediation of contaminated sediments and soils are topics increasingly {{addressed by the}} scientific community and the stakeholders. The research activity carried out on the lagoon of Venice highlights the crucial role played by analytical and environmental chemistry in assessing the environmental behavior of chemicals (i. e. occurrence level, transformation, ultimate fate) and exposure of human and environmental targets to pollutants. The extrapolation from analytical data to decisional steps was accomplished by data treatment (descriptive and multivariate statistics, spatial statistics), environmental modeling (e. g. partitioning bioaccumulation models and linear regression models), environmental risk assessment (ERA), and a GIS-based Decision Support System (DSS). Results obtained by this integrated approach supported analytical and environmental chemistry by improving the selection of priority pollutants, optimizing sampling design, and identifying critical environmental pathways. Both uncertainty minimization and cost saving of the overall decision process could be achieved. Selected results are presented here on {{the application of the}} proposed approach to the contaminated sediments of the lagoon of Venice and to the brownfield of the Porto Marghera industrial district. Both well-known persistent pollutants (e. g. polychlorinated dioxins/furans (PCDD/Fs), polychlorinated biphenyls (PCBs), polycyclic aromatic hydrocarbons (PAHs), metals and metalloids, and aromatic surfactants and their metabolites), as well as new classes of pollutants (e. g. endocrine disrupting compounds, EDCs) were investigated. The analytical data indicated that the most persistent and toxic organic and inorganic chemicals were found mainly in bottom sediments (especially those near the Porto Marghera industrial district), while substances such as surfactants and their metabolites and EDCs occurred mainly in water and were redistributed over the whole lagoon. Exposure characterization allowed Environmental Risk Assessment (ERA) to be undertaken, including the estimation of risk for both human and environmental health. The ERA procedure, developed according to a <b>tiered</b> approach, was <b>applied</b> to contaminated soils of the Porto Marghera industrial district. The ecological risk associated with contaminated lagoon sediments for the benthic community and aquatic food web was also assessed, resulting in a significant risk posed especially by mercury, cadmium and PAHs. Finally, a risk-based decision support system (DSS) for the rehabilitation of the Porto Marghera contaminated site was developed, which included environmental risk and remediation technologies...|$|R
40|$|The study assesses Adler and Barnett's (1998) three tier {{framework}} with {{a specific}} focus on the mature phase of their framework that emphasises mutual trust and collective identity as necessary conditions for establishing a security community. Adler and Barnett's (1998) three <b>tier</b> framework is <b>applied</b> to SADC's efforts of establishing a security community in the Southern African region. The study explores the reasoning behind SADC's creation {{with a specific}} focus on regional integration and how it defines its security architecture and political rationale. By focusing on regional integration and defining SADC's security architecture and political rationale the study outlines how the organisation is making efforts of establishing a security community. This is indicated by describing how SADC has attained the nascent and ascendant phase through its various initiatives and programmes such as the RISDP, SIPO I and II and MDP which provide evidence {{that there is a}} sense of cooperation and coordination among SADC member states. The study argues that SADC has reached the nascent and ascendant phase although the regional organisation has not yet progressed to the mature phase of establishing itself as a security community. The study critiques Adler and Barnett's (1998) third phase, which stresses the importance of two necessary conditions of mutual trust and collective identity. Mutual trust and collective identity are evaluated and analysed in respect of whether or not they are relatable and recognised within SADC as a possible emerging security community. The main finding of the study is that mutual trust and collective identity are not recognised in SADC in the manner in which Adler and Barnett (1998) describe them in their three tier framework. However SADC does make efforts to strengthen mutual trust, coordinate strategies and policies to develop collective identity, rather its efforts are not sufficient to make it a security community in the manner Adler and Barnett (1998) understand it. SADC continues to uphold a strict adherence to sovereignty, and is also characterised by domestic instability, lack of common norms and interests among member states and these are major problems for the organisation to create a security community. Mini Dissertation (M Security Studies) [...] University of Pretoria, 2017. Political SciencesM Security StudiesUnrestricte...|$|R
40|$|The {{wireless}} {{networks of}} the future {{are likely to be}} tiered, i. e., a heterogeneous mixture of overlaid networks that have different power, spectrum, hardware, coverage, mobility, complexity, and technology requirements. The focus of this dissertation is to improve the performance and increase the throughput of tiered networks with resource/interference management methods, node densification schemes, and transceiver designs; with their applications to advanced tiered network structures such as heterogeneous networks (i. e., picocells, femtocells, relay nodes, and distributed antenna systems), device-to-device (D 2 D) networks, and aeronautical communication networks (ACN). Over the last few decades, there has been an incredible increase in the demand for wireless services in various applications in the entire world. This increase leads to the emergence of a number of advanced wireless systems and networks whose common goal is to provide a very high data rate to countless users and applications. With the traditional macrocellular network architectures, it will be extremely challenging to meet such demand for high data rates in the upcoming years. Therefore, a mixture of different capability networks has started being built in a tiered manner. While the number and capabilities of networks are increasing to satisfy higher requirements; Modeling, managing, and maintaining the entire structure has become more challenging. The capacity of wireless networks has increased with various different advanced technologies/methodologies between 1950 - 2000 which can be summarized under three main titles: spectrum increase (x 25), spectrum efficiency increase (x 25), and network density (spectrum reuse) increase (x 1600). It is vital to note that among different schemes, the most important gain is explored with increasing the reuse and adding more nodes/cells into the system, which will be the focus of this dissertation. Increasing the reuse by adding nodes into the network in an uncoordinated (irregular in terms of power, spectrum, hardware, coverage, mobility, complexity, and technology) manner brought up heterogeneity to the traditional wireless networks: multi-tier resource management problems in uncoordinated interference environments. In this study, we present novel resource/interference management methods, node densification schemes, and transceiver designs to improve the performance of <b>tiered</b> networks; and <b>apply</b> our methodologies to heterogeneous networks, D 2 D networks, and ACN. The focus and the contributions of this research involve the following perspectives: 1. Resource Management in Tiered Networks: Providing a fairness metric for tiered networks and developing spectrum allocation models for heterogeneous network structures. 2. Network Densification in Tiered Networks: Providing the signal to interference plus noise ratio (SINR) and transmit power distributions of D 2 D networks for network density selection criteria, and developing gateway scheduling algorithms for dense tiered networks. 3. Mobility in Tiered Networks: Investigation of mobility in a two-tier ACN, and providing novel transceiver structures for high data rate, high mobility ACN to mitigate the effect of Doppler...|$|R
40|$|We {{were asked}} by the Independent Evaluation Office to outline {{political}} science methods for assessing the chances of reform implementation in an ex-ante fashion. We agreed to illustrate how these tools 'work' by using Pakistan as a case study. The recent literature on IMF-sponsored reforms points out that successful implementation not only depends on the nature and severity of the economic crisis and {{on the design of}} the reforms, but very much also on the political economy of reform politics. We have identified the following as salient political factors for identifying chances of reform success: - the power of sections of the economy and polity that will lose from effective implementation; - the political independence of reform-minded branches of government vis- 0 -vis politicians that depend on popular support; - the institutional capacity to implement reform; - a high degree of acceptance of the reforms among the major stakeholders (the 'ownership' factor). We have designed three tools that help forecast how these factors will develop in the future. Each tool comprises three dimensions of analysis: - how these factors will develop after signing an agreement, given visible trends in the immediate past (trend extrapolation); - how these factors would be influenced by an effective reform implementation (impact analysis); - how other political framework conditions will evolve and what impact this may have for the reform prospects (scenario building). The three tools belong to different types of forecasting techniques and thus illustrate the wide range methods available. They also address different combinations of the four political factors. The three tools are summarized below. Tool 1 : Stakeholder analysis This tool forecasts how the political struggle over reform will end by assessing the relative power and influence of the major stakeholders and by estimating how this balance of power will develop in the future. The three dimensions of analysis could look as follows: Trend extrapolation involves a close inspection of: 1) the reform steps undertaken during and before the negotiation period; 2) the negotiation style of the government (inclusiveness and transparency); 3) the degree of ownership of the reform idea among the major stakeholders. Impact analysis estimates how the power base of the actual government (factor 1) and the relation between civil servants and elected politicians (factor 2) will change due to effective implementation of the reforms and how this in turn influences the probability of continued implementation in the mid-term. Scenario building integrates other independent trends (e. g. declining power base of a party in power) as well as unforeseeable events (such as a foreign policy crisis) into the assessment exercise. The scenarios may be ranked by probability. Tool 2 : Institutional analysis This tool would comprise three different elements of analysis. Institutional mapping describes the network of institutions (both governmental and non-governmental and at different levels) involved in decision making and reform implementation. The veto power analysis then determines the relative power and independence of those branches of the bureaucracy that are able and determined to implement reforms. The capacity assessment would look at levels of professionalism, recruitment procedures, educational background and motivation in those branches of government. Trend extrapolation would take into account actual trends of institutional change in determining the chances of reform implementation. Under the impact analysis, the institutional consequences of the reform programme itself and their impact on capacity and willingness to reform can be assessed. The scenario technique could be used to produce different scenarios of mid-term institutional change and see how they influence the prospects for economic reform. Tool 3 : Delphi study Delphi studies belong to the pool of expert opinion tools. It consists of at least three rounds of surveys administered by a questionnaire. The experts may adjust their responses in the second and subsequent rounds after having been informed about the mean answers of the previous round. We suggest to ask at least 15 experts from think tanks, advisory bodies, the media, universities etc. to assess a) the prospects for the reforms being implemented given current political trends; b) the political impact of the reforms and how it may affect the possibility of sustained reform; and c) the probability of various mid-term political scenarios and the chances for sustaining reform under these scenarios. One of the comparative advantages of Delphi studies is that the results are not influenced by opinion leader phenomena. They can be used to quickly assess the constellations of opinions with regard to specific policy options and the probabilities associated with different future developments. In the concluding section we recommend - to apply the maximum possible number of tools in order to arrive at a solid assessment of the political feasibility of a programme from different perspectives. - to apply the 'triangulation of methods' approach whenever it is necessary to outbalance different results produced by the different tools. This means to reinterpret results and search for new evidence until more coherent overall conclusions can be reached; - to develop a multi-tier assessment system, where the basic tier, streamlined to all IMFsupported programmes, would consist in the trend extrapolation and impact assessment components of stakeholder analysis; institutional analysis would represent a second <b>tier,</b> to be <b>applied</b> to cases where doubts about implementation prospects are higher; a Delphi study, including scenario building, represents the most complex exercise reserved for the most contested cases; - to rely on careful judgement when deciding to more systematically include political factors, taking into account the risks of becoming involved in political and institutional engineering in sovereign nation states...|$|R

