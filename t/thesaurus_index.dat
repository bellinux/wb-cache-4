0|49|Public
40|$|The {{general purpose}} of the MedIndEx (Medical Indexing Expert) Project at the National Library of Medicine (NLM) is to design, develop, and test {{interactive}} knowledge-based systems for computer-assisted indexing of literature in the MEDLINE® database using terms from the MeSH® (Medical Subject Headings) thesaurus. In conventional MEDLINE indexing, although indexers enter MeSH descriptors at computer terminals, they consult the <b>thesaurus,</b> <b>indexing</b> manual, and other tools in published form. In the MedIndEx research prototype, the <b>thesaurus</b> and <b>indexing</b> rules are incorporated into a computerized knowledge base (KB) which provides specific assistance not possible in the conventional indexing system. We expect such a system, which combines principles and methods of artificial intelligence and information retrieval, will facilitate expert indexing that takes place at NLM...|$|R
40|$|Abstract. In the {{information}} retrieval context, resource collections are frequently classified using thesauri. However, the limited semantics pro-vided by thesauri restricts the collection search and browsing capabilities. This work focuses on improving these capabilities by transforming {{a set of}} resources indexed according to a thesaurus into a semantically tagged collection. The core mechanism for building this collection {{is based on the}} conversion of the domain specific <b>thesaurus</b> (<b>indexing</b> the collection of resources) into a domain ontology connected to an upper level ontol-ogy. The feasibility of this work has been tested in the urban domain by transforming the resources accessible through the European Urban Knowledge Network into a Linked Data repository. ...|$|R
40|$|A {{document}} information {{system based on}} Montague semantics(MS) and controlled natural languages(CNL) can be both symbolized and normalized-the two characteristics possessed by traditional document classification systems and <b>thesaurus</b> <b>indexing</b> systems,respectively;it can also realize complicated syntax and semantics functions,so as to improve the efficiency of {{document information}} retrievals greatly. Moreover,it can form standard paper abstracts automatically and realize translations between different languages. This paper attempted to construct such a kind of system,which includes category classification of words,expressions of sentence functions,maps/translations between natural and formal languages,as well as between Chinese and English languages. Finally,an application program was designed to test functions of the system. The {{results of this study}} show that the data structure and function simplementation of the proposed information system based on new concepts are feasible...|$|R
40|$|National audienceAutomatic {{construction}} of ontologies from text is generally based on retrieving text content. For {{a much more}} rich ontology we extend these approaches by {{taking into account the}} document structure and some external resources (like <b>thesaurus</b> of <b>indexing</b> terms of near domain). In this paper we describe how these external resources are at first analyzed and then exploited. This method has been applied on a geographical domain and the benefit has been evaluated...|$|R
50|$|Each record {{contains}} a bibliographic citation, abstract, index terms from the <b>Thesaurus</b> of Psychological <b>Index</b> Terms, keywords, classification categories, population information, the geographical {{location of the}} research population, and cited references for journal articles, book chapters, and books, mainly from 2001 to present. Records of books include the book's table to contents.|$|R
40|$|The {{application}} {{of computer technology}} {{to the construction of}} the NASA Thesaurus and in NASA Lexical Dictionary development is discussed in a brief overview. Consideration is given to the printed and online versions of the <b>Thesaurus,</b> retrospective <b>indexing,</b> the NASA RECON frequency command, demand indexing, lists of terms by category, and the STAR and IAA annual subject indexes. The evolution of computer methods in the Lexical Dictionary program is traced, from DOD and DOE subject switching to LCSH machine-aided indexing and current techniques for handling natural language (e. g., the elimination of verbs to facilitate breakdown of sentences into words and phrases) ...|$|R
5000|$|The London Education Classification is {{a library}} {{classification}} and <b>indexing</b> <b>thesaurus</b> {{used at the}} UCL Institute of Education. [...] It was devised by D.J. Foskett and Joy Foskett. It was devised to address deficiencies in general classification schemes in dealing with education. [...] It was originally devised in 1963, and revised in 1974. It is a faceted classification, inspired {{by the work of}} S.R. Ranganathan and of the Classification Research Group.|$|R
40|$|The NASA programs, publications, and {{services}} promoting the transfer and utilization of aerospace technology developed {{by and for}} NASA are briefly surveyed. Topics addressed include the corporate sources of NASA technical information and its interest for corporate users of information services; the IAA and STAR abstract journals; NASA/RECON, NTIS, and the AIAA Aerospace Database; the RECON Space Commercialization file; the Computer Software Management and Information Center file; company information in the RECON database; {{and services}} to small businesses. Also discussed are the NASA publications Tech Briefs and Spinoff, the Industrial Applications Centers, NASA continuing bibliographies on management and patent abstracts (indexed using the NASA <b>Thesaurus),</b> the <b>Index</b> to NASA News Releases and Speeches, and the Aerospace Research Information Network (ARIN) ...|$|R
40|$|Dr. Weedman’s talk {{covers the}} various {{information}} {{environments in which}} she has worked {{over the course of}} her career, starting with her work in a public library before library school, and including school libraries, various special libraries, the information industry, and then eventually moving into her Ph. D. work and experience as a faculty member. Intermixed with these are her insights and perspectives on <b>thesaurus</b> design, journal <b>indexing,</b> and vocabulary design. [URL]...|$|R
40|$|The aim of {{this study}} was to {{reassess}} and compare the usefulness of the National Library of Medicine (NLM) ’s Medical Subject Headings® (MeSH) <b>thesaurus</b> for electronic <b>indexing</b> and retrieval of chronobiologic bibliography, after 4 years from a previous study on the same subject (Portaluppi F. 2007. Consistency and accuracy of the medical subject headings <b>thesaurus</b> for electronic <b>indexing</b> and retrieval of chronobiologic references. Chronobiol Int. 24 : 1213 – 1229) which demonstrated inconsistent and inaccurate results obtained with existing chronobiologic MeSH terms and suggested the inclusion in the MeSH thesaurus of some common chronobiologic concepts and definitions. A sample set of 219 recent chronobiologic references was downloaded from the MEDLINE®’s database together with all MeSH entries associated with them. The following descriptors of obvious chronobiologic relevance were reanalyzed: ‘‘chronobiology’’, ‘‘chronobiology disorders’’, ‘‘biological clocks’’, ‘‘circadian rhythm’’, ‘‘chronotherapy’’, ‘‘periodicity’’, ‘‘seasons’’, ‘‘sleep disorders, circadian rhythm’’ and ‘‘time factors’’. Results were comparable with the previous study that we did 4 years ago. The MeSH terms of obvious chronobiologic significance are still inconsistently and inaccurately retrieving chronobiologic references, while none of the common chronobiologic concepts that we suggested have been included in the MeSH thesaurus, which remains largely incomplete for chronobiologic use...|$|R
40|$|A knowledge-based, or expert, system {{encoding}} both factual {{and procedural}} knowledge to assist users in performing an intellectual task is {{ideally suited to}} indexing. Existing thesauri, classification schemes, and indexing manuals are a good starting point, and artificial intelligence (AI) computer languages and data structures seem well suited for development of these systems. In addition, currently available workstation environments (with windows and mouse) and standard software (such as X Windows) should make possible sophisticated and portable interfaces. A unique prototype, the MedlndEx System, is being developed to assist people using the Medical Subject Headings (MeSH) <b>thesaurus</b> to <b>index</b> the MEDLINE database in MEDLARS (Medical Literature Analysis and Retrieval System) at the National Library of Medicine (NLM). MedlndEx is, in principle, applicable to any indexing system using a thesaurus and following a body of indexing rules. published or submitted for publicatio...|$|R
40|$|Latent Semantic Analysis, {{when used}} for {{automated}} essay grading, {{makes use of}} document word count vectors for scoring the essays against domain knowledge. Words in the domain knowledge documents and essays are counted, and Singular Value Decomposition is undertaken to reduce {{the dimensions of the}} semantic space. Near neighbour vector cosines and other variables are used to calculate an essay score. This paper discusses a technique for computing word count vectors where the words are first normalised using <b>thesaurus</b> concept <b>index</b> numbers. This approach leads to a vector space of 812 dimensions, does not require Singular Value Decomposition, and leads to a reduced computational load. The cosine between the vectors for the student essay and a model answer proves to be a very powerful independent variable when used in regression analysis to score essays. An example of its use in practice is discussed...|$|R
50|$|Readex, a {{division}} of NewsBank, is using original Serial Set volumes from Baker-Berry Library at Dartmouth College to create fine new digital images of every publication through 1994, including approximately 56,000 maps. Additional Serial Set materials not available from Dartmouth are being provided by the United States Senate Library, Middlebury College, the Vermont State Library and the University of Vermont. The Readex digital edition contains nearly 14,000 high-resolution color maps from the Library of Congress and more than 8,000 high-resolution color illustrations. To ensure the best search results, Readex has created all-new indexing of subject terms, executive agencies and bill numbers {{as well as of}} personal, committee and geographic names. The Readex edition contains active links from terms in bibliographic records that enable retrieval of related publications, an integrated subject <b>thesaurus,</b> map-level <b>indexing</b> and OpenURLs for each publication.|$|R
40|$|International audienceThis paper tackles {{the problem}} of term ambiguity, {{especially}} for biomedical literature. We propose and evaluate two methods of Word Sense Disambiguation (WSD) for biomedical terms and integrate them to a sense-based document indexing and retrieval framework. Ambiguous biomedical terms in documents and queries are disambiguated using the Medical Subject Headings (MeSH) <b>thesaurus</b> and semantically <b>indexed</b> with their associated correct sense. Experimental evaluation carried out on the TREC 9 -FT 2000 collection shows that our approach of WSD and sense-based indexing and retrieval is promising...|$|R
40|$|Effective and {{efficient}} publishing {{and searching for}} Universal Description, Discovery and Integration (UDDI) Web services {{is important for the}} success of Web services. Traditional approaches such as <b>index,</b> <b>thesaurus,</b> and classification have historically dominated information representation and organization; however, they have some limitations. Ontology-driven knowledge organization [An ontology-driven knowledge organization approach] is considered one of keys to the next generation of knowledge management applications because ontology provides structure and meaning to data. This research examines how ontology-driven knowledge organization could provide benefits to current UDDI Web services...|$|R
40|$|International audienceIt is {{well known}} that the main {{objective}} of conceptual retrieval models is to go beyond simple term matching by relaxing term independence assumption through concept recognition. In this paper, we present an approach of semantic indexing and retrieval of biomedical documents through the process of identifying domain concepts extracted from the Medical Subject Headings (MeSH) <b>thesaurus.</b> Our <b>indexing</b> approach relies on a purely statistical vector space model, which represents medical documents and MeSH concepts as term vectors. By leveraging a combination of the bag-of-word concept representation and word positions in the textual features, we demonstrate that our mapping method is able to extract valuable concepts from documents. The output of this semantic mapping serves as the input to our relevance document scoring in response to a query. Experiments on the OHSUMED collection show that our semantic indexing method significantly outperforms state-of-art baselines that employ word or term statistics...|$|R
40|$|In this paper, {{we argue}} on the {{interest}} of anchoring Dutch Cultural Heritage controlled vocabularies to WordNet, and demonstrate a reusable methodology for achieving this anchoring. We test it on two controlled vocabularies, namely the GTAA thesaurus, used at the Netherlands Institute for Sound and Vision (the Dutch radio and television archives), and the GTT <b>thesaurus,</b> used to <b>index</b> books of the Dutch National Library. We evaluate the two anchorings having in mind a concrete use case, namely generic alignment scenarios where concepts from one thesaurus must be aligned to concepts from the other. ...|$|R
40|$|This paper {{describes}} Image Engine, an object-oriented, microcomputer-based, {{multimedia database}} designed {{to facilitate the}} storage and retrieval of digitized biomedical still images, video, and text using inexpensive desktop computers. The current prototype runs on Apple Macintosh computers and allows network database access via peer to peer file sharing protocols. Image Engine supports both free text and controlled vocabulary indexing of multimedia objects. The latter is implemented using the TView thesaurus model developed by the author. The current prototype of Image Engine uses the National Library of Medicine's Medical Subject Headings (MeSH) vocabulary (with UMLS Meta- 1 extensions) as its <b>indexing</b> <b>thesaurus...</b>|$|R
40|$|This index, {{published}} by the Theatre/Drama and Speech Information Center, cites 30 Speech communication journals generated by national and state associations, research institutes, and publishing houses. The index is designed (1) {{as a guide to}} materials for scholarly research in speech communication, (2) as a guide for individual awareness, and (3) as a guide for speech communication education at all levels. The index contains: an introduction, general instructions, a classified section, an author index, a subject index, a speech communication <b>thesaurus,</b> a name-in-text <b>index,</b> book and media reviews, bibliographies, and a list of reference materials. (TS) Volume!, Number...|$|R
40|$|This paper {{describes}} {{the work done}} in the TIPS project about {{the construction of a}} thesaurus. This construction is a merge from a compilation of data from several web sources. These data comes from manual work, some data are real <b>thesaurus,</b> other are <b>indexing</b> recommendations. The merge is done with automatically extracted terms from large text corpora. The automatic extraction is based on both syntax and statistics. We present in this paper the way thesaurus are built and the results on Scientific corpus {{in the context of the}} TIPS project. This short paper emphasis on some technical aspects...|$|R
40|$|International audienceThe {{purpose of}} this paper is to {{identify}} automatically hypernyms for dictionary entries by exploring their definitions. In order to do this, we propose a weighting methodology that lets us assign to each lexeme a weight in a definition. This fact allows us to predict that lexemes with the highest weight are the closest hypernyms of the defined lexeme in the dictionary. The extracted semantic relation "is-a" is used for the automatic construction of a <b>thesaurus</b> for image <b>indexing</b> and retrieval. We conclude the paper by showing some experimental results to validate our method and by presenting our methodology of automatic thesaurus construction...|$|R
40|$|A {{conceptual}} glossary is a textual {{reference work}} that combines {{the features of}} a <b>thesaurus</b> and an <b>index</b> verborum. In it, the word occurrences within a given text are classified, disambiguated, and indexed according to their membership {{of a set of}} conceptual (i. e. semantic) fields. Since 1994, we have been working towards building a set of conceptual glossaries for the Latin Vulgate Bible. So far, we have published a conceptual glossary to the Gospel according to John and are at present completing the analysis of the Gospel according to Mark and the minor epistles. This paper describes the background to our project and outlines the steps by which the glossaries are developed within a relational database framework...|$|R
40|$|Abstract. This paper {{describes}} GeoTALP-IR system, a Geographical Information Retrieval (GIR) system. The {{system is}} described and evaluated {{in the context}} of our participation in the CLEF 2005 GeoCLEF Monolingual English task. The GIR system is based on Lucene and uses {{a modified version of the}} Passage Retrieval module of the TALP Question Answering (QA) system presented at CLEF 2004 and TREC 2004 QA evaluation tasks. We designed a Keyword Selection algorithm based on a Linguistic and Geographical Analysis of the topics. A Geographical Thesaurus (GT) has been built using a set of publicly available Geographical Gazetteers and a Geographical Ontology. Our experiments show that the use of a Geographical <b>Thesaurus</b> for Geographical <b>Indexing</b> and Retrieval has improved the performance of our GIR system...|$|R
5000|$|Sven Lidman {{left his}} {{previous}} employer in 1955 and took this new idea to the Swedish publishing house Almqvist & Wiksell, {{but only after}} also getting a letter of intent from German publisher Bertelsmann was he able to get his employer started. Being the first richly illustrated encyclopedia in Sweden (and several other countries), the sales were a given success. The idea of only reusing the illustrations failed, as most contracting publishers chose to translate most of the text as well. A fifth volume containing an <b>index,</b> <b>thesaurus</b> and cross-reference {{was added to the}} basic encyclopedia, not least because this increased the total [...] "number of entries" [...] from 40,000 (in the main volumes) to 100,000 (in the index), one of the strongest sales arguments for encyclopedias.|$|R
40|$|The Ionian University in {{cooperation}} with the National Documentation Centre has undertaken the task of indexing the Official Gazette using international coding and standards. The indexing process is prop up by a structured <b>thesaurus.</b> The <b>indexing</b> and <b>thesaurus</b> work secondary products are planned to be the following: 1. a "lexicon" of administrative terms 2. an administrative encyclopaedia correlated to the lexicon 3. an administrative atlas with statistical information connected to both the lexicon and the encyclopedia. The project today covers {{the early years of the}} Greek state (ruling of King Otto A' 1833 - 1862). It is already a multifaceted information tool that serves research in the field of administrative science. At the same time, it is a product of information science's basic principles. The interdisciplinary character of information science lends itself to cooperation with other fields of science, hence the particular project can serve as a point of reference for similar works and further applications. The planning and the first stage of the project's implementation were a result of research in the following domains: 	a. information science 	b. 	 administrative history 	c. 	 administrative science Furthermore, the project is geared towards research in the area of information management of government publications...|$|R
40|$|Since 1997, the National Medical Library allows {{public health}} workers to operate MEDLINE and LILACS {{database}} by themselves {{together with the}} traditional bibliographic search service provided by this entity. This significant step determines the necessary training of user in those bibliographic information retrieval systems, and, above all, the learning and use of MeSH and DeCS thesauri. This article is aimed at explaining the characteristics of thesauri used for indexing such database and analyzing their structure and components {{as well as their}} relationship with the principles applied to indexing, particularly specificity and exhaustiveness. Some key concepts of indexing, information search and retrieval are outlined. Likewise, a general view on the available retrieval techniques in Find, <b>Index,</b> <b>Thesaurus</b> software commands and F 2 key as a way of guiding users to establish their search strategies is presented...|$|R
40|$|This year, for our {{participation}} to the TREC Genomics track, we {{participated in}} the two tasks: the ad hoc and the categorization task. In this notebook report, we do not detail our experiments, which will be described more precisely in the final proceedings. This papers focuses on the ad hoc task, while experiments conducted for task 2 are described in the Aronson and al. 2005. Task I. For the ad hoc retrieval task, we used the easyIR tool, a standard vector-space engine developed at the University of Geneva. Our approach uses thesaural resources together with {{a variant of the}} Porter stemmer for string normalization. Gene and Protein Entities (GPE) in queries are marked up by dictionary look up at retrieval time in order to be expanded using a gene and protein <b>thesaurus.</b> For <b>indexing</b> the Genomic collection, the following MEDLINE records were selected: article’s titles, MeSH and RN terms, and abstract fields. Following observations made on MEDLINE documents regarding their length distribution, we decided to rely on a slightly modified dtu. dtn weighting schema. This constitutes our baseline run (Baseline= 0. 2312; Baseline+expansion= 0. 2373). Finally, we used a run provided by the University of Neuchâtel, which features thesaurus-based GPE expansion and automatic feed back (UniGeNe= 0. 2150) to produce a third run, which achieved our best results (UniGe 2 = 0. 2396) ...|$|R
40|$|A texture based image {{retrieval}} system for browsing large-scale aerial photographs is presented. The salient components of this system include texture feature extraction, image segmentation and grouping, learning similarity measure, and a texture thesaurus model for fast search and indexing. The texture features are computed by filtering the image with a bank of Gabor filters. This is followed by texture flow computation to segment each large airphoto into homogeneous regions. A hybrid neural network algorithm is used to learn the visual similarity by clustering patterns in the feature space. With learning similarity, the retrieval performance improves significantly. Finally, a texture image thesaurus is created by combining the learning similarity algorithm and a hierarchical vector quantization scheme. This <b>thesaurus</b> facilitates the <b>indexing</b> process while maintaining a good retrieval performance. Experimental results demonstrate the robustness of the overall system in sear [...] ...|$|R
40|$|Roget’s II: The New Thesaurus, Third Edition, {{allows the}} user {{to find the right}} synonym with a minimum of effort. Unlike many thesauruses, this easy-to-use {{reference}} lists main entry words alphabetically, as in a dictionary, for quick lookup. Each entry is divided into senses, with brief definitions and a full list of synonyms for each sense, to ensure that the selected usage is the most appropriate one. All special usages, such as slang terms, are labeled and grouped together {{at the end of each}} synonym list. Following each list is a cross-reference to a related entry in the <b>thesaurus’s</b> unique Category <b>Index.</b> This index leads the reader from the starting word to dozens of others that have related or opposite meanings. All these features make Roget’s II the best resource for finding the right word every time...|$|R
40|$|This paper {{describes}} GeoTALP-IR system, a Geographical Information Retrieval (GIR) system. The {{system is}} described and evaluated {{in the context}} of our participation in the CLEF 2005 GeoCLEF Monolingual English task. The system architecture has two phases that are performed sequentially: Topic Analysis and Document Retrieval. The Topic Analysis phase extracts and analyzes the relevant keywords from the topic. This phase uses a Keyword Selection algorithm based on Linguistic and Geographical Analysis of the topics. A Geographical Thesaurus has been build using a set of publicly available Geographical Gazetteers. The Document Retrieval system is based on Lucene and uses {{a modified version of the}} Passage Retrieval module used by the TALP Question Answering (QA) system at the CLEF 2004 and TREC 2004 QA evaluation tasks. The results of our experiments show that the use of a Geographical <b>Thesaurus</b> for Geographical <b>Indexing</b> and Retrieval has improved the performance of our GIR system...|$|R
40|$|Political {{texts on}} the Web, documenting laws and {{policies}} and the process leading to them, are of key importance to government, industry, and every individual citizen. Yet access to such texts is difficult due to the ever increasing volume {{and complexity of the}} content, prompting the need for indexing or annotating them with a common controlled vocabulary or ontology. In this paper, we investigate the effectiveness of different sources of evidence—such as the labeled training data, textual glosses of descriptor terms, and the <b>thesaurus</b> structure—for automatically <b>indexing</b> political texts. Our main findings are the following. First, using a learning to rank (LTR) approach integrating all features, we observe significantly better performance than previous systems. Second, the analysis of feature weights reveals the relative importance of various sources of evidence, also giving insight in the underlying classification problem. Third, a lean-and-mean system using only four features (text, title, descriptor glosses, descriptor term popularity) is able to perform at 97 % of the large LTR model...|$|R
40|$|This book, {{which is}} {{intended}} {{to serve as the}} first stage in an iterative process of detecting, predicting, and assessing the impacts of Artificial Intelligence opens with a short "one-hour course" in AI, which {{is intended to}} provide a nontechnical informative introduction to the material which follows. Next comes an overview chapter which is based on an extensive literature search, the position papers, and discussions. The next section of the book contains position papers whose richness and diversity illustrate the wealth of opinions and research directions that today fall under the umbrella term "AI research". The papers are followed by a select bibliography containing nearly 700 books, articles, and research memoranda on AI-related topics, together with a <b>thesaurus</b> and KWIC <b>index</b> to facilitate the retrieval of information. The book closes with and index and two appendices, one listing the names and addresses of the contributing scientists and the other giving details of the AI curriculum at the University of Vienna...|$|R
40|$|The AGRIS {{repository}} is a bibliographic database covering almost {{forty years}} of agricultural research. Following the conversion of its <b>indexing</b> <b>thesaurus</b> AGROVOC into a concept-based vocabulary, {{the decision was made}} to express the entire AGRIS repository in RDF as Linked Open Data. As part of this exercise, a semantic mashup named OpenAGRIS was developed in order to access the records and use them to dynamically display related data from external systems through both SPARQL queries and traditional web services. The overall process raised numerous issues regarding the relative lack of administrative metadata required to compellingly address the top proof and trust layers of the semantic web stack, both within the AGRIS repository and in external data dynamically pulled into OpenAGRIS. The team began by disambiguating the journals in which the articles were published and converting them into RDF but quickly realized this was only the beginning of a series of necessary steps in moving from a closed to an open world paradigm. Further disambiguation of institutions, authors and AGRIS Centres as well as the use of the VoiD vocabulary and of quality indicator models are discussed and evaluated...|$|R
40|$|Self-assessment of topic/task {{knowledge}} {{is a human}} metacognitive capacity that impacts information behavior, for example through selection of learning and search strategies. It is often used as a measure in experiments for evaluation of results and those measurements are taken to be generally reliable. We conducted a user study (n= 40) to test this by constructing a concept-based topic knowledge representation for each participant and then comparing it with the participant judgment of their topic knowledge elicited with Likert-scale questions. The tasks were in the genomics domain and knowledge representations were constructed from the MeSH <b>thesaurus</b> terms that <b>indexed</b> relevant documents for five topics. The participants rated their familiarity with the topic, the anticipated task difficulty, the amount of learning gained during the task, and made other knowledge-related judgments associated with the task. Although there is considerable variability over individuals, the results provide evidence that these selfassessed topic knowledge measures are correlated in the expected way with the independently-constructed topic knowledge measure. We argue the results provide evidence for the general validity of topic knowledge selfassessment and discuss ways to further explore knowledge self-assessment and its reliability for prediction of individual knowledge levels...|$|R
40|$|A general {{model of}} a bibliographic {{retrieval}} sytem is presented which has five main elements: the documents, the queries, the <b>thesaurus</b> of <b>indexing</b> terms, the search algorithms and the physical storage locations. This is adapted to produce a probabilistic model which is suitable for simulation purposes, concentrating on the assignment of index terms to documents. This is accomplished by using the distribution of terms over documents and over queries, the distribution of exhaustivity over documents and over queries, the distribution of co-occurrences (occurrences of pairs of terms), the distribution of relevant and non-relevant documents over the number of terms matching the query. Several theoretical distributions were tested against four databases {{to find the best}} fitting distributions using the chi-square criterion. The distribution of terms over documents was split into two parts. The low frequency terms were analyzed using the number of terms which occurred x times, called the frequency-size approach. The high frequency terms were ranked by the number of occurrences in documents and analyzed using the rank versus the frequency of the term, called the frequency-rank approach. It was found that a generalized Zipf distribution fit the frequency-size portion and a generalized Bradford or log-rank distribution was best for the frequency-rank part.;These distributions were incorporated into a simulation program using a probabilistic model of term occurrences and co-occurrences. Simulation of the four databases was carried out using both the independence assumption of the occurrence of terms and the dependence assumption. In most cases the dependence model gave an improvement over the independent model but did not reproduce fully the original distribution of co-occurrences.;A small experiment with the clustering of terms to incorporate term dependence was also carried out. A method of incorporating the clustered terms into a simulation model needs to be found.;More work {{needs to be done in}} incorporating dependence of index terms, especially of order higher than two, into a model of bibliographic retrieval systems. Goodness-of-fit tests and parameter estimation methods need to be devised for the type of long tailed distributions encountered...|$|R
40|$|Abstract. AGRIS {{is among}} the most {{comprehensive}} online collections of agricultural and related sciences information. It is a growing global catalog of 4. 3 million high-quality structured bibliographic records indexed from a worldwide group of provid-ers. AGRIS relies heavily on the AGROVOC <b>thesaurus</b> for its <b>indexing.</b> Following the conversion of that thesaurus into a SKOS concept-scheme and its publication as Linked Open Data (LOD), the entire set of AGRIS records was also triplified and released as LOD. As part of this exercise, OpenAGRIS, a semantic mashup application, was developed to dynamically com-bine AGRIS data with external data sources, using a mixture of SPARQL queries and traditional web services. The re-engineering of AGRIS for the semantic web raised numerous issues regarding the relative lack of administrative metadata re-quired to compellingly address the proof and trust layers of the semantic web stack, both within the AGRIS repository and in the external data pulled into OpenAGRIS. The AGRIS team began a process of disambiguation and enrichment to continue moving toward an entity-based view of its resources, beginning with {{the tens of thousands of}} journals attached to its records. The evolution of the system, the issues raised during the triplification process and the steps necessary for publishing the result as LOD content are hereby discussed and evaluated...|$|R
