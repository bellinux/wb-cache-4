0|6086|Public
50|$|See the <b>unit</b> <b>test</b> <b>cases</b> in {{the example}} C++ {{implementation}} below for more examples.|$|R
50|$|<b>Unit</b> <b>test</b> <b>cases</b> embody {{characteristics}} {{that are critical}} to the success of the unit. These characteristics can indicate appropriate/inappropriate use of a unit as well as negative behaviors that are to be trapped by the <b>unit.</b> A <b>unit</b> <b>test</b> <b>case,</b> in and of itself, documents these critical characteristics, although many software development environments do not rely solely upon code to document the product in development.|$|R
40|$|In the {{software}} development life cycle, <b>unit</b> <b>testing</b> {{is an important}} phase that helps in early detection of bugs. A <b>unit</b> <b>test</b> <b>case</b> consists of two parts: a test input, which is often a sequence of method calls, and a test oracle, which is often {{in the form of}} assertions. The effectiveness of a <b>unit</b> <b>test</b> <b>case</b> depends on its test input as well as its test oracle because the test oracle helps in exposing bugs during the execution of the test input. The task of writing effective test oracles is not trivial as this task requires domain or application knowledge and also needs knowledge of the intricate details of the class under test. In addition, when developers write new <b>unit</b> <b>test</b> <b>cases,</b> much test code (including code in test inputs or oracles) such as method argument values is the same as some previously written test code. To assist developers in writing <b>test</b> code in <b>unit</b> <b>test</b> <b>cases</b> more efficiently, we have developed an Eclipse plugin for JUnit test cases, called UnitPlus, that runs in the background and recommends test-code pieces for developers to choose (and revise when needed) to put in test oracles or test inputs. The recommendation is based on static analysis of the class under test and already written <b>unit</b> <b>test</b> <b>cases.</b> We have conducted a feasibility study for our UnitPlus plugin with four Java libraries to demonstrate its potential utility...|$|R
40|$|Abstract — We {{describe}} {{a framework for}} randomized <b>unit</b> <b>testing,</b> and give empirical evidence that generating <b>unit</b> <b>test</b> <b>cases</b> randomly and then minimizing the failing test cases results in significant benefits. Randomized generation of <b>unit</b> <b>test</b> <b>cases</b> (sequences of method calls) {{has been shown to}} allow high coverage and to be highly effective. However, failing test cases, if found, are often very long sequences of method calls. We show that Zeller and Hildebrandt’s test case minimization algorithm significantly reduces the length of these sequences. We study the resulting benefits qualitatively and quantitatively, via a case study on found open-source data structures and an experiment on lab-built data structures. I...|$|R
40|$|Graphical User Interface (GUI) APplications (GAPs) are {{ubiquitous}} {{and provide}} various services. Since many GAPs are {{not designed to}} exchange information (i. e., interoperate), companies replace legacy GAPs with web services, {{that are designed to}} interoperate over the Internet. However, it is laborious and inefficient to create <b>unit</b> <b>test</b> <b>cases</b> to test the web services. We propose to demonstrate a SysteM for Application Reference Testing (SMART) novel approach for generating tests for web services from legacy GAPs. During demonstration of Smart we will show how this tool enables nonprogrammers to generate <b>unit</b> <b>test</b> <b>cases</b> for web services by performing drag-and-drop operations on GUI elements of legacy GAPs. We published a research paper that describes our approach and the results of the evaluation of Smart [2]...|$|R
40|$|In this paper, we {{demonstrate}} {{a method that}} uses the model transformation technology of MDA to generate <b>unit</b> <b>test</b> <b>cases</b> from a platform-independent model of the system. The method we propose is based on sequence diagrams. First we model the sequence diagram and then this model is automatically transformed into a general <b>unit</b> <b>test</b> <b>case</b> model (an xUnit model which is independent of a particular <b>unit</b> <b>testing</b> framework), using model-to-model transformations. Then model-to-text transformations are applied on the xUnit model to generate platform- specific (JUnit, SUnit etc.) test cases that are concrete and executable. We have implemented the transformations in a prototype tool based on the Tefkat transformation tool and MOFScript. The paper gives details of the tool and the transformations that we have developed. We have applied the method to a small example (ATM simulation) ...|$|R
40|$|We propose test-based pointcuts, a novel pointcut {{mechanism}} for AspectJ-like aspect-oriented programming languages. The {{idea behind the}} test-based pointcuts is to specify join points through <b>unit</b> <b>test</b> <b>cases</b> associated with the target program. The test-based pointcuts improve robustness and precision of pointcut languages. The test-based pointcuts are more robust against software evolution {{because they do not}} directly rely on identifier names in a target program. The test-based pointcuts are more precise because they can distinguish fine grained execution histories including conditional branches by comparing the runtime execution histories with recorded for ones of the <b>unit</b> <b>test</b> <b>cases.</b> This paper presents design and implementation of the test-based pointcuts as an extension of an AspectJ compiler. We evaluated robustness and runtime efficiency of test-based pointcuts through case studies that applied test-based pointcuts to several versions of practical application programs...|$|R
5000|$|Kolawa {{was granted}} 20 patents for {{software}} technologies he has invented. His patents include runtime memory error detection technology (Patent [...] and [...] - granted in 1998), statically analyzing source code quality using rules (Patent [...] - granted in 1999), and automated <b>unit</b> <b>test</b> <b>case</b> generation technology (Patent [...] and [...] - granted in 1998).|$|R
50|$|<b>Unit</b> <b>testing</b> is {{commonly}} automated, but {{may still be}} performed manually. The IEEE does not favor one over the other. The objective in <b>unit</b> <b>testing</b> is to isolate a unit and validate its correctness. A manual approach to <b>unit</b> <b>testing</b> may employ a step-by-step instructional document. However, automation is efficient for achieving this, and enables the many benefits listed in this article. Conversely, if not planned carefully, a careless manual <b>unit</b> <b>test</b> <b>case</b> may execute as an integration test case that involves many software components, and thus preclude the achievement of {{most if not all}} of the goals established for <b>unit</b> <b>testing.</b>|$|R
40|$|Randomized <b>unit</b> <b>test</b> <b>cases</b> {{can be very}} {{effective}} in detecting defects. In practice, however, failing test cases often comprise long sequences of method calls that are tiresome to reproduce and debug. We present a combination of static slicing and delta debugging that automatically minimizes the sequence of failure-inducing method calls. In a case study on the EiffelBase library, the strategy minimizes failing <b>unit</b> <b>test</b> <b>cases</b> on average by 96 %. This approach improves {{on the state of}} the art by being far more efficient: in contrast to the approach of Lei and Andrews, who use delta debugging alone, our case study found slicing to be 50 × faster, while providing comparable results. The combination of slicing and delta debugging gives the best results and is 11 × faster. Categories and Subject Descriptors: D. 2. 5 [Software Engineering] Testing and Debugging–Testing tools (e. g., data generators, coverage testing...|$|R
40|$|This paper proposes test-based pointcuts, a new aspect-oriented {{programming}} language construct that uses <b>unit</b> <b>test</b> <b>cases</b> as interface of crosscutting concerns. A test-based pointcut primarily specifies {{a set of}} test cases associated to a program. At execution time, it matches the join points that have the same execution history to {{the one of the}} specified test cases. The test-based approach improves pointcut definitions in two respects. First, test-based pointcuts are less fragile with respect to program changes because rather than directly relying on type and operation names in a program, they indirectly specify join points through <b>unit</b> <b>test</b> <b>cases,</b> which are easier to be kept up-to-date. Second, test-based pointcuts can discriminate execution histories without requiring to specify detailed execution steps, as they use test cases as abstractions of execution histories. With the abstractions, the second respect contributes to the first respect. We designed and implemented the test-based pointcuts as an extension to AspectJ, and confirmed, through an case study, test-based pointcuts are more robust against evolution when used for a practical application program...|$|R
40|$|Click on the DOI link {{to access}} the article (may not be free). This paper {{develops}} a Copula-based sampling method for data-driven prognostics and health management (PHM). The principal idea is to first build statistical relationship between failure time and the time realizations at specified degradation levels {{on the basis of}} off-line training data sets, then identify possible failure times for on-line <b>testing</b> <b>units</b> based on the constructed statistical model and available on-line testing data. Specifically, three technical components are proposed to implement the methodology. First of all, a generic health index system is proposed to represent the health degradation of engineering systems. Next, a Copula-based modeling is proposed to build statistical relationship between failure time and the time realizations at specified degradation levels. Finally, a sampling approach is proposed to estimate the failure time and remaining useful life (RUL) of on-line <b>testing</b> <b>units.</b> Two <b>case</b> studies, including a bearing system in electric cooling fans and a 2008 IEEE PHM challenge problem, are employed to demonstrate the effectiveness of the proposed methodology...|$|R
40|$|Significant {{progresses}} {{have been}} made on using symbolic execution to generate <b>unit</b> <b>test</b> suites, but existing frameworks are still struggling with several issues including poor performance on heap data, lack of coverage goals tied to the space of heap configurations, and lack of support for dealing with open object-oriented systems and generating mock objects. In this paper, we demonstrate how a static analysis feedback and <b>unit</b> <b>test</b> <b>case</b> generation framework, KUnit, built on the Bogor/Kiasan symbolic execution engine addresses these issues by: (a) providing an effective <b>unit</b> <b>test</b> <b>case</b> generation for sequential heap-intensive Java programs (whose computation structures are incomplete – open systems), (b) showing how the scope and cost of Kiasan/KUnit’s analysis and test case generation can be controlled via notions of heap configuration coverage, and (c) leveraging method contract information to better deal with open object-oriented systems and to support automatic mock object creation. In a broad experimental study on twenty-two Java data structure modules, we show that KUnit is able to: (a) achieve 100 % feasible branch coverage on almost all methods by using only small heap configurations, (b) improve on competing tools for coverage achieved, size of test suites, and time to generate test suites. 1...|$|R
40|$|Abstract—Assembling unit {{models to}} perform {{integration}} testing {{often lead to}} both state explosion and the repeated work of <b>unit</b> <b>testing.</b> To mitigate the above crux, we present a novel integration testing approach, which derives integration test cases from the composition of <b>unit</b> <b>test</b> <b>cases</b> instead of <b>unit</b> models. Thus, the integration testing is performed efficiently and the difficulties of model combination are alleviated. The results of case study show that the proposed method not only bears the equal fault-detecting ability with low computational complexity, but also offers more generality compared with the model assemble-based method; therefore the development cost could be reduced...|$|R
40|$|In a {{software}} development group of IBM Retail Store Solutions, {{we built a}} non-trivial sofflware system based on a stable standard specification using a disciplined, rigorous <b>unit</b> <b>testing</b> and build approach based on the test-driven development (TDD) practice. Using this practice, we reduced our defect rate by about 50 percent compared to a similar system that was built using an ad-hoc <b>unit</b> <b>testing</b> approach. The project completed on time with minimal development productivity impact. Additionally, the suite of automated <b>unit</b> <b>test</b> <b>cases</b> created via TDD is a reusable and extendable asset {{that will continue to}} improve quality over the lifetime of the software system. The test suite will be the basis for quality checks and will serve as a quality contract between all members of the team. ...|$|R
40|$|Abstract. <b>Unit</b> <b>testing</b> is {{essential}} in the agile context. A <b>unit</b> <b>test</b> <b>case</b> written long ago may uncover an error introduced only recently, at a time at which awareness of the test and the requirement it expresses may have long vanished. Popular <b>unit</b> <b>testing</b> frameworks such as JUNIT may then detect the error at little more cost than the run of a static program checker (compiler). However, unlike such checkers current <b>unit</b> <b>testing</b> frameworks can only detect the presence of errors, they cannot locate them. With EZUNIT, we present an extension to the JUNIT ECLIPSE plug-in that serves to narrow down error locations, and that marks these locations in the source code in {{very much the same}} way syntactic and typing errors are displayed. Because EZUNIT is itself designed as a framework, it can be extended by algorithms further narrowing down error locations. ...|$|R
40|$|AbstractWe {{consider}} a generalization {{of the classical}} group testing problem. Let us be given a sample contaminated with a chemical substance. We want to estimate the unknown concentration c of this substance in the sample. There is a threshold indicator which can detect whether the concentration {{is at least a}} known threshold. We consider both the case when the threshold indicator does not affect the <b>tested</b> <b>units</b> and the more difficult case when the threshold indicator destroys the <b>tested</b> <b>units.</b> For both <b>cases,</b> we present a family of efficient algorithms each of which achieves a good approximation of c using a small number of tests and of auxiliary resources. Each member of the family provides a different tradeoff between the number of tests and the use of other resources involved by the algorithm. Previously known algorithms for this problem use more tests than most of our algorithms do...|$|R
5000|$|Parasoft C/C++test is an {{integrated}} {{set of tools}} for testing C and C++ source code that software developers use to analyze, test, find defects, and measure the quality and security of their applications. It supports software development practices {{that are part of}} development testing, including static code analysis , dynamic code analysis, <b>unit</b> <b>test</b> <b>case</b> generation and execution, [...] code coverage analysis, regression testing, runtime error detection, requirements traceability, and code review. It's a commercial tool that supports operation on Linux, Windows, and Solaris platforms as well as support for on-target embedded testing and cross compilers.|$|R
40|$|AbstractThe study aims at {{investigating}} empirically {{the ability}} of a Quality Assurance Indicator (Qi), a metric that we proposed in a previous work, to predict different levels of <b>unit</b> <b>testing</b> effort of classes in object-oriented systems. To capture the <b>unit</b> <b>testing</b> effort of classes, we used four metrics to quantify various perspectives related to the code of corresponding <b>unit</b> <b>test</b> <b>cases.</b> Classes were classified, according to the involved <b>unit</b> <b>testing</b> effort, in five categories (levels). We collected data from two open source Java software systems (ANT and JFREECHART) for which JUnit test cases exist. In order to explore {{the ability of}} the Qi metric to predict different levels of the <b>unit</b> <b>testing</b> effort of classes, we decided to explore the possibility of using the Multinomial Logistic Regression (MLR) method. The performance of the Qi metric has been compared to the performance of three well-known source code metrics related respectively to size, complexity and coupling. Results suggest that the MLR model based on the Qi metric is able to accurately predict different levels of the <b>unit</b> <b>testing</b> effort of classes...|$|R
40|$|Abstract Background: Several studies {{report that}} test driven {{development}} (TDD) has {{effects on the}} software product e. g. code quality and developers’ productivity. In recent literature reviews, the impact of TDD on source code quality {{is seen as a}} more focused area in empirical research compared to <b>unit</b> <b>testing.</b> However, the quality of production code is tightly coupled with the quality of test code. Objective: The aim {{of this study is to}} investigate the impact of test driven development approach compared to an incremental test last development (ITLD), on <b>unit</b> <b>test</b> <b>case</b> quality. The impact is measured in terms of code coverage and mutation score metrics. The hypotheses test the differences in the quality of test cases produced using TDD and ITLD approaches. Method: We conducted an experiment in an industrial setting with 24 professionals in five consecutive days. Three programing tasks, i. e. one task using ITLD and two tasks using TDD, are selected for the experiment. We extracted <b>unit</b> <b>test</b> <b>case</b> quality attributes i. e. mutation score and code coverage from the data collected on each day of the experiment. For the code coverage, we used the metrics; instruction, branch, method, cyclomatic complexity and line coverage. The difference of mutation score and code coverage metrics are then evaluated using non-parametric significance tests. Results: The results indicate that except three metrics i. e. branch coverage, method coverage, mutation score, we could not find significant differences in terms of <b>unit</b> <b>test</b> <b>case</b> quality between the treatments. Subjects wrote test cases that cover more branches during TDD practice on a green-field (as a new development) task, compared to ITLD and TDD practice on a brown-field (modifications on an existing code base) task. In terms of method coverage, test cases produced using ITLD covered more methods than both TDD tasks; whereas no significant difference is found for method coverage between the both TDD tasks. In terms of mutation score, test cases written during TDD practice on a green-field task have more defect detection abilities than test cases written during ITLD practice and TDD practice on a brown-field task. Conclusion: Our finding are different from previous studies performed at academic settings. We believe that other factors (i. e. task’s complexity, experiment duration and participant’s interest towards tasks) could influence the results. Therefore, future studies could be designed by minimizing the impact of these factors to get more generalizable results...|$|R
40|$|Abstract- In this project, {{developing}} effective suites of <b>unit</b> <b>test</b> <b>cases</b> {{presents a}} number of challenges. Specifications of unit behavior are usually informal and are often incomplete or ambiguous, leading {{to the development of}} overly general or incorrect <b>unit</b> <b>tests.</b> This project will investigate strategies for amplifying the power and applicability of testing resources. The strategies will transform existing tests into new tests that add complementary testing capabilities to the validation process. The developed strategies will be unique in their treatment of tests as data. This will require the development of test representations that can be efficiently manipulated, and test transformations to realize operations that generate new and valuable tests. We see Carving as the first of our transformations, but many others will follow...|$|R
40|$|<b>Unit</b> <b>test</b> <b>cases</b> {{are focused}} and efficient. System tests are {{effective}} at exercising complex usage patterns. Differential <b>unit</b> <b>tests</b> (DUT) are {{a hybrid of}} <b>unit</b> and system <b>tests.</b> They are generated by carving the system components, while executing a system test case, that influence {{the behavior of the}} target unit, and then re-assembling those components so that the unit can be exercised as it was by the system test. We conjecture that DUTs retain some of the advantages of <b>unit</b> <b>tests,</b> can be automatically and inexpensively generated, and have the potential for revealing faults related to intricate system executions. In this paper we present a framework for automatically carving and replaying DUTs that accounts for a wide-variety of strategies, we implement an instance of the framework with several techniques to mitigate test cost and enhance flexibility, and we empirically assess the efficacy of carving and replaying DUTs. 1...|$|R
40|$|Abstract. Several {{approaches}} {{exist to}} automatically derive test cases that check the conformance {{of the implementation}} of abstract data types (ADTs) {{with respect to their}} specification. However, they lack support for the testing of implementations of ADTs defined by generic classes. In this paper, we present a novel technique to automatically derive, from specifications, <b>unit</b> <b>test</b> <b>cases</b> for Java generic classes that, in addition to the usual testing data, encompass implementations for the type parameters. The proposed technique relies on the use of Alloy Analyzer to find model instances for each test goal. JUnit test cases and Java implementations of the parameters are extracted from these model instances. ...|$|R
40|$|Embedded {{systems like}} {{those used in}} {{automobiles}} have two peculiar attributes - they are reactive systems where each reaction {{is influenced by the}} current state of the system, and their inputs come from small domains. We hypothesise that, because inputs come from small domains, random testing is likely to cover all values in the domain and hence have an effectiveness comparable to other techniques. We also hypothesise that because of the reactive nature long sequences of interactions will be important for testing effectiveness. To test these hypotheses we conducted three experiments on three pieces of code selected from an automotive application. The first two experiments were designed to compare the effectiveness of randomly generated test cases against test cases that achieve the modified condition decision coverage (MCDC) and also evaluate the impact of length of the test cases on effectiveness. The third experiment compares the effectiveness of handwritten test cases against randomly generated test cases of similar length. Our objective is to help practitioners choose an effective technique to test their systems. Our findings from the limited experiments indicate that random test case generation is as effective as manual test generation at the system level. However, for <b>unit</b> <b>testing</b> <b>test</b> <b>case</b> generation to achieve MCDC coverage is more effective than random generation. Combining <b>unit</b> <b>test</b> <b>cases</b> with system level testing increases effectiveness. Our final observation is that increasing the test case length improves the effectiveness of a test suite both at the unit and system level. ...|$|R
40|$|QuickCheck {{allows you}} to {{validate}} if your software has particular desired properties. These properties {{can be regarded as}} an abstraction over many <b>unit</b> <b>tests.</b> QuickCheck uses generated random input data to validate such a property. If QuickCheck finds a counterexample it becomes immediately clear what we are testing. If however all tests pass it is not immediately clear what we have tested, since we don’t see the actual generated test cases. In this case it is good to think about what we have actually tested. QuickCheck offers the possibility to gather statistics about the test cases, which is very insightful. Still, after inspecting the test data distribution many QuickCheck users wonder if a particular <b>unit</b> <b>test</b> <b>case</b> has been tested. Often a property is developed with a certain set of <b>unit</b> <b>tests</b> in mind. We have developed a tool that check if a given <b>unit</b> <b>test</b> can be generated by a property. This tool helps in understanding a property, making it easier to see what it really tests and hence judge its quality...|$|R
30|$|In {{the context}} of <b>unit</b> <b>test</b> <b>case</b> {{generation}} for programs developed according to the Object-Oriented Programming (OOP) paradigm, this instance {{can be used to}} generate test cases for a class that has one attribute (parameter) which can take 2 values (21), 1 attribute that can take 4 values (41), another attribute that can take 5 values (51), ⋯, 1 attribute that can take 6 values (61). In the system and acceptance testing context, this same sample can be used to identify test scenarios (test objectives) in a model-based test case generation approach (Santiago Júnior 2011; Santiago Júnior and Vijaykumar 2012). In both cases, the test suites must meet the criteria of pairwise testing (t= 2) where each combination of 2 values of all parameters must be covered. Note that these samples were randomly selected and they cover a wide range of combinations of parameters, values, and strengths to be selected for very simple but also more complex case studies with different <b>testing</b> levels (<b>unit,</b> system, acceptance, etc.).|$|R
40|$|Software testing plays a very {{important}} role in the development process because it is an essential mean of providing reliability and quality to any software. Designing and execution of test cases consumes lot of time since it requires planning and resources manually. To overcome this it is highly required to automate the generation of test cases. UML, is a standard modeling language which supports object oriented technology. It is generally used to depict the analysis and design specification of software development. UML models are an essential and a rich source of information for test case design. In this thesis, we present a testing methodology which is used to generate the <b>unit</b> <b>test</b> <b>cases</b> from UML state chart diagram for an industrial application. Firstly, we discussed about the CIM, an industrial application which is used for transforming an Idea through a collaborative interaction into such a state so that the idea becomes patentable. This application provides a platform to the people in industry to encourage and enhance their invention skills for an enterprise. Next we proposed a testing approach to generate <b>unit</b> <b>test</b> <b>cases</b> for the phases of CIM application using UML state chart diagram. UML model provides a lot of information which can be used for testing. In our approach, firstly the state chart diagram is constructed for CIM application. Then the adjacency matrix is generated and subsequently transform the state chart diagram into a UML state chart graph. Then we traverse the graph using adjacency matrix by using DFS. Therefore test sequences are generated. Then we apply the node coverage minimization technique to generate the test cases so that maximum coverage is achieved...|$|R
30|$|The fourth version 6 was {{completed}} in July 2016 and was evaluated through experiments with thirteen libraries with the goal to reduce the code execution time of these libraries. The list of libraries that were observed is in Table 1. They were chosen based on size (measured in lines of code and presented in the LOC column), their use (measured in downloads per month), {{as well as the}} coverage and size of its suite of <b>unit</b> <b>tests.</b> The Downloads column shows the number of downloads for each library in December 2016. The suite size is measured in the number of <b>unit</b> <b>test</b> <b>cases</b> reported in the Tests column, while the coverage represents the percentage of library code statements exercised during the test case suite execution (Coverage column). As the whole process uses <b>unit</b> <b>testing</b> as an oracle, this is the primary criterion for selecting new libraries (code) for analysis. Little code coverage can generate false positives, i.e., removal or alteration of valid but untested code that will only be discovered during the results of the human analysis.|$|R
50|$|Ideally, the {{software}} must completely satisfy {{the set of}} requirements. From design, each requirement must be addressed in every single document in {{the software}} process. The documents include the HLD, LLD, source codes, <b>unit</b> <b>test</b> <b>cases,</b> integration test cases and the system test cases. In a requirements traceability matrix, the rows will have the requirements. The columns represent each document. Intersecting cells are marked when a document addresses a particular requirement with information related to the requirement ID in the document. Ideally, if every requirement is addressed in every single document, all the individual cells have valid section ids or names filled in. Then we know that every requirement is addressed. If any cells are empty, it represents that a requirement has not been correctly addressed.|$|R
40|$|In {{this paper}} an attempt {{is made to}} {{implement}} system test cases and software metrics with aid of GUI and several applications were developed to calculate the metrics and performance of the each test case, which {{can also be used}} as a stand alone method. Further an emphasis is made on different relationships with system test case and software metrics, which will helps to determine quality and quantity of software attributes measured with regard of Object-Oriented Software Development Life Cycle. We demonstrate a suggestive evaluation of system test cases in OO Systems. Developing effective suites of <b>unit</b> <b>test</b> <b>cases</b> presents a number of challenges. Specifications of unit behavior are usually informal and are often incomplete or ambiguous, leading to the development of overly general o...|$|R
50|$|When a {{particular}} problem is identified, the programs will be debugged and the fix will be done to the program. To {{make sure that the}} fix works, the program will be tested again for that criterion. Regression tests will make sure that one fix does not create some other problems in that program or in any other interface. So, a set of related test cases may have to be repeated again, to make sure that nothing else is affected by {{a particular}} fix. How {{this is going to be}} carried out must be elaborated in this section. In some companies, whenever there is a fix in one <b>unit,</b> all <b>unit</b> <b>test</b> <b>cases</b> for that <b>unit</b> will be repeated, to achieve a higher level of quality.|$|R
40|$|The {{increasing}} {{size and}} complexity of modern software-intensive systems present novel challenges when engineering high-integrity artifacts within aggressive budgetary constraints. Among these challenges, ensuring confidence in the engineered system, through validation and verification activities, represents the high cost item on many projects. The expensive nature of engineering high-integrity systems using traditional approaches can be partly attributed {{to the lack of}} analysis facilities during the early phases of the lifecycle, causing the validation and verification activities to begin too late in the engineering lifecycle. Other challenges include the management of complexity, opportunities for reuse without compromising confidence, and the ability to trace system features across lifecycle phases. The use of models as a specification mechanism provides an approach to mitigate complexity through abstraction. Furthermore, if the specification approach has formal underpinnings, the use of models can be leveraged to automate engineering activities such as formal analysis and test case generation. The research presented in this thesis proposes an engineering framework which addresses the high cost of validation and verification activities through specification-based system engineering. More specifically, the framework provides an integrated approach to embedded real-time system engineering which incorporates specification, simulation, formal verification, and test-case generation. The framework aggregates the state-of-the-art in individual software engineering disciplines to provide an end-to-end approach to embedded real-time system engineering. The key aspects of the framework include: * A novel specification language, the Timed Abstract State Machine (TASM) language, which extends the theory of Abstract State Machines (ASM). (cont.) The TASM language is a literate formal specification language which can be applied and multiple levels of abstraction and which can express the three key aspects of embedded real-time systems - function, time, and resources. * Automated verification capabilities achieved through the integration of mature analysis engines, namely the UPPAAL tool suite and the SAT 4 J SAT solver. The verification capabilities provided by the framework include completeness and consistency verification, model checking, execution time analysis, and resource consumption analysis. * Bi-directional traceability of model features across levels of abstraction and lifecycle phases. Traceability is achieved syntactically through archetypical refinement types; each refinement type provides correctness criteria, which, if met, guarantee semantic integrity through the refinement. * Automated test case generation capabilities for <b>unit</b> <b>testing,</b> integration testing, and regression <b>testing.</b> <b>Unit</b> <b>test</b> <b>cases</b> are generated to achieve TASM specification coverage through the rule coverage criterion. Integration test case generation is achieved through the hierarchical composition of <b>unit</b> <b>test</b> <b>cases.</b> Regression test case generation is achieved by leveraging the bi-directional traceability of model features. The framework is implemented into an integrated tool suite, the TASM toolset, which incorporates the UPPAAL tool suite and the SAT 4 J SAT solver. The toolset and framework are evaluated through experimentation on three industrial case studies - an automated manufacturing system, a "drive-by-wire" system used at a major automotive manufacturer, and a scripting environment used on the International Space Station. by Martin Ouimet. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, Dept. of Aeronautics and Astronautics, 2008. Includes bibliographical references (v. 2, p. 517 - 545) ...|$|R
40|$|Test Driven Development (TDD) is a {{software}} development practice in which <b>unit</b> <b>test</b> <b>cases</b> are incrementally written prior to code implementation. We ran {{a set of}} structured experiments with 24 professional pair programmers. One group developed a small Java program using TDD while the other (control group), used a waterfall-like approach. Experimental results, subject to external validity concerns, tend to indicate that TDD programmers produce higher quality code because they passed 18 % more functional black-box test cases. However, the TDD programmers took 16 % more time. Statistical analysis of {{the results showed that}} a moderate statistical correlation existed between time spent and the resulting quality. Lastly, the programmers in the control group often did not write the required automated test cases after completing their code. Hence it could be perceived that waterfall-like approaches do not encourage adequate testing. This intuitive observation supports that perception that TDD has the potential for increasing the level of <b>unit</b> <b>testing</b> in the software industry...|$|R
5000|$|Parasoft {{develops}} automated defect prevention {{technologies that}} support the Automated Defect Prevention methodology developed by Adam Kolawa. These technologies automate a number of defect prevention practices for Java, C and C++, and [...]NET. The static code analysis practice identifies coding issues that lead to security, reliability, performance, and maintainability issues later on. In 1996, Parasoft submitted a patent application for their rule-based static code analysis. Since then, the original static analysis technology has been extended to include security static analysis, data flow analysis, and software metrics. In 1996, Parasoft submitted patent applications for technology that automatically generates <b>unit</b> <b>test</b> <b>cases.</b> [...] Since then, the original <b>unit</b> <b>testing</b> technology has been extended to include code coverage analysis, regression testing, and traceability. The peer code review practice involves manually inspecting source code to examine algorithms, review design, and search for subtle errors that automated tools cannot detect. Although the peer inspection itself cannot be automated, peer code reviews preparation, notification, and tracking can be automated.|$|R
40|$|Software {{testing is}} an {{integral}} part of the software development process. Some software developers, particularly those who use the Extreme Programming testdriven development practice, continuously write automated tests to verify their code. We present a tool to complement the feedback loops created by continuous testing. The tool combines static source code metrics with dynamic test coverage for use throughout the development phase to predict a reliability estimate based on a linear combination of these values. Implemented as an open source plug-in to the Eclipse IDE, the tool facilitates the rapid transition between <b>unit</b> <b>test</b> <b>case</b> completions and testing feedback. The color-coded results highlight inadequate testing efforts as well as weaknesses in overall program structure. To illustrate the tool’s efficacy, we share the results of its use on university software engineering course projects. 1...|$|R
40|$|Evolutionary {{structural}} testing, {{an approach}} to automatically generating relevant <b>unit</b> <b>test</b> <b>cases,</b> encounters difficulties when the tested software contains boolean variables. This issue, known as the flag problem, has been studied by many researchers. However, previous work {{does not address the}} issue of function-assigned flags which constitutes a special type of the flag problem that often occurs in the context of object-orientation. This paper elaborates on a new approach to the flag problem that can also handle function-assigned flags while being applicable to the conventional flag problem, as well. It relies on a code transformation that leads to an improved fitness landscape which provides better guidance to the evolutionary search. We present four case studies including a fitness landscape analysis and empirical results. The results show that the suggested code transformation improves evolutionary structural testing in the presence of function-assigned flags. 1...|$|R
