14|10000|Public
40|$|In this paper, we {{consider}} a state estimation problem over a bandwidth limited network. A sensor network consisting of N sensors {{is used to}} observe the states of M plants, but only p ≤ N sensors can transmit their measurements to a centralized estimator at each time. Therefore a suitable scheme that schedules the proper sensors to access the network at each time so that the <b>total</b> <b>estimation</b> <b>error</b> is minimized is required. We propose four different sensor scheduling schemes. The static and stochastic schemes assume no feedback from the estimator to the scheduler, while the two dynamic schemes, Maximum Error First (MEF) and Maximum Deduction First (MDF) assume such feedback is available. We compare the four schemes via some examples and show MEF and MDF schemes perform better than the static and stochastic schemes, which demonstrates that feedback {{can play an important}} role in this remote state estimation problem. We also show that MDF performs better than MEF as MDF considers the <b>total</b> <b>estimation</b> <b>error</b> while MEF considers the individual estimation error. © 2007 IEEE...|$|E
30|$|SEATS Table 5.1 (item 35 in Table 9) {{shows the}} period-to-period growth {{estimation}} error variance for the concurrent estimator, {{so the first}} differences of the final estimation error, the revision error and the <b>total</b> <b>estimation</b> <b>error.</b> For example, the final estimation error, denoted f_t, has been analysed in Sect. 4.6. We need to compute var(∇ f_t) which is equal to var(f_t)+var(f_t- 1)- 2 cov(f_t, f_t- 1) = 2 × 0.01095 × (1 - 0.2503) = 0.016, see cell AF 40.|$|E
30|$|SEATS Table 5.4 (item 38 in Table 10) {{shows the}} annual growth {{estimation}} error variance for the concurrent estimator, {{which is a}} first seasonal difference of the final estimation error, {{the sum of the}} revision error and the <b>total</b> <b>estimation</b> <b>error.</b> As indicated, the numbers are to be multiplied by 10. For example, the first one, denoted f_t, has been analysed in Sect. 4.6. We need to compute var(∇ _ 12 f_t) which is equal to var(f_t)+var(f_t- 12)- 2 cov(f_t, f_t- 12)= 2 × 0.01095 × (1 + 0.0001) = 0.0219, see cell AF 41.|$|E
25|$|The {{conclusion}} {{from this}} hypothetical example is that measurements should be combined {{if one is}} interested in minimizing their total MSE. For example, in a telecommunication setting, {{it is reasonable to}} combine channel tap measurements in a channel estimation scenario, as the goal is to minimize the <b>total</b> channel <b>estimation</b> <b>error.</b> Conversely, there could be objections to combining channel estimates of different users, since no user would want their channel estimate to deteriorate in order to improve the average network performance.|$|R
40|$|Abstract—In this {{technical}} note a new algorithm for state estimation is {{proposed in the}} form of a multi-agent network based on a synergy between local Kalman filters and a dynamic consensus strategy between the agents. It is shown that it is possible, under general conditions concerning local resources and the network topology, to achieve asymptotic stability of the whole estimation algorithm by a proper choice of the consensus gains. It is demonstrated that the consensus gains can be obtained by minimizing the <b>total</b> mean-square <b>estimation</b> <b>error.</b> Capabilities of the network to achieve reduction of the measurement noise influence are also discussed. Index Terms—Consensus strategy, decentralized estimation, multi-agent systems, optimization, overlapping decompositions, stability analysis. I...|$|R
40|$|This paper {{develops}} a commuter location model {{able to explain}} and simulate residential location changes of commuters that result from transport improvements. The core model {{is based on the}} assumption of constant commuting time, while two extensions incorporate substitution possibilities having an upward effect on <b>total</b> commuting time. <b>Estimation</b> <b>errors</b> of the residential location of the working population with the existing transport system are limited to 7 %. With the extended model, the impacts on commuting and residential choice are investigated for six higher speed rail connections between Amsterdam, located in the urban core of the Netherlands, and Groningen, located in its rural periphery. The model outcomes strongly influenced the public policy debate in the Netherlands. ...|$|R
40|$|This paper {{investigates the}} problem of robust filter design for a class of {{nonlinear}} stochastic systems with state-dependent noise. The state and measurement are corrupted by stochastic uncertain exogenous disturbance and the dynamic system is modeled by Itô-type stochastic differential equations. For this class of nonlinear stochastic systems, the robust H∞ filter can be designed by solving linear matrix inequalities (LMIs). Moreover, a mixed H 2 /H∞ filtering problem is also solved by minimizing the <b>total</b> <b>estimation</b> <b>error</b> energy when the worst-case disturbance is considered in the design procedure. A numerical example is provided to illustrate {{the effectiveness of the}} proposed method...|$|E
40|$|Vertical scaling is {{necessary}} to facilitate comparison of scores from test forms of different difficulty levels. It is widely used to enable the tracking of student growth in academic performance over time. Most previous studies on vertical scaling methods assume relatively long tests and large samples. Little is known about their performance when the sample is small or the test is short, challenges that small testing programs often face. This study examined effects of sample size, test length, and choice of item response theory (IRT) models {{on the performance of}} IRT-based scaling methods (concurrent calibration, separate calibration with Stocking–Lord, Haebara, Mean/Mean, and Mean/Sigma transformation) in linear growth estimation when the 2 -parameter IRT model was appropriate. Results showed that IRT vertical scales could be used for growth estimation without grossly biasing growth parameter estimates when sample size was not large, as long as the test was not too short (≥ 20 items), although larger sample sizes would generally increase the stability of the growth parameter estimates. The optimal rate of return in <b>total</b> <b>estimation</b> <b>error</b> reduction as a result of increasing sample size appeared to be around 250. Concurrent calibration produced slightly lower <b>total</b> <b>estimation</b> <b>error</b> than separate calibration in the worst combination of short test length (≤ 20 items) and small sample size (n ≤ 100), whereas separate calibration, except {{in the case of the}} Mean/Sigma method, produced similar or somewhat lower amounts of total error in other conditions...|$|E
40|$|Abstract—A novel {{two-stage}} {{frequency domain}} channel es-timation method especially {{suitable for the}} estimation of long channels such as ultra wide band channels is proposed. The proposed method can efficiently use the sequences with closed form analytical expressions such as the Legendre sequences. (The suggested method {{does not require a}} computationally intense search for good training sequences which is infeasible for long training sequences.) The method is shown to present a minor improvement in the <b>total</b> <b>estimation</b> <b>error</b> variance when compared with the conventional single stage frequency domain channel estimation. In addition, the proposed method has a very efficient time domain implementation requiring at most...|$|E
40|$|Abstract: In this paper, {{we examine}} the optimal {{quantization}} of signals for system identification. We deal with memoryless quantization for the output signals and derive the optimal quantization schemes. The objective functions are the errors of least squares parameter estimation subject to a constraint {{on the number of}} subsections of the quantized signals or the expectation of the optimal code length for either high or low resolution. In the high-resolution case, the optimal quantizer is found by solving Euler–Lagrange’s equations and the solutions are simple functions of the probability densities of the regressor vector. In order to clarify the minute structure of the quantization, the optimal quantizer in the low resolution case is found by solving recursively a minimization of a one-dimensional rational function. The solution has the property that it is coarse near the origin of its input and becomes dense away from the origin in the usual situation. Finally the required quantity of data to decrease the <b>total</b> parameter <b>estimation</b> <b>error,</b> caused by quantization and noise, is discussed...|$|R
40|$|Using the Cram'er-Rao bound, we find {{training}} sequences, which {{when used}} in conjunction with an efficient estimator for a discrete-time multipath channel with additive white Gaussian noise yield the least possible mean square error in the channel estimate. We additionally show that the performance of efficient joint estimation of multiuser channels is limited by the corresponding single user case. Keywords [...] -Multi-path channel estimation, Training sequences, Cram'er-Rao bound. I. Introduction A usual requirement for a digital communications receiver to provide good performance, for signals affected by a multipath channel, is the availability of a good estimate of this channel. It is also not uncommon for known periodic training sequences to be used to assist in this estimation process. In [1], optimal sequences (in terms of <b>total</b> mean square <b>estimation</b> <b>error)</b> for the maximum likelihood (ML) estimator are found. It is well known that ML estimators are asymptotically efficient (as the [...] ...|$|R
40|$|A novel {{framework}} of learning-based super-resolution is proposed by employing {{the process of}} learning from the <b>estimation</b> <b>errors.</b> The <b>estimation</b> <b>errors</b> generated by different learning-based super-resolution algorithms are statistically shown to be sparse and uncertain. The sparsity of the <b>estimation</b> <b>errors</b> means most of <b>estimation</b> <b>errors</b> are small enough. The uncertainty of the <b>estimation</b> <b>errors</b> means the location of the pixel with larger <b>estimation</b> <b>error</b> is random. Noticing the prior information about the <b>estimation</b> <b>errors,</b> a nonlinear boosting process of learning from these <b>estimation</b> <b>errors</b> is introduced into the general {{framework of}} the learning-based super-resolution. Within the novel framework of super-resolution, a low-rank decomposition technique is used to share the information of different super-resolution estimations and to remove the sparse <b>estimation</b> <b>errors</b> from different learning algorithms or training samples. The experimental results show the effectiveness and the efficiency of the proposed framework in enhancing the performance of different learning-based algorithms...|$|R
40|$|In the {{production}} function approach, an accurate output gap assessment requires a careful {{evaluation of the}} total factor productivity (TFP) cycle. We build a common cycle model that links TFP to capacity utilization and we show that, in {{almost all of the}} pre-enlargement EU countries, using information about capacity utilization reduces both the <b>total</b> <b>estimation</b> <b>error</b> and the revisions in real-time estimates of the concurrent TFP cycle compared to a univariate decomposition. We also argue that relaxing the constant drift hypothesis in favour of a non-linear specification helps to offset a general tendency to underestimate the TFP cycle in the last decade. JRC. G. 1 -Scientific Support to Financial Analysi...|$|E
40|$|The aim of {{the present}} study was to examine if a {{correlation}} between the autonomic nervous system (ANS) and subjective time estimation exists. Part of the study was also to test the influence of external stimuli activating the ANS like hypobaric hypoxia, for example in high altitudes, on the subjective time estimation. 71 healthy male subjects underwent the test in sealevel. 21 subjects where tested in the high altitude trials. Heart rate (HR), respiratory frequency (RF), ratio (HR/RF), heart rate variability and respiratory frequency variability where measured before and during the tests, to determine the ANS status. The bloodpressure was measured once before the tests. The parameters measured while resting (before the tests) where very important. The subjects where asked to reproduce empty intervals 6 - 10 times in two different trials. In the first trial the interval length was from 600 ms to 2000 ms (short intervals) and in the second trial they where from 2000 ms to 6000 ms (long intervals). To quantify and qualify the time estimation, the percental estimation error (PEE), the <b>total</b> <b>estimation</b> <b>error</b> (TEE) and the time estimation pattern were determined. The results showed a significant correlation between the respiratory frequency during resting (rest-respiratory frequency) and the <b>total</b> <b>estimation</b> <b>error</b> for the short intervals. Subjects with a higher rest-respiratory frequency had higher total estimation errors. To confirm these results the subjects where subdivided with regards to their <b>total</b> <b>estimation</b> <b>error</b> and their rest-respiratory frequency. The same correlations were found. The pattern analysis of the estimated intervals showed that nearly all higher total estimation errors in subjects with higher rest-respiratory frequency, resulted from an underestimation of the intervals. In the altitude a activation of the heart rate was seen. The respiratory frequency was less influenced from the altitude in this case. It was acictivated by the estimation test. In this case the pattern analysis showed a shift from underestimations at sealevel to overestimations at altitude and afterwords. These results do support the hypothesis that the autonomic nervous system (ANS) has an influence on the subjective time estimation. Moreover we learned that we have an access to the ANS and the subjective time estimation throug the respiratory frequency. This could be an advantage in the working medicin. It would be possible to determine which subjects are especially qualified or are endangered (because of high accident rates) on special tasks during work. In this case the pattern analysis of the estimated intervals is verry important, because there is a big difference between under and- overestimation, especially to avoid accidents while working...|$|E
30|$|In fixed {{distribution}} scenario, the simulations {{show that}} the proposed joint allocation method can meet the MSE requirements with fewer transmitters and achieve the smaller <b>total</b> <b>estimation</b> <b>error</b> than other methods. By adjusting the estimation MSE requirements, the proposed method can control target number more flexibly when ensuring the demand of the key target. Compared with transmitted power, the signal time has more impact on the estimation performance. In addition, the random distribution scenario further validates {{the superiority of the}} proposed allocation method, which indicates that more resources like transmitters are needed for more targets or lower MSE requirements. The extensive random experiments provide the estimation performance and transmitter number for different targets number and estimation requirements, which can contribute to the evaluation on the traceability for given system sources and tracking task in fixed area in practical application.|$|E
30|$|In addition, a {{conventional}} scheme, such as FEC metric masking, {{does not take}} into account the channel <b>estimation</b> <b>error</b> for calculating LLRs. Channel <b>estimation</b> <b>error</b> is known to cause intersymbol interference (ISI) and degradation of BER performance [5, 6]. Therefore, it is required to compensate the effect of channel <b>estimation</b> <b>error.</b> In [7, 8], LLR that takes into account the channel <b>estimation</b> <b>error</b> is proposed under single carrier transmission. It is shown that considering channel <b>estimation</b> <b>error</b> for LLR results in better BER performance.|$|R
30|$|In case of [2], {{the average}} <b>{{estimation}}</b> <b>error</b> rate was 3 %, so our assumed <b>estimation</b> <b>error</b> rate {{was higher than}} 3 %. Similarly, for [10, 12], the estimation rate was assumed based on their average <b>estimation</b> <b>error</b> rate.|$|R
3000|$|... [...]) {{represents}} the <b>estimation</b> <b>error</b> which vanishes asymptotically as T→∞. The <b>estimation</b> <b>error</b> ε(T)(α;n)δ(τ−n T [...]...|$|R
30|$|The <b>total</b> <b>estimation</b> <b>error</b> of the {{concurrent}} estimator t_t defined by (4.13) {{is the sum}} of the final estimation error f_t and of the revision error r_t, which are uncorrelated. Hence, the covariance at lag j of t_t {{is the sum of}} the covariance at lag j of f_t and of the covariance at lag j of r_t. Thus the variance of t_t is the sum of the variances of f_t and of r_t: V_t=V_f+V_r. It is computed in cell AN 36 of worksheet Main and is equal to V_t= 0.01369 or 0.05871 in units of V. The correlations are deduced in the range from AN 1 to AN 37 as weighted averages of the autocorrelations of the final estimation errors and the revisions with the respective variances as weights. This corresponds to the contents of item 28 in Table 7. For example, the first-order autocorrelation equals 0.1002 and the autocorrelation for lag 12 is 0 with 4 decimals.|$|E
40|$|In today’s {{design of}} {{embedded}} systems, the power dissipation {{has become an}} important constraint. Early in the design stage, several decision are made which have great influence on the power in the final design. To take the right decisions the total design space should be explored, and investigated what the power usages are. For this a fast power estimation framework is required. In this thesis a first step is made {{and the power of}} a single microprocessor is estimated. The power framework is based on a signature based model. A signature is an abstract execution profile of an application event. This signature is mapped onto the components, where the power consumed by a single access is already acquired using a power model. The usage counts are completed by the usage counts of the memory hierarchy which are calculated with help of the miss rates of the caches. With the access counts the power can be calculated directly. According to the performed evaluation, the signature based method is a substantial improvement for the evaluation time while the <b>total</b> <b>estimation</b> <b>error</b> still remains reasonable. During testing the maximal erro...|$|E
40|$|Abstract: The {{concepts}} of potential {{growth and the}} output gap are important components in assessing the business cycle and productive capacity of an economy. However, being unobservable, these measures must be estimated. The Fiscal Compact will result in these concepts being used to judge EU Member States adherence to budgetary rules. Therefore, {{it is vital that}} the methods applied for their estimation are as accurate as possible. A bivariate Kalman Filter (KF) model using capacity utilisation (CU) as the second series has been proven to produce more reliable estimates of the Total Factor Productivity (TFP) cycle than the Hodrick Prescott (HP) filter methodology formerly used for this task. However, CU data is no longer collected in Ireland. Given the large turning point in the TFP series {{as a result of the}} financial crisis, this may no longer be the first-best approach for future TFP cycle estimation. This paper compares the existing method to an approach which uses an aggregated Purchasing Managers ’ Index (PMI) series as the second series in the bivariate KF model. This approach has the advantage that PMI data is collected on an on-going basis. The results show that PMI shares a common cycle with TFP, and that this new approach leads to a reduction in the <b>total</b> <b>estimation</b> <b>error</b> variance and revisions required to TFP cycle estimates. ...|$|E
40|$|Millimeter wave is a {{promising}} {{technology for the}} next generation of wireless systems. As it is well-known for its high path loss, the systems working in this spectrum tend to exploit the shorter wavelength to equip the transceivers with a large number of antennas to overcome the path loss issue. The large number of antennas leads to large channel matrices and consequently a challenging channel estimation problem. The channel estimation algorithms that have been proposed so far either neglect the probability of <b>estimation</b> <b>error</b> or require a high feedback overload from receivers to ensure the target probability of <b>estimation</b> <b>error.</b> In this paper, we propose a multi-stage adaptive channel estimation algorithm called robust adaptive multi-feedback (RAF). The algorithm is based on using the estimated channel coefficient to predict a lower bound for the required number of measurements. Our simulations demonstrate that compared with existing algorithms, RAF can achieve the desired probability of <b>estimation</b> <b>error</b> while on average reducing the feedback overhead by 75. 5 % and the <b>total</b> channel <b>estimation</b> time by 14 %...|$|R
40|$|International audienceComputational {{models can}} help {{understand}} the hemodynamics of the coronary circulation, {{which is of}} the upmost importance to help clinicians before, during and after a {{coronary artery bypass graft}} surgery. In this paper, we propose a multiobjective optimization method for parameter estimation of a computational model representing the coronary circulation on patients with a triple vessel disease. This estimation was not based on any assumption regarding the development of the collateral circulation, like in previous works. Indeed, the collateral development of a given patient is estimated through the model parameters. Parameter estimation was performed using clinical data from three patients, obtained before and during an off-pump coronary artery bypass graft surgery (CABG). Results showed a better performance when comparing the simulation with clinical data, since the <b>total</b> <b>error</b> <b>estimation</b> for three patients was reduced by 40 ± 22 %. Moreover, the proposed method provides new insight regarding the heterogeneous configuration of the alternative collateral vessels...|$|R
40|$|In {{statistical}} pattern recognition, {{parameters of}} distributions are usually estimated from training samples. It {{is well known}} that shortage of training samples causes <b>estimation</b> <b>errors</b> which reduce recognition accuracy. By studying <b>estimation</b> <b>errors</b> of eigenvalues, various methods of avoiding recognition accuracy reduction have been proposed. However, <b>estimation</b> <b>errors</b> of eigenvectors have not been considered enough. In this paper, we investigate <b>estimation</b> <b>errors</b> of eigenvectors to show these errors are another factor of recognition performance reduction. We propose a new method for modifying eigenvalues in order to reduce bad influence caused by <b>estimation</b> <b>errors</b> of eigenvectors. Effectiveness of the method is shown by experimental results. 1...|$|R
40|$|The spatial {{behavior}} of numerous fishing fleets is nowadays well documented thanks to satellite Vessel Monitoring Systems (VMS). Vessel positions are recorded on a frequent and regular basis which opens promising perspectives for improving fishing effort estimation and management. However, no specific information is provided {{on whether the}} vessel is fishing or not. To answer that question, existing works on VMS data usually apply simple criteria (e. g. threshold on speed). Those simple criteria generally focus in detecting true positives (a true fishing set detected as a fishing set); conversely, estimation errors are given no attention. For our case study, the Peruvian anchovy fishery, those criteria overestimate {{the total number of}} fishing sets by 182 %. To overcome this problem an artificial neural network (ANN) approach is presented here. In order to set both the optimal parameterization and use "rules" for this ANN, we perform an extensive sensitivity analysis on the optimization of (1) the internal structure and training algorithm of the ANN and (2) the "rules" used for choosing both the relative size and the composition of the databases (DBs) used for training and inferring with the ANN. The "optimized" ANN greatly improves the estimates of the number and location of fishing events. For our case study, ANN reduces the <b>total</b> <b>estimation</b> <b>error</b> on the number of fishing sets to 1 % (in average) and obtains 76 % of true positives. This spatially explicit information on effort, provided with error estimation, should greatly reduce misleading interpretations of catch per unit effort and thus significantly improve the adaptive management of fisheries. While fitted on Peruvian anchovy fishery data, this type of neural network approach has wider potential and could be implemented in any fishery relying on both VMS and at-sea observer data. In order to increase the accuracy of the ANN results, we also suggest some criteria for improving sampling design by at-sea observers and VMS data...|$|E
30|$|The {{observation}} matrix in (30) {{is derived}} from the channel <b>estimation</b> <b>error</b> δ. We evaluate how the channel <b>estimation</b> <b>error</b> affects the average throughput.|$|R
30|$|The {{average number}} of rounds to detect {{malicious}} node is very stable when the position <b>estimation</b> <b>error</b> is within 4 units of radio range. Recall that most positioning estimate algorithms have the <b>estimation</b> <b>error</b> around 1 unit of radio range. Thus, {{the performance of the}} proposed scheme is stable given realistic positioning <b>estimation</b> <b>errors.</b>|$|R
40|$|Abstract — Channel <b>estimation</b> <b>error</b> {{problem is}} among the main causes of {{performance}} degradation in wireless networks. In this paper, we investigate the impact of cooperative communications on mitigating the effect of channel <b>estimation</b> <b>error.</b> Two main performance criteria, namely, the traditional outage probability and the proposed signal-to-noise ratio (SNR) gap ratio, are utilized to characterize such impact. The SNR gap ratio measures {{the reduction in the}} SNR due to channel <b>estimation</b> <b>error.</b> Taking into consideration the channel <b>estimation</b> <b>error,</b> we show that the outage probability is reduced by utilizing cooperative transmission. We also show that cooperative transmission results in lower SNR gap ratio compared to that of the direct transmission. Thus, cooperative transmission is less susceptible to the effect of channel <b>estimation</b> <b>error</b> compared to direct transmission. Finally, we illustrate that increasing the number of cooperating relays reduces the effect of the channel <b>estimation</b> <b>error</b> more. I...|$|R
3000|$|... b To {{include the}} channel <b>estimation</b> <b>error</b> in the {{proposed}} model, H can be replaced with H[*]+[*]N in Equation 9, where N denotes the channel <b>estimation</b> <b>error</b> matrix.|$|R
3000|$|... {{denotes the}} {{root mean square}} (rms) <b>estimation</b> <b>error,</b> which {{reflects}} the degree of imperfect self-information removal due to <b>estimation</b> <b>error</b> at the destination end. The case with [...]...|$|R
3000|$|In [7] it {{has been}} shown that the effect of channel <b>estimation</b> <b>errors</b> on various decoder and {{detection}} algorithms in OFDM receivers can be well modelled by treating the <b>estimation</b> <b>error</b> as an additional white noise contribution at the receiver, with a variance given by the <b>estimation</b> <b>error</b> variance. Therefore, in Figure 8 we show the channel estimation results in terms of SNR offset due to channel <b>estimation</b> <b>errors</b> at the receiver. This performance measure makes the results directly comparable to the SNR gains and losses due to different choices of number of subcarriers [...]...|$|R
40|$|Abstract—Channel <b>estimation</b> <b>error</b> and cochannel {{interference}} (CCI) {{problems are}} among the main causes of performance degradation in wireless networks. In this paper, we investigate the impact of cooperative communications on mitigating the effect of channel <b>estimation</b> <b>error</b> and CCI. Two main performance criteria, namely, the traditional outage probability and the proposed signal-to-noise ratio (SNR) gap ratio, are utilized to characterize such impact. The SNR gap ratio measures {{the reduction in the}} SNR due to channel <b>estimation</b> <b>error</b> or CCI. Taking into consideration the channel <b>estimation</b> <b>error,</b> we show that the outage probability is reduced by utilizing cooperative transmission protocols. We also show that cooperative transmission scenarios, in which each cooperating relay forwards its signal over an orthogonal channel, result in lower SNR gap ratio compared to that of the direct transmission. Thus, cooperative transmission schemes are less susceptible to the effect of channel <b>estimation</b> <b>error</b> compared to direct transmission. Moreover, increasing the number of cooperating relays reduces the effect of the channel <b>estimation</b> <b>error</b> more. Timing synchronization error arises in distributed space-time cooperative schemes, in which the cooperating relays are simultaneously transmitting their signals over the same channel. Unlike the channel <b>estimation</b> <b>error,</b> the effect of the timing synchronization error gets worse as the the number of cooperating relays increases. In this work we also study the tradeoff between the timing synchronization error and the channel <b>estimation</b> <b>error,</b> and show their net impact on the system performance. Finally, we illustrate that CCI can be modeled in a similar fashion to the channel <b>estimation</b> <b>error,</b> and hence the cooperative transmission schemes are also less susceptible to the effect of CCI. Index Terms—Channel <b>estimation</b> <b>error,</b> cochannel interference (CCI), cooperative diversity, distributed transmit beamforming, relay selection, timing synchronization error. I...|$|R
40|$|Joint space-time (ST) coding and {{constellation}} rotation {{is considered}} {{in the presence of}} imperfect channel state information. The combined system with two transmit antennas is shown to be more resistant to channel <b>estimation</b> <b>errors</b> than ST coding. Experiments involving several cases of channel <b>estimation</b> <b>errors</b> verify related theory reported in the literature. Keywords: Space-time coding, signal space diversity, channel <b>estimation</b> <b>error,</b> transmit diversity 1...|$|R
30|$|The {{friction}} force increased {{as a result}} of tissue relaxation, the <b>estimation</b> <b>error</b> could be observed in phase III. We assumed that the <b>estimation</b> <b>error</b> increased {{as a result of}} the tissue deformation, because the <b>estimation</b> <b>error</b> increased in the negative immediately after the total insertion force decreased sharply. Thus, the <b>estimation</b> <b>error</b> increased in insertion phases II and III, especially given that these phases included tissue cutting. The RMSE, indicating the dispersion from the true value, also increased in phases II and III. This means that the RMSE increases as a result of tissue cutting.|$|R
40|$|Implementation of the Shewhart, CUSUM, and EWMA charts {{requires}} {{estimates of}} the in-control process parameters. Many researchers have shown that <b>estimation</b> <b>error</b> strongly influences the performance of these charts. However, a given amount of <b>estimation</b> <b>error</b> may differ in effect across charts. Therefore, we perform a pairwise comparison {{of the effect of}} <b>estimation</b> <b>error</b> across these charts. We conclude that the Shewhart chart is more strongly affected by <b>estimation</b> <b>error</b> than the CUSUM and EWMA charts. Furthermore, we show that the general belief that the CUSUM and EWMA charts have similar performance no longer holds under estimated parameters...|$|R
40|$|Many {{software}} companies track and analyze project performance by measuring cost estimation accuracy. A high <b>estimation</b> <b>error</b> is frequently interpreted as poor estimation skills. This {{is not necessarily}} a correct interpretation. High <b>estimation</b> <b>error</b> can also be a result of other factors, such as high estimation complexity and insufficient cost control of the project. Through a real-life example we illustrate how the lack of proper <b>estimation</b> <b>error</b> analysis technique can bias analyses of cost estimation accuracy and lead to wrong conclusions. Further, we examine a selection of cost estimation studies, and show that they frequently do not take the necessary actions to ensure meaningful interpretations of <b>estimation</b> <b>error</b> data. Motivated by these results, we propose a general framework that, we believe, will improve analyses of software cost <b>estimation</b> <b>error...</b>|$|R
