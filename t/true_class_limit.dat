0|2121|Public
5000|$|Experimental {{structures}} of proteins with essentially non-natural sequences. Not a <b>true</b> <b>class</b> ...|$|R
30|$|H_ 0 : the {{predictions}} are statistically {{independent of the}} <b>true</b> <b>class</b> labels.|$|R
30|$|H_ 1 : the {{predictions}} are statistically {{dependent on the}} <b>true</b> <b>class</b> labels.|$|R
50|$|The {{main purpose}} of feature {{learning}} algorithm {{is to find a}} new feature from sample image to test whether the classes are recognised or not. Two cases should be consider: Searching for new feature of <b>true</b> <b>class</b> and wrong class from sample image respectively. If new feature of <b>true</b> <b>class</b> is detected and the wrong class is not recognised, then the class is recognised and the algorithm should terminate. If feature of <b>true</b> <b>class</b> is not detected and of false class is detected in the sample image, false class should be prevented from being recognised and the feature should be removed from Bayesian network.|$|R
40|$|In this paper, a novel pattern {{classification}} {{approach is}} proposed by regularizing the classifier learning to maximize mutual information between the classification response and the <b>true</b> <b>class</b> label. We argue that, with the learned classifier, {{the uncertainty of}} the <b>true</b> <b>class</b> label of a data sample should be reduced by knowing its classification response as much as possible. The reduced uncertainty is measured by the mutual information between the classification response and the <b>true</b> <b>class</b> label. To this end, when learning a linear classifier, we propose to maximize the mutual information between classification responses and <b>true</b> <b>class</b> labels of training samples, besides minimizing the classification error and reduc- ing the classifier complexity. An objective function is constructed by modeling mutual information with entropy estimation, and it is optimized by a gradi- ent descend method in an iterative algorithm. Experiments on two real world pattern classification problems show the significant improvements achieved by maximum mutual information regularization...|$|R
3000|$|... = p. These probabilities are {{different}} and represent joint prior probability of the <b>true</b> <b>class</b> and the context.|$|R
50|$|The Express line {{introduced}} Mesa's Duo-Class technology. This {{technology offers}} {{the ability to}} run the power section of the amplifier in either <b>true</b> <b>class</b> A (single-ended) mode, or <b>true</b> <b>class</b> AB (push-pull) mode. This allows the operator to choose between running the amplifier at a reduced power output of 5 watts (class A), or full power (class AB). When run in 5 watt (Class A) mode, the power section is operating on only one vacuum tube.|$|R
3000|$|Class-attribute-combiner:a meta-level {{training}} instance {{includes the}} training features, the predictions {{by each of}} the base classifiers and the <b>true</b> <b>class</b> for test instance; [...]...|$|R
30|$|The {{confusion}} matrix is a convenient way to represent {{results of a}} classifier because all the metrics used to evaluate classifiers can be computed from it. In the same way, the method that we propose in this work {{is based on the}} analysis of the {{confusion matrix}}. Specifically, we propose to quantify the evidence between two alternative hypotheses about the underlying generation mechanism of the observed confusion matrix. The first hypothesis is that the predicted class labels are statistically independent from the <b>true</b> <b>class</b> labels. This is the case were the classifier is not able to discriminate the classes. The second hypothesis is that the predicted class labels are statistically dependent on the <b>true</b> <b>class</b> labels. In this case, the classifier predicted according to the <b>true</b> <b>class</b> labels. The degree of evidence in favour of one hypothesis or of the other is the measure that we propose for evaluating the classifier.|$|R
3000|$|Binary-class-combiner: {{for this}} rule, the meta-level {{instance}} consists {{of all the}} base classifiers’ predictions for all the <b>classes</b> and the <b>true</b> <b>class</b> for the test instance; [...]...|$|R
30|$|Class-combiner: it {{is similar}} to the binary-class-combiner except that this rule only {{contains}} the predictions for the predicted class from all the base classifiers and the <b>true</b> <b>class.</b>|$|R
40|$|This paper {{introduces}} a multi-label classification prob-lem {{to the field}} of human computation. The problem in-volves training data such that each instance belongs to a set of <b>classes.</b> The <b>true</b> <b>class</b> sets of all the instances are provided together with their estimations presented by m human experts. Given the training data and the class-set estimates of the m experts for a new instance, the multi-label classification problem is to estimate the <b>true</b> <b>class</b> set of that instance. To solve the problem we propose an ensemble approach. Experiments show that the ap-proach can outperform the best expert and the majority vote of the experts...|$|R
30|$|In the {{training}} phase (input T= 1) the estimated class label ŷ is {{compared with the}} <b>true</b> <b>class</b> label (input Y) {{and based on the}} outcome, feedback is applied to modify the weights {w_j}.|$|R
5000|$|Decide the {{individual}} <b>class</b> <b>limits</b> and select a suitable {{starting point of}} the first class which is arbitrary, it may be {{less than or equal}} to the minimum value. Usually it is started before the minimum value in such a way that the midpoint (the average of lower and upper <b>class</b> <b>limits</b> of the first class) is properly placed.|$|R
5000|$|Walk of My Life is Koda Kumi's twelfth studio {{album and}} {{continued}} her streak of #1 albums on the Oricon charts. It {{was released in}} CD, CD+DVD, CD+Music Card A, CD+Music Card B and CD+FC DVD. The fan club DVD contained her second [...] "First <b>Class</b> <b>Limited</b> Live," [...] {{the first of which}} was secret ~First <b>Class</b> <b>Limited</b> Live.|$|R
40|$|We apply robust Bayesian {{decision}} theory to improve both generative and discriminative learners under bias in class proportions in labeled training data, when the <b>true</b> <b>class</b> proportions are unknown. For the generative case, we derive an entropybased weighting that maximizes expected log likelihood under the worst-case <b>true</b> <b>class</b> proportions. For the discriminative case, we derive a multinomial logistic model that minimizes worst-case conditional log loss. We apply our theory to the modeling of species geographic distributions from presence data, an extreme case of labeling bias {{since there is}} no absence data. On a benchmark dataset, we find that entropy-based weighting offers an improvement over constant estimates of class proportions, consistently reducing log loss on unbiased test data. ...|$|R
5000|$|... "Back Stage Of Secret ~First <b>Class</b> <b>Limited</b> Live~ & Interview With Koda Kumi" ...|$|R
40|$|Most {{previously}} proposed mining methods on data streams make {{an unrealistic}} assumption that “labelled ” data stream {{is readily available}} and can be mined at anytime. However, in most real-world problems, labelled data streams are rarely immediately available. Due to this reason, models are refreshed periodically, that is usually synchronized with data availability schedule. There are several undesirable consequences of this “passive periodic refresh”. In this paper, we propose a new concept of demand-driven active data mining. It estimates the error of the model on the new data stream without knowing the <b>true</b> <b>class</b> labels. When significantly higher error is suspected, it investigates the <b>true</b> <b>class</b> labels of a selected number of examples {{in the most recent}} data stream to verify the suspected higher error...|$|R
5000|$|... "Maximizing [...] is {{equivalent}} to minimizing the -distance between the predicted class distribution and the <b>true</b> <b>class</b> distribution (ie: where the [...] induced by [...] are all equal to 1). A natural alternative is the KL-divergence, which induces the following objective function and gradient:" [...] (Goldberger 2005) ...|$|R
5000|$|Customer <b>class</b> <b>limited</b> (e.g. only usable by {{educational}} users; most Autodesk software {{with full}} features is free for students) ...|$|R
30|$|The {{confusion}} percentage {{for each}} <b>true</b> <b>class</b> {{is defined as}} the percentage of all occurrences which were misclassified as the predicted class. In the table, it becomes clear that our method is only confused in the most turbulent shattering cases were a great overlap in discriminative features occurs.|$|R
50|$|The {{displacement}} capacities {{must remain}} at the homologated size. Modifying thebore and stroke to reach <b>class</b> <b>limits</b> is not allowed.|$|R
50|$|When the (amateur) International Boxing Association (AIBA) {{was founded}} in 1946 to govern amateur boxing, it metricated the weight <b>class</b> <b>limits</b> by {{rounding}} them to the nearest kilogram. Subsequent alterations as outlined in the boxing at the Summer Olympics article; these have introduced further discrepancies between amateur and professional <b>class</b> <b>limits</b> and names. The lower weight classes are to be adjusted in September 2010, to establish an absolute minimum weight for adult boxers.|$|R
3000|$|... 20 is the defined {{number of}} samples that could be counted if the {{particle}} concentration were at the <b>class</b> <b>limit</b> [...]...|$|R
50|$|The {{displacement}} capacities {{must remain}} at the homologated size. Modifying the bore and stroke to reach <b>class</b> <b>limits</b> is not allowed.|$|R
3000|$|Cn, m is the <b>class</b> <b>limit</b> (number of {{particles}} / m 3) {{for the largest}} considered particle size specified for the relevant class [...]...|$|R
25|$|Maiden, novice, and limit: Jumping <b>classes</b> <b>limited</b> to horses {{with fewer}} than one, three, or six wins. Fences are usually lower and time limits more generous.|$|R
50|$|Secret First <b>Class</b> <b>Limited</b> Live (stylized as secret ~FIRST CLASS LIMITED LIVE~) DVD is Koda Kumi's {{first public}} tour and {{contains}} songs from her secret album.|$|R
50|$|The {{presentation}} of all class frequencies gives a frequency distribution, or histogram. Histograms, even when {{made from the}} same record, are different for different <b>class</b> <b>limits.</b>|$|R
5000|$|Maiden, novice, and limit: Jumping <b>classes</b> <b>limited</b> to horses {{with fewer}} than one, three, or six wins. Fences are usually lower and time limits more generous.|$|R
40|$|Abstract. A common {{assumption}} made in {{the field}} of Pattern Recog-nition is that the priors inherent to the class distributions in the training set are representative of the <b>true</b> <b>class</b> distributions. However this as-sumption does not always hold, since the true class-distributions may be different, and in fact may vary significantly. The implication of this is that the effect on cost for a given classifier may be worse than expected. In this paper we address this issue, discussing a theoretical framework and methodology to assess the effect on cost for a classifier in imbalanced conditions. The methodology can be applied to many different types of costs. Some artificial experiments show how the methodology can be used to assess and compare classifiers. It is observed that classifiers that model the underlying distributions well are more resilient to changes in the <b>true</b> <b>class</b> distribution than weaker classifiers. ...|$|R
5000|$|Given {{the binary}} nature of classification, a natural {{selection}} for a loss function (assuming equal cost for false positives and false negatives) {{would be the}} 0-1 indicator function which takes the value of 0 if the predicted classification equals that of the <b>true</b> <b>class</b> or a 1 if the predicted classification does not match the <b>true</b> <b>class.</b> This selection is modeled bywhere [...] indicates the Heaviside step function.However, this loss function is non-convex and non-smooth, and solving for the optimal solution is an NP-hard combinatorial optimization problem. [...] As a result, {{it is better to}} substitute continuous, convex loss function surrogates which are tractable for commonly used learning algorithms. In addition to their computational tractability, one can show that the solutions to the learning problem using these loss surrogates allow for the recovery of the actual solution to the original classification problem. [...] Some of these surrogates are described below.|$|R
50|$|ACS {{maintains}} a student/teacher ratio of 9:1, with kindergarten <b>classes</b> <b>limited</b> {{to a maximum}} of 16 students, elementary classes to 20-22, and middle school courses ranging from 6-24 students.|$|R
25|$|Capitalism: capitalists are {{the ruling}} class, who create and employ the <b>true</b> working <b>class.</b>|$|R
30|$|Since ROC {{curve is}} a binary {{classifier}} system {{but we have}} five class labels for the grade so we are presenting five ROC curves. For each ROC curve one class is considered as <b>True</b> <b>class</b> {{and the rest of}} the classes are considered as False class. ROC curves change when over-sampled data was used for classification which are discussed in the Section 4.5.|$|R
40|$|This article {{describes}} a case report {{in which a}} Class III patient was successfully treated with an inverted labial bow appliance. The appliance is easy to make, efficient and well tolerated by the patient. Early treatment of Class III conditions is recommended. The importance of differentiating between <b>true</b> <b>Class</b> III and pseudo Class III malocclusions is emphasized. King Saud Universit...|$|R
