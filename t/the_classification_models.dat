189|10000|Public
5000|$|In {{order to}} build <b>the</b> <b>classification</b> <b>models,</b> the samples {{belonging}} to each class need to be analysed using principal component analysis (PCA); only the significant components are retained.|$|E
30|$|Using {{large volumes}} of data but with low variety for {{training}} <b>the</b> <b>classification</b> <b>models</b> [12].|$|E
30|$|Results Best {{performance}} of <b>the</b> <b>classification</b> <b>models</b> {{was carried out}} by decision trees, in all runs.|$|E
40|$|International audienceOne of {{the biggest}} {{challenges}} in Big Data is to exploit value from large volumes of variable and changing data. For this, one must focus on analyzing the data in these Big Data sources and classify the data items according to a domain model (e. g. an ontology). To automatically classify unstructured text documents according to an ontology, a hierarchical multi-label classification process called Semantic HMC was proposed. This process uses ontologies to describe <b>the</b> <b>classification</b> <b>model.</b> To prevent cold start and user overload, <b>the</b> <b>classification</b> process automatically learns <b>the</b> ontology-described <b>classification</b> <b>model</b> from a very large set of unstructured text documents. However, data is always being generated and its statistical properties can change over time. In order to learn in such environment, <b>the</b> <b>classification</b> processes must handle streams of non-stationary data to adapt <b>the</b> <b>classification</b> <b>model.</b> This paper proposes a new adaptive learning process to consistently adapt <b>the</b> ontology-described <b>classification</b> <b>model</b> according to a non-stationary stream of unstructured text data in Big Data context. The adaptive process is then instantiated for the specific case of of the previously proposed Semantic HMC...|$|R
30|$|The {{quality of}} <b>the</b> <b>classification</b> <b>model</b> that is {{developed}} by a classification algorithm in a reasonable (default) configuration or in an automatically optimized configuration provides an indication as to whether a reliable classification is possible at all, or not; for example. if such a <b>classification</b> <b>model</b> shows an area under the ROC curve of something close to 0.5, then it is rather unlikely to increase the quality of <b>the</b> <b>classification</b> <b>model</b> to a satisfying level just by adjusting and tuning the algorithms’ parameters. The more promising way is to adjust the input set of input variables.|$|R
40|$|Classification is a Data Mining {{technique}} used {{for building a}} prototype of the data behaviour, using which an unseen data can be classified {{into one of the}} defined classes. Several researchers have proposed classification techniques but most of them did not emphasis much on the misclassified instances and storage space. In this paper, a <b>classification</b> <b>model</b> is proposed that takes into account the misclassified instances and storage space. <b>The</b> <b>classification</b> <b>model</b> is efficiently developed using a tree structure for reducing the storage complexity and uses single scan of the dataset. During the training phase, Class-based Closed Frequent ItemSets (CCFIS) were mined from the training dataset {{in the form of a}} tree structure. <b>The</b> <b>classification</b> <b>model</b> has been developed using the CCFIS and a similarity measure based on Longest Common Subsequence (LCS). Further, the Particle Swarm Optimization algorithm is applied on the generated CCFIS, which assigns weights to the itemsets and their associated classes. Most of the classifiers are correctly classifying the common instances but they misclassify the rare instances. In view of that, AdaBoost algorithm has been used to boost the weights of the misclassified instances in the previous round so as to include them in the training phase to classify the rare instances. This improves the accuracy of <b>the</b> <b>classification</b> <b>model.</b> During <b>the</b> testing phase, <b>the</b> <b>classification</b> <b>model</b> is used to classify the instances of the test dataset. Breast Cancer dataset from UCI repository is used for experiment. Experimental analysis shows that the accuracy of <b>the</b> proposed <b>classification</b> <b>model</b> outperforms <b>the</b> PSOAdaBoost-Sequence classifier by 7...|$|R
40|$|Microbial {{communities}} {{are important to}} human health. Bacterial vaginosis (BV) is a disease associated with the vagina microbiome. While the causes of BV are unknown, the microbial community in the vagina appears to play a role. We use three different machine-learning techniques to classify microbial communities into BV categories. These three techniques include genetic programming (GP), random forests (RF), and logistic regression (LR). We evaluate the classification accuracy {{of each of these}} techniques on two different datasets. We then deconstruct <b>the</b> <b>classification</b> <b>models</b> to identify important features of the microbial community. We found that <b>the</b> <b>classification</b> <b>models</b> produced by the machine learning techniques obtained accuracies above 90 % for Nugent score BV and above 80 % for Amsel criteria BV. While <b>the</b> <b>classification</b> <b>models</b> identify largely different sets of important features, the shared features often agree with past research...|$|E
30|$|Software {{used for}} this study is scikit-learn [33], an open source machine {{learning}} software package in Python. <b>The</b> <b>classification</b> <b>models</b> selected for categorization are: Naïve Bayesian, Random Forest, and Support Vector Machine [32].|$|E
30|$|Methods We used three {{non linear}} {{supervised}} machine learning approaches (multilayer perceptron neural network, decision tree and gaussian support vector machine) to classify responders vs non responders from a dataset of 115 patients using Matlab R 2015 b software. <b>The</b> <b>classification</b> <b>models</b> were run three times with respect to independent variables: mean arterial pressure (MAP), arterial elastance (Ea), VAC.|$|E
30|$|All {{of these}} {{algorithms}} above update a classifier dynamically {{using the new}} coming data. On one hand, we need not to load all data into memory at once. On the other hand, we can real-time modify <b>the</b> <b>classification</b> <b>model</b> according to <b>the</b> new training instances. Moreover, the classifier can adapt to concept drift via real-time updating to new data. However, there are still shortcomings and limitations in incremental learning algorithms. For example, it can only unceasing absorb new data-streams, it cannot remove old instances in <b>the</b> <b>classification</b> <b>model.</b> Because of these shortcomings, incremental algorithms will be helpless when comes to rigorous concept drift.|$|R
40|$|In many {{applications}} experts {{need to make}} decisions based on the analysis of multi-dimensional data. Various <b>classification</b> <b>models</b> can support <b>the</b> decision making process. To obtain an intuitive understanding of <b>the</b> <b>classification</b> <b>model,</b> interactive visualizations are essential. We argue that this is best done by a series of interactive 2 D scatterplots. In this paper, we define a set of characteristics of <b>the</b> multi-dimensional <b>classification</b> <b>model</b> that have to be visually represented in those scatterplots. Our proposed method presents those characteristics in a uniform manner for both linear and non-linear classification methods. We combine a visualization of a Voronoi based representation of multi-dimensional decision boundaries with visualization of the distances of the data elements to these boundaries. To allow the developer of the model to refine the threshold of <b>the</b> <b>classification</b> <b>model</b> and instantly observe the results, we use interactive decision point selection on a performance curve. Finally, we show how the combination of those techniques allows exploration of multi-dimensional decision boundaries in 2 D...|$|R
40|$|In <b>the</b> {{data stream}} <b>classification</b> process, in {{addition}} to the solution of massive and real-time data stream, the dynamic changes of the need to focus and study. From the angle of detecting concept drift, according to the dynamic characteristics of the data stream. This paper proposes a new classification method for data stream based on the combined use of concept drift detection and <b>classification</b> <b>model.</b> <b>The</b> data stream <b>classification</b> <b>model</b> can’t adapt to concept drift problem to solve. Before <b>the</b> <b>model</b> <b>classification,</b> <b>the</b> use of information entropy to judge the data block concept drift, the concept of history to have appeared, the use of a classifier pool mechanism to save it, to makes <b>the</b> <b>classification</b> <b>model</b> has stronger resistance to concept drift...|$|R
40|$|Abstract. Predicting {{the quality}} of {{software}} modules prior to testing or system operations allows a focused software quality improvement endeavor. Decision trees are very attractive for classification problems, because of their comprehensibility and white box modeling features. However, optimizing the classification accuracy and the tree size is a difficult problem, and to our knowledge very few studies have addressed the issue. This paper presents an automated and simplified genetic programming (gp) based decision tree modeling technique for calibrating software quality classification models. The proposed technique is based on multiobjective optimization using strongly typed gp. Two fitness functions are used to optimize the classification accuracy and tree size of <b>the</b> <b>classification</b> <b>models</b> calibrated for a real-world high-assurance software system. The performances of <b>the</b> <b>classification</b> <b>models</b> are compared with those obtained by standard gp. It is shown that the gp-based decision tree technique yielded better classification models...|$|E
40|$|Credit {{analysis}} model {{have been}} developed by many companies and researches to improve the accuracy of credit decisions. The main purpose of this method is to increase creditworthy identification. Today,more banks use classification algorithms to classify customer information. One of <b>the</b> <b>classification</b> <b>models</b> is decision tree. In this work,were used two popular decision tree algorithms to perform classification on the German credit dataset. ...|$|E
40|$|Abstract — The {{application}} of data mining and machine learning in directing clinical research into possible hidden knowledge is becoming greatly influencial in cancer research. This research presents {{a comparison of}} three data mining classification models: multi-layer perceptron neural networks, C 4. 5 decision trees and Naive Bayes. <b>The</b> <b>classification</b> <b>models</b> are built for breast cancer survivability prediction. The data set used is collected from registries across Saudi Arabia. Due to data scarcity, data synthesis had to be performed using a random seed and a double sampling procedure. After sufficiently preprocessing the data, <b>the</b> <b>classification</b> <b>models</b> were built and three performance measures were used to rank the models: Accuracy, Sensitivity and Specificity. The experiment was set up with multi-layer perceptron as the baseline scheme and with statistical significance of 0. 05. Decision Trees performed marginally better than multi-layer perceptron and Naïve Bayes performed significantly worse than the baseline scheme. The result showed that Decsion tree is the the most accurate predictor for breast cancer survivbility i...|$|E
30|$|Table  5 <b>classification</b> results include <b>the</b> <b>classification</b> of <b>the</b> {{training}} sample and <b>the</b> <b>classification</b> of <b>the</b> test sample. If the Y value of sample enterprise data is − 1 (non-creditworthy enterprise), but <b>the</b> <b>classification</b> <b>model</b> recognizes it as[*]+[*] 1 (creditworthy enterprise), then a TypeIerror {{in the model}} occurs. On the other hand, if sample enterprise data show its Y value as + 1 and <b>the</b> <b>classification</b> <b>model</b> determines it as − 1, then a Type II error occurs in the model. Altman noted that the loss induced by Type I errors {{is larger than the}} loss induced by Type II errors, which is almost 20 to 60 times greater (Altman 1980). Thus, in a single transaction, the loss that a bank may be subjected to from a Type I error is significantly higher. Therefore, we want to inspect the performance of a <b>classification</b> <b>model</b> considering not only <b>the</b> overall <b>classification</b> accuracy but also Type I errors.|$|R
40|$|We {{study the}} problem of {{building}} <b>the</b> <b>classification</b> <b>model</b> for a target class {{in the absence of}} any labeled training example for that class. To address this difficult learning problem, we extend the idea of transfer learning by assuming that the following side information is available: (i) a collection of labeled examples belonging to other classes in the problem domain, called the auxiliary classes; (ii) the class information including the prior of the target class and the correlation betweenthetargetclass andtheauxiliaryclasses. Our goal is to construct <b>the</b> <b>classification</b> <b>model</b> for <b>the</b> target class by leveraging the above data and information. We refer to this learning problem as unsupervised transfer classification. Our framework is based on the generalized maximum entropy model that is effective in transferring the label information of the auxiliary classes to the target class. A theoretical analysis shows that under certain assumption, <b>the</b> <b>classification</b> <b>model</b> obtained by <b>the</b> proposed approach converges to the optimal model when it is learned from the labeled examples for the target class. Empirical study on text categorization over four different data sets verifies the effectiveness of the proposed approach...|$|R
5000|$|<b>The</b> ETIM <b>Classification</b> <b>model</b> {{is built}} using the {{following}} categories or entities: ...|$|R
40|$|At present, the {{population}} of non-native speakers is twice that of native speakers. It is necessary to explore the text generation strategies for non-native users. However, little {{has been done in}} this field. This study investigates the features that affect the placement (where to place a cue) of because for non-native speakers. A machine learning program – C 4. 5 was applied to induce <b>the</b> <b>classification</b> <b>models</b> of the placement. ...|$|E
40|$|We {{investigate}} {{the performance of}} different classification models {{and their ability to}} recognize prostate cancer in an early stage. We build ensembles of classification models in order to increase the classification performance. We measure the performance of our models in an extensive cross-validation procedure and compare different classification models. The datasets come from clinical examinations and some of <b>the</b> <b>classification</b> <b>models</b> are already in use to support the urologists in their clinical work...|$|E
40|$|This article {{considers}} the gene ranking algorithm for the microarray data. The rank vector is estimated by classifications of the random data samples. At each iteration, {{the ranks of}} genes participating in the successful classification become higher. Unlike other methods of feature selection, the proposed algorithm allows increasing the generality of <b>the</b> <b>classification</b> <b>models</b> by construction of the balanced training samples and {{taking into account the}} descriptiveness of the gene combinations by the subset estimation...|$|E
40|$|Abstract — Human {{capital is}} {{of a high}} concern for companies’ {{management}} where their most interest is in hiring the highly qualified personnel which are expected to perform highly as well. Recently, {{there has been a}} growing interest in the data mining area, where the objective is the discovery of knowledge that is correct and of high benefit for users. In this paper, data mining techniques were utilized to build a <b>classification</b> <b>model</b> to predict <b>the</b> performance of employees. To build <b>the</b> <b>classification</b> <b>model</b> <b>the</b> CRISP-DM data mining methodology was adopted. Decision tree was the main data mining tool used to build <b>the</b> <b>classification</b> <b>model,</b> where several <b>classification</b> rules were generated. To validate the generated model, several experiments were conducted using real data collected from several companies. The model is intended to be used for predicting new applicants’ performance...|$|R
5000|$|... where [...] is the VC {{dimension}} of <b>the</b> <b>classification</b> <b>model,</b> , and [...] {{is the size}} of the training set (restriction: this formula is valid when [...] When [...] is larger, the test-error may be much higher than the training-error. This is due to overfitting).|$|R
40|$|A key {{assumption}} in supervised {{machine learning}} is that future data will be similar to historical data. This assumption is often false in real world applications, and as a result, prediction models often return predictions that are extrapolations. We compare four approaches to estimating extrapolation risk for machine learning predictions. Two builtin methods use information available from <b>the</b> <b>classification</b> <b>model</b> to decide if the model would be extrapolating for an input data point. The other two build auxiliary <b>models</b> to supplement <b>the</b> <b>classification</b> <b>model</b> and explicitly model extrapolation risk. Experiments with synthetic and real data sets show that the auxiliary models are more reliable risk detectors. To best safeguard against extrapolating predictions, however, we recommend combining builtin and auxiliary diagnostics...|$|R
30|$|One of {{the other}} feature {{selection}} methods that we used was the analysis of variance (ANOVA) feature selection. ANOVA [71] is used {{to determine if there}} are any statistically significant differences between the arithmetic means of independent groups. By using ANOVA for feature selection in our experiment, we clarify the relevance of terms by assigning a score to each based on an F-test. Top scoring terms are considered as our desired features and sent to <b>the</b> <b>classification</b> <b>models.</b>|$|E
40|$|The {{robustness}} {{of neural}} networks to intended perturbations has recently attracted significant attention. In this paper, we propose a new method, learning {{with a strong}} adversary, that learns robust classifiers from supervised data. The proposed method takes finding adversarial examples as an intermediate step. A new and simple way of finding adversarial examples is presented and experimentally shown to be efficient. Experimental results demonstrate that resulting learning method greatly improves the robustness of <b>the</b> <b>classification</b> <b>models</b> produced...|$|E
40|$|ABSTRACT Recently, thyroid {{diseases}} {{are more and}} more spread worldwide. In Romania, for example, one of eight women suffers from hypothyroidism, hyperthyroidism or thyroid cancer. Various research studies estimate that about 30 % of Romanians are diagnosed with endemic goiter. Factors that affect the thyroid function are: stress, infection, trauma, toxins, low-calorie diet, certain medication etc. It is very important to prevent such diseases rather than cure them, because the majority of treatments consist in long term medication or in chirurgical intervention. The current study refers to thyroid disease classification in two of the most common thyroid dysfunctions (hyperthyroidism and hypothyroidism) among the population. The authors analyzed and compared four classification models: Naive Bayes, Decision Tree, Multilayer Perceptron and Radial Basis Function Network. The results indicate a significant accuracy for all <b>the</b> <b>classification</b> <b>models</b> mentioned above, the best classification rate being that of the Decision Tree model. The data set used to build and to validate the classifier was provided by UCI machine learning repository and by a website with Romanian data. The framework for building and testing <b>the</b> <b>classification</b> <b>models</b> was KNIME Analytics Platform and Weka, two data mining software...|$|E
40|$|Since most {{of energy}} sources in our Universe appear {{point-like}} structures,the study of point sources detection method on astronomical images has become significant. In this paper,a point sources detection approach on X-ray astronomical image was proposed. Firstly,a thresholding method {{was used to}} separate the background noises. Then,the peak detection method was taken to detect the positions of potential point sources. After that,we extracted spectrum features of point sources and backgrounds,and generated <b>the</b> <b>classification</b> <b>model</b> using <b>the</b> Support Vector Machine. Finally,the correct point sources were got after discarding of spurious detections with <b>the</b> <b>classification</b> <b>model.</b> Our approach {{was applied to the}} X-ray image of Galaxy NGC 4552. Compared with “wavdetect”,our approach has the same performance of accuracy with a detection error rate of 5 %,but a higher efficiency...|$|R
30|$|While {{constructing}} <b>the</b> <b>classification</b> <b>model,</b> {{many variables}} may be included, {{but not all}} of these variables are actually important. Therefore, unimportant variables need to be eliminated in order to construct a simpler <b>classification</b> <b>model.</b> There is quite a number of ways to screen variables, of which the LASSO algorithm has shown excellent performance in reducing variables (Connor et al. 2015).|$|R
30|$|This paper {{proposed}} {{a kind of}} SAE higher layer visualizing feature extraction method for the small sample target objects in the sky. Firstly, the local features can be obtained through the non-transfer learning in the small sample target object images or transfer learning in the cross-domain database, and then, the global feature of the small sample target object can be obtained through the CNN model, as well as proposed to add <b>the</b> <b>classification</b> <b>model</b> to realize <b>the</b> <b>classification</b> of <b>the</b> target objects. With the help of <b>the</b> <b>classification</b> <b>model,</b> <b>the</b> different types of the target objects can be classified. Experiments verified that the algorithms this paper proposed can well classify the small sample target objects, and <b>the</b> <b>classification</b> performance comparisons between the transfer learning and non-transfer learning based on the SAE higher layer visualizing feature extraction model in the small sample target objects are realized.|$|R
40|$|Complete {{description}} of blood coagulation pathways {{with respect to}} patientspecific characterization presents a major challenge. Characteristics of blood coagulation vary drastically between patients. It is essential to characterize abnormalities in blood coagulation to diagnose and treat cardiovascular diseases better. Given the paucity of patient-specific data to characterize and model the system, {{there is a greater}} need to regularize patient-specific models and methods effectively. In this dissertation, we formulate actionable questions and describe our methodology and results. First, we explore a practical application for using models to classify acute coronary syndrome and coronary artery disease. <b>The</b> <b>classification</b> <b>models</b> were built based on a chemical kinetics model reported in the literature. In a diagnostic setting, <b>the</b> <b>classification</b> <b>models</b> could be employed to screen thousands of patients with greater certainty every year. Second, we propose a simplified model for {{a key part of the}} blood coagulation cascade that demonstrates robust predictive capabilities. The model predicts prolonged activity of thrombin, an important enzyme in the clotting process, in certain plasma factor compositions. The activity sustains beyond the time which is conventionally considered to be the end of clotting. This observation along with the simplified model is a necessary step towards effectively studying clotting in realistic geometries...|$|E
40|$|In {{this work}} we {{introduce}} {{the utilization of}} Fujisaki’s modeling of pitch contour for the task of emotion recognition. For {{the evaluation of the}} proposed features we have employed a decision tree as well as an instance based learning algorithm. The datasets utilized for training <b>the</b> <b>classification</b> <b>models,</b> were extracted from two emotional speech databases. Results showed that knowledge extracted from Fujisaki’s modeling of intonation benefited all resulted emotion recognition models. Thus, an average raise of 9, 52 % in the total accuracy of all approaches was achieved. 1...|$|E
40|$|Objectives: Machine {{learning}} {{systems can}} considerably reduce {{the time and}} effort needed by experts to perform new systematic reviews (SRs). This study investigates categorization models, which are trained on a combination of included and commonly excluded articles, which can improve performance by identifying high quality articles for new procedures or drug SRs. Methods: Test collections were built using the annotated reference files from 19 procedure and 15 drug systematic reviews. <b>The</b> <b>classification</b> <b>models,</b> using a support vector machine, were trained by the combined even data of other topics, excepting the desired topic. This approach was compared to the combination of included and commonly excluded articles with the combination of included and excluded articles. Accuracy was used for the measure of comparison. Results: On average, the performance was improved by about 15 % in the procedure topics and 11 % in the drug topics when <b>the</b> <b>classification</b> <b>models</b> trained on the combination of articles included and commonly excluded, were used. The system using the combination of included and commonly excluded articles performed better than the combination of included and excluded articles in all of the procedure topics. Conclusions: Automatically rigorous article classification using machine learning can reduce the workload of experts when they perform systematic reviews when the topic-specific data are scarce. In particular, when the combinatio...|$|E
30|$|This {{is used in}} {{evaluating}} the supervised classifier to understand its behavior when purposefully wrong labels are fed to <b>the</b> <b>classification</b> <b>model</b> [96]. Here, actual labels (ground truth) of the observations in the training set are randomized (shuffled) and fed to <b>the</b> <b>classification</b> <b>model.</b> Subsequently performance metrics like “PPV”, “sensitivity” and “total accuracy” are collected by repeatedly performing the study as explained above. Each time when the study is performed, {{a new set of}} randomized training set labels (which {{are not the same as}} ground truths) are fed to the classifier. For a robust <b>classification</b> <b>model</b> it is expected to get very low values for the above mentioned performance metrics (say, 0 - 50  %). If it is not the case then the classifier is very sensitive to potential confounding variables and correlations in the data set, making it highly volatile particularly when subjected to new test data.|$|R
40|$|This paper {{discusses}} {{in detail}} the behavior of <b>the</b> different <b>classification</b> on image segmentation data. The result predicts the different aspects of <b>the</b> <b>classification</b> <b>model.</b> It is found that NNEG is the best classifier with accuracy of 96. 2771 %. ROC|max and ROC|min are computed for different classes and {{are found to be}} interesting...|$|R
40|$|The {{increasing}} {{complexity of}} software development spawns lots of specialised tools to edit code, employ UML schemes, integrate documentation, and so on. The {{problem is that}} the tool builders themselves are responsible for making their tools interoperable with other tools or development environments. Because they cannot anticipate all other tools they can integrate with, a lot of tools cannot co-operate. This paper introduces <b>the</b> <b>classification</b> <b>model,</b> a lightweight integration medium that enables unrelated tools that were not meant to be integrated to cooperate easily. Moreover, the tool integration is done by a tool integrator, and not by the tool builder. To validate this claim, we show how to integrate several third-party tools using <b>the</b> <b>classification</b> <b>model,</b> and how it forms the foundation for the StarBrowser, a Smalltalk browser integrating di#erent tools...|$|R
