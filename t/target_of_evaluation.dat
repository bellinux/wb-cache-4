43|10000|Public
25|$|This {{fundamental}} relativity of {{the boundary}} of the TCB is exemplifed by {{the concept of the}} <b>target</b> <b>of</b> <b>evaluation</b> (TOE) in the Common Criteria security process: {{in the course of a}} Common Criteria security evaluation, one of the first decisions that must be made is the boundary of the audit in terms of the list of system components that will come under scrutiny.|$|E
5000|$|<b>Target</b> <b>Of</b> <b>Evaluation</b> (TOE) - {{the product}} or {{system that is}} the subject of the evaluation.|$|E
5000|$|... {{separation}} of internal resources {{used by the}} <b>Target</b> <b>of</b> <b>Evaluation</b> Security Functions (TSF) from exported resources made available to subjects ...|$|E
40|$|Definition and systematization of {{the concept}} <b>of</b> <b>evaluation</b> were tried from the {{viewpoint}} <b>of</b> <b>targets</b> <b>of</b> <b>evaluation,</b> timings <b>of</b> 'snapshot', evaluators, decision makers, purposes, ways <b>of</b> intervention, and <b>evaluation</b> indexes. Some practices of science communication were introduced, which evaluations were analyzed along the systematization method above. A plan to generate plural evaluation for various practices of science communication by using collective intelligence was proposed...|$|R
30|$|Due to {{the diverse}} {{nature of the}} evaluative items and <b>targets</b> <b>of</b> <b>evaluation</b> in the data, I opted for a {{completely}} manual search. However, although keyword searches are not efficient enough for finding all instances of appraisal, they are useful when analysing evaluative patterns. I conducted keyword searches {{after the first round}} of manual annotations in order to trace how certain <b>targets</b> <b>of</b> <b>evaluation,</b> such as (im)migrants, (im)migration, Europe and the EU, are appraised in the data in terms of polarity and explicitness. It is important to tie the evaluative item to its target, and Mary Macken-Horarik and Anne Isaac (2014 : 86) highlight the significance of co-text when coding appraisal as the textual environment shapes the readers’ perceptions. Studying the co-occurrence and alternation of explicit and implicit appraisal in texts can reveal how evaluative meanings are constructed.|$|R
40|$|Automatic and {{controlled}} modes <b>of</b> <b>evaluation</b> sometimes provide conflicting {{reports of the}} quality of social objects. This article presents evidence for 4 moderators of the relationship between automatic (implicit) {{and controlled}} (explicit) evaluations. Implicit and explicit preferences were measured for a variety of object pairs using a large sample. The average correlation was r �. 36, and 52 of the 57 object pairs showed a significant positive correlation. Results of multilevel modeling analyses suggested that (a) implicit and explicit preferences are related, (b) the relationship varies {{as a function of the}} objects assessed, and (c) at least 4 variables moderate the relationship: self-presentation, evaluative strength, dimensionality, and distinctiveness. The variables moderated implicit–explicit correspondence across individuals and accounted for much of the observed variation across content domains. The resulting model of the relationship between automatic and controlled evaluative processes is grounded in personal experience with the <b>targets</b> <b>of</b> <b>evaluation...</b>|$|R
50|$|The ITSEC {{has been}} largely {{replaced}} by Common Criteria, which provides similarly-defined evaluation levels and implements the <b>target</b> <b>of</b> <b>evaluation</b> concept and the Security Target document.|$|E
50|$|A PP {{states a}} {{security}} problem rigorously {{for a given}} collection of system or products, known as the <b>Target</b> <b>of</b> <b>Evaluation</b> (TOE) and to specify security requirements to address that problem without dictating how these requirements will be implemented. A PP may inherit requirements from one or more other PPs.|$|E
50|$|This {{fundamental}} relativity of {{the boundary}} of the TCB is exemplifed by {{the concept of the}} <b>target</b> <b>of</b> <b>evaluation</b> (TOE) in the Common Criteria security process: {{in the course of a}} Common Criteria security evaluation, one of the first decisions that must be made is the boundary of the audit in terms of the list of system components that will come under scrutiny.|$|E
3000|$|... the PI <b>target</b> allows <b>evaluation</b> <b>of</b> the {{neuronal}} and myocardial PI turnover with 11 C or 18 F-labelled DAG, {{and also}} with 11 C-inositol.|$|R
25|$|SemEval-2010 {{took place}} in 2010, {{followed}} by a workshop held in conjunction with ACL in Uppsala. SemEval-2010 included 18 different tasks <b>targeting</b> the <b>evaluation</b> <b>of</b> semantic analysis systems.|$|R
40|$|This <b>evaluation</b> <b>of</b> the {{openness}} of Finnish research performing and funding organisations was completed {{as part of the}} Open Science and Research Initiative (ATT) by the Ministry of Education and Culture. The <b>target</b> <b>of</b> this <b>evaluation</b> is to assess {{the openness}} of operational cultures in research organisations and research funding organisations. The key objectives, against which the assessments are made, are defined in the Open Science and Research Roadmap. More information about the evaluation can be found at openscience. fi/opencultur...|$|R
50|$|The {{product or}} system being evaluated, called the <b>target</b> <b>of</b> <b>evaluation,</b> is {{subjected}} to a detailed examination of its security features culminating in comprehensive and informed functional and penetration testing. The degree of examination depends upon the level of confidence desired in the target. To provide different levels of confidence, the ITSEC defines evaluation levels, denoted E0 through E6. Higher evaluation levels involve more extensive examination and testing of the target.|$|E
5000|$|Common Criteria for Information Technology Security Evaluation, version 3.1 Part 1 (Called “CC 3.1” or “CC”) [...] {{defines the}} Security Target (ST) as an “implementation-specific {{statement}} of security needs {{for a specific}} identified <b>Target</b> <b>of</b> <b>Evaluation</b> (TOE)”. In other words, the ST defines boundary and specifies {{the details of the}} TOE. In a product evaluation process according to the CC the ST document is provided by the vendor of the product.|$|E
50|$|The {{auditing}} {{and security}} engineering abilities have {{allowed it to}} obtain the Evaluation Assurance Level (EAL) 6 rating by the National Security Agency (NSA). The <b>Target</b> <b>of</b> <b>Evaluation</b> (TOE) Architecture in the Security Target for the evaluation excludes components such as those for file system and networking, from {{the definition of the}} TOE, focusing almost solely on the core kernel. Other operating systems, such as Windows or Linux, though evaluated at lower levels of assurance, generally include these abilities within their TOE.|$|E
40|$|The <b>evaluation</b> <b>of</b> {{the food}} chain’s {{environmental}} impacts was conducted using an environmentalaccounting model developed {{specifically for the}} Finnish food chain. The model is based on production and environmental impact data from year 2005. The model considers both Finnish production and Finnish imports {{in addition to their}} transport. The <b>targets</b> <b>of</b> the <b>evaluation</b> were the environmental impacts, in 2005, stemming from production. Environmental impacts of the end-use phase were not assessed. The changes in inventories were considered according to the national accounting methodology. ...|$|R
30|$|The fifth point {{revolves}} around countability. Every instance of appraisal {{has to be}} accounted for, and the corpus tool provides the statistics automatically (i.e. displays the exact numbers for each appraisal type) and enables making comparisons between <b>targets</b> <b>of</b> <b>evaluation.</b> Finally, point six is that all instances of appraisal have to be context-bound {{so that it is}} possible to determine who is evaluating what or who and how and eventually why. Even if official texts are often seen as plain and neutral, they are still tightly connected to certain culturally and socially bound ideas shared between the institution and its members (for details see e.g. ‘groups’ and ‘institutions’ in van Dijk 1998 : 141, 186). The EU’s core values and the ideological beliefs it promotes largely determine what kinds of evaluative and attitudinal expressions appear in its discourses. However, it is the evaluations and attitudes that run counter to these that are of special interest, and therefore, their frequency and quality in different EU documents can reveal discrepancies between the EU’s projected values and its (discursive) actions.|$|R
30|$|The {{first and}} second points on the list are closely related. The {{annotation}} process is based on manual work conducted {{with the help of}} the UAM CorpusTool, the main purpose of which is to act as a storing and counting device. It offers an interface for the user to conduct the annotations but is rather a passive participant for it cannot recognise appraisal on its own. Furthermore, a corpus tool helps in staying consistent, enabling constant revision and making changes with a few clicks. Since the corpus consists of institutional texts in which evaluations are usually very subtle and implicit appraisal is favoured, manual search was the most viable option as otherwise it would have been practically impossible to trace all the cases of appraisal. Keyword searches and reliance on explicit appraisal can be of help at the initial stages but they only get the analysis to a certain point due to diversity of evaluative items and <b>targets</b> <b>of</b> <b>evaluation</b> and due to many implicit cases of appraisal appearing “alone,” that is, without explicit instances in the vicinity.|$|R
50|$|An ST defines {{information}} assurance security and functional {{requirements for the}} given information system product, which is called the <b>Target</b> <b>of</b> <b>Evaluation</b> (TOE). An ST is a complete and rigorous description of a security problem in terms of TOE description, threats, assumptions, security objectives, security functional requirements (SFRs), security assurance requirements (SARs), and rationales. The SARs are typically given as a number 1 through 7 called Evaluation Assurance Level (EAL), indicating the depth and rigor of the security evaluation, usually {{in the form of}} supporting documentation and testing, that the product meets the SFRs.|$|E
5000|$|Consequently, {{microkernel}} designs {{have been}} used for systems designed for high-security applications, including KeyKOS, EROS and military systems. In fact common criteria (CC) at the highest assurance level (Evaluation Assurance Level (EAL) 7) has an explicit requirement that the <b>target</b> <b>of</b> <b>evaluation</b> be [...] "simple", an acknowledgment of the practical impossibility of establishing true trustworthiness for a complex system. Unfortunately, again, the term [...] "simple" [...] is misleading and ill-defined. At least the Department of Defense Trusted Computer System Evaluation Criteria introduced somewhat more precise verbiage at the B3/A1 classes: ...|$|E
50|$|EAL1 is {{applicable}} where some confidence in correct operation is required, but thethreats to security are not viewed as serious. It {{will be of}} value where independentassurance is required to support the contention that due care has been exercised withrespect {{to the protection of}} personal or similar information.EAL1 provides an evaluation of the TOE (<b>Target</b> <b>of</b> <b>Evaluation)</b> as made available to the customer, includingindependent testing against a specification, and an examination of the guidancedocumentation provided. It is intended that an EAL1 evaluation could be successfullyconducted without assistance from the developer of the TOE, and for minimal cost. Anevaluation at this level should provide evidence that the TOE functions in a mannerconsistent with its documentation, and that it provides useful protection againstidentified threats.|$|E
40|$|This paper {{introduces}} a generic {{framework for the}} <b>evaluation</b> <b>of</b> the quality of knowledge-based technologies. The evaluation process is described, including steps {{that have to be}} completed in advance, such as the selection of appropriate criteria, metrics, and references. The flexibility of this framework enables its implementation {{for a wide range of}} technologies. The authors present its applicability on the basis <b>of</b> the <b>evaluation</b> <b>of</b> Ontology Reasoning components, developed within the Core Technology Cluster (CTC) of the THESEUS program ([URL] The main <b>target</b> <b>of</b> the <b>evaluation</b> framework points to novel approaches to improve the quality of the technologies used for the acquisition, processing and better use of the knowledge from multiple sources...|$|R
30|$|The {{checklist}} {{was originally}} {{created for the}} purposes of my doctoral dissertation project (Tupala in prep. n.d.), and the intention in this article was to clarify the annotation process and to propose ways to conduct quantitative appraisal analysis in a manner that connects it to the institutional and ideological context where the EU’s migration texts were produced. As I already indicated in the first section, previous quantitative appraisal studies have mainly focused on specific instances in rather restricted sets of data. My aim, however, is to promote the use of the appraisal framework in larger scale quantitative studies that not only account for each instance of appraisal and make use of a broad range of appraisal types (with their sub-systems/categories) and their layeredness but also ultimately chart the co−/context of evaluative expressions. Following the points on the checklist can hence lead to deeper and more grounded quantitative appraisal analyses. Detailed manual annotation, although very time-consuming with a large set of data, allows the analyst to find the rare, unpredictable and implicit instances <b>of</b> <b>evaluation.</b> The use <b>of</b> a corpus tool enables the coding of the co-text of cases of appraisal which, when combined with the appraisal annotation, binds the evaluative items to their <b>targets</b> <b>of</b> <b>evaluation.</b>|$|R
40|$|The paper {{proposes a}} {{framework}} for analysis of quality in sport services from the perspective <b>of</b> (a) <b>targets</b> <b>of</b> quality, (b) standards of quality, and (c) evaluators of quality. It is proposed that any quality <b>evaluation</b> <b>of</b> a service should begin by identifying the <b>targets</b> <b>of</b> quality <b>evaluations</b> (i. e., breaking down that service into smaller discrete and distinct elements), and assessing the <b>targets</b> in terms <b>of</b> consumer and human service components. In addition, the paper highlights the relevance of different standards of quality to different <b>targets</b> <b>of</b> quality, and the relative significance of the clients, the service providers, and the managers as arbiters of quality. These segmental perspectives on quality in a service operation are expected {{to result in a}} truly Gestalt view of a service enterprise. The framework would facilitate quality initiatives such as quality assurance and benchmarking. ...|$|R
5000|$|Security Target (ST) - the {{document}} that identifies the security {{properties of the}} <b>target</b> <b>of</b> <b>evaluation.</b> The ST may claim conformance {{with one or more}} PPs. The TOE is evaluated against the SFRs (Security Functional Requirements. Again, see below) established in its ST, no more and no less. This allows vendors to tailor the evaluation to accurately match the intended capabilities of their product. This means that a network firewall does not have to meet the same functional requirements as a database management system, and that different firewalls may in fact be evaluated against completely different lists of requirements. The ST is usually published so that potential customers may determine the specific security features that have been certified by the evaluation.|$|E
40|$|This {{document}} {{provides the}} basis for an evaluation of a specific <b>Target</b> <b>of</b> <b>Evaluation</b> (TOE), the nCipher nShield Family of Hardware Security Modules (HSMs) Firmware Version 2. 33. 60. This Security Target (ST) defines a set of assumptions about the aspects of the environment, a list of threats that the product intends to counter, a set of security objectives,...|$|E
40|$|The main {{objective}} of this work is to investigate the IP stack performance on popular platforms such as Windows, RedHat and FreeBSD and to compare their performances so that the right platform for a specific network application can be recommended. Internet protocol version 6 (IPv 6) has been chosen as <b>target</b> <b>of</b> <b>evaluation</b> due to its increasing popularity in recent years...|$|E
40|$|This paper {{presents}} <b>evaluation</b> <b>of</b> support vector {{machine for}} object detection in images. A {{support vector machine}} is used to classify each pixel in the image and identify region of interest based on localized color patterns. General linear transformations of the image through linear, quadratic and radial basis function kernel are considered. The main problem in this approach is high run-time complexity of SVMs. To alleviate this problem we are using multiple kernels. The main <b>target</b> <b>of</b> this <b>evaluation</b> is introducing an object recognition system {{based on the results}} of kernel comparisons. The comparison result shows that radial basis function produces robust and efficient object detection...|$|R
40|$|The {{aim of this}} {{independent}} {{study was to investigate}} differences in efficacy between HR, (0 -[beta-hydroxyethyl]-rutosides) and D+H (500 mg, diosmin+hesperidin) in patients with chronic venous insufficiency (CVI). A first group of 90 patients with severe venous hypertension (CVI, ankle swelling) were randomized into an HR or a D+H group. The HR group received oral HR (2 g/day, 8 weeks); the D+H group received a 500 mg tablet 3 times daily for 8 weeks. A second group of comparable patients was included in a registry following the same study format. Patients were openly included; the 2 treatments were administered with the same methods and procedures. Clinical conditions were comparable to those described in the randomized study. Patients treated for at least 8 weeks were included in the registry. A number of physi-cians (specialists or general practitioners) included patients when they considered that clinical conditions were compatible with using 1 of the 2 treatments on the basis <b>of</b> their personal <b>evaluation</b> and experience. When cases were compatible with the registry, the prescribing physician communicated the case. Patients were evaluated without interfering with the treatment. Main <b>targets</b> <b>of</b> <b>evaluation</b> were skin flux at rest (RF), strain-gauge-derived rate of ankle swelling (RAS), and analogue symptoms score (ASLS). Ninety subjects completed th...|$|R
30|$|Kaltenbacher (2006 : 274 – 275) contemplates ways of {{identifying}} implicit appraisal in corpora including keyword searches (evaluative items or <b>targets</b> <b>of</b> <b>evaluation)</b> and using explicit appraisal {{as a starting}} point as implicit cases often occur in the vicinity of explicit ones. Albeit both of these offer a viable place to start, neither is enough to enable finding all cases <b>of</b> implicit <b>evaluation.</b> With keyword searches, it is possible to overlook some (less common) items that might be important for the analysis. Relying on explicit appraisal does not take the analysis very far either as, with institutional texts, it is often the case that a large number <b>of</b> the <b>evaluations</b> are carried out implicitly due to the formal nature of the texts that does not encourage strong attitudinal viewpoints. These cases would be left outside the scope of the analysis if one only considered the immediate co-text of explicit appraisal. Nevertheless, explicit cases in earlier parts of the text guide the reader’s interpretation of the following implicit cases by creating a setting through which readers are conditioned to accept certain meanings or value positions (see e.g. ‘dynamic reader positioning’ in Coffin and O’Halloran 2005, Coffin and O’Halloran 2006; White 2008). Once a meaning has been established, for instance labour migrants having a positive impact on the EU’s economy, it can be backed up later on in the text by deploying implicit means, such as statements of fact about the decrease in the domestic working-age population.|$|R
40|$|This {{document}} {{provides the}} basis for an evaluation of a specific <b>Target</b> <b>of</b> <b>Evaluation</b> (TOE), Sentinel 7. 2. 1. This Security Target (ST) defines a set of assumptions about the aspects of the environment, a list of threats that the product intends to counter, a set of security objectives, a set of security requirements and the IT security functions provided by the TOE which meet the set o...|$|E
30|$|The {{amount of}} carbon {{emissions}} in {{the life cycle of}} a timber check dam [CEF(1)] (t-C/dam) and a concrete check dam [CEFc(1)] (t-C/dam) was adapted from the {{amount of carbon}} emissions derived from fossil fuel consumption in the material production process (the process ranging from resource extraction to manufacturing in Fig.  2) and the construction process (the construction process in Fig.  2) mentioned in the previous study [8]. Additionally, the maintenance process was excluded from the <b>target</b> <b>of</b> <b>evaluation</b> on the amount of carbon emissions since this study does not take repairs into consideration due to the reasons mentioned previously. The disposal process was also not considered for the <b>target</b> <b>of</b> <b>evaluation</b> since the check dams are usually not removed or disposed of, as discussed previously. Therefore, this study adapted 20 t-CO 2 /dam (material production process: 16 and construction process: 4) for the timber check dam and 45 t-CO 2 /dam (material production process: 44 and construction process: 1) for the concrete check dam, and then converted it to the carbon content by multiplying the figure obtained from dividing the atomic weight of carbon by the molecular weight of carbon dioxide (= 12 / 44).|$|E
40|$|This {{document}} {{represents the}} development of a protection profile (PP) for the IEC (International Electrotechnical Commission) protocol TASE. 2 (Tele-control Application Service Element. 2). A protection profile states assumptions about the TOE (<b>Target</b> <b>of</b> <b>Evaluation),</b> identifies threats to the TOE based on the assumptions, gives security goals to counter the threats, and finally identifies security functions to satisfy the security goals. Developing protection profiles for each protocol is a significant step towards developing measurable security for electric power automation systems. As an extension of the PP, the authors offer a generalization to any protocol at the evaluation assurance level (EAL) 2...|$|E
25|$|SemEval-2007 (Senseval-4) {{took place}} in 2007, {{followed}} by a workshop held in conjunction with ACL in Prague. SemEval-2007 included 18 different tasks <b>targeting</b> the <b>evaluation</b> <b>of</b> systems for the semantic analysis of text. A special issue of Language Resources and Evaluation {{is devoted to the}} result.|$|R
30|$|The {{applicability}} of the presented propagation predictions {{to the study of}} a macro-femtocell hybrid scenario is presented here by means of mobile WiMAX (IEEE 802.16 e- 2005) system-level simulations with private access femtocells. The <b>target</b> <b>of</b> this experimental <b>evaluation</b> is to show how a measurements-based calibrated FDTD model can help the operator to predict common interference problems between users of the macrocell and the femtocell.|$|R
30|$|The {{categorisation}} <b>of</b> underlying <b>targets</b> <b>of</b> <b>evaluation,</b> however, is {{not that}} straightforward. Thompson (2014 : 57) contemplates underlying targets and the semantically-based view when discussing whether to label a case of appraisal as judgement or as appreciation. According to the semantically-based view, if the underlying target is human (or an institution whose actions are comparable to those <b>of</b> humans), the <b>evaluation</b> can be categorised as judgement. To clarify, I refer back to examples  6 and 7 which at the superficial level are appreciations towards the EU migration policy and (some) member states respectively. Nevertheless, they also contain implicit judgement which is targeted at the EU and the member states’ governments, hence revealing reverence towards the EU and condemnation towards the ill-behaved governments. However, Thompson (2014 : 58 – 59) rejects the semantically-based view as he indicates that the actual (literal) wording should {{be used as a}} starting point for analysis instead of attempting to decipher the underlying meaning. Here, I differ from Thompson as I use both judgement and appreciation in cases like these for the sake of increased depth of the analysis. This helps in uncovering how the EU builds its attitudinal stance towards migration and different groups of migrants since preferring the use of impersonalised appreciation instead of judgement is a value choice in itself.|$|R
