104|10000|Public
5000|$|<b>Total</b> <b>Error</b> <b>Rate</b> = ((INF + IF)/ (C + INF + IF)) * 100%Not Corrected Error Rate = (INF/ (C + INF + IF)) * 100%Corrected Error Rate = (IF/ (C + INF + IF)) * 100% ...|$|E
50|$|Multiple testing {{correction}} {{refers to}} re-calculating probabilities {{obtained from a}} statistical test which was repeated multiple times. In order to retain a prescribed family-wise error rate α in an analysis involving more than one comparison, the error rate for each comparison must be more stringent than α. Boole's inequality implies that if each of m tests is performed to have type I error rate α/m, the <b>total</b> <b>error</b> <b>rate</b> will not exceed α. This is called the Bonferroni correction, {{and is one of}} the most commonly used approaches for multiple comparisons.|$|E
3000|$|... [...]. The optimal {{number of}} CRs and {{optimized}} {{value of the}} local threshold can be calculated by minimizing the <b>total</b> <b>error</b> <b>rate</b> as shown in [7].|$|E
40|$|Studies were {{conducted}} on a method of direct inoculation of MicroScan dried overnight and of rapid panels with positive aerobic blood cultures obtained from the BacT/Alert to determine antimicrobial susceptibilities. Inocula were limited to specimens that appeared unimicrobic on Gram stain. Results were compared to those obtained from panels inoculated following subculture. For 133 gram-negative bacilli, there were 94. 7 and 93. 5 % categorical agreements between direct and standard methods for all drugs tested with overnight and rapid panels, respectively. For 104 gram-positive cocci, there were 93. 2 and 93. 1 % categorical agreements for overnight and rapid panels, respectively. The major <b>error</b> (false resistance) <b>rate</b> for gram negatives was 1. 4 % for overnight versus 0. 7 % for rapid panels. The very major <b>error</b> (false susceptibility) <b>rate</b> was 2. 7 % for overnight versus 8. 1 % for rapid panels. The <b>total</b> <b>error</b> <b>rates</b> were 1. 6 % for overnight panels and 1. 5 % for rapid panels. The major <b>error</b> <b>rates</b> for gram-positive direct susceptibility tests were 2. 6 % for overnight and 2. 5 % for rapid panels. The very major <b>error</b> <b>rates</b> were 8. 8 and 7. 2 % for overnight and rapid panels, respectively. <b>Total</b> <b>error</b> <b>rates</b> were 3. 6 % for overnight and rapid gram-positive panels. These findings suggest that susceptibility results obtained from directly inoculated gram-negative overnight panels have the greatest correlation to those obtained by standard methods. When discrepant results occur with direct-susceptibility testing, {{they are more likely}} to show false susceptibility than false resistance...|$|R
40|$|In {{this paper}} we {{investigate}} benefits of classifier combination (fusion) for a multimodal system for personal identity verification. The system uses frontal face images and speech. We {{show that a}} sophisticated fusion strategy enables the system to outperform its facial and vocal modules when taken seperately. We show that both trained linear weighted schemes and fusion by Support Vector Machine classifier leads to a significant reduction of <b>total</b> <b>error</b> <b>rates.</b> The complete system is tested on data from a publicly available audio-visual database (XM 2 VTS, 295 subjects) according to a published protocol...|$|R
40|$|In this paper, we {{describe}} {{a method to}} detect syllabic nuclei in continuous speech. It employs two basic and robust acoustic features, periodicity and energy, to detect syllable landmarks. This method is evaluated on TIMIT, noise additive TIMIT and NTIMIT datasets with typical <b>total</b> <b>error</b> <b>rates</b> of around 30 % in all the datasets, except for extremely adverse 0 dB signal-noise-ratio environments, while HMM-based systems degrade rigorously. Based on the landmarks, a vowel classifier is further constructed and achieves the same performance as HMM-based systems. Index Terms: syllable detection, robustness, vowel classification. 1...|$|R
3000|$|... {{against the}} noise attack. It follows that the <b>total</b> <b>error</b> <b>rate</b> {{in the list}} {{decoding}} scenario is considerably worse than it is with maximum heuristic decoding.|$|E
40|$|Detection of burst-related impulses, such {{as those}} {{accompanying}} plosive stop consonants, is an important problem for accurate measurement of acoustic features for recogntion (e. g., voice-onset-time) and for accurate automatic phonetic alignment. The proposed method of burst detection utilizes techniques for identifying and combining information about specific acoustic characteristics of bursts. One key element of the proposed method {{is the use of}} a measurement of intensity discrimination based on models from perceptual studies. Our experiments compared the proposed method of burst detection to the support vector machine (SVM) method, described below. The <b>total</b> <b>error</b> <b>rate</b> for the proposed method is 13. 2 % on the test-set partition of the TIMIT corpus, compared to a <b>total</b> <b>error</b> <b>rate</b> of 24 % for the SVM method. 1...|$|E
40|$|We {{develop a}} custom Bit Error Rate test bench based on Altera’s Stratix II GX {{transceiver}} signal integrity development kit, demonstrate it on point-to-point serial optical link with data rate up to 5 Gbps, {{and compare it}} with commercial stand alone tester. The 8 B/ 10 B protocol is implemented and its effects studied. A variable optical attenuator is inserted in the fibre loop to induce transmission degradation and to measure receiver sensitivity. We report comparable receiver sensitivity results using the FPGA based tester and commercial tester. The results of the FPGA also shows {{that there are more}} one-tozero bit flips than zero-to-one bit flips at lower error rate. In 8 B/ 10 B coded transmission, there are more word errors than bit flips, and the <b>total</b> <b>error</b> <b>rate</b> is less than two times that of non-coded transmission. <b>Total</b> <b>error</b> <b>rate</b> measured complies with simulation results, according to the protocol setup...|$|E
40|$|Abstract — Optimum power {{allocation}} for the V-BLAST algorithm, {{which is}} based on various criteria (average and instantaneous block and <b>total</b> <b>error</b> <b>rates</b> (BLER and TBER)), is considered. Closed-form expressions are derived for high-SNR case in a Rayleigh fading channel. It is demonstrated that, in that case, the optimization “on average ” is almost identical to the instantaneous one (while the former requires only the feedback “on average”, the latter requires instantaneous feedback and hence is of higher complexity). The BLER and TBER optimization criteria result in the same performance. Power optimization (of un-ordered BLAST) and optimal ordering result in the same performance improvement at high SNR. I...|$|R
50|$|In Canada, about 3.5% {{of total}} {{employment}} insurance expenditure was attributable to {{fraud and error}} in 2003. The <b>total</b> fraud and <b>error</b> <b>rate</b> in welfare allowances {{is estimated to be}} 3-5%.|$|R
50|$|One of the {{earliest}} uses of 2D Error Mapping {{was applied to the}} recorded errors from a transverse-scan digital tape recording device made by Ampex Corporation. The accompanying Media Scan image demonstrates that error locations are key to recognizing that multiple error-producing syndromes are occurring simultaneously and affecting the <b>total</b> bit <b>error</b> <b>rate</b> experienced during the test recording session.|$|R
40|$|This paper {{investigates the}} {{performance}} of cooperative spectrum sensing in cognitive radio networks using the stochastic geometry tools. In order {{to cope with the}} diversity of received signal-to-noise ratios at secondary users, a practical and efficient cooperative spectrum sensing model is proposed and investigated based on the generalized likelihood ratio test detector. In order to investigate the cooperative spectrum sensing system, the theoretical expressions of the probabilities of false alarm and detection of the local decision are derived. The optimal number of cooperating secondary users is then investigated to achieve the minimum <b>total</b> <b>error</b> <b>rate</b> of the final decision by assuming that the secondary users follow a homogeneous Poisson point process. Moreover, the theoretical expressions for the achievable ergodic capacity and throughput of the secondary network are derived. Furthermore, the technique of determining an appropriate number of cooperating secondary users is proposed in order to maximize the achievable ergodic capacity and throughput of the secondary network based on a target <b>total</b> <b>error</b> <b>rate</b> requirement. The analytical and simulation results validate the chosen optimal number of collaborating secondary users in terms of spectrum sensing, achievable ergodic capacity, and throughput of the secondary network...|$|E
40|$|SUMMARY This paper {{investigates the}} {{detection}} performance of an improved energy detector for a secondary user with spatially correlated multiple antennas. In an improved energy detector, an arbitrary positive power operation p replaces the squaring operation {{in a conventional}} energy detector, and the optimum value of p that gives the best detection perfor-mance may be different from 2. Firstly, for a given value of p, we derive closed-form expressions for the probability of detection and the probability of false alarm when antennas at the secondary user are exponentially cor-related. We then find the optimum value of p for two different detection criteria−maximizing the probability of detection for a target probability of false alarm, and minimizing the probability of false alarm for a target prob-ability of detection. We show that the optimum p {{is strongly dependent on}} system parameters like number of antennas, antenna correlation coeffi-cient among multiple antennas, and average received signal-to-noise ratio (SNR). From results, we infer that, in low SNR regime, the effect of an-tenna correlation is less pronounced on the optimum p. Finally, we find the optimum values of p and threshold jointly that minimize the <b>total</b> <b>error</b> <b>rate.</b> key words: cognitive radio; correlation, improved energy detector, multi-ple antennas, <b>total</b> <b>error</b> <b>rate</b> 1...|$|E
40|$|Abstract. User {{authentication}} is {{an important}} step to protect informa-tion and in this field face biometrics is advantageous. Face biometrics is natural, easy to use and less human-invasive. Unfortunately, recent work has revealed that face biometrics is vulnerable to spoofing attacks using low-tech cheap equipments. This article presents a countermeasure against such attacks based on the LBP −TOP operator combining both space and time information into a single multiresolution texture descrip-tor. Experiments carried out with the REPLAY ATTACK database show a Half <b>Total</b> <b>Error</b> <b>Rate</b> (HTER) improvement from 15. 16 % to 7. 60 %. ...|$|E
30|$|To {{evaluate}} the performance, we selected two settings: multi-modal fusion (for gait, head and height) and bi-modal fusion (for gait and head). We then evaluated the accuracy in both verification and identification scenarios with typical {{measures such as}} ROC curves, EERs, FRRs at specific FARs, area under curves (AUCs), the half <b>total</b> <b>error</b> <b>rates</b> (HTERs), which is {{the average of the}} FAR and the FRR, CMC curves, and rank-n identification rates for each subset. Here, the HTER is calculated based on [34]. We select an optimal threshold Δ based on the concept that the distributions of genuine and imposter accesses are equal and the threshold is set at a value that minimizes 1 / 2 (FAR (Δ)+FRR (Δ)). We set the threshold using the training dataset.|$|R
40|$|Assessing whether {{two models}} are {{statistically}} {{significantly different from}} each other is a very important step in research, although it has unfortunately not received enough attention in the field of person authentication. Several performance measures are often used to compare models, such as half <b>total</b> <b>error</b> <b>rates</b> (HTERs) and equal <b>error</b> <b>rates</b> (EERs), but most being aggregates of two measures (such as the false acceptance rate and the false rejection rate), simple statistical tests cannot be used as is. We show in this paper how to adapt one of these tests in order to compute a confidence interval around one HTER measure or to assess the statistical significantness of the difference between two HTER measures. We also compare our technique with other solutions that are sometimes used in the literature and show why they yield often too optimistic results (resulting in false statements about statistical significantness) ...|$|R
40|$|Abstract — In {{contrast}} to a maximum-likelihood de-coder, it is often desirable to use an incomplete de-coder that can detect its decoding errors with high probability. One common choice is the bounded dis-tance decoder. Bounds are derived for the <b>total</b> word <b>error</b> <b>rate,</b> Pw, and the undetected <b>error</b> <b>rate,</b> Pu. Excellent agreement is found with simulation results for a small code, and the bounds are shown to be tractable for a larger code. I...|$|R
40|$|In this paper, we {{introduce}} a new fingerprint enhancement algorithm which used FFT and Gaussian filter. FFT is applied on a block, of size 32 x 32 pixel and Gaussian filter is applied in each interconnection ofeach block. This make ridge smoothness and also reduce the “hairy” structure in the ridge when it isthinning. The experimental result shows that the implementation of FFT and Gaussian filter in thepreprocessing stage make less effort in the postprocessing stage and hence the <b>total</b> <b>error</b> <b>rate</b> of theproposed algorithm is very low...|$|E
40|$|International audienceProstate {{cancer is}} the most common cancer in men over 50 years of age and {{it has been shown that}} nuclear {{magnetic}} resonance spectra are sensitive enough to distinguish normal and cancer tissues. In this paper, we propose a classification technique of spectra from magnetic resonance spectroscopy. We studied automatic classification with and without quantification of metabolite signals. The dataset is composed of 22 patient datasets with a biopsy-proven cancer, from which we extracted 2464 spectra from the whole prostate and of which 1062 were localised in the peripheral zone. The spectra were manually classed into 3 different categories by a spectroscopist with 4 years experience in clinical spectroscopy of prostate cancer: undetermined, healthy and pathologic. We used different preprocessing methods (module, phase correction only, phase correction and baseline correction) as input for Support Vector Machine and for Multilayer Perceptron, and we compared the results with those from the expert. If we class only healthy and pathologic spectra we reach a <b>total</b> <b>error</b> <b>rate</b> of 4. 51 %. However, if we class all spectra (undetermined, healthy and pathologic) the <b>total</b> <b>error</b> <b>rate</b> rises to 11. 49 %. We have shown in this paper that the best results are obtained using the pre-processed spectra without quantification as input for the classifiers and we confirm that Support Vector Machine are more efficient than Multilayer Perceptron in processing high dimensional data...|$|E
40|$|We {{present a}} {{state-of-the-art}} bi-modal authentica-tion system for mobile environments, using session vari-ability modelling. We examine inter-session variabil-ity modelling (ISV) and joint factor analysis (JFA) for both face and speaker authentication and evaluate our {{system on the}} largest bi-modal mobile authentication database available, the MOBIO database, with over 61 hours of audio-visual data captured by 150 people in uncontrolled environments on a mobile phone. Our sys-tem achieves 2. 6 % and 9. 7 % half <b>total</b> <b>error</b> <b>rate</b> {{for male and female}} trials respectively – relative improve-ments of 78 % and 27 % compared to previous results. 1...|$|E
40|$|Outage and <b>error</b> <b>rate</b> {{performance}} of the ordered BLAST with more than 2 transmit antennas is evaluated for i. i. d. Rayleigh fading channels. A number of lower and upper bounds on the 1 st step outage probability at any SNR are derived, which are further used to obtain accurate approximations to average block and <b>total</b> <b>error</b> <b>rates.</b> For m Tx antennas, {{the effect of the}} optimal ordering at the first step is an m-fold SNR gain. As m increases to infinity, the BLER decreases to zero, which is a manifestation of the space-time autocoding effect in the V-BLAST. While the sub-optimal ordering (based on the before-projection SNR) suffers a few dB SNR penalty compared to the optimal one, it has a lower computational complexity and a 3 dB SNR gain compared to the unordered V-BLAST and can be an attractive solution for low-complexity/low-energy systems. Uncoded D-BLAST exhibits the same outage and <b>error</b> <b>rate</b> performance as that of the V-BLAST. An SNR penalty of the linear receiver interfaces compared to the BLAST is also evaluated. Comment: accepted by IEEE Transactions on Wireless Communication...|$|R
40|$|A unified {{analytical}} {{framework for}} optimum power allocation in the unordered V-BLAST algorithm and its comparative performance analysis are presented. Compact closed-form approximations for the optimum power allocation are derived, based on average <b>total</b> and block <b>error</b> <b>rates.</b> The {{choice of the}} criterion has little impact on the power allocation and, overall, the optimum strategy is to allocate more power to lower step transmitters and less to higher ones. High-SNR approximations for optimized average block and <b>total</b> <b>error</b> <b>rates</b> are given. The SNR gain of optimization is rigorously defined and studied using analytical tools, including lower and upper bounds, high and low SNR approximations. The gain is upper bounded {{by the number of}} transmit antennas, for any modulation format and type of fading channel. While the average optimization is less complex than the instantaneous one, its performance is almost as good at high SNR. A measure of robustness of the optimized algorithm is introduced and evaluated. The optimized algorithm is shown to be robust to perturbations in individual and total transmit powers. Based on the algorithm robustness, a pre-set power allocation is suggested as a low-complexity alternative to the other optimization strategies, which exhibits only a minor loss in performance over the practical SNR range...|$|R
40|$|Abstract—A unified {{analytical}} {{framework for}} optimum power allocation in the unordered V-BLAST algorithm and its comparative performance analysis are presented. Compact closed-form approximations for the optimum power allocation are derived, based on average <b>total</b> and block <b>error</b> <b>rates.</b> The {{choice of the}} criterion has little impact on the power allocation and, overall, the optimum strategy is to allocate more power to lower step transmitters and less to higher ones. High-SNR approximations for optimized average block and <b>total</b> <b>error</b> <b>rates</b> are given. The SNR gain of optimization is rigorously defined and studied using analytical tools, including lower and upper bounds, high and low SNR approximations. The gain is upper bounded {{by the number of}} transmit antennas, for any modulation format and type of fading channel. While the average optimization is less complex than the instantaneous one, its performance is almost as good at high SNR. A measure of robustness of the optimized algorithm is introduced and evaluated. The optimized algorithm is shown to be robust to perturbations in individual and total transmit powers. Based on the algorithm robustness, a pre-set power allocation is suggested as a low-complexity alternative to the other optimization strategies, which exhibits only a minor loss in performance over the practical SNR range. Index Terms—Multi-antenna (MIMO) system, V-BLAST, power allocation, performance analysis. I...|$|R
40|$|Abstract. Single-sensor {{approaches}} to multimodal biometric authen-tication targeting the human hand in multiple-matcher scenarios pro-vide higher security {{in terms of}} accuracy and resistance to biometric system attacks than unimodal systems. This paper introduces a novel multimodal hand biometric system using palmar images acquired by a commercially available atbed scanner. Hence, the presented approach to personal recognition is independent of specic biometric sensors, such as ngerprint readers or palmprint scanners. Experimental results with a minimum half <b>total</b> <b>error</b> <b>rate</b> of 0. 003 % using a database of 443 hand im-ages will illustrate the performance improvement when hand-geometry, ngerprint and palmprint-based features are combined. ...|$|E
40|$|This paper {{presents}} optimization {{issues of}} energy detection (ED) thresholds in cooperative spectrum sensing (CSS) {{with regard to}} general Gaussian noise. Enhanced ED thresholds are proposed to overcome sensitivity of multiple noise uncertainty. Two-steps decision pattern and convex samples thresholds have been put forward under Gaussian noise uncertainty. Through deriving the probability of detection (Pd) and the probability of false alarm (Pf) for independent and identical distribution (i. i. d.) SUs, we obtain lower <b>total</b> <b>error</b> <b>rate</b> (Qe) with proposed ED thresholds at low signal-to-noise-ratio (SNR) condition. Furthermore, simulation results show that proposed schemes outperform most other noise uncertainty plans. Comment: 5 page...|$|E
40|$|We have {{developed}} a program for rapid identity-searching of DNA sequences allowing several percentages of sequencing error rates. The program was applied to a large-scale searching of Expressed Sequence Tags (ESTs) against the GenBank sequences, and from this searching results the error information of ESTs was obtained. The 15, 666 sequences of human ESTs were searched in the primate division in GenBank release 80 within 23. 3 hours that is only one-thirty of the time needed when FASTA is used. The <b>total</b> <b>error</b> <b>rate</b> 2. 45 percent {{was obtained from the}} alignments between the ESTs and the primate sequences satisfying the identity-conditions. ...|$|E
40|$|Abstract — Outage and <b>error</b> <b>rate</b> {{performance}} of the ordered BLAST with more than 2 transmit antennas is evaluated for i. i. d. Rayleigh fading channels. A number of lower and upper bounds on the 1 st step outage probability at any SNR are derived, which are further used to obtain accurate approximations to average block and <b>total</b> <b>error</b> <b>rates.</b> For m Tx antennas, {{the effect of the}} optimal ordering at the first step is an m-fold SNR gain. As m increases to infinity, the BLER decreases to zero, which is a manifestation of the space-time autocoding effect in the V-BLAST. While the sub-optimal ordering (based on the before-projection SNR) suffers a few dB SNR penalty compared to the optimal one, it has a lower computational complexity and a 3 dB SNR gain compared to the unordered V-BLAST and can be an attractive solution for low-complexity/low-energy systems. Uncoded D-BLAST exhibits the same outage and <b>error</b> <b>rate</b> performance as that of the V-BLAST. An SNR penalty of the linear receiver interfaces compared to the BLAST is also analytically evaluated. Index Terms — Multi-antenna (MIMO) system, V-BLAST, performance analysis, autocoding effect I...|$|R
40|$|In {{contrast}} to a maximum-likelihood decoder, it is often desirable to use an incomplete decoder that can detect its decoding errors with high probability. One common choice is the bounded distance decoder. Bounds are derived for the <b>total</b> word <b>error</b> <b>rate,</b> Pw, and the undetected <b>error</b> <b>rate,</b> Pu. Excellent agreement is found with simulation results for a small code, and the bounds are shown to be tractable for a larger code. Comment: International Symposium on Information Theory (ISIT), July 2012, Recent Results session, 2 page...|$|R
40|$|Bankruptcy is {{a costly}} event. Holders of {{publicly}} traded securities {{can rely on}} security prices to reflect their risk. Other stakeholders have no such mechanism. Hence, methods for accurately forecasting bankruptcy would be valuable to them. A large body of literature has arisen on bankruptcy forecasting with statistical classification since Beaver (1967) and Altman (1968). Reported <b>total</b> <b>error</b> <b>rates</b> typically are 10 %- 20 %, suggesting that these models reveal information which otherwise is unavailable and has value after financial data is released. This conflicts with evidence on market efficiency which indicates that securities markets adjust rapidly and actually anticipate announcements of financial data. Efforts to resolve this conflict with event study methodology have run afoul of market model specification difficulties. A different approach is taken here. Most extant criticism of research design in this literature concerns inferential techniques but not sampling design. This paper attempts to resolve major sampling design issues. The most important conclusion concerns the usual choice of the individual firm as the sampling unit. While this choice is logically inconsistent with how a forecaster observes financial data over time, no evidence of bias could be found. In this paper, prediction performance is evaluated in terms of expected loss. Most authors calculate <b>total</b> <b>error</b> <b>rates,</b> which fail to reflect documented asymmetries in misclassification costs and prior probabilities. Expected loss overcomes this weakness and also offers a formal means to evaluate forecasts {{from the perspective of}} stakeholders other than investors. This study shows that cost of misclassifying bankruptcy must be at least an order of magnitude greater than cost of misclassifying nonbankruptcy before discriminant analysis methods have value. This conclusion follows from both sampling experiments on historical financial data and Monte Carlo experiments on simulated data. However, the Monte Carlo experiments reveal that as the cost ratio increases, robustness of linear discriminant rules improves; performance appears to depend more on the cost ratio than form of the distributions...|$|R
40|$|Single-sensor {{approaches}} to multimodal biometric authentication targeting the human hand in multiple-matcher scenarios provide higher security {{in terms of}} accuracy and resistance to biometric system attacks than unimodal systems. This paper introduces a novel multimodal hand biometric system using palmar images acquired by a commercially available flatbed scanner. Hence, the presented approach to personal recognition is independent of specific biometric sensors, such as fingerprint readers or palmprint scanners. Experimental results with a minimum half <b>total</b> <b>error</b> <b>rate</b> of 0. 003 % using a database of 443 hand images will illustrate the performance improvement when hand-geometry, fingerprint and palmprint-based features are combined...|$|E
40|$|The {{purpose of}} the study is to develop a {{logistic}} model based on financial statement information to identify qualified audit reports. The empirical data are retrieved from audit reports from thirty-seven publicly-traded companies (HeSE) in the years 1992, 1993 and 1994. Thus, there are in all 111 audit reports of which only eight are qualified in the way of including remarks or supplementary information. These eight qualifications concerned three companies during the period of study. The qualification decision (0 / 1) is explained by sixteen financial ratios and by the audit lag. Univariate analysis showed that the qualification of an audit report is mainly associated with low profitability, high indebtedness and low (negative) growth. The multivariate logistic model showed that the likelihood of receiving a qualification is larger, the lower the growth of the firm, the lower the share of equity in balance sheet and the smaller the number of employees. The <b>total</b> <b>error</b> <b>rate</b> of the model in Lachenbruch validation was only 5. 4 %. When two qualified reports containing remarks on an additional and a separate auditor were considered as non-qualified, the corresponding <b>total</b> <b>error</b> <b>rate</b> for a re-estimated logistic model was as low as 1. 8 %. This model also included the audit lag as an explanatory variable. The results of the study indicate that an efficient model to explain qualifications in the audit reports of Finnish publicly-traded companies can be found. ...|$|E
30|$|Throughout the {{analysis}} of the results presented here, one can clearly note that the EGG features have a strong influence on the performance of SV in a noise environment. As indicated in Figure 8 and Table 2, substantial gains in speaker verification in a high noise environment were obtained. Analyzing the SNR performance, there are the three different ranges, I, II, and III where System 1, System 3,and System 4 have the best performance, respectively. Therefore, a composite SV can adaptively select one of the three systems, based on the level of noise, achieving a <b>total</b> <b>error</b> <b>rate</b> that is lower than any single system.|$|E
40|$|In {{this paper}} we {{investigate}} benefits of classifier combination (fusion) for a multimodal system for personal identity verification. The system uses frontal face images and speech. We {{show that a}} sophisticated fusion strategy enables the system to outperform its facial and vocal modules when taken seperately. We show that both trained linear weighted schemes and fusion by Support Vector Machine classifier leads to a significant reduction of <b>total</b> <b>error</b> <b>rates.</b> The complete system is tested on data from a publicly available audio-visual database (XM 2 VTS, 295 subjects) according to a published protocol. 2 IDIAP [...] RR 98 - 18 1 Introduction Recognition systems based on biometric features (face, voice, iris, etc [...] .) have {{received a lot of}} attention in recent years Most of the proposed approaches focus on mono-modal identification. The system uses a single modality to find the closest person to the user in a database. Relatively high recognition rates were obtained for different modalities [...] ...|$|R
40|$|To {{enhance the}} {{stability}} and robustness of visual inspection system (VIS), a new surface defect target identification method for copper strip based on adaptive genetic algorithm (AGA) and feature saliency is proposed. First, the study uses gray level cooccurrence matrix (GLCM) and HU invariant moments for feature extraction. Then, adaptive genetic algorithm, which is used for feature selection, is evaluated and discussed. In AGA, <b>total</b> <b>error</b> <b>rates</b> and false alarm rates are integrated to calculate the fitness value, and the probability of crossover and mutation is adjusted dynamically according to the fitness value. At last, the selected features are optimized in accordance with feature saliency and are inputted into a support vector machine (SVM). Furthermore, for comparison, we conduct experiments using the selected optimal feature subsequence (OFS) and the total feature sequence (TFS) separately. The experimental results demonstrate that the proposed method can guarantee the correct rates of classification and can lower the false alarm rates...|$|R
40|$|Complexity-performance {{tradeoff}} {{in different}} implementations of the V-BLAST algorithm is discussed. Low-complexity {{alternatives to the}} optimal ordering procedure, such as ordering at 1 st step, adaptive power allocation and pre-set non-uniform power allocation are proposed. A unified analytical framework for the optimum power allocation in the V-BLAST algorithm is presented. Comparative performance analysis of the optimum power allocation based on various optimization criteria (average and instantaneous block and <b>total</b> <b>error</b> <b>rates)</b> is given. Uniqueness of the optimum power allocation is proven for several scenarios. Compact closed-form approximations for the optimum power allocation and for the optimized <b>error</b> <b>rates</b> are derived. The SNR gain of optimization is rigorously defined and analyzed using analytical tools, including lower and upper bounds, high and low SNR approximations. The gain is upper bounded {{by the number of}} transmitters, for any modulation format and type of fading channel. While the average optimization is less complex than the instantaneous one, its performance is almost as good at high SNR. A measure of robustness of the optimized algorithm is introduced and evaluated. The optimized algorithm is shown to be robust to perturbations in individual and total transmit powers...|$|R
