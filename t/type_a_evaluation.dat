7|10000|Public
40|$|The Guide to the Expression of Uncertainty in Measurement {{describes}} {{a method of}} evaluating measurement uncertainty that involves attributing an underlying variance to each component of the total measurement error. Assessment of such a variance by statistical methods is known as <b>Type</b> <b>A</b> <b>evaluation</b> of uncertainty, while assessment by any other means is known as Type B evaluation. The involvement of degrees of freedom with <b>Type</b> <b>A</b> <b>evaluation</b> is in accordance with statistical theory. However, the use of “degrees of freedom” with Type B evaluation is an idea primarily developed for practicality, and might be improved upon. That idea {{ignores the fact that}} the underlying distribution of the corresponding error will often be assumed to be blunter than normal, as with a uniform distribution, and it can lead to underestimation of the total error variance. Consequently, an alternative concept for Type B evaluation is presented. The resulting analysis fits easily with the use of degrees of freedom in <b>Type</b> <b>A</b> <b>evaluation</b> and leads to an overall method of evaluation that often permits the proper quotation of shorter uncertainty intervals...|$|E
40|$|This paper {{proposes a}} new {{approach}} for evaluating the uncertainty in the traceability system of force with the following features. a) Repeatability/reproducibility is evaluated by the <b>Type</b> <b>A</b> <b>evaluation.</b> b) Data taken in each orientation is treated with equality. c) Deviations from reference values and from the interpolation equation {{are considered to be}} equal to the standard deviation 1 s of a normal distribution. d) Difference of the values in increasing load and in decreasing load are considered as a half-width of the possible distribution of values to be measured. The same is the case of difference of the zero points in a loading cycle. 1...|$|E
40|$|DAccording to the Guide to the Expression of Uncertainty in Measurement, a {{coverage}} factor that produces an expanded uncertainty having an approximate {{level of confidence}} is recommended in the final expression of the measurement result. The numerical value for the coverage factor depends on the probability density function. When a <b>Type</b> <b>A</b> <b>evaluation</b> following a Gaussian (normal) distribution is combined with a Type B one following a rectangular distribution, the resulting probability density function is neither Gaussian nor rectangular, but similar to both, which we term a Flatten–Gaussian probability density function. The variance of such a function is the combined variance in the usual way, but the coverage factor must be calculated...|$|E
5000|$|In <b>Type</b> <b>A</b> <b>evaluations</b> of {{measurement}} uncertainty, {{the assumption is}} often made that the distribution best describing an input quantity [...] given repeated measured values of it (obtained independently) is a Gaussian distribution. then has expectation equal to the average measured value and standard deviation equal to the standard deviation of the average.When the uncertainty is evaluated from {{a small number of}} measured values (regarded as instances of a quantity characterized by a Gaussian distribution), the corresponding distribution can be taken as a t-distribution.Other considerations apply when the measured values are not obtained independently.|$|R
40|$|Abstract − The paper {{presents}} <b>a</b> <b>type</b> <b>A</b> (experimental) <b>evaluation</b> of {{the uncertainty}} due to systematic effects. After a brief discussion about the general problem of choosing a proper mathematical representation for systematic effects {{in the context of}} uncertainty <b>evaluation,</b> <b>a</b> pragmatic approach, based on the familiar random variable theory and the ISO 5725 norm is proposed. The approach is called “interinstrument experiment”, on the analogy of the “interlaboratory experiment ” of the ISO 5725. Preliminary experimental results, relevant to commercial digital oscilloscopes, are presented and discussed...|$|R
40|$|Abstract- A new {{approach}} to upgrading the <b>type</b> <b>A</b> uncertainty <b>evaluation</b> by investigation the trend and periodical systematic components in the regularly in time performed observations {{is presented in the}} paper. This is a part of best practice, which authors recommend as upgrading the routine procedures in uncertainty evaluation according to ISO GUM recommendation. The cleaning of raw data set by elimination the systematic components and the influence of these cleanings on standard uncertainty is presented and disused. Two numerical examples are discussed. Key words: uncertainty <b>type</b> <b>A,</b> unknown systematic components, trend, harmonic disturbance...|$|R
40|$|Since {{the end of}} 2000, a {{continuous}} sampling to monitor PCDDs and PCDFs emission was implemented on the 11 municipal waste incineration ovens in Walloon Region, to check the compliance with the EU emission limit value, 0. 1 TEQ ng/Nm³. For this purpose, uncertainty estimation {{is one of the}} most crucial points for decision making. The sampling and analytical uncertainty was estimated by two different ways: 1. Identification and quantification of the major contributions through QA/QC data (<b>type</b> <b>A</b> <b>evaluation)</b> or by other sources (type B evaluation), 2. Direct assessment through specific duplicate measurements. Both ways were found to give results in good agreement, with an extended uncertainty of about 30 to 40 %, depending on the congener, with a coverage factor k= 2. Peer reviewe...|$|E
40|$|As {{part of the}} Galileo {{project of}} Early Trials on Time Synchronization Techniques and Calibration Issues, the (near) zero-baseline GPS Common-View Time Transfer and Direct Measurement (using a Time Interval Counter) results between the time scales of two {{co-located}} clocks during 2001 were analyzed. This was done to identify the obtainable uncertainty levels with these techniques. The result of the analysis is presented. The baseline of the antennas of the two single-channel NBS-type GPS C-V receivers was 2 m, so multi-path influence is not excluded. The uncertainties from statistical origin (<b>Type</b> <b>A</b> <b>evaluation)</b> were determined by means of MDEV and TDEV. The uncertainties from other sources (Type B evaluation) were determined from calibrations. The TDEV of the GPS C-V result was less than 0. 4 ns at averaging times> 1 day, and at shorter averaging times TDEV was about 1 ns...|$|E
40|$|This thesis {{deals with}} the {{evaluation}} of accuracy of measurement and achieving the required accuracy level. This includes global analysis of the uncertainty evaluation, the calibration interval design {{and the creation of}} confirmation system. Basis of this work is in identification of all possible problems, which may occur during evaluation of accuracy and achieving the required accuracy level. The analysis is followed by suggested solutions for identified problems. It means namely the selection of probability distribution in case of uncertainty <b>type</b> <b>A,</b> <b>evaluation</b> of degrees of freedom in case of uncertainty type B, nonlinear correlation of input values, evaluation of coverage factor, the choice of method for calibration interval design and the procedure for meeting all metrological confirmation requirements. The last part of the thesis is practical measurement and result evaluation in the field of surge protection devices. This chapter demonstrates conclusions from the syntactical part...|$|E
40|$|Multiple {{dispatch}} allows {{determining the}} actual method to be executed, {{depending on the}} dynamic types of its arguments. Although some programming languages provide multiple dispatch, most widespread object-oriented languages lack this feature. Therefore, different implementation techniques are commonly used to obtain multiple dispatch in these languages. We evaluate the existing approaches, presenting a new one based on hybrid dynamic and static <b>typing.</b> <b>A</b> qualitative <b>evaluation</b> is presented, considering factors such as software maintainability and readability, code size, parameter generalization, and compile-time type checking. We also perform a quantitative assessment of runtime performance and memory consumption...|$|R
40|$|This paper {{describes}} two experiments {{carried out}} in order to investigate the role of visuals in multimodal answer presentations for a medical question answering system. First, a production experiment was carried out to determine which modalities people choose to answer different types of questions. In this experiment, participants had to create (multimodal) presentations of answers to general medical questions. The collected answer presentations were coded {{on the presence of}} visual media (i. e., photos, graphics, and animations) and their function. The results indicated that participants presented the information in a multimodal way. Moreover, significant differences were found in the presentation of different answer and question <b>types.</b> Next, <b>an</b> <b>evaluation</b> experiment was conducted to investigat...|$|R
40|$|In-process {{treatment}} of measurement data is a challenging topic of industrialmetrology. In {{the present research}} work, simultaneous estimation of measurand and related uncertainty is targeted from a measurement science standpoint. A metrological customization of the Kalman filter technique is implemented with application to basic coordinate measuring machine tasks. This is shown to accomplish Bayesian integration of the expert’s preprocess knowledge and in-process measurement results—akin to type B and <b>type</b> <b>A</b> uncertainty <b>evaluation</b> {{in the framework of}} the Joint Committee for Guides in Metrology Guide to the expression of uncertainty in measurement, respectively. Uncertainty assessment is automated accordingly. Based on recursive processing of fresh data only, relevant advantages are real-time performance and memory saving...|$|R
40|$|DOE {{described}} {{studies conducted}} {{to demonstrate that}} shielded containers comply with Class A requirements in the June 2008 <b>Type</b> <b>A</b> <b>Evaluation</b> Report (TAER) (WTS 2008). The TAER identifies the analyses, tests, and evaluations performed on the shielded container to demonstrate compliance of the packaging design with the applicable requirements of 49 CFR 178. 350. According to Section 3. 1 of the TAER: Determination of the response of a point radiation source subject to movement within the package and any associated effects on radiation levels are not provided in this document. Drop test damage information is provided in Section 4. 2. 2 {{for use by the}} shipper in determining whether a significant change in radiation level would result for a specific payload. This raises the question as to how the shipper would determine whether a significant change in radiation could result for a specific payload. We presume that, if no significant damage occurs during drop testing and no tracers or other materials are released, significant changes in radiation levels would not result. If this is the intent, why is it not so stated? What is the burden imposed on the shipper? The TAER would benefit if this ambiguous statement were clarified. This excerpt also raises the question as to where movement of point sources of radiation is addressed and what actions must be taken to prevent such movement. Similarly in Section 3. 2, the authors note that: Damage information is provided to assist the shipper in evaluating the possible dose rate changes at the surface of the package for the intended payloads to be shipped. The same concerns as outlined above apply here...|$|E
40|$|This SW is a {{deliverable}} for {{the joint}} research project Metrology for New Industrial Measurement Technologies (NIMTech). NIMTech receives cofunding from the European Community’s Seventh Framework Program ERA-NETPlus under the iMERA-Plus Project Grant Agreement 217257. A metrological customization of Kalman filter technique is implemented with application to basic Coordinate Measuring Machine (CMM) tasks. This is shown to accomplish Bayesian integration of expert’s pre-process knowledge and in-process measurement results—akin to type B and <b>type</b> <b>A</b> uncertainty <b>evaluation</b> {{in the framework of}} the Joint Committee for Guides in Metrology Guide to the expression of uncertainty in measurement, respectively. Uncertainty assessment is automated accordingly. Based on recursive processing of fresh data only, relevant advantages are real-time performance and memory saving...|$|R
5000|$|For <b>a</b> <b>Type</b> B <b>evaluation</b> of uncertainty, {{often the}} only {{available}} information is that [...] lies in a specified interval [...]In such a case, knowledge of the quantity can be characterized by a rectangular probability distribution with limits [...] and [...]If different information were available, a probability distribution consistent with that information would be used.|$|R
40|$|The {{bachelor}} thesis {{starts with}} the theory of small hydro power plants. It is complemented by a historical overview of waterworks development {{with reference to the}} uses of various turbine types. The second part focuses on the destription of the selected location waterwork Perknov of the hydropower project. Specifications of the existing turbine are validated and compared to the specifications of another turbine <b>type.</b> Finally <b>an</b> economic <b>evaluation</b> and project profitability is calculated...|$|R
40|$|The glycosylated {{hemoglobin}} {{is useful}} in monitoring diabetes regardless of its <b>type</b> allowing <b>a</b> retroactive <b>evaluation</b> over <b>a</b> period of 45 days according to the last studies, being a suggestive mirror for the glycemic status. The protein metabolism is evaluated through biochemical determinations of creatinine and urea. The medical practice is interested in how ensuring a good control or a less good control, of diabetes, will generate changes {{in the case of}} these biochemical parameters...|$|R
40|$|Idiopathic central serous chorioretinopathy (ICSC) {{is usually}} seen in young males with <b>Type</b> <b>A</b> personality. Clinical <b>evaluation</b> of the macula with fundoscopy and biomicroscopy, coupled with {{fluorescein}} angiography establishes the diagnosis. Indocyanine green angiographic studies have reinformed {{that the basic}} pathology lies in choriocapillaries and retinal pigment epithelium. Most of the ICSC resolve completely in four months, {{and some of them}} could resolve early with direct photocoagulation of the leaking site. Oral steroids have no role, and could even cause an adverse reaction...|$|R
40|$|Since {{mid-nineteenth century}} iron was widely {{introduced}} into wooden bridge truss <b>types.</b> <b>A</b> long-term creep <b>evaluation,</b> based on linear elastic model, {{is here presented}} in order to compare the effects of dead load in six different kinds of trusses in which iron is used in different truss members. The structural efficiency of the different truss forms has been evaluated in terms of initial elastic deflection and time needed to viscous deformation induced by creep so as to double the initial elastic deflection...|$|R
40|$|Due to {{the growing}} number of {{vehicles}} using the national road networks that link major urban centers, traffic noise is becoming a major issue in relation to the transportation system. Thus, it is important to determine noise model parameters to predict road traffic noise levels as part of an environmental assessment, according to traffic volume and pavement surface type. To determine the parameters of a noise prediction model, statistical pass-by and close proximity tests are required. This paper provides a parameter determination procedure for noise prediction models through an adaptive particle filter (PF) algorithm, based on using a weigh-in-motion system, which obtains vehicle velocities and types, as well as step-up microphones, which measure the combined noises emitted by various vehicle <b>types.</b> Finally, <b>an</b> <b>evaluation</b> of the adaptive noise parameter determination algorithm was carried out to assess the agreement between predictions and measurements...|$|R
40|$|This article {{deals with}} Bayesian {{inference}} and prediction for M/G/ 1 queueing systems. The general service time density is approximated with {{a class of}} Erlang mixtures which are phase type distributions. Given this phase <b>type</b> approximation, <b>an</b> explicit <b>evaluation</b> of measures such as the stationary queue size, waiting time and busy period distributions can be obtained. Given arrival and service data, a Bayesian procedure based on reversible jump Markov Chain Monte Carlo methods is proposed to estimate system parameters and predictive distributions. ...|$|R
40|$|An {{important}} {{challenge in}} software reengineering is to encapsulate collections of related data that, {{due to the}} absence of appropriate constructs for encapsulation in legacy programming languages, may be distributed throughout the code. The encapsulation of such collections is a necessary step for reengineering a legacy system into an objectoriented design or implementation. Encapsulating a set of related symbolic constants into <b>an</b> enumeration <b>type</b> is <b>an</b> instance of this problem. We present a classification of how enumeration types are modeled using symbolic constants in real-world programs, a set of heuristics to identify candidate enumeration <b>types,</b> and <b>an</b> experimental <b>evaluation</b> of these heuristics...|$|R
40|$|Refactoring is a {{well-known}} technique that is widely adopted by software engineers to improve the design and enable {{the evolution of a}} system. Knowing which refactoring operations were applied in a code change is a valuable information to understand software evolution, adapt software components, merge code changes, and other applications. In this paper, we present RefDiff, an automated approach that identifies refactorings performed between two code revisions in a git repository. RefDiff employs a combination of heuristics based on static analysis and code similarity to detect 13 well-known refactoring <b>types.</b> In <b>an</b> <b>evaluation</b> using <b>an</b> oracle of 448 known refactoring operations, distributed across seven Java projects, our approach achieved precision of 100 % and recall of 88 %. Moreover, our evaluation suggests that RefDiff has superior precision and recall than existing state-of-the-art approaches. Comment: Paper accepted at 14 th International Conference on Mining Software Repositories (MSR), pages 1 - 11, 201...|$|R
30|$|To {{address these}} shortcomings, a {{taxonomy}} for OSN data types {{was developed in}} this paper. Based on a design-oriented methodology, first, the body of literature was analyzed to identify possible data elements and terminological inconsistencies. Subsequently, a hierarchically structured taxonomy was derived by studying fundamental user activities on OSNs and step-wise classifying the identified data types into non-redundant partitions. The discussion of data types revealed that privacy mainly depends on the interplay of a data element’s content, the extent and granularity of user control, and its concrete implementation. Based on the understanding of privacy implications of different data <b>types,</b> <b>a</b> privacy relevance metric was proposed which allows to quantitatively assess privacy threats for a given context. The subsequent evaluation of applying the taxonomy to five major OSNs demonstrates its applicability to existing OSNs and reveals implementation-specific differences in privacy settings of various data <b>types.</b> <b>A</b> detailed <b>evaluation</b> using all of Facebook’s data types further shows {{that it is possible}} to map all these data types into the taxonomy.|$|R
40|$|This {{handbook}} {{was developed}} to provide principal investigators and project evaluators with {{a basic understanding of}} selected approaches to evaluation. It is aimed at people who need to learn more about both what evaluation can do and how to do <b>an</b> <b>evaluation,</b> rather than those who already have a solid base of experience in the field. Topics include <b>types</b> of <b>evaluation,</b> <b>an</b> overview of the evaluation process, collecting and analyzing the data, writing the report, examples of project evaluations, how to select an evaluator, a glossary, and supplemental reading. The Handbook discusses quantitative and qualitative methods, but the emphasis is on quantitative techniques for conducting outcome evaluations. Educational levels: Graduate or professional, Graduate or professional...|$|R
40|$|The {{evaluation}} of the quantitative parameters of migration * 991 * 993 * 997 Cs in the soils of the meadow biogenocenoses has been given, a complex approach has been developed for the {{evaluation of}} the efficiency of the protective measures on the meadows of the different <b>types.</b> <b>A</b> comparative <b>evaluation</b> has been given for the different types of meadows of the protective measures efficiency. The variation of the efficiency of the surface and original improvement of meadows in time has been shown. On the base of the data about the laws of migration * 991 * 993 * 997 Cs, and the efficiency of the protective measures, the classification of the meadow biogeocenoses has been developed by the degree of risk of the food production under the conditions of the radioactive contamination of the lands. Available from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
30|$|Subjective methods {{rely on the}} {{independent}} opinion of a panel of users judging {{the quality of the}} generated key-frame video summaries according to a known methodology. In this <b>type</b> <b>evaluation,</b> <b>a</b> panel of viewers are asked to observe both the summaries and the original sequence and then respond to questions related to some evaluation criteria, (e.g. ‘Was the key-frame summary useful?’, ‘Was the key-frame summary coherent?’) or if each key-frame is ‘good’, ‘fair’, or ‘poor’ according to the original video sequence.|$|R
50|$|Commenting to a {{newspaper}} in September 2015 he recalled, To me {{it was the most}} exciting thing on the horizon, a totally new experience. I remember watching the ground crew very carefully before take-off, wondering if they thought they were waving goodbye to me forever or whether they thought this thing was going to return. The noise it made was absolutely thunderous and it was like being in charge of a runaway train; everything changed so rapidly and I really had to have my wits about me. Brown flight tested all three of the German jet designs to see front-line action in the war: the Messerschmitt Me 262 and the Arado Ar 234, each type powered by Junkers Jumo 004 engines, and the BMW 003-powered Heinkel He 162 turbojet combat aircraft. He would later fly the He-162 at the Farnborough Air Show, and described it as having the best controls of any aircraft he had ever flown but as being difficult to handle. One of his colleagues at Farnborough died trying the aircraft <b>type</b> in <b>an</b> <b>evaluation.</b>|$|R
40|$|It {{has long}} been {{recognised}} {{in the field of}} education that there is no better teaching method than participation. Convention and event management is a relatively new area of hospitality and tourism education that offers an opportunity to provide students with the real world practical and applied experience that many seek This paper will report on the use of real world activities in a class in Convention and Events Management. Students in this class were required to plan, market, manage and evaluate a real event. Four events were planned, three were completed and evaluated. This paper will firstly describe the process used to plan and run the events. It will then review the outcomes of two <b>types</b> of <b>evaluation,</b> <b>an</b> <b>evaluation</b> of the success of each event and <b>an</b> <b>evaluation</b> of the success of using real world events as a teaching technique. The results have implications both for the general area of event management and more specifically for the teaching of convention and events management...|$|R
40|$|Query {{segmentation}} is {{the problem}} of identifying those keywords inaquery,whichtogetherformcompound conceptsorphrases like new york times. Suchsegments canhelpasearch engine tobetterinterpretauser’sintentsandtotailorthesearchresultsmore appropriately. Ourcontributionstothisproblemarethreefold. (1) We conduct the first large-scale study of human segmentation behavior based on more than 500000 segmentations. (2) We show that the traditionally applied segmentation accuracy measures are not appropriate for such large-scale corpora and introduce new, more robust measures. (3) We develop a new query segmentation approach with the basic idea that, in cases of doubt, it is often better to(partially) leavequeries without anysegmentation. Thisnewin-doubt-withoutapproachchoosesdifferentsegmentation strategies depending on query <b>types.</b> <b>A</b> large-scale <b>evaluation</b> shows substantial improvement upon {{the state of the art}} in terms of segmentation accuracy. To draw a complete picture, we also evaluate the impact of segmentation strategies on retrieval performance in a TREC setting. It turns out that more accurate segmentationnot necessarilyyields betterretrievalperformance. Basedon thisinsight,weproposeanin-doubt-withoutvariantwhichachieves the best retrieval performance despite leaving many queries unsegmented. But there is still room for improvement: the optimum segmentation strategy which always chooses the segmentation that maximizes retrieval performance, significantly outperforms all other testedapproaches...|$|R
40|$|Abstract- An {{idea of the}} {{new type}} of instrument, {{improved}} by pre-processing of the measurement data {{is presented in the}} paper. This instrument has an extensive data handling procedures, which allow to investigate different periodical and non-periodical components in raw data and to clean the data from identified influences. Such cleaning upgrades the <b>type</b> <b>A</b> uncertainty <b>evaluation.</b> I. The Challenge In raw collected measurement data, may exist some unknown periodical and non periodical components, which appear in an object under investigation or in an measurement instrumentation. The widely internationally accepted Guide to Uncertainty in Measurement, GUM, and its actual and proposed Supplements [1 - 3], [6] do not refer to treatment of raw collected data before procedure of <b>type</b> <b>A</b> uncertainty is applied. Such a pre-processing of raw data, was presented in the paper presented at IMEKO TC 4 2007 Symposium in Iasi [7]. Proposed data pre-processing refers to either regularly in time sampled data or collected in known moments in time. Based on method proposed in [9] the virtual instrument which facilitates a man, who run en experiment is developed and the results of application are presented in the paper. The idea is based on investigation of a priori unknown periodical and non-periodical components i...|$|R
40|$|This paper {{considers}} {{the effects of}} <b>an</b> interim performance <b>evaluation</b> on the decision of a principal to delegate authority to a potentially biased but better informed agent. Assuming the agents 2 ̆ 019 outside option {{to be determined by}} market beliefs about their <b>type,</b> interim <b>evaluations</b> (<b>a)</b> provide a possibility for the principal to potentially separate biased agents from unbiased agents and (b) induce an incentive for biased agents to imitate unbiased ones in order to retain the decision authority and to increase their wages in later periods (in case of public evaluation). We show that the principal always profits from <b>a</b> private <b>evaluation</b> while <b>a</b> public <b>evaluation</b> is only beneficial if the corresponding wage effects are not too costly. Nevertheless, the principal prefers public over private evaluation if the imitation incentive for the biased type is high enough. Finally, regarding implications for economic policy, we show that in view of aggregate welfare any evaluation conducted ought to be disclosed to the public...|$|R
40|$|This {{article is}} {{the first part of}} a two {{articles}} series about a calculus with higher-order polymorphic functions, recursive types with arrow and product type constructors and set-theoretic type connectives (union, intersection, and negation). In this first part we define and study the explicitly-typed version of the calculus in which type instantiation is driven by explicit instantiation annotations. In particular, we define an explicitly-typed λ-calculus with intersection <b>types</b> and <b>an</b> efficient <b>evaluation</b> model for it. In the second part, presented in a companion paper, we define <b>a</b> local <b>type</b> inference system that allows the programmer to omit explicit instantiation annotations, and <b>a</b> <b>type</b> reconstruction system that allows the programmer to omit explicit type annotations. The work presented in the two articles provides the theoretical foundations and technical machinery needed to design and implement higher-order polymorphic functional languages for semi-structured data. © 2014 ACM. This article {{is the first}} part of a two articles series about a calculus with higher-order polymorphic functions, recursive types with arrow and product type constructors and set-theoretic type connectives (union, intersection, and negation). In this first part we define and study the explicitly-typed version of the calculus in which type instantiation is driven by explicit instantiation annotations. In particular, we define an explicitly-typed λ-calculus with intersection <b>types</b> and <b>an</b> efficient <b>evaluation</b> model for it. In the second part, presented in a companion paper, we define <b>a</b> local <b>type</b> inference system that allows the programmer to omit explicit instantiation annotations, and <b>a</b> <b>type</b> reconstruction system that allows the programmer to omit explicit type annotations. The work presented in the two articles provides the theoretical foundations and technical machinery needed to design and implement higher-order polymorphic functional languages for semi-structured data. © 2014 ACM...|$|R
40|$|The {{measurement}} of engine emissions {{is important for}} their monitoring and control. However, the ability to measure these emissions in-situ is limited. We are developing a family of high temperature gas sensors which are intended to operate in harsh environments {{such as those in}} an engine. The development of these sensors is based on progress in two types of technology: (1) The development of SiC-based semiconductor technology; and (2) Improvements in micromachining and microfabrication technology. These technologies are being used to develop point-contact sensors to measure gases which are important in emission control especially hydrogen, hydrocarbons, nitrogen oxides, and oxygen. The {{purpose of this paper is}} to discuss the development of this point-contact sensor technology. The detection of each type of gas involves its own challenges in the fields of materials science and fabrication technology. Of particular importance is sensor sensitivity, selectivity, and stability in long-term, high temperature operation. An overview is presented of each sensor <b>type</b> with <b>an</b> <b>evaluation</b> of its stage of development. It is concluded that this technology has significant potential for use in engine applications but further development is necessary...|$|R
40|$|The goal	of	my	research	is	to	increase	software	reliability	by	enabling	programmers	to	give	precise, machine-checkable specifications	for	their	algorithms. An	incorrect	function	with	a	precise	specifi-cation can	be	rejected	by	a	compiler, avoiding	the	possibility	of	misbehavior	at	runtime. My	tool	of choice thus	far	has	been	dependent	types. My	dissertation	studies	what	is	needed	to add	dependent <b>types</b> to	Haskell, <b>a</b>	pure	functional	language	with	a	growing	industrial	uptake. The	research	includes my implementation	of	dependent	types into	Haskell’s	main	compiler; the	release	of	this	feature	will mark the	first	time	dependent	types	have	been	available	in	a	language	used	by	programmers	in	in-dustry. I expect	my	next	steps	to	include	code	optimizations	enabled	through	dependent	types	and {{improving}} compiler	support	for	refinement	<b>types,</b> <b>a</b>	commonly	used	subset	of	dependent	<b>types.</b> Dependent types: <b>a</b>	powerful	feature	for	program	verification	and	expressive	types Type systems	are	one	of	the	great	practical	successes	of	programming	language	theory. Theoreticians define the	structure	of	types, <b>typing</b>	rules, and	<b>an</b>	<b>evaluation</b>	relation	for	<b>a</b>	given	programming	lan-guage; using	these	definitions, they	can	then	prove	that	a	well-typed	program	does	not	“go	wrong”. With this	work	in	hand, a	compiler	writer	can	then	implement	<b>a</b>	<b>type</b>	checker	that	confirms	to	pro-grammers that	their	real, well-typed	programs	do	not	“go	wrong ”	 —	 at	least	not	in	the	way	the	typ...|$|R
40|$|Abstract. This {{article is}} {{the first part of}} a two {{articles}} series about a calculus with higher-order polymorphic functions, recursive types with arrow and product type constructors and set-theoretic type connectives (union, intersection, and negation). In this first part we define and study the explicitly-typed version of the calculus in which type instantiation is driven by explicit instantiation annota-tions. In particular, we define an explicitly-typed λ-calculus with intersection <b>types</b> and <b>an</b> efficient <b>evaluation</b> model for it. In the second part, presented in a companion paper, we define <b>a</b> local <b>type</b> inference system that allows the programmer to omit explicit instantiation annotations, and <b>a</b> <b>type</b> reconstruction system that al-lows the programmer to omit explicit type annotations. The work presented in the two articles provides the theoretical foundations and technical machinery needed to design and implement higher-order polymorphic functional languages for semi-structured data...|$|R
