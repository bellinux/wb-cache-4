6|2|Public
50|$|Research {{along these}} three {{directions}} has converged {{into a new}} generation CAT software, which is both an enterprise level <b>translation</b> <b>workbench</b> {{as well as an}} advanced research platform for integrating new MT functions, running post-editing experiments and measuring user productivity. These include: i) an advanced API for the Moses Toolkit, customizable to languages and domains, ii) ease of use through a clean and intuitive web interface that enables the collaboration of multiple users on the same project, iii) concordances, terminology databases and support for customizable quality estimation components and iv) advanced logging functionalities.|$|E
40|$|This paper {{reports on}} the results of a user {{satisfaction}} survey carried out among 16 translators using a new computer-assisted <b>translation</b> <b>workbench.</b> Participants were asked to provide feedback after performing different post-editing tasks on different configurations of the workbench, using different features and tools. Resulting from the feedback provided, we report on the utility of each of the features, identifying new ways of implementing them according to the users’ suggestions...|$|E
40|$|We {{describe}} a web-based workbench that offers advanced computer aided translation (CAT) functionality: post-editing machine translation (MT), interactive translation prediction (ITP), visualization of word alignment, extensive logging with replay mode, integration with eye trackers and e-pen. It is available open source and integrates with multiple MT systems. The {{goal of the}} CASMACAT project 1 is to develop an advanced computer aided <b>translation</b> <b>workbench.</b> At the mid-point of the 3 -year project, we release this tool as open source software. It already includes {{a wide range of}} novel advance...|$|E
50|$|<b>Translation</b> <b>workbenches</b> and TMs {{could be}} {{considered}} the most successful translation tool; however it’s restricted to specific text types.|$|R
40|$|Lacking widely {{accepted}} and reliable evaluation measures, {{the evaluation of}} Machine Translation (MT) and translation tools remains an open issue. MT developers focus on automatic evaluation measures such as BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) which primarily count n-gram overlap with reference translations and which are only indirectly linked to translation usability and quality. Commercial translation tools such as translation memories and <b>translation</b> <b>workbenches</b> are widely used and their developers claim usefulness in terms of productivity, consistency or quality. However, these claims are rarely proven using objective comparative studies. This collection dissects {{the state of the}} art in translation technology and translation tool development and provides quantitative and qualitative answers to the question how useful translation technology is. Evaluation of translation technology requires a multifaceted approach. It involves the evaluation of the textual output quality in terms of intelligibility, accuracy, fidelity to its source text, and appropriateness o...|$|R
40|$|CASMACAT is a modular, web-based <b>translation</b> <b>workbench</b> {{that offers}} ad-vanced {{functionalities}} for computer-aided translation {{and the scientific}} study of hu-man translation: automatic interaction with machine translation (MT) engines and translation memories (TM) to ob-tain raw translations or close TM matches for conventional post-editing; interactive translation prediction based on an MT en-gine’s search graph, detailed recording and replay of edit actions and translator’s gaze (the latter via eye-tracking), and the sup-port of e-pen as an alternative input device. The system is open source sofware and in-terfaces with multiple MT systems...|$|E
40|$|This paper {{describes}} {{a pilot study}} with a computed-assisted <b>translation</b> <b>workbench</b> aiming at testing the integration of online and active learning features. We investigate {{the effect of these}} features on translation productivity, using interactive translation prediction (ITP) as a baseline. User activity data were collected from five beta testers using key-logging and eye-tracking. User feedback was also collected {{at the end of the}} experiments in the form of retrospective think-aloud protocols. We found that OL performs better than ITP, especially in terms of trans- lation speed. In addition, AL provides better translation quality than ITP for the same levels of user effort. We plan to incorporate these features in the final version of the workbench...|$|E
40|$|The {{present study}} has {{surveyed}} post-editor trainees ’ views and attitudes {{before and after}} the introduction of speech technology as a front end to a computer-aided <b>translation</b> <b>workbench.</b> The aim of the survey was (i) to identify attitudes and perceptions among post-editor trainees before performing a post-editing task using automatic speech recognition (ASR); and (ii) to assess the degree to which post-editors ’ attitudes and expectations to the use of speech technology changed after actually using it. The survey was based on two questionnaires: the first one administered before the participants performed with the ASR system and the second one {{at the end of the}} session, once they have actually used ASR while post-editing machine translation outputs. Overall, the results suggest that the surveyed post-editor trainees tended to report a positive view of ASR in the context of post-editing and they would consider adopting ASR as an input method for future post-editing tasks. ...|$|E

