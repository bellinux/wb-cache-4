33|17|Public
50|$|Our example ASN.1 {{data types}} which {{we agreed to}} compare to a {{high-level}} CORBA/IDL <b>transfer</b> <b>syntax</b> specification are limited to definition of such <b>transfer</b> <b>syntax</b> only for a single instance of what we compared to an IDL interface (Information Object Set in ASN.1 terms).|$|E
50|$|JPIP is a client/server {{communication}} protocol defined in Part 9 of the JPEG2000 suite of standards, (ISO/IEC 15444-9). It is a <b>transfer</b> <b>syntax</b> in DICOM, adopted in Supplement 106. Some proprietary software incorporates streaming but DICOM supplement 106 stipulates JPIP {{as the only}} <b>transfer</b> <b>syntax</b> method by which to stream medical images in a DICOM conformant manner.|$|E
50|$|In other words, such <b>transfer</b> <b>syntax</b> is not generic {{and it is}} not reusable.|$|E
5000|$|Structured Reports, MPEG-2 and MPEG-4 <b>transfer</b> <b>syntaxes,</b> Encapsulated PDF ...|$|R
50|$|<b>Transferring</b> API <b>syntax</b> to ATEL/ATL {{requires}} coding transformation {{knowledge to}} successfully transfer.|$|R
5000|$|There {{is general}} {{agreement}} {{that the use of}} the DICOM file format is required for images, and that where images are compressed for archival or transport, standard, not proprietary, compression schemes (<b>transfer</b> <b>syntaxes)</b> need to be used. Indeed, a distinguishing feature of most VNAs as opposed to many traditional PACS is the avoidance of proprietary internal formats ostensibly used in the past for [...] "performance" [...] reasons, whilst still obtaining good performance across the interfaces.|$|R
50|$|Packed {{encoding}} rules (PER) are ASN.1 encoding {{rules for}} producing a compact <b>transfer</b> <b>syntax</b> for data structures described in ASN.1, defined in 1994.|$|E
5000|$|Now the {{high-level}} <b>transfer</b> <b>syntax</b> descriptor [...] can be parameterized {{with any}} arbitrary Information Object Set ("IDL interface") conforming to the Information Object Class specification ("IDL grammar").|$|E
5000|$|The {{reason for}} the current {{limitation}} is that we currently hard-code our Information Object Set ( [...] in case of , or [...] in case of [...] ) into our ASN.1 data types (high-level <b>transfer</b> <b>syntax</b> specification).|$|E
40|$|OBJECTIVE: To {{report on}} the use of SGML and XML (a proper subset of SGML) as <b>transfer</b> <b>syntaxes</b> for HL 7 Version 2. 3 and Version 3. 0 messages. METHODS: The {{methodology}} has focused largely on two questions: Can it be done? How best to do it? The first question is addressed by attempting to build an SGML/XML representation of HL 7 messages. The second question requires a consideration of several metrics: message length, speed of message creation and parsing, interversion compatibility, local customization, conformance determination, and the availability of software tools and skill on the format. RESULTS: Detailed specifications for expressing HL 7 in SGML and XML have been developed. Some HL 7 requirements are not readily expressed, while some ambiguous areas of the HL 7 standard are made explicit in the SGML/XML representation. With the current design, an SGML/XML parser can extract any component of any data type from a message. CONCLUSIONS: SGML and XML can both serve as implementable message specifications for HL 7 Version 2. 3 and Version 3. 0 messages. The ability to explicitly represent an HL 7 requirement in SGML/XML confers the ability to validate that requirement with an SGML parser. The optimal message representation will be a balance of functional, technical, and practical requirements...|$|R
50|$|According to some {{theories}} of prosody, the prosodic representation is derived with direct {{reference to the}} hierarchical syntactic structure. For example, Selkirk (2011, and others) proposes that prosodic structure is constructed by a process of matching, although imperfectly, prosodic constituents to syntactic constituents. Kahnemuyipour (2009) demonstrates, using evidence from several languages, how information structure can be represented in the <b>transfer</b> from <b>syntax</b> to phonology, arguing that transfer can only be uni-directional, from syntax to phonology. Oltra-Massuet and Arregi (2005) argue that the metrical structure, as well, makes reference to hierarchical syntactic structure in Spanish. The extent {{of the interaction between}} the syntax and phonology at the interface is a matter of current debate.|$|R
40|$|This {{document}} specifies an Internet standards track {{protocol for}} the Internet community, and requests discussion {{and suggestions for}} improvements. Please refer to the current edition of the "Internet Official Protocol Standards " (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. Copyright Notice Copyright (C) The Internet Society (2003). All Rights Reserved. Punycode is a simple and efficient <b>transfer</b> encoding <b>syntax</b> designed for use with Internationalized Domain Names in Applications (IDNA). It uniquely and reversibly transforms a Unicode string into an ASCII string. ASCII characters in the Unicode string are represented literally, and non-ASCII characters are represented by ASCII characters that are allowed in host name labels (letters, digits, and hyphens). This document defines a general algorithm calle...|$|R
50|$|DER (Distinguished Encoding Rules) is a {{restricted}} variant of BER for producing unequivocal <b>transfer</b> <b>syntax</b> for data structures described by ASN.1. Like CER, DER encodings are valid BER encodings. DER {{is the same}} thing as BER with all but one sender's options removed.|$|E
5000|$|The {{concepts}} used in ASN.1 {{are more}} flexible {{than the ones}} used in IDL, because, continuing the analogy, they allow to [...] "customize grammar" [...] of the [...] "IDL specification". ASN.1 encoding rules are used as a <b>transfer</b> <b>syntax</b> for remote invocations that resemble CORBA/IIOP.|$|E
5000|$|With {{the current}} set of known tools you can't define such a <b>transfer</b> <b>syntax</b> in a generic way in, say, ASN.1 {{specification}} A and then reuse it in ASN.1 specifications B and C that define concrete application-specific [...] "IDL interfaces" [...] on which A does not depend.|$|E
40|$|Reordering is a {{difficult}} task in translating between widely different languages such as Japanese and English. We employ the postordering framework proposed by (Sudoh et al., 2011 b) for Japanese to English translation and improve upon the reordering method. The existing post-ordering method reorders a sequence of target language words in a source language word order via SMT, while our method reorders the sequence by: 1) parsing the sequence to obtain syntax structures similar to a source language structure, and 2) <b>transferring</b> the obtained <b>syntax</b> structures into the syntax structures of the target language. ...|$|R
40|$|Moses is a {{well-known}} representative of the phrase-based statistical machine translation systems family, which {{are known to be}} extremely poor in explicit linguistic knowledge, operating on flat language representations, consisting only of tokens and phrases. Treex, on the other hand, is a highly linguistically motivated NLP toolkit, operating on several layers of language representation, rich in linguistic annotations. Its main application is TectoMT, a hybrid machine translation system with deep <b>syntax</b> <b>transfer.</b> We review a large number of machine translation systems that have been built over the past years by combining Moses and Treex/TectoMT in various ways...|$|R
40|$|Local {{workshop}} audience. This paper {{uses the}} study of identifying clauses in Zaar, as in [1], to illustrate a macrosyntactic approach to the interface between syntax and intonation structure (IS). The concept of macrosyntax as developed by the Aix School (Blanche-Benveniste et al. 1990) and the ANR-Rhapsodie programme (Lacheret et al. 2014) is used in a bottom-up study of IS through a detailed annotation of a corpus based on illocutionary units. This new approach to syntax, based on dependency grammar and the primacy of content words, opens new perspectives on typology in which the description of valency is central. The main weight of the description is <b>transferred</b> from the <b>syntax</b> to the lexicon where dictionary of verbal valency is essential {{in the production of}} what can be called “Word Grammars”...|$|R
50|$|CER is a {{restricted}} variant of BER for producing unequivocal <b>transfer</b> <b>syntax</b> for data structures described by ASN.1. Whereas BER gives choices {{as to how}} data values may be encoded, CER (together with DER) selects just one encoding from those allowed by the basic encoding rules, eliminating rest of the options. CER is useful when the encodings must be preserved; e.g., in security exchanges.|$|E
50|$|Note {{that the}} MDER rules {{are a subset}} of the BER rules. This MDER-encoded ASN.1 {{representation}} of objects is similar in concept to using XML as a machine-independent method of exchanging data (indeed, the XER <b>transfer</b> <b>syntax</b> transfers ASN.1 objects in XML). However the differences are that MDER messages are much smaller than the equivalent XML messages, and much simpler for machines to convert to and from internal data structures.|$|E
50|$|Support of {{the basic}} DICOM C-STORE, C-FIND, C-MOVE and preferably C-GET are {{fundamental}} and not debated. The basic uncompressed transfer syntaxes including implicit and explicit VR little-endian, and the less common big-endian <b>transfer</b> <b>syntax</b> are typically supported. The range of compressed transfer syntaxes usually includes lossless JPEG and reversible and irreversible JPEG 2000, occasionally JPEG-LS, and usually lossy JPEG for images that were supplied that way (especially true color photographs. Support for motion compression (other than multi-frame JPEG) is less common, but perhaps more common in VNAs than in PACSs, especially for storage and regurgitation without viewing.|$|E
40|$|Word-level {{alignment}} of bilingual text {{is a critical}} resource for a growing variety of tasks. Probabilistic models for word alignment present a fundamental trade-off between richness of captured constraints and correlations versus efficiency and tractability of inference. In this article, we use the Posterior Regularization framework (Graça, Ganchev, and Taskar 2007) to incorporate complex constraints into probabilistic models during learning without changing {{the efficiency of the}} underlying model. We focus on the simple and tractable hidden Markov model, and present an efficient learning algorithm for incorporating approximate bijectivity and symmetry constraints. Models estimated with these constraints produce a significant boost in performance as measured by both precision and recall of manually annotated alignments for six language pairs. We also report experiments on two different tasks where word alignments are required: phrase based machine translation and <b>syntax</b> <b>transfer,</b> and show promising improvements over standard methods. 1...|$|R
40|$|When {{teaching}} Italian as {{a foreign}} language, learners’ insecurities about using conjunctions and tenses in subordinate clauses can be noticed very early. Temporality, relative temporality and modality are signalled by Italian verb forms which function in a largely different (or apparently similar) way in comparison to Slovene. If ‘true’ similarities can be exploited {{in the process of}} teaching/learning by means of a positive transfer, apparent ‘similarities’ and ‘differences’ between the two language systems should, on the other hand, be clarified to avoid negative transfer (interference). During the whole learning process language awareness should be developed, also through contrastive insights into the linguistic systems concerned. Taking as a starting point a text suitable for level B 1, some problematic points are presented, for which a series of tasks can be suggested with the aim of helping the learner master complex clause syntax and develop language awareness. Key words: Italian, foreign/second language, complex clause, <b>syntax,</b> <b>transfer,</b> language awarenes...|$|R
40|$|Abstract—In this paper, an {{integrated}} system [predicting solder interconnection shapes (PSIS) ] is built to predict {{and analyze the}} shape of solder interconnections in electronic packaging. The pur-poses are to numerically simulate the formation of solder joints, analyze the shape of solder joints and its influential factors and provide an efficient tool for design and manufacturing engineers to improve the integrity of solder interconnections. The formation of solder joints is numerically simulated through a surface evolver program and the calculation is automated with an additional con-troller. A datafile generator is developed in which the potential en-ergies, geometry of the conjunction and boundary conditions of the initial solder joint can be generated visually and <b>transferred</b> into evolver <b>syntax</b> automatically. A post processor is built to analyze the global three-dimensional (3 -D) shape and cross section profiles of solder fillets. Also, the applications for solder interconnection shapes prediction of chip component (RC 3216) are conducted with this system. Index Terms—Integration, prediction, solder interconnection shapes, system. I...|$|R
50|$|Generic String Encoding Rules (GSER) are {{a set of}} ASN.1 {{encoding}} {{rules for}} producing a verbose, human-readable textual <b>transfer</b> <b>syntax</b> for data structures described in ASN.1. The purpose of GSER is to represent encoded data to the user or input data from the user, in a very straightforward format. GSER was originally designed for the Lightweight Directory Access Protocol (LDAP) and is rarely used outside of it. The use of GSER in actual protocols is discouraged since not all character string encodings supported by ASN.1 can be reproduced in it. The GSER encoding rules are specified in RFC 3641 and unlike other common types of encoding rules, are not standardised by ITU-T.|$|E
50|$|The Basic Encoding Rules {{were the}} {{original}} rules {{laid out by}} the ASN.1 standard for encoding abstract information into a concrete data stream. The rules, collectively {{referred to as a}} <b>transfer</b> <b>syntax</b> in ASN.1 parlance, specify the exact octet sequences which are used to encode a given data item. The syntax defines such elements as: the representations for basic data types, the structure of length information, and the means for defining complex or compound types based on more primitive types. The BER syntax, along with two subsets of BER (the Canonical Encoding Rules and the Distinguished Encoding Rules), are defined by the ITU-T's X.690 standards document, {{which is part of the}} ASN.1 document series.|$|E
40|$|Abstract. This paper {{reviews the}} process that was {{undertaken}} in revising the <b>transfer</b> <b>syntax</b> for RDF as defined in the RDF Model and Syntax W 3 C Recommendation[1] by the RDF Core Working Group[2] and the problems that are now clear especially comparing the revised RDF model and new abstract syntax. The syntax looks out of date in particular {{with the use of}} XML QNames giving unconstrained syntax terms in the XML, causing problems with newer XML technology such as XSLT, DTDS and W 3 C XML Schema[3] and other XML-constraining languages. In order to deliver a modern RDF syntax, this paper reviews the requirements for RDF in two aspects – as a canonical <b>transfer</b> <b>syntax</b> and one for end-users, targeted at HTML. It evaluates previous RDF syntax proposals against these requirements and analyses {{the pros and cons of}} XML and non-XML syntaxes. The conclusion is a summary of syntax approaches for future standardisation activity. 1 Introduction to RDF/XML This paper reviews the process revising the <b>transfer</b> <b>syntax</b> for RDF as defined in th...|$|E
40|$|In {{the field}} of second {{language}} acquisition (SLA) {{it is an important}} question, which role the native language assumes in the acquisition of another language. Studies have shown that there exists a native language influence on a phonological and lexical level. The situation in syntax is however different. Here a native language influence (transfer) is not self-evident. Researchers have been taking various stances towards this question. Full Transfer / Full Access Model (Schwartz & Sprouse 1994 / 96) claims that L 1 transfer plays {{a central role in the}} SLA, since the L 1 grammar is supposed to be the initial state of the L 2 grammar. Processability Theory (Pienemann 1998; Pienemann & Håkansson 1999) on the other hand, claims that since the acquisition of syntax is constrained by cognitive processing procedures, L 1 transfer does not change the SLA essentially. Instead the L 2 is acquired according to an implicational order irrespective of the L 1. The aim of this paper is to shed light on these opposing views concerning the role of <b>transfer</b> in <b>syntax.</b> To this end hypotheses derived from both theories will be tested on oral data of twenty German L 2 learners with Swedish as their L 1. Both languages are V 2 languages. This is an especially promising combination to test transfer phenomena, since the V 2 property can be hypothesised to transfer. It will be shown that the data of the present research supported unanimously Processability Theory whilst the prediction of the Full Transfer / Full Access Model could not b...|$|R
40|$|The {{influence}} that a student’s first language (L 1) {{can have on}} their acquisition of a second language (L 2) has been frequently noted by language teachers (Swan, 1997; Jarvis, 2007) and documented in the literature for decades. However, thinking has gradually evolved {{in terms of the}} form that influence could take. Early research work focussed on <b>transfer</b> of <b>syntax</b> or form, but recently the role that L 1 conceptual information plays in transfer has come to the fore. The 1960 s saw a plethora of contrastive studies inspired by the work of Robert Lado (1957), where languages were analysed using the prevailing structuralist approaches to language description. These contrastive studies were conceived with the view to predicting the types of errors speakers of one language would make while learning another, and this became known as the Contrastive Analysis Hypothesis (CAH) (Lado, 1957). This view was based in the behaviourist paradigm of the time which saw language learning as habit formation. This implied that learning a new language meant the transfer of elements and features from the first language to the target language, and that old ‘habits’ may interfere with second language acquisition (Aarts, 1982). Pairs of languages were compared in terms of their similarities and differences looking at linguistic units in relation to the overarching system to which they belonged (see Vinay & Darbelnet, 1960; Agard & Di Pietro, 1965, for examples). However, the CAH was severely criticised in the late 1960 s, as it {{did not seem to be}} able to predict any classroom errors that language teachers had not already noticed, and was not able to offer any solutions with regard to how to deal with these errors (Corder, 1967). peer-reviewe...|$|R
30|$|In {{light of}} such challenges, [64] claims that {{ensuring}} data portability {{is a major}} challenge for enterprises due to {{the large number of}} competing vendors for data storage and retrieval. The ability to move data also emerges as a management issue for cloud computing. Therefore, in response to the question of data movability, {{it is important to note}} that the API used for the source service may not be the same as the API used for the target service and that different tooling may be required in each case. The main aspects of data portability are the syntax and semantics of the <b>transferred</b> data. The <b>syntax</b> of the data should ideally be the same for the source service and the target service. However, if the syntax does not match (i.e., the source may use JSON syntax, but the target may use XML), it may be possible to map the data using commonly available tools. If the semantics of the transferred data does not match between the source and target services, then data portability is likely to be more difficult or even impossible. However, this might be achieved by the source service supplying the data in exactly the format that is accepted by the target service. Therefore, on a long term, achieving data portability will depend on the standardization of import and export functionality of data and its adoption by the providers. The aim is to minimize the human efforts in re-design and re-deployment of application and data when moving from one cloud to another. To this end, it becomes vital that any enterprise cloud migration project can be carried out without any disruption to data availability since data is an organisation’s most critical, ubiquitous, and essential business asset [29].|$|R
40|$|Picture Archiving and Communication System(PACS) is {{responsible}} for storing Digital Imaging and Communication in Medicine (DICOM) images fromradiology modalities into its database, images {{takes a lot of}} time to transfer to remote location through WAN due to large file size and slow transfer protocol. A PACS alternative system has been developed which performs basic functions of a generic PACS. Images directly from modalities are large in size by default <b>transfer</b> <b>syntax</b> of these images is Endian Explicit syntax. Changing this <b>transfer</b> <b>syntax</b> to lossless JPEG 2000 decreases the file size and because of lossless compression quality of image is still same as original image. These compressed images are then copied into Network Attached Storage working as PACS alternative. A series of test conducted in lab with multiple transfer protocol on Network Attached Storage (NAS) to find out which transfer protocol is faster under moderate speed and high latency network...|$|E
40|$|This report {{contains}} a detailed {{assessment of the}} session and presentation services and protocols in order to study their suitability to support high speed applications. We thus concentrate on the performance aspects of these standard protocols. The evaluation will {{be focused on the}} following topics : the assessment of the session layer, the suitability of some Interface Description Languages, data transparency and transfer syntaxes, the presentation protocol. A special interest is given to the description and evaluation of a light weight <b>transfer</b> <b>syntax</b> called FTLWS (Flat Tree Light Weight Syntax) proposed {{as an alternative to the}} ASN. 1 Basic Encoding Rules...|$|E
40|$|Abstract Syntax Notation-One (ASN. 1) is a {{standard}} external data representation language used to define messages of application layer protocols. Its encoding rules, the Basic Encoding Rules (BER), are also international standards that define the encoding/decoding of data values into/from a <b>transfer</b> <b>syntax.</b> Various approaches to automating BER encoding/decoding are examined; in particular, two widely used software packages (ISODE and CASN 1) are studied. A hardware BER encoder/decoder called VASN 1 is presented. Performance of software and hardware approaches are evaluated on real instances of file transfer using {{a standard}} FTAM protocol. Benchmarks obtained from running CASN 1 {{on one of the}} fastest workstations and from running VHDL simulations of VASN 1 indicate the superiority of the hardware approach. © 1993...|$|E
40|$|Thesis (Ph. D.) [...] University of Washington, 2015 This {{dissertation}} {{examines the}} role of L 2 learners’ first language (L 1) in acquiring a target morpho-syntactic feature (case) and learner perceptions of the L 2 grammar, specifically, Korean case and case particles. In addition to investigating what {{and how much is}} transferred from the L 1, the study also looks to see if learners’ perceptions match their actual production. In order to answer these research questions, two quantitative and one qualitative study were used. The initial quantitative study was conducted using an expanded grammaticality judgment task completed by 25 English L 1 learners of Korean, with 15 Korean L 1 controls. The Key findings from that studied suggest that English L 1 learners of Korean acquired nominative case earlier than the accusative case, patterning with Korean L 1 acquisition. Also, learners accurately identified the incorrect usage of nominative particles 60 % of the time, but only 51 % for accusative particles. Building on the findings of that study, speaking and written production tasks were completed by 70 L 2 Korean learners, who were divided into nearly equally-sized groups for three different L 1 s (22 Chinese, 27 English and 21 Japanese). An assumption of the degree of L 1 transfer to L 2 Korean was made specifically for case, which was that Chinese transferred less than English and English less than Japanese. It was hypothesized that deep transfer—that being the <b>transfer</b> of <b>syntax</b> from the L 1 —and surface transfer, which is a transfer of morphology, could be investigated (Sabourin et al 2006). The results highlight that learners, regardless of their L 1, used more correct case particles in writing than in speaking. The Japanese L 1 group had the highest proficiency for case particles, with Chinese being the least proficient. The data confirmed that morphology was transferred ‘over syntax’ from the L 1 and the surface transfer of morphology seems {{to play an important role}} (Montrul 1997, 1999, 2000; Sabourin et al 2006). Finally, to support and clarify the two quantitative studies a series, of qualitative interviews were conducted with 57 participants, and 9 key informants participated in multiple interviews. L 2 learners were aware that their linguistic backgrounds affect feature transferability and the learnability of the target language. Already knowing perceived similar language was seen as beneficial. Kellerman’s Psychotypology (1983) was used to highlights learners’ perceptual language distance between their L 1 and L 2. In this study, Chinese learners still considered Korean relatively close to their L 1, largely due to cultural associations and vocabulary, and that Korean was not difficult to acquire. Learner’s perceptions of ease were not supported by the actual production data. Different motivations also did not seem to be a main factor of the acquisition of case. Therefore, with the findings, it is argued that {{the role of}} L 1 in terms of the same morphosyntaictic features in both L 1 and L 2 is tremendous, and that psychotypology and motivation seem to be overridden by the features...|$|R
40|$|Statistical Machine Translation (SMT) via deep {{syntactic}} transfer {{employs a}} three-stage architecture, (i) parse source language (SL) input, (ii) transfer SL deep syntactic structure {{to the target}} language (TL), and (iii) generate a TL translation. The deep syntactic transfer architecture achieves {{a high level of}} language pair independence compared to other Machine Translation (MT) approaches, as translation is carried out at the more language independent deep syntactic representation. TL word order can be generated independently of SL word order and therefore no reordering model between source and target words is required. In addition, words in dependency relations are adjacent in the deep syntactic structure, allowing the extraction of more general transfer rules, compared to other rules/phrases extracted from the surface form corpus, as such words are often distant in surface form strings, as well as allowing the use of a TL deep syntax language model, which models a deeper notion of fluency than a string-based language model and may lead to better lexical choice. The deep syntactic representation also contains words in lemma form with morpho-syntactic information, and this enables new inflections of lemmas not observed in bilingual training data, that are out of coverage for other SMT approaches, to fall within coverage of deep syntactic transfer. In this thesis, we adapt existing methods already successful in Phrase-Based SMT (PB-SMT) to deep syntactic transfer as well as presenting new methods of our own. We present a new definition for consistent deep <b>syntax</b> <b>transfer</b> rules, inspired by the definition for a consistent phrase in PB-SMT, and we extract all rules consistent with the node alignment, as smaller rules provide high coverage of unseen data, while larger rules provide more fluent combinations of TL words. Since large numbers of consistent transfer rules exist per sentence pair, we also provide an efficient method of extracting rules as well as an efficient method of storing them. We also present a deep syntax translation model, as in other SMT approaches, we use a log-linear combination of features functions, and include a translation model computed from relative frequencies of transfer rules, lexical weighting, as well as a deep syntax language model and string-based language model. In addition, we describe methods of carrying out transfer decoding, the search for TL deep syntactic structures, and how we efficiently integrate a deep syntax trigram language model to decoding, as well as methods of translating morpho-syntactic information separately from lemmas, using an adaptation of Factored Models. Finally, we include an experimental evaluation, in which we compare MT output for different configurations of our SMT via deep syntactic transfer system. We investigate various methods of word alignment, methods of translating morpho-syntactic information, limits on transfer rule size, different beam sizes during transfer decoding, generating from different sized lists of TL decoder output structures, as well as deterministic versus non-deterministic generation. We also include an evaluation of the deep syntax language model in isolation to the MT system and compare it to a string-based language model. Finally, we compare the performance and types of translations our system produces with a state-of-the-art phrase-based statistical machine translation system and although the deep syntax system in general currently under-performs, it does achieve state-of-the-art performance for translation of a specific syntactic construction, the compound noun, and for translations within coverage of the TL precision grammar used for generation. We provide the software for transfer rule extraction, as well as the transfer decoder, as open source tools to assist future research...|$|R
40|$|PKCS # 12 v 1. 1 {{describes}} a <b>transfer</b> <b>syntax</b> for personal identity information, including private keys, certificates, miscellaneous secrets, and extensions. Machines, applications, browsers, Internet kiosks, and so on, that support this standard will allow a user to import, export, and exercise a single set of personal identity information. This standard supports direct transfer of personal information under several privacy and integrity modes. This document represents a republication of PKCS # 12 v 1. 1 from RSA Laboratories ’ Public Key Cryptography Standard (PKCS) series. By publishing this RFC, change control {{is transferred to}} the IETF. IESG Note The IESG thanks RSA Laboratories for transferring change control to the IETF. Enhancements to this specification that preserve backward compatibility are expected in an upcoming IETF Standards Track document. Moriarty, et al. Informational [Page 1...|$|E
