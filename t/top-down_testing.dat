5|38|Public
50|$|<b>Top-down</b> <b>testing</b> is an {{approach}} to integrated testing where the top integrated modules are tested and the branch of the module is tested step by step {{until the end of}} the related module.|$|E
30|$|<b>Top-down</b> <b>testing</b> can be {{performed}} using both a black box and a white box testing method.|$|E
30|$|<b>Top-down</b> <b>testing</b> is {{a method}} that can be widely used for {{protection}} system, especially during site acceptance testing, when {{we can assume that}} all the components of the system have already been configured and tested.|$|E
50|$|An {{alternative}} {{definition of}} a test harness is software constructed to facilitate integration testing. Where test stubs are typically components of the application under development and are replaced by working components as the application is developed (<b>top-down</b> integration <b>testing),</b> test harnesses are external to the application being tested and simulate services or functionality not available in a test environment.|$|R
50|$|Roediger, however, {{approached}} {{this phenomenon}} {{from a more}} retrieval-based standpoint. Rather than looking at intentionality of learning, he looked to {{the conditions in which}} the information was to be recalled. He saw that unintentional learning seemed to be driven by bottom-up processes (using small details from the stimulus to build meaning) and that intentional learning seemed to be driven by top-down processes (using pre-existing concepts to make sense of a stimulus). He predicted that information learned in a bottom-up manner (e.g. reading a word) would be better recalled in a bottom-up test (e.g. completing a word fragment), and information learned in a top-down manner (e.g. generating a mental image) would be better remembered in a <b>top-down</b> <b>test</b> (e.g. recalling a list of words).Roediger hypothesised that the more that the processes used in retrieval matched those used in encoding, the better memory performance would be, and called this framework 'transfer-appropriate processing'. In a number of experiments Roediger and his colleagues showed that, rather than the intentionality when remembering, it was in fact the overlap between the conditions in which learning and remembering occurred that aided memory.|$|R
40|$|Abstract: Tecs is a {{test case}} {{development}} methodology for the functional validation of large electronic systems, typically consisting of several custom hardware and software components. The methodology determines a hierarchical <b>top-down</b> <b>test</b> case development process including test case specification, validation, partitioning and implementation. The test case development process addresses the functional validation {{of the system and}} its components such as ASICs, boards, HW and software modules; it does not facilitate timing or performance verification. The system functions are used to define test cases at the system level and to derive sub-functions for the system components. Test cases are specified, using a special purpose formalism, and validated before they are applied to the system under test. Furthermore, we propose a technique to partition test cases corresponding to the partitioning of the system into sub-systems and components. This technique can significantly reduce system simulation time because it allows the full validation of system functions by simulation at the sub-system and component level. The system model must only be simulated with a reduced set of stimuli to validate the interfaces between sub-systems. We present a test case specification language and tools that support the proposed methodology. The validation of a switching function illustrates methodology, language, and tools. 1...|$|R
40|$|This article {{describes}} how we applied this philosophy {{to the development}} of a commercial secure system. The system had to meet normal commercial requirements for throughput, usability, and cost as well as stringent security requirements. We used a systematic process from requirements elicitation through formal specification, user interface prototyping, rigorous design, and coding in Spark, to ensure these objectives' achievement. System validation included tool-assisted checking of a formal process design, <b>top-down</b> <b>testing,</b> system testing with coverage analysis, and static code analysis. The system uses commercial off the shelf hardware and software but places no reliance on COTS correctness for critical security properties. We show how a process that achieves normal commercial productivity can deliver a highly reliable system that meets all its throughput and usability goals...|$|E
40|$|This paper {{reviews the}} test {{requirements}} {{necessary to ensure}} that an installed Microwave Landing System (MLS) will meet all of the exacting requirements to provide for Category I, II, and III operations. General requirements for MLS testing are presented, and testing is considered throughout the entire life cycle of the equipment, from design to service. In evaluating the requirements of a comprehensive MLS test program, the top-down processes of specifying requirements and designing to requirements are applied to system testing. Resulting from this, an overview is given of a master test plan for MLS that spans the entire system life cycle. The paper is written primarily {{from the perspective of}} the aviation agency. Guidance is given for the agency to design a master test plan, which is rigorous but also practical for implementation, by the MLS contractor. The Requirements For MLS Testing The objective of a comprehensive test program is to ensure that a system meets the technical requirements to serve its operational purpose when installed. As is true for any system, the requirements for aviation navigation systems begin as operational requirements that are based on a fundamental operational need. The operational requirements are translated into technical requirements that in turn are developed into system requirements. Figure 1 shows the relationship between the operational need and the system requirements. This relationship can be used as a framework to plan for comprehensive MLS testing. By planning from the <b>top-down,</b> <b>testing</b> can be performed from the bottom-up to ensure the system meets all of the requirements...|$|E
40|$|The {{project is}} focused on <b>top-down</b> stress <b>testing</b> of the Czech {{insurance}} sector. The aim of the present paper is to describe the advanced method for macro stress testing of insurance companies used by the CNB. We apply the presented stress test to eleven Czech insurance companies. The shocks applied are designed to replicate a macroeconomic scenario and to impact on both the asset and liability sides of the balance sheet. We consider both investment and insurance risks relating to the Czech insurance sector. An application of the simulated scenario to the Czech insurance sector illustrates that the sector is sufficiently resilient and stable...|$|R
40|$|Abstract. A {{characterisation}} of the disjunctive well-founded semantics (DWFS) {{is given}} {{in terms of the}} Gelfond-Lifschitz transformation. This characterisation is used to develop a <b>top-down</b> method of <b>testing</b> DWFS membership, employing a hyperresolutionlike operator and quasi cyclic trees to handle minimal model processing. A flexible bottom-up method of computing the DWFS is also given which admits the use of a powerful reduction operator. For finite propositional databases, all of our methods run in polynomial space...|$|R
40|$|Computer {{vision for}} realtime {{applications}} requires tremendous computational power because all images must be processed {{from the first}} to the last pixel. Ac tive vision by probing specific objects {{on the basis of}} already acquired context may lead to a significant reduction of processing. This idea is based on a few concepts from our visual cortex (Rensink, Visual Cogn. 7, 17 - 42, 2000) : (1) our physical surround can be seen as memory, i. e. {{there is no need to}} construct detailed and complete maps, (2) the bandwidth of the what and where systems is limited, i. e. only one object can be probed at any time, and (3) bottom-up, low-level feature extraction is complemented by <b>top-down</b> hypothesis <b>testing,</b> i. e. there is a rapid convergence of activities in dendritic/axonal connections...|$|R
40|$|We {{present a}} {{revised version of}} the Selective Attention for Identification Model (SAIM), using an initial feature {{detection}} process to code edge orientations. We show that the revised SAIM can simulate both efficient and inefficient human search, that it shows search asymmetries, and that top-down expectancies for targets {{play a major role}} in the model's selection. Predictions of the model for <b>top-down</b> effects are <b>tested</b> with human participants, and important similarities and dissimilarities are discussed. © 2006 Psychology Press Ltd...|$|R
40|$|The aim of {{this thesis}} is to {{determine}} the data requirements and feasibility of data-driven <b>top-down</b> stress <b>testing</b> for credit loss rates. To that end, we use the Adaptive Lasso method to simultaneously select and estimate parsimonious linear models from a very large set of potential model specifications. Adaptive Lasso is a penalized regression method which can accurately and uniquely select substantially relevant predictors and has attractive asymptotic properties. The selected models are able to give accurate forecasts in baseline and severely adverse macro-economic scenarios for the United States. We find that the loan data needs to be divided into a minimum of five categories to adequately capture the link between the macro-economy and credit loss rates. For reliable forecasts, roughly 20 years of credit loss data is required, or at least one complete business cycle must be present in the data...|$|R
40|$|Our study {{presents}} the <b>top-down</b> stress <b>testing</b> framework currently {{used by the}} Magyar Nemzeti Bank. We run separate solvency and liquidity stress tests to analyse {{the ability of the}} banking system to absorb shocks and we present their results in our Report on Financial Stability. In the former, we focus mostly on credit risk but also take into account losses due to market risks. Our study explains in detail the method we apply to quantify the impact of a negative two-year macroeconomic shock on the capital adequacy ratio. We explain the models we use for calculating profit before loan losses, PDs and LGD. We also demonstrate how we measure the impact of an intensive 30 -day liquidity shock on the banking system. Finally, we use the stress test completed in the spring of 2013 to explain in detail how the results should be interpreted and what conclusions we can draw from them...|$|R
50|$|The {{advantage}} of this model {{is that it allows}} a system to be driven, one step at a time, through its states and transitions, while observing the outputs at each step. These are witness values, that guarantee that particular functions were executed on each step. As a result, complex software systems may be decomposed into a hierarchy of Stream X-Machines, designed in a <b>top-down</b> way and <b>tested</b> in a bottom-up way. This divide-and-conquer approach to design and testing is backed by Florentin Ipate's proof of correct integration, which proves how testing the layered machines independently is equivalent to testing the composed system.|$|R
40|$|The CLASS {{model is}} a <b>top-down</b> capital stress <b>testing</b> {{framework}} that projects the effect of different macroeconomic scenarios on U. S. banking firms. The model is based on simple econometric models estimated using public data and also on assumptions about loan loss provisioning, taxes, asset growth, and other factors. We use this framework to calculate a projected industry capital gap relative to a target ratio {{at different points in}} time under a common stressful macroeconomic scenario. This estimated capital gap began rising four years before the financial crisis and peaked at the end of 2008. The gap has since fallen sharply and is now significantly below precrisis levels. In the cross-section, firms projected to be most sensitive to macroeconomic conditions have higher capital ratios, consistent with a 'precautionary' view of bank capital...|$|R
40|$|Action {{observation}} {{is central to}} human social interaction. It allows people to derive what mental states drive others' behaviour and coordinate (and compete) effectively with them. Although previous accounts have conceptualised this ability in terms of bottom‐up (motoric or conceptual) matching processes, more recent evidence suggests that such mechanisms cannot account for the complexity and uncertainty of the sensory input, even in cases where computations should be much simpler (i. e., low‐level vision). It has therefore been argued that perception in general, and social perception in particular, is better described {{as a process of}} <b>top–down</b> hypothesis <b>testing.</b> In such models, any assumption about others—their goals, attitudes, and beliefs—is translated into predictions of expected sensory input and compared with incoming stimulation. This allows perception and action to be based on these expectations or—in case of a mismatch—for one's prior assumptions to be revised until they are better aligned with the individual's behaviour. This article will give a (selective) review of recent research from experimental psychology and (social) neuroscience that supports such views, discuss the relevant underlying models, and current gaps in research. In particular, it will argue that much headway can be made when current research on predictive social perception is integrated with classic findings from social psychology, which have already shown striking effects of prior knowledge on the processing of other people's behaviour...|$|R
40|$|The recent {{financial}} crisis emphasised {{the need for}} effective financial stability analyses and tools for detecting systemic risk. This paper looks at assessment of banking sector resilience through stress testing. We argue such analyses are valuable even in emerging economies that suffer from limited data availability, short time series and structural breaks. We propose a <b>top-down</b> stress <b>test</b> methodology that employs relatively limited information to overcome this data problem. Moreover, as credit growth in emerging economies tends to be rather volatile, we rely on dynamic approach projecting key balance sheet items. Application of our proposed stress test framework to the Russian banking sector reveals a high sensitivity of the capital adequacy ratio to the economic cycle that shows up in both of the two-year macroeconomic scenarios considered: a baseline and an adverse one. Both scenarios indicate the need for capital increase in the Russian banking sector. Furthermore, given that Russia’s banking sector is small and fragmented relative to advanced economies, the loss of external financing can cause profound economic stress, especially for medium-sized and small enterprises. The Russian state has a low public debt-to-GDP ratio and plays decisive role in the banking sector. These factors allow sufficient fiscal space for recapitalisation of problematic banks under both of our proposed baseline and adverse scenarios. stress testing; bank; Russia...|$|R
40|$|The use of macro {{stress tests}} to assess bank {{solvency}} has developed rapidly {{over the past}} few years. This development was reinforced by the financial crisis, which resulted in substantial losses for banks and created general uncertainty about the banking sector's loss-bearing capacity. Macro stress testing has proved a useful instrument to help identify potential vulnerabilities within the banking sector and to gauge its resilience to adverse developments. To support its contribution to safeguarding financial stability and its financial sector-related work in the context of EU/IMF Financial Assistance Programmes, and looking ahead to the establishment of the Single Supervisory Mechanism (SSM), the ECB has developed a <b>top-down</b> macro stress <b>testing</b> framework that is used regularly for forward-looking bank solvency assessments. This paper comprehensively presents the main features of this framework and illustrates how it can be employed for various policy analysis purposes...|$|R
40|$|This article {{aimed to}} test some {{hypotheses}} about the hierarchical structure of self-determined motivation in two longitudinal studies. First, the authors verified {{the stability of}} global self-deter-mined motivation and school self-determined motivation over time. Second, they <b>tested</b> <b>top-down,</b> bottom-up, reciprocal, and horizontal effects between global self-determined motivation and school self-determined motivation. In Study 1, 122 college stu-dents were evaluated on two occasions with a 5 -year interval on their global and school self-determined motivation. In Study 2, 294 college students were evaluated on the same variables with a 1 -year interval. Results from both studies revealed that (a) global self-determined motivation was not more stable than self-deter-mined school motivation over time and (b) a cross-lag model including reciprocal effects between self-determined global and self-determined school motivation offered the best fit indices com-paratively to a model involving only horizontal (or stability) effects. Discussion emphasizes the theoretical implications of the results...|$|R
40|$|The article {{deals with}} {{research}} {{results of the}} sealing of pores in drainage filters by organic particles. Permeability tests were carried out with the constant gradient 1. 5. The water flow through the sample of soil was <b>top-down.</b> The <b>tests</b> were carried out with 2 types of samples: {{the first part of}} samples had layers (from up to down) 300 mm peat and 2 layers of geotextile, the second part consisted of 250 mm peat, 200 mm fine sand and 2 layers of geotextile. Well decomposed peatsamples were used. Peat had the following characteristics: density is 1, 05 [...] . 1, 06 g/cm 3, specific density — 1, 53 [...] . 1, 56 g/cm 3, void ratio — 12, 0 [...] . 12, 5. The duration of each test was 15 days. During testing the hydraulic conductivity of samples was decreased by 1. 3 [...] . 1. 9. After completing the tests the hydraulic conductivity of sand and geotextile were measured. The content of organic matter in geotextile and fine sand was determined as well. Dry mass of organic matter in the first layer of geotextile in the first type of samples were 1, 0 … 1, 3 g per 75 cm 2. The organic matter in the second layer of geotextile in the first type of samples and in the first layer of geotextile in the second type wasn’t exposed. Fine sands protected the drainage geotextile as a result of sealing of pore space of sands by organic matter...|$|R
40|$|This is the publisher’s final pdf. The {{published}} {{article is}} copyrighted by ASTM International {{and can be}} found at: [URL] to this item has been restricted by repository administrators {{at the request of the}} publisher until March 31, 2017. Drilled shaft foundation elements provide a cost-effective foundation alternative for the support of building and bridge superstructure loads. Bi-directional pile loading tests (BDPLTs) to evaluate the capacity of drilled shafts have become popular owing to their capacity to save time and effort as compared to the use of <b>top-down</b> loading <b>tests.</b> However, the use of BDPLTs requires that production shafts be post-grouted following testing in order to assure appropriate in-service performance. Commonly used single-acting loading cells and/or loading cell construction details can pose the potential for the development of voids following post-grouting due to their monotonic jacking action and large footprint. This paper described the development and use of high pressure bi-directional loading cells intended to minimize the possibility of post-test construction defects. First, a comparison was made between the single-acting and double-acting loading cells. Second, the results of laboratory calibrations on the pressurized loading cells were performed, as were component testing of the pumps, hoses, and hydraulic fluid synchronization lines. Then, the use of the new high pressure double-acting loading cells in production testing of instrumented shafts was described, and the efficacy of the new loading cells was illustrated. The new loading cells provided the profession with a load cell alternative for conducting BDLTs and should serve to help reduce the risk of post-test grouting defects in drilled shaft foundations...|$|R
40|$|Approved {{for public}} release, {{distribution}} is unlimitedThe automatic synthesis of a hardware description language (HDL) {{representation of a}} digital device {{has been the subject}} of significant research in the past five years. This thesis explores this topic as it applied to finite state machines and combinational logic expressed in a subset of the IEEE standard language VHDL (VHSIC Hardware Descriptive Language). It describes the subset chosen, and the development of VHDL 2 PDS, a program which automates the process of translating VHDL to PALASM, a hardware synthesis language. The PALASM description is then directly implemented into a field programmable gate array (FPGA) using the XILINX Logic Cell Array (LCA) development system. Complete examples are provided which illustrate <b>top-down</b> design and <b>testing</b> using VHDL. and the use of software to produce a FPGA. This thesis demonstrates that selected constructs in VHDL can be automatically synthesized with a resulting savings in engineering development time due to the simplicity of this approach and the ease of verifying the correctness of the design. Lieutenant, United States Nav...|$|R
30|$|The {{few studies}} that have <b>tested</b> <b>top-down</b> effects on eye {{movements}} in film have what {{may appear to be}} contradictory effects. Lahnakoski et al. (2014) found that giving viewers an explicit task to take a certain perspective (interior decorator or detective) can have a top-down effect on eye movements. Alternatively, to test the same research question as the current study, how comprehension processes affect eye movements, Loschky, Larson, Magliano, and Smith (2015) presented participants with a scene from the James Bond film Moonraker (Broccoli & Gilbert, 1979) and had them start viewing the clip earlier (Context condition) or later (No-context condition). They found that participants had large differences in comprehension due to their context condition, but there were relatively weak effects of comprehension on eye movements. The lack of a top-down effect on eye movements despite large comprehension differences was termed the “Tyranny of Film”. Put differently, the Tyranny of Film is the presence of gaze similarity between groups regardless of comprehension differences between viewers, where gaze similarity refers to groups having the same amount of gaze clustering on the same location(s) in the scene (specific details of the gaze similarity analysis are below).|$|R
40|$|M-PRESS-CreditRisk {{is a new}} <b>top-down</b> macro stress <b>testing</b> {{framework}} that can help supervisors gauge banks' capital adequacy related to credit risk. For the first time, it combines calibration of microprudential capital requirements and macroprudential buffers in a unified, coherent framework. Its core element is an advanced credit portfolio model - SystemicCreditRisk - built upon a rich, non-linear dependence structure for interconnected bank portfolios. Incorporating numerous sector/country-specific systematic factors, the model focuses on credit default concentration risk as {{a major source of}} large losses that may have systemic impact. A test run using a sample of 12 systemically important German banks provides measures for systemic credit risk and the banks' contributions to it in both baseline and stress scenarios. Capital requirements calibrated to the results combine elements of Pillar 1 and Pillar 2, whereas macroprudential buffers can internalize the system's tail risk. The maximum model-based combined requirements range between 6. 3 % and 27. 2 % of credit RWA depending on the bank. A comparison with the reported capital figures suggests that there appears to be enough capital in the banking system, but its distribution might be suboptimal from a systemic point of view as the capital level of a number of banks might need improvement...|$|R
40|$|Abstract: The main {{question}} F&S pose is whether “what {{and how we}} see is functionally independent from what and how we think, know, desire, act, etc. ” We synthesize a collection of concerns from an interdisciplinary set of co-authors regarding F&S’s assumptions and appeals to intuition, resulting in their treatment of visual perception as context-free. No perceptual task {{takes place in a}} contextual vacuum. How do we know that an effect is one of perception qua perception that does not involve other cognitive contributions? Experimental instructions alone involve various cognitive factors that guide task performance (Roepstorff & Frith, 2004). Even a request to detect simple stimulus features requires participants to understand the instructions (“language, memory”), keep track of them (“working memory”), become sensitive to them (“attention”), and pick up the necessary information to become appropriately sensitive (“perception”). These processes work in a dynamic parallelism that is required when one participates in any experiment. Any experiment with enough cognitive content to <b>test</b> <b>top-down</b> effects would seem to invoke all of these processes. From this task-level vantage point, the precise role of visual perception under strict modular assumptions seems, to us, difficult to intuit. We are, presumably, seeking theories that can also account for complex natural perceptual acts. Perceptio...|$|R
40|$|The {{auditory}} {{system is}} capable of perceptually restoring inaudible portions of speech. This restoration may be compromised {{as a result of}} hearing impairment, particularly if it is combined with advanced age, because of degradations in the bottom-up and <b>top-down</b> processes. To <b>test</b> this hypothesis, phonemic restoration was quantitatively measured with hearing-impaired listeners of varying ages and degrees of hearing impairment, as well as with a normal hearing control group. The results showed that the restoration benefit was negatively correlated with both hearing impairment and age, supporting the original hypothesis. Group data showed that listeners with mild hearing loss were able to perceptually restore the missing speech segments as well as listeners with normal hearing. By contrast, the moderately-impaired listeners showed no evidence of perceptual restoration. Further analysis using the articulation index showed that listeners with mild hearing loss were able to increase phonemic restoration with audibility. Moderately-impaired listeners, on the other hand, were unable to do so, even when the articulation index was high. The overall findings suggest that, in addition to insufficient audibility, degradations in the bottom-up and/or top-down mechanisms as a result of hearing loss may limit or entirely prevent phonemic restoration. (C) 2009 Elsevier B. V. All rights reserved...|$|R
40|$|Wong Chung-Yu. Thesis (M. Phil.) [...] Chinese University of Hong Kong, 2002. Includes bibliographical {{references}} (leaf 75). Abstracts in English and Chinese. Abstract [...] - p. iiList {{of figures}} [...] - p. viiiList of tables [...] - p. ixChapter 1. [...] - BACKGROUND [...] - p. 1 Chapter 1. 1 [...] - An analysis of manuals [...] - p. 1 Chapter 1. 2 [...] - Existing practice [...] - p. 5 Chapter 1. 3 [...] - New concept in dynamic manual [...] - p. 7 Chapter 1. 3. 1 [...] - Dynamic representation [...] - p. 7 Chapter 1. 3. 2 [...] - Machine-orientation [...] - p. 8 Chapter 2. [...] - DESIGN PHILOSOPHY [...] - p. 10 Chapter 2. 1 [...] - Concept [...] - p. 10 Chapter 2. 2 [...] - Data node [...] - p. 12 Chapter 2. 3 [...] - Characteristic of function and operator [...] - p. 14 Chapter 2. 4 [...] - Function hierarchy [...] - p. 17 Chapter 2. 5 [...] - Manual-tree (conceptual and actual) [...] - p. 18 Chapter 2. 6 [...] - Coding [...] - p. 20 Chapter 2. 7 [...] - Operation sequence [...] - p. 21 Chapter 2. 8 [...] - Parameter passing [...] - p. 22 Chapter 2. 9 [...] - Manual-tree operation [...] - p. 24 Chapter 3. [...] - IMPLEMETATION OF DYNAMIC MANUAL SYSTEM [...] - p. 25 Chapter 3. 1 [...] - Generator [...] - p. 26 Chapter 3. 1. 1 [...] - File format and generation [...] - p. 26 Chapter 3. 2 [...] - Reader [...] - p. 35 Chapter 3. 2. 1 [...] - Guide service [...] - p. 36 Chapter 3. 2. 2 [...] - Button querying service [...] - p. 37 Chapter 3. 2. 3 [...] - Personal management service [...] - p. 38 Chapter 3. 2. 3. 1 [...] - Insert [...] - p. 38 Chapter 3. 2. 3. 2 [...] - Delete [...] - p. 40 Chapter 3. 2. 3. 3 [...] - Swap [...] - p. 40 Chapter 3. 3 [...] - The graphics [...] - p. 42 Chapter 4. [...] - EXPERIMENTS [...] - p. 43 Chapter 4. 1 [...] - Experiment I (mobile phone) [...] - p. 43 Chapter 4. 1. 1 [...] - Data preparation [...] - p. 43 Chapter 4. 1. 2 [...] - Generating XML file [...] - p. 46 Chapter 4. 1. 3 [...] - Preparing multimedia material [...] - p. 46 Chapter 4. 1. 4 [...] - Design of Reader [...] - p. 47 Chapter 4. 1. 5 [...] - Testing [...] - p. 51 Chapter 4. 1. 5. 1 [...] - <b>Top-down</b> search <b>test</b> [...] - p. 51 Chapter 4. 1. 5. 2 [...] - Bottom-up search test [...] - p. 52 Chapter 4. 1. 5. 3 [...] - function hierarchy modifying test [...] - p. 53 Chapter 4. 2 [...] - Experiment II (pager) [...] - p. 54 Chapter 4. 2. 1 [...] - Data preparation [...] - p. 54 Chapter 4. 2. 2 [...] - Generating XML file [...] - p. 54 Chapter 4. 2. 3 [...] - Preparing multimedia {{material and the}} Reader [...] - p. 56 Chapter 4. 2. 4 [...] - Testing [...] - p. 57 Chapter 4. 2. 4. 1 [...] - <b>Top-down</b> search <b>test</b> [...] - p. 58 Chapter 4. 2. 4. 2 [...] - Bottom-up search test [...] - p. 58 Chapter 4. 2. 4. 3 [...] - function hierarchy modifying test [...] - p. 59 Chapter 4. 3 [...] - Control graphics constrain [...] - p. 61 Chapter 5. [...] - RESULTS [...] - p. 65 Chapter 5. 1 [...] - Change of representation [...] - p. 65 Chapter 5. 2 [...] - Storage and computation requirements [...] - p. 70 Chapter 6. [...] - CONCLUSION [...] - p. 72 References [...] - p. 75 Appendix A. 1 input file of mobile phone (function part) [...] - p. 76 Appendix A. 2 input file of mobile phone (operator part) [...] - p. 96 Appendix B. 1 input file of pager (function part) [...] - p. 105 Appendix B. 2 input file of pager (operator part) [...] - p. 111 Appendix C Function hierarchies and operator lists of the experiments [...] - p. 113 Appendix D Key words [...] - p. 11...|$|R
40|$|When we {{encounter}} {{a new person}} or place, we may easily encode it into our memories, or we may quickly forget it. Recent work finds that this likelihood of encoding a given entity - memorability - is highly consistent across viewers and intrinsic to an image; people tend to remember and forget the same images. However, several forces influence our memories beyond the memorability of the stimulus itself - for example, how attention-grabbing the stimulus is, how much attentional resources we dedicate to the task, or how primed we are for that stimulus. How does memorability interact with these various phenomena, and could any of them explain the effects of memorability found in prior work? This study uses five psychophysical experiments to explore the link between memorability and three attention-related phenomena: 1) bottom-up attention (through testing spatial cueing and visual search), 2) <b>top-down</b> attention (through <b>testing</b> cognitive control and depth of encoding), and 3) priming. These experiments find that memorability remains resilient {{to all of these}} phenomena - none are able to explain memorability effects or overcome the strong effects memorability has on determining memory performance. Thus, memorability is truly an independent, intrinsic attribute of an image that works in conjunction with these phenomena to determine if an event will ultimately be remembered. Comment: 37 pages, 9 figure...|$|R
40|$|In many {{research}} {{fields as}} high energy physics (HEP), astrophysics, nuclear medicine or space engineering with harsh operating conditions, {{the use of}} fast and flexible digital communication protocols {{is becoming more and}} more important. The possibility to have a smart and <b>tested</b> <b>top-down</b> design flow for the design of a new protocol for control/readout of front-end electronics is very useful. To this aim, and to reduce development time, costs and risks, this paper describes an innovative design/verification flow applied as example case study to a new communication protocol called FF-LYNX. After the description of the main FF-LYNX features, the paper presents: the definition of a parametric SystemC-based Integrated Simulation Environment (ISE) for high-level protocol definition and validation; the set up of figure of merits to drive the design space exploration; the use of ISE for early analysis of the achievable performances when adopting the new communication protocol and its interfaces for a new (or upgraded) physics experiment; the design of VHDL IP cores for the TX and RX protocol interfaces; their implementation on a FPGA-based emulator for functional verification and finally the modification of the FPGA-based emulator for testing the ASIC chipset which implements the rad-tolerant protocol interfaces. For every step, significant results will be shown to underline the usefulness of this design and verification approach that can be applied to any new digital protocol development for smart detectors in physics experiments...|$|R
40|$|A major {{question}} in soil ecology is whether soil food webs are regulated by resources or by predators, i. e. bottom-up (donor) or <b>top-down</b> controlled. We <b>tested</b> {{the hypothesis that}} meso- and macrofaunal soil predators can regulate fungivore populations and, thereby cause a top-down cascade effect on fungal biomass and decomposition/mineralisation processes in boreal forest soils. The study was performed as a microcosm experiment with two contrasting soils (humus layers), one poor and one rich in N, and with different combinations of fungivore and predator soil fauna added to "defaunated" soil. In comparison with control microcosms lacking mesofauna (but with nematodes and protozoans), {{the presence of a}} diverse Collembola and Oribatida fungivore community significantly reduced the FDA-active fungal biomass or tended to reduce the ergosterol fraction of the fungal biomass in the N-poor humus, but no clear effect could be detected in the N-rich humus. Fungivores as well as fungivores plus predators (a predator community consisting of gamasids, spiders and beetles or a subset thereof) reduced C mineralisation and increased net N mineralisation in both soils. The presence of predators (particularly gamasid mites) reduced collembolan numbers and alleviated the negative effect of fungivores on fungal biomass in the N-poor soil. In the N-rich soil, the presence of predators increased fungal biomass (ergosterol) in relation to the "defaunated" soil. Therefore, a top-down trophic cascade could be detected in the N-poor humus but not in the N-rich humus. Our results suggest that the degree of top-down control in soil fauna communities depends on resource quality and soil fertility...|$|R
40|$|AbstractEspecially {{after the}} recent {{financial}} crisis {{that started in}} mortgage markets and spread all over international markets, {{in order to better}} monitor financial risks, the importance and use of stress testing has increased. In this study, <b>top-down</b> macro stress <b>testing</b> from a supervisory perspective was analyzed and discussed in general terms. Then, probable effects of credit, interest rate, exchange rate, and contagion risks on capital adequacy of Turkish banking sector in specified baseline and adverse scenarios were examined. First, different satellite panel econometric models for corporate and retail loans were used to estimate loan growth and non-performing loans. Second, model results were utilized to see effects of macro-economic scenarios on Turkish banking system according to the Basel's standard and economic capital approaches. Results of the study showed that economic growth and interest rates change had significant effects on corporate loans while along with these variables unemployment rate had significant effect on retail loans. Moreover, economic growth, exchange rates, and unemployment rate had significant impacts on corporate non-performing loans while only economic growth and unemployment rate had significant impact on non-performing retail loans. According to the study results, while there was no significant impact of exchange rates on net income of the sector because of low level net foreign currency positions, the main effect was on capital adequacy via revaluation of foreign currency denominated risk weighted assets. We found that the robust capital base of Turkish banking sector was a crucial factor in resilience of the sector’ capital adequacy against financial shocks...|$|R
40|$|During natural viewing, the {{trajectories of}} saccadic eye {{movements}} often deviate dramatically from a straight-line path between objects. In human studies, saccades {{have been shown}} to deviate toward or away from salient visual distractors depending on visual- and goal-related parameters, but the neurophysiologi-cal basis for this is not well understood. Some studies suggest that deviation toward is associated with competition between simultaneously active sites within the intermediate layers of the superior colliculus (SC), a midbrain structure that integrates sensory and goal-related signals for the production of saccades. In contrast, deviation away is hypothesized to reflect a higher-level process, whereby the neural site associated with the distractor is actively suppressed via a form of endogenous, <b>top-down</b> inhibition. We <b>tested</b> this hypothesis by measuring presaccadic distractor-evoked activation of SC visuomotor neurons while monkeys performed a simple task configured specifically to induce a high degree of saccades that deviate away. In the SC, cognitive processes such as top-down expectation are represented as variation in the sustained, low-frequency presaccadic discharge. We reasoned that any inhibition at the distractor-related locus associated with saccade deviation should affect the excitability of the neuron, thereby affecting the discharge rate. We found that, although the task produced robust deviation away, {{there was no evidence of}} a relationship between saccade deviation and distractor-evoked activation outside a short perisaccadic window that began no earlier than 22 msec before saccade onset. This indicates that deviation away is not adequately explained by a form of sustained, top-down inhibition at the distractor-related locus in the SC. The results are discussed in relation to the primary sources of inhibition associated with saccadic control. © 2012 Massachusetts Institute of Technology...|$|R
40|$|The primary {{intention}} {{of this research}} has been to establish how the specialized Tairona community of Chengue was formed and how social inequality {{plays a role in}} socio-economic change from 200 BC to 1650 AD. The main questions are organized around two opposing scenarios designed to <b>test</b> <b>top-down</b> and bottom-up processes for community formation. In the top-down scenario the community would be the result of an external agent that had sufficient authority to "create" a community with the intention to extract a highly concentrated resource, marine salt. In the alternative scenario, the bottom-up process, the community would become specialized {{as a result of a}} slower process in which the changes that led to specialization are the product of decisions of the individuals who resided in Chengue and natural environmental changes. Consequently specialization would have been the role of individual agents (individuals and households) at a very small scale. Although the observed sequence had components from both scenarios, the bottom-up process appears to be the primary force in the formation of a specialized community and the production of surplus that led to social inequality. Study of soils, lagoon and coastal sediments, flora and fauna allowed the climatic reconstruction the last 2500 years. During this long span of time communal units larger than households but smaller than villages had great stability and appear to have been the motors of socio-economic change. The evidence from Chengue suggests that progressive specialization in the context of environmental limitations produced a group of people less well-off than others. Elites do not; however, appear to have had much range of political action during most of the sequence...|$|R
40|$|International audience"Travelling in time," {{a central}} feature of episodic memory is {{severely}} affected among individuals with Post Traumatic Stress Disorder (PTSD) with two opposite effects: vivid traumatic memories are unorganized in temporality (bottom-up processes), non-traumatic personal memories tend to lack spatio-temporal details and false recognitions occur more frequently {{that in the}} general population (<b>top-down</b> processes). To <b>test</b> the effect of these two types of processes (i. e. bottom-up and top-down) on emotional memory, we conducted two studies in healthy and traumatized adolescents, a period of life in which vulnerability to emotion is particularly high. Using negative and neutral images selected from the international affective picture system (IAPS), stimuli were divided into perceptual images (emotion generated by perceptual details) and conceptual images (emotion generated by the general meaning of the material). Both categories of stimuli were then used, along with neutral pictures, in a memory task with two phases (encoding and recognition). In both populations, we reported a differential effect of the emotional material on encoding and recognition. Negative perceptual scenes induced an attentional capture effect during encoding and enhanced the recollective distinctiveness. Conversely, the encoding of conceptual scenes was similar to neutral ones, but the conceptual relatedness induced false memories at retrieval. However, among individuals with PTSD, two subgroups of patients were identified. The first subgroup processed the scenes faster than controls, except for the perceptual scenes, and obtained similar performances to controls in the recognition task. The second subgroup group desmonstrated an attentional deficit in the encoding task with no benefit from the distinctiveness associated with negative perceptual scenes on memory performances. These findings provide a new perspective on how negative emotional information may have opposite influences on memory in normal and traumatized individuals. It also gives clues to understand how intrusive memories and overgeneralization takes place in PTSD...|$|R
40|$|Seagrass and seaweed {{habitats}} constitute hotspots {{for diversity}} and ecosystem services in coastal ecosystems. These habitats {{are subject to}} anthropogenic pressures, of which eutrophication is one major stressor. Eutrophication favours fast-growing ephemeral algae over perennial macroalgae and seagrasses, causing habitat degradation. However, changes in top-down control, caused by, for example, overfishing, may also have negative impacts on such habitats by decreasing grazer control of ephemeral algae. Meanwhile, systematic analyses estimating top-down effects of predator manipulations across {{a wide range of}} studies are missing, limiting the potential use of top-down control measures in coastal management. Here, we review the literature on experiments that <b>test</b> <b>top-down</b> and bottom-up controls in seagrass Zostera marina and seaweed Fucus spp. food webs in the North Atlantic. Using meta-analysis and meta-regression, we compare effect sizes of consumer and nutrient manipulations on primary producers, grazers and mesopredators. Presence of mesopredators on average doubled the biomass of ephemeral algae through trophic cascades, mainly mediated via negative effects on amphipods and isopods. Of the grazers, gastropods had twice as strong a negative effect on ephemeral algae as amphipods/isopods, but responded weakly to both predators and fertilization. In accordance with theory, top-down effects became stronger with eutrophication. Across studies, top-down effects on ephemeral algae at all trophic levels are on par with eutrophication effects. However, the few studies manipulating piscivorous fish make estimates of their top-down effects uncertain. Synthesis and applications. Consistently strong top-down effects in coastal ecosystems call for an integrated ecosystem perspective. Management should consider measures to improve stocks of predatory fish and reduce mesopredators for restoration and conservation of essential seagrass and seaweed habitats, thereby increasing the long-term viability of ecosystem services from coastal habitats. Editor's Choic...|$|R
