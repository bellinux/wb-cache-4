568|9|Public
25|$|The {{separation}} of proclitics and prefixed prepositions from nouns after {{them in order}} to ameliorate the <b>tokenization</b> for Maghrebi Arabic.|$|E
25|$|There {{is also an}} {{advanced}} data warehouse edition of Informix. This version includes the Informix Warehouse Accelerator which uses a combination of newer technologies including in-memory data, <b>tokenization,</b> deep compression, and columnar database technology to provide extreme high performance on business intelligence and data warehouse style queries.|$|E
25|$|These transcriptions also {{involved}} some innovations in the transcription of Arabic dialects that were done to let NLP analysis of Maghrebi Arabic and mainly Tunisian easier. In fact, prefixed prepositions and proclitics {{are separated from}} the nouns next to them to improve the <b>tokenization</b> of the dialects, the word beginning with il and the il- determinant are differentiated by adding a hyphen to the determinant, all emphatic letters are represented to avoid difficulties in vowel transcription, the suffix of the conjugation of verbs in present in plural and the singular third person direct object pronouns are differentiated by transcribing them differently, and the transcriptions of glottal stop and of Ta Marbūṭa are simplified.|$|E
40|$|Given the {{contemporary}} trend to modular NLP architectures and multiple annotation frameworks, {{the existence of}} concurrent <b>tokenizations</b> of the same text represents a pervasive problem in everyday’s NLP practice and poses a non-trivial theoretical problem to the integration of linguistic annotations and their interpretability in general. This paper describes a solution for integrating different <b>tokenizations</b> using a standoff XML format, and discusses the consequences for the handling of queries on annotated corpora...|$|R
40|$|Modern topic {{identification}} (topic ID) {{systems for}} speech use {{automatic speech recognition}} (ASR) to produce speech transcripts, and perform supervised classification on such ASR outputs. However, under resource-limited conditions, the manually transcribed speech required to develop standard ASR systems can be severely limited or unavailable. In this paper, we investigate alternative unsupervised solutions to obtaining <b>tokenizations</b> of speech {{in terms of a}} vocabulary of automatically discovered word-like or phoneme-like units, without depending on the supervised training of ASR systems. Moreover, using automatic phoneme-like <b>tokenizations,</b> we demonstrate that a convolutional neural network based framework for learning spoken document representations provides competitive performance compared to a standard bag-of-words representation, as evidenced by comprehensive topic ID evaluations on both single-label and multi-label classification tasks. Comment: 5 pages, 2 figures; accepted for publication at Interspeech 201...|$|R
40|$|We {{summarize}} {{the accomplishments of}} a multi-disciplinary workshop exploring the computational and scientific issues surrounding zero resource (unsupervised) speech technologies and related models of early language acquisition. Centered around the tasks of phonetic and lexical discovery, we consider unified evaluation metrics, present two new approaches for improving speaker independence {{in the absence of}} supervision, and evaluate the application of Bayesian word segmentation algorithms to automatic subword unit <b>tokenizations.</b> Finally, we present two strategies for integrating zero resource techniques into supervised settings, demonstrating the potential of unsupervised methods to improve mainstream technologies. 5 page(s...|$|R
2500|$|However, this {{solution}} was not adequate for mathematically-oriented languages such as FORTRAN (1955) and ALGOL (1958), which used the hyphen as an infix subtraction operator. These early languages instead allowed identifiers to contain unrestricted embedded spaces, determining {{the end of}} the identifier by context. This approach was abandoned in later languages due to the complexity it adds to <b>tokenization.</b> (FORTRAN initially restricted identifiers to no more than six characters, effectively preventing multi-word identifiers except those made of very short words, such as [...] "GO TO"="GOTO".) ...|$|E
50|$|Visa Inc. {{released}} Visa <b>Tokenization</b> Best Practices for <b>tokenization</b> uses {{in credit}} and debit card handling applications and services.|$|E
50|$|<b>Tokenization</b> is {{the process}} of parsing text data into smaller units (tokens) such as words and phrases. Commonly used <b>tokenization</b> methods include Bag-of-words model and N-gram model.|$|E
40|$|Noise {{data in the}} Web {{document}} {{significantly affect}} {{on the performance of}} the Web information management system. Many researchers have proposed document structure based noise data elimination methods. In this paper, we propose a different approach that uses a redundant information elimination approach in the Web documents from the same URL path. We propose a redundant word/phrase filtering method for single or multiple <b>tokenizations.</b> We conducted two experiments to examine efficiency and effectiveness of our filtering approaches. Experimental results show that our approach produces a high performance in these two criteri...|$|R
40|$|Abstract—Noise {{data in the}} Web {{document}} {{significantly affect}} {{on the performance of}} the Web information management system. Many researchers have proposed document structure based noise data elimination methods. In this paper, we propose a different approach that uses a redundant information elimination approach in the Web documents from the same URL path. We propose a redundant word/phrase filtering method for single or multiple <b>tokenizations.</b> We conducted two experiments to examine efficiency and effectiveness of our filtering approaches. Experimental results show that our approach produces a high performance in these two criteria. N Noise Elimination from the Web Documents by Using URL paths an...|$|R
50|$|All of MASC {{includes}} manually validated annotations for {{logical structure}} (headings, sections, paragraphs, etc.), sentence boundaries, three different <b>tokenizations</b> with associated {{part of speech}} tags, shallow parse (noun and verb chunks), named entities (person, location, organization, date and time), and Penn Treebank syntax. Additional manually produced or validated annotations have been produced by the MASC project for portions of the sub-corpus, including full-text annotation for FrameNet frame elements and a 100K+ sentence corpus with WordNet 3.1 sense tags, of which one-tenth are also annotated for FrameNet frame elements. Annotations of all or portions of the sub-corpus {{for a wide variety}} of other linguistic phenomena have been contributed by other projects, including PropBank, TimeBank, MPQA opinion, and several others. Co-reference annotations and clause boundaries of the entire MASC corpus are scheduled to be released by the end of 2016.|$|R
50|$|When {{properly}} validated {{and with}} appropriate independent assessment, <b>Tokenization</b> can render {{it more difficult}} for attackers to gain access to sensitive data outside of the <b>tokenization</b> system or service. Implementation of <b>tokenization</b> may simplify the requirements of the PCI DSS, as systems that no longer store or process sensitive data may have a reduction of applicable controls required by the PCI DSS guidelines.|$|E
5000|$|While {{biometric}} <b>tokenization</b> and Apple Pay are similar, biometric <b>tokenization</b> as it {{is known}} today and particularly using the term verbatim is an authentication feature that goes beyond payment convenience and security. Other distinctive features are that biometric <b>tokenization</b> can be implemented on other operating systems such as OSX, Microsoft Windows, Google Android for password-less login to desktop and mobile applications.|$|E
5000|$|<b>Tokenization,</b> {{when applied}} to data security, {{is the process of}} substituting a {{sensitive}} data element with a non-sensitive equivalent, referred to as a token, that has no extrinsic or exploitable meaning or value. The token is a reference (i.e. identifier) that maps back to the sensitive data through a <b>tokenization</b> system. The mapping from original data to a token uses methods which render tokens infeasible to reverse {{in the absence of the}} <b>tokenization</b> system, for example using tokens created from random numbers. The <b>tokenization</b> system must be secured and validated using security best practices applicable to sensitive data protection, secure storage, audit, authentication and authorization. The <b>tokenization</b> system provides data processing applications with the authority and interfaces to request tokens, or detokenize back to sensitive data. [...] The security and risk reduction benefits of <b>tokenization</b> require that the <b>tokenization</b> system is logically isolated and segmented from data processing systems and applications that previously processed or stored sensitive data replaced by tokens. Only the <b>tokenization</b> system can tokenize data to create tokens, or detokenize back to redeem sensitive data under strict security controls. The token generation method must be proven to have the property that there is no feasible means through direct attack, cryptanalysis, side channel analysis, token mapping table exposure or brute force techniques to reverse tokens back to live data.|$|E
40|$|Acoustic unit {{discovery}} (AUD) is {{a process}} of automatically identifying a categorical acoustic unit inventory from speech and producing corresponding acoustic unit <b>tokenizations.</b> AUD provides an important avenue for unsupervised acoustic model training in a zero resource setting where expert-provided linguistic knowledge and transcribed speech are unavailable. Therefore, to further facilitate zero-resource AUD process, in this paper, we demonstrate acoustic feature representations can be significantly improved by (i) performing linear discriminant analysis (LDA) in an unsupervised self-trained fashion, and (ii) leveraging resources of other languages through building a multilingual bottleneck (BN) feature extractor to give effective cross-lingual generalization. Moreover, we perform comprehensive evaluations of AUD efficacy on multiple downstream speech applications, and their correlated performance suggests that AUD evaluations are feasible using different alternative language resources when only a subset of these evaluation resources can be available in typical zero resource applications. Comment: 5 pages, 1 figure; Accepted for publication at ICASSP 201...|$|R
40|$|Identifier {{names are}} the main vehicle for {{semantic}} information during program comprehension. For tool-supported program comprehension tasks, including concept location and requirements traceability, identifier names need to be tokenised into their semantic constituents. In this paper we present an approach to the automated tokenisation of identifier names that improves on existing techniques in two ways. First, it improves the tokenisation accuracy for single-case identifier names and for identifier names containing digits, which existing techniques largely ignore. Second, performance gains over existing techniques are achieved using smaller oracles, making the approach easier to deploy. Accuracy was evaluated by comparing our algorithm to manual <b>tokenizations</b> of 28, 000 identifier names drawn from 60 well-known open source Java projects totalling 16. 5 MSLOC. Moreover, the projects were used to perform a study of identifier tokenisation features (single case, camel case, use of digits, etc.) per object-oriented construct (class names, method names, local variable names, etc.), thus providing an insight into naming conventions in industrial-scale object-oriented code. Our tokenisation tool and datasets are publicly available...|$|R
40|$|In this study, we will {{cope with}} the {{creation}} of a lingware for Natural Language Processing (NLP) applications, composed by multi word-expression terminological electronic dictionaries (in Machine-Readable Form) and by local grammars (in the form of finite-state automata and transducers). Both parts of this lingware were built and applied according to Lexis-Grammar (LG) formalization principles and methods. Actually, Lexis-Grammar is the investigation system practised by the “Maurice Gross Group” of the Communication Sciences Department at the University of Salerno (Italy). The electronic dictionaries, namely those of compound words we will deal with in this paper, include entries coming from the knowledge domains of European Community Information, and have been extracted from 15 institutional glossaries produced by European Community lexicon policies, or built according to these policies. Also, a text corpus (3. 071. 610 <b>tokenizations)</b> has been created from European Community Govern Information. These texts were extracted from European Community govern Web sites. The manual construction of finite-states automata and transducers was developed using a software for NLP. Together, dictionaries and grammars will form the basis for a Smart Information Retrieval System, an application which will automatically recognize a given set of frequently-asked questions (from here on, FAQs) on European Community Information, previously formalized as syntactic patterns inside local grammars...|$|R
5000|$|Another {{limitation}} of <b>tokenization</b> technologies is measuring {{the level of}} security for a given solution through independent validation. With the lack of standards, the latter is critical to establish the strength of <b>tokenization</b> offered when tokens are used for regulatory compliance. The PCI Council recommends independent vetting and validation of any claims of security and compliance: [...] "Merchants considering the use of <b>tokenization</b> should perform a thorough evaluation and risk analysis to identify and document the unique characteristics of their particular implementation, including all interactions with payment card data and the particular <b>tokenization</b> systems and processes" ...|$|E
5000|$|With tokenization's {{increasing}} adoption, new <b>tokenization</b> technology {{approaches have}} emerged to remove such operational risks and complexities and to enable increased scale suited to emerging big data use cases and high performance transaction processing, especially in financial services and banking. Recent examples includes Protegrity's Vaultless <b>Tokenization</b> and more recent Voltage Security's Secure Stateless <b>Tokenization</b> (SST) technology [...] which enables random mapping of live data elements to surrogate values without needing a database while retaining the isolating properties of <b>tokenization.</b> PVT and SST have been independently validated to provide significant limitation of applicable PCI Data Security Standard (PCI DSS) controls to reduce scope of assessments.|$|E
5000|$|<b>Tokenization</b> {{may be used}} to {{safeguard}} sensitive data involving, for example, bank accounts, financial statements, medical records, criminal records, driver's licenses, loan applications, stock trades, voter registrations, and other types of personally identifiable information (PII). <b>Tokenization</b> is often used in credit card processing. The PCI Council defines <b>tokenization</b> as [...] "a process by which the primary account number (PAN) is replaced with a surrogate value called a token. De-tokenization is the reverse process of redeeming a token for its associated PAN value. The security of an individual token relies predominantly on the infeasibility of determining the original PAN knowing only the surrogate value". The choice of <b>tokenization</b> as an alternative to other techniques such as encryption will depend on varying regulatory requirements, interpretation, and acceptance by respective auditing or assessment entities. This is in addition to any technical, architectural or operational constraint that <b>tokenization</b> imposes in practical use.|$|E
50|$|<b>Tokenization</b> is {{the process}} of {{replacing}} sensitive data with unique identification numbers (tokens) and storing the original data on a central server (typically in encrypted form). <b>Tokenization</b> can help thwart hackers and minimize the scope of compliance audits when it is stored in a single central location. <b>Tokenization</b> is used to protect sensitive data like credit card personal account numbers (PAN), bank account numbers, social security numbers, driver's license numbers and other personally identifiable information (PII).|$|E
5000|$|<b>Tokenization</b> is {{the process}} of {{breaking}} up a text string into words or other meaningful elements called tokens. Typically, <b>tokenization</b> occurs at the word level. However, it is sometimes difficult to define what is meant by a [...] "word". Often a tokenizer relies on simple heuristics, such as splitting the string on punctuation and whitespace characters. <b>Tokenization</b> is more challenging in languages without spaces between words, such as Chinese and Japanese. Tokenizing text in these languages requires the use of word segmentation algorithms.|$|E
50|$|Biometric <b>tokenization</b> in {{particular}} builds upon the longstanding practice of <b>tokenization</b> for sequestering secrets {{in this manner}} by having the secret, such as user credentials like usernames and passwords or other Personally Identifiable Information (PII), be represented by a substitute key in the public sphere.|$|E
50|$|Natural {{language}} processing {{is the subject}} of continuous research and technological improvement. <b>Tokenization</b> presents many challenges in extracting the necessary information from documents for indexing to support quality searching. <b>Tokenization</b> for indexing involves multiple technologies, the implementation of which are commonly kept as corporate secrets.|$|E
5000|$|Biometric <b>tokenization</b> {{like its}} non-biometric counterpart, <b>tokenization,</b> {{utilizes}} end-to-end encryption to safeguard data in transit. With biometric <b>tokenization,</b> a user initiates {{his or her}} authentication first by accessing or unlocking biometrics such as fingerprint recognition, facial recognition system, speech recognition, iris recognition or retinal scan, or combination of these biometric modalities. The user’s unique qualities are generally stored {{in one of two}} ways, either on-device in a trusted execution environment (TEE) or trusted platform module (TPM), or on a server the way other data are stored.|$|E
5000|$|<b>Tokenization</b> {{was applied}} to payment card data by Shift4 Corporation and {{released}} to the public during an industry Security Summit in Las Vegas, Nevada in 2005. The technology is meant to prevent the theft of the credit card information in storage. Shift4 defines <b>tokenization</b> as: “The concept of using a non-decryptable piece of data to represent, by reference, sensitive or secret data. In payment card industry (PCI) context, tokens are used to reference cardholder data that is managed in a <b>tokenization</b> system, application or off-site secure facility.” ...|$|E
5000|$|In practice, {{document}} clustering {{often takes}} the following steps: 1. <b>Tokenization</b> ...|$|E
5000|$|Feb. 2015: Voltage Security, {{provider}} of data-centric encryption and <b>tokenization</b> software ...|$|E
5000|$|... (Note: <b>Tokenization</b> in {{the field}} of {{computer}} security has a different meaning.) ...|$|E
5000|$|<b>Tokenization</b> (data security) - not storing {{the full}} number in {{computer}} systems ...|$|E
50|$|In March 2014, EMVCo LLC {{released}} its first payment <b>tokenization</b> specification for EMV.|$|E
50|$|November 2014, American Express {{released}} its token service which meets the EMV <b>tokenization</b> standard.|$|E
5000|$|The {{separation}} of proclitics and prefixed prepositions from nouns after {{them in order}} to ameliorate the <b>tokenization</b> for Maghrebi Arabic.|$|E
50|$|The {{result of}} {{implementing}} biometric <b>tokenization</b> is an “inverted” authentication channel in which hackers {{are unable to}} gain credentials by attacking a centralized repository of biometrics and are forced to target individual user devices. By forcing malicious attackers {{to divert attention from}} a centralized validation server to many client-side devices, biometric <b>tokenization</b> decreases the viability of an attack on the client-server authentication channel, rendering the common large-scale server-side data breach an unscalable attack vector.|$|E
50|$|The {{grammar of}} an output parser can be {{described}} in a PEG (Parsing Expression Grammar). The PEG is a top-down parsing language, and is similar to the regular expression grammar. Compared with a bottom-up parsing language, like Yacc's one, the PEG is much more intuitive and cannot be ambiguous. The PEG does not require <b>tokenization</b> to be a separate step, and <b>tokenization</b> rules can be written {{in the same way as}} any other grammar rules.|$|E
