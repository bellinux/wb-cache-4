2|17|Public
40|$|Abstract — The {{development}} {{and evolution of}} solid modeling has been motivated primarily by its potential for supporting automated applications. One of its main applications involves analysis in engineering, checking model consistency and its compatibility with the environment {{where it will be}} actually used, justifying the approval or any alteration of the project. This paper presents a survey of solid modeling aiming at process automation in Electromagnetism, specially for Finite Element Analysis, emphasizing the necessity of data <b>transfer</b> <b>standardization</b> between CAD systems and the search for informationally complete models, which allow automation during whole product’s life cycle...|$|E
40|$|Nonstandard set {{theories}} typically postulate {{validity of}} the axioms of ZFC, including the Axiom of Choice, in the standard (or internal) universe. They also postulate some versions of <b>Transfer,</b> <b>Standardization</b> and Idealization. An easy argument shows that the last three principles, together with ZF alone, imply the Boolean Prime Ideal Theorem (BPI) : Let F be a standard filter over a standard set S = ⋃F. By Idealization, there is an (internal) element x ∈ S such that x ∈ X for all standard X ∈ F. By Standardization, there is a standard U ⊆ P(S) such that X ∈ U ↔ x ∈ X, for all standard X ∈ P(S). Trivially, U is a standard ultrafilter and F ⊆ U. By Transfer, for every filter F there is an ultrafilter U such that F ⊆ U. This statement (“Ultrafilter Theorem”) {{is one of the}} forms equivalent to BPI; see Jech [5, Theorem 2. 2]. The purpose of this note is to point out that this is the strongest possible result. We use the monograph of Kanovei and Reeken [7] as a general reference to nonstandard set theories. The best known among these is Nelson’s IST, so we begin our considerations with IST. Let IST − be IST with the Axiom of Choice deleted. The argument above shows that IST − ` BPI. Theorem 1 IST − is a conservative extension of ZF + BPI. The analogous result for IST, to wit, that IST is a conservative extension of ZFC, is proved using ultrapowers. While BPI guarantees existence of many ultrafilters, it doe...|$|E
5000|$|<b>Transfer</b> pricing rules, <b>standardization</b> {{of which}} has been greatly helped by the {{promulgation}} of OECD guidelines.|$|R
5000|$|DGMK {{facilitates}} {{joint research}} to advance petroleum {{science and technology}} <b>transfer</b> to industry. <b>Standardization</b> efforts are performed by the affiliated FAM German Standardization Committee on Petroleum Products and Lubricants that cooperates with DIN. Erdöl Erdgas Kohle (Oil-Gas-Carbon) is the official journal for DGMK.http://www.oilgaspublisher.de/en/periodicals/eek.html There are approximately 1,700 members consisting of individuals, academic institutions, and industrial entities.|$|R
50|$|In 1977 Edward Nelson {{provided}} an answer following the second approach. The extended axioms are IST, which stands either for Internal Set Theory {{or for the}} initials of the three extra axioms: Idealization, <b>Standardization,</b> <b>Transfer.</b> In this system we consider that the language is extended {{in such a way}} that we can express facts about infinitesimals. The real numbers are either standard or nonstandard. An infinitesimal is a nonstandard real number that is less, in absolute value, than any positive standard real number.|$|R
5000|$|The Joint Commission on Accreditation of Hospitals (JCAH) was {{established}} in 1951 as an independent and non-profit organization that provided voluntary accreditation to hospitals that met minimum quality standards. [...] JCAH was formed by the combined forces of the American College of Physicians, the American College of Surgeons, the American Hospital Association, the American Medical Association, and the Canadian Medical Association. In 1952, the ACS formally <b>transferred</b> its Hospital <b>Standardization</b> Program to JCAH. JCAH began to charge a fee for surveys in 1964.|$|R
40|$|Calibration {{transfer}} {{has received}} considerable {{attention in the}} recent literature. Several standardization methods have been proposed for transferring calibration models between equipments. The goal {{of this paper is}} to present a general revision of calibration transfer techniques. Basic concepts will be reviewed, as well as the main advantages and drawbacks of each technique. A case study based on a set of 80 NIR spectra of maize samples recorded on two different instruments is used to illustrate the main calibration <b>transfer</b> techniques (direct <b>standardization,</b> piecewise direct standardization, orthogonal signal correction and robust variable selection) ...|$|R
40|$|Cancer centers, {{exemplary}} {{the breast}} cancer unit of the Charite universitiy clinic, work as leading units for oncological patients. Their work is described by an excellent interdisciplinary approach with intergrated standards of care. In this manusript the local <b>transfer</b> of <b>standardization</b> is shown exemplary in 5 publications. Working fields are diversified and show the heterogeneous requirements. The implementation of a clinical pathway in breast surgery showed {{the benefit of a}} structured quality assessment. We saw a reduced mean hospital stay with reduced costs. The second publication showed a good correlation between preoperative histological biospy results with the operation specimen with respect to histological markers. The third paper described a newly found marker for neuroendocrine tumors of the breast. As an example for translational research the forth paper showed the signifcant work with endothethial progenitor cells in breast cancer patients. As transsectorial work the breast care nurse education program was analysed. The aim has to be to improve the care for oncological patients...|$|R
40|$|Among the {{responsibilities}} {{assigned to the}} Office of the Manager, National Communications System (NCS), is {{the management of the}} Federal Telecommunications Standards Program. Under this program, the NCS, {{with the assistance of the}} Federal Telecommunications Standards Committee, identifies, develops, and coordinates proposed Federal Standards which either contribute to the interoperability of functionally similar Federal telecommunications systems or to the achievement of a compatible and efficient interface between computer and telecommunications systems. In developing and coordinating these standards, a considerable amount of effort is expended in initiating and pursuing joint standards development efforts with appropriate technical committees of the International Organization for Standardization, the International Telecommunications Union-Telecommunications Standardization Sector, and the American National Standards Institute. This Technical Information Bulletin presents an overview of an effort which is contributing to the development of compatible Federal and national standards in the area of Asynchronous <b>Transfer</b> Mode <b>Standardization.</b> It has been prepared to inform interested Federal and industry activities. Any comments, inputs or statements of requirements which could assist in the advancement of this work are welcome and should be addressed to: Office of the Manage...|$|R
40|$|Abstract. Standardization {{activities}} {{are recognized as}} one of the tools to incubate research results and accelerate their transfer to innovative marketable products and services. However, the European Commission (EC) research community and its associated stakeholders acknowledge the lack of research <b>transfer</b> via the <b>standardization</b> channel, generally referred to as the research-to-standardization gap. This chapter analyzes the root causes for this gap and proposes way forward. In particular research-focused standardization is considered as the instrument to address this issue. This chapter shows that pre-standardization should be supplemented by a methodology and its associated process aiming to systematically analyze the standardization aspects of research projects and by helping them out to draw their standardization strategy. ...|$|R
40|$|A {{systems study}} is {{presented}} for a transportation system which {{will follow the}} interim upper stage and spinning solid upper stage. Included are concepts, concept comparisons, trends, parametric data, etc. associated with the future system. Relevant technical and programmatic information is developed. This information is intended to focus future activity to identify attractive options and to summarize the major issues associated with the future development of the system. To establish a common basis for identifying current transportation concepts, an orbit transfer vehicle (OTV) {{is defined as a}} propulsive (velocity producing) rocket or stage. When used with a crew transfer module, a manned sortie module or other payloads, the combination becomes an orbit <b>transfer</b> system (OTS). <b>Standardization</b> of OTV's and OTS's is required...|$|R
40|$|The {{position}} of transfer air bubbles after embryo transfer {{is related to}} the pregnancy rate. With the conventional manual embryo-transfer technique {{it is not possible to}} predict the final {{position of}} the air bubbles. This position mainly depends on the catheter load speed at transfer (injection speed), a parameter that remains uncontrollable with the conventional technique even after standardization of the protocol. Therefore, the development of an automated device that generates a standardized injection speed is desirable. This study aimed to examine the variation in injection speeds in manual embryo transfer and pump-regulated embryo transfer (PRET). Seven laboratory technicians were asked to perform simulated transfers using the conventional embryo-transfer technique. Their injection speeds were compared with that of a PRET device. The results indicate that in manually performed <b>transfers,</b> even after <b>standardization</b> of the protocol, there is still a large variation in injection speed, while a PRET device generates a reliable and reproducible injection speed and therefore brings new possibilities for further standardization of the embryo-transfer procedure. Future research should reveal whether these experiments mimic real clinical circumstances and if a standardized injection speed results in more exact positioning of the transferred embryos and therefore higher pregnancy rates. The position of transfer air bubbles after embryo transfer {{is related to the}} pregnancy rate. With the currently used embryo-transfer technique, in which embryos are transferred manually with a syringe, we are not able to predict the final position of the air bubbles. This position depends on the injection speed of the syringe, which remains uncontrollable in embryo transfers that are performed manually even after standardization of the protocol. Therefore we developed an automated device that generates a reliable and reproducible injection speed, a pump-regulated embryo transfer (PRET) device. This study aimed to examine the variation in injection speeds in transfers performed manually and with the PRET device. Our results indicate that in manually performed <b>transfers,</b> even after <b>standardization</b> by protocol, there is still a large variation in injection speed and that the PRET device generates a reliable and reproducible injection speed and therefore brings new possibilities for further standardization of the embryo-transfer procedure. Future research should reveal whether our experiments mimic real clinical circumstances and if a standardized injection speed results in more exact positioning of the transferred embryos and therefore higher pregnancy rates. © 2011, Reproductive Healthcare Ltd. Published by Elsevier Ltd. All rights reserved...|$|R
40|$|A current {{trend in}} modern {{near-infrared}} spectroscopy is {{the incorporation of}} sophisticated mathematical algorithms into the computer instrumentation used to extract information from raw spectral data by applying complex multivariate models. To {{address some of the}} problems that near-infrared spectroscopy faces, the GrainNet software model that connects a MATLABRTM computing and development environment, NIR spectrometers, and MS Server data-storage for spectral data and calibration models, was developed.;GrainNet is a client-server based Internet enabled communication and analyzing model for Near-Infrared (NIR) instruments. FOSS Infratec, Perten, and Bruins Instruments are currently three brands of the NIR instruments that have been included in the project. The performance of the implemented calibration models was evaluated. Three calibration models are implemented in the GrainNet: (1) Partial Least Squares Regression; (2) Artificial Neural Network; (3) Locally Weighted Regression.;The Piecewise Direct Standardization (PDS), Direct Standardization (DS), Finite Impulse Response (FIR) and Multiplicative Scatter Corrections (MSC) models were developed in the MATLABRTM environment and tested for <b>standardization</b> <b>transfer</b> of the Bruins Instruments and Foss Infratec grain analyzers. A new calibration model for corn that uses feed-forward back-propagation neural networks with wavelets signal decomposition used as an input was developed...|$|R
40|$|This {{dissertation}} {{is motivated}} by two problems. First, existing literature characterizes patient handoff as an information transfer activity in which safety and quality are compromised by practice variation. This has prompted a movement to standardize practice. However, existing research has not closely examined how practice variations may be responses to situational and organizational factors or evidence of involved parties accomplishing important functions beyond information <b>transfer.</b> Consequently, <b>standardization</b> efforts run at least two risks: overlooking opportunities for improvement, and engendering negative unintended consequences. Second, {{despite the fact that}} roughly 50 % of all hospitalized patients are handed off from emergency departments to inpatient units, such handoffs are significantly understudied. I conducted a two-year ethnographic study of handoffs occurring between Emergency Department and General Medicine physicians when patients were admitted to one highly-specialized tertiary referral, teaching hospital. Using theoretical sampling informed by a Grounded Theory methodology, I conducted observations (n= 349 hours) and semi-structured interviews (n= 48) and recorded handoff conversations (n= 48). I analyzed data by means of immersion, various qualitative coding approaches, and memo writing. Findings are organized in three chapters. First, I challenge the dominant model of handoff as information transfer by demonstrating that physicians actively construct understandings of their patients, over time, as they encounter, interpret, assemble, and reassemble information through socially-interactive processes within particular contexts and situations. Consequently, multiple understandings of a single patient are not only possible but likely. Second, I characterize admission handoffs as negotiations, situated by entangled webs of motives and concerns which produce ambiguities. Involved parties must navigate these ambiguities as they develop their differing understandings of patients, resolve conflicts over approaches to care, and agree regarding additional work. Third, I show that boundaries between units are ongoing, effortful accomplishments, re-enacted through interactive negotiations. Over time these negotiations have the potential to shift boundaries and alter the divisions of labor in the hospital, with potential consequences for organizational outcomes. Recommendations for practical improvements and further research are presented...|$|R
40|$|Shifts {{in working}} {{temperature}} {{are an important}} issue that prevents the successful transfer of calibration models from one chemical instrument to another. This effect is of special relevance when working with gas sensor arrays modulated in temperature. In this paper, we study the use of multivariate techniques to transfer the calibration model from a temperature modulated gas sensor array to another when a global change of temperature occurs. To do so, we built 12 identical master sensor arrays composed of three different types of commercial Figaro sensors and acquired a dataset of sensor responses to three pure substances (ethanol, acetone and butanone) dosed at 7 concentrations. The master arrays are then shifted in temperature (from − 50 to 50 °C, ΔT = 10 °C) and considered as slave arrays. Data correction is performed for {{an increasing number of}} transfer samples with 4 different calibration <b>transfer</b> techniques: Direct <b>Standardization,</b> Piece-wise Direct Standardization, Orthogonal Signal Correction and Generalized Least Squares Weighting. In order to evaluate the performance of the calibration transfer, we compare the Root Mean Square Error of Prediction (RMSEP) of master and slave arrays, for each instrument correction. Best results are obtained from Piece-wise Direct standardization, which exhibits the lower RMSEP values after correction for the smaller number of transfer samples...|$|R
40|$|As a {{security}} researcher, {{my goal is}} to develop practical solutions that make computer networks more reliable, predictable, and resilient to attack. To address the grand challenges in security research, I believe that we need to go beyond traditional disciplinary barriers. For instance, cryptography is crucial to deal with hacked devices and malicious parties, but cannot {{deal with the fact that}} nodes in computer networks are often owned by entities with competing economic goals. Game theory is an emerging approach that can be used to reason about the way systems behave in the presence of selfish (rational) behavior, but often requires making strong assumptions on the economic motivations of parties in the network. At the end of the day, solutions must be implemented in extremely resource-constrained network devices, so hardware and software engineering concerns are paramount. Finally, ensuring that security research makes a real impact requires an investment in technology <b>transfer</b> activities, including <b>standardization,</b> prototyping or collaboration with industry. Indeed, my research spans all of these areas. 1. RESEARCH OVERVIEW When we purchase an item from Amazon. com, traditional cryptography prevents attackers from seeing our credit card numbers or impersonating the Amazon website. But how can we ensure that our request actually arrives at the Amazon. com server, without being dropped or corrupted along the way...|$|R
40|$|The {{advent of}} a credit risk market has profoundly altered {{the role of}} banking firms into one of asset {{originator}} and asset distributor rather than the asset holder. Banks have traditionally originated and held credit risk. It emphasis the different role of financial institutions from holders of credit risk to originators and distributors of credit risk. In this paper I aim to evaluate how the modularity and the standardization create the precondition {{for the creation of}} a credit risk transfer market in the banking industry. The intermediate market of credit risk transfer appears when the banking production processes have become more disintegrated. The vertical disintegration of the banking industry and the creation of a credit risk transfer market enables the shifting from a firm-based governance to a market-based governance. Furthermore, this paper proposes that modularity and standardization drives the creation and growth of credit risk transfer market. With the improvement of credit risk measurement methodologies and risk management practices the credit risk <b>transfer</b> market requires <b>standardization</b> and modularisation of bank lending value chain. Transaction cost economics, the dominant paradigm for understanding make or buy decisions, represents the starting point of my research. I argue that vertical integration in lending business is not only determined by transaction costs, but also by standards and modularity at product, process and industry level. I illustrate this thesis by examining how they work in mutually reinforcing ways. This perspective could open up some unexplored paths for research into economics of banking firms...|$|R
40|$|Defense Date: 10 / 05 / 2010 Examining Board: Professor Hanns Ullrich, EUI (Supervisor) Professor Steven Anderman, University of Essex Professor Gustavo Ghidini, Luiss Guido Carli University Professor Hans-W. Micklitz, EUIGreat {{prosperity}} {{is derived}} from innovation, which in turn prospers in an environment with a large public domain of free knowledge, property rights and unfettered competition. Generally, this was the basic theory for prosperity under the antitrust laws with reference to joint R&D, technology <b>transfer</b> and technology <b>standardization</b> in the US and Europe for many years. This perspective was slowly abandoned in the 1980 s and 1990 s, replaced by {{a belief that the}} greatest wealth was derived from innovators having large resources to perform R&D, the ability to cooperate with competitors and the possibility of jointly protect and exploit newly discovered knowledge through intellectual property rights, technology standardization agreements and joint licensing schemes. The antitrust policies {{on both sides of the}} Atlantic have closely and swiftly been adapted to mirror this change of theory. The thesis illustrates this transformation by analyzing the modifications and amendments made to legal acts and guidelines, and the slow shift in the scant case-law detected both under the antitrust laws of the USA and the Competition Rules of the EU. The thesis shows that the prevailing antitrust policies towards R&D collaborations, technology standardization agreements and patent pools are very similar in the US and EU and they both mirror a lenient or even supportive attitude towards collaboration between competitors in reference to creating innovation...|$|R
40|$|AbstractMany {{companies}} {{have attempted to}} introduce Lean practices in product development to gain competitive advantage in today's global market place. However, the application of Lean outside the factory floor is not straightforward—especially when it is introduced in functional areas that differ significantly from manufacturing, {{such as in the}} multifaceted context of System Engineering (SE). In this article, we investigate the extent to which SE companies are engaging in Lean product development, {{and the degree to which}} various lean practices and capabilities are implemented. The overall goal is to determine how SE companies compare to companies in other industrial sectors, and thereby gaining new insights into strategies for more contextual implementation of lean in engineering functions. An extensive literature review is conducted to synthesize prior research with regard to the principal components of Lean when applied in product development. The extract from the literature study is combined with the author's industrial experience and hypothetical reasoning to build a framework consisting of six principal Lean components, including Customer Value, Knowledge <b>Transfer,</b> Continuous Improvement, <b>Standardization,</b> Stabilization and Culture. Each of these is divided into a set of practice and capability characteristics collectively constituting the component as a whole, forming the basis for a survey for lean maturity. A survey is conducted in the Norwegian manufacturing industry to determine Lean practices from the construct of the generic model as basis. The survey was answered by 297 respondents from 56 companies, providing the opinion of individuals as to where they place their current practices and capabilities on alean maturity scale for each question, including a supplemental set of performance and productivity related assessment items. Results indicate that there seem to be explanations rooted in marked and project characteristics, for the significant differences between perceived Lean performance in Systems Engineering versus the other sectors, especially Automotive, when talconsidering Customer Value and Project Performance. In between these categories this study reveals a potential for the Systems Engineering industry to improve upon the way organizational learning is managed to develop and sustain a culture for continuous improvement. Findings and discussions underline that Lean has a stronger foothold, based on history and market conditions, in the Automotive industry, as to how organizations effectively execute development projects...|$|R

