291|2245|Public
50|$|The tests {{demonstrate}} compliance {{with either the}} WITS PSA <b>Test</b> <b>Specification,</b> or the vendors own <b>test</b> <b>specification.</b>|$|E
50|$|The <b>test</b> <b>specification</b> is software. <b>Test</b> <b>specification</b> is {{sometimes}} referred to as test sequence, which consists of test steps.|$|E
50|$|TestLink {{breaks down}} the <b>Test</b> <b>Specification</b> {{structure}} into Test Suites and Test Cases. These levels are persisted throughout the application. One Test Project has just one <b>Test</b> <b>Specification.</b>|$|E
40|$|This {{research}} {{examined the}} comparability {{of three different}} statistical models and a Holistic Judgmental Model to develop <b>test</b> <b>specifications</b> for a high-stakes credentialing examination with four unique sections. Results from the credentialing body 2 ̆ 7 s recent practice analysis were {{used as the basis}} to develop <b>test</b> <b>specifications</b> for a new credentialing examination. The credentialing body 2 ̆ 7 s practice analysis survey was developed using ratings scales of importance and frequency. Using statistical models, data analyses were performed to evaluate various models of combining ratings scales of importance and frequency to generate <b>test</b> <b>specifications.</b> The statistical models used include the Additive Model, Multiplicative Model, and Hierarchical Ranking Scheme (Raymond 2 ̆ 6 Neustel, 2006). Holistic Judgmental Model <b>test</b> <b>specifications</b> were developed with input from subject-matter experts. ^ The results {{of this study indicate that}} <b>test</b> <b>specifications</b> developed using the Holistic Judgmental Model yield <b>test</b> <b>specifications</b> different from those generated using statistical models for three of four examination sections. For the fourth examination section, minor differences were noted between <b>test</b> <b>specifications</b> generated using statistical models and those developed using the Holistic Judgmental Model. Compelling similarities were found among <b>test</b> <b>specifications</b> generated by the three statistical models. In addition, no individual statistical method generated <b>test</b> <b>specifications</b> that were more comparable than another statistical model to the Holistic Judgmental Model. ^ Based on results of the research, alternate <b>test</b> <b>specifications</b> could be selected for three of four examination sections if one or more statistical models had been chosen as the basis for development of <b>test</b> <b>specifications.</b> In addition, this research demonstrates that there should be a proportional distribution between the number of practice analysis survey statements in content areas, and the importance of these content areas identified by practitioners in the field in which the practice analysis is being conducted. ...|$|R
40|$|Testing {{equipment}} from different vendors utilise different syntax for their <b>test</b> <b>specifications,</b> but writing specifications {{for the same}} tests on different platforms is labour-consuming and error-prone. We present a prototype system for the automatic translation of <b>test</b> <b>specifications</b> between platform-specific languages, the framework of which is reusable for similar scenarios or software projects. We first design a generic representation scheme for <b>test</b> <b>specifications.</b> To perform a translation from a source language S to a target language T, a context free language parser module reverse-engineers <b>test</b> <b>specifications</b> in language S to a platform-independent equivalent in the generic syntax, marked up with XML. An XML/XSLT-driven generator module then transforms the platform-independent <b>test</b> <b>specifications</b> to language T syntax. In addition, we encapsulate knowledge about the tester domain in an ontology to facilitate the translation process. We also found ontologies to be helpful tools for requirements gathering, analysis and design...|$|R
30|$|Sims and Liu (2013) {{provided}} {{support for}} the content validity and concurrent validity of the EPE. Following Hughes’ (2003) recommendations for a content validity study, they reported that teachers who were trained in language teaching and testing, but who were not {{directly involved in the}} production of the EPE, compared <b>test</b> <b>specifications</b> and <b>test</b> content (see Sims 2006 for <b>test</b> <b>specifications).</b> These teachers concluded that the content of EPE was a valid measure of the desired <b>test</b> <b>specifications</b> for grammar, reading, and listening.|$|R
50|$|The <b>test</b> <b>specification</b> {{should be}} stored in the test {{repository}} in a text format (such as source code).Test data is sometimes generated by some test data generator tool.Test data can {{be stored in}} binary or text files.Test data should also be stored in the test repository together with the <b>test</b> <b>specification.</b>|$|E
50|$|Testing has {{confirmed}} {{compliance with the}} WITS PSA <b>Test</b> <b>Specification.</b>|$|E
5000|$|ETSI TS 102 230 - Physical, {{electrical}} and logical <b>test</b> <b>specification</b> ...|$|E
5000|$|To {{ensure that}} actual {{implementations}} of such standardised features are interoperable, the standardisation bodies also create so called <b>test</b> <b>specifications.</b> These document detail exact procedures {{on how to}} test that an implementation under test acts according to conformance requirements. Important <b>test</b> <b>specifications</b> for the (U)SIM interface are: ...|$|R
40|$|This Functional Design Criteria is {{designed}} to summarize and give guidance during the development of design, manufacturing and <b>testing</b> <b>specification</b> documents. As the overview document bounding parameters are specified with detailed acceptance criteria to be developed in the more detailed and separate design, manufacturing and <b>testing</b> <b>specification</b> documents...|$|R
40|$|Abstract: Quality {{models are}} needed to {{evaluate}} and set goals {{for the quality of}} a software product. The international ISO/IEC standard 9126 defines a general quality model for software products. Software is developed in different domains and the usage of the ISO/IEC quality model requires an instantiation for each concrete domain. One special domain is the development and maintenance of <b>test</b> <b>specifications.</b> Test <b>specifications</b> for <b>testing,</b> e. g. the Internet Protocol version 6 (IPv 6) or the Session Initiation Protocol (SIP), reach sizes of more than 40. 000 lines of test code. Such large <b>test</b> <b>specifications</b> require strict quality assurance. In this paper, we present an adaptation of the ISO/IEC 9126 quality model to <b>test</b> <b>specifications</b> and show its instantiation for <b>test</b> <b>specifications</b> written in the Testing and Test Control Notation (TTCN- 3). Example measurements of the standardised SIP test suite demonstrate the applicability of our approach. ...|$|R
5000|$|... 3GPP TS 31.121 - Universal Subscriber Identity Module (USIM) {{application}} <b>test</b> <b>specification</b> ...|$|E
5000|$|Gibson, J.B. [...] "NP-MHTGR <b>Test</b> <b>Specification</b> for CCT-1 Irradiation Capsule", CEGA-002371 (Nov 1992).|$|E
5000|$|Gibson, J.B. [...] "NP-MHTGR <b>Test</b> <b>Specification</b> for NPR-1B Fuel Irradiation Capsule", CEGA-002210 (1992).|$|E
5000|$|Test {{classes are}} also called test {{objectives}} , test templates [...] and <b>test</b> <b>specifications.</b>|$|R
5000|$|Twist allows <b>test</b> <b>specifications</b> to {{be written}} in English or any UTF-8 {{supported}} language.|$|R
50|$|MTP Level 2 {{is tested}} using the {{protocol}} tester and <b>test</b> <b>specifications</b> described inQ.755,Q.755.1,Q.780 andQ.781.|$|R
5000|$|Part 3 - Conformity <b>Test</b> <b>Specification</b> (currently at {{revision}} 1, released 16 Oct 2006) ...|$|E
50|$|Universal <b>Test</b> <b>Specification</b> Language (UTSL) is a {{programming}} language used to describe ASIC tests in a format that leads to an automated translation of the <b>test</b> <b>specification</b> into an executable test code. UTSL is platform independent and provided a code generation interface for a specific platform is available, UTSL code can be translated into the {{programming language}} of a specific Automatic Test Equipment (ATE).|$|E
50|$|Gibson, J.B., MHTGR-NPR <b>Test</b> <b>Specification</b> for Post-Irradiation Heating of HRB-17/18 Triso-Coated HEU UCO Fuel Particles", CEGA-000337 (April 1991).|$|E
5000|$|Complete the {{ecosystem}} further with requirement and <b>test</b> <b>specifications</b> for harnesses, switches, ECUs, and additional functionalities.|$|R
40|$|The {{automated}} assessment of authentic Information Technology skills relies on testing candidate's output for conformity with the operations {{which should have}} been performed during the test. Since a test is unique, <b>test</b> <b>specifications</b> are unique to each test. This paper describes transformations which {{can be applied to}} the instructions given to the candidate to generate the <b>test</b> <b>specifications</b> which drive automated IT skills assessors...|$|R
50|$|QUnit's {{assertion}} methods {{follow the}} CommonJS unit <b>testing</b> <b>specification,</b> which itself was influenced {{to some degree}} by QUnit.|$|R
5000|$|Gibson, J.B. and C.Y. Young [...] "NP-MHTGR <b>Test</b> <b>Specification</b> for Post-Irradiation Examination of Fuel Irradiation Capsules NPR-1", CEGA-002031 (April 1992).|$|E
5000|$|ETSI TS 102 841 V1.5.1 (2014-01). Extended {{wideband}} speech services - Profile <b>Test</b> <b>Specification</b> (PTS) and Test Case Library (TCL) ...|$|E
5000|$|A test {{execution}} engine by executing a <b>test</b> <b>specification,</b> it may perform {{different types of}} operations on the product, such as: ...|$|E
5000|$|TC3 also defines 1000BASE-T1 {{magnetics}} {{characteristics and}} CMC limit lines for differential and mixed mode parameters, resulting in CMC performance and <b>test</b> <b>specifications.</b>|$|R
40|$|The <b>test</b> <b>specifications</b> for {{spacecraft}} {{and components}} permit test levels {{to be reduced}} under a procedure {{commonly referred to as}} notching. A summary is provided of the steps required to comply with the notching criterion. Primary structural loads are discussed along with aspect of mathematical modelling of the spacecraft structure, a launch loads analysis, a vibration test loads analysis, and notching methods. It is shown that current, general vibration <b>test</b> <b>specifications</b> are capable of producing unrealistically high spacecraft structural loads...|$|R
40|$|A {{model for}} {{software}} component certification based on test certificates supplied by developers {{in a standard}} portable form is proposed. Purchasers are expected to use the model to determine the quality and suitability of purchased software. The model involves the designing of standard <b>test</b> <b>specifications</b> that are devoid of language specific features. The tests described by the <b>test</b> <b>specifications</b> are run by Java programming. The advantages of the approach over the certification laboratory approach includes reduced costs, added value, augmented functional requirements and guaranteed trust...|$|R
5000|$|ETSI TS 102 843 V1.1.1 (2014-01). Additional feature set nr.1 for {{extended}} wideband speech services; Profile <b>Test</b> <b>Specification</b> (PTS) and Test Case Library (TCL) ...|$|E
5000|$|ETSI TS 103 158 V1.1.1 (2014-11). Light Data Services - Software Update Over The Air (SUOTA) - Profile <b>Test</b> <b>Specification</b> (PTS) and Test Case Library (TCL) ...|$|E
50|$|The test {{execution}} engine does not carry {{any information about}} the tested product. Only the <b>test</b> <b>specification</b> and the test data carries information about the tested product.|$|E
40|$|Abstract: Today, {{development}} and implementation of automotive embedded sys-tems is based on a model-based process. A major problem of respective model-based toolchains {{can be traced back to}} inadequate testing tools, which often are not standard-ized, incomplete and cumbersome in usage. Due to these shortcomings, test specifi-cations are neither exchangeable nor reuseable. In this paper, we propose µTTCN for <b>test</b> <b>specifications</b> of continuous embedded systems. µTTCN targets at exchangeable and reuseable <b>test</b> <b>specifications</b> based upon TTCN- 3, as well as convenient handling of testcase implementations. ...|$|R
40|$|Abstract: In the {{standardization}} of <b>test</b> <b>specifications,</b> {{it is common}} that no actual systems exist against which the tests can be executed. Test specifications are developed abstractly in high level languages such as the Testing and Test Control Notation (TTCN- 3), but they can only be executed when aseparate adaptation layer is implemented. Static syntactical and semantical analyses as provided by the compiler and proper manual code reviews are the only means to find mistakes in such <b>test</b> <b>specifications</b> at early stages of design. In this paper, wedemonstrate {{that it is possible}} to execute abstract <b>test</b> <b>specifications</b> when the system does not exist yet. We use the information provided within the test cases to simulate answers of the system by generating inverse messages to expected messages in the abstract test case. By following a specific coverage-criterion strategy, weare able to execute asufficient amount of test paths to reverse-engineer behavioral models of test cases which can then again be used for the analyses of potential problems. ...|$|R
40|$|We {{extend the}} category-partition method, a specification-based method for testing software. Previous work in category-partition {{has focused on}} {{developing}} structured <b>test</b> <b>specifications</b> that describe software tests. We offer guidance in making the important decisions involved in transforming <b>test</b> <b>specifications</b> to actual <b>test</b> cases. We present a structured approach to making those decisions, including a mechanical procedure for test derivation. With this procedure, we suggest a heuristic for choosing a minimal coverage of the categories identified in the <b>test</b> <b>specifications,</b> suggest parts of the process that can be automated, and offer {{a solution to the}} problem of identifying infeasible combinations of test case values. Our method uses formal schema-based functional specifications and is illustrated with an example of a simple file system. We conclude that our approach eases test case creation and leads to effective tests of software. We also note that by basing this procedure on formal specifications, we can identify anomalies in the functional specifications...|$|R
