38|152|Public
2500|$|Boot camp is a twelve-week {{cycle of}} training, {{beginning}} with a receiving phase of in-processing or [...] "forming", followed by three numbered phases. Each phase includes intensive education and training on various topics essential for military life. Each phase consists of a predetermined number of training days, these are counted in the <b>training</b> <b>matrix</b> as [...] "T1", [...] "T2", up to [...] "T70" [...] which is Graduation Day.|$|E
5000|$|Step 1: Determine what {{training}} {{your organization}} need or prepare OSH <b>Training</b> <b>matrix</b> ...|$|E
5000|$|Let the {{training}} sequence be denoted , where the vector [...] is transmitted over the channel asBy combining the received training signals [...] for , the total training signalling becomeswith the <b>training</b> <b>matrix</b> [...] {{and the noise}} matrix [...]|$|E
30|$|Before proving Theorems 2 and 3, a lemma will {{be given}} that characterizes equivalences between {{different}} sets of feasible <b>training</b> <b>matrices</b> P.|$|R
3000|$|Output of {{the network}} is a {{hypothesis}} value h_W(x) = f(W_ 1 ^T f(W_ 2 ^T x)) estimating the segmentation error. It is calculated using (<b>trained)</b> <b>matrices</b> W [...]...|$|R
3000|$|Note that {{permutation}} ambiguity {{does not}} exist due to {{the knowledge of the}} <b>training</b> <b>matrices</b> E and F. The relation (29) can be obtained by replacing the alternative solutions [...]...|$|R
50|$|Prepare {{training}} plan by prioritising the training modules {{and develop a}} training schedule for the organization. It is recommended that the legislation and hazard management training modules are delivered first. The other modules listed within the OSH <b>Training</b> <b>matrix</b> can be delivered in an order appropriate to the worker‟s need.|$|E
50|$|The OSH <b>Training</b> <b>matrix</b> {{is a tool}} {{which can}} be use to assist {{employers}} and supervisors to identify what OSH training are required for their workers. The matrix aims to compile a list of OSH training modules necessary to equip workers with the skills, knowledge and information to effectively manage hazard exposures during their working day.|$|E
5000|$|Boot camp is a twelve-week {{cycle of}} training, {{beginning}} with a receiving phase of in-processing or [...] "forming", followed by three numbered phases. Each phase includes intensive education and training on various topics essential for military life. Each phase consists of a predetermined number of training days, these are counted in the <b>training</b> <b>matrix</b> as [...] "T1", [...] "T2", up to [...] "T70" [...] which is Graduation Day.|$|E
40|$|Abstract. A {{robust and}} {{efficient}} time integrator for dynamical tensor approximation in the tensor <b>train</b> or <b>matrix</b> product state format is presented. The method {{is based on}} splitting the projector onto the tangent space of the tensor manifold. The algorithm {{can be used for}} updating time-dependent tensors in the given data-sparse tensor <b>train</b> / <b>matrix</b> product state format and for computing an approximate solution to high-dimensional tensor differential equations within this data-sparse format. The formulation, implementation and theoretical properties of the proposed integrator are studied, and numerical experiments with problems from quantum molecular dynamics and with iterative processes in the tensor train format are included. Key words. Tensor <b>train,</b> <b>matrix</b> product state, low-rank approximation, time-varying tensors, tensor differential equations, splitting integrator. AMS subject classifications. 15 A 18, 15 A 69, 65 F 99, 65 L 05 1. Introduction. I...|$|R
40|$|Additional file 7 Title: Trainer Description: Calculating {{likelihood}}s from PFAM <b>training</b> <b>matrices</b> Additional file 8 Title: Predicter Description: Predicting {{the classes}} of a genome's PFAM domain profile, {{based on the}} likelihood file created by the trainer Additional file 9 Title: Evaluator Description: Evaluates the predictive performance of the predicter, calculated as Matthew's Correlation Coefficien...|$|R
30|$|Generally, the ALS {{algorithm}} {{is sensitive to}} the initialization, and convergence to the global minimum can be slow when all the matrix factors of the model are unknown [25]. However, in our case, we have observed that convergence to the global minimum is always achieved (e.g., within 10 to 30 iterations for medium-to-high SNRs) due to {{the knowledge of the}} AF <b>training</b> <b>matrices</b> E and F.|$|R
50|$|Employers must {{document}} all training. Creating a <b>training</b> <b>matrix</b> {{will help}} keep track of who has been trained, when they were trained, the training topic, {{and when it is}} time for refresher training. Employees must also sign an official sign-in sheet provided by the employer that can serve as proof that employees received proper training. The sign in sheet must have a broad description of what is being covered in the training. Tests or quizzes on the presented material can help gauge employee understanding of the material and highlight topics that need to be reviewed.|$|E
5000|$|If {{the channel}} and noise {{distributions}} are unknown, then the least-square estimator (also {{known as the}} minimum-variance unbiased estimator) iswhere [...] denotes the conjugate transpose. The estimation Mean Square Error (MSE) is proportional towhere [...] denotes the trace. The error is minimized when [...] is a scaled identity matrix. This can only be achieved when [...] is equal to (or larger than) the number of transmit antennas. The simplest example of an optimal <b>training</b> <b>matrix</b> is to select [...] as a (scaled) identity matrix {{of the same size}} that the number of transmit antennas.|$|E
50|$|Students and {{residents}} of the monastery practice according to Daido Roshi's Eight Gates of Zen <b>training</b> <b>matrix.</b> These gates consist of zazen, face-to-face teaching, liturgy, moral and ethical teachings, work practice, body practice, art practice and academic study. Their practice occurs either at home for lay students or at the monastery during weekend retreats and monthly week-long sesshin (meditation intensives). The monastery's schedule includes a Sunday morning program open {{to the general public}} and a variety of weekend and week-long Zen training programs, focusing variously on painting, poetry, shakuhachi performance, Zen archery (kyūdō), qigong, and many other activities.|$|E
30|$|RFR is an {{ensemble}} method that utilizes decision trees as its primary estimators. This ensemble method sub-samples the presented <b>training</b> feature <b>matrix</b> into equal partitions, {{and for each}} partition, it fits an estimator. Given a test sample, each of the trained models, is used to produce some target estimates. Subsequently, all target estimates are averaged to produce the final target estimate. One essential tuning parameter for RFR {{is the number of}} estimators it utilizes to fit the <b>training</b> feature <b>matrix.</b> In this paper, we have utilized ten estimators to fit the <b>training</b> feature <b>matrix.</b> This is the same number of estimators specified in Pedregosa et al. (2011 a). This number of estimators is assumed to avoid over-fitting issues and to achieve model generalization.|$|R
40|$|<b>Matrix</b> <b>training</b> {{techniques}} arrange {{instruction for}} stimulus relations that facilitate emergent responding to novel stimulus arrangements, {{which is a}} phenomenon known as recombinative generalization. The current study compared two common <b>matrix</b> <b>training</b> approaches, an overlapping (OV) design and a non-overlapping (NOV) design, with respect to arranging relations targeted for training. Two, typically-developing toddlers were taught compound action-object labels in either an OV or NOV <b>matrix</b> <b>training</b> design. Results suggest that an OV matrix design facilitates recombinative generalization more effectively than a NOV design...|$|R
40|$|Abstract In H. 264 /AVC, 4 × 4 {{discrete}} cosine transform (DCT) {{is performed}} on the residual signals after intra prediction for decorrelation. Actually, residual blocks with different prediction modes exhibit different frequency characteristics. Therefore, the fixed transform matrix cannot match the energetic distribution of residual signals very well, which degrades the decorrelation performance. Fortunately, the energetic distributions of residual blocks with the same mode are relatively coincident, which {{makes it possible to}} train a universally good Karhunen–Loève transform (KLT) matrix for each mode. In this paper, an optimal frequency matching (OFM) algorithm is proposed to <b>train</b> KLT <b>matrices</b> for residual blocks and nine KLT matrices corresponding to nine prediction modes of 4 × 4 intra blocks are trained. Experimental results show that KLT with <b>trained</b> <b>matrices</b> yields a persistent gain over H. 264 using 4 × 4 DCT with an average peak signal-to-noise ratio (PSNR) enhancement of 0. 22 dB and a maximum enhancement of 0. 33 dB...|$|R
5000|$|The MMSE {{estimator}} is the Bayesian {{counterpart to}} the least-square estimator and becomeswhere [...] denotes the Kronecker product and the identity matrix [...] has {{the dimension of}} the number of receive antennas. The estimation Mean Square Error (MSE) isand is minimized by a <b>training</b> <b>matrix</b> [...] that in general can only be derived through numerical optimization. But there exist heuristic solutions with good performance based on waterfilling. As opposed to least-square estimation, the estimation error for spatially correlated channels can be minimized even if [...] is smaller than the number of transmit antennas. Thus, MMSE estimation can both decrease the estimation error and shorten the required training sequence. It needs however additionally the knowledge of the channel correlation matrix [...] and noise correlation matrix [...] In absence of an accurate knowledge of these correlation matrices, robust choices need to be made to avoid MSE degradation.|$|E
3000|$|As it {{has been}} shown in[1], the optimal <b>training</b> <b>matrix</b> that {{minimizes}} the MSE under a constant training energy [...]...|$|E
30|$|The {{mathematical}} {{difference between}} ADGPP and ASGPP is the R- 1 term {{that appears in}} the constraint of the latter. This term has a clear impact {{on the structure of}} the optimal ASGPP <b>training</b> <b>matrix.</b>|$|E
30|$|Algorithm   1 uses SFA. SFA {{can fully}} exploit the {{relationship}} between DI and DS tags via co-aligning them on the bipartite graph to learn a more compact and meaningful representation of space. So our method uses the automatically created lexicons to expand feature sets in a model learned (U <b>matrices)</b> at <b>train</b> time by introducing related workflow motifs. For this, different domains of workflows are co-aligned. Finally a unique DNN based feature set {{is embedded in the}} <b>training</b> <b>matrices</b> of workflow models for further analysis. Operations of the algorithm are applied for a given complex workflow that should be reconciled.|$|R
40|$|We {{propose a}} new method to {{incorporate}} priors on {{the solution of}} nonnega-tive matrix factorization (NMF). The NMF solution is guided to follow the {{minimum mean square error}} (MMSE) estimates of the weight combinations under a Gaussian mixture model (GMM) prior. The proposed algorithm can be used for denoising or single-channel source separation (SCSS) appli-cations. NMF is used in SCSS in two main stages, the training stage and the separation stage. In the training stage, NMF is used to decompose the training data spectrogram for each source into a multiplication of a trained basis and gains matrices. In the separation stage, the mixed signal spectro-gram is decomposed as a weighted linear combination of the <b>trained</b> basis <b>matrices</b> for the source signals. In this work, to improve the separation per-formance of NMF, the <b>trained</b> gains <b>matrices</b> are used to guide the solution of the NMF weights during the separation stage. The <b>trained</b> gains <b>matrix</b> is used to train a prior GMM that captures the statistics of the valid weigh...|$|R
30|$|The {{technique}} applied here is {{to optimize}} the filter based on the covariance matrix of clutter and noise. The target statistics and waveform (orthogonal waveform initially) are also considered. The covariance <b>matrix</b> is a <b>trained</b> <b>matrix</b> of clutter statistics for K secondary data. Here, the clutter information is estimated by the received signals before the target appears. The covariance matrix of filter and clutter statistics are estimated. Using this covariance matrix, the signal covariance matrix is estimated from target, noise, clutter and filter covariance matrix. Then this waveform covariance matrix is normalized and transmitted by NxR MIMO radar system. Thus, obtained waveform is orthogonal optimal waveform.|$|R
30|$|Step 3. From {{these data}} blocks, we form a <b>training</b> <b>matrix</b> with the {{dimensions}} of Q[*]×[*]V, where Q is {{the length of the}} input vector of the network and V is the number of training vectors.|$|E
40|$|An {{approach}} {{to fulfill a}} need for additional hospital beds during emergency and disaster situations has been establishment of alternate care sites (ACS). Kern County, California, {{with a population of}} about 856, 000 and approximately 1, 500 hospital beds, is at significant risk for inability to care for the medical needs of its residents {{in the face of a}} disaster or an emergency. Even the most conservative estimates of additional hospital beds requirements in a disaster or emergency would indicate the need for nine to ten ACSs accommodating up to 50 patients each in Kern County. Recognizing the need for alternatives to manage hospital surge, the local public health department along with state and federal officials are collaborating with community partners to establish alternative sites where emergency medical facilities can be established to help deliver medical care in a catastrophe. Preparing community partners with tools to assist in establishment of ACSs is one goal in a long-term plan to assist in disaster and emergency response. While healthcare personnel will be needed in the activation and operation of an ACS, little work has been done to define what other personnel will be needed and what training would enhance staff performance of these roles. Development of an ACS staff <b>training</b> <b>matrix</b> tool was the aim of this project. Guidance exists for organization of healthcare staff during an emergency or disaster in the form of the Hospital Incident Command System (HICS). There also are numerous training modules available providing instruction in emergency and disaster response. However, no previous reference has matched the job positions defined by HICS with the training required to fulfill the requirements of the job descriptions for each of the job positions. The ACS Staff <b>Training</b> <b>Matrix</b> was designed to support emergency planners mustering appropriate staff for an ACS during a disaster or emergency. The ACS Staff <b>Training</b> <b>Matrix</b> was based on selection of job positions from HICS and coupling of those job positions with existing emergency and disaster training modules selected for availability, affordability, and relationship to the job descriptions for each position. Training capabilities selected for the ACS Staff <b>Training</b> <b>Matrix</b> were reviewed by subject matter experts to ensure the level of requirement (mandatory versus recommended). The ACS Staff <b>Training</b> <b>Matrix</b> is intended to provide guidance to emergency planners and ACS partners in training of staff who might be called upon to respond to ACS activation during a disaster or emergency. The ACS Staff <b>Training</b> <b>Matrix</b> also provides capacity to list persons qualified to fill those positions based on training completed, along with contact information for each individual. Job descriptions provided through links to HICS simplify pre-event and event planning and identification of training needs. When insufficient staff is identified as a need, the ACS Staff <b>Training</b> <b>Matrix</b> can be used to define and communicate specific requirements for completion of resource requests transmitted outside the operational area using terminology and nomenclature easily recognized by other jurisdictions. Training obtained through use of the ACS Staff <b>Training</b> <b>Matrix</b> also is applicable to a wide variety of Incident Command System (ICS) and HICS job positions, leading to flexibility of staff relative to needed job assignments in the “all-hazards” approach that is encouraged in emergency planning and training efforts...|$|E
40|$|Wireless {{communication}} requires accurate Channel State Information (CSI) for coherent detection. Due to the broadband signal transmission, dominant channel taps {{are often}} separated in large delay spread {{and thus are}} exhibited highly sparse distribution. Sparse Multi-Path Channel (SMPC) estimation using Orthogonal Matching Pursuit (OMP) algorithm has took advantage of simplification and fast implementation. However, its estimation performance suffers from large Mutual Incoherent Property (MIP) interference in dominant channel taps identification using Random <b>Training</b> <b>Matrix</b> (RTM), {{especially in the case}} of SMPC with a large delay spread or utilizing short training sequence. In this study, we propose a MIP mitigation method to improve sparse channel estimation performance. To improve the estimation performance, we utilize a designed Sensing <b>Training</b> <b>Matrix</b> (STM) to replace with RTM. Numerical experiments illustrate that the improved estimation method outperforms the conventional sparse channel methods which neglected the MIP interference in RTM...|$|E
3000|$|We ran our {{proposed}} algorithm on {{a desktop}} machine with Intel Core I 5 - 3380 2.90 GHz dual core CPU. The data {{is stored in}} SQL server database. There are many data structures to construct grid index, such as hash table, hash array, or matrix. We use hash array to index the grids. Each grid use 100 bits to store two values: facility IDs and the corresponding influence value. We train our trajectory-based Markov model with built-in toolkit [21]. We first separate every taxi trajectory into sub-trajectories which starts in one grid cell and ends in another. Then, we split the sub-trajectory database to two parts, one for <b>training</b> Markov transition <b>matrix</b> {{and the other for}} testing the effectiveness of our algorithm. We then compute and save the Markov transition matrix M using Eq. (4). It takes several hours to <b>train</b> <b>matrix</b> M using the massive trajectories. We then compute M [...]...|$|R
50|$|Public {{order is}} one of the main {{functions}} of the department and therefore all officers receive the required training and are subjected to rigorous <b>training</b> scenarios. <b>Matrix</b> has a number of baton gun trained specialist officers: two Sergeants and ten Constables.|$|R
30|$|Sentence {{productivity}} is a behavioral language process (Mackay, 2013; Skinner, 1957) that was demonstrated experimentally {{in the present}} study using <b>matrix</b> <b>training</b> (Goldstein, 1983 a, 1983 b; Goldstein et al., 1987; Goldstein & Mousetis, 1989; Mackay, 2013). Our participants had high performance in generalization probes after training. These findings converge with the literature that shows that <b>matrix</b> <b>training</b> can promote recombinative generalization within sentences for expressive and receptive repertories in different populations with normal hearing (Frampton et al., 2016; Goldstein et al., 1987; Yamamoto & Miya, 1999). These results replicate Golfeto and de’s Souza (2015) findings on recombinative generalization in children with CIs using a matrix and allows further discussion of the arrangement of <b>matrix</b> <b>training.</b>|$|R
30|$|Our model, problem formulation, {{and methods}} differ from {{existing}} {{ones in the}} following major aspects. First, we assume a random channel matrix where the channel entries follow Rayleigh flat-fading {{and the distribution of}} the channel coefficients is taken into account in the threshold optimization. On the contrary, in all existing work, the channels are assumed to be static and the distribution of the channel matrix is not used in the rank detection designs [14, 17 – 23]. Also, in our model, a general training length and unitary <b>training</b> <b>matrix</b> are considered, while most existing work (e.g., [17 – 23]) apply to identity <b>training</b> <b>matrix</b> only, where the training length equals the number of transmit antennas. Finally, in this work, we use the probability of correct rank detection as the performance measure and optimization objective, while existing work (except [23]) targeted at minimizing the MSE of the MIMO channel estimation [17 – 22].|$|E
40|$|In this paper, we {{introduce}} a fast {{implementation of the}} CT EXT algorithm for testor property identification, {{that is based on}} an accumulative binary tuple. The fast implementation of the CT EXT algorithm (one of the fastest algorithms reported), is designed to generate all the typical testors from a <b>training</b> <b>matrix,</b> requiring a reduced number of operations. Experimental results using this fast implementation and the comparison with other state-of-the-art algorithms that generate typical testors are presented...|$|E
40|$|The {{estimation}} of feature relevance {{in the framework}} of supervised classification problems has a great practical significance. Nevertheless, to solve this problem in real situations it is not always an easy task. One of the complex situations appears when the result of comparison between two coordinate values of the description of the respective objects is a real number. Goldman typical testors are useful for estimating feature relevance in supervised classification problems where feature values are compared using real criteria. Truly the computational complexity of all of these testor algorithms is too high. In real world problems very frequently occur modifications of the <b>training</b> <b>matrix.</b> Any modification to the <b>training</b> <b>matrix</b> can change the set of all Goldman typical testors, and this set must be computed again after each modification. In this paper we analyze how does the set of all Goldman typical testors of a comparison matrix change after modifications. An alternative method to calculate all Goldman typical testors of the modified matrix, more efficient than any traditional algorithm, is exposed. The new method's complexity is analyzed and some experimental results are showed. 1...|$|E
40|$|A {{robust and}} {{efficient}} time integrator for dynamical tensor approximation in the tensor <b>train</b> or <b>matrix</b> product state format is presented. The method {{is based on}} splitting the projector onto the tangent space of the tensor manifold. The algorithm {{can be used for}} updating time-dependent tensors in the given data-sparse tensor <b>train</b> / <b>matrix</b> product state format and for computing an approximate solution to high-dimensional tensor differential equations within this data-sparse format. The formulation, implementation and theoretical properties of the proposed integrator are studied, and numerical experiments with problems from quantum molecular dynamics and with iterative processes in the tensor train format are included. Comment: Submitted to SIAM Journal on Numerical Analysi...|$|R
40|$|We {{propose a}} fast block-wise and {{parallel}} training approach to train i-vector systems. This approach divides the loading ma-trix into groups according to components or acoustic feature dimensions and <b>trains</b> the loading <b>matrices</b> {{of these groups}} in-dependently and in parallel. These individually <b>trained</b> block <b>matrices</b> can be combined to approximate the original load-ing matrix, or used to derive independent i-vectors. We tested the block-wise training on speaker verification tasks based on the NIST SRE data {{and found that it}} can substantially speed up the training while retaining the quality of the resulting i-vectors. Index Terms — speaker verification, factor analysis, i-vector 1...|$|R
3000|$|The {{available}} dataset is manually annotated {{and divided}} into {{training and test}} sets, as will be described in detail in Section 4. Features extracted from all audio files in the training dataset for a particular target class are concatenated to give <b>training</b> feature <b>matrix</b> X [...]...|$|R
