10000|10000|Public
25|$|Summarization – {{providing}} a more compact representation of <b>the</b> <b>data</b> <b>set,</b> including visualization and report generation.|$|E
25|$|Further {{examining}} <b>the</b> <b>data</b> <b>set</b> {{in secondary}} analyses, to suggest new hypotheses for future study.|$|E
25|$|A bucket sort {{works best}} when the {{elements}} of <b>the</b> <b>data</b> <b>set</b> are evenly distributed across all buckets.|$|E
30|$|Hypotheses {{related to}} {{operating}} core (innovation process, cross-functional organisation, {{and implementation of}} tools/technology) and competition-informed pricing The result of H 4 is supported in <b>the</b> full <b>data</b> <b>set</b> (β =  0.170, p <  0.05) and <b>the</b> Malaysian <b>data</b> <b>set</b> (β =  0.255, p <  0.05), while in <b>the</b> Bangladeshi <b>data</b> it was not supported. The result of H 5 was supported in <b>the</b> full <b>data</b> <b>set</b> (β =  0.266, p <  0.05) and <b>the</b> Bangladeshi <b>data</b> <b>set</b> (β =  0.275, p <  0.05), while in <b>the</b> Malaysian <b>data</b> <b>set</b> it was not supported. The result of H 6 was supported in <b>the</b> full <b>data</b> <b>set</b> (β =  0.295, p <  0.01) and <b>the</b> Bangladeshi <b>data</b> <b>set</b> (β =  0.536, p <  0.01), while in <b>the</b> Malaysian <b>data</b> <b>set,</b> H 6 was not supported.|$|R
30|$|A BYEC {{classifier}} {{composed of}} 25 SVM subclassifiers was trained by 70 % {{of the original}} NEU defect <b>data</b> <b>set,</b> and <b>the</b> remaining 30 % of <b>the</b> <b>data</b> were {{used to evaluate the}} accuracy of the BYEC classifier on <b>the</b> original <b>data</b> <b>set.</b> Then, we randomly sampled 10 % of <b>the</b> processed <b>data</b> <b>sets</b> to adjust our BYEC classifier. Finally, the accuracy of the adjusted BYEC classifier and the original classifier on <b>the</b> processed <b>data</b> <b>set</b> were tested by the remaining 90 % of <b>the</b> processed <b>data.</b> As for <b>the</b> BYEC classifier, 70 % of <b>the</b> original <b>data</b> <b>set</b> were used to train the KNN, BPNN, and SVM classifier and 30 % were used to evaluate the accuracy on <b>the</b> original <b>data</b> <b>set,</b> then <b>the</b> accuracy of these classifiers on <b>the</b> processed <b>data</b> <b>set</b> was tested by 90 % of <b>the</b> processed <b>data</b> <b>set.</b> <b>The</b> BPNN and KNN parameters were determined by cross-validation testing. The average accuracy of the classifiers on every <b>data</b> <b>set</b> was run 100 times, and <b>the</b> <b>data</b> <b>sets</b> were sampled individually.|$|R
50|$|Gelman et al. in Bayesian Data Analysis (2004) {{consider}} a <b>data</b> <b>set</b> relating to speed-of-light measurements made by Simon Newcomb. <b>The</b> <b>data</b> <b>sets</b> for that book {{can be found}} via <b>the</b> Classic <b>data</b> <b>sets</b> page, and <b>the</b> book's website contains more information on <b>the</b> <b>data.</b>|$|R
25|$|If the {{uncertainty}} of the observations is not known from external sources, then the weights could be estimated from the given observations. This can be useful, for example, to identify outliers. After the outliers have been removed from <b>the</b> <b>data</b> <b>set,</b> the weights should be reset to one.|$|E
25|$|The {{number of}} species missing from <b>the</b> <b>data</b> <b>set</b> (the missing area {{to the left of}} the veil line) is simply N minus the {{number of species}} sampled. Preston did this for two {{lepidopteran}} data sets, predicting that, even after 22 years of collection, only 72% and 88% of the species present had been sampled.|$|E
25|$|The {{improvement}} in the results came from both having an extra 2 years of measurements (<b>the</b> <b>data</b> <b>set</b> runs between midnight on August 10, 2001 to midnight of August 9, 2006), as well as using improved data processing techniques and a better characterization of the instrument, most notably of the beam shapes. They also {{make use of the}} 33nbsp&GHz observations for estimating cosmological parameters; previously only the 41nbsp&GHz and 61nbsp&GHz channels had been used.|$|E
50|$|Although <b>the</b> <b>data</b> <b>sets</b> were {{constructed}} to preserve customer privacy, the Prize {{has been criticized}} by privacy advocates. In 2007 two researchers from The University of Texas at Austin {{were able to identify}} individual users by matching <b>the</b> <b>data</b> <b>sets</b> with film ratings on the Internet Movie Database.|$|R
5000|$|For {{partitioned}} <b>data</b> <b>sets,</b> IEBCOMPR considers <b>the</b> <b>data</b> <b>sets</b> equal if <b>the</b> {{following conditions}} are met: ...|$|R
5000|$|... {{specifies}} that <b>the</b> message <b>data</b> <b>set</b> is not {{to include}} a listing of <b>the</b> output <b>data</b> <b>set.</b>|$|R
25|$|The {{modified}} Thompson Tau test {{is used to}} {{find one}} outlier at a time (largest value of δ is removed if it is an outlier). Meaning, if a data point {{is found to be}} an outlier, it is removed from <b>the</b> <b>data</b> <b>set</b> and the test is applied again with a new average and rejection region. This process is continued until no outliers remain in a data set.|$|E
25|$|Q-Q plot— is {{a plot of}} the sorted {{values from}} <b>the</b> <b>data</b> <b>set</b> against the {{expected}} values of the corresponding quantiles from the standard normal distribution. That is, it's a plot of point of the form (Φ−1(pk), x(k)), where plotting points pk are equal to pk=(k−α)/(n+1−2α) and α is an adjustment constant, which can be anything between 0 and1. If the null hypothesis is true, the plotted points should approximately lie on a straight line.|$|E
25|$|Bubble sort is {{a simple}} sorting {{algorithm}}. The algorithm starts {{at the beginning of}} <b>the</b> <b>data</b> <b>set.</b> It compares the first two elements, and if the first is greater than the second, it swaps them. It continues doing this for each pair of adjacent elements to the end of <b>the</b> <b>data</b> <b>set.</b> It then starts again with the first two elements, repeating until no swaps have occurred on the last pass. This algorithm's average time and worst-case performance is O(n2), so it is rarely used to sort large, unordered data sets. Bubble sort can be used to sort a small number of items (where its asymptotic inefficiency is not a high penalty). Bubble sort can also be used efficiently on a list of any length that is nearly sorted (that is, the elements are not significantly out of place). For example, if any number of elements are out of place by only one position (e.g. 0123546789 and 1032547698), bubble sort's exchange will get them in order on the first pass, the second pass will find all elements in order, so the sort will take only 2n time.|$|E
5000|$|When {{comparing}} sequential <b>data</b> <b>sets,</b> IEBCOMPR considers <b>the</b> <b>data</b> <b>sets</b> equal if <b>the</b> {{following conditions}} are met: ...|$|R
5000|$|... #Subtitle level 2: Limitations and {{criticisms of}} <b>the</b> <b>data</b> <b>sets</b> ...|$|R
50|$|In the {{geospatial}} (GIS) domain, {{data fusion}} is often synonymous with data integration. In these applications, {{there is often}} a need to combine diverse <b>data</b> <b>sets</b> into a unified (fused) <b>data</b> <b>set</b> which includes all of <b>the</b> <b>data</b> points and time steps from <b>the</b> input <b>data</b> <b>sets.</b> <b>The</b> fused <b>data</b> <b>set</b> is different from a simple combined superset in that the points in <b>the</b> fused <b>data</b> <b>set</b> contain attributes and metadata which might not have been included for these points in <b>the</b> original <b>data</b> <b>set.</b>|$|R
25|$|A {{series of}} {{observations}} is made of the spectrum of light emitted by a star. Periodic variations in the star's spectrum may be detected, with the wavelength of characteristic spectral lines in the spectrum increasing and decreasing regularly {{over a period of}} time. Statistical filters are then applied to <b>the</b> <b>data</b> <b>set</b> to cancel out spectrum effects from other sources. Using mathematical best-fit techniques, astronomers can isolate the tell-tale periodic sine wave that indicates a planet in orbit.|$|E
25|$|Modern {{nuclear physics}} and {{particle}} physics experiments often involve {{large numbers of}} data analysts working together to extract quantitative data from complex datasets. In particular, the analysts want to report accurate systematic error estimates for all of their measurements; this is difficult or impossible {{if one of the}} errors is observer bias. To remove this bias, the experimenters devise blind analysis techniques, where the experimental result is hidden from the analysts until they've agreed—based on properties of <b>the</b> <b>data</b> <b>set</b> other than the final value—that the analysis techniques are fixed.|$|E
25|$|Expressed Sequence Tag or EST {{assembly}} {{differs from}} genome assembly in several ways. The sequences for EST assembly are the transcribed mRNA {{of a cell}} and represent only {{a subset of the}} whole genome. At a first glance, underlying algorithmical problems differ between genome and EST assembly. For instance, genomes often have large amounts of repetitive sequences, mainly in the inter-genic parts. Since ESTs represent gene transcripts, they will not contain these repeats. On the other hand, cells tend to have a certain number of genes that are constantly expressed in very high numbers (housekeeping genes), which again leads to the problem of similar sequences present in high numbers in <b>the</b> <b>data</b> <b>set</b> to be assembled.|$|E
40|$|Results {{from using}} {{different}} breast cancer <b>data</b> <b>set</b> as <b>the</b> training <b>data</b> <b>set</b> in <b>the</b> MCL+superpc approach In order {{to check the}} robustness of our MCL+superpc approach, we used each of four validation <b>data</b> <b>sets</b> as <b>the</b> training <b>data</b> <b>set,</b> and <b>the</b> remaining four <b>data</b> <b>sets</b> as validation <b>data</b> <b>sets.</b> <b>The</b> following tables show these results. Table S 1. Superpc continuous prediction results from breast cancer <b>data</b> analysis. <b>The</b> results were generated by using <b>the</b> GSE 4922 <b>data</b> <b>set</b> as <b>the</b> training <b>data</b> <b>set</b> and four independent <b>data</b> <b>sets</b> as validation <b>data</b> with a threshold value of 1. 10 and 9 selected MCL modules. <b>The</b> training <b>data</b> <b>set</b> is highlighted in red. P-values less than 0. 05 are highlighted in yellow...|$|R
30|$|A {{summary of}} <b>the</b> <b>data</b> <b>sets</b> is {{provided}} in Table  4.|$|R
40|$|Abstract Background The {{information}} from different <b>data</b> <b>sets</b> experimented under different conditions may be inconsistent {{even though they}} are performed with the same research objectives. More than that, even when <b>the</b> <b>data</b> <b>sets</b> were generated from the same platform, <b>the</b> <b>data</b> agreement may be affected by the technical variation among the laboratories. In this case, it is necessary to use <b>the</b> combined <b>data</b> <b>set</b> after adjusting <b>the</b> differences between such <b>data</b> <b>sets,</b> for detecting <b>the</b> more reliable information. Results The proposed method combines <b>data</b> <b>sets</b> posterior to <b>the</b> discretization of <b>data</b> <b>sets</b> based on <b>the</b> ranks of the gene expression ratios, and the statistical method is applied to <b>the</b> combined <b>data</b> <b>set</b> for predictive gene selection. The efficiency of the proposed method was evaluated using five colon cancer related <b>data</b> <b>sets,</b> which were experimented using cDNA microarrays with different RNA sources, and one experiment utilized oligonucleotide arrays. NCI- 60 cell lines <b>data</b> <b>sets</b> were used, which were performed with two different platforms of cDNA microarrays and Affymetrix HU 6800 oligonucleotide arrays. <b>The</b> combined <b>data</b> <b>set</b> by <b>the</b> proposed method predicted <b>the</b> test <b>data</b> <b>sets</b> more accurately than <b>the</b> separated <b>data</b> <b>sets</b> did. <b>The</b> biological significant genes were detected from <b>the</b> combined <b>data</b> <b>set,</b> which were missed on <b>the</b> separated <b>data</b> <b>sets.</b> Conclusion By transforming gene expressions using ranks, the proposed method is not influenced by systematic bias among chips and normalization method. The method may be especially more useful to find predictive genes from <b>data</b> <b>sets</b> which have different scale in gene expressions. </p...|$|R
25|$|For example, each of {{the three}} populations {0, 0, 14, 14}, {0, 6, 8, 14} and {6, 6, 8, 8} has a mean of 7. Their {{standard}} deviations are 7, 5, and 1, respectively. The third population has a much smaller standard deviation than the other two because its values are all close to 7. It will have the same units as the data points themselves. If, for instance, <b>the</b> <b>data</b> <b>set</b> {0, 6, 8, 14} represents the ages of a population of four siblings in years, the standard deviation is 5 years. As another example, the population {1000, 1006, 1008, 1014} may represent the distances traveled by four athletes, measured in meters. It has a mean of 1007 meters, and a standard deviation of 5 meters.|$|E
25|$|The PackML (Packaging Machinery Language) sub-committee’s {{focus was}} to develop a method to quickly {{integrate}} a line of machines without concern on what field bus (protocol & media-the domain of the PackConnect sub-committee) was going to carry <b>the</b> <b>data</b> <b>set</b> between machines, SCADA and MES. After several iterations the approach taken was to extend the ISA S88 Part 1 State Model concept to the Packaging Industry. Later in the development process, the concept of PackTags was introduced to provide a uniform set of naming conventions for data elements used within the state model. PackTags are used for machine-to-machine communications; for example between a Filler and a Capper. In addition, PackTags were designed to address OEE (Overall Equipment Effectiveness) calculations. PackTags can be used to provide data exchange between machines and higher level information systems like Manufacturing Operations Management and Enterprise Information Systems.|$|E
500|$|The {{method is}} {{intended}} to change existing basketball team composition paradigms to help NBA teams win championships. Alagappan used Devin Ebanks of the Los Angeles Lakers as an example, whom Alagappan considered promising for his classification as a [...] "scoring rebounder" [...] alongside Carmelo Anthony and Dirk Nowitzki despite a subpar season record. GQ reported that Ebanks would come to fulfill the model's forecast in following games for six weeks when two other players sat out and Ebanks started games. While the model couldn't have predicted Linsanity, when given college basketball stats, {{it was able to}} determine three players most similar to Lin in college, of which two were already drafted in the NBA. The forecasting is expected to improve with the growth of <b>the</b> <b>data</b> <b>set.</b>|$|E
5000|$|<b>The</b> European <b>Data</b> Portal {{provides}} {{access to}} <b>the</b> <b>data</b> <b>sets</b> in different ways, via: ...|$|R
30|$|<b>The</b> <b>data</b> <b>sets</b> are contextually {{converted}} into numeric/symbolic structured forms for analysis.|$|R
5000|$|The diagram shown [...] {{compares the}} results of {{training}} on <b>the</b> <b>data</b> <b>sets</b> ...|$|R
2500|$|... {{then the}} {{variability}} of <b>the</b> <b>data</b> <b>set</b> can be measured using three sums of squares formulas: ...|$|E
2500|$|A main {{difference}} in evolutionary models {{is how many}} parameters are estimated every time for <b>the</b> <b>data</b> <b>set</b> under ...|$|E
2500|$|In a paper {{published}} on 10 November 2009, Harvard economist Davide Cantoni tested Weber's Protestant hypothesis using population {{and economic growth}} in second-millennium Germany as <b>the</b> <b>data</b> <b>set,</b> with negative results. Cantoni writes: ...|$|E
30|$|The {{datasets}} in Table 2 were normalised between 0 and 1 using WEKA [52] {{and contained}} no missing values. <b>The</b> padestrian <b>data</b> <b>set</b> is {{extracted from the}} attributed pedestrian database [47, 48] using the Matlab code given in APiS 1.0 [53]. <b>The</b> madelon <b>data</b> <b>set</b> available at UCI repository [46]. The rest of <b>the</b> <b>data</b> <b>sets</b> are freely available at the website of authors of the related work [45].|$|R
30|$|<b>The</b> <b>data</b> <b>sets</b> {{supporting}} <b>the</b> {{conclusions of}} this article {{are included in the}} article.|$|R
40|$|THESIS 9964 This thesis {{examines}} charcoal {{methods in}} relation to archaeological sites and woodland resource usage {{using a variety of}} anthracological and palaeoecological approaches as well as literary evidence at different spatial and temporal scales. A number of hypotheses were set up to test <b>the</b> <b>data</b> <b>sets</b> with regard to woodland dynamics, human taphonomies as well as relationships between <b>the</b> <b>data</b> <b>sets...</b>|$|R
