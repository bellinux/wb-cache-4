5|31|Public
5000|$|... "Shadow Redundancy": Exchange Server 2010 {{introduces}} a transport concept called [...] "Shadow Redundancy" [...] that protects e-mail messages {{while they are}} in transit. If a Hub <b>Transport</b> <b>server</b> or an Edge <b>Transport</b> <b>server</b> fails after it has received a message for processing, but before {{it was able to}} deliver it to the next [...] "hop" [...] server, the server sending the message to the <b>transport</b> <b>server</b> is now able to detect the failure and redeliver the message to a different Hub Transport or Edge <b>Transport</b> <b>server</b> for processing...|$|E
5000|$|Remote loading (total or partial) of {{messages}} in the inbox of the <b>transport</b> <b>server.</b>|$|E
5000|$|Visualization (number, list, contents...) of the {{messages}} arrived in the inbox of the <b>transport</b> <b>server.</b>|$|E
50|$|The {{service is}} entirely web-based, {{requiring}} no downloads. Avatars {{can also be}} <b>transported</b> <b>server</b> to server and dropped into existing animations.|$|R
50|$|SOCKS {{performs}} at Layer 5 of the OSI model (the session layer, {{an intermediate}} layer between the presentation layer and the <b>transport</b> layer). SOCKS <b>server</b> accepts incoming client connection on TCP port 1080.|$|R
50|$|RADIUS is a client/server {{protocol}} {{that runs}} in the application layer, and can use either TCP or UDP as <b>transport.</b> Network access <b>servers,</b> the gateways that control access to a network, usually contain a RADIUS client component that communicates with the RADIUS server. RADIUS is often the back-end of choice for 802.1X authentication as well.|$|R
50|$|Internal {{communications}} between Exchange Servers (2000 and later) over SMTP encode {{the message in}} S/TNEF (Summary TNEF) format. The conversion between the format needed by the end client on the Internet is performed on the last Hub <b>Transport</b> <b>server</b> before final delivery, and when the Hub Transport role of an Exchange Server is about to deliver the message to a mailbox role server, the message is converted to MAPI format for storage.|$|E
5000|$|Exchange Server 2007 {{provides}} {{built-in support}} for asynchronous replication modeled on SQL Server's [...] "Log shipping" [...] in CCR (Cluster Continuous Replication) clusters, which {{are built on}} MSCS MNS (Microsoft Cluster Service—Majority Node Set) clusters, which do not require shared storage. This type of cluster can be inexpensive and deployed in one, or [...] "stretched" [...] across two data centers for protection against site-wide failures such as natural disasters. The limitation of CCR clusters {{is the ability to}} have only two nodes and the third node known as [...] "voter node" [...] or file share witness that prevents [...] "split brain" [...] scenarios, generally hosted as a file share on a Hub <b>Transport</b> <b>Server.</b> The second type of cluster is the traditional clustering that was available in previous versions, and is now being referred to as SCC (Single Copy Cluster). In Exchange Server 2007 deployment of both CCR and SCC clusters has been simplified and improved; the entire cluster install process takes place during Exchange Server installation. LCR or Local Continuous Replication has been referred to as the [...] "poor man's cluster". It is designed to allow for data replication to an alternative drive attached to the same system and is intended to provide protection against local storage failures. It does not protect against the case where the server itself fails.|$|E
40|$|Due to {{increase}} in the multimedia applications and streaming <b>servers,</b> <b>transport</b> of MPEG compressed video data through a packet switched network has become highly important. It is necessary to transport the video while ensuring appropriate video quality at the receiving end. This paper proposes a new N 2 N packet forwarding technique that reduces the packet loss and delay jitter of the video traffic. The paper also shows that {{with the use of}} the proposed packet forwarding technique, the PSNR is also improved...|$|R
50|$|PEAP {{is similar}} in design to EAP-TTLS, {{requiring}} only a server-side PKI certificate to create a secure TLS tunnel to protect user authentication, and uses server-side public key certificates to authenticate the server. It then creates an encrypted TLS tunnel between the client and the authentication server. In most configurations, the keys for this encryption are <b>transported</b> using the <b>server's</b> public key. The ensuing exchange of authentication information inside the tunnel to authenticate the client is then encrypted and user credentials are safe from eavesdropping.|$|R
40|$|In this paper, {{we develop}} a {{methodology}} for determining where the time goes between when a user clicks on a Web page {{and when the}} page appears on the display. Our methodology uses only client-based measurements, requiring no special support from the Internet or the Web server. This is crucial to our approach since performance bottlenecks may span many different administrative authorities, making coordinated measurement of the client, network and server very difficult in practice. Using only data available to any Web client, {{we are able to}} distinguish delays due to name translation, network propagation, network queueing, <b>transport,</b> the <b>server,</b> packet losses, and the browser with a combination of direct measurement, analytical modelling, and statistical inference. Our results show that there is no single network or server delay that dominates object transfer time, but that browser overheads are a significant fraction of overall latency. ...|$|R
40|$|The core of {{this thesis}} is to {{describe}} in detail the design of communication protocol for client/server communication and creation of a functioning server application prototype. REST design approach was used for this communication. This approach uses mainly GET and POST functions of HTTP <b>transport</b> protocol. The <b>server</b> application prototype is created in PHP {{with the help of}} Silex micro-framework. The protocol is designed for mobile clients, which are used for registering into queues and also for administrative clients, which can control and manage the queues. Testing of the designed protocol with server application prototype is described {{at the end of this}} thesis...|$|R
5000|$|The {{product is}} a live-virtual-constructive (LVC) system that creates cyber range environments that can train {{all types of}} cyber warriors, as {{trainees}} have to react to realistic, stressful situations in a high fidelity synthetic environment that models the information <b>transport</b> fabric between <b>servers</b> and end-point systems. The system leverages a true virtual network model so that the Trainer system represents a distributed network system, representing mobile wireless equipment as they interoperate with a wired back bone network infrastructure. Both live and virtual hosts can {{be connected to the}} virtual network model and the system can also be connected with other training simulators ...|$|R
40|$|The Session Initiation Protocol (SIP) is {{increasingly}} {{being used in}} systems that are tightly coupled with Hypertext <b>Transport</b> Protocol (HTTP) <b>servers</b> {{for a variety of}} reasons. In many of these cases, applications can benefit from being able to discover, in near realtime, when a specific HTTP resource is created, changed, or deleted. This document proposes a mechanism, based on the SIP Event Framework, for doing so. Status of This Memo This is an Internet Standards Track document. This document {{is a product of the}} Internet Engineering Task Force (IETF). It represents the consensus of the IETF community. It has received public review and has been approved for publication by th...|$|R
40|$|The use of {{floating}} licenses for {{the execution of}} the applications through the network has established the necessity of a flexible and secure License Server. FLEXlm Server is a well known example used by a great amount of applications. To improve the security and flexibility of a FLEXlm Server we propose an architecture that keeps the server unknown to the applications. The communication now is held in the application layer, by the remote call of a web service, instead of the <b>transport</b> layer where <b>Server's</b> and application's communication used to take place. The architecture assures the flexibility and the security without important consequences neither on time of the whole process(described above) nor on bandwidth of the current network...|$|R
40|$|Traversal Using Relays around NAT (TURN) Resolution Mechanism This {{document}} {{defines a}} resolution mechanism {{to generate a}} list of <b>server</b> <b>transport</b> addresses that can be tried to create a Traversal Using Relays around NAT (TURN) allocation. Status of This Memo This is an Internet Standards Track document. This document {{is a product of}} the Internet Engineering Task Force (IETF). It represents the consensus of the IETF community. It has received public review and has been approved for publication by the Internet Engineering Steering Group (IESG). Further information on Internet Standards is available in Section 2 of RFC 5741. Information about the current status of this document, any errata, and how to provide feedback on it may be obtained a...|$|R
40|$|The Internet of Things (IoT), {{which enables}} common objects to be {{intelligent}} and interactive, {{is considered the}} next evolution of the Internet. Its pervasiveness and abilities to collect and analyze data which can be converted into information have motivated a plethora of IoT applications. For the successful deployment and management of these applications, cloud computing techniques are indispensable since they provide high computational capabilities as well as large storage capacity. This paper aims at providing insights about the architecture, implementation and performance of the IoT cloud. Several potential application scenarios of IoT cloud are studied, and an architecture is discussed regarding the functionality of each component. Moreover, the implementation details of the IoT cloud are presented along with the services that it offers. The main contributions of this paper lie in {{the combination of the}} Hypertext Transfer Protocol (HTTP) and Message Queuing Telemetry <b>Transport</b> (MQTT) <b>servers</b> to offer IoT services in the architecture of the IoT cloud with various techniques to guarantee high performance. Finally, experimental results are given in order to demonstrate the service capabilities of the IoT cloud under certain conditions. Comment: 19 pages, 4 figures, IEEE Communications Magazin...|$|R
40|$|There is now {{increasing}} interest in active and programmable networking. Increasing {{amount of research}} is ongoing on topics ranging from dynamic service and network management, to control plane switch programmability, to active network capsules carrying code in every datagram. This is {{in addition to the}} already established trend of added functionality of gateways, proxies and servers in the network. Together the two are accelerating the rate at which advanced functions are being introduced into the network, where service providers attempt to provide enhanced and differentiated services. While different players have different motivation to increase the programmability of network elements, the primary motivation for a service provider is to provide differentiated services using commodity components. Whereas economic reasons drive network providers to use commodity infrastructure technologies for optical <b>transport,</b> switching and <b>servers,</b> service differentiation is essential to maintain pro [...] ...|$|R
40|$|Server applications, and in {{particular}} networkbased server applications, place a unique combination of demands on a programming language: lightweight concurrency and high I/O throughput are both important. This paper describes a prototype web server written in Concurrent Haskell, and presents two useful results: rstly, a conforming server could be written with minimal eort, leading to an implementation in less than 1500 lines of code, and secondly the naive implementation produced reasonable performance. Furthermore, making minor modications to a few time-critical components improved performance to a level acceptable for anything but the most heavily loaded web servers. 1 Introduction The Internet has spawned its own application domain: multithreaded server applications, capable of interacting with {{hundreds or thousands of}} clients simultaneously, are becoming increasingly important. Examples include FTP (File Transfer Protocol), E-Mail <b>transport,</b> DNS (name <b>servers),</b> Usenet News, [...] ...|$|R
40|$|Abstract. In {{this paper}} the first {{results on the}} Online Dial-A-Ride Problem with Time-Windows (ODARPTW for short) are presented. Re-quests for rides {{appearing}} over time consist of two points in a metric space, a source and a destination. <b>Servers</b> <b>transport</b> objects of requests from sources to destinations. Each request specifies a deadline. If a re-quest is not be served by its deadline, it will be called off. The goal is to plan the motion of servers in an online way so that {{the maximum number of}} requests is met by their deadlines. We perform competitive analysis of two deterministic strategies for the problem with a single server in two cases separately, where the server has unit capacity and where the server has infinite capacity. The competitive ratios of the strategies are obtained. We also prove a lower bound on the competitive ratio of any deterministic algorithm of 2 −...|$|R
40|$|In the {{engineering}} and scientific marketplaces, the workstation-server model of computing {{is emerging as}} the standard of the 1990 s. Implementing an object-oriented database system in this environment immediately presents the design choice of how to partition database functionality between the server and workstation processors. To better understand the alternatives to this fundamental design decision, we analyze three different workstation-server architectures, evaluating them both qualitatively and through benchmarking of prototypes. The three approaches are labeled object server, in which individual objects pass between the server and workstation, page server, in which a disk page is the unit of <b>transport</b> and the <b>server</b> buffers pages, and file server, where whole pages are transferred as well, but they are accessed directly by the workstation process via a remote file service (namely, NFS). We built prototypes of all three architectures, using a stripped-down version of the WiSS stora [...] ...|$|R
40|$|Isosurface {{rendering}} is {{a technique}} for viewing and understanding many large data sets from both science and engineering. With the advent of multi-gigabit-per-second network backbones such as Internet 2 and fast local networking technology, scientists are looking at new ways to share and explore large data sets remotely. Telemedicine, which encompasses both videoconferencing and remote visualization {{is likely to be}} in widespread use with these advances. Despite the availability of increased bandwidth, two challenges remain. First, {{the time it takes to}} locate cells intersecting an isosurface of interest must be reduced for large data sets; a cell extraction technique that scales with data size is also critical. The second challenge has to do with the mitigating the effects of network latency on the overall isosurface visualization time. We present a remote isosurface visualization technique that addresses these two challenges. Isosurface extraction delays are reduced through the use of a search-optimized, chessboarded interval tree data structure on the disk. Network transport delays are reduced by sending cells extracted from the chessboarded data on the server, compressing it by about 87 %. In addition, network transport delays are hidden effectively by overlapping data <b>transport</b> with <b>server</b> side functions. On a 100 Mbits/sec. switched LAN, the remote visualization time- the time between the issue of a query from the client side to the server and the displaying of a complete image on the client is only a few seconds for most isovalues in the well-known visible woman data set...|$|R
30|$|When {{the mode}} change is requested, an agent {{residing}} in each core prepares {{a set of}} packages with runnables to be migrated via the network. This agent is configured statically and {{is equipped with a}} table with information about runnables that need to be migrated during a particular mode change. Then the contexts of these runnables are migrated. In the following hyperperiods, runnables are <b>transported</b> using periodic <b>servers</b> of the length determined statically using schedulability analysis, as described earlier. The agent is aware of the number of periodic server jobs that have to be used during the whole migration process, and has the dynamic (volatile) portion of the context identified. This part of the context is to be transmitted in a single job of the periodic server, just after the last execution of the runnable at its old location. After migrating the dynamic part of the runnable’s context, it is removed from the earlier (migration source) core.|$|R
40|$|AbstractPC-based {{monitoring}} system for {{efficient use of}} small-scale day care centers for elderly people, whose feature is that the users live in the walking distance to it, has been developed. The developed System is composed of PCs with viewer software, network cameras, and a FTP server. In the system, images taken at the users’ house are <b>transported</b> to FTP <b>server</b> periodically, the software installed in PCs at day care centers are taken the related images from FTP servers periodically. The images are checked continuously by the staffs of the canter. Once something wrong occurs, then the staff goes to the user's house immediately and conducts the appropriate operations. The pre-registered persons, such as relatives of the users, can view the images through the Internet. If they find something wrong, then they can immediately notice that to {{the staff of the}} day care center. In this paper, the features of the developed {{monitoring system}} and an experiment conducted in a day care center, Koguma-kan, are described...|$|R
40|$|In {{the online}} {{dial-a-ride}} problem (OlDarp), objects must be <b>transported</b> by a <b>server</b> between {{points in a}} metric space. Transportation requests (“rides”) arrive online, specifying the objects to be transported and the corresponding source and destination. We investigate the OlDarp {{for the objective of}} minimizing the maximum flow time. It has been well known {{that there can be no}} strictly competitive online algorithm for this objective and no competitive algorithm at all on unbounded metric spaces. However, the question whether on metric spaces with bounded diameter there are competitive algorithms if one allows an additive constant in the definition competitive ratio, had been open for quite a while. We provide a negative answer to this question already on the uniform metric space with three points. Our negative result is complemented by a strictly 2 -competitive algorithm for the Online Traveling Salesman Problem on the uniform metric space, a special case of the problem...|$|R
40|$|In {{this paper}} the first {{results on the}} Online Dial-A-Ride Problem with Time-Windows (ODARPTW for short) are presented. Requests for rides {{appearing}} over time consist of two points in a metric space, a source and a destination. <b>Servers</b> <b>transport</b> objects of requests from sources to desti-nations. Each request specifies a deadline. If a request is not be served by its deadline, it will be called off. The goal is to plan the motion of servers in an online way so that {{the maximum number of}} requests is met by their deadlines. We perform competitive analysis of two deterministic strate-gies for the problem with a single server in two cases separately, where the server has unit capacity and where the server has infinite capacity. The competitive ratios of the strategies are obtained. We also prove a lower bound on the competitive ratio of any deterministic algorithm of 2 −T 2 T for a server with unit capacity and of 2 −...|$|R
40|$|Any network {{communication}} system involves finding a path (routing) and controlling the rate (transport) of communication. Existing routing techniques lack an efficient routing metric which reflects the up-to-date {{status of the}} network. Existing transport protocols (TCP) lack the good and up-to-date knowledge of the rate of communication. Cleverly aggregated information from the border routers (ingress and egress) gives enough information about the up-to-date load condition of the core routers. A Border Routing & <b>Transport</b> Protocol (BRTP) <b>server</b> system which has the aggregated information can then compute the best ingress-egress (in-eg) path and rate. The rate of the flows in one in-eg path is then obtained by sharing the ''capacity??? (rate) of the in-eg path obtained by the BRTP fairly or proportionally. The final rate at which a flow sends data is then the minimum of the rates (bottleneck rate) it obtains from all in-eg pairs in its path to the destination. This {{can be done by}} adding a shim layer to the TCP header of packets which can be overwritten by ingress routers in the path. unpublishednot peer reviewe...|$|R
40|$|Abstract Background Statistical {{comparison}} of peptide profiles in biomarker discovery requires fast, user-friendly software for high throughput data analysis. Important features are flexibility in changing input variables and {{statistical analysis of}} peptides that are differentially expressed between patient and control groups. In addition, integration the mass spectrometry data {{with the results of}} other experiments, such as microarray analysis, and information from other databases requires a central storage of the profile matrix, where protein id's can be added to peptide masses of interest. Results A new database application is presented, to detect and identify significantly differentially expressed peptides in peptide profiles obtained from body fluids of patient and control groups. The presented modular software is capable of central storage of mass spectra and results in fast analysis. The software architecture consists of 4 pillars, 1) a Graphical User Interface written in Java, 2) a MySQL database, which contains all metadata, such as experiment numbers and sample codes, 3) a FTP (File <b>Transport</b> Protocol) <b>server</b> to store all raw mass spectrometry files and processed data, and 4) the software package R, which is used for modular statistical calculations, such as the Wilcoxon-Mann-Whitney rank sum test. Statistic analysis by the Wilcoxon-Mann-Whitney test in R demonstrates that peptide-profiles of two patient groups 1) breast cancer patients with leptomeningeal metastases and 2) prostate cancer patients in end stage disease can be distinguished from those of control groups. Conclusion The database application is capable to distinguish patient Matrix Assisted Laser Desorption Ionization (MALDI-TOF) peptide profiles from control groups using large size datasets. The modular architecture of the application makes it possible to adapt the application to handle also large sized data from MS/MS- and Fourier Transform Ion Cyclotron Resonance (FT-ICR) mass spectrometry experiments. It is expected that the higher resolution and mass accuracy of the FT-ICR mass spectrometry prevents the clustering of peaks of different peptides and allows the identification of differentially expressed proteins from the peptide profiles. </p...|$|R
40|$|After the Technology Opportunities Showcase (TOPS), in October, 1993, Langley Research Center's (LaRC) Information Systems Division (ISD) {{accepted}} {{the challenge to}} preserve the investment in information assembled in the TOPS exhibits by establishing a data base. Following the lead of several people at LaRC and others around the world, the HyperText <b>Transport</b> Protocol (HTTP) <b>server</b> and Mosaic were the obvious tools of choice for implementation. Initially, some TOPS exhibitors began the conventional approach of constructing HyperText Markup Language (HTML) pages of their exhibits as input to Mosaic. Considering the number of pages to construct, a better approach was conceived that would automate the construction of pages. This approach allowed completion of the data base construction in a shorter period of time using fewer resources than {{would have been possible}} with the conventional approach. It also provided flexibility for the maintenance and enhancement of the data base. Since that time, this approach has been used to automate construction of other HTML data bases. Through these experiences, it is concluded that the most effective use of the HTTP/Mosaic technology will require better tools and techniques for creating, maintaining and managing the HTML pages. The development and use of these tools and techniques are the subject of this document...|$|R
40|$|This paper {{describes}} a newly developed multi-sensor network system for landslide and hazard monitoring. Landslide hazard {{is one of}} the most destructive natural disasters, which has severely affected human safety, properties and infrastructures. We report the results of designing and deploying the multi-sensor network, based on the simulated landslide model, to monitor typical landslide areas and with a goal to predict landslide hazard and mitigate damages. The integration and deployment of the prototype sensor network were carried out in an experiment area at Tongji University in Shanghai. In order to simulate a real landslide, a contractible landslide body is constructed in the experiment area by 7 m* 1. 5 m. Then, some different kind of sensors, such as camera, GPS, crackmeter, accelerometer, laser scanning system, inclinometer, etc., are installed near or in the landslide body. After the sensors are powered, continuous sampling data will be generated. With the help of communication method, such as GPRS, and certain transport devices, such as iMesh and 3 G router, all the sensor data will be <b>transported</b> to the <b>server</b> and stored in Oracle. These are the current results of an ongoing project of the center. Further research results will be updated and presented in the near future...|$|R
40|$|Mobile {{agents are}} {{processes}} {{that can be}} dispatched from source computer and be <b>transported</b> to remote <b>servers</b> for execution, have been widely argued {{to be an important}} enabling technology for future systems. Location management is a necessity for locating mobile agents in a network of mobile agent hosts for controlling, monitoring and communication during processing and it still represents an open research issue. The cost of location management strategies mainly depends on the cost of search and update. We concentrated on reducing the cost of update and improving the speed of processing of the agents. We proposed a location management technique applicable for multi-region environment in which mobile agent did not update its location at every migration. The technique named as Broadcasting with Search by Path Chase (BSPC). We used the tool time Petri net analyzer TINA to model, analyze and to simulate BSPC. We found that the BSPC behaves as expected and free from any deadlock. We measured the efficiency of BSPC and compared with some existing location management techniques by parametric evaluation. BSPC provided better scalability, location updating availability and interaction fault rate with theoretical considerations of network usage and network fault rate. It gave its best performance for applications of low CMR and having high migration rate of mobile agents within birth region...|$|R
40|$|Mobile {{agents are}} {{processes}} {{that may be}} dispatched from a source computer and be <b>transported</b> to remote <b>servers</b> for execution. In any mobile agent system, {{the ability to communicate}} with agents in real-time is essential for retrieving any data or information that they have collected, and for supporting coordination and cooperation among them. Communication with a mobile agent requires the ability to track it. In the thesis, we propose several distributed mechanisms for tracking (or locating) mobile agents. We first propose a method based on distributed hashing table (DHT) to locate mobile agents efficiently in a large-scale network. The idea is to treat mobile agents and hosts similarly to the way peers are treated in a peer-to-peer system, and to implement a lookup protocol that is based on that principle, while taking care of the mobile nature of the agents. We then introduce a new locating mechanism. Our technique is based on appropriate delays that the mobile agents must perform while moving on the network so to facilitate its tracking, should it be needed. The searching agent computes a particular searching path that will guarantee the tracking within one traversal of the network. We indicate various strategies, and we perform experiments for computing the searching path and for comparing their performances in synchronous and asynchronous settings, in arbitrary and specific topologies...|$|R
40|$|The paper {{discusses}} two implementation {{options for}} a Digital Video Library, a repository used for archiving, ac-cessing, and browsing of video medical records. Two crucial {{issues to be}} decided on are a video compression format and a video streaming platform. The paper presents numerous decision factors {{that have to be}} taken into account. The compression formats being compared are DICOM as a format representative for medical applications, both MPEGs, and several new formats targeted for an IP net-working. The comparison includes transmission rates sup-ported, compression rates, and at least options for controlling a compression process. The second part of the paper presents the ISDN technique as a solution for provi-sioning of tele-consultation services between medical parties that are accessing resources uploaded to a digital video library. There are several backbone techniques (like corpor-ate LANs/WANs, leased lines or even radio/satellite links) available, however, the availability of network resources for hospitals was the prevailing choice criterion pointing to ISDN solutions. Another way to provide access to the Digital Video Library is based on radio frequency domain solu-tions. The paper describes possibilities of both, wireless and cellular network’s data transmission service {{to be used as a}} medical video <b>server</b> <b>transport</b> layer. For the cellular net-work based solution two communication techniques are used: Circuit Switched Data and Packet Switched Data...|$|R
40|$|BioHealthBase NetCTL data {{production}} pipeline {{is composed}} of three JAVA scripts to semi-automate the submission of sequences to the NetCTL web server at the Technical Danish University (TDU) center for CTL epitope predictions. The NetCTL method integrates prediction of peptide MHC class I binding, proteasomal C terminal cleavage and TAP <b>transport</b> efficiency. The <b>server</b> allows for predictions of CTL epitopes restricted to 12 MHC class I supertype. MHC class I binding and proteasomal cleavage is performed using artificial neural networks. TAP transport efficiency is predicted using weight matrix. 3. Input Data Preparation Influenza sequences in BioHealthBase are partitioned into group of files each containing 100 FASTA sequences in it. This {{is because of the}} limit set by TDU server for a maximum of 100 sequences in each file submitted. These files were then submitted to the NetCTL web server sing a Java script. This Java script allows one to submit a controlled number of input files to TDU server because of the limit of 50 submitted files per day. Each submission with PostData is specific to one epitope supertype. BioHealthBase currently provides data for five supertypes: A 2, A 3, A 24, B 44 and B 7 4. Output Data, Processing and Display All the data returned from TDU server are grouped by MHC supertypes. These data are then post-processed to insure the data conformity for database loading. The NetCTL-predicted epitopes are displayed in both Gene Details page and the genome browser on BioHealthBase website for the public use. A database search tool called “Epitope Search” is provided for users to retrieve epitopes corresponding to specified genes. Here is an example of NetCTL Epitope Summary table in Gene Details page...|$|R
40|$|In {{this thesis}} we propose {{and explore the}} Overlay Distribution Network (ODN) concept as an alternative, more cost {{effective}} and more flexible deployment approach for Content Distribution Networks (CDN). Currently CDNs are established by deploying a dedicated server infrastructure spanning the Internet, which is prohibitively expensive. With ODN, we propose to lease <b>transport</b> and <b>server</b> resources from server and network providers to establish an overlay network of virtual servers and connecting virtual links, which is then used to deliver content {{in a similar manner}} to traditional CDNs. Such deployment strategy is expected to be less costly and could allow content network topologies and capacities to be adjusted on-the-fly according to demand. The aim of this thesis is to address the research issues that arise {{as a result of this}} new deployment approach. One of the major issues considered is ODN provisioning, which involves topology planning, resource dimensioning and content replica placement. ODN provisioning is significantly different from and more complex than its traditional CDN counterpart due to the nature of the shared infrastructure environment. ODN provisioning would not only have a different optimization objective but also require topology planning, resource dimensioning and content replica placement problems, which are currently addressed independently, to be addressed jointly. To address this ODN provisioning challenge, we develop a provisioning framework that could be used to formulate ODN provisioning models that meet these new requirements. ODN provisioning models for a number of key content distribution applications, including web content distribution, pay-per-view content distribution and live streaming multimedia distribution, are then developed and studied. The models are formulated as mixed integer linear programming optimizations that aim to determine the optimal ODN topology, capacity and content replication pattern, which deliver satisfactory service performance at the minimum cost or maximum profit. As the above ODN provisioning models belong to the NP (Non-deterministic Polynomial) class of problems, they have extremely high complexity and cannot be solved efficiently for realistically large networks. To tackle this difficulty we develop heuristics that aim to find near optimal solutions with less computation effort. Experimental results show that our proposed heuristics are efficient and able find solutions reasonably close to the optimal (within 36 % for the web ODN provisioning problem and 20 % for the pay-per-view and live streaming multimedia ODN provisioning problems). Our study also demonstrates that provisioning could become significantly inefficient if the heuristics were not designed properly. For example in the web content ODN provisioning problem, a greedy heuristic adapted from existing CDN replica placement heuristics could produce ODN topologies up to 2. 5 times more costly compared to a Lagrangian heuristic designed based the problem formulation structure. As part of the ODN provisioning study, we also explore the use of content clustering within the ODN provisioning process. By grouping similar content items together into clusters, which are then considered as a single item during provisioning, content clustering would help reduce provisioning complexity and allow the provisioning models to handle problems with significantly larger number of content items. We show that clustering methods previously developed for CDNs do not work well in the ODN environment. Thus we propose a new hierarchical clustering scheme, where content items are clustered based on first delivery resource requirements and then spatial demand distribution. Experimental results demonstrate that this clustering scheme has significant performance improvements over the existing ones. In this thesis we also look into the future and study the ability of the ODN to support applications that require QoS network paths among servers. To enable better support for such applications, we propose to enhance the ODN architecture with switching capabilities that allow ODN owners to control the flow of traffic within their ODN backbone. We examine and demonstrate the benefits of such capabilities using both quantitative and qualitative studies and then develop a shared switch architecture that could be used to provide such support in a shared infrastructure environment...|$|R

