91|590|Public
5000|$|The FY14 DOT&E report {{assessed}} the Mk 54 (BUG) torpedo as not operationally effective in its intended role. [...] "During operationally challenging and realistic scenarios, the Mk 54 (BUG) demonstrated below <b>threshold</b> <b>performance</b> and exhibited {{many of the}} same failure mechanisms observed during the FY 2004 initial operational testing". Shortfalls were also identified with the employing platforms’ tactics and tactical documentation, and interoperability problems with some platform fire control systems.|$|E
50|$|An HPI sensor reports status {{information}} about the hardware being monitored {{through a set of}} up to 15 individual bits, called Event States. Each Event State can be individually asserted or deasserted, and when an Event State changes, asynchronous events can be generated to report this to an HPI user. The interpretation of each Event State can vary according to a defined Sensor Category (e.g., <b>threshold,</b> <b>performance,</b> presence, severity), or can be unique to a specific Sensor. Sensors in the threshold category have additional capabilities. Threshold sensors report when a value being monitored is above or below configurable threshold values. Up to three upper thresholds and three lower thresholds may be defined for Minor, Major and Critical deviations from the norm in either direction.|$|E
50|$|Two-point {{discrimination}} (2PD) is {{the ability}} to discern that two nearby objects touching the skin are truly two distinct points, not one. It is often tested with two sharp points during a neurological examination and is assumed to reflect how finely innervated an area of skin is.In clinical settings, two-point discrimination is a widely used technique for assessing tactile perception. It relies on the ability and/or willingness of the patient to subjectively report what they are feeling and should be completed with the patient’s eyes closed. The therapist may use calipers or simply a reshaped paperclip to do the testing. The therapist may alternate randomly between touching the patient with one point or with two points on the area being tested (e.g. finger, arm, leg, toe). The patient is asked to report whether one or two points was felt. The smallest distance between two points that still results in the perception of two distinct stimuli is recorded as the patient's two-point <b>threshold.</b> <b>Performance</b> on the two extremities can be compared for discrepancies. Although the test is still commonly used clinically, it has been roundly criticized by many researchers as providing an invalid measure of tactile spatial acuity, and several highly regarded alternative tests have been proposed to replace it.|$|E
40|$|<b>Performance</b> <b>thresholds</b> are {{commonly}} used in executive compensation contracts. We examine the contractual nonlinearity associated with <b>performance</b> <b>thresholds</b> and its incentive implications. Incorporating a <b>performance</b> <b>threshold</b> into a standard principal-agent model of a linear contract, we show that pay schemes using a <b>performance</b> <b>threshold</b> are optimal. By truncating a linear scheme at poor <b>performance,</b> the <b>threshold</b> mitigates agency {{costs associated with the}} downside risk of production. Examining CEO compensation data, we find evidence of the role of <b>performance</b> <b>thresholds.</b> As a consequence of under-threshold performance, the tobit estimator is shown to increase pay-performance sensitivity, notably improving upon the standard OLS estimator. ...|$|R
40|$|This {{chapter is}} {{concerned}} with the design, fabrication, and characterization of deeply-etched III–V semiconductor ring lasers (SRL) whose radius is scaled down to sub- 10 [*]µm values. The laser <b>threshold</b> <b>performances</b> are experimentally investigated for different laser geometries. Compact racetrack-shape SRLs are shown to exhibit robust unidirectional bistable operation with direct extinction ratio and single mode rejection ratio of 20 and 30 dB, respectively...|$|R
40|$|This paper {{presents}} {{an analysis of}} the <b>threshold</b> region <b>performance</b> of deterministic maximum likelihood direction of arrival estimation using sensor arrays. The threshold effect is caused by outliers and thus is not captured by standard analysis tools such as the Cramer-Rao bound and Taylor expansions, since these are local in nature. The work presented in this paper provides a global analysis {{that can be used to}} predict the <b>threshold</b> region <b>performance</b> of the maximum likelihood estimator with high accuracy. It extends previous results from the author on the single source to the multiple sources case...|$|R
40|$|The error floor {{performance}} of finite-length irregular low-density parity-check (LDPC) codes {{can be very}} poor if code degree distributions are chosen to optimize the <b>threshold</b> <b>performance.</b> In this paper we show that by constraining the optimization process, a balance between threshold and error floor performance can be obtained. The resulting degree distributions give the best <b>threshold</b> <b>performance</b> subject to some minimum requirement on the error floor...|$|E
40|$|Abstract For {{direction}} of arrival (DOA) estimation in the threshold region, {{it has been}} shown that use of Random Matrix Theory (RMT) eigensubspace estimates provides significant improvement in MUSIC performance. Here we investigate whether these RMT methods can also improve the <b>threshold</b> <b>performance</b> of unconditional (stochastic) maximum likelihood DOA estimation (MLE) ...|$|E
40|$|This letter {{presents}} a new {{formulation of the}} extended Kalman filter (EKF) for use in frequency tracking. A brief summary of previous EKF approaches is given and the new approach detailed. Simulation studies of the standard and new algorithms show that a significant improvement in tracking and <b>threshold</b> <b>performance</b> is achieved...|$|E
40|$|Abstract—In non {{destructive}} testing by radiography, a perfect knowledge of the weld defect shape is {{an essential step to}} appreciate the quality of the weld and make decision on its acceptability or rejection. Because of the complex nature of the considered images, and in order that the detected defect region represents the most accurately possible the real defect, the choice of thresholding methods must be done judiciously. In this paper, performance criteria are used to conduct a comparative study of four non parametric histogram thresholding methods for automatic extraction of weld defect in radiographic images. Keywords—Radiographic images, non parametric methods, histogram <b>thresholding,</b> <b>performance</b> criteria. T I...|$|R
40|$|Functional Magnetic Resonance Imaging is a noninvasive tool used {{to study}} brain function. Detecting {{activation}} is challenged by many factors, {{and even more so}} in low-signal scenarios that arise in the performance of high-level cognitive tasks. We provide a fully automated and fast adaptive smoothing and thresholding (FAST) algorithm that uses smoothing and extreme value theory on correlated statistical parametric maps for <b>thresholding.</b> <b>Performance</b> on simulation experiments spanning a range of low-signal settings is very encouraging. The methodology also performs well in a study to identify the cerebral regions that perceive only-auditory-reliable and only-visual-reliable speech stimuli as well as those that perceive one but not the other...|$|R
40|$|This paper {{considers}} {{the problem of}} detecting the support (sparsity pattern) of a sparse vector from random noisy measurements. Conditional power of {{a component of the}} sparse vector is defined as the energy conditioned on the component being nonzero. Analysis of a simplified version of orthogonal matching pursuit (OMP) called sequential OMP (SequOMP) demonstrates the importance of knowledge of the rankings of conditional powers. When the simple SequOMP algorithm is applied to components in nonincreasing order of conditional power, the detrimental effect of dynamic range on <b>thresholding</b> <b>performance</b> is eliminated. Furthermore, under the most favorable conditional powers, the performance of SequOMP approaches maximum likelihood performance at high signal-to-noise ratio. Comment: 13 page...|$|R
30|$|In the {{following}} sessions, coherence detection thresholds with varying parameters were assessed {{in order to}} test the influence of these parameters on the seal’s performance within experiments and to assess if the seal’s performance increases with experience over experiments. The seal was trained until reaching a low <b>threshold</b> <b>performance</b> in this coherent motion detection experiment.|$|E
40|$|A new entropy-based {{thresholding}} {{is presented}} in this paper. As a result, a gray [eve/minimizing {{the difference between the}} entropy of the object and that of the background will be the desired <b>threshold.</b> <b>Performance</b> of the proposed method has been tested Oil some ultrasonic images. The results obtained from the method proposed by Kapur et al. are also given for comparison. 1...|$|E
40|$|Procedures for {{reducing}} the computational burden of current models of spatial vision are described, the simplifications being consistent with the prediction of the complete model. A method for using pattern-sensitivity measurements to estimate the initial linear transformation is also proposed {{which is based on}} the assumption that detection performance is monotonic with the vector length of the sensor responses. It is shown how contrast-threshold data can be used to estimate the linear transformation needed to characterize <b>threshold</b> <b>performance...</b>|$|E
40|$|From a {{positive}} viewpoint, {{business as usual}} decisions unlike other de-cisions do not generally require further justi¯cations. And when a justi¯ca-tion is required, decision makers are often judged by results: if the resulting performance is above a pre-de¯ned threshold this is ¯ne; otherwise this is considered a failure. This paper explores the cost and bene¯t of integration under such a Simonian working of reviewing processes. We identify the fol-lowing cost of non-integration: bargaining ine±ciencies arise because the <b>threshold</b> <b>performances</b> that determine the criterion of success need not be adapted to every single negotiation. And the main cost of integration is its exposure to opportunism because the opportunism in one division has spillover e®ects in the other divisions of integrated organizations...|$|R
40|$|Abstract A {{performance}} analysis procedure that analyses {{the properties of}} a class of iterative image thresholding algorithms i described. The image under consideration is modeled as consisting of two maximum-entropy primary images, {{each of which has}} a quasi-Gaussian probability density function. Three iterative thresholding algorithms identified to share a common iteration architecture are employed for thresholding 4595 synthetic images and 24 practical images. The average performance characteristics including accuracy, stability, speed and consistency are analysed and compared among the algorithms. Both analysis and practical thresholding results are presented. Copyright © 1996 Pattern Recognition Society. Published by Elsevier Science Ltd. Image <b>thresholding</b> <b>Performance</b> analysis Image segmentation Maximum entropy Iterative algorithm An image is a collection of picture elements (pixels) representing some scene of interest for an observer. In practice, many images are a representation f scene...|$|R
40|$|This study {{investigated}} typical performance of approximation algorithms known as belief propagation, greedy algorithm, and linear-programming relaxation for maximum coverage problems on sparse biregular random graphs. After using the cavity method for a corresponding hard-core lattice [...] gas model, {{results show that}} two distinct thresholds of replica-symmetry and its breaking exist in the typical <b>performance</b> <b>threshold</b> of belief propagation. In the low-density region, the superiority of three algorithms {{in terms of a}} typical <b>performance</b> <b>threshold</b> is obtained by some theoretical analyses. Although the greedy algorithm and linear-programming relaxation have the same approximation ratio in worst-case performance, their typical <b>performance</b> <b>thresholds</b> are mutually different, indicating the importance of typical performance. Results of numerical simulations validate the theoretical analyses and imply further mutual relations of approximation algorithms. Comment: 10 pages, 5 figure...|$|R
40|$|Abstract-The <b>threshold</b> <b>performance</b> of deep-space teleme-try is {{characterized}} for four turbo codes. The mathematical models given here {{are based on}} simulations that account for imperfect carrier synchronization. The required &/No de-pends on the code, the threshold frame error rate, the bit rate, and the signal-to-noise ratio and bandwidth of the carrier syn-chronization loop. The performance models are valid for co-herent detection of binary phase-shift keyed (BPSK) teleme-try. For residual-carrier tracking, these mathematical models are {{used to calculate the}} optimum modulation. 1...|$|E
40|$|This {{contribution}} {{focuses on}} error-correcting codes for analog signals. Generally, error-correcting codes exhibit a threshold effect, which degrades {{performance at the}} {{low signal to noise}} ratio (SNR) range. We propose two error correction alternatives based on chaotic dynamical systems, which improve <b>threshold</b> <b>performance</b> relative to previously proposed methods. The first one, which is based on principles of diversity, exhibits a very simple decoder. The second alternative, which is based on a low-density-parity-check (LDPC) code, attains a significantly lower threshold. Theoretical analysis and simulations are provided...|$|E
40|$|In {{simultaneous}} masking, {{performance on}} a foveally presented target is impaired {{by one or more}} flanking elements. Previous studies have demonstrated strong effects of the grouping of the target and the flankers on the strength of masking (e. g., Malania, Herzog & Westheimer, 2007). These studies have predominantly examined performance by measuring offset discrimination thresholds as a measure of performance, and it is therefore unclear whether other measures of performance provide similar outcomes. A recent study, which examined the role of grouping on error rates and response times in a speeded vernier offset discrimination task, similar to that used by Malania et al. (2007), suggested a possible dissociation between the two measures, with error rates mimicking <b>threshold</b> <b>performance,</b> but response times showing differential results (Panis & Hermens, 2014). We here report the outcomes of three experiments examining this possible dissociation, and demonstrate an overall similar pattern of results for error rates and response times across a broad range of mask layouts. Moreover, the pattern of results in our experiments strongly correlates with <b>threshold</b> <b>performance</b> reported earlier (Malania et al., 2007). Our results suggest that outcomes in a simultaneous masking paradigm do not critically depend on the outcome measure used, and therefore provide evidence for a common underlying mechanism...|$|E
30|$|The {{organization}} {{of this paper}} is as follows. Section 2 discusses quantization techniques. After some preliminaries (Section 3), the sibling points and the quantile helper data are treated in Section 4. Section 5 discusses the optimal reconstruction <b>thresholds.</b> The <b>performance</b> analysis in the Gaussian model is presented in Section 6.|$|R
3000|$|... {{control a}} weight on images with smooth background. Of course the two <b>thresholds</b> affect the <b>performance</b> of the algorithm, for example, the smaller [...]...|$|R
40|$|International audienceWe study two {{nonlinear}} {{methods for}} statistical linear inverse problems when the operator is not known. The two constructions combine Galerkin regularization and wavelet <b>thresholding.</b> Their <b>performances</b> {{depend on the}} underlying structure of the operator, quantified by an index of sparsity. We prove their rate-optimality and adaptivity properties over Besov classes...|$|R
40|$|The primary {{experimental}} {{objective of}} Project Echo was {{the transmission of}} radio communications between points on the earth by reflection from the balloon satellite. Owing to the large path losses from transmitter to receiver via the satellite, a wide-band frequency modulation technique was used in which bandwidth was traded for signal-to-noise ratio. This paper describes the FM receiving demodulators employed. Negative feedback applied to the local oscillator reduces the FM modulation index in the receiver IF amplifiers, resulting in <b>threshold</b> <b>performance</b> superior to that of conventional FM receivers...|$|E
40|$|Literature data {{on light}} {{detection}} by cone and rod vision at absolute threshold are analysed in order (1) {{to decide whether}} the <b>threshold</b> <b>performance</b> of dark-adapted cone vision can, like that of rod vision, be consistently explained as limited by noise from a “dark light”; (2) to obtain comparable estimates of the dark noise and dark light of (foveal) cones and (peripheral) rods. The dark noise was estimated by a maximum-likelihood procedure from frequency-of-seeing data and compared with the dark light derived from increment-threshold functions. In both cone and rod vision, the estimated dark noise coincides with Poisson fluctuations of the estimated dark light if 17 % (best estimate) of λmax-quanta incident at the cornea produce excitations. At that fraction of quanta exciting, dark lights are equivalent to 112 isomerisations per sec in each foveal cone and 0. 011 isomerisations per sec in each rod. It is concluded that (1) the <b>threshold</b> <b>performance</b> of dark-adapted cone as well as rod vision can be consistently described as noise-limited, but not by postulating a multi-quantum coincidence requirement for single receptors; (2) die underlying intrinsic activity in both the cone and the rod system is light-like as regards correspondence between noise effect and background adaptation effect. One possibility is that this activity is largely composed of events identical to the single-photon response, originating in the visual pigment, in cones as well as in rods...|$|E
40|$|Methods for {{developing}} more compact femtosecond Cr:LiSAF laser sources are examined. By careful modeling {{of the low}} <b>threshold</b> <b>performance</b> and intracavity dispersion characteristics of these cavities, a highly asymmetric z-cavity design with a single prism for dispersion compensation is developed. Transform-limited pulses as short as 113 fs and modelocked output powers up to 20 mW are demonstrated for less than 110 mW of laser-diode pump power. The complete laser system (including the laser diode pump system and drivers) has a footprint of 21. 5 × 28 cm 2, {{about the size of}} a sheet of US letter or A 4 pape...|$|E
40|$|The {{influence}} of training status on the fatigue <b>threshold</b> and <b>performance</b> during high intensity exercise were studied in six endurance trained cyclists and seven recreationally students during 1 min anaerobic TT and 10 min aerobic TT. The peripheral fatigue was expressed as {{reduction in the}} potentiated single quadriceps twitch force (Qtw-pot) from baseline from pre to post cycling exercise measured with supra maximal femoral nerve electro stimulation. The central fatigue was estimated from voluntary activation (VA) using the superimposed twitch technique. The central motor drive (CMD) was estimated from quadriceps EMG (mean RMS normM). On different days, both groups performed two maximal cycle tests in the 1 min TT and the 10 min TT where each TT was repeated two times as test 1 and test 2 with neuromuscular assessment between tests. The reduction in Qtw-pot from baseline was not significant different from test 1 to test 2 (P< 0. 07) and indicate the fatigue threshold. The training status influence on the fatigue <b>threshold</b> and <b>performance</b> was the same for anaerobic 1 min TT and aerobic 10 min TT. The endurance-trained group had a lower reduction in Qtw-pot from baseline that means a lower fatigue level on the fatigue threshold (P< 0. 01), a higher CMD (P< 0. 05) and higher power output (P< 0. 01) than the recreational group. The 1 min TT had a higher reduction in Qtw-pot and {{a higher level of}} fatigue on the fatigue threshold than the 10 min TT. The central fatigue increase from test 1 to test 2 and had higher influence on the aerobic 10 min TT than the anaerobic 1 min TT. The results show that quadriceps fatigue related to biochemical changes in the muscle and central fatigue related to decrease in VA was responsible for the fatigue threshold and decrease in performance. These findings show that training status have an influence on the fatigue <b>threshold</b> and <b>performance</b> where higher training status leads to higher fatigue <b>threshold</b> and <b>performance...</b>|$|R
40|$|In this paper, the RD-OPT {{algorithm}} for optimizing Discrete Cosine Transform quantization tables [RL 95] {{is extended}} to incorporate global <b>thresholding.</b> <b>Performance</b> gains are possible by zeroing off some DCT coefficients in DCTbased image compression. We describe a global thresholding scheme, {{in which the}} zeroing thresholds for coefficients can be arbitrarily different from those determined by the quantization table. Unlike local thresholding [CR 95], the zeroing decisions are not made separately for each image block. This simplifies the use of thresholding, is easier to optimize and is almost as effective as local thresholding. 1 Introduction The Discrete Cosine Transform (DCT) [ANR 74] {{lies at the heart}} of most commonly used lossy image and video compression schemes [PM 93, MP 91]. The extent of compression achieved depends upon the coarseness of quantization of the transform coefficients. Most DCT-based compression schemes use uniform scalar quantization of the coefficients, as determin [...] ...|$|R
40|$|Freight {{cars are}} an {{important}} and {{integral part of the}} railroad system of the United States and Canada. These cars are equipped with a component, known as "truck", which act as an interface between the car body and the truck, and as a suspension system. A high level of expense is incurred on truck maintenance. In particular, two significant costs may be incurred due to the difficulty in knowing the true truck condition, especially the internal truck condition. The research presented by this thesis attempts to address the problem by applying two statistical forecasting techniques, discrete choice method and <b>performance</b> <b>threshold</b> method, to develop a more cost-effective inspection approach. The techniques presented were also applied to a case study. In terms of underlying behavioral theory, the <b>performance</b> <b>threshold</b> method is considered stronger than discrete choice method. From a computational point of view, <b>performance</b> <b>threshold</b> and discrete choice methods are acceptable in that well-writte...|$|R
40|$|It is {{now well}} {{understood}} that ℓ_ 1 minimization algorithm {{is able to}} recover sparse signals from incomplete measurements and sharp recoverable sparsity thresholds have also been obtained for the l 1 minimization algorithm. In this paper, we investigate a new iterative reweighted ℓ_ 1 minimization algorithm and showed that the new algorithm can increase the sparsity recovery threshold of ℓ_ 1 minimization when decoding signals from relevant distributions. Interestingly, we observed that the recovery <b>threshold</b> <b>performance</b> of the new algorithm depends on the behavior, more specifically the derivatives, of the signal amplitude probability distribution at the origin...|$|E
40|$|The Assured Forwarding (AF) based {{service in}} a Differentiated Services (Diffserv) network fails to provide {{bandwidth}} assurance to aggregates in some circumstances. Several intelligent marking mechanisms {{have been proposed}} in the literature to improve bandwidth assurance for aggregates using the knowledge gathered at the ingress nodes. In this paper, we apply a control theoretic approach to this problem. We design a nonlinear Proportional-Integral (NPI) controller, called NPI-ACT, for adapting the CIR <b>threshold.</b> <b>Performance</b> results using extensive simulations demonstrate significant improvement using NPI-ACT in achieving bandwidth assurance {{over a wide range}} of network conditions compared to earlier mechanisms proposed in the literature...|$|E
40|$|Evidence is accumulating that {{handwriting}} has {{an important}} role in written composition. In particular, handwriting automaticity appears to relate to success in composition. This relationship has been little explored in British contexts and we currently have little idea of what <b>threshold</b> <b>performance</b> levels might be. In this paper, we report on two linked studies that attempted to identify performance levels in handwriting automaticity for children at two ages, below which their success in writing composition might be considered to be at risk. We conclude by suggesting interpolated levels for children at different ages, although we recognise the tentative nature of these suggestions. Full text HTML PD...|$|E
50|$|QMU {{focuses on}} {{quantification}} of {{the ratio of}} design margin to model output uncertainty. The process begins with {{the identification of the}} key <b>performance</b> <b>thresholds</b> for the system, which can frequently be found in the systems requirements documents. These thresholds (also referred to as performance gates) can specify an upper bound of performance, a lower bound of performance, or both in the case where the metric must remain within the specified range. For each of these <b>performance</b> <b>thresholds,</b> the associated <b>performance</b> margin must be identified. The margin represents the targeted range the system is being designed to operate in to safely avoid the upper and lower performance bounds. These margins account for aspects such as the design safety factor the system is being developed to as well as the confidence level in that safety factor. QMU focuses on determining the quantified uncertainty of the simulation results as they relate to the <b>performance</b> <b>threshold</b> margins. This total uncertainty includes all forms of uncertainty related to the computational model as well as the uncertainty in the threshold and margin values. The identification and characterization of these values allows the ratios of margin-to-uncertainty (M/U) to be calculated for the system. These M/U values can serve as quantified inputs that can help authorities make risk-informed decisions regarding how to interpret and act upon results based on simulations.|$|R
40|$|We study two {{nonlinear}} {{methods for}} statistical linear inverse problems when the operator is not known. The two constructions combine Galerkin regularization and wavelet <b>thresholding.</b> Their <b>performances</b> {{depend on the}} underlying structure of the operator, quantified by an index of sparsity. We prove their rate-optimality and adaptivity properties over Besov classes. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
50|$|The primary caveat of overvolting is {{increased}} heat: the power dissipated by a circuit increases with {{the square of}} the voltage applied, so even small voltage increases significantly affect power. At higher temperatures, transistor performance is adversely affected, and at some <b>threshold,</b> the <b>performance</b> reduction due to the heat exceeds the potential gains from the higher voltages. Overheating and damage to circuits can occur very quickly when using high voltages.|$|R
