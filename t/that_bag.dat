182|1807|Public
25|$|The {{vertices}} {{in a bag}} can {{be thought}} of as the terminals of a subgraph of G, represented by the subtree of the tree decomposition descending from <b>that</b> <b>bag.</b> When G has bounded treewidth, it has a tree decomposition in which all bags have bounded size, and such a decomposition can be found in fixed-parameter tractable time. Moreover, it is possible to choose this tree decomposition so that it forms a binary tree, with only two child subtrees per bag. Therefore, it is possible to perform a bottom-up computation on this tree decomposition, computing an identifier for the equivalence class of the subtree rooted at each bag by combining the edges represented within the bag with the two identifiers for the equivalence classes of its two children.|$|E
500|$|Many leads were {{followed}} {{that appeared to}} match {{the circumstances of the}} case. A woman witnessed throwing a garbage bag over a bridge was later questioned by police, but it was found that <b>that</b> <b>bag</b> contained only spoiled fruit. Other individuals stated they had seen toddlers resembling the Jane Doe, one at a store and another at a playground, seen with a woman clothed in a [...] "burka." [...] Investigators were unable to find any trace of the girl at the store through information gathered by security cameras, and the subject at the playground has yet to be located. A man stated he believed he had seen the toddler in November 2014 at a laundromat with a [...] "heavyset" [...] and [...] "dirty blond" [...] woman pushing the child in a stroller.|$|E
500|$|On May 2, 1969, the Experience {{performed}} at Cobo Hall in Detroit. According to Mitchell, {{while they were}} {{getting ready for the}} show the band and their entourage were informed about a possible drug bust planned for the following day. The group's road crew warned everyone to take precautions against any potential for drugs to be planted on them. Mitchell responded by wearing a suit without pockets and not wearing any underwear. Tour managers Gerry Stickells and Tony Ruffino expressed their concern to Hendrix and asked him if he had any drugs on him to which he replied: [...] "No." [...] After arriving in Toronto, he was awoken by concert promoter Ron Terry who told him: [...] "Whatever you got in <b>that</b> <b>bag,</b> get rid of it." [...] Terry then took him into the plane's bathroom and dumped anything that might be mistaken for illegal drugs into the toilet. Terry commented: [...] "I thought he was clean." ...|$|E
40|$|Abstract This paper {{introduces}} bagging in the evidence-theoretic K-nearest neighbor rule (K-NN). It {{is known}} <b>that</b> <b>bagging</b> decreases {{the variability of}} classifiers, so the main idea here is to build stable belief structures associated to query samples, before decisions are made. In order to compare bagged K-NN with the classical algorithm in a precisely controlled environment, data sets were generated according to known distributions. Results show <b>that</b> <b>bagging</b> improves classification, especially for ambiguous cases and outliers. Moreover, bagged belief structures give pignistic probabilities closer to the a posteriori class probabilities than the original method...|$|R
40|$|Theoretical and {{experimental}} analyses of <b>bagging</b> indicate <b>that</b> {{it is primarily}} a variance reduction technique. This suggests <b>that</b> <b>bagging</b> should be applied to learning algorithms tuned to minimize bias, even {{at the cost of}} some increase in variance. We test this idea with Support Vector Machines (SVMs) by employing out-of-bag estimates of bias and variance to tune the SVMs. Experiments indicate <b>that</b> <b>bagging</b> of low-bias SVMs (the "lobag" algorithm) never hurts generalization performance and often improves it compared with well-tuned single SVMs and to bags of individually well-tuned SVMs...|$|R
40|$|English {{definite}} descriptions {{like the}} 9 kg <b>that</b> this <b>bag</b> weighs exhibit two properties {{to be discussed}} in this paper: they entail <b>that</b> the <b>bag</b> weighs exactly 9 kg, and they also presuppose <b>that</b> the <b>bag</b> weighs exactly 9 kg. This is normally explained by analyzing the definite article as having presuppositions of existence and maximality. The goal of thi...|$|R
6000|$|... "There was twenty-three pounds freight {{money in}} <b>that</b> <b>bag</b> {{when we left}} London," [...] said the skipper, finding his voice at last.|$|E
6000|$|... “Nothing important,” Churchill {{echoed in}} a faint, small voice. Then {{he spoke with}} decision: “Louis, what’s in <b>that</b> <b>bag?</b> I want to know.” ...|$|E
6000|$|... "Nothing important," [...] Churchill {{echoed in}} a faint, small voice. Then {{he spoke with}} decision: [...] "Louis, what's in <b>that</b> <b>bag?</b> I want to know." ...|$|E
5000|$|Traditional climbers or adventurers {{may argue}} <b>that</b> peak <b>bagging</b> devalues the {{experience}} of climbing in favour of the achievement of reaching an arbitrary point on a map; <b>that</b> <b>bagging</b> reduces climbing {{to the status of}} stamp collecting or train spotting; or that is seen as obsessive and beside the point. For example, in explaining why he chose to remove some minor peaks from his guidebook, climber Steve Roper wrote: ...|$|R
50|$|Hooda {{was part}} of the Haryana team <b>that</b> <b>bagged</b> gold at the Senior National level tournament in Patna in the 2014. He made his debut for the {{national}} team at the 2016 South Asian Games, where his team won a gold medal.|$|R
5000|$|Jumper: A <b>bag</b> <b>that</b> strikes another <b>bag</b> on {{the board}} causing it to jump up into the cornhole.|$|R
6000|$|... "You {{go and get}} <b>that</b> <b>bag</b> of mine ready," [...] said Lapham sullenly. [...] "I guess I {{can take}} care of myself. And Milton K. Rogers too," [...] he added.|$|E
6000|$|Jimmy Skunk chuckled. [...] "One more question, Unc' Billy," [...] said he. [...] "Did {{you ever}} know {{me to pick}} a quarrel and use <b>that</b> <b>bag</b> of scent without being attacked?" ...|$|E
6000|$|... "Twenty-five francs a day {{for twenty}} days," [...] {{continued}} Mr. Holiday, [...] "is five hundred francs. Bring me <b>that</b> <b>bag</b> of gold, Rollo, out of my secretary. Here is the key." ...|$|E
50|$|It is {{expected}} <b>that</b> self-service <b>bag</b> drop {{will be much}} more common.|$|R
50|$|Maga Khans Marked, a shop <b>that</b> sells <b>bags,</b> hats, {{and other}} merchandise.|$|R
25|$|Investigators {{discovered}} <b>that</b> a <b>bag</b> {{had been}} routed onto PA103 via the interline baggage system at Frankfurt, {{from the station}} and approximate time at which bags were unloaded from flight KM180 from Malta. Although documentation for flight KM180 indicated <b>that</b> all <b>bags</b> on <b>that</b> flight were accounted for, the court inferred <b>that</b> the <b>bag</b> came from <b>that</b> flight and that it contained the bomb. In 2009, {{it was revealed that}} security guard Ray Manley had reported that Heathrow's Pan Am baggage area had been broken into 17 hours before flight 103 took off. Police lost the report and it was never investigated or brought up at trial.|$|R
6000|$|... "Mrs. Baker," [...] said Charley, with {{infinite}} gravity, [...] "if <b>that</b> <b>bag</b> SHOULD TUMBLE OFF A DOZEN TIMES {{between this}} and Laurel Hill, I'll hop down and {{pick it up}} myself." ...|$|E
6000|$|... "If {{she starts}} talking about <b>that</b> <b>bag,</b> head her off on to {{something}} else," [...] he said. [...] "I don't want her to get imagining trouble every time we leave the ranch." ...|$|E
6000|$|... "I don't {{understand}} it," [...] said Werner lamely. [...] "I left <b>that</b> <b>bag</b> {{there in}} the station master's care while I and the others went to get something to eat. Now my bag is gone." ...|$|E
40|$|Abstract — The use of bagging is {{explored}} {{to create}} an ensemble of fuzzy classifiers. The learning algorithm used was ANFIS (Adaptive Neuro-Fuzzy Inference Systems). We compare results from bagging to those of a single classifier using both crisp and fuzzy classifier combination methods. Results on 20 data sets show <b>that</b> <b>bagging</b> results in a significantly more accurate classifier. I...|$|R
50|$|At the Brixham Breakwater job he had {{a narrow}} escape: He found a small hollow under the {{breakwater}} and moved some bags of cement in to fill it. When he tried to swim out again he found <b>that</b> <b>bags</b> of cement carelessly slung from above had blocked his exit. He had to fight his way out with air running low.|$|R
5000|$|Let [...] be {{the set of}} {{positively}} labeled {{bags and}} let [...] be the set of negatively labeled bags, then the best candidate for the representative instance is given by , where the diverse density [...] under the assumption <b>that</b> <b>bags</b> are independently distributed given the concept [...] Letting [...] denote the jth instance of bag i, the noisy-or model gives: ...|$|R
6000|$|... "Pitch <b>that</b> <b>bag</b> in behind, porter," [...] ordered my new acquaintance. [...] "Now, then, Mr. Morton, if you're ready we'll be off. Your train's {{half an hour}} late, and Cis will be {{wondering}} what's become of us." ...|$|E
6000|$|... "I'll get <b>that</b> <b>bag</b> back, or I'll {{make the}} station master pay for it," [...] grumbled Gabe Werner, {{and then he}} and his cronies turned on their heels and walked back in the {{direction}} of the railroad station.|$|E
6000|$|... "You {{would better}} follow me, now," [...] said Miriam, {{who had taken}} some parcels from the wagon, [...] "and bring <b>that</b> <b>bag</b> into the pantry. I do not like Mike to come into {{our part of the}} house with his boots." ...|$|E
40|$|A {{common problem}} in out-of-sample {{prediction}} {{is that there}} are potentially many relevant predictors that individually have only weak explanatory power. We propose bootstrap aggre-gation of pre-test predictors (or bagging for short) as a means of constructing forecasts from multiple regression models with local-to-zero regression parameters and errors subject to pos-sible serial correlation or conditional heteroskedasticity. Bagging is designed for situations in which the number of predictors (M) is moderately large relative to the sample size (T). We show how to implement bagging in the dynamic multiple regression model and provide asymptotic justification for the bagging predictor. A simulation study shows <b>that</b> <b>bagging</b> tends to pro-duce large reductions in the out-of-sample prediction mean squared error and provides a useful alternative to forecasting from factor models when M is large, but much smaller than T. We also find <b>that</b> <b>bagging</b> indicators of real economic activity greatly redcues the prediction mean squared error of forecasts of U. S. CPI inflation at horizons of one month and one year...|$|R
40|$|Pattern {{recognition}} {{systems have}} been widely used in adversarial classification tasks like spam filtering and intrusion detection in computer networks. In these applications a malicious adversary may successfully mislead a classifier by “poisoning” its training data with carefully designed attacks. Bagging is a well-known ensemble construction method, where each classifier in the ensemble is trained on a different bootstrap replicate of the training set. Recent work has shown <b>that</b> <b>bagging</b> can reduce the influence of outliers in training data, especially if the most outlying observations are resampled with a lower probability. In this work we argue that poisoning attacks {{can be viewed as a}} particular category of outliers, and, thus, bagging ensembles may be effectively exploited against them. We experimentally assess the effectiveness of bagging on a real, widely used spam filter, and on a web-based intrusion detection system. Our preliminary results suggest <b>that</b> <b>bagging</b> ensembles can be a very promising defence strategy against poisoning attacks, and give us valuable insights for future research work...|$|R
40|$|Abstract. Bagging {{constructs}} an estimator {{by averaging}} predictors trained on bootstrap samples. Bagged estimates almost consistently improve {{on the original}} predictor. It is thus {{important to understand the}} reasons for this success, and also for the occasional failures. It is widely believed <b>that</b> <b>bagging</b> is effective thanks to the variance reduction stemming from averaging predictors. However, seven years from its introduction, bagging is still not fully understood. This paper provides experimental evidence supporting the hypothesis <b>that</b> <b>bagging</b> stabilizes prediction by equalizing the influence of training examples. This effect is detailed in two different frameworks: estimation on the real line and regression. Bagging’s improvements/deteriorations are explained by the goodness/badness of highly influential examples, in situations where the usual variance reduction argument is at best questionable. Finally, reasons for the equalization effect are advanced. They support that other resampling strategies such as half-sampling should provide qualitatively identical effects while being computationally less demanding than bootstrap sampling...|$|R
60|$|PETKOFF. You'd {{better go}} and slam <b>that</b> <b>bag,</b> too, down on Miss Raina's ice pudding! (This {{is too much}} for Nicola. The bag drops from his hands on Petkoff's corns, {{eliciting}} a roar of anguish from him.) Begone, you butter-fingered donkey.|$|E
6000|$|... "Run! Everybody run!" [...] was the yell {{from the}} Roxley contingent, {{and while the}} batter dropped his stick and sped toward first, the man on <b>that</b> <b>bag</b> legged it for second and the man on second rushed madly toward third.|$|E
6000|$|... "With <b>that</b> <b>bag</b> {{under his}} chin, of course," [...] replied Jimmy Skunk. [...] "Don't you see it's only {{when that is}} swelled out that he sings? It's a regular music bag. And I didn't know he had any such bag there at all." ...|$|E
40|$|Bagging is {{a device}} {{intended}} {{for reducing the}} prediction error of learning algorithms. In its simplest form, bagging draws bootstrap samples from the training sample, applies the learning algorithm to each bootstrap sample, and then averages the resulting prediction rules. Heuristically, the averaging process should reduce the variance component of the prediction error. This is supported by empirical evidence suggesting <b>that</b> <b>bagging</b> can indeed reduce prediction error {{and appears to be}} most e#ective for cart trees, which are highly unstable functions of the data. We study the e#ects of bagging for the simple class of U-statistics. While these do not describe cart trees, U-statistics have the advantage of admitting a complete and rigorous analysis. We find <b>that</b> <b>bagging</b> always inceases bias, but the e#ects on variance and mean squared error depend on the specifics of the U-statistic and its distribution. We also find a correspondence to order 1 /N 2 for bagging based on resampling with a [...] ...|$|R
40|$|Bootstrap {{aggregating}} or Bagging, {{introduced by}} Breiman (1996 a), {{has been proved}} to be effective to improve on unstable forecast. Theoretical and empirical works using classification, regression trees, variable selection in linear and non-linear regression have shown <b>that</b> <b>bagging</b> can generate substantial prediction gain. However, most of the existing literature on bagging has been limited to the cross sectional circumstances with symmetric cost functions. In this paper, we extend the application of bagging to time series settings with asymmetric cost functions, particularly for predicting signs and quantiles. We use quantile predictions to construct a binary predictor and the majority-voted bagging binary prediction. We show <b>that</b> <b>bagging</b> may improve the binary prediction in small sample, {{but it does not}} improve in large sample. Various bagging forecast combination weights are used such as equal weighted and Bayesian model averaging (BMA) weighted combinations. For demonstration, we present results from Monte Carlo experiments and from empirical applications using monthly S&P 500 and NASDAQ stock index returns...|$|R
5000|$|There {{is a key}} {{change to}} the way {{sleeping}} bags are labeled. A European criterion means <b>that</b> all sleeping <b>bags</b> adhering to the criterion will have the temperature ratings set by a standard laboratory test. This means <b>that</b> sleeping <b>bags</b> from different manufacturers will all have comparable temperatures.|$|R
