187|30|Public
50|$|The {{top-level}} {{part of this}} {{algorithm is}} general and does not use a particular existing type of <b>tree-search,</b> {{but it can be}} easily specialized to fit any type of non-distributed <b>tree-search.</b>|$|E
50|$|However, {{artificial}} intelligence algorithms that don't need evaluation functions, like the Monte Carlo <b>tree-search</b> algorithm, {{have no problem}} in playing this game. The Monte Carlo <b>tree-search</b> relies on random simulations of games {{in order to determine}} how good a position is instead of a positional evaluation, and is therefore able to accurately access how good a current position is. Therefore, computer implementations using these algorithms tend to outperform minimax solutions, and can consistently beat human opponents.|$|E
50|$|One of {{the more}} {{controversial}} examples is Big-Data processing. In applications like Google Search Engine, Facebook, YouTube, search needs to be optimized to keep waiting time inside a reasonable window. This could be achieved through the plain use of DTS, but other algorithms are used in place (for example data-hashing in SQL databases), or in conjunction (Facebook's Haystack algorithm groups parallel <b>tree-search,</b> data-hashing and memory-ordering/sorting).|$|E
40|$|The {{problem of}} devising a {{mechanical}} procedure for playing chess is fundamentally {{the problem of}} searching the very large move-tree associated with a chess position. This <b>tree-searching</b> problem is representative of a large class of problems. Consequently, we will first present briefly a general theory of <b>tree-searching</b> problems. This theory will be useful in clarifying the intention of our proposed research...|$|R
40|$|In MIMO {{communication}} systems maximum-likelihood (ML) decoding can be formulated as a <b>tree-searching</b> problem. This paper presents a <b>tree-searching</b> approach that combines {{the features of}} classical depth-first and breadth-first approaches to achieve close to ML performance while minimizing the number of visited nodes. A detailed outline of the algorithm is given, including the required storage. The effects of storage size on BER performance and complexity in terms of search space are also studied. Our result demonstrates that with a proper choice of storage size the proposed method visits 40 % fewer nodes than a sphere decoding algorithm at {{signal to noise ratio}} (SNR) = 20 dB and by an order of magnitude at 0 dB SNR...|$|R
40|$|A new {{strategy}} for high-resolution nucleotide sequence analysis has been developed. The strategy involves an exhaustive <b>tree-searching</b> algorithm which examines all possible combinations of short regions of sequence alignments, followed by culling of unsuitable sequence relationships. The new algorithm can detect sequence homologies invisible to existing algorithms, and {{is capable of}} detecting all possible sequence relationships...|$|R
50|$|A Variation {{can refer}} to a {{specific}} sequence of successive moves in a turn-based game, often used to specify a hypothetical future state of a game that is being played. Although the term is most commonly used {{in the context of}} Chess analysis, it has been applied to other games. It also is a useful term used when describing computer <b>tree-search</b> algorithms (for example minimax) for playing games such as Go or Chess.|$|E
5000|$|This class also {{includes}} various tree search algorithms, that view the elements as vertices of a tree, and traverse that tree in some special order. Examples {{of the latter}} include the exhaustive methods such as depth-first search and breadth-first search, as well as various heuristic-based search tree pruning methods such as backtracking and branch and bound. Unlike general metaheuristics, which at best work only in a probabilistic sense, many of these <b>tree-search</b> methods are guaranteed to find the exact or optimal solution, if given enough time. This is called [...] "completeness".|$|E
50|$|One of {{the more}} {{important}} limits of DTS {{is the fact that it}} requires a tree as input. Trees are a sub-instance of a data structure known as Graphs, which means every Graph can be converted into a tree. Although there currently exists no better way to search through trees than Korf-Ferguson's algorithm, each task has different particularities and in most cases, there will exist more efficient data structures to represent the problem and solve it than through <b>tree-search.</b> And so there exist instances of tree structures with cycles that cannot possibly be faster than a graph-search on the same structure with the same processing power.|$|E
5000|$|The {{principal}} variation {{refers to}} the particular variation {{that is the most}} advantageous to the current player, assuming each other player will respond with the move that best improves their own position. In other words, it is the [...] "best" [...] or [...] "correct" [...] line of play. In the context of <b>tree-searching</b> game Artificial Intelligence - in which this term is most common - it may also refer to the sequence of moves which is currently believed to be the most advantageous, but is not guaranteed due to the technical limitations of the algorithm.|$|R
40|$|Abstract — Coded {{transmission}} over channels introducing {{intersymbol interference}} in {{presence of a}} co-channel interferer and additive white Gaussian noise poses a problem for single antenna receivers. In this context we present a bidirectional listsequential (LISS) interference canceller which performs joint detection of the desired user and the interferer. The treebased LISS algorithm is derived from a decoding technique for convolutional codes with high memory, namely sequential decoding, and adapted to the soft-in/soft-out detection problem. Due to bidirectional <b>tree-searching</b> with two independent LISS algorithms, the proposed detector allows parallel processing with a controllable complexity even for high memory channels. I...|$|R
40|$|We {{describe}} a pipelined systolic architecture for implementing predictive <b>Tree-Searched</b> Vector Quantization (PTSVQ) for real- time image and speech coding applications. This architecture uses identical processors {{for both the}} encoding and decoding processes. the overall design is regular and the control is simple. Input data is processed {{at a rate of}} 1 pixel per clock cycle, which allows real-time processing of images at video rates. We implemented these processors using 1. 2 um CMOS technology. Spice simulations indicate correct operation at 40 MHz. Prototype version of these chips fabricated using 2 um CMOS technology work at 20 MHz...|$|R
5000|$|Several simple {{algorithms}} {{exist to}} construct a tree directly from pairwise distances, including UPGMA and neighbor joining (NJ), but these will not necessarily produce the best tree for the data. To counter potential complications noted above, and {{to find the best}} tree for the data, distance analysis can also incorporate a <b>tree-search</b> protocol that seeks to satisfy an explicit optimality criterion. Two optimality criteria are commonly applied to distance data, minimum evolution (ME) and least squares inference. Least squares is part of a broader class of regression-based methods lumped together here for simplicity. These regression formulae minimize the residual differences between path-distances along the tree and pairwise distances in the data matrix, effectively [...] "fitting" [...] the tree to the empirical distances. In contrast, ME accepts the tree with the shortest sum of branch lengths, and thus minimizes the total amount of evolution assumed. ME is closely akin to parsimony, and under certain conditions, ME analysis of distances based on a discrete character dataset will favor the same tree as conventional parsimony analysis of the same data.|$|E
40|$|Abstract-This paper {{presents}} a new vector quantization (VQ) algorithm exploiting {{the features of}} <b>tree-search</b> as well as finite state VQ’s for imagdvideo coding. In the <b>tree-search</b> VQ, mul-tiple candidates are identified for ongoing search to optimally determine an index of the minimum distortion. In addition, the desired codebook has been reorganized hierarchically to meet the concept of multipath search of neighboring trees so that picture quality can be improved by 4 dB on the average. In the finite state VQ, adaptation to the stqte codebooks is added to enhance the hit ratio of the index produced by the <b>tree-search</b> VQ. Thus, compressed bits can be further reduced. An identifier code is then included to indicate to which output indexes belong. Therefore, this modified algorithm not only reaches a higher compression ratio, but also achieves better quality compared to conventional finite-state and <b>tree-search</b> VQ’s. Finally, suitable VLSI architectures for real-time performance are proposed here 1) to remove the bottleneck of iteration bound in finite-state VQ and 2) to provide parallel computing structure for <b>tree-search</b> VQ to meet computational requirements. I...|$|E
40|$|This paper {{presents}} a new vector quantization (VQ) algorithm exploiting {{the features of}} <b>tree-search</b> as well as finite state VQs for image/video coding. In the <b>tree-search</b> VQ, multiple candidates are identified for on-going search to optimally determine an index of the minimum distortion. In addition, the desired codebook has been reorganized hierarchically to meet the concept of multi-path search of neighboring trees so that picture quality can be improved by 4 dB on the average. In the finite state VQ, adaptation to the state codebooks is added to enhance the hit-ratio of the index produced by the <b>tree-search</b> VQ and hence to further reduce compressed bits. An identifier code is then included to indicate to which output indices belong. Our proposed algorithm not only reaches a higher compression ratio but also achieves better quality compared to conventional Anite-state and <b>tree-search</b> VQs. 1...|$|E
40|$|Traditional {{approaches}} to image sequence coding {{rely on the}} Discrete Cosine Transform (DCT) to achieve high compression rates. In this paper, an encoding scheme for image sequences is presented that trades off some image quality for high compression rates and low computational load. The coder is essentially integer-based and works as follows: i) removes temporal redundancy by a simple, block-based movement detection strategy; ii) provides a first image block approximation using <b>tree-searched</b> Vector Quantization (VQ); iii) then progres-sively transmits complementary information spread along a few frame periods if no further movement is detected. The coder also accepts the DCT in place of VQ...|$|R
40|$|A lightweight, low-power, {{real-time}} data compressor {{design for the}} Earth Observing System (Eos) onboard synthetic aperture radar (SAR) processor is presented. The implementation is based on VLSI design of a pipelined binary <b>tree-searched</b> vector quantizer (VQ), utilizing space-qualifiable 1. 25 -micron CMOS technology. The implementation exploits VLSI system design principles such as the modularity, regular data flow, simple interconnection, localized communication, simple global control, and parallel/pipelined processing. The overall system requires 30 chips with only one VLSI processing element design. The total weight is about 1. 2 lbs, with an estimated power dissipation of approximately 4 watts operating at the maximum input data rate. The projected throughput rate exceeds 5 MHz...|$|R
30|$|The sphere {{detector}} (SD) {{was introduced}} to achieve an optimal performance with low complexity, by employing a <b>tree-searching</b> algorithm [1, 2]. However, the variable complexity of the SD is a major drawback for practical systems that require data to be processed at a constant rate. To overcome this drawback, the fixed-complexity SD (FSD) [3, 4] and the K-best detector [5] have been developed. These detectors {{have the advantage of}} constant throughput, because there is no feedback in the data flow. In particular, the FSD approaches optimal performance in a fixed number of operations. Nevertheless, the complexity of these algorithms remains high in MIMO systems with a large number of transmit antennas and higher order modulation.|$|R
30|$|Below, {{we present}} the {{following}} rotation optimization algorithms: the exhaustive-search algorithm (SNR, R,) {{which is similar}} 2 to the algorithm from [14]; the <b>tree-search</b> algorithm (MoL, R) from [17]; and (MoL+SNR, R), which is a novel extension of (MoL, R). In addition, the corresponding reduced-complexity algorithms are derived, by limiting the search space in the tree; this requires turning (SNR, R) into a <b>tree-search</b> algorithm.|$|E
3000|$|... where i= 1, 2,…,n. Transforming (2) into a <b>tree-search</b> {{problem and}} using the SD {{detection}} allows efficient computation of the LLRs [22, 23].|$|E
40|$|ABSTRACT- A {{new method}} {{to solve the}} {{binocular}} correspondence problem[1] is developed. It {{is based on the}} integration of fuzzy systems [3] and heuristic <b>tree-search</b> theories[4]. Fuzzy logic is used to obtain a measure of similarity between a pair of tokens, through a reasoning process which emulates that of a human observer. A <b>tree-search</b> strategy is used for the task of generating the set of potential pairs required to find the best matching of all tokens in the scene. An illustrative computational point-based stereo matching system has been implemented and its performance evaluated. ...|$|E
50|$|In 1964, System Development Corporation began a {{major effort}} in the {{development}} of metacompilers. This effort includes powerful metacompilers, Bookl, and Book2 written in LISP which have extensive <b>tree-searching</b> and backup capability. An outgrowth of one of the Q-32 systems at SDC is Meta 5. The Meta 5 system incorporates backup of the input stream and enough other facilities to parse any context-sensitive language. This system was successfully released to a wide number of users and had many string-manipulation applications other than compiling. It has many elaborate push-down stacks, attribute setting and testing facilities, and output mechanisms. The fact that Meta 5 successfully translates JOVIAL programs to PL/l programs clearly demonstrates its power and flexibility.|$|R
40|$|A recent taxonomic {{revision}} of Procellariiformes by Penhallurick and Wink (2004) based on cytochrome b sequence data contains analytical and conceptual flaws that compromise {{the validity of}} the taxonomic recommendations. We identify two major shortcomings in the work. First, we question the practice of basing taxonomic recommendations on tree clades that receive no statistical support, and also highlight inconsistences in the <b>tree-searching</b> methods used. Second, we question Penhallurick and Wink’s claim to be following the multidimensional biological species concept, because they have put forward taxonomic proposals that violate this species concept (as well as the phylogenetic species concept). We discuss these analytical and conceptual shortcomings and make recommendations against the taxonomic rearrangements proposed by Penhallurick and Wink. Frank E. Rheindt and Jeremy J. Austi...|$|R
40|$|Since 1967 {{there has}} again been great {{interest}} in chess programming. This paper demonstrates that the structure of today's most successful programs cannot be extended to play Master level chess. Certain basic requirements of a Master player's performance are shown to be outside the performance limits to which a program of this type could be extended. The paper also examines a basic weakness in the <b>tree-searching</b> model approach when applied to situations that cannot be searched to completion. This is the Horizon Effect, which causes unpredictable evaluation errors due to an interaction between the static evaluation function and the rules for search termination. The outline of a model of chess playing that avoids the Horizon Effect and appears extendable to play Master level chess is presented, together with some results already achieved...|$|R
3000|$|The {{proposed}} {{algorithm is}} a generalization {{of a class}} of bit-allocation algorithms in which a small fraction ΔB of the total bit-budget B is allocated to the [...] "most deserving" [...] quantizer among a set of quantizers in an incremental fashion, until the entire bit-budget is exhausted [[4], Section 8.4]. Unfortunately, this type of a greedy search cannot guarantee that the final solution is overall optimal and can yield poor results in our problem where the bit allocation among three sets of dependent quantizers must be achieved. On the other hand, if the increment ΔB is chosen small enough, a near-optimal solution can be found by resorting to a <b>tree-search.</b> Even though a full <b>tree-search</b> is intractable, a simple algorithm {{referred to as the}} (M, L)-algorithm[18] exists for detecting the minimum cost path in the tree with a high probability. We use this insight to formulate a <b>tree-search</b> algorithm for solving the unconstrained bit allocation problem, in which a set of constrained bit allocation problems are solved in each iteration.|$|E
40|$|Abstract — The game of Amazons is {{a fairly}} young member {{of the class of}} territory-games. The best Amazons {{programs}} play now at a high level, but can still be defeated by humans expert of the game. Our focus here is on the solving of endgames, with the goal of improving the general playing level of Amazons programs. Our comparative study of four solvers, DFPN, WPNS, Alpha/Beta and Monte-Carlo <b>Tree-Search,</b> shows that usual game-playing methods used for Amazons, namely Alpha-Beta and Monte-Carlo <b>Tree-Search,</b> are best suited for this task. This result also emphasizes the need of an evaluation function for the game of Amazons. I...|$|E
30|$|A {{plethora of}} MIMO {{detectors}} {{have appeared in}} the literature on this subject, offering various performance-complexity tradeoffs. Suboptimal zero-forcing (ZF) and minimum mean-squared error (MMSE) detectors [10], as well as the nonlinear parallel and successive interference cancellation schemes [11 - 14], require relatively low complexity but sacrifice performance. On the other hand, <b>tree-search</b> or list-based detectors require substantially higher complexity but can offer (near-)ML performance such as the well-known sphere decoding algorithm [2 - 4, 15 - 19]. Other <b>tree-search</b> schemes, such as the K-Best algorithm [20 - 26], address the non-deterministic throughput aspects of sphere decoders. Practical implementation aspects have been investigated in [18, 23, 25 - 39].|$|E
40|$|Tree searching is a {{fundamental}} and computationally intensive problem in artificial intelligence. Parallelization of <b>tree-searching</b> algorithms is one method of improving the speed of these algorithms. However, a high-performance parallel two-player game-tree search algorithm has eluded researchers. Most parallel game-tree search approaches follow synchronous methods, where the work is concentrated within a specific part of the tree, or a given search depth. This thesis shows that asynchronous gametree search algorithms can be as efficient as synchronous methods in determining the minimax value. A taxonomy of previous work in parallel game-tree search is presented. A theoretical model is developed for comparing the efficiency of synchronous and asynchronous search algorithms under realistic assumptions. APHID, a portable parallel game-tree search library, has been built based on the asynchronous parallel game-tree search algorithm proposed in the comparison. The library is easy to imple [...] ...|$|R
40|$|This paper {{summarizes}} {{the investigation of}} SAR image data compression for an on-line archive data distribution system. This system is planned for the ground processing system of Alaska SAR Facility (ASF) and Shuttle Imaging Radar (SIR-C). The objective of the SAR image data compression is to enable the data archive system to provide the remote users a large data base with good image quality, short response time, low transfer cost, and minimal decoding complexity. The requirements and limitations of the on-line archive data distribution system are presented. The effects of SAR image data characteristics on data compression are addressed. The users' survey results suggest that compression ratios between 10 : 1 and 20 : 1 appear suitable. Based on the algorithm evaluation results, the two-level <b>tree-searched</b> vector quantization technique has been recommended as the SAR image data compression algorithm for the on-line archive data distribution system...|$|R
40|$|This paper {{presents}} the algorithm and VLSI architecture of a configurable <b>tree-searching</b> approach that combines {{the features of}} classical depth-first and breadth-first methods. Based on this approach, techniques to reduce complexity while providing both hard and soft outputs decoding are presented. Furthermore, a single programmable parameter allows the user to tradeoff throughput versus BER performance. The proposed multiple-input-multiple-output decoder supports a 4 × 4 64 -QAM system and was synthesized with 65 -nm CMOS technology at 333 MHz clock frequency. For the hard output scheme the design can achieve an average throughput of 257. 8 Mbps at 24 dB signal-to-noise ratio (SNR) with area equivalent to 54. 2 Kgates and a power consumption of 7. 26 mW. For the soft output scheme it achieves an average throughput of 83. 3 Mbps across the SNR range of interest with an area equivalent to 64 Kgates and a power consumption of 11. 5 mW. © 2011 IEEE...|$|R
40|$|In many {{application}} domains constraint-based {{methods in}} <b>tree-search</b> are {{the technology of}} choice to solve NP-complete problems today. However, when actu-ally applying the algorithms out-of-the-box, i. e. without further customization, we have often experienced inacceptable performance. This results from vari...|$|E
40|$|International audienceConstraint Programming (CP) solvers classically {{explore the}} {{solution}} space using tree search-based heuristics. Monte-Carlo <b>Tree-Search</b> (MCTS), a <b>tree-search</b> based method aimed at sequential decision making under uncertainty, simultaneously estimates the reward associated to the sub-trees, and gradually biases the exploration toward {{the most promising}} regions. This paper examines the tight combination of MCTS and CP on the job shop problem (JSP). The contribution is twofold. Firstly, a reward function compliant with the CP setting is proposed. Secondly, a biased MCTS node-selection rule based on this reward is proposed, that is suitable in a multiple-restarts context. Its integration within the Gecode constraint solver is shown to compete with JSP-specific CP approaches on difficult JSP instances...|$|E
40|$|SelfSplit is {{a simple}} {{mechanism}} to convert a sequential <b>tree-search</b> code into a parallel one. In this paradigm, <b>tree-search</b> is dis-tributed among a set of workers, {{each of which is}} able to autonomously determine—without any communication with the other workers—the job parts it has to process. SelfSplit already proved quite effective in paral-lelizing Constraint Programming solvers. In the present paper we investi-gate the performance of SelfSplit when applied to a Mixed-Integer Lin-ear Programming (MILP) solver. Both ad-hoc and general purpose MILP codes have been considered. Computational results show that SelfSplit, in spite of its simplicity, can achieve good speedups even in the MILP context. The role of performance variability in limiting scalability is also discussed...|$|E
40|$|The {{results of}} a study of {{techniques}} for spatial compression of synthetic-aperture-radar (SAR) imagery are summarized. Emphasis is on image-data volume reduction for archive and online storage applications while preserving the image resolution and radiometric fidelity. A quantitative analysis of various techniques, including vector quantization (VQ) and adaptive discrete cosine transform (ADCT), is presented. Various factors such as compression ratio, algorithm complexity, and image quality are considered in determining the optimal algorithm. The compression system requirements are established for electronic access of an online archive system based on the {{results of a}} survey of the science community. The various algorithms are presented and their results evaluated considering the effects of speckle noise and the wide dynamic range inherent in SAR imagery. The conclusion is that although the ADCT produces the best signal-to-distortion-noise ratio for a given compression ratio, the two-level <b>tree-searched</b> VQ technique is preferred due to its simplicity of decoding and near-optimal performance...|$|R
40|$|While Maximum-Likelihood (ML) is {{the optimum}} {{decoding}} scheme for most communication scenarios, practical implementation difficulties limit its use, especially for Multiple Input Multiple Output (MIMO) systems {{with a large}} number of transmit or receive antennas. <b>Tree-searching</b> type decoder structures such as Sphere decoder and K-best decoder present an interesting trade-off between complexity and performance. Many algorithmic developments and VLSI implementations have been reported in literature with widely varying performance to area and power metrics. In this semi-tutorial paper we present a holistic view of different Sphere decoding techniques and K-best decoding techniques, identifying the key algorithmic and implementation trade-offs. We establish a consistent benchmark framework to investigate and compare the delay cost, power cost, and power-delay-product cost incurred by each method. Finally, using the framework, we propose and analyze a novel architecture and compare that to other published approaches. Our goal is to explicitly elucidate the overall advantages and disadvantages of each proposed algorithms in one coherent framework. © 2010 World Scientific Publishing Company...|$|R
40|$|An {{approach}} to global restriction mapping is described that is applicable to any complex source DNA. By analyzing a single restriction digest for {{each member of}} a redundant set of lambda clones, a data base is constructed that contains fragment-size lists for all the clones. The clones are then grouped into subsets, each member of which is related {{to at least one}} other member by a significant overlap. Finally, a <b>tree-searching</b> algorithm seeks restriction maps that are consistent with the fragment-size lists for all the clones in each subset. The feasibility of the approach has been demonstrated by collecting data on 5000 lambda clones containing random 15 -kilobase inserts of yeast DNA. It is shown that these data can be analyzed to produce regional maps of the yeast genome, extending in some cases for over 100 kilobases. In combination with hybridization probes to previously cloned genes, these local maps are already useful for defining the physical arrangement of closely linked genes. They may in the future serve as building blocks {{for the construction of a}} continuous global map...|$|R
