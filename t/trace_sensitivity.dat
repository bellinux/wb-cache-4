6|40|Public
40|$|A hollow {{waveguide}} mid-infrared gas sensor {{operating from}} 1000 cm{sup - 1 } to 4000 cm{sup - 1 } has been developed, optimized, and its performance characterized by combining a FT-IR spectrometer with Ag/Ag-halide hollow core optical fibers. The hollow core waveguide simultaneously {{serves as a}} light guide and miniature gas cell. CH{sub 4 } was used as test analyte during exponential dilution experiments for accurate determination of the achievable limit of detection (LOD). It is shown that the optimized integration of an optical gas sensor module with FT-IR spectroscopy provides <b>trace</b> <b>sensitivity</b> at the few hundreds of parts-per-billion concentration range (ppb, v/v) for CH{sub 4 }...|$|E
40|$|Until 1989, ion {{chromatography}} (IC) was the baseline technology {{selected for the}} Specific Ion Analyzer, an in-flight inorganic water quality monitor being designed for Space Station Freedom. Recent developments in capillary electrophoresis (CE) may offer significant savings of consumables, power consumption, and weight/volume allocation, relative to IC technology. A thorough evaluation of CE's analytical capability, however, is necessary before {{one of the two}} techniques is chosen. Unfortunately, analytical methods currently available for inorganic CE are unproven for NASA's target list of anions and cations. Thus, CE electrolyte chemistry and methods to measure the target contaminants must be first identified and optimized. This paper reports the status of a study to evaluate CE's capability with regard to inorganic and carboxylate anions, alkali and alkaline earth cations, and transition metal cations. Preliminary results indicate that CE has an impressive selectivity and <b>trace</b> <b>sensitivity,</b> although considerable methods development remains to be performed...|$|E
40|$|Dynamic {{languages}} such as Python allow {{programs to}} be written more easily using high-level constructs such as comprehensions for queries and using generic code. Efficient execution of programs then requires powerful optimizations— incrementalization of expensive queries and specialization of generic code. Effective incrementalization and specialization of dynamic languages require precise and scalable alias analysis. This paper describes the development and experimental evaluation of a may-alias analysis for a full dynamic objectoriented language, for program optimization by incrementalization and specialization. The analysis is flow-sensitive; we show that this is necessary for effective optimization of dynamic languages. It uses precise type analysis and a powerful form of context sensitivity, called <b>trace</b> <b>sensitivity,</b> to further improve analysis precision. It uses a compressed representation to significantly reduce the memory used by flowsensitive analyses. We {{evaluate the effectiveness of}} this analysis and 17 variants of it for incrementalization and specialization of Python programs, and we evaluate the precision, memory usage, and running time of these analyses on programs of diverse sizes. The results show that our analysis has acceptable precision and efficiency and represents the best trade-off between them compared to the variants...|$|E
40|$|Attribution License, which permits {{unrestricted}} use, distribution, {{and reproduction}} in any medium, provided the original work is properly cited. A new sensitive sensor was fabricated for simultaneous determination of codeine and acetaminophen based on 4 -hydroxy- 2 -(triphenylphosphonio) phenolate (HTP) and multiwall carbon nanotubes paste electrode at <b>trace</b> levels. The <b>sensitivity</b> of codeine determination was deeply affected by spiking multiwall carbon nanotubes and a modifier in carbon paste. Electron transfer coefficient...|$|R
40|$|Cephalalgia, {{a common}} complaint, often {{presents}} both a diagnostic and a therapeutic problem. In {{a significant number}} of cases the cause may be <b>traced</b> to allergic <b>sensitivity.</b> “Allergic headache” is no different from other allergic diseases either as to the factors that produce the head pain or as to the specific treatment required. Taking a careful history and carrying out definitive skin testing may lead to diagnosis in cases that otherwise might be considered functional...|$|R
40|$|Electron energy-loss {{spectroscopy}} (EELS) is an efficient microanalytical technique for gaining chemical and structural information of thin samples with high <b>trace</b> element <b>sensitivity.</b> Although in principle possible, detecting single atoms [1], is not trivial. Estimating elemental detection limits or the {{minimum number of}} detectable atoms accurately requires a.) reliable signal extraction schemes, as the signal is often superimposed on a large background or overlaps with another edge, b.) a good estimation of the statistical uncertainty associated with background removal c.) accurately known ionization cross-sections and d.) {{a good understanding of}} the noise introduced by the spectrometer´s detector. In this paper systematic studies have been performed to evaluate the best possible way for signal extraction and optimizing the signal-to-noise ratio. Natural ruby was chosen as a complex sample, featuring overlapping edges, containing 0. 4 at % Cr (L 23 edge at 575 eV) on top of the alumina background (background plus O-K edge at 532 eV) ...|$|R
40|$|In {{the context}} of an {{increasing}} need for safety in concrete structures under blast and impact loading condition, the behavior of concrete under high strain rate condition has been an important issue. Since concrete subjected to impact loading associated with high strain rate shows quite different material behavior from that in the static state, several material models are proposed and used to describe the high strain rate behavior under blast and impact loading. In the process of modelling high strain rate conditions with these material models, mesh dependency in the used finite element(FE) is the key problem because simulation results under high strain-rate condition are quite sensitive to applied FE mesh size. This paper introduces an criterion which can minimize the mesh-dependency of simulation results {{on the basis of the}} fracture energy concept, and HJC(Holmquist Johnson Cook) model is examined to <b>trace</b> <b>sensitivity</b> to the used FE mesh size. To coincide with the purpose of the perforation simulation with a concrete plate under a projectile(bullet), the residual velocities of projectile after perforation are compared. The analytical results show that the variation of residual velocity with the used FE mesh size is quite reduced and accuracy of simulation results are improved by applying a unique failure strain value determined according to the proposed criterion...|$|E
40|$|Objectives: Hypoxic ischaemic {{encephalopathy}} is {{a significant}} cause of mortality and morbidity in the term infant. Electroencephalography (EEG) is a useful tool {{in the assessment of}} newborns with HIE. This systematic review of published literature identifies those background features of EEG in term neonates with HIE that best predict neurodevelopmental outcome. Methods: A literature search was conducted using the PubMed, EMBASE and CINAHL databases from January 1960 to April 2014. Studies included in the review described recorded EEG background features, neurodevelopmental outcomes at a minimum age of 12 months and were published in English. Pooled sensitivities and specificities of EEG background features were calculated and meta-analyses were performed for each background feature. Results: Of the 860 articles generated by the initial search strategy, 52 studies were identified as potentially relevant. Twenty-one studies were excluded as they did not distinguish between different abnormal background features, leaving 31 studies from which data were extracted for the meta-analysis. The most promising neonatal EEG features are: burst suppression (sensitivity 0. 87 [95 % CI (0. 78 - 0. 92) ]; specificity 0. 82 [95 % CI (0. 72 - 0. 88) ]), low voltage (sensitivity 0. 92 [95 % CI (0. 72 - 0. 97) ]; specificity 0. 99 [95 % CI (0. 88 - 1. 0) ]), and flat <b>trace</b> (<b>sensitivity</b> 0. 78 [95 % CI (0. 58 - 0. 91) ]; specificity 0. 99 [95 % CI (0. 88 - 1. 0) ]). Conclusion: Burst suppression, low voltage and flat trace in the EEG of term neonates with HIE most accurately predict long term neurodevelopmental outcome. Significance: This structured review and meta-analysis provides quality evidence of the background EEG features that best predict neurodevelopmental outcome...|$|E
40|$|The Advanced Photon Source is {{developing}} a suite of new X-ray beamlines to study materials and devices across many length scales and under real conditions. One of the flagship beamlines of the APS upgrade is the In Situ Nanoprobe (ISN) beamline, which will provide in situ and operando characterization of advanced energy materials and devices under varying temperatures, gas ambients, and applied fields, at previously unavailable spatial resolution and throughput. Examples of materials systems include inorganic and organic photovoltaic systems, advanced battery systems, fuel cell components, nanoelectronic devices, advanced building materials and other scientifically and technologically relevant systems. To characterize these systems at very high spatial resolution and <b>trace</b> <b>sensitivity,</b> the ISN will use both nanofocusing mirrors and diffractive optics to achieve spots sizes as small as 20 nm. Nanofocusing mirrors in Kirkpatrick–Baez geometry will provide several orders of magnitude increase in photon flux at a spatial resolution of 50 nm. Diffractive optics such as zone plates and/or multilayer Laue lenses will provide a highest spatial resolution of 20 nm. Coherent diffraction methods {{will be used to}} study even small specimen features with sub- 10 nm relevant length scale. A high-throughput data acquisition system will be employed to significantly increase operations efficiency and usability of the instrument. The ISN will provide full spectroscopy capabilities to study the chemical state of most materials in the periodic table, and enable X-ray fluorescence tomography. Insitu electrical characterization will enable operando studies of energy and electronic devices such as photovoltaic systems and batteries. We describe the optical concept for the ISN beamline, the technical design, and the approach for enabling a broad variety of in situ studies. We furthermore discuss the application of hard X-ray microscopy to study defects in multi-crystalline solar cells, one of the lines of inquiries for which the ISN is being developed...|$|E
40|$|Motivated by the {{challenge}} of capturing complex hierarchical chemical detail in natural material {{from a wide range}} of applications, the Maia detector array and integrated realtime processor have been developed to acquire X-ray fluorescence images using X-ray Fluorescence Microscopy (XFM). Maia has been deployed initially at the XFM beamline at the Australian Synchrotron and more recently, demonstrating improvements in energy resolution, at the P 06 beamline at Petra III in Germany. Maia captures fine detail in element images beyond 100 M pixels. It combines a large solid-angle annular energy-dispersive 384 detector array, stage encoder and flux counter inputs and dedicated FPGA-based real-time event processor with embedded spectral deconvolution. This enables high definition imaging and enhanced <b>trace</b> element <b>sensitivity</b> to capture complex trace element textures and place them in a detailed spatial context. Maia hardware and software methods provide per pixel correction for dwell, beam flux variation, dead-time and pileup, as well as off-line parallel processing for enhanced throughput. Methods have been developed for real-time display of deconvoluted SXRF element images, depth mapping of rare particles and the acquisition of 3 D datasets for fluorescence tomography and XANES imaging using a spectral deconvolution method that tracks beam energy variation. ...|$|R
40|$|This paper {{shows the}} {{application}} of adjoint sensitivity analysis to flash flood wave propagation in a river channel. The adjoint sensitivity analysis is used to assess flood hazard in a coastal area caused by river discharge. The numerical model determines the sensitivities of predicted water levels to uncertainties in key controls such as inflow hydrograph, channel topography, frictional resistance and infiltration rate. Sensitivities are calculated using the adjoint equations and are specified in terms of water levels being greater than certain safe threshold levels along the channel. The flood propagation model {{is based on the}} St. Venant equations while the propagation of sensitivity information is based on the corresponding adjoint equations. This analysis is achieved using a numerical model that integrates The St. Venant equations forward in time using a staggered finite difference scheme. An enhanced method of characteristics at the downstream boundary provides open boundary conditions and overcomes the problem of reflections from the boundaries. Then, the adjoint model is integrated backwards in time to <b>trace</b> the <b>sensitivity</b> information back through the model domain towards the inflow control boundary. The adjoint model has been verified by means of an identical twin experiment...|$|R
5000|$|As long as {{photographic}} {{materials were}} usefully sensitive only to blue-green, blue, violet and ultraviolet, three-color photography {{could never be}} practical. In 1873 German chemist Hermann Wilhelm Vogel discovered that the addition of small amounts of certain aniline dyes to a photographic emulsion could add sensitivity to colors which the dyes absorbed. He identified dyes which variously sensitized for all the previously ineffective colors except true red, to which only a marginal <b>trace</b> of <b>sensitivity</b> could be added. In the following year, Edmond Becquerel discovered that chlorophyll was a good sensitizer for red. Although it would be many more years before these sensitizers (and better ones developed later) found much use beyond scientific applications such as spectrography, they were quickly and eagerly adopted by Louis Ducos du Hauron, Charles Cros and other color photography pioneers. Exposure times for the [...] "problem" [...] colors could now be reduced from hours to minutes. As ever-more-sensitive gelatin emulsions replaced the old wet and dry collodion processes, the minutes became seconds. New sensitizing dyes introduced early in the 20th century eventually made so-called [...] "instantaneous" [...] color exposures possible.|$|R
40|$|Determining the {{compositional}} {{properties of}} surfaces {{in the environment}} is an important visual capacity. One such property is specular reflectance, which encompasses the range from matte to shiny surfaces. Visual estimation of specular reflectance can be informed by characteristic motion profiles; a surface with a specular reflectance {{that is difficult to}} determine while static can be confidently disambiguated when set in motion. Here, we used fMRI to <b>trace</b> the <b>sensitivity</b> of human visual cortex to such motion cues, both with and without photometric cues to specular reflectance. Participants viewed rotating blob-like objects that were rendered as images (photometric) or dots (kinematic) with either matte-consistent or shiny-consistent specular reflectance profiles. We were unable to identify any areas in low and mid-level human visual cortex that responded preferentially to surface specular reflectance from motion. However, univariate and multivariate analyses identified several visual areas; V 1, V 2, V 3, V 3 A/B, and hMT+, capable of differentiating shiny from matte surface flows. These results indicate that the machinery for extracting kinematic cues is present in human visual cortex, but the areas involved in integrating such information with the photometric cues necessary for surface specular reflectance remain unclear. © 2015 Kam, Mannion, Lee, Doerschner and Kersten...|$|R
40|$|Abstract. The small {{dispersion}} in {{the observed}} relation between period and intrinsic opening angles of the conal emission regions in pulsars {{can be used}} to study their geometry. Applying this direct method shows the corresponding hollow cones to be consistent with being circular; an upper limit of approximately 10 % is derived for the deviation from a circular geometry. Previous investigations have relied on assumptions regarding the statistical properties of the pulsar sample used in the analysis. The disparate conclusions reached in the past concerning the geometry of pulsar beams is <b>traced</b> to the <b>sensitivity</b> of such indirect methods to the validity of these assumptions. It is further argued that there is no observational evidence for an evolution with pulsar age of the angle between the cone axis and rotation axis. Key words: pulsars: general 1...|$|R
40|$|A new {{sensitive}} sensor was fabricated for simultaneous {{determination of}} codeine and acetaminophen based on 4 -hydroxy- 2 -(triphenylphosphonio) phenolate (HTP) and multiwall carbon nanotubes paste electrode at <b>trace</b> levels. The <b>sensitivity</b> of codeine determination was deeply affected by spiking multiwall carbon nanotubes and a modifier in carbon paste. Electron transfer coefficient, α, catalytic electron rate constant, k, and the exchange current density, j 0, for oxidation of codeine at the HTP-MWCNT-CPE were calculated using cyclic voltammetry. The calibration curve was linear {{over the range}} 0. 2 – 844. 7 [*]μM with two linear segments, and the detection limit of 0. 063 [*]μM of codeine was obtained using differential pulse voltammetry. The modified electrode was separated codeine and acetaminophen signals by differential pulse voltammetry. The modified electrode was applied for the determination of codeine and acetaminophen in biological and pharmaceutical samples with satisfactory results...|$|R
40|$|Abstract — Hosts {{participating}} in overlay multicast applications {{have a wide}} range of heterogeneity in bandwidth and participation characteristics. In this paper, we highlight and show the need to systematically consider prioritization as a key criterion in the design of protocols for overlay multicast. We identify trade-offs in the design of prioritization heuristics in two important contexts. The first part of the paper considers prioritization strategies in the context of heterogeneity in node outgoing bandwidth and node stay time durations, and a lack of correlation between the two dimensions. The second part of the paper considers bandwidth allocation and prioritization policies with multi-tree data delivery in environments with heterogeneity in outgoing bandwidth and a certain degree of altruistic behavior. We conduct a systematic study of the trade-offs using both real <b>trace</b> data, and <b>sensitivity</b> studies using synthetic workloads. To the best of our knowledge, this is the first work to identify and study these trade-offs, and to demonstrate the potential benefits of the resulting prioritization heuristics. I...|$|R
40|$|Accelerator mass {{spectrometry}} (AMS) quantifies attomole (10 {sup - 18 }) amounts of {sup 14 }C in milligram sized samples. This sensitivity {{is used to}} trace nutrients, toxins and therapeutics in humans and animals at less than {micro}g/kg doses containing 1 - 100 nCi of {sup 14 }C. Widespread use of AMS in pharmaceutical development and biochemical science has been hampered by the size and expense of the typical spectrometer that has been developed for high precision radiocarbon dating. The precision of AMS can be relaxed for biochemical <b>tracing,</b> but <b>sensitivity,</b> accuracy and throughput are important properties that must be maintained in spectrometers designed for routine quantification. We are completing installation of a spectrometer that will maintain the high throughput of our primary spectrometer but which requires less than 20 % of the floor space and of the cost. Sensitivity and throughput are kept high by using the LLNL intense cesium sputter ion source with solid graphitic samples. Resultant space-charge effects are minimized by careful modeling to find optimal ion transport in the spectrometer. A long charge-changing ''stripper gas'' volume removes molecular isobars at potentials {{of a few hundred}} kiloVolts, reducing the size of the accelerating component. Fast ion detectors count at high rates to keep a wide dynamic range for 14 C concentrations. Solid sample presentation eliminates the sample cross contamination that degrades accuracy and the effects of ''memory'' in the ion source. Automated processes are under development for conversion of liquid and solid biological samples to the preferred graphitic form for the ion source...|$|R
40|$|Although {{numerical}} {{heat transfer}} models based on conduction mode of heat transfer {{have become a}} strong basis for the quantitative analysis of fusion welding, they still find limited use in actual design for three primary reasons. First, these traditional models consider a volumetric heat source term, which ironically requires a-priori knowledge of the final weld pool dimensions. Second, the numerical models need confident values of a few parameters, e. g. arc efficiency and arc radius, which are usually uncertain and requires many trial and error simulations to realise their suitable values. Third, these models are rarely attempted for the prediction of possible weld conditions for a requisite or target weld dimensions, which is of paramount interest in design for welding. The present work attempts to circumvent these issues by linking a genetic algorithm (GA) based global optimisation scheme with a finite element based three-dimensional numerical heat transfer model. The numerical model includes a volumetric heat source that adapts itself to the computed weld pool geometry at any instant. The GA module identifies the optimum values {{of a set of}} uncertain parameters needed for the reliable modelling calculations and next, identifies the suitable values of the process variables, e. g. weld current, for a target weld dimension. In each case, the GA module guides the numerical model to compute weld dimensions for a given set of inputs, <b>traces</b> the <b>sensitivity</b> of the error in prediction on the inputs being optimised, updates them accordingly and reuses the numerical model to finally obtain their optimised values. The complete integrated model is validated with a number of experimental results in gas tungsten arc spot welding processes...|$|R
40|$|International audienceBackground While massively {{parallel}} DNA sequencing methods {{continue to evolve}} rapidly, the benchmark technique for detection and verification of rare (particularly disease-causing) sequence variants remains four-colour dye-terminator sequencing by capillary electrophoresis. The high throughput and long read lengths currently available have shifted the bottleneck in mutation detection away from data generation to data analysis. While excellent computational methods {{have been developed for}} quantifying sequence accuracy and detecting variants, either during de novo sequence assembly or for SNP detection, the identification, verification and annotation of very rare sequence variants remains a rather labour-intensive process for which few software aids exist. Results Here we describe GeneScreen, a program that analyses capillary electropherograms and compares their sequences to a known reference for identification of mutations. The detected sequence variants are then made available for rapid assessment and annotation via a graphical user interface, allowing any selected variants to be exported for reporting and archiving. Conclusion Using GeneScreen, a single user requires only a few minutes to identify rare mutations in hundreds of sequence <b>traces,</b> with comparable <b>sensitivity</b> to expensive commercial products...|$|R
40|$|We apply novel jet {{techniques}} {{to investigate the}} spin and CP quantum numbers of a heavy resonance X, singly produced in pp -> X -> ZZ -> l(+) l(-) jj at the LHC. We take into account all dominant background processes to show that this channel, which has been considered unobservable until now, can qualify under realistic conditions to supplement measurements of the purely leptonic decay channels X -> ZZ -> 4 l. We perform a detailed investigation of spin- and CP-sensitive angular observables on the fully-simulated final state for various spin and CP quantum numbers of the state X, <b>tracing</b> how potential <b>sensitivity</b> communicates through all the steps of a subjet analysis. This allows us to elaborate on the prospects and limitations of performing such measurements with the semihadronic final state. We find our analysis particularly sensitive to a CP-even or CP-odd scalar resonance, while, for tensorial and vectorial resonances, discriminative features are diminished in the boosted kinematical regime. Comment: 12 pages, 7 figures, 2 tables, published versio...|$|R
40|$|The {{objective}} of this dissertation {{is to develop a}} 3 -D domain-overlapping coupling method that leverages the superior flow field resolution of the Computational Fluid Dynamics (CFD) code STAR-CCM+ and the fast execution of the System Thermal Hydraulic (STH) code TRACE to efficiently and accurately model thermal hydraulic transport properties in nuclear power plants under complex conditions of regulatory and economic importance. The primary contribution is the novel Stabilized Inertial Domain Overlapping (SIDO) coupling method, which allows for on-the-fly correction of TRACE solutions for local pressures and velocity profiles inside multi-dimensional regions {{based on the results of}} the CFD simulation. The method is found to outperform the more frequently-used domain decomposition coupling methods. An STH code such as TRACE is designed to simulate large, diverse component networks, requiring simplifications to the fluid flow equations for reasonable execution times. Empirical correlations are therefore required for many sub-grid processes. The coarse grids used by <b>TRACE</b> diminish <b>sensitivity</b> to small scale geometric details such as Reactor Pressure Vessel (RPV) internals. A CFD code such as STAR-CCM+ uses much finer computational meshes that are sensitive to the geometric details of reactor internals. In turbulent flows, it is infeasible to fully resolve the flow solution, but the correlations used to model turbulence are at a low level. The CFD code can therefore resolve smaller scale flow processes. The development of a 3 -D coupling method was carried out with the intention of improving predictive capabilities of transport properties in the downcomer and lower plenum regions of an RPV in reactor safety calculations. These regions are responsible for the multi-dimensional mixing effects that determine the distribution at the core inlet of quantities with reactivity implications, such as fluid temperature and dissolved neutron absorber concentration...|$|R
40|$|Abstract Background It is {{well known}} that the {{normalization}} step of microarray data makes a difference in the downstream analysis. All normalization methods rely on certain assumptions, so differences in results can be <b>traced</b> to different <b>sensitivities</b> to violation of the assumptions. Illustrating the lack of robustness, in a striking spike-in experiment all existing normalization methods fail because of an imbalance between up- and down-regulated genes. This means it is still important to develop a normalization method that is robust against violation of the standard assumptions Results We develop a new algorithm based on identification of the least-variant set (LVS) of genes across the arrays. The array-to-array variation is evaluated in the robust linear model fit of pre-normalized probe-level data. The genes are then used as a reference set for a non-linear normalization. The method is applicable to any existing expression summaries, such as MAS 5 or RMA. Conclusion We show that LVS normalization outperforms other normalization methods when the standard assumptions are not satisfied. In the complex spike-in study, LVS performs similarly to the ideal (in practice unknown) housekeeping-gene normalization. An R package called lvs is available in [URL]. </p...|$|R
40|$|Background While massively {{parallel}} DNA sequencing methods {{continue to evolve}} rapidly, the benchmark technique for detection and verification of rare (particularly disease-causing) sequence variants remains four-colour dye-terminator sequencing by capillary electrophoresis. The high throughput and long read lengths currently available have shifted the bottleneck in mutation detection away from data generation to data analysis. While excellent computational methods {{have been developed for}} quantifying sequence accuracy and detecting variants, either during de novo sequence assembly or for single-nucleotide polymorphism detection, the identification, verification and annotation of very rare sequence variants remains a rather labour-intensive process for which few software aids exist. Aim To provide a freely available, intuitive software application for highly efficient mutation screening of large sequence batches. Methods and results The authors developed GeneScreen, a desktop program that analyses capillary electropherograms and compares their sequences with a known reference for identification of mutations. The detected sequence variants are then made available for rapid assessment and annotation via a graphical user interface, allowing chosen variants to be exported for reporting and archiving. The program was validated using more than 16 000 diagnostic laboratory sequence traces. Conclusion Using GeneScreen, a single user requires only a few minutes to identify rare mutations in hundreds of sequence <b>traces,</b> with comparable <b>sensitivity</b> to expensive commercial products...|$|R
40|$|Proteasome inhibitors such as bortezomib exhibit {{clinical}} efficacy {{in multiple}} myeloma, but studies in {{acute myeloid leukemia}} (AML) have been disappointing to date. The apparent failure in AML likely reflects a lack of biological understanding that might clarify applications of proteosome inhibitors in this disease. Here we show that AML cells are considerably less sensitive than control noncancerous cells to bortezomib-induced cytotoxicity, permitting most bortezomib-treated AML cells to survive treatment. We <b>traced</b> reduced bortezomib <b>sensitivity</b> to increased basal levels of nuclear Nrf 2, a transcription factor that stimulates protective antioxidant enzymes. Bortezomib stimulates cytotoxicity through accumulation of reactive oxygen species (ROS) but elevated basal levels of nuclear Nrf 2 present in AML cells reduced ROS levels, permitting AML cells to survive drug treatment. We further found that the Nrf 2 transcriptional repressor Bach 1 is rapidly inactivated by bortezomib, allowing rapid induction of Nrf 2 -regulated cytoprotective and detoxification genes that protect AML cells from bortezomib-induced apoptosis. By contrast, nonmalignant control cells lacked constitutive activation of Nrf 2, such that bortezomib-mediated inactivation of Bach 1 led to a delay in induction of Nrf 2 -regulated genes, effectively preventing the manifestation of apoptotic protection that is seen in AML cells. Together, our findings argue that AML might be rendered sensitive to proteasome inhibitors by cotreatment with either an Nrf 2 -inhibitory or Bach 1 -inhibitory treatment, rationalizing a targeted therapy against AML...|$|R
40|$|There {{are many}} {{excellent}} reasons {{to examine the}} surface composition {{of a wide range}} of Martian samples. The existing spectral data indicate that many dust and soil particles have a thin Fe(+ 3) layer with a typical particle size in the 10 micrometer to 400 micrometer range. In view of the high CO 2 content of the atmosphere, one might expect that surface carbonates should be present. In addition to chemisorbed material there will probably exist physisorbed atmospheric components of the atmosphere including oxygen, nitrogen and water vapor. The latter could possibly give rise to some hydrated minerals. Using ultra-high-vacuum/mass spectrographic techniques it should be possible to detect physisorbed and moderately strong chemisorbed species on the particle surfaces with a temperature programmed degassing procedure. In some instances such an approach is capable of helping distinguish between volcanic and impact generated materials by detecting the presence of fumerolic gases. Such gases typically condense on the exterior of the ejected particles. Additionally surface atomic and chemical compositions should be examined by a combination of modern surface analytical techniques. The combination we currently have in Buffalo at SUNY would appear to be one of the best available including ESCA (150 micrometer spot capability) Auger (SAM) with 300 A focussing for surface compositional surveys, SIMS for high <b>sensitivity</b> <b>trace</b> element detection and ISS for immediate surface layer analysis...|$|R
40|$|In {{order to}} {{evaluate}} the accuracy of a urine reagent dipstick (Multistix 10 SG®) to determine ascitic fluid leukocyte count, we prospectively studied 106 cirrhotic patients from April 2003 to December 2004, in two different centers (Federal University of São Paulo - UNIFESP-EPM and Federal University of Juiz de Fora - HU-UFJF) for the rapid bedside diagnosis of spontaneous bacterial peritonitis. The mean age 54 ± 12 years, there was a predominance of males (eighty-two patients, 77 %), and alcohol was the most frequent etiology (43 %). Forty-four percent of patients were classified as Child B and fifty-one as Child C (51 %). Abdominal paracentesis was performed both in outpatient and inpatient settings and the Multistix 10 SG® was tested. Eleven cases of spontaneous bacterial peritonitis were identified by means of polymorphonuclear count. If we considered the positive Multistix 10 SG® result of 3 or more, the sensitivity, specificity, positive and negative predictive value were respectively 71 %, 99 %, 91 % and 98 %. With a positive reagent strip result taken as grade 2 (<b>traces)</b> or more, <b>sensitivity</b> was 86 % and specificity was 96 % with positive and negative predictive values of 60 % and 99 %, respectively. Diagnostic accuracy was 95 %. We concluded {{that the use of}} a urine reagent dipstick (Multistix 10 SG®) could be considered a quick, easy and cheap method for ascitic fluid cellularity determination in SBP diagnosis...|$|R
40|$|The spatial {{distribution}} of trace gases exhibit large spatial heterogeneity over the Indian region with an elevated pollution loading over densely populated Gangetic Plains (IGP). The contending role and importance of anthropogenic emissions and meteorology in deciding the trace gases level and distribution over Indian region, however, is poorly investigated. In this paper, we use an online regional chemistry transport model (WRF/Chem) to simulate the {{spatial distribution}} of trace gases over Indian region during one representative month of only three meteorological seasons namely winter, spring/summer and monsoon. The base simulation, using anthropogenic emissions from SEAC(4) RS inventory, is used to simulate the general meteorological conditions and the realistic {{spatial distribution of}} <b>trace</b> gases. A <b>sensitivity</b> simulation is conducted after removing the spatial heterogeneity in the anthropogenic emissions, i. e., with spatially uniform emissions to decouple the role of anthropogenic emissions and meteorology and their role in controlling the distribution of trace gases over India. The concentration levels of Ozone, CO, SO 2 and NO 2 {{were found to be}} lower over IGP when the emissions are uniform over India. A comparison of the base run with the sensitivity run highlights that meteorology plays a dominant role in controlling the spatial distribution of relatively longer-lived species like CO and secondary species like Ozone while short-lived species like NOX and SO 2 are predominantly controlled by the spatial variability in anthropogenic emissions over the Indian region...|$|R
40|$|When retinas from dark-adapted C 57 BL/ 6 mice were {{incubated}} in {{the dark}} for 5 min at 37 degrees C in Earle's medium, they contained 80 - 120 pmol/mg protein of cGMP and about 13 pmol/mg protein of cAMP. When the incubation in darkness was in calcium-deficient Earle's medium with 3 mM EGTA, a 10 - 20 fold increase occurred in the cGMP level, peaking at 2 - 3 min, but no change occurred in cAMP. This elevated level fell in 3 min to normal dark levels on return to normal Earle's medium, but was still about three times that of control levels after 15 min in EGTA- containing solution. Bright light after 2 min of dark incubation of dark-adapted retinas resulted in a 40 - 50 % fall in cGMP, and bright light sharply reduced the elevated dark cGMP level of retinas in calcium-deficient media with 3 mM EDTA. However, no depression of normal dark levels of cGMP has thus far been obtained by increasing external calcium levels, even {{in the presence of the}} ionophore A 23187. All the above phenomena involving dark cGMP levels and calcium are similar in Earle's medium with 100 mM of K+ substituted for Na+. Congenic rodless (rd/rd) mouse retinas have less than 5 % of control cGMP and show only <b>traces</b> of calcium <b>sensitivity.</b> Thus, the above phenomena in controls are likely to be largely occurring in rods. The data suggest a dependency of the dark cGMP level on the calcium level, but that the light-induced fall in cGMP may largely be calcium insensitive...|$|R
40|$|We {{report on}} 25 sub-arcsecond binaries, {{detected}} {{for the first}} time by means of lunar occultations in the near-infrared as part of a long-term program using the ISAAC instrument at the ESO Very Large Telescope. The primaries have magnitudes in the range K= 3. 8 to 10. 4, and the companions in the range K= 6. 4 to 12. 1. The magnitude differences have a median value of 2. 8, with the largest being 5. 4. The projected separations are in the range 6 to 748 milliarcseconds and with a median of 18 milliarcseconds, or about 3 times less than the diffraction limit of the telescope. Among our binary detections are a pre-main sequence star and an enigmatic Mira-like variable previously suspected to have a companion. Additionally, we quote an accurate first-time near-IR detection of a previously known wider binary. We discuss our findings on an individual basis as far as made possible by the available literature, and we examine them from a statistical point of view. We derive a typical frequency of binarity among field stars of ~ 10 %, in the resolution and sensitivity range afforded by the technique (~ 0. 003 " to ~ 0. 5 ", and K~ 12 mag, respectively). This is in line with previous results by the same technique but we point out interesting differences that we can <b>trace</b> up to <b>sensitivity,</b> time sampling, and average distance of the targets. Finally, we discuss the prospects for further follow-up studies. Comment: 8 Pages, 6 Figures, 2 Tables. Accepted for publication in A...|$|R
40|$|AbstractThe Virtual Fields Method (VFM – Pierron and Grediac, 2012), {{an inverse}} method {{based on the}} {{principle}} of virtual work (PVW), is being increasingly used to estimate mechanical properties of materials from full-field deformations obtained from techniques such as Digital Image Correlation, moiré and speckle interferometry and grid methods. By making specific choices for virtual fields (VFs) in PVW, one obtains a system of algebraic equations, which is then solved for the unknown material constants. Recently, a new variant of VFM, known as the Eigenfunction Virtual Fields Method (EVFM) has been proposed (Subramanian, 2013). In EVFM, principal components of the measured (i. e. true) strain fields are used to systematically generate VFs. We extend EVFM to orthotropic elastic materials in this work, and estimate the relevant material parameters from full-field strain data generated from a finite-element model of an unnotched Iosipescu test. Varying levels of Gaussian white noise are added to the synthetic strain data to evaluate the sensitivity of EVFM to input noise. It is observed that for low to moderate noise, the material properties estimated by the proposed method are relatively insensitive to noise. However, when noise levels are high, the proposed method yields large variance in some of the computed properties when compared to the state-of-the-art optimized piecewise continuous VFM (Toussaint et al., 2006; Pierron and Grediac, 2012). Some of the large variance in properties estimated from noisy data using EVFM is <b>traced</b> to the <b>sensitivity</b> of the third dominant eigenfunction and modifications to the proposed method to address this issue are suggested...|$|R
40|$|ABSTRACT When retinas from dark-adapted C 57 BL/ 6 mice were {{incubated}} in {{the dark}} for 5 rain at 37 ~ in Earle's medium, they contained 80 - 120 pmol/mg protein of cGMP and about 13 pmol/mg protein of cAMP. When the incubation in darkness was in calcium-deficient Earle's medium with 3 mM EGTA, a 10 - 20 -fold increase occurred in the cGMP level, peaking at 2 - 3 min, but no change occurred in cAMP. This elevated level fell in 3 min to normal dark levels on return to normal Earle's medium, but was still about three times that of control levels after 15 rain in EGTA-containing solution. Bright light after 2 min of dark incubation of darkadapted retinas resulted in a 40 - 50 % fall in cGMP, and bright light sharply reduced the elevated dark cGMP level of retinas in calcium-deficient media with 3 mM EDTA. However, no depression of normal dark levels of cGMP has thus far been obtained by increasing external calcium levels, even {{in the presence of the}} ionophore A 23187. All the above phenomena involving dark cGMP levels and calcium are similar in Earle's medium with 100 mM of K + substituted for Na +. Congenic rodless (rd/rd) mouse retinas have < 5 % of control cGMP and show only <b>traces</b> of calcium <b>sensitivity.</b> Thus, the above phenomena in controls are likely to be largely occurring in rods. The data suggest a dependency of the dark cGMP level on the calcium level, but that the light-induced fall in cGMP may largely be calcium insensitive...|$|R
40|$|Background: The urine {{dipstick}} {{is widely}} used as an initial screening tool {{for the evaluation of}} proteinuria; however, its diagnostic accuracy has not yet been sufficiently evaluated. Therefore, we evaluated its diagnostic accuracy using spot urine albumin/creatinine ratio (ACR) and total protein/creatinine ratio (PCR) in proteinuria. Methods: Using PCR ≥ 0. 2  g/g or ≥ 0. 5  g/g and ACR ≥ 300  mg/g or ≥ 30  mg/g as the reference standard, we calculated the diagnostic accuracy profile: sensitivity, specificity, positive and negative predictive value, and the area under the curve (AUC) of the receiver operating characteristic curve. Results: PCR and ACR were available for 10, 348 and 3, 873 instances of dipstick testing. The proportions with PCR ≥ 0. 2  g/g, ≥ 0. 5  g/g and ACR ≥ 300  mg/g, ≥ 30  mg/g were 38. 2 %, 24. 6 % and 8. 9 %, 31. 7 %, respectively. The AUCs for PCR ≥ 0. 2  g/g, ≥ 0. 5  g/g, and ACR ≥ 300  mg/g were 0. 935 (trace: closest to ideal point), 0. 968 (1 +), and 0. 983 (1 +), respectively. Both sensitivity and specificity were > 80 % except for PCR ≥ 0. 5  g/g with trace cutoff. For the reference standard of ACR ≥ 30  mg/g, the AUC was 0. 797 (<b>trace)</b> and the <b>sensitivity</b> was 63. 5 %. Conclusion: Urine dipstick test can be used for screening in older outpatients with ACR ≥ 300  mg/g or PCR as the reference standard for proteinuria. However, we cannot recommend the test as a screening tool with ACR ≥ 30  mg/g as the reference owing to its low sensitivity...|$|R
40|$|The Next-Generation Very Large Array (ngVLA) will be {{critical}} for understanding how galaxies are built and evolve at the earliest epochs. The sensitivity and frequency coverage will allow {{for the detection of}} cold gas and dust in `normal' distant galaxies, including the low-J transitions of molecular gas tracers such as CO, HNC, and HCO+; synchrotron and free-free continuum emission; and even the exciting possibility of thermal dust emission at the highest (z~ 7) redshifts. In particular, by enabling the total molecular gas reservoirs to be <b>traced</b> to unprecedented <b>sensitivities</b> across a huge range of epochs simultaneously [...] something no other radio or submillimeter facility will be capable of [...] the detection of the crucial low-J transitions of CO in a diverse body of galaxies will be the cornerstone of ngVLA's contribution to high-redshift galaxy evolution science. The ultra-wide bandwidths will allow a complete sampling of radio SEDs, as well as the detection of emission lines necessary for spectroscopic confirmation of elusive dusty starbursts. The ngVLA will also deliver unique contributions to our understanding of cosmic magnetism and to science accessible through microwave polarimetry. Finally, the superb angular resolution will move the field beyond detection experiments and allow detailed studies of the morphology and dynamics of these systems, including dynamical modeling of disks/mergers, determining the properties of outflows, measuring black hole masses from gas disks, and resolving multiple AGN nuclei. We explore the contribution of a ngVLA to these areas and more, as well as synergies with current and upcoming facilities including ALMA, SKA, large single-dish submillimeter observatories, GMT/TMT, and JWST. Comment: 52 pages, 13 figures, NRAO Next Generation Very Large Array Memos Series: [URL]...|$|R
40|$|Ambulatory polysomnography (PSG) {{does not}} {{commonly}} include an objective measure {{of light to}} determine the time of lights off (Loff), and thus cannot be used to calculate important indices such as sleep onset latency and sleep efficiency. This study examined the technical specifications and appropriateness of a prototype light sensor (LS) for use in ambulatory Compumedics Somte PSG. Two studies were conducted. The first examined the light measurement characteristics of the LS when used with a portable PSG device, specifically recording <b>trace</b> range, linearity, <b>sensitivity,</b> and stability. This involved the LS being exposed to varying incandescent and fluorescent light levels in a light controlled room. Secondly, the LS was trialled in 24 home and 12 hospital ambulatory PSGs to investigate whether light levels in home and hospital settings were within the recording range of the LS, and to quantify the typical light intensity reduction {{at the time of}} Loff. A preliminary exploration of clinical utility was also conducted. Linearity between LS voltage and lux was demonstrated, and the LS trace was stable over 14 hours of recording. The observed maximum voltage output of the LS/PSG device was 250 mV, corresponding to a maximum recording range of 350 lux and 523 lux for incandescent and fluorescent light respectively. At the time of Loff, light levels were within the recording range of the LS, and on average dropped by 72 lux (9 - 245) in the home and 76 lux (4 - 348) in the hospital setting. Results suggest that clinical utility was greatest in hospital settings where patients are less mobile. The LS was a simple and effective objective marker of light level in portable PSG, which can be used to identify Loff in ambulatory PSG. This allows measurement of additional sleep indices and support with clinical decisions...|$|R
40|$|Developing a {{fair and}} an {{equitable}} real and reactive power allocation method has been an active topic of power research particularly in the new paradigm, with many transactions taking place at any time. However, due to non linear nature of power flow, {{it is difficult to}} determine transmission usage allocations accurately. Therefore, it required using circuit theories, approximate models, <b>tracing</b> algorithms or <b>sensitivity</b> indices for usage allocation. This project suggests Modified Nodal Equations methodology to allocate the real and reactive power output of individual generators to system loads. The purpose is to represent each load current as a function of the generatorsâ€™ current and load voltages. Starting from the concept of circuit theory, it uses the modified admittance matrix to decompose the load voltage dependent term into components of generator dependent terms. By applying Kirchhoffâ€™s laws and using the decompose components of load current and load voltages, the real and reactive power contribution from each generator to loads in a deregulated power system operating as a power pool are determined. Moreover, the line usage allocation for pool and bilateral trades is developed on the basis of tracing and circuit method respectively. It further enhanced the methodology by utilizing an appropriate Artificial Neural Network (ANN) to solve the generatorâ€™s contribution to loads and line usage allocation in a simpler and faster manner. In this project, a feedforward and Radial Basis Function Network (RBFN) architecture have been chosen to incorporate into Modified Nodal Equations method for the proposed ANN transmission usage allocation technique. Both the feedforward and RBFN output provide the results in a faster and convenient manner with very good accuracy. Adaptation of appropriate ANN architectures for a large test system is expected to deliver a considerable efficiency in computation time especially during training processes. Finally, a new and an improved transmission usage allocation method have been developed for future of the Malaysian electric energy sector...|$|R
40|$|In {{this thesis}} we explore four new {{applications}} of the Local Ensemble Transform Kalman Filter (LETKF), namely adaptive observations, analysis sensitivity, observation impact, and multivariate humidity assimilation. In each of these applications we have obtained promising results. In the adaptive observation studies, we found that ensemble spread strategy, where adaptive observations are selected among the points with largest ensemble spread (with the constraint that observations cannot be contiguous {{in order to avoid}} clusters of adaptive observations) is very effective and close to optimal sampling. The application on simulated Doppler Wind Lidar (DWL) adaptive observation studies shows that 3 D-Var is as effective as LETKF with 10 % adaptive observations sampled with the ensemble spread strategy. With 2 % adaptive observations, 3 D-Var is not as effective as the LETKF. In the analysis sensitivity study, we proposed to calculate this quantity within the LETKF with low additional computational time. Unlike in 4 D-Var (Cardinali et al., 2004), in the LETKF, the computation is exact and satisfies the theoretical value limits (between 0 and 1). The results from simulated experiments show that the <b>trace</b> of analysis <b>sensitivity</b> qualitatively reflects the observation impact obtained from independently computed data addition or data denial OSSE experiments. In the observation impact study, we derived a formula to estimate the impact of observations on short-range forecasts as in Langland and Baker (2004), but without using an adjoint model. Both methods estimate more than 90 % accuracy the actual observation impact on the short-range forecast error improvement. Like the adjoint method, the method we proposed detects observations that have either large random error or unaccounted bias. This method can be easily calculated within the LETKF, and provides a powerful tool to estimate the quality of observations. Finally, for the first time, we assimilate humidity observations multivariately in both perfect model experiments and real data assimilation. We found that multivariate assimilation is better than univariate assimilation. The assimilation of pseudo-RH (Dee and da Silva, 2003) is better than the choice of specific humidity and relative humidity. The multivariate assimilation of AIRS specific humidity retrievals on NCEP GFS system shows positive impact on the winds analysis...|$|R
