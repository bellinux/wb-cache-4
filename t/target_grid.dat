29|139|Public
50|$|The LAV-25A2 {{includes}} the Improved Thermal Sight System (ITSS) developed by Raytheon, scheduled for fielding {{by the end}} of 2007. The ITSS provides the gunner and commander with thermal images, an eye-safe laser range finder, a fire-control solution and far-target location <b>target</b> <b>grid</b> information.|$|E
50|$|Spider9 {{was founded}} on control system {{technology}} patents licensed from the University of Michigan Real-Time Computing Laboratory. In the summer of 2011, the Spider9 leadership team brought the technology out of the Office of Technology Transfer {{where it had been}} incubating. In July 2011, Spider9 pivoted the technology’s business plan to <b>target</b> <b>grid</b> energy storage and solar field optimization rather than electric vehicles. On November 3, 2011, Spider9 received a Michigan Economic Development Corporation (MEDC) grant to install a solar field {{on the roof of the}} Water Wheel Centre in Northville, MI.|$|E
5000|$|On 19 April 2010 the Army {{issued a}} [...] "solicitation for sources sought" [...] from defense {{contractors}} for a munition for the Shadow {{system with a}} deadline for proposals due no later than 10 May 2010. Although no specific munition has been chosen yet, some possible munitions include the General Dynamics 81 mm 4.5 kg (10-pound) air-dropped guided mortar, {{as well as the}} QuickMEDS system for delivering medical supplies to remote and stranded troops. The Army subsequently slowed work, and the Marine Corps then took the lead on arming the RQ-7 Shadow. Raytheon has conducted successful flight tests with the Small Tactical Munition, and Lockheed Martin has tested the Shadow Hawk glide weapon from an RQ-7. On 1 November 2012, General Dynamics successfully demonstrated their guided 81 mm Air Dropped Mortar, with three launches at 7,000 ft hitting within seven meters of the <b>target</b> <b>grid.</b>|$|E
40|$|In {{this paper}} a {{two-phase}} compressive sensing (CS) and received signal strength (RSS) -based target localization approach is {{proposed to improve}} position accuracy by dealing with the unknown target population {{and the effect of}} grid dimensions on position error. In the coarse localization phase, by formulating target localization as a sparse signal recovery problem, grids with recovery vector components greater than a threshold are chosen as the candidate <b>target</b> <b>grids.</b> In the fine localization phase, by partitioning each candidate <b>grid,</b> the <b>target</b> position in a grid is iteratively refined by using the minimum residual error rule and the least-squares technique. When all the candidate <b>target</b> <b>grids</b> are iteratively partitioned and the measurement matrix is updated, the recovery vector is re-estimated. Threshold-based detection is employed again to determine the <b>target</b> <b>grids</b> and hence the target population. As a consequence, both the target population and the position estimation accuracy can be significantly improved. Simulation results demonstrate that the proposed approach achieves the best accuracy among all the algorithms compared...|$|R
40|$|Abstract Virtualized datacenters {{and clouds}} are being {{increasingly}} considered for traditional High-Performance Computing (HPC) workloads that have typically <b>targeted</b> <b>Grids</b> and conventional HPC platforms. However, maximizing {{energy efficiency and}} utilization of datacenter resources, and minimizing undesired thermal behavior while ensuring application performance and other Quality of Service (QoS) guarantees for HPC applications requires careful consideration of important and extremely challenging tradeoffs. Virtual Machine (VM) migration is one of th...|$|R
40|$|Component-based {{development}} {{has emerged as}} an effective approach to building flexible systems, {{but there is little}} experience in applying this approach to Grid programming. This paper presents our experience with reengineering a high performance numerical solver to become a component-based Grid application. The adopted component model is an extension of the generic Fractal model that specifically <b>targets</b> <b>grid</b> environments. The paper provides qualitative and quantitative evidence that componentisation has improved the modifiability and reusability of the application while not significantly affecting performance. 1...|$|R
3000|$|..., as {{it would}} collect the {{centroid}} locations of all targets that {{are present in the}} image given a certain target configuration hypothesis. Different targets could be assumed to move independently of each other when present and to disappear only when they move out of the <b>target</b> <b>grid</b> as discussed in Section 2. Likewise, a change in target configuration hypotheses would result in new targets appearing in uniformly random locations as in (5).|$|E
40|$|Abstract. This work {{summarizes}} the possibilities offered by parallel programming environment ASSIST by outlining {{some of the}} features that will be demonstrated at the conference demo session. We’ll substantially show how this environment can be deployed on a Linux workstation network/cluster, how applications can be compiled and run using ASSIST and eventually, we’ll discuss some ASSIST scalability and performance features. We’ll also outline how the ASSIST environment {{can be used to}} <b>target</b> <b>GRID</b> architectures...|$|E
40|$|This work {{summarizes}} the possibilities offered by parallel programming environment ASSIST by outlining {{some of the}} features that will be demonstrated at the conference demo session. We 9 ̆ 2 ll substantially show how this environment can be deployed on a Linux workstation network/cluster, how applications can be compiled and run using ASSIST and eventually, we 9 ̆ 2 ll discuss some ASSIST scalability and performance features. We 9 ̆ 2 ll also outline how the ASSIST environment {{can be used to}} <b>target</b> <b>GRID</b> architectures...|$|E
40|$|Building and {{evolving}} Grid applications is complex. Component-based {{development has}} emerged as an effective approach to building flexible systems, {{but there is little}} experience in applying this approach to Grid programming. This paper presents our experience with reengineering a high performance numerical solver to become a component-based Grid application. The adopted component model is an extension of the generic Fractal model that specifically <b>targets</b> <b>grid</b> environments. The paper provides qualitative and quantitative evidence that componentisation has improved the modifiability and reusability of the application while not significantly affecting performance...|$|R
40|$|The Computational Grid is a {{promising}} {{platform for the}} ecient execution of parameter sweep applications over large parameter spaces. To achieve performance on the Grid, such applications must be scheduled so that shared data les are strategically placed to maximize re-use, and so that the application execution can adapt to the deliverable performance potential of target heterogeneous, distributed and shared resources. Parameter sweep applications are an important class of applications and would greatly benet from the development of Grid middleware that embeds a scheduler for performance and <b>targets</b> <b>Grid</b> resources transparently...|$|R
40|$|Many {{analyses}} and parameter estimations undertaken in astronomy require a large set (> 10 ^ 5) of non-analytical, theoretical spectra, {{each of these}} defined by multiple parameters. We describe {{the construction of an}} N-dimensional grid which is suitable for generating such spectra. The theoretical spectra are designed to correspond to a <b>targeted</b> parameter <b>grid</b> but otherwise to random positions in the parameter space, and they are interpolated on-the-fly through a pre-calculated grid of spectra. The initial grid is designed to be relatively low in parameter resolution and small in occupied hard disk space and therefore can be updated efficiently when a new model is desired. In a pilot study of stellar population synthesis of galaxies, the mean square errors on the estimated parameters are found to decrease with the <b>targeted</b> <b>grid</b> resolution. This scheme of generating a large model grid is general for other areas of studies, particularly if they are based on multi-dimensional parameter space and are focused on contrasting model differences. Comment: 7 pages, 5 figures, 1 table. Accepted for publication in A...|$|R
40|$|This paper studies a novel multigrid {{approach}} to the solution for a second order upwind biased discretization of the convection equation in two dimensions. This approach is based on semi-coarsening and well balanced explicit correction terms added to coarse-grid operators to maintain on coarse-grid the same cross-characteristic interaction as on the target (fine) grid. Colored relaxation schemes are used on all the levels allowing a very efficient parallel implementation. The results of the numerical tests can be summarized as follows: 1) The residual asymptotic convergence rate of the proposed V(0, 2) multigrid cycle is about 3 per cycle. This convergence rate far surpasses the theoretical limit (4 / 3) predicted for standard multigrid algorithms using full coarsening. The reported efficiency does not deteriorate with increasing the cycle, depth (number of levels) and/or refining the target-grid mesh spacing. 2) The full multi-grid algorithm (FMG) with two V(0, 2) cycles on the <b>target</b> <b>grid</b> and just one V(0, 2) cycle on all the coarse grids always provides an approximate solution with the algebraic error less than the discretization error. Estimates of the total work in the FMG algorithm are ranged between 18 and 30 minimal work units (depending on the target (discretizatioin). Thus, the overall efficiency of the FMG solver closely approaches (if does not achieve) {{the goal of the}} textbook multigrid efficiency. 3) A novel {{approach to}} deriving a discrete solution approximating the true continuous solution with a relative accuracy given in advance is developed. An adaptive multigrid algorithm (AMA) using comparison of the solutions on two successive target grids to estimate the accuracy of the current target-grid solution is defined. A desired relative accuracy is accepted as an input parameter. The final <b>target</b> <b>grid</b> on which this accuracy can be achieved is chosen automatically in the solution process. the actual relative accuracy of the discrete solution approximation obtained by AMA is always better than the required accuracy; the computational complexity of the AMA algorithm is (nearly) optimal (comparable with the complexity of the FMG algorithm applied to solve the problem on the optimally spaced <b>target</b> <b>grid)</b> ...|$|E
40|$|Grid {{computing}} {{is appropriate}} for supporting cooperative work Many designers and engineers from dijerent companies or institutions can anamically form a virtual organization for a given design task. In order to protect each compaq‘s sensitive data and services, access control is therefore necessary and important, In this paper, we present {{a new approach to}} authorize and udminisrrate access requests, in which the requests are obliged to negotiate with a policy enforcement point in order {{to gain access to the}} <b>target</b> <b>Grid</b> service. The new access control model will exploit semantic web technologv, and use machine reasoning about the messages and policies at a semantic level...|$|E
40|$|This paper {{proposes a}} weakly- and self-supervised deep {{convolutional}} neural network (WSSDCNN) for content-aware image retargeting. Our network takes a source image and a target aspect ratio, and then directly outputs a retargeted image. Retargeting is performed through a shift map, which is a pixel-wise mapping from the source to the <b>target</b> <b>grid.</b> Our method implicitly learns an attention map, which leads to a content-aware shift map for image retargeting. As a result, discriminative parts in an image are preserved, while background regions are adjusted seamlessly. In the training phase, pairs of an image and its image-level annotation are used to compute content and structure losses. We demonstrate the effectiveness of our proposed method for a retargeting application with insightful analyses. Comment: 10 pages, 11 figures. To appear in ICCV 2017, Spotlight Presentatio...|$|E
40|$|In the past, product {{identification}} and promotion for market have been policy-led. This paper demonstrates {{how to develop}} an alternative, demand-led strategy. The approach is applied to sustainable social marketing channels for agroforest commodities, notably non-timber products and services. Their development will often benefit the poor social minorities living near or in forests but will not succeed unless other markets – for inputs, credit, labor, and even foreign exchange – exist and are free of inappropriate policies. The successive stages of developing a strategy are presented in turn. These include the acquisition of a new managerial outlook, the four p’s, strategic <b>targeting</b> <b>grids,</b> verification of input and complementary markets, detailed market channel and margin analysis, and Bayesian decision trees to evaluate new information...|$|R
30|$|Besides the <b>target</b> voxel <b>grid,</b> the {{interpolation}} method directly influences the deviation of SUVmax, MTV, and TLG. The {{interpolation method}} {{implemented in the}} used coregistration software is trilinear interpolation. Therefore, our results are strictly speaking only valid for the applied coregistration software. However, similar effects can be expected with other coregistration software.|$|R
40|$|Abstract—Virtualized datacenters {{and clouds}} are being {{increasingly}} considered for traditional High-Performance Computing (HPC) workloads that have typically <b>targeted</b> <b>Grids</b> and conventional HPC platforms. However, maximizing energy efficiency, cost-effectiveness, and utilization of datacenter resources while ensuring performance and other Quality of Service (QoS) guarantees for HPC applications requires careful consideration of important and extremely challenging tradeoffs. An innovative application-centric energy-aware strategy for Virtual Machine (VM) allocation is presented. The proposed strategy ensures high resource utilization and energy efficiency through VM consolidation while satisfying application QoS. While existing VM allocation solutions {{are aimed at}} satisfying only the resource utilization requirements of applications along only one dimension (CPU utilization), the proposed approach is more generic as it employs knowledge obtained through application profiling along multiple dimensions. The results of our evaluation show that the proposed VM allocation strategy enables significant reduction either in energy consumption or in execution time, depending on the optimization goals. I...|$|R
40|$|A {{promising}} {{recent development}} on acoustic source localization and source strength estimation is the generalized inverse beamforming, {{which is based}} on the microphone array cross-spectral matrix eigenstructure. This method presents several advantages over the conventional beamforming, including a higher accuracy on the source center localization and strength estimation even with distributed coherent sources. This paper aims to improve the strength estimation of the generalized inverse beamforming method with an automated regularization factor definition. Also in this work, a virtual <b>target</b> <b>grid</b> is introduced, and source mapping and strength estimation are obtained disregarding, as much as possible, the reflections influence. Two simple problems are used to compare the generalized inverse performance with fixed regularization factor to performance obtained using the optimized regularization strategy. Numerical and experimental data are used, and two other strength estimation methods are also evaluated for reference. status: publishe...|$|E
40|$|AbstractWe suggest {{methods for}} the {{analysis}} of the spatial distribution of plant species in a research area divided into a quadrat lattice. In particular, information about the topography and the spaces without plants is used {{for the analysis}}. At sites with a homogeneous substratum, we classify the topography by whether a <b>target</b> <b>grid</b> is concave or convex with respect to a standard surface of altitude. At other sites, we classify the topography according to whether the grid is located at the edge of rock and/or at a water pool. Information about the topography and the plant existence is used for constructing 2 × 2 contingency tables. In order to determine the strength of dependence between the topography and plant existence, the Akaike information criterion (AIC) is used. The methods are applied to data of the microtopography and distribution of mosses in continental Antarctica...|$|E
40|$|In Numerical Weather Prediction (NWP), to {{initialise}} lake {{variables for}} parameterisation of lakes, lake climatology is required. We obtained model lake climatology through offline runs {{of a lake}} model FLake (Freshwater Lake model) with different values of lake depth parameter. As a result, a global dataset with the resolution of 1 &#x 00 B 0; was developed. To project lake climatology onto a particular NWP model grid, data are extracted from the dataset depending on the lake depth value provided on the <b>target</b> <b>grid.</b> To prevent drifting of the bottom temperature in warm deep lakes, relaxation to the long-term mean air temperature was applied in FLake. Lake model climatology was validated against observations for different types of lakes. We suppose that detected errors for boreal lakes in spring are connected with inaccuracies in forcing data, but further study of errors is needed...|$|E
40|$|In this paper, {{we study}} the {{implementation}} of dense linear algebra kernels, such as matrix multiplication or linear system solvers, on heterogeneous networks of workstations. The uniform block-cyclic data distribution scheme commonly used for homogeneous collections of processors limits the performance of these linear algebra kernels on heterogeneous grids to {{the speed of the}} slowest processor. We present and study more sophisticated data allocation strategies that balance the load on heterogeneous platforms with respect to the performance of the processors. When <b>targeting</b> unidimensional <b>grids,</b> the load-balancing problem can be solved rather easily. When <b>targeting</b> two-dimensional <b>grids,</b> which are the key to scalability and efficiency for numerical kernels, the problem turns out to be surprisingly difficult. We formally state the 2 D load-balancing problem and prove its NP-completeness. Next, we introduce a data allocation heuristic, which turns out to be very satisfactory: Its practical usefulness is demonstrated by MPI experiments conducted with a heterogeneous network of workstations...|$|R
30|$|For this purpose, we {{obtain the}} degree {{distributions}} of RT-Smallworld networks under different α, β, and p rw in advance, {{and use them}} as an offline database. When the degree distribution of the <b>targeted</b> power <b>grid</b> topology P(x) is given, we compare the values of D KL(P||Q) and choose the RT-Smallworld network with the smallest D KL(P||Q). Then the corresponding values of α, β, and p rw are obtained.|$|R
40|$|Grid {{computing}} is a {{new generation}} of distributedcomputing. The <b>target</b> of <b>grid</b> paradigm is how to constructstrong processing power and storage resources by manysmall and weak resources. Gird computing is a mesh of interconnected resourcesworldwide which constructs massive powerful capabilities. The user of the grid has the ability to use any (or many) ofthese interconnected resources in the grid to solve hisproblems, which cannot be solved by locally ownedresources capabilities...|$|R
40|$|Production of the molybdenum- 99 isotope at the Annular Core Research Reactor {{requires}} highly enriched, {{uranium oxide}} loaded targets to be irradiated {{for several days}} in the high neutron-flux region of the core. This report presents the safety analysis for the irradiation of up to seven Cintichem-type targets in the central region of the core and compares the results to the Annular Core Research Reactor Safety Analysis Report. A 19 <b>target</b> <b>grid</b> configuration is presented that allows one to seven targets to be irradiated, with {{the remainder of the}} grid locations filled with aluminum ''void'' targets. Analyses of reactor, neutronic, thermal hydraulics, and heat transfer calculations are presented. Steady-state operation and accident scenarios are analyzed with the conclusion that the reactor can be operated safely with seven targets in the grid, and no additional risk to the public...|$|E
40|$|We suggest {{methods for}} the {{analysis}} of the spatial distribution of plant species in a research area divided into a quadrat lattice. In particular, information about the topography and the spaces without plants is used {{for the analysis}}. At sites with a homogeneous substratum, we classify the topography by whether a <b>target</b> <b>grid</b> is concave or convex with respect to a standard surface of altitude. At other sites, we classify the topography according to whether the grid is located at the edge of rock and/or at a water pool. Information about the topography and the plant existence is used for constructing 2 × 2 contingency tables. In order to determine the strength of dependence between the topography and plant existence, the Akaike information criterion (AIC) is used. The methods are applied to data of the microtopography and distribution of mosses in continental Antarctica. 極域の乾燥地帯において、植物が地形の起伏や岩・水たまりなどの環境にどのように対応して分布しているかを、分割表統計量を用いて評価する方法を提案しました。実例として南極昭和基地周辺の沿岸露岩域において、まばらに蘚類の分布する区画において格子状に調査データを取得し、その地域に多く見られた３種について評価を行い、それぞれの分布の特徴を表現しました...|$|E
40|$|International audienceSUMMARY Classical {{approaches}} to geostatistical simulations {{are not applicable}} directly on irregular reservoir models (such as Voronoi polygon and tetrahedron meshed models). One of the main difficulties is that the block marginal distributions are unique for every block due to volume support effect. We propose a methodology for geostatistical simulations which overcomes this difficulty in an analytical manner and provides a robust utilization of the small support petrophysical property distribution and the covariance model for irregular reservoir models. The proposed solution {{is based on the}} discrete Gaussian (DGM) model and operates directly on blocks of the <b>target</b> <b>grid.</b> This solution is also capable {{to improve the quality of}} the classical reservoir models, such as tartan meshes, by including the volume support effect into consideration and thus-providing geologically more realistic results. Applications to Voronoi polygon grid with local grid refinements and to a tartan-meshed offshore gas reservoir model are demonstrated...|$|E
40|$|The asteroseismic {{analysis}} of white dwarfs {{allows us to}} peer below their photospheres and determine their internal structure. At ~ 28, 000 K EC 20058 - 5234 is the hottest known pulsating helium atmosphere white dwarf. As such, it constitutes an important link {{in the evolution of}} white dwarfs down the cooling track. It is also astrophysically interesting because it is at a temperature where white dwarfs are expected to cool mainly through the emission of plasmon neutrinos. In the present work, we perform an asteroseismic {{analysis of}} EC 20058 - 5234 and place the results in the context of stellar evolution and time dependent diffusion calculations. We use a parallel genetic algorithm complemented with <b>targeted</b> <b>grid</b> searches to find the models that fit the observed periods best. Comparing our results with similar modeling of EC 20058 - 5234 's cooler cousin CBS 114, we find a helium envelope thickness consistent with time dependent diffusion calculations and obtain a precise mode identification for EC 20058 - 5234. Comment: 6 pages, 2 figures, 7 tables, accepted for publication in MNRA...|$|R
50|$|There {{was a more}} {{sophisticated}} interaction between yaw, roll and other sensors: a gyrocompass (set by swinging in a hangar before launch) gave feedback to control the dynamics of pitch and roll, but it was angled away from the horizontal so that controlling these degrees of freedom interacted: the gyroscope remained true {{on the basis of}} feedback received from a magnetic compass, and from the fore and aft pendulum. This interaction meant that rudder control was sufficient for steering and no banking mechanism was needed. In a V-1 that landed in March 1945 between Tilburg and Goirle, Netherlands, without detonating, several rolled issues of the German wartime propaganda magazine Signal were found inserted into the left wing's tubular steel spar, used for weight to preset the missile's static equilibrium before launching. Several of the earliest V-1s to be launched were provided with a small radio transmitter (using a triode valve marked 'S3' but equivalent to a then-current power valve, type RL 2,4T1) to check the general direction of flight related to the launching place's and the <b>target's</b> <b>grid</b> coordinates by radio bearing (navigation).|$|R
30|$|A and B are {{matrices}} of {{the same}} size m×n and s is a vector of size n× 1. The sparse representation dictionary Ψ contains all the possible signal reflected from the <b>target</b> in any <b>grid</b> of time-frequency plane.|$|R
40|$|A multigrid {{approach}} using conditional coarsening {{in constructing}} solvers for non-elliptic equations on a rectangular grid is presented. Such an approach permits {{the achievement of}} a full multigrid efficiency even in the case where the equation characteristics do not align with the grid. The 2 D sonic-flow equation linearized over a constant velocity field has been chosen as the model problem. An efficient FMG solver for the problem is demonstrated. Key words. multigrid methods, conditional coarsening, sonic flow, non-alignment. AMS subject classifications. 65 N 55, 76 H 05, 76 M 20. 1. Introduction. Full-multigrid algorithms {{have proved to be}} the most efficient solvers for discretized elliptic problems. They solve a general elliptic system of discretized partial differential equations in just several minimal work units, where a minimal work unit is defined as the number of computer operations required for the simplest discretization of the problem on the <b>target</b> <b>grid.</b> This efficiency was [...] ...|$|E
40|$|ABSTRACT: The {{generation}} of accurate and reliable unstructured 3 D models for reservoir simulation remains a challenge. In this paper, new developments for grid generation, upscaling and streamline simulation for such models are described. In combination, these techniques provide a prototype workflow {{for the construction}} of unstructured simulation models. The grid generation framework described here allows the incorporation of both geometrical constraints and grid-resolution targets. Flow adaptation of the unstructured grid (i. e. higher grid density in key regions) is accomplished through the use of single-phase flow calculations on the under-lying geocellular grid, which are used to generate <b>target</b> <b>grid</b> resolution maps for the unstructured coarse model. A novel transmissibility upscaling procedure is introduced to capture the effects of fine-scale heterogeneity. A new method for streamline simulation on unstructured grids is also introduced. This technique provides an efficient flow-based diagnostic for the assessment of the coarse simulation model in terms of flow response. The performance of the various components of the methodology is demonstrated using several examples...|$|E
40|$|The {{empirical}} {{relationship between}} salinity and {{temperature in the}} South Atlantic is quantified {{with the aid of}} local regression. To capture the spatial character of the TS relationship, models are fitted to data for each point on a three-dimensional grid with spacing of 1 ° in latitude, 2 ° in longitude, and 25 dbar in the vertical. To ensure sufficient data for statistical reliability each fit is to data from a region extending over several grid points weighted so that more remote data exert less influence than those closer to the <b>target</b> <b>grid</b> point. Both temperature and its square are used as regressors to capture the curvature seen in TS plots, and latitude and longitude are used to capture systematic spatial variations over the fitting regions. In addition to using statistics of residuals to characterize how well the models fit the data, errors for data not used in fitting are examined to verify the models ’ abilities to simulate independent data. The best model overall for the entire region at all depths is quadratic in temperature and linear in longitude and latitude...|$|E
40|$|Abstract—Clouds {{provide the}} {{abstraction}} of nearly-unlimited computing resources through the elastic use of federated resource pools (virtualized datacenters). They are being increasingly considered for HPC applications, which have traditionally <b>targeted</b> <b>grids</b> and supercomputing clusters. However, maximizing {{energy efficiency and}} utilization of cloud datacenter resources, avoiding undesired thermal hotspots (due to overheating of over-utilized computing equipment), and ensuring quality of service guarantees for HPC applications are all conflicting objectives, which require joint consideration of multiple pairwise tradeoffs. The novel concept of heat imbalance, which captures the unevenness in heat generation and extraction, at different regions inside a HPC cloud datacenter is introduced. This thermal awareness enables proactive datacenter management through prediction of future temperature trends {{as opposed to the}} state-of-the-art reactive management based on current temperature measurements. VMAP, an innovative proactive thermal-aware virtual machine consolidation technique is proposed to maximize computing resource utilization, to minimize datacenter energy consumption for computing, and to improve the efficiency of heat extraction. The effectiveness of the proposed technique is verified through experimental evaluations with HPC workload traces under singleas well as federated-datacenter scenarios (in the machine rooms at Rutgers University and University of Florida). Index Terms—Virtualized datacenters, thermal awareness, heat imbalance, consolidation. I...|$|R
2500|$|A 2003 report, by Carbon Trust and the UK Department of Trade and Industry (DTI), {{projected}} {{costs of}} £1.6 to £2.4 billion for reinforcement and new build of transmission and [...] distribution systems to support 10% renewable {{electricity in the}} UK by 2010, and £3.2bn to £4.5bn for 20% by 2020. The study classified [...] "Intermittency" [...] as [...] "Not a significant issue" [...] for the 2010 target but a [...] "Significant Issue" [...] for the 2020 <b>target.</b> See <b>grid</b> balancing ...|$|R
40|$|AbstractIn the paper, support vector {{machine is}} {{proposed}} in IR <b>target</b> recognition. <b>Grid</b> method is used to select the appropriate parameters of SVM to avoid over-fitting due to the choice of inappropriate parameters. We employ coal mine IR monitoring images to testify the IR target recognition ability of SVM. And features and category of the coal mine IR monitoring images are given. The experimental results illustrate that the IR target recognition accuracy of SVM is 100 %. Thus, SVM is an excellent IR target recognition method...|$|R
