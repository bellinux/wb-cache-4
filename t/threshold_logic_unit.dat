14|975|Public
5000|$|The first {{artificial}} neuron was the <b>Threshold</b> <b>Logic</b> <b>Unit</b> (TLU), or Linear Threshold Unit, first proposed by Warren McCulloch and Walter Pitts in 1943. The model was specifically targeted as a computational {{model of the}} [...] "nerve net" [...] in the brain. As a transfer function, it employed a threshold, equivalent to using the Heaviside step function. Initially, only a simple model was considered, with binary inputs and outputs, some restrictions on the possible weights, and a more flexible threshold value. Since the beginning it was already noticed that any boolean function could be implemented by networks of such devices, what is easily seen {{from the fact that}} one can implement the AND and OR functions, and use them in the disjunctive or the conjunctive normal form.Researchers also soon realized that cyclic networks, with feedbacks through neurons, could define dynamical systems with memory, but most of the research concentrated (and still does) on strictly feed-forward networks because of the smaller difficulty they present.|$|E
40|$|A <b>Threshold</b> <b>Logic</b> <b>Unit</b> (TLU) is a {{mathematical}} function {{conceived as a}} crude model, or abstraction of biological neurons. Threshold logic units are the constitutive units in an artificial neural network. In this paper a positive clock-edge triggered T flip-flop is designed using Perceptron Learning Algorithm, which is a basic design algorithm of threshold logic units. Then this T flip-flop is used to design a two-bit up-counter that goes through the states 0, 1, 2, 3, 0, 1 … Ultimately, {{the goal is to}} show how to design simple logic units based on threshold logic based perceptron concepts...|$|E
40|$|Patterns of {{volatile}} metabolites in urine, as {{obtained by}} glass-capillary gas chromatography, were investigated {{by use of}} a nonparametric pattern-recognition method, {{in an effort to}} detect abnormalities associated with diabetes. We used <b>threshold</b> <b>logic</b> <b>unit</b> analysis on a data set consisting of normal subjects and those with diabetes mellitus, and could predict patterns for volatile metabolites as belonging to the proper class in 94. 83 % of the cases examined. In addition, a feature-extraction algorithm isolated those volatile constituents that are most useful in making the normal/diabetic classification. We used gas chromatography/mass spectrometry to identify important profile constituents. Finally, these same pattern-recognition methods indicated strong sex-related patterns in these volatiles...|$|E
40|$|We present {{simulation}} {{mechanisms by}} which any network of <b>threshold</b> <b>logic</b> <b>units</b> with either symmetric or asymmetric interunit connections (i. e., a symmetric or asymmetric "Hopfield net") can be simulated on a network of the same type, but without any a priori constraints {{on the order of}} updates of the units. Together with earlier constructions, the results show that the truly asynchronous network model is computationally equivalent to the seemingly more powerful models with either ordered sequential or fully parallel updates...|$|R
40|$|In this paper, {{we present}} a {{hardware}} friendly binary decision tree (DT) classifier for gas identification. The DT classifier {{is based on an}} axis-parallel decision tree implemented as threshold networks—one layer of <b>threshold</b> <b>logic</b> <b>units</b> (TLUs) followed by a programmable binary tree implemented using combinational logic circuits. The proposed DT classifier circuit removes the need for multiplication operation enabling up to 80 % savings in terms of silicon area and power compared to oblique based-DT while achieving 91. 36 % classification accuracy without throughput degradation. The circuit was designed in 0. 18 μm Charter CMOS process and tested using a data set acquired with in-house fabricated tin-oxide gas sensors...|$|R
40|$|An {{analysis}} {{is made of}} the sensitivity of feedforward layered networks of Adaline elements (<b>threshold</b> <b>logic</b> <b>units)</b> to weight errors. An approximation is derived which expresses the probability of error for an output neuron of a large network (a network with many neurons per layer) {{as a function of}} the percentage change in the weights. As would be expected, the probability of error increases with the number of layers in the network and with the percentage change in the weights. The probability of error is essentially independent of the number of weights per neuron and of the number of neurons per layer, as long as these numbers are large (on the order of 100 or more) ...|$|R
40|$|This paper {{describes}} a hardware implementation of tree classifiers {{based on a}} custom VLSI chip and a CPLD chip. The tree classifier comprises a first layer of <b>threshold</b> <b>logic</b> <b>unit,</b> implemented on a reconfigurable custom chip, followed by a logical function implemented using a CPLD chip. We first describe the architecture of tree classifiers and compare its performance with support vector machine (SVM) for different data sets. The reconfigurability of the hardware (number of classifiers, topology and precision) is discussed. Experimental {{results show that the}} hardware presents a number of interesting feature such as reconfigurability, as well as improved fault tolerance. © 2004 IEEE...|$|E
40|$|Lately, {{interference}} cancellation {{is a hard}} {{challenge in}} a cellular communication system. In this paper, we consider the interference cancellation using logic OR gate from the {{multiple input multiple output}} (MIMO) interference channel where each transmitter and receiver equipped with single or multiple antennas. In our system, we have used a <b>threshold</b> <b>logic</b> <b>unit</b> which is operated by logic OR with NOT operation in the MIMO receiver for interference cancellation. The threshold unit has two logic threshold signals which are separated by two vectors. These two logic thresholds are denoted by High and Low signal in the threshold unit. The High and Low signals are represented by the desired and interference signal, respectively. Our approach is to practically achieve interference alignment and absolutel...|$|E
40|$|A self growing {{binary tree}} neural network is {{introduced}} for the on-line identification of multi-class systems. Instead {{of solving the}} traditional two class problem with a single neuron a time shrinking <b>threshold</b> <b>logic</b> <b>unit</b> is introduced such that the modified neuron has the capability of partitioning the raw data records into three different regions. Incorporating a Least Mean Squares (LMS) learning algorithm provides the capability of detecting and creating new classes and this allows clustering and partitioning of the raw data records into model classes and yields estimates of the parameters. The models which describe the behaviour of the system at different operating regions can be recovered by inspection of the connection weights of the individual neurons. Optimisation procedures for the on-line estimation are also proposed. Simulation studies are included to illustrate the concepts...|$|E
40|$|Abstract. We {{investigate}} {{the generation of}} neural networks through the induction of binary trees of <b>threshold</b> <b>logic</b> <b>units</b> (TLUs). Initially, we describe the framework for our tree construction algorithm and how such trees can be transformed into an isomorphic neural network topology. Several methods for learning the linear discriminant functions at each node of the tree structure are examined and shown to produce accuracy results that are comparable to classical information theoretic methods for constructing decision trees (which use single feature tests at each node). Our TLU trees, however, are smaller and thus easier to understand. Moreover, we show {{that it is possible}} to simultaneously learn both the topology and weight settings of a neural network simply using the training data set that we are given. ...|$|R
40|$|We present {{simulation}} {{mechanisms by}} which any network of <b>threshold</b> <b>logic</b> <b>units</b> with either symmetric or asymmetric interunit connections (i. e., a symmetric or asymmetric "Hopfield net") can be simulated on a network of the same type, but without any a priori constraints {{on the order of}} updates of the units. Together with earlier constructions, the results show that the truly asynchronous network model is computationally equivalent to the seemingly more powerful models with either ordered sequential or fully parallel updates. 1 Introduction A somewhat unsatifying feature of many otherwise interesting constructions of recurrent <b>threshold</b> <b>logic</b> networks (or, more generally, automata networks) is their use of a global synchronizing mechanism. It is commonly assumed that either the computational units in the network update their states fully synchronously in parallel (e. g. [8, 10, 11, 16, 24]), or there is some a priori imposed sequential update order (e. g. [2, 25]), or some intermediate f [...] ...|$|R
40|$|A {{class of}} higher-order {{networks}} called Pi-Sigma networks {{has recently been}} introduced for function approximation and classification [4]. These networks combine the fast training abilities of single-layered feedforward networks with the non-linear mapping of higher-order networks, while using much fewer number of units. In this paper, we investigate the applicability of these networks for shift, scale and rotation invariant pattern recognition. Results obtained using a database of English and Persian characters compare favorably with other neural network based approaches [2, 3]. 1. Introduction Feedforward networks based on single layer of linear <b>threshold</b> <b>logic</b> <b>units</b> (TLUs) can exhibit fast learning, but have limited capabilities. For instance, the ADALINE and the simple perceptron can only realize linearly separable dichotomies [1]. The addition of a layer of hidden units dramatically increases the power of layered feedforward networks. Indeed, networks with a single hidden layer, [...] ...|$|R
40|$|This paper explores some {{algorithms}} for automatic quantization of real-valued datasets using thermometer {{codes for}} pattern classification applications. Experimental {{results indicate that}} a relatively simple randomized thermometer code generation technique can result in quantized datasets that when used to train simple perceptrons, can yield generalization on test data that is substantially better than that obtained with their unquantized counterparts. 1 Introduction Artificial neural networks offer a particularly attractive framework {{for the design of}} pattern classification and inductive knowledge acquisition systems {{for a number of reasons}} including their potential for parallelism and fault tolerance. A single <b>threshold</b> <b>logic</b> <b>unit</b> (TLU), also known as perceptron, is a simple neural network that can be trained to classify a set of input patterns into one of two classes. A TLU is an elementary processing unit that computes a function of the weighted sum of its inputs. Assuming that the [...] ...|$|E
40|$|The {{subject of}} this thesis is the design and {{evaluation}} of an adaptive weight unit based on the transfluxor magnetic core. The search for multi-level or analog storage devices {{has led to the}} study and development of many different types of storage mechanisms, ranging from mechanical and chemical devices to magnetic core devices. This thesis presents the characteristics of a special type of magnetic core, called the transfluxor, used as an adaptive weight element. Design criteria and limits have been established and the relationship of component tolerance and stability of operation is discussed. A number of weight units were constructed and were used in conjuction with a <b>threshold</b> <b>logic</b> <b>unit</b> in order to demonstrate the dynamic characteristics of the weight units. Simple tests were carried out on this system and the results are presented. Conclusions are drawn as {{to the success of the}} design...|$|E
40|$|Many tasks can {{be reduced}} to the problem of pattern {{recognition}} {{and the vast majority of}} applications of learning machines is concerned with such problems. The examples of pattern recognition are speech recognition, handprinted characters recognition, weather forecasting, automatic control of technological processes, etc. The subject matter of this work is the detailed analysis of the basic element of the neuron-net-like learning systems - the Linear <b>Threshold</b> <b>Logic</b> <b>Unit.</b> As a mathematical model a many-dimensional vector space is used. This approach gives clear insight into the properties of the element and is particularly fruitful in the analysis of process of training. In the first part of the work, the general problem of the pattern recognition is presented and some properties of the basic element of the learning machine are discussed. The second part is concerned with the training procedures for the LTLU. Both, geometrical and analytical treatments of the Error Correction Procedures are discussed in details. Other learning procedures are also surveyed...|$|E
40|$|We {{show that}} a {{randomly}} selected N-tuple x of points ofRn with probability> 0 is such that any multi-layer percept ron with the first hidden layer composed of hi <b>threshold</b> <b>logic</b> <b>units</b> can imple-ment exactly 2 2 :~~ ~ (Nil) different dichotomies of x. If N> hin then such a perceptron must have all units of the first hidden layer fully connected to inputs. This implies the maximal capacities (in the sense of Cover) of 2 n input patterns per hidden unit and 2 input patterns per synaptic weight of such networks (both capacities are achieved by networks with single hidden layer and {{are the same as}} for a single neuron). Comparing these results with recent estimates of VC-dimension we find that in contrast to the single neuron case, for sufficiently large nand hl, the VC-dimension exceeds Cover's capacity. ...|$|R
40|$|Constructive {{learning}} algorithms {{offer an}} approach to incremental construction of near-minimal artificial neural networks for pattern classification. Examples of such algorithms include Tower, Pyramid, Upstart, and Tiling algorithms which construct multilayer networks of <b>threshold</b> <b>logic</b> <b>units</b> (or, multilayer perceptrons). These algorithms differ {{in terms of the}} topology of the networks that they construct which in turn biases the search for a decision boundary that correctly classifies the training set. This paper presents an analysis of such algorithms from a geometrical perspective. This analysis helps in a better characterization of the search bias employed by the different algorithms in relation to the geometrical distribution of examples in the training set. Simple experiments with non linearly separable training sets support the results of mathematical analysis of such algorithms. This suggests the possibility of designing more efficient constructive algorithms that dynamically c [...] ...|$|R
40|$|Multi-layer {{networks}} of <b>threshold</b> <b>logic</b> <b>units</b> offer an attractive {{framework for the}} design of pattern classification systems. A new constructive neural network learning algorithm (DistAl) based on inter-pattern distance is introduced. DistAl constructs a single hidden layer of hyperspherical threshold neurons. Each neuron is designed to exclude a cluster of training patterns belonging to the same class. The weights and thresholds of the hidden neurons are determined directly by comparing the interpattern distances of the training patterns. This offers a significant advantage over other constructive learning algorithms that use an iterative (and often time consuming) weight modification strategy to train individual neurons. The individual clusters (represented by the hidden neurons) are combined by a single output layer of threshold neurons. The speed of DistAl makes it a good candidate for datamining and knowledge acquisition from very large datasets. The paper presents results of expe [...] ...|$|R
40|$|This paper {{introduces}} a higher-order neural network called the Binary Pi-sigma Network (BPSN), {{which is a}} feedforward network with a single "hidden" layer and product units in the output layer. As training proceeds, the BPSN forms an internal representation of the conjunctive normal form expression corresponding to the Boolean function to be learned. This enables the network to have a regular structure and to exhibit fast learning. We formally prove that the BPSN can realize any Boolean function. Simulation {{results show that the}} network converges very fast and in a stable manner. Introduction Since the introduction of the McCulloch-Pitts neuron, there have been many efforts to model logical expressions using neural networks [1]. The McCullochPitts neuron {{can be used as a}} <b>threshold</b> <b>logic</b> <b>unit</b> and hence can implement an AND or OR function of its inputs. Negation of the inputs also allows the NOT function. Thus, any Boolean expression can be realized using either the disjunctive normal [...] ...|$|E
40|$|The {{determination}} of the non-linear behaviour of multivariate dynamic systems often presents a challenging and demanding problem. Slope stability estimation is an engineering problem that involves several parameters. The impact of these parameters on the stability of slopes is investigated {{through the use of}} computational tools called neural networks. A number of networks of <b>threshold</b> <b>logic</b> <b>unit</b> were tested, with adjustable weights. The computational method for the training process was a back-propagation learning algorithm. In this paper, the input data for slope stability estimation consist of values of geotechnical and geometrical input parameters. As an output, the network estimates the factor of safety (FS) that can be modelled as a function approximation problem, or the stability status (S) that can be modelled either as a function approximation problem or as a classification model. The performance of the network is measured and the results are compared to those obtained by means of standard analytical methods. Furthermore, the relative importance of the parameters is studied using the method of the partitioning of weights and compared to the results obtained through the use of Index Information Theory. © Springer 2005...|$|E
40|$|The {{instability}} of river bank {{can result in}} considerable human and land losses. The Po river {{is the most important}} in Italy, characterized by main banks of significant and constantly increasing height. This study presents multilayer perceptron of artificial neural network (ANN) to construct prediction models for the stability analysis of river banks along the Po River, under various river and groundwater boundary conditions. For this aim, a number of networks of <b>threshold</b> <b>logic</b> <b>unit</b> are tested using different combinations of the input parameters. Factor of safety (FS), as an index of slope stability, is formulated in terms of several influencing geometrical and geotechnical parameters. In order to obtain a comprehensive geotechnical database, several cone penetration tests from the study site have been interpreted. The proposed models are developed upon stability analyses using finite element code over different representative sections of river embankments. For the validity verification, the ANN models are employed to predict the FS values of a part of the database beyond the calibration data domain. The results indicate that the proposed ANN models are effective tools for evaluating the slope stability. The ANN models notably outperform the derived multiple linear regression models...|$|E
40|$|Multi-layer {{networks}} of <b>threshold</b> <b>logic</b> <b>units</b> offer an attractive {{framework for the}} design of pattern classification systems. A new constructive neural network learning algorithm (DistAl) based on inter-pattern distance is introduced. DistAl uses spherical threshold neurons in a hidden layer to find a cluster of patterns to be covered (or classified) by each hidden neuron. It does not depend on an iterative, expensive and time-consuming perceptron training algorithm to find the weight settings for the neurons in the network, and thus extremely fast even for large data sets. The experimental results (in terms of generalization capability and network size) of DistAl on a number of benchmark classification problems show reasonable performance compared to other learning algorithms despite its simplicity and fast learning time. Therefore, DistAl is a good candidate to various tasks that involve very large data sets (such as largescale datamining and knowledge acquisition) or that require re [...] ...|$|R
40|$|Mehran Sahami Computer Science Department, Stanford University, Stanford, CA 94305, USA Email: sahami@CS. Stanford. EDU Abstract. We {{investigate}} {{the generation of}} neural networks through the induction of binary trees of <b>threshold</b> <b>logic</b> <b>units</b> (TLUs). Initially, we describe the framework for our tree construction algorithm and how such trees can be transformed into an isomorphic neural network topology. Several methods for learning the linear discriminant functions at each node of the tree structure are examined and shown to produce accuracy results that are comparable to classical information theoretic methods for constructing decision trees (which use single feature tests at each node). Our TLU trees, however, are smaller and thus easier to understand. Moreover, we show {{that it is possible}} to simultaneously learn both the topology and weight settings of a neural network simply using the training data set that we are given. 1 Introduction We present a non-incremental algorithm that [...] ...|$|R
40|$|Abstract|Multi-layer {{networks}} of <b>threshold</b> <b>logic</b> <b>units</b> offer an attractive {{framework for the}} design of pattern classication systems. A new constructive neural network learning algorithm (DistAl) based on inter-pattern distance is introduced. DistAl constructs a single hidden layer of spherical threshold neurons. Each neuron is designed to exclude a cluster of training patterns belonging to the same class. The weights and thresholds of the hidden neurons are determined directly by comparing the inter-pattern distances of the training patterns. This o ers a signi cant advantage over other constructive learning algorithms that use an iterative (and often time consuming) weight modi cation strategy to train individual neurons. The individual clusters (represented by the hidden neurons) are combined by a single output layer of threshold neurons. The speed of DistAl makes it a good candidate for datamining and knowledge acquisition from very large datasets. Results of experiments on several arti cial and real-world datasets show that DistAl compares favorably with other neural network learning algorithms for pattern classi cation...|$|R
30|$|The {{discipline}} of neural networks, as other fields of science, {{has a long}} history of evolution with lots of ups and downs. In 1943 Warren McCulloch and Walter Pitts presented the first model of artificial neurons, named the <b>Threshold</b> <b>Logic</b> <b>Unit</b> (TLU). In the last few decades, the subjective analysis of neural networks (NNs) has received huge attention because of its strong applications in numerous fields such as signal and image processing, associative memories, combinatorial optimization and many others [1 – 6]. However, such practical applications of NNs are strongly dependent on the qualitative behaviors of NNs. In both biological and physical models, the occurrence of time delays plays an important role. Time delay, which happens usually due to system process and information flow to a particular part of dynamical systems, is unavoidable. Time delays in NNs may cause unexpected dynamical behaviors, like oscillation and poor performance, in networks; see [7 – 11]. Thus, the analysis on NNs with the effects of time delays has attracted the attention of many researchers and results have been published [12 – 17]. The stability of neural networks with both leakage delay and a reaction-diffusion term is discussed, and several sufficient conditions were obtained with the help of analysis technique and Lyapunov theory [16]. The problem of fixed-time synchronization of memristive neural networks was studied in [17].|$|E
40|$|Graduation date: 1975 The {{well-known}} local adjustment algorithm {{for training}} a <b>threshold</b> <b>logic</b> <b>unit,</b> TLU, is extended {{to a local}} adjustment algorithm for training a network of TLUs Computer simulations show that the extension is unsatisfactory. A new logic for a committee of TLUs, called modified veto logic, and a local adjustment algorithm for training a modified veto committee are described. Unlike a majority committee, a modified veto committee may have members added during training, and the modified veto committee is free to attain a size needed to solve a problem. Computer simulations show that a modified veto committee can solve difficult pattern recognition problems and, in the instances tested, does so more successfully than a majority committee. A technique for using a number of 2 -class classifiers to perform function estimation is described. The 2 -class classifiers are trained {{on a set of}} ordered pairs belonging to the function being estimated; no information about the form of the function is needed; the function can have any number of independent variables; and the accuracy of the estimate increases with the number of 2 -class classifiers used. Computer simulations on artificially generated data and on "real life" data show that this technique provides accurate estimates of functions. It is shown that replacing non-binary variables by binary variables can greatly increase the recognition rate of a TLU...|$|E
40|$|Constructive {{learning}} algorithms o er {{an approach}} to incremental construction of near-minimal arti cial neural networks for pattern classi cation. Examples of such algorithms include Tower, Pyramid, Upstart, and Tiling algorithms which construct multilayer networks of <b>threshold</b> <b>logic</b> <b>units</b> (or, multilayer perceptrons). These algorithms di er {{in terms of the}} topology of the networks that they construct which in turn biases the search for a decision boundary that correctly classi es the training set. This paper presents an analysis of such algorithms from a geometrical perspective. This analysis helps in a better characterization of the search bias employed by the di erent algorithms in relation to the geometrical distribution of examples in the training set. Simple experiments with non linearly separable training sets support the results of mathematical analysis of such algorithms. This suggests the possibility of designing more e cient constructive algorithms that dynamically choose among di erent biases to build near-minimal networks for pattern classi cation. ...|$|R
40|$|During the {{reporting}} period, {{the development of}} the theory and application of methodologies for decision making under uncertainty was addressed. Two subreports are included; the first on properties of general hybrid operators, while the second considers some new research on generalized <b>threshold</b> <b>logic</b> <b>units.</b> In the first part, the properties of the additive gamma-model, where the intersection part is first considered to be the product of the input values and the union part is obtained by an extension of De Morgan's law to fuzzy sets, is explored. Then the Yager's class of union and intersection is used in the additive gamma-model. The inputs are weighted to some power that represents their importance and thus their contribution to the compensation process. In the second part, the extension of binary logic synthesis methods to multiple valued logic synthesis methods to enable the synthesis of decision networks when the input/output variables are not binary is discussed...|$|R
40|$|In this article, a {{hierarchical}} classifier is proposed for classification of ground-cover types of a satellite image of Kangaroo Island, South Australia. The image contains seven ground-cover types, which are categorized {{into three groups}} using principal component analysis. The first group contains clouds only, the second consists of sea and cloud shadow over land, and the third contains land and three types of forest. The sea and shadow over land classes are classified with 99 % accuracy using a network of <b>threshold</b> <b>logic</b> <b>units.</b> The land and forest classes are classified by multilayer perceptrons (MLPs) using texture features and intensity values. The average performance achieved by six trained MLPs is 91 %. In order to improve the classification accuracy even further, the outputs of the six MLPs were combined using several committee machines. All committee machines achieved significant improvement in performance over the multilayer perceptron classifiers, with the best machine achieving over 92 % correct classification...|$|R
40|$|Knowledge based {{artificial}} {{neural networks}} offer an approach for connectionist theory refinement. We present an algorithm for refining and extending the domain theory incorporated in a knowledge based neural network using constructive neural network learning algorithms. The initial domain theory comprising of propositional rules is {{translated into a}} knowledge based network of <b>threshold</b> <b>logic</b> <b>units</b> (<b>threshold</b> neuron). The domain theory is modified by dynamically adding neurons to the existing network. A constructive neural network learning algorithm is used to add and train these additional neurons using a sequence of labeled examples. We propose a novel hybrid constructive learning algorithm based on the Tiling and Pyramid constructive learning algorithms that allows knowledge based neural network to handle patterns with continuous valued attributes. Results of experiments on two non-trivial tasks (the ribosome binding site prediction and the financial advisor) show that our algorithm compares favorably with other algorithms for connectionist theory refinement {{both in terms of}} generalization accuracy and network size...|$|R
40|$|This paper {{investigates the}} {{generation}} of neural networks through the induction of binary trees of <b>threshold</b> <b>logic</b> <b>units</b> (TLUs). Initially, we describe the framework for our tree construction algorithm and show how it helps {{to bridge the gap}} between pure connectionist (neural network) and symbolic (decision tree) paradigms. We also show how the trees of threshold units that we induce can be transformed into an isomorphic neural network topology. Several methods for learning the linear discriminant functions at each node of the tree structure are examined and shown to produce accuracy results that are comparable to classical information theoretic methods for constructing decision trees (which use single feature tests at each node), but produce trees that are smaller and thus easier to understand. Moreover, our results also show that it is possible to simultaneously learn both the topology and weight settings of a neural network simply using the training data set that we are initially gi [...] ...|$|R
40|$|A bagging {{ensemble}} {{consists of}} a set of classifiers trained independently and-combined by a majority vote. Such a combination improves generalization performance but can require. large amounts of memory and computation, a serious drawback for addressing portable real-time pattern recognition applications. We report here a compact three-dimensional (3 -D) multiprecision very large-scale integration (VLSI) implementation of a bagging ensemble. In our circuit, individual classifiers are decision trees implemented as threshold networks-one layer of <b>threshold</b> <b>logic</b> <b>units</b> (TLUs) followed by combinatorial logic functions. The hardware was fabricated using 0. 7 -mum CMOS technology and packaged using MCM-V micro-packaging technology. The 3 -D chip implements up to 192 TLUs operating at a speed of up to 48 GCPPS and implemented in a volume of (w x L x h) = (2 x 2 x 0. 7) cm(3). The 3 -D circuit features a high level of programmability and flexibility offering the possibility to make an efficient use of the hardware resources {{in order to reduce the}} power consumption. Successful operation of the 3 -D chip for various precisions and ensemble sizes is demonstrated through an electronic nose application...|$|R
40|$|We {{prove that}} the problem of {{counting}} the number of stable states in a given Hopfield net is #P-complete, and the problem of computing the size of the attraction domain of a given stable state is NP-hard. 1 Introduction A binary associative memory network, or "Hopfield net" [6], consists of n fully interconnected <b>threshold</b> <b>logic</b> <b>units,</b> or "neurons". Associated to each pair of neurons i; j is an interconnection weight w ij, and to each neuron i a threshold value t i. At any given moment a neuron i can be in one of two states, x i = 1 or x i = Γ 1. Its state at the next moment depends on the current states of the other neurons and the interconnection weights; if sgn(P n j= 1 w ij x j Γ t i) 6 = x i, the neuron may switch to the opposite state. (Here sgn is the signum function, sgn(x) = 1 for x 0, and sgn(x) = Γ 1 for x ! 0.) Whether the state change actually occurs depends on whether the neuron is selected for updating at this moment. In the synchronous update rule, al [...] ...|$|R
40|$|Article dans revue scientifique avec comité de lecture. nationale. National audienceA bagging {{ensemble}} {{consists of}} a set of classifiers trained independently and combined by a majority vote. Such a combination improves generalization performance but can require large amounts of memory and computation, a serious drawback for addressing portable real-time pattern recognition applications. We report here a compact three-dimensional multiprecision VLSI implementation of a bagging ensemble. In our circuit, individual classifiers are decision trees implemented as threshold networks -one layer of <b>threshold</b> <b>logic</b> <b>units</b> (TLUs) followed by combinatorial logic functions. The hardware was fabricated using 0. 7 μ m CMOS technology and packaged using MCM-V micro-packaging technology. The 3 D chip implements up to 192 TLUs operating at a speed of up to 48 GCPPS and implemented in a volume of (w × L × h) =(2 × 2 × 0. 7) cm^ 3. The 3 D circuit features a high level of programmability and flexibility offering the possibility to make an efficient use of the hardware resources {{in order to reduce the}} power consumption. Successful operation of the 3 D chip for various precisions and ensemble sizes is demonstrated through an electronic nose application...|$|R
5000|$|Several {{studies have}} {{implemented}} asynchronous models {{and found that}} their behaviour differs from the synchronous ones. Bersini and Detours (1994) have shown how sensitive Conway's Game of Life is to the updating scheme. Any interesting behaviour disappears in the asynchronous case. Harvey and Bossomaier (1997) pointed out that stochastic updating in random boolean networks results in the expression of point attractors only: there is no repeatable cyclic behaviour, although they introduced the concept of loose cyclic attractors. Kanada (1994) has shown that some one-dimensional CA models that generate non-chaotic patterns when updated synchronously generate edge of chaos patterns when randomised. Orponen (1997) has demonstrated that any synchronously updated network of <b>threshold</b> <b>logic</b> <b>units</b> (see Artificial neuron) can be simulated by a network that has no constraints {{on the order of}} updates. Sipper et al. (1997) investigated the evolution of non-uniform CAs that perform specific computing tasks. These models relax the normal requirement of all nodes having the same update rule. In their models, nodes were organised into blocks. Nodes within a block were updated synchronously, but blocks were updated asynchronously. They experimented with three schemes: (1) at each time step, a block is chosen at random with replacement; (2) at each time step, a block is chosen at random without replacement; (3) at each time step, a block is chosen according to a fixed update order.|$|R
40|$|AbstTact- Knowledge based {{artificial}} {{neural networks}} offer an approach for connectionist theory refinement. We present an algorithm for refining and extending the domain theory incorporated in a knowledge based neural network using constructive neural network learning algorithms. The initial domain theory comprising of propositional rules is {{translated into a}} knowledge based network of <b>threshold</b> <b>logic</b> <b>units</b> (<b>threshold</b> neuron). The domain theory is modified by dynamically adding neurons to the existing network. A constructive neural network learning algorithm is used to add and train these additional neurons using a sequence o € labeled examples. We propose a novel hybrid constructive learning algorithm based on the Tiling and Pyramid constructive learning algorithms that allows knowledge based neural network to handle patterns with continuous valued attributes. Results of experiments on two non-trivial tasks (the ribosome binding side prediction and the financial advisor) show that our algorithm compares favorably with other algorithms for connectionist theory refinement {{both in terms of}} generalization accuracy and network size. tively recruit neurons as needed to improve classification accuracyl lend themselves well to incorporation of prior knowledge into the learning process. The domain theory can be translated into an initial network topology and new rules can be incorporated and inaccuracies in the existing rules (if any) can be corrected by dynamically adding new neurons to the network. These new neurons can be trained using a sequence of labeled examples. This process is illustrate...|$|R
