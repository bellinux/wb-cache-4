36|10000|Public
50|$|Exponential {{smoothing}} {{is commonly}} applied <b>to</b> <b>smooth</b> <b>data,</b> as many window functions are in signal processing, acting as low-pass filters to remove high frequency noise. This method is proceeded by Poisson's use of recursive exponential window functions in convolutions from the 19th century, {{as well as}} Kolmogorov and Zurbenko's use of recursive moving averages from their studies of turbulence in the 1940s.|$|E
50|$|In applied mathematics, the {{regressive}} discrete Fourier series (RDFS) is a {{generalization of}} the discrete Fourier transform where the Fourier series coefficients are computed in a least squares sense and the period is arbitrary, i.e., not necessarily {{equal to the}} length of the data. It was first proposed by Arruda (1992a,1992b). It can be used <b>to</b> <b>smooth</b> <b>data</b> in one or more dimensions and to compute derivatives from the smoothed curve, surface, or hypersurface.|$|E
40|$|A new MOSFET {{parameter}} extraction tool is presented. This tool uses {{digital signal processing}} techniques to increase the signal/noise ratio within well-known extraction methods based on determining maxima of derivatives. This signal processing is related with calculations based on Fourier transform and filtering procedures <b>to</b> <b>smooth</b> <b>data.</b> The procedure is applied to extract three different parameters, and results are compared with other methods using experimental data...|$|E
40|$|<b>Smoothed</b> <b>data</b> maps {{permit the}} reader to {{identify}} general spatial trends by removing the background noise of random variability often present in raw <b>data.</b> <b>To</b> <b>smooth</b> mortality <b>data</b> from 798 small areas comprising the contiguous United States, we extended the head-banging algorithm to allow for differential weighting of the values <b>to</b> be <b>smoothed.</b> Actual and simulated data sets {{were used to determine}} how head-banging smoothed spike and edge features in the data, and to observe the degree to which weighting affected the results. As expected, spikes were generally removed while edges and clusters of high rates near the U. S. borders were maintained by the unweighted head-banging algorithm. Incorporating weights inversely proportional to standard errors had a substantial effect on <b>smoothed</b> <b>data,</b> for example determining whether observed spikes were retained or removed. The process used <b>to</b> obtain the <b>smoothed</b> <b>data,</b> including the choice of head-banging parameters, is discussed. Results are considered in the context of general spatial trends...|$|R
5000|$|In statistics, {{additive}} smoothing, {{also called}} Laplace <b>smoothing</b> (not <b>to</b> {{be confused with}} Laplacian smoothing), or Lidstone smoothing, is a technique used <b>to</b> <b>smooth</b> categorical <b>data.</b> Given an observation x = (x1, …, xd) from a multinomial distribution with N trials and parameter vector θ = (θ1, …, θd), a [...] "smoothed" [...] version of the data gives the estimator: ...|$|R
40|$|In {{this paper}} {{we show that}} linear regularization methods can be {{decomposed}} as a smoothing of the generalized inverse or equivalently as the generalized inverse applied <b>to</b> <b>smoothed</b> <b>data.</b> We give conditions {{on the degree of}} the smoothing based on estimates in Sobolev norms. We also show that the regularization methods form a semi group. Conditions for the order optimality of the methods are provided. Finally we verify the results for the Radon transform as the model in computerized tomography. 1 Introduction We study operator equations Af = g for linear compact operators between Hilbert spaces X and Y. It is well known {{that in the case of}} infinite dimensional range of A the problem of solving Af = g is ill-posed. In order to stabilize the solution regularization methods are applied. Prominent examples are Tikhonov-Phillips regularization, linear iterative methods, the truncated singular value decomposition which all can be written as filtered versions of the pseudoinverse, see [3, 6, 7 [...] ...|$|R
40|$|Velleman and Hoaglin (1981) {{describe}} the nonparametric smoother 4253 H,Twice {{which uses a}} combination of running medians and Hanning <b>to</b> <b>smooth</b> <b>data</b> sequences. Velleman and Hoaglin (1981) suggest that the minimum sample size recommended for this nonparametric smoother is seven. A proof and working example presented here suggest that the recommended minimum data points should be higher than seven. Nonparametric smoothing 4253 H,Twice smoother...|$|E
40|$|We {{present the}} {{discrete}} version of heat kernel smoothing on graph data structure. The method is used <b>to</b> <b>smooth</b> <b>data</b> in an irregularly shaped domains in 3 D images. New statistical properties are derived. As an application, we show how {{to filter out}} data in the lung blood vessel trees obtained from computed tomography. The method can be further used in representing the complex vessel trees parametrically and extracting the skeleton representation of the trees...|$|E
40|$|Abstract. A 17 degree-of-freedom {{humanoid}} robot is developed, and a {{humanoid robot}} teaching {{system based on}} RGB-D sensor is proposed. Through {{the analysis of the}} robot’s joint configuration, the transformation formula from the Cartesian coordinate space to the joint space was deduced. Using Microsoft Kinect, the human body skeleton is mapped to the humanoid robot joint. <b>To</b> <b>smooth</b> <b>data</b> without causing delay, a Velocity-based Selective Mean Filter (VSMF) was proposed. The teaching system was applied on the small humanoid robot and the experiment results verified the effectiveness...|$|E
30|$|The best timing for {{an initial}} public {{offering}} (IPO) is when investors are bullish and when, therefore, high returns are expected (see Baker and Wurgler 2006). This is the theoretical background for the number of IPOs being a good proxy for investor sentiment. SDC Platinum provides the number of IPOs per market and month. As in Baker and Wurgler (2006), we use the sum of IPOs over the last 12   months <b>to</b> <b>smooth</b> the <b>data.</b>|$|R
30|$|Essentially, Eq. (14) is a {{weighted}} average of slopes calculated from data points {{on either side}} of data point i. A three-stage procedure for assessing the effectiveness of different smoothing techniques on pressure derivative data is adopted in this work as follows. First, a noisy pressure signal is created by adding a certain amount of Gaussian noise. Next, various single or combined smoothing techniques are employed <b>to</b> <b>smooth</b> pressure <b>data.</b> Finally, pressure derivative is calculated using Eq. (14).|$|R
40|$|In this paper, {{we propose}} a semi-supervised {{learning}} (SSL) algorithm based on {{local and global}} regularization. In the local regularization part, our algorithm constructs a regularized classifier for each data point using its neighborhood, while the global regularization part adopts a Laplacian regularizer <b>to</b> <b>smooth</b> the <b>data</b> labels predicted by those local classifiers. We show that some existing SSL algorithms {{can be derived from}} our framework. Finally we present some experimental results to show the effectiveness of our method...|$|R
40|$|Last Thursday, April, 14 th, the European Parliament {{gave the}} last kick for the {{adoption}} of the directive for data protection in the police and justice sectors. Jan Philip Albrecht, Marju Lauristin and Vera Jourova presented the new directive as key to improve cooperation across Europe in the fight against terrorism and other serious crime. The Directive is said <b>to</b> <b>smooth</b> <b>data</b> exchanges by setting a common standard of data protection and to protect EU citizens against mass surveillance or indiscriminate bulk data collection. status: publishe...|$|E
30|$|<b>To</b> <b>smooth</b> <b>data</b> {{using this}} model, it is {{necessary}} for the data to be stationary or can be transformed to achieve stationarity. A stationary data has a constant mean with no temporal trend. A quadratic trend in the generated data (Fig.  2) was found when analyzing the data. We removed the trend by first applying a cubic spline fit on the data and second subtracting the original data from the fitted values. All the detrending process was performed using the ITSM time series analysis software (Brockwell and Davis 2013).|$|E
40|$|An {{algorithm}} is presented for generating an m-ary summation tree. The {{algorithm is}} completely general {{and may be}} applied to any length input string. For an N length sequence summed in groups of m sub l at each level l a maximum of 3 L - 2 storage is required. A special case of the general m-ary tree where all m sub l are equal is used <b>to</b> <b>smooth</b> <b>data</b> in a radio frequency interference experiment. The maximum storage required when m l sub l = m for all l reduces to the closed form 3 log m N - 2...|$|E
30|$|If the {{surfaces}} are polygonal a differentiated contact position may jump {{in a way}} that is not intended or evident in the graphics displayed. <b>To</b> <b>smooth</b> the calculated velocity, it is best <b>to</b> <b>smooth</b> the positional <b>data</b> before differentiating. This introduces some latency whose effect is masked to some extent by the dominant low latency contribution of the contact force to the excitation.|$|R
40|$|Universal time (UT 1) {{measurements}} obtained using VLBI, lunar laser ranging (LLR), and BIH optical astronomy are compared. The JPL Kalman filter for {{the earth}} rotation and the polar motion is utilized <b>to</b> <b>smooth</b> one <b>data</b> set or combine data sets for intercomparison. The differences between raw UT 1 <b>data</b> and independently <b>smoothed</b> <b>data</b> are employed <b>to</b> assess {{the accuracy of the}} measured series and the sufficiency of the error budget. Systematic errors in periodic signatures and length of day estimates are analyzed. The data reveal that the techniques of LLR and VLBI agree to within their formal errors. It is determined that the residual error for the LLR is too large and the residual error estimated for the BIH data is too small...|$|R
30|$|Savitzky and Golay (S. Golay) [17] {{derivative}} {{was applied}} to scaling-pretreated spectrum. This pretreatment is based on performing a {{least squares linear regression}} fit of a polynomial around each point in the spectrum <b>to</b> <b>smooth</b> the <b>data.</b> The derivative was then the derivative of the fitted polynomial at each point. The derivative math pretreatment is often used to reduce the nonzero bias in near-infrared spectroscopy [18]. The derivative was calculated with 5 point (1667  Hz) segment smoothing by linear fitting by Matlab software.|$|R
40|$|It is {{occasionally}} necessary <b>to</b> <b>smooth</b> <b>data</b> over domains in "R" -super- 2 {{with complex}} irregular boundaries or interior holes. Traditional methods of smoothing which {{rely on the}} Euclidean metric or which measure smoothness over the entire real plane may then be inappropriate. This paper introduces a bivariate spline smoothing function defined as the minimizer of a penalized sum-of-squares functional. The roughness penalty {{is based on a}} partial differential operator and is integrated only over the problem domain by using finite element analysis. The method is motivated by and applied to two sample smoothing problems and is compared with the thin plate spline. Copyright 2002 The Royal Statistical Society. ...|$|E
40|$|Anisotropic regularization PDE’s (Partial Differential Equation) {{raised a}} strong {{interest}} in the field of image processing. The benefit of PDE-based regularization methods lies in the ability <b>to</b> <b>smooth</b> <b>data</b> in a nonlinear way, allowing the preservation of important image features (contours, corners or other discontinuities). In this article, a selective diffusion approach based on the framework of Extreme Physical Information theory is presented. It is shown that this particular framework leads to a particular regularization PDE which makes the integration of prior knowledge possible within the diffusion scheme. As a proof of feasibility, results of oriented pattern extractions are first presented on ad hoc images and second on a particular medical application: Tagged cardiac MRI (Magnetic Resonance Imaging) enhancement...|$|E
40|$|International audienceAnisotropic regularization PDE's (Partial Differential Equation) {{raised a}} strong {{interest}} in the field of image processing. The benefit of PDE-based regularization methods lies in the ability <b>to</b> <b>smooth</b> <b>data</b> in a nonlinear way, allowing the preservation of important image features (contours, corners or other discontinuities). In this article, a selective diffusion approach based on the framework of Extreme Physical Information theory is presented. It is shown that this particular framework leads to a particular regularization PDE which makes it possible integration of prior knowledge within diffusion scheme. As a proof a feasibility, results of oriented pattern extractions are presented on ad hoc images. This approach may find applicability in vision in robotics...|$|E
50|$|In {{statistics}} and image processing, <b>to</b> <b>smooth</b> a <b>data</b> set {{is to create}} an approximating function that attempts to capture important patterns in the data, while leaving out noise or other fine-scale structures/rapid phenomena. In <b>smoothing,</b> the <b>data</b> points of a signal are modified so individual points (presumably because of noise) are reduced, and points that are lower than the adjacent points are increased leading <b>to</b> a <b>smoother</b> signal. Smoothing may be used in two important ways that can aid in data analysis (1) by being able to extract more information from the data as long as the assumption of smoothing is reasonable and (2) by being able to provide analyses that are both flexible and robust. Many different algorithms are used in smoothing.|$|R
5000|$|In each {{iteration}} of the algorithm, [...] {{is performed}} for all [...] simultaneously. The first question, then, {{is how to}} estimate the density function given a sparse set of samples. One of the simplest approaches is <b>to</b> just <b>smooth</b> the <b>data,</b> e.g., by convolving it with a fixed kernel of width , ...|$|R
5000|$|... #Caption: Climate {{change during}} the last 65 million years as {{expressed}} by the oxygen isotope composition of benthic foraminifera. The Paleocene-Eocene Thermal Maximum (PETM) {{is characterized by a}} brief but prominent negative excursion, attributed to rapid warming. Note that the excursion is understated in this graph due <b>to</b> the <b>smoothing</b> of <b>data.</b>|$|R
40|$|The usual power {{function}} error estimates do not capture the true order of uniform accuracy for thin plate spline interpolation <b>to</b> <b>smooth</b> <b>data</b> functions in one variable. In this {{paper we propose}} a new type of {{power function}} and we show, through numerical experiments, that the error estimate based upon it does match the expected order. As a by-product of our investigations we also discover the precise form of the Peano kernel for univariate thin plate spline interpolation. In addition we demonstrate that a theoretical estimate of the decay rate of the L 2 ¡norm of the Peano kernel is a crucial ingredient for establishing the improved error bound. We close the paper with a brief investigation of the 2 ¡dimensional case...|$|E
40|$|Stochastic {{simulation}} models utilize probability distributions {{to represent a}} multitude of randomly occurring events. Theoretical distributions are commonly used to model the randomness of a real process because they help <b>to</b> <b>smooth</b> <b>data</b> irregularities that may exist due to the values missed {{during the data collection}} phase. These distributions can be selected either by fitting a distribution to the data collected, or based on the known properties of the process being modelled. The incompatibility between specific characteristics of the theoretical distribution and assumptions of simulation and mathematical calculus present an actual problem in supply chains. The paper is based on the analysis of mentioned contradictions. Different approaches to deal with theoretical probability distributions in supply chains are described in the paper...|$|E
40|$|International audienceNon-linear or {{anisotropic}} regularization PDE's (Partial Differential Equation) {{raised a}} strong interest in the field of medical image processing. The benefit of PDE-based regularization methods lies in the ability <b>to</b> <b>smooth</b> <b>data</b> in a nonlinear way, allowing the preservation of important image features (contours, corners or other discontinuities). In this article, we propose a PDE-based method restoration approach integrating a double-well potential as diffusive function. It is shown that this particular potential leads to a particular regularization PDE which makes the integration of prior knowledge about the gradient intensity level to enhance possible. The corresponding method shows interesting properties regarding stability and preservation of fine structures. As a proof a feasibility, results of restoration are presented on natural images to show potentialities of the proposed method. We also address a particular medical application: enhancement of tagged cardiac MRI...|$|E
40|$|Adaptive box-filtering {{algorithms}} {{to remove}} random bit errors and <b>to</b> <b>smooth</b> noisy <b>data</b> have been developed. For both procedures, {{the standard deviation}} of those pixels within a local box surrounding each pixel is used. A series {{of two or three}} filters with decreasing box sizes can be run to clean up extremely noisy images and to remove bit errors near sharp edges. The second filter, for noise <b>smoothing,</b> is similar <b>to</b> the 'sigma filter' of Lee (1983). The technique effectively reduces speckle in radar images without eliminating fine details...|$|R
40|$|The {{thin plate}} spline method {{is a widely}} used data fitting {{technique}} which has the ability <b>to</b> <b>smooth</b> noisy <b>data.</b> We present some example applications of a new mixed finite element discretisation of the thin plate spline method. The new approach works {{with a pair of}} bases for the gradient and the Lagrange multiplier forming a biorthogonal system, thus ensuring that the scheme is numerically efficient and the formulation is stable. We overview of the theoretical foundations of the new approach and give numerical examples in both two and three dimensions...|$|R
40|$|This study {{presents}} a new algorithm for nonlinear rational model identification. The new algorithm {{consists of a}} two-step procedure: a nonlinear rational function smoother is initially designed and used <b>to</b> <b>smooth</b> the <b>data,</b> system identification is then performed based on the smoothed signal. By using the smoothed signal instead of the raw data, the severe noise problems, which arise in the rational model identification, are avoided. The new approach significantly simplifies the procedure for dynamic nonlinear rational model identification, compared with earlier estimators and provides unbiased estimates with {{the same degree of}} accuracy...|$|R
40|$|We {{investigate}} {{the possibility of}} using Gaussian process regression <b>to</b> <b>smooth</b> <b>data</b> on the current past null-cone for use as the input to a relativistic integration scheme. The algorithm we present is designed to reconstruct the metric of spacetime within the class of spherically symmetric dust universes, with or without a cosmological constant. Assuming that gravity is well described by General Relativity, we demonstrate how the algorithm can be employed to test the Copernican principle based on currently available observations. It is shown that currently available data is not sufficient for a conclusive result. The intrinsic noise present in realistic data presents a challenge for our smoothing algorithm and we discuss some of its limitations as well as possible extensions to it. We conclude by demonstrating how a direct determination of the cosmological constant is possible using redshift drift data. Comment: 29 pages, 9 figures, published versio...|$|E
40|$|This paper {{presents}} a new MAC protocol to achieve {{energy efficiency and}} low transmission latency for the static wireless sensor networks. The proposed new protocol is called A-MAC, an Alternative MAC, which assigns modified alternative wakeup schedules to different sensor nodes to minimize the probability of transmission collision. In A-MAC, after all sensor nodes are deployed, the base station will start an initial process to set a height for each node. When the height of each node (= the node’s hop counts to the BS) is decided, the active interval of nodes whose height difference = 1 will be set to be continuous to reduce transmission latency, whereas the active interval of nodes with the same height will stagger to avoid collision (i. e., to reduce the probability of simultaneous transmission). Operating by such alternative wakeup schedules, the new protocol is able <b>to</b> <b>smooth</b> <b>data</b> transmission and meanwhile conserve energy consumption for the energyconstrained wireless sensor networks...|$|E
40|$|The R package cpr {{provides}} {{tools for}} selection of parsimonious B-spline regression models via algorithms coined `control polygon reduction' (CPR) and `control net reduction' (CNR). B-Splines {{are commonly used}} in regression models <b>to</b> <b>smooth</b> <b>data</b> and approximate unknown functional forms. B-Splines are defined by a polynomial order and a knot sequence. Defining the knot sequence is non-trivial, but is critical {{with respect to the}} quality of the regression models. The focus of the CPR and CNR algorithms is to reduce a large knot sequence down to a parsimonious collection of elements while maintaining a high quality of fit. The algorithms are quick to implement and are flexible enough to support many types of data and regression approaches. The cpr package provides the end user collections of tools for the construction of B-spline basis matrices, construction of control polygons and control nets, and the use of diagnostics of the CPR and CNR algorithms...|$|E
40|$|A {{functional}} <b>smoothing</b> approach <b>to</b> {{the analysis}} of PET time course data is presented. By borrowing information across space and accounting for this pooling {{through the use of}} a nonparametric covariate adjustment, it is possible <b>to</b> <b>smooth</b> the PET time course data thus reducing the noise. A new model for functional data analysis, the Multiplicative Nonparametric Random Effects Model, is introduced to more accurately account for the variation in the data. A locally adaptive bandwidth choice helps to determine the correct amount of smoothing at each time point. This preprocessing step <b>to</b> <b>smooth</b> the <b>data</b> then allows Subsequent analysis by methods Such as Spectral Analysis to be substantially improved in terms of their mean squared error...|$|R
40|$|Abstract: Currie, Durban & Eilers (2004) used 2 -dimensional P-splines <b>to</b> <b>smooth</b> {{mortality}} <b>data</b> classi¯ed by age {{at death}} and year of death. In this paper we ap-ply this model to data classi¯ed by {{age at death}}, year of death and year of birth. Discrete cohort e®ects {{are added to the}} model using a method similar to the overdispersion model of Perperoglou and Eilers (2006). This model allows us to decompose the mortality surface into (i) a smooth 2 -dimensional surface in age and time and (ii) discrete cohort e®ects. We illustrate our remarks with the analysis of some German mortality data...|$|R
40|$|This paper {{studies the}} {{estimation}} of conditional quantiles of counts. Given the discreteness of the data, some smoothness has to be artificially imposed on the problem. The methods currently available to estimate quantiles of count data either assume that the counts result from the discretization of a continuous process, or {{are based on a}} smoothed objective function. However, these methods have several drawbacks. We show that it is possible <b>to</b> <b>smooth</b> the <b>data</b> in a way that allows inference to be performed using standard quantile regression techniques. The performance and implementation of the estimator are illustrated by simulations and an application. ...|$|R
