63|1198|Public
50|$|Continuous Queries run periodically, storing {{results in}} a <b>target</b> <b>measurement.</b>|$|E
5000|$|Determine the {{elevation}} {{angle of the}} <b>target</b> (<b>measurement</b> can be made using various devices, e.g. sight attached unit) ...|$|E
5000|$|Determine the {{slant range}} to the <b>target</b> (<b>measurement</b> can be {{performed}} using various forms of range finders, e.g. laser rangefinder) ...|$|E
40|$|This paper investigates {{a method}} for the {{real-time}} design and execution of a space–time sampling strategy {{in the context of}} flood forecasting. Measurements of water level taken by a network of wireless sensors were assimilated into a one-dimensional hydrodynamic model using an ensemble Kalman filter, to create a forecasting model. This research focused on methods for <b>targeting</b> <b>measurements</b> in real-time to be assimilated by the forecasting model, such that the power-limited but flexible sensor network could be used optimally. Two targeting methods were developed. The first <b>targeted</b> <b>measurements</b> systematically over space and time until the forecasting model predicted that the probability of the water level exceeding a pre-defined threshold was less than 5 %. The second method <b>targeted</b> <b>measurements</b> based on the expected decrease in forecasted water level error variance at a validation time and location, quickly calculated for various sets of measurements by an ensemble transform Kalman filter. <b>Targeting</b> <b>measurements</b> based on the decrease in forecast error variance was shown to be more efficient than a systematic sampling method...|$|R
3000|$|In {{addition}} to <b>target</b> <b>measurements,</b> the sensor also returns clutter measurements, which follow Poisson distributions [21]. The clutter measurement density ρ [...]...|$|R
30|$|As a consequence, the SP-JIPDA {{algorithm}} delivers {{much better}} tracking performance {{in terms of}} both track management and trajectory estimation. This can be explained by that the track state at each scan in SP-JIPDA algorithm is improved multiple times via sequential updating with respect to various illuminators, wherein the <b>target</b> <b>measurements</b> detected by different pair of illuminator-receivers are updated by incrementally accurate predicted track states at single scan; therefore, the likelihood of <b>target</b> <b>measurements</b> obtained by the SP-JIPDA algorithm is higher than that by the MJIPDA algorithm, which gives a faster increasing rate of the probability of target existence.|$|R
5000|$|The {{observation}} model [...] {{cannot be}} directly estimated from the data, requiring assumptions {{to be made}} in order to estimate it. Isard 1998 assumes that the clutter which may make the object not visible is a Poisson random process with spatial density [...] and that any true <b>target</b> <b>measurement</b> is unbiased and normally distributed with standard deviation [...]|$|E
50|$|In live {{environment}} MVLPO execution, a special tool makes dynamic changes to a page so that visitors are directed to different executions of landing pages created {{according to an}} experimental design. The system keeps track of the visitors and their behavior—including their conversion rate, time spent on the page, etc. Once sufficient data has accumulated, the system estimates the impact of individual components on the <b>target</b> <b>measurement</b> (e.g., conversion rate).|$|E
5000|$|SLOs should {{generally}} be specified {{in terms of}} an achievement value or service level, a <b>target</b> <b>measurement,</b> a measurement period, and where and how they are measured. As an example, [...] "90% of calls to the helpdesk should be answered in less than 20 seconds measured over a one-month period as reported by the ACD system". Results can be reported as a percent of time that the target answer time was achieved and then compared to the desired service level (90%).|$|E
5000|$|... “On the Relative Importance of Customer Satisfaction and Trust as Determinants of Customer Retention and Positive Word of Mouth,” with Chatura Ranaweera, Journal of <b>Targeting,</b> <b>Measurement</b> and Analysis for Marketing, 12(1), September 2003, 82-90.|$|R
30|$|There are L {{measurement}} paths, {{and each}} target {{can be detected}} at most one time through each of those paths. This {{suggests that there are}} at most L <b>target</b> <b>measurements</b> from each <b>target</b> at each scan.|$|R
40|$|The {{movement}} of computational power and communications capabilities onto networks of sensors {{in the environment}} through the concept of pervasive or ubiquitous computing has initiated opportunities for the delivery of ground-based data in real-time {{and the development of}} adaptive monitoring systems. Measurements of water level taken by a network ofwireless sensors called 'FloodNet' were assimilated into a one-dimensional hydrodynamic model using an ensemble Kalman filter, to create a forecasting model. The ensemble Kalman filter led to an increase in forecast accuracy of between 50 % and 70 % depending on location for forecast lead times of less than 4 hours. This research then focused on methods for <b>targeting</b> <b>measurements</b> in real-time, such that the power limited but flexible resources deployed by the FloodNet project could be used optimally. Two targeting methods were developed. The first <b>targeted</b> <b>measurements</b> systematically over space and time until the forecasting model predicted that the probability of the water level exceeding a pre-defined threshold was less than 5 %. The second method <b>targeted</b> <b>measurements</b> based on the expected decrease in forecasted water level error variance at a validation time and location, quickly calculated for various sets of measurements by an ensemble transform Kalman filter. Estimates of forecast error covariance from the ensemble Kalman filter and ensemble transform Kalman filter were significantly correlated, with correlations ranging between 0. 979 and 0. 292. <b>Targeting</b> <b>measurements</b> based on the decrease in forecast error variance was found to be more efficient than the systematic sampling method. The ensemble transform Kalman filter based targeting method was also used to estimate the 'signal variance' oftheoretical measurements at any computational node in the hydrodynamic model. Furthermore, time series data, different sensors types and measurements of floodplain stage could all be taken into account either as part of the targeting process or prior to <b>measurement</b> <b>targeting.</b> EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Experience in {{applications}} of TLS for deformation monitoring of concrete structures {{have shown the}} need of further exhaustive study {{about the quality of}} retro-reflecting <b>target</b> <b>measurement.</b> Indeed, this factor is highly influencing the georeferencing that is of primary importance to compare scans acquired at different epochs. Here some tests carried out by means of a RIEGL LMS-Z 420 i laser scanner are reported. In addition, several algorithms for automatic retro-reflecting <b>target</b> <b>measurement</b> have been applied, and new ones proposed. Results obtained from lab experiments have revealed a good repeatability on <b>target</b> <b>measurement,</b> while the accuracy is highly affected by a bias on the measured range. This mainly depends on the distance of the target from TLS, and on the angle of incidence of laser beam. In order to compensate for this error, two approaches have been successfully tested: the estimation of a corrective function, and the use of an algorithm for automatic retro-reflecting <b>target</b> <b>measurement</b> which locally estimates the bias on the range. 1...|$|E
3000|$|From {{the above}} equation, Lt {{measurement}} accuracy depends on DNt, DN 0, α, τR 0, LR 0, τ_R^'/τ_R 0 ^', L_R^'/L_R 0 ^', τ 1, and L 1. The above uncertainty parameters are analyzed as follows: <b>target</b> <b>measurement</b> output value DNt {{is better than}} 1 [...]...|$|E
40|$|This study {{examined}} the impact of 2 ̆ 2 ability-based 2 ̆ 2 and 2 ̆ 2 effort-based 2 ̆ 2 verbal reinforcement cues prior to task engagement in traditional sixth grade students attending Christian schools in northwest Indiana. Perseverance levels were measured during a numerically-based, problem solving task by tracking time signatures (in seconds) of the first, second, and third use of restricted 2 ̆ 2 clues. 2 ̆ 2 The research population (n = 102) was randomly assigned into two groups (ability-cued and effort-cued). Statistical significance was found at all three measurements. <b>Target</b> <b>measurement</b> one revealed ME = 518. 7 (SD = 310. 7), MA = 402. 4 (SD = 293. 5), with two sample t(100 df) = 1. 94, p = 0. 027. <b>Target</b> <b>measurement</b> two revealed ME = 645. 9 (SD = 287. 1), MA = 494. 0 (SD = 296. 8), with two sample t(100 df) = 2. 62, p = 0. 004. <b>Target</b> <b>measurement</b> three revealed ME = 738. 6 (SD = 249. 1), MA = 586. 6 (SD = 285. 6), with two sample t(100 df) = 2. 86, p = 0. 002. The null hypothesis stating ability-cued students would show greater levels of perseverance was rejected at all three measurement targets. Students receiving effort commendations prior to task engagement showed greater levels of perseverance than students receiving ability commendations...|$|E
30|$|Tracks are {{initialized}} {{and updated}} using the measurements obtained in each scan. Track initialization in a cluttered environment results in both true tracks and false tracks. True tracks always follow the <b>target</b> <b>measurements,</b> whereas false tracks {{do not follow}} the <b>target</b> <b>measurements.</b> A technique called false track discrimination (FTD) is used to confirm the true tracks and terminate the false tracks. Almost all target-tracking algorithms employ track quality measures to achieve FTD. But the standard probabilistic data association (PDA) [1] {{does not provide a}} track quality measure for FTD. The multiple hypothesis tracker (MHT) [2] uses a sequential probability ratio as a track quality measure for FTD. In [3], the authors proposed a track-oriented MHT algorithm and applied a smoothing velocity vector for reduction of the false track establishment.|$|R
30|$|Figures  7 a, b confirm {{also the}} {{considerations}} of Section 5.2 {{regarding the use}} of single <b>target</b> <b>measurements</b> for subspace computation; occupancy detection and data reduction/filtering are therefore obtained just comparing the properties of the empty environment with respect to the ones of the occupied environment when only one target is present.|$|R
40|$|An {{efficient}} {{sampling strategy}} should address knowledge gaps, rather than exhaustively collect redundant data. In this study, spatial uncertainty in DEM estimates {{was used to}} locate targeted sampling areas in the field. An agricultural vehicle equipped with RTK-DGPS was driven across a 2. 3 ha field area to measure the field elevation. Data were collected at 3. 05 m (10 ft) intervals in a continuous fashion at a speed of 9. 6 mph. A geostatistical simulation technique was used to simulate field DEMs with different measurement pass intervals and to quantitatively assess the spatial uncertainty of the DEM estimates. The high uncertainty areas for each DEMs were classified using image segmentation methods and targeted sampling was performed on those areas. The resulting DEMs were compared {{with each other to}} evaluate the effect of including <b>targeted</b> <b>measurement</b> on DEM accuracy. The addition of <b>targeted</b> <b>measurements</b> significantly reduced the time dedicated for the re-sampling effort and resulted in DEMs with lower RMSE. For the widest interval between sampling passes, the RMSE of 0. 46 m of the DEM was reduced to 0. 25 m after adding the <b>targeted</b> <b>measurements</b> which was close to the 0. 22 m RMSE of DEM with whole field re-sampling. The results show that spatial uncertainty models are useful to design targeted sampling for field mapping. The method is not limited to map elevation data but can be extended for mapping other spatial data...|$|R
40|$|This {{document}} is the December 1948 Physics Division Progress Report from the Mound Laboratory. Items covered include: (1) {{measurement of the}} vapor pressure of polonium, (2) x-ray diffraction analysis of tantalum, (3) use of the vacuum balance, and (4) updates on efforts to prepare an x-ray <b>target,</b> <b>measurement</b> of the resistivity of polonium, and construction of polonium gamma standards...|$|E
40|$|Absorptive A/sub 2 / {{exchange}} is studied using the recent high statistics pi /sup -/p to pi /sup -/ pi /sup +/n CERN-Munich data at 17. 2 GeV/c. The model amplitudes are {{compared with a}} recent amplitude analysis and their importance for polarized <b>target</b> <b>measurement</b> (imaginary parts of density matrix elements) are discussed and high- energy predictions given. (23 refs) ...|$|E
40|$|In {{order to}} improve the {{measurement}} accuracy of the linear CCD in the intersection of vertical <b>target</b> <b>measurement</b> system, the calibration model of linear CCD’s internal parameters is established and the various factors that affect the accuracy in the calibration process are analyzed. Making use of the linear CCD imaging optical principle, the optical center and the effective focal length are calibrated by using upright and inverted mirror state theory and N-style array streak model respectively. It can be proved through theoretical analysis and argument that the method can effectively reduce {{the impact of the}} factors as real object height to calibration accuracy and obtain the high precision calibration results which are satisfied the application indicators of linear CCD in the intersection of vertical <b>target</b> <b>measurement</b> system. It can be known from the experimental data that the optical canter calibration accuracy is less than 0. 29 pixels and the focal length is less than 2. 5 ‰...|$|E
40|$|The {{following}} {{article is}} copyrighted material by Henry Stewart Publications, and {{is scheduled for}} publication in a forth-coming issue of the Journal of <b>Targeting</b> <b>Measurement</b> Analysis for Marketing Neither the above titled article nor any part may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopying, microfilming, and recording, or by any information storage or retrieval system, without prio...|$|R
40|$|Calibrated {{cameras are}} an {{extremely}} useful resource for computer vision scenarios. Typically, cameras are calibrated through calibration <b>targets,</b> <b>measurements</b> {{of the observed}} scene, or self-calibrated through features matched between cameras with overlapping fields of view. This paper considers an approach to camera calibration based on observations of a pedestrian and compares the resulting calibration to a commonly used approach requiring that measurements {{be made of the}} scene...|$|R
40|$|We {{discuss the}} QCD and electroweak physics that becomes {{accessible}} by {{the analysis of}} semi-leptonic neutral weak amplitudes in polarized electron-light ion collisions at an EIC. Specifically, we discuss the reach for precise weak mixing angle measurements at much higher Q 2 than fixed <b>target</b> <b>measurements,</b> new neutral current spin-independent and -dependent interference structure functions, and searches for e − τ charged lepton flavor violation...|$|R
40|$|A Gaussian Mixture (GM) target {{tracking}} solution {{is a natural}} consequence of the multi-{{target tracking}} in clutter, given a linear target trajectory propagation and a linear <b>target</b> <b>measurement</b> equation. We examine and compare two prominent GM target trackers: the Multi Hypothesis Tracking (MHT) and the Integrated Track Splitting. Both incorporate the false track discrimination capabilities, enabling automatic target tracking {{in the presence of}} clutter measurements and missed detections...|$|E
40|$|ABSTRACT — This paper {{presents}} {{an overview of}} nonlinear measurement techniques of microwave power devices and amplifiers. Several useful measurement techniques of nonlinear components available in Europe are described. Trends, especially {{in the area of}} high power and time domain measurements, are discussed. Finally, a summary of the <b>TARGET</b> <b>measurement</b> related tasks is proposed, in order to show how TARGET can improve the European capabilities in terms of nonlinear measurements. I...|$|E
30|$|According to {{specific}} dynamic target infrared radiation measurement requirements, the above measurement model is improved. The calibration and {{data processing system}} involved in the model is composed of calibration equipment, atmospheric correction equipment, and target characteristics data processing equipment. The system mainly completes the radiation calibration of the infrared measurement equipment and the inversion processing of the <b>target</b> <b>measurement</b> data, and finally obtains the infrared radiation characteristics of the target.|$|E
3000|$|In target tracking, sensors detect targets as well {{as various}} {{unwanted}} objects in the surveillance area. Information about the targets’ prior existence in the surveillance area is not known. The objects’ sources are also unknown, with some possible sources being terrain reflections, thermal noise, and clouds. The unwanted objects are generally known as clutter. In a cluttered environment, the <b>target</b> <b>measurements</b> are present with a low probability of detection P [...]...|$|R
40|$|Summary: ChromoScan is an {{implementation}} of a genome-based scan statistic that detects genomic regions which are statistically significant for <b>targeted</b> <b>measurements</b> such as genetic associations with disease, gene expression profiles, DNA copy number variations, {{as well as other}} genome based measurements. A Java graphic user interface (GUI) is provided to allow users to select appropriate data transformations and thresholds for defining the significant events. Availability: ChromoScan is freely available fro...|$|R
40|$|Sensor Web observing {{systems may}} have the {{potential}} to significantly improve our ability to monitor, understand, and predict the evolution of rapidly evolving, transient, or variable environmental features and events. This improvement will come about by integrating novel data collection techniques, new improved instruments, emerging communications technologies, and interoperable planning and scheduling systems. In contrast with today’s observing systems, “event-driven ” sensor webs will synthesize near-real time measurements and information from other platforms and then reconfigure themselves to invoke new measurement modes and adaptive observation strategies. Meteorological prediction models may also serve to initiate new measurement modes (e. g., higher spatial, temporal resolution) or to target observations to specific regions. These “model-driven ” sensor webs will complement event-driven measurements. Platforms will be tasked to <b>target</b> <b>measurements</b> within specific areas where sensitivity to initial conditions may cause ensemble forecasts to diverge when predicting the future state of atmospheric features (e. g., hurricane track) or when discriminating subtle yet critical differences in atmospheric states (e. g., winter precipitation type and location). The <b>targeted</b> <b>measurements</b> would then be assimilated t...|$|R
40|$|This paper {{presents}} {{an overview of}} nonlinear measurement techniques of microwave power devices and amplifiers. Several useful measurement techniques of nonlinear components available in Europe are described. Trends, especially {{in the area of}} high power and time domain measurements, are discussed. Finally, a summary of the <b>TARGET</b> <b>measurement</b> related tasks is proposed, in order to show how TARGET can improve the European capabilities in terms of nonlinear measurements...|$|E
40|$|Keywords：Monte-Carlo, Objective measurement, {{correction}} threshold Abstract. Aiming at {{the coupling}} effect of target radiation measurement error and trajectory correction threshold to the hitting accuracy on the Last revised bullets, the optimal design solution based on Monte- Carlo method is proposed, and the CEP curve under {{the conditions of}} different correction threshold and <b>target</b> <b>measurement</b> errors by simulation is obtained. Moreover, objective indicators of measurement error and the corresponding threshold of amendments are given. A design basis for system development is obtained...|$|E
40|$|Based on the {{principle}} of CCD vertical <b>target</b> <b>measurement</b> system, this article researched the factors of the CCD camera that affects the target detection performance, including CCD sensitivity and exposure, background illumination and target illumination, CCD distinguish ability, contrast between background and objectives and gave the relevant calculation methods. Through theoretical analysis, the article got the relationship among the target length, scan time, image distance, object distance and the detection speed, the detection distance. What’s more, the simulation result shows the effectiveness of the analytic conclusions. ...|$|E
25|$|Position keeper: This {{computer}} {{generates a}} continuously updated {{estimate of the}} target position based on earlier <b>target</b> position <b>measurements.</b>|$|R
40|$|Geant 4 is a {{software}} toolkit for the simulation {{of the passage}} of particles through matter. It has abundant hadronic models from thermal neutron interactions to ultra relativistic hadrons. An overview of validations in Geant 4 hadronic physics is presented based on thin <b>target</b> <b>measurements.</b> In most cases, good agreement is available between Monte Carlo prediction and experimental data; however, several problems have been detected which require some improvement in the models...|$|R
40|$|Evidence for {{the onset}} of deconfinement in central Pb+Pb {{collisions}} was reported by NA 49 in fixed <b>target</b> <b>measurements</b> at beam momentum 30 A GeV/c. This observation motivated the NA 61 /SHINE program started in 2009 at the CERN SPS, which, in particular, aims to study properties of {{the onset of}} deconfinement by measurements of hadron production in proton-proton, proton-nucleus and nucleus-nucleus collisions. This contribution presents spectra of charged pions produced in p+p interactions and $^ 7 $Be+$^ 9 $Be collisions at 20 A- 158 A GeV/c beam momentum. The NA 61 /SHINE results are compared with the corresponding NA 49 data from central Pb+Pb collisions at the same beam momenta per nucleon. Evidence for the onset of deconfinement in central Pb+Pb collisions was reported by NA 49 in fixed <b>target</b> <b>measurements</b> at beam momentum 30 A GeV/c. This observation motivated the NA 61 /SHINE program started in 2009 at the CERN SPS, which, in particular, aims to study properties of the onset of deconfinement by measurements of hadron production in proton-proton, proton-nucleus and nucleus-nucleus collisions. This contribution presents spectra of charged pions produced in p+p interactions and $^ 7 $Be+$^ 9 $Be collisions at 20 A- 158 A GeV/c beam momentum. The NA 61 /SHINE results are compared with the corresponding NA 49 data from central Pb+Pb collisions at the same beam momenta per nucleon. Evidence for the onset of deconfinement in central Pb[*]+[*]Pb collisions was reported by NA 49 in fixed <b>target</b> <b>measurements</b> at beam momentum 30 A GeV/c. This observation motivated the NA 61 /SHINE program started in 2009 at the CERN SPS, which, in particular, aims to study properties of the onset of deconfinement by measurements of hadron production in proton-proton, proton–nucleus and nucleus-nucleus collisions...|$|R
