10|6274|Public
50|$|The first desktop in the D Series was the D400. The D400 desktop was {{designed}} as a home server, offering up to 8TB of storage space, support for multiple external storage devices with five USB ports. An eSATA port allowed high speed data transfer. Additional features of the desktop included the ability <b>to</b> <b>duplicate</b> <b>data</b> on multiple hard disks and remote access to the server.|$|E
40|$|Abstract—In data grids, data {{replication}} on variant nodes can change some {{problems such as}} response time and availability. Also, in {{data replication}}, there are some challenges to finding the best replica efficiently in relation to performance and location of physical storage systems. In this paper, various replica placement strategies are discussed. These replica placement strategies {{are available in the}} works. Replica placement contains recognizing the best possible node <b>to</b> <b>duplicate</b> <b>data</b> based on network latency and user request. These strategies measure and analyze different parameters such as access cost, bandwidth consumption, scalability, execution time and storage consumption. This paper also analyses the performance of various strategies with respect to the parameters mentioned above in data grid...|$|E
40|$|International audienceWe {{focus on}} circuit {{switching}} optical networks and on repetitive multicast demands whose source and destinations are à priori known by an operator. He may have corresponding trees to be allocated" and adapt his network infrastructure {{according to these}} transmissions. This adjustment consists in setting branching routers in the selected nodes of a predefined tree. The branching nodes are opto-electronic nodes which are able <b>to</b> <b>duplicate</b> <b>data</b> and retransmit it in several directions. We {{are interested in the}} choice of nodes of a multicast tree where the limited number of branching routers should be located to minimize the amount of required bandwidth. After formally stating the problem we solve it by proposing a polynomial algorithm whose optimality we prove. We perform computations for different methods of the tree construction and conclude by giving dimensioning guidelines...|$|E
30|$|Among the 6 meta-analyses {{including}} duplicate publications, we {{were unable}} to collect a full dataset of 4 meta-analyses. To evaluate the impact of duplicate publication on the results of meta-analyses, we thus reviewed 2 cases of meta-analyses including duplicate publications (meta-analyses 2 and 3 in Table  1). When a meta-analysis was performed without <b>duplicated</b> <b>data,</b> the mean effect size was 2.0054 (95 % confidence interval [CI]: 1.8553, 2.1554) in meta-analysis 3. However, the mean effect size was increased to 2.1394 (95 % CI: 1.6248, 2.6570) with <b>duplicated</b> <b>data.</b> The fail-safe number was also increased <b>to</b> 209.1 with <b>duplicated</b> <b>data</b> compared <b>to</b> 203.6 without <b>duplicated</b> <b>data.</b> In the case of meta-analysis 2, {{there was no difference in}} the mean effect size without <b>duplicated</b> <b>data</b> compared <b>to</b> the original article. However, the fail-safe number was increased <b>to</b> 17.5 with <b>duplicated</b> <b>data</b> compared <b>to</b> 14.7 without <b>duplicated</b> <b>data.</b>|$|R
40|$|Accommodating {{disjoint}} security realms is {{a challenge}} for administrators who have <b>to</b> maintain <b>duplicate</b> <b>data</b> sets and for users who need to recall multiple pass phrases, yet joining security realms together can expose one realm to the weaknesses of the other. In this paper, we compare the Kerberos and NetWare security realms, examine methods o...|$|R
50|$|To date, data {{deduplication}} has predominantly {{been used}} with secondary storage systems. The {{reasons for this}} are two-fold. First, data deduplication requires overhead to discover and remove the <b>duplicate</b> <b>data.</b> In primary storage systems, this overhead may impact performance. The second reason why deduplication is applied to secondary data, is that secondary data tends <b>to</b> have more <b>duplicate</b> <b>data.</b> Backup application in particular commonly generate significant portions of <b>duplicate</b> <b>data</b> over time.|$|R
40|$|The {{existing}} Computerized On-Line Police System in {{use by the}} Naval Postgraduate School Police Department {{lacks the}} information storage, retrieval, query, and data-sharing functionality necessary to maintain personnel and related vehicle and citation records of approximately 7, 000 people. The flat file system in use by two different users creates data inconsistencies and wastes time and storage resources by trying <b>to</b> <b>duplicate</b> <b>data</b> on two different machines. The approach taken {{to solve this problem}} was the application of a modified Waterfall development cycle to create a new database application that would replace the flat-file system. This thesis describes the development effort from analysis through implementation. The resulting application is called COPS for Windows. COPS for Windows allows more than seventy-five people simultaneous access to the same data files while providing different views of the information and maintains data integrity and consistency. The program generates reports such as the weekly docket, suspension letters, and probation lists. Data entry screens and reports are modeled on existing paper documents. COPS for Windows is a network-ready, multi-user, information system with extensive querying, reporting, and storage capabilities to integrate support cost effectively. (AN) NANAU. S. Marine Corps (U. S. M. C.) author...|$|E
40|$|The lack of wide {{deployment}} of IP multicast in the Internet has prompted researchers to pro-pose end system multicast or application-level multicast as an alternate approach. However, end system multicast, {{by its very}} nature, suffers from several performance limitations, including, high communication overheads due <b>to</b> <b>duplicate</b> <b>data</b> transfers over same physical links, uneven load distribution caused by widely varying resource availabilities at nodes, and highly failure-prone na-ture of end hosts. This paper presents a self-configuring, efficient and failure-resilient end-system multicast system called PeerCast. Three unique features distinguish PeerCast from existing ap-proaches to application-level multicasting. First, {{with the aim of}} exploiting network proximity of end-system nodes for efficient multicast subscription management and fast information dissemi-nation, we propose a novel Internet-landmark signature technique to cluster the end hosts of the overlay network. Second, we propose a capacity aware overlay construction technique to balance the multicast workload among heterogeneous end-system nodes. Third, we develop a dynamic pas-sive replication scheme to provide reliable end system multicast services in an inherently dynamic environment of unreliable peers. We also present a set of experiments showing the feasibility and the effectiveness of the proposed mechanisms and techniques. 1...|$|E
40|$|Operators of {{networks}} covering large areas {{are confronted with}} demands from some of their customers who are virtual service providers. These providers may call for the connectivity service which fulfils the specificity of their services, for instance a multicast transition with allocated bandwidth. On the other hand, network operators want to make profit by trading the connectivity service of requested quality to their customers and to limit their infrastructure investments (or do not invest anything at all). We focus on circuit switching optical networks and work on repetitive multicast demands whose source and destinations are a ̀ priori known by an operator. He may therefore have corresponding trees “ready to be allocated ” and adapt his network in-frastructure according to these recurrent transmissions. This adjustment consists in setting available branching routers in the selected nodes of a predefined tree. The branching nodes are opto-electronic nodes which are able <b>to</b> <b>duplicate</b> <b>data</b> and re-transmit it in several directions. These nodes are, however, more expensive and more energy consuming than transparent ones. In this paper {{we are interested in}} the choice of nodes of a multicast tree where the limited number of branching routers should be located in order to minimize the amount of required bandwidth. After formally stating the problem we solve it by proposing a polynomial algorithm whose optimality we prove. We perform exhaustive computations to show an operator gain obtained by using our algorithm. These computations are 1 a...|$|E
50|$|Cryptographic hash {{functions}} {{have many}} information-security applications, notably in digital signatures, message authentication codes (MACs), {{and other forms}} of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, <b>to</b> detect <b>duplicate</b> <b>data</b> or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.|$|R
5000|$|In 2009 {{the company}} {{released}} the product {{as a system}} image for VMware virtual machines. In 2009 the company acquired intellectual property and engineering resources from Kadena Systems in a deal that added source-based data deduplication to the software. The technology uses a sliding window approach <b>to</b> identify <b>duplicate</b> <b>data</b> which checks the data stream one byte {{at a time until}} it finds blocks that match what the application has seen before. [...] The current version, 9.0, automatically adjusts block sizes based on file type in order to maximize dedupe ratios.|$|R
5000|$|Client backup {{deduplication}}. This is {{the process}} where the deduplication hash calculations are initially created on the source (client) machines. Files that have identical hashes to files already in the target device are not sent, the target device just creates appropriate internal links <b>to</b> reference the <b>duplicated</b> <b>data.</b> The benefit {{of this is that}} it avoids data being unnecessarily sent across the network thereby reducing traffic load.|$|R
40|$|International audienceOperators of {{networks}} covering large areas {{are confronted with}} demands from some of their customers who are virtual service providers. These providers may call for the connectivity service which fulfills the specificity of their services, for instance a multicast transmission with allocated bandwidth. On the other hand, network operators want to make profit by trading the connectivity service of requested quality to their customers and to limit their infrastructure investments (or do not invest anything at all). We focus on circuit switching optical networks and work on repetitive multicast demands whose source and destinations are à priori known by an operator. He may therefore have corresponding trees "ready to be allocated" and adapt his network infrastructure according to these recurrent transmissions. This adjustment consists in setting available branching routers in the selected nodes of a predefined tree. The branching nodes are opto-electronic nodes which are able <b>to</b> <b>duplicate</b> <b>data</b> and retransmit it in several directions. These nodes are, however, more expensive and more energy consuming than transparent ones. In this paper {{we are interested in}} the choice of nodes of a multicast tree where the limited number of branching routers should be located in order to minimize the amount of required bandwidth. After formally stating the problem we solve it by proposing a polynomial algorithm whose optimality we prove. We perform exhaustive computations to show an operator gain obtained by using our algorithm. These computations are made for different methods of the multicast tree construction. We conclude by giving dimensioning guidelines and outline our further work. Keywords: optical network, multicast, optimization algorithm, complexity Mots-clés : réseau optique, multicast, algorithme d'optimisation, complexit...|$|E
40|$|Unintended {{consequences}} of electronic health records represent undesired effects on individuals or systems, which may contradict initial goals and impact patient care. The {{purpose of this}} study was to determine the extent to which a new quantitative measure called the Carrington-Gephart Unintended Consequences of Electronic Health Record Questionnaire (CG-UCE-Q) was valid and reliable. Then, it was used to describe acute care nurses' experience with unintended {{consequences of}} electronic health records and relate them to the professional practice environment. Acceptable content validity was achieved for two rounds of surveys with nursing informatics experts (n = 5). Then, acute care nurses (n = 144) were recruited locally and nationally to complete the survey and describe the frequency with which they encounter unintended consequences in daily work. Principal component analysis with oblique rotation was applied to evaluate construct validity. Correlational analysis with measures of the professional practice environment and workarounds was used to evaluate convergent validity. Test-retest reliability was measured in the local sample (N = 68). Explanation for 63 % of the variance across six subscales (patient safety, system design, workload issues, workarounds, technology barriers, and sociotechnical impact) supported construct validity. Relationships were significant between subscales for electronic health record-related threats to patient safety and low autonomy/leadership (P <. 01), poor communication about patients (P <. 01), and low control over practice (P <. 01). The most frequent sources of unintended consequences were increased workload, interruptions that shifted tasks from the computer, altered workflow, and the need <b>to</b> <b>duplicate</b> <b>data</b> entry. Convergent validity of the CG-UCE-Q was moderately supported with both the context and processes of workarounds with strong relationships identified for when nurses perceived a block and altered process to work around it to subscales in the CG-UCE-Q for electronic health record system design (P <. 01) and technological barriers (P <. 01) ...|$|E
40|$|Objectives To {{determine}} the uptake of multiple eHealth facilities enabled by the NHS Scotland Electronic Clinical Communications Implementation Programme (ECCI) and to ascertain {{primary and secondary}} care users' perceptions. Design Prospective monthly measurement of 37 indicators of roll-out and adoption. Retrospective questionnaire survey of users. Setting Scottish health board regions. Quantitative implementation indicators were gathered in primary and secondary care across all 16 regions. Questionnaire data were obtained from recorded users in five representative regions (112 general practices, 92 secondary care units). Outcome measures Change in uptake levels of ECCI facilities over a 15 -month period. Users' perceptions of benefits, facilitators and barriers. Results All health boards participated in the monthly data set collection. The response rate to the survey was 62 % in primary care and 37 % in secondary care. Across Scotland as a whole, the process of implementation was gradual. While there were marked gains inthe availability of ECCI facilities over the observation period, rates of adoption lagged behind and varied across alternative facilities. Electronic results were widely used, with most laboratories offering them and around half of general practices receiving them. More modest rates of adoption were observed for e-discharge letters, e-referrals, e-clinic letters and clinical email. E-booking was used very little. Among engaged users responding to the survey, electronic access to test results was the most frequently utilised facility and electronic outpatient booking the least. Perceived benefits of ECCI facilities included convenience, ease of use, time-saving and provision of an audit trail. Perceived barriers included the need <b>to</b> <b>duplicate</b> <b>data</b> entry where new systems were not universally implemented, technological difficulties, time, training and resources. Conclusions Significant progress was observed {{in the implementation of}} ECCI facilities across Scotland. Users reported that these improved communication and were beneficial, but system reliability, incompatibility and duplication of data hindered more widespread uptake. Data were collected at a transitional phase of the programme. Whilst, among users of ECCI facilities, perceptions of the programme and its potential benefits were generally positive, its full impact will not become evident until the new electronic tools are implemented nationally and have been more fully integrated into normal work routines...|$|E
30|$|Universities Canada, and Colleges and Institutes Canada (CICAN), both {{publish a}} list of their members, but {{with regard to the}} universities, a number of members are colleges or {{affiliates}} of a larger institution that awards their credentials. Although the membership of Universities Canada was used as a major source to identify Canadian universities, the affiliated institutions and religious-based colleges were included within their parent university in order <b>to</b> avoid <b>duplicating</b> <b>data</b> collection. Also not all members of Universities Canada are provincially accredited. Furthermore not all public post-secondary colleges are members of CICAN. Some universities are also members of CICAN as well as Universities Canada.|$|R
40|$|The aim of {{this paper}} is to take {{advantage}} of distributed systems for fault-tolerance, but keeping in mind that some advantages may be greatly diminished by delays in detection of failures and/or to the transmission of information. The failures which can be considered may be due to hardware or software. The method used is to record some information in a buffer before using it, and the study is concerned with the method of determining when the information can be used. The necessity of using such a mechanism in order to be able <b>to</b> recover <b>duplicated</b> <b>data</b> is illustrated in particular for the nonstop operation of a telephone system...|$|R
40|$|There is an {{increasing}} demand for systems that can provide secure data storage in a cost-effective manner. Having duplicate records occupies more space and even increases the access time. Thus {{there is a need}} <b>to</b> eliminate <b>duplicate</b> records. This sounds to be simple but requires an tedious work since duplicate records do not share a common key and/or they contain errors that make duplicate matching a difficult task. Errors are also introduced as the result of transcription errors, incomplete information, lack of standard formats, or any combination of these factors. Several approaches areproposed <b>to</b> eliminate <b>duplicate</b> <b>data</b> first at the file level and then at the chunk level to reduce the duplicatelookup complexity. In this paper,few of the methods are discussed with its advantages and disadvantages. And also a better solution is proposed...|$|R
40|$|Graduation date: 1994 Parallel loops {{are one of}} {{the main}} sources of {{parallelism}} in scientific applications, and many parallel loops do not have a uniform iteration execution time. To achieve good performance for such applications on a parallel computer, iterations of a parallel loop have to be assigned to processors {{in such a way that}} each processor has roughly the same amount of work in terms of execution time. A parallel computer with a large number of processors tends to have distributed-memory. To run a parallel loop on a distributed-memory machine, data distribution also needs to be considered. This research investigates the scheduling of non-uniform parallel loops on both shared-memory and distributed-memory parallel computers. We present Safe Self-Scheduling (SSS), a new scheduling scheme that combines the advantages of both static and dynamic scheduling schemes. SSS has two phases: a static scheduling phase and a dynamic self-scheduling phase that together reduce the scheduling overhead while achieving a well balanced workload. The techniques introduced in SSS can be used by other self-scheduling schemes. The static scheduling phase further improves the performance by maintaining a high cache hit ratio resulting from increased affinity of iterations to processors. SSS is also very well suited for distributed-memory machines. We introduce methods <b>to</b> <b>duplicate</b> <b>data</b> on a number of processors. The methods eliminate data movement during computation and increase the scalability of problem size. We discuss a systematic approach to implement a given self-scheduling scheme on a distributed-memory. We also show a multilevel scheduling scheme to self-schedule parallel loops on a distributed-memory machine with a large number of processors to eliminate the bottleneck resulting from a central scheduler. We proposed a method using abstractions to automate both self-scheduling methods and data distribution methods in parallel programming environments. The abstractions are tested using CHARM, a real parallel programming environment. Methods are also developed to tolerate processor faults caused by both physical failure and reassignment of processors by the operating system during the execution of a parallel loop. We tested the techniques discussed using simulations and real applications. Good results have been obtained on both shared-memory and distributed-memory parallel computers...|$|E
40|$|The reuse of {{routinely}} collected clinical {{data for}} clinical research is being explored {{as part of}} the drive <b>to</b> reduce <b>duplicate</b> <b>data</b> entry and <b>to</b> start making full use of the big data potential in the healthcare domain. Clinical researchers often need to extract data from patient registries and other patient record datasets for data analysis as part of clinical studies. In the TRANSFoRm project, researchers define their study requirements via a Query Formulation Workbench. We use a standardised approach to data extraction to retrieve relevant information from heterogeneous data sources, using semantic interoperability enabled via detailed clinical modelling. This approach is used for data extraction from data sources for analysis and for pre-population of electronic Case Report Forms from electronic health records in primary care clinical system...|$|R
40|$|Data {{movement}} operations, {{such as the}} C-style memcpy function, {{are often}} used <b>to</b> <b>duplicate</b> or communicate <b>data.</b> This type of function typically produces {{a significant amount of}} off-chip traffic. For current microprocessors, communication with off-chip memory is an increasing limitation to attain higher performance as well as a significant source of energy consumption. To decrease the amount of communication between a CPU and the off-chip memory system, we propose a system that implements a hardware memcpy in the memory level where the source data is located. ...|$|R
50|$|The Voodoo2 {{introduced}} Scan-Line Interleave (SLI) {{capability to}} the consumer PC market. In SLI mode, two Voodoo2 boards were installed in a PC and ran in parallel, with each unit drawing half {{the lines of the}} display. Voodoo2 SLI not only doubled rendering throughput, it also increased the total framebuffer memory, and thus the maximum supported screen resolution increased to a then-impressive 1024 × 768. However, texture memory was not doubled because each card needed <b>to</b> <b>duplicate</b> the scene <b>data.</b> The original Voodoo Graphics also had SLI capability, but it was only used in the arcade and professional markets.|$|R
40|$|For a {{software}} developing company, {{it is important}} to have a system that keeps track of different projects with related documentation and data. Without a proper system, it can be hard to find needed information and time consuming, especially if there is much data in the system. RealTest AB is an IT consultant company located in Västerås that deals with this problem. All of their projects and their information are stored in a folder structure on a server that the employees have access to. This “system” does not have a smart way of finding data and there is no kind of version control of the existing information and this often leads <b>to</b> <b>duplicated</b> <b>data.</b> The purpose of this project was to provide a smarter and more efficient way of handling projects and documents for RealTest. The project resulted in an intranet solution based on Microsoft SharePoint that takes care of the mentioned problems. This paper describes the design and implementation of certain features of the intranet, such as document management, generation of unique project/document numbers and dynamic webpages. The paper also reviews strengths and weaknesses of existing intranet solutions and application platforms. The conclusions of the conducted research together with meetings with our advisor at RealTest and feedback from demonstrations for the employees led to the first version of the intranet. Since the project only lasted for 10 weeks, time limited us from developing all of the planned features and therefore we were not able to deliver a finished product. However, we managed develop a stable foundation for the intranet which will be finalized by the employees of the company...|$|R
30|$|For our study, we {{selected}} all Dutch academic hospitals (related {{to a university}} medical school) and non-academic hospitals with an independent adult ICU department. A web-based questionnaire (Additional file 1), deployed using the website [URL] {{was sent to the}} medical staff of these ICUs by the end of April 2013. A reminder was sent after two weeks, four weeks and six weeks after the questionnaire was originally sent. All hospitals received one questionnaire, as it is currently common in our country to have one adult ICU center with a combined medical staff and a mixed patient population. For data analysis, we included all questionnaires that were answered within eight weeks {{from the start of the}} study, with a response of more than 80 %. The questionnaire we deployed was based on the questionnaire used previously in 2006 [35]. After confirming a specific hospital’s response, this response was made anonymously. <b>To</b> prevent <b>duplicated</b> <b>data,</b> additional responses from the same hospital were not included.|$|R
40|$|Due to {{the lack}} of {{dependency}} on beacon messages for location exchange, the beaconless geographic routing protocol has attracted considerable attention from the research community. However, existing beaconless geographic routing protocols are likely <b>to</b> generate <b>duplicated</b> <b>data</b> packets when multiple winners in the greedy area are selected. Furthermore, these protocols are designed for a uniform sensor field, so they cannot be directly applied to practical irregular sensor fields with partial voids. To prevent the failure of finding a forwarding node and to remove unnecessary duplication, in this paper, we propose a region-based collision avoidance beaconless geographic routing protocol to increase forwarding opportunities for randomly-deployed sensor networks. By employing different contention priorities into the mutually-communicable nodes {{and the rest of the}} nodes in the greedy area, every neighbor node in the greedy area can be used for data forwarding without any packet duplication. Moreover, simulation results are given to demonstrate the increased packet delivery ratio and shorten end-to-end delay, rather than well-referred comparative protocols...|$|R
40|$|Deduplication, {{a form of}} {{compression}} aiming <b>to</b> eliminate <b>duplicates</b> in <b>data,</b> {{has become}} an important feature of most commercial and research backup systems. Since the advent of deduplication, most research efforts have focused on maximizing deduplication efficiency—i. e., the offered compression ratio—and have achieved near-optimal usage of raw storage. However, the capacity goals of next-generation Petabyte systems requires a highly scalable design, able to overcome the current scalability limitations of deduplication. We advocate a shift towards scalability-centric design principles for deduplication systems, and present some of the mechanisms used in our prototype, aiming at high scalability, good deduplication efficiency, and high throughput. ...|$|R
40|$|OBJECTIVE: To {{quantify}} {{the impact of}} <b>duplicate</b> <b>data</b> on estimates of efficacy. DESIGN: Systematic search for published full reports of randomised controlled trials investigating ondansetron's effect on postoperative emesis. Abstracts were not considered. DATA SOURCES: Eighty four trials (11, 980 patients receiving ondansetron) published between 1991 and September 1996. MAIN OUTCOME MEASURES: Percentage of duplicated trials and patient data. Estimation of antiemetic efficacy (prevention of emesis) of the most duplicated ondansetron regimen. Comparison between the efficacy of non-duplicated and <b>duplicated</b> <b>data.</b> RESULTS: Data from nine trials had been published in 14 further reports, <b>duplicating</b> <b>data</b> from 3335 patients receiving ondansetron; none used a clear cross reference. Intravenous ondansetron 4 mg versus placebo was investigated in 16 reports not subject <b>to</b> <b>duplicate</b> publication, three reports subject <b>to</b> <b>duplicate</b> publication, and six duplicates of those three reports. The number needed to treat to prevent vomiting within 24 hours was 9. 5 (95 % confidence interval 6. 9 to 15) in the 16 non-duplicated reports and 3. 9 (3. 3 to 4. 8) in the three reports which were duplicated (P < 0. 00001). When these 19 were combined the number needed to treat was 6. 4 (5. 3 to 7. 9). When all original and duplicate reports were combined (n = 25) the apparent number needed to treat improved to 4. 9 (4. 4 to 5. 6). CONCLUSIONS: By searching systematically we found 17 % of published full reports of randomised trials and 28 % of the patient <b>data</b> were <b>duplicated.</b> Trials reporting greater treatment effect {{were significantly more likely}} <b>to</b> be <b>duplicated.</b> Inclusion of <b>duplicated</b> <b>data</b> in meta-analysis led to a 23 % overestimation of ondansetron's antiemetic efficacy...|$|R
40|$|Abstract — Creation of test {{programs}} {{and analysis of}} their execution is the main approach to system-level verification of microprocessors. A lot of techniques have been proposed to automate test program generation, ranging from completely random to well directed ones. However, no “silver bullet ” has been found. In good industrial practices, various methods are combined complementing each other. Unfortunately, there is no solution that could integrate all (or at least most) of the techniques in a single framework. Engineers are forced to use a number of tools, {{which leads to the}} following problems: (1) it is required <b>to</b> maintain <b>duplicating</b> <b>data</b> (each tool uses its own representation of the target design); (2) to be used together, tools need to be integrated (engineers have to deal with different formats and interfaces). This paper proposes a concept of an extendable framework (MicroTESK) that follows a unified methodology for defining test program generation techniques. The framework supports random and combinatorial generation and (what is even more important) can be easily extended with new techniques being implemented as the framework’s plugins. I...|$|R
40|$|Replication of the Escherichia coli {{chromosome}} is initiated at {{a unique}} site, oriC. Concurrent initiation occurs at all oriC sites {{present in a}} cell once, and only once, per cell cycle. A mechanism to ensure cyclic initiation events was found operating through the chromosomal site, datA, a 1 -kb segment located at 94. 7 min on the genetic map that titrates exceptionally large amounts of the bacterial initiator protein, DnaA. A strain lacking datA grew normally but exhibited an asynchronous initiation phenotype {{as a result of}} extra initiation events. This mutant phenotype was suppressed by DnaA-titrating plasmids. Furthermore, mutations in a 9 -bp DnaA-binding sequence (the DnaA box) in datA were enough to induce the mutant phenotype. Thus, datA is a novel chromosomal element that appears to adjust a balance between free and bound DnaA for a single initiation event at a fixed time in the bacterial cell cycle. Titration of DnaA <b>to</b> newly <b>duplicated</b> <b>datA</b> during oriC sequestration, which is mediated by hemimethylated GATC sequences in oriC and the SeqA protein, would contribute to prevention of reinitiations when oriC is desequestered...|$|R
40|$|German {{authorities}} who confiscate optical media use an automatic copying machine <b>to</b> <b>duplicate</b> the <b>data</b> onto external hard drives. This allows for efficient {{analysis of the}} data. In the current mode of operation, the copied data is used as evidence {{during the course of}} internal investigations, lawsuits, government investigations, audits, and other formal matters. However, any party having access to the drive can modify data which resides on it. It is not possible to verify the integrity and authenticity of the copied data. This bachelor thesis describes a concept that enhances the functionality of the copying machine by utilizing secure logging with the intent of providing digital evidence. This concept incorporates state of the art secure logging approaches, and is resistant to the copying and verification attacks described in this document. In addition, a Java based prototype is implemented, as part of this bachelor thesis, which demonstrates the functionality described in the concept. Ehrenwörtliche Erklärung Ich erkläre hiermit ehrenwörtlich, dass ich die vorliegende Arbeit selbstständig angefertigt habe. Sämtliche aus fremden Quellen direkt oder indirekt übernommenen Gedanken sind als solche kenntlich gemacht...|$|R
40|$|Abstract — An {{important}} issue in ad hoc sensor networks is the limited energy supply within network nodes. Therefore, power consumption {{is crucial in}} the routing design. Cluster schemes are efficient in energy saving. This paper proposes a new algorithm called dynamic cluster in which energy in the entire network is distributed and unique route from the source to the destination is designed. In this algorithm, energy efficiency is distributed and improved by (1) optimizing the selection of clusterheads in which both residual energy of the nodes and total power consumption of the cluster are considered; (2) optimizing the number of nodes in the clusters according {{to the size of}} the networks and the total power consumption of the cluster; (3) rotating the roles of clusterheads to average the power consumption among clusterheads and normal nodes; and (4) breaking the clusters and reforming them to compensate the difference of the power consumption in different area. Energy efficiency is also improved by defining a unique route to reduce flooding in route discovery and <b>to</b> avoid <b>duplicate</b> <b>data</b> transmission by multiple routes. Index Terms — ad hoc sensor networks, power consumption, cluster schemes, dynamic cluster algorithm. I...|$|R
40|$|Objective 1. 9 (O 1. 9) {{involves}} the experimental evaluation of techniques {{defined in the}} context of work-package WP 1 for supporting the design and specification of a data integration system. This report describes the results obtained when testing some of the main techniques that were developed by the ICAR-CNR research unit only. More specifically, in Section 1, we describe the results of tests we carried out to assess the validity of the approach <b>to</b> recognizing <b>duplicate</b> <b>data</b> instances, defined in [12], {{in the context of}} Objective O 1. 7. Then, in Section 2, we illustrate a number of experiments performed on the different techniques which we defined in the context of Objective O 1. 6, in order to support the construction and usage of extensional metadata, represented as multi-dimensional data synopses. 1 Experimenting Techniques Supporting Consistency Management As described in ReportO 1. 7 [10], our approach to discovering clusters of duplicate tuples, works in an incremental way and basically exploits of a indexing technique which, for any newly arrived tuple t, allows to efficiently retrieve a set of tuples in the database that are mostly similar t, and that are likely to refer to the same real-world entity...|$|R
40|$|In {{monitoring}} systems, multiple sensor nodes {{can detect}} a single target of interest simultaneously {{and the data}} collected are usually highly correlated and redundant. If each node sends data to the base station, energy will be wasted and thus the network energy will be depleted quickly. Data aggregation is an important paradigm for compressing data so that {{the energy of the}} network is spent efficiently. In this paper, a novel data aggregation algorithm called Redundancy Elimination for Accurate Data Aggregation (READA) has been proposed. By exploiting the range of spatial correlations of data in the network, READA applies a grouping and compression mechanism <b>to</b> remove <b>duplicate</b> <b>data</b> in the aggregated set of data to be sent to the base station without largely losing the accuracy of the final aggregated data. One peculiarity of READA is that it uses a prediction model derived from cached values to confirm whether any outlier is actually an event which has occurred. From the various simulations conducted, it was observed that in READA the accuracy of data has been highly preserved taking into consideration the energy dissipated for aggregating the dat...|$|R
40|$|Abstract — We are {{interested}} in how to best communicate a (usually real valued) source {{to a number of}} destinations (sinks) over a network with capacity constraints in a collective fidelity metric over all the sinks, a problem which we call joint networksource coding. Unlike the lossless network coding problem, lossy reconstruction of the source at the sinks is permitted. We make a first attempt to characterize the set of all distortions achievable by a set of sinks in a given network. While the entire region of all achievable distortions remains largely an open problem, we find a large, non-trivial subset of it using ideas in multiple description coding. The achievable region is derived over all balanced multiple-description codes and over all network flows, while the network nodes are allowed <b>to</b> forward and <b>duplicate</b> <b>data</b> packets. I...|$|R
40|$|We are {{interested}} in how to best communicate a (usually real valued) source {{to a number of}} destinations (sinks) over a network with capacity constraints in a collective fidelity metric over all the sinks, a problem which we call joint network-source coding. Unlike the lossless network coding problem, lossy reconstruction of the source at the sinks is permitted. We make a first attempt to characterize the set of all distortions achievable by a set of sinks in a given network. While the entire region of all achievable distortions remains largely an open problem, we find a large, non-trivial subset of it using ideas in multiple description coding. The achievable region is derived over all balanced multiple-description codes and over all network flows, while the network nodes are allowed <b>to</b> forward and <b>duplicate</b> <b>data</b> packets. Comment: submitted to IEEE Informatoin Theory Workshop 200...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThe Corporate Information Management (CIM) initiave in the Department of Defense (DoD) {{is an attempt}} <b>to</b> eliminate <b>duplicate</b> Automatic <b>Data</b> Processing (ADP) systems through the standardization of functional area requirements across all DoD agencies. CIM initiative management recognized that the DoD organizational culture might impact or be impacted by such an all-encompassing initiative. The purpose of this thesis is to estimate the possible impact of culture on information systems implementations by conducting a literature review of cultural theory, change theory, resistance to change, and information systems implementation. The thesis concludes with a recommendation for implementation of the CIM initiative through a four part plan {{based on the findings}} of the literature review. [URL] United States NavyLieutenant Commander, United States Naval Reserv...|$|R
