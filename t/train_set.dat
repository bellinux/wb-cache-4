560|10000|Public
5|$|The three {{individual}} {{cars that}} make up a <b>train</b> <b>set</b> are distinguished by the second digit. For example, <b>train</b> <b>set</b> 3001 consists of the cars 3101, 3201, and 3301.|$|E
5|$|In 1966, a five-gondola <b>train</b> <b>set</b> with green-and-white-striped awnings and a five-gondola <b>train</b> <b>set</b> with blue-and-white-striped awnings, {{referred}} to by Disneyland employees as Holiday Green and Holiday Blue respectively, {{were added to the}} DRR's rolling stock. Both train sets had side-facing bench seating like the Holiday Red freight train. By the time that the new Holiday Green and Holiday Blue trains sets were introduced in 1966, the DRR's original roundhouse, located {{on the end of a}} spur line connected to the main line near the Rivers of America in the Frontierland section, had been replaced by a larger roundhouse, located on the end of a new spur line connected to the main line in the Tomorrowland section. The new roundhouse, where the DRR's locomotives and train cars are stored and maintained, was also built to house the storage and maintenance facility for the Disneyland Monorail.|$|E
5|$|During the Christmas {{season of}} 1952, {{aspiring}} photographer Therese Belivet {{is working in}} Frankenberg's department store in Manhattan. She meets a glamorous woman, Carol Aird, who is searching for a doll for her daughter Rindy. At Therese's recommendation, Carol purchases a model <b>train</b> <b>set.</b> When Carol departs she leaves her gloves on the counter. Therese mails them to her using Frankenberg's sales slip with Carol's name and address.|$|E
3000|$|Inducing {{the initial}} {{detection}} model {{from the initial}} available <b>training</b> <b>set,</b> i.e., <b>training</b> <b>set</b> available up to day d (the initial <b>training</b> <b>set</b> includes 574 PDF files) [...]...|$|R
3000|$|Several {{candidate}} <b>training</b> <b>sets</b> were processed {{to produce}} a mixture pdf among which were the multichannel <b>training</b> <b>set</b> of [2] (1 minute of an orchestra recording), a white noise <b>training</b> <b>set,</b> a Brownian noise <b>training</b> <b>set,</b> and a pink noise <b>training</b> <b>set.</b> Pink noise {{proved to be the}} most suitable <b>training</b> <b>set</b> and led to smaller cepstral reconstruction errors (up to 5 % less in all subbands compared to the other sets) during enhancement of the 4 generic music pieces. The power spectrum of pink noise is proportional to [...]...|$|R
40|$|Various {{computational}} {{models have}} been developed to transfer annotations of gene essentiality between organisms. However, despite the increasing number of microorganisms with well-characterized sets of essential genes, selection of appropriate <b>training</b> <b>sets</b> for predicting the essential genes of poorly-studied or newly sequenced organisms remains challenging. In this study, a machine learning approach was applied reciprocally to predict the essential genes in 21 microorganisms. Results showed that <b>training</b> <b>set</b> selection greatly influenced predictive accuracy. We determined four criteria for <b>training</b> <b>set</b> selection: (1) essential genes in the selected <b>training</b> <b>set</b> should be reliable; (2) the growth conditions in which essential genes are defined should be consistent in training and prediction sets; (3) species used as <b>training</b> <b>set</b> should be closely related to the target organism; and (4) organisms used as <b>training</b> and prediction <b>sets</b> should exhibit similar phenotypes or lifestyles. We then analyzed the performance of an incomplete <b>training</b> <b>set</b> and an integrated <b>training</b> <b>set</b> with multiple organisms. We found that the size of the <b>training</b> <b>set</b> should be at least 10 % of the total genes to yield accurate predictions. Additionally, the integrated <b>training</b> <b>sets</b> exhibited remarkable increase in stability and accuracy compared with single sets. Finally, we compared the performance of the integrated <b>training</b> <b>sets</b> with the four criteria and with random selection. The results revealed that a rational selection of <b>training</b> <b>sets</b> based on our criteria yields better performance than random selection. Thus, our results provide empirical guidance on <b>training</b> <b>set</b> selection for th...|$|R
5|$|The {{addition}} of the Grand Canyon Diorama in 1958 prompted changes to the Retlaw 2 freight train pulled by the DRR's No. 1 locomotive, which involved adding side-facing bench seating pointed towards Disneyland and red-and-white striped awnings {{on all of the}} cattle cars and gondolas. The walls on the cattle cars facing the park were also removed to allow for better views of the diorama. That same year, a third gondola with the same modifications as the other gondolas was added, and a fourth gondola with the same attributes was added in 1959. This brought the total number of freight cars in the <b>train</b> <b>set,</b> now referred to by Disneyland employees as Holiday Red, to eight. Prior to these modifications, the cattle cars and gondolas of this <b>train</b> <b>set</b> had no seating, requiring passengers to stand {{for the duration of the}} ride. Despite safety concerns voiced by Ward Kimball related to the lack of seats on these train cars, Walt Disney, for the purpose of authenticity, had insisted that there be no seats on them; he wanted the passengers to feel like cattle on an actual cattle train.|$|E
5|$|Out of {{a desire}} to have four trains {{regularly}} running at once each day on the DRR, in the mid-1990s the Disneyland park began to search for an additional narrow-gauge steam locomotive to add to the railroad's rolling stock. One such locomotive was acquired from Bill Norred in 1996 in exchange for the combine car and four coaches from the DRR's retired Retlaw 1 passenger <b>train</b> <b>set,</b> but after the park received it, the new locomotive was deemed to be too large for the DRR's operations. It was then sent to the Walt Disney World Railroad in the Magic Kingdom park of Walt Disney World in Bay Lake, Florida, where the locomotive was dedicated, despite being too small for the railroad's operations, and named after Disney animator and rail enthusiast Ward Kimball. Still needing a fifth locomotive for the DRR, the park traded the Ward Kimball locomotive in 1999 to the Cedar Point & Lake Erie Railroad in the Cedar Point amusement park in Sandusky, Ohio, for a new locomotive suitable for the railroad. Named Maud L., the locomotive was built by Baldwin Locomotive Works in 1902 and was originally used to haul sugar cane at the Laurel Valley Sugar Plantation in Louisiana owned by the Barker and Lepine Company. After arriving in Disneyland, the Maud L., later renamed Ward Kimball like the locomotive for which it was traded, was given a new cab built by Disney and a new boiler built by Hercules Power, which was subcontracted by Superior Boiler Works. Due to budget issues, the restoration of the locomotive was suspended not long after its arrival, and its parts were planned to be placed in long-term storage in late 2003. The Ward Kimball locomotive's restoration efforts were resurrected soon after, when it was decided that its addition to the DRR would be incorporated into the celebration of Disneyland's fiftieth anniversary in July 2005. In late 2004, Boschan Boiler and Restorations led by Paul Boschan, a former roundhouse manager and engineer at the Roaring Camp & Big Trees Narrow Gauge Railroad in Felton, California, was awarded the contract to complete the restoration of the Ward Kimball. The restoration work performed included installing new driving wheels, attaching a new smokebox door, and applying gold-leaf silhouettes of Kimball's Jiminy Cricket character {{on the sides of the}} headlamp. The Ward Kimball locomotive, which entered service on June 25, 2005, became the DRR's No. 5 locomotive, and on February 15 the following year, John Kimball, the son of Ward Kimball, who died in 2002, christened the locomotive during its dedication ceremony.|$|E
25|$|The last Nord-Sud <b>train</b> <b>set</b> was {{decommissioned}} on 15May 1972.|$|E
30|$|In {{the third}} {{experiment}} {{we wanted to}} determine the importance of updating the <b>training</b> <b>set</b> over time. Thus, we divided the test collection into years and evaluated <b>training</b> <b>sets</b> of selected years on the next years. Evaluation results show that an update in the <b>training</b> <b>set</b> is needed. Using 10 % malicious files in the <b>training</b> <b>set</b> showed a clear trend in which the performance improves when the <b>training</b> <b>set</b> is updated on a yearly basis.|$|R
30|$|The GVCA method {{based on}} {{horizontally}} grouped <b>training</b> <b>set</b> is {{as shown in}} Algorithm  2. Meanwhile, regarding the problem of too high eigenvector dimension of <b>training</b> <b>set,</b> a feasible approach is to group the features, i.e., to randomly group a feature set into several non-intersecting subsets, and project the originally integral <b>training</b> <b>set</b> into those feature sets, thereby composing several new grouped <b>training</b> <b>sets.</b> After that, the original VCA method is used to obtain vanishing component polynomial respectively, and finally, the vanishing component polynomial in multiple grouped <b>training</b> <b>sets</b> is combined into the vanishing component polynomial in an integral <b>training</b> <b>set.</b> This approach is called the vertical grouping method. The GVCA method based on vertically grouping <b>training</b> <b>sets</b> is as shown in Algorithm  3. Furthermore, to handle both problems of over-scaled <b>training</b> <b>set</b> and too high eigenvector dimension, the horizontal and vertical grouping approaches can be applied simultaneously.|$|R
40|$|Compression of <b>training</b> <b>sets</b> is a {{technique}} for reducing <b>training</b> <b>set</b> size without degrading classification accuracy. By reducing {{the size of a}} <b>training</b> <b>set,</b> <b>training</b> will be more efficient in addition to saving storage space. In this paper, an incremental clustering algorithm, the Leader algorithm, is used {{to reduce the size of}} a <b>training</b> <b>set</b> by effectively subsampling the <b>training</b> <b>set.</b> Experiments on several standard data sets using SVM and KNN as classifiers indicate that the proposed method is more efficient than CONDENSE in reducing the size of <b>training</b> <b>set</b> without degrading the classification accuracy. While the compression ratio for the CONDENSE method is fixed, the proposed method offers variable compression ratio through the cluster threshold value...|$|R
25|$|The NimbleBit game Pocket Trains {{contains}} a <b>train</b> <b>set</b> named the Daylight, which is {{painted in the}} colors of The Daylight.|$|E
25|$|Refurbishment works {{has begun}} since early 2016, {{beginning}} with <b>train</b> <b>set</b> 217/218. It will also include STARIS version 2.0, {{of which it}} will consist of the dynamic route map displays. The first refurbished <b>train</b> <b>set</b> is expected to undergo dynamic tests and commence revenue service in late 2017. As of late August 2016, 2 prototype train cars have been built by SRE for evaluation, signalling that the project is unaffected by termination of the End-of Life Refurbishments for the 66 C151 train sets. It will also be {{the third and final}} rolling stock to be painted in SMRT's unique pixel livery while newer rolling stock starting from the upcoming C151C trains will bear LTA corporate's livery.|$|E
25|$|There were older, box-type metro cars, {{built by}} Nippon Sharyo until {{the system was}} closed for reconstruction. After re-opening, 114 CNR Changchun Railway Vehicle Company Limited Rapid <b>Train</b> <b>set</b> Vehicle cars entered service & {{replaced}} the old ones. Unlike the old cars, the CNR cars contain air conditioning.|$|E
40|$|In {{this paper}} {{we present a}} {{technique}} for reverse engineering vector quantizers by synthesizing a <b>training</b> <b>set</b> that has sim-ilar statistics to the original <b>training</b> <b>set</b> used in designing the vector quantizer. Most VQ codebooks are designed using the LBG or generalized Lloyd algorithm {{which is similar to}} the construction of nonuniform bin histograms. Thus the VQ codebook and the number of <b>training</b> <b>set</b> vectors allocated to each of its codebook vectors is an approximation of the underlying pdf of the <b>training</b> <b>set.</b> This observation is used to synthesize a <b>training</b> <b>set</b> that has a histogram similar to the original <b>training</b> <b>set.</b> This synthesized <b>training</b> <b>set</b> can be used to construct VQs to describe subspaces of the original vector space or spaces transformed by a linear transforma-tion. 1...|$|R
50|$|Over the years, three {{versions}} of rolling stock were used on this line, {{as well as}} on the through services between Nanshijiao and Beitou. Originally, the line used a large fleet of C301 <b>train</b> <b>sets.</b> In 1999, only a few C341 <b>train</b> <b>sets</b> were used. In 2007, some C371 <b>train</b> <b>sets</b> were introduced. Today, the entire fleet used on this line is the C371 <b>train</b> <b>sets</b> after the original C301 <b>train</b> <b>sets</b> were confined to the Tamsui and Xindian Lines upon the opening of Dongmen Station on September 30, 2012.|$|R
40|$|Introduction In recent {{research}} in combining predictors, {{it has been}} recognized that the critical thing to success in combining low-bias predictors such as trees and neural nets has been through methods that reduce the variability in the predictor due to <b>training</b> <b>set</b> variability. Assume that the <b>training</b> <b>set</b> consists of N independent draws from the same underlying distribution. Conceptually, <b>training</b> <b>sets</b> of size N can be drawn repeatedly and the same algorithm used to construct a predictor on each <b>training</b> <b>set.</b> These predictors will vary, {{and the extent of}} the variability is a dominant factor in the generalization prediction error. 2 Given a <b>training</b> <b>set</b> {(y n,x n),n= 1, [...] . N} where the y's are either class labels or numerical values, the most common way of reducing variability is by perturbing the <b>training</b> <b>set</b> to produce alternative <b>training</b> <b>sets,</b> growing a predictor o...|$|R
25|$|On May 16, 1842, {{the second}} {{organized}} wagon <b>train</b> <b>set</b> out from Elm Grove, Missouri, {{with more than}} 100 pioneers. The party was led by Elijah White. The group broke up after passing Fort Hall {{with most of the}} single men hurrying ahead and the families following later.|$|E
25|$|This shows Transrapid train {{sets are}} likely to cost over {{twice as much as}} ICE 3 {{conventional}} fast rail train sets at this time. However each Transrapid <b>train</b> <b>set</b> is more than twice as efficient due to their faster operating speed and acceleration according to UK Ultraspeed. In their case study only 44% as many Transrapid train sets are needed to deliver the same amount of passengers as conventional high-speed trains.|$|E
25|$|The {{service is}} subject to {{overcrowding}} during rush hours. To help alleviate this, the operator introduced a new queuing system to help passengers line up when the trains are coming. The lines are painted {{on the floor with}} three colour codes representing each of the <b>train</b> <b>set.</b> The system was first implemented at KL Sentral station on 17 October 2008. The operator is also considering introducing express services between Sungai Buloh and Kajang stations and between Kuala Lumpur and Shah Alam station during rush hours by the end of 2008.|$|E
40|$|Abstract—Bug triage is an {{important}} step in the process of bug fixing. The goal of bug triage is to assign a new-coming bug to the correct potential developer. The existing bug triage approaches are based on machine learning algorithms, which build classifiers from the <b>training</b> <b>sets</b> of bug reports. In practice, these approaches suffer from the large-scale and low-quality <b>training</b> <b>sets.</b> In this paper, we propose the <b>training</b> <b>set</b> reduction with both feature selection and instance selection techniques for bug triage. We combine feature selection with instance selection to improve the accuracy of bug triage. The feature selection algorithm 2 -testχ, instance selection algorithm Iterative Case Filter, and their combinations are studied in this paper. We evaluate the <b>training</b> <b>set</b> reduction on the bug data of Eclipse. For the <b>training</b> <b>set,</b> 70 % words and 50 % bug reports are removed after the <b>training</b> <b>set</b> reduction. The experimental results show that the new and small <b>training</b> <b>sets</b> can provide better accuracy than the original one. Keywords-bug triage; <b>training</b> <b>set</b> reduction; feature selection; instance selection; software quality I...|$|R
40|$|The {{passenger}} <b>train</b> <b>sets</b> is {{the carrier}} of railway passenger transport production and reasonable {{use of the}} passenger <b>train</b> <b>sets</b> {{is one of the}} key goals of railway transportation plan. In order to improve the operation efficiency of passenger <b>train</b> <b>sets,</b> optimization model of railway passenger <b>train</b> <b>sets</b> assignment have been built to minimize the non-production staying time of passenger <b>train</b> <b>sets</b> at passenger station based on established train diagram and established configuration of <b>train</b> <b>sets</b> system. On that basis, considering that the parameters of simulated Annealing Algorithm (SA) directly affect the efficiency and precision of solving, SA parameters is optimized based on nested partitions, then improved simulated annealing algorithm (ISA) is designed to solve this model. Finally, a case study has been carried out taking Zhengzhou railway station of china as an example in order to testify validity of this model and its algorithm by using calculating and comparing analysis and further solved practical problems is analyzed. The results show that the number of passenger <b>train</b> <b>sets</b> required and carriage staying time at passenger station are reduced and this model and its algorithm can be used to optimize railway passenger <b>train</b> <b>sets</b> assignment to improve efficiency of railway passenger <b>train</b> <b>sets</b> assignment...|$|R
5000|$|... 13 October 2006: Prasarana signs an {{agreement}} with a Bombardier-Hartasuma joint venture {{for the purchase of}} 22 four-car <b>train</b> <b>sets</b> for the Kelana Jaya Line with an option to purchase an additional 13 <b>train</b> <b>sets</b> for RM1.2 billion. First 22 <b>train</b> <b>sets</b> to be delivered in 2008.|$|R
25|$|Boggs {{traveled}} overland to California in 1846 and {{is frequently}} mentioned among the notable emigrants of that year. His traveling companions widely believed that his move {{was rooted in}} his fear of the Mormons. When the <b>train</b> <b>set</b> out in early May, he campaigned to be elected its captain, but lost to William H. Russell; when Russell resigned on June 18, the group was thereafter led by Boggs. Among the Boggs Company were most of the emigrants who later separated from the group to form the Donner Party.|$|E
25|$|In 1956, the New York, New Haven and Hartford Railroad {{ordered a}} custom-built, six-car <b>train</b> <b>set</b> {{based on the}} RDC design named the Roger Williams. It {{consisted}} of two single-ended cab units and four intermediate cars to make a complete train. The units were fitted with third-rail shoes, electric traction motors, and associated gear for operation into Grand Central Terminal, though this was short-lived. In the New Haven's later years, the set was broken up, and used with regular New Haven RDCs, and by Amtrak into the 1980s.|$|E
25|$|The first {{train in}} the United Kingdom with {{corridor}} connections between all carriages entered service on 7 March 1890 on the Paddington to Birkenhead route, and further corridor trains were introduced on all the main routes {{over the next few}} years. In 1900 a new Milford Boat <b>Train</b> <b>set</b> introduced electric lights and the communication cord was moved inside the train; until now a passenger needing to stop the train in an emergency had to lean out of the window and pull a cord above the door. At this time carriages generally had a clerestory roof but elliptical roofs were fitted to the GWR steam rail motors in 1903.|$|E
5000|$|Given a <b>training</b> <b>set</b> S of size m, we will build, {{for all i}} = 1....,m, {{modified}} <b>training</b> <b>sets</b> as follows: ...|$|R
40|$|We {{consider}} empirical risk minimization {{for large-scale}} datasets. We introduce Ada Newton as an adaptive algorithm that uses Newton's method with adaptive sample sizes. The main idea of Ada Newton {{is to increase}} the size of the <b>training</b> <b>set</b> by a factor larger than one {{in a way that the}} minimization variable for the current <b>training</b> <b>set</b> is in the local neighborhood of the optimal argument of the next <b>training</b> <b>set.</b> This allows to exploit the quadratic convergence property of Newton's method and reach the statistical accuracy of each <b>training</b> <b>set</b> with only one iteration of Newton's method. We show theoretically and empirically that Ada Newton can double the size of the <b>training</b> <b>set</b> in each iteration to achieve the statistical accuracy of the full <b>training</b> <b>set</b> with about two passes over the dataset...|$|R
40|$|This paper {{exploits}} {{the criteria}} {{to optimize the}} <b>training</b> <b>set</b> construction for video annotation. Most existing learningbased semantic annotation approaches require a large <b>training</b> <b>set</b> to achieve good generalization capacity, in which {{a considerable amount of}} labor-intensively manual labeling is desirable. However, it is observed that the generalization capacity of a classifier highly depends on the geometrical distribution rather than the size of the training data. We argue that a <b>training</b> <b>set</b> which includes most temporal and spatial distribution of the whole data will achieve a satisfying performance even in the case of limited size of <b>training</b> <b>set.</b> In order to capture the geometrical distribution characteristics of a given video collection, we propose the following four metrics for constructing an optimal <b>training</b> <b>set,</b> including Salience, Time Dispersiveness, Spatial Dispersiveness and Diversity. Moreover, based on these metrics, we propose a set of optimization rules to capture the most distribution information of the whole data for a <b>training</b> <b>set</b> with a given size. Experimental results demonstrate that these rules are effective for <b>training</b> <b>set</b> construction for video annotation, and significantly outperform random <b>training</b> <b>set</b> selection as well...|$|R
25|$|After recuperating at Mudros, the <b>Train</b> <b>set</b> sail under {{command of}} Lt. Bond for Lake Timsah on the Suez Canal on 17 January, {{arriving}} {{there on the}} 21st. Here, at Suez Canal No. 2 Section, the Train was responsible for manning and controlling existing bridges, building new bridges, control of tugboats and lighters {{and the distribution of}} stores. On 11 February, the Train split into three sections, with Lt. Bond commanding 57 men at a Serapeum halfway between Lake Tismah and Bitter Lakes, Sub-Lieutenant Charles Hicks with 65 men at the northern approach to Lake Tismah, a place known as Ferry Point. Lt Commander Bracegirdle had joined up with the Train again on 30 January, and was in command at the main camp on Lake Tismah.|$|E
25|$|The {{problem that}} is slowing down {{high-speed}} rail in Sweden is the present signaling system (ATC), which does not allow speeds over 200km/h. It can be upgraded, {{but it will not}} be done since it shall be replaced by the European signaling system ERTMS level 2 on major lines in the near future, allowing high speeds up to 250km/h. ERTMS level 2 has been installed and is being tried out on Botniabanan, and that railway allows 250km/h, although no passenger train goes above 200 for now. The <b>train</b> <b>set</b> X55-Regina has been delivered to the rail company SJ with the max speed of 200km/h but with the option to upgrade the EMU to 250km/h when possible. Also the mix with freight trains slow down the practical speed.|$|E
25|$|After the {{bankrupt}} Tunisia became an autonomous republic in 1957 and the then reigning Bey from the Husainid Dynasty had lost both his political influence and his substantial inherited personal wealth, the <b>train</b> <b>set</b> with its luxurious Belle Époque interiors of brocaded velvet armchairs, overhead antique-globed lighting, brass fittings, mahogany marquetry and panoramic windows was confiscated {{by the new}} government but due to its symbolical value stored in a depot and left to its destiny. After some thirty years in decay it was however restored by the Tunisian state and the SNCFT (La Société National de Chemins de Fer Tunisiens) and today the Red Lizard and the train line Metlaoui-Tozeur, {{often referred to as}} the North-African Orient Express, is again running and one of the country's greatest tourist attractions.|$|E
30|$|We {{focus only}} {{on the part of}} the <b>training</b> <b>set,</b> which {{describes}} inputs. The given <b>training</b> <b>set,</b> based on the robot configuration and the distribution of the scanned area into five parts, comprises 1, 048, 576 patterns. The <b>training</b> <b>set</b> also contains patterns that admit a possibility of the existence of obstacles in several given intervals. Therefore, it is necessary to specify a group of inputs I_i belonging to a given sensor. In the case that the group comprises more than one value of,, 1 “, such a pattern is not included in the <b>training</b> <b>set.</b> The reduced <b>training</b> <b>set</b> (RTS) comprises 1296 patterns, see Table  1.|$|R
30|$|Grouping the {{training}} set: this paper proposed the GVCA method to address two situations of over-scaled <b>training</b> <b>set</b> and too high dimension of eigenvector. Regarding {{the problem of}} over-scaled <b>training</b> <b>set,</b> it is a practicable way to group the examples, i.e., to horizontally segment the entire <b>training</b> <b>set</b> into several <b>training</b> subsets (for instance, take 10 / 20 / 30 / 40 / 50 examples as a group), then acquire the features (i.e., vanishing component polynomial) by the original VCA method respectively and combine the vanishing component polynomial in multiple grouped <b>training</b> <b>sets</b> into the vanishing component polynomial in an integral <b>training</b> <b>set.</b> This approach is called the horizontal grouping method.|$|R
30|$|We {{exploit the}} {{criteria}} to optimize <b>training</b> <b>set</b> construction for the large-scale video semantic classification. Due {{to the large}} gap between low-level features and higher-level semantics, {{as well as the}} high diversity of video data, it is difficult to represent the prototypes of semantic concepts by a <b>training</b> <b>set</b> of limited size. In video semantic classification, most of the learning-based approaches require a large <b>training</b> <b>set</b> to achieve good generalization capacity, in which large amounts of labor-intensive manual labeling are ineluctable. However, it is observed that the generalization capacity of a classifier highly depends on the geometrical distribution of the training data rather than the size. We argue that a <b>training</b> <b>set</b> which includes most temporal and spatial distribution information of the whole data will achieve a good performance even if the size of <b>training</b> <b>set</b> is limited. In order to capture the geometrical distribution characteristics of a given video collection, we propose four metrics for constructing/selecting an optimal <b>training</b> <b>set,</b> including salience, temporal dispersiveness, spatial dispersiveness, and diversity. Furthermore, based on these metrics, we propose a set of optimization rules to capture the most distribution information of the whole data using a <b>training</b> <b>set</b> with a given size. Experimental results demonstrate these rules are effective for <b>training</b> <b>set</b> construction in video semantic classification, and significantly outperform random <b>training</b> <b>set</b> selection.|$|R
