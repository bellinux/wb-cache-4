182|122|Public
50|$|Another {{meaning of}} {{fundamental}} analysis is beyond bottom-up company analysis, {{it refers to}} <b>top-down</b> <b>analysis</b> from first analyzing the global economy, followed by country analysis and then sector analysis, and finally the company level analysis.|$|E
50|$|Where, Sj is {{the muscle}} {{strength}} moment at joint, j, and Mj/L is the external {{moment at the}} joint, j, due to load, L and the body segments preceding the joint in the <b>top-down</b> <b>analysis.</b>|$|E
50|$|The LOT {{system was}} {{developed}} during 1966 at Stanford Research Institute and was modeled very closely after Meta II. It had new special-purpose constructs {{allowing it to}} generate a compiler which would in turn be able to compile a subset of PL/l. This system had extensive statistic-gathering facilities and was used to study the characteristics of <b>top-down</b> <b>analysis.</b>|$|E
50|$|Employing {{a mix of}} a {{top-down}} and bottom-up approaches, ACPI invests {{according to}} the relative return-risk profiles of various asset classes. The <b>top-down</b> macro <b>analysis</b> determines the overall asset allocation of each portfolio, whilst the bottom-up approach determines the composition of each asset.|$|R
40|$|Today, {{companies}} {{are required to}} be in control of the security of their IT assets. This is especially challenging in the presence of limited budgets and conflicting requirements. Here, we present Risk-Based Requirements Elicitation and Prioritization (RiskREP), a method for managing IT security risks by combining the results of a <b>top-down</b> requirements <b>analysis</b> with a bottom-up threat <b>analysis.</b> <b>Top-down,</b> it prioritizes security goals and from there derives verifiable requirements. Bottom-up, it analyzes architectures in order to identify security risks in the form of critical components. Linking these critical components to security requirements helps to analyze the effects of these requirements on business goals, and to prioritize security requirements. The security requirements also are the basis for deriving test cases for security analysis and compliance monitoring...|$|R
50|$|The {{application}} of this design principle requires performing a <b>top-down</b> service-oriented <b>analysis</b> process in order {{to arrive at a}} complete set of candidate services. This clearly requires increased resources both in the form of time and efforts. The {{application of}} the Logic Centralization design pattern may introduce cultural issues e.g. service developers showing reluctance in reusing other’s services, project managers not willing to incorporate use of existing services as it might need solution design adaptation, etc.|$|R
50|$|Static {{strength}} {{prediction is}} {{the method of}} predicting the strength capabilities of a person or a population (based on anthropometry) for a particular task and/or posture (an isometric contraction). Manual calculations are usually performed using the <b>top-down</b> <b>analysis</b> on a six or seven-link model, based on available information about the case and then compared to standard guidelines, {{such as the one}} provided by the National Institute for Occupational Safety and Health, to predict capability.|$|E
50|$|<b>Top-down</b> <b>analysis</b> is {{the method}} of calculating the {{reactive}} moments and forces at each joint starting at the hand, all the way till the ankle and foot. In a 6-segment model, the joints considered are elbow, shoulder, L5/S1 disc of the spine, hip, knee and ankle. It is common to ignore the wrist joint in manual calculations. Software intended for such calculation use the wrist joint also, dividing the lower arm into hand and forearm segments.|$|E
50|$|AC 25.1309 - 1 {{recommended}} that <b>top-down</b> <b>analysis</b> should identify each system function and evaluate its criticality, i.e., either non-essential, essential, or critical. The terms Error, Failure, and Failure Condition were defined. Functions were classified Critical, Essential, and Non-Essential {{according to the}} severity of the failure conditions they could contribute to; but the conditions were not expressly classified. Failures of Critical, Essential, and Non-Essential functions were expected to be, respectively, Extremely Improbable (10 - 9 or less), Improbable (10 - 5 or less), or no worse than Probable (10 - 5).|$|E
50|$|In {{order to}} be sure that all the {{services}} that are being built follow the same design standards, as part of the application of this design pattern, a service inventory blueprint needs to be created. Such a blueprint only consists of candidate services containing candidate capabilities. By coming up with such a service inventory blueprint, the overall scope of the potential enterprise-wide service inventory is established. This usually requires the application of the <b>top-down</b> service-oriented <b>analysis.</b>|$|R
40|$|The {{purpose of}} this study was to test the {{hypothesis}} that mitochondrial permeability transition might be implicated in mitochondrial and intact organ dysfunctions associated with damage induced by reperfusion after cold ischaemia. Energetic metabolism was assessed continuously by 31 P-NMR on a model system of isolated perfused rat liver; mitochondria were extracted from the livers and studied by using <b>top-down</b> control <b>analysis.</b> During the temperature transition from hypothermic to normothermic perfusion (from 4 to 37 degrees C) the ATP content of the perfused organ fell rapidly, and <b>top-down</b> metabolic control <b>analysis</b> of damaged mitochondria revealed a specific control pattern characterized by a dysfunction of the phosphorylation subsystem leading to a decreased response to cellular ATP demand. Both dysfunctions were fully prevented by cyclosporin A, a specific inhibitor of the mitochondrial transition pore (MTP). These results strongly suggest the involvement of the opening of MTP in vivo during the transition to normothermia on rat liver mitochondrial function and organ energetics...|$|R
40|$|This course {{covers the}} basics of writing a {{compiler}} to translate from a simple high-level language to machine code. Topics include lexical <b>analysis,</b> <b>top-down</b> and LR parsing, syntax-directed translation, and code generation and optimization. Students will write a small compiler. The format of the course will be lecture-discussions, assignments. Students are strongl...|$|R
5000|$|A key {{strand of}} free market {{economic}} {{thinking is that}} the market's [...] "invisible hand" [...] guides an economy to prosperity more efficiently than central planning using an economic model. One reason, emphasized by Friedrich Hayek, is the claim {{that many of the}} true forces shaping the economy can never be captured in a single plan. This is an argument that cannot be made through a conventional (mathematical) economic model, because it says that there are critical systemic-elements that will always be omitted from any <b>top-down</b> <b>analysis</b> of the economy.|$|E
50|$|The AVM {{deals with}} encoding, verification, and {{decision}} operations. Encoding {{is used to}} describe the early operations that lead to the unconscious activation of learned units in memory. After encoding, verification occurs. Verification often leads to the conscious recognition of a single lexical entry from the respondents. Verification is to be viewed as an independent, <b>top-down</b> <b>analysis</b> of stimulus that is guided by the stored, or previously learned, representation of a word. Real-time processing in verification can be mimicked by a computer simulation. Lastly, the factors affecting speed and accuracy of performance in a particular paradigm depend on whether decisions are based primarily on information from encoding or verification.|$|E
50|$|Prior the 1990s and the methodologies of Orna, Henczel, Wood, Buchanan and Gibb, IA {{approaches}} and methodologies focused mainly upon an identification of formal information resources (IR). Later approaches included an organisational analysis and the mapping of the Information flow. This gave context to analysis within an organisation’s information systems and a holistic {{view of their}} IR and as such could {{contribute to the development}} of the Information Systems Architecture (ISA). In recent years the IA has been overlooked in favour of the systems development process which can be less expensive than the IA, yet more heavily technically focused, project specific (not holistic) and does not favour the <b>top-down</b> <b>analysis</b> of the IA.|$|E
40|$|Graduation date: 2013 Oregon State University has {{conducted}} research {{in collaboration with}} TerraPower, LLC, to perform a <b>top-down</b> scaling <b>analysis</b> of an integrated test facility. The goal of this facility is to simulate transient and quasi-steady phenomena at a reduced scale, including steady-state operation, pump coastdown, natural circulation, reactor head heat transfer, and coolant stratification. To support this goal, this thesis presents the methodology and analysis by which approximate facility dimensions were generated. This analysis includes implementation of the hierarchical two-tiered scaling methodology, as outlined by the Nuclear Regulatory Commission and optimization through the general reduced gradient methodology...|$|R
40|$|Generalized Constant Propagation (GCP) statically {{estimates}} the ranges of variables throughout a program. GCP is a <b>top-down</b> compositional compiler <b>analysis</b> {{in the style}} of abstract intepretation. In this paper we present an implementation of both intraprocedural and interprocedural GCP {{within the context of the}} C language. We compare the accuracy and utility of GCP information for several versions of GCP using experimental results from an actual implementation. 1 Introduction Generalized Constant Propagation (GCP) is a <b>top-down</b> compositional compiler <b>analysis</b> based on the style of abstract interpretation [CC 77]. A GCP analysis statically approximates the possible values each variable could take at each point in the program. As an extension of constant propagation (CP), GCP estimates ranges for variables rather than their precise value: each variable at each point is associated with a minimum and maximum value. We have implemented GCP for the full C language, in both intraprocedural [...] ...|$|R
40|$|A {{methodology}} {{is described}} {{for developing a}} task complexity index based on combining the six basic motion primitives (three translation, three orientation) with force control and accuracy requirements. The result of this development {{is a set of}} complexity values that can be assigned to the high-level task primitives derived from a relatively shallow <b>top-down</b> mission <b>analysis.</b> These values are then averaged to arrive at total average mission complexities, such as for the mission of exchanging the Hubble Space Telescope (HST) battery modules. Application of this metric to a candidate set of NASA Flight Telerobotic Servicer evaluation tasks is discussed using the HST battery module mission for an in-depth example...|$|R
40|$|In {{this section}} we will {{introduce}} some terminology related to computer vision. Terms such as computational vision, image processing, computer vision, image understanding, low-level vision, intermediate-level vision, high-level vision, <b>top-down</b> <b>analysis,</b> bottom-up analysis, and other terms {{are often used}} to describe some aspect o...|$|E
40|$|Interprocedural {{analyses}} are compositional when they compute over-approximations of procedures in a bottom-up fashion. These {{analyses are}} usually more scalable than top-down analyses which compute a different procedure summary for every calling context. However, compositional analyses are rare in practice, {{because it is}} difficult to develop such analyses with enough precision. In this paper, we establish a connection between compositional analyses and so called modular lattices, which require certain associativity between the lattice join and meet operations. Our connection provides sufficient conditions for building a compositional analysis that has the same precision as a <b>top-down</b> <b>analysis.</b> It also sheds light on the limitations of our approach: the transfer functions should be expressed as joins and meets with constant elements. Following our recipe, we develop a compositional version of the connection analysis by Ghiya and Hendren, which was motivated by the need to parallelize sequential code. Our version is slightly more conservative than the original <b>top-down</b> <b>analysis</b> in order to meet our modularity requirement. We implemented and applied our compositional connection analysis to real-world Java programs. As expected, the analysis scales much better than the original top-down version. The <b>top-down</b> <b>analysis</b> times out in the largest two of our five programs, and the amount of precision lost due to the modularity requirement in the remaining programs ranges only between 2 - 5 %. 1...|$|E
40|$|Abstract:In this paper, it {{introduces}} the design processing and implementation method of digital technology based on <b>Top-down,</b> <b>analysis</b> {{the advantages of}} the method using Top-down in the product design. To deflection yoke for example, completed digital design and assembly of the deflection yoke based on Top-down, and completed the NC processing by using numerical control machine tool...|$|E
40|$|AbstractWe {{investigated}} the {{mechanism by which}} 3, 5 -diiodo-l-thyronine (T 2) affects skeletal muscle mitochondrial bioenergetic parameters following its acute administration to hypothyroid rats. One hour after injection, T 2 increased both coupled and uncoupled respiration rates by + 27 % and + 42 %, respectively. <b>Top-down</b> elasticity <b>analysis</b> revealed that these effects {{were the result of}} increases in the substrate oxidation and mitochondrial uncoupling. Discriminating between proton-leak and redox-slip processes, we identified an increased mitochondrial proton conductance as the “pathway” underlying the effect of T 2 on mitochondrial uncoupling. As a whole, these results may provide a mechanism by which T 2 rapidly affects energy metabolism in hypothyroid rats...|$|R
30|$|We {{continue}} the <b>analysis</b> <b>top-down</b> {{the stack of}} layers by analyzing contributions manifested in the source code repository. While executable artifacts exist {{in the form of}} executable specifications (content and workflow schemata) in the content repository, too, the majority of executable artifacts in the analyzed system are classical software artifacts (programs written in Tcl, Javascript, SQL and HTML templates as used by OpenACS).|$|R
40|$|Metabolic control {{analysis}} allows {{the study of}} metabolic regulation. We applied both single- and double-manipulation <b>top-down</b> control <b>analysis</b> to examine the control of lipid accumulation in developing oilseed rape (Brassica napus) embryos. The biosynthetic pathway was conceptually divided into two blocks of reactions (fatty acid biosynthesis (Block A), lipid assembly (Block B)) connected by a single system intermediate, the acyl-coenzyme A (acyl-CoA) pool. Single manipulation used exogenous oleate. Triclosan was used to inhibit specifically Block A, whereas diazepam selectively manipulated flux through Block B. Exogenous oleate inhibited the radiolabelling of fatty acids from [1 - 14 C]acetate, but stimulated that from [U- 14 C]glycerol into acyl lipids. The calculation of group flux control coefficients showed that c. 70...|$|R
40|$|The paper {{describes}} the basic {{ideas and the}} main features of {{a new class of}} constitutive laws, in the framework of incrementally non-linear constitutive equations. CLoE is a generic name for that new class of laws, with reference to consistency at the limit surface, and explicit localization analysis. A <b>top-down</b> <b>analysis</b> of the model is presented, and illustrated by examples. Peer reviewe...|$|E
40|$|Interprocedural {{analyses}} are compositional when they com- pute over-approximations of procedures in a bottom-up fashion. These {{analyses are}} usually more scalable than top-down analyses, which com- pute a different procedure summary for every calling context. However, compositional analyses are rare in practice {{as it is}} difficult to develop them with enough precision. We establish a connection between compositional analyses and mod- ular lattices, which require certain associativity between the lattice join and meet operations, and use it to develop a compositional version of the connection analysis by Ghiya and Hendren. Our version is slightly more conservative than the original <b>top-down</b> <b>analysis</b> in order to meet our modularity requirement. When applied to real-world Java programs our analysis scaled much better than the original top-down version: The <b>top-down</b> <b>analysis</b> times out in the largest two of our five programs, while ours incurred only 2 – 5 % of precision loss in the remaining programs...|$|E
40|$|In this paper, we {{describe}} a <b>top-down</b> <b>analysis</b> and simulation approach to size the bandwidths of a store-andforward network {{for a given}} network topology, a mission traffic scenario, {{and a set of}} data types with different latency requirements. We use these techniques to estimate the wide area network (WAN) bandwidths of the ground links for different architecture options of the proposed Integrated Space Communication and Navigation (SCaN) Network...|$|E
40|$|International audienceOur global {{activities}} concern (i) {{information analysis}} and interpretation and (ii) the design of numerical distributed and adaptive algorithms in interaction with biology and medical science. To better understand cortical signals, we use: - a <b>top-down</b> approach: data <b>analysis</b> techniques extract properties of underlying neural activity. - a bottom-up approach: a mathematical neural modelling aims for the understanding of experimental results. The following activities are linked to Brain-Computer Interface challenges...|$|R
40|$|A linear {{ion trap}} (LIT) with {{electrospray}} ionization (ESI) for <b>top-down</b> protein <b>analysis</b> has been constructed. An independent atmospheric sampling glow discharge ionization (ASGDI) source produces reagent ions for ion/ion reactions. The device is also meant to enable {{a wide variety}} of ion/ion reaction studies. To reduce the instrument's complexity and make it available for wide dissemination, only a few simple electronics components were custom built. The instrument functions as both a reaction vessel for gas-phase ion/ion reactions and a mass spectrometer using mass-selective axial ejection. Initial results demonstrate trapping efficiency of 70 % to 90 % and the ability to perform proton transfer reactions on intact protein ions, including dual polarity storage reactions, transmission mode reactions, and ion parking...|$|R
40|$|A finite graph {{model is}} defined to {{describe}} all the computations, finite or infinite, {{generated by a}} formally defined complex of interacting digital systems. The graph, called a finite process structure, is an abstraction which can be formulated and computed directly from system representations. Finite process structures have properties making them excellent tools for design analysis; in particular, the infinite variety of graphs describing the processes of a single system {{at all levels of}} detail arranges itself into a lattice [...] so it is possible to find desired characterizations algorithmically. Thus <b>top-down</b> hierarchical <b>analysis</b> of systems becomes a plausible goal. Within this context, a semantic theory of process structuring is initiated: a process definition is given, and compared to state-of the-art definitions of processes...|$|R
40|$|In {{this paper}} we apply a plant-wide control design {{procedure}} presented by Larsson and Skogestad (Larsson & Skogestad 2001) to a distillation column heat-integrated {{by using a}} heatpump. <b>Top-down</b> <b>analysis</b> is performed to select primary controlled variables (based on ideas of self-optimizing control) and bottlenecks. Bottom-up design is performed to design the control system. This includes selecting extra measurements (secondary con-trolled variables) for stabilization and local disturbance rejection. 1...|$|E
40|$|HPCToolkit is an {{open-source}} {{suite of}} multi-platform tools for profile-based performance analysis of sequential and parallel applications. The toolkit consists of components for collecting performance measurements of fully-optimized executables without adding instrumentation, analyzing application binaries {{to understand the}} structure of optimized code, correlating measurements with program structure, and a user interface that supports <b>top-down</b> <b>analysis</b> of performance data. This paper {{provides an overview of}} HPCToolkit and demonstrates its utility for application performance analysis on a Cray XD 1. ...|$|E
40|$|Most tourism-related {{activities}} require energy {{directly in}} the form of fossil fuels or in{{directly in the}} form of electricity often generated from petroleum, coal or gas. This consumption leads to the emission of greenhouse gases, mainly carbon dioxide. Tourism is not a traditional sector in the System of National Accounts and as a result no country possesses comprehensive national statistics on the energy demand or emissions specifically resulting from tourism. This paper suggests two approaches for accounting for carbon dioxide emissions from tourism: a bottom-up analysis involving industry and tourist analyses, and a <b>top-down</b> <b>analysis</b> using environmental accounting. Using the case study of New Zealand, we demonstrate that both approaches result in similar estimates {{of the degree to which}} tourism contributes to national carbon dioxide emissions. The bottom-up analysis provides detailed information on energy end-uses and the main drivers of carbon dioxide emissions. These results can be used for the development of targeted industry-based greenhouse gas reduction strategies. The <b>top-down</b> <b>analysis</b> allows assessment of tourism as a sector within the wider economy, for example with the purpose of comparing tourism's eco-efficiency with other sectors, or the impact of macroeconomic instruments such as carbon charges. No Full Tex...|$|E
40|$|Electron {{transfer}} dissociation (ETD) {{is commonly}} employed in ion traps utilizing rf fields that facilitate efficient electron transfer reactions. Here, we explore performing ETD in the HCD collision cell on an Orbitrap Velos instrument by applying a static DC gradient axially to the rods. This gradient enables simultaneous three dimensional, charge sign independent, trapping of cations and anions, initiating electron transfer reactions {{in the center}} of the HCD cell where oppositely charged ions clouds overlap. Here, we evaluate this mode of operation for a number of tryptic peptide populations and the <b>top-down</b> sequence <b>analysis</b> of ubiquitin. Our preliminary data show that performing ETD in the HCD cell provides similar fragmentation as ion trap-ETD but requires further optimization to match performance of ion trap-ETD...|$|R
40|$|We {{examined}} {{the potential of}} overt carnitine palmitoyltransferase (CPT I) to control the hepatic catabolism of palmitoyl-CoA in suckling and adult rats, using a conceptually simplified model of fatty acid oxidation and ketogenesis. By applying <b>top-down</b> control <b>analysis,</b> we quantified the control exerted by CPT I over total carbon flux from palmitoyl-CoA to ketone bodies and carbon dioxide. Our results show that in both suckling and adult rat, CPT I exerts very significant control over the pathways under investigation. However, under the sets of conditions we studied, less control is exerted by CPT I over total carbon flux in mitochondria isolated from suckling rats than in those isolated from adult rats. Furthermore the flux control coefficient of CPT I changes with malonyl-CoA concentration and ATP turnover rate...|$|R
40|$|Producing publishable quality {{research}} articles {{is a difficult}} task for novice scholarly writers. Particularly challenging is writing the Discussion/Conclusion section, which requires taking evaluative and interpretive stances on obtained results and substantiating claims regarding the worth of the scholarly contribution of the article to scientific knowledge. Conforming to {{the expectations of the}} target disciplinary community adds another dimension to the challenge. Corpus-based genre analysis can foster postgraduate writing instruction by providing insightful descriptions of rhetorical patterns and variation in disciplinary discourse. This paper introduces a pedagogically-oriented cross-disciplinary model of moves and steps devised through <b>top-down</b> corpus <b>analysis.</b> The model was applied to pedagogical materials and tasks designed to enhance genre and corpus-based teaching of Discussion/ Conclusions with an explicit focus on rhetorical conventions...|$|R
