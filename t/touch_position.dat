20|117|Public
40|$|This {{research}} {{proposes a}} Single Camera Finger <b>Touch</b> <b>Position</b> Detection System. Image processing based single camera figure <b>touch</b> <b>position</b> detection approach has several significant advantages: (1) No sensing devices {{need to be}} instrumented {{on the surface of}} the touch screen. (2) Minimum sensor construction can reduce the failure rate to realize maintenance free system. (3) This approach enables an easy installation and a low-cost touch sensing. The problem of using a single camera is how to detect the touch action and the position of the finger from a single view image. In order to solve this problem, we use the reflected fingertip image appears on the back of the screen. Detecting the fingertip and the reflected image on the screen effectively, we enable to detect <b>touch</b> <b>position</b> only using single camera. In this paper, we provide concrete method, algorithm, implement details, and several experimental results...|$|E
40|$|We {{propose a}} new image processing-based finger <b>touch</b> <b>position</b> {{detection}} sensing method using a reflected image reflected on the screen. Using single camera by image processing to detect <b>touch</b> <b>position</b> has several significant advantages: (1) Installation of camera becomes easily (2) This system can reduce the failure rate to realize maintenance free operation. (2) This approach enables easy attachment and low-cost touch sensing. The problem when using single camera is impossible to detect {{the depth of the}} finger from capture image. In order to solve this problem, we use the reflected image of the finger appears on the screen. Detecting the fingertip and the reflected image on the screen effectively, we enable to detect <b>touch</b> <b>position</b> only using single camera. In this paper, we provide concrete method to extract fingertip and reflected image, implementation details, results of evaluation experiment, and outline future works. The 2 nd International Conference on Intelligent Systems and Image Processing 2014, Nishinippon Institute of Technology, Kitakyushu, Japan, September 26 - 29, 201...|$|E
40|$|We {{present an}} {{interaction}} tool based on rendering distance cues for ordering sound sources in depth. The user interface {{consists of a}} linear position tactile sensor made by conductive material. The <b>touch</b> <b>position</b> is mapped onto the listening position on a rectangular virtual membrane, modeled by a bidimensional Digital Waveguide Mesh and providing distance cues. Spatialization of sound sources in depth allows a hierarchical display of multiple audio streams, as in auditory menus. Besides, the similar geometries of the haptic interface and the virtual auditory environment allow a direct mapping between the <b>touch</b> <b>position</b> and the listening position, providing an intuitive and continuous interaction tool for auditory navigation...|$|E
40|$|This paper {{deals with}} a new scheme for {{automatic}} segmentation of unconstrained handwritten connected numerals. The scheme is mainly based on features obtained from a new concept based on water reservoir. A reservoir is a metaphor to illustrate the region where numerals touch. Reservoir is obtained by considering accumulation of water poured from the top or {{from the bottom of}} the numerals. At first, considering reservoir location and size, <b>touching</b> <b>positions</b> (top, middle and bottom) are decided. Next, analyzing the reservoir boundary, <b>touching</b> <b>position</b> and topological features of the touching pattern, the best cutting point is determined. Finally, combined with morphological structural features the cutting path for segmentation is generated...|$|R
40|$|Colloque avec actes et comité de lecture. internationale. International audienceThis paper {{deals with}} a new scheme for {{automatic}} segmentation of unconstrained handwritten connected numerals. The scheme is mainly based on features obtained from a new concept based on water reservoir. A reservoir is a metaphor to illustrate the region where numerals touch. Reservoir is obtained by considering accumulation of water poured from the top or {{from the bottom of}} the numerals. At first, considering reservoir location and size, <b>touching</b> <b>positions</b> (top, middle and bottom) are decided. Next, analyzing the reservoir boundary, <b>touching</b> <b>position</b> and topological features of the touching pattern, the best cutting point is determined. Finally, combined with morphological structural features the cutting path for segmentation is generated...|$|R
6000|$|Thus far, no doubt, her {{strange and}} <b>touching</b> <b>position</b> has been plainly {{revealed}} to you. But can I feel quite {{so sure that}} you understand how seriously she has {{been affected by the}} anxiety, disappointment, and suspense which have combined together to torture her at this critical interval in her life? ...|$|R
40|$|Gaze has the {{potential}} to complement multi-touch for interaction on the same surface. We present gaze-touch, a technique that combines the two modalities based on the principle of ''gaze selects, touch manipulates''. Gaze is used to select a target, and coupled with multi-touch gestures that the user can perform anywhere on the surface. Gaze-touch enables users to manipulate any target from the same <b>touch</b> <b>position,</b> for whole-surface reachability and rapid context switching. Conversely, gaze-touch enables manipulation of the same target from any <b>touch</b> <b>position</b> on the surface, for example to avoid occlusion. Gaze-touch is designed to complement direct-touch as the default interaction on multi-touch surfaces. We provide a design space analysis of the properties of gaze-touch versus direct-touch, and present four applications that explore how gaze-touch can be used alongside direct-touch. The applications demonstrate use cases for interchangeable, complementary and alternative use of the two modes of interaction, and introduce novel techniques arising from the combination of gaze-touch and conventional multi-touch...|$|E
40|$|We {{demonstrate}} that front-of-screen targeting on mobile phones can be predicted from back-of-device grip manipulations. Using simple, low-resolution capacitive touch sensors placed around a standard phone, we outline a machine learning approach to modelling the grip modulation and inferring front-of-screen touch targets. We experimentally {{demonstrate that}} grip is a remarkably good predictor of touch, {{and we can}} predict <b>touch</b> <b>position</b> 200 ms before contact with an accuracy of 18 mm...|$|E
40|$|Abstract — In {{this paper}} {{we present a}} web camera based {{touchscreen}} system which uses a simple technique to detect and locate finger. We have used two cameras and regular screen to achieve our goal. By capturing the video and calcu-lating position of finger on the screen, we can determine the <b>touch</b> <b>position</b> and do some function on that location. Our method is very easy and simple to implement. Even our system requirement is less expensive compare to other techniques...|$|E
40|$|Article dans revue scientifique avec comité de lecture. This paper {{deals with}} a new {{technique}} for automatic segmentation of unconstrained handwritten connected numerals. To take care of variability involved in the writing style of different individuals a robust scheme is presented here. The scheme is mainly based on features obtained from a concept based on water reservoir. A reservoir is a metaphor to illustrate the region where numerals touch. Reservoir is obtained by considering accumulation of water poured from the top or {{from the bottom of}} the numerals. At first, considering reservoir location and size, <b>touching</b> <b>position</b> (top, middle or bottom) is decided. Next, analyzing the reservoir boundary, <b>touching</b> <b>position</b> and topological features of the touching pattern, the best cutting point is determined. Finally, combined with morphological structural features the cutting path for segmentation is generated. The proposed scheme is tested on French bank check data and an accuracy about 94. 8 % is obtained from the system...|$|R
50|$|Functions in <b>touch,</b> body <b>position,</b> pain, temperature, itch, taste, and arousal. Modulates tremor {{in certain}} pathology.|$|R
50|$|The RPM Albums Chart between October 1988 and April 1989 is not available. The <b>Touch</b> peak <b>position</b> {{is taken}} from May 1989.|$|R
40|$|This paper {{reports on}} downsampling-based {{techniques}} to achieve {{low power consumption}} and fast readout for capacitive touch screen panels. Here, touch interactivity is processed as an image, which is downsampled and reconstructed to estimate the <b>touch</b> <b>position.</b> After the reconstruction, a regional scan is performed around the reconstructed touch location to retrieve accurate touch information. Based on experimental and simulation results, we successfully decreased readout time and power consumption by 11. 3 ms (68 %) and 8. 79 mW (68. 7 %), respectively, when only 25 % sensors were selected. The presented technique yields higher responsivity and lower power consumption while maintaining detection accuracy...|$|E
40|$|Recent multi-touch {{multi-user}} tabletop systems offer rich touch contact {{properties to}} applications. Not only they provide touch positions, but also finger orientations. Applications can use these properties separated for each finger or derive information {{by combining the}} given touch contact data. In this paper, we present an approach to map fingers to their associated joined hand contributing to potential enhancements for gesture recognition and user interaction. For instance, a gesture can be composed of multiple fingers of one hand or different hands. Therefore, we present a simple heuristic for mapping fingers to hands that makes use of constraints applied to the <b>touch</b> <b>position</b> combined with the finger orientation. We tested our approach with collected diverse touch contact data and analyze the results...|$|E
40|$|We {{present the}} FuwaFuwa sensor module, a round, handsize, {{wireless}} device {{for measuring the}} shape deformations of soft objects such as cushions and plush toys. It can be embedded in typical soft objects in the household without complex installation procedures and without spoiling the softness of the object because it requires no physical connection. Six LEDs in the module emit IR light in six orthogonal directions, and six corresponding photosensors measure the reflected light energy. One can easily convert almost any soft object into a touch-input device that can detect both <b>touch</b> <b>position</b> and surface displacement by embedding multiple FuwaFuwa sensor modules in the object. A variety of example applications illustrate {{the utility of the}} FuwaFuwa sensor module. An evaluation of the proposed deformation measurement technique confirms its effectiveness. ACM Classification: H 5. m. Information interfaces and presentation (e. g., HCI) : Miscellaneous...|$|E
40|$|With {{the rise}} of current smartphones, virtual {{keyboards}} for touchscreens became the dominant mobile text entry technique. We developed a typing game that records how users touch on the standard Android keyboard to investigate users' typing behaviour. 47, 770, 625 keystrokes from 72, 945 installations have been collected by publishing the game. By visualizing the touch distribution we identified a systematic skew and derived a function that compensates this skew by shifting touch events. By updating the game we conduct an experiment that investigates the effect of shifting touch events, changing the keys' labels, and visualizing the <b>touched</b> <b>position.</b> Results based on 6, 603, 659 keystrokes and 13, 013 installations show that visualizing the <b>touched</b> <b>positions</b> using a simple dot decreases the error rate of the Android keyboard by 18. 3 % but also decreases the speed by 5. 2 % with no positive effect on learnability. The Android keyboard outperforms the control condition but the constructed shift function further improves the performance by 2. 2 % and decreases the error rate by 9. 1 %. We argue that the shift function can improve existing keyboards at no costs...|$|R
40|$|Most of the {{previous}} 3 DOF (Degree Of Freedom) 3 D <b>touch</b> <b>positioning</b> techniques require more than one finger (usually two hands) to be performed, which limits their using space on small mobile devices such as phones and tablets that need one hand {{to be held in}} most occasions. Given that the pressure sensitive touch screen would become ubiquitous in near future, we present the pressure-based 3 DOF 3 D objects positioning and manipulating techniques that only use one hand in operating...|$|R
40|$|AbstractThis paper aims to {{emphasize}} the importance of a multidisciplinary approach that may be able to account for tthe results in haptic feedback studies and to describe a framework for future research to achieve optimal surgical performance in haptiic feedback robotic surgery. Specifically, the value of integrating and unders standing the sensorimotor weighting mechanisms underlyin ng laparoscopic performance will be highlighted to develop future design, traiining and use of haptic feedback robotic surgery devices. TThe results presented in this study show that the sagittal distance estimate betw ween <b>touched</b> <b>positions</b> on the upper limb can be systematiccally biased by manipulating the <b>touched</b> <b>position</b> on the arm, although the s sagittal separation remains unaltered (12. 7 cm). These resu ults are somewhat surprising regarding the participants apparent neglect of pro oprioceptive input which would indicate limb position follo owing changes in elbow and shoulder joint angles and muscle length. If the pa articipant was optimizing the use of their proprioceptive inp put it would be expected that movement of the limb would translate into a com mputation {{to maintain the status quo}} of the perceived separattion of the probe and button. These results suggest that reporting tactile separa ation may be more difficult across limbs compared to withiin one limb and that sensory cues about tactile <b>position</b> (<b>touched</b> body segme ent and the distance from anatomical landmarks) outweig gh joint angle/muscle length positional information...|$|R
40|$|Spatialization {{of sound}} sources in depth allows a {{hierarchical}} display of multiple audio streams and therefore {{may be an}} efficient tool for developing novel auditory interfaces. In this paper we present an audio-haptic interface for audio browsing based on rendering distance cues for ordering sound sources in depth. The haptic interface includes a linear position tactile sensor made by conductive material. The <b>touch</b> <b>position</b> on the ribbon is mapped onto the listening position on a rectangular virtual membrane, modeled by a bidimensional Digital Waveguide Mesh and providing distance cues of four equally spaced sound sources. Furthermore a knob of a MIDI controller controls {{the position of the}} mesh along the playlist, which allows to browse the whole set of files. Subjects involved in a user study found the interface intuitive and entertaining. In particular the interaction with the stripe was highly appreciated...|$|E
40|$|Interactions {{in mobile}} devices {{normally}} happen in an explicit manner, {{which means that}} they are initiated by the users. Yet, users are typically unaware that they also interact implicitly with their devices. For instance, our hand pose changes naturally when we type text messages. Whilst the touchscreen captures finger touches, hand movements during this interaction however are unused. If this implicit hand movement is observed, it can be used as additional information to support or to enhance the users’ text entry experience. This thesis investigates how implicit sensing can be used to improve existing, standard interaction technique qualities. In particular, this thesis looks into enhancing front-of-device interaction through back-of-device and hand movement implicit sensing. We propose the investigation through machine learning techniques. We look into problems on how sensor data via implicit sensing can be used to predict a certain aspect of an interaction. For instance, {{one of the questions that}} this thesis attempts to answer is whether hand movement during a touch targeting task correlates with the <b>touch</b> <b>position.</b> This is a complex relationship to understand but can be best explained through machine learning. Using machine learning as a tool, such correlation can be measured, quantified, understood and used to make predictions on future <b>touch</b> <b>position.</b> Furthermore, this thesis also evaluates the predictive power of the sensor data. We show this through a number of studies. In Chapter 5 we show that probabilistic modelling of sensor inputs and recorded touch locations can be used to predict the general area of future touches on touchscreen. In Chapter 7, using SVM classifiers, we show that data from implicit sensing from general mobile interactions is user-specific. This can be used to identify users implicitly. In Chapter 6, we also show that touch interaction errors can be detected from sensor data. In our experiment, we show that there are sufficient distinguishable patterns between normal interaction signals and signals that are strongly correlated with interaction error. In all studies, we show that performance gain can be achieved by combining sensor inputs...|$|E
40|$|The {{fundamental}} {{dosimetric quantity}} {{used to measure}} the level of exposure due to cellular phones is the Specific Absorption Rate (SAR). The compliance of a device to the exposure limit is determined by following the guidelines described in IEEE measurement standards. The compliance assessment process can be quite complicated and time consuming, due to the many different test conditions that have to be considered and the slow speed of the measurement equipment. Depending on the frequency, use configuration of the device and form factor the SAR values can vary significantly from test case to test case even for the same device. This work shows that for a particular form factor, indicated as clam shell, where the phone is made of two halves that rotating around a hinge, one of the required test condition, described as tilt position, always provides lower SAR values than <b>touch</b> <b>position,</b> the other test condition. Different frequencies, antenna locations and antenna types have been considered. The results obtained with the well-known Finite Difference Time-Domain (FDTD) method have been validated with measurements. A robotic assessment tool (DASY 4) is used for measurement purpose...|$|E
5000|$|... flageolet: {{creating}} harmonics by <b>touching</b> overtone <b>positions</b> on {{the string}} with the finger {{of one hand}} and hitting the respective key {{with the other hand}} ...|$|R
6000|$|... [143] Lange's Gesch. d. Materialismus, i. 369; {{where the}} author [...] shows how entirely Voltaire failed to <b>touch</b> Holbach's <b>position</b> as to [...] {{the meaning of}} Order in the universe.|$|R
40|$|Although many {{augmented}} tabletop {{systems have}} shown the potential and usability of finger-based interactions and paper-based interfaces, they have mainly dealt with each of them separately. In this paper, we introduce a novel method aimed to improve human natural interactions on augmented tabletop systems, which enables multiple users to use both fingertips and physical papers as mediums for interaction. This method uses computer vision techniques to detect multi-fingertips both over and touching the surface in real-time regardless of their orientations. Fingertip and <b>touch</b> <b>positions</b> would then be used in combination with paper tracking to provide a richer set of interaction gestures that the users can perform in collaborative scenarios...|$|R
40|$|Touch {{interaction}} is an increasingly ubiquitous input modality on modern devices. It appears on devices including phones, tablets, smartwatches {{and even some}} recent laptops. Despite its popularity, touch as an input technology suffers from {{a high level of}} measurement un-certainty. This stems from issues such as the ‘fat finger problem’, where the soft pad of the finger creates an ambiguous contact region with the screen that must be approximated by a single touch point. In addition to these physical uncertainties, there are issues of uncertainty of intent when the user is unsure of the goal of a touch. Perhaps the most common example is when typing a word, the user may be unsure of the spelling leading to touches on the wrong keys. The uncertainty of touch leads to an offset between the user’s intended target and the <b>touch</b> <b>position</b> recorded by the device. While numerous models have been proposed to model and correct for these offsets, existing techniques in general have assumed that the offset is a de-terministic function of the input. We observe {{that this is not the}} case — touch also exhibits a random component. We propose in this dissertation that this property makes touch an ex...|$|E
40|$|A {{piezoelectric}} cantilever type sensor for {{locating the}} precise weak-impact or <b>touch</b> <b>position</b> {{on a plate}} is presented in this paper. Since the importance of human-computer interface such as a touch panel system has been rapidly increasing recently, this study could suggest an appropriate sensor {{for the detection of}} a weak-impact point effectively and accurately for such a system. This sensor detects the out-of-plane vibration of a panel when a touch with a finger or pen is applied on it. The sensor is made with a steel beam and a single crystal PMN-PT patch is bonded on the beam, which is designed to detect the base vibration of the panel. The sensor was designed, manufactured to verify the detect ability of a weak-impact and attached on two different plates of a glass of 400 × 400 × 4 mm and a wooden MDF of 600 × 600 × 9 mm. The experiment result of the sensor was compared with that of an accelerometer which can also be used for the same purpose and shows clear weak-impact responses with a narrow-band property at its resonant frequency. It is expected that the cantilever type sensor in this study could be applied to make a simple flat plate into a touch panel when the time difference of arrivals method is used to locate the weak-impact point...|$|E
40|$|Touch {{technology}} is rapidly evolving, and soon deformable, movable and malleable touch interfaces {{may be part}} of everyday computing. While {{there has been a lot}} of work on understanding touch interactions on flat surfaces, as well as recent work about pointing on curved surfaces, little is known about how surface deformation affects touch interactions. This paper presents the study of how different features of deformable surfaces affect touch selection accuracy, both in terms of position and control of the deformation distance, which refers to the distance traveled by the finger when deforming the surface. We conducted three separate user studies, investigating how touch interactions on a deformable surface are affected not only by the compliant force feedback generated by the elastic surface, but also by the use of visual feedback, the use of a tactile delimiter to indicate the maximum deformation distance, and the use of hemispherical surface shape. The results indicate that, when provided with visual feedback, users can achieve sub-millimeter precision for deformation distance. In addition, without visual feedback, users tend to overestimate deformation distance especially in conditions that require less deformation and therefore provide less surface tension. While the use of a tactile delimiter to indicate maximum deformation improves the distance estimation accuracy, it does not eliminate overestimation. Finally, the shape of the surface also affects touch selection accuracy for both <b>touch</b> <b>position</b> and deformation distance...|$|E
50|$|The user {{interacted with}} the globe by {{touching}} {{the surface of}} the globe with an attached pen. The globe then calculated a latitude/longitude position which was used to determine the location <b>touched.</b> This <b>position</b> sensing was implemented using a patented technology called NearTouch developed by Explore Technologies.|$|R
40|$|We {{envision}} a nomadic model of interaction where {{the personal computer}} fits in your pocket. Such a computer is extremely limited in screen space. A technique is described for “spilling ” the display of a hand held computer onto a much larger table top display surface. Because our model of nomadic computing frequently {{involves the use of}} untrusted display services we restrict interactive input to the hand held. Navigation techniques such as scrolling or turning the display can be expressed through the table top. The orientation and position of the hand held on the table top is detected using three conductive feet that appear to the touch table like three finger touches. An algorithm is given for detecting the three <b>touch</b> <b>positions</b> from the table’s sensing mechanism. 1...|$|R
5000|$|Makin' Bacon (or Oinker) - If both pigs are <b>touching</b> in any <b>position,</b> {{then the}} total score is reset to 0 and the turn changes to the next player ...|$|R
40|$|Since {{conventional}} audio-video teleconferencing {{systems have}} reached their limits, {{the needs of}} integrating new sensations to improve users' telecommunication experience are growing. Haptics, the sense of touch, which includes handshake, comforting hug, encouraging pat, and other physical contacts, is of great importance for interpersonal communication, since it allows people to express and receive intimate affection, intention or emotion efficiently. Motivated by a few haptic telecommunication softwares, this thesis presents an innovative webcam-based touchscreen to replace the haptic device, which is used in HugMe system as human hand, to further increase people's degree of immersion with the audio-video-haptic teleconferencing system at a more reasonable price. With our webcam-based touchscreen, the user could directly use his/her bare hand to touch {{the image of the}} person who he/she is chatting with, instead of controlling a haptic device as the medium to realize indirect touch. This thesis also gives details of the touchscreen method and our proposed mathematical models for <b>touch</b> <b>position</b> calculation. Experimental results show that our system is accurate and robust, while maintaining high compatibility with conventional audio-video teleconferencing systems for combination. With our haptic jacket, the passive user can feel the touch of the active user at the right position. Also, additional applications with the touchscreen, such as writing and drawing, are developed and tested. Finally, we draw the conclusions and talk about future work...|$|E
40|$|AbstractThis paper {{describes}} {{the laws and}} regulations that affect the practice of energy medicine. State law often has more impact on a health care practice than federal law, but federal law provides a common denominator among states. Device law is emphasized here because practitioners of energy medicine {{are more likely to}} use devices than drugs. For purposes of this paper, energy medicine is defined as practices that measure or benefit energy flow and overall energy in the body. This broad definition encompasses things as diverse as certain forms of exercise, measurement of meridian resistance, the use of electrical current or magnetic pulses to relieve pain, and the use of light, sound, scent, <b>touch,</b> <b>position,</b> or movement to stimulate the body's own electrical systems. What is of greatest importance in determining legal implications of a practice is whether there are any health-related claims. Two federal entities are pivotal. The Food and Drug Administration (“FDA”) is authorized to protect health and safety and the Federal Trade Commission (“FTC”) is authorized to protect consumers from false or misleading advertising. There are 5 things that FDA looks at: 1) intended use, 2) claims made in advertising and in labeling, 3) substantial equivalence to a predicate, 4) safety, and 5) effectiveness. A concern regarding any one of these can be the basis for denying clearance to market a device. The FTC looks at whether statements are true and substantiated and whether they might be misleading. The FTC often consults with the FDA on the interpretation of technical information...|$|E
40|$|Interactive {{handheld}} electronic displays use hydrogenated {{amorphous silicon}} (a-Si:H) thin film transistor (TFT) as a backplane and a Touch Screen Panel (TSP) on top as an input device. The low mobility and instability of a-Si:H TFT threshold voltage are major two issues for driving constant current as required for Active Matrix Organic Light Emitting Ddiode (AMOLED) displays. Low mobility is compensated by increasing transistor width or resorting to more expensive material TFTs. On the other hand, the ever increasing threshold voltage shift degrades the drain current under electrical operation causing OLED display to dim. Mutual capacitive TSP, the current cell phone standard, requires two layers of metals and a dielectric {{to be put in}} front of the display, further dimming the device and adding to visual noise due to sun reflection, not to mention increased integration cost and decreased yield. This thesis focuses on the aforementioned technological hurdles of a handheld electronic display by proposing a dual-gate TFT used as an OLED current driving TFT and a novel phase response readout scheme that can be applied to a one metal track TSP. Our dual-gate TFT has shown on average 20 % increase in drive current over a single gate TFT fabricated in the same batch, attributed to the aid of a top channel to the convention bottom channel TFT. Furthermore the dual gate TFT shows three times the Poole-Frenkel current than the single gate TFT attributed to the increase in gate to drain overlap. The dual-gate TFT shows a 50 % improvement in threshold voltage shift over a single gate TFT at room temperature, but only ~ 8 % improvement under 75 ºC. This is an important observation as it shows an accelerated threshold voltage shift in the dual-gate. This difference in the rate of threshold voltage change under varying temperature is attributed to the difference in interface states, supporting Libsch and Kanicki’s multi-level temperature dependant dielectric trapping model. The phase response TSP readout scheme requires IC only {{on one side of the}} display. Cadence Spectre simulation results showed that both touch occurrence and <b>touch</b> <b>position</b> can be obtained using only one metal layer...|$|E
5000|$|The {{principal}} sensory {{nucleus of}} the trigeminal nerve represents <b>touch</b> and <b>position</b> information {{of the head and}} face, but not the neck or back of the head, which are innervated by the cervical nerves. Pain and temperature information is also not represented within the principle nucleus, but rather in the spinal trigeminal nucleus, which is caudal to the pontine tegmentum in the medulla.|$|R
6000|$|... "The {{interesting}} penitent (expecting Lady Janet's visit) was, of course, {{discovered in}} a <b>touching</b> domestic <b>position!</b> She had a foundling baby asleep on her lap; and she was teaching the alphabet to an ugly little vagabond girl whose acquaintance she had first made in the street. Just the sort of artful tableau vivant to impose on an old lady--was it not? ...|$|R
40|$|International audienceThe smart watch, {{increasingly}} gaining popularity, {{has limited}} {{input and output}} capabilities due to its size and thus mostly used as a surrogate device to the smart phone. In this poster, we propose “Touch Skin (TS) ” that enlarges the interaction space of the smart watch using the hand (or skin) surface and proprioceptive sense. While the input interface is displayed on the small smart watch screen, the interaction is carried out by touching on the larger hand surface to which the input interface elements (e. g. graphical buttons and keys) are mapped. We hypothesize {{that even though the}} display and interaction surface are separated, the humans are nevertheless able to perform competently based on one’s proprioceptive sense. While sensing for finger <b>touch</b> <b>positions</b> on the hand/skin surface remains to be a technical hurdle, we first explore whether our hypothesis is valid through an enactment study comparing the performance the Touch Skin input to that of the nominal smart phones...|$|R
