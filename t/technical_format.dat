15|88|Public
5000|$|... ==CDWA Lite== ARTstor, the J. Paul Getty Trust, and RLG Programs/OCLC {{have worked}} {{together}} to develop an XML schema to describe cultural materials and their surrogates to provide an easier and more sustainable model for contributing to union resources. This initiative was driven {{by the absence of}} a data content standard specifically designed for unique cultural works, and a <b>technical</b> <b>format</b> for expressing this data in a machine-readable format.|$|E
5000|$|GTX is envisioned as {{a privately}} operated, {{self-sustaining}} (e.g. user-fee based) trade information {{system that will}} collect commercial transaction data not currently available to us from parties in the supply chain who have contracted or provided services for the production/movement of international shipments and allow government and trade community participants to input and access trade data through an information broker on an expanded global basis. Data maintained by the GTX broker {{will be available to}} participating customs agencies 24X7 in the appropriate <b>technical</b> <b>format.</b>|$|E
50|$|System 7 {{were one}} of the first techno groups to play live, and have {{developed}} a live performance approach that is a bit more extreme than their records, with tougher beats and hypnotic live echo loops. The first System 7 live show was in December 1990 at Linford Studios in Battersea, London, consisting of Steve playing guitar and Alex Paterson playing samples and DJing pre-release versions of the upcoming first System 7 and Orb albums. In 1991 there were some more shows and a tour to support the System 7 album release, with Miquette's synthesisers incorporated into the live sound. In 1993 System 7, now playing as a duo, developed a new live <b>technical</b> <b>format</b> and joined Orbital, The Drum Club, and Aphex Twin on the Midi Circus tour of the UK with the legendary Megadog party crew.|$|E
5000|$|... #Subtitle level 3: <b>Technical</b> {{documentation}} <b>formats</b> for {{source data}} ...|$|R
50|$|Initially, Venevisión {{broadcast}} live because they hadn't yet installed the videotape system. Except for the news, {{the elaboration of}} their programs utilized the <b>technical</b> <b>formats</b> used in movies at that time. In {{a short period of}} time, Venevisión greatly expanded nationally, and was seen in most of Venezuela on many VHF and UHF channels.|$|R
40|$|Annotating {{linguistic}} data {{has become a}} major field of interest, both for supplying the necessary data for machine learning approaches to NLP applications, and as a research issue in its own right. This comprises issues of <b>technical</b> <b>formats,</b> tools, and methodologies of annotation. We provide a brief overview of these notions and then introduce the papers assembled in this special issue. Department of Chinese and Bilingual Studie...|$|R
40|$|The Piggin Stemma is an SVG {{document}} that reconstructs the fifth-century Great Stemma, presenting it {{in a modern}} idiom as a digital document with regular typefaces, strict graphic alignments and twelve explanatory overlays. This is the most complete edition to date of this late antique diagrammatic chronicle of biblical history and genealogy, and the <b>technical</b> <b>format</b> and choices involved in the publication are explained here...|$|E
40|$|Procs of Computer Supported Collaborative Learning (CSCL 2009), Rhodos, Greece - PosterWe {{point out}} the need for CSCL {{community}} to reach large scale validation for its results by addressing the lack of sharing of interaction indicators and data. The main goal of the Mulce project is a definition for teaching and learning corpora (especially for interaction tracks), a <b>technical</b> <b>format</b> to organize data and a platform for corpus sharing, providing analysis and visualization tools...|$|E
40|$|This chapter {{provides}} a critical discourse analysis of French-speaking players’ personal and collective identity construction in World of Warcraft. Based on sixteen semi-structured interviews conducted online, we have analyzed how players introduce their avatars, {{the extent to}} which avatars correspond to or differ from players’ real selves, and how players perceive and construct collective identity within their guilds. The study revealed that most players make use of avatar introduction as a rhetorical pretext to fabricate narratives of personal experiences related to their game practice. In addition, players’ discourses made it evident that avatars constituted either transparent (extensions) or translucent (enhancements) representations of their real selves. Finally, collective identity within guilds was mostly perceived and experienced through shared values that transcend the <b>technical</b> <b>format</b> of the game including sense of belonging, trust, respect, putting things into perspective, and modesty...|$|E
40|$|Contemporary {{metadata}} {{principles and}} standards tended {{to result in}} document-centric rather than data-centric; human-readable rather than machine-processable metadata. In order for libraries to create and harness shareable, mashable and re-usable metadata, a conceptual shift {{can be achieved by}} adjusting current library models such as Resource Description and Access (RDA) and Functional Requirements for Bibliographic Records (FRBR) to models based on Linked Data principles. In relation to <b>technical</b> <b>formats,</b> libraries can leapfrog to Linked Data <b>technical</b> <b>formats</b> such as the Resource Description Framework (RDF), without disrupting current library metadata operations. This paper provides six key recommendations for libraries and standards agencies. These include rising to the challenges and embracing the opportunities presented by current technological trends, adopting minimal requirements of Linked Data principles, developing ontologies, deciding on what needs to be retained from current library models, becoming part of the Linked Data cloud, and developing mixed-metadata (standards-based and socially-constructed) approaches. Finally, the paper concludes by identifying and discussing five major benefits of such metadata re-conceptualisation. The benefits include metadata openness and sharing, serendipitous discovery of information resources, identification of zeitgeist and emergent metadata, facet-based navigation and metadata enriched with links...|$|R
50|$|Format {{shifting}} {{is central}} to preservation and archiving, particularly for sound recordings and films. In addition to efforts to preserve works created in deteriorating formats format shifting is also necessary to keep works accessible. As technology develops the <b>technical</b> <b>formats</b> get outdated and the technology necessary for accessing original formats is no longer available. Copyright law of the United Kingdom may not allow libraries and archives to format shift for preservation and archiving purposes. By the time copyright term in a work ends the original work may have disintegrated or deteriorated {{to such an extent}} that the cost of preservation increases.|$|R
50|$|In 1896, {{the school}} was once again renamed, this time to Christian Brothers College, a name meant to reflect the institution's great {{emphasis}} on advanced secondary learning in a classical, rather than <b>technical,</b> curricular <b>format.</b>|$|R
40|$|New product {{introductions}} rely on {{technologies that}} are often subject to strongly contested standards wars. In an attempt {{to ensure that the}} technical formats that their products are built upon, are the ones that gain widespread market acceptance and thereby emerge as industry standards, firms often engage in alliances to develop and/or market these technical formats. This research examines the relationships between the characteristics of standard setting alliances, those of the alliance partners, the technical formats and the market acceptance of the formats. In doing so, I seek to complement prior research by developing and empirically testing a theoretical framework of these relationships. While a few studies (Axelrod et al. 1995; Chiao, Lerner and Tirole 2007) have examined how firms form and organize standard setting alliances, the relationship between the characteristics of such alliances and their success (i. e., the market?s acceptance of the <b>technical</b> <b>format</b> supported by the alliance) is an under-researched subject. A format that is widely accepted by the market (adopted in more products and adopted by more firms) is in turn more likely to emerge as a standard. Using a unique data set of formats and standard setting alliances in the consumer electronics industry, assembled from multiple sources, I examine this link between standard setting alliances and format characteristics, and the market?s acceptance of the format. Results indicate that the relationship between the size of a standard setting alliance (number of partners in alliance) and the market acceptance of a format is inverted U-shaped. This suggests that a larger membership in the development alliance does not always imply that the alliance activities will lead to market acceptance of the format. I find that alliances with a greater proportion of generalists are shown to be capable of developing formats that find greater acceptance in the market. Marketing intensity in the years prior to forming the alliance is found to be important. The results also suggest that the broader the applicability of a <b>technical</b> <b>format</b> across industries, the greater its market acceptance. Interestingly though, the hypothesis that formalized alliances lead to greater market acceptance of the format was not supported by the data. I conclude with a discussion of the potential contributions and implications of the findings for marketing practice and future research...|$|E
40|$|IP {{specifies}} the <b>technical</b> <b>format</b> of packets and the {{addressing scheme}} for computers to communicate over a network. Most networks combine IP with a higher-level protocol called Transmission Control Protocol (TCP), which establishes a virtual connection between a destination and a source. When any organization wants to implement IPv 6 network in their service area, {{it is not}} imaginable to implement {{all of a sudden}} in an entire area. It necessities slow migration from IPv 4 to IPv 6 without greatly affecting the Service. The Dual stack deployment using NAT and tunneling concept has been proposed for this migration. While IPv 6 deployment from the inside of the network leading to the edges with success, the transition remains an issue today. To fill this gap, we propose a technique to provide the supporting tools and algorithms to enable this transition to become automatic and enable inbuilt security features. Based on a model of an IPv 4 network, we design and implement IPv 6 network and it provisions auto-configuration to the host and security is inbuilt with this protocol...|$|E
40|$|Abstract Digital {{preservation}} can encompass a {{range of}} activities, from simple replication and storage to more complex transformation, depending on the assessed value and risk to the target content. These activities require planning and, in most cases, begin with a {{need to know the}} <b>technical</b> <b>format</b> of the target content. In this case, the target is the content deposited in institutional repositories (IRs). The Preserv project [1] set out to investigate the use of The National Archives' (TNA) PRONOM-DROID service (PRONOM is the online registry of technical information; DROID is the downloadable file format identification tool) for file format identification on two pilot IRs using EPrints software, and instead produced format profiles (Preserv profiles) of over 200 repositories presented via the Registry of Open Access Repositories (ROAR). Thus a primary element of preservation planning {{has been shown to be}} possible based on a standard Web interface (OAI) and no formal arrangement between repository and provider. The implications of this go beyond the numbers towards a reconceptualisation of repository preservation service provider models. Repositories and providers can shape preservation services at different cost levels that could range from comprehensive 'black-box' preservation to pick-and-mix lightweight Web-based services that build on the common starting point, format identification. This article describes the evolution of a series of models that have informed progress towards this conception of flexible and distributed preservation services for IRs...|$|E
50|$|In {{addition}} to regional {{meetings in the}} United States, an annual meeting is held each September at the Kimmel Collection in Michigan. The meeting has a <b>technical</b> conference <b>format</b> mixed with demonstrations of steam systems and performance trials of steam cars.|$|R
50|$|Key management, trust bootstrapping, {{federation}} and {{agreement on}} the <b>technical</b> details (ciphers, <b>formats,</b> algorithms) is {{outside the scope of}} WS-Security.|$|R
5000|$|The station began {{broadcasting}} in 1947 as CHEX-FM. In 1976, CHEX-FM became CFMP-FM, meaning ("FMP" [...] FM Peterborough). Over {{the years}} since CHEX-FM {{went on the air}} in the 1940s, the station went through a number of ownerships, <b>technical</b> and <b>format</b> changes until 1992, CFMP adopted its current callsign and format.|$|R
40|$|This is {{a conference}} paper. The Question and Test Interoperability (QTI) {{specification}} describes an XML-based <b>technical</b> <b>format</b> for the coding and exchange of assessment content from individual questions through to complete tests. The specification {{was first published}} by the IMS Global Learning Consortium in June 2000 and since then {{an increasing number of}} developers have used it as a guide to implementing assessment functionality in their e-learning systems, in addition to the more established assessment tool providers who have added QTI export/import facilities to their products. In September 2003, IMS approved a project to develop version 2 of the specification, concentrating on the format of individual 'items' and the way they can be packaged and integrated into authored learning experiences based on the companion specifications known as Simple Sequencing and Learning Design. At the time of writing, a draft of version 2 is being prepared for public review and should be available for review from the IMS website two weeks before the CAA 2004 Conference. Version 2 will represent a significant step forward for the specification, addressing many of the issues that have been raised by developers and providing some powerful new capabilities based on some of the extensions to version 1 that have developed within the community. Readers are encouraged to review the public draft for detailed technical information. This paper will concentrate on the more general issues raised with version 1 of the specification and how they are being addressed. It also attempts to address some more general criticisms of technical standardization work as a whole...|$|E
40|$|ABSTRACT: During {{test and}} {{integration}} (step 5 in FEDEP), federate developers {{get together to}} ensure the technical interoperability of the participating systems. This includes testing if their systems exchange data according to the federation agreements. In HLA 1516, the <b>technical</b> <b>format</b> (encoding) of the data that is exchanged over the RTI is fully specified in the Federation Object Model. Each federate {{is responsible for the}} encoding and decoding of data based on this format. Still, this {{does not necessarily mean that}} all developers have successfully interpreted and implemented encoding/decoding. A surprisingly large portion of today’s federation efforts experience problems with this. Incorrect encoding/decoding can make federates or federations crash, hang or even worse; misinterpreting the received data. All of this introduce risk, delay and cost to integration. What is even worse, it may cause the final federation to run with incorrect outputs or provide inadequate training. While it is possible to use tools such as data loggers to check the correctness of the encoding, the fundamental question remains: Why does it have to be so hard to get the data that is exchanged over the RTI correctly encoded and decoded? This problem has already been solved in other interoperability solutions for distributed systems, such as CORBA, DCOM and Web Services. A solution for HLA, the Encoding Helpers, has been suggested for HLA Evolved based on an idea originating from the DMSO RTI Java API. A Tiger Team has been formed to provide the details. The encoding helpers are available both to C++ and Java developers and can also be used together with the Web Service HLA API...|$|E
40|$|Objective: Cancer {{patients}} receiving chemotherapy or a Stem Cell Transplantation (SCT) {{are in need}} of information about their disease, treatment options and side effects. Patient education usually has to be given within limited time. Under these circumstances,, patients may find it difficult to completely understand and to retain the information given. Methods: As a supplement to standard information methods we developed an interactive CD-ROM with information on SCT. This CD-ROM provides both medical information and more subjective patients' experiences. Part one provides information regarding the treatment course from diagnosis through to post-discharge care. The second part consists of interviews with former patients and describes their experiences. As the system is interactive, it can be utilised according to the patient's individual preferences. The CD-ROM comprises audio, video, animations, Pictures,, and text. Printing of certain sections is optional. The <b>technical</b> <b>format</b> of the CD-ROM makes it relatively simple to utilise the information and to make it suitable for other institutions or even other treatments. In this preliminary Study the acceptability of the interactive CDROM by patients undergoing a SCT is described. Results: Patients' overall evaluations of the interactive CD-ROM were highly positive. For example, 90. 2 % (N = 5 1) found it interesting, clear, useful and valued getting information by means of a CD-ROM. Most patients would recommend the interactive CD-ROM to other patients in the same situation. Conclusion: The content of the CD-ROM on SCT as well as the computer-based interactive method are well accepted by patients. Practice implications: Computer-based education may enhance patient education and thus the quality of patient care. We must now establish the program's effectiveness. Moreover, plans have been developed to disseminate the information on SCT over the Internet. Future development of comparable programs and their evaluation should be encouraged to promote the well-being of cancer patients. (C) 2008 Elsevier Ireland Ltd. All rights reserve...|$|E
40|$|Version 0. 6 b These {{profiles}} are {{a work in}} progress. They {{are presented}} {{to the international community}} for review and comment. The profiles are undergoing continual updating for <b>technical</b> content, <b>formatting,</b> grammar, and other issues. Each country profile will be modified on a continuous basis as new information is made available...|$|R
5000|$|Owanto {{has used}} pop, {{conceptual}} and minimal art in her creation of universal symbols, which remind spectators {{of where the}} solutions to our world may lie, and how a society lacking moral strength may begin to heal. Using her sculptures as starting points, Owanto has created a series of icons which she presents in highly <b>technical</b> <b>formats</b> such as light-boxes and traffic signs - media previously explored by Maurizio Cattelan, Rogelio López Cuenca, Gabriel Acuña and Michael Pinsky. Owantoʼs discourse focuses on images of a family group and a child playing, which suggest a happier world to come. The pieces have a double intention: to alert us to solutions to our global predicament, and to suggest a change in governing attitudes and rules. The light-boxes, like torches and lighthouses, illuminate a future characterised by tolerance, unity and hope.” ...|$|R
40|$|Normal 0 Increasing {{demand to}} manage and {{preserve}} 3 -dimensional models {{for a variety of}} physical phenomena (e. g., building and engineering designs, computer games, or scientific visualizations) is creating new challenges for digital archives. Preserving 3 D models requires identifying <b>technical</b> <b>formats</b> for the models that can be maintained over time, and the available formats offer different advantages and disadvantages depending on the intended future uses of the models. Additionally, the metadata required to manage 3 D models is not yet standardized, and getting intellectual proposal rights for digital models is uncharted territory.   The FACADE Project at MIT is investigating these challenges in the architecture, engineering and construction (AEC) industry and has developed recommendations and systems to support digital archives in dealing with digital 3 D models and related data. These results can also be generalized to other domains doing 3 D modeling...|$|R
40|$|Joint Reseach Centre - Institute for Environment and Sustainability, Italy The Life Cycle Data Network, {{available}} through the European Platform on Life Cycle Assessment (EPLCA) aims at providing a globally usable infrastructure for consistent and quality assured LCA data (i. e. Life Cycle Inventory and Life Cycle Impact Assessment datasets) from different organisations. It is a web-based infrastructure to ensure LCA data are easy to access via searches, filtering, and sorting, and, thus, it helps reducing assessment cost overall. The provision of authoritative requirements by the European Commission, {{may lead to the}} availability of coherent, quality-assured life cycle data and studies is aimed to harmonize the LCA and the environmental footprint methods in business and in policy. Any data provider from any country can join. The datasets in the web-based, remote network may come from any kind of data developer/owner/ provider, e. g. industry, single organizations, national and international LCA projects, research groups, and consultants. The data will be published by the contributors under its own conditions (e. g. free, for fee, or via registration). Metadata needs to be accessible for free. Businesses, governments, academia, and consultancies can hence provide their data to this decentralized network, based on their own conditions. Dataset quality within the ILCD DN is ensured through the ILCD Entry-Level requirements 1. The compliance with at least these requirements (or the full ILCD compliance) is a pre-requisite for the registration to the ILCD DN, both at the EU and international level, towards the agreement on common strategies and harmonisation. The ILCD format provides the <b>technical</b> <b>format</b> (using XML technology) basis for ILCD Data sets and Databases. It has been released after an extensive international consultation, and has been developed starting from the ISO/TS 14048. It’s suitable both for aggregated and unit processes, and gives the possibility to include additional and pictures/charts. JRC. H. 8 -Sustainability Assessmen...|$|E
40|$|Supplementary {{material}} {{included on}} disc with print copy. iDCC {{is the new}} implement of Digital Command Control (DCC) with a better user interface and a much better user experience compared to the existing DCC controllers on the market. The iDCC project was carried out firstly at 2009 and the proof-of-concept prototype was available {{at the beginning of}} this project. The goal of this project is to prepare this prototype for a commercialization through the real-world industry arrangement. This is also a new framework of a Master degree project. The author of this thesis has acted as a project manager and a hardware engineer to work with a 4 -member software team and a 2 -member marketing team whom all are the students of the University of Waikato. As hardware engineers, the duties were to test the previous prototype and develop a brand new hardware which has more functionality and stability. With the unique scanning technology developed in this project it eliminates the complexity of the model train operation and simplifies the <b>technical</b> <b>format</b> of DCC controllers. The enclosure case also has been designed by the author to meet the market need. The software team developed and tested the firmware to fulfill the concept of iDCC with the author. The market team prepared logos, product names, and advertising materials to achieve the market promotion requirement. The author also acts as the project manager to lead the teams together to achieve the goal. The final product is ready to be released to the market as an entry level DCC controller, and the result of this project also shows that this type of framework is sustainable so that it can be applied in any Master or even Phd level project...|$|E
40|$|The Life Cycle Data Network (LCDN), {{available}} through the European Platform on Life Cycle Assessment (EPLCA) aims at providing a globally usable infrastructure for consistent and quality assured LCA data (i. e. Life Cycle Inventory and Life Cycle Impact Assessment datasets) from different organisations. It is a web-based infrastructure to ensure LCA data are easy to access via searches, filtering, and sorting, and, thus, it helps reducing assessment cost overall. The provision of authoritative requirements by the European Commission, {{may lead to the}} availability of coherent, quality-assured life cycle data and studies is aimed to harmonize the LCA and the environmental footprint methods in business and in policy. Any data provider from any country can join. The datasets in the web-based, remote network may come from any kind of data developer/owner/ provider, e. g. industry, single organizations, national and international LCA projects, research groups, and consultants. The data will be published by the contributors under its own conditions (e. g. free, for fee, or via registration). Metadata needs to be accessible for free. Businesses, governments, academia, and consultancies can hence provide their data to this decentralized network, based on their own conditions. Dataset quality within the LCDN is ensured through the ILCD Entry-Level requirements. The compliance with these requirements is a pre-requisite for the registration to the LCDN, both at the EU and international level, towards the agreement on common strategies and harmonisation. The ILCD format provides the <b>technical</b> <b>format</b> (using XML technology) basis for ILCD Data sets and Databases. It has been released after an extensive international consultation, and has been developed starting from the ISO/TS 14048. It’s suitable both for aggregated and unit processes, and gives the possibility to include additional and pictures/charts. JRC. H. 8 -Sustainability Assessmen...|$|E
50|$|It was {{originally}} created {{for use by}} local authorities and other organisations {{in the production of}} accessibility strategies, which was a requirement for Local Transport Plan. The data repository was first set up in 2004, and an annual snapshot of data has been created each year since then. Data is collected for all public transport services running in Great Britain during a full week in October each year. It is supplied by Traveline regions and the Association of Train Operating Companies (ATOC) and processed into files for each local authority, broken down further into files for each transport mode. It {{should be noted that the}} data in the repository is in both ATCO.CIF and TransXChange <b>technical</b> <b>formats,</b> rather than in a format that would be readily recognised as a timetable. NPTDR was made available as Open Data in September 2010, with new October 2010 data released in March 2011 (Previously there was a charge made for this dataset).|$|R
5000|$|A workforce-focused example {{includes}} varying <b>technical</b> {{and digital}} <b>formats</b> by describing data literacy as [...] "... competence in finding, manipulating, managing, and interpreting data, including not just numbers but also text and images." ...|$|R
40|$|A {{summary of}} the {{analyses}} of the orbital operations study is presented. Objectives, scope of study, and <b>technical</b> documentation <b>format</b> are discussed. A {{summary of the}} mission analyses including generic mission models, element pair interactions, and interfacing activities are presented. The analyses associated with each interfacing activity are also summarized. Significant implications derived {{during the course of}} the study on the EOS orbiter, space tug, RAM, and MSS are indicated...|$|R
40|$|Increasing {{demand to}} manage and {{preserve}} 3 -dimensional models {{for a variety of}} physical phenomena (e. g., building and engineering designs, computer games, or scientific visualizations) is creating new challenges for digital archives. Preserving 3 D models requires identifying <b>technical</b> <b>formats</b> for the models that can be maintained over time, and the available formats offer different advantages and disadvantages depending on the intended future uses of the models. Additionally, the metadata required to manage 3 D models is not yet standardized, and getting intellectual proposal rights for digital models is uncharted territory. The FACADE Project at MIT is investigating these challenges in the architecture, engineering and construction (AEC) industry and has developed recommendations and systems to support digital archives in dealing with digital 3 D models and related data. These results can also be generalized to other domains doing 3 D modeling. The International Journal of Digital Curation is an international journal committed to scholarly excellence and dedicated to the advancement of digital curation across a wide range of sectors. ISSN: 1746 - 8256 The IJDC i...|$|R
50|$|It was a {{commercial}} failure due to many <b>technical</b> problems and <b>format</b> incompatibilities. Quadraphonic audio formats were {{more expensive to}} produce than standard two-channel stereo. Playback required additional speakers and specially designed decoders and amplifiers.|$|R
5000|$|<b>Technical</b> details: The <b>format,</b> layout, font should {{resemble}} {{as closely}} as possible to those volumes already published in the Serio, size 21 x 15 cm, printed area 162 x 100 mm, of 9- or 10-point font.|$|R
40|$|The project {{described}} in this paper is funded by the French Ministry of Research. It aims at providing producers of Language Resources, and HLT players in general, with a guide which offers technical, legal and strategic recommendations/guidelines for the reuse of their Language Resources. The guide is dedicated in particular to academic laboratories who produce Language Resources and may benefit from further advice to start development, but also to any HLT player who wishes to follow the best practices in this field. The guidelines focus on different steps of a Language Resource “life”, i. e. specifications, production, validation, distribution, and maintenance. This paper gives {{a brief overview of}} the guide, and describes a) <b>technical</b> <b>formats,</b> standards and best practices which correspond to {{the current state of the}} art, for different types of resources, whether written or spoken, at different steps of the production line, b) legal issues and models/templates which can be used for the dissemination of Language Resources as widely as possible, c) strategic issues, by offering a dissemination plan which takes into account all types of constraints faced by HLT community players...|$|R
50|$|Star Trek: The Animated Series {{was named}} the 96th best {{animated}} series by IGN. They declared that although the series suffered from <b>technical</b> limitations, its <b>format</b> allowed the writers far greater freedom and creativity than was possible in the original live-action series.|$|R
40|$|CAIA has {{traditionally}} provided OpenOffice templates to generate <b>Technical</b> Report <b>formatted</b> papers. This approach has limitations, modifying an existing Technical Report for submission {{to a conference}} of journal can be time consuming. LATEX {{on the other hand}} provides consistent generation of formatted documents direct to PDF output and simple modification to new formats. In this paper we present the installation instructions for the CAIA Technical Report LATEX Class files on a FreeBSD based system and pointers for generating CAIA Technical Reports...|$|R
