292|265|Public
25|$|Oversewing, {{where the}} {{signatures}} {{of the book}} start off as loose pages which are then clamped together. Small vertical holes are punched through the far left-hand edge of each signature, and then the signatures are sewn together with lock-stitches to form the <b>text</b> <b>block.</b> Oversewing {{is a very strong}} method of binding and can be done on books up to five inches thick. However, the margins of oversewn books are reduced and the pages will not lie flat when opened.|$|E
25|$|Sewing {{through the}} fold (also called Smyth Sewing), where the {{signatures}} {{of the book}} are folded and stitched through the fold. The signatures are then sewn and glued together at the spine to form a <b>text</b> <b>block.</b> In contrast to oversewing, through-the-fold books have wide margins and can open completely flat. Many varieties of sewing stitches exist, from basic links to the often used Kettle Stitch. While Western books are generally sewn through punched holes or sawed notches along the fold, some Asian bindings, such as the Retchoso or Butterfly Stitch of Japan, use small slits instead of punched holes.|$|E
25|$|It {{is known}} from Hammond & Anderson {{that the first}} {{printing}} appeared on March 1, 1938. A library copy of the third printing has been seen with a check-out date of November 28, 1938. If we accept, then, that the first edition consisted of multiple runs on the evidence of emendations to the <b>text</b> <b>block,</b> the most surprising fact arising out of the dating {{is that the first}} three printings appeared within nine months of each other. It is not known whether a planned larger, single run was shortened and broken up for the corrections, or whether demand was simply strong enough to warrant new runs with corrections made opportunistically.|$|E
50|$|The website {{can combine}} the {{original}} and modified <b>text</b> <b>blocks</b> into one pane with all differences highlighted. Alternatively the marked-up original and modified <b>text</b> <b>blocks</b> can be displayed in individual panes.|$|R
40|$|This paper {{presents}} an efficient compound image compression approach based on H. 264 /AVC intra coding. The <b>text</b> <b>blocks</b> are {{distinguished from the}} picture blocks and compressed with a new coding mode. In particular, the <b>text</b> <b>blocks</b> are represented by base colors and index map in spatial domain. A color quantization algorithm optimized for compression is designed to generate this representation. As for the entropy coding of <b>text</b> <b>blocks,</b> a structure-aware context-based arithmetic coder is developed. The mode selection algorithm based on rate distortion optimization is used to select the <b>text</b> <b>blocks</b> along with H. 264 /AVC intra modes, which can adapt to the targeted bit-rate. Experimental {{results show that the}} proposed scheme can achieve 2. 8 dB gain on average for compound images compared with H. 264 /AVC intra coding. Index Terms — Compound image compression, color quantization, entropy coding, mode decision. 1...|$|R
40|$|BACKGROUND: The {{widespread}} use of gastrointestinal endoscopy for diagnosis and treatment requires effective, standardised report systems. This need is further increased by the limited storage of images, and by the need for structured databases for surveillance and epidemiology. We therefore aimed for a report system which would be quick, easy to learn, and suitable for use in busy daily practice. METHODS: Endobase III is an endoscopy information system offering three different ways of report writing, i. e. standard reports, <b>text</b> <b>blocks</b> and Minimal Standard Terminology (MST). A working group of two university and four general hospitals worked as a reference group {{for the development of}} standard reports and <b>text</b> <b>blocks.</b> Guidelines from various gastrointestinal endoscopy societies were followed to compose the reports. RESULTS: Standard reports were based on a list of distinct diagnoses; <b>text</b> <b>blocks</b> were based on anatomic landmarks and individual procedures. As such, 316 standard reports were developed for upper and lower gastrointestinal endoscopy, and endoscopic retrograde cholangiopancreatography (ERCP). In this way selecting one diagnosis produces a complete report. A total of 1571 different <b>text</b> <b>blocks</b> were additionally developed for each part of the gastrointestinal tract and for procedures during endoscopy. This module allowed generation of a full report on the combination of <b>text</b> <b>blocks.</b> Reports could be composed and printed within two minutes for 90 % of cases. CONCLUSION: Standard reports and <b>text</b> <b>blocks</b> are a quick, user-friendly way of report writing accepted and used by a number of gastroenterologists in the Netherland...|$|R
25|$|Corrections to the <b>text</b> <b>block</b> as {{described}} above account for three distinct printings. A strong {{case can be made}} for at least one more. Some books with third printing characteristics come bound in one of three tan book cloths that differ from those used in the other printings. The earlier bindings contain a yellowish-green pall that two of these later bindings lack completely. Another difference from earlier printings {{can be found in the}} endpaper maps. They are printed on stock that has not been calendered as smoothly as the earlier printings. Lastly, slight deterioration of the printing plates betrays their age. Type defects apparently unique to this last printing are found in the table below as broken A.|$|E
25|$|Examples {{have been}} found with the maps as free leaves, rather than pasted down. Invariably these have been a library edition. Most of those seen have been bound in orange cloth. The front cover mimics the first edition dust jacket, silk-screened in black. Because several {{identical}} examples of the library edition have been reported, and because the endpaper maps show no evidence of having been glued down originally, {{it would not have}} come about as ad hoc rebinds by libraries. Rather, a major bindery is assumed to have sourced the <b>text</b> <b>block</b> directly from the publisher. Not enough specimens have been examined to know if library bindings were supplied for all the printings. This library edition trimmed a full 5mm from the text block's outside margin and a combined 3mm vertically.|$|E
25|$|This last {{printing}} {{may have}} consisted of several printing runs or perhaps was contracted out to different printers. While no deliberate {{changes to the}} <b>text</b> <b>block</b> have been noted, the binding cloth and the text block's paper stock both changed at times, and flaws developed in the printing plates over time. One of the cloth variants {{is similar to the}} first printing's, but even yellower. Another shows an even weave with light, very even oatmeal coloring that can even seem faintly pink in some light. Its cloth shows deep, vivid blues in the lettering, whereas the others look obviously paler. Yet another variant carries a variegated linen-like weave wherein individual threads can stand out for several centimeters. This latter is as dark as the binding of the third printing (while lacking the yellow cast), but the coloring is so uneven that it might be mistaken as soiled even when clean. Soiling on the other variants, on the other hand, is immediately apparent.|$|E
25|$|In {{addition}} to the above <b>texts,</b> <b>block</b> books include some calendars and almanacs.|$|R
40|$|Abstract. This paper proposes an {{automatic}} method for extracting information from academic conference Web pages, and organizes these information as on-tologies, then matches these ontologies {{to the academic}} linked data. The main contributions include: (1) A page segmentation algorithm is proposed to divide conference Web pages into <b>text</b> <b>blocks.</b> (2) According to vision, key words and other text features, all <b>text</b> <b>blocks</b> are classified as 10 categories using bayes network model. The context information of <b>text</b> <b>blocks</b> are introduced to repair the initial classified results, which are improved to 96 % precision and 98 % re-call. (3) An ontology is generated for each conference website, then all ontolo-gies are matched as an academic linked data...|$|R
50|$|In Sanskrit {{and other}} Indian languages, <b>text</b> <b>blocks</b> {{used to be}} written in stanzas. Two bars || {{represented}} a pilcrow.|$|R
500|$|Each {{rectangular}} page has {{the proportions}} 1.1 to 1, while the block of text has the reciprocal proportions, 0.91 (the same proportions, rotated 90°). If the gutters between the columns were removed, the <b>text</b> <b>block</b> would mirror the page's proportions. Typographer Robert Bringhurst {{referred to the}} codex as a [...] "subtle piece of craftsmanship".|$|E
2500|$|... rough: The {{paper the}} maps are printed on is no {{smoother}} than the <b>text</b> <b>block.</b>|$|E
2500|$|Double-fan {{adhesive}} binding starts off with two signatures of loose pages, which are run over a roller—"fanning" [...] the pages—to apply {{a thin layer}} of glue to each page edge. Then the two signatures are perfectly aligned to form a <b>text</b> <b>block,</b> and glue edges of the <b>text</b> <b>block</b> are attached to a piece of cloth lining to form the spine. Double-fan adhesive bound books can open completely flat and have a wide margin. However, certain types of paper do not hold adhesive well, and, with wear and tear, the pages can come loose.|$|E
40|$|Abstract—In this paper, a {{generalized}} framework {{has been proposed}} to identify different Indian scripts with an observation that every script has a distinct visual appearance. Directional morphological transformations were employed to extract directional linear features of <b>text</b> <b>blocks.</b> Totally, 6460 <b>text</b> <b>blocks</b> of eleven Indian scripts of different scales were classified using K-nearest neighbor and Support Vector Machines (SVM). The results were quite encouraging. Index Terms — Script identification, document images and directional morphological transformations I...|$|R
50|$|In Sanskrit {{and other}} Indian languages, <b>text</b> <b>blocks</b> {{used to be}} written in stanzas. Two bars || {{represent}} {{the equivalent of a}} pilcrow.|$|R
40|$|Wavelet-based image coders {{generally}} {{perform well}} on natural images, which are typically characterized by slowly varying image intensities. Their performance suffers, however, on compound images containing both text and image data. We modify a quadtree wavelet coder {{to perform well}} on text image data by treating <b>text</b> <b>blocks</b> differently from non-text blocks. We combine wavelet domain processing of non-text blocks with spatial domain processing of <b>text</b> <b>blocks,</b> and achieve improved performance over purely wavelet domain techniques for compound images. 1...|$|R
2500|$|The Bible {{is bound}} in 2 volumes, with 244 {{leaves in the}} first and 214 in the second. It is likely that one {{preliminary}} and two end leaves are missing. [...] The binding is of plain pigskin over wooden boards and is more or less contemporary {{with the rest of the}} book. The <b>text</b> <b>block</b> is secured by nine cords with head and tail bands of red, white, and green silk.|$|E
2500|$|The textblock's {{paper stock}} for the [...] "oatmeal" [...] variant is {{brighter}} than the others. This is most apparent when comparing the <b>text</b> <b>block</b> edges of the three variants. It is known from owner inscriptions that this lighter variant appeared no later than 1946, and that the variegated linen appeared on the market no later than 1942. No dates have been seen on the yellower variant. While not enough dates have been seen to be truly useful, the relative order of the printing of the sheets associated with each binding can be discerned from the type flaws described in the table below.|$|E
5000|$|<b>Text</b> <b>block</b> {{correspondence}} (documents created {{based on}} pre-defined text blocks) ...|$|E
50|$|Rice wrote two {{teaching}} <b>texts,</b> <b>Block</b> Printing in the Schools (1929) and Block Prints: How To Make Them (1941), both {{published by}} Bruce Publishing Company.|$|R
40|$|AbstractWith {{the rapid}} {{increase}} of on-line video resources, {{there is an}} urgent demand for text detection and recognition technologies to build content-based video indexing and retrieval systems. Chinese news video texts contain highly condensed and rich information, but the low resolution of videos on the Internet and the complexity of Chinese character structures bring challenges for text detection. In this paper, we present a multi-stage scheme for Chinese news video text detection. We propose an improved Stroke Width Transform (SWT) method by incorporating text color consistency constraint for candidate <b>text</b> <b>blocks</b> generation. Then we use “divide and conquer” strategy to distinguish candidate <b>text</b> <b>blocks</b> into three sub-spaces according to their geometric shapes and size. For each sub-space, a neural network is designed to filter the candidates into <b>text</b> or non-text <b>blocks.</b> Finally, the <b>text</b> <b>blocks</b> are merged into text lines based on the stroke width, color and other heuristic information. Experimental results on self-collected Chinese news video dataset and ICDAR 2013 dataset show that the proposed method is effective to detect both news video captions and scene texts...|$|R
40|$|This paper {{objective}} {{is to improve the}} current method for generating an Arabic Calligraphy <b>text</b> <b>blocks.</b> We test on seven types of Arabic Calligraphy text. We apply  projection profiles and a proposed filter to discriminate each line of the Arabic Calligraphy scripts. After performing text detection, skew correction, text and line normalization subsequently, we generate Arabic Calligraphy <b>text</b> <b>blocks</b> for global texture analysis purposes. We compare our proposed filter with current method and median filter. The results show that the proposed filter  is outperformed. The proposed method can be further  improved to boost the overall performance...|$|R
5000|$|Oversewn {{bindings}} are {{a type of}} bookbinding {{produced by}} sewing together loose leaves of paper to form a <b>text</b> <b>block.</b> Threads pass through small holes that have been punched in the signature's gutter margin (nearest the spine), forming overlock stitches that attach it to previously attached sections. [...] This method of stitching is sometimes called stab sewing. A piece of linen is then glued to the <b>text</b> <b>block</b> spine for further support. The book's spine may be rounded and backed {{to keep it from}} caving in, but if the <b>text</b> <b>block</b> is too thick, the spine is sometimes left flat. A strip of cloth called a super is then often affixed to the spine of the <b>text</b> <b>block</b> and then to the boards of the case. Oversewing can be done by hand but is usually done with a machine in a bindery.|$|E
5000|$|PNGinside an [...] "iTXt" [...] <b>text</b> <b>block</b> {{with the}} keyword [...] "XML:com.adobe.xmp" ...|$|E
5000|$|... rough: The {{paper the}} maps are printed on is no {{smoother}} than the <b>text</b> <b>block.</b>|$|E
30|$|All the {{interviews}} that were recorded were subsequently transcribed. The researchers who had partaken {{in the interview}} wrote a detailed report based on the notes taken. Each partner carried out the first reduction of the data (Miles and Huberman 1994). This was achieved by picking out key <b>text</b> <b>blocks</b> from the transcribed interviews and interview notes. The <b>text</b> <b>blocks</b> could be a citation or interpretation and {{a summary of the}} reply. The <b>text</b> <b>blocks</b> were then put into a table similar to Table  4, which was shared between the partners. The authors, aiming at finding patterns (similarities and differences) with regard to the research questions across the networks, analysed the <b>text</b> <b>blocks</b> in the table. This is a “pattern matching” method of analysis (Yin 2009). While this work led to a new reduction of the material, at the same time, it also led to an increase, as it could be seen that other questions than those included in Table  4 could have answers that were relevant to the research questions. In addition, directly after {{the interviews}}, the researchers reflected upon the received answers and what had been learnt from the interviews, singling out similarities and peculiarities. Notes made on these reflections were used as an additional source when analysing the data. The collected material, analyses of the interviews and the researchers’ reflections were discussed in workshops where all the involved researchers in the RECIN project took part.|$|R
40|$|In this paper, {{we propose}} a {{gradient}} difference based approach to text localization in videos and scene images. The input video frame/ image is first compressed using multilevel 2 -D wavelet transform. The edge information of the reconstructed image is found which is further used for finding the maximum gradient {{difference between the}} pixels and then {{the boundaries of the}} detected <b>text</b> <b>blocks</b> are computed using zero crossing technique. We perform logical AND operation of the <b>text</b> <b>blocks</b> obtained by gradient difference and the zero crossing technique followed by connected component analysis to eliminate the false positives. Finally, the morphological dilation operation is employed on the detected <b>text</b> <b>blocks</b> for scene <b>text</b> localization. The experimental results obtained on publicly available standard datasets illustrate that the proposed method can detect and localize the texts of various sizes, fonts and colors. Comment: 11 pages, Second International Conference on Emerging Research in Computing, Information, Communications and Applications, Elsevier Publications, ISBN: 9789351072638, vol. III, pp: 299 - 308, held at NMIT, Bangalore August 201...|$|R
40|$|In this paper, {{we propose}} {{a new system}} for text {{information}} extraction from news videos. First of all, a method that integrates text detecting and text tracking is developed to locate text areas in the key-frames (images), together with a scheme to evaluate the performance of this approach. To get better recognition results, we then {{enhance the quality of}} the detected <b>text</b> <b>blocks</b> by multi-frame averaging. Finally, we use an adaptive thresholding method to binarize the <b>text</b> <b>blocks</b> and recognize the text using an off-the-shelf OCR module. The detection and recognition rate of the proposed system are 94. 7 % and 67. 5 % respectively. 1...|$|R
5000|$|Finally, one can {{calculate}} a bounding box for each <b>text</b> <b>block,</b> and the document layout analysis is complete.|$|E
50|$|Gauze used in {{bookbinding}} {{is called}} mull, {{and is used}} in case binding to adhere the <b>text</b> <b>block</b> to the book cover.|$|E
5000|$|<b>Text</b> <b>block</b> law: Linguistic units (e.g. words, letters, {{syntactic}} {{functions and}} constructions) show a specific frequency distribution in equally large text blocks.|$|E
5000|$|The [...] "MediaWiki:" [...] {{namespace}} {{was also}} originally used for creating custom <b>text</b> <b>blocks</b> that {{could then be}} dynamically loaded into other pages using a special syntax. This content was later moved into its own namespace, [...] "Template:".|$|R
5000|$|When Bob {{receives}} the message, he will first decrypt the message by reversing the encryption process which Alice applied, using the cipher <b>text</b> <b>blocks</b> [...] The tampered message, delivered to Bob in replacement of Alice's original, is [...]|$|R
40|$|ABSTRACT. In {{order to}} index Web images, the whole {{associated}} texts are partitioned into {{a sequence of}} <b>text</b> <b>blocks,</b> then the local relevance of a term to the corresponding image is calculated with respect to both its local occurrence in the block and {{the distance of the}} block to the image. Thus, the overall relevance of a term is determined as the sum of all its local weight values multiplied by the corresponding distance factors of the <b>text</b> <b>blocks.</b> In the present approach, the associated text of a Web image is firstly partitioned into three parts, including a page oriented text (TM), a link oriented text (LT) and a caption oriented text (BT). Since the big size and semantic divergence, the caption oriented text is further partitioned into finer blocks based on the tree structure of the tag elements within the BT text. During the processing, all heading nodes are pulled up in order to correlate with their semantic scopes, and a collapse algorithm is also exploited to remove the empty blocks. In our system, the relevant factors of the <b>text</b> <b>blocks</b> are determined by using a greedy Two-Way-Merging algorithm...|$|R
