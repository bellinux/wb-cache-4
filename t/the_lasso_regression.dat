16|2978|Public
40|$|Order {{selection}} of linear regression models has been thoroughly researched in the statistical community for some time. Different shrinkage {{methods have been}} proposed, such as the Ridge and Lasso regression methods. Especially <b>the</b> <b>Lasso</b> <b>regression</b> has won fame because {{of its ability to}} set less important parameters exactly to zero. However, these methods do not take dynamical systems into account, where the regressors are ordered via the time lag. To this end, a modified variant of the nonnegative garrote method will be analyzed...|$|E
40|$|Interactions between cancer {{drugs and}} dietary {{supplements}} are clinically important {{and have not}} been extensively investigated through mining of the biomedical literature. We report on a previously introduced method now enhanced by machine learning-based filtering. Potential interactions are extracted by using relationships {{in the form of}} semantic predications. Semantic predications stored in SemMedDB, a database of structured knowledge generated from MEDLINE, were filtered and connected by two interaction pathways to explore potential drug-supplement interactions (DSIs). <b>The</b> <b>lasso</b> <b>regression</b> filter was trained by using SemRep output features in an expert annotated corpus and used to rank retrieved predications by predicted precision. We found not only known interactions but also inferred several unknown potential DSIs by appropriate filtering and linking of semantic predications...|$|E
40|$|Abstract In this chapter, {{we study}} {{different}} gene regulatory network learning methods based on penalized linear regressions (<b>the</b> <b>Lasso</b> <b>regression</b> and the Dantzig Selector), Bayesian networks, and random forests. We also replicated the learning scheme using bootstrapped sub-samples of the observations. The biological motiva-tion {{relies on a}} tough nut to crack in Systems Biology: understanding the intertwined action of genome elements and gene activity to model gene regulatory features of an organism. We introduce the used methodologies, and then assess the methods on simulated “Systems Genetics ” (or genetical genomics) datasets. Our results show that methods have very different performances depending on tested simulation set-tings: total number of genes in the considered network, sample size, gene expression heritability, and chromosome length. We observe that the proposed approaches ar...|$|E
40|$|Inspired by {{the success}} of <b>the</b> <b>Lasso</b> for <b>regression</b> analysis, it seems {{attractive}} to estimate the graph of a multivariate normal distribution by l 1 -norm penalized likelihood maximization. We examine some properties of the estimator and show that care has to be taken with interpretation of results as the estimator is not consistent for some graphs. ...|$|R
40|$|In the U. S. Forest Service’s Forest Inventory and Analysis (FIA) program, as {{in other}} natural {{resource}} surveys, many auxiliary variables are available for use in model-assisted inference about finite population parameters. Some of this auxiliary information may be extraneous, and therefore model selection is appropriate to improve {{the efficiency of the}} survey regression estimators of finite population totals. A model-assisted survey <b>regression</b> estimator using <b>the</b> <b>lasso</b> is presented and extended to <b>the</b> adaptive <b>lasso.</b> For a sequence of finite populations and probability sampling designs, asymptotic properties of <b>the</b> <b>lasso</b> survey <b>regression</b> estimator are derived, including design consistency and central limit theory for the estimator and design consistency of a variance estimator. To estimate multiple finite population quantities with <b>the</b> method, <b>lasso</b> survey <b>regression</b> weights are developed, using both a model calibration approach and a ridge regression approximation. The gains in efficiency of <b>the</b> <b>lasso</b> estimator over <b>the</b> full regression estimator are demonstrated through a simulation study estimating tree canopy cover for a region in Utah...|$|R
40|$|Inspired by {{the success}} of <b>the</b> <b>Lasso</b> for <b>regression</b> {{analysis}} (Tibshirani, 1996), it seems attractive to estimate the graph of a multivariate normal distribution by ℓ 1 -norm penalised likelihood maximisation. The objective function is convex and the graph estimator can thus be computed efficiently, even for very large graphs. However, we show in this note that the resulting estimator is not consistent for some graphs. ...|$|R
40|$|There is {{a demand}} for {{decision}} support tools that can model the electricity markets and allows to forecast the hourly electricity price. Many different ap- proach such as artificial neural network or support vector regression {{are used in the}} literature. This thesis provides comparison of several different estima- tors under one settings using available data from Czech electricity market. The resulting comparison of over 5000 different estimators led to a selection of several best performing models. The role of historical weather data (temper- ature, dew point and humidity) is also assesed within the comparison and it was found that while the inclusion of weather data might lead to overfitting, it is beneficial under the right circumstances. The best performing approach was <b>the</b> <b>Lasso</b> <b>regression</b> estimated using modified Lars. ...|$|E
40|$|Background: Gene {{expression}} is regulated via highly coordinated epigenetic changes, the most studied {{of which is}} DNA methylation (DNAm). Many {{studies have shown that}} DNAm is linearly associated with age, and some have even used DNAm data to build predictive models of human age, which are immensely important considering that DNAm can predict health outcomes, such as all-cause mortality, better than chronological age. Nevertheless, few studies have investigated non-linear relationships between DNAm and age, which could potentially improve these predictive models. While such investigations are relevant to predicting health outcomes, non-linear relationships between DNAm and age can also add to our understanding of biological responses to late-life events, such as diseases that afflict the elderly. Objectives: We aim to (1) examine non-linear relationships between DNAm and age at specific loci on the genome and (2) build upon regularization methods by comparing prediction errors between models with both non-transformed and square-root transformed predictors to models that include only non-transformed predictors. We used both the sparse partial least squares (SPLS) regression model and <b>the</b> <b>lasso</b> <b>regression</b> model to make our comparisons. Results: We found two age-differentially methylated sites implicated in the regulation of a gene known as KLF 14, which could be involved in an immunosenescent phenotype. Inclusion of the square-root transformed variables had little effect on the prediction error of the SPLS model. On the other hand, the prediction error increased substantially in <b>the</b> <b>lasso</b> <b>regression</b> model, particularly when few predictors (70) were included. Conclusion: The growing amount and complexity of biological data coupled with advances in computational technology are indispensable to our understanding of biological pathways and perplexing biological phenomena. Moreover, high-dimensional biological data have enormous implications for clinical practice. Our findings implicate a possible biological pathway involved in immunosenescence. While we were unable to improve the predictive models of human age, future research should investigate other possible non-linear relationships between DNAm and human age, considering that such statistical methods can improve predictions of health outcomes...|$|E
40|$|Neutron {{spectrum}} unfolding is used {{to adjust}} a known distribution in a reactor that is known to contain uncertainty. This known neutron spectrum is commonly determined in high resolution using transport codes such as MCNP. Measured reaction probabilities can be obtained through neutron activation analysis {{and can be used}} to adjust the simulated neutron spectrum. A linear system is defined that represents a relationship between the observed reaction probabilities and the calculated reaction probabilities that are a result of the simulated spectrum, and can be solved for an adjustment. The purpose of this investigation is to create a linear model with a simulated neutron spectrum and measured reaction probabilities, and to solve the model using <b>the</b> <b>Lasso</b> <b>regression</b> {{in the form of an}} elastic net regression. It uses benchmarks provided by the IAEA REAL- 201 X program to adjust the spectrum of the Annual Core Research Reactor in varying configurations to complete a proof of concept...|$|E
40|$|Abstract — Computational {{prediction}} of cis-regulatory elements {{for a set}} of co-expressed genes based on sequence analysis provides an overwhelming volume of potential transcription factor binding sites. It presents a challenge to prioritize transcription factors for regulatory functional studies. A novel approach based on <b>the</b> use of <b>Lasso</b> <b>regression</b> models is proposed to address this problem. We examine the ability of <b>the</b> <b>Lasso</b> model using time-course microarray data obtained from a comprehensive study of gene expression profiles in skin and mucosal wounds in mouse over all stages of wound healing. I...|$|R
40|$|This paper {{presents}} an efficient algorithm {{based on the}} combination of Newton Raphson and Gradient Ascent, for using <b>the</b> fused <b>lasso</b> <b>regression</b> method to construct a genome-based classifier. The characteristic structure of copy number data suggests that feature selection should take genomic location into account for producing more interpretable results for genome-based classifiers. <b>The</b> fused <b>lasso</b> penalty, an extension of <b>the</b> <b>lasso</b> penalty, encourages sparsity of the coefficients and their differences by penalizing the L 1 -norm {{for both of them}} at the same time, thus using genomic location. The major advantage of the algorithm over other existing fused lasso optimization techniques is its ability to predict binomial as well as survival response efficiently. We apply our algorithm to two publicly available datasets in order to predict survival and binary outcomes...|$|R
5000|$|... "Glmnet: Lasso and elastic-net regularized {{generalized}} linear models" [...] is software {{which is}} implemented as an R source package. This includes fast algorithms for estimation of generalized linear models with ℓ1 (<b>the</b> <b>lasso),</b> ℓ2 (ridge <b>regression)</b> and mixtures {{of the two}} penalties (the elastic net) using cyclical coordinate descent, computed along a regularization path.|$|R
40|$|Building and {{discovering}} useful features when constructing machine learning models {{is the central}} task for the machine learning practitioner. Good features are useful not only in increasing the predictive power of a model but also in illuminating the underlying drivers of a target variable. In this research we propose a novel feature learning technique in which Symbolic regression is endowed with a "Range Terminal 2 ̆ 72 ̆ 7 that allows it to explore functions of the aggregate of variables over time. We test the Range Terminal on a synthetic data set and a real world data in which we predict seasonal greenness using satellite derived temperature and snow data over {{a portion of the}} Arctic. On the synthetic data set we find Symbolic regression with the Range Terminal outperforms standard Symbolic regression and Lasso regression. On the Arctic data set we find it outperforms standard Symbolic regression, fails to beat <b>the</b> <b>Lasso</b> <b>regression,</b> but finds useful features describing the interaction between Land Surface Temperature, Snow, and seasonal vegetative growth in the Arctic...|$|E
40|$|We {{consider}} the least angle regression and forward stagewise algorithms for solving penalized least squares regression problems. In Efron, Hastie, Johnstone & Tibshirani (2004) it is {{proved that the}} least angle regression algorithm, with a small modification, solves <b>the</b> <b>lasso</b> <b>regression</b> problem. Here we give an analogous result for incremental forward stagewise regression, showing that it solves {{a version of the}} lasso problem that enforces monotonicity. One consequence of this is as follows: while lasso makes optimal progress in terms of reducing the residual sum-of-squares per unit increase in L_ 1 -norm of the coefficient β, forward stage-wise is optimal per unit L_ 1 arc-length traveled along the coefficient path. We also study a condition under which the coefficient paths of the lasso are monotone, and hence the different algorithms coincide. Finally, we compare the lasso and forward stagewise procedures in a simulation study involving a large number of correlated predictors. Comment: Published at [URL] in the Electronic Journal of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|Predictive {{modelling}} is a data-analysis task {{common in}} many scientific fields. However, {{it is rather}} unknown that multiple predictive models can be equally well-performing for the same problem. This multiplicity often leads to poor reproducibility when searching for a unique solution in datasets with low number of samples, high dimensional feature space and/or high levels of noise, a common scenario in biology and medicine. <b>The</b> <b>Lasso</b> <b>regression</b> {{is one of the}} most powerful and popular regularization methods, yet it also produces a single, sparse solution. In this paper, we show that nearly-optimal Lasso solutions, whose out-of-sample statistical error is practically indistinguishable from the optimal one, exist. We formalize various notions of equivalence between Lasso solutions, and we devise an algorithm to enumerate the ones that are equivalent in a statistical sense: we define a tolerance on the root mean square error (RMSE) which creates a RMSE-equivalent Lasso solution space. Results in both regression and classification tasks reveal that the out-of-sample error due to the RMSE relaxation is within the range of the statistical error due to the sampling size. Comment: 10 pages, 3 figure...|$|E
40|$|<b>Lasso</b> <b>regression</b> {{tends to}} assign zero weights to most {{irrelevant}} or redundant features, and hence is a promising technique for feature selection. Its limitation, however, {{is that it}} only offers solutions to linear models. Kernel machines with feature scaling techniques have been studied for feature selection with non-linear models. However, such approaches require to solve hard non-convex optimization problems. This paper proposes a new approach named the Feature Vector Machine (FVM). It reformulates <b>the</b> standard <b>Lasso</b> <b>regression</b> into a form isomorphic to SVM, and this form can be easily extended for feature selection with non-linear models by introducing kernels defined on feature vectors. FVM generates sparse solutions in the nonlinear feature space and {{it is much more}} tractable compared to feature scaling kernel machines. Our experiments with FVM on simulated data show encouraging results in identifying the small number of dominating features that are non-linearly correlated to the response, a task <b>the</b> standard <b>Lasso</b> fails to complete. ...|$|R
40|$|International audienceThe aim of {{this paper}} is to provide a {{comprehensive}} introduction for the study of L 1 -penalized estimators in the context of dependent observations. We define a general ℓ_ 1 -penalized estimator for solving problems of stochastic optimization. This estimator turns out to be <b>the</b> <b>LASSO</b> in <b>the</b> <b>regression</b> estimation setting. Powerful theoretical guarantees on the statistical performances of <b>the</b> <b>LASSO</b> were provided in recent papers, however, they usually only deal with the iid case. Here, we study our estimator under various dependence assumptions...|$|R
40|$|<b>The</b> <b>LASSO</b> sparse <b>regression</b> {{method has}} {{recently}} received attention {{in a variety}} of applications from image compression techniques to parameter estimation problems. This paper addresses the problem of regularization parameter selection in this method in a general case of complex-valued regressors and bases. Generally, this parameter controls the degree of sparsity or equivalently, the estimated model order. However, with the same sparsity/model order, the smallest regularization parameter is desired. We relate such points to the nonsmooth points in <b>the</b> path of <b>LASSO</b> solutions and give an analytical expression for them. Then, we introduce a numerically fast method of approximating the desired points by a recursive algorithm. The procedure decreases the necessary number of solutions of <b>the</b> <b>LASSO</b> problem dramatically, which is an important issue due to the polynomial computational cost of the convex optimization techniques. We illustrate our method in the context of DOA estimation...|$|R
40|$|<b>The</b> <b>LASSO</b> <b>regression</b> {{has been}} studied {{extensively}} in the statistics and signal processing community, especially {{in the realm of}} sparse parameter estimation from linear measurements. We analyze the convergence rate of a first-order method applied on a smooth, strictly convex, and parametric upper bound on the LASSO objective function. The upper bound approaches the true non-smooth objective as the parameter tends to infinity. We show that a gradient-based algorithm, applied to minimize the smooth upper bound, yields a convergence rate of O (1 /K), where K denotes the number of iterations performed. The analysis also reveals the optimum value of the parameter that achieves a desired prediction accuracy, provided that the total number of iterations is decided a priori. The convergence rate of the proposed algorithm and the amount of computation required in each iteration are same as that of the iterative soft thresholding technique. However, the proposed algorithm does not involve any thresholding operation. The performance of the proposed technique, referred to as smoothed LASSO, is validated on synthesized signals. We also deploy smoothed LASSO for estimating an image from its blurred and noisy measurement, and compare the performance with the fast iterative shrinkage thresholding algorithm for a fixed run-time budget, in terms of the reconstruction peak signal-to-noise ratio and structural similarity index...|$|E
40|$|Study design: Psychometric study {{analyzing}} {{the data of}} a cross-sectional, multicentric study with 1048 persons {{with spinal cord injury}} (SCI). Objective: To shed light on how to apply the Brief Core Sets for SCI of the International Classification of Functioning, Disability and Health (ICF) by determining whether the ICF categories contained in the Core Sets capture differences in overall health. Methods: Lasso regression was applied using overall health, rated by the patients and health professionals, as dependent variables and the ICF categories of the Comprehensive ICF Core Sets for SCI as independent variables. Results: The ICF categories that best capture differences in overall health refer to areas of life such as self-care, relationships, economic self-sufficiency and community life. Only about 25 % of the ICF categories of the Brief ICF Core Sets for the early post-acute and for long-term contexts were selected in <b>the</b> <b>Lasso</b> <b>regression</b> and differentiate, therefore, among levels of overall health. Conclusion: ICF categories such as d 570 Looking after one's health, d 870 Economic self-sufficiency, d 620 Acquisition of goods and services and d 910 Community life, which capture changes in overall health in patients with SCI, should be considered in addition to those of the Brief ICF Core Sets in clinical and epidemiological studies in persons with SCI...|$|E
40|$|If {{customers}} respond {{differently to}} a campaign, {{it is worthwhile}} to find those customers who respond most positively and direct the campaign towards them. This {{can be done by}} using so called incremental response analysis where respondents from a campaign are compared with respondents from a control group. Customers with the highest increased response from the campaign will be selected and thus may increase the company’s return. Incremental response analysis is applied to the mobile operator Tres historical data. The thesis intends to investigate which method that best explain the incremental response, namely to find those customers who give the highest incremental response of Tres customers, and what characteristics that are important. The analysis is based on various classification methods such as logistic regression, Lassoregression and decision trees. RMSE which is the root mean square error of the deviation between observed and predicted incremental response, is used to measure the incremental response prediction error. The classification methods are evaluated by Hosmer-Lemeshow test and AUC (Area Under the Curve). Bayesian logistic regression is also used to examine the uncertainty in the parameter estimates. <b>The</b> <b>Lasso</b> <b>regression</b> performs best compared to the decision tree, the ordinary logistic regression and the Bayesian logistic regression seen to the predicted incremental response. Variables that significantly affect the incremental response according to Lasso regression are age and how long the customer had their subscription...|$|E
5000|$|The least {{absolute}} {{selection and}} shrinkage (LASSO) method is another popular choice. In <b>lasso</b> <b>regression,</b> <b>the</b> <b>lasso</b> penalty function [...] is the [...] norm, i.e.|$|R
40|$|Global COE Program Education-and-Research Hub for Mathematics-for-IndustryグローバルCOEプログラム「マス･フォア･インダストリ教育研究拠点」We {{consider}} <b>the</b> Bayesian <b>lasso</b> for <b>regression,</b> {{which is}} an L 1 penalized regression based on Bayesian approach. In Bayesian theory, a crucial issue is the specification of prior distributions for parameters, {{which leads to the}} selection of values of hyperparameters included in the prior distributions. In order to select the values of the hyperparameters, we introduce a model selection criterion by evaluating the Bayesian predictive distribution for <b>the</b> Bayesian <b>lasso.</b> Several numerical studies are presented to illustrate the effectiveness of our proposed modeling procedure...|$|R
40|$|<b>The</b> <b>lasso</b> is an {{important}} method for sparse, high-dimensional regression problems, with efficient algorithms available, {{a long history of}} practical success, and a large body of theoretical results supporting and explaining its performance. But even with the best available algorithms, finding <b>the</b> <b>lasso</b> solutions remains a computationally challenging task in cases where the number of covariates vastly exceeds the number of data points. Marginal regression, where each dependent variable is regressed separately on each covariate, offers a promising alternative in this case because the estimates can be computed roughly two orders faster than <b>the</b> <b>lasso</b> solutions. <b>The</b> question that remains is how the statistical performance of the method compares to that of <b>the</b> <b>lasso</b> in these cases. In this paper, we study the relative statistical performance of <b>the</b> <b>lasso</b> and marginal <b>regression</b> for sparse, high-dimensional regression problems. We consider the problem of learning which coefficients are non-zero. Our main results are as follows: (i) we compare the conditions under which <b>the</b> <b>lasso</b> and marginal <b>regression</b> guarantee exact recovery in the fixed design, noise free case; (ii) we establish conditions under which marginal regression provides exact recovery with high probability in the fixed design, noise free, random coefficients case; and (iii) we derive rates of convergence for both procedures, where performance is measured by the number of coefficients with incorrect sign, and characterize the regions in the parameter space recovery is and is not possible under this metric. In light of the computational advantages of marginal regression in very high dimensional problems, our theoretical and simulations results suggest that the procedure merits further study...|$|R
40|$|Finishing highway {{projects}} within {{budget is}} critical for state highway agencies (SHAs) because budget overruns can result in severe damage to their reputation and credibility. Cost overruns in highway projects have plagued public agencies globally. Hence, this research aims to develop a parametric cost estimation model for SHAs to forecast the completed project cost prior to project execution to take necessary measures to prevent cost escalation. Ordinary least square (OLS) regression has been a commonly used parametric estimation method in the literature. However, OLS regression has certain limitations. It, for instance, requires strict statistical assumptions. This paper proposes an alternative approach— least absolute shrinkage and selection operator (LASSO) —that has proved in other fields of research to be significantly better than the OLS method in many respects, including automatic feature selection, the ability to handle highly correlated data, ease of interpretability, and numerical stability of the model predictions. Another contribution {{to the body of}} knowledge is that this study simultaneously explores project-related variables with some economic factors that have not been used in previous research, but economic conditions are widely considered to be influential on highway construction costs. The data were separated into two groups: one for training the model and the other for validation purposes. Using the same dataset, both LASSO and OLS were used to build models, and then their performance was evaluated based on the mean absolute error, mean absolute percentage error, and root mean square error. The results showed that <b>the</b> <b>LASSO</b> <b>regression</b> model outperformed the OLS regression model based on the criteria...|$|E
40|$|The present article reports an {{improvement}} in the INSAT Multispectral Rainfall Algorithm which is currently operational in the Indian Meteorological Department (IMD). The proposed Modified-IMSRA (M-IMSRA) algorithm deviates from original IMSRA in two ways: first is by improvement in rain/no-rain area detection scheme using a Multi Index Rain Detection (MIRD) index; second {{is based on the}} climate region-wise correction through Least Absolute Shrinkage and Selection Operator (LASSO) models developed for each climate regions using rainfall (obtained based on first improvement) and static topographic variables extracted from Digital Elevation Model (DEM). The overall results indicate that the M-IMSRA is performing better than the IMSRA in all climatic regions when compared with the IMD gridded gauge data. However, the improvement is not uniform in all the regions. The inclusion of the MIRD index led to considerable improvement in M-IMSRA-based rainfall estimates mainly in the arid regions. Likewise, the results obtained after <b>the</b> <b>LASSO</b> <b>regression</b> corrections indicate that they are necessary only for the orographic regions where significant improvements are observed in the rainfall estimates. Finally, the inter-comparison of the simple hybrid M-IMSRA estimates with Tropical Rainfall Measuring Mission (TRMM) 3 B 42 V 7 and TRMM 3 B 42 -RT V 7 illustrates that the M-IMSRA performs nearly as well as even better (except in terms of Correlation Coefficient) than the complex multi-satellite-based rainfall estimates in all the climate regions of India. Considering the above results, {{it can be said that}} the performance of simple hybrid algorithms such as IMSRA can be improved to match the quality or even outperform complex multi-satellite rainfall estimates by incorporating appropriate corrections. (C) 2016 Elsevier Inc. All rights reserved...|$|E
40|$|The matrix {{factorization}} (MF) {{technique has}} been widely adopted for solving the rating prediction problem in recommender systems. The MF technique utilizes the latent factor model to obtain static user preferences (user latent vectors) and item characteristics (item latent vectors) based on historical rating data. However, {{in the real world}} user preferences are not static but full of dynamics. Though there are several previous works that addressed this time varying issue of user preferences, it seems (to the best of our knowledge) that none of them is specifically designed for tracking concept drift in individual user preferences. Motivated by this, we develop a Temporal Matrix Factorization approach (TMF) for tracking concept drift in each individual user latent vector. There are two key innovative steps in our approach: (i) we develop a modified stochastic gradient descent method to learn an individual user latent vector at each time step, and (ii) by <b>the</b> <b>Lasso</b> <b>regression</b> we learn a linear model for the transition of the individual user latent vectors. We test our method on a synthetic dataset and several real datasets. In comparison with the original MF, our experimental results show that our temporal method is able to achieve lower root mean square errors (RMSE) for both the synthetic and real datasets. One interesting finding is that the performance gain in RMSE is mostly from those users who indeed have concept drift in their user latent vectors at the time of prediction. In particular, for the synthetic dataset and the Ciao dataset, {{there are quite a few}} users with that property and the performance gains for these two datasets are roughly 20 % and 5 %, respectively...|$|E
40|$|We {{consider}} the generic regularized optimization problem ˆ β(λ) = arg minβ L(y, Xβ) + λJ(β). Recently, Efron et al. (2004) {{have shown that}} for <b>the</b> <b>Lasso</b> – that is, if L is squared error loss and J(β) = ‖β‖ 1 is the l 1 norm of β – the optimal coefficient path is piecewise linear, i. e., ∂ ˆ β(λ) /∂λ is piecewise constant. We derive a general characterization of the properties of (loss L, penalty J) pairs which give piecewise linear coefficient paths. Such pairs allow for efficient generation of the full regularized coefficient paths. We investigate the nature of efficient path following algorithms which arise. We use our results to suggest robust versions of <b>the</b> <b>Lasso</b> for <b>regression</b> and classification, and to develop new, efficient algorithms for existing problems in the literature, including Mammen & van de Geer’s Locally Adaptive Regression Splines. ...|$|R
40|$|This paper {{explores the}} {{following}} question: {{what kind of}} statistical guarantees can be given when doing variable selection in high-dimensional models? In particular, {{we look at the}} error rates and power of some multi-stage regression methods. In the first stage we fit a set of candidate models. In the second stage we select one model by cross-validation. In the third stage we use hypothesis testing to eliminate some variables. We refer to the first two stages as "screening" and the last stage as "cleaning. " We consider three screening methods: <b>the</b> <b>lasso,</b> marginal <b>regression,</b> and forward stepwise regression. Our method gives consistent variable selection under certain conditions. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|We {{consider}} the generic regularized optimization problem β̂(λ) =_βL(y,Xβ) +λ J(β). Efron, Hastie, Johnstone and Tibshirani [Ann. Statist. 32 (2004) 407 [...] 499] {{have shown that}} for <b>the</b> <b>LASSO</b> [...] that is, if L is squared error loss and J(β) =β_ 1 is the ℓ_ 1 norm of β [...] the optimal coefficient path is piecewise linear, that is, ∂β̂(λ) /∂λ is piecewise constant. We derive a general characterization of the properties of (loss L, penalty J) pairs which give piecewise linear coefficient paths. Such pairs allow for efficient generation of the full regularized coefficient paths. We investigate the nature of efficient path following algorithms which arise. We use our results to suggest robust versions of <b>the</b> <b>LASSO</b> for <b>regression</b> and classification, and to develop new, efficient algorithms for existing problems in the literature, including Mammen and van de Geer's locally adaptive regression splines. Comment: Published at [URL] in the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|Chronic {{subdural}} hematoma (CSDH) {{is characterized}} by an "old" encapsulated collection of blood and blood breakdown products between the brain and its outermost covering (the dura). Recognized risk factors for development of CSDH are head injury, old age and using anticoagulation medication, but its underlying pathophysiological processes are still unclear. It is assumed that a complex local process of interrelated mechanisms including inflammation, neomembrane formation, angiogenesis and fibrinolysis {{could be related to}} its development and propagation. However, the association between the biomarkers of inflammation and angiogenesis, and the clinical and radiological characteristics of CSDH patients, need further investigation. The high number of biomarkers compared to the number of observations, the correlation between biomarkers, missing data and skewed distributions may limit the usefulness of classical statistical methods. We therefore explored lasso regression to assess the association between 30 biomarkers of inflammation and angiogenesis at the site of lesions, and selected clinical and radiological characteristics in a cohort of 93 patients. Lasso regression performs both variable selection and regularization to improve the predictive accuracy and interpretability of the statistical model. The results from <b>the</b> <b>lasso</b> <b>regression</b> showed analysis exhibited lack of robust statistical association between the biomarkers in hematoma fluid with age, gender, brain infarct, neurological deficiencies and volume of hematoma. However, there were associations between several of the biomarkers with postoperative recurrence requiring reoperation. The statistical analysis with lasso regression supported previous findings that the immunological characteristics of CSDH are local. The relationship between biomarkers, the radiological appearance of lesions and recurrence requiring reoperation have been inclusive using classical statistical methods on these data, but lasso regression revealed an association with inflammatory and angiogenic biomarkers in hematoma fluid. We thus suggest that lasso regression should be a recommended statistical method in research on biological processes in CSDH patients...|$|E
40|$|Regularization aims {{to improve}} {{prediction}} performance by trading {{an increase in}} training error for better agreement between training and prediction errors, which is often captured through decreased degrees of freedom. In this paper we give examples which show that regularization can increase the degrees of freedom in common models, including <b>the</b> <b>lasso</b> and ridge <b>regression.</b> In such situations, both training error and degrees of freedom increase, making the regularization inherently without merit. Two important scenarios are described where the expected reduction in degrees of freedom is guaranteed: all symmetric linear smoothers and convex constrained linear regression models like ridge <b>regression</b> and <b>the</b> <b>lasso,</b> when compared to unconstrained linear regression...|$|R
40|$|Recently, {{variable}} selection by penalized likelihood has attracted much research interest. In this paper, we propose adaptive <b>Lasso</b> quantile <b>regression</b> (BALQR) from a Bayesian perspective. The method extends <b>the</b> Bayesian <b>Lasso</b> quantile <b>regression</b> by allowing different penalization parameters for different regression coefficients. Inverse gamma prior distributions {{are placed on}} the penalty parameters. We treat the hyperparameters of the inverse gamma prior as unknowns and estimate them {{along with the other}} parameters. A Gibbs sampler is developed to simulate the parameters from the posterior distributions. Through simulation studies and analysis of a prostate cancer data set, we compare the performance of the BALQR method proposed with six existing Bayesian and non-Bayesian methods. The simulation studies and the prostate cancer data analysis indicate that the BALQR method performs well in comparision to the other approaches. Gibbs sampler, <b>Lasso,</b> Quantile <b>regression,</b> Skewed Laplace distribution. ...|$|R
40|$|God {{knows the}} last thing we need is another {{algorithm}} for the lasso” Stephen Boyd, Sept 28, 2010 This is not quite a talk about algorithms for <b>the</b> <b>lasso</b> – but ideas for speeding up existing algorithms. Also reveals interesting aspects of convex statistical problems. 3 Top 7 reasons why this Lasso/L 1 stuff may have gone too far 1. One of Tibshirani’s students just wrote a paper on the “Generalized adaptive doubly sparse grouped relaxed lasso” 2. One of Candes’s students just wrote a paper on the “Near-optimality of the generalized adaptive doubly sparse grouped relaxed lasso” 3. One of Donoho’s students just wrote a paper on “Higher criticism for near-optimality of the generalized adaptive doubly sparse grouped relaxed lasso” 4. Papers are now being rejected out-of-hand from Statistica Sinica if 5. There are now more Lasso algorithms than millionaires at FaceBook 6. Someone discovered a computational fact about <b>the</b> <b>lasso</b> that’s NOT an exercise in Boyd’s Convex Optimization book 7. The Bayesians are getting really pissed off! 4 <b>The</b> <b>Lasso</b> Usual <b>regression</b> setting N ×p matrix of predictors X, N-vector of outcomes y. ˆβ = argmin...|$|R
40|$|Kernel methods implicitly {{map data}} points from the input space to some feature space where even {{relatively}} simple algorithms such as linear methods can deliver very impressive performance. Of crucial importance though {{is the choice}} of the kernel function, which determines the mapping between the input space and the feature space. The past few years have seen many efforts in learning either the kernel function or the kernel matrix. In this paper, we study the problem of learning the kernel hyperparameter in the context of <b>the</b> kernelized <b>LASSO</b> <b>regression</b> model. Specifically, we propose a solution path algorithm with respect to the hyperparameter of the kernel function. As the kernel hyperparameter changes its value, the solution path can be traced exactly without having to train the model multiple times. As a result, the optimal solution can be identified efficiently. Some simulation results will be presented to demonstrate the effectiveness of our proposed kernel path algorithm...|$|R
40|$|<b>The</b> <b>lasso</b> {{estimate}} for linear regression {{corresponds to a}} posterior mode when independent, double-exponential prior distributions are placed on the regression coefficients. This paper introduces new aspects of the broader Bayesian treatment of <b>lasso</b> <b>regression.</b> A direct characterization of the regression coefficients' posterior distribution is provided, and computation and inference under this characterization is shown to be straightforward. Emphasis is placed on point estimation using the posterior mean, which facilitates prediction of future observations via the posterior predictive distribution. It is shown that <b>the</b> standard <b>lasso</b> prediction method does not necessarily agree with model-based, Bayesian predictions. A new Gibbs sampler for Bayesian <b>lasso</b> <b>regression</b> is introduced. Copyright 2009, Oxford University Press. ...|$|R
