68|95|Public
2500|$|Compare {{each new}} string with the <b>target</b> <b>string</b> [...] "METHINKS IT IS LIKE A WEASEL", and give each a score (the number of {{letters in the}} string that are correct and in the correct position).|$|E
5000|$|... or [...] - [...] Perform {{the match}} {{more than once}} on a given <b>target</b> <b>string.</b>|$|E
5000|$|Compare {{each new}} string with the <b>target</b> <b>string</b> [...] "METHINKS IT IS LIKE A WEASEL", and give each a score (the number of {{letters in the}} string that are correct and in the correct position).|$|E
50|$|A code is a prefix code if no <b>target</b> bit <b>string</b> in {{the mapping}} is a prefix of the <b>target</b> bit <b>string</b> of a {{different}} source symbol in the same mapping. This means that symbols can be decoded instantaneously after their entire codeword is received. Other commonly used names for this concept are prefix-free code, instantaneous code, or context-free code.|$|R
40|$|This paper {{describes}} a novel model using dependency structures on the source side for syntax-based statistical machine transla-tion: Dependency Treelet String Correspon-dence Model (DTSC). The DTSC model maps source dependency structures to tar-get strings. In this model translation pairs of source treelets and <b>target</b> <b>strings</b> with their word alignments are learned automatically from the parsed and aligned corpus. The DTSC model allows source treelets and tar-get strings with variables {{so that the}} model can generalize to handle dependency struc-tures with the same head word but with dif-ferent modifiers and arguments. Addition-ally, <b>target</b> <b>strings</b> can be also discontinuous by using gaps which are corresponding to the uncovered nodes which {{are not included in}} the source treelets. A chart-style decod-ing algorithm with two basic operations– substituting and attaching–is designed for the DTSC model. We argue that the DTSC model proposed here is capable of lexical-ization, generalization, and handling discon-tinuous phrases which are very desirable for machine translation. We finally evaluate our current implementation of a simplified ver-sion of DTSC for statistical machine trans-lation. ...|$|R
40|$|The {{design of}} a CACC system and {{corresponding}} experiments are presented. The design <b>targets</b> <b>string</b> stable system behavior, which is assessed using a frequency-domain-based approach. Following this approach, it is shown that the available wireless information enables small inter-vehicle distances, while maintaining string stable behavior. The theoretical results are validated by experiments with two CACC-equipped vehicles. Measurement results showing string stable as well as string unstable behavior are discussed. © 2010 AACC...|$|R
5000|$|... and [...] Strings can be freely intermixed when {{manipulating}} strings; the compiler will do silent conversion when required. Note {{that if the}} <b>target</b> <b>string</b> type is , silent truncation {{might happen}} due to the maximum length allowed.|$|E
5000|$|A regex pattern matches a <b>target</b> <b>string.</b> The {{pattern is}} {{composed}} of a sequence of atoms. An atom is a single point within the regex pattern which it tries to match to the <b>target</b> <b>string.</b> The simplest atom is a literal, but grouping parts of the pattern to match an atom will require using [...] as metacharacters. Metacharacters help form: atoms; quantifiers telling how many atoms (and {{whether it is a}} greedy quantifier or not); a logical OR character, which offers a set of alternatives, and a logical NOT character, which negates an atom's existence; and backreferences to refer to previous atoms of a completing pattern of atoms. A match is made, not when all the atoms of the string are matched, but rather when all the pattern atoms in the regex have matched. The idea is to make a small pattern of characters stand for a large number of possible strings, rather than compiling a large list of all the literal possibilities.|$|E
5000|$|Folding is {{controlled}} by the [...] "all" [...] command. It permits one to display and work on only those lines in a file that contain a given pattern. For example, the command: [...] will display only the lines that include [...] "string"; any global changes one makes on this slice (for example replace string command) will be reflected in the file. (In most cases this is a more convenient way to make global changes in the file.) In order to restore visibility of all lines one needs to enter: all (without a <b>target</b> <b>string).</b>|$|E
40|$|By {{following}} the guidelines set {{in one of}} our previous papers, in this paper we face the problem of Kolmogorov complexity estimate for binary strings by making use of a Genetic Programming approach. This consists in evolving a population of Lisp programs looking for the "optimal" program that generates a given string. By taking into account several <b>target</b> binary <b>strings</b> belonging to different formal languages, we show the effectiveness of our approach in obtaining an approximation from the above of the Kolmogorov complexity function. Moreover, the adequate choice of "similar" <b>target</b> <b>strings</b> allows our system to show very interesting computational strategies. Experimental results indicate that our tool achieves promising compression rates for binary strings belonging to formal languages. Furthermore, even for more complicated strings our method can work, provided that some degree of loss is accepted. These results constitute a first step in using Kolmogorov complexit [...] ...|$|R
40|$|This paper {{shows the}} common {{framework}} that underlies the translation systems based on phrases or driven by finite state transducers, and summarizes a first comparison between them. In both approaches the translation process {{is based on}} pairs of source and <b>target</b> <b>strings</b> of words (segments) related by word alignment. Their main difference comes from the statistical modeling of the translation context. The experimental study {{has been carried out}} on an English/Spanish version of the VERB-MOBIL corpus. Under the constrain of a monotone composition of translated segments to generate the target sentence, the finite state based translation outperforms the phrase based counterpart. 1...|$|R
40|$|This paper {{describes}} DFKI’s {{participation in}} the NEWS 2011 shared task on machine transliteration. Our primary system participated in the evaluation for English-Chinese and Chinese-English language pairs. We extended the joint sourcechannel model on the transliteration task into a multi-to-multi joint source-channel model, which allows alignments between substrings of arbitrary lengths in both source and <b>target</b> <b>strings.</b> When the model is integrated into a modified phrasebased statistical machine translation system, around 20 % of improvement is observed. The primary system achieved 0. 320 on English-Chinese and 0. 133 on Chinese-English in terms of top- 1 accuracy. ...|$|R
5000|$|The {{problem of}} {{modeling}} the probability distribution [...] has been approached {{in a number}} of ways. One approach which lends itself well to computer implementation is to apply Bayes Theorem, that is , where the translation model [...] is the probability that the source string is the translation of the <b>target</b> <b>string,</b> and the language model [...] is the probability of seeing that target language string. This decomposition is attractive as it splits the problem into two subproblems. Finding the best translation [...] is done by picking up the one that gives the highest probability: ...|$|E
3000|$|... [...]. Finally, the {{decision}} was made by comparing the Hamming distance between the query and the <b>target</b> <b>string.</b>|$|E
40|$|Given {{a source}} string u and a <b>target</b> <b>string</b> w, {{to decide whether}} w can be {{obtained}} by applying a string morphism on u (i. e., uniformly replacing the symbols in u by strings) constitutes an NP-complete problem. For example, the <b>target</b> <b>string</b> w := baaba can be obtained from the source string u := aba, by replacing a and b in u by the strings ba and a, respectively. In this paper, we contribute to the recently started investigation of the computational complexity of the string morphism problem by studying it in the framework of parameterised complexity...|$|E
40|$|A major {{challenge}} in {{statistical machine translation}} is mitigating the word order differences between source and <b>target</b> <b>strings.</b> While reordering and lexical translation choices are often conducted in tandem, source string permutation prior to translation is attractive for studying reordering using hierarchical and syntactic structure. This work contributes an approach for learning source string permutation via transfer of the source syntax tree. We present a novel discriminative, probabilistic tree transduction model, and contribute a set of empirical upperbounds on translation performance for Englishto-Dutch source string permutation under sequence and parse tree constraints. Finally, the translation performance of our learning model is shown to outperform the state-of-the-art phrase-based system significantly. ...|$|R
40|$|Abstract. Although {{evolutionary}} algorithms (EAs) {{are often}} successfully {{used for the}} optimization of dynamically changing objective function, there are only very few theoretical results for EAs in this scenario. In this paper we analyze the (1 + 1) EA for a dynamically changing OneMax, whose <b>target</b> bit <b>string</b> changes bitwise, i. e. possibly {{by more than one}} bit in a step. We compute the movement rate of the <b>target</b> bit <b>string</b> resulting in a polynomial expected first hitting time of the (1 + 1) EA asymptotically exactly. This strengthens a previous result, where the dynamically changing OneMax changed only at most one bit at a time. ...|$|R
40|$|According to the self-teaching {{hypothesis}} (Share, 1995), word-specific orthographic representations are acquired {{primarily as}} a result of the self-teaching opportunities provided by the phonological recoding of novel letter strings. This hypothesis was tested by asking normal second graders to read aloud short texts containing embedded pseudoword targets. Three days later, target spellings were correctly identified more often, named more quickly, and spelled more accurately than alternate homophonic spellings. Experiment 2 examined whether this rapid orthographic learning can be attributed to mere visual exposure to <b>target</b> <b>strings.</b> It was found that viewing the <b>target</b> letter <b>strings</b> under conditions designed to minimize phonological processing significantly attenuated ortho-graphic learning. Experiment 3 went on to show that this reduced orthographic learning was not attributable to alternative nonphonological factors (brief exposure durations or decontextualized presentation). The results of a fourth experiment suggested that the contribution of pure visual exposure to orthographic learning is marginal. It was con-cluded that phonological recoding is critical to the acquisition of word-specific ortho-graphic representations as proposed by the self-teaching hypothesis. © 1999 Academic Press Key Words: reading development; orthographic learning; phonology; self-teaching; children. An extensive research literature has linked individual differences in reading ability to basic phonological processing (speech perception, immediate, short-term and long-term memory for speech-based information) and to phonological awareness (awareness of the segmental nature of speech) (for reviews se...|$|R
40|$|We {{present a}} novel {{translation}} model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a <b>target</b> <b>string.</b> A TAT {{is capable of}} generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a <b>target</b> <b>string.</b> Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models...|$|E
40|$|Sequencing by Hybridization (SBH) is {{a method}} for reconstructing an unknown DNA string based on {{substring}} queries: Using hybridization experiments, one can determine for each string in a given set of strings, whether the string appears in the <b>target</b> <b>string,</b> and use this information to reconstruct the <b>target</b> <b>string.</b> We study the problem when the queries are performed in rounds, where the queries in each round depend on {{the answers to the}} queries in the previous rounds. We give an algorithm that can reconstruct almost all strings of length n using 2 rounds with O(n log # n/ log # log # n) queries per round, and an algorithm that uses log # # n -# 16 rounds with O(n) queries per round, where # is the size of the alphabet...|$|E
40|$|One {{green and}} one red string of letters were {{presented}} on each trial, {{one to the}} left and one to the right of fixation. Participants had to attend to a <b>target</b> <b>string</b> defined by color while ignoring the other distractor string. The <b>target</b> <b>string</b> could be a word or a nonword and the task was a delayed lexical decision. The distractor was always a word. When the target was a word, target and distractor were associatively related on half of the trials and not related in the other trials. The event-related potential time-locked to the onset of the letter strings produced an N 2 pc (a greater negativity at scalp sites contralateral to the target relative to the ipsilateral sites). The N 2 pc amplitude was reduced when the words were related relative to when they were not related. The results provide direct, online evidence that activation of meaning by visual words is very rapid, and interacts with the mechanisms responsible for the deployment of spatial attention...|$|E
40|$|Abstract. The {{expected}} {{number of}} n-base long sequences {{consistent with a}} given SBH spectrum grows exponentially with n, which severely limits the potential range of applicability of SBH even in an error-free setting. Restriction enzymes (RE) recognize specific patterns and cut the DNA molecule at all locations of that pattern. The output of a restriction assay is the set of lengths of the resulting fragments. By augmenting the SBH spectrum with the <b>target</b> <b>string’s</b> RE spectrum, we can eliminate much of the ambiguity of SBH. In this paper, we build on [20] to enhance the resolving power of restriction enzymes. We give a hardness result for the SBH+RE problem, and supply improved heuristics for the existing backtracking algorithm. We prove a lower bound on the number restric-tion enzymes required for unique reconstruction, and show experimental results that are not far from this bound. ...|$|R
40|$|We build a multi-source machine {{translation}} model and train it {{to maximize the}} probability of a <b>target</b> English <b>string</b> given French and German sources. Using the neural encoder-decoder framework, we explore several combination methods and report up to + 4. 8 Bleu increases on top of a very strong attention-based neural translation model. Comment: 5 pages, 6 figure...|$|R
5000|$|The mapping [...] {{is uniquely}} decodable (this can be {{demonstrated}} {{by looking at the}} follow-set after each <b>target</b> bit <b>string</b> in the map, because each bitstring is terminated as soon as we see a 0 bit which cannot follow any existing code to create a longer valid code in the map, but unambiguously starts a new code).|$|R
40|$|We {{introduce}} a novel search algorithm for statisti-cal machine translation based on dynamic program-ming (DP). During the search process two statis-tical knowledge sources are combined: a translation model and a bigram language model. This search al-gorithm expands hypotheses along {{the positions of}} the <b>target</b> <b>string</b> while guaranteeing progressive cov-erage of the words in the source string. We present experimental results on the Verbmobil task. ...|$|E
40|$|Example: {{generate}} {{parameters for}} Java API method String. substring(int index) for benchmarking – Parameter constraints: index ≥ 0 and index < str. length() – Need a non-null invocation <b>target</b> (<b>String</b> instance, e. g. str) ▪ Constraints clear to humans – unavailable to machines ▪ Violation of constraints → exceptions at runtime ▪ Java platform API: thousands of methods – Manual parameter generation: too costly – Random parameter generation: ignores above constraint...|$|E
40|$|A {{test of the}} {{possible}} functional interaction between mechanisms subserving spatial attention and lexical access was devised by displaying one green and one red string of letters, one {{to the left and}} one to the right of fixation, and having participants attend to a <b>target</b> <b>string</b> defined by color while ignoring the other distractor string. The <b>target</b> <b>string</b> for a delayed lexical decision task could be a word or a nonword. The distractor was always a word. When the target was a word, target and distractor were associatively related on half of the trials and not related in the other trials. The event-related potential time-locked to the onset of the letter strings produced an N 2 pc (a greater negativity at scalp sites contralateral to the target relative to the ipsilateral sites arising at about 170 ms poststimulus). N 2 pc amplitude was reduced when the words were related relative to when they were not related. The results provide direct, online evidence that the rapid activation of meaning by visual words can influence the efficiency of the deployment of spatial attention...|$|E
40|$|Four {{experiments}} are reported investigating orthographic priming effects in French by varying {{the number and}} the position of letters shared by prime and target stimuli. Using both standard masked priming and the novel incremental priming technique (Jacobs, Grainger, & Ferrand, 1995), it is shown that net priming effects are affected {{not only by the}} number of letters shared by prime and target stim- uli but also by the number of letters in the prime not present in the target. Several null results are thus explained as a tradeoff between the facilitation generated by common letters and the inhibition gen- erated by different letters. Inhibition was significantly reduced when different letters were replaced by nonalphabetic symbols. Facilitation effects disappeared when the common letters did not have the same relative position in the prime and <b>target</b> <b>strings,</b> thus supporting a relative-position coding scheme for letters in words...|$|R
40|$|Exposure to a repeating set of <b>target</b> <b>strings</b> {{generated}} by an artificial grammar in a speeded matching task generates both explicit and implicit knowledge. Previous {{research has shown}} that implicit knowledge (assessed via a priming measure) is preserved after a retention interval of one week but explicit knowledge (assessed via recognition) is significantly reduced (Tunney, 2003). In two experiments, we replicated and extended Tunney's findings. Experiment 1 was a partial replication of the experiment conducted by Tunney (2003), and demonstrated that the decline in recognition shown by Tunney was not due to a repetition of test items at the pre and post times of assessment. In addition, Experiment 1 lends credibility to Tunney's assumption that recognition scores assess explicit rather than implicit knowledge. Experiment 2 extended Tunney's findings theoretically by demonstrating that interference can produce the pattern of findings demonstrated in the present Experiment 1 as well as in Tunney (2003) ...|$|R
40|$|This paper {{proposes a}} novel Example-Based Machine Translation (EBMT) method based on Tree String Correspondence (TSC) and {{statistical}} generation. In this method, the translation examples are represented as TSC, {{which consists of}} three parts: a parse tree in the source language, a <b>string</b> in the <b>target</b> language, and the correspondences between the leaf nodes of the source language tree and the substrings of the <b>target</b> language <b>string.</b> During the translation, the input sentence is first parsed into a tree. Then the TSC forest is searched out if it is best matched with the parse tree. The translation is generated by using a statistical generation model to combine the <b>target</b> language <b>strings</b> in the TSCs. The generation model consists of three parts: the semantic similarity between words, the word translation probability, and the target language model. Based on the above method, we build an English-to-Chinese Machine Translation (ECMT) system. Experimental {{results indicate that the}} performance of our system is comparable with that of the state-of-the-art commercial ECMT systems. ...|$|R
40|$|First, we give intuition for the {{recurrence}} in Lemma 2. 8, {{and then}} we sketch a proof of its correctness. We note that the proof of correctness for Lemma 2. 8 mirrors, in many ways, the proof of correctness of Theorem 1 in (Kahn et al., 2010) that gives a recurrence for computing a minimum-length feasible generator for a source string X and a <b>target</b> <b>string</b> Y; here, instead, we want to count {{the total number of}} feasible generators ΨX that have a fixed length k. We state a lemma that we proved in (Kahn et al., 2010) that describes an important structural property of the subsequences comprising a feasible generator ΨX. Lemma 1. 1 (Non-overlapping Property, (Kahn et al., 2010)). Consider a source string X and a sequence of duplicate operations of the form δX(si, ti, pi) that generates the final <b>target</b> <b>string</b> Y from an initially empty <b>target</b> <b>string.</b> The substrings Xsi,ti of X that are duplicated during the construction of Y appear as mutually non-overlapping (Def. 2. 3) subsequences of Y. The recurrence for computing N (k) X (Y) is efficient because the non-overlapping property allows us to subdivide the characters of the <b>target</b> <b>string</b> Y into independent subproblems. For example, if we are considering the set of feasible generators that contain some subsequence S of Y, SS = {ΨX: S ∈ ΨX}, then for every ΨX ∈ SS, all other elements of ΨX cannot overlap the characters in S. Therefore, the substrings of Y in between successive characters of S define subproblems that can be computed independently. Note that every character of Y must appear at least once in X. In order to count the number of feasible generators ΨX with length k> 1, we must consider all subsequences of Y that could have been generated by a single duplicate operation and the number of ways we could combine exactly k of those subsequences to form a feasible generator ΨX. The recurrence is based on the observation that in any feasible generator, ΨX, y 1 must be the first (i. e. leftmost) character in some element of ΨX. There are then two cases to consider: either (1) y 1 was the last (or rightmost) character in the substring that was duplicated from X to generate y 1, or (2) y 1 was not the last character in the substring that was duplicated from X to generate y 1...|$|E
40|$|Decoding {{algorithms}} for syntax based {{machine translation}} suffer from high computational complexity, {{a consequence of}} intersecting a language model with a context free grammar. Left-to-right decoding, which generates the <b>target</b> <b>string</b> in order, can improve decoding efficiency by simplifying the language model evaluation. This paper presents a novel left to right decoding algorithm for tree-to-string translation, using a bottom-up parsing strategy and dynamic future cost estimation for each partial translation. Our method outperforms previously published tree-to-string decoders, including a competing left-to-right method. ...|$|E
40|$|We {{introduce}} synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts {{a source}} tree to a <b>target</b> <b>string.</b> Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by + 0. 7 BLEU over the baseline system only allowing for tree substitution on NIST Chinese-English test sets. ...|$|E
40|$|Three {{experiments}} are reported concerning {{the role of}} the syllable in the perception of spoken Dutch. Ss monitored spoken words for the presence of <b>target</b> <b>strings</b> that did or did not correspond to the words' first syllable. Effects of syllabic match were obtained for spoken words with unambiguous syllabic structure, as well as for words containing ambisyllabic consonants, which are shared by 2 syllables. For both types of words, monitoring latencies were shorter if the target matched the first syllable of the spoken word. Syllable effects were independent of the relation between targets and stem morphemes of spoken words. Commonalities and differences between these results and those obtained in other languages such as English and French are discussed. Copyright 1993 by the American Psychological Association, Inc. T his article is not currently available via ORA, but {{you may be able to}} access it via the publisher copy link on this record page...|$|R
40|$|The {{ability of}} English {{speakers}} to monitor internally and externally generated words for syllables {{was investigated in}} this paper. An internal speech monitoring task required participants to silently generate a carrier word on hearing a semantically related prompt word (e. g., reveal—divulge). These productions were monitored for prespecified <b>target</b> <b>strings</b> that were either a syllable match (e. g., /dai/), a syllable mismatch (e. g., /daiv/), or unrelated (e. g., /hju:/) to the initial syllable of the word. In all three experiments the longer target sequence was monitored for faster. However, this tendency reached significance only when the longer string also matched a syllable in the carrier word. External speech versions of each experiment were run that yielded a similar influence of syllabicity but only when the syllable match string also had a closed structure. It was concluded that any influence of syllabicity found using either task reflected the properties of a shared perception-based monitoring system. </p...|$|R
50|$|When Gu-nam {{arrives in}} South Korea, he {{carefully}} scopes out his target for days, while also {{searching for his}} wife. When the time arrives for Gu-nam to take out his <b>target,</b> a <b>string</b> of unexpected events occurs, leaving him desperately {{looking for a way}} out. Meanwhile, the police, the South Korean mob, as well as the ethnic Korean Chinese mafia all frantically search for Gu-nam.|$|R
