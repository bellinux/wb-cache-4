0|121|Public
40|$|In {{applying}} {{economic theory}} to evaluate antitrust laws, Judge Robert Bork explicitly favors a partial equilibrium over a general equilibrium approach. He believes the general model assumes away too many <b>real-world</b> <b>aspects</b> to be usefully {{employed as a}} criterion by which to judge real-world laws. Copyright 1984 Western Economic Association International. ...|$|R
3000|$|Multi-agent {{simulation}} To gain better {{understandings of}} the system and investigate how individual behaviours may influence the overall performance, multi-agent simulation, a powerful technique in OR and AI, will be a more suitable method. Miyashita [81] investigated different GP-based agent models and showed interesting preliminary results. In the future studies, different <b>real-world</b> <b>aspects</b> (e.g. human factors) should be considered to see how GP-based agents will behave.|$|R
25|$|The International Management MBA Track {{features}} an eight-week global experience {{in which students}} travel to Western and Eastern Europe to study emerging, transitional and competitive economies. During study abroad, students experience <b>real-world</b> <b>aspects</b> of classroom work through manufacturing tours, presentations at financial institutions, meetings with government and non-governmental organizations {{as well as the}} experience of living in an international setting. This program operates in partnership with the WHU-Otto Beisheim School of Management.|$|R
50|$|The EV Charger Maps website is a {{volunteer}} run effort coordinated by EV Charger News that catalogs EV charging station information across the U.S. It contains information <b>targeted</b> for <b>real-world</b> use by electric vehicle owners.|$|R
40|$|My {{research}} interests are in human-computer interaction. In particular, I {{am interested in}} important problems {{at the intersection of}} human-computer interaction and machine learning. My dissertation focuses on how to design effective end-user interaction with machine learning. In this research, I <b>target</b> <b>real-world</b> problems that can benefit from end-user driven machine learning. Concrete examples include image classification, access control in online social networks, and alarm triage in large-scale computer networks. Throughout my work, I identify challenges and opportunities for improving the interactive machine learning process and design new and balanced solutions. I also distill guiding principles applicable in a broader context, providing a foundation for future end-user interactive machine learning systems...|$|R
30|$|A {{learning}} {{performance evaluation}} module. This module evaluates and records learners’ performance by conducting tests online {{or in the}} real world. For a real-world test, the learners might be asked to find {{the answer to a}} test item via observing or interacting with <b>real-world</b> <b>targets</b> (i.e., the <b>real-world</b> objects related to the learning goals, such as a plant on school campus).|$|R
40|$|This paper {{analyses}} {{the reasons}} underlying children's difficulties when solving realistic wor(l) d {{problems in the}} mathematics class. In order to do so, first, international studies on these difficulties are reported. Second, we describe how characteristics of the current textbooks and classroom interaction led children to learn that mathematical problems must be solved by using only mathematical information and procedures, and not by attending to relevant <b>real-world</b> <b>aspects</b> of these problems, and how this learning process moves away the mathematics from the school from real life. Third, an intervention study that showed how the classroom practice and culture can be successfully modified is described. Finally, some comments about the limits of these ideas are made. status: publishe...|$|R
5000|$|The {{various members}} of the Zoo Crew lived on a {{parallel}} Earth that, during DC's pre-Crisis multiverse system, was named [...] "Earth-C." [...] Earth-C consisted of a world where various anthropomorphized talking animals existed; the series featured many animal-themed pun names for <b>real-world</b> <b>aspects.</b> For instance, the Zoo Crew operated out of [...] "Follywood, Califurnia," [...] a parody of Hollywood, California. Similarly named Earth-C cities include [...] "Gnu York" [...] (New York City), [...] "Tallahatchee" [...] (Tallahassee, Florida), and [...] "Loondon" [...] (London). Countries on Earth-C include [...] "Cornada" [...] (Canada), and the [...] "United Species of America" [...] (United States of America); the capital of the United Species was [...] "Waspington, DC" [...] (Washington, DC).|$|R
40|$|We {{study the}} problem of {{constructing}} synthetic graphs that resemble real-world directed graphs {{in terms of their}} degree correlations. We define {{the problem of}} directed 2 K construction (D 2 K) that takes as input the directed degree sequence (DDS) and a joint degree and attribute matrix (JDAM) so as to capture degree correlation specifically in directed graphs. We provide necessary and sufficient conditions to decide whether a target D 2 K is realizable, and we design an efficient algorithm that creates realizations with that target D 2 K. We evaluate our algorithm in creating synthetic graphs that <b>target</b> <b>real-world</b> directed graphs (such as Twitter) and we show that it brings significant benefits compared to state-of-the-art approaches...|$|R
30|$|Teachers {{perceive}} that the cross-curricular nature of STEM education is beneficial to student learning, but secondary teachers may perceive barriers or challenges to cross-curricular programs (finding 4). They believe including engineering {{with the other}} subjects adds valuable problem-solving and <b>real-world</b> <b>aspects</b> to instruction that will give students an advantage in preparation for their futures. The cross-curricular connections students make are seen as an advantage to STEM education as they give students necessary skills to approach and solve problems similar to those they will encounter in future careers (Asghar et al. 2012; Bruce-Davis et al. 2014; Dare et al. 2014; McMullin and Reeve 2014; Smith et al. 2015). Technology teachers were particularly interested in using this integrated cross-curricular approach in their classrooms (Asghar et al. 2012).|$|R
40|$|According {{to recent}} studies, an {{enormous}} rise in location-based mobile services {{is expected in}} future. People are interested in getting and acting on the localized information retrieved from their vicinity like local events, shopping offers, local food, etc. These studies also suggested that local businesses intend to maximize the reach of their localized offers/advertisements by pushing them to the maxi- mum number of interested people. The scope of such localized services can be augmented by leveraging the capabilities of smartphones through the dissemination of such information to other interested people. To enable local businesses (or publishers) of localized services to take in- formed decision and assess {{the performance of their}} dissemination-based localized services in advance, we need to predict the performance of data dissemination in complex real-world scenarios. Some of the questions relevant to publishers could be the maximum time required to disseminate information, best relays to maximize information dissemination etc. This thesis addresses these questions and provides a solution called INDIGO that enables the prediction of data dissemination performance based on the availability of physical and social proximity information among people by collectively considering different <b>real-world</b> <b>aspects</b> of data dissemination process. INDIGO empowers publishers to assess the performance of their localized dissemination based services in advance both in physical as well as the online social world. It provides a solution called INDIGO–Physical for the cases where physical proximity plays the fundamental role and enables the tighter prediction of data dissemination time and prediction of best relays under real-world mobility, communication and data dissemination strategy aspects. Further, this thesis also contributes in providing the performance prediction of data dissemination in large-scale online social networks where the social proximity is prominent using INDIGO–OSN part of the INDIGO framework under different <b>real-world</b> dissemination <b>aspects</b> like heterogeneous activity of users, type of information that needs to be disseminated, friendship ties and the content of the published online activities. INDIGO is the first work that provides a set of solutions and enables publishers to predict the performance of their localized dissemination based services based on the availability of physical and social proximity information among people and different <b>real-world</b> <b>aspects</b> of data dissemination process in both physical and online social networks. INDIGO outperforms the existing works for physical proximity by providing 5 times tighter upper bound of data dissemination time under <b>real-world</b> data dissemination <b>aspects.</b> Further, for social proximity, INDIGO is able to predict the data dissemination with 90 % accuracy and differently, from other works, it also provides the trade-off between high prediction accuracy and privacy by introducing the feature planes from an online social networks...|$|R
40|$|Abstract—X-band {{radar systems}} {{represent}} a flexible and low-cost tool for ship detection and tracking. These systems suffer the interference of the sea-clutter {{but at the}} same time they can provide high measurement resolutions, both in space and time. Such features offer the opportunity to get accurate information about the target’s state and shape. Accordingly, here we exploit an extended target tracking methodology based on the popular Probability Hypothesis Density to get information about the targets observed in an actual X-band radar dataset. For each target track we estimate the target’s position, velocity and acceleration, as well as its size and the expected number of radar returns. Index Terms—Multiple target tracking, GGIW-PHD, X-band radar, extended <b>targets,</b> <b>real-world</b> experimental results I. MOTIVATION AND RELATED WORK...|$|R
40|$|Wireless {{networks}} {{are starting to}} be populated by interconnected devices that reveal remarkable hardware and software differences. This fact raises {{a number of questions}} on the applicability of available results on dependability-related aspects of communication protocols, since they were obtained for wireless networks with homogeneous nodes. In this work, we study the impact of heterogeneous communication and computation capabilities of nodes on dependability aspects of diffusion protocols for wireless networks. We build a detailed stochastic model of the logic layers of the communication stack with the SAN formalism. The model takes into account relevant <b>real-world</b> <b>aspects</b> of wireless communication, such as transitional regions and capture effect, and heterogeneous node capabilities. Dependability-related metrics are evaluated with analytical solutions techniques for small networks, while simulation is employed in the case of large networks...|$|R
40|$|International audienceThis paper {{reports on}} the {{procedure}} and learning models we adopted for the 'PAN 2011 Author Identification' challenge <b>targetting</b> <b>real-world</b> email messages. The novelty of our approach lies in a design which combines shallow characteristics of the emails (words and trigrams frequencies) {{with a large number}} of ad hoc linguistically-rich features addressing different language levels. For the author attribution tasks, all these features were used to train a maximum entropy model which gave very good results. For the single author verification tasks, a set of features exclusively based on the linguistic description of the emails' messages was considered as input for symbolic learning techniques (rules and decision trees), and gave weak results. This paper presents in detail the features extracted from the corpus, the learning models and the results obtained...|$|R
50|$|Tom {{tries to}} put a non-violent end to the trail of deaths unknowingly caused by a young woman whose vengeful dreams assault their <b>real-world</b> <b>targets.</b>|$|R
40|$|Abstract Many {{critical}} {{goods and}} services in modern-day economies are produced and distributed through complex institutional arrangements. Agent-based computational economics (ACE) modeling tools are capable of handling this degree of complexity. In concrete support of this claim, this study presents an ACE test bed designed to permit the exploratory study of restructured U. S. wholesale power markets with transmission grid congestion managed by locational marginal prices (LMPs). Illustrative findings are presented showing how spatial LMP cross-correlation patterns vary systematically in response {{to changes in the}} price responsiveness of wholesale power demand when wholesale power sellers have learning capabilities. These findings highlight several distinctive features of ACE modeling: namely, an emphasis on process rather than on equilibrium; an ability to capture complicated structural, institutional, and behavioral <b>real-world</b> <b>aspects</b> (micro-validation); and an ability to study the effects of changes in these aspects on spatial and temporal outcome distributions. ...|$|R
40|$|In recent years, the government, of African Countries {{has assumed}} major {{responsibilities}} for economic reforms and growth. In attempting {{to describe their}} economies, economists (policymakers) in many African Countries have applied certain models that are by now widely known: Linear programming models, input-output models, macro-econometric models, vector auto regression models and computable general equilibrium models. Unfortunately, economies are complicated systems encompassing micro behaviors, interaction patterns and global regularities. Whether partial or general in scope, studies of economic systems must consider how to handle difficult <b>real-world</b> <b>aspects</b> such as asymmetric information, imperfect competition, strategic interaction, collective learning and multiple equilibria possibility. This paper therefore argues for the adoption of alternative modeling (bottom-up culture-dish) approach known as AGENT-BASED Computational Economics (ACE), which is the computational study of African economies modeled as evolving systems of autonomous interacting agents. However, the software bottleneck (what rules to write for our agents) remains the primary challenge ahead. ...|$|R
3000|$|This {{research}} {{has previously been}} published as a conference abstract presented at the Vision Science Society annual meeting 2015 (Riggs, C., Cornes, K., Godwin, H., Guest, R., & Donnelly, N. (2015). The Importance of Slow Consistent Movement when Searching for Hard-to-Find <b>Targets</b> in <b>Real-World</b> Visual Search. Journal of Vision, 15 (12), 1355 - 1355.) [...]...|$|R
500|$|I Love Bees is {{credited}} with helping drive attention to Halo 2; former Electronic Gaming Monthly editor Dan Hsu stated {{in an interview that}} [...] "I Love Bees really got existing gamers and other consumers talking about the universe of [...]" [...] Billy Pidgeon, a game analyst, noted that I Love Bees achieved what it had been designed to do: [...] "This kind of viral guerrilla marketing worked... Everyone started instant messaging about it and checking out the site." [...] I Love Bees not only received coverage from gaming publications, but attracted mainstream press attention as well. At its height, ilovebees received between two and three million unique visitors over the course of three months. 9,000 people also actively participated in the <b>real-world</b> <b>aspects</b> of the game. The players of I Love Bees themselves were quite varied. The target demographic for the promotion was younger males, but one player noted that even middle-aged men and women were engaged in the game.|$|R
40|$|Part 1 : Long and Short Papers (Continued) International audienceSocial {{networks}} {{increase the}} challenges of designing <b>real-world</b> <b>aspects</b> whose computational abstraction is not simple. This includes death and digital legacy, strongly influenced by cultural phenomena, such as religion. Therefore, {{it is important to}} analyze youngsters’ concepts of death in the web, as the Internet Generation outnumbers other groups of social network users. Besides, due to their age, many of them face other people’s death {{for the first time on}} the web. This paper analyzes to what extent these users’ religion and the belief in afterlife may signal guidelines for a social network project that considers volition towards digital legacy. The data herein analyzed qualitatively and quantitatively come from a survey-based research with Brazilian high school students. The contributions for Human-Computer Interaction (HCI) studies comprise design solutions that may consider aspects of religion, death and digital legacy, also improving users’ and designers’ understanding on these issues in system design...|$|R
40|$|Many {{critical}} {{goods and}} services in modern-day economies are produced and distributed through complex institutional arrangements. Agent-based computational economics (ACE) modeling tools are capable of handling this degree of complexity. In concrete support of this claim, this study presents an ACE test bed designed to permit the exploratory study of restructured U. S. wholesale power markets with transmission grid congestion managed by locational marginal prices (LMPs). Illustrative findings are presented showing how spatial LMP cross-correlation patterns vary systematically in response {{to changes in the}} price responsiveness of wholesale power demand when wholesale power sellers have learning capabilities. These findings highlight several distinctive features of ACE modeling: namely, an emphasis on process rather than on equilibrium; an ability to capture complicated structural, institutional, and behavioral <b>real-world</b> <b>aspects</b> (micro-validation); and an ability to study the effects of changes in these aspects on spatial and temporal outcome distributions. Institutional Design; agent-based computational economics; U. S. Electricity Market; Locational marginal pricing; Spatial Cross-Correlations; AMES Test Bed...|$|R
40|$|Abstract. Social {{networks}} {{increase the}} challenges of designing <b>real-world</b> <b>aspects</b> whose computational abstraction is not simple. This includes death and digital legacy, strongly influenced by cultural phenomena, such as religion. Therefore, {{it is important to}} analyze youngsters ‟ concepts of death in the web, as the Internet Generation outnumbers other groups of social network users. Besides, due to their age, many of them face other people´s death {{for the first time on}} the web. This paper analyzes to what extent these users ‟ religion and the belief in afterlife may signal guidelines for a social network project that considers volition towards digital legacy. The data herein analyzed qualitatively and quantitatively come from a survey-based research with Brazilian high school students. The contributions for Human-Computer Interaction (HCI) studies comprise design solutions that may consider aspects of religion, death and digital legacy, also improving users ‟ and designers ‟ understanding on these issues in system design...|$|R
40|$|Economies are {{complicated}} systems encompassing micro behaviors, interaction patterns, and global regularities. Whether partial or general in scope, studies of economic systems must consider {{how to handle}} difficult <b>real-world</b> <b>aspects</b> such as asymmetric information, imperfect competition, strategic interaction, collective learning, {{and the possibility of}} multiple equilibria. Recent advances in analytical and computational tools are permitting new approaches to the quantitative study of these aspects. One such approach is Agent-based Computational Economics (ACE), the computational study of economic processes modeled as dynamic systems of interacting agents. This chapter explores the potential advantages and disadvantages of ACE for the study of economic systems. General points are concretely illustrated using an ACE model of a two-sector decentralized market economy. Six issues are highlighted: Constructive understanding of production, pricing, and trade processes; the essential primacy of survival; strategic rivalry and market power; behavioral uncertainty and learning; the role of conventions and organizations; and the complex interactions among structural attributes, institutional arrangements, and behavioral dispositions. ...|$|R
5000|$|I Love Bees is {{credited}} with helping drive attention to Halo 2; former Electronic Gaming Monthly editor Dan Hsu stated {{in an interview that}} [...] "I Love Bees really got existing gamers and other consumers talking about the universe of Halo." [...] Billy Pidgeon, a game analyst, noted that I Love Bees achieved what it had been designed to do: [...] "This kind of viral guerrilla marketing worked... Everyone started instant messaging about it and checking out the site." [...] I Love Bees not only received coverage from gaming publications, but attracted mainstream press attention as well. At its height, ilovebees received between two and three million unique visitors over the course of three months. 9,000 people also actively participated in the <b>real-world</b> <b>aspects</b> of the game. The players of I Love Bees themselves were quite varied. The target demographic for the promotion was younger males, but one player noted that even middle-aged men and women were engaged in the game.|$|R
40|$|The {{research}} {{described in}} this thesis describes work towards making Active Contours fully autonomous mechanisms capable of tracking biological <b>targets</b> in <b>real-world</b> situations. The problem of tracking the apparent motion of cauliflower plants photographed from a tractor was of special interest. In order to reach this simple goal {{it was necessary to}} complete research {{in a number of different}} areas. Activ...|$|R
40|$|A major {{assumption}} {{underlying the}} use of contrast sensitivity testing is that it predicts whether a patient has difficulty seeing objects encountered in everyday life. However, {{there has been no}} large-scale attempt to examine whether this putative relationship actually exists. We have examined this assumption using a clinic based sample of adults aged 20 - 77 years. Contrast thresholds were measured for both: (1) gratings of 0. 5 - 22. 8 cycles/degree; and (2) <b>real-world</b> <b>targets</b> (faces, road signs, objects). Multiple regression techniques indicated that the best predictors of thresholds for <b>real-world</b> <b>targets</b> were age and middle to low spatial frequencies. Models incorporating these variables accounted for 25 - 40 % of the variance. Although acuity significantly correlated with thresholds for <b>real-world</b> <b>targets,</b> the inclusion of acuity as a predictor variable did not improve the model. These data provide direct evidence that spatial contrast sensitivity can effectively predict how well patients see targets typical of everyday life...|$|R
30|$|This article {{builds on}} our earlier work on {{flooding}} protocols [1, 2], where {{we proposed a}} flooding protocol called prioritized flooding with self-pruning (PFS) and compared it to other common flooding protocols. In [1], we proposed {{the first version of}} this protocol and simulated it in the standard configuration of ns 2 [3], which is a very popular simulation package for wireless networking. In [2], we implemented the same protocol in a real wireless network test-bed and again measured its performance and compared it to blind flooding and counter-based broadcasting (CBB) [4]. We found that PFS had problems in real networks, and we had to modify the protocol. We also noticed discrepancies in the performance results obtained in the simulator in comparison to what we observed in the real network. Hence, we wanted to better understand this and see if it is possible to find a model that more closely simulates a real network. With such a simulator, it becomes possible to study more parameters and more protocols compared to a test-bed but without loosing important <b>real-world</b> <b>aspects.</b>|$|R
40|$|The Keystroke Biometrics Ongoing Competition (KBOC) {{presented}} an anomaly detection challenge {{with a public}} keystroke dataset containing {{a large number of}} subjects and <b>real-world</b> <b>aspects.</b> Over 300 subjects typed case-insensitive repetitions of their first and last name, and as a result, keystroke sequences could vary in length and order depending on the usage of modifier keys. To deal with this, a keystroke alignment preprocessing algorithm was developed to establish a semantic correspondence between keystrokes in mismatched sequences. The method is robust in the sense that query keystroke sequences need only approximately match a target sequence, and alignment is agnostic to the particular anomaly detector used. This paper describes the fifteen best-performing anomaly detection systems submitted to the KBOC, which ranged from auto-encoding neural networks to ensemble methods. Manhattan distance achieved the lowest equal error rate of 5. 32 %, while all fifteen systems performed better than any other submission. Performance gains are shown to be due in large part not to the particular anomaly detector, but to preprocessing and score normalization techniques...|$|R
30|$|We {{have shown}} that it is {{possible}} to use a recursive approach to classify radar tracks from kinematic data. We have also shown {{that it is possible}} to use an alpha beta filter together with the random forest such that stationary targets are classified as stationary. The study used both simulated data, which is simulated to behave as real <b>targets,</b> and <b>real-world</b> data. We have shown both scenario and confusion matrix to get an overview of the performance.|$|R
40|$|We {{investigated}} {{the efficiency of}} attack strategies to network nodes when targeting several complex model and real-world networks. We tested 5 attack strategies, 3 of which were introduced in this work for the first time, to attack 3 model (Erdos and Renyi, Barabasi and Albert preferential attachment network, and scale-free network configuration models) and 3 real networks (Gnutella peer-to-peer network, email network of the University of Rovira i Virgili, and immunoglobulin interaction network). Nodes were removed sequentially according to the importance criterion defined by the attack strategy. We used {{the size of the}} largest connected component (LCC) as a measure of network damage. We found that the efficiency of attack strategies (fraction of nodes to be deleted for a given reduction of LCC size) depends on the topology of the network, although attacks {{based on the number of}} connections of a node and betweenness centrality were often the most efficient strategies. Sequential deletion of nodes in decreasing order of betweenness centrality was the most efficient attack strategy when <b>targeting</b> <b>real-world</b> networks. In particular for networks with power-law degree distribution, we observed that most efficient strategy change during the sequential removal of nodes. Comment: 18 pages, 4 figure...|$|R
40|$|Transactional Memory (TM) {{promises}} to simplify concurrent programming, {{which has been}} notoriously difficult but crucial in realizing the performance benefit of multi-core processors. Software Transaction Memory (STM), in particular, represents a body of important TM technologies since it provides a mechanism to run transactional programs when hardware TM support is not available, or when hardware TM resources are exhausted. Nonetheless, most previous studies on STMs were constrained to executing trivial, small-scale workloads. The assumption was that the same techniques applied to small-scale workloads could readily be applied to real-life, large-scale workloads. However, by executing several nontrivial workloads such as particle dynamics simulation and game physics engine on {{a state of the}} art STM, we noticed that this assumption does not hold. Specifically, we identified four major performance bottlenecks that were unique to the case of executing large-scale workloads on an STM: false conflicts, overinstrumentation, privatization-safety cost, and poor amortization. We believe that these bottlenecks would be common for any STM <b>targeting</b> <b>real-world</b> applications. In this paper, we describe those identified bottlenecks in detail, and we propose novel solutions to alleviate the issues. We also thoroughly validate these approaches with experimental results on real machines...|$|R
40|$|Network {{structures}} (graphs) {{have become}} {{a natural part of}} everyday life and their anal-ysis helps to gain an understanding of their inherent structure and the <b>real-world</b> <b>aspects</b> thereby expressed. The exploration of graphs is largely supported and driven by visual means. The aim of this thesis is to give a comprehensive view on the problems associated with these visual means and to detail concrete solution approaches for them. This is done on all three levels involved: the data level, the representation level, and the task level. In a first step, the intricate dependencies on the representation level alone are discussed for the case of implicit tree visualizations. With the awareness of the representation issues involved, a second step takes the characteristics on data level into account, which are most importantly graph size and graph type. Their influence on the visualization design are discussed specifically for large trees and bipartite graphs. Finally, with bringing in the task level, the entire exploration workflow from computational preprocessing to the interaction with the visualization can be captured and thus discussed in terms of its consequences for the visualization design. This includes issues of necessary confirmative elements in the exploratory analysis, as well as selecting appropriate data sets from a heterogeneous data pool for an analysis. Concrete visualization techniques are introduced to underline the value of this com-prehensive discussion for supporting explorative graph visualization...|$|R
40|$|In this paper, we {{have solved}} the unique problem of {{casualty}} transportation problem under Ebola by developing dynamic policies for vehicle routing {{in order to}} provide practical decision support. The objective of the proposed model is to minimize the total transmission risk. We describe the problem with real- constraints and parameters based on the empirical data of the 2014 Ebola outbreak in Liberia. The casualty transportation problem is a variant of Dynamic Pick-up and Delivery Problems (DPDP), in which a vehicle is dispatched to demand location in real time and it then transports the demands to the aimed area (destination). DPDP integrates various sources of dynamic events as well as <b>real-world</b> <b>aspects.</b> All decision made in dynamic vehicle routing problems are taken in real-time. In same light, the casualty transportation problem is solved in real-time under infectious disease environment. Particularly in case of Ebola, the prompt burial or cremation of deceased Ebola victims would is critical in reducing the transmission of the virus thus casualty transportation problem should be considered mandatory to stop transmission. To solve the casualty transportation problem, our approach involves developing adequate policies for vehicle routing construction under Ebola situation, to evaluate those policies performance through simulations, and to recommend the best policy resulted from experimental results that can reduce the total transmission risk a lot. ...|$|R
40|$|Memorization of {{a method}} is a {{commonly}} used re-factoring wherein developer modules {{the code of}} a method to save return values for some or all incoming parameter values. Whenever a parameter-tuple is received for the second or subsequent time, the method's execution can be elided and the corresponding saved value can be returned. It is quite challenging for developers to identify suitable methods for memorization, as these {{may not necessarily be}} the methods that account for a high fraction of the running time in the program. What are really sought are the methods that cumulatively incur signi_cant execution time in invocations that receive repeat parameter values. Our primary contribution is a novel dynamic analysis approach that emits a report that contains, for each method in an application, an estimate of the execution time savings to be expected from memorizing this method. The key technical novelty of our approach is a set of design elements that allow our approach to <b>target</b> <b>real-world</b> programs, and to compute the estimates in a re-grained manner. We describe our approach in detail, and evaluate an implementation of it on several real-world programs. Our evaluation reveals that there do exist many methods with good estimated savings that the approach is reasonably ancient, and that it has good precision (relative to actual savings) ...|$|R
40|$|Real-world {{robotics}} problems {{often occur}} in domains that {{differ significantly from}} the robot's prior training environment. For many robotic control tasks, real world experience is expensive to obtain, but data is easy to collect in either an instrumented environment or in simulation. We propose a novel domain adaptation approach for robot perception that adapts visual representations learned on a large easy-to-obtain source dataset (e. g. synthetic images) to a <b>target</b> <b>real-world</b> domain, without requiring expensive manual data annotation of real world data before policy search. Supervised domain adaptation methods minimize cross-domain differences using pairs of aligned images that contain the same object or scene in both the source and target domains, thus learning a domain-invariant representation. However, they require manual alignment of such image pairs. Fully unsupervised adaptation methods rely on minimizing {{the discrepancy between the}} feature distributions across domains. We propose a novel, more powerful combination of both distribution and pairwise image alignment, and remove the requirement for expensive annotation by using weakly aligned pairs of images in the source and target domains. Focusing on adapting from simulation to real world data using a PR 2 robot, we evaluate our approach on a manipulation task and show that by using weakly paired images, our method compensates for domain shift more effectively than previous techniques, enabling better robot performance in the real world...|$|R
40|$|Information Technology (IT) {{curricula}} 2 ̆ 7 s strong application component and {{its focus}} on user centeredness and team work require that students experience directly real-world projects for real users of IT solutions. Although the merit of this IT educational tenet is universally recognized, delivering collaborative and experiential learning has its challenges. Reaching out to identify projects formulated by actual organizations adds significantly to course preparation. There is {{a certain level of}} risk involved with delivering a useful solution while, at the same time, enough room should be allowed for students to experiment with, be wrong about, review, and learn. Challenges pertaining to the <b>real-world</b> <b>aspect</b> of problem-based learning are compounded by managing student teams and assessing their work such that both individual and collective contributions are taken into account. Finally, the quality of the project releases is not the only measure of student learning. Students should be given meaningful opportunities to practice, improve, and demonstrate their communication and interpersonal skills. In this paper we present our experience with two courses in which teams of students worked on real-world projects involving three external partners. We describe how each of the challenges listed above has impacted the course requirements, class instruction, team dynamics, assessment, and learning in these courses. Course assessment and survey data from students are linked to learning outcomes and point to areas where the collaborative and experiential learning model needs improvement...|$|R
