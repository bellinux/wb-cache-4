21|10000|Public
50|$|Accelerated {{life testing}} {{is the process}} of testing a product by {{subjecting}} it to conditions (stress, strain, temperatures, voltage, vibration rate, pressure etc.) in excess of its normal service parameters in an effort <b>to</b> <b>uncover</b> <b>faults</b> and potential modes of failure in a short amount of time. By analyzing the product's response to such tests, engineers can make predictions about the service life and maintenance intervals of a product.|$|E
40|$|Abstract Evolutionary {{testing is}} an optimisation-based testcase {{generation}} technique. It {{can be applied}} to timing analysis of real-time systems where it is used to uncover temporal errors. Testability is the ability of the test technique <b>to</b> <b>uncover</b> <b>faults.</b> Evolutionary testability is the ability of an evolutionary algorithm to successfully generate test-cases with the goal <b>to</b> <b>uncover</b> <b>faults,</b> in this instance violations of the timing specification. Some properties of real-time programs were found to greatly inhibit evolutionary testability. These are small path domains, high-data dependence, large input vectors, and nesting. This paper defines source code measures which aim to express the effects of these properties on evolutionary testing. The measurement and prediction system developed from the experiments is able to forecast evolutionary testability with almost 90 % accuracy. The prediction system will be used to assess whether the application of evolutionary testing to a real-time system will be sufficient for successful dynamic timing analysis, or whether additional testing strategies are needed...|$|E
40|$|Two {{major factors}} {{influence}} {{the number of}} faults uncovered by a fault-detection process applied to a software artifact (e. g., specification, code) : ability of the process <b>to</b> <b>uncover</b> <b>faults,</b> quality of the artifact (number of existing faults). These two factors must be assessed separately, so that one can: switch to a different process if the one being used is not effective enough, or stop the process {{if the number of}} remaining faults is acceptable. The fault-detection process assessment model can be applied to all sorts of artifacts produced during software development, and provides measures for both the `effectiveness of a fault-detection process' and the `number of existing faults in the artifact'. The model is valid even when there are zero defects in the artifact or the fault-detection process is intrinsically unable <b>to</b> <b>uncover</b> <b>faults.</b> More specifically, the times between fault discoveries are modeled via reliability-based techniques with an exponential distribution. The hazard rate is the product of `effectiveness of the fault-detection process' and `number of faults in the artifact'. Based on general hypotheses, the number of faults in an artifact follows a Poisson distribution. The unconditional distribution, whose parameters are estimated via maximum likelihood, is obtained...|$|E
40|$|Most {{approaches}} to testing use branch coverage {{to decide on}} the quality of a given test suite. The intuition is that covering branches relates directly <b>to</b> <b>uncovering</b> <b>faults.</b> In this article we present an empirical study that applied random testing to 14 Eiffel classes for a total of 2520 hours and recorded the number of <b>uncovered</b> <b>faults</b> and the branch coverage over time. Our results show that: (1) in the tested classes, random testing reaches 93 % branch coverage (2) it exercises almost the same set of branches every time, (3) it detects different faults from time to time, (4) during the first 10 minutes of testing while branch coverage increases rapidly, there is a strong correlation between branch coverage and the number of <b>uncovered</b> <b>faults,</b> (5) over 50 % of the faults are detected at a time where branch coverage hardly changes and the correlation between branch coverage and the number of <b>uncovered</b> <b>faults</b> is weak. These results provide evidence that branch coverage is not a good stopping criterion for random testing. They also show that branch coverage is not a good indicator for the effectiveness of a test suite...|$|R
40|$|Abstract. Most {{approaches}} to testing use branch coverage {{to decide on}} the qual-ity of a given test suite. The intuition is that covering branches relates directly <b>to</b> <b>uncovering</b> <b>faults.</b> The empirical study reported here applied random testing to 14 Eiffel classes for a total of 2520 hours and recorded the number of <b>uncovered</b> <b>faults</b> and the branch coverage over time. For the tested classes, (1) random test-ing reaches 93 % branch coverage (2) it exercises almost the same set of branches every time, (3) it detects different faults from execution to execution, (4) during the first 10 minutes of testing, while branch coverage increases rapidly, there is a strong correlation between branch coverage and the number of <b>uncovered</b> <b>faults,</b> (5) over 50 % of the faults are detected at a time where branch coverage hardly changes, and the correlation between branch coverage and the number of uncov-ered faults is weak. These results provide evidence that branch coverage is not a good stopping cri-terion for random testing. They also show that branch coverage is not a good indicator for the effectiveness of a test suite...|$|R
40|$|This paper {{discusses}} {{sensitivity analysis}} {{and its relationship}} to random black box testing. Sensitivity analysis estimates the impact that a programming fault at a particular location would have on the program's input/output behavior. Locations that are relatively "insensitive" to faults can render random black box testing unlikely <b>to</b> <b>uncover</b> programming <b>faults.</b> Therefore, sensitivity analysis gives new insight when interpreting random black box testing results. Although sensitivity analysis is computationally intensive, it requires no oracle and no human intervention...|$|R
40|$|The {{problem of}} testing {{programs}} without test oracles is well known. A commonly used {{approach is to}} use special values in testing but this is often insufficient to ensure program correctness. This paper demonstrates the use of metamorphic testing <b>to</b> <b>uncover</b> <b>faults</b> in programs, {{which could not be}} detected by special test values. Metamorphic testing {{can be used as a}} complementary test method to special value testing. In this paper, the sine function and a search function are used as examples to demonstrate the usefulness of metamorphic testing. This paper also examines metamorphic relationships and the extent of their usefulness in program testing. 1...|$|E
40|$|Evolutionary testing (ET) is a {{test case}} {{generation}} technique based upon the application of an evolutionary algorithm. It {{can be applied to}} timing analysis of real-time systems. In this instance, timing analysis is equivalent to testing. The test objective is to uncover temporal errors. This corresponds to the violation of the system's timing specification. Testability is the ability of the test technique <b>to</b> <b>uncover</b> <b>faults.</b> Evolutionary testability is the ability of an evolutionary algorithm to successfully generate test cases with the goal <b>to</b> <b>uncover</b> <b>faults,</b> in this instance violations of the timing specification. This process attempts to find the best- and worst-case execution time of a real-time system. Some attributes of real-time systems were found to greatly inhibit the successful generation of the best- and worst- case execution times through ET. These are small path domains, high-data dependence, large input vectors, and nesting. This work defines software metrics which aim to express the effects of these attributes on evolutionary testing. ET is applied to generate the best- and worst-case execution paths of test programs. Their extreme timing paths are determined analytically and the average success of ET to cover these paths is assessed. This empirical data is mapped against the software metrics to derive a prediction system for evolutionary testability. The measurement and prediction system developed from the experiments is able to forecast evolutionary testability with almost 90 % accuracy. The prediction system will be used to assess whether the application of evolutionary testing to a real-time system will be sufficient for successful dynamic timing analysis, or whether additional testing strategies are needed...|$|E
40|$|In practice, {{available}} testing budgets {{limit the}} number of test cases that can be executed. Thus, a representative subset of all possible test cases must be chosen to guarantee adequate coverage of a test object. In risk-based testing, the probability of a fault and the damage that this fault can cause when leading to a failure is considered for test case prioritization. Existing approaches for risk-based testing provide guidelines for deriving test cases. However, those guidelines lack the level of detail and precision needed for automation. In this contribution, we introduce the risk-based testing technique RiteDAP, which automatically generates system test cases from activity diagrams and prioritizes those test cases based on risk. The results of applying the technique to a practical example are presented and the ability of different prioritization strategies <b>to</b> <b>uncover</b> <b>faults</b> is evaluated...|$|E
40|$|Unit {{testing is}} a {{scalable}} and e#ective way <b>to</b> <b>uncover</b> software <b>faults.</b> In the JNuke project, automated regression tests combined with coverage measurement ensured high code quality throughout the project. By using a custom testing environment, functionality was extended {{beyond what is}} commonly available by unit test frameworks. Low-overhead memory leak detection was implemented through wrapping. Automated support for log files {{made it possible to}} track the internal state of objects, which is often much more expedient than writing test code. These extensions allowed the easyto -use unit test framework to scale up to large-scale tests. The techniques can be ported to existing test frameworks...|$|R
40|$|We {{propose a}} method for {{diagnosis}} of parametric faults in analog circuits using polynomial coefficients {{of the circuit model}} [15]. As a sequel to our recent work [14], where circuit response is modeled as polynomial for <b>uncovering</b> parametric <b>faults</b> in nonlinear circuits, we propose diagnosis of such faults using sensitivity of coefficients of the estimated polynomial to circuit parameters. The proposed method requires no design for test hardware as might be added to the circuit by some other methods. The proposed method is illustrated for a benchmark elliptic filter. It is shown <b>to</b> <b>uncover</b> several parametric <b>faults</b> causing deviations as small as 5 % from the nominal values. 1...|$|R
40|$|ABSTRACT Unit {{testing is}} a {{scalable}} and effective way <b>to</b> <b>uncover</b> software <b>faults.</b> In the JNuke project, automated regression tests combined with coverage measurement ensured high code quality throughout the project. By using a custom testing environment, functionality was extended {{beyond what is}} commonly available by unit test frameworks. Low-overhead memory leak detection was implemented through wrapping. Automated support for log files {{made it possible to}} track the internal state of objects, which is often much more expedient than writing test code. These extensions allowed the easyto-use unit test framework to scale up to large-scale tests. The techniques can be ported to existing test frameworks. Categories and Subject Descriptors D. 2. 5 [Software Engineering]: Testing and Debuggin...|$|R
40|$|Software {{requirements}} specifications (SRS) {{are often}} validated manually. One such process is inspection, in which several reviewers independently analyze {{all or part}} of the specification and search for faults. These faults are then collected at a meeting of the reviewers and author(s). Usually, reviewers use Ad Hoc or Checklist methods <b>to</b> <b>uncover</b> <b>faults.</b> These methods force all reviewers to rely on nonsystematic techniques to search for a wide variety of faults. We hypothesize that a Scenario-based method, in which each reviewer uses different, systematic techniques to search for different, specific classes of faults, will have a significantly higher success rate. In previous work we evaluated this hypothesis using 48 graduate students in computer science as subjects. We now have replicated this experiment using 18 professional developers from Lucent Technologies as subjects. Our goals were to (1) extend the external credibility of our results by studying professional developers, and t [...] ...|$|E
40|$|Software {{requirements}} specifications (SRS) {{are often}} validated manually. One such process is inspection, in which several reviewers independently analyze {{all or part}} of the specification and search for faults. These faults are then collected at a meeting of the reviewers and author(s). Usually, reviewers use Ad Hoc or Checklist methods <b>to</b> <b>uncover</b> <b>faults.</b> These methods force all reviewers to rely on nonsystematic techniques to search for a wide variety of faults. We hypothesize that a Scenario-based method, in which each reviewer uses different, systematic techniques to search for different, specific classes of faults, will have a significantly higher success rate. We evaluated this hypothesis using a 3 Θ 2 4 partial factorial, randomized experimental design. Forty eight graduate students in computer science participated in the experiment. They were assembled into sixteen, three-person teams. Each team inspected two SRS using some combination of Ad Hoc, Checklist or Scenario meth [...] ...|$|E
40|$|Systematic design testing, {{in which}} {{executable}} models of behaviours are tested using inputs that exercise scenarios, can help reveal flaws in designs {{before they are}} implemented in code. In this paper a technique for testing executable forms of UML (Unified Modelling Language) models is described and test adequacy criteria based on UML model elements are proposed. The criteria {{can be used to}} define test objectives for UML designs. The UML design test criteria are based on the same premise underlying code test criteria: coverage of relevant building blocks of models is highly likely <b>to</b> <b>uncover</b> <b>faults.</b> The test adequacy criteria proposed in this paper are based on building blocks for UML class and interaction diagrams. Class diagram criteria are used to determine the object configurations on which tests are run, while interaction diagram criteria are used to determine the sequences of messages that should be tested. Copyright c ○ 2003 Joh...|$|E
5000|$|The psalm {{considers}} {{the glory of}} God in creation, and moves {{to reflect on the}} character and use of [...] "the law of the [...] ". A comparison is made between the law and the sun, which lends a degree of unity to the psalm. C. S. Lewis suggested that in verse 7, the Psalmist starts talking about something else, [...] "which hardly seems to him like something else because it is so like the all-piercing, all-detecting sunshine." [...] Like the Sun, the law is able <b>to</b> <b>uncover</b> hidden <b>faults,</b> and nothing can hide from it. As the Psalmist meditates on the excellencies of the law, he feels that his sins have been laid open before God's word, and asks for forgiveness and help.|$|R
40|$|Abstract — Engineering designs {{involve the}} use of science, technology, intuition and {{information}} to achieve transformation of concept into reality in pursuant of specific objectives. An inherent problem is that errors made in the design will affect the quality and performance of any system since reliability of a system is affected at every stage in its production. Data {{for this study was}} obtained by direct interview, use of questionnaire and review of samples. This paper showed the importance of review of design at early stages <b>to</b> <b>uncover</b> possible <b>faults,</b> reduce risks of flaws, thereby ensuring that it meets the durability requirement in a safe, functional and cost effective manner. Significantly 94 % of examined designs had one or more flaws, while about 33 % of designers were unaware of the need for the design review...|$|R
40|$|International audienceTesting data-intensive {{systems is}} {{paramount}} {{to increase our}} reliance on e-governance services. An incorrectly computed tax can have catastrophic consequences {{in terms of public}} image. Testers at Norwegian Customs and Excise reveal that faults occur from interactions between database features such as field values. Taxation rules, for example, are triggered due to an interaction between 10, 000 items, 88 country groups, and 934 tax codes. There are about 12. 9 trillion 3 -wise interactions. Finding interactions <b>to</b> <b>uncover</b> specific <b>faults</b> is like finding a needle in a haystack. Can we surgically generate a test database for interactions that interest testers? We address this question with a methodology and tool to automatically populate a test database that covers all T-wise interactions for selected features. generates a constraint model of interactions in and solves it using a divide-and-combine strategy. Our experiments demonstrate scalability of our methodology and we project its industrial applications...|$|R
40|$|Wireless Sensor Networks (WSNs) {{are widely}} adopted for {{applications}} ranging from surveillance to environmental monitoring. While powerful and relatively inexpensive, they {{are subject to}} behavioural faults which make them unreliable. Due to the complex interactions between network nodes, it is difficult <b>to</b> <b>uncover</b> <b>faults</b> in a WSN by resorting to formal techniques for verification and analysis, or to testing. This paper proposes an evolutionary framework to detect anomalous behaviour related to energy consumption in WSN routing protocols. Given a collection protocol, the framework creates candidate topologies and evaluates them through simulation on the basis of metrics measuring the radio activity on nodes. Experimental results using the standard Collection Tree Protocol show that the proposed approach is able to unveil topologies plagued by excessive energy depletion over one or more nodes, and thus could be used as an offline debugging tool to understand and correct the issues before network deployment and during the development of new protocol...|$|E
40|$|This paper {{investigates the}} use of Graphical Interval Logic for {{specification}} and testing of concurrent and real-time systems. Past research produced a toolset for creating specifications in GIL and for reasoning about properties of systems that satisfy their specifications. Current research is exploring {{the use of}} GIL specifications during system design and implementation. Our work to date has focused primarily on using GIL specifications as oracles during testing of executable designs. The paper presents an example in which GIL specifications for an Ada design are used <b>to</b> <b>uncover</b> <b>faults</b> during testing. It also shows how the graphical representation of formulas {{can be used to}} advantage in displaying a trace that violates a specification. The testing methods have yet to be automated. 1 Introduction This paper describes ongoing research on the use of formal specifications and testing during design of concurrent and real-time systems. We use a highly intuitive visual notation, called Gr [...] ...|$|E
40|$|Symmetries {{often appear}} as {{properties}} of many artifical settings. In Program Testing, {{they can be}} viewed as properties of programs and can be given by the tester to check the correctness of the computed outcome. In this paper, we consider symmetries to be permutation relations between program executions and use them to automate the testing process. We introduce a software testing paradigm called Symmetric Testing, where automatic test data generation is coupled with symmetries checking <b>to</b> <b>uncover</b> <b>faults</b> inside the programs. A practical procedure for checking that a program satisfies a given symmetry relation is described. The paradigm makes use of Group theoretic results as a formal basis to minimize the number of outcome comparisons required by the method. This approach appears to be of particular interest for programs for which neither an oracle, nor any formal specification is available. We implemented Symmetric Testing by using the primitive operations of the Java unit testing tool Roast [9]. The experimental results we got on faulty versions of classical programs of the Software Testing community tend to show the effectiveness of the approach. 1...|$|E
40|$|Abstract. Web 2. 0 {{applications}} {{rely heavily}} on JAVASCRIPT and client-side run-time manipulation of the DOM tree. One way to provide assurance about the cor-rectness of such highly evolving and dynamic applications is through regression testing. However, JAVASCRIPT is loosely typed, dynamic, and notoriously chal-lenging to analyze and test. We propose an automated technique for JAVASCRIPT regression testing, {{which is based on}} on-the-fly JAVASCRIPT source code instru-mentation and dynamic analysis to infer invariant assertions. These obtained as-sertions are injected back into the JAVASCRIPT code <b>to</b> <b>uncover</b> regression <b>faults</b> in subsequent revisions of the web application under test. Our approach is imple-mented in a tool called JSART. We present our case study conducted on nine open source web applications to evaluate the proposed approach. The results show that our approach is able to effectively generate stable assertions and de-tect JAVASCRIPT regression faults {{with a high degree of}} accuracy and minimal performance overhead...|$|R
40|$|Assessing {{reliability}} of software programs during validation is a challenging task for engineers. The assessment {{is not only}} required to be unbiased, but it needs to provide tight variance (hence, tight confidence interval) with as few test cases as possible. Statistical sampling is a theoretically sound approach for reliability testing, but it is often impractical in its current form, because of too many test cases required to achieve desired confidence levels, especially when the software has few residual faults inside. We claim that the potential of statistical sampling methods is largely underestimated. This paper presents an adaptive sampling-based testing (AST) strategy for reliability assessment. A two-stage conceptual framework is defined, where adaptiveness is included <b>to</b> <b>uncover</b> residual <b>faults</b> earlier, while various sampling-based techniques are proposed to improve the efficiency (in terms of variance-test cases tradeoff) by better exploiting the information available to tester. An empirical study is conducted to assess the AST performance and compare the proposed sampling techniques {{to each other on}} real programs...|$|R
40|$|A {{method of}} testing for {{parametric}} faults of analog circuits based on a polynomial representation of fault-free function of the circuit is presented. The response of the circuit under test (CUT) is estimated as a polynomial in the applied input voltage at relevant frequencies in addition to DC. Classification or Cur {{is based on a}} comparison of the estimated polynomial coefficients with those of the fault free circuit. This testing method requires no design for test hardware as might be added to the circuit fly some other methods. The proposed method is illustrated for a benchmark elliptic filter. It is shown <b>to</b> <b>uncover</b> several parametric <b>faults</b> causing deviations as small as 5 % from the nominal values...|$|R
40|$|Abstract. Testing {{application}} {{behavior in}} the presence of I/O failures is extremely difficult. The resources used for testing usually work without failure. Failures typically cannot be initiated on the test suite level and are usually not tested sufficiently. Essentially, each interaction of the application with the environment can result in a failure. The Enforcer tool identifies such potential failures and automatically tests all relevant outcomes of such actions. It combines the structure of unit tests with coverage information and fault injection. By taking advantage of a unit test infrastructure, performance can be improved by orders of magnitude compared to previous approaches. This paper introduces the usage of the Enforcer tool. 1 Introduction Testing is a scalable, economic, and effective way <b>to</b> <b>uncover</b> <b>faults</b> in soft-ware [28, 29]. Even though it is limited to a finite set of example scenarios, it is very flexible and by far the most widespread quality assurance method today. Testing is often carried out without formal rigor. However, coverage measurement tools provide a quantitative measure of the quality of a test suite [14, 29]. Un-covered (and thus untested) code may still contain faults. Tested code is known to have executed successfully under at least one occasion. For this reason, it isdesirable to test each block of code at least once...|$|E
40|$|Hardware {{description}} languages (hdls) {{have been}} used in industry since the 1960 s to document and simulate hardware designs. Simulation of hdl descriptions is very useful to find design faults without the need to manufacture the design. A well known drawback of simulation, however, is that the number of input combinations (or test vectors) increases exponentially. For modern designs, it would take years to fully simulate a design. In practice a limited number of test vectors are used to probe the circuit, possibly failing <b>to</b> <b>uncover</b> <b>faults.</b> Formal Hardware Verification Research into formal hardware verification of hardware designs aims to address this problem. A circuit and its specification are given by mathematical descriptions. A mathematical proof system is then used to prove correctness of the design with respect to its specification. Although the hardware verification field is still young, notable achievements include the formally verified viper microprocessor which is manufactured and used commercially. However, most proposed methodologies are not automated, and need considerable expertise to be used. Although formal verification methods remove the exponential explosion occurring for simulation, verification takes substantial time. A severe drawback is that many different notations and tools are employed. Industrial hardware description languages have not yet been used, alienating the hardware verification field from the industrial designers...|$|E
40|$|Hardware {{description}} languages {{have been}} used in industry since the 1960 s to document and simulate hardware designs. Simulation is a very useful tool to find design faults without the need to manufacture the design. A well known drawback of simulation, however, is that the number of inputs combinations (or test vectors) increases exponentially. For modern designs, it would take years to fully simulate a design. In practice a limited number of test vectors are used to probe the circuit, possibly failing <b>to</b> <b>uncover</b> <b>faults.</b> A response to this situation has resulted in formal verification of hardware designs. This entails using a mathematical proof system to describe the design, followed by proving correctness with respect to its specification. Most proposed methodologies are not automated, and need considerable expertise to be used. Although formal verification methods remove the exponential explosion occurring for simulation, verification takes considerable time. A severe drawback is that many notations are employed, depending on the tools which are used. Industrial hardware description languages have not yet been used, alienating the hardware verification field from the industrial designers. This work aims to address this problem. To be able to use a hardware description language in conjunction with a proof system it must have a firm mathematical basis. This work ha...|$|E
40|$|Abstract—A {{method of}} testing for {{parametric}} faults of analog circuits based on a polynomial representaion of fault-free function of the circuit is presented. The response of the circuit under test (CUT) is estimated as a polynomial in the applied input voltage at relevant frequencies apart from DC. Classification of CUT {{is based on a}} comparison of the estimated polynomial coefficients with those of the fault free circuit. The method needs very little augmentation of circuit to make it testable as only output parameters are used for classification. This procedure is shown <b>to</b> <b>uncover</b> several parametric <b>faults</b> causing smaller than 5 % deviations the nominal values. Fault diagnosis based upon sensitivity of polynomial coefficients at relevant frequencies is also proposed. ...|$|R
40|$|Abstract—A {{method of}} testing for {{parametric}} faults of analog circuits based on a polynomial representation of fault-free function of the circuit is presented. The response of the circuit under test (CUT) is estimated as a polynomial in the applied input voltage at relevant frequencies in addition to DC. Classification of CUT {{is based on a}} comparison of the estimated polynomial coefficients with those of the fault free circuit. This testing method requires no design for test hardware as might be added to the circuit by some other methods. The proposed method is illustrated for a benchmark elliptic filter. It is shown <b>to</b> <b>uncover</b> several parametric <b>faults</b> causing deviations as small as 5 % from the nominal values...|$|R
40|$|Developers of {{fault-tolerant}} {{distributed systems}} must {{guarantee that the}} fault tolerance mechanisms they build are, themselves, reliable. Otherwise, these mechanisms might end up contributing negatively to overall system dependability, thus defeating the purpose of introducing fault tolerance into the system. To achieve the desired levels of reliability, the development of mechanisms for detecting and handling errors should be rigorous or formal. We present an approach to modeling and verifying fault-tolerant distributed systems that use exception handling as the main fault tolerance mechanism. The proposed approach {{is based on a}} formal model for specifying the structure of a system in terms of cooperating participants that handle exceptions in a coordinated manner. We employ coordinated atomic actions as a representative of mechanisms for exception handling in concurrent systems. We have validated the proposed approach by means of two case studies: (i) a system responsible for managing a production cell; and (ii) a medical control system. For both systems, the proposed approach helped us <b>to</b> <b>uncover</b> design <b>faults</b> in the form of implicit assumptions and omissions in the original specifications...|$|R
40|$|Abstract. Software {{requirements}} specifications (SRS) {{are often}} validated manually. One such process is inspec-tion, in which several reviewers independently analyze {{all or part}} of the specification and search for faults. These faults are then collected at a meeting of the reviewers and author(s). Usually, reviewers use Ad Hoc or Checklist methods <b>to</b> <b>uncover</b> <b>faults.</b> These methods force all reviewers to rely on nonsystematic techniques to search for a wide variety of faults. We hypothesize that a Scenario-based method, in which each reviewer uses different, systematic techniques to search for different, specific classes of faults, will have a significantly higher success rate. In previous work we evaluated this hypothesis using 48 graduate students in computer science as subjects. We now have replicated this experiment using 18 professional developers from Lucent Technologies as sub-jects. Our goals were to (1) extend the external credibility of our results by studying professional developers, and to (2) compare the performances of professionals with that of the graduate students to better understand how generalizable the results of the less expensive student experiments were. For each inspection we performed four measurements: (1) individual fault detection rate, (2) team fault detec-tion rate, (3) percentage of faults first identified at the collection meeting (meeting gain rate), and (4) percentage of faults first identified by an individual, but never reported at the collection meeting (meeting loss rate) ...|$|E
40|$|Testing is an {{indispensable}} {{part of the}} software development life cycle. It is performed to improve the performance, quality and reliability of the software. Various types of testing such as functional testing and structural testing are performed on software to uncover the faults caused by incorrect code, interaction of input parameters etc. One of the major factors in deciding the quality of testing is the design of relevant test cases which is very crucial {{for the success of}} testing. In this paper we concentrate on generating test cases <b>to</b> <b>uncover</b> <b>faults</b> caused by the interaction of input parameters. It is advisable to perform thorough testing but the number of test cases grows exponentially with the increase in number of input parameters, which makes exhaustive testing of interaction of input parameters imprudent. An alternative to exhaustive testing is combinatorial interaction testing (CIT) which requires that every t-way interaction of input parameters be covered by at least one test case. Here, we present a novel strategy ABC-CAG (Artificial Bee Colony-Covering Array Generator) based on Artificial Bee Colony (ABC) algorithm to generate covering array and mixed covering array for pair-wise testing. The proposed ABC-CAG strategy is implemented in a tool and experiments are conducted on various benchmark problems to evaluate the efficacy of the proposed approach. Experimental results show that ABC-CAG generates better/comparable results as compared to the existing state-of-the-art algorithms...|$|E
40|$|Model based {{testing is}} an {{approach}} for automatic generation of test cases {{on the basis}} of a representative model of the system. Recent studies show that model based testing has many possible advantages over manual test generation techniques including a gain in effectiveness, efficiency and reuse. The effectiveness (ability <b>to</b> <b>uncover</b> <b>faults</b> in a system) of a model based testing process is determined by the correctness of the model and by the number of requirements represented in the model. In practice, test models for model based test automation techniques are usually created from requirement or design specifications of the software and hence, these techniques overtly rely on such specifications for the completeness of the test models. This may lead to failure in testing some critical requirements specific to the application domain because the user, who helps in defining the requirements, may fail to consider certain domain specific requirements. To him some may appear to be too trivial to be specified explicitly in the requirements document and the others, he may forget. Even if the requirement is complete with domain specific requirements, testers may not realize criticality of such requirements or may find them too complex to model. In all such cases, testing is incomplete and ineffective. This dissertation describes a new model based testing technique developed to remedy such situations. The new technique is based on modeling the system under test using a strongly typed domain specific language (DSL). In the new technique, information about domain specific requirements of an application are captured automatically by exploiting properties of the DSL and are subsequently introduced in the test model. The new technique is applied to generate test cases for the applications interfacing with relational databases and the example DSL chosen for that purpose is HaskellDB. Test suites generated using the new technique are enriched with test cases addressing domain specific implicit requirements and therefore, are more effective in finding faults. This dissertation will present details of the technique and describe an experiment and a case study to explore its effectiveness, efficiency, usability and industrial applicability...|$|E
40|$|The {{preparation}} for, conduct of, {{and results}} of a simulation based fault injection experiment conducted using the AIRLAB Diagnostic Emulation facilities is described. An objective of this experiment {{was to determine the}} effectiveness of the diagnostic self-test sequences used <b>to</b> <b>uncover</b> latent <b>faults</b> in a logic network providing the key fault tolerance features for a flight control computer. Another objective was to develop methods, tools, and techniques for conducting the experiment. More than 1600 faults were injected into a logic gate level model of the Data Communicator/Interstage (C/I). For each fault injected, diagnostic self-test sequences consisting of over 300 test vectors were supplied to the C/I model as inputs. For each test vector within a test sequence, the outputs from the C/I model were compared to the outputs of a fault free C/I. If the outputs differed, the fault was considered detectable for the given test vector. These results were then analyzed to determine the effectiveness of some test sequences. The results established coverage of selt-test diagnostics, identified areas in the C/I logic where the tests did not locate faults, and suggest fault latency reduction opportunities...|$|R
40|$|Abstract This {{paper is}} an {{exposition}} of recent advances made in polynomial coefficient and V-transform coefficient based testing of parametric faults in linear and non-linear analog circuits. V-transform is a nonlinear transform {{that increases the}} sensitivity of polynomial coefficients with respect to circuit component variations by three to five times. In addition, it makes the original polynomial coefficients monotonic. Using simulation, the proposed test method is shown <b>to</b> <b>uncover</b> most parametric <b>faults</b> {{in the range of}} 5 – 15 % on a low noise amplifier (LNA) and an elliptic filter benchmark. Diagnosis of parametric faults clearly illustrates the effect of enhanced sensitivity through V-transform. Finally, we report an experimental validation of the polynomial coefficient based test scheme, with and without V-transform, using the National Instruments’ ELVIS bench-top testbed. The result demonstrates the benefit of V-transform...|$|R
40|$|Software {{testing is}} {{particularly}} expensive for developers of high-assurance software, such as software that is produced for commercial airborne systems. One {{reason for this}} expense is the Federal Aviation Administration's requirement that test suites be modified condition/decision coverage (MC/DC) adequate. Despite its cost, {{there is evidence that}} MC/DC is an effective verification technique and can help <b>to</b> <b>uncover</b> safety <b>faults.</b> As the software is modified and new test cases are added to the test suite, the test suite grows and the cost of regression testing increases. To address the test-suite size problem, researchers have investigated the use of test-suite reduction algorithms, which identify a reduced test suite that provides the same coverage of the software according to some criterion as the original test suite, and test-suite prioritization algorithms, which identify an ordering of the test cases in the test suite according to some criteria or goals. Existing test-suite reduction and prioritization techniques, however, may not be effective in reducing or prioritizing MC/DC-adequate test suites because they do not consider the complexity of the criterion. This paper presents new algorithms for test-suite reduction and prioritization that can be tailored effectively for use with MC/DC. The paper also presents the results of empirical studies of these algorithms...|$|R
