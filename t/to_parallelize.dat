1748|765|Public
5|$|Applications {{are often}} {{classified}} {{according to how}} often their subtasks need to synchronize or communicate with each other. An application exhibits fine-grained parallelism if its subtasks must communicate many times per second; it exhibits coarse-grained parallelism {{if they do not}} communicate many times per second, and it exhibits embarrassing parallelism if they rarely or never have to communicate. Embarrassingly parallel applications are considered the easiest <b>to</b> <b>parallelize.</b>|$|E
25|$|Samplesort {{can be used}} <b>to</b> <b>parallelize</b> any of the non-comparison sorts, by {{efficiently}} distributing {{data into}} several buckets and then passing down sorting to several processors, with no need to merge as buckets are already sorted between each other.|$|E
25|$|Quicksort {{has some}} {{disadvantages}} {{when compared to}} alternative sorting algorithms, like merge sort, which complicate its efficient parallelization. The depth of quicksort's divide-and-conquer tree directly impacts the algorithm's scalability, and this depth is highly dependent on the algorithm's choice of pivot. Additionally, it is difficult <b>to</b> <b>parallelize</b> the partitioning step efficiently in-place. The use of scratch space simplifies the partitioning step, but increases the algorithm's memory footprint and constant overheads.|$|E
40|$|AbstractModel {{checking}} {{procedures have}} already proved useful for system verification. They are successfully applied to find subtle bugs in complex system. However, they {{are limited to}} medium size systems because of their high space requirements. Many approaches to overcome this problem have been suggested. In recent {{years there has been}} a growing interest in parallelizing the model checking problem, thus obtaining greater space capacity and computation power. In this talk we will survey some of the approaches <b>to</b> <b>parallelizing</b> different aspects of model checking. We will consider algorithms that handle reachability, as well as LTL, CTL and Mu-calculus model checking. We will mention symbolic as well as explicit algorithms. We will also refer <b>to</b> <b>parallelizing</b> operations on BDDs, which is the data structure used in symbolic model checking...|$|R
40|$|Today's PCs {{incorporate}} multiple CPUs and GPUs and {{are easily}} arranged in clusters for high-performance, interactive graphics. We present an approach based on hierarchical, screen-space tiles <b>to</b> <b>parallelizing</b> rendering with level of detail. Adapt tiles, render tiles, and machine tiles {{are associated with}} CPUs, GPUs, and PCs, respectively, <b>to</b> efficiently <b>parallelize</b> the workload with good resource utilization. Adaptive tile sizes provide load balancing while our level of detail system allows total and independent management of the load on CPUs and GPUs. We demonstrate our approach on parallel configurations consisting of both single PCs and a cluster of PCs...|$|R
40|$|This paper {{presents}} {{two approaches}} <b>to</b> <b>parallelizing</b> the Snort network {{intrusion detection system}} (NIDS). One scheme parallelizes NIDS processing conservatively across independent network flows, while the other optimistically achieves intra-flow parallelism by exploiting the observation that certain intra-flow dependences are uncommon and may be ignored under certain circumstances. Both schemes achieve average speedup over 2 on four cores, with an average throughput over 1 Gbps on 5 traces tested...|$|R
25|$|It is {{relatively}} straightforward <b>to</b> <b>parallelize</b> {{a number of}} steps in ABC algorithms based on rejection sampling and sequential Monte Carlo methods. It has also been demonstrated that parallel algorithms may yield significant speedups for MCMC-based inference in phylogenetics, which may be a tractable approach also for ABC-based methods. Yet an adequate model for a complex system is very likely to require intensive computation irrespectively of the chosen method of inference, and {{it is up to}} the user to select a method that is suitable for the particular application in question.|$|E
25|$|The {{algorithm}} {{attempts to}} set up a congruence of squares modulo n (the integer to be factorized), which often leads to a factorization of n. The algorithm works in two phases: the data collection phase, where it collects information that may lead to a congruence of squares; and the data processing phase, where it puts all the data it has collected into a matrix and solves it to obtain a congruence of squares. The data collection phase can be easily parallelized to many processors, but the data processing phase requires large amounts of memory, and is difficult <b>to</b> <b>parallelize</b> efficiently over many nodes or if the processing nodes do not each have enough memory to store the whole matrix. The block Wiedemann algorithm can be used {{in the case of a}} few systems each capable of holding the matrix.|$|E
2500|$|The maximal {{independent}} set {{problem was}} originally {{thought to be}} non-trivial <b>to</b> <b>parallelize</b> {{due to the fact}} that the lexicographical maximal independent set proved to be P-Complete; however, it has been shown that a deterministic parallel solution could be given by an [...] reduction from either the maximum set packing or the maximal matching problem or by an [...] reduction from the 2-satisfiability problem. Typically, the structure of the algorithm given follows other parallel graph algorithms - that is they subdivide the graph into smaller local problems that are solvable in parallel by running an identical algorithm.|$|E
5000|$|The {{conditional}} {{inside this}} loop {{makes it difficult}} <b>to</b> safely <b>parallelize</b> this loop. When we unswitch the loop, this becomes: ...|$|R
40|$|This {{case study}} {{describes}} the enablement of a parallel application on Cornell Theory Center's KSR 1, a highly parallel machine from Kendall Square Research. The application is from statistics, and exposes some interesting {{facets of the}} KSR {{as well as some}} parallel programming tools and techniques. Statistics are a novel application area for supercomputing, at least at the Cornell Theory Center. First we describe the application itself, then the approaches <b>to</b> <b>parallelizing</b> it, and finally present some results...|$|R
40|$|Increasingly, {{genome-wide}} association {{studies are}} being used to identify positions within the human genome that have a link with a disease condition. The number of genomic locations studied means that computationally intensive and bioinformatic intensive solutions will have {{to be used in the}} analysis of these data sets. In this paper we present an integrated Workbench that provides user-friendly access <b>to</b> <b>parallelized</b> statistical genetics analysis codes for clinical researchers. In addition we biologically annotate statistical analysis results through the reuse of existing bionformatic Taverna workflows...|$|R
5000|$|Is it safe <b>to</b> <b>parallelize</b> the loop? Answering this {{question}} needs accurate dependence analysis and alias analysis ...|$|E
50|$|Scale-out, in-memory {{technology}} allows smart caching {{for small}} or huge datasets using more computers <b>to</b> <b>parallelize</b> the efforts, as needed.|$|E
5000|$|Likewise, {{there is}} a formula for {{combining}} the covariances of two sets {{that can be used}} <b>to</b> <b>parallelize</b> the computation: ...|$|E
40|$|We exhibit {{some simple}} gadgets useful in {{designing}} shallow parallel circuits for quantum algorithms. We prove that any quantum circuit composed entirely of controlled-not gates or of diagonal gates can be <b>parallelized</b> <b>to</b> logarithmic depth, while circuits composed of both cannot. Finally, while we note the Quantum Fourier Transform can be <b>parallelized</b> <b>to</b> linear depth, we exhibit a simple quantum circuit related {{to it that}} we believe cannot be <b>parallelized</b> <b>to</b> less than linear depth, and therefore {{might be used to}} prove that QNC < QP...|$|R
40|$|Abstract. A new {{approach}} <b>to</b> <b>parallelizing</b> harmonic balance simulation is presented. The technique leverages circuit substructure to expose potential parallelism {{in the form}} of a directed, acyclic graph (dag) of computations. This dag is then allocated and scheduled using various linear clustering techniques. The result is a highly scalable and efcient approach to harmonic balance simulation. Two large examples, one from the integrated circuit regime and another from the communication regime, executed on three di erent parallel computers are used to demonstrate the e cacy of the approach. ...|$|R
40|$|We {{demonstrate}} {{that for a}} broad class of physical systems that can be described using classical field theory, automated runtime translation of the physical equations <b>to</b> <b>parallelized</b> finite-element numerical simulation code is feasible. This allows the implementation of multiphysics extension modules to popular scripting languages (such as Python) that handle the complete specification of the physical system at script level. We discuss two example applications that utilize this framework: the micromagnetic simulation package "Nmag" {{as well as a}} short Python script to study morphogenesis in a reaction-diffusion model. Comment: 50 pages, 5 figure...|$|R
5000|$|... if: This {{will cause}} the threads <b>to</b> <b>parallelize</b> the task only if a {{condition}} is met. Otherwise the code block executes serially.|$|E
5000|$|Is it {{worthwhile}} <b>to</b> <b>parallelize</b> it? This answer {{requires a}} reliable estimation (modeling) {{of the program}} workload and {{the capacity of the}} parallel system.|$|E
50|$|Although the data—and usually {{also the}} {{computational}} cost—is doubled for interactions over the periodic boundary, {{this approach has}} the advantage of being straightforward to implement and very easy <b>to</b> <b>parallelize,</b> since cells will only interact with their geographical neighbours.|$|E
40|$|Abstract: Nowadays, {{more and}} more computations, in {{artificial}} intelligence, knowledge representation, and scientific computations to name a few, require complex data processing and sophisticated algorithms, which are NP hard. Solutions to such problems might range from succinct data representations <b>to</b> <b>parallelized</b> and incremental algorithms. In this paper Catalan related problems are discussed. For efficient computation of Catalan combinations a succinct representation is used and several algorithms are developed. Results show that the suggested approach can be successfully used for solving different Catalan problems. AMS Subject Classification: 68 P 0...|$|R
40|$|Global {{atmospheric}} circulation models (GCM) typically have three primary algorithmic components: columnar physics, spectral transform, and semi-Lagrangian transport. In developing parallel implementations, these three components are equally important {{and can be}} examined somewhat independently. A two-dimensional horizontal data decomposition of the three-dimensional computational grid leaves all physics computations on processor, and the only efficiency issues arise in load balancing. A recently completed study by the authors of different approaches <b>to</b> <b>parallelizing</b> the spectral transform showed several viable algorithms. Preliminary results of an analogous study of algorithmic alternatives for parallel semi-Lagrangian transport are described here...|$|R
50|$|Cosmology@Home uses an {{innovative}} way of using machine learning <b>to</b> effectively <b>parallelize</b> a large computational task that involves many inherently sequential calculations over an extremely {{large number of}} distributed computers.|$|R
50|$|Samplesort {{can be used}} <b>to</b> <b>parallelize</b> any of the non-comparison sorts, by {{efficiently}} distributing {{data into}} several buckets and then passing down sorting to several processors, with no need to merge as buckets are already sorted between each other.|$|E
5000|$|Parallelism {{transparency}} - The {{system is}} responsible for exploiting any ability <b>to</b> <b>parallelize</b> task execution without user knowledge or interaction. Arguably the most difficult aspect of transparency, and described by Tanenbaum as the [...] "Holy grail" [...] for distributed system designers.|$|E
5000|$|When a loop has a loop-carried dependence, one way <b>to</b> <b>parallelize</b> {{it is to}} {{distribute}} the loop into several different loops. Statements that are not dependent on each other are separated so that these distributed loops can be executed in parallel. For example, consider the following code.|$|E
30|$|Chibli’s {{implementation}} was {{tailored to}} the bioinformatics search problem. He assumed that the data and the signatures are stored locally on each processor’s disk and the matrix computation is the part that needs <b>to</b> be <b>parallelized.</b> <b>To</b> evaluate the implementation, a workstation of 13 processors was used to run the experiments. We re-implement Chibli’s approach in this paper for comparison purposes. We managed to achieve a significant speedup over Chibli’s work using Phoenix and MAPCG.|$|R
5000|$|IEEE Fellow, for {{contributions}} <b>to</b> optimizing and <b>parallelizing</b> compilers. 2005 ...|$|R
5000|$|Loop skewing: By [...] "skewing" [...] {{the logical}} {{shape of an}} array, this loop {{optimization}} can (when combined with loop interchange) eliminate loop-carried dependencies, allowing an inner loop <b>to</b> be <b>parallelized.</b>|$|R
50|$|The Example 4 above {{makes it}} {{impossible}} to predict anything from that loop. Unless the functions themselves are trivial (constant), {{there is no way to}} know where the loop will start, stop and how much it'll increment each iteration. Those loops are not only hard <b>to</b> <b>parallelize,</b> but they also perform horribly.|$|E
5000|$|Another {{disadvantage}} of SCU {{is that it}} is serial, compiling all included source files in sequence in one process, and thus cannot be parallelized, as can be done in separate compilation (via distcc or similar programs). Thus SCU requires explicit partitioning (manual partitioning or [...] "sharding" [...] into multiple units) <b>to</b> <b>parallelize</b> compilation.|$|E
50|$|A {{subset of}} {{traditional}} applications are often difficult <b>to</b> <b>parallelize</b> and {{make use of}} additional CPU hardware available on the platform, restraining applications to use only one CPU. Core Multiplexing Technology would allow for a process to be split into multiple threads at compilation time and execution time {{by the introduction of}} speculative multithreading.|$|E
40|$|We {{propose a}} {{definition}} of QNC, the quantum analog of the efficient parallel class NC. We exhibit several useful gadgets and prove that various classes of circuits can be <b>parallelized</b> <b>to</b> logarithmic depth, including circuits for encoding and decoding standard quantum error-correcting codes, or more generally any circuit consisting of controlled-not gates, controlled pi-shifts, and Hadamard gates. Finally, while we note the Quantum Fourier Transform can be <b>parallelized</b> <b>to</b> linear depth, we conjecture that an even simpler `staircase' circuit cannot be <b>parallelized</b> <b>to</b> less than linear depth, and {{might be used to}} prove that QNC < QP...|$|R
40|$|Parallel {{processing}} and applied mathematicsOpenMosix as the computing platform is presented {{and applied to}} the Monte Carlo study of continuous phase transitions, whose character was not unambigously resolved yet, in 3 D Ashkin-Teller model on a cubic lattice. Calculating the critical exponent y(h), we show that these phase transitions are not of Ising character and we give some indications to establish their universality class. It is demonstrated that OpenMosix ensures stability and good load-balancing of the Linux PCs cluster, but the best performance is achieved when applying it <b>to</b> <b>parallelized</b> jobs, which allowed us to perform calculations with larger samples. status: publishe...|$|R
40|$|It {{has been}} shown that the {{combination}} of adaptive grid refinement and multigrid solution, known as adaptive multilevel methods, provide effective methods for solving partial differential equations on sequential computers. Recently, research has been performed on parallelizing these procedures. Effective parallelization is difficult because of the irregular nature of both adaptively refined grids and the multigrid process. This is particularly true on cluster computers which have slow communication channels that require algorithms with infrequent communication. An approach <b>to</b> <b>parallelizing</b> adaptive multilevel methods with few communication steps is presented. Numerical results on an 8 -processor PC cluster demonstrate 60 - 90 % efficiency...|$|R
