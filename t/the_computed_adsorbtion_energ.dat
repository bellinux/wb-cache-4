0|10000|Public
40|$|Abstract: This article {{examines}} the development of <b>the</b> <b>computing</b> field. Our account considers <b>the</b> <b>computing</b> field in four stages: infancy (1935 - 1950), childhood (1950 - 1970), adolescence (1970 - 1990), and young adulthood (1990 - 2010). <b>The</b> <b>computing</b> profession {{is a product of}} the fourth stage. <b>The</b> relationships between <b>computing</b> and other fields are vitally important...|$|R
30|$|Comprehensively {{considering}} <b>the</b> <b>computing</b> {{efficiency and}} <b>the</b> <b>computing</b> accuracy, <b>the</b> ABdC-N scheme {{can be more}} effective to solve the quanto option pricing problems.|$|R
50|$|<b>The</b> <b>Computing</b> Scale Company of America was {{a holding}} company, {{organized}} in 1901 and amalgamating as subsidiary companies <b>The</b> <b>Computing</b> Scale Company, Dayton, Ohio; The Moneyweight Scale Company, Chicago, Illinois; The W.F. Simpson Company of Detroit, Michigan; and <b>The</b> Stimpson <b>Computing</b> Scale Company of Elkhart, Indiana.In 1891, Edward Canby and Orange O. Ozias, two businessmen from Dayton, Ohio, purchased the patents for <b>the</b> newly invented <b>computing</b> scale and incorporated <b>the</b> <b>Computing</b> Scale Company {{for the production}} of commercial scales.|$|R
40|$|During {{the past}} 5 years, <b>the</b> <b>computing</b> {{environment}} at Carnegie-Mellon University, CMU, t has been undergoing major evolution {{from the central}} data center model to one of distributed processing. Until the late 70 's, with few exceptions such as the Computer Science department, most departments relied on <b>the</b> <b>computing</b> resources provided by <b>the</b> <b>Computing</b> Center. Sinc...|$|R
40|$|Since its {{inception}} in the 1930 s, <b>the</b> <b>computing</b> field has passed through childhood, adolescence, young adulthood, and maturity. In its maturity it is a complex of fields gathered under a large umbrella sometimes called Computing and sometimes Information Technology body of knowledgeApril 2008 (rev 9 / 14 / 08) This article examines the development of <b>the</b> <b>computing</b> field. Our account considers <b>the</b> <b>computing</b> field in four stages: infancy (1935 - 1950), childhood (1950 - 1970), adolescence (1970 - 1990), and young adulthood (1990 - 2010). <b>The</b> <b>computing</b> profession {{is a product of}} the fourth stage. <b>The</b> relationships between <b>computing</b> and other fields are vitally important...|$|R
50|$|From 2007 to 2013 {{he served}} as Founding Chair of <b>the</b> <b>Computing</b> Community Consortium, a {{national}} effort to engage <b>the</b> <b>computing</b> research community in fundamental research motivated by tackling societal challenges.|$|R
50|$|A major {{drawback}} of <b>the</b> <b>Computing</b> Surface architecture {{was poor}} I/O bandwidth for general data shuffling. Although aggregate bandwidth for special case data shuffling {{could be very}} high, the general case has very poor performance relative to <b>the</b> <b>compute</b> bandwidth. This made <b>the</b> Meiko <b>Computing</b> Surface uneconomic for many applications.|$|R
40|$|AbstractStraightforward {{solution}} of discrete ill-posed least-squares problems with error-contaminated data does not, in general, give meaningful results, because propagated error destroys <b>the</b> <b>computed</b> solution. Error propagation {{can be reduced}} by imposing constraints on <b>the</b> <b>computed</b> solution. A commonly used constraint is the discrepancy principle, which bounds the norm of <b>the</b> <b>computed</b> solution when applied in conjunction with Tikhonov regularization. Another approach, which recently has received considerable attention, is to explicitly impose a constraint on the norm of <b>the</b> <b>computed</b> solution. For instance, <b>the</b> <b>computed</b> solution {{may be required to}} have the same Euclidean norm as the unknown {{solution of}} the error-free least-squares problem. We compare these approaches and discuss numerical methods for their implementation, among them a new implementation of the Arnoldi–Tikhonov method. Also solution methods which use both the discrepancy principle and a solution norm constraint are considered...|$|R
5000|$|Samarendra Kumar Mitra was the Founder of <b>the</b> <b>Computing</b> Machines and Electronics Division at the Indian Statistical Institute (ISI), Calcutta and {{was also}} the first to Head <b>the</b> <b>Computing</b> Machines and Electronics Laboratory at ISI, Calcutta.1 ...|$|R
3000|$|... is {{the maximum}} {{probable}} absolute error. The absolute error {{is the absolute}} difference of the actual aggregate from <b>the</b> <b>computed</b> aggregate. <b>The</b> maximum probable absolute error is the maximum possible absolute difference that the actual aggregate and <b>the</b> <b>computed</b> aggregate can have. Note that the convergence of accuracy is particularly interesting {{for the evaluation of}} DIAS as it outlines its speed and adaptivity in <b>the</b> <b>computed</b> aggregates. Matching μ is based on the calculation of the correlation coefficient and indicates the closeness of the distribution of <b>the</b> <b>computed</b> aggregates to <b>the</b> distribution of the actual aggregates. This metric is especially useful for the evaluation of DIAS under asynchronous changes.|$|R
40|$|The fungal {{index is}} a {{biological}} climate-parameter, {{which represents the}} environmental capacity to allow fungal growth. The author developed software that determines <b>the</b> <b>computed</b> fungal index, which was estimated using the Excel software "INDEX " from the measured temperature and relative humidity. <b>The</b> <b>computed</b> fungal index and the measured fungal index, determined using a fungal detector encapsulating fungal spores, were determined in 10 rooms in six dwelling houses. <b>The</b> <b>computed</b> fungal index values were generally close {{to those of the}} measured fungal index in rooms. <b>The</b> <b>computed</b> and measured fungal index values showed a positive correlation, and the correlation coefficient between these fungal index values was 0. 91 (p= 0. 01) ...|$|R
50|$|<b>The</b> <b>computing</b> center.|$|R
40|$|Straightforward {{solution}} of discrete ill-posed least-squares problems with error-contaminated data does not, in general, give meaningful results, because propagated error destroys <b>the</b> <b>computed</b> solution. Error propagation {{can be reduced}} by imposing constraints on <b>the</b> <b>computed</b> solution. A commonly used constraint is the discrepancy principle, which bounds the norm of <b>the</b> <b>computed</b> solution when applied in conjunction with Tikhonov regularization. Another approach, which recently has received considerable attention, is to explicitly impose a constraint on the norm of <b>the</b> <b>computed</b> solution. For instance, <b>the</b> <b>computed</b> solution {{may be required to}} have the same Euclidean norm as the unknown {{solution of}} the error-free least-squares problem. We compare these approaches and discuss numerical methods for their implementation, among them a new implementation of the Arnoldi-Tikhonov method. Also solution methods which use both the discrepancy principle and a solution norm constraint are considered. Key words: Ill-posed problem, regularization, solution norm constraint, Arnoldi-Tikhonov, discrepancy principl...|$|R
30|$|<b>The</b> <b>computed</b> entropy is also {{listed in}} Table  4. From <b>the</b> <b>computed</b> entropy using Eq. (40), {{it is seen}} that the Gumbel–Hougaard copula yielded the highest mutual {{information}} (the absolute value of the copula entropy) among all the copula candidates.|$|R
5000|$|... #Caption: <b>The</b> Blue Gene <b>compute</b> hierarchy. A CNK {{instance}} runs {{on each of}} <b>the</b> <b>compute</b> nodes.|$|R
30|$|If <b>the</b> <b>computed</b> {{ratio of}} (Ca + Mg)/SO 4 fall within 0.8 – 1.2 {{indicating}} dedolomitization. The result reflects {{that there is}} an effective dedolomitization process indicated by about 70  % of <b>the</b> <b>computed</b> ratios of this function for the groundwater samples.|$|R
40|$|Abstract. FMM is an {{efficient}} algorithm in computing N-body problem. This paper firstly subdivides the FMM into 10 procedures. Based on <b>the</b> analysis <b>the</b> <b>computing</b> type of each procedure, we choose key procedures accelerated on FPGA, GPU and Cell BE. And then we present the speedup ratio of each accelerated procedure through experiments. Finally we analyze <b>the</b> <b>computing</b> characteristic of FMM on <b>the</b> <b>computing</b> architecture on accelerator FPGA and GPU {{on the side}} of P, M and C...|$|R
30|$|From Figure  8 and Table  6 we {{see that}} {{when the number of}} grid points we need {{calculated}} is greater than a certain range, the ASE-I and ASI-E schemes of this paper show a clear superiority in computation time. With the increase of the grid number, <b>the</b> <b>computing</b> time of <b>the</b> difference schemes rises for the nonlinear Leland equation. But the increased amplitude of <b>the</b> <b>computing</b> time of <b>the</b> C-N scheme is greater than that of the ASE-I, ASC-N schemes. <b>The</b> <b>computing</b> time of <b>the</b> ASE-I scheme saves nearly 81 % for the C-N scheme by calculating and saves nearly 40 % for the ASC-N scheme, showing <b>the</b> <b>computing</b> efficiency of <b>the</b> ASE-I scheme is best.|$|R
50|$|A {{lease on}} a house was {{obtained}} in 1957 and operation started in 1958, initially as <b>the</b> <b>Computing</b> Laboratory at 9 South Parks Road, a Victorian building, now demolished {{to make way for}} the Experimental Psychology and Zoology departments. In 1963, due to space problems, the staff and computers moved to 19 Parks Road, the old Engineering Building. In 1970, <b>the</b> <b>Computing</b> Service occupied 17 and 19 Banbury Road, having split from <b>the</b> <b>Computing</b> Laboratory, which became the academic computer science department for the University. By 1975, <b>the</b> <b>Computing</b> Service had taken over all of 7 to 19 Banbury Road, as it still does today. An outpost at 59 George Street in central Oxford closed in the mid 1990s.|$|R
40|$|The {{research}} {{documented in}} this paper attempted {{to answer the question}} of how relevant the content of <b>the</b> <b>Computing</b> courses offered within programs of <b>the</b> <b>Computing</b> Department at <b>the</b> National University of Samoa (NUS) were {{to meet the needs of}} industry and the workforce. The RINCCII study which was conducted in 2013 to 2014, surveyed 13 institutions and 19 graduates from <b>the</b> <b>Computing</b> programs. Findings from the survey indicated that the current course offerings within <b>the</b> <b>Computing</b> department are relevant to the needs of industry and the workplace. However there are aspects or topics which need inclusion or better coverage. The study also recommended regular surveys to gauge relevance of curricula to needs of industry...|$|R
40|$|The {{ubiquity of}} <b>the</b> cloud <b>computing</b> allows <b>the</b> mobile devices to connect and use <b>the</b> {{traditional}} cloud <b>computing</b> services [1]. However, unlike <b>the</b> normal <b>computing</b> machines, <b>the</b> mobile devices are resource constrained. The precincts of low processing power, less storage capacity, limited energy, and the capricious internet connectivity {{does not allow}} <b>the</b> <b>compute</b> and storag...|$|R
40|$|Abstract. Straightforward {{solution}} of discrete ill-posed least-squares problems with errorcontaminated data does not, in general, give meaningful results, because propagated error destroys <b>the</b> <b>computed</b> solution. Error propagation {{can be reduced}} by imposing constraints on <b>the</b> <b>computed</b> solution. A commonly used constraint is the discrepancy principle, which bounds the norm of <b>the</b> <b>computed</b> solution when applied in conjunction with Tikhonov regularization. Another approach, which recently has received considerable attention, is to explicitly impose a constraint on the norm of <b>the</b> <b>computed</b> solution. For instance, <b>the</b> <b>computed</b> solution {{may be required to}} have the same Euclidean norm as the unknown {{solution of}} the error-free least-squares problem. We compare these approaches and discuss numerical methods for their implementation, among them a new implementation of the Arnoldi–Tikhonov method. Also solution methods which use both the discrepancy principle and a solution norm constraint are considered. Key words. Ill-posed problem, regularization, solution norm constraint, Arnoldi–Tikhonov, discrepancy principle AMS subject classifications. 65 F 10, 65 F 22, 65 R 30. 1. Introduction. Minimizatio...|$|R
50|$|In 1978 {{he joined}} Bell Laboratories {{and served as}} Director of <b>the</b> <b>Computing</b> Principles Research Department {{starting}} from 1991 until 2001, when he left Bell laboratories and joined Avaya Laboratories. There he served as Director of <b>the</b> <b>Computing</b> Principles Research Department until 2002.|$|R
5000|$|She {{became a}} {{director}} of <b>the</b> <b>computing</b> center at <b>the</b> School of Electrical Engineering of Belgrade University in 2001, that was quite devastated at the time by the anti-university politics during the previous decade. She helped <b>the</b> <b>computing</b> center reach <b>the</b> world standard.|$|R
40|$|Abstract: Grid {{computing}} {{is nothing}} but <b>the</b> <b>computing</b> {{environment in which the}} resources are shared by multiple systems to obtain a goal. In day today life performance analysis of very large data sets in <b>the</b> <b>computing</b> environment is necessary in many applications. The distributed grids are formed from <b>the</b> <b>computing</b> resources of multiple individuals or multiple administrative domains. This can make easier to perform commercial transactions. This paper is an introduction to the Grid infrastructure and the potential for machine learning tasks...|$|R
50|$|<b>The</b> <b>Computing</b> Community Consortium (CCC) is an {{organization}} whose goal is to catalyze and empower <b>the</b> U.S. <b>computing</b> research community to pursue audacious, high-impact research.|$|R
40|$|Abstract. A new {{computational}} method of preloading the earthwork {{has been proposed}} in this paper. Firstly, the various factors affecting <b>the</b> <b>computed</b> method for <b>the</b> foundation of the average degree of consolidation have been analyzed. Secondly, the settlement prediction method has been described according to the settlement to calculate the degree of consolidation. Then the consolidation coefficient has been calculated according to the least square method. <b>The</b> <b>computed</b> algorithm <b>compute</b> <b>the</b> preloading height based on the Takagi Shunsuke method and the measured data is presented. Finally, two examples illustrate the validity and the feasibility of <b>the</b> <b>computed</b> method in this paper...|$|R
3000|$|... are {{integrated}} of order one, or I(1). Therefore, if <b>the</b> <b>computed</b> F-statistic {{is smaller than}} the lower bound value, then the null hypothesis is not rejected and we conclude {{that there is no}} long-run relationship between poverty and its determinants. Conversely, if <b>the</b> <b>computed</b> F-statistic is greater than the upper bound value, then agriculture expenditure and its determinants share a long-run level relationship. On the other hand, if <b>the</b> <b>computed</b> F-statistic falls between the lower and upper bound values, then the results are inconclusive.|$|R
40|$|Effective atomic numbers' (Zeff) {{effective}} {{electron density}} (Nel) for human organs and tissues have been <b>computed</b> in <b>the</b> energy region of 1 keV to 100 GeV using WinXCOM. <b>The</b> <b>computed</b> data of Zeff and Nel are tabulated. <b>The</b> <b>computed</b> values are compared with previous results. <b>The</b> <b>computed</b> data of Zeff and Nel {{for almost all}} tissues (34 tissues of different human organs) in the given energy range are not available in literature and find application in radiotherapy and dosimetry. Copyright Â© 2013 Health Physics Society...|$|R
40|$|When using {{thousands}} of processors simultaneously, the application developer {{can no longer}} assume that <b>the</b> <b>computing</b> platform is failure free. The probability that one of <b>the</b> <b>computing</b> processors crashes drastically increases {{with the number of}} processors [9]. Safety issues not only occur in large scale computin...|$|R
40|$|This essay {{reviews the}} history of <b>the</b> <b>computing</b> {{profession}} from its beginnings many thousands of years ago, describes the persistent bifurcation between reckoning and accounting, describes also {{the history of}} the printing profession, and points out lessons <b>the</b> <b>computing</b> profession might learn from the printing profession...|$|R
40|$|The {{contribution}} estimates different geotechnical {{profiles of}} site condition change and their influences on <b>the</b> <b>computed</b> seismic response spectra and time histories final values and forms applying on the seismic structures loading. The mentioned problems methodics attitude solution {{is based on}} <b>the</b> <b>computed</b> seismic motion parameters...|$|R
5000|$|Furthermore, <b>the</b> <b>computed</b> [...] gives <b>the</b> cluster indicator, i.e.,if , {{that fact}} {{indicates}} input data belongs to [...] cluster. And <b>the</b> <b>computed</b> [...] gives <b>the</b> cluster centroids, i.e., the [...] column gives the cluster centroid of cluster. This centroids representation can be significantly enhanced by convex NMF.|$|R
50|$|<b>The</b> <b>Computing</b> Services Centre is the Consultancy {{arm of the}} UCSC. It was {{established}} in 1990 to provide Consultancy Services to the IT and related industries. The activities undertaken by <b>the</b> <b>Computing</b> Services Centre are threefold. Those are short term training courses, software development and consultancy services.|$|R
40|$|In this paper, <b>the</b> grid <b>computing</b> {{system is}} seemed as an ecosystem. The {{object of the}} {{optimization}} resource management is to promote the balance and evolution of <b>the</b> <b>computing</b> ecosystem. <b>The</b> architecture of the ecosystem model based grid resource management system is presented, which has the self-aware and selfoptimization mechanism. The knowledge discovery based self-aware mechanism {{has the ability to}} reveal the behavior patterns knowledge hide in the history grid information system. The discovered knowledge can be used to predict the resource requirement and to optimize the resource allocation. The antigen identification mechanism is studied which can identify the factors related to <b>the</b> <b>computing</b> ecosystem unbalance state. With the self-optimization mechanism of <b>the</b> <b>computing</b> ecosystem, <b>the</b> resource allocation problem can be abstracted as a multi-objects optimization problem. <b>The</b> <b>computing</b> expectation, ecosystem environment, and the application characteristic are considered to design policy based adaptive resource allocation and job scheduling algorithm. Key words...|$|R
5000|$|... #Article: <b>The</b> <b>Computing</b> Centre of Madrid Complutense University ...|$|R
