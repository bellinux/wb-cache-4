47|342|Public
5000|$|Dictionary Methods: the {{researcher}} pre-selects {{a set of}} keywords (n-gram) for each category. The machine then uses these keywords to classify each <b>text</b> <b>unit</b> into a category.|$|E
50|$|The unique {{identifiers}} are remembered during translation {{so that the}} target language document is 'exactly' aligned at the <b>text</b> <b>unit</b> level. If the source document is subsequently modified, then those text units that have not changed can be directly transferred to the new target version of the document {{without the need for}} any translator interaction. This is the concept of 'exact' or 'perfect' matching to the translation memory. xml:tm can also provide mechanisms for in-document leveraged and fuzzy matching.|$|E
5000|$|TextRank is {{a general}} purpose {{graph-based}} ranking algorithm for NLP. Essentially, it runs PageRank on a graph specially designed for a particular NLP task. For keyphrase extraction, it builds a graph using some set of text units as vertices. Edges are based on some measure of semantic or lexical similarity between the <b>text</b> <b>unit</b> vertices. Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. Once the graph is constructed, {{it is used to}} form a stochastic matrix, combined with a damping factor (as in the [...] "random surfer model"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).|$|E
30|$|Thirdly, the {{transcription}} of the Corning manuscript used by Johnson [10] is not especially reliable because Smith and Hawthorne [1] only split around 92 <b>text</b> <b>units</b> out of 293 of the <b>text</b> <b>units.</b>|$|R
3000|$|... ii. The {{sequence}} of thirteen recipes missing of the Lucca 490 manuscript is present, in a balanced order, {{in at least}} four witnesses of the two traditions and one exemplar of the Compositiones tradition. An initial comparison shows the following results: all 13 <b>text</b> <b>units</b> are present in London, British Library, Add. 41486 (ff. 96 r– 97 r), Sélestat, Bibliothèque Humaniste, 17 (ff. 48 r– 49 r), and Corning, Phillipps 3175 (ff. 37 r– 38 v). Ten out of 13 <b>text</b> <b>units</b> are held in Oxford, Bodleian Library, Bodley 679 (ff. 29 r– 29 v). The manuscript Città del Vaticano, Biblioteca Apostolica Vaticana, Reg. Lat. 2079, (ff. 78 r– 78 v) of the Compositiones tradition includes at least 11 out of 13 <b>text</b> <b>units.</b>|$|R
3000|$|... i. None of {{the texts}} of the Mappæ Clavicula’s {{critical}} edition correspond {{to any of the}} thirteen Klosterneuburg <b>text</b> <b>units.</b>|$|R
30|$|The phrase ‘text unit’ {{has been}} chosen {{to refer to}} any single text, {{in place of the}} more common, yet {{misleading}} term, ‘recipe’. In such a way, it becomes possible to include both prescriptive—i.e. true recipes—and descriptive text segments. A <b>text</b> <b>unit</b> refers to a meaningful text segment, which is often but not necessarily separated from the preceding and successive text segments with blank spaces or other palaeographical devices (title, rubrications, large initials, etc.). But this first step may be insufficient and, as a further step, one should compare a given <b>text</b> <b>unit</b> with its witnesses. This will permit one to verify whether the text at hand {{is the result of a}} fusion of two or more text units, or the splitting of a <b>text</b> <b>unit</b> into two or more segments, or to confirm the initial evaluation obtained in the first step.|$|E
30|$|In summary, the {{attribution}} of this fragment to the Mappæ or the Compositiones traditions involves different {{meanings of the}} same terms or labels, different procedures of assignation of a given <b>text</b> <b>unit</b> {{to one of the}} two traditions, and different empirical bases.|$|E
30|$|The {{position}} of each <b>text</b> <b>unit</b> inside the manuscript {{is indicated by}} a progressive integer, which starts from the first text. For the Corning and Lucca 490 manuscripts we used the published indexes [1, 37], the latter being updated (see note 11).|$|E
40|$|We {{introduce}} five {{methods for}} summarizing parts of Web pages on handheld devices, such as personal digital assistants (PDAs), or cellular phones. Each Web page is broken into <b>text</b> <b>units</b> that can each be hidden, partially displayed, made fully visible, or summarized. The methods accomplish summarization by different means. One method extracts significant keywords from the <b>text</b> <b>units,</b> another {{attempts to find}} each <b>text</b> <b>unit's</b> most significant sentence {{to act as a}} summary for the unit. We use information retrieval techniques, which we adapt to the World-Wide Web context. We tested the relative performance of our five methods by asking human subjects to accomplish single-page information search tasks using each method. We found that the combination of keywords and single-sentence summaries provides significant improvements in access times and number of pen actions, as compared to other schemes...|$|R
40|$|The {{description}} of the language "superior" units becomes an important scientific subject {{at the end of}} the XXth century. This description becames a subject of many disciplines as a synthetic or analytical strategy. The analytical strategy of study of the <b>text</b> superior <b>units</b> helps to identify the supersentence and the semantic and syntactic block as <b>text</b> <b>units.</b> ...|$|R
40|$|Our {{summarization}} {{approach is}} based on the assumption that quality of the summary is influenced by a set of factors, dependent on lexical and grammatical features of <b>text</b> <b>units</b> selected and arranged while composing the summary. The system has been developed with taking into account six factors influencing the final quality: compliance with the genre "summary", relevance, focusing, compliance with the requested granularity, topic coverage, and cohesion-based ordering. Preliminary selected <b>text</b> <b>units</b> get positive or negative scores depending on the relative impact of features that are responsible for the respective factors. Balancing the impacts of different text features was performed on empirical basis. After initial ranking of <b>text</b> <b>units</b> by the first four factors, they are ordered so that to achieve higher coverage and cohesion. The top-ranked passages, whose total words number does not exceed the required size, are submitted as the resulting summary...|$|R
40|$|The {{purpose of}} this paper is to explore the {{external}} disclosure of human capital (HC). This study aims to examine the nature of human capital reporting of public listed companies in Malaysia, investigate the motivational motive towards the extent of human capital reporting from preparer’s point of view and identify the problems that might be faced in reporting human capital information. This study is based on a cross-sectional examination of disclosures on HCR by 20 public listed companies in Malaysia. The information about human capital presented in annual reports is grouped into five themes defined in the literature. A content analysis of these 20 annual reports is conducted to measure the nature and level of HCR. Besides the number of sentence count, narrative <b>text</b> <b>unit</b> adopted from Beattie et al. (2004) is also applied as unit of analysis. A questionnaire survey for HR managers of the Malaysian listed companies is conducted to find out the respondents’ motivation to disclose information in respect of reported items of HCR. Responses are compared with the results obtained from the measured HCR through content analysis. The findings demonstrate the most reported items of human capital, a comprehensive and descriptive profile of narrative disclosures of human capital items as well as the rank of motives for disclosures. Trading and the services industry have the highest score for the number of sentence count and the narrative <b>text</b> <b>unit.</b> The most reported items by sentence and narrative <b>text</b> <b>unit</b> are the working environment and employee’s involvement in the community...|$|E
3000|$|... [Om] Oxford, Magdalen College, 173, fourteenth century, England, parchment, 264 folios, 27 – 33 {{lines in}} a single column, ff. 79 r– 95 v in two columns. It is an alchemical and medical codex that {{includes}} texts from the De Coloribus et Mixtionibus, Mappæ Clavicula, Compositiones Lucenses tradition, with excerpts from the first book of De diversis artibus by Theophilus. One <b>text</b> <b>unit</b> from Mappæ is inserted within the transcription of the Compositiones tradition.|$|E
40|$|SUMMARY We {{propose a}} definitional {{question}} answering system that extracts phrases using syntactic patterns which are easily constructed manually and {{can reduce the}} coverage problem. Experimental results show that our phrase extraction system outperforms a sentence extraction system, especially for selecting concise answers, in terms of recall and precision, and indicate that the proper <b>text</b> <b>unit</b> of answer candidates and the final answer has {{a significant effect on}} the system performance. key words: definitional question answering, phrase extraction 1...|$|E
40|$|The most of {{existing}} text compression methods {{is based on}} the same base concept. First the Input text is divided into sequence of <b>text</b> <b>units.</b> These <b>text</b> <b>units</b> cat be single symbols, syllables or words. When compressing large text files, searching for redundancies over longer <b>text</b> <b>units</b> is usually more effective than searching over the shorter ones. But if we choose words as base units we cannot anymore catch redundancies over symbols and syllables. In this paper we propose a new text compression method called Hierarchical compresssion. It constructs hierarchical grammar to store redundancies over syllables, words and upper levels of text. The code of the text then consists of code of this grammer. We proposed a strategy for constructing hierarchical grammar for concrete input text and we proposed an effective way how to encode it. Above mentioned our proposed method is compared with some other common methods of text compression...|$|R
40|$|We {{present a}} design for {{displaying}} and manipulating HTML pages on small handheld {{devices such as}} personal digital assistants (PDAs), or cellular phones. We introduce methods for summarizing parts of Web pages. Each page is broken into <b>text</b> <b>units</b> that can each be hidden, partially displayed, made fully visible, or summarized. A variety of methods are introduced that summarize the <b>text</b> <b>units.</b> We found {{that the combination of}} keywords and single-sentence summaries provides significant improvements in access times and number of required pen actions, as compared to other schemes...|$|R
40|$|Modern {{organizations}} are accumulating huge volumes of textual documents. To turn archives into valuable know-ledge sources, textual content must become explicit and queryable. Semantic tagging with markup languages such as XML satisfies both requirements. We thus introduce the DIAsDEM framework for extracting semantics from struc-tural <b>text</b> <b>units</b> (e. g., sentences), assigning XML tags {{to them and}} deriving a flat XML DTD for the archive. DIAsDEM fo-cuses on archives characterized by a peculiar terminology and by an implicit structure such as court filings and com-pany reports. In the knowledge discovery phase, <b>text</b> <b>units</b> are iteratively clustered by similarity of their content. Each iteration outputs clusters satisfying a set of quality criteria. <b>Text</b> <b>units</b> contained in these clusters are tagged with semi-automatically determined cluster labels and XML tags re-spectively. Additionally, extracted named entities (e. g., per-sons) serve as attributes of XML tags. We apply the frame-work in a case study on the German Commercial Register. 1...|$|R
30|$|In {{order to}} obtain Table  3, we firstly indexed every <b>text</b> <b>unit</b> of each {{manuscript}} with a progressive number, so that it became possible to obtain the consecutio (the sequence or ordinal structure) 18 of the text units {{within each of the}} four manuscripts examined here and their respective corresponding folios. The first column of Table  3 refers to the name of the textual nucleus, and the second to the title of the text units, while columns 3 – 6 indicate the relevant folio and the position of each text-unity within its own manuscript.|$|E
40|$|A {{growing body}} of works address {{automated}} mining of biochemical knowledge from digital repositories of scientific literature, such as MEDLINE. Some of these works use abstracts as the unit of text from which to extract facts. Others use sentences for this purpose, while still others use phrases. Here we compare abstracts, sentences, and phrases in MEDLINE using the standard information retrieval performance measures of recall, precision, and effectiveness, for the task of mining interactions among biochemical terms based on term co-occurrence. Results show statistically significant differences that can impact the choice of <b>text</b> <b>unit.</b> ...|$|E
40|$|The article {{highlights}} stylistic and syntactic {{devices of}} expansion, which act as compositional means, vary normative syntactic structure of an advertising text, contribute to sense formation, creating {{conditions for the}} purpose of advertiser’s intent. By means of these language elements expressing invariant tactic sense the advertiser consciously expands and/or complicates the informative complex of dicteme, an acting <b>text</b> <b>unit,</b> transmitting superfluous impressive information together with factual one. Combination of factual and impressive items of information activates both rational and emotional perceptional channels of prospective consumer, intensifies the positioning process of an advertised article...|$|E
3000|$|... [K] Klosterneuburg, Stiftsbibliothek, W. 8.293, ninth century, France, Belgium or Germany, parchment, 27 {{lines in}} a single column, 30 texts. This fragmentary {{manuscript}} was probably formed from at least 119 <b>text</b> <b>units.</b>|$|R
3000|$|Two further {{witnesses}} {{contain a}} few <b>text</b> <b>units</b> of the Compositiones (Nos. 25 – 26). They represent, {{together with the}} above-mentioned manuscripts Nos. 10 – 12, the fragmentary tradition of the Compositiones Lucenses. 10 [...]...|$|R
30|$|Let {{us first}} {{deal with the}} <b>text</b> <b>units</b> {{recorded}} in the Lucca 490 manuscript. We updated the latest edition edited by Caffaro of the Lucca 490 manuscript, which records 160 texts: four of these are evaluated as ‘texts of uncertain origin’ (on ff. 223 r– 223 v, Nos. 77 – 80, [37]), because {{they are likely to}} have derived from a different, ancient translation of the same textual material that can be read in Mappæ ([38], p. 47; see [29], <b>text</b> <b>units</b> Nos. xxxvii, xliv, xlii, xlvii). We further divided three recipes, 11 and the final result consists of 160 texts belonging to the Compositiones tradition.|$|R
40|$|Abstract — Online social {{networks}} usage are pervasive now a days. Mining the text present in online {{social networks}} {{will be useful}} for predictive analytic. Predicting information from unstructured data present in the social networks is a challenging research problem. Extracting, identifying or otherwise characterizing the sentiment content of the <b>text</b> <b>unit</b> using statistics and machine learning methods are referred as sentiment analysis or text analysis. In this work sentiment analysis using Decision trees and Support vector machines, which are machine learning algorithms will be demonstrated using WEKA tool. Sentiment analysis using Support vector machines showed high accuracy when compared to Decision trees...|$|E
40|$|Deep {{analysis}} of the SRRs from Web databases (WDB) plays major important role to find exact searching for each user and improve the web pages by analysis results. The number of web pages returned from the WDB is known search result records (SRRs); it consists of the several numbers of pages. These returned web pages results becomes complex to analysis each data unit since it consists of several number of data unit. To overcome the problem of web analysis in earlier work proposes an annotation methods where the data and <b>text</b> <b>unit</b> nodes are converted i...|$|E
40|$|A {{growing body}} of work {{addresses}} automated mining for biochemical information from digital repositories of scientific literature such as MEDLINE. Some of this work uses abstracts as the unit of text from which to extract facts. Other work uses sentences for this purpose, while still other work uses phrases. Here, we compare abstracts, sentences, and phrases in MEDLINE using the standard information retrieval performance measures of recall, precision, and effectiveness for the task of mining interactions among biochemical terms based on term cooccurrence. Results show statistically significant differences that can impact the choice of <b>text</b> <b>unit,</b> although no one of these three text units is unambiguously superior to the others. ...|$|E
30|$|The {{following}} three manuscripts belong {{only to the}} Compositiones Lucenses tradition: they are fundamental witnesses of the tradition, two of these include large amounts of <b>text</b> <b>units,</b> and two are very old. All three are independent from the Mappæ tradition.|$|R
3000|$|In the {{following}} paragraphs, we record the titles, the progressive {{number of the}} <b>text</b> <b>units,</b> and the relevant folios from the Vatican Reg. lat. 2079 of the remaining four nuclei (Nos 7 – 10), which are not represented in Table  3 : [...]...|$|R
50|$|Unsupervised Ideological Scaling (i.e. wordsfish): {{algorithms}} that allocate <b>text</b> <b>units</b> into {{an ideological}} continuum depending on shared grammatical content. Contrary to supervised scaling {{methods such as}} wordscores, methods such as wordfish do not require that the researcher provides samples of extreme ideological texts.|$|R
40|$|One impersonator and {{a number}} of his {{different}} voice imitations have been studied in order to gain some insights into the flexibility of the human voice and speech. A whole <b>text</b> <b>unit</b> and one selected word, which occur in all the recordings, have been analyzed and a comparison made between the different recordings. Both auditory and acoustic analyses have been attempted. The results indicate that this impersonator is able to adopt a range of articulatory-phonetic configurations in order to achieve the target speakers. This very fact raises questions concerning features that are hard to change in the voice and the possibility to find some kind of long-term signature for one speaker. 1...|$|E
40|$|Textual {{databases}} {{which are}} enriched with analytic informa-tion {{tend to be}} a seemingly excessive factor larger than the original text. Ambiguity in the text {{is one of the main}} contributors to this growth This paper examines mechan-isms that make it possible to restrain the amount of storage required by ambiguous structures. It will not only consider the resulting storage efficiency, but will also question their usability as an interface to a human disambiguator. This perspective is just as (if not even more) important, seeing that ambiguity resolution by humans is by far the safest way to arrive at correct readings What is an Ambiguous Structure? Ambiguity, in the traditional sense, is the presence of two or more interpretations for the same <b>text</b> <b>unit</b> (word...|$|E
30|$|Ganzenmüller’s {{procedure}} {{draws on}} a limited set of manuscripts and his conclusion is coherent with their contents. An effective procedure requires precise definitions of the witnesses in terms of manuscripts and texts they include, as well as well-defined procedures for assigning a given <b>text</b> <b>unit</b> {{to one of the}} two traditions (see below). However, the Corning manuscript is heterogeneous and includes both the texts from Mappæ and Compositiones (see below); for this reason it is not a reliable reference for separating and identifying the two textual traditions. In the present paper, we move the focus from manuscripts to texts and collections of the same text within different manuscripts. We interpret the Lucca 490 as belonging to a different tradition, on the basis of substantial evidence, and the Corning and Sélestat manuscripts as witnesses of both traditions (see the inventories below).|$|E
3000|$|The Klosterneuburg {{fragment}} {{is composed}} of two folios of recipes (ff. 1 r and 1 v, with recipes nos. 1 – 18; ff. 2 r and 2 v with recipes nos. 19 – 30) that appear in other witnesses as contiguous sequences, locally ordered. The manuscripts which show the recipes from the first folio are not mixed with any from the second folio, and vice versa. The poor condition of the Klosterneuburg fragment {{makes it difficult to}} collate all its recipes: if we compare the 13 Klosterneuburg texts missing in the Lucca 490 manuscript with the <b>text</b> <b>units</b> of the critical edition of Mappæ and the <b>text</b> <b>units</b> that we think likely candidates of the Compositiones Lucenses tradition, we can reach the following conclusions: [...]...|$|R
40|$|Conference paperVector space models (VSMs) are mathematically {{well-defined}} frameworks {{that have}} been widely used in text processing. In these models, high-dimensional, often sparse vectors represent <b>text</b> <b>units.</b> In an application, the similarity of vectors and hence the <b>text</b> <b>units</b> that they represent is computed by a distance formula. The high dimensionality of vectors, however, is a barrier to the performance of methods that employ VSMs. Consequently, a dimensionality reduction technique is employed to alleviate this problem. This paper introduces a new method, called Random Manhattan Indexing (RMI), for the construction of L 1 normed VSMs at reduced dimensionality. RMI combines the construction of a VSM anddimension reduction into an incremental, and thus scalable, procedure. In order to attain its goal, RMI employs the sparse Cauchy random projections...|$|R
40|$|Applying a {{coding scheme}} to {{discrete}} <b>text</b> <b>units</b> {{has long been}} the most common method for estimating substantive quantities of interest about the authors of these texts, whether for political, social, economic, or other substantive reasons. In political analysis, researchers typically build scales of policy positions from the relative frequencies of <b>text</b> <b>units</b> coded as left versus right policy categories. In this paper we reexamine the theoretical and linguistic basis for such scales, proposing a new alternative based on the logarithm of odds-ratios that is consistent with this underlying political and linguistic mechanism. We contrast this scale to previous approaches using <b>text</b> <b>units</b> coded into political categories from the discipline’s longest-running content analysis dataset, that of the Comparative Manifesto Project (CMP). We demonstrate that the logit scale avoids widely acknowledged flaws in previous approaches and validate it through comparison to independent expert surveys of policy positions. Applied to existing CMP data, without requiring any estimation or inferential procedures, we show how to unlock more policy dimensions, for more years, than have ever been provided before, and we make this new dataset available along with estimates of uncertainty for each measure. Finally, we draw some lessons for the future design of coding schemes for political texts...|$|R
