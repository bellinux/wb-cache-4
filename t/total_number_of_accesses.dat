9|10000|Public
40|$|Abstract. Scientific data in {{the life}} {{sciences}} is distributed over various independent multi-format databases and is constantly expanding. We discuss a scenario where a life science research lab monitors over time the results of queries to remote databases beyond their control. Queries are registered at a local system and get executed {{on a daily basis}} in batch mode. The goal of the paper is to study evaluation strategies minimizing the <b>total</b> <b>number</b> <b>of</b> <b>accesses</b> to databases when evaluating all queries in bulk. We use an abstraction based on the relational model with fan-out constraints and conjunctive queries. We show that the above problem remains np-hard in two restricted settings: queries of bounded depth and the scenario with a fixed schema. We further show that both restrictions taken together results in a tractable problem. As the constant for the latter algorithm is too high to be feasible in practice, we present four heuristic methods that are experimentally compared on randomly generated and biologically motivated schemas. Our algorithms are based on a greedy method and approximations for the shortest common super sequence problem. ...|$|E
30|$|Figure 4 {{provides}} an overview of the caching architecture. First, the image is logically subdivided into sub-blocks. The cache and prefetch unit generates the order of first occurrence of each memory block for a given set of image warping parameters online. This order is stored locally and subsequently used to fill the cache blocks optimally. Additionally, the total number of pixel accesses for each block is generated whenever a new set of parameters is loaded and stored in a lookup table. During operation, the current number of pixel accesses is counted for each of the currently used cache blocks. If the number of current pixel accesses matches the predetermined <b>total</b> <b>number</b> <b>of</b> <b>accesses,</b> this cache block can be updated with a new memory block. If the cache memory is configured to hold enough lines, no memory block needs to be read more than once using this caching strategy. To be able to read four values from cache in parallel, we subdivide the cache into four parallel cache blocks and store values column and row interleaved, since the bilinear interpolation always requires 2 -by- 2 blocks.|$|E
40|$|Heuristic for {{profiling}} bandwidths in {{object oriented}} applications Abstract — The performance of object oriented applications is severely {{influenced by the}} access time to the processed data. On a multiprocessor system with distributed shared memory, the average access time can be optimised by allocating the objects in the most appropriate memory. Knowledge of the bandwidths between an object and the different processing elements is important to choose the best allocation. We want to establish dynamic memory allocation on a mixed hardware/software platform. Objects will be moved from one memory to another {{in order to increase}} the system performance. Therefore the bandwidth to each object should be measured on the fly, both in hardware and in software. Due to the large number of objects in typical applications and the limited resources, it is not feasible to count data accesses to every object. Only a small number of objects can be tracked. We present a hardware friendly heuristic for dynamically identifying the most accessed objects. Our heuristic also provides estimations of the <b>total</b> <b>number</b> <b>of</b> <b>accesses</b> to each object. Comparison of these estimations with full bandwidth measurements in software shows that the presented heuristic is rather accurate: for some benchmark programs the objects locally responsible for up to 80 % of all data accesses are identified...|$|E
25|$|The <b>total</b> <b>number</b> <b>of</b> people lacking <b>access</b> to {{at least}} basic {{sanitation}} in 2015 was 32 million people.|$|R
2500|$|The <b>total</b> <b>number</b> <b>of</b> people lacking <b>access</b> to [...] "at least basic" [...] {{water in}} 2015 was 19 million people.|$|R
40|$|Cataloged from PDF {{version of}} article. The {{inverted}} index partitioning problem is investigated for parallel text retrieval systems. The {{objective is to}} perform efficient query processing on an inverted index distributed across a PC cluster. Alternative strategies are considered and evaluated for inverted index partitioning, where index entries are distributed according to their document-ids or term-ids. The performance of both partitioning schemes depend on the <b>total</b> <b>number</b> <b>of</b> disk <b>accesses</b> and the total volume of communication in the system. In document-id partitioning, the total volume of communication is naturally minimum, whereas the <b>total</b> <b>number</b> <b>of</b> disk <b>accesses</b> may be larger compared to term-id partitioning. On the other hand, in term-id partitioning the <b>total</b> <b>number</b> <b>of</b> disk <b>accesses</b> is already equivalent to the lower bound achieved by the sequential algorithm, albeit the total communication volume may be quite large. The studies done so far perform these partitioning schemes in a round-robin fashion and compare the performance of them by simulation. In this work, a parallel text retrieval system is designed and implemented on a PC cluster. We adopted hypergraph-theoretical partitioning models and carried out performance comparison of round-robin and hypergraph-theoretical partitioning schemes on our parallel text retrieval system. We also designed and implemented a query interface and a user interface of our system. Çatal, AytülM. S...|$|R
40|$|Previously {{we worked}} in the RAM or cell probe models, in which {{the cost of an}} {{algorithm}} only depended on the <b>total</b> <b>number</b> <b>of</b> <b>accesses</b> to memory locations. However, these models do not reflect the memory hierarchy of real computers that have more than one layer of memory, with different access characteristics. In a modern computer, the CPU operates on values in its registers, and can fetch values from main memory, usually via several layers of caches. Large files are stored on disk, and data is transferred from disk to main memory. The memory closest to the CPU is fastest but smallest, and the memory farthest from the CPU is largest but has the highest latency. Moreover, the slower layers of memory have greater parallelism: it takes about as long to fetch a consecutive kilobyte from disk as it does to fetch a single byte, since most of the cost is latency for the disk to find the right location. So we might as well always transfer a full block of data at a time; the slower layers of memory have larger block sizes. This prompts the need for algorithms and data structures that exploit locality of reference, arranging the layout of memory with data that is frequently accessed together placed in the same blocks, in order to minimize the number of blocks that need to be accessed. This lecture covered two models of computation that take this memory hierarchy into account: th...|$|E
40|$|We {{present a}} power-saving method for {{large-scale}} storage systems of cloud data sharing services, particularly those providing media (video and photograph) sharing services. The idea behind our {{method is to}} periodically rearrange stored data in a disk array, so that the workload is skewed toward a small subset of disks, while other disks {{can be sent to}} standby mode. This idea is borrowed from the Popular Data Concentration (PDC) technique, but to avoid an increase in response time caused by the accesses to disks in standby mode, we introduce a function that predicts future access frequencies of the uploaded files. This function uses the correlation of potential future accesses with the combination of elapsed time after upload and the <b>total</b> <b>number</b> <b>of</b> <b>accesses</b> in the past. We obtain this function in statistical analysis of the real access patterns of 50, 000 randomly selected publicly available photographs on Flickr over 7, 000 hours (around 10 months). Moreover, to adapt to a constant massive influx of data, we propose a mechanism that effectively packs the continuously uploaded data into the disk array in a storage system based on the PDC. To evaluate the effectiveness of our method, we measured the performance in simulations and a prototype implementation. We observed that our method consumed 12. 2 % less energy than the static configuration (in which all disks are in active mode). At the same time, our method maintained a preferred response time, with 0. 23 % of the total accesses involving disks in standby mode...|$|E
40|$|In this lecture, we {{examined}} data structures for two new models of computation. Previously we {{worked in the}} RAM or cell probe models, in which {{the cost of an}} algorithm depends on the <b>total</b> <b>number</b> <b>of</b> <b>accesses</b> to memory locations. However, this doesn’t reflect the memory hierarchy of real computers: {{there is more than one}} layer of memory, with different access characteristics. In a modern computer, the CPU operates on values in its registers, and can fetch values from main memory, usually via several layers of caches. Large files are stored on disk, and data is transferred from disk to main memory. The memory closest to the CPU is fastest but smallest, and the memory farthest from the CPU is largest but has the highest latency. Moreover, the slower layers of memory have greater parallelism: it takes about as long to fetch a consecutive kilobyte from disk as it does to fetch a single byte, since most of the cost is latency for the disk to find the right location. So we might as well always transfer a full block of data at a time; the slower layers of memory have larger block sizes. This prompts the need for algorithms and data structures that exploit locality of reference, arranging the layout of memory with data that is frequently accessed together placed in the same blocks, in order to minimize the number of blocks that need to be accessed. This lecture covered two models of computation that take this memory hierarchy into account: th...|$|E
30|$|A data {{preparation}} {{stage of the}} process starts from defining the time units and partitioning an audit trail over time units. Next, the syntax trees are obtained from an audit trail, and then the trees are compressed, reduced, and not important ones are eliminated. The discovery stage consists of finding elementary periodic patterns and applying the composition rule to derive the required complex periodic patterns. The computational complexity of search for elementary periodic patterns is approximately O(k*n^ 3) where 0 <k< 1 / 8 and n is the <b>total</b> <b>number</b> <b>of</b> partitions in an audit trail. Complexity of search over syntax trees is hard to estimate as {{it depends on the}} <b>total</b> <b>number</b> <b>of</b> <b>access</b> methods to relational tables, complexity of SQL statements, and a level of sharing common components among SQL statements.|$|R
3000|$|... {{denotes the}} <b>total</b> <b>number</b> <b>of</b> node <b>accesses</b> {{required}} {{to answer a}} point query {{for each of the}} N elements stored in the metric tree. The value of the absolute fat-factor will always be in the range [0, 1]. The value zero indicates a tree with no overlapping and the value 1 implies a tree with all the nodes overlapped.|$|R
40|$|Query {{processing}} at runtime is {{an important}} issue for data-centric applications. A faster query execution is highly required which means searching and returning the appropriate data of database. Different techniques have been proposed over the time and materialized view construction is one of them. The efficiency of a materialized view (MV) is measured based on hit ratio, which indicates the ratio <b>of</b> <b>number</b> <b>of</b> successful search to <b>total</b> <b>numbers</b> <b>of</b> <b>accesses.</b> Literature survey shows that few research works has been carried out to analyze the relationship between the attributes based on nonlinear equations for materialized view creation. However, as nonlinear regression is slower, in this research work they are mapped into linear equations to keep the benefit of both the approaches. This approach is applied to recently executed query set to analyze the attribute affinity and then the materialized view is formed based on the result of attribute affinity...|$|R
40|$|Personal {{use of this}} {{material}} is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing {{this material}} for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. G. Martínez-Muñoz and E. Pulido, "Using a SPOC to flip the classroom," 2015 IEEE Global Engineering Education Conference (EDUCON), Tallinn, 2015, pp. 431 - 436. doi: 10. 1109 /EDUCON. 2015. 7096007 The benefits of using SPOC platforms into face-to-face education are yet to be completely analysed. In this work we propose to use SPOCs and video contents to flip the classroom. The objective {{is to improve the}} involvement and satisfaction of students with the course, to reduce the drop-out rates and to improve the face-to-face course success rate. We apply these ideas to an undergraduate first year course on Data Structures and Algorithms. The study is validated by collecting data from two consecutive editions of the course, one in which the flipped classroom model and videos were used and other in which they were not. The gathered data included online data about the students' interaction with the SPOC materials and offline data collected during lectures and exams. In the edition where the SPOC materials were available, we have observed a correlation between the students' final marks and their percentage rate of video accesses with respect to the <b>total</b> <b>number</b> <b>of</b> <b>accesses,</b> which indicates a better academic performance for students who prefer videos over documents. Authors acknowledge financial support from the Spanish Dirección General de Investigación project TIN 2013 - 42351 -P and from Comunidad de Madrid project CASI-CAM S 2013 /ICE- 2845...|$|E
40|$|Each RAID level {{reflects}} a different design architecture. Associated with {{each is a}} backdrop of imposed limitations, as well as possibilities which may be exploited within the architectural constraints of that level. There are three unique features that differentiate RAID 7 from all other levels. RAID 7 is asynchronous with respect to usage of I/O data paths. Each I/O drive (includes all data and one parity drives) as well as each host interface (there may be multiple host interfaces) has independent control and data paths. This means that each can be accessed completely, independently, of the other. This is facilitated by a separate device cache for each device/interface as well. RAID 7 is asynchronous with respect to device hierarchy and data bus utilization. Each drive and each interface is connected to a high speed data bus controlled by the embedded operating system to make independent transfers to and from central cache. RAID 7 is asynchronous {{with respect to the}} operation of an embedded real time process oriented operating system. This means that exclusive and independent of the host, or multiple host paths, the embedded OS manages all I/O transfers asynchronously across the data and parity drives. A key factor to consider is that of the RAID 7 's ability to anticipate and match host I/O usage patterns. This yields the following benefits over RAID's built around micro-code based architectures. RAID 7 appears to the host as a normally connected Big Fast Disk (BFD). RAID 7 appears, {{from the perspective of the}} individual disk devices, to minimize the <b>total</b> <b>number</b> <b>of</b> <b>accesses</b> and optimize read/write transfer requests. RAID 7 smoothly integrates the random demands of independent users with the principles of spatial and temporal locality. This optimizes small, large, and time sequenced I/O requests which results in users having an I/O performance which approaches performance to that of main memory...|$|E
40|$|The {{main goal}} of this thesis is to {{investigate}} the complexity {{of a variety of}} problems related to text indexing and text searching. We present new data structures that can be used as building blocks for full-text indices which occupies minute space (FM-indexes) and wavelet trees. These data structures also can be used to represent labeled trees and posting lists. Labeled trees are applied in XML documents, and posting lists in search engines. The main emphasis of this thesis is on lower bounds for time-space tradeoffs for the following problems: the rank/select problem, the problem of representing a string of balanced parentheses, the text retrieval problem, the problem of computing a permutation and its inverse, and the problem of representing a binary relation. These results are divided in two groups: lower bounds in the cell probe model and lower bounds in the indexing model. The cell probe model is the most natural and widely accepted framework for studying data structures. In this model, we are concerned with the total space used by a data structure and the <b>total</b> <b>number</b> <b>of</b> <b>accesses</b> (probes) it performs to memory, while computation is free of charge. The indexing model imposes an additional restriction on the storage: the object in question must be stored in its raw form together with a small index that facilitates an efficient implementation of a given set of queries, e. g. finding rank, select, matching parenthesis, or an occurrence of a given pattern in a given text (for the text retrieval problem). We propose a new technique for proving lower bounds in the indexing model and use it to obtain lower bounds for the rank/select problem and the balanced parentheses problem. We also improve the existing techniques of Demaine and Lopez-Ortiz using compression and present stronger lower bounds for the text retrieval problem in the indexing model. The most important result of this thesis is a new technique for cell probe lower bounds. We demonstrate its strength by proving new lower bounds for the problem of representing permutations, the text retrieval problem, and the problem of representing binary relations. (Previously, there were no non-trivial results known for these problems.) In addition, we note that the lower bounds for the permutations problem and the binary relations problem are tight {{for a wide range of}} parameters, e. g. the running time of queries, the size and density of the relation...|$|E
40|$|We {{live in a}} {{world where}} the job {{circulation}} vacancies information never stops and the demand on it is always high. Meanwhile, advances in mobile technology and the decrease in the price of mobile devices result in the rise of mobile web. Mobile web gives significant contribution to <b>total</b> <b>number</b> <b>of</b> <b>access</b> to internet. The three major mobile operating system Android, BlackBerry OS, and iOS- bring us even smoother experience while browsing the web through mobile devices. Many websites now offer mobile version of their website to gain direct access from mobile devices. This brings an opportunity for job vacancies websites to expand their service to broader audience. JobsDB™, one of the prominent job vacancies website, has taken the opportunity. But the resulting mobile site has not satisfied the design and usability principle in the domain of Human Computer Interaction. This paper tries to explain this and offer solution to it...|$|R
40|$|Abstract—This {{paper is}} about {{designing}} optimal highthroughput hashing schemes that minimize the <b>total</b> <b>number</b> <b>of</b> memory <b>accesses</b> {{needed to build}} and access an hash table. Recent schemes often promote the use of multiple-choice hashing. However, such a choice also implies {{a significant increase in}} the <b>number</b> <b>of</b> memory <b>accesses</b> to the hash table, which translates into higher power consumption and lower throughput. In this paper, we propose to only use choice when needed. Given some target hash table overflow rate, we provide a lower bound on the <b>total</b> <b>number</b> <b>of</b> needed memory <b>accesses.</b> Then, we design and analyze schemes that provably achieve this lower bound over a large range of target overflow values. Further, for the multilevel hash table scheme, we prove that the optimum occurs when its subtable sizes decrease in a geometric way, thus formally confirming a heuristic rule-of-thumb. A. Background I...|$|R
40|$|This paper proposes an {{approach}} for reducing access count to register-files based on operand data reuse. The key {{idea is to}} compare source and destination operands of the current and previous instructions {{and if they are}} the same, omit the corresponding register file activation during operand fetch, thus saving energy consumption. Simulations show that using this technique we can decrease the <b>total</b> <b>number</b> <b>of</b> register-file <b>accesses</b> up to 62 % on peak and 39 % on average in comparison to a conventional approach...|$|R
30|$|Progress in Earth and Planetary Science (PEPS) {{was founded}} in late 2013, and the first article was {{published}} in March 2014. As {{of the end of}} December 2015, PEPS has published 75 articles (including 22 review articles); the <b>total</b> <b>number</b> <b>of</b> online <b>accesses</b> exceeds 140, 000 and more than 70, 000 papers have been downloaded. Further, PEPS articles have been cited in international journals registered by Web of Science more than 84 times. We feel that these numbers indicate that PEPS continues to make steady progress.|$|R
40|$|In Web, {{users in}} the same {{workgroup}} might have similar interests and habits. In this paper we study intelligent proxy techniques for people who use a proxy to access Web. Our intelligent proxy has two parts: cache and prefetch. We researched three replacement policies for proxy cache: LRU and two variations of LRU. Our simulation showed that basic LRU produces worst hit rate, while the other two policies achieve roughly the same results. Therefore, we choose a mixture of them as our proposed scheme. By introducing the prediction algorithm and threshold algorithm, The proxy can predict which Web files will be needed {{in the near future}} and download some of them before they are really requested by the user group. We can get a more accurate probability by running it on the proxy server than on the client site, when client user has not visited a file in a Web server often enough. By implementing the techniques of prefetch and cache on the proxy server, we can reduce the Web latency perceived by users and also the <b>total</b> <b>number</b> <b>of</b> <b>access</b> requests to Web server, thus propose a reasonable way of decreasing their cost to access the Web for developing countries. 1...|$|R
50|$|Cray {{decided that}} the 8600 would include four {{complete}} CPUs sharing the main memory. In order to improve overall throughput, the machine could be operated in a special mode in which a single instruction was sent to all four processors with different data. This technique, today known as SIMD, reduced the <b>total</b> <b>number</b> <b>of</b> memory <b>accesses</b> because the instruction was only read once, instead of four times. Each processor was about 2.5 {{times as fast as}} a 7600, so with all four running the machine as a whole would be about 10 times as fast, at about 100 MFLOPS.|$|R
40|$|We {{propose a}} method–recursive {{partition}}ing–to partition a static IP router table {{so that when}} each partition is represented using a base structure such as a multibit trie or a hybrid shape shifting trie there is a reduction in both the total memory required for the router table {{as well as in}} the <b>total</b> <b>number</b> <b>of</b> memory <b>accesses</b> needed to search the table. The efficacy of recursive partitioning is compared to that of the popular front-end table method to partition IP router tables. Our proposed recursive partitioning method outperformed the front-end method of all our test sets...|$|R
40|$|Abstract: Today’s network {{processors}} (NPs) support {{mechanisms to}} hide long memory access latencies; however, {{they often do}} not support data caches that are effective in reducing average memory access latency. In this paper, we study a wide-rage of packet processing applications and demonstrate that accesses to many data structures used in these applications exhibit considerable temporal locality; further, these accesses constitute a significant fraction <b>of</b> the <b>total</b> <b>number</b> <b>of</b> memory <b>accesses</b> made while processing a packet. Consequently, utilizing a cache for these data structures can (1) speedup packet processing, and (2) reduce the total off-chip memory bandwidth requirement considerably. ...|$|R
40|$|Among {{the many}} bibliometric {{criteria}} {{used to evaluate}} biomedical journals, the impact factor is the most commonly used. Despite its limitations, it quantifies {{the influence of a}} journal on secondary publications. It does not however evaluate the practical usefulness of primary documents. Usefulness is field-related and varies greatly among specialities. We introduce a new bibliographic criterion, the “reading factor”, and define it as the ratio between the <b>number</b> <b>of</b> electronic consultations of a particular journal (i. e., <b>number</b> <b>of</b> clicks on a hyper-link) and the mean <b>number</b> <b>of</b> electronic consultations of all the journals studied (itself calculated by dividing the <b>total</b> <b>number</b> <b>of</b> electronic <b>accesses</b> by the <b>number</b> <b>of</b> journals in the database). We describe its observed distribution...|$|R
40|$|Abstract. This paper {{focuses on}} the Cyclops 64 {{computer}} architecture and presents an analytical model and performance simulation results for the preloading and loop unrolling approaches to optimize the performance of SVD (Singular Value Decomposition) benchmark. A performance model for dissecting the total execution cycles is presented. The data preloading using “memcpy ” or hand optimized “inline ” assembly code, and the loop unrolling approach are implemented and compared {{with each other in}} terms <b>of</b> the <b>total</b> <b>number</b> <b>of</b> memory <b>access</b> cycles. The key idea is to preload data from offchip to onchip memory and store the data back after the computation. These approaches can reduce the total memory access cycles and can thus improve the benchmark performance significantly. ...|$|R
30|$|Finally, for any {{academics}} {{concerned about}} the submission fee, {{it should be noted}} that Springer have a generous waiver policy which ensures the large majority of worthy articles are published. Open access is becoming increasingly common in academic publishing and SpringerOpen (including Biomed Central) now has around 170 journals with an impact factor (41 % <b>of</b> the <b>total</b> <b>number</b> <b>of</b> their open <b>access</b> journals). While most of these journals are in the sciences there is a continuing growth in the <b>number</b> <b>of</b> humanities journals.|$|R
40|$|This paper {{provides}} {{the first full}} description {{of the status of}} Australian institutional repositories. Australia presents an interesting case because of the government?s support of institutional repositories and open <b>access.</b> A survey <b>of</b> all 39 Australian universities conducted in September 2008 shows that 32 institutions have active repositories and by end of 2009, 37 should have repositories. The <b>total</b> <b>number</b> <b>of</b> open <b>access</b> items has risen dramatically since January 2006. Five institutions reported they have an institution?wide open access mandate, and eight are planning to implement one. Only 20 universities have funding for their repository staff and 24 universities have funding for their repository platform, either as ongoing recurrent budgeting or absorbed into their institutions? budgets. The remaining are still project funded. The platform most frequently used for Australian repositories is Fedora with Vital. Most of the remaining sites use EPrints or DSpace...|$|R
30|$|In this scheme, each MTC device {{receives}} {{dedicated access}} slots {{to perform the}} RA mechanism and, other than this, always remains in sleep mode [108]. Using ID and RA cycle number, each device can calculate its allowable access slots. An eNB broadcasts the RA cycle number which is a multiple of radio frames. The advantage of the given scheme is that MTC devices can share the access slot if the <b>total</b> <b>number</b> <b>of</b> unique <b>access</b> slots {{is less than the}} <b>number</b> <b>of</b> devices within a cell. However, the drawback of this scheme is the large RA request delays if the RA cycle length is increased. However, it can significantly reduce the collisions at the cost of large RA request delays. In the LTE-A RA procedure, the influence <b>of</b> an increasing <b>number</b> <b>of</b> transmission attempts on throughput and delay of the slotted ALOHA-based preamble contention is also studied in [112].|$|R
40|$|LLC (www. teleadvs. com). In 2004, {{he retired}} after twenty years with Booz Allen & Hamilton, {{where he was}} a Lead Partner, a member of the firm’s Leadership Team and Head of the US and Latin America {{telecommunication}} practices. His last book-El papel de las TIC en el desarrollo: Propuesta de América Latina a los retos económicos actuales- was published in 2009. This paper estimates the demand for broadband technology in Latin America and quantifies the macroeconomic impact of broadband technology on employment and productivity. While the <b>total</b> <b>number</b> <b>of</b> broadband <b>access</b> lines is 26. 8 million and has increased 38 % in the last year, the region still needs to grow the <b>number</b> <b>of</b> lines by 41 % (adding 11 million lines) to respond {{to the needs of the}} economy. If that were to be achieved, it is estimated that the deployment could result in, at least, 378, 000 new jobs...|$|R
30|$|We {{propose that}} instead of storing all the {{credentials}} in one backup node, X would randomly divide all its credentials into r equal-sized groups and distribute each group to r different nodes to store. Since each credential is signed by X, it cannot be tampered with. If X becomes unreachable, Z can contact each of the r nodes in turn to obtain the complete set of X's credentials. Using this approach, we do not increase the storage or bandwidth requirements, but we {{make it harder for}} an attacker to disable <b>access</b> to all <b>of</b> X's credentials (the <b>total</b> <b>number</b> <b>of</b> nodes <b>accessed</b> by Z is increased, however). To disable <b>access</b> to all <b>of</b> X's credentials, the attacker must bring down all r nodes along with X. Further redundancy can be introduced by storing each group of credentials on more than one node. This would increase storage requirement though, so a balance needs to be reached between storage requirement and the preferred degree of redundancy.|$|R
40|$|Abstract. We {{present an}} FPGA-based {{parallel}} hardware-software architecture for the computation of the Discrete Wavelet Transform (DWT), using the Recursive Merge Filtering (RMF) algorithm. The DWT is {{built in a}} bottom-up fashion in logN steps, successively building complete DWTs by “merging ” two smaller DWTs and applying the wavelet filter to only the “smooth ” or DC coefficient from the smaller DWTs. The main bottleneck of this algorithm is the data routing process, which can be reduced by separating the computations into two types to introduce parallelism. This is achieved by using a virtual mapping structure to map the input. The data routing bottleneck {{has been transformed into}} simple arithmetic computations on the mapping structure. Due {{to the use of the}} FPGA-RAM for the mapping structure, the <b>total</b> <b>number</b> <b>of</b> data <b>accesses</b> to the main memory are reduced. This architecture shows how data routing in this problem can be transformed into a series of index computations...|$|R
40|$|In this paper, we {{introduce}} a profile-driven online page migration scheme and investigate {{its impact on}} the performance of multithreaded applications. We use lightweight, inexpensive plug-in hardware counters to profile the memory <b>access</b> behavior <b>of</b> an application, and then migrate pages to memory local to the most frequently accessing processor. Using the Dyninst runtime instrumentation combined with hardware counters, we were able to add page migration capabilities to the system without having to modify the operating system kernel, or to re-compile application programs. This approach reduced the <b>total</b> <b>number</b> <b>of</b> non-local memory <b>accesses</b> <b>of</b> applications by up to 90 %. Even on a system with small remote to local memory access latency rations, this resulted in up to 16 % improvement in execution time. 1...|$|R
40|$|The {{proposed}} {{approach in}} this paper selects a fixed Visitor Location Register (VLR) as a Fixed Local Anchor (FLA) for each group of Registration Areas (RAs). During call delivery process, the calling VLR/FLA caches are updated with the called Mobile Terminal’s (MT’s) location information and the called VLR and FLA caches are updated with the calling MT’s location information. Furthermore, the FLA and the old VLR caches are updated with MT’s new location information during inter-RA handoff {{as a part of}} informing this to the FLA of that region. But for another case, it updates the new FLA, old FLA, and old VLR caches with new location information together with directly informing this to the HLR. This location caching policy in local anchor strategy maximizes the probability of finding MTs’ location information in caches. As a result, it minimizes the <b>total</b> <b>number</b> <b>of</b> HLR <b>access</b> for finding MT’s location information prior to deliver a call. So, it significantly reduces the total location management cost in terms of location registration cost and call delivery cost. The analytical and experimental results also demonstrate that the proposed method outperforms all other previous methods regardless of the MT’s calling and mobility pattern...|$|R
50|$|This {{technique}} is conceptually simpler than the load queue search, and it eliminates a second CAM and its power-hungry search (the load queue {{can now be}} a simple FIFO queue). Since the load must re-access the memory system just before retirement, the access must be very fast, so this scheme relies on a fast cache. No matter how fast the cache is, however, the second memory system access for every out-of-order load instruction does increase instruction retirement latency and increases the <b>total</b> <b>number</b> <b>of</b> cache <b>accesses</b> that must be performed by the processor. The additional retire-time cache access can be satisfied by re-using an existing cache port; however, this creates port resource contention with other loads and stores in the processor trying to execute, and thus may cause a decrease in performance. Alternatively, an additional cache port can be added just for load disambiguation, but this increases the complexity, power, and area of the cache. Some recent work (Roth 2005) has shown ways to filter many loads from re-executing if {{it is known that}} no RAW dependence violation could have occurred; such a technique would help or eliminate such latency and resource contention.|$|R
40|$|Traditional {{object-oriented}} languages use typed {{classes to}} specify the shape and behaviour of objects. In JavaScript, object behaviour is less constrained. Learning how JavaScript programmers create objects can help us understand whether the programmers employ the notion of "classes", like those in Java, in JavaScript. In other words, {{we are trying to}} compare the creation of objects in JavaScript with Java classes, which create objects and describe the state and behaviour that the type of the object support. Our results have the potential to improve the performance of JavaScript and generality of typed extensions to JavaScript. An access site is the JavaScript code that <b>accesses</b> the properties <b>of</b> objects. We studied the access site to determine whether objects at the access site arise from different "creators", which are functions creating the objects, and to discover how prevalent multiple-creator access sites are. Moreover, whether the types of properties ever change is noteworthy: in classes, types of properties may not change. To answer the questions, we executed, recorded, and analyzed many JavaScript applications to obtain real-world data. We collected and analyzed logs from 7, 753 web sites. The logs consist of nearly 390 GB of JSON strings. We collected these data through dynamically running modern JavaScript applications, or web sites, in our tool with the browser Firefox. Our results show that for half of selected web sites, there exists only one creator at every access site. And in fewer than 50 % of web sites, there exists more than one creator, though the <b>total</b> <b>number</b> <b>of</b> <b>access</b> sites with multiple creators is extremely small. Although creators with distinct names are considered as different by our definition, different creators may create same objects. Therefore, most object accesses in JavaScript do not come from different creators. As a result, we could employ type annotations for objects. However, our results show that the types of the properties in objects do sometimes change, especially in objects generated from user-defined functions. Thus, even if objects are mostly from one creator, that creator may not correspond neatly to a type. This may limit the advantages of using types for better performance. Combining these results, we conclude that while we can apply Java-like types to JavaScript, these types are far more fluid in JavaScript applications, and so there may be limited benefit...|$|R
40|$|In {{evaluating}} the operational {{performance of the}} department’s web site for the period between 2006 and 2013, {{we found that the}} <b>total</b> <b>number</b> <b>of</b> page <b>accesses</b> peaked in 2010 and subsequently trended downward. We also noted that the ratio of applicants to places was unaffected by this downward trend, and that, based on a questionnaire given to freshman classes, 25 % of incoming freshman had used the web site to collect information about our department. These findings led us to conclude that the content of the web site is sufficient to attract prospective students’ interest and that our renewal should preserve most of this content, while introducing state-of-the-art web technologies (i. e., HTML 5 and CSS 3). Accordingly, we implemented a <b>number</b> <b>of</b> new web pages and validated their standards conformance using established services. Although the <b>number</b> <b>of</b> site visitors has not significantly increased since the new pages went live, we believe that the changes successfully demonstrate the department’s embrace of new information and communication technologies. We will continue to update the department’s web site in accordance with the PDCA (plan-do-check-act) cycle. Our aim is to increase the attractiveness and technical proficiency of the site, while maintaining or improving the quality of its content...|$|R
