18|17|Public
50|$|Imipenem was {{patented}} in 1975. It {{was discovered}} via a lengthy <b>trial-and-error</b> <b>search</b> {{for a more}} stable version of the natural product thienamycin, which is produced by the bacterium Streptomyces cattleya. Thienamycin has antibacterial activity, but is unstable in aqueous solution, so impractical to administer to patients. Imipenem has {{a broad spectrum of}} activity against aerobic and anaerobic, Gram-positive and Gram-negative bacteria. It is particularly important for its activity against Pseudomonas aeruginosa and the Enterococcus species. It is not active against MRSA, however.|$|E
3000|$|... [...]. Since the {{transient}} state {{can be seen}} as an stochastic (<b>trial-and-error)</b> <b>search</b> for a collision free schedule, setting [...]...|$|E
40|$|Abstract-This paper {{introduces}} a novel switch approach for redundant capacitive DACs of a 2 b-per-cycle SAR ADC. By using the proposed multi-merged switching algorithm, the conventional <b>trial-and-error</b> <b>search</b> procedure is prevented, {{which leads to}} significant switching energy and DAC settling time reductions. The conversion power and speed analysis are presented, which is also verified in behavior simulations of a 6 -bit 2 b/cycle SAR ADC. The simulation {{results show that the}} proposed method can achieve about 37 % power saving as compared to the conventional one. I...|$|E
40|$|The inventive {{process has}} been often modeled as a bounded {{iterative}} <b>trial-and-error</b> recombinant <b>search</b> over complex landscape. Our main research question is whether such approximation is empirically valid. To investigate it, we develop a single-industry measure of invention interdependence and provide a relatively direct test of the canonical NK model. Our findings indicate that the NK model correctly predicts most of the empirical patterns. We also find that the consistency between the empirical estimates and the model predictions deteriorates with expanded definition of industry boundaries. Our results suggest that models representing iterative <b>trial-and-error</b> recombinant <b>search</b> are applicable as approximations of the inventive process when one looks at single mature industries {{where most of the}} knowledge originates from within the same technological domain. When inventors draw from a broad knowledge base and fundamentally new knowledge is created then applicability of simple models of recombinant search may be limited...|$|R
40|$|Dielectric {{measurements}} were utilized {{to follow the}} advancement of cure in an epoxy/amine formulation. In contrast to earlier studies, complex impedance was measured during cure and used to calculate ionic resistivity. By using complex impedance {{we were able to}} separate according to their frequency dependence the contributions to overall polarization from electrode blocking layers, migrating charges, and dipole relaxations. At any stage of cure, there is a unique frequency at which ionic resistivity can be singularly measured. Our approach does not involve <b>trial-and-error</b> frequency <b>search</b> and is conducive to the development of phenomenological models based on equivalent circuits. Excellent agreement was reported between the calculated values of normalized degree of cure obtained by dielectric and calorimetric measurements...|$|R
5000|$|The early texts show a man who, {{in his own}} {{thinking}} and discussion with others, earnestly searches a way to approach {{the essence of the}} Buddha's Teaching by repeated <b>trial-and-error.</b> This <b>search</b> has finally yielded its fruit when, after suffering from amoebiasis, Ñāṇavīra Thera claimed to have attained sotāpatti, or stream-entry, on 27 June 1959. The one who has [...] "entered the stream" [...] has ipso facto abandoned personality-view (sakkāya-ditthi), which is the self-view implicit in the experience of an ordinary worldling not free from ignorance, and understood the essential meaning of the Buddha's teaching on the Four Noble Truths. Ñāṇavīra Thera's writings after 1960 express this very kind of certainty: no more wandering in the dark, no more doubt or speculative guessing.|$|R
40|$|Aplysia feeding {{behavior}} is highly variable from cycle to cycle. In some cycles, when the variability causes a mismatch between the animal’s movements and {{the requirements of}} the feeding task, the variability makes the behavior unsuccessful. We propose that the behavior is variable nevertheless because the variability serves a higher-order functional purpose. When the animal is faced with a new and only imperfectly known feeding task in each cycle, the variability implements a <b>trial-and-error</b> <b>search</b> through the space of possible feeding movements. Over many cycles, this may be the animal’s optimal strategy in an uncertain and changing feeding environment. r 2006 Elsevier B. V. All rights reserved...|$|E
40|$|I study {{a dynamic}} model of <b>trial-and-error</b> <b>search</b> in which agents {{do not have}} {{complete}} knowledge of how choices are mapped into outcomes. Agents learn about the mapping by observing the choices of earlier agents and the outcomes that are realized. The key novelty is that the mapping is represented as the realized path of a Brownian motion. I characterize for this environment the optimal behavior each period {{as well as the}} trajectory of experimentation and learning through time. Applied to new product development, the model shares features of the data with the well-known Product Life Cycle. (JEL D 81, D 83, D 92, L 26) ...|$|E
40|$|Conventional surface {{crystallography}} by low-energy electron diffraction (LEED) {{employs a}} <b>trial-and-error</b> <b>search</b> controlled {{at each step}} by human effort. This trial-and-error approach becomes very cumbersome and unreliable to solve complex surfaces {{with a large number}} of unknown structural parameters. We discuss automatic optimization procedures for LEED, which combine numerical search algorithms with efficient methods of determining the diffracted intensities for varying structures. Such approaches can reduce the computer time required for an entire structure determination by many orders of magnitude, while fitting many times more unknown structural parameters. Thereby, relatively complex structures, with typically 10 adjustable atoms (or 30 adjustable coordinates), can be readily determined on today's workstations. These include non-symmetrically relaxed structures, surface reconstructions and adsorbate-induced substrate distortions. We also address the theoretical and experimental requirements for an accurate structural determination. ...|$|E
40|$|The EUO gene of {{chlamydia}} {{is highly}} expressed {{early in the}} developmental cycle, relative to other genes, but continues to be expressed throughout the active growth phases. The precise function of EUO protein is not known, but it binds to DNA in vitro. In this study, we developed a selection and amplification scheme for identifying chlamydial genomic fragments to which EUO preferentially binds in vitro. The scheme involved mixing recombinant EUO with a Chlamydia psittaci genomic library in a pBluescript plasmid vector in vitro, trapping EUO-bound plasmid clones on filters, and amplifying the clones in Escherichia coli. After nine rounds of enrichment, the EUO binding sites of the three most highly enriched clones were identified by DNase I footprint analysis. All three clones had multiple binding sites of various sizes with no clear distinguishing feature other than they were AT-rich and were usually not located in putative promoter regions. We used limited site-specific mutagenesis to characterize the strongest binding site of the most-highly-enriched clone, which represented about 50 % of the population after nine rounds. This mutagenesis identified a core binding site of 15 nucleotides (nt) whose sequence was used to find related sequences {{within each of the}} strong binding sites in the other two clones. Using the frequency of bases at specific positions within this group of sequences as a guide, we carried out <b>trial-and-error</b> <b>searching</b> with many related sequences, eliminating those which identified nonfootprinted sites. This process led us to the consensus 15 -nt sequence AHGAAAWVTYTWDAY, which, when allowing two mismatches, picked out all of the strong binding sites and no nonfootprinting sites within the three enriched clones. This sequence may be useful for predicting additional possible EUO binding sites in the chlamydial genome...|$|R
40|$|Interactive data {{visualization}} is inherently an iterative <b>trial-and-error</b> process <b>searching</b> for an ideal {{set of parameters}} for classifying and rendering features {{of interest in the}} data. This paper presents 3 -d region growing based techniques that can assist the users to locate and define features of interest in volume data more quickly and more accurately. One technique employs partial region growing to generate a 2 -d transfer function that effectively reveals the full features of interest. The other technique uses the result of full region growing to systematically construct a boundary surface for the extracted features. The resulting polygonal representation of the boundary surface can facilitate comparison, measurement, and simulation. A visual assessment method is suggested by using the extracted volume and surface information. These techniques either shorten or completely eliminate the typical trial-and-error step in the process of interactive data exploration. 1...|$|R
5000|$|Around {{autumn of}} 1961 the English monk Ven. Ñānavīra Thera, who lived 40 km from {{her in a}} kuti in the jungle as a hermit, had sent her a text he had written, A Note on Paticca Samuppāda, wherein he criticized the extension-over-three-lives interpretation. Thereupon an {{intensive}} exchange of letters followed. The early letters show a woman who, in her own thinking and discussion with Ven. Ñānavīra, earnestly searches a way to approach {{the essence of the}} Buddha's Teaching by repeated <b>trial-and-error.</b> This <b>search</b> finally yielded its fruit when she, by her own account (as given in a letter to Ñānavīra Thera), attained sotāpatti, or Stream-entry in late January 1962. The one who has [...] "entered the stream" [...] has ipso facto abandoned personality-view (sakkāya-ditthi), which is the self-view implicit in the experience of an ordinary worldling not free from ignorance, and understood the essential meaning of the Buddha's teaching on the Four Noble Truths. But the rapidity and intensity of the change of her views caused a kind of nervous breakdown and she disrobed, returning to Germany on 22 February 1962.|$|R
40|$|Reinforcement {{learning}} is {{the learning of}} a mapping from situations to actions so as to maximize a scalar reward or reinforcement signal. The learner is not told which action to take, as in most forms of learning, but instead must discover which actions yield the highest reward by trying them. In the most interesting and challenging cases, actions affect not only the immediate reward, but also the next situation, and through that all subsequent rewards. These two characteristics [...] <b>trial-and-error</b> <b>search</b> and delayed reward [...] {{are the two most}} important distinguishing features of reinforcement learning. In this paper I present a brief overview of the development of reinforcement learning architectures over the past decade, including reinforcement-comparison, actor-critic, and Q-learning architectures. Finally, I present Dyna, a class of architectures based on reinforcement learning but which go beyond trial-and-error learning to include a learned internal model of the world. By [...] ...|$|E
40|$|Many {{applications}} {{provide a}} form-like interface for requesting information: the user fills in some fields, submits the form, {{and the application}} presents corresponding results. Such a procedure becomes burdensome if (1) the user must submit many different requests, for example in pursuing a <b>trial-and-error</b> <b>search,</b> (2) results from one application are {{to be used as}} inputs for another, requiring the user to transfer them by hand, or (3) the user wants to compare results, but only the results from one request can be seen at a time. We describe how users can reduce this burden by creating custom interfaces using three mechanisms: clipping of input and result elements from existing applications to form cells on a spreadsheet; connecting these cells using formulas, thus enabling result transfer between applications; and cloning cells so that multiple requests can be handled side by side. We demonstrate a prototype of these mechanisms, initially specialised for handling Web applications, and show how it lets users build new interfaces to suit their individual needs...|$|E
40|$|Abstract-In general, a Fuzzy Neural Network (FNN) is {{characterized}} by its learning algorithm and its linguistic knowledge representation. However, {{it does not necessarily}} interact with its environment when the training data is assumed to be an accurate description of the environment under consideration. In interactive problems, it would be more appropriate for an agent to learn from its own experience through interactions with the environment, i. e. reinforcement learning. In this work, three clustering algorithms are developed based on the reinforcement learning paradigm. This allows a more accurate description of the clusters as the clustering process is influenced by the reinforcement signal, They are the Reinforce Clustering Technique I (RCT-I), the Reinforce Clustering Technique II (RCT-II), and the Episodic Reinforce Clustering Technique (ERCT). we have implemented, the integrations of the RCT-I, the RCT-II, and the ERCT within the pseudo-outer product truth value restriction (POPTVR), which is a Fuzzy neural network integrated with the truth restriction value (TVR) inference scheme in its five layered feed forward neural network. The three reinforcement-based clustering techniques applied to the POPTVR network are able to exhibit the <b>trial-and-error</b> <b>search</b> characteristic that yields higher qualitative performance. Index Terms—Clustering, Fuzzy Neural Network...|$|E
30|$|We {{accept the}} {{argument}} that success in solving the word problem has an inverse connection {{with the needs of}} the structure. The word problems could cause a difficult situation, stress, and uncertainty for students with a high personal need for structure. As reported by John et al. (2000) uncertainty cannot be omitted from the learning process, but its impact can be examined and to what measure can be this impact minimalized. This fact will be in the focus of our next interest. Brown (2000) recommends to teach individuals toward tolerance for ambiguity (intuitive behaviour, <b>trial-and-error,</b> to not <b>search</b> for rules and algorithm for each time) and to accept it as a part of learning.|$|R
40|$|Abstract—This paper {{presents}} a self-organizing hierarchical cerebellar model arithmetic computer (HCMAC) neural-network classifier, which contains a self-organizing input space module and an HCMAC neural network. The conventional CMAC {{can be viewed}} as a basis function network (BFN) with supervised learning, and performs well in terms of its fast learning speed and local generalization capability for approximating nonlinear functions. However, the conventional CMAC has an enormous memory requirement for resolving high-dimensional classification problems, and its performance heavily depends on the approach of input space quantization. To solve these problems, this paper {{presents a}} novel supervised HCMAC neural network capable of resolving high-dimensional classification problems well. Also, in order to reduce what is often <b>trial-and-error</b> parameters <b>searching</b> for constructing memory allocation automatically, proposed herein is a self-organizing input space module that uses Shannon’s entropy measure and the golden-section search method to appropriately determine the input space quantization according to the various distributions of training data sets. Experimental results indicate that the self-organizing HCMAC indeed has a fast learning ability and low memory requirement. It is a better performing network than the conventional CMAC for resolving high-dimensional classification problems. Furthermore, the self-organizing HCMAC classifier has a better classification ability than other compared classifiers. Index Terms—Cerebellar model arithmetic computer (CMAC), golden-section search method, self-organizing hierarchical CMAC (HCMAC) classifier, Shannon’s entropy measure. I...|$|R
30|$|In web-based {{visualization}} {{systems for}} adults, data are usually spatially organized into a network or polar {{system in which}} the information nodes are web sites and databases. The path between two nodes indicates the queried relationship between two databases or the sequential searching actions of web users. For example, the Mapstan.com web tool records collective search results by fixing a map-path width (weight) inferred from the number of web-visited users between two nodes (databases). The relationship between two nodes is clearly defined and implied the content of nearby database groups. Since children are naturally curious and enjoy exploring unknown worlds, they may not use the most efficient search procedure. For example, they may use <b>trial-and-error</b> or fuzzy <b>searches</b> of possible target groups to find their target. Thus, enabling children to apply a free way-finding approach to searching an information node in the virtual environment is the first design guideline.|$|R
40|$|In general, a Fuzzy Neural Network (FNN) is {{characterized}} by its learning algorithm and its linguistic knowledge representation. However, {{it does not necessarily}} interact with its environment when the training data is assumed to be an accurate description of the environment under consideration. In interactive problems, it would be more appropriate for an agent to learn from its own experience through interactions with the environment, i. e. reinforcement learning. In this work, three clustering algorithms are developed based on the reinforcement learning paradigm. This allows a more accurate description of the clusters as the clustering process is influenced by the reinforcement signal, They are the Reinforce Clustering Technique I (RCT-I), the Reinforce Clustering Technique II (RCT-II), and the Episodic Reinforce Clustering Technique (ERCT). we have implemented, the integrations of the RCT-I, the RCT-II, and the ERCT within the pseudo-outer product truth value restriction (POPTVR), which is a Fuzzy neural network integrated with the truth restriction value (TVR) inference scheme in its five layered feed forward neural network. The three reinforcement-based clustering techniques applied to the POPTVR network are able to exhibit the <b>trial-and-error</b> <b>search</b> characteristic that yields higher qualitative performance...|$|E
40|$|Many novel {{techniques}} for reconstructing rainfall-runoff processes require hydrometeorologic and geomorphologic information for modelling. However, certain {{information is not}} always measurable. In this paper, we employ a special recurrent neural network to reconstruct the rainfall-runoff process by using collected rainfall data. In addition, we propose an indirect system identification to overcome the drawback of a traditional, time-consuming <b>trial-and-error</b> <b>search.</b> The indirect system identification is an efficient method to recognize {{the structure of a}} recurrent neural network. The unit hydrograph can be derived directly from the weights of the network due to a state-space form embedded in the recurrent neural network. This improves the link between the weights of the network and the physical concepts that most neural networks fail to connect. The case studies of 41 events from 1966 to 1997 have been implemented in Taiwan’s Wu-Tu watershed, where the runoff path-lines are short and steep. Two recurrent neural networks and one state-space model are utilized to simulate the rainfall-runoff processes for comparison. The results are validated by four criteria: coefficient of efficiency; peak discharge error; time to peak arrival error; total discharge volume error. The resulting data from the recurrent neural network reveal that the neural network proposed herein is appropriate for hydrological systems. Copyright © 2005 John Wiley & Sons, Ltd. KEY WORDS recurrent neural network; system identification; state space; rainfall-runoff process; unit hydrograp...|$|E
40|$|The paper {{provides}} an {{brief overview of}} the “state of the art” {{in the theory of}} rational decision making since the 1950 ’s, and focuses specially on the evolutionary justification of rationality. It is claimed that this justification, and more generally the economic methodology inherited from the Chicago school, becomes untenable once taking into account Kauffman’s Nk model, showing that if evolution it is based on <b>trial-and-error</b> <b>search</b> process, it leads generally to sub- optimal stable solutions: the ‘as if’ justification of perfect rationality proves therefore to be a fallacious metaphor. The normative interpretation of decision-making theory is therefore questioned, and the two challenging views against this approach, Simon’s bounded rationality and Allais’ criticism to expected utility theory are discussed. On this ground it is shown that the cognitive characteristics of choice processes {{are becoming more and more}} important for explanation of economic behavior and of deviations from rationality. In particular, according to Kahneman’s Nobel Lecture, it is suggested that the distinction between two types of cognitive processes – the effortful process of deliberate reasoning on the one hand, and the automatic process of unconscious intuition on the other – can provide a different map with which to explain a broad class of deviations from pure ‘olympian’ rationality. This view requires re-establishing and revising connections between psychology and economics: an on-going challenge against the normative approach to economic methodology. Bounded Rationality, Behavioral Economics, Evolution, As If...|$|E
40|$|Frontal beta {{oscillations}} {{are associated}} with top-down control mechanisms but also change over time during a task. It is unclear whether change over time represents another control function or a neural instantiation of vigilance decrements over time, the time-on-task effect. We investigated how frontal beta oscillations are modulated by cognitive control and time. We used frontal chronic electrocorticography in monkeys performing a <b>trial-and-error</b> task, comprising <b>search</b> and repetition phases. Specific beta oscillations in the delay period of each trial were modulated by task phase and adaptation to feedback. Beta oscillations in this same period showed a significant within-session change. These separate modulations of beta oscillations did not interact. Crucially, and in contrast to previous investigations, we examined modulations of beta around spontaneous pauses in work. After pauses, the beta power modulation was reset and the cognitive control effect was maintained. Cognitive performance was also maintained whereas behavioral signs of fatigue continued to increase. We propose that these beta oscillations reflect multiple factors contributing to the regulation of cognitive control. Due {{to the effect of}} pauses, the time-sensitive factor cannot be a neural correlate of time-on-task but may reflect attentional effort...|$|R
40|$|Differential {{evolution}} (DE) is an efﬁcient {{and powerful}} population-based stochastic search technique for solving optimization problems over continuous space, {{which has been}} widely applied in many scientiﬁc and engineering ﬁelds. However, the success of DE in solving a speciﬁc problem crucially depends on appropriately choosing trial vector generation strategies and their associated control parameter values. Employing a <b>trial-and-error</b> scheme to <b>search</b> for the most suitable strategy and its associated parameter settings requires high computational costs. Moreover, {{at different stages of}} evolution, different strategies coupled with different parameter settings may be required in order to achieve the best performance. In this paper, we propose a self-adaptive DE (SaDE) algorithm, in which both trial vector generation strategies and their associated control parameter values are gradually self-adapted by learning from their previous experiences in generating promising solutions. Consequently, a more suitable generation strategy along with its parameter settings can be determined adaptively to match different phases of the search process/evolution. The performance of the SaDE algorithm is extensively evaluated (using codes available from P. N. Suganthan) on a suite of 26 bound-constrained numerical optimization problems and compares favorably with the conventional DE and several state-of-the-art parameter adaptive DE variants...|$|R
40|$|F. M. S. and C. R. E. W. contributed {{equally to}} this work (co-first authors). Frontal beta {{oscillations}} are associatedwith top-down controlmechanisms but also {{change over time}} during a task. It is unclear whether change over time represents another control function or a neural instantiation of vigilance decrements over time, the time-on-task effect. We investigated how frontal beta oscillations aremodulated by cognitive control and time. We used frontal chronic electrocorticography in monkeys performing a <b>trial-and-error</b> task, comprising <b>search</b> and repetition phases. Specific beta oscillations in the delay period of each trial weremodulated by task phase and adaptation to feedback. Beta oscillations in this same period showed a significant within-session change. These separate modulations of beta oscillations did not interact. Crucially, and in contrast to previous investigations, we examined modulations of beta around spontaneous pauses in work. After pauses, the beta powermodulationwas reset and the cognitive control effectwasmaintained. Cognitive performancewas also maintained whereas behavioral signs of fatigue continued to increase. We propose that these beta oscillations reflect multiple factors contributing to the regulation of cognitive control. Due {{to the effect of}} pauses, the time-sensitive factor cannot be a neural correlate of time-on-task but may reflect attentional effort...|$|R
40|$|The optimal {{modeling}} of the lattice enthalpy of metal halides, of the boiling {{points of the}} mixed class of alcohols plus amines and of two activities of chlorofluorocarbons, the rates of hydrogen abstraction and the minimum anesthetic concentrations, has been achieved thanks {{to the introduction of}} three new types of higher-order terms. The first type of term, the mixed molecular connectivity-pseudoconnectivity term, Z = fX, Y), is function of the molecular connectivity term, X, and of the molecular pseudoconnectivity term, Y, only. Terms X and Y are function of a basis index β that can be either a molecular connectivity index, χ, or a molecular pseudoconnectivity index, ψ. The Z term can be found by trying different operational combinations of X and Y terms. The second and third type of mixed higher-order terms, Z 2 ̆ 7 = f(Z, β) and Zf = f(X, Y, β), can be found by the aid of a <b>trial-and-error</b> <b>search</b> procedure in which Z or X and Y terms are held constant throughout the search. Modeled properties show these three types of higher-order mixed terms at work: the two different properties of chlorofluorocarbons are modeled by the aid of a Z = f(X, Y) term, the lattice enthalpy of metal halides is instead modeled by a Z 2 ̆ 7 = f(Z, β) term, and the boiling point of the mixed class alcohols plus amines is modeled by a Z 2 ̆ 7 = f(X, Y, β) term...|$|E
40|$|A {{continuous}} time {{model for}} multiagent systems governed by reinforcement learning with scale-free memory is developed. The agents {{are assumed to}} act {{independently of one another}} in optimizing their choice of possible actions via <b>trial-and-error</b> <b>search.</b> To gain awareness about the action value the agents accumulate in their memory the rewards obtained from taking a specific action at each moment of time. The contribution of the rewards in the past to the agent current perception of action value is described by an integral operator with a power-law kernel. Finally a fractional differential equation governing the system dynamics is obtained. The agents are considered to interact with one another implicitly via the reward of one agent depending on the choice of the other agents. The pairwise interaction model is adopted to describe this effect. As a specific example of systems with non-transitive interactions, a two agent and three agent systems of the rock-paper-scissors type are analyzed in detail, including the stability analysis and numerical simulation. Scale-free memory is demonstrated to cause complex dynamics of the systems at hand. In particular, it is shown that there can be simultaneously two modes of the system instability undergoing subcritical and supercritical bifurcation, with the latter one exhibiting anomalous oscillations with the amplitude and period growing with time. Besides, the instability onset via this supercritical mode may be regarded as "altruism self-organization". For the three agent system the instability dynamics is found to be rather irregular and can be composed of alternate fragments of oscillations different in their properties. Comment: 17 pages, 7 figur...|$|E
40|$|In {{the present}} study, the {{possibility}} of retention modeling in the HILIC mode was investigated, testing several different literature relationships {{over a wide range}} of different analytical conditions (column chemistries and mobile phase pH) and using analytes possessing diverse physico-chemical properties. Furthermore, it was investigated how the retention prediction depends on the number of isocratic or gradient trial or initial scouting runs. The most promising set of scouting runs seems to be a combination of three isocratic runs (95, 90 and 70 %ACN) and one gradient run (95 to 65 %ACN in 10 min), as the average prediction errors were lower than using six equally spaced isocratic runs and because it is common in Method development (MD) to perform at least one scouting gradient run in the screening step to find out the best column, temperature and pH conditions. Overall, the retention predictions were much less accurate in HILIC than what is usually experienced in RPLC. This has severe implications for MD, as it restricts the use of commercial software packages that require the simulation of the retention of every peak in the chromatogram. To overcome this problem, the recently proposed predictive elution window shifting and stretching (PEWS 2) approach can be used. In this computer-assisted MD strategy, only an (approximate) prediction of the retention of the first and the last peak in the chromatogram is required to conduct a well-targeted <b>trial-and-error</b> <b>search,</b> with suggested search conditions uniformly covering the entire possible search and elution space. This strategy was used to optimize the separation of three representative pharmaceutical mixtures possessing diverse physico-chemical properties (pteridins, saccharides and cocktail of drugs/metabolites). All problems could be successfully handled in less than 2. 5 h of instrument time (including equilibration) ...|$|E
40|$|Abstract—Differential {{evolution}} (DE) is {{an efficient}} and powerful population-based stochastic search technique for solving optimiza-tion problems over continuous space, {{which has been}} widely applied in many scientific and engineering fields. However, the success of DE in solving a specific problem crucially depends on appropriately choosing trial vector generation strategies and their associated control parameter values. Employing a <b>trial-and-error</b> scheme to <b>search</b> for the most suitable strategy and its associated parameter settings requires high computational costs. Moreover, {{at different stages of}} evolution, different strategies coupled with different parameter settings may be required in order to achieve the best performance. In this paper, we propose a self-adaptive DE (SaDE) algorithm, in which both trial vector generation strategies and their associated control parameter values are gradually self-adapted by learning from their previous experiences in generating promising solutions. Consequently, a more suitable generation strategy along with its parameter settings can be determined adaptively to match different phases of the search process/evolution. The performance of the SaDE algorithm is extensively evaluated (using codes avail-able from P. N. Suganthan) on a suite of 26 bound-constrained numerical optimization problems and compares favorably with the conventional DE and several state-of-the-art parameter adaptive DE variants. Index Terms—Differential evolution (DE), global numerical optimization, parameter adaptation, self-adaptation, strategy adaptation. I...|$|R
40|$|In this thesis, {{we apply}} machine {{learning}} {{to the problem}} of controlling mobile robots in difficult, complex terrain. The motivation for this research is the desire to have an autonomous rescue robot that is able to overcome rubble, kerbs and other obstacles and perform a task such as finding survivors. Traditionally, this control problem has been solved by deriving control equations from mathematical models that encapsulate the interactions between the robot and the terrain. As the terrain becomes increasingly complex, these models become intractably difficult to construct. We have developed three control agents for a mobile rescue robot. They observe the terrain through the robot&# 146;s on-board sensors and use machine learned models to decide on actions to take to achieve a goal. These models encapsulate information that is automatically extracted from the performance of a demonstrator. We use a human expert and an autonomous demonstrator based on a <b>trial-and-error</b> forward <b>search</b> in simulation. The first agent uses a simple Situation-Action formulation and is related to Behavioural Cloning. It directly learns a model of the demonstrator&# 146;s behaviour. The second agent generalises this by learning from the demonstrator&# 146;s successes and failures. It learns a model for the desirability of each action. The third agent learns a probabilistic motion model that is used in a reinforcement learning style short-range planner. We have evaluated the control agents on unseen terrain in simulation and reality. This includes evaluating, on the real robot, agents that were trained purely in simulation. We have demonstrated that we can train control agents that approach human levels of performance. We conclude {{that it is possible to}} learn control agents for controlling mobile robots in these complex environments and that even a simple Situation-Action formulation can perform well on this task. The contributions of this thesis include the application of machine learning to a robot control task that cannot be easily modelled, an investigation of feature extractors for modelling complex terrain, a survey of learning techniques for modelling the decisions to be made in this domain and an autonomous search-based demonstrator from which we learn viable behaviours for the real robot...|$|R
40|$|The {{pressure}} of the global competition, continuously asking for lower costs and improved productivity, is forcing companies to seek global supply chains to cut production costs down. As a result, it {{is becoming more and}} more difficult to accurately monitor each step of a production process and to protect products from economically motivated fraud, adulterations and counterfeiting. In such context, traditional methods for product quality characterization, such as lab assays, are expensive, destructive, time-consuming, and for these reasons they have become inadequate in several applications. On the other hand, other approaches, such as absorption spectroscopy and computer vision, have been gaining much attention in the last decade, successfully contributing to speed up and automate the quality assessment exercise. Statistical modeling tools, particularly latent variable models (LVMs), are usually employed to exploit the information embedded in the large amount of highly correlated data (spectra and images) that absorption spectroscopy and computer vision generate. In the food and pharmaceutical sectors, product quality assessment still relies mainly on the judgment (of product color, odor, form, taste, etc.) of a panel of trained experts. Although the number of applications of LVMs as predictive tools for product quality monitoring is growing in these sectors, the use of LVMs for product quality assessment is usually tailored to each application, and general approaches to product quality assessment based on LVMs are lacking. The main objective of the research presented in this Dissertation is to overcome some of the limitations that hinder the diffusion of LVM tools in the food and pharmaceutical industrial practice. Three main strategies for product quality assessment are explored, namely the use of computer vision, the use of absorption spectroscopy, and the possibility of combining the information derived from different analytical instruments. With respect to the use of computer vision systems, the problem of maintaining such systems is discussed. Computer vision systems are deemed to be quick, accurate, objective and able to return reproducible results. However, likewise all other measurement systems, they need to be maintained. Alterations or failures (e. g. of the illuminating system or of the camera sensors) can dramatically affect measurement reproducibility, leading to a wrong product quality characterization. The problem of how to detect and manage these alterations or failures is discussed through a pharmaceutical engineering case study. General strategies are proposed to adapt a quality assessment model, which has been calibrated under certain environmental conditions, to new conditions. Results show that long downtime periods, which may be necessary to recalibrate the quality assessment model after a failure of the camera or of the lighting system, can be significantly reduced. Additionally, it is shown how image analysis can be effectively used not only to characterize the quality of a product, but also to improve the understanding on the production process (e. g., for troubleshooting or optimization purposes). In a specific pharmaceutical application, image analysis is used to investigate the causes leading to the erosion of tablets, allowing one to evaluate the effect of different physical phenomena occurring in the film-coating process. Additionally, the model relating the process conditions to the tablets quality is shown to be useful for process monitoring purposes. With respect to the use of absorption spectroscopy, a novel methodology to preprocess and classify spectral data is proposed. Traditionally, LVMs are built after some preprocessing of the raw spectra, and the optimal preprocessing strategy is chosen trough a time consuming trial-and-error procedure. Results from three different food engineering case studies show that the proposed methodology performs similarly to other existing approaches, but it uses a sequence of totally automated preprocessing steps, with no need for <b>trial-and-error</b> <b>searches.</b> Especially in the food industry, LVMs are usually tailored on the specific product being analyzed. For instance, for the detection of the fresh/frozen-thawed substitution fraud in fish fillets, a model is calibrated for each fish species possibly subject to substitution. This Dissertation considers a different approach: some strategies are proposed to design a multi-species, and possibly species-independent, classification model to detect this substitution fraud. The most promising strategy decomposes the information embedded in the spectral data using a single model, and it is shown to return the same overall accuracy of traditional approaches that employ one classification model for each species under investigation. Finally, with respect to the use of data fusion, it is shown how to effectively combine the information derived from different analytical instruments (such as spectrometers, digital cameras, texture analyzers, etc.) to enhance product quality characterization. Results on two food engineering case studies show that fusing the available information, rather than using them separately, improves the ability of assessing product quality...|$|R
40|$|Variability in nervous {{systems is}} often {{taken to be}} merely “noise. ” Yet {{in some cases it}} may play a positive, active role in the {{production}} of behavior. The central pattern generator (CPG) that drives the consummatory feeding behaviors of Aplysia generates large, quasi-random variability in the parameters of the feeding motor programs from one cycle to the next; the variability then propagates through the firing patterns of the motor neurons to the contractions of the feeding muscles. We have proposed that, when the animal is faced with a new, imperfectly known feeding task in each cycle, the variability implements a <b>trial-and-error</b> <b>search</b> through the space of possible feeding movements. Although this strategy will not be successful in every cycle, over many cycles it may be the optimal strategy for feeding in an uncertain and changing environment. To play this role, however, the variability must actually appear in the feeding movements and, presumably, in the functional performance of the feeding behavior. Here we have tested this critical prediction. We have developed a technique to measure, in intact, freely feeding animals, the performance of Aplysia swallowing behavior, by continuously recording with a length transducer the movement of the seaweed strip being swallowed. Simultaneously, we have recorded with implanted electrodes activity at each of the internal levels, the CPG, motor neurons, and muscles, of the feeding neuromusculature. Statistical analysis of a large dataset of these recordings suggests that functional performance is not determined strongly by one or a few parameters of the internal activity, but weakly by many. Most importantly, the internal variability does emerge in the behavior and its functional performance. Even when the animal is swallowing a long, perfectly regular seaweed strip, remarkably, the length swallowed from cycle to cycle is extremely variable, as variable as the parameters of the activity of the CPG, motor neurons, and muscles...|$|E
40|$|A {{computationally}} {{efficient algorithm}} for minimizing the flight time of an aircraft in a variable wind field has been invented. The algorithm, {{referred to as}} Neighboring Optimal Wind Routing (NOWR), is based upon neighboring-optimal-control (NOC) concepts and achieves minimum-time paths by adjusting aircraft heading according to wind conditions at an arbitrary number of wind measurement points along the flight route. The NOWR algorithm may either {{be used in a}} fast-time mode to compute minimum- time routes prior to flight, or may be used in a feedback mode to adjust aircraft heading in real-time. By traveling minimum-time routes instead of direct great-circle (direct) routes, flights across the United States can save an average of about 7 minutes, and as much as one hour of flight time during periods of strong jet-stream winds. The neighboring optimal routes computed via the NOWR technique {{have been shown to be}} within 1. 5 percent of the absolute minimum-time routes for flights across the continental United States. On a typical 450 -MHz Sun Ultra workstation, the NOWR algorithm produces complete minimum-time routes in less than 40 milliseconds. This corresponds to a rate of 25 optimal routes per second. The closest comparable optimization technique runs approximately 10 times slower. Airlines currently use various <b>trial-and-error</b> <b>search</b> techniques to determine which of a set of commonly traveled routes will minimize flight time. These algorithms are too computationally expensive for use in real-time systems, or in systems where many optimal routes need to be computed in a short amount of time. Instead of operating in real-time, airlines will typically plan a trajectory several hours in advance using wind forecasts. If winds change significantly from forecasts, the resulting flights will no longer be minimum-time. The need for a computationally efficient wind-optimal routing algorithm is even greater in the case of new air-traffic-control automation concepts. For air-traffic-control automation, thousands of wind-optimal routes may need to be computed and checked for conflicts in just a few minutes. These factors motivated the need for a more efficient wind-optimal routing algorithm...|$|E
40|$|Parallelized {{versions}} of genetic algorithms (GAs) are popular primarily for three reasons: the GA is an inherently parallel algorithm, typical GA applications are very compute intensive, and powerful computing platforms, especially Beowulf-style computing clusters, {{are becoming more}} affordable and easier to implement. In addition, the low communication bandwidth required allows the use of inexpensive networking hardware such as standard office ethernet. In this paper we describe a parallel GA and its use in automated high-level circuit design. Genetic algorithms are a type of <b>trial-and-error</b> <b>search</b> technique that are guided by principles of Darwinian evolution. Just as the genetic material of two living organisms can intermix to produce offspring that are better adapted to their environment, GAs expose genetic material, frequently strings of 1 s and Os, to the forces of artificial evolution: selection, mutation, recombination, etc. GAs start with a pool of randomly-generated candidate solutions which are then tested and scored {{with respect to their}} utility. Solutions are then bred by probabilistically selecting high quality parents and recombining their genetic representations to produce offspring solutions. Offspring are typically subjected to a small amount of random mutation. After a pool of offspring is produced, this process iterates until a satisfactory solution is found or an iteration limit is reached. Genetic algorithms have been applied {{to a wide variety of}} problems in many fields, including chemistry, biology, and many engineering disciplines. There are many styles of parallelism used in implementing parallel GAs. One such method is called the master-slave or processor farm approach. In this technique, slave nodes are used solely to compute fitness evaluations (the most time consuming part). The master processor collects fitness scores from the nodes and performs the genetic operators (selection, reproduction, variation, etc.). Because of dependency issues in the GA, it is possible to have idle processors. However, as long as the load at each processing node is similar, the processors are kept busy nearly all of the time. In applying GAs to circuit design, a suitable genetic representation 'is that of a circuit-construction program. We discuss one such circuit-construction programming language and show how evolution can generate useful analog circuit designs. This language has the desirable property that virtually all sets of combinations of primitives result in valid circuit graphs. Our system allows circuit size (number of devices), circuit topology, and device values to be evolved. Using a parallel genetic algorithm and circuit simulation software, we present experimental results as applied to three analog filter and two amplifier design tasks. For example, a figure shows an 85 dB amplifier design evolved by our system, and another figure shows the performance of that circuit (gain and frequency response). In all tasks, our system is able to generate circuits that achieve the target specifications...|$|E
40|$|Fallen trees {{participate}} in several important forest processes, which motivates {{the need for}} information about their spatial distribution in forest ecosystems. Several {{studies have shown that}} airborne LiDAR is a valuable tool for obtaining such information. In this paper, we propose an integrated method of detecting fallen trees from ALS point clouds based on merging small segments into entire fallen stems via the Normalized Cut algorithm. A new approach to specifying the segment similarity function for the clustering algorithm is introduced, where the attribute weights are learned from labeled data instead of being determined manually. We notice the relationship between Normalized Cut’s similarity function and a class of regression models, which leads us to the idea of approximating the task of learning the similarity function with the simpler task of learning a classifier. Moreover, we set up a virtual fallen tree generation scheme to simulate complex forest scenarios with multiple overlapping fallen stems. The classifier trained on this simulated data yields a similarity function for Normalized Cut. Tests on two sample plots from the Bavarian Forest National Park with manually labeled reference data show that the trained function leads to high-quality segmentations. Our results indicate that the proposed data-driven approach can be a successful alternative to time consuming <b>trial-and-error</b> or grid <b>search</b> methods of finding good feature weights for graph cut algorithms. Also, the methodology can be generalized to other applications of graph cut clustering in remote sensing...|$|R
40|$|With {{the advent}} of {{ubiquitous}} computing devices and their associated capabilities for capturing photos, videos, audio, and text, user-generated media content and its large-scale distribution have become a reality. While then years ago the production and dissemination of most digital media was practically restricted to commercial content providers and a few enthusiasts, nowadays, most people {{have the means to}} create, publish, and consume high-quality media. What is interesting, however, is that despite this shift in paradigm, one of the most fundamental underlying mechanisms – navigation in digital media – has hardly changed at all and is in many cases still cumbersome and difficult. Users’ navigation goals are mostly formulated in terms of the actual contents of a medium; finding the point in a movie where two story lines finally meet or accessing the part of a text where a certain argument is being made are useful and clearly defined goals. Navigation interfaces, however, are still formulated in terms of the generalizable form of a medium; they allow users to move to certain timestamps or page numbers. This means that the goal of a navigation task is rarely concerned with the functional entities that current navigation techniques operate on. As a result, the users have to provide the capability to convert between their semantic navigation goals that refer to the content and a syntactic expression of that goal that is compatible with the interface language. This conversion, of course, is non-trivial and usually has to be found out by the users through <b>trial-and-error</b> or exhaustive <b>search.</b> One of the obvious reasons for this mismatch of languages between semantic user intent and syntactic navigation interface is the analog heritage of the latter: Interfaces to control and navigate analog media {{do not have access to}} the contents of these media and, consequently, have to operate on the form as the common structure. Computer interfaces for digital media, on the other hand, could have the ability to inspect the content and allow users to formulate in semantic terms. Still, most of these interfaces are directly modeled after their analog counterparts with- out leveraging this potential of the medium’s digital representation. In this thesis, we present an interaction model for navigation in digital media that represents the interface together with the form and content of the medium. This combination of structural representations of the interface and the medium is novel. It gives us a holistic framework in which we can describe existing navigation interfaces, analyze their shortcomings for semantic navigation goals, and guide the creation of semantic navigation interfaces that allow to directly express such goals. In addition, we propose a simple four-step design guideline that builds upon this framework and helps interface designers to create semantic navigation interfaces. The viability of both the interaction model and the design guideline is demonstrated in the context of four research projects that each create and evaluate a semantic navigation interface for a different type of medium. The interfaces and navigation techniques that have emanated from these projects have already been shown to outperform conventional methods as well in measured efficiency as in perceived ease of use and subjective preference. While each thus is a contribution in itself to the respective field, their main purpose in the context of this work is to each exemplify and illustrate one specific step of our four-step guideline in detail...|$|R

