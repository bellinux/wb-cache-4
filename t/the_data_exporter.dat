0|10000|Public
40|$|The Weather and Climate Toolkit (WCT) is free, {{platform}} independent software distributed from NOAA’s National Climatic <b>Data</b> Center (NCDC). <b>The</b> WCT allows <b>the</b> visualization and <b>data</b> export of weather and climate data, including NEXRAD Radar, GOES Satellite, NOMADS Model and surface in-situ <b>data.</b> By leveraging <b>the</b> NetCDF for Java library and Common <b>Data</b> Model, <b>the</b> WCT is extremely scalable {{and capable of}} supporting many new datasets in the future [4; 6]. In addition, the WCT provides access to remote web services for instant access to products such as the Drought Monitor, NEXRAD reflectivity mosaics, multi-sensor precipitation totals and <b>the</b> Severe Weather <b>Data</b> Inventory. <b>The</b> WCT Viewer provides tools for custom data overlays, Web Map Service (WMS) background maps, animations and basic filtering. The export of images and movies is provided in multiple formats. <b>The</b> WCT <b>Data</b> <b>Exporter</b> allows for <b>data</b> export in both vector polygon (Shapefile, Well-Known Text) and raster (GeoTIFF, ESRI Grid, VTK, Gridded NetCDF) formats [Appendix A]. These data export features promote the interoperability of weather and climate information with various scientific communities and common software packages includin...|$|R
40|$|This paper {{proposes a}} nonparametric test {{in order to}} {{establish}} the level of accuracy of the foreign trade statistics of 17 Latin American countries when contrasted with the trade statistics of the main partners in 1925. The Wilcoxon Matched-Pairs Ranks test is used to determine whether the differences between <b>the</b> <b>data</b> registered by <b>exporters</b> and importers are meaningful, and if so, whether the differences are systematic in any direction. The paper tests for the reliability of <b>the</b> <b>data</b> registered for two homogeneous products, petroleum and coal, both in volume and value. The conclusion of the several exercises performed is that we cannot accept the existence of statistically significant differences between <b>the</b> <b>data</b> provided by <b>the</b> <b>exporters</b> and the registered by the importing countries in most cases. The qualitative historiography of Latin American describes its foreign trade statistics as mostly unusable. Our quantitative results contest this view. Latin America, statistical accuracy, international trade data, nonparametric methods, petroleum trade, coal trade. ...|$|R
40|$|Abstract. With {{the growing}} numbers of status update {{websites}} and related wrappers, initiatives modelling sensor data in RDF, {{as well as the}} dynamic nature of many Linked <b>Data</b> <b>exporters,</b> {{there is a need for}} protocols enabling real-time notification and broadcasting of RDF data updates. In this paper we present a flexible approach that provides such notifications to be delivered in real-time to any RSS or Atom reader. Our framework enables the active delivery of SPARQL query results through the PubSubHubbub (PuSH) protocol upon the arrival of new information in RDF stores. Our open source implementation can be plugged on any SPARQL endpoint and can directly reuse PuSH hubs that are already deployed in scalable clouds (e. g. Google’s). ...|$|R
40|$|This paper makes three contributions. (1) It {{summarizes}} in tabular form {{a recent}} literature made of 36 micro-econometric studies for 16 different {{countries on the}} relationship between export destination and firm performance. (2) It reports estimates of the productivity premium of German firms exporting to the Euro-zone and beyond, controlling for unobserved time invariant firm specific effects, and tests for self-selection of more productive firms into exporting beyond the Euro-zone. (3) It corrects a serious flaw in hitherto published studies that ignore the potentially disastrous consequences of extreme observations, or outliers. The paper shows that estimates of the exporter productivity premium by destination are driven by a small share of outliers. Using a "clean" sample without outliers the estimated productivity premium of firms that export to the Euro-zone only is no longer much smaller that the premium of firms that export beyond the Euro-zone, too, and the premium itself over firms that serve the German market only is tiny. Furthermore, an ex-ante differential that is statistically significant and large only shows up for enterprises that exported to the Euro-zone already and start to export to countries outside the Euro-zone. These conclusions differ considerably from those based on non-robust standard regression analyses. robust estimation, panel <b>data,</b> <b>exporter</b> productivity premium, export destinations...|$|R
40|$|Both {{theoretical}} and empirical models were developed in this paper to examine how exporters 2 ̆ 7 response to real exchange rate volatility (RERV) and {{real exchange rate misalignment}} (RERM) varies across industries in China. The theoretical model indicates that the impact of RERV depends on exporters 2 ̆ 7 attitude to risk while the effect of RERM is ambiguous. Using disaggregated industry <b>data,</b> Chinese <b>exporters</b> were found to be averse to RERV and RERM. This suggests that the negative impact on China 2 ̆ 7 s exports resulting from a revaluation of the RMB will be mitigated by a positive impact due to the reduction of RERM...|$|R
40|$|This paper {{investigates the}} link between {{exporting}} and importing activities and firm performance using a rich dataset on Egyptian and Moroccan firms. We test the export premium, self-selection and learning-by-exporting hypotheses using a number of firm characteristics. Our analysis also includes importing activities {{as a source of}} learning and considers their effects on productivity changes. A differences-in-differences matching estimator is used to address the endogeneity bias of target variables. The main results for Egyptian firms echo those reported for other countries using firm-level <b>data,</b> namely <b>exporters</b> are larger and more productive than non-exporters. In contrast, Moroccan exporters and non-exporters are strikingly similar. More specifically, no evidence is found of pre or post-entry differences in labour productivity for Moroccan firms...|$|R
40|$|This paper {{investigates the}} effects of {{connectivity}} charges (communication costs) on bilateral exports in Sub Saharan Africa (SSA). <b>Data</b> from 19 <b>exporter</b> countries was used together with communication costs data in a {{gravity model of trade}} setup. <b>The</b> export <b>data</b> derive from <b>the</b> IMF Direction of Trade and the COMTRADE databases, while <b>the</b> communication cost <b>data</b> was collated {{from a variety of sources}} including direct contact with service providers. We find that communication cost is an important factor in bilateral trade in the region. Communications have a significant negative effect on export intensity. The study also reveals that countries with high communication costs generally have lower export intensity than countries with low communication costs. The results suggest that investment in ICT infrastructure that brings down international communication costs will have a positive effect on regional trade in the long run. ...|$|R
40|$|This paper reports {{challenges}} and opportunities that arise when introducing advanced data analysis and visualization techniques to science policy practice. It discusses a rich set of practical questions that could be answered using existing tools and presents first results from an exemplary analysis of MEDLINE Publication Output by the National Institutes of Health (NIH) using nine years of <b>ExPORTER</b> <b>data.</b> We conclude with {{suggestions on how to}} improve the transfer of academic expertise and tools to science policy practice. Challenges and Opportunities The transfer of advanced data mining and visualization algorithms and tools developed in academia to science policy practice poses major challenges. Among <b>the</b> challenges are <b>data</b> access and privacy issues (familiarity with <b>the</b> <b>data</b> is key to developing and testing meaningful tools and workflows, but data can be highly sensitive); software certification and installation needs (new algorithms become available continuously, tool development is iterative); as well as workload priorities and practices (Most agency staff have regular work hours and may have competing responsibilities and duties). Successful adoption of new analytical techniques b...|$|R
50|$|A pool of Metering Processes {{collects}} data packets at {{one or more}} Observation Points, optionally filters {{them and}} aggregates information about these packets. An Exporter then gathers each of the Observation Points together into an Observation Domain and sends this information via the IPFIX protocol to a Collector. Exporters and Collectors are in a many-to-many relationship: One <b>Exporter</b> can send <b>data</b> to many Collectors and one Collector can receive <b>data</b> from many <b>Exporters.</b>|$|R
40|$|We use {{detailed}} <b>data</b> on <b>exporters</b> from Costa Rica, Ecuador and Uruguay {{as well as}} {{on their}} buyers to show that: aggregate exports are disproportionally driven by few multi-buyers exporters; and each multi-buyer exporter's foreign sales of any product are in turn accounted for by few dominant buyers. We propose an analytically solvable multi-country model of endogenous selection in which dominant exporters, dominant products and dominant buyers emerge in parallel as multi-product sellers with heterogeneous technologies compete for buyers with heterogeneous needs. The model not only provides an explanation of the existence of dominant buyers but also makes specific predictions on how the relative importance of dominant buyers should vary across export destinations depending on their market size and accessibility. We show that these predictions are borne out by our data and discuss their welfare implications in terms of gains from trade...|$|R
40|$|Detailed firm-level <b>data</b> on French <b>exporters</b> {{suggests}} most of {{the trade}} collapse occurred in exporters' volumes rather {{than the number of}} exporters. Small exporters suffered similarly to their larger counterparts. There is clear evidence that the impact was greatest on firms in sectors that rely most heavily on external finance. Thus, the crisis may not have long-lasting effects on aggregate export capacity - the reservoir of small and promising firms has not been decimated- but firms may reorganise to reduce vulnerability to external financing...|$|R
5000|$|Useful {{functions}} {{that use the}} class row are: void write(table &t, row &r, int idx); //write <b>the</b> <b>data</b> in <b>the</b> <b>data</b> file of <b>the</b> table void read(table &t, row &r, int idx); //read <b>the</b> <b>data</b> from <b>the</b> <b>data</b> file of <b>the</b> table void del(char *file, table &t, int idx); //delete <b>the</b> <b>data</b> from <b>the</b> <b>data</b> file of <b>the</b> table ...|$|R
30|$|HB {{designed}} the project, participated {{in developing the}} analysis plan, and drafted the manuscript and manuscript revisions. JC participated in designing the project and developing the analysis plan and drafted portions of the original and revised versions of the manuscript. CM participated in collecting <b>the</b> <b>data</b> and developing <b>the</b> analysis plan, coordinated <b>the</b> <b>data</b> analysis, and helped to draft the manuscript. RA participated in collecting <b>the</b> <b>data,</b> developing <b>the</b> analysis plan, and conducting <b>the</b> <b>data</b> analysis. AF participated in collecting <b>the</b> <b>data,</b> developing <b>the</b> analysis plan, and conducting <b>the</b> <b>data</b> analysis. Susan M participated in collecting <b>the</b> <b>data,</b> developing <b>the</b> analysis plan, and conducting <b>the</b> <b>data</b> analysis. Suki M participated in collecting <b>the</b> <b>data,</b> developing <b>the</b> analysis plan, and conducting <b>the</b> <b>data</b> analysis. AMV participated in collecting <b>the</b> <b>data,</b> developing <b>the</b> analysis plan, and conducting <b>the</b> <b>data</b> analysis. All authors read and approved the final manuscript.|$|R
40|$|One of {{the most}} {{prominent}} features {{in the evolution of the}} European Union (EU) has been its geographical expansion. Using a dynamic general equilibrium approach, this paper predicts the effects of future eastward expansions of the EU on both inter- and intra-national flows of trade and labor. Underlying the simulations is a spatial model of the EU incorporating heterogeneous firms, intra-industry trade, iceberg trade costs, and many possible locations. Locations are populated by a large number of potential firms, and these firms employ labor that varies across countries in its relative skill. The dynamics of the model are such that unprofitable firms are forced to exit in the long run, and workers have the opportunity to migrate in response to steep gradients in real compensation. Novel features of <b>the</b> <b>data</b> used here are that locations are defined in a very precise way and that the simulations take as their starting point a proxy for the actual distribution of economic activity across the European landmass. The model is calibrated to match aggregate trade and migration <b>data</b> from <b>the</b> 2004 enlargement as well as <b>data</b> on <b>exporter</b> characteristics. Simulations of enlargement predict an increase in aggregate exports of potential new members to the previous EU- 15 of 4. 7 percent of GDP in the five-year period following adoption of the acquis communautaire and net migration flows from potential new members to the previous EU- 15 of 1. 3 percent of aggregate acceding country population over the same period. Moreover, the simulations deliver many of the stylized facts of economic geography. ...|$|R
40|$|UNU-MERIT Working Papers {{intend to}} {{disseminate}} preliminary results of research {{carried out at}} UNU-MERIT and MGSoG to stimulate discussion on the issues raised. Communication Costs and Trade in Sub‐Saharan Africa by Evans Mupela * and Adam Szirmai** (����UNU‐MERIT, �����������June 2012) This paper investigates the effects of connectivity charges (communication costs) on bilateral exports in Sub Saharan Africa (SSA). <b>Data</b> from 19 <b>exporter</b> countries was used together with communication costs data in a {{gravity model of trade}} setup. <b>The</b> export <b>data</b> derive from <b>the</b> IMF Direction of Trade and the COMTRADE databases, while <b>the</b> communication cost <b>data</b> was collated {{from a variety of sources}} including direct contact with service providers. We find that communication cost is an important factor in bilateral trade in the region. Communications have a significant negative effect on export intensity. The study also reveals that countries with high communication costs generally have lower export intensity than countries with low communication costs. The results suggest that investment in ICT infrastructure that brings down international communication costs will have a positive effect on regional trade in the long run...|$|R
5000|$|<b>The</b> logical <b>data</b> {{model is}} kept simple {{in order to}} {{minimize}} the amount of description accompanying <b>the</b> <b>data</b> and to decrease the difficulty of understanding the structure of <b>the</b> <b>data.</b> <b>The</b> <b>data</b> model chosen for the UVC-based preservation method linearizes <b>the</b> <b>data</b> elements into a hierarchy of tagged elements organized using a XML-like approach.The tagged data elements are extracted from <b>the</b> <b>data</b> stream of <b>the</b> digital file. A tag specifies the role that <b>the</b> <b>data</b> element plays in <b>the</b> <b>data</b> structure. <b>The</b> element tags hold the specific information about the content of <b>the</b> <b>data</b> in a technology-independent manner. Furthermore, <b>the</b> <b>data</b> elements tagged according to the schema are returned to the client in a Logical Data View (LDV) ...|$|R
40|$|Sorting of data is a {{processing}} {{which an}} arrangement of <b>the</b> original <b>data</b> in <b>the</b> random condition can be sequential, <b>the</b> <b>data</b> either from <b>the</b> smallest to <b>the</b> biggest <b>data,</b> or otherwise of <b>the</b> largest <b>data</b> until <b>the</b> <b>data</b> is <b>the</b> smallest. There are several methods that are quite popularin the process of sorting <b>the</b> <b>data</b> {{and one of them}} is the method of selection. Sorting <b>the</b> <b>data</b> by <b>the</b> method of selection can be explained with a simple way of searching for <b>data</b> that is <b>the</b> smallest of the composition of <b>the</b> initial <b>data</b> from <b>the</b> first <b>data</b> to recent <b>data.</b> Once <b>the</b> <b>data</b> is <b>the</b> smallest discovered later exchanged <b>data</b> with <b>the</b> first <b>data.</b> After <b>the</b> <b>data</b> was sought again the smallest but starting from <b>the</b> second <b>data</b> until after <b>the</b> last <b>data</b> and then exchanged with <b>the</b> <b>data</b> obtained both. And so on until <b>the</b> <b>data</b> obtained sequence from beginning to end...|$|R
30|$|<b>The</b> <b>data</b> {{provider}} returns <b>the</b> interest request {{which is}} related to <b>the</b> <b>data</b> package to <b>the</b> <b>data</b> requester.|$|R
40|$|While {{production}} of ICT equipment plays a subordinate role {{for economic growth}} {{in most of these}} countries, they do benefit from capital deepening arising from falling prices of ICT equipment. Adapting established growth accounting approaches to <b>the</b> <b>data</b> environment of low-income countries, we quantify the growth impacts of absorption of ICT equipment, finding that ICT-related capital deepening contributed 0. 2 percentage points to growth in low-income countries, and 0. 3 percentage points in low-middle-income countries. The latter is about half the level typically found for industrialized countries. Economic growth;Development;Developing countries;Capital;Economic models;Emerging markets;Information technology;Low-income developing countries;Productivity;Telephone systems;trade data, technological advances, net exports, factor shares, technologies, trade flows, output growth, domestic production, technological change, partner country, technological progress, partner countries, information and communication technologies, world economy, data processing, data processing equipment, economic cooperation, data reporting, trade effect, multiplier effects, <b>data</b> sources, net <b>exporter,</b> imperfect competition, world trade, world growth, global trade, communication technology, trade partners, trade patterns, gross exports, information technologies, global market, information processing, bilateral trade, import tariffs, indirect taxes, commodity trade, bilateral trade flows, computer price, patterns of trade, world exports, information and communication technology...|$|R
30|$|<b>The</b> <b>data</b> {{clustering}} algorithm compares <b>the</b> <b>data</b> {{obtained with}} <b>the</b> set of clusters. If <b>the</b> <b>data</b> sample fits in some cluster, {{it will be}} marked as normal. Otherwise, <b>the</b> <b>data</b> will be labeled as an anomaly.|$|R
50|$|The {{binary search}} {{algorithm}} {{is a method}} of searching a sorted array for a single element by cutting the array in half with each recursive pass. The trick is to pick a midpoint {{near the center of}} the array, compare <b>the</b> <b>data</b> at that point with <b>the</b> <b>data</b> being searched and then responding to one of three possible conditions: <b>the</b> <b>data</b> is found at <b>the</b> midpoint, <b>the</b> <b>data</b> at <b>the</b> midpoint is greater than <b>the</b> <b>data</b> being searched for, or <b>the</b> <b>data</b> at <b>the</b> midpoint is less than <b>the</b> <b>data</b> being searched for.|$|R
30|$|DB {{researched}} <b>the</b> <b>data</b> {{and wrote}} <b>the</b> manuscript. RC researched <b>the</b> <b>data</b> and VMA reviewed <b>the</b> <b>data.</b> FCRC researched <b>the</b> <b>data.</b> RGM researched <b>the</b> <b>data,</b> contributed to <b>the</b> discussion, and reviewed/edited the manuscript. JCBJ researched <b>the</b> <b>data.</b> RA contributed to writing the manuscript, {{contributed to the}} discussion, and reviewed/edited the manuscript. ABS contributed to writing the manuscript, contributed to the discussion, and reviewed/edited manuscript. All authors read and approved the final manuscript.|$|R
40|$|When {{trying to}} use {{scientific}} <b>data,</b> members of <b>the</b> research community {{need to know how}} <b>the</b> <b>data</b> can be found, how <b>the</b> <b>data</b> have been reviewed, whether <b>the</b> <b>data</b> have been preserved for future use, how <b>the</b> <b>data</b> are managed throughout <b>the</b> <b>data</b> lifecycle, and how <b>the</b> <b>data</b> have been used. The presentation answers these questions by describing a strategic approach to managing the workflow for the stewardship and curation of scientific data...|$|R
40|$|Quick Response (QR) Codes {{helps us}} in {{encoding}} <b>the</b> <b>data</b> in an efficient manner. <b>The</b> <b>data</b> capacity is limited according to <b>the</b> various <b>data</b> formats used. For increasing <b>the</b> <b>data</b> capacity, {{data to be}} encoded can first be compressed using any of <b>the</b> <b>data</b> compression techniques. Then, <b>the</b> <b>data</b> can be encoded. This paper suggests a technique for data compression which in turn helped to increase <b>the</b> <b>data</b> capacity of QR Codes. Results are compared with the normal QR Codes to find {{the efficiency of the}} new technique of encoding followed by compression...|$|R
30|$|RAS {{collected}} <b>the</b> <b>data,</b> {{participated in}} <b>the</b> analysis of <b>the</b> <b>data,</b> {{and participated in}} the drafting of the manuscript. CNM participated {{in the analysis of}} <b>the</b> <b>data.</b> SBC participated in the analysis of <b>the</b> <b>data</b> and in <b>the</b> drafting of the manuscript. FGM participated in the analysis of <b>the</b> <b>data</b> and in <b>the</b> drafting of the manuscript.|$|R
40|$|This manual {{details the}} input {{instructions}} to <b>the</b> <b>data</b> bank, and {{explanation of the}} program and its output. <b>The</b> <b>data</b> bank was developed in satisfaction of two of the study tasks, the equipment thermal requirement catalog and the equipment characteristics and constraints catalog. <b>The</b> <b>data</b> bank contains 109 components within space tug avionics system. Other systems were not included in <b>the</b> <b>data</b> bank due to the available information, however, with some program modification, other systems could be incorporated into <b>the</b> <b>data</b> bank program. <b>The</b> <b>data</b> bank was developed and checked out and is compatible with the Univac 1108, and the CDC 6500 operating systems. <b>The</b> <b>data</b> contained in <b>the</b> <b>data</b> bank is general in content with emphasis on the component thermal design. <b>The</b> <b>data</b> is applicable to any spacecraft program where the components contained in <b>the</b> <b>data</b> bank can be applied in satisfaction of the system and subsystem requirements...|$|R
30|$|Patient role: In program I, {{the patient}} belongs to <b>the</b> <b>data</b> owner, has <b>the</b> direct {{operation}} authority to <b>the</b> <b>data,</b> through <b>the</b> medical cloud’s after the examination, {{is responsible for}} uploads <b>the</b> local medical <b>data,</b> and protects <b>the</b> medical cloud <b>data’s</b> uniqueness. At <b>the</b> same time, the patient through the keyword search can get <b>the</b> <b>data</b> on <b>the</b> cloud. In program II, the patient no longer belongs to <b>the</b> <b>data</b> owner and cannot upload <b>the</b> medical <b>data,</b> even though <b>the</b> <b>data</b> originates from <b>the</b> patient; although the patient cannot directly access <b>the</b> <b>data</b> which searches <b>the</b> cloud, the patient may access it through the medical personnel who carries the keyword fuzzy search to carry on <b>the</b> <b>data</b> understanding [25].|$|R
40|$|Data {{analytics}} (such as {{association rule}} mining and decision tree mining) can discover useful statistical knowledge {{from a big}} data set. But protecting the privacy of <b>the</b> <b>data</b> provider and <b>the</b> <b>data</b> user in <b>the</b> process of analytics is a serious issue. Usually, the privacy of both parties cannot be fully protected simultaneously by a classical algorithm. In this paper, we present a quantum protocol for data mining that can much better protect privacy than the known classical algorithms: (1) if both <b>the</b> <b>data</b> provider and <b>the</b> <b>data</b> user are honest, <b>the</b> <b>data</b> user can know nothing about the database except the statistical results, and <b>the</b> <b>data</b> provider can get nearly no information about the results mined by <b>the</b> <b>data</b> user; (2) if <b>the</b> <b>data</b> user is dishonest and tries to disclose private information of the other, she/he will be detected with a high probability; (3) if <b>the</b> <b>data</b> provider tries to disclose the privacy of <b>the</b> <b>data</b> user, she/he cannot get any useful information since <b>the</b> <b>data</b> user hides his privacy among noises. Comment: 50 pages, 1 figur...|$|R
3000|$|If {{we decide}} to {{transmit}} <b>the</b> <b>data</b> row wise, <b>the</b> <b>data</b> to be sent is ([...] <b>the</b> <b>data</b> are separated by space) [...]...|$|R
30|$|The kinetic {{models were}} calculated, and <b>the</b> <b>data</b> showed that <b>the</b> second-order kinetic {{model is the}} best {{linearity}} with <b>the</b> <b>data</b> {{more than any other}} kinetic model. Intraparticle diffusion was likely to take place. <b>The</b> <b>data</b> have been analyzed using the isotherm models, and <b>the</b> <b>data</b> showed that Langmuir is the best linearity with <b>the</b> <b>data</b> more than any other isotherm model.|$|R
40|$|Abstract. <b>The</b> <b>Data</b> Handbook {{contains}} {{important information}} on <b>the</b> HST <b>data</b> GOs and GTOs receive, including explanations on <b>the</b> <b>data</b> format, on <b>the</b> <b>data</b> reduction steps {{performed by the}} automatic pipelines, on sources of uncertainties in <b>the</b> <b>data,</b> and on available tools for <b>data</b> analysis. <b>The</b> NICMOS sections of <b>the</b> <b>Data</b> Handbook will soon be available, and a draft can be already retrieved from the NICMOS WWW Documentation Page. In this Poster we present {{the organization of the}} NICMOS sections of <b>the</b> <b>Data</b> Handbook, and <b>the</b> relevant topics there covered. 1...|$|R
40|$|<b>The</b> <b>data</b> {{system of}} <b>the</b> {{spaceborne}} eight-beam NASA scatterometer for measuring ocean backscatter is a nonreal-time ground-based science {{data processing system}} which inputs backscatter telemetry, processes <b>the</b> <b>data</b> into wind vectors, and archives and distributes <b>the</b> wind vector <b>data</b> and other products. Special attention is given to changes to the baseline design intended to meet new requirements for <b>the</b> <b>data</b> granularity, <b>the</b> <b>data</b> needs of <b>the</b> science team receiving <b>the</b> <b>data,</b> and <b>the</b> telemetry <b>data</b> source...|$|R
40|$|The {{main concern}} in today’s growing IT sectors is the storage and {{maintenance}} of <b>the</b> <b>data.</b> As <b>the</b> <b>data</b> keep on updating according {{to the needs of}} users, there is a huge overhead in the maintenance of the hardware by the company. One of the solutions to this problem is the use of cloud storage for this enormous <b>data.</b> <b>The</b> cloud storage uses <b>the</b> huge <b>data</b> centers, which are remotely located, to store <b>the</b> <b>data.</b> In addition to the easy storage of <b>the</b> <b>data,</b> this huge data center also reduces the cost of maintenance of <b>the</b> <b>data.</b> However this distinct feature of cloud storage leads to many security issues which should be lucidly understood by the IT sectors. One of the emerging security issue would be the integrity of <b>the</b> <b>data</b> stored in <b>the</b> <b>data</b> center i. e. to check whether the cloud provider misuses <b>the</b> <b>data</b> or not. <b>The</b> cloud provider can misuses <b>the</b> <b>data</b> in many ways like they can copy or modify the file. Due to <b>the</b> storage of <b>data</b> on <b>the</b> <b>data</b> center, user is not able to access <b>the</b> <b>data</b> physically thus there should be the way by which user can check the reliability of <b>the</b> <b>data</b> on <b>the</b> cloud. In this paper we provide the scheme to check the reliability of <b>the</b> <b>data</b> and this scheme can be agreed upon by both the user and the cloud provider...|$|R
50|$|Sharing the {{architectural}} core with Stratature +EDM, Master Data Services uses a Microsoft SQL Server database as <b>the</b> physical <b>data</b> store. It {{is a part}} of <b>the</b> Master <b>Data</b> Hub, which uses the database to store and manage data entities. It is a database with the software to validate and manage <b>the</b> <b>data,</b> and keep it synchronized with the systems that use <b>the</b> <b>data.</b> <b>The</b> master <b>data</b> hub has to extract <b>the</b> <b>data</b> from <b>the</b> source system, validate, sanitize and shape <b>the</b> <b>data,</b> remove duplicates, and update the hub repositories, as well as synchronize the external sources. The entity schemas, attributes, data hierarchies, validation rules and access control information are specified as metadata to <b>the</b> Master <b>Data</b> Services runtime. Master Data Services does not impose any limitation on <b>the</b> <b>data</b> model. Master Data Services also allows custom Business rules, used for validating and sanitizing <b>the</b> <b>data</b> entering <b>the</b> <b>data</b> hub, to be defined, which is then run against <b>the</b> <b>data</b> matching <b>the</b> specified criteria. All changes made to <b>the</b> <b>data</b> are validated against the rules, and a log of the transaction is stored persistently. Violations are logged separately, and optionally the owner is notified, automatically. All <b>the</b> <b>data</b> entities can be versioned.|$|R
5000|$|... #Caption: TPM drift: top: The green graph is <b>the</b> <b>data</b> of <b>the</b> TPM experiment, {{the black}} curve is {{smoothing}} of <b>the</b> <b>data</b> (<b>the</b> drift).bottom: Subtraction of the drift from <b>the</b> <b>data.</b>|$|R
40|$|In today’s {{information}} age, <b>the</b> <b>data</b> is {{an important}} asset of the organization so the security of <b>the</b> <b>data</b> is {{a vital role in}} the industry. In order to achieve the above aspect, <b>the</b> <b>data</b> masking is used. <b>The</b> <b>data</b> masking is mainly <b>the</b> <b>data</b> is replaced with realistic but not <b>the</b> original <b>data.</b> <b>The</b> main objective is to make sensitive information is not made available outside of <b>the</b> environment. <b>The</b> <b>data</b> masking is to just provide the copy of <b>the</b> production <b>data</b> in support of the development environment and thus it controls the leakage of <b>the</b> <b>data.</b> Data masking are designed to be repeatable so referential integrity is maintained. We have explored <b>the</b> <b>data</b> masking architecture, techniques with realistic data and order of masking. We hope this paper will help quality analyst in pulling data from production environments into test environment for quality analysis by safe guarding the original information...|$|R
