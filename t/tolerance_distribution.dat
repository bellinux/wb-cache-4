20|88|Public
50|$|Dilution assays can be {{direct or}} indirect. In a direct {{dilution}} assay {{the amount of}} dose needed to produce a specific (fixed) response is measured, so that the dose is a stochastic variable defining the <b>tolerance</b> <b>distribution.</b> Conversely, in an indirect dilution assay the dose levels are administered at fixed dose levels, so that the response is a stochastic variable.|$|E
40|$|In {{developing}} mechanistic PK–PD models, {{incidence of}} toxic responses {{in a population}} has to be described in relation to measures of biologically effective dose (BED). We have developed a simple dose-incidence model that links incidence with BED for compounds that cause toxicity by depleting critical cellular target molecules. The BED in this model was the proportion of target molecule adducted by the dose of toxic compound. Our modeling approach first estimated the proportion depleted for each dose and then calculated the <b>tolerance</b> <b>distribution</b> for toxicity in relation to either administered dose or log of administered dose. We first examined cases where {{the mean of the}} <b>tolerance</b> <b>distribution</b> for toxicity occurred when a significant proportion of target had been adducted (i. e., more than half). When a normal distribution was assumed to exist for the relationship of incidence and BED, the <b>tolerance</b> <b>distribution</b> based on administered dos...|$|E
40|$|Why do some {{individuals}} survive after exposure to chemicals while others die? Either, the tolerance threshold is distributed among {{the individuals in}} a population, and its exceedance leads to certain death, or all individuals share the same threshold above which death occurs stochastically. The previously published General Unified Threshold model of Survival (GUTS) established a mathematical {{relationship between the two}} assumptions. According to this model stochastic death would result in systematically faster compensation and damage repair mechanisms than individual tolerance. Thus, we face a circular conclusion dilemma because inference about the death mechanism is inherently linked to the speed of damage recovery. We provide empirical evidence that the stochastic death model consistently infers much faster toxicodynamic recovery than the individual tolerance model. Survival data can be explained by either, slower damage recovery and a wider individual <b>tolerance</b> <b>distribution,</b> or faster damage recovery paired with a narrow <b>tolerance</b> <b>distribution.</b> The toxicodynamic model parameters exhibited meaningful patterns in chemical space, which is why we suggest toxicodynamic model parameters as novel phenotypic anchors for in vitro to in vivo toxicity extrapolation. GUTS appears to be a promising refinement of traditional survival curve analysis and dose response models...|$|E
5000|$|Unlike {{traditional}} {{confidence intervals}} which cannot usually be propagated through mathematical calculations, c-boxes {{can be used}} in calculations in ways that preserve the ability to obtain arbitrary confidence intervals for the results. For instance, they can be used to compute probability boxes for both prediction and <b>tolerance</b> <b>distributions.</b> [...] C-boxes can be computed {{in a variety of ways}} directly from random sample data. There are confidence boxes for both parametric problems where the family of the underlying distribution from which the data were randomly generated is known (including normal, lognormal, exponential, Bernoulli, binomial, Poisson), and nonparametric problems in which the shape of the underlying distribution is unknown. Confidence boxes account for the uncertainty about a parameter that comes from the inference from observations, including the effect of small sample size, but also potentially the effects of imprecision in the data and demographic uncertainty which arises from trying to characterize a continuous parameter from discrete data observations.|$|R
40|$|Graduation date: 1997 The {{fundamental}} {{objective of}} a design engineer in performing tolerance technology is to transform functional requirements into tolerances on individual parts based on existing data and algorithms for design tolerance analysis and synthesis. The transformation of functional requirements into tolerances must {{also consider the}} existing process capabilities and manufacturing costs to determine the optimal tolerances and processes. The main objective {{of this research is}} to present an integrated but modular system for Computer Aided Tolerance Allocation, Tolerance Synthesis and Process Selection. The module is implemented in AutoCAD using the ARX 1. 1 (AutoCAD Runtime Extension Libraries), MFC 4. 2, Visual C++ 4. 2, Access 7. 0, AutoCAD Development System, AutoLISP, and Other AutoCAD Customization tools. The integrated module has two functions: a. Tolerance analysis and allocation: This module uses several statistical and optimization techniques to aggregate component tolerances. Random number generators are used to simulate historical data used by most of the optimization techniques to perform tolerance analysis. Various component <b>tolerance</b> <b>distributions</b> are considered (Beta, Normal, and Uniform). The proposed analysis technique takes into consideration the distribution of each fabrication of the component, this provides designers. The proposed tolerance analysis method takes into consideration the distribution of each fabrication process of the assembly. For assemblies with non-normal natural process <b>tolerance</b> <b>distributions,</b> this method allows designers to assign assembly tolerances that are closer to actual assembly tolerances when compared to other statistical methods. This is verified by comparing the proposed tolerance analysis method to the results of Monte Carlo simulations. The method results in assembly tolerances similar to those provided by Monte Carlo simulation yet is significantly less computationally-intensive. b. Process Selection: This thesis introduces a methodology for concurrent design that considers the allocation of tolerances and manufacturing processes for minimum cost. This methodology brings manufacturing concerns into the design process. A simulated annealing technique is used to solve the optimization problem. Independent, unordered, manufacturing processes are assumed for each assembly. The optimization technique uses Monte Carlo simulation. A simulated annealing technique is used to control the Monte Carlo analysis. In this optimization technique, tolerances are allocated using the cost-tolerance curves for each of the individual components. A cost-tolerance curve is defined for each component part in the assembly. The optimization algorithm varies the tolerance for each component and searches systematically for the combination of tolerances that minimizes the cost. The proposed tolerance allocation/process selection method was found to be superior to other tolerance allocation methods based on manufacturing costs...|$|R
40|$|This {{thesis is}} {{concerned}} with the use of Artificial Intelligence techniques to support human designers. The thesis argues that support for human designers can be improved by adopting an Al-based rather than a geometry-based approach to engineering design. Design Support Systems (DSSs) are proposed as an effective means of delivering this improved support. Representing and reasoning about tolerance statements in design is introduced as a valid area to test these claims. Tolerance statements describe the allowable variations in the geometry of a designed artefact. Two distinct, but related problems involving the use of toler¬ ance statements in design are tackled, namely: tolerance combination (including the way <b>tolerance</b> <b>distributions</b> combine), and <b>tolerance</b> allocation. The problem of <b>tolerance</b> combination (and <b>distribution)</b> involves determining the necessary consequences of the application of known tolerance statements to one or more designed artefact features. Tolerance allocation concerns the assignment of tol¬ erance statements during the design process. Solutions to this second problem are essential before manufactured instances of designed artefacts can be tested for compliance with design descriptions. The use of an experimental DSS, the Edinburgh Designer System (EDS), to solve design problems is illustrated. The implementation of techniques to im¬ prove the support of tolerance combination and tolerance allocation is described and where possible has been tested using EDS. The way that design is situated within the product creation process is investigated and the derivation of parts list information from an EDS design description is demonstrated. The thesis con¬ cludes that the Al-based approach can improve support for human designers, but that further research will be required to demonstrate the effective delivery of this support through DSSs...|$|R
40|$|The {{introduction}} of innovative optimization techniques {{based on a}} genetic algorithm for tolerance synthesis, as proposed in [7], should {{be recognized as a}} major step toward the development of reliable and effective computer aided tolerancing tools. Aim of the present work is to investigate some implications of this approach and to suggest improved procedures for the <b>tolerance</b> <b>distribution</b> process that are verified to provide a significant reduction in the optimal cost when compared with the approaches published in previous studies...|$|E
40|$|More {{and more}} works are {{done on the}} design of the Unified Modeling Language (UML) which is {{designed}} to help us for modeling effective object oriented software, Existing Object-Oriented design methods are not mature enough to capture non-functional requirement such as concurrency, fault <b>tolerance,</b> <b>distribution</b> and persistence of a software approach. Our approach proposed to use aspect-oriented software development (AOSD) mechanisms to solve the issues for interactions of the communication diagram in UML that support only the Object-Oriented mechanisms,thus AOSD allow to design programs that are out of reach of strict Object-Orientation and could possibly improve the structures and implementations. Comment: 6 pager, 7 figures,journa...|$|E
40|$|The article {{describes}} the algorithm for applying the decomposition method to determine the requirements for the system of repair of military vehicles in assessing the effectiveness of its operation. If it is determined {{that the value of}} the efficiency indicator does not correspond to the normative value, then {{it will be necessary to}} influence the partial indicators, and for this it is necessary to know the requirements for these indicators. Therefore, in order to calculate the requirements for partial performance indicators for the said system, it is necessary to apply the method of <b>tolerance</b> <b>distribution</b> based on the fact that a known requirement for a generalized indicator of system efficiency...|$|E
40|$|We {{investigate}} {{the behavior of}} (univariate) cumulative distribution functions which are defined on an abstract linearly ordered space. Special emphasis {{is given to the}} study of a class of linearly ordered spaces which J. H. B. Kemperman introduced into the subject of nonparametric <b>tolerance</b> regions. <b>Distribution</b> functions on such spaces can be decomposed. Considerable attention is given to applications. In particular, it is shown how a number of nonparametric statistical procedures can be extended to include situations of multivariate and time dependent data...|$|R
50|$|DNS serves other {{purposes}} {{in addition to}} translating names to IP addresses. For instance, mail transfer agents use DNS {{to find the best}} mail server to deliver e-mail. The domain to mail exchanger mapping provided by MX records may present an additional layer of fault <b>tolerance</b> and load <b>distribution.</b>|$|R
25|$|DNS serves other {{purposes}} {{in addition to}} translating names to IP addresses. For instance, mail transfer agents use DNS {{to find the best}} mail server to deliver e-mail: An MX record provides a mapping between a domain and a mail exchanger; this can provide an additional layer of fault <b>tolerance</b> and load <b>distribution.</b>|$|R
40|$|Web service {{orchestration}} {{is widely}} spread {{for the creation}} of composite web services using standard specifications such as BPEL 4 WS. The myriad of specifications and aspects that should be considered in orchestrated web services are resulting in increasing complexity. This complexity leads to software infrastructures difficult to maintain with interwoven code involving different aspects such as security, fault <b>tolerance,</b> <b>distribution,</b> etc. In this paper, we present Zen-Flow a reflective BPEL engine that enables to separate the implementation of different aspects among them and from the implementation of the regular orchestration functionality of the BPEL engine. We illustrate its capabilities and performance exercising the reflective interface through a decentralized orchestration use case...|$|E
40|$|The {{articulated}} arm {{coordinate measuring machine}} (AACMM) is a new type coordinate measuring machine (CMM) base on the linkage structure. The kinematic model of a 6 -DOF AACMM with DH method was established, and from the kinematic model the coordinate systems and joint structural parameters of the AACMM are obtained. The Jacobian matrix was deduced by differential transformation from the kinematic model of the AACMM, {{and according to the}} Jacobian matrix, the error transfer coefficients of the joint structural parameters were calculated. Then with the calculation results the influence of the joint structural parameters on the measuring accuracy of the AACMM was analyzed, which provides a theoretical basis for calibration, <b>tolerance</b> <b>distribution</b> of the joint parts and components' selection of the AACMM...|$|E
40|$|A {{volumetric}} {{error compensation}} method for a machining center that has multiple cutting tools operating simultaneously has been developed. Due to axis sharing, the geometric errors of multi-spindle, concurrent cutting processes {{are characterized by}} a significant coupling of error components in each cutting tool. As a result, {{it is not possible}} to achieve exact volumetric error compensation for all axes. To minimize the overall volumetric error in simultaneous cutting, a method to determine compensation amount using weighted least squares has been proposed. This method also allows <b>tolerance</b> <b>distribution</b> of machining accuracy for different surfaces of a workpiece. A geometric error model has been developed using an arch-type, multi-spindle machine tool, and the error compensation simulation results based on this model are presented. The simulation results demonstrated effectiveness of the proposed error compensation algorithm for use with multi-spindle simultaneous cutting applications...|$|E
40|$|This paper {{introduces}} CIEL, {{a universal}} execution engine for distributed data-flow programs. Like previous execution engines, CIEL masks {{the complexity of}} distributed programming. Unlike those systems, a CIEL job can make data-dependent control-flow decisions, which enables it to compute iterative and recursive algorithms. We have also developed Skywriting, a Turingcomplete scripting language that runs directly on CIEL. The execution engine provides transparent fault <b>tolerance</b> and <b>distribution</b> to Skywriting scripts and highperformance code written in other programming languages. We have deployed CIEL on a cloud computing platform, and demonstrate that it achieves scalable performance for both iterative and non-iterative algorithms. ...|$|R
40|$|Twenty {{patients}} with {{carcinoma of the}} pancreas identified with ultrasonography and/or CT and confirmed by histology, were examined with MR before and after administration of an oral super-paramagnetic contrast medium. Ten patients were examined after administration of the contrast medium through a duodenal tube. Ten patients drank the contrast medium mixed with a viscosity-increasing agent. Organ delineation and diagnostic information were improved in the postcontrast scans in both groups and the viscous contrast suspension yielded better delineation and diagnostic information than the aqueous suspension. The viscous contrast suspension also showed better <b>tolerance,</b> contrast <b>distribution</b> and less artifacts than the aqueous suspension...|$|R
50|$|This {{species is}} {{classified}} as Least Concern (LC) according to the IUCN Red List of Threatened Species (v3.1, 2001). So listed due to its wide <b>distribution,</b> <b>tolerance</b> of {{a broad range of}} habitats, presumed large population, and because it is unlikely to be declining fast enough to qualify for listing in a more threatened category.|$|R
40|$|A {{pest control}} model with {{multiple}} treatments is presented. Population dynamics and damage rates are temperature dependent with predation aHected by pesticide application. Pesticide <b>tolerance</b> <b>distribution</b> {{is used for}} constructing the estimated kill function. Mathematical-numerical optimization is applied, selecting frequency and dosage which minimize control costs and crop damage. Key words: pest control; kill function; numerical optimization; population dynamics; multiple treatments. T H E APPLICATION of pesticides as a means ofpest control is becoming an increasinglymore important issue and, hence, is attract-ing more attention of economists [3, 5, 8, 9, 10, and 14]. Pollution aspects, reinforced by the current "energy crisis " which will undoubtedly boost the price of pesticides, add more dimen-sions to {{the widespread use of}} pesticides. There are good reasons to believe that farmers overuse pesticides as a means of insurance against pes...|$|E
40|$|Master of ScienceDepartment of StatisticsWeixing SongThe {{potency of}} a {{pesticide}} or some materials is widely studied in agricultural and biological fields. The {{level of a}} stimulus that results in a response by 50 % of individuals in a population under study is an important characterizing parameter and it is denoted by the median lethal concentration (LC 50) or the median lethal dose (LD 50) or median. Estimation of LC 50 {{is a type of}} quantal response assays that belong to qualitative indirect bioassays. In this report, seven methods of estimating LC 50 are reviewed with reference to two normal distributions of tolerance in four different cases. Some modified methods are also discussed. Simulation shows that the maximum likelihood method generally outperforms all other traditional methods, if the true <b>tolerance</b> <b>distribution</b> is available. The comparison results indicate that the modified Dragstedt-Behrens method and modified Reed-Muench method are good substitutes for the original ones in most scenarios...|$|E
40|$|AbstractTolerances of {{components}} can accumulate {{to result in}} quality variations when these components are assembled. One {{way to reduce the}} assembled variation is to tighten tolerance specifications of each component that, however, increases product cost. This paper investigates the effect of grouping for components with uniform and normal distributions by the developed “grouped random assembly” method. It is a method that first sorts and divides components into several groups and then assemble each group with corresponding group in order to reducing assembly <b>tolerance.</b> <b>Distribution</b> of resultant dimension based on the “grouped random assembly” approach is then analyzed. The results showed that, without changing components’ tolerance specifications, assembly tolerance depends on the number of grouping. Tolerance stack-up can be dramatically reduced with a suitable grouping strategy. The merit of this research is to develop a theoretical foundation for the grouped random assembly method. The results lead to a design for assembly strategy that assembly tolerance can be effectively reduced without tightening components’ tolerances...|$|E
5000|$|Pristimantis paulodutrai is a {{very common}} frog living on low {{vegetation}} inside primary and secondary forest (up to 130m above sea level). It is not considered threatened (listed as [...] "Least Concern") by the IUCN because of its wide <b>distribution,</b> <b>tolerance</b> of habitat modification, presumed large population, and the unlikelihood of imminent rapid decline.|$|R
5000|$|The International Union for Conservation of Nature has {{assessed}} this toad's conservation status as being of [...] "least concern" [...] {{on the basis}} of its [...] "wide <b>distribution,</b> <b>tolerance</b> of a degree of habitat modification, presumed large population, and because it is unlikely to be declining fast enough to qualify for listing in a more threatened category." ...|$|R
25|$|Hostnames and IP {{addresses}} are {{not required}} to match in a one-to-one relationship. Multiple hostnames may correspond to a single IP address, which is useful in virtual hosting, in which many web sites are served from a single host. Alternatively, a single hostname may resolve to many IP addresses to facilitate fault <b>tolerance</b> and load <b>distribution</b> to multiple server instances across an enterprise or the global Internet.|$|R
40|$|AbstractHigh {{demand of}} wide use of {{three-dimensional}} operation models in process planning {{has resulted in}} an urgent exploration of new approaches to 3 D manufacturing process design. In this paper, 3 D process dimension and manufacturing tolerance design issue is studied. A 3 D tolerance zone calculation method is developed. The method consists of two procedures including 3 D tolerance zone modeling and its calculation. Small displacement torsors (SDT), {{in conjunction with the}} robotics kinematics, are utilized to establish the algebra model of the 3 D tolerance zone and the tolerance propagation model. Convex sets theory is applied in the calculation and optimization {{of the size of the}} zone. The proposed method facilitates the design of the tolerance of the zone by using the variation ranges of uncertain parameters instead of the <b>tolerance</b> <b>distribution.</b> An example of 3 D manufacturing tolerance design for a prismatic part is provided with Monte Carlo simulation results to illustrate the effectiveness of the proposed approach...|$|E
40|$|International audienceAmong {{the whole}} {{manufacturing}} cycle of a product, {{a sequence of}} manufacturing stages needs to be optimized using the increasingly available computing resources. Computer aided process planning {{is seen as the}} missing link between CAD and CAM, which relates to the translation of design tolerances into manufacturing tolerances to be executed in the shop floor. A computerized module for process plan simulation, taking into account the manufacturing dispersions, has been developed. The process plan simulation program, which consists of three procedures, uses a combination of the minimal transfer method and a modified form of the dispersions method. The first procedure performs a verification of the feasibility of the project's process plans through tolerance transfer. The second procedure performs an optimization of the <b>tolerance</b> <b>distribution</b> using the process capability data. The third procedure computes the manufacturing dimensions, which ensure the quality of the components and products. The simulation module has been validated on complex problems and shows that it gives good results in a short time. The manual work requires several days to solving this manufacturing problem...|$|E
40|$|Previously we {{reported}} a criterion for choosing an appropriate statistical model {{to describe the}} <b>tolerance</b> <b>distribution</b> for estimation of the median effec-tive dose [1]. In the present paper we have shown how this statistical approach {{can be used to}} assess the precision of toxicity test data and the predictive abili-ties of test methods. Specifically, we conducted statistical analyses of data generated by using the University of Pittsburgh (UP) and the National Bureau of Standards (NBS) combustion toxicity protocols. These evaluations revealed that 30 % of the LD 50 values from the UP test were not valid while with the NBS test 26 % and 5 % of the LD 50 values under flaming and non-flaming conditions, respectively, were not valid. Comparisons of the two test methods also showed that there was not agreement on ranking across protocols except for the very few with extreme toxicities. These results indicated that caution should be exercised in the review and interpretation of combustion toxicity data and that these tests are probably only useful in identifying consistently super-toxic chemicals...|$|E
40|$|Tolerance {{analysis}} of an assembly {{is an important}} issue for mechanical design. Among various tolerance analysis methods, statistical analysis is the most commonly employed method. However, the conventional statistical tolerance method is often based on the normal distribution. It fails to predict the resultant tolerance of an assembly of parts with non-normal distributions. In this paper, a novel method based on statistical moments is proposed. <b>Tolerance</b> <b>distributions</b> of parts are first transferred into statistical moments that are then used for computing tolerance stack-up. The computed moments, particularly the variance, the skewness and the kurtosis, are then mapped back to probability distributions in order to calculate the resultant tolerance of the assembly. The proposed method can be used to analyse the resultant tolerance specification for non-normal distributions with different skewness and kurtosis. Simulated results showed that tail coefficients of different distributions with the same kurtosis are close to each other for normalised probabilities between 3 and 3. That is, the tail coefficients of a statistical distribution can be predicted by the coefficients of skewness and kurtosis. Two examples are illustrated in the paper to demonstrate the proposed method. The predicted resultant tolerances of the two examples are only 0. 5 % and 1. 5 % differences compared with that by the Monte Carlo simulation for 1, 000, 000 samples. The proposed method is much faster in computation with higher accuracy than conventional statistical tolerance methods. The merit of the proposed method is that the computation is fast and comparatively accurate for both symmetrical and unsymmetrical distributions, particularly when the required probability is between 2 and 3...|$|R
40|$|Currently most {{distributed}} telecoms {{software is}} engineered using low and mid-level distributed technologies, {{but there is}} a drive to use high-level distribution. This paper reports the first systematic comparison of a high-level distributed programming language in the context of substantial commercial products. Our research strategy is to reengineer some C++/CORBA telecoms applications in Erlang, a high-level distributed language, and make comparative measurements. Investigating the potential advantages of the high-level Erlang technology shows that two significant benefits are realised. Firstly, robust configurable systems are easily developed using the high-level constructs for fault <b>tolerance,</b> and <b>distribution.</b> The Erlang code exhibits resilience: sustaining throughput at extreme loads and automatically recovering when load drops; availability: remaining available despite repeated and multiple failures; dynamic reconfigurability: with throughput scaling near-linearl...|$|R
40|$|In {{this paper}} a {{procedure}} of construction of [beta]-expectation tolerance {{regions in the}} framework of the structural method of inference has been developed. The procedure has been applied to the generalized multivariate model and the [beta]-expectation tolerance region for this case has been constructed assuming the normal distribution for the error variables of the model. Tolerance region [beta]-expectation <b>tolerance</b> region prediction <b>distribution</b> structural model structural distribution generalized multivariate model generalized Beta distribution...|$|R
40|$|This paper proposes an {{analytical}} solution for fast tolerance {{analysis of the}} assembly of components with a mean shift or drift {{in the form of}} a doubly-truncated normal distribution. The assembly of components with a mean shift or drift {{in the form of a}} uniform distribution (the Gladman model) can be calculated by this method as well since the uniform distribution is a special form of the doubly-truncated normal distribution. Integration formulae of the first four moments of the truncated normal distribution are first derived. The first four moments of the resultant <b>tolerance</b> <b>distribution</b> are then calculated. As a result, the resultant tolerance specification is represented as a function of the standard deviation and the coefficient of kurtosis of the resultant distribution. Based on this method, the calculated resultant tolerance specification is more accurate than that predicted by the Gladman's model or the simplified truncated normal model. The difference between this model and the Monte Carlo method with 1, 000, 000 simulation samples is less than 0. 5 %. The merit of the proposed method is that it is fast and accurate which is crucial for engineering applications in tolerance analysis...|$|E
40|$|Objectives: To {{test the}} {{feasibility}} of human immune globulin (IG, Gamimune N, 10 %) as a new treatment for endoph-thalmitis, the ocular <b>tolerance,</b> <b>distribution,</b> and ability of intravitreal IG to attenuate the toxic effects of Staphyloccocus aureus culture supernatant were evaluated in a rabbit model. Methods: Effects of intravitreally injected IG were assessed histologically and with Western blot analysis performed 1 to 5 days after injection. IG reactivity to products of S aureus strain RN 4220 was tested by Western blotting, using known toxins (beta hemolysin and toxic shock syndrome toxin- 1) and a concentrated culture supernatant containing S aureus exotoxins (pooled toxin, PT). Endophthalmitis was induced by intravitreal PT injection. For treatment, IG and PT were mixed and injected simultaneously, or IG was injected immediately after, or 6 hours after, PT injection. PT toxicity was graded clinically and histologically over 9 days. Results: IG persisted intravitreally at least 5 days, inducing no clinical inflammation and minimal mononuclear cell infil-tration. In the endophthalmitis model, toxicity from PT was significantly reduced when IG was mixed with PT and injected simultaneously, or when IG was delivered immediately after PT. Only minimal clinically detectable reductions were observed when IG delivery was delayed 6 hours. Conclusions: Intravitreal IG is well tolerated in the rabbit eye and attenuates the toxicity of culture supernatant contain-ing S aureus exotoxins. Because toxin elaboration likely occurs gradually in true infection, reduced effects observed with delayed treatment in this toxin-injected model do not preclude clinical application. IG may represent a novel adjunct in endophthalmitis treatment. Trans Am Ophthalmol Soc 2004; 102 : 305 - 32...|$|E
40|$|Dose-response {{relationships}} for incidence {{are based}} on quantal response measures. A defined effect is either present or not present in an individual. The dose-incidence curve therefore reflects differences in individual susceptibility (the "tolerance distribution”). At low dose, only the more susceptible individuals manifest the effect, while higher doses are required for more resistant individuals to be recruited into the affected fraction of the group. Here, we analyze how such dose-incidence relationships are related to mechanism-based dose-response relationships for biological effects described on a continuous scale. As an example, we use the quantal effect "cell division” triggered by occupancy of growth factor receptors (R) by a hormone or mitogenic ligand (L). The biologically effective dose (BED) is receptor occupancy (RL). The dose-BED relationship is described by the hyperbolic Michaelis-Menten function, RL/Rtot = L / (L + KD). For {{the conversion of the}} dose-BED relationship to a dose-cell division relationship, the dose-BED curve has to be combined with a function that describes the distribution of susceptibilities among the cells to be triggered into mitosis. We assumed a symmetrical sigmoid curve for this function, approximated by a truncated normal distribution. Because of the supralinear dose-BED relationship due to the asymptotic saturation of the Michaelis-Menten function, the composite curve that describes cell division (incidence) as a function of dose becomes skewed to the right. Logarithmic transformation of the dose axis reverses this skewing and provides a nearly perfect fit to a normal distribution in the central 95 % incidence range. This observation may explain why dose-incidence relationships can often be described by a cumulative normal curve using the logarithm of the administered dose. The dominant role of the <b>tolerance</b> <b>distribution</b> for dose-incidence relationships is also illustrated with the example of a linear dose-BED relationship, using adducts to protein or DNA as the BED. Superimposed by a sigmoid distribution of individual susceptibilities, a sigmoid dose-incidence curve results. Linearity is no longer observed. We conclude that differences in susceptibility should always be considered for toxicological risk assessment and extrapolation to low dos...|$|E
50|$|Emiliania huxleyi is {{considered}} a ubiquitous species. It exhibits {{one of the largest}} temperature ranges (1-30oC) of any coccolithophores species. It has been observed under a range of nutrient levels from oligotrophic (subtropical gyres) to eutrophic waters (upwelling zones/ Norwegian fjords). Its presence in plankton communities from the surface to 200m depth indicates a high tolerance for both fluctuating and low light conditions. This extremely wide tolerance of environmental conditions is believed to be explained by the existence of a range of environmentally adapted ecotypes within the species. As a result of these <b>tolerances</b> its <b>distribution</b> ranges from the sub-Arctic to the sub-Antarctic and from coastal to oceanic habitats. Within this range it is present in nearly all euphotic zone water samples and accounts for 20-50% or more of the total coccolithophore community.|$|R
40|$|Abstract. The paper {{describes}} a metaobject architecture for distributed fault tolerant systems. Basically metaobject protocols enables functional objects {{to be independent}} from meta-functional properties implemented by metaobjects. Metaobjects can thus be specialised for fault <b>tolerance,</b> security, <b>distribution</b> and used on a case-by-case basic within application. The runtime support for metaobjects must include basic common services required in distributed fault tolerant computing (i. e. atomic multicast protocols and group management facilities, detection mechanisms). Offthe-shelf microkernels correspond to the very basic layer of the system. Architectural issues, application issues, development issues, experimental and performance issues are presented. Some implementation details and properties (ease of use, reusability, configurability, etc., namely flexibility) of our system are also discussed. Two prototypes have been developed today, the last one being based on the Chorus microkernel. 1...|$|R
40|$|Modern {{verification}} {{systems such}} as PVS are now reaching the stage of development where the formal verification of critical algorithms is feasible with reasonable effort. This paper describes one such verification {{in the field of}} fault <b>tolerance.</b> The <b>distribution</b> of single-source data to replicated computing channels (Interactive Consistency or Byzantine Agreement) is a central problem in this field. The classic Oral Messages (OM) algorithm solves this problem under the assumption that all channels are either nonfaulty or arbitrarily (Byzantine) faulty. Thambidurai and Park have introduced a "hybrid" fault model that distinguishes additional fault modes, along with a modified version of OM. They gave an informal proof that their algorithm withstands the same number of arbitrary faults, but more "nonmalicious" faults than OM. We detected a flaw in this algorithm while undertaking its formal verification using PVS. The discipline of mechanically-checked formal verification helped us to d [...] ...|$|R
