5|31|Public
5000|$|So the {{derivation}} of triangular {{codes of}} a function [...] consists {{in determining the}} <b>triangular</b> <b>code</b> of the partial derivative [...] and its multiplication by the known <b>triangular</b> <b>code</b> of the derivative [...] The determination of the <b>triangular</b> <b>code</b> of the partial derivative [...] {{is based on the}} correlation ...|$|E
5000|$|A <b>triangular</b> <b>code</b> {{is called}} R-nary (and is denoted as [...] ), if the numbers [...] take their values {{from the set}} ...|$|E
5000|$|For example, a <b>triangular</b> <b>code</b> is a ternary code , if , and {{quaternary}} , if [...]For R-nary triangular codes {{the following}} equalities are valid: ...|$|E
5000|$|... in R-nary <b>triangular</b> <b>codes</b> {{is based}} on using the correlation: ...|$|R
5000|$|... of R-nary <b>triangular</b> <b>codes.</b> The {{derivative}} of function , defined above, is ...|$|R
5000|$|... of R-nary <b>triangular</b> <b>codes.</b> A {{function}} {{represented by}} {{series of the}} form ...|$|R
50|$|R-nary <b>triangular</b> <b>code</b> is {{accompanied}} by a scale factor M, similar to exponent for floating-point number. Factor M permits to display all coefficients of the coded series as integer numbers. Factor M is multiplied by R at the code truncation. For addition factors M are aligned, to do so one of added codes must be truncated. For multiplication the factors M are also multiplied.|$|E
5000|$|... with integer {{coefficients}} , may {{be represented}} by R-nary triangular codes, for these coefficients and functions [...] have R-nary triangular codes (which was mentioned {{in the beginning of}} the section). On the other hand, R-nary <b>triangular</b> <b>code</b> may {{be represented by}} the said series, as any term [...] in the positional expansion of the function (corresponding to this code) may be represented by a similar series.|$|E
5000|$|... in R-nary <b>triangular</b> <b>codes</b> {{differs from}} the one-digit {{addition}} only {{by the fact that}} in the given -digit the value [...] is determined by the formula ...|$|R
5000|$|... of R-nary <b>triangular</b> <b>codes</b> {{consists}} (as in positional {{codes of}} numbers) in subsequently performed one-digit operations. Mind that the one-digit operations in all digits of each column are performed simultaneously.|$|R
5000|$|... of R-nary <b>triangular</b> <b>codes.</b> Multiplication of a code [...] by -digit {{of another}} code [...] {{consists}} in -shift {{of the code}} , i.e. its shift k columns left and m rows up. Multiplication of codes [...] and [...] consists in subsequent -shifts of the code [...] and addition of the shifted code [...] with the part-product (as in the positional codes of numbers).|$|R
50|$|<b>Triangular</b> network <b>coding</b> {{therefore}} essentially {{addresses the}} high encoding and decoding computational complexity without degrading the throughput performance, with code rate {{comparable to that}} of linear network coding.|$|R
40|$|This paper {{describes}} an algorithm for still image compression called B-tree <b>triangular</b> <b>coding</b> (BTTC). The coding scheme {{is based on}} the recursive decomposition of the image domain into right-angled triangles arranged in a binary tree. The method is attractive because of its fast encoding, O(n log n), and decoding, 2 (n), where n is the number of pixels, and because it is easy to implement and to parallelize. Experimental studies indicate that BTTC produces images of satisfactory quality from a subjective and objective point of view. One advantage of BTTC over JPEG is its shorter execution time...|$|R
40|$|Abstract—Video {{is one of}} {{the main}} causes of the {{dramatic}} increase in data traffic over cellular networks. Caching is an effective mechanism that decreases the download rate from base stations and, as a result, the load on the base station, by storing the most popular files or videos on the caches and providing them to the users. The problem of efficient content placement on the caches is known as an NP-complete problem. In this paper, we study the role of network coding by increasing the amount of available data to the users through the cache nodes. We propose a network coding-based content placement method, and we compare it to the best uncoded content placement and the best <b>triangular</b> network <b>coding</b> strategies. Our method not only increases the amount of available data to the users, but also results in a fair distribution of data. Index Terms—Linear network <b>coding,</b> <b>triangular</b> network <b>coding,</b> caching, content placement, wireless networks. I...|$|R
5000|$|... of R-nary <b>triangular</b> <b>codes.</b> This is {{the name}} of an {{operation}} of reducing the number of [...] "non"-zero columns. The necessity of truncation appears at the emergence of carries beyond the digit net. The truncation consists in division by parameter R. All coefficients of the series represented by the code are reduced R times, and the fractional parts of these coefficients are discarded. The first term of the series is also discarded. Such reduction is acceptable if it is known that the series of functions converge. Truncation consists in subsequently performed one-digit operations of division by parameter R. The one-digit operations in all the digits of a row are performed simultaneously, and the carries from lower row are discarded.|$|R
40|$|Abstract—In this paper, we {{consider}} the cascade and triangular rate-distortion problems where the same side information {{is available at the}} source node and user 1, and the side information available at user 2 is a degraded version of the side information at the source node and user 1. We characterize the rate-distortion region for these problems. For the cascade setup, we show that, at user 1, decoding and rebinning the codeword sent by the source node for user 2 is optimum. We then extend our results to the two-way cascade and triangular setting, where the source node is interested in lossy reconstruction of the side information at user 2 via a rate limited link from user 2 to the source node. We characterize the rate-distortion regions for these settings. Complete explicit characterizations for all settings are given in the quadratic Gaussian case. We conclude with two further extensions: a <b>triangular</b> source <b>coding</b> problem with a helper, and an extension of our two-way cascade setting in the quadratic Gaussian case. Index Terms—Cascade source <b>coding,</b> <b>triangular</b> source <b>coding,</b> two-way source coding, quadratic Gaussian, source coding with a helper. I...|$|R
40|$|The incompressible, Lagrangian, <b>triangular</b> grid <b>code,</b> SPLISH, was {{converted}} {{for the study}} of flows in and around fuel droplets. This involved developing, testing and incorporating algorithms for surface tension and viscosity. The major features of the Lagrangian method and the algorithms are described. Benchmarks of the algorithms are given. Several calculations are presented for kerosene droplets in air. Finally, extensions which make the code compressible and three dimensional are discussed...|$|R
5000|$|In <b>coding</b> theory, <b>triangular</b> network <b>coding</b> (TNC) is {{a network}} coding based packet coding scheme {{introduced}} by [...]Previously, packet coding for network coding was done using linear network coding (LNC). The drawback of LNC over large finite field {{is that it}} resulted in high encoding and decoding computational complexity. While linear encoding and decoding over GF(2) alleviates the concern of high computational complexity, coding over GF(2) comes at the tradeoff cost of degrading throughput performance.|$|R
40|$|Signal-to-Noise Ratio (SNR) {{scalable}} coding over priority networks {{has been}} used as a robust mechanism to transmission errors, since the most important information (base layer) can be sent over an error free channel as in the case of ATM networks or over protected channel as in the case of IP networks. SNR scalability ensures a minimum video quality at the receiver end by sacrificing the least important information (enhancement layer) when losses occur. However, one disadvantage of scalable coding is its lower compression performance with respect to the one-layer encoding (~ 10 - 15 %, [1]). In this work, we propose an error resilient scheme for MPEG- 2 SNR scalable video transmission that is compression efficient, in addition to providing scalable error resynchronization mechanism to enhancement layer that impedes the propagation of errors beyond the physically affected area in the bitstream. In order to provide robustness to errors during transmission, the proposed scheme uses <b>triangular</b> <b>coding</b> in the transform domain [2] and a modified variable end of block based on the number of bits per unit of block. Our scheme yields significantly better compression ratios compared to the MPEG- 2 standard (3 % to 6. 7 %) while at the same time improves considerably the quality of the received data (2 - 11 db) under different conditions of data losses. 1...|$|R
40|$|This {{thesis is}} a {{collection}} of ideas with the general goal of building, at least in the abstract, a local fault-tolerant quantum computer. The connection between quantum information and topology has proven to be an active area of research in several fields. The introduction of the toric code by Alexei Kitaev demonstrated the usefulness of topology for quantum memory and quantum computation. Many quantum codes used for quantum memory are modeled by spin systems on a lattice, with operators that extract syndrome information placed on vertices or faces of the lattice. It is natural to wonder whether the useful codes in such systems can be classified. This thesis presents work that leverages ideas from topology and graph theory to explore the space of such codes. Homological stabilizer codes are introduced and it is shown that, under a set of reasonable assumptions, any qubit homological stabilizer code is equivalent to either a toric code or a color code. Additionally, the toric code and the color code correspond to distinct classes of graphs. Many systems have been proposed as candidate quantum computers. It is very desirable to design quantum computing architectures with two-dimensional layouts and low complexity in parity-checking circuitry. Kitaev's surface codes provided the first example of codes satisfying this property. They provided a new route to fault tolerance with more modest overheads and thresholds approaching 1 %. The recently discovered color codes share many properties with the surface codes, such as the ability to perform syndrome extraction locally in two dimensions. Some families of color codes admit a transversal implementation of the entire Clifford group. 	 This work investigates color codes on the 4. 8. 8 lattice known as <b>triangular</b> <b>codes.</b> I develop a fault-tolerant error-correction strategy for these codes in which repeated syndrome measurements on this lattice generate a three-dimensional space-time combinatorial structure. I then develop an integer program that analyzes this structure and determines the most likely set of errors consistent with the observed syndrome values. I implement this integer program to find the threshold for depolarizing noise on small versions of these <b>triangular</b> <b>codes.</b> Because the threshold for magic-state distillation is likely to be higher than this value and because logical CNOT gates can be performed by code deformation in a single block instead of between pairs of blocks, the threshold for fault-tolerant quantum memory for these codes is also the threshold for fault-tolerant quantum computation with them. Since the advent of a threshold theorem for quantum computers much has been improved upon. Thresholds have increased, architectures have become more local, and gate sets have been simplified. The overhead for magic-state distillation has been studied, but not nearly to the extent of the aforementioned topics. A method for greatly reducing this overhead, known as reusable magic states, is studied here. While examples of reusable magic states exist for Clifford gates, I give strong reasons to believe they do not exist for non-Clifford gates. PhysicsDoctoralUniversity of New Mexico. Dept. of Physics & AstronomyCaves, CarltonLandahl, AndrewDeutsch, IvanLoring, Terr...|$|R
40|$|An {{algorithm}} {{has been}} implemented for calculation of steady solutions of the two-dimensional Euler equations using an unstructured <b>triangular</b> mesh. The <b>code</b> runs on distributed or shared memory or sequential machines, and is written using the Distributed Irregular Mesh Environment (DIME). DIME is a programming environment for calculations with such meshes, with adaptive mesh refinement and dynamic load balancing...|$|R
40|$|International audienceIn {{this paper}} we propose a low-rate coding method, suited for application-layer forward error correction. Depending on channel conditions, the coding scheme we propose can switch from a fixed-rate LDPC code to various low-rate GLDPC codes. The source symbols are ï¬rst encoded by using a {{staircase}} or <b>triangular</b> LDPC <b>code.</b> If additional symbols are needed, the encoder is then switched to the GLDPC mode and extra-repair symbols are produced, on demand. In order to ensure small overheads, we consider irregular distributions of extra-repair symbols optimized by density evolution techniques. We also show that increasing the number of extra-repair symbols improves the successful decoding probability, which becomes very close to 1 for sufficiently many extra-repair symbols...|$|R
40|$|A {{progressive}} mesh connectivity {{compression technique}} is proposed in this paper. Our method {{is based on}} the edge collapse and vertex splitting technique for progressive mesh compressiondecompression. This is an efficient and reversible method {{that can be used for}} progressive-to-lossless transmission. We derive a theoretical upper bound for the lossless connectivity compression bit rate, when the isolated vertices are ignored. Our experiments show that all the meshes we have used can be compressed better than the derived bit rate upper bound. Furthermore, our method can ensure that less than 10 % of the vertices generate accidental code. According to our analysis, the optimal bound for <b>triangular</b> mesh <b>coding</b> can be 3, smaller than some of the reported results in recent literature. 1...|$|R
40|$|We {{consider}} secure multi-terminal {{source coding}} {{problems in the}} presence of a public helper. Two main scenarios are studied: 1) source coding with a helper where the coded side information from the helper is eavesdropped by an external eavesdropper; 2) <b>triangular</b> source <b>coding</b> with a helper where the helper is considered as a public terminal. We are interested in how the helper can support the source transmission subject to a constraint on the amount of information leaked due to its public nature. We characterize the tradeoff between transmission rate, incurred distortion, and information leakage rate at the helper/eavesdropper in the form of a rate-distortion-leakage region for various classes of problems. Comment: 45 pages, 12 figures, a short version to be presented at ISIT 201...|$|R
40|$|Abstract—The Gaussian multiple-input multiple-output two-way relay {{channel is}} considered. By {{applying}} linear pre- and post-processing, the channel matrices are transformed into triangular form having equal diagonals. Over the obtained <b>triangular</b> chan-nels, dirty-paper <b>coding</b> is applied, yielding parallel symmetric scalar two-way relay channels; thus, reducing the coding task {{to that of}} coding over the scalar symmetric two-way relay channel. Any existing coding technique can then be readily applied over these resulting channels. This technique allows to obtain new achievable rates in the symmetric case. I...|$|R
40|$|Abstract—The index coding {{problem is}} a {{fundamental}} transmission problem which occurs {{in a wide range}} of multicast networks. Network coding over a large finite field size has been shown to be a theoretically efficient solution to the index coding problem. However the high computational complexity of packet encoding and decoding over a large finite field size, and its subsequent penalty on encoding and decoding throughput and higher energy cost makes it unsuitable for practical implementation in processor and energy constraint devices like mobile phones and wireless sensors. While network coding over GF(2) can alleviate these concerns, it comes at a tradeoff cost of degrading throughput performance. To address this tradeoff, we propose a throughput optimal <b>triangular</b> network <b>coding</b> scheme over GF(2). We show that such a coding scheme can supply unlimited number of innovative packets and the decoding involves the simple back substitution. Such a coding scheme provides an efficient solution to the index coding problem and its lower computation and energy cost makes it suitable for practical implementation on devices with limited processing and energy capacity. I...|$|R
40|$|We {{consider}} the Cascade and Triangular rate-distortion problems where {{the same side}} information {{is available at the}} source node and User 1, and the side information available at User 2 is a degraded version of the side information at the source node and User 1. We characterize the rate-distortion region for these problems. For the Cascade setup, we showed that, at User 1, decoding and re-binning the codeword sent by the source node for User 2 is optimum. We then extend our results to the Two way Cascade and Triangular setting, where the source node is interested in lossy reconstruction of the side information at User 2 via a rate limited link from User 2 to the source node. We characterize the rate distortion regions for these settings. Complete explicit characterizations for all settings are also given in the Quadratic Gaussian case. We conclude with two further extensions: A <b>triangular</b> source <b>coding</b> problem with a helper, and an extension of our Two Way Cascade setting in the Quadratic Gaussian case. Comment: 29 pages, 9 figure...|$|R
40|$|The index coding {{problem is}} a {{fundamental}} transmission problem which occurs {{in a wide range}} of multicast networks. Network coding over a large finite field size has been shown to be a theoretically efficient solution to the index coding problem. However the high computational complexity of packet encoding and decoding over a large finite field size, and its subsequent penalty on encoding and decoding throughput and higher energy cost makes it unsuitable for practical implementation in processor and energy constraint devices like mobile phones and wireless sensors. While network coding over GF(2) can alleviate these concerns, it comes at a tradeoff cost of degrading throughput performance. To address this tradeoff, we propose a throughput optimal <b>triangular</b> network <b>coding</b> scheme over GF(2). We show that such a coding scheme can supply unlimited number of innovative packets and the decoding involves the simple back substitution. Such a coding scheme provides an efficient solution to the index coding problem and its lower computation and energy cost makes it suitable for practical implementation on devices with limited processing and energy capacity...|$|R
40|$|Abstract. This paper {{analyzes}} the cache efficiency of two high-performance sparse Cholesky factorization algorithms: the multifrontal algorithm and the left-looking algorithm. These two are essentially {{the only two}} algorithms {{that are used in}} current codes; generalizations of these algorithms are used in general-symmetric and generalunsymmetric sparse <b>triangular</b> factorization <b>codes.</b> Our theoretical analysis shows that while both algorithms sometimes enjoy a high level of data reuse in the cache, they are incomparable: there are matrices on which one is cache efficient and the other is not, and vice versa. The theoretical analysis is backed up by detailed experimental evidence, which shows that our theoretical analyses do predict cache-miss rates and performance in practice, even though the theory uses a fairly simple cache model. We also show, experimentally, that on matrices arising from finite-element structural analysis, the left-looking algorithm consistently outperforms the multifrontal algorithm. Direct cache-miss measurements indicate that the difference in performance is largely due to differences in the number of level- 2 cache misses that the two algorithms generate. Finally, we also show that there are matrices where the multifrontal algorithm may require significantly more memory than the left-looking algorithm. On the other hand, the left-looking algorithm never uses more memory than the multifrontal one. 1...|$|R
40|$|The topological {{color code}} and the toric code are two leading {{candidates}} for realizing fault-tolerant quantum computation. Here {{we show that}} the color code on a d-dimensional closed manifold is equivalent to multiple decoupled copies of the d-dimensional toric code up to local unitary transformations and adding or removing ancilla qubits. Our result not only generalizes the proven equivalence for d = 2, but also provides an explicit recipe of how to decouple independent components of the color code, highlighting the importance of colorability {{in the construction of}} the code. Moreover, for the d-dimensional color code with d + 1 boundaries of d + 1 distinct colors, we find that the code is equivalent to multiple copies of the d-dimensional toric code which are attached along a (d - 1) -dimensional boundary. In particular, for d = 2, we show that the (<b>triangular)</b> color <b>code</b> with boundaries is equivalent to the (folded) toric code with boundaries. We also find that the d-dimensional toric code admits logical non-Pauli gates from the dth level of the Clifford hierarchy, and thus saturates the bound by Bravyi and König. In particular, we show that the logical d-qubit control-Z gate can be fault-tolerantly implemented on the stack of d copies of the toric code by a local unitary transformation...|$|R
40|$|The {{entanglement}} {{properties of}} a class of topological stabilizer states, the so called topological color codes defined on a two-dimensional lattice or 2 -colex, are calculated. The topological entropy is {{used to measure the}} entanglement of different bipartitions of the 2 -colex. The dependency of the ground state degeneracy on the genus of the surface shows that the color code can support a topological order, and the contribution of the color in its structure makes it interesting to compare with the Kitaev’s toric code. While a qubit is maximally entangled with rest of the system, two qubits are no longer entangled showing that the color code is genuinely multipartite entangled. For a convex region, it is found that entanglement entropy depends only on the degrees of freedom living on the boundary of two subsystems. The boundary scaling of entropy is supplemented with a topological subleading term which for a color code defined on a compact surface is twice than the toric code. From the entanglement entropy we construct a set of bipartitions in which the diverging term arising from the boundary term is washed out, and the remaining non-vanishing term will have a topological nature. Besides the color code on the compact surface, we also analyze the entanglement properties of a version of color code with border, i. e <b>triangular</b> color <b>code.</b> PACS numbers: 03. 67. Lx, 03. 67. Mn, 03. 65. Ud, 42. 50. Dv I...|$|R
40|$|A two-step model able {{to predict}} the {{non-linear}} response of FRP strengthened threedimensional masonry structures is presented. In the first step, non-strengthened masonry is substituted by a macroscopically equivalent homogeneous material through a kinematic model based on finite elements and working on a heterogeneous assemblage of blocks. Non-linearity is concentrated exclusively on joints reduced to interfaces, exhibiting a frictional behaviour with limited tensile and compressive strength with softening. The homogenized stress-strain behaviour evaluated at the meso-scale is then implemented at a structural level in a finite element non-linear code, relying on an assemblage of rigid infinitely resistant six-noded wedge elements and non-linear interfaces, with deterioration of the mechanical properties. FRP reinforcing strips are modelled through rigid triangles and non-linear interfaces between adjoining triangles. Delamination from the support is accounted for, by modelling FRP-masonry bond by means of non-linear softening <b>triangular</b> interfaces. Italian <b>code</b> CNR DT 200 0 formulas are used to evaluate peak interface tangential strength and post peak behaviour. A structural examples relying into a masonry deep beam is presented for validation purposes...|$|R
40|$|A {{suitable}} {{and simple}} two-step model {{able to predict}} the non-linear response of FRP strengthened 13 three-dimensional masonry structures is presented. In the first step, non-strengthened masonry is 14 substituted by a macroscopically equivalent homogeneous material through a kinematic model 15 based on finite elements and working on a heterogeneous assemblage of blocks. Non-linearity is 16 concentrated exclusively on joints reduced to interfaces, exhibiting a frictional behavior with 17 limited tensile and compressive strength with softening. The homogenized stress-strain behavior 18 evaluated at the meso-scale is then implemented at a structural level in a finite element non-linear 19 code, relying on an assemblage of rigid infinitely resistant six-noded wedge elements and non-linear 20 interfaces, exhibiting deterioration of the mechanical properties. FRP reinforcing strips are modeled 21 through rigid triangles and non-linear interfaces between adjoining triangles. Delamination from the 22 support is accounted for, by modeling FRP-masonry bond by means of non-linear softening 23 <b>triangular</b> interfaces. Italian <b>code</b> CNR DT 200 (2004) formulas are used to evaluate peak interface 24 tangential strength and post peak behavior. In this first part, the theoretical base of the model and 25 the non-linear stress strain behavior at a cell level are discussed. Structural examples will be 26 analyzed in the accompanying paper devoted to the structural scale...|$|R
40|$|With the {{advancement}} of computer graphics in the recent years, {{an increasing number of}} pictures, video and 3 D content is generated by synthesis processing rather than acquired with capture devices such as cameras or scanners. Several techniques have been developed for compression of discrete (i. e. piece-wise planar) 3 D models, in the form of 3 D polygonal meshes. However, no important attempt has been made to compress the smooth surfaces of artificially generated 3 D models, that are most often represented as parametric surfaces, of which Non-Uniform Rational B-Spline (NURBS) is a popular form. This paper presents a method for compressing NURBS 3 D models with a small and controllable loss. The scheme uses a differential pulse coded modulation (DPCM) coder with different predictors for knot values and control points, coupled with a uniform scalar quantizer, followed by a bitplane arithmetic entropy coder. The multiplicity of knots is preserved by the use of a multiplicity map. The rate-distortion characteristics of the proposed scheme are evaluated on various models. When compared to MPEG- 4 [8, 9] and Touma-Gotsman [19] compressed triangular meshes, the proposed scheme achieves more than five times better compression, for equivalent L 2 error and much better visual quality. Key words: B-Spline, NURBS, compression, 3 D model, <b>coding,</b> <b>triangular</b> mes...|$|R
40|$|The 3 -D flow {{modelling}} {{of groundwater}} systems of realistic size generally requires a big effort for {{the preparation of}} the input data as well as large computational costs. A numerical finite element model (MAITHREE) is developed for the efficient analysis of the steady and unsteady behaviour of natural confined 3 -D basins. Starting from an initial <b>triangular</b> grid the <b>code</b> automatically generates a set of tetrahedral elements in each of the geologic units or subunits specified by the user in the vertical profile. The original element incidences list is then rearranged in order to provide conforming 3 -D elements throughout the domain. The model is designed with a view to saving much of the labour involved in setting a 3 -D grid and to providing flexibility as well as economical convenience through a high computational efficiency. The latter task is achieved by the aid of a solver based on the modified conjugate gradient (MCG) method which has proved to be an excellent technique for the solution of large linear finite element sets of sparse 3 -D subsurface equations. Some examples derived from both hypothetical and real-world situations are discussed to illustrate the innovative features of MAITHREE and its computational performance...|$|R

