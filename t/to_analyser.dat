2|335|Public
40|$|The {{study area}} {{is located in}} Kota Kinabalu area which is along Inanam to Penampang {{covering}} an area approximately about 100 kilometre square and bounded by the latitude 05 ° 53 'N to 05 ° 57 'N and longitude 116 ° 05 'E to 116 ° 11 'E. The objectives of this research are to study the geologicabl ackgrounda nd producel atest geologicalm ap, <b>to</b> <b>analyser</b> ock goemechanicsc haracterizationin perspectiveo f rock aggregatet ests on study area and to assess the suitability of rocks in study area as potential construction materials. The study consists of the Crocker Formation and the Quartemary Alluvium. The Crocker Formation aged from Late Eocene to Early Miocene consists of four rock units include grey shale, red shale, thick sandstone and interbedded sandstone with shale. From petrographic analysis, quartz is the dominant mineral in sandstone that is 58...|$|E
40|$|Measurement of {{nitrogen}} dioxide using passive diffusion tube over 22 months in Cambridge, U. K. are analysed {{as a function}} of sampler exposure time, and compared with NO 2 concentrations obtained from a co-located chemiluminescence analyser. The average ratios of passive sampler <b>to</b> <b>analyser</b> NO 2 at a city centre site (mean NO 2 concentration 22 ppb) are 1. 27 (n = 22), 1. 16 (n = 34) and 1. 11 (n = 7) for exposures of 1, 2 and 4 -weeks, respectively. Modelling the generation of extra NO 2 arising from chemical reaction between co-diffusing NO and O 3 in the tube gave a ratio (modelled/measured) of 1. 31 for 1 -week exposures. Such overestimation is greatest when NO 2 constitutes, on average, about half of total NOx (= NO + NO 2) at the monitoring locality. Although 4 - week exposures gave concentrations which were not significantly different from analyser NO 2, there was no correlation between the datasets. At both the city-centre site and another semi-rural site (mean NO 2 concentration 11 ppb) the average of the aggregate of four consecutive 1 -week sampler exposures or of two consecutive 2 -week sampler exposures was systematically greater than for a single 4 -week exposure. The results indicate two independent and opposing systematic biases in measurement of NO 2 by passive diffusion sampler: an exposure-time independent chemical overestimation with magnitude determined by local relative concentrations of NO and O 3 to NO 2, and an exposuretime dependent reduction in sampling efficiency. The impact of these and other potential sources of systematic bias on the application of passive diffusion tubes for assessing ambient concentrations of NO 2 in short (1 -week) or long (4 -week) exposures are discussed in detail...|$|E
40|$|When a {{short-lived}} signal is recorded by a transient recorder which then relays it <b>to</b> a spectrum <b>analyser</b> <b>to</b> resolve the frequency components of that signal, distortions may occur which suggest {{the presence of}} spurious resonances in the spectrum. The source and nature of these distortions are discussed here...|$|R
5000|$|The {{pressure}} {{of a sample}} stream can be lowered by a pressure reducing valve, particularly since many analysers are not designed to withstand high pressure. Such pressure reducing or similar valves {{may be used to}} control the flow rate <b>to</b> the online <b>analyser.</b> If the process pressure is insufficient to allow a sample stream to flow by itself <b>to</b> the <b>analyser,</b> a small pump may be used to move it there. The temperature of a hot sample may be lowered by use of an online sample cooler. The sampling and analysis can be done at the command of an operator, periodically (for example, every 15 minutes), or continuously - providing an analysis result vs. time graph on a chart recorder, computer, or other device. For periodic sampling, valves (or other devices) can be switched open to allow a fluid sample stream to flow <b>to</b> the <b>analyser</b> and shut when not sampling.|$|R
40|$|We {{report on}} {{plans for a}} {{multiplexed}} neutron analyser option for the PANDA spectrometer. The key design concept is <b>to</b> have many <b>analysers</b> positioned <b>to</b> give a large coverage in the scattering plane, and multiple arcs of these <b>analysers</b> <b>to</b> measure different energy transfers simultaneously. The main goal is to bring intensity gains and improved reciprocal-space and energy mapping capabilities to the existing cold triple-axis spectrometer...|$|R
50|$|Alexis Spectral Data can export the {{trichromatic}} values, {{calculated from}} the spectral curves, <b>to</b> Alexis <b>Analyser,</b> software that handles only trichromatic data. The earliest information {{about the development of}} this software comes from a paper published by a student at the University Politehnica Bucharest in 1993.|$|R
50|$|Antibodies {{are used}} by some <b>analysers</b> <b>to</b> detect many {{substances}} by immunoassay and other reactions that employ the use of antibody-antigen reactions.|$|R
5000|$|Ball-and-disk integrator, 1886 - William Thomson {{used it in}} his Harmonic <b>Analyser</b> <b>to</b> measure tide heights by calculating {{coefficients}} of a Fourier series.|$|R
50|$|The {{implementation}} is in Perl. A programmer supplied action file is {{input to}} FleXML; the output is a file suitable for input <b>to</b> Flex lexical <b>analyser.</b>|$|R
50|$|The subject can exhale {{directly}} into a measurement device ('online' technique), or into a reservoir that can afterwards be connected <b>to</b> the <b>analyser</b> ('offline' technique). With the former technique, the early and later NO in the breath sample can be analysed separately. The test requires little coordination from the subject, and children older than 4 can be tested successfully.|$|R
40|$|International audienceIn-situ, time-resolved FTIR {{spectroscopy}} {{along with}} gravimetric analysis {{have been used}} to investigate water sorption and transport in several glassy polymeric matrices, characterized by different levels of interaction with water, as well as on polymer-silica hybrids. This technique has also been coupled <b>to</b> a dynamical-mechanical <b>analyser</b> <b>to</b> gather information on the water sorption kinetics and thermodynamics in a polymer sample submitted to stretching deformation and load. Results have been modelled by coupling the mass balance and momentum balance, using a theoretical approach developed for elastic matrices and low sorbed amounts by Larché and Cahn...|$|R
5000|$|... #Caption: Laboratory {{equipment}} for determination of γ-radiation spectrum with a scintillation counter. The output from the scintillation counter goes <b>to</b> a Multichannel <b>Analyser</b> which processes and formats the data.|$|R
50|$|Face mask (breath by breath): Indirect {{calorimetry}} {{tests are}} also often performed {{with a face}} mask, {{which is used to}} convey exhaled and inhaled gas through a turbine flowmeter able to measure the patient’s breath by breath minute ventilation, {{at the same time a}} sample of gas is conveyed <b>to</b> the <b>analyser</b> and VO2 and VCO2 are measured and converted in energy expenditure.|$|R
40|$|Beside general {{requirements}} for modern automated systems, immunoassay automation involves specific requirements as a separation step for heterogeneous immunoassays. Systems are designed {{according to the}} solid phase selected: dedicated or open robots for coated tubes and wells, systems nearly similar <b>to</b> chemistry <b>analysers</b> {{in the case of}} magnetic particles, and a completely original design for those using porous and film materials...|$|R
40|$|On {{behalf of}} the ATLAS Collaboration Abstract: The ATLAS {{experiment}} is taking data steadily since Autumn 2009, and collected so far over 5 fb- 1 of data (several petabytes of raw and reconstructed data per year of data-taking). Data are calibrated, reconstructed, distributed and analysed at over 100 different sites using the World-wide LHC Computing Grid and the tools produced by the ATLAS Distributed Computing project. In addition to event data, ATLAS produces {{a wealth of information}} on detector status, luminosity, calibrations, alignments, and data processing conditions. This information is stored in relational databases, online and offline, and made transparently available <b>to</b> <b>analysers</b> of ATLAS data world-wide through an infrastructure consisting of distributed database replicas and web servers that exploit caching technologies. This paper reports on the experience of using this distributed computing infrastructure with real data and in real time, on the evolution of the computing model driven by this experience, and on the system performance during {{the first two years of}} operation...|$|R
40|$|The ATLAS {{experiment}} is taking data steadily since Autumn 2009, and collected so far over 5 fb- 1 of data (several petabytes of raw and reconstructed data per year of data-taking). Data are calibrated, reconstructed, distributed and analysed at over 100 different sites using the World-wide LHC Computing Grid and the tools {{produced by the}} ATLAS Distributed Computing project. In addition to event data, ATLAS produces {{a wealth of information}} on detector status, luminosity, calibrations, alignments, and data processing conditions. This information is stored in relational databases, online and offline, and made transparently available <b>to</b> <b>analysers</b> of ATLAS data world-wide through an infrastructure consisting of distributed database replicas and web servers that exploit caching technologies. This paper reports on the experience of using this distributed computing infrastructure with real data and in real time, on the evolution of the computing model driven by this experience, and on the system performance during {{the first two years of}} operation...|$|R
40|$|This is {{a thesis}} {{study of the}} design {{considerations}} involved in interfacing medical <b>analysers</b> <b>to</b> computer systems for remote monitoring. Most medical analysers provide a serial port connection for interfacing to a computer. Due to the limitations of this physical link, {{it is necessary for}} the interfacing computer to be within a radius of several metres. This computer can perform all the data monitoring and processing of the medical analyser. Monitoring the operational performance of a medical analyser can assist in maintenance programs. If it relays relevant data to a remote computer, a powerful remote maintenance and monitoring system can be developed when the remote monitoring computer collects data from a number of remote medical <b>analysers.</b> <b>To</b> satisfy the demands of remote maintenance and monitoring the Transmission Control Protocol/Internet Protocol (TCP/P) network protocol was investigated. The existing worldwide base of support for this protocol on numerous platforms and its universal addressing scheme made it the preferred choice for “openness” concerns. The relative merits of User Datagram Protocol (UDP) and Transmission Control Protocol (TCP) were evaluated {{and the nature of the}} reliable stream service offered by TCP was selected as more appropriate to the medical environment where data loss is unacceptable. A Client/Server architecture was investigated with a central server and remote clients which were connected <b>to</b> clinical <b>analysers.</b> The monitoring computer local <b>to</b> an <b>analyser</b> connects <b>to</b> the server using the TCP/IP network protocol. This Client-Server configuration is particularly suited to the distributed nature of medical laboratories where instruments are typically not centralised and the lack of availability of powerful computers makes it necessary to resort to using simple computers locally to the instrument to relay data to the more powerful server computer. The work was carried out with the co-operation of the Central Pathology Laboratory of Saint James’s hospital in Dublin...|$|R
50|$|Since {{electron}} spectroscopy detects several physical phenomena {{from the}} electrons emitted from samples, {{it is necessary}} to transport the electrons <b>to</b> the electron <b>analyser.</b> Electrostatic lenses satisfy the general properties of lenses.|$|R
40|$|This paper {{focuses on}} the {{inference}} of modes for which a logic program is guaranteed to terminate. This generalises traditional termination analysis where an <b>analyser</b> tries <b>to</b> verify termination for a specified mode. Our contribution is a methodology in which components of traditional termination analysis are combined with backwards analysis <b>to</b> obtain an <b>analyser</b> for termination inference. We identify a condition on {{the components of the}} analyser which guarantees that termination inference will infer all modes which can be checked to terminate. The application of this methodology to enhance a traditional termination <b>analyser</b> <b>to</b> perform also termination inference is demonstrated. ...|$|R
30|$|As expected, {{monitors}} ship {{data to the}} AaaS cloud, {{which is}} synchronised and merged therein. A monitor merger service is assumed to merge the monitored data and send it <b>to</b> the <b>analyser</b> service. The latter behaves exactly as before. The particularity is that it now evokes services for the necessary quantitative analysis. It is still responsible for triggering {{the need for a}} reconfiguration by analysis of the user-uploaded constraints.|$|R
40|$|The ATLAS {{experiment}} at the LHC collider recorded {{more than}} 5 ~fb$^{- 1 }$ data of $pp$ collisions at a centre-of-mass energy of 7 ~TeV during 2011. The recorded data are promptly reconstructed in two steps {{at a large}} computing farm at CERN to provide fast access to high quality data for physics analysis. In the first step, {{a subset of the}} data corresponding to 10 ~Hz is processed in parallel with data taking. Data quality, detector calibration constants, and the beam spot position are determined using the reconstructed data within 48 hours. In the second step all recorded data are processed with the updated parameters. The LHC significantly increased the instantaneous luminosity and the number of interactions per bunch crossing in 2011; the data recording rate by ATLAS exceeds 400 ~Hz. To cope with these challenges the performance and reliability of the ATLAS reconstruction software have been improved. In this paper we describe how the prompt data reconstruction system quickly and stably provides high quality data <b>to</b> <b>analysers...</b>|$|R
25|$|As {{an intern}} at Bell Labs, Schmidt did a {{complete}} re-write of Lex, {{a software program}} <b>to</b> generate lexical <b>analysers</b> for the UNIX computer operating system. From 1997 to 2001, he was Chief Executive Officer (CEO) of Novell.|$|R
5000|$|... #Caption: The {{experimental}} {{setup for}} determination of γ-radiation spectrum with a scintillation counter. A {{high voltage power supply}} {{is connected to}} the scintillation counter. The scintillation counter is connected <b>to</b> the Multichannel <b>Analyser</b> which sends information to the computer.|$|R
50|$|He {{spent the}} rest of the decade {{applying}} the differential <b>analyser</b> <b>to</b> find solutions of differential equations arising in physics. These included control theory and laminar boundary layer theory in fluid dynamics making significant contributions to each of the fields.|$|R
40|$|We examine {{standard}} deep lexical acquisition {{features in}} automatically predicting {{the gender of}} noun types and tokens by bootstrapping from a small annotated corpus. Using a knowledge-poor approach to simulate prediction in unseen languages, we observe results comparable <b>to</b> morphological <b>analysers</b> trained specifically on our target languages of German and French. These results describe further scope in analysing other properties in languages displaying a more challenging morphosyntax, {{in order to create}} language resources in a language-independent manner. ...|$|R
5000|$|Many {{methods of}} {{introducing}} samples into the analyser have been invented. This can involve placing test tubes of sample into racks, {{which can be}} moved along a track, or inserting tubes into circular carousels that rotate to make the sample available. Some <b>analysers</b> require samples <b>to</b> be transferred to sample cups. However, the effort to protect {{the health and safety}} of laboratory staff has prompted many manufacturers <b>to</b> develop <b>analysers</b> that feature closed tube sampling, preventing workers from direct exposure to samples., ...|$|R
40|$|The CAMEA ESS neutron {{spectrometer}} {{is designed to}} achieve a high detection efficiency in the horizontal scattering plane, and to maximize {{the use of the}} long pulse European Spallation Source. It is an indirect geometry time-of-flight spectrometer that uses crystal <b>analysers</b> <b>to</b> determine the final energy of neutrons scattered from the sample. Unlike other indirect gemeotry spectrometers CAMEA will use ten concentric arcs of <b>analysers</b> <b>to</b> analyse scattered neutrons at ten different final energies, which can be increased to 30 final energies by use of prismatic analysis. In this report we will outline the CAMEA instrument concept, the large performance gain, and the potential scientific advancements that can be made with this instrument. Comment: 7 pages, 3 figures, 3 table...|$|R
40|$|The {{purpose of}} this paper is to examine the {{differences}} between “pure” and “mixed” marketing strategies in terms of implementation practices and performance. The strategies compared use the Miles and Snow (1978) typology to develop Pure Prospectors, Pure Defenders, Reactors and Mixed strategies the latter strategy type being similar <b>to</b> <b>Analysers.</b> Previous strategy type implementation research has used debatable strategy classification methodologies and has not isolated “pure” marketing strategies. The {{purpose of this}} paper is to clearly identify and separate “pure” marketing strategies from “mixed” strategies. In terms of strategy implementation a key finding was that the Miles and Snow implementation recommendations made in the 1970 ’s no longer appear to be appropriate in the 2000 ’s This appears to be the case because no need to align human resource practices and organisational structure with strategy was apparent in our findings. In terms of strategy performance differences a key finding was that Pure Prospectors outperformed Reactor strategies in terms of new markets, sales growth, new products and market share. However, the financial costs of investing in new markets and new products undertaken by the aggressive Pure Prospector strategies results in only similar ROI performance to other strategies. This is consistent with the concept of performance equifinality. <br /...|$|R
50|$|In 2012, OAG {{launched}} OAG <b>Analyser</b> <b>to</b> deliver airline schedule analysis via {{an online}} accessible tool. In 2013, OAG added to its analytical suite with {{the launch of}} Traffic Analyser, a product developed in partnership with Travelport; a leading distribution services and e-commerce provider for the global travel industry.|$|R
40|$|A Spanish {{hazardous}} waste from tertiary aluminium industry {{was used as}} a raw material for the synthesis of calcium aluminate. An amorphous precursor was obtained by a hydrothermal method at different values of pH. The transformation of the precursor in a crystalline aluminate was followed by TG/DTA up to 1300 ºC. At temperatures between 719 and 744 ºC, the precursors evolve towards the formation of C 12 A 7 which becomes CA at circa 1016 ºC. Mass spectrometry coupled <b>to</b> thermal <b>analyser</b> allowed the identification of the decomposition products...|$|R
40|$|We {{recently}} {{demonstrated a}} new method to efficiently analyse the {{orbital angular momentum}} (OAM) states of light by application of an optical geometric transformation (Berkhout et al 2010 Phys. Rev. Lett. 105 153601). Here we study the performance of such a system to measure {{the change in the}} observed OAM spectrum, as the input beam is misaligned with respect <b>to</b> the <b>analyser.</b> We present modelled and experimental results which show that our reformatting approach does correctly measure the OAM spectrum for lateral and tilt misalignment of the input beam...|$|R
40|$|The {{purpose of}} this paper is to {{advocate}} and encourage the application of fuzzy-neuro algorithms in the risk assessment of Distributed Real-Time (DRT) systems, where object-oriented and formal techniques by means of UML-SDL models are used as knowledge (data) providers <b>to</b> the fuzzy-neuro <b>analyser.</b> The paper includes an introductory description of DRT systems and the presentation of the UML and SDL techniques as software modelling languages, along with their advantages and limitations. Next, a combination of UML and SDL for requirement and detailed design modelling is suggested to produce the proper knowledge bases <b>to</b> a risk <b>analyser.</b> An example of a fuzzy-neuro system is then utilised to illustrate how it can be trained to assess various kinds of risks by using the combined UML-SDL models as input and through deductive reasoning...|$|R
40|$|The {{requirements}} for accurate measurements of thermodynamic properties of polymer surfaces using gas chromatography are detailed. - 0. " Modifications to an existing chromatograph and multichannel <b>analyser</b> <b>to</b> allow digital acquisition {{and analysis of}} data are described. Preliminary results demonstrate {{the utility of the}} resulting system. Approved for Public Releas...|$|R
40|$|Abstract. This paper {{describes}} {{the first attempt}} <b>to</b> evaluate morphological <b>analysers</b> for Portuguese with an evaluation contest. It emphasizes the options {{that had to be}} taken and that highlight considerable disagreement among the participating groups. It {{describes the}} trial intended to prepare the real contest in June 2003, its goals and preliminary results. ...|$|R
40|$|Abstract. The {{modeling}} {{and analysis of}} hybrid systems is a recent and challenging research area which is actually dominated by two main lines: a functional analysis based on {{the description of the}} system in terms of discrete state (hybrid) automata (whose goal is to ascertain for confor-mity and reachability properties), and a stochastic analysis (whose aim is to provide performance and dependability measures). This paper inves-tigates a unifying view between formal methods and stochastic methods by proposing an analysis methodology of hybrid systems based on Fluid Petri Nets (FPN). It is shown that the same FPN model can be fed <b>to</b> a functional <b>analyser</b> for model checking as well as <b>to</b> a stochastic <b>analyser</b> for performance evaluation. We illustrate our approach and show its use-fulness by applying it to a “real world ” hybrid system: the temperature control system of a co-generative plant. ...|$|R
40|$|We {{describe}} {{in this paper}} a semi-automatic acquisition of morphological rules for morphological analyser {{in the case of}} under-resourced language, which is Iban language. We modify ideas from previous automatic morphological rules acquisition approaches, where the input requirements has become constraints <b>to</b> develop the <b>analyser</b> for under-resourced language. This work introduces three main steps in acquiring the rules from the under-resourced language, which are morphological data acquisition, morphological information validation and morphological rules extraction. The experiment shows that this approach gives successful results with 0. 76 of precision and 0. 99 of recall. Our findings also suggest that the availability of linguistic references and the selection of assorted techniques for morphology analysis could lead to the design of the workflow. We believe this workflow will assist other researchers <b>to</b> build morphological <b>analyser</b> with the validated morphological rules for the under-resourced languages...|$|R
